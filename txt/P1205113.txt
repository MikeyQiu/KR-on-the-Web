CNNdroid: GPU-Accelerated Execution of Trained
Deep Convolutional Neural Networks on Android

∗
Seyyed Salar Latiﬁ Oskouei, Hossein Golestani, Matin Hashemi
Sharif University of Technology
salarlatiﬁ@ee.sharif.edu, hossein_golestani@ee.sharif.edu, matin@sharif.edu
Soheil Ghiasi
University of California, Davis
ghiasi@ucdavis.edu

6
1
0
2
 
t
c
O
 
5
1
 
 
]

C
D
.
s
c
[
 
 
2
v
6
7
3
7
0
.
1
1
5
1
:
v
i
X
r
a

ABSTRACT
Many mobile applications running on smartphones and wear-
able devices would potentially beneﬁt from the accuracy and
scalability of deep CNN-based machine learning algorithms.
However, performance and energy consumption limitations
make the execution of such computationally intensive algo-
rithms on mobile devices prohibitive. We present a GPU-
accelerated library, dubbed CNNdroid [1], for execution of
trained deep CNNs on Android-based mobile devices. Em-
pirical evaluations show that CNNdroid achieves up to 60X
speedup and 130X energy saving on current mobile devices.
The CNNdroid open source library is available for download
at https://github.com/ENCP/CNNdroid

Keywords
Deep Learning, Deep Convolutional Neural Network (CNN),
Mobile GPU, Performance Optimization, Low Energy Con-
sumption, Open Source Software, Android, RenderScript

1.

INTRODUCTION

Mobile platforms such as smartphones, wearable devices,
tiny autonomous robots and IoT devices have been increas-
ingly ﬁnding their way into many areas (Figure 1). Nu-
merous applications, such as speech recognition and image
recognition [2], would potentially beneﬁt from local execu-
tion of accurate machine learning algorithms on mobile de-
vices. Local execution allows data to stay on the mobile
device and hence avoids latency issues of cloud-assisted pro-
cessing.

Deep CNNs can achieve state-of-the-art results in terms of
both prediction accuracy and scalability. However, they are
highly computationally intensive, and hence, not practical
on current mobile devices without acceleration.
∗Contact author: Matin Hashemi, matin@sharif.edu

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
MM ’16, October 15 - 19, 2016, Amsterdam, Netherlands
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-3603-1/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2964284.2973801

Figure 1: Example applications of deep CNNs in mobile systems.
Image credits: IBM Research, Guardianlv, Nixie, Android Wear.

Many hardware-based solutions have been proposed for
acceleration of deep CNNs [3, 4]. IBM has also introduced a
neuromorphic CMOS chip for execution of learning applica-
tions on smartphones and IoT devices [5]. While promising,
such solutions are still in early stages of development and
not available on current mobile devices.

As opposed to hardware-based engines, GPU already ex-
ists in many current mobile devices and can be programmed
completely in software. Therefore, parallel processing capa-
bilities of mobile GPUs can be exploited to accelerate deep
CNN computations on current mobile devices.

On server and desktop platforms, there exists many GPU-
accelerated deep CNN libraries [6, 7, 8, 9, 10, 11, 12]. How-
ever, because of architecture diﬀerences (Section 2.1), mere
porting of such libraries to mobile platforms yields sub-
optimal performance or is impossible in some cases (Section
2.2).

On mobile platforms, to the best of our knowledge, such
GPU-accelerated libraries are not available. The few ex-
isting mobile libraries for CNN computations [13, 14, 15,
16] are limited to the processing power of multi-core mobile
CPUs (Section 2.3).

We present an open source GPU-accelerated library, dubbed

CNNdroid, which is speciﬁcally designed and optimized for
execution of trained deep CNNs on Android-based mobile
devices. The main highlights of CNNdroid are as follows.

1. Support for nearly all CNN layer types (Section 3.1).

2. Compatible with CNN models trained by common desk-
top/server libraries, namely, Caﬀe [6], Torch [7] and
Theano [8] (Section 3.2).

3. Easy to conﬁgure and integrate into any Android app
without additional software requirements (Section 3.3).

4. User-speciﬁed maximum memory usage (Section 3.4).

5. GPU or CPU acceleration of supported CNN layers

(Section 3.5).

6. Automatic tuning of performance (Section 3.6).

7. Up to 60X speedup and up to 130X energy saving on

current mobile devices (Section 4).

2. BACKGROUND AND RELATED WORK

2.1 Comparing Mobile and Desktop GPUs

A modern graphics processing unit (GPU), in addition to
computer graphics, can be programmed for general purpose
computations as well. While desktop GPUs have long been
programmable, major mobile chip manufacturers have re-
cently made the GPU hardware available for general purpose
computations. Due to strict area and power constraints,
mobile GPUs have important diﬀerences with their desktop
counterparts.

A modern mobile GPU is typically composed of several
programmable parallel computing units called Shader Cores
(SC). Every shader core is composed of several parallel ALUs.
For example, Samsung Exynos 5433 chip is composed of
ARM A53/A57 CPU and Mali T-760 GPU (Figure 2). Each
SC in T-760 GPU has two 128-bit ALUs in VLIW format.
Each 128-bit ALU is capable of performing SIMD opera-
tions, i.e., two 64-bit, four 32-bit or eight 16-bit operations in
parallel [17]. In comparison with desktop GPUs, the above
parallel ALU architecture relies more on software and com-
piler than dynamic hardware scheduler in eﬃcient execution
of parallel threads.

More importantly, fast shared memory in thread blocks
which are present in desktop GPUs and widely employed
in many CUDA-based desktop libraries are not available in
mobile GPUs.

There are some diﬀerences on the software side as well.
For example in RenderScript, Android’s parallel computing
platform [18], thread synchronization is not available. In ad-
dition, there must be a one-to-one correspondence between
parallel threads and the data items inside one of the memory
buﬀers that parallel threads work on.

2.2 Comparing CNNdroid with Desktop

Libraries

On server and desktop platforms, there exists many li-
braries such as Caﬀe [6], Torch [7], Theano [8], Tensor-
Flow [9], cuDNN [10], cuda-convnet [11], and Velesnet [12],
which employ GPU-based parallel processing for accelera-
tion of deep CNN computations. However, the acceleration
methodologies and parallel algorithms of such libraries could
not be directly utilized in mobile platforms due to the exist-
ing hardware and software diﬀerences.

For example in Caﬀe [6], the convolution operation is un-
rolled and converted to matrix multiplication, which requires

Figure 2: Example: Exynos 5433 mobile processor with ARM
A53 / A57 CPU and Mali T-760 GPU (SC: Shader Core, VLIW:
Very Long Instruction Word, SIMD: Single Instruction Multiple
Data).

considerable amount of memory and therefore is not suitable
for mobile devices with small cache and memory sizes. As
another example, the parallel algorithm in Theano [8] is sim-
ilar to CNNdroid but without eﬃcient use of SIMD units in
mobile GPUs. Refer to Section 3.5 for details.

More importantly, desktop libraries take advantage of thread

management facilities provided by desktop GPUs and CUDA
framework, such as fast shared memory and thread synchro-
nization, which are not available in mobile GPUs and Ren-
derScript.

2.3 Comparing CNNdroid with Mobile

Libraries

On mobile platforms, to the best of our knowledge, only
few deep CNN libraries exist [13, 14, 15, 16]. All such li-
braries, including Caﬀe Mobile [13] and Torch Mobile [14],
are limited to the processing power of multi-core mobile
CPUs, while CNNdroid eﬃciently employs both GPU and
CPU (Section 3.5).

In addition, CNNdroid is compatible with CNN models
trained by Caﬀe [6], Torch [7] and Theano [8], which facil-
itates the process of porting the trained models to mobile
devices (Section 3.2).

The existing libraries require installation of Android NDK
alongside Android SDK, while in CNNdroid, only Android
SDK is required.

3. CNNDROID LIBRARY

3.1 CNN Layer Types

CNNdroid library supports nearly all common types of
CNN layers, namely, convolution, max/mean pooling, fully
connected, rectiﬁed linear unit, local response normalization
and softmax. Detailed description of every layer type and its
corresponding parameters are available in the library docu-
mentations [1]. Other new layers may be added as well, due
to the open source nature of the library.

3.2 Model Preparation

Model Conversion Scripts: Figure 3 shows an overview
of the steps involved in deploying trained CNN models on
mobile devices. CNNdroid library provides a set of scripts
which take the models trained by common desktop/server
libraries, namely, Caﬀe [6], Torch [7] and Theano [8], as in-
put and convert them into CNNdroid format. Therefore, the
models which are trained by these libraries can be executed
by CNNdroid library on mobile devices.

It is possible to write similar scripts for other libraries as
well. CNNdroid uses MessagePack serialization format [19]
for storing layer parameters in the trained model. Detailed
procedure is presented in the library documentations [1].

NetFile: The developer needs to prepare a .txt ﬁle, called
N etF ile, similar to the .prototxt ﬁle in Caﬀe [6]. The N etF ile
speciﬁes layer setup of the trained model, i.e., order of the
CNN layers along with their conﬁgurations, e.g., padding
and stride values of convolution layers. Figure 4 presents an
example. Detailed instructions for preparing the N etF ile as
well as a few examples are included in the library documen-
tations [1].

The N etF ile also speciﬁes three conﬁguration parame-
ters (Figure 4), namely, allocated_ram which speciﬁes the
maximum amount of memory that CNNdroid engine is al-
lowed to allocate at runtime (Section 3.4), execution_mode
which selects between sequential or parallel execution modes
of the library (Section 3.5), and auto_tuning which speciﬁes
whether or not auto-tuning is turned on (Section 3.6).

3.3 Model Execution

Once the trained model and its corresponding N etF ile
are both uploaded to mobile device (Figure 3), the model
can be executed in the target Android application in a few
simple steps as described below (Figure 5).

The ﬁrst step is to include the provided CNNdroid library
ﬁles. Note that CNNdroid library is self-suﬃcient and does
not require installation of third-party libraries. In addition,
it does not require installation of Android NDK alongside
Android SDK.

Next, RenderScript and CNNdroid objects are constructed
(Figure 5, steps 2 and 3). The CNNdroid constructor takes
the provided N etF ile as input and automatically creates the
corresponding objects for the network layers.

Finally, compute function of the constructed CNNdroid
object is called, which automatically executes the trained
model on either a single image or a batch of images.

3.4 Memory Allocation

The trained CNN model, which is uploaded to SD card
of the mobile device, contains layer parameters in form of
matrices. Inside the compute function (Figure 5, step 5), and
before execution of every layer, the corresponding matrices
are automatically loaded from SD card into RAM, which
incurs an overhead.

In order to reduce this overhead, CNNdroid selects certain
layers and keeps their data in RAM, while other layers are
allocated and de-allocated every time. The selection proce-
dure is automatically done in CNNdroid constructor (Figure
5, step 3). Starting from the largest layer, as many layers
as possible are selected, till the sum of their memory sizes
reaches the maximum developer-speciﬁed amount, i.e., the
allocated_ram parameter in the N etF ile.

Note that the allocated_ram parameter cannot be ar-
bitrarily large due to practical limitations. For example,
Android 5 limits the memory usage of every app to 512MB.

3.5 Acceleration Methods

Diﬀerent methods are employed in acceleration of diﬀer-
ent layers in CNNdroid. Convolution and fully connected
layers, which are data-parallel and normally more compute
intensive, are accelerated on the mobile GPU using Render-
Script framework.

A considerable portion of these two layers can be ex-
pressed as dot products. In speciﬁc, in the convolution layer,
kernels get convoluted with the input frames, and in the fully
connected layer, the computation can be expressed as a ma-

Figure 3: Overview of CNNdroid’s model deployment procedure.

Figure 4: Example NetFile showing three layers of AlexNet [20],
along with allocated_ram, execution_mode and auto_tuning pa-
rameters.

Figure 5: Simple steps involved in using CNNdroid for execution
of trained deep CNN models on Android apps. Refer to the library
documentation for up-to-date details and sample projects [1].

trix to vector multiplication. The dot products are more
eﬃciently calculated on SIMD units of the target mobile
GPU. Therefore, we divide the computation in many vec-
tor operations and use the pre-deﬁned dot function of the
RenderScript framework. In other words, we explicitly ex-
press this level of parallelism in software, and as opposed
to CUDA-based desktop libraries, do not leave it to GPU’s
hardware scheduler.

Comparing with convolution and fully connected layers,
other layers are relatively less compute intensive and not ef-
ﬁcient on mobile GPU. Therefore, they are accelerated on
multi-core mobile CPU via multi-threading. Since ReLU
layer usually appears after a convolution or fully connected
layer, it is embedded into its previous layer in order to in-
crease the performance in cases where multiple images are
fed to the CNNdroid engine.

In addition to above parallel implementations, CNNdroid
also includes sequential (single-thread) implementations of
all layers. The execution will be sequential or parallel de-
pending on the execution_mode parameter speciﬁed in the
N etF ile (Figure 4).

3.6 Auto-Tuning

In order to reach better performance across diﬀerent An-
droid based mobile devices, our GPU-accelerated parallel
algorithms are developed with tuning parameters which se-
lect the amount of work assigned to parallel GPU threads
and the amount of work assigned to SIMD ALUs in execu-
tion of every GPU thread. The tuning parameters basically
determine the granularity of parallelism.

If turned on in the N etF ile (Figure 4), the auto-tuner is
automatically executed when the Android app is launched
for the ﬁrst time. It executes the CNN model for a number
of predeﬁned scenarios on the mobile device, measures their
runtime and saves the optimum tuning parameters for future
executions. As a result, the ﬁrst launch of the application
takes much longer time. For the purpose of clear and fair
comparisons, auto-tuning is turned oﬀ in our experiments in
Section 4.

4. EMPIRICAL EVALUATION

CNNdroid is empirically evaluated on two mobile devices,
namely, Samsung Galaxy Note 4 and HTC One M9. We
employ three well-known benchmark CNNs, namely, LeNet
network for MNIST dataset [21], Alex Krizhevsky’s network
for CIFAR-10 (Alex’s CIFAR-10) [22] and Alex Krizhevsky’s
network for ImageNet 2012 dataset (AlexNet) [20].

Layer setup of the benchmark CNNs are shown in Figure
6. We also measured the storage and memory requirement
of the benchmark CNNs when ported to CNNdroid format.
The results are reported in Figure 7.

We execute forward paths of the benchmark CNNs on
the mobile devices and measure their accuracy, runtime and
energy consumption. All benchmarks accept batches of 16
images as input in our experiments. Before running every
experiment, mobile devices are fully charged and put into
airplane mode and minimum screen brightness. The mea-
surements reported below are only for CNN execution and
not for loading the network parameters from SD card, be-
cause in our benchmarks network parameters are loaded only
once in the beginning but CNN execution is performed on
every input image.

Layer
1
2
3
4
5
6
7
8
9
10
11
12
13

LeNet
Conv
Pooling
Conv
Pooling
FC+ReLU
FC
-
-
-
-
-
-
-

Alex’s CIFAR-10
Conv
Pooling+ReLU
Conv+ReLU
Pooling
Conv+ReLU
Pooling
FC
FC
-
-
-
-
-

AlexNet
Conv+ReLU
LRN
Pooling
Conv+ReLU
LRN
Pooling
Conv+ReLU
Conv+ReLU
Conv+ReLU
Pooling
FC+ReLU
FC+ReLU
FC

Figure 6: Layer setup of benchmark CNNs.

Figure 7: Storage and memory requirements of benchmark CNNs
when ported to CNNdroid format.

4.1 Accuracy

In order to measure CNNdroid accuracy, output of the
last network layer in both CNNdroid and Caﬀe are com-
pared for the same input. The resulting mean square error
is in the order of 10−12, which means there is no meaningful
diﬀerence and CNNdroid is correctly implemented.

4.2 Performance

Figure 8.a shows the total measured runtime for CPU-
only sequential CNN implementation as well as the speedup
gained by GPU acceleration. The reported values are the
average of ten executions.

Note that real-time performance is achieved on mobile
devices in LeNet and Alex’s CIFAR-10 benchmarks. For
instance on the HTC One M9 device, the accelerated im-
plementation achieved 60.2 and 32.2 frames per second for
LeNet and Alex’s CIFAR-10 benchmarks, respectively.

We also measured runtime of the heaviest convolution
layer in order to observe direct impact of the GPU-based
acceleration (Figure 8.b). The highest achieved speedup is
63.4X for AlexNet benchmark on Galaxy Note 4 device with
Mali-T760 GPU. This GPU has 6 shader cores, each with
two 128-bit ALUs. Since all elements of the matrices in
our CNN model are 32-bit ﬂoating point values, a maxi-
mum of 6 × 2 × 128
32 = 48 operations may run in parallel. In
other words, the maximum theoretically achievable speedup
is 48X. Therefore, the measured 63.4X speedup most proba-
bly comes from other factors such as software language per-
formance of RenderScript(c99) over Java or cache eﬀects.

Note that the benchmark CNNs in our experiments ac-
cept batches of 16 images as input, and the runtime values
reported in Figure 8 are per image. It is recommended to
process a batch of input images rather than a single image
to get higher performance in CNNdroid.

As for the performance comparison of our mobile devices,
we see that the overall speedup in AlexNet, which is a large
network, is approximately 30% higher on Galaxy Note 4
compared with HTC One M9. This can be either the result
of lower GPU frequency of HTC One M9 or its aggressive
throttling policy in order to prevent overheating issues in
long runtimes.

[5] Paul A Merolla, John V Arthur, Rodrigo

Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp
Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo,
Yutaka Nakamura, Bernard Brezzo, Ivan Vo, Steven K
Esser, Rathinakumar Appuswamy, Brian Taba, Arnon
Amir, Myron D Flickner, William P Risk, Rajit
Manohar, and Dharmendra S Modha. A million
spiking-neuron integrated circuit with a scalable
communication network and interface. Science,
345(6197):668–673, 2014.

[6] Yangqing Jia, Evan Shelhamer, Jeﬀ Donahue, Sergey

Karayev, Jonathan Long, Ross Girshick, Sergio
Guadarrama, and Trevor Darrell. Caﬀe: Convolutional
architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.

[7] Torch. http://torch.ch/. Accessed 2016-08-01.
[8] James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,

Pascal Lamblin, Razvan Pascanu, Guillaume
Desjardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. Theano: a CPU and GPU math
expression compiler. In Proceedings of the Python for
Scientiﬁc Computing Conference, 2010.

[9] TensorFlow. https://www.tensorﬂow.org. Accessed

[10] Nvidia cuDNN. https://developer.nvidia.com/cudnn.

2016-08-01.

Accessed 2016-08-01.

[11] cuda-convent.

https://code.google.com/p/cuda-convnet/. Accessed
2016-08-01.

[12] Velesnet. https://velesnet.ml/. Accessed 2016-08-01.
[13] Caﬀe Android Library.

https://github.com/sh1r0/caﬀe-android-lib. Accessed
2016-08-01.

[14] Torch-7 for Android.

https://github.com/soumith/torch-android. Accessed
2016-08-01.

[15] A convolutional neural network for the Android

phone. https://github.com/radiodee1/
awesome-cnn-android-python. Accessed 2016-08-01.

[16] Facial attractiveness prediction on Android.
https://github.com/eldog/fmobile. Accessed
2016-08-01.

[17] ARM. Mali-T600 Series GPU OpenCL, Version 1.1.0,

Developer Guide. Accessed 2016-08-01.

[18] Android RenderScript Developers Guide.

http://developer.android.com/guide/topics/
renderscript/compute.html. Accessed 2016-08-01.

[19] Messagepack. http://msgpack.org/index.html.
[20] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E.

Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in Neural
Information Processing Systems, 2012.

[21] Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner.
Gradient-based learning applied to document
recognition. Proceedings of the IEEE,
86(11):2278–2324, Nov 1998.

[22] Alex Krizhevsky. Learning multiple layers of features
from tiny images. Technical report, University of
Toronto, 2009.

[23] Trepn power proﬁler. https://developer.qualcomm.

com/software/trepn-power-proﬁler.

Figure 8: Average runtime of (a) the entire CNN and (b) the
heaviest convolution layer, per image in a batch of 16 images,
and the corresponding speedup rate.

4.3 Energy Consumption

We measured power and energy consumption per image
for AlexNet benchmark on HTC One M9 by employing “Qual-
comm Trepn Proﬁler” application [23].

The GPU accelerated execution consumes around 523 mW
power and 0.4 J energy while the CPU-only sequential exe-
cution consumes 2338 mW power and 51.6 J energy. As a
result, the GPU accelerated execution consumes 51.6÷0.4 =
129X less battery energy.

It should be noted that we observed about 20% variability
in our measurements which is expected since Trepn Proﬁler
provides a software-only method for measuring energy con-
sumption of a single app.

5. CONCLUSIONS

We introduced CNNdroid, an open source GPU acceler-
ated deep CNN library for Android-based mobile devices.
Empirical evaluations demonstrated up to 60X speedup and
up to 130X energy saving. The source code, documentation
and sample projects are published online [1].

6. REFERENCES

[1] CNNdroid open source GPU-accelerated library.

https://github.com/ENCP/CNNdroid.

[2] Inchul Song, Hyun-Jun Kim, and Paul Barom Jeon.
Deep learning for real-time robust facial expression
recognition on a smartphone. In IEEE International
Conference on Consumer Electronics, pages 564–567,
Jan 2014.

[3] Yu-Hsin Chen, Tushar Krishna, Joel Emer, and
Vivienne Sze. 14.5 eyeriss: an energy-eﬃcient
reconﬁgurable accelerator for deep convolutional
neural networks. In IEEE International Solid-State
Circuits Conference, pages 262–263, Jan 2016.
[4] Mohammad Motamedi, Philipp Gysel, Venkatesh

Akella, and Soheil Ghiasi. Design space exploration of
fpga-based deep convolutional neural networks. In
Asia and South Paciﬁc Design Automation
Conference, pages 575–580, Jan 2016.

CNNdroid: GPU-Accelerated Execution of Trained
Deep Convolutional Neural Networks on Android

∗
Seyyed Salar Latiﬁ Oskouei, Hossein Golestani, Matin Hashemi
Sharif University of Technology
salarlatiﬁ@ee.sharif.edu, hossein_golestani@ee.sharif.edu, matin@sharif.edu
Soheil Ghiasi
University of California, Davis
ghiasi@ucdavis.edu

6
1
0
2
 
t
c
O
 
5
1
 
 
]

C
D
.
s
c
[
 
 
2
v
6
7
3
7
0
.
1
1
5
1
:
v
i
X
r
a

ABSTRACT
Many mobile applications running on smartphones and wear-
able devices would potentially beneﬁt from the accuracy and
scalability of deep CNN-based machine learning algorithms.
However, performance and energy consumption limitations
make the execution of such computationally intensive algo-
rithms on mobile devices prohibitive. We present a GPU-
accelerated library, dubbed CNNdroid [1], for execution of
trained deep CNNs on Android-based mobile devices. Em-
pirical evaluations show that CNNdroid achieves up to 60X
speedup and 130X energy saving on current mobile devices.
The CNNdroid open source library is available for download
at https://github.com/ENCP/CNNdroid

Keywords
Deep Learning, Deep Convolutional Neural Network (CNN),
Mobile GPU, Performance Optimization, Low Energy Con-
sumption, Open Source Software, Android, RenderScript

1.

INTRODUCTION

Mobile platforms such as smartphones, wearable devices,
tiny autonomous robots and IoT devices have been increas-
ingly ﬁnding their way into many areas (Figure 1). Nu-
merous applications, such as speech recognition and image
recognition [2], would potentially beneﬁt from local execu-
tion of accurate machine learning algorithms on mobile de-
vices. Local execution allows data to stay on the mobile
device and hence avoids latency issues of cloud-assisted pro-
cessing.

Deep CNNs can achieve state-of-the-art results in terms of
both prediction accuracy and scalability. However, they are
highly computationally intensive, and hence, not practical
on current mobile devices without acceleration.
∗Contact author: Matin Hashemi, matin@sharif.edu

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
MM ’16, October 15 - 19, 2016, Amsterdam, Netherlands
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-3603-1/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2964284.2973801

Figure 1: Example applications of deep CNNs in mobile systems.
Image credits: IBM Research, Guardianlv, Nixie, Android Wear.

Many hardware-based solutions have been proposed for
acceleration of deep CNNs [3, 4]. IBM has also introduced a
neuromorphic CMOS chip for execution of learning applica-
tions on smartphones and IoT devices [5]. While promising,
such solutions are still in early stages of development and
not available on current mobile devices.

As opposed to hardware-based engines, GPU already ex-
ists in many current mobile devices and can be programmed
completely in software. Therefore, parallel processing capa-
bilities of mobile GPUs can be exploited to accelerate deep
CNN computations on current mobile devices.

On server and desktop platforms, there exists many GPU-
accelerated deep CNN libraries [6, 7, 8, 9, 10, 11, 12]. How-
ever, because of architecture diﬀerences (Section 2.1), mere
porting of such libraries to mobile platforms yields sub-
optimal performance or is impossible in some cases (Section
2.2).

On mobile platforms, to the best of our knowledge, such
GPU-accelerated libraries are not available. The few ex-
isting mobile libraries for CNN computations [13, 14, 15,
16] are limited to the processing power of multi-core mobile
CPUs (Section 2.3).

We present an open source GPU-accelerated library, dubbed

CNNdroid, which is speciﬁcally designed and optimized for
execution of trained deep CNNs on Android-based mobile
devices. The main highlights of CNNdroid are as follows.

1. Support for nearly all CNN layer types (Section 3.1).

2. Compatible with CNN models trained by common desk-
top/server libraries, namely, Caﬀe [6], Torch [7] and
Theano [8] (Section 3.2).

3. Easy to conﬁgure and integrate into any Android app
without additional software requirements (Section 3.3).

4. User-speciﬁed maximum memory usage (Section 3.4).

5. GPU or CPU acceleration of supported CNN layers

(Section 3.5).

6. Automatic tuning of performance (Section 3.6).

7. Up to 60X speedup and up to 130X energy saving on

current mobile devices (Section 4).

2. BACKGROUND AND RELATED WORK

2.1 Comparing Mobile and Desktop GPUs

A modern graphics processing unit (GPU), in addition to
computer graphics, can be programmed for general purpose
computations as well. While desktop GPUs have long been
programmable, major mobile chip manufacturers have re-
cently made the GPU hardware available for general purpose
computations. Due to strict area and power constraints,
mobile GPUs have important diﬀerences with their desktop
counterparts.

A modern mobile GPU is typically composed of several
programmable parallel computing units called Shader Cores
(SC). Every shader core is composed of several parallel ALUs.
For example, Samsung Exynos 5433 chip is composed of
ARM A53/A57 CPU and Mali T-760 GPU (Figure 2). Each
SC in T-760 GPU has two 128-bit ALUs in VLIW format.
Each 128-bit ALU is capable of performing SIMD opera-
tions, i.e., two 64-bit, four 32-bit or eight 16-bit operations in
parallel [17]. In comparison with desktop GPUs, the above
parallel ALU architecture relies more on software and com-
piler than dynamic hardware scheduler in eﬃcient execution
of parallel threads.

More importantly, fast shared memory in thread blocks
which are present in desktop GPUs and widely employed
in many CUDA-based desktop libraries are not available in
mobile GPUs.

There are some diﬀerences on the software side as well.
For example in RenderScript, Android’s parallel computing
platform [18], thread synchronization is not available. In ad-
dition, there must be a one-to-one correspondence between
parallel threads and the data items inside one of the memory
buﬀers that parallel threads work on.

2.2 Comparing CNNdroid with Desktop

Libraries

On server and desktop platforms, there exists many li-
braries such as Caﬀe [6], Torch [7], Theano [8], Tensor-
Flow [9], cuDNN [10], cuda-convnet [11], and Velesnet [12],
which employ GPU-based parallel processing for accelera-
tion of deep CNN computations. However, the acceleration
methodologies and parallel algorithms of such libraries could
not be directly utilized in mobile platforms due to the exist-
ing hardware and software diﬀerences.

For example in Caﬀe [6], the convolution operation is un-
rolled and converted to matrix multiplication, which requires

Figure 2: Example: Exynos 5433 mobile processor with ARM
A53 / A57 CPU and Mali T-760 GPU (SC: Shader Core, VLIW:
Very Long Instruction Word, SIMD: Single Instruction Multiple
Data).

considerable amount of memory and therefore is not suitable
for mobile devices with small cache and memory sizes. As
another example, the parallel algorithm in Theano [8] is sim-
ilar to CNNdroid but without eﬃcient use of SIMD units in
mobile GPUs. Refer to Section 3.5 for details.

More importantly, desktop libraries take advantage of thread

management facilities provided by desktop GPUs and CUDA
framework, such as fast shared memory and thread synchro-
nization, which are not available in mobile GPUs and Ren-
derScript.

2.3 Comparing CNNdroid with Mobile

Libraries

On mobile platforms, to the best of our knowledge, only
few deep CNN libraries exist [13, 14, 15, 16]. All such li-
braries, including Caﬀe Mobile [13] and Torch Mobile [14],
are limited to the processing power of multi-core mobile
CPUs, while CNNdroid eﬃciently employs both GPU and
CPU (Section 3.5).

In addition, CNNdroid is compatible with CNN models
trained by Caﬀe [6], Torch [7] and Theano [8], which facil-
itates the process of porting the trained models to mobile
devices (Section 3.2).

The existing libraries require installation of Android NDK
alongside Android SDK, while in CNNdroid, only Android
SDK is required.

3. CNNDROID LIBRARY

3.1 CNN Layer Types

CNNdroid library supports nearly all common types of
CNN layers, namely, convolution, max/mean pooling, fully
connected, rectiﬁed linear unit, local response normalization
and softmax. Detailed description of every layer type and its
corresponding parameters are available in the library docu-
mentations [1]. Other new layers may be added as well, due
to the open source nature of the library.

3.2 Model Preparation

Model Conversion Scripts: Figure 3 shows an overview
of the steps involved in deploying trained CNN models on
mobile devices. CNNdroid library provides a set of scripts
which take the models trained by common desktop/server
libraries, namely, Caﬀe [6], Torch [7] and Theano [8], as in-
put and convert them into CNNdroid format. Therefore, the
models which are trained by these libraries can be executed
by CNNdroid library on mobile devices.

It is possible to write similar scripts for other libraries as
well. CNNdroid uses MessagePack serialization format [19]
for storing layer parameters in the trained model. Detailed
procedure is presented in the library documentations [1].

NetFile: The developer needs to prepare a .txt ﬁle, called
N etF ile, similar to the .prototxt ﬁle in Caﬀe [6]. The N etF ile
speciﬁes layer setup of the trained model, i.e., order of the
CNN layers along with their conﬁgurations, e.g., padding
and stride values of convolution layers. Figure 4 presents an
example. Detailed instructions for preparing the N etF ile as
well as a few examples are included in the library documen-
tations [1].

The N etF ile also speciﬁes three conﬁguration parame-
ters (Figure 4), namely, allocated_ram which speciﬁes the
maximum amount of memory that CNNdroid engine is al-
lowed to allocate at runtime (Section 3.4), execution_mode
which selects between sequential or parallel execution modes
of the library (Section 3.5), and auto_tuning which speciﬁes
whether or not auto-tuning is turned on (Section 3.6).

3.3 Model Execution

Once the trained model and its corresponding N etF ile
are both uploaded to mobile device (Figure 3), the model
can be executed in the target Android application in a few
simple steps as described below (Figure 5).

The ﬁrst step is to include the provided CNNdroid library
ﬁles. Note that CNNdroid library is self-suﬃcient and does
not require installation of third-party libraries. In addition,
it does not require installation of Android NDK alongside
Android SDK.

Next, RenderScript and CNNdroid objects are constructed
(Figure 5, steps 2 and 3). The CNNdroid constructor takes
the provided N etF ile as input and automatically creates the
corresponding objects for the network layers.

Finally, compute function of the constructed CNNdroid
object is called, which automatically executes the trained
model on either a single image or a batch of images.

3.4 Memory Allocation

The trained CNN model, which is uploaded to SD card
of the mobile device, contains layer parameters in form of
matrices. Inside the compute function (Figure 5, step 5), and
before execution of every layer, the corresponding matrices
are automatically loaded from SD card into RAM, which
incurs an overhead.

In order to reduce this overhead, CNNdroid selects certain
layers and keeps their data in RAM, while other layers are
allocated and de-allocated every time. The selection proce-
dure is automatically done in CNNdroid constructor (Figure
5, step 3). Starting from the largest layer, as many layers
as possible are selected, till the sum of their memory sizes
reaches the maximum developer-speciﬁed amount, i.e., the
allocated_ram parameter in the N etF ile.

Note that the allocated_ram parameter cannot be ar-
bitrarily large due to practical limitations. For example,
Android 5 limits the memory usage of every app to 512MB.

3.5 Acceleration Methods

Diﬀerent methods are employed in acceleration of diﬀer-
ent layers in CNNdroid. Convolution and fully connected
layers, which are data-parallel and normally more compute
intensive, are accelerated on the mobile GPU using Render-
Script framework.

A considerable portion of these two layers can be ex-
pressed as dot products. In speciﬁc, in the convolution layer,
kernels get convoluted with the input frames, and in the fully
connected layer, the computation can be expressed as a ma-

Figure 3: Overview of CNNdroid’s model deployment procedure.

Figure 4: Example NetFile showing three layers of AlexNet [20],
along with allocated_ram, execution_mode and auto_tuning pa-
rameters.

Figure 5: Simple steps involved in using CNNdroid for execution
of trained deep CNN models on Android apps. Refer to the library
documentation for up-to-date details and sample projects [1].

trix to vector multiplication. The dot products are more
eﬃciently calculated on SIMD units of the target mobile
GPU. Therefore, we divide the computation in many vec-
tor operations and use the pre-deﬁned dot function of the
RenderScript framework. In other words, we explicitly ex-
press this level of parallelism in software, and as opposed
to CUDA-based desktop libraries, do not leave it to GPU’s
hardware scheduler.

Comparing with convolution and fully connected layers,
other layers are relatively less compute intensive and not ef-
ﬁcient on mobile GPU. Therefore, they are accelerated on
multi-core mobile CPU via multi-threading. Since ReLU
layer usually appears after a convolution or fully connected
layer, it is embedded into its previous layer in order to in-
crease the performance in cases where multiple images are
fed to the CNNdroid engine.

In addition to above parallel implementations, CNNdroid
also includes sequential (single-thread) implementations of
all layers. The execution will be sequential or parallel de-
pending on the execution_mode parameter speciﬁed in the
N etF ile (Figure 4).

3.6 Auto-Tuning

In order to reach better performance across diﬀerent An-
droid based mobile devices, our GPU-accelerated parallel
algorithms are developed with tuning parameters which se-
lect the amount of work assigned to parallel GPU threads
and the amount of work assigned to SIMD ALUs in execu-
tion of every GPU thread. The tuning parameters basically
determine the granularity of parallelism.

If turned on in the N etF ile (Figure 4), the auto-tuner is
automatically executed when the Android app is launched
for the ﬁrst time. It executes the CNN model for a number
of predeﬁned scenarios on the mobile device, measures their
runtime and saves the optimum tuning parameters for future
executions. As a result, the ﬁrst launch of the application
takes much longer time. For the purpose of clear and fair
comparisons, auto-tuning is turned oﬀ in our experiments in
Section 4.

4. EMPIRICAL EVALUATION

CNNdroid is empirically evaluated on two mobile devices,
namely, Samsung Galaxy Note 4 and HTC One M9. We
employ three well-known benchmark CNNs, namely, LeNet
network for MNIST dataset [21], Alex Krizhevsky’s network
for CIFAR-10 (Alex’s CIFAR-10) [22] and Alex Krizhevsky’s
network for ImageNet 2012 dataset (AlexNet) [20].

Layer setup of the benchmark CNNs are shown in Figure
6. We also measured the storage and memory requirement
of the benchmark CNNs when ported to CNNdroid format.
The results are reported in Figure 7.

We execute forward paths of the benchmark CNNs on
the mobile devices and measure their accuracy, runtime and
energy consumption. All benchmarks accept batches of 16
images as input in our experiments. Before running every
experiment, mobile devices are fully charged and put into
airplane mode and minimum screen brightness. The mea-
surements reported below are only for CNN execution and
not for loading the network parameters from SD card, be-
cause in our benchmarks network parameters are loaded only
once in the beginning but CNN execution is performed on
every input image.

Layer
1
2
3
4
5
6
7
8
9
10
11
12
13

LeNet
Conv
Pooling
Conv
Pooling
FC+ReLU
FC
-
-
-
-
-
-
-

Alex’s CIFAR-10
Conv
Pooling+ReLU
Conv+ReLU
Pooling
Conv+ReLU
Pooling
FC
FC
-
-
-
-
-

AlexNet
Conv+ReLU
LRN
Pooling
Conv+ReLU
LRN
Pooling
Conv+ReLU
Conv+ReLU
Conv+ReLU
Pooling
FC+ReLU
FC+ReLU
FC

Figure 6: Layer setup of benchmark CNNs.

Figure 7: Storage and memory requirements of benchmark CNNs
when ported to CNNdroid format.

4.1 Accuracy

In order to measure CNNdroid accuracy, output of the
last network layer in both CNNdroid and Caﬀe are com-
pared for the same input. The resulting mean square error
is in the order of 10−12, which means there is no meaningful
diﬀerence and CNNdroid is correctly implemented.

4.2 Performance

Figure 8.a shows the total measured runtime for CPU-
only sequential CNN implementation as well as the speedup
gained by GPU acceleration. The reported values are the
average of ten executions.

Note that real-time performance is achieved on mobile
devices in LeNet and Alex’s CIFAR-10 benchmarks. For
instance on the HTC One M9 device, the accelerated im-
plementation achieved 60.2 and 32.2 frames per second for
LeNet and Alex’s CIFAR-10 benchmarks, respectively.

We also measured runtime of the heaviest convolution
layer in order to observe direct impact of the GPU-based
acceleration (Figure 8.b). The highest achieved speedup is
63.4X for AlexNet benchmark on Galaxy Note 4 device with
Mali-T760 GPU. This GPU has 6 shader cores, each with
two 128-bit ALUs. Since all elements of the matrices in
our CNN model are 32-bit ﬂoating point values, a maxi-
mum of 6 × 2 × 128
32 = 48 operations may run in parallel. In
other words, the maximum theoretically achievable speedup
is 48X. Therefore, the measured 63.4X speedup most proba-
bly comes from other factors such as software language per-
formance of RenderScript(c99) over Java or cache eﬀects.

Note that the benchmark CNNs in our experiments ac-
cept batches of 16 images as input, and the runtime values
reported in Figure 8 are per image. It is recommended to
process a batch of input images rather than a single image
to get higher performance in CNNdroid.

As for the performance comparison of our mobile devices,
we see that the overall speedup in AlexNet, which is a large
network, is approximately 30% higher on Galaxy Note 4
compared with HTC One M9. This can be either the result
of lower GPU frequency of HTC One M9 or its aggressive
throttling policy in order to prevent overheating issues in
long runtimes.

[5] Paul A Merolla, John V Arthur, Rodrigo

Alvarez-Icaza, Andrew S Cassidy, Jun Sawada, Filipp
Akopyan, Bryan L Jackson, Nabil Imam, Chen Guo,
Yutaka Nakamura, Bernard Brezzo, Ivan Vo, Steven K
Esser, Rathinakumar Appuswamy, Brian Taba, Arnon
Amir, Myron D Flickner, William P Risk, Rajit
Manohar, and Dharmendra S Modha. A million
spiking-neuron integrated circuit with a scalable
communication network and interface. Science,
345(6197):668–673, 2014.

[6] Yangqing Jia, Evan Shelhamer, Jeﬀ Donahue, Sergey

Karayev, Jonathan Long, Ross Girshick, Sergio
Guadarrama, and Trevor Darrell. Caﬀe: Convolutional
architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.

[7] Torch. http://torch.ch/. Accessed 2016-08-01.
[8] James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,

Pascal Lamblin, Razvan Pascanu, Guillaume
Desjardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. Theano: a CPU and GPU math
expression compiler. In Proceedings of the Python for
Scientiﬁc Computing Conference, 2010.

[9] TensorFlow. https://www.tensorﬂow.org. Accessed

[10] Nvidia cuDNN. https://developer.nvidia.com/cudnn.

2016-08-01.

Accessed 2016-08-01.

[11] cuda-convent.

https://code.google.com/p/cuda-convnet/. Accessed
2016-08-01.

[12] Velesnet. https://velesnet.ml/. Accessed 2016-08-01.
[13] Caﬀe Android Library.

https://github.com/sh1r0/caﬀe-android-lib. Accessed
2016-08-01.

[14] Torch-7 for Android.

https://github.com/soumith/torch-android. Accessed
2016-08-01.

[15] A convolutional neural network for the Android

phone. https://github.com/radiodee1/
awesome-cnn-android-python. Accessed 2016-08-01.

[16] Facial attractiveness prediction on Android.
https://github.com/eldog/fmobile. Accessed
2016-08-01.

[17] ARM. Mali-T600 Series GPU OpenCL, Version 1.1.0,

Developer Guide. Accessed 2016-08-01.

[18] Android RenderScript Developers Guide.

http://developer.android.com/guide/topics/
renderscript/compute.html. Accessed 2016-08-01.

[19] Messagepack. http://msgpack.org/index.html.
[20] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E.

Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in Neural
Information Processing Systems, 2012.

[21] Y. Lecun, L. Bottou, Y. Bengio, and P. Haﬀner.
Gradient-based learning applied to document
recognition. Proceedings of the IEEE,
86(11):2278–2324, Nov 1998.

[22] Alex Krizhevsky. Learning multiple layers of features
from tiny images. Technical report, University of
Toronto, 2009.

[23] Trepn power proﬁler. https://developer.qualcomm.

com/software/trepn-power-proﬁler.

Figure 8: Average runtime of (a) the entire CNN and (b) the
heaviest convolution layer, per image in a batch of 16 images,
and the corresponding speedup rate.

4.3 Energy Consumption

We measured power and energy consumption per image
for AlexNet benchmark on HTC One M9 by employing “Qual-
comm Trepn Proﬁler” application [23].

The GPU accelerated execution consumes around 523 mW
power and 0.4 J energy while the CPU-only sequential exe-
cution consumes 2338 mW power and 51.6 J energy. As a
result, the GPU accelerated execution consumes 51.6÷0.4 =
129X less battery energy.

It should be noted that we observed about 20% variability
in our measurements which is expected since Trepn Proﬁler
provides a software-only method for measuring energy con-
sumption of a single app.

5. CONCLUSIONS

We introduced CNNdroid, an open source GPU acceler-
ated deep CNN library for Android-based mobile devices.
Empirical evaluations demonstrated up to 60X speedup and
up to 130X energy saving. The source code, documentation
and sample projects are published online [1].

6. REFERENCES

[1] CNNdroid open source GPU-accelerated library.

https://github.com/ENCP/CNNdroid.

[2] Inchul Song, Hyun-Jun Kim, and Paul Barom Jeon.
Deep learning for real-time robust facial expression
recognition on a smartphone. In IEEE International
Conference on Consumer Electronics, pages 564–567,
Jan 2014.

[3] Yu-Hsin Chen, Tushar Krishna, Joel Emer, and
Vivienne Sze. 14.5 eyeriss: an energy-eﬃcient
reconﬁgurable accelerator for deep convolutional
neural networks. In IEEE International Solid-State
Circuits Conference, pages 262–263, Jan 2016.
[4] Mohammad Motamedi, Philipp Gysel, Venkatesh

Akella, and Soheil Ghiasi. Design space exploration of
fpga-based deep convolutional neural networks. In
Asia and South Paciﬁc Design Automation
Conference, pages 575–580, Jan 2016.

