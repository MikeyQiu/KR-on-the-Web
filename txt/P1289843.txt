Implicit Quantile Networks for Distributional Reinforcement Learning

8
1
0
2
 
n
u
J
 
4
1
 
 
]

G
L
.
s
c
[
 
 
1
v
3
2
9
6
0
.
6
0
8
1
:
v
i
X
r
a

Will Dabney * 1 Georg Ostrovski * 1 David Silver 1 R´emi Munos 1

Abstract

In this work, we build on recent advances in dis-
tributional reinforcement learning to give a gener-
ally applicable, ﬂexible, and state-of-the-art dis-
tributional variant of DQN. We achieve this by
using quantile regression to approximate the full
quantile function for the state-action return distri-
bution. By reparameterizing a distribution over
the sample space, this yields an implicitly deﬁned
return distribution and gives rise to a large class of
risk-sensitive policies. We demonstrate improved
performance on the 57 Atari 2600 games in the
ALE, and use our algorithm’s implicitly deﬁned
distributions to study the effects of risk-sensitive
policies in Atari games.

1. Introduction

Distributional reinforcement learning (Jaquette, 1973; Sobel,
1982; White, 1988; Morimura et al., 2010b; Bellemare et al.,
2017) focuses on the intrinsic randomness of returns within
the reinforcement learning (RL) framework. As the agent in-
teracts with the environment, irreducible randomness seeps
in through the stochasticity of these interactions, the ap-
proximations in the agent’s representation, and even the
inherently chaotic nature of physical interaction (Yu et al.,
2016). Distributional RL aims to model the distribution over
returns, whose mean is the traditional value function, and to
use these distributions to evaluate and optimize a policy.

Any distributional RL algorithm is characterized by two
aspects: the parameterization of the return distribution, and
the distance metric or loss function being optimized. To-
gether, these choices control assumptions about the random
returns and how approximations will be traded off. Cat-
egorical DQN (Bellemare et al., 2017, C51) combines a
categorical distribution and the cross-entropy loss with the
Cram´er-minimizing projection (Rowland et al., 2018). For

*Equal contribution 1DeepMind, London, UK. Correspondence
to: Will Dabney <wdabney@google.com>, Georg Ostrovski <os-
trovski@google.com>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

this, it assumes returns are bounded in a known range and
trades off mean-preservation at the cost of overestimating
variance.

C51 outperformed all previous improvements to DQN on
a set of 57 Atari 2600 games in the Arcade Learning En-
vironment (Bellemare et al., 2013), which we refer to as
the Atari-57 benchmark. Subsequently, several papers have
built upon this successful combination to achieve signiﬁ-
cant improvements to the state-of-the-art in Atari-57 (Hessel
et al., 2018; Gruslys et al., 2018), and challenging continu-
ous control tasks (Barth-Maron et al., 2018).

These algorithms are restricted to assigning probabilities to
an a priori ﬁxed, discrete set of possible returns. Dabney
et al. (2018) propose an alternate pair of choices, parameter-
izing the distribution by a uniform mixture of Diracs whose
locations are adjusted using quantile regression. Their algo-
rithm, QR-DQN, while restricted to a discrete set of quan-
tiles, automatically adapts return quantiles to minimize the
Wasserstein distance between the Bellman updated and cur-
rent return distributions. This ﬂexibility allows QR-DQN to
signiﬁcantly improve on C51’s Atari-57 performance.

In this paper, we extend the approach of Dabney et al.
(2018), from learning a discrete set of quantiles to learning
the full quantile function, a continuous map from probabil-
ities to returns. When combined with a base distribution,
such as U ([0, 1]), this forms an implicit distribution capable
of approximating any distribution over returns given suf-
ﬁcient network capacity. Our approach, implicit quantile
networks (IQN), is best viewed as a simple distributional
generalization of the DQN algorithm (Mnih et al., 2015),
and provides several beneﬁts over QR-DQN.

First, the approximation error for the distribution is no
longer controlled by the number of quantiles output by
the network, but by the size of the network itself, and the
amount of training. Second, IQN can be used with as few, or
as many, samples per update as desired, providing improved
data efﬁciency with increasing number of samples per train-
ing update. Third, the implicit representation of the return
distribution allows us to expand the class of policies to more
fully take advantage of the learned distribution. Speciﬁcally,
by taking the base distribution to be non-uniform, we ex-
pand the class of policies to (cid:15)-greedy policies on arbitrary
distortion risk measures (Yaari, 1987; Wang, 1996).

Implicit Quantile Networks for Distributional Reinforcement Learning

We begin by reviewing distributional reinforcement learn-
ing, related work, and introducing the concepts surround-
ing risk-sensitive RL. In subsequent sections, we introduce
our proposed algorithm, IQN, and present a series of ex-
periments using the Atari-57 benchmark, investigating the
robustness and performance of IQN. Despite being a simple
distributional extension to DQN, and forgoing any other im-
provements, IQN signiﬁcantly outperforms QR-DQN and
nearly matches the performance of Rainbow, which com-
bines many orthogonal advances. In fact, in human-starts
as well as in the hardest Atari games (where current RL
agents still underperform human players) IQN improves
over Rainbow.

2. Background / Related Work

We consider the standard RL setting, in which the interaction
of an agent and an environment is modeled as a Markov
, R, P, γ) (Puterman, 1994), where
Decision Process (
denote the state and action spaces, R the (state- and
x, a) the transition
x)

X
action-dependent) reward function, P (
kernel, and γ
maps a state to a distribution over actions.

(0, 1) a discount factor. A policy π(

and

A

A

X

∈

·|

·|

,

For an agent following policy π,
the discounted sum
of future rewards is denoted by the random variable
Z π(x, a) = (cid:80)∞t=0 γtR(xt, at), where x0 = x, a0 = a,
xt). The action-value
xt
∼
function is deﬁned as Qπ(x, a) = E [Z π(x, a)], and can be
characterized by the Bellman equation

1), and at

1, at

P (

π(

xt

∼

·|

·|

−

−

Qπ(x, a) = E [R(x, a)] + γEP,π [Qπ(x(cid:48), a(cid:48))] .

The objective in RL is to ﬁnd an optimal policy π∗, which
maximizes E[Z π], i.e. Qπ∗
Qπ(x, a) for all π and
(x, a)
all x, a. One approach is to ﬁnd the unique ﬁxed point
Q∗ = Qπ∗
of the Bellman optimality operator (Bellman,
1957):

≥

Q(x, a) =

T

Q(x, a) := E [R(x, a)] + γEP max
a(cid:48)

Q(x(cid:48), a(cid:48)).

To this end, Q-learning (Watkins, 1989) iteratively improves
an estimate, Qθ, of the optimal action-value function, Q∗,
by repeatedly applying the Bellman update:

Qθ(x, a)

E [R(x, a)] + γEP

←

(cid:104)

max
a(cid:48)

(cid:105)
Qθ(x(cid:48), a(cid:48))

.

The action-value function can be approximated by a param-
eterized function Qθ (e.g. a neural network), and trained by
minimizing the squared temporal difference (TD) error,

(cid:20)

δ2
t =

rt + γ max
∈A

a(cid:48)

Qθ(xt+1, a(cid:48))

Qθ(xt, at)

,

−

(cid:21)2

over samples (xt, at, rt, xt+1) observed while following an
(cid:15)-greedy policy over Qθ. This policy acts greedily with re-
(cid:15) and uniformly at random
spect to Qθ with probability 1

−

otherwise. DQN (Mnih et al., 2015) uses a convolutional
neural network to parameterize Qθ and the Q-learning algo-
rithm to achieve human-level play on the Atari-57 bench-
mark.

2.1. Distributional RL

In distributional RL, the distribution over returns (the law
of Z π) is considered instead of the scalar value function
Qπ that is its expectation. This change in perspective has
yielded new insights into the dynamics of RL (Azar et al.,
2012), and been a useful tool for analysis (Lattimore &
Hutter, 2012). Empirically, distributional RL algorithms
show improved sample complexity and ﬁnal performance,
as well as increased robustness to hyperparameter variation
(Barth-Maron et al., 2018).

An analogous distributional Bellman equation of the form

Z π(x, a)

D
= R(x, a) + γZ π(X (cid:48), A(cid:48))

D
can be derived, where A
= B denotes that two random
variables A and B have equal probability laws, and the
random variables X (cid:48) and A(cid:48) are distributed according to
P (

x(cid:48)), respectively.

x, a) and π(

·|

·|

Morimura et al. (2010a) deﬁned the distributional Bellman
operator explicitly in terms of conditional probabilities, pa-
rameterized by the mean and scale of a Gaussian or Laplace
distribution, and minimized the Kullback-Leibler (KL) di-
vergence between the Bellman target and the current esti-
mated return distribution. However, the distributional Bell-
man operator is not a contraction in the KL.

As with the scalar setting, a distributional Bellman optimal-
ity operator can be deﬁned by

Z(x, a)

D
:= R(x, a) + γZ(X (cid:48), arg max

E Z(X (cid:48), a(cid:48))),

T

a(cid:48)

∈A

x, a). While the distri-
with X (cid:48) distributed according to P (
butional Bellman operator for policy evaluation is a contrac-
tion in the p-Wasserstein distance (Bellemare et al., 2017),
this no longer holds for the control case. Convergence to the
optimal policy can still be established, but requires a more
involved argument.

·|

Bellemare et al. (2017) parameterize the return distribution
as a categorical distribution over a ﬁxed set of equidistant
points and minimize the KL divergence to the projected
distributional Bellman target. Their algorithm, C51, outper-
formed previous DQN variants on the Atari-57 benchmark.
Subsequently, Hessel et al. (2018) combined C51 with en-
hancements such as prioritized experience replay (Schaul
et al., 2016), n-step updates (Sutton, 1988), and the dueling
architecture (Wang et al., 2016), leading to the Rainbow
agent, current state-of-the-art in Atari-57.

Implicit Quantile Networks for Distributional Reinforcement Learning

The categorical parameterization, using the projected KL
loss, has also been used in recent work to improve the critic
of a policy gradient algorithm, D4PG, achieving signiﬁ-
cantly improved robustness and state-of-the-art performance
across a variety of continuous control tasks (Barth-Maron
et al., 2018).

2.2. p-Wasserstein Metric

∈

∞

[1,

], plays a key role
The p-Wasserstein metric, for p
in recent results in distributional RL (Bellemare et al., 2017;
Dabney et al., 2018). It has also been a topic of increasing
interest in generative modeling (Arjovsky et al., 2017; Bous-
quet et al., 2017; Tolstikhin et al., 2017), because unlike the
KL divergence, the Wasserstein metric inherently trades off
approximate solutions with likelihoods.

The p-Wasserstein distance is the Lp metric on inverse cu-
mulative distribution functions (c.d.f.), also known as quan-
tile functions (M¨uller, 1997). For random variables U and
V with quantile functions F −
V , respectively, the
p-Wasserstein distance is given by

1
U and F −

1

(cid:18)(cid:90) 1

Wp(U, V ) =

1
U (ω)
F −

F −

1
V (ω)
|

pdω

−

0 |

(cid:19)1/p

.

The class of optimal transport metrics express distances
between distributions in terms of the minimal cost for trans-
porting mass to make the two distributions identical. This
R≥
0,
cost is given in terms of some metric, c :
on the underlying space
. The p-Wasserstein metric cor-
responds to c = Lp. We are particularly interested in the
Wasserstein metrics due to the predominant use of Lp spaces
in mean-value reinforcement learning.

X × X →

X

2.3. Quantile Regression for Distributional RL

Bellemare et al. (2017) showed that the distributional Bell-
man operator is a contraction in the p-Wasserstein metric,
but as the proposed algorithm did not itself minimize the
Wasserstein metric, this left a theory-practice gap for distri-
butional RL. Recently, this gap was closed, in both direc-
tions. First and most relevant to this work, Dabney et al.
(2018) proposed the use of quantile regression for distribu-
tional RL and showed that by choosing the quantile targets
suitably the resulting projected distributional Bellman op-
erator is a contraction in the
-Wasserstein metric. Con-
currently, Rowland et al. (2018) showed the original class
of categorical algorithms are a contraction in the Cram´er
distance, the L2 metric on cumulative distribution functions.

∞

By estimating the quantile function at precisely chosen
points, QR-DQN minimizes the Wasserstein distance to
the distributional Bellman target (Dabney et al., 2018). This
estimation uses quantile regression, which has been shown
to converge to the true quantile function value when mini-

mized using stochastic approximation (Koenker, 2005).

In QR-DQN, the random return is approximated by a uni-
form mixture of N Diracs,

Zθ(x, a) := 1
N

δθi(x,a),

N
(cid:88)

i=1

with each θi assigned a ﬁxed quantile target, ˆτi = τi−1+τi
N , where τi = i/N . These quantile estimates
for 1
are trained using the Huber (1964) quantile regression loss,
with threshold κ,

≤

≤

i

2

ρκ
τ (δij) =

κ(δij) =

L

I
τ
−
|
{
(cid:40) 1
2 δ2
ij,
δij
κ(
|

δij < 0

L

}|

1
2 κ),

| −

, with

κ(δij)
κ
if
δij
| ≤
|
otherwise

κ

,

on the pairwise TD-errors

δij = r + γθj(x(cid:48), π(x(cid:48)))

θi(x, a).

−

At the time of this writing, QR-DQN achieves the best per-
formance on Atari-57, human-normalized mean and median,
of all agents that do not combine distributional RL, priori-
tized replay, and n-step updates (Dabney et al., 2018; Hessel
et al., 2018; Gruslys et al., 2018).

2.4. Risk in Reinforcement Learning

Distributional RL algorithms have been theoretically jus-
tiﬁed for the Wasserstein and Cram´er metrics (Bellemare
et al., 2017; Rowland et al., 2018), and learning the dis-
tribution over returns, in and of itself, empirically results
in signiﬁcant improvements to data efﬁciency, ﬁnal perfor-
mance, and stability (Bellemare et al., 2017; Dabney et al.,
2018; Gruslys et al., 2018; Barth-Maron et al., 2018). How-
ever, in each of these recent works the policy used was
based entirely on the mean of the return distribution, just
as in standard reinforcement learning. A natural question
arises: can we expand the class of policies using information
provided by the distribution over returns (i.e. to the class
of risk-sensitive policies)? Furthermore, when would this
larger policy class be beneﬁcial?

Here, ‘risk’ refers to the uncertainty over possible outcomes,
and risk-sensitive policies are those which depend upon
more than the mean of the outcomes. At this point, it is
important to highlight the difference between intrinsic un-
certainty, captured by the distribution over returns, and
parametric uncertainty, the uncertainty over the value es-
timate typically associated with Bayesian approaches such
as PSRL (Osband et al., 2013) and Kalman TD (Geist &
Pietquin, 2010). Distributional RL seeks to capture the

Implicit Quantile Networks for Distributional Reinforcement Learning

former, which classic approaches to risk are built upon1.

Expected utility theory states that if a decision policy is
consistent with a particular set of four axioms regarding
its choices then the decision policy behaves as though it is
maximizing the expected value of some utility function U
(von Neumann & Morgenstern, 1947),

π(x) = arg max

a

E
Z(x,a)

[U (z)].

This is perhaps the most pervasive notion of risk-sensitivity.
A policy maximizing a linear utility function is called risk-
neutral, whereas concave or convex utility functions give
rise to risk-averse or risk-seeking policies, respectively.
Many previous studies on risk-sensitive RL adopt the util-
ity function approach (Howard & Matheson, 1972; Marcus
et al., 1997; Maddison et al., 2017).

(cid:31)

A crucial axiom of expected utility is independence: given
random variables X, Y and Z, such that X
Y (X pre-
ferred over Y ), any mixture between X and Z is preferred to
the same mixture between Y and Z (von Neumann & Mor-
genstern, 1947). Stated in terms of the cumulative probabil-
ity functions, αFX +(1
∈
[0, 1]. This axiom in particular has troubled many re-
searchers because it is consistently violated by human be-
havior (Tversky & Kahneman, 1992). The Allais paradox
is a frequently used example of a decision problem where
people violate the independence axiom of expected utility
theory (Allais, 1990).

αFY +(1

α)FZ,

α)FZ

−

≥

−

α

∀

However, as Yaari (1987) showed, this axiom can be re-
placed by one in terms of convex combinations of outcome
values, instead of mixtures of distributions. Speciﬁcally,
[0, 1] and random
if as before X
∈
(cid:31)
1
1
1
Z .
variable Z, αF −
Y + (1
αF −
α)F −
X + (1
This leads to an alternate, dual, theory of choice than that
of expected utility. Under these axioms the decision policy
behaves as though it is maximizing a distorted expectation,
for some continuous monotonic function h:

Y , then for any α

1
Z ≥

α)F −

−

−

π(x) = arg max

a

(h

FZ(x,a))(z) dz.

◦

(cid:90) ∞

−∞

z

∂
∂z

Such a function h is known as a distortion risk measure,
as it distorts the cumulative probabilities of the random
variable (Wang, 1996). That is, we have two fundamentally
equivalent approaches to risk-sensitivity. Either, we choose
a utility function and follow the expectation of this utility.
Or, we choose a reweighting of the distribution and compute
expectation under this distortion measure. Indeed, Yaari
(1987) further showed that these two functions are inverses
of each other. The choice between them amounts to a choice

1One exception is the recent work (Moerland et al., 2017) to-
wards combining both forms of uncertainty to improve exploration.

Figure 1. Network architectures for DQN and recent distributional
RL algorithms.

over whether the behavior should be invariant to mixing
with random events or to convex combinations of outcomes.

Distortion risk measures include, as special cases, cumula-
tive probability weighting used in cumulative prospect the-
ory (Tversky & Kahneman, 1992), conditional value at risk
(Chow & Ghavamzadeh, 2014), and many other methods
(Morimura et al., 2010b). Recently Majumdar & Pavone
(2017) argued for the use of distortion risk measures in
robotics.

3. Implicit Quantile Networks

We now introduce the implicit quantile network (IQN), a
deterministic parametric function trained to reparameterize
samples from a base distribution, e.g. τ
U ([0, 1]), to
the respective quantile values of a target distribution. IQN
provides an effective way to learn an implicit representa-
tion of the return distribution, yielding a powerful function
approximator for a new DQN-like agent.

∼

1

Z (τ ) be the quantile function at τ

[0, 1] for the
Let F −
random variable Z. For notational simplicity we write
1
U ([0, 1]) the resulting state-
Zτ := F −
action return distribution sample is Zτ (x, a)

Z (τ ), thus for τ

Z(x, a).

∼

∈

∼

We propose to model the state-action quantile function as
a mapping from state-actions and samples from some base
distribution, typically τ
U ([0, 1]), to Zτ (x, a), viewed as
samples from the implicitly deﬁned return distribution.

∼

Let β : [0, 1]
[0, 1] be a distortion risk measure, with
identity corresponding to risk-neutrality. Then, the distorted
expectation of Z(x, a) under β is given by

→

Qβ(x, a) :=

E
U ([0,1])

τ

∼

(cid:2)Zβ(τ )(x, a)(cid:3) .

1

Notice that the distorted expectation is equal to the ex-
pected value of F −
Z(x,a) weighted by β, that is, Qβ =
(cid:82) 1
Z (τ )dβ(τ ). The immediate implication of this is that
0 F −
for any β, there exists a sampling distribution for τ such that
the mean of Zτ is equal to the distorted expectation of Z

1

Implicit Quantile Networks for Distributional Reinforcement Learning

under β, that is, any distorted expectation can be represented
as a weighted sum over the quantiles (Dhaene et al., 2012).
Denote by πβ the risk-sensitive greedy policy

πβ(x) = arg max

Qβ(x, a).

(1)

a

∈A

For two samples τ, τ (cid:48)
∼
pled temporal difference (TD) error at step t is

U ([0, 1]), and policy πβ, the sam-

(cid:12)

computed by the convolutional layers and f : Rd
R|A|
the subsequent fully-connected layers mapping ψ(x) to the
estimated action-values, such that Q(x, a)
f (ψ(x))a. For
our network we use the same functions ψ and f as in DQN,
Rd comput-
but include an additional function φ : [0, 1]
ing an embedding for the sample point τ . We combine these
to form the approximation Zτ (x, a)
φ(τ ))a,
where

denotes the element-wise (Hadamard) product.

f (ψ(x)

→

→

≈

≈

(cid:12)

(cid:12)

(cid:12)

As the network for f is not particularly deep, we use the
multiplicative form, ψ
φ, to force interaction between the
convolutional features and the sample embedding. Alter-
native functional forms, e.g. concatenation or a ‘residual’
function ψ
(1 + φ), are conceivable, and φ(τ ) can be
parameterized in different ways. To investigate these, we
compared performance across a number of architectural
variants on six Atari 2600 games (ASTERIX, ASSAULT,
BREAKOUT, MS.PACMAN, QBERT, SPACE INVADERS).
Full results are given in the Appendix. Despite minor varia-
tion in performance, we found the general approach to be
robust to the various choices. Based upon the results we
used the following function in our later experiments, for
embedding dimension n = 64:

n
1
(cid:88)
−
φj(τ ) := ReLU(

i=0

cos(πiτ )wij + bj).

(4)

After settling on a network architecture, we study the effect
of the number of samples, N and N (cid:48), used in the estimate
terms of Equation 3.

We hypothesized that N , the number of samples of τ
∼
U ([0, 1]), would affect the sample complexity of IQN, with
larger values leading to faster learning, and that with N = 1
one would potentially approach the performance of DQN.
This would support the hypothesis that the improved perfor-
mance of many distributional RL algorithms rests on their
effect as auxiliary loss functions, which would vanish in
the case of N = 1. Furthermore, we believed that N (cid:48), the
U ([0, 1]), would affect the vari-
number of samples of τ (cid:48)
ance of the gradient estimates much like a mini-batch size
hyperparameter. Our prediction was that N (cid:48) would have the
greatest effect on variance of the long-term performance of
the agent.

∼

We used the same set of six games as before, with our
chosen architecture, and varied N, N (cid:48)
. In
}
Figure 2 we report the average human-normalized scores on
the six games for each conﬁguration. Figure 2 (left) shows
the average performance over the ﬁrst ten million frames,
while (right) shows the average performance over the last
ten million (from 190M to 200M).

1, 8, 32, 64

∈ {

As expected, we found that N has a dramatic effect on early
performance, shown by the continual improvement in score
as the value increases. Additionally, we observed that N (cid:48)

δτ,τ (cid:48)
t = rt + γZτ (cid:48)(xt+1, πβ(xt+1))

Zτ (xt, at).

(2)

−

Then, the IQN loss function is given by

(xt, at, rt, xt+1) =

L

1
N (cid:48)

N
(cid:88)

N (cid:48)
(cid:88)

i=1

j=1

(cid:16)

τi,τ (cid:48)
j
t

δ

(cid:17)

,

ρκ
τi

(3)

where N and N (cid:48) denote the respective number of iid sam-
U ([0, 1]) used to estimate the loss. A cor-
ples τi, τ (cid:48)j ∼
responding sample-based risk-sensitive policy is obtained
by approximating Qβ in Equation 1 by K samples of
˜τ

U ([0, 1]):

∼

˜πβ(x) = arg max

Zβ(˜τk)(x, a).

1
K

K
(cid:88)

k=1

a

∈A

≈

Implicit quantile networks differ from the approach of Dab-
ney et al. (2018) in two ways. First, instead of approximat-
ing the quantile function at n ﬁxed values of τ we approxi-
mate it with Zτ (x, a)
f (ψ(x), φ(τ ))a for some differen-
tiable functions f , ψ, and φ. If we ignore the distributional
interpretation for a moment and view each Zτ (x, a) as a
separate action-value function, this highlights that implicit
quantile networks are a type of universal value function
approximator (UVFA) (Schaul et al., 2015). There may
be additional beneﬁts to implicit quantile networks beyond
the obvious increase in representational ﬁdelity. As with
UVFAs, we might hope that training over many different
τ ’s (goals in the case of the UVFA) leads to better gener-
alization between values and improved sample complexity
than attempting to train each separately.

Second, τ , τ (cid:48), and ˜τ are sampled from continuous, inde-
pendent, distributions. Besides U ([0, 1]), we also explore
risk-sentive policies πβ, with non-linear β. The independent
sampling of each τ , τ (cid:48) results in the sample TD errors being
decorrelated, and the estimated action-values go from being
the true mean of a mixture of n Diracs to a sample mean
of the implicit distribution deﬁned by reparameterizing the
sampling distribution via the learned quantile function.

3.1. Implementation

Consider the neural network structure used by the DQN
Rd be the function
agent (Mnih et al., 2015). Let ψ :

X →

Implicit Quantile Networks for Distributional Reinforcement Learning

Second, we consider the distortion risk measure proposed by
1 are taken to be the standard
Wang (2000), where Φ and Φ−
Normal cumulative distribution function and its inverse:

Wang(η, τ ) = Φ(Φ−

1(τ ) + η).

For η < 0, this produces risk-averse policies and we in-
clude it due to its simple interpretation and ability to switch
between risk-averse and risk-seeking distortions.

Third, we consider a simple power formula for risk-averse
(η < 0) or risk-seeking (η > 0) policies:

Pow(η, τ ) =

(cid:40)
τ

1
1+|η| ,

1

(1

−

−

1
1+|η| ,

τ )

0

if η
≥
otherwise

.

Finally, we consider conditional value-at-risk (CVaR):

CVaR(η, τ ) = ητ.

CVaR has been widely studied in and out of reinforcement
learning (Chow & Ghavamzadeh, 2014). Its implementation
as a modiﬁcation to the sampling distribution of τ is partic-
U ([0, η]).
ularly simple, as it changes τ
Another interesting sampling distribution, not included in
our experiments, is denoted Norm(η) and corresponds to τ
sampled by averaging η samples from U ([0, 1]).

U ([0, 1]) to τ

∼

∼

In Figure 3 (right) we give an example of a distribution
(Neutral) and how each of these distortion measures affects
the implied distribution due to changing the sampling distri-
bution of τ . Norm(3) and CPW(.71) reduce the impact of
the tails of the distribution, while Wang and CVaR heavily
shift the distribution mass towards the tails, creating a risk-
averse or risk-seeking preference. Additionally, while CVaR
entirely ignores all values corresponding to τ > η, Wang
gives these non-zero, but vanishingly small, probability.

By using these sampling distributions we can induce various
risk-sensitive policies in IQN. We evaluate these on the same
set of six Atari 2600 games previously used. Our algorithm
simply changes the policy to maximize the distorted expec-
tations instead of the usual sample mean. Figure 3 (left)
shows our results in this experiment, with average scores
reported under the usual, risk-neutral, evaluation criterion.

Intuitively, we expected to see a qualitative effect from
risk-sensitive training, e.g. strengthened exploration from a
risk-seeking objective. Although we did see qualitative dif-
ferences, these did not always match our expectations. For
two of the games, ASTERIX and ASSAULT, there is a very
signiﬁcant advantage to the risk-averse policies. Although
CPW tends to perform almost identically to the standard
risk-neutral policy, and the risk-seeking Wang(1.5) per-
forms as well or worse than risk-neutral, we ﬁnd that both
risk-averse policies improve performance over standard IQN.
However, we also observe that the more risk-averse of the

Figure 2. Effect of varying N and N (cid:48), the number of samples used
in the loss function in Equation 3. Figures show human-normalized
agent performance, averaged over six Atari games, averaged over
ﬁrst 10M frames of training (left) and last 10M frames of training
(right). Corresponding values for baselines: DQN (32, 253) and
QR-DQN (144, 1243).

affected performance very differently than expected: it had
a strong effect on early performance, but minimal impact
on long-term performance past N (cid:48) = 8.

Overall, while using more samples for both distributions is
generally favorable, N = N (cid:48) = 8 appears to be sufﬁcient
to achieve the majority of improvements offered by IQN
for long-term performance, with variation past this point
largely insigniﬁcant. To our surprise we found that even for
N = N (cid:48) = 1, which is comparable to DQN in the number
of loss components, the longer term performance is still
quite strong (

DQN).

3
×

≈

In an informal evaluation, we did not ﬁnd IQN to be sensi-
tive to K, the number of samples used for the policy, and
have ﬁxed it at K = 32 for all experiments.

4. Risk-Sensitive Reinforcement Learning

In this section, we explore the effects of varying the distor-
tion risk measure, β, away from identity. This only affects
the policy, πβ, used both in Equation 2 and for acting in the
environment. As we have argued, evaluating under different
distortion risk measures is equivalent to changing the sam-
pling distribution for τ , allowing us to achieve various forms
of risk-sensitive policies. We focus on a handful of sampling
distributions and their corresponding distortion measures.
The ﬁrst one is the cumulative probability weighting param-
eterization proposed in cumulative prospect theory (Tversky
& Kahneman, 1992; Gonzalez & Wu, 1999):

CPW(η, τ ) =

τ η
(τ η + (1

−

.

τ )η)

1
η

In particular, we use the parameter value η = 0.71 found
by Wu & Gonzalez (1996) to most closely match human
subjects. This choice is interesting as, unlike the others we
consider, it is neither globally convex nor concave. For small
values of τ it is locally concave and for larger values of τ it
becomes locally convex. Recall that concavity corresponds
to risk-averse and convexity to risk-seeking policies.

Implicit Quantile Networks for Distributional Reinforcement Learning

Figure 3. Effects of various changes to the sampling distribution, that is various cumulative probability weightings.

two, CVaR(0.1), suffers some loss in performance on two
other games (QBERT and SPACE INVADERS).

Additionally, we note that the risk-seeking policy signif-
icantly underperforms the risk-neutral policy on three of
the six games. It remains an open question as to exactly
why we see improved performance for risk-averse policies.
There are many possible explanations for this phenomenon,
e.g. that risk-aversion encodes a heuristic to stay alive longer,
which in many games is correlated with increased rewards.

5. Full Atari-57 Results

Finally, we evaluate IQN on the full Atari-57 benchmark,
comparing with the state-of-the-art performance of Rainbow,
a distributional RL agent that combines several advances in
deep RL (Hessel et al., 2018), the closely related algorithm
QR-DQN (Dabney et al., 2018), prioritized experience re-
play DQN (Schaul et al., 2016), and the original DQN agent
(Mnih et al., 2015). Note that in this section we use the
risk-neutral variant of the IQN, that is, the policy of the IQN
agent is the regular (cid:15)-greedy policy with respect to the mean
of the state-action return distribution.

It is important to remember that Rainbow builds upon the
distributional RL algorithm C51 (Bellemare et al., 2017),
but also includes prioritized experience replay (Schaul et al.,
2016), Double DQN (van Hasselt et al., 2016), Dueling
Network architecture (Wang et al., 2016), Noisy Networks
(Fortunato et al., 2017), and multi-step updates (Sutton,
1988). In particular, besides the distributional update, n-
step updates and prioritized experience replay were found to
have signiﬁcant impact on the performance of Rainbow. Our
other competitive baseline is QR-DQN, which is currently
state-of-the-art for agents that do not combine distributional
updates, n-step updates, and prioritized replay.

Thus, between QR-DQN and the much more complex Rain-

bow we compare to the two most closely related, and best
performing, agents in published work. In particular, we
would expect that IQN would beneﬁt from the additional
enhancements in Rainbow, just as Rainbow improved sig-
niﬁcantly over C51.

Figure 4 shows the mean (left) and median (right) human-
normalized scores during training over the Atari-57 bench-
mark. IQN dramatically improves over QR-DQN, which
itself improves on many previously published results. At
100 million frames IQN has reached the same level of perfor-
mance as QR-DQN at 200 million frames. Table 1 gives a
comparison between the same methods in terms of their best,
human-normalized, scores per game under the 30 random
no-op start condition. These are averages over the given
number of seeds. Additionally, using human-starts, IQN
achieves 162% median human-normalized score, whereas
Rainbow reaches 153% (Hessel et al., 2018), see Table 2.

Mean Median Human Gap
0.334
228%
DQN
0.178
434%
PRIOR.
0.152
C51
701%
RAINBOW 1189%
0.144
QR-DQN
0.165
864%
0.141
1019%
IQN

79%
124%
178%
230%
193%
218%

Seeds
1
1
1
2
3
5

Table 1. Mean and median of scores across 57 Atari 2600 games,
measured as percentages of human baseline (Nair et al., 2015).
Scores are averages over number of seeds.

Human-starts (median)

DQN PRIOR. A3C
68%

C51
128% 116% 125%

RAINBOW
153%

IQN
162%

Table 2. Median human-normalized scores for human-starts.

Implicit Quantile Networks for Distributional Reinforcement Learning

Figure 4. Human-normalized mean (left) and median (right) scores on Atari-57 for IQN and various other algorithms. Random seeds
shown as traces, with IQN averaged over 5, QR-DQN over 3, and Rainbow over 2 random seeds.

Finally, we took a closer look at the games in which each al-
gorithm continues to underperform humans, and computed,
on average, how far below human-level they perform2. We
refer to this value as the human-gap3 metric and give results
in Table 1. Interestingly, C51 outperforms QR-DQN in this
metric, and IQN outperforms all others. This shows that the
remaining gap between Rainbow and IQN is entirely from
games on which both algorithms are already super-human.
The games where the most progress in RL is needed happen
to be the games where IQN shows the greatest improvement
over QR-DQN and Rainbow.

6. Discussion and Conclusions

We have proposed a generalization of recent work based
around using quantile regression to learn the distribution
over returns of the current policy. Our generalization leads
to a simple change to the DQN agent to enable distribu-
tional RL, the natural integration of risk-sensitive policies,
and signiﬁcantly improved performance over existing meth-
ods. The IQN algorithm provides, for the ﬁrst time, a fully
integrated distributional RL agent without prior assumptions
on the parameterization of the return distribution.

IQN can be trained with as little as a single sample from
each state-action value distribution, or as many as computa-
tional limits allow to improve the algorithm’s data efﬁciency.
Furthermore, IQN allows us to expand the class of control
policies to a large class of risk-sensitive policies connected
to distortion risk measures. Finally, we show substantial
gains on the Atari-57 benchmark over QR-DQN, and even
halving the distance between QR-DQN and Rainbow.

Despite the signiﬁcant empirical successes in this paper

2Details of how this is computed can be found in the Appendix.
3Thanks to Joseph Modayil for proposing this metric.

there are many areas in need of additional theoretical analy-
sis. We highlight a few particularly relevant open questions
we were unable to address in the present work. First, sample-
based convergence results have been recently shown for a
class of categorical distributional RL algorithms (Rowland
et al., 2018). Could existing sample-based RL convergence
results be extended to the QR-based algorithms?

Second, can the contraction mapping results for a ﬁxed grid
of quantiles given by Dabney et al. (2018) be extended to
the more general class of approximate quantile functions
studied in this work? Finally, and particularly salient to
our experiments with distortion risk measures, theoretical
guarantees for risk-sensitive RL have been building over
recent years, but have been largely limited to special cases
and restricted classes of risk-sensitive policies. Can the
convergence of the distribution of returns under the Bellman
operator be leveraged to show convergence to a ﬁxed-point
in distorted expectations? In particular, can the control
results of Bellemare et al. (2017) be expanded to cover
some class of risk-sensitive policies?

There remain many intriguing directions for future research
into distributional RL, even on purely empirical fronts. Hes-
sel et al. (2018) recently showed that distributional RL
agents can be signiﬁcantly improved, when combined with
other techniques. Creating a Rainbow-IQN agent could
yield even greater improvements on Atari-57. We also recall
the surprisingly rich return distributions found by Barth-
Maron et al. (2018), and hypothesize that the continuous
control setting may be a particularly fruitful area for the
application of distributional RL in general, and IQN in par-
ticular.

Implicit Quantile Networks for Distributional Reinforcement Learning

References

Allais, M. Allais paradox. In Utility and Probability, pp.

3–9. Springer, 1990.

Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein
In Proceedings of
Generative Adversarial Networks.
the 34th International Conference on Machine Learning
(ICML), 2017.

Azar, M. G., Munos, R., and Kappen, H. J. On the sample
complexity of reinforcement learning with a generative
model. In Proceedings of the International Conference
on Machine Learning (ICML), 2012.

Barth-Maron, G., Hoffman, M. W., Budden, D., Dabney, W.,
Horgan, D., TB, D., Muldal, A., Heess, N., and Lillicrap,
T. Distributional policy gradients. In Proceedings of the
International Conference on Learning Representations
(ICLR), 2018.

Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.
The Arcade Learning Environment: an evaluation plat-
form for general agents. Journal of Artiﬁcial Intelligence
Research, 47:253–279, 2013.

Bellemare, M. G., Dabney, W., and Munos, R. A distribu-
tional perspective on reinforcement learning. Proceedings
of the 34th International Conference on Machine Learn-
ing (ICML), 2017.

Bellman, R. E. Dynamic Programming. Princeton Univer-

sity Press, Princeton, NJ, 1957.

Bousquet, O., Gelly, S., Tolstikhin, I., Simon-Gabriel, C.-J.,
and Schoelkopf, B. From optimal transport to gener-
the vegan cookbook. arXiv preprint
ative modeling:
arXiv:1705.07642, 2017.

Chow, Y. and Ghavamzadeh, M. Algorithms for CVaR opti-
mization in MDPs. In Advances in Neural Information
Processing Systems, pp. 3509–3517, 2014.

Dabney, W., Rowland, M., Bellemare, M. G., and Munos,
R. Distributional reinforcement learning with quantile
regression. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, 2018.

Dhaene, J., Kukush, A., Linders, D., and Tang, Q. Remarks
on quantiles and distortion risk measures. European
Actuarial Journal, 2(2):319–328, 2012.

Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I.,
Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin,
O., et al. Noisy networks for exploration. arXiv preprint
arXiv:1706.10295, 2017.

Gonzalez, R. and Wu, G. On the shape of the probability
weighting function. Cognitive Psychology, 38(1):129–
166, 1999.

Gruslys, A., Dabney, W., Azar, M. G., Piot, B., Bellemare,
M. G., and Munos, R. The Reactor: a fast and sample-
efﬁcient actor-critic agent for reinforcement learning. In
Proceedings of the International Conference on Learning
Representations (ICLR), 2018.

Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Os-
trovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M.,
and Silver, D. Rainbow: combining improvements in
deep reinforcement learning. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, 2018.

Howard, R. A. and Matheson, J. E. Risk-sensitive markov
decision processes. Management Science, 18(7):356–369,
1972.

Huber, P. J. Robust estimation of a location parameter. The
Annals of Mathematical Statistics, 35(1):73–101, 1964.

Jaquette, S. C. Markov decision processes with a new opti-
mality criterion: discrete time. The Annals of Statistics, 1
(3):496–505, 1973.

Koenker, R. Quantile Regression. Cambridge University

Press, 2005.

Lattimore, T. and Hutter, M. PAC bounds for discounted
In International Conference on Algorithmic

MDPs.
Learning Theory, pp. 320–334. Springer, 2012.

Maddison, C. J., Lawson, D., Tucker, G., Heess, N., Doucet,
A., Mnih, A., and Teh, Y. W. Particle value functions.
arXiv preprint arXiv:1703.05820, 2017.

Majumdar, A. and Pavone, M. How should a robot assess
risk? Towards an axiomatic theory of risk in robotics.
arXiv preprint arXiv:1710.11040, 2017.

Marcus, S. I., Fern´andez-Gaucherand, E., Hern´andez-
Hernandez, D., Coraluppi, S., and Fard, P. Risk sensitive
markov decision processes. In Systems and Control in
the Twenty-First Century, pp. 263–279. Springer, 1997.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-
land, A. K., Ostrovski, G., et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):
529–533, 2015.

Geist, M. and Pietquin, O. Kalman temporal differences.
Journal of Artiﬁcial Intelligence Research, 39:483–532,
2010.

Moerland, T. M., Broekens, J., and Jonker, C. M. Efﬁcient
exploration with double uncertain value networks. arXiv
preprint arXiv:1711.10789, 2017.

Implicit Quantile Networks for Distributional Reinforcement Learning

van Hasselt, H., Guez, A., and Silver, D. Deep reinforce-
ment learning with double Q-learning. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, 2016.

von Neumann, J. and Morgenstern, O. Theory of Games and
Economic Behavior. Princeton University Press, 1947.

Wang, S. Premium calculation by transforming the layer
premium density. ASTIN Bulletin: The Journal of the
IAA, 26(1):71–92, 1996.

Wang, S. S. A class of distortion operators for pricing ﬁnan-
cial and insurance risks. Journal of Risk and Insurance,
pp. 15–36, 2000.

Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot,
M., and de Freitas, N. Dueling network architectures
for deep reinforcement learning. In Proceedings of the
International Conference on Machine Learning (ICML),
2016.

Watkins, C. J. C. H. Learning from delayed rewards. PhD

thesis, King’s College, Cambridge, 1989.

White, D. J. Mean, variance, and probabilistic criteria in
ﬁnite markov decision processes: a review. Journal of
Optimization Theory and Applications, 56(1):1–29, 1988.

Wu, G. and Gonzalez, R. Curvature of the probability
weighting function. Management Science, 42(12):1676–
1690, 1996.

Yaari, M. E. The dual theory of choice under risk. Econo-
metrica: Journal of the Econometric Society, pp. 95–115,
1987.

Yu, K.-T., Bauza, M., Fazeli, N., and Rodriguez, A. More
than a million ways to be pushed. a high-ﬁdelity exper-
imental dataset of planar pushing. In Intelligent Robots
and Systems (IROS), 2016 IEEE/RSJ International Con-
ference on, pp. 30–37. IEEE, 2016.

Morimura, T., Hachiya, H., Sugiyama, M., Tanaka, T., and
Kashima, H. Parametric return density estimation for
reinforcement learning. In Proceedings of the Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), 2010a.

Morimura, T., Sugiyama, M., Kashima, H., Hachiya, H.,
and Tanaka, T. Nonparametric return distribution approx-
imation for reinforcement learning. In Proceedings of
the 27th International Conference on Machine Learning
(ICML), pp. 799–806, 2010b.

M¨uller, A. Integral probability metrics and their generating
classes of functions. Advances in Applied Probability, 29
(2):429–443, 1997.

Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon,
R., De Maria, A., Panneershelvam, V., Suleyman, M.,
Beattie, C., and Petersen, S. e. a. Massively parallel meth-
ods for deep reinforcement learning. In ICML Workshop
on Deep Learning, 2015.

Osband, I., Russo, D., and Van Roy, B. (more) efﬁcient
reinforcement learning via posterior sampling. In Ad-
vances in Neural Information Processing Systems, pp.
3003–3011, 2013.

Puterman, M. L. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. John Wiley & Sons,
Inc., 1994.

Rowland, M., Bellemare, M. G., Dabney, W., Munos, R.,
and Teh, Y. W. An analysis of categorical distributional
reinforcement learning. In AISTATS, 2018.

Schaul, T., Horgan, D., Gregor, K., and Silver, D. Uni-
In International
versal value function approximators.
Conference on Machine Learning, pp. 1312–1320, 2015.

Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prior-
itized experience replay. In Proceedings of the Interna-
tional Conference on Learning Representations (ICLR),
2016.

Sobel, M. J. The variance of discounted markov decision
processes. Journal of Applied Probability, 19(04):794–
802, 1982.

Sutton, R. S. Learning to predict by the methods of temporal

differences. Machine Learning, 3(1):9–44, 1988.

Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf,
arXiv preprint

B. Wasserstein auto-encoders.
arXiv:1711.01558, 2017.

Tversky, A. and Kahneman, D. Advances in prospect theory:
cumulative representation of uncertainty. Journal of Risk
and Uncertainty, 5(4):297–323, 1992.

Implicit Quantile Networks for Distributional Reinforcement Learning

Appendix

Architecture and Hyperparameters

We considered multiple architectural variants for parame-
terizing an IQN. All of these build on the Q-network of a
regular DQN (Mnih et al., 2015), which can be seen as the
Rd and an
composition of a convolutional stack ψ :
X →
R|A|, and extend it by an embedding of
MLP f : Rd
Rd, and a merging function
the sample point, φ : [0, 1]
m : Rd

Rd, resulting in the function

Rd

→

→

×

→
IQN(x, τ ) = f (m(ψ(x), φ(τ ))).

For the embedding φ, we considered a number of variants: a
learned linear embedding, a learned MLP embedding with a
single hidden layer of size n, and a learned linear function of
n cosine basis functions of the form cos(πiτ ), i = 1, . . . , n.
Each of those was followed by either a ReLU or sigmoid
nonlinearity.

For the merging function m, the simplest choice would
be a simple vector concatenation of ψ(x) and φ(τ ). Note
however, that the MLP f which takes in the output of m and
outputs the action-value quantiles, only has a single hidden
layer in the DQN network. Therefore, to force a sufﬁciently
early interaction between the two representations, we also
φ,
considered a multiplicative function m(ψ, φ) = ψ
where
denotes the element-wise (Hadamard) product of
two vectors, as well as a ‘residual’ function m(ψ, φ) =
ψ

(1 + φ).

(cid:12)

(cid:12)

(cid:12)

Early experiments showed that a simple linear embedding
of τ was insufﬁcient to achieve good performance, and the
residual version of m didn’t show any marked difference
to the multiplicative variant, so we do not include results
for these here. For the other conﬁgurations, Figure 5 shows
pairwise comparisons between 1) a cosine basis function
embedding and a completely learned MLP embedding, 2)
an embedding size (hidden layer size or number of cosine
basis elements) 32 and 64, 3) ReLU and sigmoid nonlinear-
ity following the embedding, and 4) concatenation and a

multiplicative interaction between ψ(x) and φ(τ ).

Each comparison ‘violin plot’ can be understood as a
marginalization over the other variants of the architecture,
with the human-normalized performance at the end of train-
ing, averaged across six Atari 2600 games, on the y-axis.
Each white dot corresponds to a conﬁguration (each repre-
sented by two seeds), the black dots show the position of our
preferred conﬁguration. The width of the colored regions
corresponds to a kernel density estimate of the number of
conﬁgurations at each performance level.

Our ﬁnal choice is a multiplicative interaction with a linear
function of a cosine embedding, with n = 64 and a ReLU
nonlinearity (see Equation 4), as this conﬁguration yielded
the highest performance consistently over multiple seeds.
Also noteworthy is the overall robustness of the approach to
these variations: most of the conﬁgurations consistently out-
perform the QR-DQN baseline shown as a grey horizontal
line for comparison.

We give pseudo-code for the IQN loss in Algorithm 1. All
other hyperparameters for this agent correspond to the ones
used by Dabney et al. (2018). In particular, the Bellman tar-
get is computed using a target network. Notice that IQN will
generally be more computationally expensive per-sample
than QR-DQN. However, in practice IQN requires many
fewer samples per update than QR-DQN so that the actual
running times are comparable.

Algorithm 1 Implicit Quantile Network Loss

Require: N, N (cid:48), K, κ and functions β, Z
input x, a, r, x(cid:48), γ

[0, 1)

∈
1
K

(cid:80)K

arg maxa(cid:48)

k Z˜τk (x(cid:48), a(cid:48)),

# Compute greedy next action
a∗
←
# Sample quantile thresholds
N (cid:48)
1
U ([0, 1]),
τi, τ (cid:48)j ∼
# Compute distributional temporal differences
δij
# Compute Huber quantile loss

Zτi(x, a),

r + γZτ (cid:48)

N, 1

i, j

˜τk

≤

∼

≤

≤

≤

∀

j

i

←
output (cid:80)N
i=1

j (x(cid:48), a∗)
(cid:2)ρκ

−
τi(δij)(cid:3)

Eτ (cid:48)

)
β(
·

Figure 5. Comparison of architectural variants.

Implicit Quantile Networks for Distributional Reinforcement Learning

Evaluation

The human-normalized scores reported in this paper are
given by the formula (van Hasselt et al., 2016; Dabney et al.,
2018)

score =

agent
human

random
random

,

−
−

where agent, human and random are the per-game raw
scores (undiscounted returns) for the given agent, a refer-
ence human player, and random agent baseline (Mnih et al.,
2015).

The ‘human-gap’ metric referred to at the end of Section 5
builds on the human-normalized score, but emphasizes the
remaining improvement for the agent to reach super-human
performance. It is given by gap = max(1
score, 0), with
a value of 1 corresponding to random play, and a value of
0 corresponding to super-human level of performance. To
avoid degeneracies in the case of human < random, the
quantity is being clipped above at 1.

−

Implicit Quantile Networks for Distributional Reinforcement Learning

Figure 6. Complete Atari-57 training curves.

Implicit Quantile Networks for Distributional Reinforcement Learning

GAMES
Alien
Amidar
Assault
Asterix
Asteroids
Atlantis
Bank Heist
Battle Zone
Beam Rider
Berzerk
Bowling
Boxing
Breakout
Centipede
Chopper Command
Crazy Climber
Defender
Demon Attack
Double Dunk
Enduro
Fishing Derby
Freeway
Frostbite
Gopher
Gravitar
H.E.R.O.
Ice Hockey
James Bond
Kangaroo
Krull
Kung-Fu Master
Montezumas Revenge
Ms. Pac-Man
Name This Game
Phoenix
Pitfall!
Pong
Private Eye
Q*Bert
River Raid
Road Runner
Robotank
Seaquest
Skiing
Solaris
Space Invaders
Star Gunner
Surround
Tennis
Time Pilot
Tutankham
Up and Down
Venture
Video Pinball
Wizard Of Wor
Yars Revenge
Zaxxon

RANDOM
227.8
5.8
222.4
210.0
719.1
12,850.0
14.2
2,360.0
363.9
123.7
23.1
0.1
1.7
2,090.9
811.0
10,780.5
2,874.5
152.1
-18.6
0.0
-91.7
0.0
65.2
257.6
173.0
1,027.0
-11.2
29.0
52.0
1,598.0
258.5
0.0
307.3
2,292.3
761.4
-229.4
-20.7
24.9
163.9
1,338.5
11.5
2.2
68.4
-17,098.1
1,236.3
148.0
664.0
-10.0
-23.8
3,568.0
11.4
533.4
0.0
16,256.9
563.5
3,092.9
32.5

HUMAN
7,127.7
1,719.5
742.0
8,503.3
47,388.7
29,028.1
753.1
37,187.5
16,926.5
2,630.4
160.7
12.1
30.5
12,017.0
7,387.8
35,829.4
18,688.9
1,971.0
-16.4
860.5
-38.7
29.6
4,334.7
2,412.5
3,351.4
30,826.4
0.9
302.8
3,035.0
2,665.5
22,736.3
4,753.3
6,951.6
8,049.0
7,242.6
6,463.7
14.6
69,571.3
13,455.0
17,118.0
7,845.0
11.9
42,054.7
-4,336.9
12,326.7
1,668.7
10,250.0
6.5
-8.3
5,229.2
167.6
11,693.2
1,187.5
17,667.9
4,756.5
54,576.9
9,173.3

DQN
1,620.0
978.0
4,280.4
4,359.0
1,364.5
279,987.0
455.0
29,900.0
8,627.5
585.6
50.4
88.0
385.5
4,657.7
6,126.0
110,763.0
23,633.0
12,149.4
-6.6
729.0
-4.9
30.8
797.4
8,777.4
473.0
20,437.8
-1.9
768.5
7,259.0
8,422.3
26,059.0
0.0
3,085.6
8,207.8
8,485.2
-286.1
19.5
146.7
13,117.3
7,377.6
39,544.0
63.9
5,860.6
-13,062.3
3,482.8
1,692.3
54,282.0
-5.6
12.2
4,870.0
68.1
9,989.9
163.0
196,760.4
2,704.0
18,098.9
5,363.0

PRIOR. DUEL.
3,941.0
2,296.8
11,477.0
375,080.0
1,192.7
395,762.0
1,503.1
35,520.0
30,276.5
3,409.0
46.7
98.9
366.0
7,687.5
13,185.0
162,224.0
41,324.5
72,878.6
-12.5
2,306.4
41.3
33.0
7,413.0
104,368.2
238.0
21,036.5
-0.4
812.0
1,792.0
10,374.4
48,375.0
0.0
3,327.3
15,572.5
70,324.3
0.0
20.9
206.0
18,760.3
20,607.6
62,151.0
27.5
931.6
-19,949.9
133.4
15,311.5
125,117.0
1.2
0.0
7,553.0
245.9
33,879.1
48.0
479,197.0
12,352.0
69,618.1
13,886.0

QR-DQN
4,871
1,641
22,012
261,025
4,226
971,850
1,249
39,268
34,821
3,117
77.2
99.9
742
12,447
14,667
161,196
47,887
121,551
21.9
2,355
39.0
34.0
4,384
113,585
995
21,395
-1.7
4,703
15,356
11,447
76,642
0.0
5,821
21,890
16,585
0.0
21.0
350
572,510
17,571
64,262
59.4
8,268
-9,324
6,740
20,972
77,495
8.2
23.6
10,345
297
71,260
43.9
705,662
25,061
26,447
13,112

IQN
7,022
2,946
29,091
342,016
2,898
978,200
1,416
42,244
42,776
1,053
86.5
99.8
734
11,561
16,836
179,082
53,537
128,580
5.6
2,359
33.8
34.0
4,324
118,365
911
28,386
0.2
35,108
15,487
10,707
73,512
0.0
6,349
22,682
56,599
0.0
21.0
200
25,750
17,765
57,900
62.5
30,140
-9,289
8,007
28,888
74,677
9.4
23.6
12,236
293
88,148
1,318
698,045
31,190
28,379
21,772

Figure 7. Raw scores for a single seed across all games, starting with 30 no-op actions. Reference values from (Wang et al., 2016).

