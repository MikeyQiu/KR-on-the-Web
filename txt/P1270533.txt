Analyzing Compositionality-Sensitivity of NLI Models

Yixin Nie* Yicheng Wang* Mohit Bansal
Department of Computer Science
University of North Carolina at Chapel Hill
{yixin1, yicheng, mbansal}@cs.unc.edu

8
1
0
2
 
v
o
N
 
6
1
 
 
]
L
C
.
s
c
[
 
 
1
v
3
3
0
7
0
.
1
1
8
1
:
v
i
X
r
a

Abstract

Success in natural language inference (NLI) should require a
model to understand both lexical and compositional seman-
tics. However, through adversarial evaluation, we ﬁnd that
several state-of-the-art models with diverse architectures are
over-relying on the former and fail to use the latter. Further,
this compositionality unawareness is not reﬂected via stan-
dard evaluation on current datasets. We show that remov-
ing RNNs in existing models or shufﬂing input words dur-
ing training does not induce large performance loss despite
the explicit removal of compositional information. Therefore,
we propose a compositionality-sensitivity testing setup that
analyzes models on natural examples from existing datasets
that cannot be solved via lexical features alone (i.e., on
which a bag-of-words model gives a high probability to
one wrong label), hence revealing the models’ actual com-
positionality awareness. We show that this setup not only
highlights the limited compositional ability of current NLI
models, but also differentiates model performance based on
design, e.g., separating shallow bag-of-words models from
deeper, linguistically-grounded tree-based models. Our eval-
uation setup is an important analysis tool: complementing
currently existing adversarial and linguistically driven di-
agnostic evaluations, and exposing opportunities for future
work on evaluating models’ compositional understanding.

1

Introduction

Natural Language Inference (NLI) is a task in which a sys-
tem is asked to classify the relationship between a pair of
premise and hypothesis as one of either entailment, contra-
diction, or neutral. This task is considered to be the basis
of many downstream, higher-level NLP tasks that require
complex natural language understanding such as question-
answering and summarization. Large annotated datasets
such as the Stanford Natural Language Inference (Bowman
et al. 2015) (SNLI) and the Multi-Genre Natural Language
Inference (Williams, Nangia, and Bowman 2018) (MNLI)
have promoted the development of many different neural
NLI models, including encoding and co-attention models us-
ing both sequential and recursive representations (Nie and
Bansal 2017; Choi, Yoo, and Lee 2017; Chen et al. 2017;

Gong, Luo, and Zhang 2017; Ghaeini et al. 2018; Parikh
et al. 2016; Wang, Hamza, and Florian 2017), all achieving
near human-level performance on standard datasets.

Despite their high performance, it is unclear if these mod-
els employ semantic understanding of natural language to
classify these pairs, or are simply performing word/phrase-
level pattern matching. We ﬁrst conduct investigative ex-
periments with rule-based adversaries and show empirically
that seven state-of-the-art NLI models, spanning a variety
of architectures, are all unable to recognize simple seman-
tic differences when the word-level information remains un-
changed (e.g., the swapping of the subject and object or the
addition of the same modiﬁer to different governors).1 Their
failure on these examples contrasts sharply with their high
performance on the standard evaluation set, indicating that
standard evaluation does not sufﬁciently assess sentence-
level understanding.

Next, to further show the insufﬁciency of standard eval-
uation for testing compositional understanding, we conduct
two additional experiments in which the compositional in-
formation is removed or diluted. Firstly, we train and eval-
uate the state-of-the-art models with their RNNs replaced
by fully-connected layers. Secondly, we train these models
with input words shufﬂed and evaluate them on the origi-
nal evaluation datasets. In both of the experiments, models
are still able to achieve high performance on the standard
evaluation datasets, demonstrating that standard evaluation
is unable to sufﬁciently separate models with composition-
ality understanding capabilities from those without.

We also show the limitation of adversarial evaluation se-
tups by demonstrating their narrow scope: each type of ad-
versary is only capable of testing a model’s ability to process
one speciﬁc type of compositional semantics. We show via
adversarial training that success on one type of adversary
does not generalize to other types of adversaries, but instead
induces errors caused by over-ﬁtting the training data. Thus,
while adversarial evaluation is useful for exposing the issue
of lexical over-stability, it is not a robust measure of models’
ability to understand semantic compositional information.

Hence, in order to analyze a model’s abilities to rea-

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

* indicates equal contribution.

1We release our adversaries, compositionality-sensitivity test-
ing setups, and all code at https://github.com/easonnie/
analyze-compositionality-sensitivity-NLI.

son beyond the lexical level and reveal its sensitivity to
compositional differences, we present a ‘compositionality-
sensitivity’ testing setup: we select examples for which a
bag-of-words model is misguided (assigns a high probability
to one wrong label), this allows us to directly measure how
much compositional information the model takes into con-
sideration. By effectively punishing models’ over-reliance
on lexical features, this testing setup could encourage the
development of models that are sensitive to compositional
clues regarding the pairs’ logical relationships.

We show that although our seven models and their vari-
ants have comparable performance on standard evaluation
(SNLI and MNLI), our new compositionality-sensitivity
testing differentiates them by their capability to capture
compositional information (e.g., bag-of-words-like models
perform worse than sequential models, which in turn per-
form worse than syntactic-tree based models). Unlike ad-
versarial evaluation, this setup uses natural examples that
are not conﬁned to any speciﬁc linguistic context or domain
by leveraging existing NLI datasets to the largest extent
possible for compositional testing. We hope that this new
setup could inspire the collections of datasets that control
for lexical-features, to explicitly evaluate the compositional
ability of NLI models.

We end by discussing how our compositionality-
sensitivity evaluation setup complements other recently
proposed evaluation setups. Speciﬁcally, while certain
linguistically-driven diagnostic datasets are useful in test-
ing for a model’s performance in a speciﬁc realistic setting,
model-driven evaluations such as ours gives insight into why
models succeed and fail in these speciﬁc linguistic scenarios.
The main contributions of this paper are three-fold: 1) we in-
troduce two new adversarial setups that expose current state-
of-the-art models’ inability to process simple sentence-level
semantics when lexical features give no information; 2) we
rigorously test and expose the limits of standard and adver-
sarial evaluations; 3) we propose a novel compositionality-
sensitivity test that analyzes a model’s ability to recognize
compositional semantics beyond the lexical level, and show
its effectiveness in separating models based on architecture.

2 Models and Motivation
In this section, we start with an overview of the designs of
seven recently proposed, high-performing natural language
inference models and outline several commonalities that are
counter-intuitive. We argue that these shared traits hint at
their over-reliance on lexical features for prediction, and
they are not modeling the compositional nature of language.
This motivates our further investigation in later sections.

2.1 NLI Models
Many different models have been proposed for the NLI task;
they all fall under one of two broad categories: sentence
encoding-based (sentence encoders) or co-attention based.
Sentence encoders independently encode each sentence as a
ﬁxed-length vector, and then make a prediction, while co-
attention models make inferences by jointly processing the

Model

SNLI

Type

Representation

RSE

86.47
G-TLSTM 85.04

Enc
Enc
85.88 CoAtt
88.17 CoAtt
88.10 CoAtt Recursive (syntax)
88.10 CoAtt
DR-BiLSTM 88.28 CoAtt

Sequential
Recursive (latent)
Bag-of-Words
Sequential

DAM
ESIM
S-TLSTM
DIIN

Sequential
Sequential

Table 1: Summary of the models we evaluate, including their
performance, type, and sentence representation.
‘Enc’ = Sentence Encoder ‘CoAtt’ = Co-Attention Model

premise and hypothesis. The constraint of independent pro-
cessing for sentence encoders was put in place to encourage
the development of effective ﬁxed-length sentence repre-
sentations generalizable to higher-level tasks. However, co-
attention models with recursive or sequential modeling have
achieved much better performance on popular NLI datasets.
In this paper, we analyze 7 different models spanning both
categories, which are, or were, state-of-the-art in their re-
spective category.2 We give a brief description of each model
below (see Table 1):
RSE Residual Sentence Encoder (Nie and Bansal 2017) is
an encoding-based model that ﬁrst uses multiple layers of
residually-connected BiLSTM to encode the tokens in a sen-
tence and then obtain the sentence’s ﬁxed-length represen-
tation by max pooling over RNN’s hidden states from all
timesteps. It is one of the top performing sentence encoders
on the Multi-NLI dataset.
G-TLSTM Gumbel-TreeLSTM (Choi, Yoo, and Lee 2017)
is a recursive encoding-based model that learns latent-tree
representations for sentences via reinforcement learning.
DAM Decomposable Attention Model (Parikh et al. 2016)
is a light-weight co-attention model that performs cross-
attention at the word level with decomposable matrices.
ESIM Enhanced Sequential Inference Model (Chen et al.
2017) is a strong co-attention model that uses BiLSTM to
encode tokens within each sentence, and perform cross-
attention on these encoded token representations.
S-TLSTM Syntactic TreeLSTM (Chen et al. 2017) is iden-
tical to ESIM except it encodes sentence tokens via a TreeL-
STM based on the dependency parse instead of sequential
BiLSTM. It is the highest performing NLI model with a re-
cursive component.
DIIN Densely Interactive Inference Network (Gong, Luo,
and Zhang 2017) is a novel co-attention model that ex-
tracts phrase-level alignment features using densely con-
nected convolutional layers a word-level interactive matrix.
DR-BiLSTM Dependent Reading Bidirectional LSTM
(Ghaeini et al. 2018) is a model that modiﬁes on ESIM with
a dependent reading mechanism that encodes each sentence
conditioned on the other.

We were able to obtain original implementations for RSE,
G-TLSTM, S-TLSTM and DIIN. We used our own im-

2https://nlp.stanford.edu/projects/snli/

https://repeval2017.github.io/shared/

Figure 1: Examples of adversaries generated for our experiments. On the left, we have an example of the SOSWAP adversary,
where the swapped subject and object are marked in yellow in p(cid:48). On the right, we have an example of the ADDAMOD adversary,
where the added adjective modiﬁer is marked in yellow.

SNLI
dev
86.5
85.9
85.0
88.2
88.1
88.1
DR-BiLSTM 88.3

Model
RSE
G-TLSTM
DAM
ESIM
S-TLSTM
DIIN

Human

-

SOSWAP
C
2.1
1.2
0.3
2.1
4.4
4.5
5.5
84

N
5.5
1.5
0.0
1.5
3.5
10.6
4.8
14

E
92.5
97.2
99.7
96.4
92.1
84.9
89.7
2

ADDAMOD
C
0.2
1.2
0.0
9.6
1.1
0.4
8.9
2

E
95.2
95.9
99.9
85.6
90.4
55.0
82.1
10

N
4.6
2.9
0.1
4.8
8.5
44.6
9.0
88

Table 2: Model performance on SNLI and % of predictions
on the adversarial test sets. E, C, N indicate the classiﬁca-
tion where E is entailment, C is contradiction and N is neu-
tral. (Note that SOSWAP mostly creates contradictory pairs,
while ADDAMOD mostly creates neutral pairs).

plementation for the other three models and were able to
achieve comparable results on standard evaluation sets.

2.2 Motivation
Many top-performing sentence encoders (such as RSE) use
max-pooling as the ﬁnal layer to encode the sentences (Nan-
gia et al. 2017), and except DIIN, most top-performing co-
attention models calculate cross-alignment on the RNN hid-
den state of each token. These design trends are counterin-
tuitive because max-pooling and attention mechanisms are
communicative operations which are not affected by word
order, making the RNN layers the only way for the models
to capture the compositional structure of sentences. How-
ever, past studies have shown that RNNs (especially sequen-
tial RNNs) are insufﬁcient for effectively capturing logical
compositional structure that is often present in NLI (Evans
et al. 2018; Nangia and Bowman 2018). These indicate an
over-focus on lexical information in neural NLI approaches
which is very different from how humans approach the task.

3 Adversarial Evaluation
We test our intuition that the models do not sufﬁciently
capture the compositional nature of sentences by evaluat-
ing them on a couple of rule-based adversaries, where we
change the semantics of the sentences by perturbing the
compositionality without modifying any lexical features. We
found that none of the models were able to successfully use

the compositional difference to reason with these examples.

3.1 Adversarial Examples

To test our hypothesis that models are over-reliant on word-
level information and have limited ability to process com-
positional structures, we created adversarial test sets com-
posed of pairs of sentences whose logical relations cannot
be extracted from lexical information alone. Speciﬁcally, we
conduct experiments with the following two types of adver-
sarial data in which we change the semantics of the sentence
by only modifying its compositional structure:
SOSWAP Adversaries We take a premise from the SNLI
dataset, p, that contains a subject-verb-object structure, and
create the hypothesis p(cid:48) by swapping the subject and object.
This results in a contradictory pair as the semantic roles of
the premise are swapped in the hypothesis. An example is
shown on the left side of Fig. 1. We were able to create 971
examples of this type.
ADDAMOD Adversaries In this setup, we take a premise
from the SNLI dataset, p, that has at least two different noun
entities. We then pick an adjective modiﬁer from the SNLI
dataset that has been used to describe both nouns, and create
the premise p(cid:48) by adding the modiﬁer to one of the nouns,
and the hypothesis h by adding it to the other. This results
in a neutral pair as the hypothesis contains additional infor-
mation and is neither implied nor refuted by the premise. An
example of this is shown on the right side of Fig. 1. We were
able to create 1783 examples of this type.

The intuition behind both of the adversaries described
above is that, while the semantic difference resulting from
compositional change is obvious for humans, the two input
sentences will be almost identical for models that take no
compositional information into consideration.3

3.2 Adversarial Evaluation Results

We trained our 7 models on the SNLI training set and tested
them on the adversarial test sets – the results are shown
in Table 2. To ensure that the intuitions behind our adver-
sarial generation algorithms were correct, we conducted hu-
man evaluation for a sample of 100 examples for each eval-

3To create the adversarial data, we used the Stanford Parser
(Chen and Manning 2014) from CoreNLP 3.8.0 to get the depen-
dency parse of the sentences, on which we apply our strategires.

Model

SNLI

MNLI Matched

MNLI MisMatched

Original BoW WS

Original BoW WS

Original BoW WS

86.47
RSE
ESIM
88.17
DR-BiLSTM 88.28

85.02
82.37
82.81

–
86.79
86.90

72.80
76.16
76.90

70.02
68.98
70.11

–
73.70
73.27

74.00
76.22
77.49

71.10
69.77
70.70

–
74.20
73.25

Table 3: The ‘Original’ columns show results for vanilla models on the resp. validation sets. The ‘BoW’ column show results
for BoW-like variants created replacing their RNNs with fully-connected layers. The ‘WS’ columns show results for models
trained with shufﬂed input sentences.

SOSWAP
E/C/N

ADDAMOD
E/C/N

None
SOSWAP
ADDAMOD

96.4/2.1/1.5
0.9/99.1/0.0
73.1/1.0/25.9

85.6/9.6/4.8
66.7/26.9/6.5
0.3/0.1/99.6

Table 4: The percentages of predicting E/C/N by ESIM with
different types of adversarial training, where an underlined
number indicates the accuracy on the correct label.

uation set.4 On both experiments, despite a majority of the
examples being marked as non-entail by our human eval-
uators, the models classiﬁed them overwhelmingly as en-
tailment, indicating the models’ inability to recognize or
process compositional semantic information5. The models’
poor performance on these adversarial test sets contrasts
sharply with their high performance on standard evaluation,
raising doubts on the effectiveness and reliability of standard
evaluation. However, adversarial evaluation as done here has
its own issues. We discuss problems with current evaluation
further in the next section.

4 Limitations of Existing Evaluations
In this section, we show that models’ performance on stan-
dard evaluation does not reﬂect their compositional under-
standing capabilities, which we suspect leads to the lack of
focus on this type of modeling in the current literature.

4.1 Regular Evaluation Limitations
The gap in model performance between standard evaluation
and adversarial evaluation (see Table 2) indicates the limita-
tions of regular evaluation at testing a model’s ability to pro-
cess sentences’ compositional structure. More importantly,
regular evaluation fails to separate or differentiate mod-
els that are relying on lexical pattern-matching from those
with deeper compositional understanding. To further illus-
trate this point, we conduct the following two experiments

4The adversaries are intended to use to highlight models’ com-
positional unawareness and motivate further analysis rather than to
be a general-purpose evaluation set. Human evaluation results in-
dicates that the the majority of the data are correct.

5Note that DIIN’s relatively high performance on ADDAMOD
is likely due to its convolutional structure successfully capturing
the modiﬁer relationship, but we see that it still fails on adversaries
with longer-range dependency requirements such as SOSWAP.

in which we intentionally force the models to be unaware
of compositional information by either removing RNN con-
nections in their architectures or by randomizing word order
during training.
RNN Replacement: We create strong bag-of-words-like
models by replacing RNN layers in RSE, ESIM, and DR-
BiLSTM with fully-connected layers, and train them on the
standard training set.
Word-Shufﬂed Training: We train the ESIM and DR-
BiLSTM models with the words of the two input sentences
shufﬂed, such that the compositional information is diluted
and hard to learn.

The results of these models and their corresponding vari-
ants on SNLI, MNLI matched, and MNLI mismatched de-
velopment set are shown in Table 3, where we see that their
performance is not too far from that of their original, recur-
rent counterparts. To be speciﬁc, there is roughly 6-7 points
drop in accuracy when RNNs connections are removed and
only 2-3 points drop when words are shufﬂed during train-
ing. These counter-intuitive ﬁndings indicate that even a
model which only considers shallow lexical features is able
to get a decent result on standard evaluation, despite using a
mechanism that is very different from human reasoning.

4.2 Adversarial Evaluation Limitations
Although, rule-based adversaries were able to expose the
models’ lack of knowledge of compositional semantics, they
have their own limitations and do not serve well as a general
analysis tool. Due to the recursive nature of language, there
are inﬁnitely many ways for compositional information to
affect a sentence’s meaning, but each type of rule-based ad-
versary only tests for one speciﬁc compositional rule. Thus,
success on one type of adversary only demonstrates knowl-
edge of that single rule, and does not indicate general knowl-
edge of compositionality rules. The easiest way to see this is
via adversarial training and data-augmentation: we trained
the ESIM model with data augmentation from either type
of adversaries,6 and re-evaluated the retrained models on
both SOSWAP and ADDAMOD. As shown in Table 4, while
adversarial data-augmentation leads to improvement on the
same type of adversary, it does not generalize to other types
of adversaries. In fact, we see that focusing on one type of
adversarial performance may lead to over-ﬁtting that par-

6We add 20,000 adversarial examples into training at each
epoch. Adv-Training data was created from SNLI training set while
Adv-Evaluation set was created from SNLI dev set.

ticular adversary, and hurt overall robustness. For example,
in Table 4, we see that adversarial training with SOSWAP
leads to an increase in incorrect ‘contradiction’ predictions
on ADDAMOD, and adversarial training with ADDAMOD
actually leads to a decrease in performance on SOSWAP
while incorrectly increasing ‘neutral’ predictions.7 These re-
sults indicate that models’ success on an enumerable set of
adversarial evaluation is still far from validating its general
compositional ability.

Thus, we propose an alternative evaluation strategy that
leverages existing data to evaluate a model’s general com-
positional understanding capabilities.

5 Compositionality-Sensitivity Testing
In this section, we ﬁrst formulate the role of composi-
tionality in the context of NLI task, and then propose a
compositionality-sensitivity testing setup as an analysis tool
to explicit reveal how much compositional information the
models take into consideration for inference.

5.1 Problem Formulation
NLI is a complex task with many variables – almost all pre-
vious approaches model the task as the distribution p(y | x)
of the logical relation y conditioned on the pair of input sen-
tences x = (P, H), where y ∈ {entailment, contradiction,
neutral} and P, H are the premise and hypothesis, respec-
tively. This conditional distribution is often parameterized
by some neural models and trained end-to-end by maximiz-
ing the probability of ‘ground-truth’ label. For the sake of
studying models’ insensitivity to compositional information,
we consider a factorization of the two input sentences as tu-
ples (Sp, Πh) and (Sh, Πh), where Sp and Sh are the sets
of tokens that make up the premise P and hypothesis H as
lexical factors, and Πp and Πh are the sets of compositional
rules that combine those tokens into meaningful sentences
as compositional factors.8 A perfect modeling of NLI that
is capable of taking all lexical and compositional informa-
tion into account is formalized as Eqn. 1, whereas a entirely
bag-of-words (BoW) model is formalized as Eqn. 2.

p(y | x) = fθ(Sp, Sh, Πp, Πh)
p(y | x) = gθ(Sp, Sh)

(1)
(2)

The models we discuss are neither perfect models nor en-
tirely BoW models, but rather a combination of both, where

7While it is true that we can use data augmentation from both
types of adversaries to improve performance on both types of eval-
uations, we can easily come up with a third, different type of ad-
versary (e.g., swapping the verbs between the main sentence and a
clause) that is still difﬁcult for the 2-adversarially trained model.
Enumerating rule-based adversaries to cover all frequently-used
compositional structural changes in a language is prohibitively
costly, as generating high-quality (natural and grammatical) data
following a single rule already takes tons of time and resources.

8Due to the complexity of language, lexical elements are often
intertwined with compositional rules and this factorization of p will
make Πp intractable in practice. However, we isolate compositional
factors from lexical factors in order to analysis of model behavior.

they are able to detect and use some lexical features and
some semantic rules:

p(y | x) = ˆfθ( ˜Sp, ˜Sh, ˜Πp, ˜Πh)

(3)

where ˜Sp ⊆ Sp and ˜Sh ⊆ Sh are the sets of lexical fea-
tures of the sentences that the model is capable of using, and
similarly ˜Πp ⊆ Πp and ˜Πh ⊆ Πh are sets of compositional
rules that the model is capable of using. The issue we ex-
plored in previous sections is that current models are overly
relying on Sp and Sh, but have limited ability to detect and
use Πp and Πh. In other words, ˜Πp (cid:28) Πp and ˜Πh (cid:28) Πh.
For instance, the adversaries we created Sec. 3.2 have sen-
tence pairs which have the same lexical elements but differ-
ent compositional structures, i.e., Sp = Sh but Πp (cid:54)= Πh.
To an entirely BoW model (Eqn. 2), this looks identical
to the scenario where the same sentence is repeated twice.
Thus in those cases, inferences necessarily require knowl-
edge of compositional information. This provides intuition
into our new evaluation setup: In order to evaluate models’
compositionality-sensitivity, we need to evaluate their per-
formance on data which can not be solved by lexical fea-
tures alone, i.e., cannot be solved by an entirely BoW model.
We thus seek to evaluate models on a subset of the standard
evaluation set that ﬁts this criterion.

5.2 Approximating BoW Model
To obtain such a subset, we must ﬁrst approximate an en-
tirely BoW model. Speciﬁcally, we use a softmax regression
classiﬁer that takes in only lexical features for prediction.
More formally,

v = h(x)

p(c | x) =

(cid:80)

exp(w(cid:62)c v)

c(cid:48)

L exp(w(cid:62)c(cid:48) v)
∈

(4)

(5)

where h is a function that maps the raw input pair x to its
lexical feature vector v ∈ Rd, p(c|x) is the probability given
to label c by the softmax regression classiﬁer. The lexical
feature vector v is an indicator vector that contains the fol-
lowing lexical features from the input pair:
• Unigrams appearance within the premise.
• Unigrams appearance within the hypothesis.
• Word pairs (cross-unigrams) where one appears in the

premise and the other in the hypothesis.

For unigram and cross-unigram features, we only pick words
that are nouns, verbs, adjectives or adverbs to reduce spar-
sity. We train the regression model on both SNLI and MNLI
and use it to approximate an entirely BoW model.

5.3 Lexically-Misleading Score
Since the softmax regression classiﬁer we used is not an en-
tirely BoW model, i.e., it does not capture and use all aspects
of lexical semantics. Examples that it predicted incorrectly
might still be solvable with the correct lexical information.
Thus, to preserve the integrity of our evaluation, we fur-
ther remove examples that the softmax regression classiﬁer
is ambivalent about, and only look at examples where the

regression model was conﬁdently wrong, i.e., cases where
they were ‘misled’ by lexical features. We do so because
in cases where the regression has insufﬁcient lexical knowl-
edge (e.g., rare/unseen words), it is likely going to give a
less conﬁdent prediction, whereas in cases where the model
was misled, it had the lexical knowledge to make a decision,
and hence a wrong prediction indicates the need for compo-
sitional knowledge.

Formally, we deﬁne the Lexically-Misleading Score

(LMS) of an NLI datapoint (x, c∗) as:

(6)

p(c | x)

fLM S(x, c∗) = max
c∗
\{

L

}

c
∈
where c∗ is the ground truth label, p(c | x) is the prob-
ability generated by our regression model, and L =
{entailment, contradiction, neutral} is the label set. In other
words, fLM S of a data point is the maximum probability the
regression gave on an incorrect label. The idea behind LMS
is that: the more lexically misleading an example is, the more
conﬁdent we are that compositional information is required
to solve it. We thus use LMS to select examples from exist-
ing evaluation sets for our evaluation.

i=1, we create CSλ,

5.4 Subsampling and Testing
Given a standard evaluation set and associated ‘ground-
truth’ labels, D = {(xi, ci)}N
the
compositionality-sensitivity evaluation set of conﬁdence λ:
CSλ = {(xi, ci) ∈ D | fLM S(xi, ci) ≥ λ}
The choice of λ represents a trade-off between being
the individual examples’ ability to test
conﬁdent about
compositionality-sensitivity and keeping a decent sample
size of evaluation data. CS0 is equivalent to testing on the en-
tire evaluation dataset, whereas CS0.95 (in a 3-way classiﬁer)
gives us an extremely small evaluation set (e.g. CS0.95 on
SNLI only has 148 examples) with highly misleading lexi-
cal features. Empirically, we found that for SNLI and MNLI,
λ = 0.7 gives a good balance between size of the evaluation
set and its ability to test compositionality-sensitivity (e.g.,
CS0.7 on SNLI has 999 examples). Fig. 2 shows examples
of sentence pairs with high LMS from the SNLI validation
set that were in CS0.7 for SNLI.9

5.5 Usage and Limitations
It is worth noting that we do not wish this subset to be used
as a benchmark for models to compete on, but rather an
analysis tool to explicitly reveal models’ compositionality-
awareness. Even though the testing setup has its own limita-
tions such as data sparsity and noisiness, it still serves as an
initial step to highlight the problem of compositionality un-
derstanding (and gain some important insights into models’
behaviors, as shown below), which has been largely unex-
plored in the current neural literature. But more importantly,
we hope that this inspires future works on data collection
that explicitly address the issue by adding compositionality
requirements and lexical-feature balancing into the collec-
tion process.

9We release the LMS values of

development
analyze-compositionality-sensitivity-NLI.

set

at

the SNLI and MNLI
https://github.com/easonnie/

5.6 Evaluation of Existing Models on CSλ
Table 5 shows the performance of our seven models re-
evaluated with CSλ at different λ values.
General Trend: We see that in general, model performance
decreases as λ increases, whereas human performance10 suf-
fered much less with increasing λ values. This is consistent
with our hypothesis that there are signiﬁcant differences be-
tween human-style deep reasoning (with both lexical and
compositional knowledge) and inference by current models,
which overly relies on lexical information. We also noticed
that for all the models on SNLI, MNLI matched, and MNLI
mismatch dev set, there is a big gap between the accuracy on
the whole dev set and those on CS0.7. This demonstrates that
our models have very limited ability to utilize or even recog-
nize compositional information for semantic understanding.
These ﬁndings indicate the space and need for further re-
search on structured sentence modeling.
Seqential Model vs. Structured Model: The results on
CS0.7 differentiates models based on their architectures.
More importantly, it explicitly reveals models’ composi-
tional understanding which is otherwise largely hidden on
the standard evaluation. Speciﬁcally, the results for ESIM
and S-TLSTM (row 3 and 4) give a clear comparison be-
tween sequential and recursive modeling since the two mod-
els have the same architecture with the exception that ESIM
uses sequential RNN and S-TLSTM uses recursive RNN to
encode the sentences. We see that S-TLSTM is better than
ESIM on CS0.7, despite ESIM getting better results on all
three standard evaluation datasets. This indicates that the re-
cursive model with additional syntactic tree input does in
fact induce more compostional understanding ability, which
is completely invisible if we merely focus on the results
of standard evaluation. Moreover, DIIN (row 5) obtains the
best results on all the CS0.7 subsets, substantially surpass-
ing that of DR-BiLSTM (row 6), the most powerful sequen-
tial model in the table. This is also consistent with the in-
tuition that DIIN’s convolutional network and phrase-level
alignment provide much more compositional information
than simple RNN-based sequential models. Another inter-
esting fact is the difference in performance between a recur-
sive model trained with explicit external linguistic supervi-
sion (S-TLSTM) and one trained via latent tree learning (G-
TLSTM). We see that S-TLSTM is able to capture composi-
tional information more effectively than G-TLSTM (row 2),
which is consistent with ﬁndings from diagnostic datasets
regarding recursive modeling (Nangia and Bowman 2018).
Necessity of Compositional Information: In the lower side
of the table (row 9-13), we evaluate models with either sev-
ered RNNs connections or word-shufﬂed training data. The
results represent models with limited compositional acces-
sibility or awareness. As expected, the results on CS0.7 are
similar to or even below the majority vote even though their
performance on standard evaluation is on the same level as

10We approximate human performance by the mechanism pro-
posed by Gong, Luo, and Zhang (2017): we choose one of the an-
notator labels (out of 5) and compare it against the ground truth.
Due to noisy data collection procedure, the actual ceiling of human
performance should be much higher than this value.

Figure 2: Two examples with high LMS. Correct prediction for the 1st example requires recognizing that ‘not standing’ and
‘sitting’ are the same state, rather than focusing on the superﬁcial lexical clues such as ‘not’ and the cross unigram (‘sitting’,
‘standing’) that both mislead to ‘contradiction’. For the 2nd example, word-overlap misleads the classiﬁer to predict ‘entailment’.

Model

1 RSE
2 G-TLSTM
3 ESIM
4
5 DIIN
6 DR-BiLSTM

S-TLSTM

7 Human

8 Majority Vote

Whole Dev CS0.5

SNLI

59.01
57.27
62.76
64.60
64.28
62.92

MNLI (Matched)

MNLI (MisMatched)

CS0.6 CS0.7 Whole Dev CS0.5
48.48
55.59
45.32
53.68
52.76
58.58
53.92
60.57
59.49
60.57
55.26
58.50

52.73
50.28
55.28
57.51
57.17
55.28

72.80
70.70
76.16
76.06
78.70
76.90

CS0.6 CS0.7 Whole Dev CS0.5
49.30
43.57
46.33
41.20
54.06
49.96
55.60
51.54
59.79
56.12
57.39
52.72

74.00
70.81
76.22
76.04
78.38
77.49

39.62
38.14
48.31
48.90
54.05
50.07

CS0.6 CS0.7
40.85
45.84
38.87
42.03
48.32
51.26
52.40
50.61
57.44
53.66
53.04
55.37

81.87

80.40

80.76

42.13

42.96

43.27

86.00

86.03

86.45

36.23

35.04

35.20

85.53

85.35

84.45

34.22

35.39

34.00

Models in which compositional information removed or diluted

9 RSE (BoW)
10 ESIM (BoW)
11 DR-BiLSTM (BoW)
12 ESIM (WS)
13 DR-BiLSTM (WS)

52.82
48.64
48.97
58.41
58.46

47.93
44.18
44.33
50.61
50.39

43.60
40.49
41.38
45.49
44.77

40.69
38.59
37.97
44.20
45.77

34.57
33.44
33.07
41.20
41.20

31.66
30.34
28.42
41.09
37.85

43.66
41.00
40.73
49.39
46.33

38.60
35.93
35.09
45.39
42.03

34.30
32.32
30.79
41.77
38.26

86.47
85.88
88.17
88.10
88.08
88.28

88.32

33.82

85.02
82.37
82.81
86.79
86.90

88.45

35.45

70.02
68.98
70.11
73.70
73.27

89.30

35.22

71.10
69.77
70.70
74.20
73.25

Table 5: Results of models, human, and majority-vote baseline on different levels of compositionality-sensitivity testing. Results
of models with limited compositional information are in the bottem on the table.

that of the original models, indicating that compositionality
understanding is required to obtain a good result on CS0.7.

6 Related Work and Discussion
Over-Stability: Jia and Liang (2017) used adversarial eval-
uation to show that models trained on the Stanford QA
Dataset (Rajpurkar et al. 2016) were reliant on syntactic
similarity for answering, revealing the over-stability of QA
models. With similar motivation, we study the task of NLI
by showing that models are overly focused on lexical fea-
tures and have limited ability of compositionality.
Existing Analysis on NLI: Previous work on analyzing NLI
models (Glockner, Shwartz, and Goldberg 2018; Carmona,
Mitchell, and Riedel 2018) has focused on models’ lim-
ited ability in identifying lexical semantics that were rare
or unseen during training. Our work complements theirs by
demonstrating models’ limited understanding of composi-
tionality encoded in the sentences. Gururangan et al. (2018),
Poliak et al. (2018b) and Tsuchiya (2018) concurrently
showed hypothesis bias in NLI and RTE datasets. In par-
ticular, Gururangan et al. (2018) also proposed to evaluate
models on a harder and better subset of standard evaluation.
We instead focus on exposing models’ compositionality-
insensitivity by selecting our evaluation dataset based on

LMS (lexically-misleading score).
Compositionality: Nangia and Bowman (2018) introduce a
dataset to study the ability of latent tree models. Evans et al.
(2018) introduce a dataset of logical entailments for mea-
suring models’ ability to capture the structure of logical ex-
pressions against an entailment prediction task. Dasgupta et
al. (2018) study the inference behavior of models using sen-
tence embedding on a compositional comparisons dataset.
However, we conduct a rigorous study on compositionality-
sensitivity, covering a broader range of NLI models and
show how to use a ﬁltered subset of existing NLI datasets
to test models’ compositional ability.
Linguistic Diagnostic Evaluation: Multiple linguistic di-
agnostic datasets have been published to test NLI mod-
els’ ability to process certain linguistic phenomena such as
coreference, double negation, etc. (Williams, Nangia, and
Bowman 2018; Nangia et al. 2017; Poliak et al. 2018a;
Wang et al. 2018). These datasets are helpful in that they ex-
plore the potential usefulness of existing models by demon-
strating their abilities in speciﬁc scenarios. However, the
way models approach language might not have any linguis-
tic grounding. Consider an example where the premise is
‘We can’t not go to sleep.’ and hypothesis is ‘We have to
go to sleep.’ Understanding the ﬁrst sentence should need

compositionality for processing the double negation. How-
ever, given that the example’s LMS score is only 0.2376
(which means our BoW regression model was able to solve
this correctly), models can solve this particular example via
a lexical feature shortcut. Thus, a model resolving of a spe-
ciﬁc linguistic problem does not necessarily indicate its un-
derstanding of the linguistic rule and its generalizability to
other compositionality rules. Thus, our work complements
linguistic diagnostic datasets well, since a model perform-
ing well on both our evaluation and the linguistic diagnostic
setup is likely using compositional rules (i.e., human-like
reasoning) rather than other pattern-matching procedures to
obtain seemingly-compositional behavior.

7 Conclusion
In this paper, we show that current NLI models achieve mis-
leadingly high results on standard evaluation due to its in-
ability to test models’ compositional semantic understand-
ing. We further show that typical adversarial evaluation is
also limited in terms of evaluating generalizability. There-
fore, to encourage the design of models with general un-
derstanding capabilities, we propose our compositionality-
sensitivity testing that evaluates using compositional infor-
mation which is not conﬁned to any speciﬁc type. Our work
complements other recent advancements in evaluation in
the community and we hope that this not only encourages
the development of structured compositional-aware mod-
els, but also highlights the need of more lexical-feature-
controlled data collections for semantically demanding tasks
(e.g. NLI), such that they will require not only distributional
semantics, which is often captures via large scale unsuper-
vised learning, but also compositional semantics, which tend
to be overlooked but a harder problem in the community.

Acknowledgments
We thank the reviewers for their helpful comments. This
work was supported by faculty research awards from Verisk,
Google, and Facebook.

References
Bowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D.
2015. A large annotated corpus for learning natural language
inference. In EMNLP.
Carmona, V. I. S.; Mitchell, J.; and Riedel, S. 2018. Behav-
ior analysis of nli models: Uncovering the inﬂuence of three
factors on robustness. NAACL.
Chen, D., and Manning, C. 2014. A fast and accurate de-
pendency parser using neural networks. In EMNLP.
Chen, Q.; Zhu, X.; Ling, Z.-H.; Wei, S.; Jiang, H.; and
Inkpen, D. 2017. Enhanced lstm for natural language in-
ference. In ACL.
Choi, J.; Yoo, K. M.; and Lee, S.-g. 2017. Learning to
compose task-speciﬁc tree structures. In AAAI.
Dasgupta, I.; Guo, D.; Stuhlm¨uller, A.; Gershman, S. J.; and
Goodman, N. D. 2018. Evaluating compositionality in sen-
tence embeddings. arXiv preprint arXiv:1802.04302.

Evans, R.; Saxton, D.; Amos, D.; Kohli, P.; and Grefenstette,
E. 2018. Can neural networks understand logical entail-
ment? ICLR.
Ghaeini, R.; Hasan, S. A.; Datla, V.; Liu, J.; Lee, K.; Qadir,
A.; Ling, Y.; Prakash, A.; Fern, X. Z.; and Farri, O. 2018.
Dr-bilstm: Dependent reading bidirectional lstm for natural
language inference. NAACL.
Glockner, M.; Shwartz, V.; and Goldberg, Y. 2018. Breaking
nli systems with sentences that require simple lexical infer-
ences. NAACL.
Gong, Y.; Luo, H.; and Zhang, J. 2017. Natural language
inference over interaction space. ICLR.
Gururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz, R.;
Bowman, S. R.; and Smith, N. A. 2018. Annotation artifacts
in natural language inference data. NAACL.
Jia, R., and Liang, P. 2017. Adversarial examples for evalu-
ating reading comprehension systems. EMNLP.
Nangia, N., and Bowman, S. R. 2018. Listops: A diagnostic
dataset for latent tree learning. ACL-SRW.
Nangia, N.; Williams, A.; Lazaridou, A.; and Bowman, S. R.
2017. The repeval 2017 shared task: Multi-genre natural
language inference with sentence representations. RepEval.
Nie, Y., and Bansal, M. 2017. Shortcut-stacked sentence
encoders for multi-domain inference. RepEval.
Parikh, A. P.; T¨ackstr¨om, O.; Das, D.; and Uszkoreit, J.
2016. A decomposable attention model for natural language
inference. In EMNLP.
Poliak, A.; Haldar, A.; Rudinger, R.; Hu, J. E.; Pavlick, E.;
White, A. S.; and Van Durme, B. 2018a. Towards a uniﬁed
natural language inference framework to evaluate sentence
representations. arXiv preprint arXiv:1804.08207.
Poliak, A.; Naradowsky, J.; Haldar, A.; Rudinger, R.; and
Van Durme, B. 2018b. Hypothesis only baselines in natural
language inference. *SEM.
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
Squad: 100,000+ questions for machine comprehension of
text. EMNLP.
Tsuchiya, M. 2018. Performance impact caused by hid-
den bias of training data for recognizing textual entailment.
LREC.
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
Bowman, S. R.
2018. Glue: A multi-task benchmark
and analysis platform for natural language understanding.
EMNLP.
Wang, Z.; Hamza, W.; and Florian, R. 2017. Bilateral multi-
perspective matching for natural language sentences. IJCAI.
Williams, A.; Nangia, N.; and Bowman, S. R. 2018. A
broad-coverage challenge corpus for sentence understanding
through inference. NAACL.

