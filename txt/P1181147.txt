RTSEG: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY

Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand

mennatul@ualberta.ca, senthil.yogamani@valeo.com
University of Alberta, Valeo Vision Systems, Cairo University

9
1
0
2
 
t
c
O
 
7
1
 
 
]

V
C
.
s
c
[
 
 
4
v
8
5
7
2
0
.
3
0
8
1
:
v
i
X
r
a

ABSTRACT

Semantic segmentation beneﬁts robotics related applications,
especially autonomous driving. Most of the research on se-
mantic segmentation only focuses on increasing the accuracy
of segmentation models with little attention to computation-
ally efﬁcient solutions. The few work conducted in this di-
rection does not provide principled methods to evaluate the
different design choices for segmentation. In this paper, we
address this gap by presenting a real-time semantic segmen-
tation benchmarking framework with a decoupled design for
feature extraction and decoding methods. The framework is
comprised of different network architectures for feature ex-
traction such as VGG16, Resnet18, MobileNet, and Shuf-
ﬂeNet. It is also comprised of multiple meta-architectures for
segmentation that deﬁne the decoding methodology. These
include SkipNet, UNet, and Dilation Frontend. Experimen-
tal results are presented on the Cityscapes dataset for urban
scenes. The modular design allows novel architectures to
emerge, that lead to 143x GFLOPs reduction in comparison to
SegNet. This benchmarking framework is publicly available
at 1.

Index Terms— realtime; semantic segmentation; bench-

marking framework

1. INTRODUCTION

Semantic segmentation has made progress in the recent years
with deep learning. The ﬁrst prominent work in this ﬁeld was
fully convolutional networks(FCNs) [1]. FCN was proposed
as an end-to-end method to learn pixel-wise classiﬁcation,
where transposed convolution was used for upsampling. Skip
architecture was used to reﬁne the segmentation output, that
utilized higher resolution feature maps. That method paved
the road to subsequent advances in the segmentation accu-
racy. Multi-scale approaches [2, 3], structured models [4, 5],
and spatio-temporal architectures [6] introduced different di-
rections for improving accuracy. All of the above approaches
focused on accuracy and robustness of segmentation. Well
known benchmarks and datasets for semantic segmentation
such as Pascal [7], NYU RGBD [8], Cityscapes [9], and Map-

1https://github.com/MSiam/TFSegmentation

Fig. 1: Overview of the different components in the frame-
work with the decoupling of feature extraction module and
decoding method.

illary [10] boosted the competition toward improving accu-
racy.

However, little attention is given to the computational ef-
ﬁciency of these networks. Although, when it comes to appli-
cations such as autonomous driving this would have tremen-
dous impact. There exists few work that tries to address the
segmentation networks efﬁciency such as [11, 12]. The sur-
vey on semantic segmentation [13] presented a comparative
study between different segmentation architectures including
ENet [12]. Yet, there is no principled comparison of differ-
ent networks and meta-architectures. These previous stud-
ies compared different networks as a whole, without com-
paring the effect of different modules. That does not enable
researchers and practitioners to pick the best suited design
choices for the required task.

In this paper we propose the ﬁrst framework toward
benchmarking real-time architectures in segmentation. Our
main contributions are: (1) we provide a modular decoupling
of the segmentation architecture into feature extraction and
decoding method which is termed as meta-architecture as
shown in Figure 1. The separation helps in understanding the
impact of different parts of the network on real-time perfor-
mance.
(2) A detailed ablation study with highlighting the
trade-off between accuracy and computational efﬁciency is

(a)

(b)

Fig. 2: Different Meta Architectures using MobileNet as the feature extraction network. a) SkipNet. b) UNet.

presented. (3) The modular design of our framework allowed
the emergence of two novel segmentation architectures using
MobileNet [14] and ShufﬂeNet [15] with multiple decod-
ing methods. ShufﬂeNet lead to 143x GFLOPs reduction
in comparison to SegNet. Our framework is built on top of
Tensorﬂow and is publicly available. Although our frame-
work delivers less operations we have not been able to deliver
higher inference speed.

2. BENCHMARKING FRAMEWORK

2.1. Meta-Architectures

Three meta-architectures are integrated in our benchmarking
software: (1) SkipNet meta-architecture[1]. (2) U-Net meta-
architecture[16]. (3) Dilation Frontend meta-architecture[3].
The meta-architectures for semantic segmentation identify the
decoding method for in the network upsampling. All of the
network architectures share the same down-sampling factor
of 32. The downsampling is achieved either by utilizing pool-
ing layers, or strides in the convolutional layers. This en-
sures that different meta architectures have a uniﬁed down-
sampling factor to assess the effect of the decoding method
only.

SkipNet architecture denotes a similar architecture to
FCN8s [1]. The main idea of the skip architecture is to ben-
eﬁt from feature maps from higher resolution to improve the
output segmentation. SkipNet applies transposed convolution
on heatmaps in the label space instead of performing it on
feature space. This entails a more computationally efﬁcient
decoding method than others. Feature extraction networks
have the same downsampling factor of 32, so they follow the
8 stride version of skip architecture. Higher resolution feature
maps are followed by 1x1 convolution to map from feature
space to label space that produces heatmaps corresponding
to each class. The ﬁnal heatmap with downsampling factor
of 32 is followed by transposed convolution with stride 2.

Elementwise addition between this upsampled heatmaps and
the higher resolution heatmaps is performed. Finally, the
output heat maps are followed by a transposed convolution
for up-sampling with stride 8. Figure 2(a) shows the SkipNet
architecture utilizing a MobileMet encoder.

U-Net architecture denotes the method of decoding that
up-samples features using transposed convolution corre-
sponding to each downsampling stage. The up-sampled
features are fused with the corresponding features maps from
the encoder with the same resolution. The stage-wise up-
sampling provides higher accuracy than one shot 8x upsam-
pling. The current fusion method used in the framework is
element-wise addition. Concatenation as a fusion method can
provide better accuracy, as it enables the network to learn
the weighted fusion of features. Nonetheless, it increases the
computational cost, as it is directly affected by the number
of channels. The upsampled features are then followed by
1x1 convolution to output the ﬁnal pixel-wise classiﬁcation.
Figure 2(b) shows the UNet architecture using MobileNet as
a feature extraction network.

Dilation Frontend architecture utilizes dilated convolu-
tion instead of downsampling the feature maps. Dilated con-
volution enables the network to maintain an adequate recep-
tive ﬁeld, but without degrading the resolution from pooling
or strided convolution. However, a side-effect of this method
is that computational cost increases, since the operations are
performed on larger resolution feature maps. The encoder
network is modiﬁed to incorporate a downsampling factor of
8 instead of 32. The decrease of the downsampling is per-
formed by either removing pooling layers or converting stride
2 convolution to stride 1. The pooling or strided convolutions
are then replaced with two dilated convolutions[3] with dila-
tion factor 2 and 4 respectively.

Table 1: Comparison of different encoders and decoding methods in accuracy on cityscapes validation set. The modular
decoupled design in RTSeg enabled such comparison. Coarse indicates whether the network was pre-trained on the coarse
annotation or not.

Decoder Encoder
SkipNet MobileNet No
ShufﬂeNet No
SkipNet
ResNet18
UNet
No
UNet
MobileNet No
ShufﬂeNet No
UNet
Dilation MobileNet No
Dilation
ShufﬂeNet No
SkipNet MobileNet Yes
ShufﬂeNet Yes
SkipNet

Coarse mIoU Road
95.9
61.3
94.8
55.5
95.8
57.9
95.2
61.0
95.1
57.0
95.6
57.8
95.2
53.9
62.4
95.4
94.6
59.3

Sidewalk Building
73.6
68.6
73.2
71.3
69.5
72.3
68.5
73.9
70.5

86.9
83.9
85.8
86.8
83.7
85.9
84.1
86.6
85.5

Sign
57.6
50.5
57.5
60.9
54.3
57.0
57.3
57.4
54.9

Sky
91.2
88.6
91.0
92.8
89.0
91.4
90.3
91.1
90.8

Person Car
89.0
66.4
86.5
60.8
88.6
66.0
68.1
88.8
87.8
61.7
87.8
64.9
86.6
62.9
88.4
65.7
87.5
60.2

Bicycle Truck
63.6
58.8
63.2
65.0
59.9
62.8
60.2
63.3
58.8

45.9
29.6
31.4
41.3
35.5
26.3
23.3
45.3
45.4

2.2. Feature Extraction Architectures

(2) ResNet18[18].

In order to achieve real-time performance multiple network
architectures are integrated in the benchmarking framework.
The framework includes four state of the art real-time net-
work architectures for feature extraction. These are:
(1)
(4)
(3) MobileNet[14].
VGG16[17].
ShufﬂeNet [15]. The reason for using VGG16 is to act as a
baseline method to compare against as it was used in [1]. The
other architectures have been used in real-time systems for
detection and classiﬁcation. ResNet18 incorporates the usage
of residual blocks that directs the network toward learning the
residual representation on identity mapping.

MobileNet network architecture is based on depthwise
separable convolution. It is considered the extreme case of
the inception module, where separate spatial convolution for
each channel is applied denoted as depthwise convolutions.
Then 1x1 convolution with all the channels to merge the out-
put denoted as pointwise convolutions is used. The sepa-
ration in depthwise and pointwise convolution improve the
computational efﬁciency on one hand. On the other hand it
improves the accuracy as the cross channel and spatial corre-
lations mapping are learned separately.

ShufﬂeNet encoder is based on grouped convolution that
is a generalization of depthwise separable convolution. It uses
channel shufﬂing to ensure the connectivity between input
and output channels. This eliminates connectivity restrictions
posed by the grouped convolutions.

3. EXPERIMENTS

In this section experimental setup, detailed ablation study and
results in comparison to the state of the art are reported.

3.1. Experimental Setup

Through all of our experiments, weighted cross entropy loss
from [12] is used, to overcome the class imbalance. Adam op-
timizer [19] learning rate is set to 1e−4. Batch normalization

[20] is incorporated. L2 regularization with weight decay rate
of 5e−4 is utilized to avoid over-ﬁtting. The feature extractor
part of the network is initialized with the pre-trained corre-
sponding encoder trained on Imagenet. A width multiplier
of 1 for MobileNet to include all the feature channels is per-
formed through all the experiments. The number of groups
used in ShufﬂeNet is 3. Based on previous [15] results on
classiﬁcation and detection three groups provided adequate
accuracy.

Results are reported on Cityscapes dataset [9] which con-
tains 5000 images with ﬁne annotation, with 20 classes in-
cluding the ignored class. Another section of the dataset con-
tains coarse annotations with 20,000 labeled images. These
are used in the case of Coarse pre-training that improves the
results of the segmentation. Experiments are conducted on
images with resolution of 512x1024.

Table 2: Comparison of the most promising models in our
benchmarking framework in terms of GFLOPs and frames per
second, this is computed on image resolution 512x1024.

Model
SkipNet-MobileNet
UNet-MobileNet

GFLOPs
13.8
55.9

3.2. Semantic Segmentation Results

Semantic segmentation is evaluated using mean intersection
over union (mIoU), per-class IoU, and per-category IoU.
Table1 shows the results for the ablation study on different
encoders-decoders with mIoU and GFLOPs to demonstrate
the accuracy and computations trade-off. The main insight
gained from our experiments is that, UNet decoding method
provides more accurate segmentation results than Dilation
Frontend. This is mainly due to the transposed convolution
by 8x in the end of the Dilation Frontend, unlike the UNet
stage-wise upsampling method. The SkipNet architecture
provides on par results with UNet decoding method. In some

Table 3: Comparison of some of the models from our benchmarking framework with the state of the art segmentation networks
on cityscapes test set. GFLOPs is computed on image resolution 360x640.

Model
SegNet[21]
ENet[12]
DeepLab[2]
SkipNet-VGG16[1]
SkipNet-ShufﬂeNet
SkipNet-MobileNet

GFLOPs Class IoU Class iIoU Category IoU Category iIoU
286.03
3.83
-
-
2.0
6.2

66.4
64.0
67.7
70.1
62.2
63.0

56.1
58.3
70.4
65.3
58.3
61.5

34.2
24.4
42.6
41.7
32.4
35.2

79.8
80.4
86.4
85.7
80.2
82.0

(a)

(c)

(b)

(d)

Fig. 3: Qualitative Results on CityScapes. (a) Original Image. (b) SkipNet-MobileNet pretrained with Coarse Annotations. (c)
UNet-Resnet18. (d) SkipNet-ShufﬂeNet pretrained with Coarse Annotations.

architectures such as SkipNet-ShufﬂeNet it is less accurate
than UNet counter part by 1.5%.

The UNet method of incrementally upsampling with-in
the network provides the best in terms of accuracy. However,
Table 2 clearly shows that SkipNet architecture is more com-
putationally efﬁcient with 4x reduction in GFLOPs. This is
explained by the fact that transposed convolutions in UNet
are applied in the feature space unlike in SkipNet that are ap-
plied in label space. Table 1 shows that Coarse pre-training
improves the overall mIoU with 1-4%. The underrepresented
classes are the ones that often beneﬁt from pre-training.

Experimental results on the cityscapes test set are shown
in Table 3. Although, DeepLab provides best results in terms
of accuracy, it is not computationally efﬁcient. ENet [12]
is compared to SkipNet-ShufﬂeNet and SkipNet-MobileNet
in terms of accuracy and GFLOPs. SkipNet-ShufﬂeNet out-
performs ENet in terms of GFLOPs, yet it maintains on par
mIoU. However, we have not been able to outperform ENet
in terms of inference speed. Both SkipNet-ShufﬂeNet and
SkipNet-MobileNet outperform SegNet [21] in terms of com-

putational cost and accuracy with reduction up to 143x in
GFLOPs. Figure 3 shows qualitative results for different en-
coders including MobileNet, ShufﬂeNet and ResNet18.
It
shows that MobileNet provides more accurate segmentation
results than the later two. SkipNet-MobileNet is able to cor-
rectly segment the pedestrian and the signs on the right unlike
the others.

4. CONCLUSION

In this paper we present the ﬁrst principled approach for
benchmarking real-time segmentation networks. The decou-
pled design of the framework separates modules for better
quantitative comparison. The ﬁrst module is comprised of
the feature extraction network architecture,
the second is
the meta-architecture that provides the decoding method.
Three different meta-architectures are included in our frame-
work, including Skip architecture, UNet, and Dilation Fron-
tend. Different network architectures for feature extraction
are included, which are ShufﬂeNet, MobileNet, VGG16,

and ResNet-18. Our benchmarking framework provides re-
searchers and practitioners with a mean to evaluate design
choices for their tasks.

5. REFERENCES

[1] Jonathan Long, Evan Shelhamer, and Trevor Darrell,
“Fully convolutional networks for semantic segmenta-
tion,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015, pp. 3431–
3440.

[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokki-
“Deeplab:
nos, Kevin Murphy, and Alan L Yuille,
Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected crfs,”
arXiv preprint arXiv:1606.00915, 2016.

[3] Fisher Yu and Vladlen Koltun,

“Multi-scale context
aggregation by dilated convolutions,” arXiv preprint
arXiv:1511.07122, 2015.

[4] Guosheng Lin, Chunhua Shen, Anton van den Hen-
gel, and Ian Reid, “Exploring context with deep struc-
tured models for semantic segmentation,” arXiv preprint
arXiv:1603.03183, 2016.

[5] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du,
Chang Huang, and Philip HS Torr, “Conditional random
ﬁelds as recurrent neural networks,” in Proceedings of
the IEEE International Conference on Computer Vision,
2015, pp. 1529–1537.

[6] Evan Shelhamer, Kate Rakelly, Judy Hoffman, and
Trevor Darrell, “Clockwork convnets for video semantic
segmentation,” in Computer Vision–ECCV 2016 Work-
shops. Springer, 2016, pp. 852–868.

[7] Mark Everingham, Luc Van Gool, Christopher KI
“The
Williams, John Winn, and Andrew Zisserman,
pascal visual object classes (voc) challenge,” Interna-
tional journal of computer vision, vol. 88, no. 2, pp.
303–338, 2010.

[8] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and
Rob Fergus, “Indoor segmentation and support infer-
ence from rgbd images,” Computer Vision–ECCV 2012,
pp. 746–760, 2012.

[9] Marius Cordts, Mohamed Omran, Sebastian Ramos,
Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele,
“The
cityscapes dataset for semantic urban scene understand-
ing,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 3213–
3223.

[10] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul`o,
and Peter Kontschieder, “The mapillary vistas dataset
for semantic understanding of street scenes,” in Pro-
ceedings of the International Conference on Computer
Vision (ICCV), Venice, Italy, 2017, pp. 22–29.

[11] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jian-
“Icnet for real-time seman-
arXiv

ping Shi, and Jiaya Jia,
tic segmentation on high-resolution images,”
preprint arXiv:1704.08545, 2017.

[12] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and
Eugenio Culurciello, “Enet: A deep neural network ar-
chitecture for real-time semantic segmentation,” arXiv
preprint arXiv:1606.02147, 2016.

[13] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu
Oprea, Victor Villena-Martinez, and Jose Garcia-
“A review on deep learning techniques
Rodriguez,
arXiv preprint
applied to semantic segmentation,”
arXiv:1704.06857, 2017.

[14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam, “Mobilenets: Efﬁcient
convolutional neural networks for mobile vision appli-
cations,” arXiv preprint arXiv:1704.04861, 2017.

[15] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian
Sun, “Shufﬂenet: An extremely efﬁcient convolutional
arXiv preprint
neural network for mobile devices,”
arXiv:1707.01083, 2017.

[16] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-net: Convolutional networks for biomedical image
segmentation,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention.
Springer, 2015, pp. 234–241.

[17] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni-
tion,” arXiv preprint arXiv:1409.1556, 2014.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[19] Diederik Kingma and Jimmy Ba,
method for stochastic optimization,”
arXiv:1412.6980, 2014.

“Adam: A
arXiv preprint

[20] Sergey Ioffe and Christian Szegedy, “Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift,” in International Conference on
Machine Learning, 2015, pp. 448–456.

[21] Vijay Badrinarayanan, Alex Kendall, and Roberto
Cipolla,
“Segnet: A deep convolutional encoder-
decoder architecture for image segmentation,” arXiv
preprint arXiv:1511.00561, 2015.

RTSEG: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY

Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand

mennatul@ualberta.ca, senthil.yogamani@valeo.com
University of Alberta, Valeo Vision Systems, Cairo University

9
1
0
2
 
t
c
O
 
7
1
 
 
]

V
C
.
s
c
[
 
 
4
v
8
5
7
2
0
.
3
0
8
1
:
v
i
X
r
a

ABSTRACT

Semantic segmentation beneﬁts robotics related applications,
especially autonomous driving. Most of the research on se-
mantic segmentation only focuses on increasing the accuracy
of segmentation models with little attention to computation-
ally efﬁcient solutions. The few work conducted in this di-
rection does not provide principled methods to evaluate the
different design choices for segmentation. In this paper, we
address this gap by presenting a real-time semantic segmen-
tation benchmarking framework with a decoupled design for
feature extraction and decoding methods. The framework is
comprised of different network architectures for feature ex-
traction such as VGG16, Resnet18, MobileNet, and Shuf-
ﬂeNet. It is also comprised of multiple meta-architectures for
segmentation that deﬁne the decoding methodology. These
include SkipNet, UNet, and Dilation Frontend. Experimen-
tal results are presented on the Cityscapes dataset for urban
scenes. The modular design allows novel architectures to
emerge, that lead to 143x GFLOPs reduction in comparison to
SegNet. This benchmarking framework is publicly available
at 1.

Index Terms— realtime; semantic segmentation; bench-

marking framework

1. INTRODUCTION

Semantic segmentation has made progress in the recent years
with deep learning. The ﬁrst prominent work in this ﬁeld was
fully convolutional networks(FCNs) [1]. FCN was proposed
as an end-to-end method to learn pixel-wise classiﬁcation,
where transposed convolution was used for upsampling. Skip
architecture was used to reﬁne the segmentation output, that
utilized higher resolution feature maps. That method paved
the road to subsequent advances in the segmentation accu-
racy. Multi-scale approaches [2, 3], structured models [4, 5],
and spatio-temporal architectures [6] introduced different di-
rections for improving accuracy. All of the above approaches
focused on accuracy and robustness of segmentation. Well
known benchmarks and datasets for semantic segmentation
such as Pascal [7], NYU RGBD [8], Cityscapes [9], and Map-

1https://github.com/MSiam/TFSegmentation

Fig. 1: Overview of the different components in the frame-
work with the decoupling of feature extraction module and
decoding method.

illary [10] boosted the competition toward improving accu-
racy.

However, little attention is given to the computational ef-
ﬁciency of these networks. Although, when it comes to appli-
cations such as autonomous driving this would have tremen-
dous impact. There exists few work that tries to address the
segmentation networks efﬁciency such as [11, 12]. The sur-
vey on semantic segmentation [13] presented a comparative
study between different segmentation architectures including
ENet [12]. Yet, there is no principled comparison of differ-
ent networks and meta-architectures. These previous stud-
ies compared different networks as a whole, without com-
paring the effect of different modules. That does not enable
researchers and practitioners to pick the best suited design
choices for the required task.

In this paper we propose the ﬁrst framework toward
benchmarking real-time architectures in segmentation. Our
main contributions are: (1) we provide a modular decoupling
of the segmentation architecture into feature extraction and
decoding method which is termed as meta-architecture as
shown in Figure 1. The separation helps in understanding the
impact of different parts of the network on real-time perfor-
mance.
(2) A detailed ablation study with highlighting the
trade-off between accuracy and computational efﬁciency is

(a)

(b)

Fig. 2: Different Meta Architectures using MobileNet as the feature extraction network. a) SkipNet. b) UNet.

presented. (3) The modular design of our framework allowed
the emergence of two novel segmentation architectures using
MobileNet [14] and ShufﬂeNet [15] with multiple decod-
ing methods. ShufﬂeNet lead to 143x GFLOPs reduction
in comparison to SegNet. Our framework is built on top of
Tensorﬂow and is publicly available. Although our frame-
work delivers less operations we have not been able to deliver
higher inference speed.

2. BENCHMARKING FRAMEWORK

2.1. Meta-Architectures

Three meta-architectures are integrated in our benchmarking
software: (1) SkipNet meta-architecture[1]. (2) U-Net meta-
architecture[16]. (3) Dilation Frontend meta-architecture[3].
The meta-architectures for semantic segmentation identify the
decoding method for in the network upsampling. All of the
network architectures share the same down-sampling factor
of 32. The downsampling is achieved either by utilizing pool-
ing layers, or strides in the convolutional layers. This en-
sures that different meta architectures have a uniﬁed down-
sampling factor to assess the effect of the decoding method
only.

SkipNet architecture denotes a similar architecture to
FCN8s [1]. The main idea of the skip architecture is to ben-
eﬁt from feature maps from higher resolution to improve the
output segmentation. SkipNet applies transposed convolution
on heatmaps in the label space instead of performing it on
feature space. This entails a more computationally efﬁcient
decoding method than others. Feature extraction networks
have the same downsampling factor of 32, so they follow the
8 stride version of skip architecture. Higher resolution feature
maps are followed by 1x1 convolution to map from feature
space to label space that produces heatmaps corresponding
to each class. The ﬁnal heatmap with downsampling factor
of 32 is followed by transposed convolution with stride 2.

Elementwise addition between this upsampled heatmaps and
the higher resolution heatmaps is performed. Finally, the
output heat maps are followed by a transposed convolution
for up-sampling with stride 8. Figure 2(a) shows the SkipNet
architecture utilizing a MobileMet encoder.

U-Net architecture denotes the method of decoding that
up-samples features using transposed convolution corre-
sponding to each downsampling stage. The up-sampled
features are fused with the corresponding features maps from
the encoder with the same resolution. The stage-wise up-
sampling provides higher accuracy than one shot 8x upsam-
pling. The current fusion method used in the framework is
element-wise addition. Concatenation as a fusion method can
provide better accuracy, as it enables the network to learn
the weighted fusion of features. Nonetheless, it increases the
computational cost, as it is directly affected by the number
of channels. The upsampled features are then followed by
1x1 convolution to output the ﬁnal pixel-wise classiﬁcation.
Figure 2(b) shows the UNet architecture using MobileNet as
a feature extraction network.

Dilation Frontend architecture utilizes dilated convolu-
tion instead of downsampling the feature maps. Dilated con-
volution enables the network to maintain an adequate recep-
tive ﬁeld, but without degrading the resolution from pooling
or strided convolution. However, a side-effect of this method
is that computational cost increases, since the operations are
performed on larger resolution feature maps. The encoder
network is modiﬁed to incorporate a downsampling factor of
8 instead of 32. The decrease of the downsampling is per-
formed by either removing pooling layers or converting stride
2 convolution to stride 1. The pooling or strided convolutions
are then replaced with two dilated convolutions[3] with dila-
tion factor 2 and 4 respectively.

Table 1: Comparison of different encoders and decoding methods in accuracy on cityscapes validation set. The modular
decoupled design in RTSeg enabled such comparison. Coarse indicates whether the network was pre-trained on the coarse
annotation or not.

Decoder Encoder
SkipNet MobileNet No
ShufﬂeNet No
SkipNet
ResNet18
UNet
No
UNet
MobileNet No
ShufﬂeNet No
UNet
Dilation MobileNet No
Dilation
ShufﬂeNet No
SkipNet MobileNet Yes
ShufﬂeNet Yes
SkipNet

Coarse mIoU Road
95.9
61.3
94.8
55.5
95.8
57.9
95.2
61.0
95.1
57.0
95.6
57.8
95.2
53.9
62.4
95.4
94.6
59.3

Sidewalk Building
73.6
68.6
73.2
71.3
69.5
72.3
68.5
73.9
70.5

86.9
83.9
85.8
86.8
83.7
85.9
84.1
86.6
85.5

Sign
57.6
50.5
57.5
60.9
54.3
57.0
57.3
57.4
54.9

Sky
91.2
88.6
91.0
92.8
89.0
91.4
90.3
91.1
90.8

Person Car
89.0
66.4
86.5
60.8
88.6
66.0
68.1
88.8
87.8
61.7
87.8
64.9
86.6
62.9
88.4
65.7
87.5
60.2

Bicycle Truck
63.6
58.8
63.2
65.0
59.9
62.8
60.2
63.3
58.8

45.9
29.6
31.4
41.3
35.5
26.3
23.3
45.3
45.4

2.2. Feature Extraction Architectures

(2) ResNet18[18].

In order to achieve real-time performance multiple network
architectures are integrated in the benchmarking framework.
The framework includes four state of the art real-time net-
work architectures for feature extraction. These are:
(1)
(4)
(3) MobileNet[14].
VGG16[17].
ShufﬂeNet [15]. The reason for using VGG16 is to act as a
baseline method to compare against as it was used in [1]. The
other architectures have been used in real-time systems for
detection and classiﬁcation. ResNet18 incorporates the usage
of residual blocks that directs the network toward learning the
residual representation on identity mapping.

MobileNet network architecture is based on depthwise
separable convolution. It is considered the extreme case of
the inception module, where separate spatial convolution for
each channel is applied denoted as depthwise convolutions.
Then 1x1 convolution with all the channels to merge the out-
put denoted as pointwise convolutions is used. The sepa-
ration in depthwise and pointwise convolution improve the
computational efﬁciency on one hand. On the other hand it
improves the accuracy as the cross channel and spatial corre-
lations mapping are learned separately.

ShufﬂeNet encoder is based on grouped convolution that
is a generalization of depthwise separable convolution. It uses
channel shufﬂing to ensure the connectivity between input
and output channels. This eliminates connectivity restrictions
posed by the grouped convolutions.

3. EXPERIMENTS

In this section experimental setup, detailed ablation study and
results in comparison to the state of the art are reported.

3.1. Experimental Setup

Through all of our experiments, weighted cross entropy loss
from [12] is used, to overcome the class imbalance. Adam op-
timizer [19] learning rate is set to 1e−4. Batch normalization

[20] is incorporated. L2 regularization with weight decay rate
of 5e−4 is utilized to avoid over-ﬁtting. The feature extractor
part of the network is initialized with the pre-trained corre-
sponding encoder trained on Imagenet. A width multiplier
of 1 for MobileNet to include all the feature channels is per-
formed through all the experiments. The number of groups
used in ShufﬂeNet is 3. Based on previous [15] results on
classiﬁcation and detection three groups provided adequate
accuracy.

Results are reported on Cityscapes dataset [9] which con-
tains 5000 images with ﬁne annotation, with 20 classes in-
cluding the ignored class. Another section of the dataset con-
tains coarse annotations with 20,000 labeled images. These
are used in the case of Coarse pre-training that improves the
results of the segmentation. Experiments are conducted on
images with resolution of 512x1024.

Table 2: Comparison of the most promising models in our
benchmarking framework in terms of GFLOPs and frames per
second, this is computed on image resolution 512x1024.

Model
SkipNet-MobileNet
UNet-MobileNet

GFLOPs
13.8
55.9

3.2. Semantic Segmentation Results

Semantic segmentation is evaluated using mean intersection
over union (mIoU), per-class IoU, and per-category IoU.
Table1 shows the results for the ablation study on different
encoders-decoders with mIoU and GFLOPs to demonstrate
the accuracy and computations trade-off. The main insight
gained from our experiments is that, UNet decoding method
provides more accurate segmentation results than Dilation
Frontend. This is mainly due to the transposed convolution
by 8x in the end of the Dilation Frontend, unlike the UNet
stage-wise upsampling method. The SkipNet architecture
provides on par results with UNet decoding method. In some

Table 3: Comparison of some of the models from our benchmarking framework with the state of the art segmentation networks
on cityscapes test set. GFLOPs is computed on image resolution 360x640.

Model
SegNet[21]
ENet[12]
DeepLab[2]
SkipNet-VGG16[1]
SkipNet-ShufﬂeNet
SkipNet-MobileNet

GFLOPs Class IoU Class iIoU Category IoU Category iIoU
286.03
3.83
-
-
2.0
6.2

66.4
64.0
67.7
70.1
62.2
63.0

79.8
80.4
86.4
85.7
80.2
82.0

56.1
58.3
70.4
65.3
58.3
61.5

34.2
24.4
42.6
41.7
32.4
35.2

(a)

(c)

(b)

(d)

Fig. 3: Qualitative Results on CityScapes. (a) Original Image. (b) SkipNet-MobileNet pretrained with Coarse Annotations. (c)
UNet-Resnet18. (d) SkipNet-ShufﬂeNet pretrained with Coarse Annotations.

architectures such as SkipNet-ShufﬂeNet it is less accurate
than UNet counter part by 1.5%.

The UNet method of incrementally upsampling with-in
the network provides the best in terms of accuracy. However,
Table 2 clearly shows that SkipNet architecture is more com-
putationally efﬁcient with 4x reduction in GFLOPs. This is
explained by the fact that transposed convolutions in UNet
are applied in the feature space unlike in SkipNet that are ap-
plied in label space. Table 1 shows that Coarse pre-training
improves the overall mIoU with 1-4%. The underrepresented
classes are the ones that often beneﬁt from pre-training.

Experimental results on the cityscapes test set are shown
in Table 3. Although, DeepLab provides best results in terms
of accuracy, it is not computationally efﬁcient. ENet [12]
is compared to SkipNet-ShufﬂeNet and SkipNet-MobileNet
in terms of accuracy and GFLOPs. SkipNet-ShufﬂeNet out-
performs ENet in terms of GFLOPs, yet it maintains on par
mIoU. However, we have not been able to outperform ENet
in terms of inference speed. Both SkipNet-ShufﬂeNet and
SkipNet-MobileNet outperform SegNet [21] in terms of com-

putational cost and accuracy with reduction up to 143x in
GFLOPs. Figure 3 shows qualitative results for different en-
coders including MobileNet, ShufﬂeNet and ResNet18.
It
shows that MobileNet provides more accurate segmentation
results than the later two. SkipNet-MobileNet is able to cor-
rectly segment the pedestrian and the signs on the right unlike
the others.

4. CONCLUSION

In this paper we present the ﬁrst principled approach for
benchmarking real-time segmentation networks. The decou-
pled design of the framework separates modules for better
quantitative comparison. The ﬁrst module is comprised of
the feature extraction network architecture,
the second is
the meta-architecture that provides the decoding method.
Three different meta-architectures are included in our frame-
work, including Skip architecture, UNet, and Dilation Fron-
tend. Different network architectures for feature extraction
are included, which are ShufﬂeNet, MobileNet, VGG16,

and ResNet-18. Our benchmarking framework provides re-
searchers and practitioners with a mean to evaluate design
choices for their tasks.

5. REFERENCES

[1] Jonathan Long, Evan Shelhamer, and Trevor Darrell,
“Fully convolutional networks for semantic segmenta-
tion,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015, pp. 3431–
3440.

[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokki-
“Deeplab:
nos, Kevin Murphy, and Alan L Yuille,
Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected crfs,”
arXiv preprint arXiv:1606.00915, 2016.

[3] Fisher Yu and Vladlen Koltun,

“Multi-scale context
aggregation by dilated convolutions,” arXiv preprint
arXiv:1511.07122, 2015.

[4] Guosheng Lin, Chunhua Shen, Anton van den Hen-
gel, and Ian Reid, “Exploring context with deep struc-
tured models for semantic segmentation,” arXiv preprint
arXiv:1603.03183, 2016.

[5] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du,
Chang Huang, and Philip HS Torr, “Conditional random
ﬁelds as recurrent neural networks,” in Proceedings of
the IEEE International Conference on Computer Vision,
2015, pp. 1529–1537.

[6] Evan Shelhamer, Kate Rakelly, Judy Hoffman, and
Trevor Darrell, “Clockwork convnets for video semantic
segmentation,” in Computer Vision–ECCV 2016 Work-
shops. Springer, 2016, pp. 852–868.

[7] Mark Everingham, Luc Van Gool, Christopher KI
“The
Williams, John Winn, and Andrew Zisserman,
pascal visual object classes (voc) challenge,” Interna-
tional journal of computer vision, vol. 88, no. 2, pp.
303–338, 2010.

[8] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and
Rob Fergus, “Indoor segmentation and support infer-
ence from rgbd images,” Computer Vision–ECCV 2012,
pp. 746–760, 2012.

[9] Marius Cordts, Mohamed Omran, Sebastian Ramos,
Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele,
“The
cityscapes dataset for semantic urban scene understand-
ing,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 3213–
3223.

[10] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul`o,
and Peter Kontschieder, “The mapillary vistas dataset
for semantic understanding of street scenes,” in Pro-
ceedings of the International Conference on Computer
Vision (ICCV), Venice, Italy, 2017, pp. 22–29.

[11] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jian-
“Icnet for real-time seman-
arXiv

ping Shi, and Jiaya Jia,
tic segmentation on high-resolution images,”
preprint arXiv:1704.08545, 2017.

[12] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and
Eugenio Culurciello, “Enet: A deep neural network ar-
chitecture for real-time semantic segmentation,” arXiv
preprint arXiv:1606.02147, 2016.

[13] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu
Oprea, Victor Villena-Martinez, and Jose Garcia-
“A review on deep learning techniques
Rodriguez,
arXiv preprint
applied to semantic segmentation,”
arXiv:1704.06857, 2017.

[14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam, “Mobilenets: Efﬁcient
convolutional neural networks for mobile vision appli-
cations,” arXiv preprint arXiv:1704.04861, 2017.

[15] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian
Sun, “Shufﬂenet: An extremely efﬁcient convolutional
arXiv preprint
neural network for mobile devices,”
arXiv:1707.01083, 2017.

[16] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-net: Convolutional networks for biomedical image
segmentation,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention.
Springer, 2015, pp. 234–241.

[17] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni-
tion,” arXiv preprint arXiv:1409.1556, 2014.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[19] Diederik Kingma and Jimmy Ba,
method for stochastic optimization,”
arXiv:1412.6980, 2014.

“Adam: A
arXiv preprint

[20] Sergey Ioffe and Christian Szegedy, “Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift,” in International Conference on
Machine Learning, 2015, pp. 448–456.

[21] Vijay Badrinarayanan, Alex Kendall, and Roberto
Cipolla,
“Segnet: A deep convolutional encoder-
decoder architecture for image segmentation,” arXiv
preprint arXiv:1511.00561, 2015.

RTSEG: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY

Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand

mennatul@ualberta.ca, senthil.yogamani@valeo.com
University of Alberta, Valeo Vision Systems, Cairo University

9
1
0
2
 
t
c
O
 
7
1
 
 
]

V
C
.
s
c
[
 
 
4
v
8
5
7
2
0
.
3
0
8
1
:
v
i
X
r
a

ABSTRACT

Semantic segmentation beneﬁts robotics related applications,
especially autonomous driving. Most of the research on se-
mantic segmentation only focuses on increasing the accuracy
of segmentation models with little attention to computation-
ally efﬁcient solutions. The few work conducted in this di-
rection does not provide principled methods to evaluate the
different design choices for segmentation. In this paper, we
address this gap by presenting a real-time semantic segmen-
tation benchmarking framework with a decoupled design for
feature extraction and decoding methods. The framework is
comprised of different network architectures for feature ex-
traction such as VGG16, Resnet18, MobileNet, and Shuf-
ﬂeNet. It is also comprised of multiple meta-architectures for
segmentation that deﬁne the decoding methodology. These
include SkipNet, UNet, and Dilation Frontend. Experimen-
tal results are presented on the Cityscapes dataset for urban
scenes. The modular design allows novel architectures to
emerge, that lead to 143x GFLOPs reduction in comparison to
SegNet. This benchmarking framework is publicly available
at 1.

Index Terms— realtime; semantic segmentation; bench-

marking framework

1. INTRODUCTION

Semantic segmentation has made progress in the recent years
with deep learning. The ﬁrst prominent work in this ﬁeld was
fully convolutional networks(FCNs) [1]. FCN was proposed
as an end-to-end method to learn pixel-wise classiﬁcation,
where transposed convolution was used for upsampling. Skip
architecture was used to reﬁne the segmentation output, that
utilized higher resolution feature maps. That method paved
the road to subsequent advances in the segmentation accu-
racy. Multi-scale approaches [2, 3], structured models [4, 5],
and spatio-temporal architectures [6] introduced different di-
rections for improving accuracy. All of the above approaches
focused on accuracy and robustness of segmentation. Well
known benchmarks and datasets for semantic segmentation
such as Pascal [7], NYU RGBD [8], Cityscapes [9], and Map-

1https://github.com/MSiam/TFSegmentation

Fig. 1: Overview of the different components in the frame-
work with the decoupling of feature extraction module and
decoding method.

illary [10] boosted the competition toward improving accu-
racy.

However, little attention is given to the computational ef-
ﬁciency of these networks. Although, when it comes to appli-
cations such as autonomous driving this would have tremen-
dous impact. There exists few work that tries to address the
segmentation networks efﬁciency such as [11, 12]. The sur-
vey on semantic segmentation [13] presented a comparative
study between different segmentation architectures including
ENet [12]. Yet, there is no principled comparison of differ-
ent networks and meta-architectures. These previous stud-
ies compared different networks as a whole, without com-
paring the effect of different modules. That does not enable
researchers and practitioners to pick the best suited design
choices for the required task.

In this paper we propose the ﬁrst framework toward
benchmarking real-time architectures in segmentation. Our
main contributions are: (1) we provide a modular decoupling
of the segmentation architecture into feature extraction and
decoding method which is termed as meta-architecture as
shown in Figure 1. The separation helps in understanding the
impact of different parts of the network on real-time perfor-
mance.
(2) A detailed ablation study with highlighting the
trade-off between accuracy and computational efﬁciency is

(a)

(b)

Fig. 2: Different Meta Architectures using MobileNet as the feature extraction network. a) SkipNet. b) UNet.

presented. (3) The modular design of our framework allowed
the emergence of two novel segmentation architectures using
MobileNet [14] and ShufﬂeNet [15] with multiple decod-
ing methods. ShufﬂeNet lead to 143x GFLOPs reduction
in comparison to SegNet. Our framework is built on top of
Tensorﬂow and is publicly available. Although our frame-
work delivers less operations we have not been able to deliver
higher inference speed.

2. BENCHMARKING FRAMEWORK

2.1. Meta-Architectures

Three meta-architectures are integrated in our benchmarking
software: (1) SkipNet meta-architecture[1]. (2) U-Net meta-
architecture[16]. (3) Dilation Frontend meta-architecture[3].
The meta-architectures for semantic segmentation identify the
decoding method for in the network upsampling. All of the
network architectures share the same down-sampling factor
of 32. The downsampling is achieved either by utilizing pool-
ing layers, or strides in the convolutional layers. This en-
sures that different meta architectures have a uniﬁed down-
sampling factor to assess the effect of the decoding method
only.

SkipNet architecture denotes a similar architecture to
FCN8s [1]. The main idea of the skip architecture is to ben-
eﬁt from feature maps from higher resolution to improve the
output segmentation. SkipNet applies transposed convolution
on heatmaps in the label space instead of performing it on
feature space. This entails a more computationally efﬁcient
decoding method than others. Feature extraction networks
have the same downsampling factor of 32, so they follow the
8 stride version of skip architecture. Higher resolution feature
maps are followed by 1x1 convolution to map from feature
space to label space that produces heatmaps corresponding
to each class. The ﬁnal heatmap with downsampling factor
of 32 is followed by transposed convolution with stride 2.

Elementwise addition between this upsampled heatmaps and
the higher resolution heatmaps is performed. Finally, the
output heat maps are followed by a transposed convolution
for up-sampling with stride 8. Figure 2(a) shows the SkipNet
architecture utilizing a MobileMet encoder.

U-Net architecture denotes the method of decoding that
up-samples features using transposed convolution corre-
sponding to each downsampling stage. The up-sampled
features are fused with the corresponding features maps from
the encoder with the same resolution. The stage-wise up-
sampling provides higher accuracy than one shot 8x upsam-
pling. The current fusion method used in the framework is
element-wise addition. Concatenation as a fusion method can
provide better accuracy, as it enables the network to learn
the weighted fusion of features. Nonetheless, it increases the
computational cost, as it is directly affected by the number
of channels. The upsampled features are then followed by
1x1 convolution to output the ﬁnal pixel-wise classiﬁcation.
Figure 2(b) shows the UNet architecture using MobileNet as
a feature extraction network.

Dilation Frontend architecture utilizes dilated convolu-
tion instead of downsampling the feature maps. Dilated con-
volution enables the network to maintain an adequate recep-
tive ﬁeld, but without degrading the resolution from pooling
or strided convolution. However, a side-effect of this method
is that computational cost increases, since the operations are
performed on larger resolution feature maps. The encoder
network is modiﬁed to incorporate a downsampling factor of
8 instead of 32. The decrease of the downsampling is per-
formed by either removing pooling layers or converting stride
2 convolution to stride 1. The pooling or strided convolutions
are then replaced with two dilated convolutions[3] with dila-
tion factor 2 and 4 respectively.

Table 1: Comparison of different encoders and decoding methods in accuracy on cityscapes validation set. The modular
decoupled design in RTSeg enabled such comparison. Coarse indicates whether the network was pre-trained on the coarse
annotation or not.

Decoder Encoder
SkipNet MobileNet No
ShufﬂeNet No
SkipNet
ResNet18
UNet
No
UNet
MobileNet No
ShufﬂeNet No
UNet
Dilation MobileNet No
Dilation
ShufﬂeNet No
SkipNet MobileNet Yes
ShufﬂeNet Yes
SkipNet

Coarse mIoU Road
95.9
61.3
94.8
55.5
95.8
57.9
95.2
61.0
95.1
57.0
95.6
57.8
95.2
53.9
62.4
95.4
94.6
59.3

Sidewalk Building
73.6
68.6
73.2
71.3
69.5
72.3
68.5
73.9
70.5

86.9
83.9
85.8
86.8
83.7
85.9
84.1
86.6
85.5

Sign
57.6
50.5
57.5
60.9
54.3
57.0
57.3
57.4
54.9

Sky
91.2
88.6
91.0
92.8
89.0
91.4
90.3
91.1
90.8

Person Car
89.0
66.4
86.5
60.8
88.6
66.0
68.1
88.8
87.8
61.7
87.8
64.9
86.6
62.9
88.4
65.7
87.5
60.2

Bicycle Truck
63.6
58.8
63.2
65.0
59.9
62.8
60.2
63.3
58.8

45.9
29.6
31.4
41.3
35.5
26.3
23.3
45.3
45.4

2.2. Feature Extraction Architectures

(2) ResNet18[18].

In order to achieve real-time performance multiple network
architectures are integrated in the benchmarking framework.
The framework includes four state of the art real-time net-
work architectures for feature extraction. These are:
(1)
(4)
(3) MobileNet[14].
VGG16[17].
ShufﬂeNet [15]. The reason for using VGG16 is to act as a
baseline method to compare against as it was used in [1]. The
other architectures have been used in real-time systems for
detection and classiﬁcation. ResNet18 incorporates the usage
of residual blocks that directs the network toward learning the
residual representation on identity mapping.

MobileNet network architecture is based on depthwise
separable convolution. It is considered the extreme case of
the inception module, where separate spatial convolution for
each channel is applied denoted as depthwise convolutions.
Then 1x1 convolution with all the channels to merge the out-
put denoted as pointwise convolutions is used. The sepa-
ration in depthwise and pointwise convolution improve the
computational efﬁciency on one hand. On the other hand it
improves the accuracy as the cross channel and spatial corre-
lations mapping are learned separately.

ShufﬂeNet encoder is based on grouped convolution that
is a generalization of depthwise separable convolution. It uses
channel shufﬂing to ensure the connectivity between input
and output channels. This eliminates connectivity restrictions
posed by the grouped convolutions.

3. EXPERIMENTS

In this section experimental setup, detailed ablation study and
results in comparison to the state of the art are reported.

3.1. Experimental Setup

Through all of our experiments, weighted cross entropy loss
from [12] is used, to overcome the class imbalance. Adam op-
timizer [19] learning rate is set to 1e−4. Batch normalization

[20] is incorporated. L2 regularization with weight decay rate
of 5e−4 is utilized to avoid over-ﬁtting. The feature extractor
part of the network is initialized with the pre-trained corre-
sponding encoder trained on Imagenet. A width multiplier
of 1 for MobileNet to include all the feature channels is per-
formed through all the experiments. The number of groups
used in ShufﬂeNet is 3. Based on previous [15] results on
classiﬁcation and detection three groups provided adequate
accuracy.

Results are reported on Cityscapes dataset [9] which con-
tains 5000 images with ﬁne annotation, with 20 classes in-
cluding the ignored class. Another section of the dataset con-
tains coarse annotations with 20,000 labeled images. These
are used in the case of Coarse pre-training that improves the
results of the segmentation. Experiments are conducted on
images with resolution of 512x1024.

Table 2: Comparison of the most promising models in our
benchmarking framework in terms of GFLOPs and frames per
second, this is computed on image resolution 512x1024.

Model
SkipNet-MobileNet
UNet-MobileNet

GFLOPs
13.8
55.9

3.2. Semantic Segmentation Results

Semantic segmentation is evaluated using mean intersection
over union (mIoU), per-class IoU, and per-category IoU.
Table1 shows the results for the ablation study on different
encoders-decoders with mIoU and GFLOPs to demonstrate
the accuracy and computations trade-off. The main insight
gained from our experiments is that, UNet decoding method
provides more accurate segmentation results than Dilation
Frontend. This is mainly due to the transposed convolution
by 8x in the end of the Dilation Frontend, unlike the UNet
stage-wise upsampling method. The SkipNet architecture
provides on par results with UNet decoding method. In some

Table 3: Comparison of some of the models from our benchmarking framework with the state of the art segmentation networks
on cityscapes test set. GFLOPs is computed on image resolution 360x640.

Model
SegNet[21]
ENet[12]
DeepLab[2]
SkipNet-VGG16[1]
SkipNet-ShufﬂeNet
SkipNet-MobileNet

GFLOPs Class IoU Class iIoU Category IoU Category iIoU
286.03
3.83
-
-
2.0
6.2

66.4
64.0
67.7
70.1
62.2
63.0

56.1
58.3
70.4
65.3
58.3
61.5

34.2
24.4
42.6
41.7
32.4
35.2

79.8
80.4
86.4
85.7
80.2
82.0

(a)

(c)

(b)

(d)

Fig. 3: Qualitative Results on CityScapes. (a) Original Image. (b) SkipNet-MobileNet pretrained with Coarse Annotations. (c)
UNet-Resnet18. (d) SkipNet-ShufﬂeNet pretrained with Coarse Annotations.

architectures such as SkipNet-ShufﬂeNet it is less accurate
than UNet counter part by 1.5%.

The UNet method of incrementally upsampling with-in
the network provides the best in terms of accuracy. However,
Table 2 clearly shows that SkipNet architecture is more com-
putationally efﬁcient with 4x reduction in GFLOPs. This is
explained by the fact that transposed convolutions in UNet
are applied in the feature space unlike in SkipNet that are ap-
plied in label space. Table 1 shows that Coarse pre-training
improves the overall mIoU with 1-4%. The underrepresented
classes are the ones that often beneﬁt from pre-training.

Experimental results on the cityscapes test set are shown
in Table 3. Although, DeepLab provides best results in terms
of accuracy, it is not computationally efﬁcient. ENet [12]
is compared to SkipNet-ShufﬂeNet and SkipNet-MobileNet
in terms of accuracy and GFLOPs. SkipNet-ShufﬂeNet out-
performs ENet in terms of GFLOPs, yet it maintains on par
mIoU. However, we have not been able to outperform ENet
in terms of inference speed. Both SkipNet-ShufﬂeNet and
SkipNet-MobileNet outperform SegNet [21] in terms of com-

putational cost and accuracy with reduction up to 143x in
GFLOPs. Figure 3 shows qualitative results for different en-
coders including MobileNet, ShufﬂeNet and ResNet18.
It
shows that MobileNet provides more accurate segmentation
results than the later two. SkipNet-MobileNet is able to cor-
rectly segment the pedestrian and the signs on the right unlike
the others.

4. CONCLUSION

In this paper we present the ﬁrst principled approach for
benchmarking real-time segmentation networks. The decou-
pled design of the framework separates modules for better
quantitative comparison. The ﬁrst module is comprised of
the feature extraction network architecture,
the second is
the meta-architecture that provides the decoding method.
Three different meta-architectures are included in our frame-
work, including Skip architecture, UNet, and Dilation Fron-
tend. Different network architectures for feature extraction
are included, which are ShufﬂeNet, MobileNet, VGG16,

and ResNet-18. Our benchmarking framework provides re-
searchers and practitioners with a mean to evaluate design
choices for their tasks.

5. REFERENCES

[1] Jonathan Long, Evan Shelhamer, and Trevor Darrell,
“Fully convolutional networks for semantic segmenta-
tion,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015, pp. 3431–
3440.

[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokki-
“Deeplab:
nos, Kevin Murphy, and Alan L Yuille,
Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected crfs,”
arXiv preprint arXiv:1606.00915, 2016.

[3] Fisher Yu and Vladlen Koltun,

“Multi-scale context
aggregation by dilated convolutions,” arXiv preprint
arXiv:1511.07122, 2015.

[4] Guosheng Lin, Chunhua Shen, Anton van den Hen-
gel, and Ian Reid, “Exploring context with deep struc-
tured models for semantic segmentation,” arXiv preprint
arXiv:1603.03183, 2016.

[5] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du,
Chang Huang, and Philip HS Torr, “Conditional random
ﬁelds as recurrent neural networks,” in Proceedings of
the IEEE International Conference on Computer Vision,
2015, pp. 1529–1537.

[6] Evan Shelhamer, Kate Rakelly, Judy Hoffman, and
Trevor Darrell, “Clockwork convnets for video semantic
segmentation,” in Computer Vision–ECCV 2016 Work-
shops. Springer, 2016, pp. 852–868.

[7] Mark Everingham, Luc Van Gool, Christopher KI
“The
Williams, John Winn, and Andrew Zisserman,
pascal visual object classes (voc) challenge,” Interna-
tional journal of computer vision, vol. 88, no. 2, pp.
303–338, 2010.

[8] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and
Rob Fergus, “Indoor segmentation and support infer-
ence from rgbd images,” Computer Vision–ECCV 2012,
pp. 746–760, 2012.

[9] Marius Cordts, Mohamed Omran, Sebastian Ramos,
Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele,
“The
cityscapes dataset for semantic urban scene understand-
ing,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 3213–
3223.

[10] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul`o,
and Peter Kontschieder, “The mapillary vistas dataset
for semantic understanding of street scenes,” in Pro-
ceedings of the International Conference on Computer
Vision (ICCV), Venice, Italy, 2017, pp. 22–29.

[11] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jian-
“Icnet for real-time seman-
arXiv

ping Shi, and Jiaya Jia,
tic segmentation on high-resolution images,”
preprint arXiv:1704.08545, 2017.

[12] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and
Eugenio Culurciello, “Enet: A deep neural network ar-
chitecture for real-time semantic segmentation,” arXiv
preprint arXiv:1606.02147, 2016.

[13] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu
Oprea, Victor Villena-Martinez, and Jose Garcia-
“A review on deep learning techniques
Rodriguez,
arXiv preprint
applied to semantic segmentation,”
arXiv:1704.06857, 2017.

[14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam, “Mobilenets: Efﬁcient
convolutional neural networks for mobile vision appli-
cations,” arXiv preprint arXiv:1704.04861, 2017.

[15] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian
Sun, “Shufﬂenet: An extremely efﬁcient convolutional
arXiv preprint
neural network for mobile devices,”
arXiv:1707.01083, 2017.

[16] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-net: Convolutional networks for biomedical image
segmentation,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention.
Springer, 2015, pp. 234–241.

[17] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni-
tion,” arXiv preprint arXiv:1409.1556, 2014.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[19] Diederik Kingma and Jimmy Ba,
method for stochastic optimization,”
arXiv:1412.6980, 2014.

“Adam: A
arXiv preprint

[20] Sergey Ioffe and Christian Szegedy, “Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift,” in International Conference on
Machine Learning, 2015, pp. 448–456.

[21] Vijay Badrinarayanan, Alex Kendall, and Roberto
Cipolla,
“Segnet: A deep convolutional encoder-
decoder architecture for image segmentation,” arXiv
preprint arXiv:1511.00561, 2015.

RTSEG: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY

Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand

mennatul@ualberta.ca, senthil.yogamani@valeo.com
University of Alberta, Valeo Vision Systems, Cairo University

9
1
0
2
 
t
c
O
 
7
1
 
 
]

V
C
.
s
c
[
 
 
4
v
8
5
7
2
0
.
3
0
8
1
:
v
i
X
r
a

ABSTRACT

Semantic segmentation beneﬁts robotics related applications,
especially autonomous driving. Most of the research on se-
mantic segmentation only focuses on increasing the accuracy
of segmentation models with little attention to computation-
ally efﬁcient solutions. The few work conducted in this di-
rection does not provide principled methods to evaluate the
different design choices for segmentation. In this paper, we
address this gap by presenting a real-time semantic segmen-
tation benchmarking framework with a decoupled design for
feature extraction and decoding methods. The framework is
comprised of different network architectures for feature ex-
traction such as VGG16, Resnet18, MobileNet, and Shuf-
ﬂeNet. It is also comprised of multiple meta-architectures for
segmentation that deﬁne the decoding methodology. These
include SkipNet, UNet, and Dilation Frontend. Experimen-
tal results are presented on the Cityscapes dataset for urban
scenes. The modular design allows novel architectures to
emerge, that lead to 143x GFLOPs reduction in comparison to
SegNet. This benchmarking framework is publicly available
at 1.

Index Terms— realtime; semantic segmentation; bench-

marking framework

1. INTRODUCTION

Semantic segmentation has made progress in the recent years
with deep learning. The ﬁrst prominent work in this ﬁeld was
fully convolutional networks(FCNs) [1]. FCN was proposed
as an end-to-end method to learn pixel-wise classiﬁcation,
where transposed convolution was used for upsampling. Skip
architecture was used to reﬁne the segmentation output, that
utilized higher resolution feature maps. That method paved
the road to subsequent advances in the segmentation accu-
racy. Multi-scale approaches [2, 3], structured models [4, 5],
and spatio-temporal architectures [6] introduced different di-
rections for improving accuracy. All of the above approaches
focused on accuracy and robustness of segmentation. Well
known benchmarks and datasets for semantic segmentation
such as Pascal [7], NYU RGBD [8], Cityscapes [9], and Map-

1https://github.com/MSiam/TFSegmentation

Fig. 1: Overview of the different components in the frame-
work with the decoupling of feature extraction module and
decoding method.

illary [10] boosted the competition toward improving accu-
racy.

However, little attention is given to the computational ef-
ﬁciency of these networks. Although, when it comes to appli-
cations such as autonomous driving this would have tremen-
dous impact. There exists few work that tries to address the
segmentation networks efﬁciency such as [11, 12]. The sur-
vey on semantic segmentation [13] presented a comparative
study between different segmentation architectures including
ENet [12]. Yet, there is no principled comparison of differ-
ent networks and meta-architectures. These previous stud-
ies compared different networks as a whole, without com-
paring the effect of different modules. That does not enable
researchers and practitioners to pick the best suited design
choices for the required task.

In this paper we propose the ﬁrst framework toward
benchmarking real-time architectures in segmentation. Our
main contributions are: (1) we provide a modular decoupling
of the segmentation architecture into feature extraction and
decoding method which is termed as meta-architecture as
shown in Figure 1. The separation helps in understanding the
impact of different parts of the network on real-time perfor-
mance.
(2) A detailed ablation study with highlighting the
trade-off between accuracy and computational efﬁciency is

(a)

(b)

Fig. 2: Different Meta Architectures using MobileNet as the feature extraction network. a) SkipNet. b) UNet.

presented. (3) The modular design of our framework allowed
the emergence of two novel segmentation architectures using
MobileNet [14] and ShufﬂeNet [15] with multiple decod-
ing methods. ShufﬂeNet lead to 143x GFLOPs reduction
in comparison to SegNet. Our framework is built on top of
Tensorﬂow and is publicly available. Although our frame-
work delivers less operations we have not been able to deliver
higher inference speed.

2. BENCHMARKING FRAMEWORK

2.1. Meta-Architectures

Three meta-architectures are integrated in our benchmarking
software: (1) SkipNet meta-architecture[1]. (2) U-Net meta-
architecture[16]. (3) Dilation Frontend meta-architecture[3].
The meta-architectures for semantic segmentation identify the
decoding method for in the network upsampling. All of the
network architectures share the same down-sampling factor
of 32. The downsampling is achieved either by utilizing pool-
ing layers, or strides in the convolutional layers. This en-
sures that different meta architectures have a uniﬁed down-
sampling factor to assess the effect of the decoding method
only.

SkipNet architecture denotes a similar architecture to
FCN8s [1]. The main idea of the skip architecture is to ben-
eﬁt from feature maps from higher resolution to improve the
output segmentation. SkipNet applies transposed convolution
on heatmaps in the label space instead of performing it on
feature space. This entails a more computationally efﬁcient
decoding method than others. Feature extraction networks
have the same downsampling factor of 32, so they follow the
8 stride version of skip architecture. Higher resolution feature
maps are followed by 1x1 convolution to map from feature
space to label space that produces heatmaps corresponding
to each class. The ﬁnal heatmap with downsampling factor
of 32 is followed by transposed convolution with stride 2.

Elementwise addition between this upsampled heatmaps and
the higher resolution heatmaps is performed. Finally, the
output heat maps are followed by a transposed convolution
for up-sampling with stride 8. Figure 2(a) shows the SkipNet
architecture utilizing a MobileMet encoder.

U-Net architecture denotes the method of decoding that
up-samples features using transposed convolution corre-
sponding to each downsampling stage. The up-sampled
features are fused with the corresponding features maps from
the encoder with the same resolution. The stage-wise up-
sampling provides higher accuracy than one shot 8x upsam-
pling. The current fusion method used in the framework is
element-wise addition. Concatenation as a fusion method can
provide better accuracy, as it enables the network to learn
the weighted fusion of features. Nonetheless, it increases the
computational cost, as it is directly affected by the number
of channels. The upsampled features are then followed by
1x1 convolution to output the ﬁnal pixel-wise classiﬁcation.
Figure 2(b) shows the UNet architecture using MobileNet as
a feature extraction network.

Dilation Frontend architecture utilizes dilated convolu-
tion instead of downsampling the feature maps. Dilated con-
volution enables the network to maintain an adequate recep-
tive ﬁeld, but without degrading the resolution from pooling
or strided convolution. However, a side-effect of this method
is that computational cost increases, since the operations are
performed on larger resolution feature maps. The encoder
network is modiﬁed to incorporate a downsampling factor of
8 instead of 32. The decrease of the downsampling is per-
formed by either removing pooling layers or converting stride
2 convolution to stride 1. The pooling or strided convolutions
are then replaced with two dilated convolutions[3] with dila-
tion factor 2 and 4 respectively.

Table 1: Comparison of different encoders and decoding methods in accuracy on cityscapes validation set. The modular
decoupled design in RTSeg enabled such comparison. Coarse indicates whether the network was pre-trained on the coarse
annotation or not.

Decoder Encoder
SkipNet MobileNet No
ShufﬂeNet No
SkipNet
ResNet18
UNet
No
UNet
MobileNet No
ShufﬂeNet No
UNet
Dilation MobileNet No
Dilation
ShufﬂeNet No
SkipNet MobileNet Yes
ShufﬂeNet Yes
SkipNet

Coarse mIoU Road
95.9
61.3
94.8
55.5
95.8
57.9
95.2
61.0
95.1
57.0
95.6
57.8
95.2
53.9
62.4
95.4
94.6
59.3

Sidewalk Building
73.6
68.6
73.2
71.3
69.5
72.3
68.5
73.9
70.5

86.9
83.9
85.8
86.8
83.7
85.9
84.1
86.6
85.5

Sign
57.6
50.5
57.5
60.9
54.3
57.0
57.3
57.4
54.9

Sky
91.2
88.6
91.0
92.8
89.0
91.4
90.3
91.1
90.8

Person Car
89.0
66.4
86.5
60.8
88.6
66.0
68.1
88.8
87.8
61.7
87.8
64.9
86.6
62.9
88.4
65.7
87.5
60.2

Bicycle Truck
63.6
58.8
63.2
65.0
59.9
62.8
60.2
63.3
58.8

45.9
29.6
31.4
41.3
35.5
26.3
23.3
45.3
45.4

2.2. Feature Extraction Architectures

(2) ResNet18[18].

In order to achieve real-time performance multiple network
architectures are integrated in the benchmarking framework.
The framework includes four state of the art real-time net-
work architectures for feature extraction. These are:
(1)
(4)
(3) MobileNet[14].
VGG16[17].
ShufﬂeNet [15]. The reason for using VGG16 is to act as a
baseline method to compare against as it was used in [1]. The
other architectures have been used in real-time systems for
detection and classiﬁcation. ResNet18 incorporates the usage
of residual blocks that directs the network toward learning the
residual representation on identity mapping.

MobileNet network architecture is based on depthwise
separable convolution. It is considered the extreme case of
the inception module, where separate spatial convolution for
each channel is applied denoted as depthwise convolutions.
Then 1x1 convolution with all the channels to merge the out-
put denoted as pointwise convolutions is used. The sepa-
ration in depthwise and pointwise convolution improve the
computational efﬁciency on one hand. On the other hand it
improves the accuracy as the cross channel and spatial corre-
lations mapping are learned separately.

ShufﬂeNet encoder is based on grouped convolution that
is a generalization of depthwise separable convolution. It uses
channel shufﬂing to ensure the connectivity between input
and output channels. This eliminates connectivity restrictions
posed by the grouped convolutions.

3. EXPERIMENTS

In this section experimental setup, detailed ablation study and
results in comparison to the state of the art are reported.

3.1. Experimental Setup

Through all of our experiments, weighted cross entropy loss
from [12] is used, to overcome the class imbalance. Adam op-
timizer [19] learning rate is set to 1e−4. Batch normalization

[20] is incorporated. L2 regularization with weight decay rate
of 5e−4 is utilized to avoid over-ﬁtting. The feature extractor
part of the network is initialized with the pre-trained corre-
sponding encoder trained on Imagenet. A width multiplier
of 1 for MobileNet to include all the feature channels is per-
formed through all the experiments. The number of groups
used in ShufﬂeNet is 3. Based on previous [15] results on
classiﬁcation and detection three groups provided adequate
accuracy.

Results are reported on Cityscapes dataset [9] which con-
tains 5000 images with ﬁne annotation, with 20 classes in-
cluding the ignored class. Another section of the dataset con-
tains coarse annotations with 20,000 labeled images. These
are used in the case of Coarse pre-training that improves the
results of the segmentation. Experiments are conducted on
images with resolution of 512x1024.

Table 2: Comparison of the most promising models in our
benchmarking framework in terms of GFLOPs and frames per
second, this is computed on image resolution 512x1024.

Model
SkipNet-MobileNet
UNet-MobileNet

GFLOPs
13.8
55.9

3.2. Semantic Segmentation Results

Semantic segmentation is evaluated using mean intersection
over union (mIoU), per-class IoU, and per-category IoU.
Table1 shows the results for the ablation study on different
encoders-decoders with mIoU and GFLOPs to demonstrate
the accuracy and computations trade-off. The main insight
gained from our experiments is that, UNet decoding method
provides more accurate segmentation results than Dilation
Frontend. This is mainly due to the transposed convolution
by 8x in the end of the Dilation Frontend, unlike the UNet
stage-wise upsampling method. The SkipNet architecture
provides on par results with UNet decoding method. In some

Table 3: Comparison of some of the models from our benchmarking framework with the state of the art segmentation networks
on cityscapes test set. GFLOPs is computed on image resolution 360x640.

Model
SegNet[21]
ENet[12]
DeepLab[2]
SkipNet-VGG16[1]
SkipNet-ShufﬂeNet
SkipNet-MobileNet

GFLOPs Class IoU Class iIoU Category IoU Category iIoU
286.03
3.83
-
-
2.0
6.2

66.4
64.0
67.7
70.1
62.2
63.0

34.2
24.4
42.6
41.7
32.4
35.2

79.8
80.4
86.4
85.7
80.2
82.0

56.1
58.3
70.4
65.3
58.3
61.5

(a)

(c)

(b)

(d)

Fig. 3: Qualitative Results on CityScapes. (a) Original Image. (b) SkipNet-MobileNet pretrained with Coarse Annotations. (c)
UNet-Resnet18. (d) SkipNet-ShufﬂeNet pretrained with Coarse Annotations.

architectures such as SkipNet-ShufﬂeNet it is less accurate
than UNet counter part by 1.5%.

The UNet method of incrementally upsampling with-in
the network provides the best in terms of accuracy. However,
Table 2 clearly shows that SkipNet architecture is more com-
putationally efﬁcient with 4x reduction in GFLOPs. This is
explained by the fact that transposed convolutions in UNet
are applied in the feature space unlike in SkipNet that are ap-
plied in label space. Table 1 shows that Coarse pre-training
improves the overall mIoU with 1-4%. The underrepresented
classes are the ones that often beneﬁt from pre-training.

Experimental results on the cityscapes test set are shown
in Table 3. Although, DeepLab provides best results in terms
of accuracy, it is not computationally efﬁcient. ENet [12]
is compared to SkipNet-ShufﬂeNet and SkipNet-MobileNet
in terms of accuracy and GFLOPs. SkipNet-ShufﬂeNet out-
performs ENet in terms of GFLOPs, yet it maintains on par
mIoU. However, we have not been able to outperform ENet
in terms of inference speed. Both SkipNet-ShufﬂeNet and
SkipNet-MobileNet outperform SegNet [21] in terms of com-

putational cost and accuracy with reduction up to 143x in
GFLOPs. Figure 3 shows qualitative results for different en-
coders including MobileNet, ShufﬂeNet and ResNet18.
It
shows that MobileNet provides more accurate segmentation
results than the later two. SkipNet-MobileNet is able to cor-
rectly segment the pedestrian and the signs on the right unlike
the others.

4. CONCLUSION

In this paper we present the ﬁrst principled approach for
benchmarking real-time segmentation networks. The decou-
pled design of the framework separates modules for better
quantitative comparison. The ﬁrst module is comprised of
the feature extraction network architecture,
the second is
the meta-architecture that provides the decoding method.
Three different meta-architectures are included in our frame-
work, including Skip architecture, UNet, and Dilation Fron-
tend. Different network architectures for feature extraction
are included, which are ShufﬂeNet, MobileNet, VGG16,

and ResNet-18. Our benchmarking framework provides re-
searchers and practitioners with a mean to evaluate design
choices for their tasks.

5. REFERENCES

[1] Jonathan Long, Evan Shelhamer, and Trevor Darrell,
“Fully convolutional networks for semantic segmenta-
tion,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015, pp. 3431–
3440.

[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokki-
“Deeplab:
nos, Kevin Murphy, and Alan L Yuille,
Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected crfs,”
arXiv preprint arXiv:1606.00915, 2016.

[3] Fisher Yu and Vladlen Koltun,

“Multi-scale context
aggregation by dilated convolutions,” arXiv preprint
arXiv:1511.07122, 2015.

[4] Guosheng Lin, Chunhua Shen, Anton van den Hen-
gel, and Ian Reid, “Exploring context with deep struc-
tured models for semantic segmentation,” arXiv preprint
arXiv:1603.03183, 2016.

[5] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du,
Chang Huang, and Philip HS Torr, “Conditional random
ﬁelds as recurrent neural networks,” in Proceedings of
the IEEE International Conference on Computer Vision,
2015, pp. 1529–1537.

[6] Evan Shelhamer, Kate Rakelly, Judy Hoffman, and
Trevor Darrell, “Clockwork convnets for video semantic
segmentation,” in Computer Vision–ECCV 2016 Work-
shops. Springer, 2016, pp. 852–868.

[7] Mark Everingham, Luc Van Gool, Christopher KI
“The
Williams, John Winn, and Andrew Zisserman,
pascal visual object classes (voc) challenge,” Interna-
tional journal of computer vision, vol. 88, no. 2, pp.
303–338, 2010.

[8] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and
Rob Fergus, “Indoor segmentation and support infer-
ence from rgbd images,” Computer Vision–ECCV 2012,
pp. 746–760, 2012.

[9] Marius Cordts, Mohamed Omran, Sebastian Ramos,
Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele,
“The
cityscapes dataset for semantic urban scene understand-
ing,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 3213–
3223.

[10] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul`o,
and Peter Kontschieder, “The mapillary vistas dataset
for semantic understanding of street scenes,” in Pro-
ceedings of the International Conference on Computer
Vision (ICCV), Venice, Italy, 2017, pp. 22–29.

[11] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jian-
“Icnet for real-time seman-
arXiv

ping Shi, and Jiaya Jia,
tic segmentation on high-resolution images,”
preprint arXiv:1704.08545, 2017.

[12] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and
Eugenio Culurciello, “Enet: A deep neural network ar-
chitecture for real-time semantic segmentation,” arXiv
preprint arXiv:1606.02147, 2016.

[13] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu
Oprea, Victor Villena-Martinez, and Jose Garcia-
“A review on deep learning techniques
Rodriguez,
arXiv preprint
applied to semantic segmentation,”
arXiv:1704.06857, 2017.

[14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam, “Mobilenets: Efﬁcient
convolutional neural networks for mobile vision appli-
cations,” arXiv preprint arXiv:1704.04861, 2017.

[15] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian
Sun, “Shufﬂenet: An extremely efﬁcient convolutional
arXiv preprint
neural network for mobile devices,”
arXiv:1707.01083, 2017.

[16] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-net: Convolutional networks for biomedical image
segmentation,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention.
Springer, 2015, pp. 234–241.

[17] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni-
tion,” arXiv preprint arXiv:1409.1556, 2014.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[19] Diederik Kingma and Jimmy Ba,
method for stochastic optimization,”
arXiv:1412.6980, 2014.

“Adam: A
arXiv preprint

[20] Sergey Ioffe and Christian Szegedy, “Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift,” in International Conference on
Machine Learning, 2015, pp. 448–456.

[21] Vijay Badrinarayanan, Alex Kendall, and Roberto
Cipolla,
“Segnet: A deep convolutional encoder-
decoder architecture for image segmentation,” arXiv
preprint arXiv:1511.00561, 2015.

RTSEG: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY

Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand

mennatul@ualberta.ca, senthil.yogamani@valeo.com
University of Alberta, Valeo Vision Systems, Cairo University

9
1
0
2
 
t
c
O
 
7
1
 
 
]

V
C
.
s
c
[
 
 
4
v
8
5
7
2
0
.
3
0
8
1
:
v
i
X
r
a

ABSTRACT

Semantic segmentation beneﬁts robotics related applications,
especially autonomous driving. Most of the research on se-
mantic segmentation only focuses on increasing the accuracy
of segmentation models with little attention to computation-
ally efﬁcient solutions. The few work conducted in this di-
rection does not provide principled methods to evaluate the
different design choices for segmentation. In this paper, we
address this gap by presenting a real-time semantic segmen-
tation benchmarking framework with a decoupled design for
feature extraction and decoding methods. The framework is
comprised of different network architectures for feature ex-
traction such as VGG16, Resnet18, MobileNet, and Shuf-
ﬂeNet. It is also comprised of multiple meta-architectures for
segmentation that deﬁne the decoding methodology. These
include SkipNet, UNet, and Dilation Frontend. Experimen-
tal results are presented on the Cityscapes dataset for urban
scenes. The modular design allows novel architectures to
emerge, that lead to 143x GFLOPs reduction in comparison to
SegNet. This benchmarking framework is publicly available
at 1.

Index Terms— realtime; semantic segmentation; bench-

marking framework

1. INTRODUCTION

Semantic segmentation has made progress in the recent years
with deep learning. The ﬁrst prominent work in this ﬁeld was
fully convolutional networks(FCNs) [1]. FCN was proposed
as an end-to-end method to learn pixel-wise classiﬁcation,
where transposed convolution was used for upsampling. Skip
architecture was used to reﬁne the segmentation output, that
utilized higher resolution feature maps. That method paved
the road to subsequent advances in the segmentation accu-
racy. Multi-scale approaches [2, 3], structured models [4, 5],
and spatio-temporal architectures [6] introduced different di-
rections for improving accuracy. All of the above approaches
focused on accuracy and robustness of segmentation. Well
known benchmarks and datasets for semantic segmentation
such as Pascal [7], NYU RGBD [8], Cityscapes [9], and Map-

1https://github.com/MSiam/TFSegmentation

Fig. 1: Overview of the different components in the frame-
work with the decoupling of feature extraction module and
decoding method.

illary [10] boosted the competition toward improving accu-
racy.

However, little attention is given to the computational ef-
ﬁciency of these networks. Although, when it comes to appli-
cations such as autonomous driving this would have tremen-
dous impact. There exists few work that tries to address the
segmentation networks efﬁciency such as [11, 12]. The sur-
vey on semantic segmentation [13] presented a comparative
study between different segmentation architectures including
ENet [12]. Yet, there is no principled comparison of differ-
ent networks and meta-architectures. These previous stud-
ies compared different networks as a whole, without com-
paring the effect of different modules. That does not enable
researchers and practitioners to pick the best suited design
choices for the required task.

In this paper we propose the ﬁrst framework toward
benchmarking real-time architectures in segmentation. Our
main contributions are: (1) we provide a modular decoupling
of the segmentation architecture into feature extraction and
decoding method which is termed as meta-architecture as
shown in Figure 1. The separation helps in understanding the
impact of different parts of the network on real-time perfor-
mance.
(2) A detailed ablation study with highlighting the
trade-off between accuracy and computational efﬁciency is

(a)

(b)

Fig. 2: Different Meta Architectures using MobileNet as the feature extraction network. a) SkipNet. b) UNet.

presented. (3) The modular design of our framework allowed
the emergence of two novel segmentation architectures using
MobileNet [14] and ShufﬂeNet [15] with multiple decod-
ing methods. ShufﬂeNet lead to 143x GFLOPs reduction
in comparison to SegNet. Our framework is built on top of
Tensorﬂow and is publicly available. Although our frame-
work delivers less operations we have not been able to deliver
higher inference speed.

2. BENCHMARKING FRAMEWORK

2.1. Meta-Architectures

Three meta-architectures are integrated in our benchmarking
software: (1) SkipNet meta-architecture[1]. (2) U-Net meta-
architecture[16]. (3) Dilation Frontend meta-architecture[3].
The meta-architectures for semantic segmentation identify the
decoding method for in the network upsampling. All of the
network architectures share the same down-sampling factor
of 32. The downsampling is achieved either by utilizing pool-
ing layers, or strides in the convolutional layers. This en-
sures that different meta architectures have a uniﬁed down-
sampling factor to assess the effect of the decoding method
only.

SkipNet architecture denotes a similar architecture to
FCN8s [1]. The main idea of the skip architecture is to ben-
eﬁt from feature maps from higher resolution to improve the
output segmentation. SkipNet applies transposed convolution
on heatmaps in the label space instead of performing it on
feature space. This entails a more computationally efﬁcient
decoding method than others. Feature extraction networks
have the same downsampling factor of 32, so they follow the
8 stride version of skip architecture. Higher resolution feature
maps are followed by 1x1 convolution to map from feature
space to label space that produces heatmaps corresponding
to each class. The ﬁnal heatmap with downsampling factor
of 32 is followed by transposed convolution with stride 2.

Elementwise addition between this upsampled heatmaps and
the higher resolution heatmaps is performed. Finally, the
output heat maps are followed by a transposed convolution
for up-sampling with stride 8. Figure 2(a) shows the SkipNet
architecture utilizing a MobileMet encoder.

U-Net architecture denotes the method of decoding that
up-samples features using transposed convolution corre-
sponding to each downsampling stage. The up-sampled
features are fused with the corresponding features maps from
the encoder with the same resolution. The stage-wise up-
sampling provides higher accuracy than one shot 8x upsam-
pling. The current fusion method used in the framework is
element-wise addition. Concatenation as a fusion method can
provide better accuracy, as it enables the network to learn
the weighted fusion of features. Nonetheless, it increases the
computational cost, as it is directly affected by the number
of channels. The upsampled features are then followed by
1x1 convolution to output the ﬁnal pixel-wise classiﬁcation.
Figure 2(b) shows the UNet architecture using MobileNet as
a feature extraction network.

Dilation Frontend architecture utilizes dilated convolu-
tion instead of downsampling the feature maps. Dilated con-
volution enables the network to maintain an adequate recep-
tive ﬁeld, but without degrading the resolution from pooling
or strided convolution. However, a side-effect of this method
is that computational cost increases, since the operations are
performed on larger resolution feature maps. The encoder
network is modiﬁed to incorporate a downsampling factor of
8 instead of 32. The decrease of the downsampling is per-
formed by either removing pooling layers or converting stride
2 convolution to stride 1. The pooling or strided convolutions
are then replaced with two dilated convolutions[3] with dila-
tion factor 2 and 4 respectively.

Table 1: Comparison of different encoders and decoding methods in accuracy on cityscapes validation set. The modular
decoupled design in RTSeg enabled such comparison. Coarse indicates whether the network was pre-trained on the coarse
annotation or not.

Decoder Encoder
SkipNet MobileNet No
ShufﬂeNet No
SkipNet
ResNet18
UNet
No
UNet
MobileNet No
ShufﬂeNet No
UNet
Dilation MobileNet No
Dilation
ShufﬂeNet No
SkipNet MobileNet Yes
ShufﬂeNet Yes
SkipNet

Coarse mIoU Road
95.9
61.3
94.8
55.5
95.8
57.9
95.2
61.0
95.1
57.0
95.6
57.8
95.2
53.9
62.4
95.4
94.6
59.3

Sidewalk Building
73.6
68.6
73.2
71.3
69.5
72.3
68.5
73.9
70.5

86.9
83.9
85.8
86.8
83.7
85.9
84.1
86.6
85.5

Sign
57.6
50.5
57.5
60.9
54.3
57.0
57.3
57.4
54.9

Sky
91.2
88.6
91.0
92.8
89.0
91.4
90.3
91.1
90.8

Person Car
89.0
66.4
86.5
60.8
88.6
66.0
68.1
88.8
87.8
61.7
87.8
64.9
86.6
62.9
88.4
65.7
87.5
60.2

Bicycle Truck
63.6
58.8
63.2
65.0
59.9
62.8
60.2
63.3
58.8

45.9
29.6
31.4
41.3
35.5
26.3
23.3
45.3
45.4

2.2. Feature Extraction Architectures

(2) ResNet18[18].

In order to achieve real-time performance multiple network
architectures are integrated in the benchmarking framework.
The framework includes four state of the art real-time net-
work architectures for feature extraction. These are:
(1)
(4)
(3) MobileNet[14].
VGG16[17].
ShufﬂeNet [15]. The reason for using VGG16 is to act as a
baseline method to compare against as it was used in [1]. The
other architectures have been used in real-time systems for
detection and classiﬁcation. ResNet18 incorporates the usage
of residual blocks that directs the network toward learning the
residual representation on identity mapping.

MobileNet network architecture is based on depthwise
separable convolution. It is considered the extreme case of
the inception module, where separate spatial convolution for
each channel is applied denoted as depthwise convolutions.
Then 1x1 convolution with all the channels to merge the out-
put denoted as pointwise convolutions is used. The sepa-
ration in depthwise and pointwise convolution improve the
computational efﬁciency on one hand. On the other hand it
improves the accuracy as the cross channel and spatial corre-
lations mapping are learned separately.

ShufﬂeNet encoder is based on grouped convolution that
is a generalization of depthwise separable convolution. It uses
channel shufﬂing to ensure the connectivity between input
and output channels. This eliminates connectivity restrictions
posed by the grouped convolutions.

3. EXPERIMENTS

In this section experimental setup, detailed ablation study and
results in comparison to the state of the art are reported.

3.1. Experimental Setup

Through all of our experiments, weighted cross entropy loss
from [12] is used, to overcome the class imbalance. Adam op-
timizer [19] learning rate is set to 1e−4. Batch normalization

[20] is incorporated. L2 regularization with weight decay rate
of 5e−4 is utilized to avoid over-ﬁtting. The feature extractor
part of the network is initialized with the pre-trained corre-
sponding encoder trained on Imagenet. A width multiplier
of 1 for MobileNet to include all the feature channels is per-
formed through all the experiments. The number of groups
used in ShufﬂeNet is 3. Based on previous [15] results on
classiﬁcation and detection three groups provided adequate
accuracy.

Results are reported on Cityscapes dataset [9] which con-
tains 5000 images with ﬁne annotation, with 20 classes in-
cluding the ignored class. Another section of the dataset con-
tains coarse annotations with 20,000 labeled images. These
are used in the case of Coarse pre-training that improves the
results of the segmentation. Experiments are conducted on
images with resolution of 512x1024.

Table 2: Comparison of the most promising models in our
benchmarking framework in terms of GFLOPs and frames per
second, this is computed on image resolution 512x1024.

Model
SkipNet-MobileNet
UNet-MobileNet

GFLOPs
13.8
55.9

3.2. Semantic Segmentation Results

Semantic segmentation is evaluated using mean intersection
over union (mIoU), per-class IoU, and per-category IoU.
Table1 shows the results for the ablation study on different
encoders-decoders with mIoU and GFLOPs to demonstrate
the accuracy and computations trade-off. The main insight
gained from our experiments is that, UNet decoding method
provides more accurate segmentation results than Dilation
Frontend. This is mainly due to the transposed convolution
by 8x in the end of the Dilation Frontend, unlike the UNet
stage-wise upsampling method. The SkipNet architecture
provides on par results with UNet decoding method. In some

Table 3: Comparison of some of the models from our benchmarking framework with the state of the art segmentation networks
on cityscapes test set. GFLOPs is computed on image resolution 360x640.

Model
SegNet[21]
ENet[12]
DeepLab[2]
SkipNet-VGG16[1]
SkipNet-ShufﬂeNet
SkipNet-MobileNet

GFLOPs Class IoU Class iIoU Category IoU Category iIoU
286.03
3.83
-
-
2.0
6.2

66.4
64.0
67.7
70.1
62.2
63.0

56.1
58.3
70.4
65.3
58.3
61.5

34.2
24.4
42.6
41.7
32.4
35.2

79.8
80.4
86.4
85.7
80.2
82.0

(a)

(c)

(b)

(d)

Fig. 3: Qualitative Results on CityScapes. (a) Original Image. (b) SkipNet-MobileNet pretrained with Coarse Annotations. (c)
UNet-Resnet18. (d) SkipNet-ShufﬂeNet pretrained with Coarse Annotations.

architectures such as SkipNet-ShufﬂeNet it is less accurate
than UNet counter part by 1.5%.

The UNet method of incrementally upsampling with-in
the network provides the best in terms of accuracy. However,
Table 2 clearly shows that SkipNet architecture is more com-
putationally efﬁcient with 4x reduction in GFLOPs. This is
explained by the fact that transposed convolutions in UNet
are applied in the feature space unlike in SkipNet that are ap-
plied in label space. Table 1 shows that Coarse pre-training
improves the overall mIoU with 1-4%. The underrepresented
classes are the ones that often beneﬁt from pre-training.

Experimental results on the cityscapes test set are shown
in Table 3. Although, DeepLab provides best results in terms
of accuracy, it is not computationally efﬁcient. ENet [12]
is compared to SkipNet-ShufﬂeNet and SkipNet-MobileNet
in terms of accuracy and GFLOPs. SkipNet-ShufﬂeNet out-
performs ENet in terms of GFLOPs, yet it maintains on par
mIoU. However, we have not been able to outperform ENet
in terms of inference speed. Both SkipNet-ShufﬂeNet and
SkipNet-MobileNet outperform SegNet [21] in terms of com-

putational cost and accuracy with reduction up to 143x in
GFLOPs. Figure 3 shows qualitative results for different en-
coders including MobileNet, ShufﬂeNet and ResNet18.
It
shows that MobileNet provides more accurate segmentation
results than the later two. SkipNet-MobileNet is able to cor-
rectly segment the pedestrian and the signs on the right unlike
the others.

4. CONCLUSION

In this paper we present the ﬁrst principled approach for
benchmarking real-time segmentation networks. The decou-
pled design of the framework separates modules for better
quantitative comparison. The ﬁrst module is comprised of
the feature extraction network architecture,
the second is
the meta-architecture that provides the decoding method.
Three different meta-architectures are included in our frame-
work, including Skip architecture, UNet, and Dilation Fron-
tend. Different network architectures for feature extraction
are included, which are ShufﬂeNet, MobileNet, VGG16,

and ResNet-18. Our benchmarking framework provides re-
searchers and practitioners with a mean to evaluate design
choices for their tasks.

5. REFERENCES

[1] Jonathan Long, Evan Shelhamer, and Trevor Darrell,
“Fully convolutional networks for semantic segmenta-
tion,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015, pp. 3431–
3440.

[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokki-
“Deeplab:
nos, Kevin Murphy, and Alan L Yuille,
Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected crfs,”
arXiv preprint arXiv:1606.00915, 2016.

[3] Fisher Yu and Vladlen Koltun,

“Multi-scale context
aggregation by dilated convolutions,” arXiv preprint
arXiv:1511.07122, 2015.

[4] Guosheng Lin, Chunhua Shen, Anton van den Hen-
gel, and Ian Reid, “Exploring context with deep struc-
tured models for semantic segmentation,” arXiv preprint
arXiv:1603.03183, 2016.

[5] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du,
Chang Huang, and Philip HS Torr, “Conditional random
ﬁelds as recurrent neural networks,” in Proceedings of
the IEEE International Conference on Computer Vision,
2015, pp. 1529–1537.

[6] Evan Shelhamer, Kate Rakelly, Judy Hoffman, and
Trevor Darrell, “Clockwork convnets for video semantic
segmentation,” in Computer Vision–ECCV 2016 Work-
shops. Springer, 2016, pp. 852–868.

[7] Mark Everingham, Luc Van Gool, Christopher KI
“The
Williams, John Winn, and Andrew Zisserman,
pascal visual object classes (voc) challenge,” Interna-
tional journal of computer vision, vol. 88, no. 2, pp.
303–338, 2010.

[8] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and
Rob Fergus, “Indoor segmentation and support infer-
ence from rgbd images,” Computer Vision–ECCV 2012,
pp. 746–760, 2012.

[9] Marius Cordts, Mohamed Omran, Sebastian Ramos,
Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele,
“The
cityscapes dataset for semantic urban scene understand-
ing,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 3213–
3223.

[10] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul`o,
and Peter Kontschieder, “The mapillary vistas dataset
for semantic understanding of street scenes,” in Pro-
ceedings of the International Conference on Computer
Vision (ICCV), Venice, Italy, 2017, pp. 22–29.

[11] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jian-
“Icnet for real-time seman-
arXiv

ping Shi, and Jiaya Jia,
tic segmentation on high-resolution images,”
preprint arXiv:1704.08545, 2017.

[12] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and
Eugenio Culurciello, “Enet: A deep neural network ar-
chitecture for real-time semantic segmentation,” arXiv
preprint arXiv:1606.02147, 2016.

[13] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu
Oprea, Victor Villena-Martinez, and Jose Garcia-
“A review on deep learning techniques
Rodriguez,
arXiv preprint
applied to semantic segmentation,”
arXiv:1704.06857, 2017.

[14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam, “Mobilenets: Efﬁcient
convolutional neural networks for mobile vision appli-
cations,” arXiv preprint arXiv:1704.04861, 2017.

[15] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian
Sun, “Shufﬂenet: An extremely efﬁcient convolutional
arXiv preprint
neural network for mobile devices,”
arXiv:1707.01083, 2017.

[16] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-net: Convolutional networks for biomedical image
segmentation,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention.
Springer, 2015, pp. 234–241.

[17] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni-
tion,” arXiv preprint arXiv:1409.1556, 2014.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[19] Diederik Kingma and Jimmy Ba,
method for stochastic optimization,”
arXiv:1412.6980, 2014.

“Adam: A
arXiv preprint

[20] Sergey Ioffe and Christian Szegedy, “Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift,” in International Conference on
Machine Learning, 2015, pp. 448–456.

[21] Vijay Badrinarayanan, Alex Kendall, and Roberto
Cipolla,
“Segnet: A deep convolutional encoder-
decoder architecture for image segmentation,” arXiv
preprint arXiv:1511.00561, 2015.

RTSEG: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY

Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand

mennatul@ualberta.ca, senthil.yogamani@valeo.com
University of Alberta, Valeo Vision Systems, Cairo University

9
1
0
2
 
t
c
O
 
7
1
 
 
]

V
C
.
s
c
[
 
 
4
v
8
5
7
2
0
.
3
0
8
1
:
v
i
X
r
a

ABSTRACT

Semantic segmentation beneﬁts robotics related applications,
especially autonomous driving. Most of the research on se-
mantic segmentation only focuses on increasing the accuracy
of segmentation models with little attention to computation-
ally efﬁcient solutions. The few work conducted in this di-
rection does not provide principled methods to evaluate the
different design choices for segmentation. In this paper, we
address this gap by presenting a real-time semantic segmen-
tation benchmarking framework with a decoupled design for
feature extraction and decoding methods. The framework is
comprised of different network architectures for feature ex-
traction such as VGG16, Resnet18, MobileNet, and Shuf-
ﬂeNet. It is also comprised of multiple meta-architectures for
segmentation that deﬁne the decoding methodology. These
include SkipNet, UNet, and Dilation Frontend. Experimen-
tal results are presented on the Cityscapes dataset for urban
scenes. The modular design allows novel architectures to
emerge, that lead to 143x GFLOPs reduction in comparison to
SegNet. This benchmarking framework is publicly available
at 1.

Index Terms— realtime; semantic segmentation; bench-

marking framework

1. INTRODUCTION

Semantic segmentation has made progress in the recent years
with deep learning. The ﬁrst prominent work in this ﬁeld was
fully convolutional networks(FCNs) [1]. FCN was proposed
as an end-to-end method to learn pixel-wise classiﬁcation,
where transposed convolution was used for upsampling. Skip
architecture was used to reﬁne the segmentation output, that
utilized higher resolution feature maps. That method paved
the road to subsequent advances in the segmentation accu-
racy. Multi-scale approaches [2, 3], structured models [4, 5],
and spatio-temporal architectures [6] introduced different di-
rections for improving accuracy. All of the above approaches
focused on accuracy and robustness of segmentation. Well
known benchmarks and datasets for semantic segmentation
such as Pascal [7], NYU RGBD [8], Cityscapes [9], and Map-

1https://github.com/MSiam/TFSegmentation

Fig. 1: Overview of the different components in the frame-
work with the decoupling of feature extraction module and
decoding method.

illary [10] boosted the competition toward improving accu-
racy.

However, little attention is given to the computational ef-
ﬁciency of these networks. Although, when it comes to appli-
cations such as autonomous driving this would have tremen-
dous impact. There exists few work that tries to address the
segmentation networks efﬁciency such as [11, 12]. The sur-
vey on semantic segmentation [13] presented a comparative
study between different segmentation architectures including
ENet [12]. Yet, there is no principled comparison of differ-
ent networks and meta-architectures. These previous stud-
ies compared different networks as a whole, without com-
paring the effect of different modules. That does not enable
researchers and practitioners to pick the best suited design
choices for the required task.

In this paper we propose the ﬁrst framework toward
benchmarking real-time architectures in segmentation. Our
main contributions are: (1) we provide a modular decoupling
of the segmentation architecture into feature extraction and
decoding method which is termed as meta-architecture as
shown in Figure 1. The separation helps in understanding the
impact of different parts of the network on real-time perfor-
mance.
(2) A detailed ablation study with highlighting the
trade-off between accuracy and computational efﬁciency is

(a)

(b)

Fig. 2: Different Meta Architectures using MobileNet as the feature extraction network. a) SkipNet. b) UNet.

presented. (3) The modular design of our framework allowed
the emergence of two novel segmentation architectures using
MobileNet [14] and ShufﬂeNet [15] with multiple decod-
ing methods. ShufﬂeNet lead to 143x GFLOPs reduction
in comparison to SegNet. Our framework is built on top of
Tensorﬂow and is publicly available. Although our frame-
work delivers less operations we have not been able to deliver
higher inference speed.

2. BENCHMARKING FRAMEWORK

2.1. Meta-Architectures

Three meta-architectures are integrated in our benchmarking
software: (1) SkipNet meta-architecture[1]. (2) U-Net meta-
architecture[16]. (3) Dilation Frontend meta-architecture[3].
The meta-architectures for semantic segmentation identify the
decoding method for in the network upsampling. All of the
network architectures share the same down-sampling factor
of 32. The downsampling is achieved either by utilizing pool-
ing layers, or strides in the convolutional layers. This en-
sures that different meta architectures have a uniﬁed down-
sampling factor to assess the effect of the decoding method
only.

SkipNet architecture denotes a similar architecture to
FCN8s [1]. The main idea of the skip architecture is to ben-
eﬁt from feature maps from higher resolution to improve the
output segmentation. SkipNet applies transposed convolution
on heatmaps in the label space instead of performing it on
feature space. This entails a more computationally efﬁcient
decoding method than others. Feature extraction networks
have the same downsampling factor of 32, so they follow the
8 stride version of skip architecture. Higher resolution feature
maps are followed by 1x1 convolution to map from feature
space to label space that produces heatmaps corresponding
to each class. The ﬁnal heatmap with downsampling factor
of 32 is followed by transposed convolution with stride 2.

Elementwise addition between this upsampled heatmaps and
the higher resolution heatmaps is performed. Finally, the
output heat maps are followed by a transposed convolution
for up-sampling with stride 8. Figure 2(a) shows the SkipNet
architecture utilizing a MobileMet encoder.

U-Net architecture denotes the method of decoding that
up-samples features using transposed convolution corre-
sponding to each downsampling stage. The up-sampled
features are fused with the corresponding features maps from
the encoder with the same resolution. The stage-wise up-
sampling provides higher accuracy than one shot 8x upsam-
pling. The current fusion method used in the framework is
element-wise addition. Concatenation as a fusion method can
provide better accuracy, as it enables the network to learn
the weighted fusion of features. Nonetheless, it increases the
computational cost, as it is directly affected by the number
of channels. The upsampled features are then followed by
1x1 convolution to output the ﬁnal pixel-wise classiﬁcation.
Figure 2(b) shows the UNet architecture using MobileNet as
a feature extraction network.

Dilation Frontend architecture utilizes dilated convolu-
tion instead of downsampling the feature maps. Dilated con-
volution enables the network to maintain an adequate recep-
tive ﬁeld, but without degrading the resolution from pooling
or strided convolution. However, a side-effect of this method
is that computational cost increases, since the operations are
performed on larger resolution feature maps. The encoder
network is modiﬁed to incorporate a downsampling factor of
8 instead of 32. The decrease of the downsampling is per-
formed by either removing pooling layers or converting stride
2 convolution to stride 1. The pooling or strided convolutions
are then replaced with two dilated convolutions[3] with dila-
tion factor 2 and 4 respectively.

Table 1: Comparison of different encoders and decoding methods in accuracy on cityscapes validation set. The modular
decoupled design in RTSeg enabled such comparison. Coarse indicates whether the network was pre-trained on the coarse
annotation or not.

Decoder Encoder
SkipNet MobileNet No
ShufﬂeNet No
SkipNet
ResNet18
UNet
No
UNet
MobileNet No
ShufﬂeNet No
UNet
Dilation MobileNet No
Dilation
ShufﬂeNet No
SkipNet MobileNet Yes
ShufﬂeNet Yes
SkipNet

Coarse mIoU Road
95.9
61.3
94.8
55.5
95.8
57.9
95.2
61.0
95.1
57.0
95.6
57.8
95.2
53.9
62.4
95.4
94.6
59.3

Sidewalk Building
73.6
68.6
73.2
71.3
69.5
72.3
68.5
73.9
70.5

86.9
83.9
85.8
86.8
83.7
85.9
84.1
86.6
85.5

Sign
57.6
50.5
57.5
60.9
54.3
57.0
57.3
57.4
54.9

Sky
91.2
88.6
91.0
92.8
89.0
91.4
90.3
91.1
90.8

Person Car
89.0
66.4
86.5
60.8
88.6
66.0
68.1
88.8
87.8
61.7
87.8
64.9
86.6
62.9
88.4
65.7
87.5
60.2

Bicycle Truck
63.6
58.8
63.2
65.0
59.9
62.8
60.2
63.3
58.8

45.9
29.6
31.4
41.3
35.5
26.3
23.3
45.3
45.4

2.2. Feature Extraction Architectures

(2) ResNet18[18].

In order to achieve real-time performance multiple network
architectures are integrated in the benchmarking framework.
The framework includes four state of the art real-time net-
work architectures for feature extraction. These are:
(1)
(4)
(3) MobileNet[14].
VGG16[17].
ShufﬂeNet [15]. The reason for using VGG16 is to act as a
baseline method to compare against as it was used in [1]. The
other architectures have been used in real-time systems for
detection and classiﬁcation. ResNet18 incorporates the usage
of residual blocks that directs the network toward learning the
residual representation on identity mapping.

MobileNet network architecture is based on depthwise
separable convolution. It is considered the extreme case of
the inception module, where separate spatial convolution for
each channel is applied denoted as depthwise convolutions.
Then 1x1 convolution with all the channels to merge the out-
put denoted as pointwise convolutions is used. The sepa-
ration in depthwise and pointwise convolution improve the
computational efﬁciency on one hand. On the other hand it
improves the accuracy as the cross channel and spatial corre-
lations mapping are learned separately.

ShufﬂeNet encoder is based on grouped convolution that
is a generalization of depthwise separable convolution. It uses
channel shufﬂing to ensure the connectivity between input
and output channels. This eliminates connectivity restrictions
posed by the grouped convolutions.

3. EXPERIMENTS

In this section experimental setup, detailed ablation study and
results in comparison to the state of the art are reported.

3.1. Experimental Setup

Through all of our experiments, weighted cross entropy loss
from [12] is used, to overcome the class imbalance. Adam op-
timizer [19] learning rate is set to 1e−4. Batch normalization

[20] is incorporated. L2 regularization with weight decay rate
of 5e−4 is utilized to avoid over-ﬁtting. The feature extractor
part of the network is initialized with the pre-trained corre-
sponding encoder trained on Imagenet. A width multiplier
of 1 for MobileNet to include all the feature channels is per-
formed through all the experiments. The number of groups
used in ShufﬂeNet is 3. Based on previous [15] results on
classiﬁcation and detection three groups provided adequate
accuracy.

Results are reported on Cityscapes dataset [9] which con-
tains 5000 images with ﬁne annotation, with 20 classes in-
cluding the ignored class. Another section of the dataset con-
tains coarse annotations with 20,000 labeled images. These
are used in the case of Coarse pre-training that improves the
results of the segmentation. Experiments are conducted on
images with resolution of 512x1024.

Table 2: Comparison of the most promising models in our
benchmarking framework in terms of GFLOPs and frames per
second, this is computed on image resolution 512x1024.

Model
SkipNet-MobileNet
UNet-MobileNet

GFLOPs
13.8
55.9

3.2. Semantic Segmentation Results

Semantic segmentation is evaluated using mean intersection
over union (mIoU), per-class IoU, and per-category IoU.
Table1 shows the results for the ablation study on different
encoders-decoders with mIoU and GFLOPs to demonstrate
the accuracy and computations trade-off. The main insight
gained from our experiments is that, UNet decoding method
provides more accurate segmentation results than Dilation
Frontend. This is mainly due to the transposed convolution
by 8x in the end of the Dilation Frontend, unlike the UNet
stage-wise upsampling method. The SkipNet architecture
provides on par results with UNet decoding method. In some

Table 3: Comparison of some of the models from our benchmarking framework with the state of the art segmentation networks
on cityscapes test set. GFLOPs is computed on image resolution 360x640.

Model
SegNet[21]
ENet[12]
DeepLab[2]
SkipNet-VGG16[1]
SkipNet-ShufﬂeNet
SkipNet-MobileNet

GFLOPs Class IoU Class iIoU Category IoU Category iIoU
286.03
3.83
-
-
2.0
6.2

66.4
64.0
67.7
70.1
62.2
63.0

56.1
58.3
70.4
65.3
58.3
61.5

34.2
24.4
42.6
41.7
32.4
35.2

79.8
80.4
86.4
85.7
80.2
82.0

(a)

(c)

(b)

(d)

Fig. 3: Qualitative Results on CityScapes. (a) Original Image. (b) SkipNet-MobileNet pretrained with Coarse Annotations. (c)
UNet-Resnet18. (d) SkipNet-ShufﬂeNet pretrained with Coarse Annotations.

architectures such as SkipNet-ShufﬂeNet it is less accurate
than UNet counter part by 1.5%.

The UNet method of incrementally upsampling with-in
the network provides the best in terms of accuracy. However,
Table 2 clearly shows that SkipNet architecture is more com-
putationally efﬁcient with 4x reduction in GFLOPs. This is
explained by the fact that transposed convolutions in UNet
are applied in the feature space unlike in SkipNet that are ap-
plied in label space. Table 1 shows that Coarse pre-training
improves the overall mIoU with 1-4%. The underrepresented
classes are the ones that often beneﬁt from pre-training.

Experimental results on the cityscapes test set are shown
in Table 3. Although, DeepLab provides best results in terms
of accuracy, it is not computationally efﬁcient. ENet [12]
is compared to SkipNet-ShufﬂeNet and SkipNet-MobileNet
in terms of accuracy and GFLOPs. SkipNet-ShufﬂeNet out-
performs ENet in terms of GFLOPs, yet it maintains on par
mIoU. However, we have not been able to outperform ENet
in terms of inference speed. Both SkipNet-ShufﬂeNet and
SkipNet-MobileNet outperform SegNet [21] in terms of com-

putational cost and accuracy with reduction up to 143x in
GFLOPs. Figure 3 shows qualitative results for different en-
coders including MobileNet, ShufﬂeNet and ResNet18.
It
shows that MobileNet provides more accurate segmentation
results than the later two. SkipNet-MobileNet is able to cor-
rectly segment the pedestrian and the signs on the right unlike
the others.

4. CONCLUSION

In this paper we present the ﬁrst principled approach for
benchmarking real-time segmentation networks. The decou-
pled design of the framework separates modules for better
quantitative comparison. The ﬁrst module is comprised of
the feature extraction network architecture,
the second is
the meta-architecture that provides the decoding method.
Three different meta-architectures are included in our frame-
work, including Skip architecture, UNet, and Dilation Fron-
tend. Different network architectures for feature extraction
are included, which are ShufﬂeNet, MobileNet, VGG16,

and ResNet-18. Our benchmarking framework provides re-
searchers and practitioners with a mean to evaluate design
choices for their tasks.

5. REFERENCES

[1] Jonathan Long, Evan Shelhamer, and Trevor Darrell,
“Fully convolutional networks for semantic segmenta-
tion,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015, pp. 3431–
3440.

[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokki-
“Deeplab:
nos, Kevin Murphy, and Alan L Yuille,
Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected crfs,”
arXiv preprint arXiv:1606.00915, 2016.

[3] Fisher Yu and Vladlen Koltun,

“Multi-scale context
aggregation by dilated convolutions,” arXiv preprint
arXiv:1511.07122, 2015.

[4] Guosheng Lin, Chunhua Shen, Anton van den Hen-
gel, and Ian Reid, “Exploring context with deep struc-
tured models for semantic segmentation,” arXiv preprint
arXiv:1603.03183, 2016.

[5] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du,
Chang Huang, and Philip HS Torr, “Conditional random
ﬁelds as recurrent neural networks,” in Proceedings of
the IEEE International Conference on Computer Vision,
2015, pp. 1529–1537.

[6] Evan Shelhamer, Kate Rakelly, Judy Hoffman, and
Trevor Darrell, “Clockwork convnets for video semantic
segmentation,” in Computer Vision–ECCV 2016 Work-
shops. Springer, 2016, pp. 852–868.

[7] Mark Everingham, Luc Van Gool, Christopher KI
“The
Williams, John Winn, and Andrew Zisserman,
pascal visual object classes (voc) challenge,” Interna-
tional journal of computer vision, vol. 88, no. 2, pp.
303–338, 2010.

[8] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and
Rob Fergus, “Indoor segmentation and support infer-
ence from rgbd images,” Computer Vision–ECCV 2012,
pp. 746–760, 2012.

[9] Marius Cordts, Mohamed Omran, Sebastian Ramos,
Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele,
“The
cityscapes dataset for semantic urban scene understand-
ing,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 3213–
3223.

[10] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul`o,
and Peter Kontschieder, “The mapillary vistas dataset
for semantic understanding of street scenes,” in Pro-
ceedings of the International Conference on Computer
Vision (ICCV), Venice, Italy, 2017, pp. 22–29.

[11] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jian-
“Icnet for real-time seman-
arXiv

ping Shi, and Jiaya Jia,
tic segmentation on high-resolution images,”
preprint arXiv:1704.08545, 2017.

[12] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and
Eugenio Culurciello, “Enet: A deep neural network ar-
chitecture for real-time semantic segmentation,” arXiv
preprint arXiv:1606.02147, 2016.

[13] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu
Oprea, Victor Villena-Martinez, and Jose Garcia-
“A review on deep learning techniques
Rodriguez,
arXiv preprint
applied to semantic segmentation,”
arXiv:1704.06857, 2017.

[14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam, “Mobilenets: Efﬁcient
convolutional neural networks for mobile vision appli-
cations,” arXiv preprint arXiv:1704.04861, 2017.

[15] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian
Sun, “Shufﬂenet: An extremely efﬁcient convolutional
arXiv preprint
neural network for mobile devices,”
arXiv:1707.01083, 2017.

[16] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-net: Convolutional networks for biomedical image
segmentation,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention.
Springer, 2015, pp. 234–241.

[17] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni-
tion,” arXiv preprint arXiv:1409.1556, 2014.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[19] Diederik Kingma and Jimmy Ba,
method for stochastic optimization,”
arXiv:1412.6980, 2014.

“Adam: A
arXiv preprint

[20] Sergey Ioffe and Christian Szegedy, “Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift,” in International Conference on
Machine Learning, 2015, pp. 448–456.

[21] Vijay Badrinarayanan, Alex Kendall, and Roberto
Cipolla,
“Segnet: A deep convolutional encoder-
decoder architecture for image segmentation,” arXiv
preprint arXiv:1511.00561, 2015.

RTSEG: REAL-TIME SEMANTIC SEGMENTATION COMPARATIVE STUDY

Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand

mennatul@ualberta.ca, senthil.yogamani@valeo.com
University of Alberta, Valeo Vision Systems, Cairo University

9
1
0
2
 
t
c
O
 
7
1
 
 
]

V
C
.
s
c
[
 
 
4
v
8
5
7
2
0
.
3
0
8
1
:
v
i
X
r
a

ABSTRACT

Semantic segmentation beneﬁts robotics related applications,
especially autonomous driving. Most of the research on se-
mantic segmentation only focuses on increasing the accuracy
of segmentation models with little attention to computation-
ally efﬁcient solutions. The few work conducted in this di-
rection does not provide principled methods to evaluate the
different design choices for segmentation. In this paper, we
address this gap by presenting a real-time semantic segmen-
tation benchmarking framework with a decoupled design for
feature extraction and decoding methods. The framework is
comprised of different network architectures for feature ex-
traction such as VGG16, Resnet18, MobileNet, and Shuf-
ﬂeNet. It is also comprised of multiple meta-architectures for
segmentation that deﬁne the decoding methodology. These
include SkipNet, UNet, and Dilation Frontend. Experimen-
tal results are presented on the Cityscapes dataset for urban
scenes. The modular design allows novel architectures to
emerge, that lead to 143x GFLOPs reduction in comparison to
SegNet. This benchmarking framework is publicly available
at 1.

Index Terms— realtime; semantic segmentation; bench-

marking framework

1. INTRODUCTION

Semantic segmentation has made progress in the recent years
with deep learning. The ﬁrst prominent work in this ﬁeld was
fully convolutional networks(FCNs) [1]. FCN was proposed
as an end-to-end method to learn pixel-wise classiﬁcation,
where transposed convolution was used for upsampling. Skip
architecture was used to reﬁne the segmentation output, that
utilized higher resolution feature maps. That method paved
the road to subsequent advances in the segmentation accu-
racy. Multi-scale approaches [2, 3], structured models [4, 5],
and spatio-temporal architectures [6] introduced different di-
rections for improving accuracy. All of the above approaches
focused on accuracy and robustness of segmentation. Well
known benchmarks and datasets for semantic segmentation
such as Pascal [7], NYU RGBD [8], Cityscapes [9], and Map-

1https://github.com/MSiam/TFSegmentation

Fig. 1: Overview of the different components in the frame-
work with the decoupling of feature extraction module and
decoding method.

illary [10] boosted the competition toward improving accu-
racy.

However, little attention is given to the computational ef-
ﬁciency of these networks. Although, when it comes to appli-
cations such as autonomous driving this would have tremen-
dous impact. There exists few work that tries to address the
segmentation networks efﬁciency such as [11, 12]. The sur-
vey on semantic segmentation [13] presented a comparative
study between different segmentation architectures including
ENet [12]. Yet, there is no principled comparison of differ-
ent networks and meta-architectures. These previous stud-
ies compared different networks as a whole, without com-
paring the effect of different modules. That does not enable
researchers and practitioners to pick the best suited design
choices for the required task.

In this paper we propose the ﬁrst framework toward
benchmarking real-time architectures in segmentation. Our
main contributions are: (1) we provide a modular decoupling
of the segmentation architecture into feature extraction and
decoding method which is termed as meta-architecture as
shown in Figure 1. The separation helps in understanding the
impact of different parts of the network on real-time perfor-
mance.
(2) A detailed ablation study with highlighting the
trade-off between accuracy and computational efﬁciency is

(a)

(b)

Fig. 2: Different Meta Architectures using MobileNet as the feature extraction network. a) SkipNet. b) UNet.

presented. (3) The modular design of our framework allowed
the emergence of two novel segmentation architectures using
MobileNet [14] and ShufﬂeNet [15] with multiple decod-
ing methods. ShufﬂeNet lead to 143x GFLOPs reduction
in comparison to SegNet. Our framework is built on top of
Tensorﬂow and is publicly available. Although our frame-
work delivers less operations we have not been able to deliver
higher inference speed.

2. BENCHMARKING FRAMEWORK

2.1. Meta-Architectures

Three meta-architectures are integrated in our benchmarking
software: (1) SkipNet meta-architecture[1]. (2) U-Net meta-
architecture[16]. (3) Dilation Frontend meta-architecture[3].
The meta-architectures for semantic segmentation identify the
decoding method for in the network upsampling. All of the
network architectures share the same down-sampling factor
of 32. The downsampling is achieved either by utilizing pool-
ing layers, or strides in the convolutional layers. This en-
sures that different meta architectures have a uniﬁed down-
sampling factor to assess the effect of the decoding method
only.

SkipNet architecture denotes a similar architecture to
FCN8s [1]. The main idea of the skip architecture is to ben-
eﬁt from feature maps from higher resolution to improve the
output segmentation. SkipNet applies transposed convolution
on heatmaps in the label space instead of performing it on
feature space. This entails a more computationally efﬁcient
decoding method than others. Feature extraction networks
have the same downsampling factor of 32, so they follow the
8 stride version of skip architecture. Higher resolution feature
maps are followed by 1x1 convolution to map from feature
space to label space that produces heatmaps corresponding
to each class. The ﬁnal heatmap with downsampling factor
of 32 is followed by transposed convolution with stride 2.

Elementwise addition between this upsampled heatmaps and
the higher resolution heatmaps is performed. Finally, the
output heat maps are followed by a transposed convolution
for up-sampling with stride 8. Figure 2(a) shows the SkipNet
architecture utilizing a MobileMet encoder.

U-Net architecture denotes the method of decoding that
up-samples features using transposed convolution corre-
sponding to each downsampling stage. The up-sampled
features are fused with the corresponding features maps from
the encoder with the same resolution. The stage-wise up-
sampling provides higher accuracy than one shot 8x upsam-
pling. The current fusion method used in the framework is
element-wise addition. Concatenation as a fusion method can
provide better accuracy, as it enables the network to learn
the weighted fusion of features. Nonetheless, it increases the
computational cost, as it is directly affected by the number
of channels. The upsampled features are then followed by
1x1 convolution to output the ﬁnal pixel-wise classiﬁcation.
Figure 2(b) shows the UNet architecture using MobileNet as
a feature extraction network.

Dilation Frontend architecture utilizes dilated convolu-
tion instead of downsampling the feature maps. Dilated con-
volution enables the network to maintain an adequate recep-
tive ﬁeld, but without degrading the resolution from pooling
or strided convolution. However, a side-effect of this method
is that computational cost increases, since the operations are
performed on larger resolution feature maps. The encoder
network is modiﬁed to incorporate a downsampling factor of
8 instead of 32. The decrease of the downsampling is per-
formed by either removing pooling layers or converting stride
2 convolution to stride 1. The pooling or strided convolutions
are then replaced with two dilated convolutions[3] with dila-
tion factor 2 and 4 respectively.

Table 1: Comparison of different encoders and decoding methods in accuracy on cityscapes validation set. The modular
decoupled design in RTSeg enabled such comparison. Coarse indicates whether the network was pre-trained on the coarse
annotation or not.

Decoder Encoder
SkipNet MobileNet No
ShufﬂeNet No
SkipNet
ResNet18
UNet
No
UNet
MobileNet No
ShufﬂeNet No
UNet
Dilation MobileNet No
Dilation
ShufﬂeNet No
SkipNet MobileNet Yes
ShufﬂeNet Yes
SkipNet

Coarse mIoU Road
95.9
61.3
94.8
55.5
95.8
57.9
95.2
61.0
95.1
57.0
95.6
57.8
95.2
53.9
62.4
95.4
94.6
59.3

Sidewalk Building
73.6
68.6
73.2
71.3
69.5
72.3
68.5
73.9
70.5

86.9
83.9
85.8
86.8
83.7
85.9
84.1
86.6
85.5

Sign
57.6
50.5
57.5
60.9
54.3
57.0
57.3
57.4
54.9

Sky
91.2
88.6
91.0
92.8
89.0
91.4
90.3
91.1
90.8

Person Car
89.0
66.4
86.5
60.8
88.6
66.0
68.1
88.8
87.8
61.7
87.8
64.9
86.6
62.9
88.4
65.7
87.5
60.2

Bicycle Truck
63.6
58.8
63.2
65.0
59.9
62.8
60.2
63.3
58.8

45.9
29.6
31.4
41.3
35.5
26.3
23.3
45.3
45.4

2.2. Feature Extraction Architectures

(2) ResNet18[18].

In order to achieve real-time performance multiple network
architectures are integrated in the benchmarking framework.
The framework includes four state of the art real-time net-
work architectures for feature extraction. These are:
(1)
(4)
(3) MobileNet[14].
VGG16[17].
ShufﬂeNet [15]. The reason for using VGG16 is to act as a
baseline method to compare against as it was used in [1]. The
other architectures have been used in real-time systems for
detection and classiﬁcation. ResNet18 incorporates the usage
of residual blocks that directs the network toward learning the
residual representation on identity mapping.

MobileNet network architecture is based on depthwise
separable convolution. It is considered the extreme case of
the inception module, where separate spatial convolution for
each channel is applied denoted as depthwise convolutions.
Then 1x1 convolution with all the channels to merge the out-
put denoted as pointwise convolutions is used. The sepa-
ration in depthwise and pointwise convolution improve the
computational efﬁciency on one hand. On the other hand it
improves the accuracy as the cross channel and spatial corre-
lations mapping are learned separately.

ShufﬂeNet encoder is based on grouped convolution that
is a generalization of depthwise separable convolution. It uses
channel shufﬂing to ensure the connectivity between input
and output channels. This eliminates connectivity restrictions
posed by the grouped convolutions.

3. EXPERIMENTS

In this section experimental setup, detailed ablation study and
results in comparison to the state of the art are reported.

3.1. Experimental Setup

Through all of our experiments, weighted cross entropy loss
from [12] is used, to overcome the class imbalance. Adam op-
timizer [19] learning rate is set to 1e−4. Batch normalization

[20] is incorporated. L2 regularization with weight decay rate
of 5e−4 is utilized to avoid over-ﬁtting. The feature extractor
part of the network is initialized with the pre-trained corre-
sponding encoder trained on Imagenet. A width multiplier
of 1 for MobileNet to include all the feature channels is per-
formed through all the experiments. The number of groups
used in ShufﬂeNet is 3. Based on previous [15] results on
classiﬁcation and detection three groups provided adequate
accuracy.

Results are reported on Cityscapes dataset [9] which con-
tains 5000 images with ﬁne annotation, with 20 classes in-
cluding the ignored class. Another section of the dataset con-
tains coarse annotations with 20,000 labeled images. These
are used in the case of Coarse pre-training that improves the
results of the segmentation. Experiments are conducted on
images with resolution of 512x1024.

Table 2: Comparison of the most promising models in our
benchmarking framework in terms of GFLOPs and frames per
second, this is computed on image resolution 512x1024.

Model
SkipNet-MobileNet
UNet-MobileNet

GFLOPs
13.8
55.9

3.2. Semantic Segmentation Results

Semantic segmentation is evaluated using mean intersection
over union (mIoU), per-class IoU, and per-category IoU.
Table1 shows the results for the ablation study on different
encoders-decoders with mIoU and GFLOPs to demonstrate
the accuracy and computations trade-off. The main insight
gained from our experiments is that, UNet decoding method
provides more accurate segmentation results than Dilation
Frontend. This is mainly due to the transposed convolution
by 8x in the end of the Dilation Frontend, unlike the UNet
stage-wise upsampling method. The SkipNet architecture
provides on par results with UNet decoding method. In some

Table 3: Comparison of some of the models from our benchmarking framework with the state of the art segmentation networks
on cityscapes test set. GFLOPs is computed on image resolution 360x640.

Model
SegNet[21]
ENet[12]
DeepLab[2]
SkipNet-VGG16[1]
SkipNet-ShufﬂeNet
SkipNet-MobileNet

GFLOPs Class IoU Class iIoU Category IoU Category iIoU
286.03
3.83
-
-
2.0
6.2

66.4
64.0
67.7
70.1
62.2
63.0

34.2
24.4
42.6
41.7
32.4
35.2

79.8
80.4
86.4
85.7
80.2
82.0

56.1
58.3
70.4
65.3
58.3
61.5

(a)

(c)

(b)

(d)

Fig. 3: Qualitative Results on CityScapes. (a) Original Image. (b) SkipNet-MobileNet pretrained with Coarse Annotations. (c)
UNet-Resnet18. (d) SkipNet-ShufﬂeNet pretrained with Coarse Annotations.

architectures such as SkipNet-ShufﬂeNet it is less accurate
than UNet counter part by 1.5%.

The UNet method of incrementally upsampling with-in
the network provides the best in terms of accuracy. However,
Table 2 clearly shows that SkipNet architecture is more com-
putationally efﬁcient with 4x reduction in GFLOPs. This is
explained by the fact that transposed convolutions in UNet
are applied in the feature space unlike in SkipNet that are ap-
plied in label space. Table 1 shows that Coarse pre-training
improves the overall mIoU with 1-4%. The underrepresented
classes are the ones that often beneﬁt from pre-training.

Experimental results on the cityscapes test set are shown
in Table 3. Although, DeepLab provides best results in terms
of accuracy, it is not computationally efﬁcient. ENet [12]
is compared to SkipNet-ShufﬂeNet and SkipNet-MobileNet
in terms of accuracy and GFLOPs. SkipNet-ShufﬂeNet out-
performs ENet in terms of GFLOPs, yet it maintains on par
mIoU. However, we have not been able to outperform ENet
in terms of inference speed. Both SkipNet-ShufﬂeNet and
SkipNet-MobileNet outperform SegNet [21] in terms of com-

putational cost and accuracy with reduction up to 143x in
GFLOPs. Figure 3 shows qualitative results for different en-
coders including MobileNet, ShufﬂeNet and ResNet18.
It
shows that MobileNet provides more accurate segmentation
results than the later two. SkipNet-MobileNet is able to cor-
rectly segment the pedestrian and the signs on the right unlike
the others.

4. CONCLUSION

In this paper we present the ﬁrst principled approach for
benchmarking real-time segmentation networks. The decou-
pled design of the framework separates modules for better
quantitative comparison. The ﬁrst module is comprised of
the feature extraction network architecture,
the second is
the meta-architecture that provides the decoding method.
Three different meta-architectures are included in our frame-
work, including Skip architecture, UNet, and Dilation Fron-
tend. Different network architectures for feature extraction
are included, which are ShufﬂeNet, MobileNet, VGG16,

and ResNet-18. Our benchmarking framework provides re-
searchers and practitioners with a mean to evaluate design
choices for their tasks.

5. REFERENCES

[1] Jonathan Long, Evan Shelhamer, and Trevor Darrell,
“Fully convolutional networks for semantic segmenta-
tion,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015, pp. 3431–
3440.

[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokki-
“Deeplab:
nos, Kevin Murphy, and Alan L Yuille,
Semantic image segmentation with deep convolutional
nets, atrous convolution, and fully connected crfs,”
arXiv preprint arXiv:1606.00915, 2016.

[3] Fisher Yu and Vladlen Koltun,

“Multi-scale context
aggregation by dilated convolutions,” arXiv preprint
arXiv:1511.07122, 2015.

[4] Guosheng Lin, Chunhua Shen, Anton van den Hen-
gel, and Ian Reid, “Exploring context with deep struc-
tured models for semantic segmentation,” arXiv preprint
arXiv:1603.03183, 2016.

[5] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du,
Chang Huang, and Philip HS Torr, “Conditional random
ﬁelds as recurrent neural networks,” in Proceedings of
the IEEE International Conference on Computer Vision,
2015, pp. 1529–1537.

[6] Evan Shelhamer, Kate Rakelly, Judy Hoffman, and
Trevor Darrell, “Clockwork convnets for video semantic
segmentation,” in Computer Vision–ECCV 2016 Work-
shops. Springer, 2016, pp. 852–868.

[7] Mark Everingham, Luc Van Gool, Christopher KI
“The
Williams, John Winn, and Andrew Zisserman,
pascal visual object classes (voc) challenge,” Interna-
tional journal of computer vision, vol. 88, no. 2, pp.
303–338, 2010.

[8] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and
Rob Fergus, “Indoor segmentation and support infer-
ence from rgbd images,” Computer Vision–ECCV 2012,
pp. 746–760, 2012.

[9] Marius Cordts, Mohamed Omran, Sebastian Ramos,
Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele,
“The
cityscapes dataset for semantic urban scene understand-
ing,” in Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2016, pp. 3213–
3223.

[10] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul`o,
and Peter Kontschieder, “The mapillary vistas dataset
for semantic understanding of street scenes,” in Pro-
ceedings of the International Conference on Computer
Vision (ICCV), Venice, Italy, 2017, pp. 22–29.

[11] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jian-
“Icnet for real-time seman-
arXiv

ping Shi, and Jiaya Jia,
tic segmentation on high-resolution images,”
preprint arXiv:1704.08545, 2017.

[12] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and
Eugenio Culurciello, “Enet: A deep neural network ar-
chitecture for real-time semantic segmentation,” arXiv
preprint arXiv:1606.02147, 2016.

[13] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu
Oprea, Victor Villena-Martinez, and Jose Garcia-
“A review on deep learning techniques
Rodriguez,
arXiv preprint
applied to semantic segmentation,”
arXiv:1704.06857, 2017.

[14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam, “Mobilenets: Efﬁcient
convolutional neural networks for mobile vision appli-
cations,” arXiv preprint arXiv:1704.04861, 2017.

[15] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian
Sun, “Shufﬂenet: An extremely efﬁcient convolutional
arXiv preprint
neural network for mobile devices,”
arXiv:1707.01083, 2017.

[16] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,
“U-net: Convolutional networks for biomedical image
segmentation,” in International Conference on Medical
Image Computing and Computer-Assisted Intervention.
Springer, 2015, pp. 234–241.

[17] Karen Simonyan and Andrew Zisserman, “Very deep
convolutional networks for large-scale image recogni-
tion,” arXiv preprint arXiv:1409.1556, 2014.

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,” in
Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.

[19] Diederik Kingma and Jimmy Ba,
method for stochastic optimization,”
arXiv:1412.6980, 2014.

“Adam: A
arXiv preprint

[20] Sergey Ioffe and Christian Szegedy, “Batch normaliza-
tion: Accelerating deep network training by reducing
internal covariate shift,” in International Conference on
Machine Learning, 2015, pp. 448–456.

[21] Vijay Badrinarayanan, Alex Kendall, and Roberto
Cipolla,
“Segnet: A deep convolutional encoder-
decoder architecture for image segmentation,” arXiv
preprint arXiv:1511.00561, 2015.

