8
1
0
2
 
t
c
O
 
3
1
 
 
]
I

A
.
s
c
[
 
 
1
v
1
5
8
5
0
.
0
1
8
1
:
v
i
X
r
a

Overview of CAIL2018: Legal Judgment Prediction Competition

Haoxi Zhong1∗ Chaojun Xiao1∗ Zhipeng Guo1 Cunchao Tu1 Zhiyuan Liu1
Maosong Sun1 Yansong Feng2 Xianpei Han3 Zhen Hu4 Heng Wang4 Jianfeng Xu5
1Department of Computer Science and Technology, Tsinghua University, China
2Institute of Computer Science and Technology, Peking University, China
3Institute of Software, Chinese Academy of Sciences, China
4China Justice Big Data Institute
5Supreme People’s Court, China

Abstract

Speciﬁcally,

In this paper, we give an overview of the
Legal Judgment Prediction (LJP) compe-
tition at Chinese AI and Law challenge
(CAIL2018). This year’s competition fo-
cuses on LJP which aims to predict the
judgment results according to the given
facts.
in CAIL2018 , we
proposed three subtasks of LJP for the
contestants,
i.e., predicting relevant law
articles, charges and prison terms given
the fact descriptions. CAIL2018 has at-
tracted several hundreds participants (601
teams, 1, 144 contestants from 269 orga-
In this paper, we provide a
nizations).
detailed overview of the task deﬁnition,
related works, outstanding methods and
competition results in CAIL2018.

1 Introduction

Legal Judgment Prediction is a traditional task in
the combination of artiﬁcial intelligence and laws.
It aims to train a machine judge to predict the judg-
ment results (e.g., relevant law articles, charges,
prison terms and so on) automatically according
to the facts. A well-performed LJP system can not
only beneﬁt those who are not familiar with laws
but also provide a reference to professionals, e.g.,
lawyers and judges.

In order to promote the development of legal
intelligence,
this year’s AI and Law challenge,
CAIL2018 , focuses on how artiﬁcial intelligence
can help the LJP system. Firstly, we published a
large-scale criminal dataset constructed from Chi-
nese law documents (Xiao et al., 2018). Based on
this dataset, we propose three subtasks of LJP for
contestants, including predicting relevant law arti-

∗ indicates equal contribution.

cles, charges and prison terms given the fact de-
scriptions from law documents.

The goal of CAIL2018 is to explore how NLP
techniques and legal knowledge beneﬁt the perfor-
mance of LJP. For the three subtasks in LJP, there
are several major challenges for contestants as fol-
lows:

• The distributions of various law articles,
charges, and prison terms are quite imbal-
anced. According to the statistics，the top-
10 charges covers over 79.0% cases while the
bottom-10 charges only cover about 0.12%
cases. The imbalanced distribution makes it
difﬁcult to predict low-frequency categories.

• Predicting the prison terms via the fact de-
scriptions is more challenging than other sub-
tasks. In real-world scenarios, when deciding
the prison terms of a case, the judge will be
affected by plenty of factors, e.g., ages of de-
fendants, amount of money involved in the
case and so on. It’s challenging for a machine
to deﬁne and extract sufﬁcient features from
fact description.

For example,

• There are usually complex logic dependen-
cies between subtasks.
the
charges of the criminals should refer to the
relevant articles as in Chinese Criminal Law,
and the decision of prison terms should ac-
cord with the stipulations in law articles. So it
is crucial for the contestants to understand the
rules contained in law articles and discover
the logic dependencies among subtasks.

• There exists many confusing categories pairs
in these subtasks, such as the charges of rob-
In Chinese Criminal Law,
bery and theft.
there are only a few differences between
the deﬁnitions of many charge pairs, which

make it difﬁcult to distinguish these confus-
ing charges.

In this year’s competition,

there are 202
teams who have submitted their models to the
contests, and the best-performed models reach
90.62, 87.91, 78.22 in the three subtasks. Com-
paring with the performance at the early stage of
this competition, all the subtasks achieve signiﬁ-
cant improvements.

In the following parts, we will give a detailed
introduction to CAIL2018 including the task deﬁ-
nition and evaluation metrics. In addition, we will
introduce the best-performed models submitted by
contestants and discuss the reminding challenges.

2 Related Work

LJP is a hot topic in the ﬁeld of legal intelligence
and has been studied for several years.
In early
years, the studies of LJP usually concentrate on
how to utilize mathematical and statistical meth-
ods to build LJP systems in some speciﬁc sce-
narios (Kort, 1957; Ulmer, 1963; Nagel, 1963;
Keown, 1980; Segal, 1984; Lauderdale and Clark,
2012).

With the development of machine learning
techniques, more works propose to employ ex-
isting machine learning models to improve the
performance on LJP. In these works, they usu-
ally formalize LJP as a text classiﬁcation prob-
lem and focus on extracting efﬁcient shallow
features from the given facts and additional re-
sources (Liu and Hsieh, 2006; Lin et al., 2012;
Aletras et al., 2016; Sulea et al., 2017). These
works integrate machine learning methods into
LJP tasks and achieve a promising performance
of LJP. However, these conventional methods can
only extract well-deﬁned shallow textual features
from the fact descriptions.

In recent years, with the successful usage of
deep learning techniques on NLP tasks (Kim,
2014a; Baharudin et al., 2010; Tang et al., 2015),
researchers propose to employ neural models to
solve LJP tasks. For example, Luo et al. (2017)
adopt attention mechanism between facts and rel-
evant law articles for charge prediction. Hu et al.
(2018) introduce several charge attributes to pre-
dict few-shot and confusing charges. Jiang et al.
(2018) employ deep reinforcement learning to ex-
tract rationales for interpretable charge predic-
tion. Zhong et al. (2018) model the dependencies
among the different subtasks in LJP as a Directed

Acyclic Graph (DAG), and propose a topologi-
cal learning model to solve these tasks simultane-
ously. Ye et al. (2018) integrate Seq2Seq model
and predicted charges to generate the court view
with fact descriptions.

3 Task Deﬁnition and Evaluation Metrics

In this section, we give the detailed dataset
evalu-
construction,
All
this
ation metrics
the
from
also
https://github.com/thunlp/CAIL.

and
competition.
be

task
of
can

deﬁnition,

achieved

details

3.1 Dataset Construction

construct

the CAIL2018 dataset

We
from
5, 730, 302 criminal documents collected from
China Judgment Online1. As all the law docu-
ments are written in a standard format, it is easy
to extract the fact description and the judgment
results from these documents. During the prepro-
cessing period, we ﬁlter out some case documents
with low-frequency categories or multiple defen-
dants. Finally, there are 183 different criminal law
articles and 202 different charges in this dataset.

We randomly selected 1, 710, 856 documents as
the training set. There are two stages in the con-
test. In the ﬁrst stage, we selected 217, 016 docu-
ments for testing. After all participants conﬁrmed
their ﬁnal models, we collected 35, 922 emerging
documents for testing in the second stage.

3.2 Task Deﬁnition

LJP takes the fact description of a speciﬁc case as
the input and predicts the judgment results as the
output. The judgment results consist of three parts
as follows:

• Law articles. The contestants should give a
list of relevant articles as there might be mul-
tiple law articles relevant to one case.

• Charges. The contestants should give a list
of charges that the defendant in the case is
convicted of.

• Prison terms. The contestants should give the
prison term that the defendant in the case is
sentenced to. The prison terms should be an
integer which stands for how much months
the prison terms should be.

1http://wenshu.court.gov.cn/

N

X
i=1
N

X
i=1
N

X
i=1

We denote the prediction of

law articles,
charges, and prison terms as task 1, 2, and 3 re-
spectively.

calculate:

TPmicro =

TPi,

3.3 Evaluation Metrics

FPmicro =

FPi,

(4)

For task 1 and task 2, we take them as text clas-
siﬁcation problems. For a speciﬁc task, suppose
there are N categories and M documents in total.
We denote the ground truth category as y and the
predicted label as ¯y. If the j-th documents are an-
notated with the i-th category, then yij should be
1 and 0 otherwise. Then we can get the following
metrics for all classes:

TPi =

[yij = 1, ¯yij = 1],

FPi =

[yij = 0, ¯yij = 1],

FNi =

[yij = 1, ¯yij = 0],

TNi =

[yij = 0, ¯yij = 0].

M

X
j=1

M

X
j=1

M

X
j=1

M

X
j=1

These four metrics represent the true positive,
false positive, false negative and true negative
value for the i-th category. Then we can calcu-
late the precision, recall and F value for the i-th
category as follows:

:

Pi =

Ri =

Fi =

,

TPi
TPi + FPi
TPi
TPi + FNi
2 × Pi × Ri
Pi + Ri

,

.

Fmacro = P

N
i=1 Fi
N

Here, p and r represent precision and recall re-
spectively. With these evaluation results for all
categories, we can calculate the macro-level F
value as follows:

Besides, we also evaluate the performance in
micro-level. For micro-level evaluation, we ﬁrst

FNmicro =

FNi.

Similarly, we can calculate the precision, recall,

and F values in the micro-level as follows:

Pmicro =

Rmicro =

Fmicro =

,

TPmicro
TPmicro + FPmicro
TPmicro
TPmicro + FNmicro
2 × Pmicro × Rmicro
Pmicro + Rmicro

,

.

(5)

(6)

(8)

(9)

Finally, we calculate overall score S as

(1)

S = 100 ×

Fmicro + Fmacro
2

For task 3, we employ the difference of the pre-
dicted prison terms and the ground-truth ones as
the evaluation metric. Assume that the ground-
truth prison term of the i-th case is ti and the pre-
dicted result is ¯ti. Then, we deﬁne the difference
di as

di = |log(ti + 1) − log(¯ti + 1)| .

(7)

After that, we deﬁne the score function f (v) as

(2)

f (v) =

1.0,
0.8,
0.6,
0.4,
0.2,
0.0,






if v ≤ 0.2,
if 0.2 < v ≤ 0.4,
if 0.4 < v ≤ 0.6,
if 0.6 < v ≤ 0.8,
if 0.8 < v ≤ 1,
if 1 < v.

S =

M

X
i=1

f (di)
M

Then the ﬁnal score of task 3 should be:

4 Approach Overview

(3)

There are over 200 teams who have registered for
CAIL2018 and submitted their ﬁnal models. The
ﬁnal scores show that neural models can achieve
considerable results on task 1 and task 2, but it
is still challenging to predict the prison terms. In

Charges

Tasks
Evaluation Metrics
nevermore
jiachx
xlzhang
HFL
大大大师师师兄兄兄

Law Articles
Fmicro Fmacro Fmicro Fmacro
0.836
0.958
0.815
0.952
0.811
0.952
0.811
0.953
0.816
0.945
安安安徽徽徽高高高院院院类类类案案案指指指引引引研研研发发发团团团队队队 0.946
0.803
0.811
0.952
0.801
0.948
0.755
0.945
0.791
0.940
0.772
0.934

AI judge
只只只看看看看看看不不不说说说话话话
DG
SXU AILAW
中中中电电电28所所所联联联合合合部部部落落落

0.962
0.958
0.958
0.958
0.951
0.950
0.956
0.954
0.949
0.950
0.937

0.781
0.748
0.760
0.769
0.757
0.756
0.766
0.738
0.717
0.728
0.740

Prison Terms
Score
77.57
69.64
69.64
77.70
73.16
72.24
–
77.54
76.18
76.49
75.77

Table 1: Performance of participants on CAIL2018 .

Table 1, we list the scores of top-6 participants of
each subtask. Here, we evaluate these models on
the testing set in the second stage, which contains
35, 922 cases.

We have collected the technical reports of these
contestants.
In the following parts, we summa-
rize their methods and tricks according to these
reports.

4.1 General Architecture

Pre-processing. For most contestants, they con-
duct the following pre-processing steps to trans-
form the raw documents into the format which is
suitable for their models.

• Word Segmentation. As all the documents are
written in Chinese, it is important for the con-
testants to conduct a high-quality word seg-
mentation. For word segmentation, the con-
testants usually choose jieba2, ICTCLAS3,
THULAC4 or other Chinese word segmenta-
tion tools.

• Word Embedding.

After word segmen-
tation, we need to transform the dis-
crete word symbols into continuous word
the contestants
Generally,
embeddings.
employ word2vec (Mikolov et al., 2013),
Glove (Pennington et al., 2014), or Fast-
Text (Joulin et al., 2017) to pre-train word
embeddings on these criminal cases.

Text Classiﬁcation Models. After preprocess-
ing, we need to classify these processed fact de-

2https://github.com/fxsjy/jieba
3http://ictclas.nlpir.org/
4http://thulac.thunlp.org/

scriptions into corresponding categories. For most
contestants, they employ existing neural network
based text classiﬁcation models to extract efﬁcient
text features. The most commonly used text clas-
siﬁcation models are listed as follows:

• Text-CNN (Kim, 2014b): CNN with multiple

ﬁlter widths.

• LSTM (Hochreiter and Schmidhuber, 1997))

or bidirectional LSTM.

• GRU, Gated Recurrent Unit

(Cho et al.,

2014).

• HAN,

Hierarchical

Attention

Net-

works (Yang et al., 2016).

• RCNN, Recurrent Convolutional Neural Net-

works (Lai et al., 2015).

• DPCNN, Deep Pyramid Convolutional Neu-
ral Networks (Johnson and Zhang, 2017).

According to the technical reports of contes-
tants, it has been proven that these neural models
can achieve good performance in high-frequency
categories.

4.2 Promising Tricks

In predicting relevant law articles and charges,
these traditional models can achieve promising re-
sults in high-frequency categories. However, due
to the imbalance issue, it is challenging to reach
a good performance on the low-frequency ones.
Therefore, how to address the problem of imbal-
anced data becomes the most important thing in
the ﬁrst two subtasks.

In the task of predicting prison terms, sim-
ple linear regression methods perform poorly than
classiﬁcation models. Thus, most participants still
treat it as a text classiﬁcation problem. How-
ever, how to divide the intervals is challenging
and will badly inﬂuence the classiﬁcation perfor-
mance. Meanwhile, the prison terms are affected
by many factors and explicit features, rather than
implicit semantic meanings in the text. All these
issues make the task 3 the most difﬁcult subtask.

According to the technical reports,

there are
some useful tricks which can address these issues
and improve the text classiﬁcation models signiﬁ-
cantly. We summarize them as follows:

• Word Embeddings. It has been proven by
participants that a better word embedding
model, such as ELMO (Peters et al., 2018)
could achieve a better performance than
Skip-Gram(Mikolov et al., 2013). Moreover,
training word embeddings on a larger legal
corpus can also improve the performance of
LJP models.

• Data Balance. Undersampling and oversam-
pling methods are the most common ways to
address the imbalance issue of categories in
this competition.

• Joint Learning. As there are dependencies
among these subtasks, some participants em-
ploy multi-task learning models to solve them
jointly.

• Additional Attributes. Inspired by Hu et al.
(2018), participants improve their perfor-
mance on few-shot and confusing category
pairs by predicting their legal attributes.

• Additional Features. Many participants at-
tempted to extract features manually, e.g.,
amount involved, named entities, ages and so
on. These manually deﬁned features can im-
prove the performance of task 3 greatly.

loss functions.

• Loss Function. Most models use cross-
entropy as their
How-
ever, some models adopt more promising
loss functions, such as focal loss (Lin et al.,
2018), to enhance the performance on low-
frequency categories.
the loss
weights of various categories and the activa-
tion functions of the output layer also have
great inﬂuence on the ﬁnal performance.

Besides,

• Ensemble. Most participants train several
different classiﬁcation models and combine
them with simple voting or weighted average
strategies to combine their predicting results.

4.3 Conclusion

In CAIL2018, we employ Legal Judgement Pre-
diction as the competition topic.
In this compe-
tition, we construct and release a large-scale LJP
dataset. The performance of 3 LJP subtasks sig-
niﬁcantly raised with the efforts of over 200 par-
ticipants. In this paper, we summarize the general
architecture and promising tricks they employed,
which are expected to beneﬁt further researches on
legal intelligence.

References

Nikolaos Aletras, Dimitrios Tsarapatsanis, Daniel
Preotiuc-Pietro, and Vasileios Lampos. 2016. Pre-
dicting judicial decisions of the european court of
human rights: A natural language processing per-
spective. PeerJ Computer Science, 2.

Baharum Baharudin, Lam Hong Lee, and Khairullah
Khan. 2010. A review of machine learning algo-
rithms for text-documents classiﬁcation. Journal of
Advances in Information Technology, 1(1):4–20.

Kyunghyun Cho, Bart Van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. Computer Sci-
ence.

Sepp Hochreiter and Jurgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Zikun Hu, Xiang Li, Cunchao Tu, Zhiyuan Liu, and
Maosong Sun. 2018. Few-shot charge prediction
with discriminative legal attributes. In Proceedings
of COLING.

Xin Jiang, Hai Ye, Zhunchen Luo, WenHan Chao, and
Wenjia Ma. 2018. Interpretable rationale augmented
charge prediction system. In Proceedings of COL-
ING: System Demonstrations, pages 146–151.

Rie Johnson and Tong Zhang. 2017. Deep pyramid
convolutional neural networks for text categoriza-
tion. In Proceedings of ACL, volume 1, pages 562–
570.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efﬁcient text
classiﬁcation. In Proceedings of EACL, volume 2,
pages 427–431.

R Keown. 1980. Mathematical models for legal pre-

diction. Computer/LJ, 2:829.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
sentiment classiﬁcation. In Proceedings of EMNLP,
pages 1422–1432.

S Sidney Ulmer. 1963. Quantitative analysis of judi-
cial processes: Some practical and theoretical appli-
cations. Law and Contemporary Problems, 28:164.

Chaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao
Tu, Zhiyuan Liu, Maosong Sun, Yansong Feng,
Xianpei Han, Zhen Hu, Heng Wang, et al. 2018.
Cail2018: A large-scale legal dataset for judgment
prediction. arXiv preprint arXiv:1807.02478.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classiﬁcation.
In
Proceedings of NAACL, pages 1480–1489.

Hai Ye, Xin Jiang, Zhunchen Luo, and Wenhan Chao.
2018. Interpretable charge predictions for criminal
cases: Learning to generate court views from fact
descriptions. In Proceedings of NAACL.

Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun
Xiao, Zhiyuan Liu, and Maosong Sun. 2018. Le-
gal judgment prediction via topological learning. In
Proceedings of EMNLP.

Yoon Kim. 2014a. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of EMNLP.

Yoon Kim. 2014b.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Fred Kort. 1957. Predicting supreme court decisions
mathematically: A quantitative analysis of the ”right
to counsel” cases. American Political Science Re-
view, 51(1):1–12.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for text
classiﬁcation. In Proceedings of AAAI, volume 333,
pages 2267–2273.

Benjamin E Lauderdale and Tom S Clark. 2012. The
supreme court’s many median justices. American
Political Science Review, 106(4):847–866.

Tsung-Yi Lin, Priyal Goyal, Ross Girshick, Kaiming
He, and Piotr Doll´ar. 2018. Focal loss for dense ob-
ject detection. IEEE TPAMI.

Wanchen Lin, Tsung Ting Kuo, and Tung Jia Chang.
2012. Exploiting machine learning models for chi-
nese legal documents labeling, case classiﬁcation,
and sentencing prediction. In Processdings of RO-
CLING, page 140.

Chaolin Liu and Chwen Dar Hsieh. 2006. Exploring
phrase-based classiﬁcation of judicial documents for
criminal charges in chinese. In Proceedings of IS-
MIS, pages 681–690.

Bingfeng Luo, Yansong Feng, Jianbo Xu, Xiang
Zhang, and Dongyan Zhao. 2017. Learning to pre-
dict charges for criminal cases with legal basis. In
Proceedings of EMNLP.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119.

Stuart S Nagel. 1963. Applying correlation analysis to

case prediction. Texas Law Review, 42:1006.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
In Proceedings of EMNLP, pages
representation.
1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Jeffrey A Segal. 1984. Predicting supreme court cases
probabilistically: The search and seizure cases,
1962-1981. American Political Science Review,
78(4):891–900.

Octavia Maria Sulea, Marcos Zampieri, Mihaela Vela,
and Josef Van Genabith. 2017. Exploring the use of
text classi cation in the legal domain. In Proceedings
of ASAIL workshop.

