Large-scale Distance Metric Learning with Uncertainty

Qi Qian

Shenghuo Zhu
Hao Li
Alibaba Group, Bellevue, WA, 98004, USA
{qi.qian, jiasheng.tjs, lihao.lh, shenghuo.zhu, jinrong.jr}@alibaba-inc.com

Jiasheng Tang

Rong Jin

8
1
0
2
 
y
a
M
 
5
2
 
 
]

G
L
.
s
c
[
 
 
1
v
4
8
3
0
1
.
5
0
8
1
:
v
i
X
r
a

Abstract

Distance metric learning (DML) has been studied ex-
tensively in the past decades for its superior performance
with distance-based algorithms. Most of the existing meth-
ods propose to learn a distance metric with pairwise or
triplet constraints. However, the number of constraints is
quadratic or even cubic in the number of the original ex-
amples, which makes it challenging for DML to handle the
large-scale data set. Besides, the real-world data may con-
tain various uncertainty, especially for the image data. The
uncertainty can mislead the learning procedure and cause
the performance degradation. By investigating the image
data, we ﬁnd that the original data can be observed from
a small set of clean latent examples with different distor-
tions. In this work, we propose the margin preserving metric
learning framework to learn the distance metric and latent
examples simultaneously. By leveraging the ideal proper-
ties of latent examples, the training efﬁciency can be im-
proved signiﬁcantly while the learned metric also becomes
robust to the uncertainty in the original data. Furthermore,
we can show that the metric is learned from latent examples
only, but it can preserve the large margin property even for
the original data. The empirical study on the benchmark
image data sets demonstrates the efﬁcacy and efﬁciency of
the proposed method.

1. Introduction

Distance metric learning (DML) aims to learn a distance
metric where examples from the same class are well sepa-
rated from examples of different classes. It is an essential
task for distance-based algorithms, such as k-means clus-
tering [18], k-nearest neighbor classiﬁcation [17] and infor-
mation retrieval [2]. Given a distance metric M , the squared
Mahalanobis distance between examples xi and xj can be
computed as

D2

M (xi, xj) = (xi − xj)(cid:62)M (xi − xj)

Most of existing DML methods propose to learn the met-
ric by minimizing the number of violations in the set of

Figure 1. Illustration of the proposed method. Let round and
square points denote the target data and impostors, respectively.
Let triangle points denote the corresponding latent examples. Data
points with the same color are from the same class. It demonstrates
that the metric learned with latent examples not only separates the
dissimilar latent data with a large margin but also preserves the
large margin for the original data.

pairwise or triplet constraints. Given a set of pairwise con-
straints, DML tries to learn a metric such that the distances
between examples from the same class are sufﬁciently small
(e.g., smaller than a predeﬁned threshold) while those be-
tween different ones are large enough [3, 18]. Different
from pairwise constraints, each triplet constraint consists of
three examples (xi, xj, xk), where xi and xj have the same
label and xk is from a different class. An ideal metric can
push away xk from xi and xj by a large margin [17]. Learn-
ing with triplet constraints optimizes the local positions of
examples and is more ﬂexible for real-world applications,
where deﬁning the appropriate thresholds is hard for pair-
wise constraints. In this work, we will focus on DML with
triplet constraints.

Optimizing the metric with a set of triplet constraints is
challenging since the number of triplet constraints can be
up to O(n3), where n is the number of the original train-
ing examples. It makes DML computationally intractable
for the large-scale problems. Many strategies have been de-
veloped to deal with this challenge and most of them fall
into two categories, learning by stochastic gradient descent
(SGD) and learning with the active set. With the strategy
of SGD, DML methods can sample just one constraint or
a mini-batch of constraints at each iteration to observe an
unbiased estimation of the full gradient and avoid comput-

ing the gradient from the whole set [2, 10]. Other methods
learn the metric with a set of active constraints (i.e., vio-
lated by the current metric), where the size can be signif-
icantly smaller than the original set [17].
It is a conven-
tional strategy applied by cutting plane methods [1]. Both
of these strategies can alleviate the large-scale challenge but
have inherent drawbacks. Approaches based on SGD have
to search through the whole set of triplet constraints, which
results in the slow convergence, especially when the num-
ber of active constraints is small. On the other hand, the
methods relying on the active set have to identify the set at
each iteration. Unfortunately, this operation requires com-
puting pairwise distances with the current metric, where the
cost is O(n2) and is too expensive for large-scale problems.
Besides the challenge from the size of data set, the un-
certainty in the data is also an issue, especially for the image
data, where the uncertainty can come from the differences
between individual examples and distortions, e.g., pose, il-
lumination and noise. Directly learning with the original
data will lead to a poor generalization performance since the
metric tends to overﬁt the uncertainty in the data. By further
investigating the image data, we ﬁnd that most of original
images can be observed from a much smaller set of clean la-
tent examples with different distortions. The phenomenon
is illustrated in Fig. 5. This observation inspires us to learn
the metric with latent examples in lieu of the original data.
The challenge is that latent examples are unknown and only
images with uncertainties are available.

In this work, we propose a framework to learn the dis-
It suf-
tance metric and latent examples simultaneously.
ﬁciently explores the properties of latent examples to ad-
dress the mentioned challenges. First, due to the small
size of latent examples, the strategy of identifying the ac-
tive set becomes affordable when learning the metric. We
adopt it to accelerate the learning procedure via avoiding the
attempts on inactive constraints. Additionally, compared
with the original data, the uncertainty in latent examples
decreases signiﬁcantly. Consequently, the metric directly
learned from latent examples can focus on the nature of the
data rather than the uncertainty in the data. To further im-
prove the robustness, we adopt the large margin property
that latent examples from different classes should be pushed
away with a data dependent margin. Fig. 1 illustrates that
an appropriate margin for latent examples can also preserve
the large margin for the original data. We conduct the em-
pirical study on benchmark image data sets, including the
challenging ImageNet data set, to demonstrate the efﬁcacy
and efﬁciency of the proposed method.

The rest of the paper is organized as follows: Section 2
summarizes the related work of DML. Section 3 describes
the details of the proposed method and Section 4 summa-
rizes the theoretical analysis. Section 5 compares the pro-
posed method to the conventional DML methods on the

benchmark image data sets. Finally, Section 6 concludes
this work with future directions.

2. Related Work

Many DML methods have been proposed in the past
decades [3, 17, 18] and comprehensive surveys can be
found in [7, 19]. The representative methods include Xing’s
method [18], ITML [3] and LMNN [17].
ITML learns
a metric according to pairwise constraints, where the dis-
tances between pairs from the same class should be smaller
than a predeﬁned threshold and the distances between pairs
from different classes should be larger than another prede-
ﬁned threshold. LMNN is developed with triplet constraints
and a metric is learned to make sure that pairs from the same
class are separated from the examples of different classes
with a large margin. Compared with pairwise constraints,
triplet constraints are more ﬂexible to depict the local ge-
ometry.

To handle the large number of constraints, some meth-
ods adopt SGD or online learning to sample one constraint
or a mini-batch of constraints at each iteration [2, 10]. OA-
SIS [2] randomly samples one triplet constraint at each it-
eration and computes the unbiased gradient accordingly.
When the size of the active set is small, these methods re-
quire extremely large number of iterations to improve the
model. Other methods try to explore the concept of the ac-
tive set. LMNN [17] proposes to learn the metric effec-
tively at each iteration by collecting an active set that con-
sists of constraints violated by the current metric within the
k-nearest neighbors for each example. However, it requires
O(n2) to obtain the appropriate active set.

Besides the research about conventional DML, deep
metric learning has attracted much attention recently [9,
13, 15, 16]. These studies also indicate that sampling ac-
tive triplets is essential for accelerating the convergence.
FaceNet [15] keeps a large size of mini-batch and searches
hard constraints within a mini-batch. LeftedStruct [16] gen-
erates the mini-batch with the randomly selected positive
examples and the corresponding hard negative examples.
Proxy-NCA [9] adopts proxy examples to reduce the size
of triplet constraints. Once an anchor example is given, the
similar and dissimilar examples will be searched within the
set of proxies. In this work we propose to learn the metric
only with latent examples which can dramatically reduce
the computational cost of obtaining the active set. Besides,
the triangle inequality dose not hold for the squared dis-
tance, which makes our analysis signiﬁcantly different from
the existing work.

3. Margin Preserving Metric Learning

Given a training set {(xi, yi)|i = 1, · · · , n}, where xi ∈
Rd is an example and yi is the corresponding label, DML

aims to learn a good distance metric such that

∀xi, xj, xk D2

M (xi, xk) − D2

M (xi, xj) ≥ 1

where xi and xj are from the same class and xk is different.
Given the distance metric M ∈ S d×d
+ , the squared distance
is deﬁned as

D2

M (xi, xj) = (xi − xj)(cid:62)M (xi − xj)

where S d×d
+
(PSD) matrices.

denotes the set of d × d positive semi-deﬁnite

For the large-scale image data set, we assume that each
observed example is from a latent example with certain zero
mean distortions, i.e.,

∀i, E[xi] = zo:f (i)=o

where f (·) projects the original data to its corresponding
latent example.

Then, we consider the expected distance [20] between
observed data and the objective is to learn a metric such
that

Once the metric is observed, the margin for the expected
distances between original data (i.e., as in Eqn. 1) is also
guaranteed. Compared with the original constraints, the
margin between latent examples is increased by the factor of
E[D2
M (xi, zo)]. This term indicates the expected distance
between the original data and its corresponding latent ex-
ample. It means that the tighter a local cluster is, the less a
margin should be increased. Furthermore, each class takes
a different margin, which depends on the distribution of the
original data and makes it more ﬂexible than a global mar-
gin.

With the set of triplets {zt

o, zt

p, zt

q}, the optimization

problem can be written as

min

L(M, z) =

(cid:96)(zt

o, zt

p, zt

q; M )

M ∈S d×d

+ ,(cid:107)M (cid:107)F ≤δ,z∈Rd×m

(cid:88)

t

where m (cid:28) n is the number of latent examples. We add a
constraint for the Frobenius norm of the learned metric to
prevent it from overﬁtting. (cid:96)(·) is the loss function and the
hinge loss is applied in this work.

∀xi, xj, xk E[D2

M (xi, xk)] − E[D2

M (xi, xj)] ≥ 1

(1)

M (xt

i, zt

o)] − (D2

M (zt

o, zt

q) − D2

M (zt

o, zt

p))]+

o, zt

p, zt
(cid:96)(zt
= [1 + E[D2

q; M )

Let zo, zp and zq denote latent examples of xi, xj and
xk respectively. For the distance between examples from
the same class, we have

M (xi, xj)] = E[(xi − zo + zo)(cid:62)M (xi − zo + zo)]
i M xj]

E[D2
+ E[(xj − zp + zp)(cid:62)M (xj − zp + zp)] − E[2x(cid:62)
= D2
M (xj, zp)]
= D2

M (zo, zp) + E[D2
M (zo, zp) + 2E[D2

M (xi, zo)] + E[D2
M (xi, zo)]

(2)

The last equation is due to the fact that xi and xj are i.i.d,
since they are from the same class.

By applying the same analysis for the dissimilar pair, we

have

E[D2
+ E[D2

M (xi, xk)] = D2

M (zo, zq) + E[D2

M (xi, zo)]

M (xk, zq)] ≥ D2

M (zo, zq) + E[D2

M (xi, zo)]

(3)

The inequality is because that M is a PSD matrix.

Combining Eqns. 2 and 3, we ﬁnd that the difference
between the distances in the original triplet can be lower
bounded by those in the triplet consisting of latent examples

E[D2
≥ D2

M (xi, xk)] − E[D2
M (zo, zq) − D2

M (xi, xj)]
M (zo, zp) − E[D2

M (xi, zo)]

Therefore, the metric can be learned with the constraints

deﬁned on latent examples such that

∀zo, zp, zq D2

M (zo, zq)−D2

M (zo, zp) ≥ 1+E[D2

M (xi, zo)]

This problem is hard to solve since both the metric and la-
tent examples are the variables to be optimized. Therefore,
we propose to solve it in an alternating way and the detailed
steps are demonstrated below.

3.1. Update z with Upper Bound

When ﬁxing Mk−1, the subproblem at the k-th iteration

becomes

min
z

L(Mk−1, z) =

(cid:88)

(cid:104)

t

1 + E[D2
(cid:124)

(xt

i, zt

o)]
(cid:125)

− (D2
(cid:124)

(zt

o, zt

Mk−1

(zt

o, zt

Mk−1

q) − D2
(cid:123)(cid:122)
b

(4)

Mk−1
(cid:123)(cid:122)
a
(cid:105)

p))
(cid:125)

+

The variable z appears in both the term of margin a and the
term of the triplet difference b, which makes it hard to op-
timize directly. Our strategy is to ﬁnd an appropriate upper
bound for the original problem and solve the simple prob-
lem instead.

Theorem 1. The function L(Mk−1, z) can be upper
bounded by the series of functions (cid:80)
r Fr(z). For the r-
th class, we have

Fr(z) = c1E[D2

Mk−1

(xi, zo)]+c2+c3

D2

Mk−1

(zo, zk−1
o

)

(cid:88)

o

where c1, c2 and c3 are constants and (cid:80)
L(Mk−1, zk−1).

r Fr(zk−1) =

The detailed proof can be found in Section 4.
After removing the constant terms and rearrange the co-
efﬁcients, optimizing Fr(z) is equivalent to optimizing the
following problem

Theorem 2. The function L(M, zk) can be upper bounded
by the function H(M ) which is

H(M ) =

(cid:107)M − Mk−1(cid:107)2

F +

(cid:88)

(cid:104)
1 + E[D2

(xt

i, zt

o)]

Mk−1

λ
2

z∈Rd×mr ,µ:µi,o∈{0,1},(cid:80)

o µi,o=1

min

˜Fr(z) =

(5)

− (D2

M (zt

o, zt

q) − D2

M (zt

o, zt

p))

(cid:88)

(cid:88)

i:y(i)=r

o

(cid:88)

o

µi,oD2

Mk−1

(xi, zo) + γ

D2

Mk−1

(zo, zk−1
o

)

where λ is a constant and H(Mk−1) = L(Mk−1, zk).

t
(cid:105)

+

where µ denotes the membership that assigns a latent ex-
ample for each original example.

Till now, it shows that the original objective L(Mk−1, z)
can be upper bounded by (cid:80)
r Fr(z). Minimizing the upper
bound is similar to k-means but with the distance deﬁned
on the metric Mk−1. So we can solve it by the standard EM
algorithm.

When ﬁxing µ, latent examples can be updated by the

closed-form solution

∀o,

zo =

1
i µi,o + γ

(cid:80)

(cid:88)

(

i

µi,oxi + γzk−1

)

o

(6)

When ﬁxing z, µ just assigns each original example to
its nearest latent example with the distance deﬁned on the
metric Mk−1

∀i, µi,o =

(cid:26) 1
0

o = arg mino D2
o.w.

Mk−1

(xi, zo)

Alg. 1 summarizes the method for solving ˜Fr(z).

Algorithm 1 Algorithm of Updating z

Input: data set {X, Y }, zk−1, Mk−1, γ and S
Initialize z = zk−1
for s = 1 to S do

Fix z and obtain the assignment µ as in Eqn. 7
Fix µ and update z as in Eqn. 6

end for
return zk = z

3.2. Update M with Upper Bound

When ﬁxing zk at the k-th iteration, the subproblem be-

comes

min
M ∈S d×d
+
(cid:88)

L(M, zk) =

(8)

[1 + E[D2
(cid:124)

M (xt
(cid:123)(cid:122)
a

i, zt

o)]
(cid:125)

− (D2
(cid:124)

t

M (zt

o, zt

q) − D2
(cid:123)(cid:122)
b

M (zt

o, zt

p))
]+
(cid:125)

where M also appears in multiple terms. With the similar
procedure, an upper bound can be found to make the opti-
mization simpler.

(7)

Algorithm 2 Algorithm of Updating M

Minimizing H(M ) is a standard DML problem. Since
the number of latent examples zk is small, many existing
DML methods can handle the problem well. In this work
we solve the problem by SGD but sample one epoch active
constraints at each stage. The active constraints contain the
triplets of zk that incur the hinge loss with the distance de-
ﬁned on Mk−1. This strategy enjoys the efﬁciency of SGD
and the efﬁcacy of learning with the active set. To further
improve the efﬁciency, one projection paradigm is adopted
to avoid the expensive PSD projection which costs O(d3).
It performs the PSD projection once at the end of the learn-
ing algorithm and shows to be effective in many applica-
tions [2, 11]. Finally, since the problem is strongly convex,
we apply the α-sufﬁx averaging strategy, which averages
the solutions over the last several iterations, to obtain the
optimal convergence rate [12]. The complete approach for
obtaining Mk is shown in Alg. 2.

Input: data set {X, Y }, zk, Mk−1, δ, λ and S
Initialize M0 = Mk−1
Sample one epoch active constraints A according to zk
and Mk−1
for s = 1 to S do

Randomly sample one constraint from A
Compute the stochastic gradient g = ∇H(M )
Update the metric as M (cid:48)
λs g
Check the Frobenius norm Ms = Πδ(M (cid:48)
s)

s = Ms−1 − 1

end for
Project the learned matrix onto the PSD cone
Mk = ΠP SD( 2
S
return Mk

s=S/2+1 Ms)

(cid:80)S

Alg. 3 summarizes the proposed margin preserving met-
ric learning framework. Different from the standard alter-
nating method, we only optimize the upper bound for each
subproblem. However, the method converges as shown in
the following theorem.
Theorem 3. Let (zk−1, Mk−1) and (zk, Mk) denote the
results obtained by applying the algorithm in Alg. 3 at (k −
1)-th and k-th iterations respectively. Then, we have

L(zk, Mk) ≤ L(zk−1, Mk−1)

which means the proposed method can converge.

Note that (cid:107)Mk−1(cid:107)2 ≤ (cid:107)Mk−1(cid:107)F ≤ δ and z is in the convex
hull of the original data, and the constant c can be set as
c = 8δ maxi (cid:107)xi(cid:107)2
2.

With the similar procedure, we have the bound for the

distance of the similar pair as

M (zo, zp) ≤ D2

D2
+(c + 2)D2

M (zk−1
o
M (zo, zk−1

o

, zk−1
p

)

) + (c + 2)D2

M (zp, zk−1

p

)

(10)

Taking Eqns. 9 and 10 back to the original function
L(Mk−1, z) and using the property of the hinge loss, the
original one can be upper bounded by

G(z) =

[1 + E[D2

M (xt

i, zt

o)] − (D2

M (zt:k−1
o

, zt:k−1
q

)

(cid:88)

t

− D2

M (zt:k−1
o

, zt:k−1
p

))]+ + c3

D2

M (zo, zk−1

o

)

m
(cid:88)

o

where c3 = O(T c) is a constant. By investigating the struc-
ture of this problem, we ﬁnd that each class is independent
in the optimization problem and the subproblem for the r-th
class can be written as

(cid:88)

[E[D2

M (xi, zo)] + ct]+

min
z∈Rd×mr

Gr(z) =

(cid:88)

+ c3

o:y(zo)=r

t:y(zt
o)=r
M (zo, zk−1

o

D2

)

where mr is the number of latent examples for the r-th class
and ct is a constant as

ct = 1 − (D2

M (zt:k−1
o

, zt:k−1
q

) − D2

M (zt:k−1
o

, zt:k−1
p

))

Next we try to upper bound the hinge loss in Gr(z) with a
linear function in the interval of [ct, E[D2
)]+ct],
where the hinge loss incurred by the optimal solution zk is
guaranteed to be in it.

M (xi, zk−1

o

Algorithm 3 Margin Preserving Metric Learning
(MaPML)

Input: data set {X, Y }, δ, m, γ, λ and K
Initialize M0 = I
for k = 1 to K do

Fix Mk−1 and obtain latent examples zk by Alg. 1
Fix zk and update the metric Mk by Alg. 2

end for
return MK and zK

Computational Complexity The proposed method con-
sists of two parts: obtaining latent examples and met-
the cost is linear in
ric learning. For the former one,
the number of latent examples and original examples as
O(mn). For the latter one, the cost of sampling an ac-
tive set dominates the learning procedure. Since the number
of iterations is ﬁxed, the complexity of sampling becomes
min{O(Sm), O(m2)}. Therefore, the whole algorithm can
be linear in the number of latent examples. Note that the ef-
ﬁciency can be further improved with distributed computing
since many components of MaPML can be implemented in
parallel. For example, when updating z, each class is inde-
pendent and all subproblems can be solved simultaneously.

4. Theoretical Analysis

4.1. Proof of Theorem 1

Proof. First, for the distance of the dissimilar pair in term b
of Eqn. 4, we have

, zk−1
q

M (zk−1
)
o
) + 2(zo − zk−1
) − 2(zq − zk−1
)(cid:62)M (zq − zk−1
)

o

q

o

M (zo, zq) = D2
D2
M (zo, zk−1
+ D2
M (zq, zk−1
+ D2
− 2(zo − zk−1
o
≥ D2
M (zk−1
, zk−1
o
q
− 2DM (zq, zk−1

q

q

q
) − 2DM (zo, zk−1

)DM (zk−1

o

o
, zk−1
)
q

)(cid:62)M (zk−1
)(cid:62)M (zk−1

o − zk−1
o − zk−1

q

q

)

)

)DM (zk−1

o

, zk−1
q

)

where zk−1 are latent examples from the last iteration.
We let M denote Mk−1 in this proof for simplicity. The
inequality is from that M is a PSD matrix and can be
decomposed as M = LL(cid:62). Then it is obtained by
applying the Cauchy-Schwarz inequality. With the as-
sumptions that ∀o, DM (zo, zk−1
) is sufﬁciently large and
DM (zk−1
2 , the inequality
can be simpliﬁed as

o
) is bounded by a constant c

, zk−1
q

o

D2
D2

M (zo, zq) ≥
M (zk−1
o

, zk−1
q

) − cD2

M (zo, zk−1

o

) − cD2

M (zq, zk−1

q

)

The assumption is easy to verify since

Figure 2. Illustration of bounding the hinge loss. The hinge loss
between [ct, α + ct] is upper bounded by the linear function de-
noted by the red line.

(9)

M (xi, zk−1

Let α = E[D2

)], which is the expected dis-
tance between the original data of the r-th class and the cor-
responding latent examples from the last iteration, and β be
a constant sufﬁciently large as

o

DM (zk−1

o

, zk−1
q

) ≤ (cid:107)zk−1

o − zk−1

q

(cid:107)2
2(cid:107)Mk−1(cid:107)2

β ≥ − min

ct

t

Then, for each active hinge loss (i.e., α + ct > 0), if

E[D2

M (xi, zo)] ≤ α

(11)

we have

[E[D2

M (xi, zo)] + ct]+
α + ct
α + ct + β

(E[D2

≤

M (xi, zo)] + ct + β)

Fig. 2 illustrates the linear function that can bound the hinge
loss and the proof is straightforward. We will show that the
condition in Eqn. 11 can be satisﬁed throughout the algo-
rithm later.

With the upper bound of the hinge loss, Gr(z) can be

bounded by

Fr(z) = c1E[D2

M (xi, zo)] + c2 + c3

D2

M (zo, zk−1

o

)

(cid:88)

o

where

and

(cid:88)

c1 =

t:y(zt

o)=r

α + ct
αt + ct + β

I(α + ct)

(cid:88)

c2 =

t:y(zt

o)=r

α + ct
αt + ct + β

(ct + β)I(α + ct)

I(·) is an indicator function as I(ν) =

(cid:26) 1
0

ν > 0
o.w.

Finally, we check the condition in Eqn. 11. Let zk denote
latent examples obtained by optimizing ˜F(z) with Alg. 1.
Since we use zk−1 as the starting point to optimize ˜Fr(z),
it is obvious that

˜Fr(zk) ≤ ˜Fr(zk−1)

At the same time, we have

(cid:88)

o

D2

M (zk

o, zk−1
o

) ≥

D2

M (zk−1
o

, zk−1
o

) = 0

(cid:88)

o

It is observed that Eqn. 11 is satisﬁed by combining these
inequalities.

4.2. Proof of Theorem 2

Proof. For the term a in Eqn. 8, we have

M (xi, zo)]

E[D2
= E[D2
≤ E[D2

(xi, zo) + (xi − zo)(cid:62)(M − Mk−1)(xi − zo)]
(cid:107)xi − zo(cid:107)2
(xi, zo)] + max

2(cid:107)M − Mk−1(cid:107)F

Mk−1

Mk−1

≤ E[D2

Mk−1

i
(xi, zo)] + ˜c(cid:107)M − Mk−1(cid:107)2
F

where we assume that (cid:107)M − Mk−1(cid:107)F is sufﬁciently large
and ˜c is a constant which has maxi (cid:107)xi − zo(cid:107)2
2 ≤ ˜c and can
be set as ˜c = 4 maxi (cid:107)xi(cid:107)2
2.

Therefore, the original function L(M, zk) can be upper

bounded by

H(M ) =

(cid:107)M − Mk−1(cid:107)2

F +

(cid:88)

(cid:104)
1 + E[D2

(xt

i, zt

o)]

Mk−1

λ
2

− (D2

M (zt

o, zt

q) − D2

M (zt

o, zt

p))

t
(cid:105)

+

where λ = O(T ˜c).

4.3. Proof of Theorem 3

Proof. When ﬁxing Mk−1 at the k-th iteration, we have
(cid:88)

(cid:88)

L(Mk−1, zk) ≤

Gr(zk) ≤

Fr(zk)

r

r

Fr(zk−1) = L(Mk−1, zk−1)

(cid:88)

≤

r

When ﬁxing zk, we have

L(Mk, zk) ≤ H(Mk) ≤ H(Mk−1) = L(Mk−1, zk)

Therefore, after each iteration, we have

L(Mk, zk) ≤ L(Mk−1, zk−1)

Since the value of L(·) is bounded, the sequence will con-
verge after a ﬁnite number of iterations.

5. Experiments

We conduct the empirical study on four benchmark im-
age data sets. 3-nearest neighbor classiﬁer is applied to ver-
ify the efﬁcacy of the learned metrics from different meth-
ods. The methods in the comparison are summarized as
follows.

• Euclid: 3-NN with Euclidean distance.

• LMNN [17]:

the state-of-the-art DML method that
identiﬁes a set of active triplets with the current met-
ric at each iteration. The active triplets are searched
within 3-nearest neighbors for each example.

• OASIS [2]: an online DML method that receives one
It only updates the

random triplet at each iteration.
metric when the triplet constraint is active.

• HR-SGD [10]: one of the most efﬁcient DML meth-
ods with SGD. We adopt the version that randomly
samples a mini-batch of triplets at each iteration in the
comparison. After sampling, a Bernoulli random vari-
able is generated to decide if updating the current met-
ric or not. With the PSD projection, it guarantees that
the learned metric is in the PSD cone at each iteration.

• MaPMLτ : the proposed method that learns the metric
and latent examples simultaneously, where τ denotes
the ratio between the number of latent examples and
the number of original ones

τ % =

m
n

Different from other methods, 3-NN is implemented
with latent examples as reference points. The method
takes 3-NN with original data is referred as
that
MaPMLτ -O.

The parameters of OASIS, HR-SGD and MaPML are
searched in {10i : i = −3, · · · , 3}. The size of mini-batch
in HR-SGD is set to be 10 as suggested [10]. To train the
model sufﬁciently, the number of iterations for LMNN is set
to be 103 while the number of randomly sampled triplets
is 105 for OASIS and HR-SGD. The number of iterations
for MaPML is set as K = 10 while the number of max-
imal iterations for solving Mk in the subproblem is set as
S = 104, which roughly has the same number of triplets
as OASIS and HR-SGD. All experiments are implemented
on a server with 96 GB memory and 2 Intel Xeon E5-2630
CPUs. Average results with standard deviation over 5 trails
are reported.

5.1. MNIST

First, we evaluate the performance of different algo-
It consists of 60, 000 handwritten
rithms on MNIST [8].
digit images for training and 10, 000 images for test. There
are 10 classes in the data set, which are corresponding to
the digits 0 - 9. Each example is a 28 × 28 grayscale image
which leads to the 784-dimensional features and they are
normalized to the range of [0, 1].

metric with only a small amount of latent examples (i.e.,
10%). Finally, both MaPML and MaPML-O work well
with the metric obtained by MaPML, which veriﬁes that
the learned metric can preserve the large margin property
for both the original and latent data. Note that when the
number of latent examples is small, the performance of k-
NN with latent examples is slightly worse than that with the
whole training set. However, k-NN with latent examples
can be more robust in real-world applications.

To demonstrate the robustness, we conduct another ex-
periment that randomly introduces the zero mean Gaussian
noise (i.e., N (0, σ2)) to each pixel of the original train-
ing images. The standard deviation of the Gaussian noise
is varied in the range of [50/255, 250/255] and τ is ﬁxed
as 10. Fig. 3 (b) summarizes the results.
It shows that
MaPML10 has the comparable performance as MaPML10-
O and LMNN when the noise level is low. However, with
the increasing of the noise, the performance of LMNN
drops dramatically. This can be interpreted by the fact
that the metric learned with the original data has been mis-
led by the noisy information. In contrast, the errors made
by MaPML and MaPML-O increase mildly and it demon-
strates that the learned metric is more robust than the one
learned from the original data. MaPML performs best
among all methods and it is due to the reason that the un-
certainty in latent examples are much less than that in the
original ones. It implies that k-NN with latent examples is
more appropriate for real-world applications with large un-
certainty.

(a) Comparison of error rate

(b) Comparison with Gaussian noise

Figure 3. Comparisons on MNIST.

Fig. 3 (a) compares the performance of different metrics
on the test set. For MaPML, we vary the ratio of latent ex-
amples from 5% to 25%. First of all, It is obvious that the
metrics learned with the active set outperform those from
random triplets. It conﬁrms that the strategy of sampling
triplets randomly can not explore the data set sufﬁciently
due to the extremely large number of triplets. Secondly, the
performance of MaPML10-O is comparable with LMNN,
which shows that the proposed method can learn a good

(a) CPU time for training

(b) Convergence curve of MaPML

Figure 4. Illustration of the efﬁciency of the proposed method.

Then, we compare the CPU time cost by different algo-
rithms to evaluate the efﬁciency. The results can be found
in Fig. 4 (a). First, as expected, all algorithms with SGD
are more efﬁcient than LMNN, which has to compute the
full gradient from the redeﬁned active set at each iteration.
Moreover, the running time of MaPML10 is comparable to
that of HR-SGD, which shows the efﬁciency of MaPML
with the small set of latent examples. Note that OASIS has
the extremely low cost, since it allows the internal metric
to be out of the PSD cone. Fig. 4 (b) illustrates the con-
vergence curve of MaPML and shows that the proposed
method converges fast in practice.

Finally, since we apply the proposed method to the orig-

inal pixel features directly, the learned latent examples can
be recovered as images. Fig. 5 illustrates the learned la-
tent examples and the corresponding examples in the origi-
nal training set. It is obvious that the original examples are
from latent examples with different distortions as claimed.

Figure 5. Illustration of the learned latent examples and corre-
sponding original examples from MNIST. The left column indi-
cates latent examples while ﬁve original images from each corre-
sponding cluster are on the right.

5.2. CIFAR-10 & CIFAR-100

CIFAR-10 contains 10 classes with 50, 000 color images
of size 32 × 32 for training and 10, 000 images for test.
CIFAR-100 has the same number of images in training and
test but for 100 classes [6]. Since deep learning algorithms
show the overwhelming performance on these data sets, we
adopt ResNet18 [4] in Caffe [5], which is pre-trained on Im-
ageNet ILSVRC 2012 data set [14], as the feature extractor
and each image is represented by a 512-dimensional feature
vector.

Table 1. Comparison of error rate (%) on CIFAR-10 and CIFAR-
100.

CIFAR-10
Methods
16.81
Euclid
15.22 ± 0.18
OASIS
15.16 ± 0.22
HR-SGD
13.62 ± 0.12
LMNN
MaPML10-O 13.59 ± 0.14
12.64 ± 0.16
MaPML10

CIFAR-100
42.57
42.46 ± 0.21
42.53 ± 0.19
40.05 ± 0.13
40.49 ± 0.15
34.70 ± 0.16

Table 1 summarizes error rates of methods in the com-
parison. First, we have the same observation as on MNIST,
where the performance of methods adopting active triplets
is much better than that of the methods with randomly sam-
pled triplets. Different from MNIST, MaPML10 outper-
forms LMNN on both of the data sets. It is because that
images in these data sets describe natural objects which con-
tain much more uncertainty than digits in MNIST. Finally,
the performance of MaPML10-O is superior over OASIS
and HR-SGD, which shows the learned metric can work
well with the original data represented by deep features. It
conﬁrms that the large margin property is preserved even
for the original data.

5.3. ImageNet

Finally, we demonstrate that the proposed method can
handle the large-scale data set with ImageNet. ImageNet
ILSVRC 2012 consists of 1, 281, 167 training images and
50, 000 validation data. The same feature extraction proce-
dure as above is applied for each image. Given the large
number of training data, we increase the number of triplets
for OASIS and HR-SGD to 106. Correspondingly, the num-
ber of maximal iterations for solving the subproblem in
MaPML is also raised to 105.

Table 2. Comparison of error rate (%) on ImageNet.

Test error (%)
Methods
35.65
Euclid
36.51 ± 0.08
OASIS
36.15 ± 0.08
HR-SGD
MaPML5-O 35.59 ± 0.03
33.92 ± 0.09
MaPML5

LMNN does not ﬁnish the training after 24 hours so the
result is not reported for it. In contrast, MaPML obtains the
metric within about one hour. The performance of available
methods can be found in Table 2. Since ResNet18 is trained
on ImageNet, the extracted features are optimized for this
data set and it is hard to further improve the performance.
However, with latent examples, MaPML can further reduce
the error rate by 1.7%. It indicates that latent examples with
low uncertainty are more appropriate for the large-scale data
set as the reference points. Note that the small number of
reference points will also accelerate the test phase. For ex-
ample, it costs 0.15s to predict the label of an image with the
original set while the cost is only 0.007s if evaluating with
latent examples. It makes MaPML with latent examples a
potential method for real-time applications.

6. Conclusion

In this work, we propose a framework to learn the dis-
tance metric and latent examples simultaneously. By learn-
ing from a small set of clean latent examples, MaPML can
sample the active triplets efﬁciently and the learning pro-
cedure is robust to the uncertainty in the real-world data.
Moreover, MaPML can preserve the large margin prop-
erty for the original data when learning merely with latent
examples. The empirical study conﬁrms the efﬁcacy and
efﬁciency of MaPML. In the future, we plan to evaluate
MaPML on different tasks (e.g., information retrieval) and
different types of data. Besides, incorporating the proposed
strategy to deep metric learning is also an attractive direc-
tion. It can accelerate the learning for deep embedding and
the resulting latent examples may further improve the per-
formance.

[16] H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese.
Deep metric learning via lifted structured feature em-
bedding. In CVPR, pages 4004–4012, 2016. 2
[17] K. Q. Weinberger and L. K. Saul. Distance metric
learning for large margin nearest neighbor classiﬁca-
tion. JMLR, 10:207–244, 2009. 1, 2, 6

[18] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. J. Russell.
Distance metric learning with application to clustering
with side-information. In NIPS, pages 505–512, 2002.
1, 2

[19] L. Yang and R. Jin. Distance metric learning: a com-

prehensive survery. 2006. 2

[20] H. Ye, D. Zhan, X. Si, and Y. Jiang. Learning maha-
lanobis distance metric: Considering instance distur-
bance helps. In IJCAI, pages 3315–3321, 2017. 3

References

[1] S. Boyd and L. Vandenberghe. Convex Optimization.
Cambridge University Press, New York, NY, USA,
2004. 2

[2] G. Chechik, V. Sharma, U. Shalit, and S. Ben-
gio. Large scale online learning of image similarity
through ranking. JMLR, 11:1109–1135, 2010. 1, 2, 4,
6

[3] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon.
Information-theoretic metric learning. In ICML, pages
209–216, 2007. 1, 2

[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual
learning for image recognition. In CVPR, pages 770–
778, 2016. 8

[5] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell. Caffe:
Convolutional architecture for fast feature embedding.
In ACM MM, pages 675–678, 2014. 8

[6] A. Krizhevsky. Learning multiple layers of features

from tiny images. 2009. 8

[7] B. Kulis. Metric learning: A survey. Foundations and

Trends in Machine Learning, 5(4):287–364, 2013. 2

[8] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324,
1998. 7

[9] Y. Movshovitz-Attias, A. Toshev, T. K. Leung,
S. Ioffe, and S. Singh. No fuss distance metric learn-
ing using proxies. In ICCV, pages 360–368, 2017. 2

[10] Q. Qian, R. Jin, J. Yi, L. Zhang, and S. Zhu. Efﬁcient
distance metric learning by adaptive sampling and
mini-batch stochastic gradient descent (SGD). ML,
99(3):353–372, 2015. 2, 6, 7

[11] Q. Qian, R. Jin, S. Zhu, and Y. Lin. Fine-grained vi-
sual categorization via multi-stage metric learning. In
CVPR, pages 3716–3724, 2015. 4

[12] A. Rakhlin, O. Shamir, and K. Sridharan. Making gra-
dient descent optimal for strongly convex stochastic
optimization. In ICML, 2012. 4

[13] O. Rippel, M. Paluri, P. Doll´ar, and L. D. Bourdev.
Metric learning with adaptive density discrimination.
In ICLR, 2016. 2
[14] O. Russakovsky,

J. Krause,
S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. S. Bernstein, A. C. Berg, and F. Li. Imagenet large
scale visual recognition challenge. IJCV, 115(3):211–
252, 2015. 8

J. Deng, H. Su,

[15] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet:
A uniﬁed embedding for face recognition and cluster-
ing. In CVPR, pages 815–823, 2015. 2

