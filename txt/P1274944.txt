Adversarial Multi-task Learning for Text Classiﬁcation

Pengfei Liu Xipeng Qiu Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{pﬂiu14,xpqiu,xjhuang}@fudan.edu.cn

7
1
0
2
 
r
p
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
2
4
7
5
0
.
4
0
7
1
:
v
i
X
r
a

Abstract

Neural network models have shown their
promising opportunities
for multi-task
learning, which focus on learning the
shared layers to extract the common and
task-invariant features. However, in most
existing approaches, the extracted shared
features are prone to be contaminated by
task-speciﬁc features or the noise brought
by other tasks. In this paper, we propose
an adversarial multi-task learning frame-
work, alleviating the shared and private la-
tent feature spaces from interfering with
each other. We conduct extensive exper-
iments on 16 different text classiﬁcation
tasks, which demonstrates the beneﬁts of
our approach. Besides, we show that the
shared knowledge learned by our proposed
model can be regarded as off-the-shelf
knowledge and easily transferred to new
tasks. The datasets of all 16 tasks are pub-
licly available at http://nlp.fudan.
edu.cn/data/

1

Introduction

Multi-task learning is an effective approach to
improve the performance of a single task with
the help of other related tasks. Recently, neural-
based models for multi-task learning have be-
come very popular, ranging from computer vision
(Misra et al., 2016; Zhang et al., 2014) to natural
language processing (Collobert and Weston, 2008;
Luong et al., 2015), since they provide a conve-
nient way of combining information from multiple
tasks.

However, most existing work on multi-task
learning (Liu et al., 2016c,b) attempts to divide the
features of different tasks into private and shared
spaces, merely based on whether parameters of

(a) Shared-Private Model

(b) Adversarial Shared-Private Model

Figure 1: Two sharing schemes for task A and task
B. The overlap between two black circles denotes
shared space. The blue triangles and boxes repre-
sent the task-speciﬁc features while the red circles
denote the features which can be shared.

some components should be shared. As shown in
Figure 1-(a), the general shared-private model in-
troduces two feature spaces for any task: one is
used to store task-dependent features, the other is
used to capture shared features. The major lim-
itation of this framework is that the shared fea-
ture space could contain some unnecessary task-
speciﬁc features, while some sharable features
could also be mixed in private space, suffering
from feature redundancy.

Taking the following two sentences as exam-
ples, which are extracted from two different senti-
ment classiﬁcation tasks: Movie reviews and Baby
products reviews.

The infantile cart is simple and easy to use.
This kind of humour is infantile and boring.
The word “infantile” indicates negative senti-
ment in Movie task while it is neutral in Baby task.
However, the general shared-private model could
place the task-speciﬁc word “infantile” in a
shared space, leaving potential hazards for other
tasks. Additionally, the capacity of shared space
could also be wasted by some unnecessary fea-
tures.

To address this problem,

in this paper we
propose an adversarial multi-task framework, in
which the shared and private feature spaces are in-

herently disjoint by introducing orthogonality con-
straints. Speciﬁcally, we design a generic shared-
private learning framework to model the text se-
quence. To prevent the shared and private latent
feature spaces from interfering with each other, we
introduce two strategies: adversarial training and
orthogonality constraints. The adversarial training
is used to ensure that the shared feature space sim-
ply contains common and task-invariant informa-
tion, while the orthogonality constraint is used to
eliminate redundant features from the private and
shared spaces.

The contributions of this paper can be summa-

rized as follows.

1. Proposed model divides the task-speciﬁc and
shared space in a more precise way, rather
than roughly sharing parameters.

2. We extend the original binary adversarial
training to multi-class, which not only en-
ables multiple tasks to be jointly trained, but
allows us to utilize unlabeled data.

3. We can condense the shared knowledge
among multiple tasks into an off-the-shelf
neural layer, which can be easily transferred
to new tasks.

2 Recurrent Models for Text

Classiﬁcation

There are many neural sentence models, which
can be used for text modelling, involving recurrent
neural networks (Sutskever et al., 2014; Chung
et al., 2014; Liu et al., 2015a), convolutional neu-
ral networks (Collobert et al., 2011; Kalchbren-
ner et al., 2014), and recursive neural networks
(Socher et al., 2013). Here we adopt recurrent neu-
ral network with long short-term memory (LSTM)
due to their superior performance in various NLP
tasks (Liu et al., 2016a; Lin et al., 2017).

network

(Hochreiter

Long Short-term Memory Long short-term
and
(LSTM)
memory
Schmidhuber, 1997) is a type of recurrent neural
network (RNN) (Elman, 1990), and speciﬁcally
addresses the issue of learning long-term de-
pendencies. While there are numerous LSTM
variants, here we use the LSTM architecture used
by (Jozefowicz et al., 2015), which is similar to
the architecture of (Graves, 2013) but without
peep-hole connections.

We deﬁne the LSTM units at each time step t to
be a collection of vectors in Rd: an input gate it, a

forget gate ft, an output gate ot, a memory cell ct
and a hidden state ht. d is the number of the LSTM
units. The elements of the gating vectors it, ft and
ot are in [0, 1].

The LSTM is precisely speciﬁed as follows.













˜ct
ot
it
ft

=













tanh
σ
σ
σ

(cid:18)

Wp

(cid:21)

(cid:20) xt
ht−1

(cid:19)

+ bp

,

(1)

ct = ˜ct (cid:12) it + ct−1 (cid:12) ft,
ht = ot (cid:12) tanh (ct) ,

(2)

(3)

where xt ∈ Re is the input at the current time step;
Wp ∈ R4d×(d+e) and bp ∈ R4d are parameters of
afﬁne transformation; σ denotes the logistic sig-
moid function and (cid:12) denotes elementwise multi-
plication.

The update of each LSTM unit can be written

precisely as follows:

ht = LSTM(ht−1, xt, θp).

(4)

Here, the function LSTM(·, ·, ·, ·) is a shorthand
for Eq. (1-3), and θp represents all the parameters
of LSTM.

Text Classiﬁcation with LSTM Given a text
sequence x = {x1, x2, · · · , xT }, we ﬁrst use a
lookup layer to get the vector representation (em-
beddings) xi of the each word xi. The output at
the last moment hT can be regarded as the repre-
sentation of the whole sequence, which has a fully
connected layer followed by a softmax non-linear
layer that predicts the probability distribution over
classes.

ˆy = softmax(WhT + b)

(5)

where ˆy is prediction probabilities, W is the
weight which needs to be learned, b is a bias term.
Given a corpus with N training samples
(xi, yi), the parameters of the network are trained
to minimise the cross-entropy of the predicted and
true distributions.

L(ˆy, y) = −

i log(ˆyj
yj
i ),

(6)

N
(cid:88)

C
(cid:88)

i=1

j=1

where yj
probabilities, and C is the class number.

i is the ground-truth label; ˆyj

i is prediction

(a) Fully Shared Model (FS-MTL)

(b) Shared-Private Model (SP-MTL)

Figure 2: Two architectures for learning multiple
tasks. Yellow and gray boxes represent shared and
private LSTM layers respectively.

3 Multi-task Learning for Text

Classiﬁcation

The goal of multi-task learning is to utilizes the
correlation among these related tasks to improve
classiﬁcation by learning tasks in parallel. To facil-
itate this, we give some explanation for notations
used in this paper. Formally, we refer to Dk as a
dataset with Nk samples for task k. Speciﬁcally,

Dk = {(xk

i , yk

i )}Nk
i=1

(7)

where xk
i and yk
sponding label for task k.

i denote a sentence and corre-

3.1 Two Sharing Schemes for Sentence

Modeling

The key factor of multi-task learning is the sharing
scheme in latent feature space. In neural network
based model, the latent features can be regarded as
the states of hidden neurons. Speciﬁc to text clas-
siﬁcation, the latent features are the hidden states
of LSTM at the end of a sentence. Therefore, the
sharing schemes are different in how to group the
shared features. Here, we ﬁrst introduce two shar-
ing schemes with multi-task learning: fully-shared
scheme and shared-private scheme.

Fully-Shared Model (FS-MTL)
In fully-shared
model, we use a single shared LSTM layer to ex-
tract features for all the tasks. For example, given
two tasks m and n, it takes the view that the fea-
tures of task m can be totally shared by task n and
vice versa. This model ignores the fact that some
features are task-dependent. Figure 2a illustrates
the fully-shared model.

Shared-Private Model (SP-MTL) As shown in
Figure 2b, the shared-private model introduces
two feature spaces for each task: one is used to
store task-dependent features, the other is used
to capture task-invariant features. Accordingly, we
can see each task is assigned a private LSTM layer
and shared LSTM layer. Formally, for any sen-
tence in task k, we can compute its shared rep-
t and task-speciﬁc representation hk
resentation sk
t
as follows:

t = LSTM(xt, sk
sk
t = LSTM(xt, hm
hk

t−1, θs),
t−1, θk)

(8)

(9)

where LSTM(., θ) is deﬁned as Eq. (4).

The ﬁnal features are concatenation of the fea-

tures from private space and shared space.

3.2 Task-Speciﬁc Output Layer

For a sentence in task k, its feature h(k), emitted
by the deep muti-task architectures, is ultimately
fed into the corresponding task-speciﬁc softmax
layer for classiﬁcation or other tasks.

The parameters of the network are trained to
minimise the cross-entropy of the predicted and
true distributions on all the tasks. The loss Ltask
can be computed as:

LT ask =

αkL(ˆy(k), y(k))

(10)

K
(cid:88)

k=1

where αk is the weights for each task k respec-
tively. L(ˆy, y) is deﬁned as Eq. 6.

4

Incorporating Adversarial Training

Although the shared-private model separates the
feature space into the shared and private spaces,
there is no guarantee that sharable features can not
exist in private feature space, or vice versa. Thus,
some useful sharable features could be ignored in
shared-private model, and the shared feature space
is also vulnerable to contamination by some task-
speciﬁc information.

Therefore, a simple principle can be applied
into multi-task learning that a good shared feature
space should contain more common information
and no task-speciﬁc information. To address this
problem, we introduce adversarial training into
multi-task framework as shown in Figure 3 (ASP-
MTL).

Task Discriminator Discriminator is used to
map the shared representation of sentences into a
probability distribution, estimating what kinds of
tasks the encoded sentence comes from.

D(sk

T , θD) = softmax(b + Usk
T )

(12)

where U ∈ Rd×d is a learnable parameter and b ∈
Rd is a bias.

Adversarial Loss Different with most existing
multi-task learning algorithm, we add an extra task
adversarial loss LAdv to prevent task-speciﬁc fea-
ture from creeping in to shared space. The task
adversarial loss is used to train a model to pro-
duce shared features such that a classiﬁer cannot
reliably predict the task based on these features.
The original loss of adversarial network is limited
since it can only be used in binary situation. To
overcome this, we extend it to multi-class form,
which allow our model can be trained together
with multiple tasks:

(cid:32)

LAdv = min
θs

λmax
θD

K
(cid:88)

(
k=1

Nk(cid:88)

i=1

i log[D(E(xk))])
dk

(13)

(cid:33)

where dk
i denotes the ground-truth label indicating
the type of the current task. Here, there is a min-
max optimization and the basic idea is that, given
a sentence, the shared LSTM generates a repre-
sentation to mislead the task discriminator. At the
same time, the discriminator tries its best to make
a correct classiﬁcation on the type of task. After
the training phase, the shared feature extractor and
task discriminator reach a point at which both can-
not improve and the discriminator is unable to dif-
ferentiate among all the tasks.

Semi-supervised Learning Multi-task Learning
We notice that the LAdv requires only the input
sentence x and does not require the correspond-
ing label y, which makes it possible to combine
our model with semi-supervised learning. Finally,
in this semi-supervised multi-task learning frame-
work, our model can not only utilize the data from
related tasks, but can employ abundant unlabeled
corpora.

4.3 Orthogonality Constraints

We notice that there is a potential drawback of the
above model. That is, the task-invariant features
can appear both in shared space and private space.
Motivated by recently work(Jia et al., 2010;
Salzmann et al., 2010; Bousmalis et al., 2016)

Figure 3: Adversarial shared-private model. Yel-
low and gray boxes represent shared and private
LSTM layers respectively.

4.1 Adversarial Network

Adversarial networks have recently surfaced and
are ﬁrst used for generative model (Goodfellow
et al., 2014). The goal is to learn a generative dis-
tribution pG(x) that matches the real data distri-
bution Pdata(x) Speciﬁcally, GAN learns a gen-
erative network G and discriminative model D,
in which G generates samples from the genera-
tor distribution pG(x). and D learns to determine
whether a sample is from pG(x) or Pdata(x). This
min-max game can be optimized by the following
risk:

(cid:16)

φ = min

G

max
D

Ex∼Pdata[log D(x)]

+ Ez∼p(z)[log(1 − D(G(z)))]

(11)

(cid:17)

While originally proposed for generating random
samples, adversarial network can be used as a gen-
eral tool to measure equivalence between distri-
butions (Taigman et al., 2016). Formally, (Ajakan
et al., 2014) linked the adversarial loss to the
H-divergence between two distributions and suc-
cessfully achieve unsupervised domain adaptation
with adversarial network. Motivated by theory on
domain adaptation (Ben-David et al., 2010, 2007;
Bousmalis et al., 2016) that a transferable feature
is one for which an algorithm cannot learn to iden-
tify the domain of origin of the input observation.

4.2 Task Adversarial Loss for MTL

Inspired by adversarial networks (Goodfellow
et al., 2014), we proposed an adversarial shared-
private model for multi-task learning, in which a
shared recurrent neural layer is working adversar-
ially towards a learnable multi-layer perceptron,
preventing it from making an accurate prediction
about the types of tasks. This adversarial training
encourages shared space to be more pure and en-
sure the shared representation not be contaminated
by task-speciﬁc features.

Dataset Train Dev. Test Unlab. Avg. L Vocab.

5 Experiment

Books
Elec.
DVD
Kitchen
Apparel
Camera
Health
Music
Toys
Video
Baby
Mag.
Soft.
Sports
IMDB
MR

1400
1398
1400
1400
1400
1397
1400
1400
1400
1400
1300
1370
1315
1400
1400
1400

200
200
200
200
200
200
200
200
200
200
200
200
200
200
200
200

400
400
400
400
400
400
400
400
400
400
400
400
400
400
400
400

2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
2000
475
2000
2000
2000

159
101
173
89
57
130
81
136
90
156
104
117
129
94
269
21

62K
30K
69K
28K
21K
26K
26K
60K
28K
57K
26K
30K
26K
30K
44K
12K

Table 1: Statistics of the 16 datasets. The columns
2-5 denote the number of samples in training, de-
velopment, test and unlabeled sets. The last two
columns represent the average length and vocabu-
lary size of corresponding dataset.

on shared-private latent space analysis, we intro-
duce orthogonality constraints, which penalize re-
dundant latent representations and encourages the
shared and private extractors to encode different
aspects of the inputs.

After exploring many optional methods, we ﬁnd
below loss is optimal, which is used by Bousmalis
et al. (2016) and achieve a better performance:

5.1 Dataset

To make an extensive evaluation, we collect 16
different datasets from several popular review cor-
pora.

The ﬁrst 14 datasets are product reviews, which
contain Amazon product reviews from different
domains, such as Books, DVDs, Electronics, ect.
The goal is to classify a product review as either
positive or negative. These datasets are collected
based on the raw data 1 provided by (Blitzer et al.,
2007). Speciﬁcally, we extract the sentences and
corresponding labels from the unprocessed orig-
inal data 2. The only preprocessing operation of
these sentences is tokenized using the Stanford to-
kenizer 3.

The remaining two datasets are about movie re-
views. The IMDB dataset4 consists of movie re-
views with binary classes (Maas et al., 2011). One
key aspect of this dataset is that each movie review
has several sentences. The MR dataset also con-
sists of movie reviews from rotten tomato website
with two classes 5(Pang and Lee, 2005).

All the datasets in each task are partitioned ran-
domly into training set, development set and test-
ing set with the proportion of 70%, 20% and 10%
respectively. The detailed statistics about all the
datasets are listed in Table 1.

Ldiﬀ =

K
(cid:88)

k=1

(cid:13)
(cid:13)Sk(cid:62)
(cid:13)

Hk(cid:13)
2
(cid:13)
(cid:13)
F

,

5.2 Competitor Methods for Multi-task

(14)

Learning

F is the squared Frobenius norm. Sk
where (cid:107) · (cid:107)2
and Hk are two matrics, whose rows are the out-
put of shared extractor Es(, ; θs) and task-speciﬁc
extrator Ek(, ; θk) of a input sentence.

4.4 Put It All Together

The ﬁnal loss function of our model can be written
as:

L = LT ask + λLAdv + γLDif f

(15)

where λ and γ are hyper-parameter.

The multi-task frameworks proposed by previous
works are various while not all can be applied to
the tasks we focused. Nevertheless, we chose two
most related neural models for multi-task learning
and implement them as competitor methods.

• MT-CNN: This model is proposed by Col-
lobert and Weston (2008) with convolutional
layer, in which lookup-tables are shared par-
tially while other layers are task-speciﬁc.

1https://www.cs.jhu.edu/˜mdredze/

datasets/sentiment/

2Blitzer et al. (2007) also provides two extra processed
datasets with the format of Bag-of-Words, which are not
proper for neural-based models.

3http://nlp.stanford.edu/software/

The networks are trained with backpropagation
and this minimax optimization becomes possible
via the use of a gradient reversal layer (Ganin and
Lempitsky, 2015).

tokenizer.shtml

4https://www.cs.jhu.edu/˜mdredze/
datasets/sentiment/unprocessed.tar.gz
5https://www.cs.cornell.edu/people/

pabo/movie-review-data/.

Task

Books
Electronics
DVD
Kitchen
Apparel
Camera
Health
Music
Toys
Video
Baby
Magazines
Software
Sports
IMDB
MR

AVG

20.5
19.5
18.3
22.0
16.8
14.8
15.5
23.3
16.8
18.5
15.3
10.8
15.3
18.3
18.3
27.3

18.2

Single Task

Multiple Tasks

LSTM BiLSTM sLSTM Avg. MT-DNN MT-CNN

FS-MTL

SP-MTL

ASP-MTL

19.0
21.5
19.5
18.8
14.0
14.0
21.3
22.8
15.3
16.3
16.5
8.5
14.3
16.0
15.0
25.3

17.4

18.0
23.3
22.0
19.5
16.3
15.0
16.5
23.0
16.8
16.3
15.8
12.3
14.5
17.5
18.5
28.0

18.3

19.2
21.4
19.9
20.1
15.7
14.6
17.8
23.0
16.3
17.0
15.9
10.5
14.7
17.3
17.3
26.9

18.0

17.8(−1.4)
18.3(−3.1)
15.8(−4.1)
19.3(−0.8)
15.0(−0.7)
13.8(−0.8)
14.3(−3.5)
15.3(−7.7)
12.3(−4.0)
15.0(−2.0)
12.0(−3.9)
10.5(+0.0)
14.3(−0.4)
16.8(−0.5)
16.8(−0.5)
24.5(−2.4)

15.5(−3.7)
16.8(−4.6)
16.0(−3.9)
16.8(−3.3)
16.3(+0.6)
14.0(−0.6)
12.8(−5.0)
16.3(−6.7)
10.8(−5.5)
18.5(+1.5)
12.3(−3.6)
12.3(+1.8)
13.5(−1.2)
16.0(−1.3)
13.8(−3.5)
25.5(−1.4)

17.5(−1.7)
14.3(−7.1)
16.5(−3.4)
14.0(−6.1)
15.5(−0.2)
13.5(−1.1)
12.0(−5.8)
18.8(−4.2)
15.5(−0.8)
16.3(−0.7)
12.0(−3.9)
7.5(−3.0)
13.8(−0.9)
14.5(−2.8)
17.5(+0.2)
25.3(−1.6)

18.8(−0.4)
15.3(−6.1)
16.0(−3.9)
14.8(−5.3)
13.5(−2.2)
12.0(−2.6)
12.8(−5.0)
17.0(−6.0)
14.8(−1.5)
16.8(−0.2)
13.3(−2.6)
8.0(−2.5)
13.0(−1.7)
12.8(−4.5)
15.3(−2.0)
24.0(−2.9)

16.0(−3.2)
13.2(−8.2)
14.5(−5.4)
13.8(−6.3)
13.0(−2.7)
10.8(−3.8)
11.8(−6.0)
17.5(−5.5)
12.0(−4.3)
15.5(−1.5)
11.8(−4.1)
7.8(−2.7)
12.8(−1.9)
14.3(−3.0)
14.5(−2.8)
23.3(−3.6)

15.7(−2.2)

15.5(−2.5)

15.3(−2.7)

14.9(−3.1)

13.9(−4.1)

Table 2: Error rates of our models on 16 datasets against typical baselines. The numbers in brackets
represent the improvements relative to the average performance (Avg.) of three single task baselines.

• MT-DNN: The model is proposed by Liu
et al. (2015b) with bag-of-words input and
multi-layer perceptrons, in which a hidden
layer is shared.

5.3 Hyperparameters

The word embeddings for all of the models are ini-
tialized with the 200d GloVe vectors ((Pennington
et al., 2014)). The other parameters are initialized
by randomly sampling from uniform distribution
in [−0.1, 0.1]. The mini-batch size is set to 16.

For each task, we take the hyperparameters
which achieve the best performance on the devel-
opment set via an small grid search over com-
binations of the initial learning rate [0.1, 0.01],
λ ∈ [0.01, 0.1], and γ ∈ [0.01, 0.1]. Finally, we
chose the learning rate as 0.01, λ as 0.05 and γ as
0.01.

5.4 Performance Evaluation

Table 2 shows the error rates on 16 text clas-
siﬁcation tasks. The column of “Single Task”
shows the results of vanilla LSTM, bidirectional
LSTM (BiLSTM), stacked LSTM (sLSTM) and
the average error rates of previous three models.
The column of “Multiple Tasks” shows the re-
sults achieved by corresponding multi-task mod-
els. From this table, we can see that the perfor-
mance of most tasks can be improved with a large
margin with the help of multi-task learning, in
which our model achieves the lowest error rates.
More concretely, compared with SP-MTL, ASP-

MTL achieves 4.1% average improvement sur-
passing SP-MTL with 1.0%, which indicates the
importance of adversarial learning. It is notewor-
thy that for FS-MTL, the performances of some
tasks are degraded, since this model puts all pri-
vate and shared information into a uniﬁed space.

5.5 Shared Knowledge Transfer

With the help of adversarial learning, the shared
feature extractor Es can generate more pure task-
invariant representations, which can be considered
as off-the-shelf knowledge and then be used for
unseen new tasks.

To test the transferability of our learned shared
extractor, we also design an experiment, in which
we take turns choosing 15 tasks to train our model
MS with multi-task learning, then the learned
shared layer are transferred to a second network
MT that is used for the remaining one task. The
parameters of transferred layer are kept frozen,
and the rest of parameters of the network MT are
randomly initialized.

More formally, we investigate two mechanisms
towards the transferred shared extractor. As shown
in Figure 4. The ﬁrst one Single Channel (SC)
model consists of one shared feature extractor Es
from MS, then the extracted representation will
be sent to an output layer. By contrast, the Bi-
Channel (BC) model introduces an extra LSTM
layer to encode more task-speciﬁc information. To
evaluate the effectiveness of our introduced adver-
sarial training framework, we also make a compar-

Source Tasks

φ (Books)
φ (Electronics)
φ (DVD)
φ (Kitchen)
φ (Apparel)
φ (Camera)
φ (Health)
φ (Music)
φ (Toys)
φ (Video)
φ (Baby)
φ (Magazines)
φ (Software)
φ (Sports)
φ (IMDB)
φ (MR)

AVG

20.5
19.5
18.3
22.0
16.8
14.8
15.5
23.3
16.8
18.5
15.3
10.8
15.3
18.3
18.3
27.3

18.2

Single Task

Transfer Models

LSTM BiLSTM sLSTM Avg.

SP-MTL-SC SP-MTL-BC ASP-MTL-SC ASP-MTL-BC

19.0
21.5
19.5
18.8
14.0
14.0
21.3
22.8
15.3
16.3
16.5
8.5
14.3
16.0
15.0
25.3

17.4

18.0
23.3
22.0
19.5
16.3
15.0
16.5
23.0
16.8
16.3
15.8
12.3
14.5
17.5
18.5
28.0

18.3

19.2
21.4
19.9
20.1
15.7
14.6
17.8
23.0
16.3
17.0
15.9
10.5
14.7
17.3
17.3
26.9

18.0

17.8(−1.4)
15.3(−6.1)
14.8(−5.1)
15.0(−5.1)
14.8(−0.9)
13.3(−1.3)
14.5(−3.3)
20.0(−3.0)
13.8(−2.5)
14.3(−2.7)
16.5(+0.6)
10.5(+0.0)
13.0(−1.7)
16.3(−1.0)
12.8(−4.5)
26.0(−0.9)

16.3(−2.9)
14.8(−6.6)
15.5(−4.4)
16.3(−3.8)
12.0(−3.7)
12.5(−2.1)
14.3(−3.5)
17.8(−5.2)
12.5(−3.8)
15.0(−2.0)
16.8(+0.9)
10.3(−0.2)
12.8(−1.9)
16.3(−1.0)
12.8(−4.5)
26.5(−0.4)

15.6(−2.4)

15.2(−2.8)

16.8(−2.4)
17.8(−3.6)
14.5(−5.4)
16.3(−3.8)
12.5(−3.2)
11.8(−2.8)
12.3(−5.5)
17.5(−5.5)
13.0(−3.3)
14.8(−2.2)
13.5(−2.4)
8.8(−1.7)
14.5(−0.2)
13.3(−4.0)
12.5(−4.8)
24.8(−2.1)

14.7(−3.3)

16.3(−2.9)
16.8(−4.6)
14.3(−5.6)
15.0(−5.1)
13.8(−1.9)
10.3(−4.3)
13.5(−4.3)
18.3(−4.7)
11.8(−4.5)
14.8(−2.2)
12.0(−3.9)
9.5(−1.0)
11.8(−2.9)
13.5(−3.8)
13.3(−4.0)
23.5(−3.4)

14.3(−3.7)

Table 3: Error rates of our models on 16 datasets against vanilla multi-task learning. φ (Books) means
that we transfer the knowledge of the other 15 tasks to the target task Books.

(a) Single Channel

(b) Bi-Channel

Figure 4: Two transfer strategies using a pre-
trained shared LSTM layer. Yellow box denotes
shared feature extractor Es trained by 15 tasks.

ison with vanilla multi-task learning method.

Results and Analysis As shown in Table 3, we
can see the shared layer from ASP-MTL achieves
a better performance compared with SP-MTL. Be-
sides, for the two kinds of transfer strategies, the
Bi-Channel model performs better. The reason is
that the task-speciﬁc layer introduced in the Bi-
Channel model can store some private features.
Overall, the results indicate that we can save the
existing knowledge into a shared recurrent layer
using adversarial multi-task learning, which is
quite useful for a new task.

5.6 Visualization

To get an intuitive understanding of how the intro-
duced orthogonality constraints worked compared
with vanilla shared-private model, we design an
experiment to examine the behaviors of neurons
from private layer and shared layer. More con-
cretely, we refer to htj as the activation of the j-
neuron at time step t, where t ∈ {1, . . . , n} and

j ∈ {1, . . . , d}. By visualizing the hidden state
hj and analyzing the maximum activation, we can
ﬁnd what kinds of patterns the current neuron fo-
cuses on.

Figure 5 illustrates this phenomenon. Here, we
randomly sample a sentence from the validation
set of Baby task and analyze the changes of the
predicted sentiment score at different time steps,
which are obtained by SP-MTL and our proposed
model. Additionally, to get more insights into
how neurons in shared layer behave diversely
towards different input word, we visualize the
activation of two typical neurons. For the positive
“Five stars, my baby can
sentence
fall asleep soon in the stroller”,
both models capture the informative pattern
“Five stars” 6. However, SP-MTL makes a
wrong prediction due to misunderstanding of the
word “asleep”.

By contrast, our model makes a correct predic-
tion and the reason can be inferred from the acti-
vation of Figure 5-(b), where the shared layer of
SP-MTL is so sensitive that many features related
to other tasks are included, such as ”asleep”,
which misleads the ﬁnal prediction. This indicates
the importance of introducing adversarial learning
to prevent the shared layer from being contami-
nated by task-speciﬁc features.

We also list some typical patterns captured by

6For this case, the vanilla LSTM also give a wrong answer

due to ignoring the feature “Five stars”.

(a) Predicted Sentiment Score by Two Models

(b) Behaviours of Neuron hs

18 and hs

21

Figure 5: (a) The change of the predicted sentiment score at different time steps. Y-axis represents the
sentiment score, while X-axis represents the input words in chronological order. The darker grey horizon-
tal line gives a border between the positive and negative sentiments. (b) The purple heat map describes the
behaviour of neuron hs
18 from shared layer of SP-MTL, while the blue one is used to show the behaviour
of neuron hs

21, which belongs to the shared layer of our model.

Model

Shared Layer

Task-Movie

Task-Baby

SP-MTL

ASP-MTL

good, great
bad, love,
simple, cut,
slow, cheap,
infantile

good, great,
love, bad
poor

good, great,
well-directed,
pointless, cut,
cheap, infantile

love, bad,
cute, safety,
mild, broken
simple

well-directed,
pointless, cut,
cheap, infantile

cute, safety,
mild, broken
simple

Table 4: Typical patterns captured by shared layer
and task-speciﬁc layer of SP-MTL and ASP-MTL
models on Movie and Baby tasks.

neurons from shared layer and task-speciﬁc layer
in Table 4, and we have observed that: 1) for
SP-MTL, if some patterns are captured by task-
speciﬁc layer, they are likely to be placed into
shared space. Clearly, suppose we have many tasks
to be trained jointly, the shared layer bear much
pressure and must sacriﬁce substantial amount
of capacity to capture the patterns they actu-
ally do not need. Furthermore, some typical task-
invariant features also go into task-speciﬁc layer.
2) for ASP-MTL, we ﬁnd the features captured by
shared and task-speciﬁc layer have a small amount
of intersection, which allows these two kinds of
layers can work effectively.

6 Related Work

There are two threads of related work. One thread
is multi-task learning with neural network. Neu-
ral networks based multi-task learning has been
proven effective in many NLP problems (Col-
lobert and Weston, 2008; Glorot et al., 2011).

Liu et al. (2016c) ﬁrst utilizes different LSTM
layers to construct multi-task learning framwork

for text classiﬁcation. Liu et al. (2016b) proposes
a generic multi-task framework, in which different
tasks can share information by an external mem-
ory and communicate by a reading/writing mech-
anism. These work has potential limitation of just
learning a shared space solely on sharing param-
eters, while our model introduce two strategies to
learn the clear and non-redundant shared-private
space.

Another thread of work is adversarial network.
Adversarial networks have recently surfaced as a
general tool measure equivalence between distri-
butions and it has proven to be effective in a va-
riety of tasks. Ajakan et al. (2014); Bousmalis
et al. (2016) applied adverarial training to domain
adaptation, aiming at transferring the knowledge
of one source domain to target domain. Park and
Im (2016) proposed a novel approach for multi-
modal representation learning which uses adver-
sarial back-propagation concept.

Different from these models, our model aims to
ﬁnd task-invariant sharable information for mul-
tiple related tasks using adversarial training strat-
egy. Moreover, we extend binary adversarial train-
ing to multi-class, which enable multiple tasks to
be jointly trained.

7 Conclusion

In this paper, we have proposed an adversarial
multi-task learning framework, in which the task-
speciﬁc and task-invariant features are learned
non-redundantly, therefore capturing the shared-
private separation of different
tasks. We have
demonstrated the effectiveness of our approach by
applying our model to 16 different text classiﬁca-
tion tasks. We also perform extensive qualitative

analysis, deriving insights and indirectly explain-
ing the quantitative improvements in the overall
performance.

Yaroslav Ganin and Victor Lempitsky. 2015. Unsu-
pervised domain adaptation by backpropagation. In
Proceedings of the 32nd International Conference
on Machine Learning (ICML-15). pages 1180–1189.

Acknowledgments

We would like to thank the anonymous review-
ers for their valuable comments and thank Kaiyu
Qian, Gang Niu for useful discussions. This work
was partially funded by National Natural Sci-
ence Foundation of China (No. 61532011 and
61672162), the National High Technology Re-
search and Development Program of China (No.
2015AA015408), Shanghai Municipal Science
and Technology Commission (No. 16JC1420401).

References

Hana Ajakan, Pascal Germain, Hugo Larochelle,
Franc¸ois Laviolette, and Mario Marchand. 2014.
Domain-adversarial neural networks. arXiv preprint
arXiv:1412.4446 .

Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine learning 79(1-2):151–175.

Shai Ben-David, John Blitzer, Koby Crammer, Fer-
nando Pereira, et al. 2007. Analysis of represen-
tations for domain adaptation. Advances in neural
information processing systems 19:137.

John Blitzer, Mark Dredze, Fernando Pereira, et al.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classiﬁ-
cation. In ACL. volume 7, pages 440–447.

Konstantinos Bousmalis, George Trigeorgis, Nathan
Silberman, Dilip Krishnan, and Dumitru Erhan.
2016. Domain separation networks. In Advances in
Neural Information Processing Systems. pages 343–
351.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .

Ronan Collobert and Jason Weston. 2008. A uniﬁed
architecture for natural language processing: Deep
In Pro-
neural networks with multitask learning.
ceedings of ICML.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The JMLR 12:2493–2537.

Jeffrey L Elman. 1990. Finding structure in time. Cog-

nitive science 14(2):179–211.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
In Pro-
classiﬁcation: A deep learning approach.
ceedings of the 28th International Conference on
Machine Learning (ICML-11). pages 513–520.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
In Advances in Neural Information
versarial nets.
Processing Systems. pages 2672–2680.

Alex Graves. 2013.

recurrent neural networks.
arXiv:1308.0850 .

Generating sequences with
arXiv preprint

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Yangqing Jia, Mathieu Salzmann, and Trevor Darrell.
2010. Factorized latent spaces with structured spar-
sity. In Advances in Neural Information Processing
Systems. pages 982–990.

Rafal

Jozefowicz, Wojciech Zaremba,

and Ilya
Sutskever. 2015. An empirical exploration of recur-
In Proceedings of The
rent network architectures.
32nd International Conference on Machine Learn-
ing.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130 .

Pengfe Liu, Xipeng Qiu, Jifan Chen, and Xuanjing
Huang. 2016a. Deep fusion LSTMs for text seman-
tic matching. In Proceedings of ACL.

PengFei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu,
and Xuanjing Huang. 2015a. Multi-timescale long
short-term memory neural network for modelling
In Proceedings of the
sentences and documents.
Conference on EMNLP.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016b.
Deep multi-task learning with shared memory.
In
Proceedings of EMNLP.

PengFei Liu, Xipeng Qiu, and Xuanjing Huang. 2016c.
Recurrent neural network for text classiﬁcation with
multi-task learning. In Proceedings of International
Joint Conference on Artiﬁcial Intelligence.

Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,
Kevin Duh, and Ye-Yi Wang. 2015b. Representation
learning using multi-task deep neural networks for
semantic classiﬁcation and information retrieval. In
NAACL.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2015. Multi-task
arXiv preprint
sequence to sequence learning.
arXiv:1511.06114 .

Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis.
In
Proceedings of the ACL. pages 142–150.

Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and
Martial Hebert. 2016. Cross-stitch networks for
In Proceedings of the IEEE
multi-task learning.
Conference on Computer Vision and Pattern Recog-
nition. pages 3994–4003.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
In Proceedings of
with respect to rating scales.
the 43rd annual meeting on association for compu-
tational linguistics. Association for Computational
Linguistics, pages 115–124.

Gwangbeen Park and Woobin Im. 2016.

Image-text
multi-modal representation learning by adversarial
backpropagation. arXiv preprint arXiv:1612.08354
.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. Proceedings of the EMNLP 12:1532–
1543.

Mathieu Salzmann, Carl Henrik Ek, Raquel Urtasun,
and Trevor Darrell. 2010. Factorized orthogonal la-
tent spaces. In AISTATS. pages 701–708.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in NIPS. pages 3104–3112.

Yaniv Taigman, Adam Polyak, and Lior Wolf.
2016. Unsupervised cross-domain image genera-
tion. arXiv preprint arXiv:1611.02200 .

Zhanpeng Zhang, Ping Luo, Chen Change Loy, and
Xiaoou Tang. 2014. Facial landmark detection by
deep multi-task learning. In European Conference
on Computer Vision. Springer, pages 94–108.

