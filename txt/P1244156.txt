7
1
0
2
 
t
c
O
 
5
 
 
]

V
C
.
s
c
[
 
 
2
v
8
5
4
3
0
.
6
0
7
1
:
v
i
X
r
a

Deep Learning for Precipitation Nowcasting:
A Benchmark and A New Model

Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{xshiab,zgaoag,lelausen,hwangaz,dyyeung}@cse.ust.hk

Wai-kin Wong, Wang-chun Woo
Hong Kong Observatory
Hong Kong, China
{wkwong,wcwoo}@hko.gov.hk

Abstract

With the goal of making high-resolution forecasts of regional rainfall, precipita-
tion nowcasting has become an important and fundamental technology underlying
various public services ranging from rainstorm warnings to ﬂight safety. Recently,
the Convolutional LSTM (ConvLSTM) model has been shown to outperform tradi-
tional optical ﬂow based methods for precipitation nowcasting, suggesting that deep
learning models have a huge potential for solving the problem. However, the con-
volutional recurrence structure in ConvLSTM-based models is location-invariant
while natural motion and transformation (e.g., rotation) are location-variant in gen-
eral. Furthermore, since deep-learning-based precipitation nowcasting is a newly
emerging area, clear evaluation protocols have not yet been established. To address
these problems, we propose both a new model and a benchmark for precipitation
nowcasting. Speciﬁcally, we go beyond ConvLSTM and propose the Trajectory
GRU (TrajGRU) model that can actively learn the location-variant structure for
recurrent connections. Besides, we provide a benchmark that includes a real-world
large-scale dataset from the Hong Kong Observatory, a new training loss, and a
comprehensive evaluation protocol to facilitate future research and gauge the state
of the art.

1

Introduction

Precipitation nowcasting refers to the problem of providing very short range (e.g., 0-6 hours) forecast
of the rainfall intensity in a local region based on radar echo maps1, rain gauge and other observation
data as well as the Numerical Weather Prediction (NWP) models. It signiﬁcantly impacts the daily
lives of many and plays a vital role in many real-world applications. Among other possibilities,
it helps to facilitate drivers by predicting road conditions, enhances ﬂight safety by providing
weather guidance for regional aviation, and avoids casualties by issuing citywide rainfall alerts.
In addition to the inherent complexities of the atmosphere and relevant dynamical processes, the
ever-growing need for real-time, large-scale, and ﬁne-grained precipitation nowcasting poses extra
challenges to the meteorological community and has aroused research interest in the machine learning
community [25, 27].

1The radar echo maps are Constant Altitude Plan Position Indicator (CAPPI) images which can be converted

to rainfall intensity maps using the Marshall-Palmer relationship or Z-R relationship [21].

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

The conventional approaches to precipitation nowcasting used by existing operational systems rely
on optical ﬂow [30]. In a modern day nowcasting system, the convective cloud movements are
ﬁrst estimated from the observed radar echo maps by optical ﬂow and are then used to predict the
future radar echo maps using semi-Lagrangian advection. However, these methods are unsupervised
from the machine learning point of view in that they do not take advantage of the vast amount of
existing radar echo data. Recently, progress has been made by utilizing supervised deep learning [17]
techniques for precipitation nowcasting. Shi et al. [25] formulated precipitation nowcasting as a
spatiotemporal sequence forecasting problem and proposed the Convolutional Long Short-Term
Memory (ConvLSTM) model, which extends the LSTM [8] by having convolutional structures in both
the input-to-state and state-to-state transitions, to solve the problem. Using the radar echo sequences
for model training, the authors showed that ConvLSTM is better at capturing the spatiotemporal
correlations than the fully-connected LSTM and gives more accurate predictions than the Real-time
Optical ﬂow by Variational methods for Echoes of Radar (ROVER) algorithm [30] currently used by
the Hong Kong Observatory (HKO).

However, despite their pioneering effort in this interesting direction, the paper has some deﬁciencies.
First, the deep learning model is only evaluated on a relatively small dataset containing 97 rainy
days and only the nowcasting skill score at the 0.5mm/h rain-rate threshold is compared. As
real-world precipitation nowcasting systems need to pay additional attention to heavier rainfall
events such as rainstorms which cause more threat to the society, the performance at the 0.5mm/h
threshold (indicating raining or not) alone is not sufﬁcient for demonstrating the algorithm’s overall
performance [30]. In fact, as the area Deep Learning for Precipitation Nowcasting is still in its early
stage, it is not clear how models should be evaluated to meet the need of real-world applications.
Second, although the convolutional recurrence structure used in ConvLSTM is better than the fully-
connected recurrent structure in capturing spatiotemporal correlations, it is not optimal and leaves
room for improvement. For motion patterns like rotation and scaling, the local correlation structure
of consecutive frames will be different for different spatial locations and timestamps. It is thus
inefﬁcient to use convolution which uses a location-invariant ﬁlter to represent such location-variant
relationship. Previous attempts have tried to solve the problem by revising the output of a recurrent
neural network (RNN) from the raw prediction to be some location-variant transformation of the
input, like optical ﬂow or dynamic local ﬁlter [5, 3]. However, not much research has been conducted
to address the problem by revising the recurrent structure itself.

In this paper, we aim to address these two problems by proposing both a benchmark and a new
model for precipitation nowcasting. For the new benchmark, we build the HKO-7 dataset which
contains radar echo data from 2009 to 2015 near Hong Kong. Since the radar echo maps arrive in
a stream in the real-world scenario, the nowcasting algorithms can adopt online learning to adapt
to the newly emerging patterns dynamically. To take into account this setting, we use two testing
protocols in our benchmark: the ofﬂine setting in which the algorithm can only use a ﬁxed window
of the previous radar echo maps and the online setting in which the algorithm is free to use all the
historical data and any online learning algorithm. Another issue for the precipitation nowcasting
task is that the proportions of rainfall events at different rain-rate thresholds are highly imbalanced.
Heavier rainfall occurs less often but has a higher real-world impact. We thus propose the Balanced
Mean Squared Error (B-MSE) and Balanced Mean Absolute Error (B-MAE) measures for training
and evaluation, which assign more weights to heavier rainfalls in the calculation of MSE and MAE.
We empirically ﬁnd that the balanced variants of the loss functions are more consistent with the
overall nowcasting performance at multiple rain-rate thresholds than the original loss functions.
Moreover, our experiments show that training with the balanced loss functions is essential for deep
learning models to achieve good performance at higher rain-rate thresholds. For the new model, we
propose the Trajectory Gated Recurrent Unit (TrajGRU) model which uses a subnetwork to output the
state-to-state connection structures before state transitions. TrajGRU allows the state to be aggregated
along some learned trajectories and thus is more ﬂexible than the Convolutional GRU (ConvGRU) [2]
whose connection structure is ﬁxed. We show that TrajGRU outperforms ConvGRU, Dynamic Filter
Network (DFN) [3] as well as 2D and 3D Convolutional Neural Networks (CNNs) [22, 29] in both a
synthetic MovingMNIST++ dataset and the HKO-7 dataset.

Using the new dataset, testing protocols, training loss and model, we provide extensive empirical
evaluation of seven models, including a simple baseline model which always predicts the last frame,
two optical ﬂow based models (ROVER and its nonlinear variant), and four representative deep
learning models (TrajGRU, ConvGRU, 2D CNN, and 3D CNN). We also provide a large-scale

2

benchmark for precipitation nowcasting. Our experimental validation shows that (1) all the deep
learning models outperform the optical ﬂow based models, (2) TrajGRU attains the best overall
performance among all the deep learning models, and (3) after applying online ﬁne-tuning, the models
tested in the online setting consistently outperform those in the ofﬂine setting. To the best of our
knowledge, this is the ﬁrst comprehensive benchmark of deep learning models for the precipitation
nowcasting problem. Besides, since precipitation nowcasting can be viewed as a video prediction
problem [24, 29], our work is the ﬁrst to provide evidence and justiﬁcation that online learning could
potentially be helpful for video prediction in general.

2 Related Work

Deep learning for precipitation nowcasting and video prediction For the precipitation nowcast-
ing problem, the reﬂectivity factors in radar echo maps are ﬁrst transformed to grayscale images
before being fed into the prediction algorithm [25]. Thus, precipitation nowcasting can be viewed
as a type of video prediction problem with a ﬁxed “camera”, which is the weather radar. Therefore,
methods proposed for predicting future frames in natural videos are also applicable to precipitation
nowcasting and are related to our paper. There are three types of general architecture for video
prediction: RNN based models, 2D CNN based models, and 3D CNN based models. Ranzato et
al. [24] proposed the ﬁrst RNN based model for video prediction, which uses a convolutional RNN
with 1 × 1 state-state kernel to encode the observed frames. Srivastava et al. [26] proposed the LSTM
encoder-decoder network which uses one LSTM to encode the input frames and another LSTM to
predict multiple frames ahead. The model was generalized in [25] by replacing the fully-connected
LSTM with ConvLSTM to capture the spatiotemporal correlations better. Later, Finn et al. [5] and De
Brabandere et al. [3] extended the model in [25] by making the network predict the transformation of
the input frame instead of directly predicting the raw pixels. Ruben et al. [28] proposed to use both an
RNN that captures the motion and a CNN that captures the content to generate the prediction. Along
with RNN based models, 2D and 3D CNN based models were proposed in [22] and [29] respectively.
Mathieu et al. [22] treated the frame sequence as multiple channels and applied 2D CNN to generate
the prediction while [29] treated them as the depth and applied 3D CNN. Both papers show that
Generative Adversarial Network (GAN) [6] is helpful for generating sharp predictions.

Structured recurrent connection for spatiotemporal modeling From a higher-level perspective,
precipitation nowcasting and video prediction are intrinsically spatiotemporal sequence forecasting
problems in which both the input and output are spatiotemporal sequences [25]. Recently, there is
a trend of replacing the fully-connected structure in the recurrent connections of RNN with other
topologies to enhance the network’s ability to model the spatiotemporal relationship. Other than the
ConvLSTM which replaces the full-connection with convolution and is designed for dense videos, the
SocialLSTM [1] and the Structural-RNN (S-RNN) [13] have been proposed sharing a similar notion.
SocialLSTM deﬁnes the topology based on the distance between different people and is designed for
human trajectory prediction while S-RNN deﬁnes the structure based on the given spatiotemporal
graph. All these models are different from our TrajGRU in that our model actively learns the recurrent
connection structure. Liang et al. [19] have proposed the Structure-evolving LSTM, which also has the
ability to learn the connection structure of RNNs. However, their model is designed for the semantic
object parsing task and learns how to merge the graph nodes automatically. It is thus different from
TrajGRU which aims at learning the local correlation structure for spatiotemporal data.

Benchmark for video tasks There exist benchmarks for several video tasks like online object
tracking [31] and video object segmentation [23]. However, there is no benchmark for the precipitation
nowcasting problem, which is also a video task but has its unique properties since radar echo map is
a completely different type of data and the data is highly imbalanced (as mentioned in Section 1).
The large-scale benchmark created as part of this work could help ﬁll the gap.

3 Model

In this section, we present our new model for precipitation nowcasting. We ﬁrst introduce the general
encoding-forecasting structure used in this paper. Then we review the ConvGRU model and present
our new TrajGRU model.

3

3.1 Encoding-forecasting Structure

t , H2

t , . . . , Hn

We adopt a similar formulation of the precipitation nowcasting problem as in [25]. Assume that the
radar echo maps form a spatiotemporal sequence (cid:104)I1, I2, . . .(cid:105). At a given timestamp t, our model gen-
erates the most likely K-step predictions, ˆIt+1, ˆIt+2, . . . , ˆIt+K, based on the previous J observations
including the current one: It−J+1, It−J+2, . . . , It. Our encoding-forecasting network ﬁrst encodes
the observations into n layers of RNN states: H1
t = h(It−J+1, It−J+2, . . . , It), and
then uses another n layers of RNNs to generate the predictions based on these encoded states:
ˆIt+1, ˆIt+2, . . . , ˆIt+K = g(H1
t ). Figure 1 illustrates our encoding-forecasting structure
for n = 3, J = 2, K = 2. We insert downsampling and upsampling layers between the RNNs, which
are implemented by convolution and deconvolution with stride. The reason to reverse the order of the
forecasting network is that the high-level states, which have captured the global spatiotemporal repre-
sentation, could guide the update of the low-level states. Moreover, the low-level states could further
inﬂuence the prediction. This structure is more reasonable than the previous structure [25] which does
not reverse the link of the forecasting network because we are free to plug in additional RNN layers
on top and no skip-connection is required to aggregate the low-level information. One can choose any
type of RNNs like ConvGRU or our newly proposed TrajGRU in this general encoding-forecasting
structure as long as their states correspond to tensors.

t , . . . , Hn

t , H2

3.2 Convolutional GRU

The main formulas of the ConvGRU used in this paper are given as follows:

Zt = σ(Wxz ∗ Xt + Whz ∗ Ht−1),
Rt = σ(Wxr ∗ Xt + Whr ∗ Ht−1),
H(cid:48)
Ht = (1 − Zt) ◦ H(cid:48)

t + Zt ◦ Ht−1.

t = f (Wxh ∗ Xt + Rt ◦ (Whh ∗ Ht−1)),

(1)

The bias terms are omitted for notational simplicity. ‘∗’ is the convolution operation and ‘◦’ is the
t ∈ RCh×H×W are the memory state, reset gate, update gate,
Hadamard product. Here, Ht, Rt, Zt, H(cid:48)
and new information, respectively. Xt ∈ RCi×H×W is the input and f is the activation, which is
chosen to be leaky ReLU with negative slope equals to 0.2 [20] througout the paper. H, W are the
height and width of the state and input tensors and Ch, Ci are the channel sizes of the state and input
tensors, respectively. Every time a new input arrives, the reset gate will control whether to clear the
previous state and the update gate will control how much the new information will be written to the
state.

3.3 Trajectory GRU

When used for capturing spatiotemporal correlations, the deﬁciency of ConvGRU and other
ConvRNNs is that the connection structure and weights are ﬁxed for all the locations. The convolution
operation basically applies a location-invariant ﬁlter to the input. If the inputs are all zero and the
reset gates are all one, we could rewrite the computation process of the new information at a speciﬁc
location (i, j) at timestamp t, i.e, H(cid:48)

t,:,i,j, as follows:

|N h
i,j |
(cid:88)

l=1

H(cid:48)

t,:,i,j = f (Whhconcat((cid:104)Ht−1,:,p,q | (p, q) ∈ N h

i,j(cid:105))) = f (

Wl

hhHt−1,:,pl,i,j ,ql,i,j ).

(2)

Here, N h
i,j is the ordered neighborhood set at location (i, j) deﬁned by the hyperparameters of the
state-to-state convolution such as kernel size, dilation and padding [32]. (pl,i,j, ql,i,j) is the lth
neighborhood location of position (i, j). The concat(·) function concatenates the inner vectors in the
set and Whh is the matrix representation of the state-to-state convolution weights.
As the hyperparameter of convolution is ﬁxed, the neighborhood set N h
i,j stays the same for all
locations. However, most motion patterns have different neighborhood sets for different locations.
For example, rotation and scaling generate ﬂow ﬁelds with different angles pointing to different
directions. It would thus be more reasonable to have a location-variant connection structure as

H(cid:48)

t,:,i,j = f (

Wl

hhHt−1,:,pl,i,j (θ),ql,i,j (θ)),

(3)

L
(cid:88)

l=1

4

(a) For convolutional RNN, the recurrent
connections are ﬁxed over time.

Figure 1: Example of the encoding-forecasting structure used
in the paper. In the ﬁgure, we use three RNNs to predict two
future frames ˆI3, ˆI4 given the two input frames I1, I2. The spatial
coordinates G are concatenated to the input frame to ensure the
network knows the observations are from different locations. The
RNNs can be either ConvGRU or TrajGRU. Zeros are fed as input
to the RNN if the input link is missing.

(b) For trajectory RNN, the recurrent con-
nections are dynamically determined.

Figure 2: Comparison of the connection
structures of convolutional RNN and tra-
jectory RNN. Links with the same color
share the same transition weights. (Best
viewed in color)

where L is the total number of local links, (pl,i,j(θ), ql,i,j(θ)) is the lth neighborhood parameterized
by θ.

Based on this observation, we propose the TrajGRU, which uses the current input and previous
state to generate the local neighborhood set for each location at each timestamp. Since the location
indices are discrete and non-differentiable, we use a set of continuous optical ﬂows to represent these
“indices”. The main formulas of TrajGRU are given as follows:

Ut, Vt = γ(Xt, Ht−1),

Zt = σ(Wxz ∗ Xt +

W l

hz ∗ warp(Ht−1, Ut,l, Vt,l)),

Rt = σ(Wxr ∗ Xt +

W l

hr ∗ warp(Ht−1, Ut,l, Vt,l)),

(4)

H(cid:48)

t = f (Wxh ∗ Xt + Rt ◦ (

W l

hh ∗ warp(Ht−1, Ut,l, Vt,l))),

Ht = (1 − Zt) ◦ H(cid:48)

t + Zt ◦ Ht−1.

Here, L is the total number of allowed links. Ut, Vt ∈ RL×H×W are the ﬂow ﬁelds that store the
local connection structure generated by the structure generating network γ. The W l
hr, W l
hh
are the weights for projecting the channels, which are implemented by 1 × 1 convolutions. The
warp(Ht−1, Ut,l, Vt,l) function selects the positions pointed out by Ut,l, Vt,l from Ht−1 via the
bilinear sampling kernel [12, 10]. If we denote M = warp(I, U, V) where M, I ∈ RC×H×W and
U, V ∈ RH×W , we have:

hz, W l

Mc,i,j =

Ic,m,n max(0, 1 − |i + Vi,j − m|) max(0, 1 − |j + Ui,j − n|).

(5)

H
(cid:88)

W
(cid:88)

m=1

n=1

The advantage of such a structure is that we could learn the connection topology by learning the
parameters of the subnetwork γ. In our experiments, γ takes the concatenation of Xt and Ht−1 as
the input and is ﬁxed to be a one-hidden-layer convolutional neural network with 5 × 5 kernel size
and 32 feature maps. Thus, γ has only a small number of parameters and adds nearly no cost to the
overall computation. Compared to a ConvGRU with K × K state-to-state convolution, TrajGRU
is able to learn a more efﬁcient connection structure with L < K 2. For ConvGRU and TrajGRU,
the number of model parameters is dominated by the size of the state-to-state weights, which is
h) for ConvGRU. If L is chosen to be smaller than K 2, the
O(L × C 2

h) for TrajGRU and O(K 2 × C 2

L
(cid:88)

l=1

L
(cid:88)

l=1

L
(cid:88)

l=1

5

Table 1: Comparison of TrajGRU and the baseline models in the MovingMNIST++ dataset. ‘Conv-Kα-Dβ’
refers to the ConvGRU with kernel size α × α and dilation β × β. ‘Traj-Lλ’ refers to the TrajGRU with λ links.
We replace the output layer of the ConvGRU-K5-D1 model to get the DFN.

Conv-K3-D2 Conv-K5-D1 Conv-K7-D1 Traj-L5 Traj-L9 Traj-L13 TrajGRU-L17

DFN

Conv2D Conv3D

#Parameters
Test MSE ×10−2
Standard Deviation ×10−2

2.84M
1.495
0.003

4.77M
1.310
0.004

8.01M
1.254
0.006

2.60M 3.42M
1.247
1.351
0.015
0.020

4.00M
1.170
0.022

4.77M
1.138
0.019

4.83M 29.06M 32.52M
1.637
1.681
1.461
0.002
0.001
0.002

number of parameters of TrajGRU can also be smaller than the ConvGRU and the TrajGRU model
is able to use the parameters more efﬁciently. Illustration of the recurrent connection structures of
ConvGRU and TrajGRU is given in Figure 2. Recently, Jeon & Kim [14] has used similar ideas to
extend the convolution operations in CNN. However, their proposed Active Convolution Unit (ACU)
focuses on the images where the need for location-variant ﬁlters is limited. Our TrajGRU focuses on
videos where location-variant ﬁlters are crucial for handling motion patterns like rotations. Moreover,
we are revising the structure of the recurrent connection and have tested different number of links
while [14] ﬁxes the link number to 9.

4 Experiments on MovingMNIST++

Before evaluating our model on the more challenging precipitation nowcasting task, we ﬁrst compare
TrajGRU with ConvGRU, DFN and 2D/3D CNNs on a synthetic video prediction dataset to justify
its effectiveness.

The previous MovingMNIST dataset [26, 25] only moves the digits with a constant speed and is not
suitable for evaluating different models’ ability in capturing more complicated motion patterns. We
thus design the MovingMNIST++ dataset by extending MovingMNIST to allow random rotations,
scale changes, and illumination changes. Each frame is of size 64 × 64 and contains three moving
digits. We use 10 frames as input to predict the next 10 frames. As the frames have illumination
changes, we use MSE instead of cross-entropy for training and evaluation 2. We train all models
using the Adam optimizer [16] with learning rate equal to 10−4 and momentum equal to 0.5. For
the RNN models, we use the encoding-forecasting structure introduced previously with three RNN
layers. All RNNs are either ConvGRU or TrajGRU and all use the same set of hyperparameters. For
TrajGRU, we initialize the weight of the output layer of the structure generating network to zero.
The strides of the middle downsampling and upsampling layers are chosen to be 2. The numbers
of ﬁlters for the three RNNs are 64, 96, 96 respectively. For the DFN model, we replace the output
layer of ConvGRU with a 11 × 11 local ﬁlter and transform the previous frame to get the prediction.
For the RNN models, we train them for 200,000 iterations with norm clipping threshold equal to
10 and batch size equal to 4. For the CNN models, we train them for 100,000 iterations with norm
clipping threshold equal to 50 and batch size equal to 32. The detailed experimental conﬁguration of
the models for the MovingMNIST++ experiment can be found in the appendix. We have also tried to
use conditional GAN for the 2D and 3D models but have failed to get reasonable results.

Table 1 gives the results of different models on the same test set that contains 10,000 sequences. We
train all models using three different seeds to report the standard deviation. We can ﬁnd that TrajGRU
with only 5 links outperforms ConvGRU with state-to-state kernel size 3 × 3 and dilation 2 × 2 (9
links). Also, the performance of TrajGRU improves as the number of links increases. TrajGRU
with L = 13 outperforms ConvGRU with 7 × 7 state-to-state kernel and yet has fewer parameters.
Another observation from the table is that DFN does not perform well in this synthetic dataset. This
is because DFN uses softmax to enhance the sparsity of the learned local ﬁlters, which fails to model
illumination change because the maximum value always gets smaller after convolving with a positive
kernel whose weights sum up to 1. For DFN, when the pixel values get smaller, it is impossible for
them to increase again. Figure 3 visualizes the learned structures of TrajGRU. We can see that the
network has learned reasonable local link patterns.

2The MSE for the MovingMNIST++ experiment is averaged by both the frame size and the length of the

predicted sequence.

6

Figure 3: Selected links of TrajGRU-L13 at different frames and layers. We choose one of the 13 links and
plot an arrow starting from each pixel to the pixel that is referenced by the link. From left to right we display
the learned structure at the ﬁrst, second and third layer of the encoder. The links displayed here have learned
behaviour for rotations. We sub-sample the displayed links for the ﬁrst layer for better readability. We include
animations for all layers and links in the supplementary material. (Best viewed when zoomed in.)

5 Benchmark for Precipitation Nowcasting

5.1 HKO-7 Dataset

The HKO-7 dataset used in the benchmark contains radar echo data from 2009 to 2015 collected by
HKO. The radar CAPPI reﬂectivity images, which have resolution of 480 × 480 pixels, are taken from
an altitude of 2km and cover a 512km × 512km area centered in Hong Kong. The data are recorded
every 6 minutes and hence there are 240 frames per day. The raw logarithmic radar reﬂectivity factors
are linearly transformed to pixel values via pixel = (cid:98)255 × dBZ+10
70 + 0.5(cid:99) and are clipped to be
between 0 and 255. The raw radar echo images generated by Doppler weather radar are noisy due to
factors like ground clutter, sea clutter, anomalous propagation and electromagnetic interference [18].
To alleviate the impact of noise in training and evaluation, we ﬁlter the noisy pixels in the dataset and
generate the noise masks by a two-stage process described in the appendix.

As rainfall events occur sparsely, we select the rainy days based on the rain barrel information to form
our ﬁnal dataset, which has 812 days for training, 50 days for validation and 131 days for testing.
Our current treatment is close to the real-life scenario as we are able to train an additional model that
classiﬁes whether or not it will rain on the next day and applies our precipitation nowcasting model if
this coarser-level model predicts that it will be rainy. The radar reﬂectivity values are converted to
rainfall intensity values (mm/h) using the Z-R relationship: dBZ = 10 log a + 10b log R where R is
the rain-rate level, a = 58.53 and b = 1.56. The overall statistics and the average monthly rainfall
distribution of the HKO-7 dataset are given in the appendix.

5.2 Evaluation Methodology

As the radar echo maps arrive in a stream, nowcasting algorithms can apply online learning to adapt
to the newly emerging spatiotemporal patterns. We propose two settings in our evaluation protocol:
(1) the ofﬂine setting in which the algorithm always receives 5 frames as input and predicts 20 frames
ahead, and (2) the online setting in which the algorithm receives segments of length 5 sequentially and
predicts 20 frames ahead for each new segment received. The evaluation protocol is described more
systematically in the appendix. The testing environment guarantees that the same set of sequences is
tested in both the ofﬂine and online settings for fair comparison.

For both settings, we evaluate the skill scores for multiple thresholds that correspond to different
rainfall levels to give an all-round evaluation of the algorithms’ nowcasting performance. Table 2
shows the distribution of different rainfall levels in our dataset. We choose to use the thresholds 0.5,
2, 5, 10, 30 to calculate the CSI and Heidke Skill Score (HSS) [9]. For calculating the skill score at a
speciﬁc threshold τ , which is 0.5, 2, 5, 10 or 30, we ﬁrst convert the pixel values in prediction and
ground-truth to 0/1 by thresholding with τ . We then calculate the TP (prediction=1, truth=1), FN
(prediction=0, truth=1), FP (prediction=1, truth=0), and TN (prediction=0, truth=0). The CSI score is
calculated as
(TP+FN)(FN+TN)+(TP+FP)(FP+TN) . During
the computation, the masked points are ignored.

TP+FN+FP and the HSS score is calculated as

TP×TN−FN×FP

TP

7

Table 2: Rain rate statistics in the HKO-7 benchmark.

Rain Rate (mm/h)

Proportion (%) Rainfall Level

0 ≤ x < 0.5

0.5 ≤ x < 2
2 ≤ x < 5
5 ≤ x < 10
10 ≤ x < 30
30 ≤ x

90.25
4.38
2.46
1.35
1.14
0.42

No / Hardly noticeable
Light
Light to moderate
Moderate
Moderate to heavy
Rainstorm warning

As shown in Table 2, the frequencies of different rainfall levels are highly imbalanced. We propose
to use the weighted loss function to help solve this problem. Speciﬁcally, we assign a weight






x < 2
1,
2 ≤ x < 5
2,
5,
5 ≤ x < 10
10, 10 ≤ x < 30
30, x ≥ 30

w(x) to each pixel according to its rainfall intensity x: w(x) =

. Also, the

n=1

(cid:80)N

(cid:80)480

(cid:80)480
i=1

masked pixels have weight 0. The resulting B-MSE and B-MAE scores are computed as B-MSE =
1
j=1 wn,i,j|xn,i,j −
N
ˆxn,i,j|, where N is the total number of frames and wn,i,j is the weight corresponding to the (i, j)th
pixel in the nth frame. For the conventional MSE and MAE measures, we simply set all the weights
to 1 except the masked points.

j=1 wn,i,j(xn,i,j − ˆxn,i,j)2 and B-MAE = 1
N

(cid:80)480
i=1

(cid:80)480

(cid:80)N

n=1

5.3 Evaluated Algorithms

We have evaluated seven nowcasting algorithms, including the simplest model which always predicts
the last frame, two optical ﬂow based methods (ROVER and its nonlinear variant), and four deep
learning methods (TrajGRU, ConvGRU, 2D CNN, and 3D CNN). Speciﬁcally, we have evaluated
the performance of deep learning models in the online setting by ﬁne-tuning the algorithms using
AdaGrad [4] with learning rate equal to 10−4. We optimize the sum of B-MSE and B-MAE during
ofﬂine training and online ﬁne-tuning. During the ofﬂine training process, all models are optimized
by the Adam optimizer with learning rate equal to 10−4 and momentum equal to 0.5 and we train
these models with early-stopping on the sum of B-MSE and B-MAE. For RNN models, the training
batch size is set to 4. For the CNN models, the training batch size is set to 8. For TrajGRU and
ConvGRU models, we use a 3-layer encoding-forecasting structure with the number of ﬁlters for the
RNNs set to 64, 192, 192. We use kernel size equal to 5 × 5, 5 × 5, 3 × 3 for the ConvGRU models
while the number of links is set to 13, 13, 9 for the TrajGRU model. We also train the ConvGRU
model with the original MSE and MAE loss, which is named “ConvGRU-nobal”, to evaluate the
improvement by training with the B-MSE and B-MAE loss. The other model conﬁgurations including
ROVER, ROVER-nonlinear and deep models are included in the appendix.

5.4 Evaluation Results

The overall evaluation results are summarized in Table 3. In order to analyze the conﬁdence interval
of the results, we train 2D CNN, 3D CNN, ConvGRU and TrajGRU models using three different
random seeds and report the standard deviation in Table 4. We ﬁnd that training with balanced loss
functions is essential for good nowcasting performance of heavier rainfall. The ConvGRU model that
is trained without balanced loss, which best represents the model in [25], has worse nowcasting score
than the optical ﬂow based methods at the 10mm/h and 30mm/h thresholds. Also, we ﬁnd that all the
deep learning models that are trained with the balanced loss outperform the optical ﬂow based models.
Among the deep learning models, TrajGRU performs the best and 3D CNN outperforms 2D CNN,
which shows that an appropriate network structure is crucial to achieving good performance. The
improvement of TrajGRU over the other models is statistically signiﬁcant because the differences in
B-MSE and B-MAE are larger than three times their standard deviation. Moreover, the performance
with online ﬁne-tuning enabled is consistently better than that without online ﬁne-tuning, which
veriﬁes the effectiveness of online learning at least for this task.

8

Table 3: HKO-7 benchmark result. We mark the best result within a speciﬁc setting with bold face and the
second best result by underlining. Each cell contains the mean score of the 20 predicted frames. In the online
setting, all algorithms have used the online learning strategy described in the paper. ‘↑’ means that the score is
higher the better while ‘↓’ means that the score is lower the better. ‘r ≥ τ ’ means the skill score at the τ mm/h
rainfall threshold. For 2D CNN, 3D CNN, ConvGRU and TrajGRU models, we train the models with three
different random seeds and report the mean scores.

r ≥ 0.5

r ≥ 2

r ≥ 10

r ≥ 30

r ≥ 0.5

r ≥ 2

r ≥ 10

r ≥ 30

B-MSE ↓ B-MAE ↓

Algorithms

Last Frame
ROVER + Linear
ROVER + Non-linear
2D CNN
3D CNN
ConvGRU-nobal
ConvGRU
TrajGRU

2D CNN
3D CNN
ConvGRU
TrajGRU

0.4022
0.4762
0.4655
0.5095
0.5109
0.5476
0.5489
0.5528

0.5112
0.5106
0.5511
0.5563

0.3266
0.4089
0.4074
0.4396
0.4411
0.4661
0.4731
0.4759

0.4363
0.4344
0.4737
0.4798

Ofﬂine Setting

0.1574
0.2146
0.2164
0.2392
0.2424
0.2138
0.2789
0.2835

0.2435
0.2427
0.2843
0.2914

0.0692
0.1067
0.0951
0.1093
0.1185
0.0712
0.1776
0.1856

0.1263
0.1299
0.1837
0.1933

0.5207
0.6038
0.5896
0.6366
0.6334
0.6756
0.6701
0.6731

0.6365
0.6355
0.6712
0.6760

0.4531
0.5473
0.5436
0.5809
0.5825
0.6094
0.6104
0.6126

0.5756
0.5736
0.6105
0.6164

Online Setting

0.2512
0.3301
0.3318
0.3690
0.3734
0.3286
0.4159
0.4207

0.3744
0.3733
0.4226
0.4308

0.1193
0.1762
0.1576
0.1885
0.2034
0.1160
0.2893
0.2996

0.2162
0.2220
0.2981
0.3111

15274
11651
10945
7332
7202
9087
5951
5816

6654
6690
5724
5589

28042
23437
22857
18091
17593
19642
15000
14675

17071
16903
14772
14465

Table 4: Conﬁdence intervals of selected deep models in the HKO-7 benchmark. We train 2D CNN, 3D CNN,
ConvGRU and TrajGRU using three different random seeds and report the standard deviation of the test scores.

Algorithms

2D CNN
3D CNN
ConvGRU
TrajGRU

2D CNN
3D CNN
ConvGRU
TrajGRU

r ≥ 0.5

r ≥ 2

r ≥ 10

r ≥ 30

r ≥ 0.5

r ≥ 2

r ≥ 10

r ≥ 30

B-MSE

B-MAE

0.0032
0.0043
0.0022
0.0020

0.0002
0.0004
0.0006
0.0008

0.0023
0.0027
0.0018
0.0024

0.0005
0.0003
0.0012
0.0004

Ofﬂine Setting

0.0001
0.0024
0.0008
0.0031

0.0002
0.0003
0.0019
0.0002

0.0025
0.0024
0.0022
0.0031

0.0012
0.0008
0.0024
0.0002

0.0032
0.0042
0.0022
0.0019

0.0002
0.0004
0.0006
0.0007

0.0025
0.0028
0.0021
0.0024

0.0005
0.0004
0.0012
0.0004

Online Setting

0.0003
0.0031
0.0010
0.0039

0.0003
0.0004
0.0023
0.0002

0.0043
0.0041
0.0038
0.0045

0.0019
0.0001
0.0031
0.0003

90
44
52
18

12
23
30
10

95
26
81
32

12
27
69
20

Table 5: Kendall’s τ coefﬁcients between skill scores. Higher absolute value indicates stronger correlation. The
numbers with the largest absolute values are shown in bold face.

Skill Scores

r ≥ 0.5

r ≥ 2

r ≥ 10

r ≥ 30

r ≥ 0.5

r ≥ 2

r ≥ 10

r ≥ 30

MSE
MAE
B-MSE
B-MAE

-0.24
-0.41
-0.70
-0.74

-0.39
-0.57
-0.57
-0.59

-0.07
-0.25
-0.86
-0.82

-0.01
-0.27
-0.84
-0.92

-0.33
-0.50
-0.62
-0.67

-0.42
-0.60
-0.55
-0.57

-0.06
-0.24
-0.86
-0.83

0.01
-0.26
-0.84
-0.92

HSS
r ≥ 5

-0.39
-0.55
-0.61
-0.59

CSI ↑
r ≥ 5

0.2401
0.3151
0.3226
0.3406
0.3415
0.3526
0.3720
0.3751

0.3364
0.3345
0.3742
0.3808

CSI
r ≥ 5

0.0015
0.0016
0.0031
0.0025

0.0002
0.0002
0.0017
0.0002

CSI
r ≥ 5

-0.39
-0.55
-0.61
-0.58

HSS ↑
r ≥ 5

0.3582
0.4516
0.4590
0.4851
0.4862
0.4981
0.5163
0.5192

0.4790
0.4766
0.5183
0.5253

HSS
r ≥ 5

0.0018
0.0018
0.0040
0.0028

0.0002
0.0003
0.0019
0.0002

Based on the evaluation results, we also compute the Kendall’s τ coefﬁcients [15] between the MSE,
MAE, B-MSE, B-MAE and the CSI, HSS at different thresholds. As shown in Table 5, B-MSE and
B-MAE have stronger correlation with the CSI and HSS in most cases.

6 Conclusion and Future Work

In this paper, we have provided the ﬁrst large-scale benchmark for precipitation nowcasting and have
proposed a new TrajGRU model with the ability of learning the recurrent connection structure. We
have shown TrajGRU is more efﬁcient in capturing the spatiotemporal correlations than ConvGRU.
For future work, we plan to test if TrajGRU helps improve other spatiotemporal learning tasks like
visual object tracking and video segmentation. We will also try to build an operational nowcasting
system using the proposed algorithm.

9

This research has been supported by General Research Fund 16207316 from the Research Grants
Council and Innovation and Technology Fund ITS/205/15FP from the Innovation and Technology
Commission in Hong Kong. The ﬁrst author has also been supported by the Hong Kong PhD
Fellowship.

Acknowledgments

References

[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio

Savarese. Social LSTM: Human trajectory prediction in crowded spaces. In CVPR, 2016.

[2] Nicolas Ballas, Li Yao, Chris Pal, and Aaron Courville. Delving deeper into convolutional networks for

learning video representations. In ICLR, 2016.

[3] Bert De Brabandere, Xu Jia, Tinne Tuytelaars, and Luc Van Gool. Dynamic ﬁlter networks. In NIPS, 2016.
[4] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

[5] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through

video prediction. In NIPS, 2016.

[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron

Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.

[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing

human-level performance on imagenet classiﬁcation. In ICCV, 2015.

[8] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780,

1997.

[9] Robin J Hogan, Christopher AT Ferro, Ian T Jolliffe, and David B Stephenson. Equitability revisited: Why

the “equitable threat score” is not equitable. Weather and Forecasting, 25(2):710–726, 2010.

[10] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox.

Flownet 2.0: Evolution of optical ﬂow estimation with deep networks. In CVPR, 2017.

[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal

covariate shift. In ICML, 2015.

[12] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In NIPS, 2015.
[13] Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena. Structural-RNN: Deep learning on

spatio-temporal graphs. In CVPR, 2016.

[14] Yunho Jeon and Junmo Kim. Active convolution: Learning the shape of convolution for image classiﬁcation.

In CVPR, 2017.

[15] Maurice G Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81–93, 1938.
[16] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[17] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015.
[18] Hansoo Lee and Sungshin Kim. Ensemble classiﬁcation for anomalous propagation echo detection with

clustering-based subset-selection method. Atmosphere, 8(1):11, 2017.

[19] Xiaodan Liang, Liang Lin, Xiaohui Shen, Jiashi Feng, Shuicheng Yan, and Eric P Xing. Interpretable

structure-evolving LSTM. In CVPR, 2017.

[20] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural network

[21] John S Marshall and W Mc K Palmer. The distribution of raindrops with size. Journal of Meteorology,

[22] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean

[23] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander
Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In
CVPR, 2016.

[24] MarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, and Sumit
Chopra. Video (language) modeling: a baseline for generative models of natural videos. arXiv preprint
arXiv:1412.6604, 2014.

[25] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun Woo. Convolu-
tional LSTM network: A machine learning approach for precipitation nowcasting. In NIPS, 2015.

acoustic models. In ICML, 2013.

5(4):165–166, 1948.

square error. In ICLR, 2016.

10

[26] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video represen-

tations using LSTMs. In ICML, 2015.

[27] Juanzhen Sun, Ming Xue, James W Wilson, Isztar Zawadzki, Sue P Ballard, Jeanette Onvlee-Hooimeyer,
Paul Joe, Dale M Barker, Ping-Wah Li, Brian Golding, et al. Use of NWP for nowcasting convective
precipitation: Recent progress and challenges. Bulletin of the American Meteorological Society, 95(3):409–
426, 2014.

[28] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, and Honglak Lee. Decomposing motion and

content for natural video sequence prediction. In ICLR, 2017.

[29] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In NIPS,

2016.

[30] Wang-chun Woo and Wai-kin Wong. Operational application of optical ﬂow techniques to radar-based

rainfall nowcasting. Atmosphere, 8(3):48, 2017.

[31] Yi Wu, Jongwoo Lim, and Ming-Hsuan Yang. Online object tracking: A benchmark. In CVPR, 2013.
[32] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016.

11

Appendix

A Weight Initialization

The weights and biases of all models are initialized with the MSRA initializer [7] except that the weights and
biases of the structure generating network in TrajGRUs are initialized to be zero.

B Structure Generating Network in TrajGRU

The structure generating network takes the concatenation of the state tensor and the input tensor as the input. We
ﬁx the network to have two convolution layers. The ﬁrst convolution layer uses 5 × 5 kernel size, 2 × 2 padding
size, 32 ﬁlters and uses the leaky ReLU activation. The second convolution layer uses 5 × 5 kernel size, 2 × 2
padding and 2L ﬁlters where L is the number of links.

C Details about the MovingMNIST++ Experiment

C.1 Generation Process

For each sequence, we choose three digits randomly from the MNIST dataset3. Each digit will move, rotate,
scale up or down at a randomly sampled speed. Also, we multiply the pixel values by an illumination factor
every time to make the digits have time-varying appearances. The hyperparameters of the generation process are
given in Table 6. In our experiment, we always generate a length-20 sequence and use the ﬁrst 10 frames to
predict the last 10 frames.

Table 6: Hyperparameters of the MovingMNIST++ dataset. We choose the velocity, scaling factor, rotation
angle and illumination factor uniformly within the range listed in the table.

Hyper-parameter

Value

Number of digits
Frame size
Velocity
Scaling factor
Rotation angle
Illumination factor

3
64 × 64
[0, 3.6)
[ 1
1.1 , 1.1)
12 , π
[ −π
12 )
[0.6, 1.0)

C.2 Network Structures

The general structure of the 2D CNN, 3D CNN and the DFN model used in the paper are illustrated in Figure 4.
We always use batch normalization [11] in 2D and 3D CNNs.

The detailed network conﬁgurations of 2D CNN, 3D CNN, ConvGRU, DFN and TrajGRU are described in
Table 7, 8, 9, 10, 11.

Table 7: The details of the 2D CNN model. The two dimensions in kernel, stride, pad and other features
represent for height and width. We set the base ﬁlter number c to 70. We derive the 2D model from the 3D
model by multiplying the number of channels with the respective kernel size of the 3D model. The 10 channels
in the input of ‘enc1’ and the output of ‘vid5’ correspond to the input and output frames, respectively.

Name Kernel

Stride

Pad

Ch I/O

In Res

Out Res

enc1
enc2
enc3
enc4

vid1
vid2
vid3
vid4
vid5

4 × 4
4 × 4
4 × 4
4 × 4

1 × 1
4 × 4
4 × 4
4 × 4
4 × 4

2 × 2
2 × 2
2 × 2
2 × 2

1 × 1
2 × 2
2 × 2
2 × 2
2 × 2

1 × 1
1 × 1
1 × 1
1 × 1

0 × 0
1 × 1
1 × 1
1 × 1
1 × 1

10/4c
4c/8c
8c/12c
12c/16c

16c/16c
16c/16c
16c/8c
8c/4c
4c/10

64 × 64
32 × 32
16 × 16
8 × 8

4 × 4
4 × 4
8 × 8
16 × 16
32 × 32

32 × 32
16 × 16
8 × 8
4 × 4

Type

Conv
Conv
Conv
Conv

4 × 4
Deconv
8 × 8
Deconv
16 × 16 Deconv
32 × 32 Deconv
64 × 64 Deconv

Input

in
enc1
enc2
enc3

enc4
vid1
vid2
vid3
vid4

3MNIST dataset:http://yann.lecun.com/exdb/mnist/

12

Illustration of

(a)
the 2D/3D
CNNs used in the paper. In this
example, we use 4 convolution
layers to get the representation of
the 5 input frames, which is fur-
ther used to forecast the 5 future
frames. We use either 2D convo-
lution or 3D convolution in the
encoder and the forecaster.

(b) Illustration of the DFN model used in the paper. In this
example, we use 2 frames to predict 2 frames. The ˆSs are
the predicted local ﬁlters, which are used to transform the
last input frame or the previous predicted frame. We use
ConvGRU as the RNN model in the experiment.

Figure 4: Illustration of the 2D CNN, 3D CNN and DFN models used in the paper.

Table 8: The details of the 3D CNN model. The three dimensions in kernel, stride, pad and other features
represent for depth, height and width. We set the base ﬁlter number c to 128.

Name

Kernel

Stride

Pad

Ch I/O

In Res

Out Res

enc1
enc2
enc3
enc4

vid1
vid2
vid3
vid4
vid5

4 × 4 × 4
4 × 4 × 4
4 × 4 × 4
4 × 4 × 4

2 × 1 × 1
4 × 4 × 4
4 × 4 × 4
4 × 4 × 4
3 × 4 × 4

2 × 2 × 2
2 × 2 × 2
2 × 2 × 2
2 × 2 × 2

1 × 1 × 1
2 × 2 × 2
2 × 2 × 2
2 × 2 × 2
1 × 2 × 2

1 × 1 × 1
1 × 1 × 1
1 × 1 × 1
2 × 1 × 1

0 × 0 × 0
1 × 1 × 1
2 × 1 × 1
2 × 1 × 1
1 × 1 × 1

1/c
c/2c
2c/3c
3c/4c

4c/8c
8c/4c
4c/2c
2c/c
c/1

10 × 64 × 64
5 × 32 × 32
2 × 16 × 16
1 × 8 × 8

1 × 4 × 4
2 × 4 × 4
4 × 8 × 8
6 × 16 × 16
10 × 32 × 32

5 × 32 × 32
2 × 16 × 16
1 × 8 × 8
1 × 4 × 4

Type

Conv
Conv
Conv
Conv

2 × 4 × 4
Deconv
4 × 8 × 8
Deconv
6 × 16 × 16
Deconv
10 × 32 × 32 Deconv
10 × 64 × 64 Deconv

Input

in
enc1
enc2
enc3

enc4
vid1
vid2
vid3
vid4

Table 9: The details of the ConvGRU model. The ‘In Kernel‘, ‘In Stride‘ and ‘In Pad‘ are the kernel, stride and
padding in the input-to-state convolution. ‘State Ker.‘ and ‘State Dila.‘ are the kernel size and dilation size of the
state-to-state convolution. We set k and d as stated in the paper. The ‘In State‘ is the initial state of the RNN
layer.

Name

In Kernel

In Stride

In Pad

State Ker.

State Dila. Ch I/O

In Res

Out Res

Type

In

In State

econv1
ernn1
edown1
ernn2
edown2
ernn3

frnn1
fup1
frnn2
fup2
frnn3
fconv4
fconv5

3 × 3
3 × 3
3 × 3
3 × 3
3 × 3
3 × 3

3 × 3
4 × 4
3 × 3
4 × 4
3 × 3
3 × 3
1 × 1

1 × 1
1 × 1
2 × 2
1 × 1
2 × 2
1 × 1

1 × 1
2 × 2
1 × 1
2 × 2
1 × 1
1 × 1
1 × 1

1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
1 × 1

1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
0 × 0

-
k × k
-
k × k
-
k × k

k × k
-
k × k
-
k × k
-
-

-
d × d
-
d × d
-
d × d

d × d
-
d × d
-
d × d
-
-

4/16
16/64
64/64
64/96
96/96
96/96

96/96
96/96
96/96
96/96
96/64
64/16
16/1

64 × 64
64 × 64
64 × 64
32 × 32
32 × 32
16 × 16

16 × 16
16 × 16
32 × 32
32 × 32
64 × 64
64 × 64
64 × 64

Conv

64 × 64
in
64 × 64 ConvGRU
econv1
32 × 32
ernn1
Conv
32 × 32 ConvGRU edown1
16 × 16
ernn2
Conv
16 × 16 ConvGRU edown2

Deconv

16 × 16 ConvGRU
32 × 32
32 × 32 ConvGRU
64 × 64
64 × 64 ConvGRU
64 × 64
64 × 64

Conv
Conv

Deconv

-
frnn1
fup1
frnn2
fup2
frnn3
fconv4

-
-
-
-
-
-

ernn3
-
ernn2
-
ernn1
-
-

13

Table 10: The details of the DFN model. The output of the ‘fconv4‘ layer will be used to transform the previous
prediction or the last input frame. All hyperparameters have the same meaning as in Table 9.

Name

In Kernel

In Stride

In Pad

State Ker.

State Dila. Ch I/O

In Res

Out Res

Type

In

In State

econv1
ernn1
edown1
ernn2
edown2
ernn3

frnn1
fup1
frnn2
fup2
frnn3
fconv4

3 × 3
3 × 3
3 × 3
3 × 3
3 × 3
3 × 3

3 × 3
4 × 4
3 × 3
4 × 4
3 × 3
3 × 3

1 × 1
1 × 1
2 × 2
1 × 1
2 × 2
1 × 1

1 × 1
2 × 2
1 × 1
2 × 2
1 × 1
1 × 1

1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
1 × 1

1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
1 × 1

-
k × k
-
k × k
-
k × k

k × k
-
k × k
-
k × k
-

-
d × d
-
d × d
-
d × d

d × d
-
d × d
-
d × d
-

4/16
16/64
64/64
64/96
96/96
96/96

96/96
96/96
96/96
96/96
96/64
64/121

64 × 64
64 × 64
64 × 64
32 × 32
32 × 32
16 × 16

16 × 16
16 × 16
32 × 32
32 × 32
64 × 64
64 × 64

Conv

64 × 64
in
64 × 64 ConvGRU
econv1
32 × 32
ernn1
Conv
32 × 32 ConvGRU edown1
16 × 16
ernn2
Conv
16 × 16 ConvGRU edown2

Deconv

16 × 16 ConvGRU
32 × 32
32 × 32 ConvGRU
64 × 64
64 × 64 ConvGRU
64 × 64

Deconv

Conv

-
frnn1
fup1
frnn2
fup2
frnn3

ernn3
-
ernn2
-
ernn1
-

Table 11: The details of the TrajGRU model. ‘L‘ is the number of links in the state-to-state transition. We set l
as stated in the paper. All other hyperparameters have the same meaning as in Table 9.
Type

In Pad L Ch I/O

In Kernel

In Stride

Out Res

In State

In Res

Name

In

econv1
ernn1
edown1
ernn2
edown2
ernn3

frnn1
fup1
frnn2
fup2
frnn3
fconv4
fconv5

3 × 3
3 × 3
3 × 3
3 × 3
3 × 3
3 × 3

3 × 3
4 × 4
3 × 3
4 × 4
3 × 3
3 × 3
1 × 1

1 × 1
1 × 1
2 × 2
1 × 1
2 × 2
1 × 1

1 × 1
2 × 2
1 × 1
2 × 2
1 × 1
1 × 1
1 × 1

1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
1 × 1

1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
0 × 0

-
l
-
l
-
l

l
-
l
-
l
-
-

4/16
16/64
64/64
64/96
96/96
96/96

96/96
96/96
96/96
96/96
96/64
64/16
16/1

64 × 64
64 × 64
64 × 64
32 × 32
32 × 32
16 × 16

16 × 16
16 × 16
32 × 32
32 × 32
64 × 64
64 × 64
64 × 64

Conv

64 × 64
in
64 × 64 TrajGRU
econv1
32 × 32
ernn1
Conv
32 × 32 TrajGRU edown1
16 × 16
ernn2
Conv
16 × 16 TrajGRU edown2

16 × 16 TrajGRU
32 × 32
Deconv
32 × 32 TrajGRU
64 × 64
Deconv
64 × 64 TrajGRU
64 × 64
64 × 64

Conv
Conv

-
frnn1
fup1
frnn2
fup2
frnn3
fconv4

ernn3
-
ernn2
-
ernn1
-
-

-
-
-
-
-
-

-
-
-
-
-
-

D Details about the HKO-7 Benchmark

D.1 Overall Data Statistics

The

overall

statistics

of

the HKO-7

dataset

is

given

in

Figure

5

and Table

12.

Table 12: Overall statistics of the HKO-7 dataset.

Train

Validate

Test

Years
#Days
#Frames

2009-2014
812
192,168

2009-2014
50
11,736

2015
131
31,350

Figure 5: Average rainfall intensity of
different months in the HKO-7 dataset.

D.2 Denoising Process

We ﬁrst remove the ground clutter and sun spikes, which appear at a ﬁxed position, by detecting the out-
lier locations in the image. For each in-boundary location i in the frame, we use the ratio of its pixel value
i=1 xi
equal to 1, 2, ..., 255 as the feature xi ∼ R255 and estimate these features’ sample mean ˆµ =
N
and covariance matrix ˆS =

. We then calculate the Mahalanobis distance DM (x) =

i=1(xi−µ)(xi−µ)T
N −1

(cid:80)N

(cid:80)N

14

(cid:113)

(x − ˆµ)T ˆS†(x − ˆµ)4 of these features using the estimated mean and covariance. Locations that have the
Mahalanobis distances higher than the mean distance plus three times the standard deviation are classiﬁed as
outliers. After out-lier detection, the 480 × 480 locations in the image are divided into 177316 inliers, 2824
outliers and 50260 out-of-boundary points. The outlier detection process is illustrated in Figure 6. After out-lier
detection, we further remove other types of noise, like sea clutter, by ﬁltering out the pixels with value smaller
than 71 and larger than 0. Two examples that compare the original radar echo sequence and the denoised
sequence are included in the attached “denoising” folder.

(a) Mahalanobis distance of a random portion
of 10000 in-lier locations and 157 out-lier
locations. The threshold is chosen to be the
mean distance plus three times the standard
deviation. (Best viewed in color.)

(b) Outlier locations that are excluded in
learning and evaluation. The purple points
are the out-of-boundary locations and the red
points are the outliers. (Best viewed in color.)

Figure 6: Illustration of the outlier detection process and the ﬁnal outlier mask obtained in HKO-7 dataset.

D.3 Evaluation Protocol

We illustrate our evaluation protocol in Algorithm 1. We can choose the evaluation type to be ‘ofﬂine’ or ‘online’.
In the online setting, the model is able to store the previously seen sequences in a buffer and ﬁne-tune the
parameters using the sampled training batches from the buffer. For algorithms that are tested in the online setting
in the paper, we sample the last 25 consecutive frames in the buffer to update the model if these frames are
available. The buffer will be made empty once a new episode ﬂag is received, which indicates that the newly
observed 5-frame segment is not consecutive to the previous frames.

(cid:46) fe indicates whether it is a new episode

Algorithm 1 Evaluation protocol in the HKO-7 benchmark

env ← GETENV(type)
while not env.end() do

1: procedure HKO7TEST(model, type)
2:
3:
4:
5:
6:
7:
8:
9:
10:

I1:J , fe ← env.next()
model.store(I1:J , fe)
if type = online then
model.update()

ˆIJ+1:J+K ← model.predict()
env.upload( ˆIJ+1:J+K)

env.save()

4We use Moore-Penrose pseudoinverse in the implementation.

15

D.4 Details of Optical Flow based Algorithms

For the ROVER algorithm, we use the same hyperparameters as [25]. For the ROVER-nonlinear algorithm, we
follow the implementation in [30]. We ﬁrst non-linearly transform the input frames and then calculate the optical
ﬂow based on the transformed frames.

D.5 Network Structures

We use the general structure for 2D and 3D CNNs illustrated in Figure 4a. The network conﬁgurations of the 2D
CNN, 3D CNN, ConvGRU and TrajGRU models are described in Table 13, 14, 15, 16.

Table 13: The details of the 2D CNN model. The two dimensions in kernel, stride, pad and other features
represent for height and width. We set the base ﬁlter number c to 70. We derive the 2D model from the 3D
model by multiplying the number of channels with the respective kernel size of the 3D model. The ﬁrst 5 and
last 20 channels respectively correspond to the in- and output frames.

Name Kernel

Stride

Pad

Ch I/O

In Res

enc0
enc1
enc2
enc3
enc4

vid1
vid2
vid3
vid4
vid5
vid6
vid7

7 × 7
4 × 4
4 × 4
4 × 4
4 × 4

1 × 1
4 × 4
4 × 4
4 × 4
5 × 5
7 × 7
3 × 3

5 × 5
3 × 3
2 × 2
2 × 2
2 × 2

1 × 1
2 × 2
2 × 2
2 × 2
3 × 3
5 × 5
1 × 1

1 × 1
1 × 1
1 × 1
1 × 1
1 × 1

0 × 0.
1 × 1.
1 × 1.
1 × 1.
1 × 1.
1 × 1.
1 × 1.

5/c
c/c
c/8c
8c/12c
12c/16c

16c/16c
16c/16c
16c/8c
8c/4c
4c/24
24/24
24/20

480 × 480
96 × 96
32 × 32
16 × 16
8 × 8

4 × 4
4 × 4
8 × 8
16 × 16
32 × 32
96 × 96
480 × 480

Out Res

96 × 96
32 × 32
16 × 16
8 × 8
4 × 4

Type

Conv
Conv
Conv
Conv
Conv

4 × 4
Deconv
8 × 8
Deconv
16 × 16
Deconv
32 × 32
Deconv
96 × 96
Deconv
480 × 480 Deconv
480 × 480 Deconv

Input

in
enc0
enc1
enc2
enc3

enc4
vid1
vid2
vid3
vid4
vid5
vid6

Table 14: The details of the 3D CNN model. The three dimensions in kernel, stride, pad and other features
represent for channel, height and width. We set the base ﬁlter number c to 128.
Pad

Out Res

Ch I/O

Kernel

In Res

Stride

Name

Input

Type

enc0
enc1
enc2
enc3
enc4

vid1
vid2
vid3
vid4
vid5
vid6
vid7

1 × 7 × 7
1 × 4 × 4
4 × 4 × 4
4 × 4 × 4
4 × 4 × 4

2 × 1 × 1
4 × 4 × 4
4 × 4 × 4
4 × 4 × 4
3 × 5 × 5
3 × 7 × 7
3 × 3 × 3

1 × 5 × 5
1 × 3 × 3
2 × 2 × 2
2 × 2 × 2
2 × 2 × 2

1 × 1 × 1
2 × 2 × 2
2 × 2 × 2
2 × 2 × 2
1 × 3 × 3
1 × 5 × 5
1 × 1 × 1

0 × 1 × 1
0 × 1 × 1
1 × 1 × 1
1 × 1 × 1
2 × 1 × 1

0 × 0 × 0.
1 × 1 × 1.
0 × 1 × 1.
1 × 1 × 1.
1 × 1 × 1.
1 × 1 × 1.
1 × 1 × 1.

1/c
c/c
c/2c
2c/3c
3c/4c

4c/8c
8c/4c
4c/2c
2c/c
c/8
8/8
8/1

5 × 480 × 480
5 × 96 × 96
5 × 32 × 32
2 × 16 × 16
1 × 8 × 8

1 × 4 × 4
2 × 4 × 4
4 × 8 × 8
10 × 16 × 16
20 × 32 × 32
20 × 96 × 96
20 × 480 × 480

5 × 96 × 96
5 × 32 × 32
2 × 16 × 16
1 × 8 × 8
1 × 4 × 4

Conv
Conv
Conv
Conv
Conv

2 × 4 × 4
Deconv
4 × 8 × 8
Deconv
10 × 16 × 16
Deconv
20 × 32 × 32
Deconv
20 × 96 × 96
Deconv
20 × 480 × 480 Deconv
20 × 480 × 480 Deconv

Table 15: The details of the ConvGRU model. All hyperparameters have the same meaning as in Table 9.
In State
Name

State Dila.

State Ker.

In Kernel

In Stride

Out Res

Ch I/O

In Res

In Pad

Type

In

econv1
ernn1
edown1
ernn2
edown2
ernn3

frnn1
fup1
frnn2
fup2
frnn3
fdeconv4
fconv5

7 × 7
3 × 3
5 × 5
3 × 3
3 × 3
3 × 3

3 × 3
4 × 4
3 × 3
5 × 5
3 × 3
7 × 7
1 × 1

5 × 5
1 × 1
3 × 3
1 × 1
2 × 2
1 × 1

1 × 1
2 × 2
1 × 1
3 × 3
1 × 1
5 × 5
1 × 1

1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
1 × 1

1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
0 × 0

-
5 × 5
-
5 × 5
-
3 × 3

3 × 3
-
5 × 5
-
5 × 5
-
-

-
1 × 1
-
1 × 1
-
1 × 1

1 × 1
-
1 × 1
-
1 × 1
-
-

4/8
8/64
64/64
64/192
192/192
192/192

192/192
192/192
192/192
192/192
192/64
64/8
8/1

480 × 480
96 × 96
96 × 96
32 × 32
32 × 32
16 × 16

16 × 16
16 × 16
32 × 32
32 × 32
96 × 96
96 × 96
480 × 480

96 × 96
96 × 96
32 × 32
32 × 32
16 × 16
16 × 16

16 × 16
32 × 32
32 × 32
96 × 96
96 × 96
480 × 480
480 × 480

Conv
ConvGRU
Conv
ConvGRU
Conv
ConvGRU

ConvGRU
Deconv
ConvGRU
Deconv
ConvGRU
Deconv
Conv

in
econv1
ernn1
edown1
ernn2
edown2

-
frnn1
fup1
frnn2
fup2
frnn3
fdeconv4

in
enc0
enc1
enc2
enc3

enc4
vid1
vid2
vid3
vid4
vid5
vid6

-
-
-
-
-
-

ernn3
-
ernn2
-
ernn1
-
-

16

Table 16: The details of the TrajGRU model. All hyperparameters have the same meaning as in Table 11.
In State
Name

In Kernel

In Stride

Out Res

Ch I/O

In Res

In Pad

Type

In

L

econv1
ernn1
edown1
ernn2
edown2
ernn3

frnn1
fup1
frnn2
fup2
frnn3
fdeconv4
fconv5

7 × 7
3 × 3
5 × 5
3 × 3
3 × 3
3 × 3

3 × 3
4 × 4
3 × 3
5 × 5
3 × 3
7 × 7
1 × 1

5 × 5
1 × 1
3 × 3
1 × 1
2 × 2
1 × 1

1 × 1
2 × 2
1 × 1
3 × 3
1 × 1
5 × 5
1 × 1

1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
1 × 1

1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
1 × 1
0 × 0

-
13
-
13
-
9

9
-
13
-
13
-
-

4/8
8/64
64/64
64/192
192/192
192/192

192/192
192/192
192/192
192/192
192/64
64/8
8/1

480 × 480
96 × 96
96 × 96
32 × 32
32 × 32
16 × 16

16 × 16
16 × 16
32 × 32
32 × 32
96 × 96
96 × 96
480 × 480

96 × 96
96 × 96
32 × 32
32 × 32
16 × 16
16 × 16

16 × 16
32 × 32
32 × 32
96 × 96
96 × 96
480 × 480
480 × 480

Conv
TrajGRU
Conv
TrajGRU
Conv
TrajGRU

TrajGRU
Deconv
TrajGRU
Deconv
TrajGRU
Deconv
Conv

in
econv1
ernn1
edown1
ernn2
edown2

-
frnn1
fup1
frnn2
fup2
frnn3
fdeconv4

-
-
-
-
-
-

ernn3
-
ernn2
-
ernn1
-
-

17

