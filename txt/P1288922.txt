7
1
0
2
 
y
a
M
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
3
3
6
0
.
7
0
6
1
:
v
i
X
r
a

Uncovering Causality from Multivariate Hawkes Integrated
Cumulants

Massil Achab∗1, Emmanuel Bacry1, Stéphane Gaiffas1, Iacopo Mastromatteo2, and
Jean-François Muzy1,3

1Centre de Mathématiques Appliquées, CNRS, Ecole Polytechnique, UMR 7641, 91128 Palaiseau, France
2Capital Fund Management, 23 rue de l’Université, 75007 Paris, France
3Laboratoire Sciences Pour l’Environnement, Université de Corse, 7 Avenue Jean Nicoli, 20250 Corte, France

May 31, 2017

Abstract

We design a new nonparametric method that allows one to estimate the matrix of integrated
kernels of a multivariate Hawkes process. This matrix not only encodes the mutual inﬂuences of each
node of the process, but also disentangles the causality relationships between them. Our approach is
the ﬁrst that leads to an estimation of this matrix without any parametric modeling and estimation of
the kernels themselves. As a consequence, it can give an estimation of causality relationships between
nodes (or users), based on their activity timestamps (on a social network for instance), without
knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment
matching method that ﬁts the second-order and the third-order integrated cumulants of the process. A
theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we
show, on numerical experiments, that our approach is indeed very robust with respect to the shape of
the kernels and gives appealing results on the MemeTracker database and on ﬁnancial order book data.

Keywords. Hawkes Process, Causality Inference, Cumulants, Generalized Method of Moments

1 Introduction

In many applications, one needs to deal with data containing a very large number of irregular timestamped
events that are recorded in continuous time. These events can reﬂect, for instance, the activity of users
on a social network, see Subrahmanian et al. (2016), the high-frequency variations of signals in ﬁnance,
see Bacry et al. (2015), the earthquakes and aftershocks in geophysics, see Ogata (1998), the crime
activity, see Mohler et al. (2011) or the position of genes in genomics, see Reynaud-Bouret and Schbath
(2010). The succession of the precise timestamps carries a great deal of information about the dynamics
of the underlying systems. In this context, multidimensional counting processes based models play a
paramount role. Within this framework, an important task is to recover the mutual inﬂuence of the nodes
(i.e., the different components of the counting process), by leveraging on their timestamp patterns, see,
for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou
et al. (2013a); Gomez-Rodriguez et al. (2013); Farajtabar et al. (2015); Xu et al. (2016).

Consider a set of nodes I = {1, . . . , d}. For each i ∈ I, we observe a set Zi of events, where each
τ ∈ Zi labels the occurrence time of an event related to the activity of i. The events of all nodes can

∗massil.achab@m4x.org

1

be represented as a vector of counting processes N t = [N 1
events of node i until time t ∈ R+, namely N i
λt = [λ1

t = (cid:80)
t ](cid:62) associated with the multivariate counting process N t is deﬁned as
P(N i

t ](cid:62), where N i

t counts the number of
τ ∈Zi 1{τ ≤t}. The vector of stochastic intensities

t · · · N d

t · · · λd

t = 1|Ft)

λi
t = lim
dt→0

t+dt − N i
dt

for i ∈ I, where the ﬁltration Ft encodes the information available up to time t. The coordinate λi
t gives
the expected instantaneous rate of event occurrence at time t for node i. The vector λt characterizes
the distribution of N t, see Daley and Vere-Jones (2003), and patterns in the events time-series can be
captured by structuring these intensities.

The Hawkes process introduced in Hawkes (1971) corresponds to an autoregressive structure of
the intensities in order to capture self-excitation and cross-excitation of nodes, which is a phenomenon
typically observed, for instance, in social networks, see for instance Crane and Sornette (2008). Namely,
N t is called a Hawkes point process if the stochastic intensities can be written as

t = µi +
λi

φij(t − t(cid:48))dN j
t(cid:48),

d
(cid:88)

(cid:90) t

j=1

0

where µi ∈ R+ is an exogenous intensity and φij are positive, integrable and causal (with support in R+)
functions called kernels encoding the impact of an action by node j on the activity of node i. Note that
when all kernels are zero, the process is a simple homogeneous multivariate Poisson process.

Most of the litterature uses a parametric approach for estimating the kernels. With no doubt, the
most popular parametrization form is the exponential kernel φij(t) = αijβije−βij t because it deﬁnitely
simpliﬁes the inference algorithm (e.g., the complexity needed for computing the likelihood is much
smaller). When d is large, in order to reduce the number of parameters, some authors choose to arbitrarily
share the kernel shapes across the different nodes. Thus, for instance, in Yang and Zha (2013); Zhou
et al. (2013b); Farajtabar et al. (2015), they choose φij(t) = αijh(t) with αij ∈ R+ quantiﬁes the
intensity of the inﬂuence of j on i and h(t) a (normalized) function that characterizes the time-proﬁle
of this inﬂuence and that is shared by all couples of nodes (i, j) (most often, it is chosen to be either
exponential h(t) = βe−βt or power law h(t) = βt−(β+1)). Both approaches are, most of the time,
highly non-realistic. On the one hand there is a priori no reason for assuming that the time-proﬁle of the
inﬂuence of a node j on a node i does not depend on the pair (i, j). On the other hand, assuming an
exponential shape or a power law shape for a kernel arbitrarily imposes an event impact that is always
instantly maximal and that can only decrease with time, while in practice, there may exist a latency
between an event and its maximal impact.

In order to have more ﬂexibility on the shape of the kernels, nonparametric estimation can be
considered. Expectation-Maximization algorithms can be found in Lewis and Mohler (2011) (for d = 1)
or in Zhou et al. (2013a) (d > 1). An alternative method is proposed in Bacry and Muzy (2016) where
the nonparametric estimation is formulated as a numerical solving of a Wiener-Hopf equation. Another
nonparametric strategy considers a decomposition of kernels on a dictionary of function h1, . . . , hK,
namely φij(t) = (cid:80)K
k are estimated, see Hansen et al. (2015);
Lemonnier and Vayatis (2014) and Xu et al. (2016), where group-lasso is used to induce a sparsity pattern
on the coefﬁcients aij

k hk(t), where the coefﬁcients aij

k that is shared across k = 1, . . . , K.

k=1 aij

Such methods are heavy when d is large, since they rely on likelihood maximization or least squares
minimization within an over-parametrized space in order to gain ﬂexibility on the shape of the kernels.
This is problematic, since the original motivation for the use of Hawkes processes is to estimate the
inﬂuence and causality of nodes, the knowledge of the full parametrization of the model being of little

2

interest for causality purpose.

Our paper solves this problem with a different and more direct approach. Instead of trying to estimate
the kernels φij, we focus on the direct estimation of their integrals. Namely, we want to estimate the
matrix G = [gij] where

gij =

φij(u) du ≥ 0 for 1 ≤ i, j ≤ d.

(1)

(cid:90) +∞

0

As it can be seen from the cluster representation of Hawkes processes (Hawkes and Oakes (1974)), this
integral represents the mean total number of events of type i directly triggered by an event of type j, and
then encodes a notion of causality. Actually, as detailed below (see Section 2.1), such integral can be
related to the Granger causality (Granger (1969)).

The main idea of the method we developed in this paper is to estimate the matrix G directly using a
matching cumulants (or moments) method. Apart from the mean, we shall use second and third-order
cumulants which correspond respectively to centered second and third-order moments. We ﬁrst compute
an estimation (cid:99)M of these centered moments M (G) (they are uniquely deﬁned by G). Then, we look for
a matrix (cid:98)G that minimizes the L2 error (cid:107)M ( (cid:98)G) − (cid:99)M (cid:107)2. Thus the integral matrix (cid:98)G is directly estimated
without making hardly any assumptions on the shape the involved kernels. As it will be shown, this
approach turns out to be particularly robust to the kernel shapes, which is not the case of all previous
Hawkes-based approaches that aim causality recovery. We call this method NPHC (Non Parametric
Hawkes Cumulant), since our approach is of nonparametric nature. We provide a theoretical analysis
that proves the consistency of the NPHC estimator. Our proof is based on ideas from the theory of
Generalized Method of Moments (GMM) but requires an original technical trick since our setting strongly
departs from the standard parametric statistics with i.i.d observations. Note that moment and cumulant
matching techniques proved particularly powerful for latent topic models, in particular Latent Dirichlet
Allocation, see Podosinnikova et al. (2015). A small set of previous works, namely Da Fonseca and
Zaatour (2014); Aït-Sahalia et al. (2010), already used method of moments with Hawkes processes, but
only in a parametric setting. Our work is the ﬁrst to consider such an approach for a nonparametric
counting processes framework.

The paper is organized as follows: in Section 2, we provide the background on the integrated kernels
and the integrated cumulants of the Hawkes process. We then introduce the method, investigate its
complexity and explain the consistency result we prove. In Section 3, we estimate the matrix of Hawkes
kernels’ integrals for various simulated datasets and for real datasets, namely the MemeTracker database
and ﬁnancial order book data. We then provide in Section 4 the technical details skipped in the previous
parts and the proof of our consistency result. Section 5 contains concluding remarks.

2 NPHC: The Non Parametric Hawkes Cumulant method

In this Section, we provide the background on integrals of Hawkes kernels and integrals of Hawkes
cumulants. We then explain how the NPHC method enables estimating G.

2.1 Branching structure and Granger causality

From the deﬁnition of Hawkes process as a Poisson cluster process, see Jovanovi´c et al. (2015) or Hawkes
and Oakes (1974), gij can be simply interpreted as the average total number of events of node i whose
direct ancestor is a given event of node j (by direct we mean that interactions mediated by any other
intermediate event are not counted). In that respect, G not only describes the mutual inﬂuences between

3

nodes, but it also quantiﬁes their direct causal relationships. Namely, introducing the counting function
N i←j
that counts the number of events of i whose direct ancestor is an event of j, we know from Bacry
t
et al. (2015) that

E[dN i←j
(2)
t
where we introduced Λi as the intensity expectation, namely satisfying E[dN i
t ] = Λidt. Note that Λi does
not depend on time by stationarity of N t, which is known to hold under the stability condition (cid:107)G(cid:107) < 1,
where (cid:107)G(cid:107) stands for the spectral norm of G. In particular, this condition implies the non-singularity of
Id − G.

t ] = gijΛjdt,

] = gijE[dN j

Since the question of a real causality is too complex in general, most econometricians agreed on
the simpler deﬁnition of Granger causality Granger (1969). Its mathematical formulation is a statistical
hypothesis test: X causes Y in the sense of Granger causality if forecasting future values of Y is more
successful while taking X past values into account. In Eichler et al. (2016), it is shown that for N t
a multivariate Hawkes process, N j
t w.r.t N t if and only if φij(u) = 0 for
u ∈ R+. Since the kernels take positive values, the latter condition is equivalent to (cid:82) ∞
0 φij(u)du = 0. In
the following, we’ll refer to learning the kernels’ integrals as uncovering causality since each integral
encodes the notion of Granger causality, and is also linked to the number of events directly caused from a
node to another node, as described above at Eq. (2).

t does not Granger-cause N i

2.2

Integrated cumulants of the Hawkes process

A general formula for the integral of the cumulants of a multivariate Hawkes process is provided
in Jovanovi´c et al. (2015). As explained below, for the purpose of our method, we only need to consider
cumulants up to the third order. Given 1 ≤ i, j, k ≤ d, the ﬁrst three integrated cumulants of the Hawkes
process can be deﬁned as follows thanks to stationarity:

Λidt = E(dN i
t )
(cid:90)
(cid:16)

Cijdt =

E(dN i

t dN j

t+τ ) − E(dN i

t )E(dN j

(cid:17)
t+τ )

τ ∈R

(cid:90) (cid:90)

(cid:16)

τ,τ (cid:48)∈R2
− E(dN i

Kijkdt =

E(dN i

t dN j

t+τ dN k

t+τ (cid:48)) + 2E(dN i

t )E(dN j

t+τ )E(dN k

t+τ (cid:48))

t dN j

t+τ )E(dN k

t+τ (cid:48)) − E(dN i

t dN k

t+τ (cid:48))E(dN j

t+τ ) − E(dN j

t+τ dN k

(cid:17)
t+τ (cid:48))E(dN i
t )

,

where Eq. (3) is the mean intensity of the Hawkes process, the second-order cumulant (4) refers to the
integrated covariance density matrix and the third-order cumulant (5) measures the skewness of N t. Using
the martingale representation from Bacry and Muzy (2016) or the Poisson cluster process representation
from Jovanovi´c et al. (2015), one can obtain an explicit relationship between these integrated cumulants
and the matrix G. If one sets

R = (Id − G)−1,

straightforward computations (see Section 4) lead to the following identities:

Λi =

Rimµm

Cij =

ΛmRimRjm

d
(cid:88)

m=1
d
(cid:88)

m=1
d
(cid:88)

m=1

4

Kijk =

(RimRjmCkm + RimCjmRkm + CimRjmRkm − 2ΛmRimRjmRkm).

(3)

(4)

(5)

(6)

(7)

(8)

(9)

Equations (8) and (9) are proved in Section 4. Our strategy is to use a convenient subset of Eqs. (3), (4)
and (5) to deﬁne M , while we use Eqs. (7), (8) and (9) in order to construct the operator that maps a
candidate matrix R to the corresponding cumulants M (R). By looking for (cid:98)R that minimizes R (cid:55)→
(cid:107)M (R) − (cid:99)M (cid:107)2, we obtain, as illustrated below, good recovery of the ground truth matrix G using
Equation (6).

The simplest case d = 1 has been considered in Hardiman and Bouchaud (2014), where it is shown
that one can choose M = {C11} in order to compute the kernel integral. Eq. (8) then reduces to a simple
second-order equation that has a unique solution in R (and consequently a unique G) that accounts for
the stability condition ((cid:107)G(cid:107) < 1).

Unfortunately, for d > 1, the choice M = {Cij}1≤i≤j≤d is not sufﬁcient to uniquely determine the
kernels integrals. In fact, the integrated covariance matrix provides d(d + 1)/2 independent coefﬁcients,
while d2 parameters are needed. It is straightforward to show that the remaining d(d − 1)/2 conditions
can be encoded in an orthogonal matrix O, reﬂecting the fact that Eq. (8) is invariant under the change
R → OR, so that the system is under-determined.

Our approach relies on using the third order cumulant tensor K = [Kijk] which contains (d3 + 3d2 +
2d)/6 > d2 independent coefﬁcients that are sufﬁcient to uniquely ﬁx the matrix G. This can be justiﬁed
intuitively as follows: while the integrated covariance only contains symmetric information, and is thus
unable to provide causal information, the skewness given by the third order cumulant in the estimation
procedure can break the symmetry between past and future so as to uniquely ﬁx G. Thus, our algorithm
consists of selecting d2 third-order cumulant components, namely M = {Kiij}1≤i,j≤d. In particular, we
deﬁne the estimator of R as (cid:98)R ∈ argminRL(R), where

L(R) = (1 − κ)(cid:107)Kc(R) − (cid:100)Kc(cid:107)2

2 + κ(cid:107)C(R) − (cid:98)C(cid:107)2
2,

(10)

where (cid:107) · (cid:107)2 stands for the Frobenius norm, Kc = {Kiij}1≤i,j≤d is the matrix obtained by the contraction
of the tensor K to d2 indices, C is the covariance matrix, while (cid:100)Kc and (cid:98)C are their respective estimators,
see Equations (12), (13) below. It is noteworthy that the above mean square error approach can be seen
as a peculiar Generalized Method of Moments (GMM), see Hall (2005). This framework allows us to
determine the optimal weighting matrix involved in the loss function. However, this approach is unusable
in practice, since the associated complexity is too high. Indeed, since we have d2 parameters, this matrix
has d4 coefﬁcients and GMM calls for computing its inverse leading to a O(d6) complexity. In this work,
we use the coefﬁcient κ to scale the two terms, as

see Section 4.4 for an explanation about the link between κ and the weighting matrix. Finally, the
estimator of G is straightforwardly obtained as

from the inversion of Eq. (6). Let us mention an important point: the matrix inversion in the previous
formula is not the bottleneck of the algorithm. Indeed, its has a complexity O(d3) that is cheap compared
to the computation of the cumulants when n = maxi |Zi| (cid:29) d, which is the typical scaling satisﬁed
in applications. Solving the considered problem on a larger scale, say d (cid:29) 103, is an open question,
even with state-of-the-art parametric and nonparametric approaches, see for instance Zhou et al. (2013a);
Xu et al. (2016); Zhou et al. (2013b); Bacry and Muzy (2016), where the number of components d in
experiments is always around 100 or smaller. Note that, actually, our approach leads to a much faster
algorithm than the considered state-of-the-art baselines, see Tables 1–4 from Section 3 below.

κ =

(cid:107)(cid:100)Kc(cid:107)2
2
2 + (cid:107) (cid:98)C(cid:107)2
2

,

(cid:107)(cid:100)Kc(cid:107)2

(cid:98)G = Id − (cid:98)R

−1

,

5

2.3 Estimation of the integrated cumulants

In this section we present explicit formulas to estimate the three moment-based quantities listed in the
previous section, namely, Λ, C and K. We ﬁrst assume there exists H > 0 such that the truncation
from (−∞, +∞) to [−H, H] of the domain of integration of the quantities appearing in Eqs. (4) and (5),
introduces only a small error. In practice, this amounts to neglecting border effects in the covariance
density and in the skewness density that is a good approximation if the support of the kernel φij(t) is
smaller than H and the spectral norm (cid:107)G(cid:107) satisﬁes (cid:107)G(cid:107) < 1.
In this case, given a realization of a stationary Hawkes process {N t : t ∈ [0, T ]}, as shown in Section 4,
we can write the estimators of the ﬁrst three cumulants (3), (4) and (5) as

(cid:98)Λi =

(cid:88)

1 =

N i
T
T

1
T

1
T

1
T

τ ∈Zi
(cid:88)

(cid:16)

τ ∈Zi
(cid:88)

(cid:16)

(cid:98)Cij =

(cid:98)Kijk =

τ ∈Zi
(cid:98)Λi
T

−

(cid:88)

(cid:88)

τ ∈Zj

τ (cid:48)∈Zk

N j

τ +H − N j

τ −H − 2H (cid:98)Λj(cid:17)

N j

τ +H − N j

τ −H − 2H (cid:98)Λj(cid:17)

·

(cid:16)

N k

τ +H − N k

τ −H − 2H (cid:98)Λk(cid:17)

(2H − |τ (cid:48) − τ |)+ + 4H 2 (cid:98)Λi (cid:98)Λj (cid:98)Λk.

(11)

(12)

(13)

Let us mention the following facts.

Bias. While the ﬁrst cumulant ˆΛi is an unbiased estimator of Λi, the other estimators (cid:98)Cij and (cid:98)Kijk
introduce a bias. However, as we will show, in practice this bias is small and hardly affects
numerical estimations (see Section 3). This is conﬁrmed by our theoretical analysis, which proves
that if H does not grow too fast compared to T , then these estimated cumulants are consistent
estimators of the theoretical cumulants (see Section 2.6).

Complexity. The computations of all the estimators of the ﬁrst, second and third-order cumulants
have complexity respectively O(nd), O(nd2) and O(nd3), where n = maxi |Zi|. However, our
algorithm requires a lot less than that: it computes only d2 third-order terms, of the form (cid:98)Kiij,
leaving us with only O(nd2) operations to perform.

Symmetry. While the values of Λi, Cij and Kijk are symmetric under permutation of the indices, their
estimators are generally not symmetric. We have thus chosen to symmetrize the estimators by
averaging their values over permutations of the indices. Worst case is for the estimator of Kc,
which involves only an extra factor of 2 in the complexity.

2.4 The NPHC algorithm

The objective to minimize in Equation (10) is non-convex. More precisely, the loss function is a
polynomial of R of degree 6. However, the expectations of cumulants Λ and C deﬁned in Eq. (4) and (5)
that appear in the deﬁnition of L(R) are unknown and should be replaced with (cid:98)Λ and (cid:98)C. We denote
(cid:101)L(R) the objective function, where the expectations of cumulants Λi and Cij have been replaced with
their estimators in the right-hand side of Eqs. (8) and (9):

(cid:101)L(R) = (1 − κ)(cid:107)R(cid:12)2 (cid:98)C

+ 2[R (cid:12) ( (cid:98)C − R (cid:98)L)]R(cid:62) − (cid:100)Kc(cid:107)2

2 + κ(cid:107)R (cid:98)LR(cid:62) − (cid:98)C(cid:107)2
2

(14)

(cid:62)

As explained in Choromanska et al. (2015), the loss function of a typical multilayer neural network
with simple nonlinearities can be expressed as a polynomial function of the weights in the network,

6

whose degree is the number of layers. Since the loss function of NPHC writes as a polynomial of
degree 6, we expect good results using optimization methods designed to train deep multilayer neural
networks. We used the AdaGrad from Duchi et al. (2011), a variant of the Stochastic Gradient Descent
with adaptive learning rates. AdaGrad scales the learning rates coordinate-wise using the online variance
of the previous gradients, in order to incorporate second-order information during training. The NPHC
method is summarized schematically in Algorithm 1.

Algorithm 1 Non Parametric Hawkes Cumulant method
Input: N t
Output: (cid:98)G
1: Estimate (cid:98)Λi, (cid:98)Cij, (cid:98)Kiij from Eqs. (11, 12, 13)
2: Design (cid:101)L(R) using the computed estimators.
3: Minimize numerically (cid:101)L(R) so as to obtain (cid:98)R
−1
4: Return (cid:98)G = Id − (cid:98)R

.

Our problem being non-convex, the choice of the starting point has a major effect on the convergence.
Here, the key is to notice that the matrices R that match Equation (8) writes C1/2OL−1/2, with
L = diag(Λ) and O an orthogonal matrix. Our starting point is then simply chosen by setting O = Id in
the previous formula, leading to nice convergence results. Even though our main concern is to retrieve the
matrix G, let us notice we can also obtain an estimation of the baseline intensities’ from Eq. (3), which
leads to (cid:98)µ = (cid:98)R
(cid:98)Λ. An efﬁcient implementation of this algorithm with TensorFlow, see Abadi et al.
(2016), is available on GitHub: https://github.com/achab/nphc.

−1

2.5 Complexity of the algorithm

Compared with existing state-of-the-art methods to estimate the kernel functions, e.g., the ordinary
differential equations-based (ODE) algorithm in Zhou et al. (2013a), the Granger Causality-based
algorithm in Xu et al. (2016), the ADM4 algorithm in Zhou et al. (2013b), and the Wiener-Hopf-based
algorithm in Bacry and Muzy (2016), our method has a very competitive complexity. This can be
understood by the fact that those methods estimate the kernel functions, while in NPHC we only estimate
their integrals. The ODE-based algorithm is an EM algorithm that parametrizes the kernel function
with M basis functions, each being discretized to L points. The basis functions are updated after
solving M Euler-Lagrange equations. If n denotes the maximum number of events per component (i.e.
n = max1≤i≤d |Zi|) then the complexity of one iteration of the algorithm is O(M n3d2 + M L(nd + n2)).
The Granger Causality-based algorithm is similar to the previous one, without the update of the basis
functions, that are Gaussian kernels. The complexity per iteration is O(M n3d2). The algorithm ADM4
is similar to the two algorithms above, as EM algorithm as well, with only one exponential kernel as
basis function. The complexity per iteration is then O(n3d2). The Wiener-Hopf-based algorithm is not
iterative, on the contrary to the previous ones. It ﬁrst computes the empirical conditional laws on many
points, and then invert the Wiener-Hopf system, leading to a O(nd2L + d4L3) computation. Similarly,
our method ﬁrst computes the integrated cumulants, then minimize the objective function with Niter
iterations, and invert the resulting matrix (cid:98)R to obtain (cid:98)G. In the end, the complexity of the NPHC method
is O(nd2 + Niterd3). According to this analysis, summarized in Table 1 below, one can see that in the
regime n (cid:29) d, the NPHC method outperforms all the other ones.

2.6 Theoretical guarantee: consistency

The NPHC method can be phrased using the framework of the Generalized Method of Moments (GMM).
GMM is a generic method for estimating parameters in statistical models. In order to apply GMM,

7

Table 1: Complexity of state-of-the-art methods. NPHC’s complexity is very low , especially in the
regime n (cid:29) d.

Method

ODE Zhou et al. (2013a)
GC Xu et al. (2016)
ADM4 Zhou et al. (2013b)
WH Bacry and Muzy (2016) O(nd2L + d4L3)
O(nd2 + Niterd3)
NPHC

Total complexity
O(NiterM (n3d2 + L(nd + n2)))
O(NiterM n3d2)
O(Nitern3d2)

we have to ﬁnd a vector-valued function g(X, θ) of the data, where X is distributed with respect to a
distribution Pθ0, which satisﬁes the moment condition: E[g(X, θ)] = 0 if and only if θ = θ0, where θ0 is
the “ground truth” value of the parameter. Based on i.i.d. observed copies x1, . . . , xn of X, the GMM
method minimizes the norm of the empirical mean over n samples, (cid:107) 1
i=1 g(xi, θ)(cid:107), as a function of
n
θ, to obtain an estimate of θ0.

(cid:80)n

In the theoretical analysis of NPHC, we use ideas from the consistency proof of the GMM, but the
proof actually relies on very different arguments. Indeed, the integrated cumulants estimators used in
NPHC are not unbiased, as the theory of GMM requires, but asymptotically unbiased. Moreover, the
setting considered here, where data consists of a single realization {N t} of a Hawkes process strongly
departs from the standard i.i.d setting. Our approach is therefore based on the GMM idea but the proof is
actually not using the theory of GMM.

In the following, we use the subscript T to refer to quantities that only depend on the process (Nt) in
the interval [0, T ] (e.g., the truncation term HT , the estimated integrated covariance (cid:98)CT or the estimated
kernel norm matrix (cid:98)GT ). In the next equation, (cid:12) stands for the Hadamard product and (cid:12)2 stands for
the entrywise square of a matrix. We denote G0 = Id − R−1
the true value of G, and the R2d×d valued
0
vector functions

g0(R) =

(cid:98)gT (R) =

(cid:20)

C − RLR(cid:62)
Kc − R(cid:12)2C(cid:62) − 2[R (cid:12) (C − RL)]R(cid:62)
(cid:34)

(cid:21)

(cid:98)CT − R (cid:98)LT R(cid:62)
(cid:62)
T − 2[R (cid:12) ( (cid:98)CT − R (cid:98)LT )]R(cid:62) .

(cid:35)

(cid:100)Kc

T − R(cid:12)2 (cid:98)C

Using these notations, (cid:101)LT (R) can be seen as the weighted squared Frobenius norm of (cid:98)gT (R). Moreover,
P
when T → +∞, one has (cid:98)gT (R)
→
stands for convergence in probability.

P
→ g0(R) under the conditions of the following theorem, where

Theorem 2.1 (Consistency of NPHC). Suppose that (Nt) is observed on R+ and assume that

1. g0(R) = 0 if and only if R = R0;

2. R ∈ Θ, where Θ is a compact set;

4. HT → ∞ and H 2

T /T → 0.

Then

3. the spectral radius of the kernel norm matrix satisﬁes (cid:107)G0(cid:107) < 1;

(cid:18)

(cid:98)GT = Id −

arg min
R∈Θ

(cid:101)LT (R)

(cid:19)−1

P
→ G0.

8

The proof of the Theorem is given in Section 4.5 below. Assumption 3 is mandatory for stability of
the Hawkes process, and Assumptions 3 and 4 are sufﬁcient to prove that the estimators of the integrated
cumulants deﬁned in Equations (11), (12) and (13) are asymptotically consistent. Assumption 2 is a
very mild standard technical assumption allowing to prove consistency for estimators based on moments.
Assumption 1 is a standard asymptotic moment condition, that allows to identity parameters from the
integrated cumulants.

3 Numerical Experiments

In this Section, we provide a comparison of NPHC with the state-of-the art, on simulated datasets with
different kernel shapes, the MemeTracker dataset (social networks) and the order book dynamics dataset
(ﬁnance).

Simulated datasets. We simulated several datasets with Ogata’s Thinning algorithm Ogata (1981)
using the open-source library tick1, each corresponding to a shape of kernel: rectangular, exponential
or power law kernel, see Figure 1 below.

φt
αβ

log φt
log αβγ

φt
αβ

slope ≈ −(1 + γ)

0 γ

γ + 1/β

t

− log β

log t

0

1/β

t

(a) Rectangular kernel
φt = αβ1[0,1/β](t − γ)

(b) Power law kernel on log-log scale
φt = αβγ(1 + βt)−(1+γ)

(c) Exponential kernel
φt = αβ exp(−βt)

Figure 1: The three different kernels used to simulate the datasets.

The integral of each kernel on its support equals α, 1/β can be regarded as a characteristic time-scale
and γ is the scaling exponent for the power law distribution and a delay parameter for the rectangular
one. We consider a non-symmetric block-matrix G to show that our method can effectively uncover
causality between the nodes, see Figure 3. The matrix G has constant entries α on the three blocks -
α = gij = 1/6 for dimension 10 and α = gij = 1/10 for dimension 100 -, and zero outside. The two
other parameters’ values are the same for dimensions 10 and 100. The parameter γ is set to 1/2 on the
three blocks as well, but we set three very different β0, β1 and β2 from one block to the other, with ratio
βi+1/βi = 10 and β0 = 0.1. The number of events is roughly equal to 105 on average over the nodes.
We ran the algorithm on three simulated datasets: a 10-dimensional process with rectangular kernels
named Rect10, a 10-dimensional process with power law kernels named PLaw10 and a 100-dimensional
process with exponential kernels named Exp100.

MemeTracker dataset. We use events of the most active sites from the MemeTracker dataset2. This
dataset contains the publication times of articles in many websites/blogs from August 2008 to April
2009, and hyperlinks between posts. We extract the top 100 media sites with the largest number of
documents, with about 7 million of events. We use the links to trace the ﬂow of information and establish
an estimated ground truth for the matrix G. Indeed, when an hyperlink j appears in a post in website i,
the link j can be regarded as a direct ancestor of the event. Then, Eq. (2) shows gij can be estimated by
N i←j
T

T = #{links j → i}/N j
T .

/N j

1https://github.com/X-DataInitiative/tick
2https://www.memetracker.org/data.html

9

Order book dynamics. We apply our method to ﬁnancial data, in order to understand the self and cross-
inﬂuencing dynamics of all event types in an order book. An order book is a list of buy and sell orders for
a speciﬁc ﬁnancial instrument, the list being updated in real-time throughout the day. This model has ﬁrst
been introduced in Bacry et al. (2016), and models the order book via the following 8-dimensional point
, C(a)
process: Nt = (P (a)
t ), where P (a) (resp. P (b)) counts the number
t
of upward (resp. downward) price moves, T (a) (resp. T (b)) counts the number of market orders at the
ask3 (resp. at the bid) that do not move the price, L(a) (resp. L(b)) counts the number of limit orders at
the ask4 (resp. at the bid) that do not move the price, and C(a) (resp. C(b)) counts the number of cancel
orders at the ask5 (resp. at the bid) that do not move the price. The ﬁnancial data has been provided by
QuantHouse EUROPE/ASIA, and consists of DAX future contracts between 01/01/2014 and 03/01/2014.

, T (a)
t

, P (b)
t

, L(a)
t

, T (b)
t

, L(b)
t

, C(b)

t

Baselines. We compare NPHC to state-of-the art baselines: the ODE-based algorithm (ODE) by Zhou
et al. (2013a), the Granger Causality-based algorithm (GC) by Xu et al. (2016), the ADM4 algorithm
(ADM4) by Zhou et al. (2013b), and the Wiener-Hopf-based algorithm (WH) by Bacry and Muzy (2016).

Metrics. We evaluate the performance of the proposed methods using the computing time, the Relative
Error

RelErr(A, B) =

{aij (cid:54)=0} + |bij|1
1

{aij =0}

1
d2

(cid:88)

i,j

|aij − bij|
|aij|

and the Mean Kendall Rank Correlation

MRankCorr(A, B) =

RankCorr([ai•], [bi•]),

1
d

d
(cid:88)

i=1

where RankCorr(x, y) = 2
d(d−1) (Nconcordant(x, y) − Ndiscordant(x, y)) with Nconcordant(x, y) the number
of pairs (i, j) satisfying xi > xj and yi > yj or xi < xj and yi < yj and Ndiscordant(x, y) the number of
pairs (i, j) for which the same condition is not satisﬁed.

Note that RankCorr score is a value between −1 and 1, representing rank matching, but can take

smaller values (in absolute value) if the entries of the vectors are not distinct.

Figure 2: On Exp100 dataset, estimated (cid:98)G with ADM4 (left), with NPHC (middle) and the ground-
truth matrix G (right). Both ADM4 and NPHC estimates recover the three blocks. However, ADM4
overestimates the integrals on two of the three blocks, while NPHC gives the same value on each blocks.

3i.e. buy orders that are executed and removed from the list
4i.e. buy orders added to the list
5i.e. the number of times a limit order at the ask is canceled: in our dataset, almost 95% of limit orders are canceled before

execution.

10

Figure 3: Estimated (cid:98)G via NPHC on DAX order book data.

Table 2: Metrics on Rect10: comparable rank correlation, strong improvement for relative error and
computing time.

Method

ODE GC

ADM4 WH

NPHC

RelErr
MRankCorr
Time (s)

0.007
0.33
846

0.15
0.02
768

0.10
0.21
709

0.005
0.34
933

0.001
0.34
20

Discussion. We perform the ADM4 estimation, with exponential kernel, by giving the exact value
β = β0 of one block. Let us stress that this helps a lot this baseline, in comparison to NPHC where
nothing is speciﬁed on the shape of the kernel functions. We used M = 10 basis functions for both ODE
and GC algorithms, and L = 50 quadrature points for WH. We did not run WH on the 100-dimensional
datasets, for computing time reasons, because its complexity scales with d4. We ran multi-processed
versions of the baseline methods on 56 cores, to decrease the computing time.

Our method consistently performs better than all baselines, on the three synthetic datasets, on
MemeTracker and on the ﬁnancial dataset, both in terms of Kendall rank correlation and estimation error.
Moreover, we observe that our algorithm is roughly 50 times faster than all the considered baselines.

On Rect10, PLaw10 and Exp100 our method gives very impressive results, despite the fact that it
does not uses any prior shape on the kernel functions, while for instance the ADM4 baseline do. On
Figure 3, we observe that the matrix (cid:98)G estimated with ADM4 recovers well the block for which β = β0,
i.e. the value we gave to the method, but does not perform well on the two other blocks, while the matrix
(cid:98)G estimated with NPHC approximately reaches the true value for each of the three blocks. On these
simulated datasets, NPHC obtains a comparable or slightly better Kendall rank correlation, but improves
a lot the relative error.

On MemeTracker, the baseline methods obtain a high relative error between 9% and 19% while our
method achieves a relative error of 7% which is a strong improvement. Moreover, NPHC reaches a much
better Kendall rank correlation, which proves that it leads to a much better recovery of the relative order
of estimated inﬂuences than all the baselines. Indeed, it has been shown in Zhou et al. (2013a) that kernels
of MemeTracker data are not exponential, nor power law. This partly explains why our approach behaves

11

Table 3: Metrics on PLaw10: comparable rank correlation, strong improvement for relative error and
computing time.

Table 4: Metrics on Exp100: comparable rank correlation, strong improvement for relative error and
computing time.

Method

ODE GC

ADM4 WH

NPHC

RelErr
MRankCorr
Time (s)

0.011
0.31
870

0.09
0.26
781

0.053
0.24
717

0.009
0.34
946

0.0048
0.33
18

Method

ODE GC

ADM4 NPHC

RelErr
MRankCorr
Time (s)

0.092
0.032
3215

0.112
0.009
2950

0.079
0.049
2411

0.008
0.041
47

better.

On the ﬁnancial data, the estimated kernel norm matrix obtained via NPHC, see Figure 3, gave some

interpretable results (see also Bacry et al. (2016)):

1. Any 2 × 2 sub-matrix with same kind of inputs (i.e. Prices changes, Trades, Limits or Cancels) is

symmetric. This shows empirically that ask and bid have symmetric roles.

2. The prices are mostly cross-excited, which means that a price increase is very likely to be followed
by a price decrease, and conversely. This is consistent with the wavy prices we observe on ﬁnancial
markets.

3. The market, limit and cancel orders are strongly self-excited. This can be explained by the
persistence of order ﬂows, and by the splitting of meta-orders into sequences of smaller orders.
Moreover, we observe that orders impact the price without changing it. For example, the increase
of cancel orders at the bid causes downward price moves.

We show in this section how to obtain the equations stated above, the estimators of the integrated
cumulants and the scaling coefﬁcient κ that appears in the objective function. We then prove the theorem
of the paper.

4 Technical details

4.1 Proof of Equation (8)

We denote ν(z) the matrix

νij(z) = Lz

t →

(cid:16)

E(dN i

u+t)

udN j
dudt

− ΛiΛj(cid:17)

,

where Lz(f ) is the Laplace transform of f , and ψt = (cid:80)
refers to the nth auto-
convolution of φt. Then we use the characterization of second-order statistics, ﬁrst formulated in Hawkes
(1971) and fully generalized in Bacry and Muzy (2016),

, where φ((cid:63)n)

n≥1 φ((cid:63)n)

t

t

ν(z) = (Id + L−z(Ψ))L(Id + Lz(Ψ))(cid:62),

12

Table 5: Metrics on MemeTracker: strong improvement in relative error, rank correlation and computing
time.

Method

ODE GC

ADM4 NPHC

RelErr
MRankCorr
Time (s)

0.162
0.07
2944

0.19
0.053
2780

0.092
0.081
2217

0.071
0.095
38

where Lij = Λiδij with δij the Kronecker symbol. Since Id + Lz(Ψ) = (Id − Lz(Φ))−1, taking z = 0
in the previous equation gives

ν(0) = (Id − G)−1L(Id − G(cid:62))−1,

C = RLR(cid:62),

which gives us the result since the entry (i, j) of the last equation gives Cij = (cid:80)

m ΛmRimRjm.

4.2 Proof of Equation (9)

We start from Jovanovi´c et al. (2015), cf. Eqs. (48) to (51), and group some terms:

Kijk =

(cid:88)

ΛmRimRjmRkm

+

+

+

m
(cid:88)

m
(cid:88)

m
(cid:88)

m

RimRjm

ΛnRknL0(ψmn)

RimRkm

ΛnRjnL0(ψmn)

RjmRkm

ΛnRinL0(ψmn).

(cid:88)

n
(cid:88)

n
(cid:88)

n

Using the relations L0(ψmn) = Rmn − δmn and Cij = (cid:80)

m ΛmRimRjm, proves Equation (9).

4.3

Integrated cumulants estimators

For H > 0 let us denote ∆H N i
domain to (−H, H) in Eqs. (4) and (5), one gets by permuting integrals and expectations:

t−H . Let us ﬁrst remark that, if one restricts the integration

t+H − N i

t = N i

(cid:16)

Cijdt = E

Λidt = E(dN i
t )
t (∆H N j
dN i
t (∆H N j
(cid:16)

Kijkdt = E

dN i

(cid:16)

(cid:17)
t − 2HΛj)

t − 2HΛj)(∆H N k

t − 2HΛk)

(cid:17)

− dtΛiE

(∆H N j

t − 2HΛj)(∆H N k

(cid:17)
t − 2HΛk)

.

The estimators (11) and (12) are then naturally obtained by replacing the expectations by their empirical
counterparts, notably

E(dN i
t f (t))
dt

→

1
T

(cid:88)

τ ∈Zi

f (τ ).

13

For the estimator (13), we shall also notice that

E((∆H N j
(cid:90) (cid:90)

=

=

(cid:90)

t − 2HΛj)(∆H N k

t − 2HΛk))

1[−H,H](t)1[−H,H](t(cid:48))Cjk

t−t(cid:48)dtdt(cid:48)

(2H − |t|)+Cjk

t dt.

We estimate the last integral with the remark above.

4.4 Choice of the scaling coefﬁcient κ

Following the theory of GMM, we denote m(X, θ) a function of the data, where X is distributed with
respect to a distribution Pθ0, which satisﬁes the moment conditions g(θ) = E[m(X, θ)] = 0 if and
only if θ = θ0, the parameter θ0 being the ground truth. For x1, . . . , xN observed copies of X, we
denote (cid:98)gi(θ) = m(xi, θ), the usual choice of weighting matrix is (cid:99)WN (θ) = 1
i=1 (cid:98)gi(θ)(cid:98)gi(θ)(cid:62), and
the objective to minimize is then

(cid:80)N

N

(cid:32)

1
N

N
(cid:88)

i=1

(cid:33)

(cid:16)

(cid:98)gi(θ)

(cid:99)WN (θ1)

(cid:32)

(cid:17)−1

(cid:33)

(cid:98)gi(θ)

,

1
N

N
(cid:88)

i=1

(15)

where θ1 is a constant vector. Instead of computing the inverse weighting matrix, we rather use its
projection on {αId : α ∈ R}. It can be shown that the projection choses α as the mean eigenvalue of
(cid:99)WN (θ1). We can easily compute the sum of its eigenvalues:

Tr((cid:99)WN (θ1)) =

Tr((cid:98)gi(θ1)(cid:98)gi(θ1)(cid:62)) =

Tr((cid:98)gi(θ1)(cid:62)

(cid:98)gi(θ1)) =

||(cid:98)gi(θ1)||2
2.

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

i=1

(cid:104)
vec[(cid:100)Kc − Kc(R)], vec[ (cid:98)C − C(R)]

In our case, (cid:98)g(R) =
. Considering a block-wise
weighting matrix, one block for (cid:100)Kc − Kc(R) and the other for (cid:98)C − C(R), the sum of the eigenvalues of
the ﬁrst block becomes (cid:107)(cid:100)Kc − Kc(R)(cid:107)2
2, and (cid:107) (cid:98)C − C(R)(cid:107)2
2 for the second. We compute the previous
terms with R1 = 0. All together, the objective function to minimize is

∈ R2d2

(cid:105)(cid:62)

1
(cid:107)(cid:100)Kc(cid:107)2
2

(cid:16)

(cid:107)Kc(R) − (cid:100)Kc(cid:107)2

2 +

(cid:107)C(R) − (cid:98)C(cid:107)2
2.

(16)

1
(cid:107) (cid:98)C(cid:107)2
2

(cid:17)−1

, and setting κ = (cid:107)(cid:100)Kc(cid:107)2

2/((cid:107)(cid:100)Kc(cid:107)2

2 + (cid:107) (cid:98)C(cid:107)2

2), we

2 + 1/(cid:107) (cid:98)C(cid:107)2
Dividing this function by
2
obtaind the loss function given in Equation (10).

1/(cid:107)(cid:100)Kc(cid:107)2

4.5 Proof of the Theorem

The main difference with the usual Generalized Method of Moments, see Hansen (1982), relies in the
relaxation of the moment conditions, since we have E[(cid:98)gT (θ0)] = mT (cid:54)= 0. We adapt the proof of
consistency given in Newey and McFadden (1994).

We can relate the integral of the Hawkes process’s kernels to the integrals of the cumulant densities,
from Jovanovi´c et al. (2015). Our cumulant matching method would fall into the usual GMM framework
if we could estimate - without bias - the integral of the covariance on R, and the integral of the skewness
on R2. Unfortunately, we can’t do that easily. We can however estimate without bias (cid:82) f T
t dt

t Cij

14

t Kijk

and (cid:82) f T
t dt with f T a compact supported function on [−HT , HT ] that weakly converges to 1,
t = 1[−HT ,HT ](t). Denoting (cid:98)Cij,(T ) the estimator of
with HT −→ ∞. In most cases we will take f T
(cid:82) f T
t Cij
t dt − Cij| can be considered a proxy to the distance to
the classical GMM. This distance has to go to zero to make the rest of GMM’s proof work: the estimator
(cid:98)Cij,(T ) is then asymptotically unbiased towards Cij when T goes to inﬁnity.

t dt, the term |E[ (cid:98)Cij,(T )] − Cij| = | (cid:82) f T

t Cij

4.5.1 Notations

We observe the multivariate point process (N t) on R+, with Zi the events of the ith component. We will
often write covariance / skewness instead of integrated covariance / skewness. In the rest of the document,
we use the following notations.

Hawkes kernels’ integrals Gtrue = (cid:82) Φtdt = ((cid:82) φij

t dt)ij = Id − (Rtrue)−1

Theoretical mean matrix L = diag(Λ1, . . . , Λd)

Theoretical covariance C = RtrueL(Rtrue)(cid:62)

Theoretical skewness Kc = (Kiij)ij = (Rtrue)(cid:12)2C(cid:62) + 2[Rtrue (cid:12) (C − RtrueL)](Rtrue)(cid:62)

Filtering function

f T ≥ 0

supp(f T ) ⊂ [−HT , HT ]

F T = (cid:82) f T

s ds

t = f T
(cid:101)f T
−t

Events sets Zi,T,1 = Zi ∩ [HT , T + HT ]

Zj,T,2 = Zj ∩ [0, T + 2HT ]

Estimators of the mean

(cid:98)Λi =

N i

T +HT
T

−N i

HT

Estimator of the covariance

(cid:98)Cij,(T ) = 1
T

(cid:80)

τ ∈Zi,T,1

τ (cid:48)∈Zj,T,2 fτ (cid:48)−τ − (cid:101)ΛjF T (cid:17)

Estimator of the skewness6

N j
T +2HT
T +2HT

(cid:101)Λj =
(cid:16)(cid:80)





(cid:98)Kijk,(T ) =

1
T





(cid:88)

(cid:88)

fτ (cid:48)−τ − (cid:101)ΛjF T





(cid:88)

fτ (cid:48)−τ − (cid:101)ΛkF T

τ ∈Zi,T,1

−

(cid:98)Λi
T + 2HT

τ (cid:48)∈Zj,T,2


(cid:88)



τ (cid:48)∈Zj,T,2

τ (cid:48)(cid:48)∈Zk,T,2

τ (cid:48)(cid:48)∈Zk,T,2

(cid:88)

(f T (cid:63) (cid:101)f T )τ (cid:48)−τ (cid:48)(cid:48) − (cid:101)Λk(F T )2









6When f T

t = 1[−HT ,HT ](t), we remind that (f T (cid:63) (cid:101)f T )t = (2HT − |t|)+. This leads to the estimator we showed in the

article.

15

GMM related notations

θ = R and

θ0 = Rtrue

(cid:20)

g0(θ) = vec

C − RLR(cid:62)

C(cid:62) − 2[R (cid:12) (C − RL)]R(cid:62)

(cid:21)

∈ R2d2

Kc − R(cid:12)2
(cid:34)

(T )

(cid:98)C
(T )

− R (cid:98)LR(cid:62)
)(cid:62) − 2[R (cid:12) ( (cid:98)C

(T )

− R (cid:98)L)]R(cid:62)

(cid:35)

∈ R2d2

(cid:98)gT (θ) = vec

(cid:100)Kc(T )

− R(cid:12)2

( (cid:98)C

Q0(θ) = g0(θ)(cid:62)W g0(θ)
(cid:98)QT (θ) = (cid:98)gT (θ)(cid:62)(cid:99)WT (cid:98)gT (θ)

4.5.2 Consistency

First, let’s remind a useful theorem for consistency in GMM from Newey and McFadden (1994).

Theorem 4.1. If there is a function Q0(θ) such that (i) Q0(θ) is uniquely maximized at θ0; (ii) Θ
is compact; (iii) Q0(θ) is continuous; (iv) (cid:98)QT (θ) converges uniformly in probability to Q0(θ), then
(cid:98)θT = arg max (cid:98)QT (θ)

−→ θ0.

P

We can now prove the consistency of our estimator.

Theorem 4.2. Suppose that (Nt) is observed on R+, (cid:99)WT

P

−→ W , and

1. W is positive semi-deﬁnite and W g0(θ) = 0 if and only if θ = θ0,

2. θ ∈ Θ, which is compact,

3. the spectral radius of the kernel norm matrix satisﬁes ||Φ||∗ < 1,
4. ∀i, j, k ∈ [d], (cid:82) f T

u du and (cid:82) f T

u du → (cid:82) Cij

u,v dudv → (cid:82) Kijk

u f T

v Kijk

u Cij

u,v dudv,

5. (F T )2/T

−→ 0 and ||f ||∞ = O(1).

P

Then

P

(cid:98)θT

−→ θ0.

Remark 1. In practice, we use a constant sequence of weighting matrices: (cid:99)WT = Id.

Proof. Proceed by verifying the hypotheses of Theorem 2.1 from Newey and McFadden (1994). Con-
dition 2.1(i) follows by (i) and by Q0(θ) = [W 1/2g0(θ)](cid:62)[W 1/2g0(θ)] > 0 = Q0(θ0). Indeed, there
exists a neighborhood N of θ0 such that θ ∈ N \{θ0} and g0(θ) (cid:54)= 0 since g0(θ) is a polynom. Condition
2.1(ii) follows by (ii). Condition 2.1(iii) is satisﬁed since Q0(θ) is a polynom. Condition 2.1(iv) is
harder to prove. First, since (cid:98)gT (θ) is a polynom of θ, we prove easily that E[supθ∈Θ |(cid:98)gT (θ)|] < ∞. Then,
by Θ compact, g0(θ) is bounded on Θ, and by the triangle and Cauchy-Schwarz inequalities,

(cid:12) (cid:98)QT (θ) − Q0(θ)(cid:12)
(cid:12)
(cid:12)

≤ (cid:12)
(cid:12)((cid:98)gT (θ) − g0(θ))(cid:62)(cid:99)WT ((cid:98)gT (θ) − g0(θ))(cid:12)
(cid:12)
(cid:12) + (cid:12)
T )((cid:98)gT (θ) − g0(θ))(cid:12)
+ (cid:12)
(cid:12)g0(θ)(cid:62)((cid:99)WT + (cid:99)W (cid:62)
≤ (cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)2(cid:107)(cid:99)WT (cid:107) + 2(cid:107)g0(θ)(cid:107)(cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)(cid:107)(cid:99)WT (cid:107) + (cid:107)g0(θ)(cid:107)2(cid:107)(cid:99)WT − W (cid:107).

(cid:12)g0(θ)(cid:62)((cid:99)WT − W )g0(θ)(cid:12)
(cid:12)

To prove supθ∈Θ
Θ compact, it is sufﬁcient to prove that (cid:107)(cid:98)L−L(cid:107)

P
−→ 0, we should now prove that supθ∈Θ(cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)
−C(cid:107)

−→ 0, and (cid:107)(cid:100)Kc(T )

(cid:12) (cid:98)QT (θ) − Q0(θ)(cid:12)
(cid:12)
(cid:12)

−→ 0, (cid:107) (cid:98)C

(T )

P

P

−Kc(cid:107)

−→ 0. By
P

−→ 0.

P

16

Proof that (cid:107) (cid:98)L − L(cid:107)

P

−→ 0

The estimator of L is unbiased so let’s focus on the variance of (cid:98)L.

E[((cid:98)Λi − Λi)2] = E

(cid:19)2(cid:35)

(dN i

t − Λidt)

E[(dN i

t − Λidt)(dN i

t(cid:48) − Λidt(cid:48))]

(cid:90) T +HT

(cid:34)(cid:18) 1
T

HT
(cid:90) T +HT

(cid:90) T +HT

HT

HT

(cid:90) T +HT

(cid:90) T +HT

HT

(cid:90) T +HT

HT

HT

Ciidt =

−→ 0

Cii

t(cid:48)−tdtdt(cid:48)

Cii
T

P

=

=

≤

1
T 2

1
T 2

1
T 2

By Markov inequality, we have just proved that (cid:107)(cid:98)L − L(cid:107)

−→ 0.

Proof that (cid:107) (cid:98)C

(T )

P

− C(cid:107)

−→ 0

First, let’s remind that E( (cid:98)C

) (cid:54)= C. Indeed,

E

(cid:16)

(cid:98)Cij,(T )(cid:17)

= E

(T )

(cid:18) 1
T
(cid:18) 1
T

(cid:90) T +HT

(cid:90) T +2HT

HT

(cid:90) T +HT

(cid:90) T +2HT −t

dN i
t

dN i
t

0

−t
(cid:16)

HT
(cid:90) T +HT

(cid:90) HT

HT
fsCij

−HT
s ds + (cid:15)ij,T,HT F T

= E

1
T
(cid:90)

=

=

Now,

dN j

t(cid:48)ft(cid:48)−t − (cid:98)Λi (cid:101)ΛjF T

(cid:19)

(cid:19)

dN j

t+sfs − ΛiΛjF T

+ (cid:15)ij,T,HT F T

fsE

dN i

t dN j

t+s − ΛiΛjds

+ (cid:15)ij,T,HT F T

(cid:17)

(cid:15)ij,T,HT = E

(cid:16)

ΛiΛj − (cid:98)Λi (cid:101)Λj(cid:17)
(cid:90) T +HT
1
T 2

0

HT

(cid:90) T +2HT

(cid:90) T +HT

(cid:90) T +2HT

= −

= −

1
T 2

1
T

HT
(cid:90) (cid:32)

= −

1 +

Cij
t−t(cid:48)dtdt(cid:48)
(cid:19)−(cid:33)+

Cij

t dt

0

(cid:18) HT − |t|
T

(cid:16)

E

dN i

t dN j

t(cid:48) − ΛiΛjdtdt(cid:48)(cid:17)

(T )

Since f satisﬁes F T = o(T ), we have E( (cid:98)C
E( (cid:98)C
Let’s now focus on the variance of (cid:98)Cij,(T ) : V( (cid:98)Cij,(T )) = E

) −→ C.

−→ 0.

)(cid:107)

P

(T )

It remains now to prove that (cid:107) (cid:98)C

(T )

−

(cid:16)

( (cid:98)Cij,(T ))2(cid:17)

− E( (cid:98)Cij,(T ))2.

17

Now,

(cid:16)

E

( (cid:98)Cij,(T ))2(cid:17)


1
T 2

1
T 2
(cid:90)

= E



(cid:32)

= E

=

1
T 2

And,

(cid:88)

(fτ (cid:48)−τ − F T /(T + 2HT ))(fη(cid:48)−η − F T /(T + 2HT ))

(τ,η,τ (cid:48),η(cid:48))∈(Zi,T,1)2×(Zj,T,2)2
(cid:90)

(cid:90)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)

(cid:90)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)∈[0,T +2HT ]

dN i

t dN j

t(cid:48)dN i

sdN j

s(cid:48)(ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))

(cid:16)

E

dN i

t dN j

t(cid:48)dN i

sdN j
s(cid:48)

(cid:17)

· (ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))





(cid:33)

E( (cid:98)Cij,(T ))2
1
T 2

=

(cid:90)

(cid:90)

(cid:16)

E

dN i

t dN j
t(cid:48)

(cid:17)

(cid:16)

E

dN i

sdN j
s(cid:48)

(cid:17)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)∈[0,T +2HT ]

· (ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))

Then, the variance involves the integration towards the difference of moments µr,s,t,u − µr,sµt,u. Let’s
write it as a sum of cumulants, since cumulants density are integrable.

µr,s,t,u − µr,sµt,u = κr,s,t,u + κr,s,tκu[4] + κr,sκt,u[3] + κr,sκtκu[6] + κrκsκtκu − (κr,s + κrκs)(κt,u + κtκu)

= κr,s,t,u
+ κr,s,tκu + κu,r,sκt + κt,u,rκs + κs,t,uκr
+ κr,tκs,u + κr,uκs,t
+ κr,tκsκu + κr,uκsκt + κs,tκrκu + κs,tκrκu

In the rest of the proof, we denote at = 1t∈[HT ,T +HT ], bt = 1t∈[0,T +2HT ], ct = 1t∈[−HT ,HT ], gt =
ft − 1
Before starting the integration of each term, let’s remark that:

T +2HT

F T

u,v (skewness density) and M ijkl

u,v,w (fourth cumulant density) are positive
· with positive coefﬁcients. The integrals of the singular parts are

1. Ψt = (cid:80)

n≥1 Φ((cid:63)n)

t ≥ 0 since Φt ≥ 0.

2. The regular parts of Cij

u , Kijk

3.

as polynoms of integrals of ψab
positive as well.
(a) (cid:82) atbt(cid:48)ft(cid:48)−tdtdt(cid:48) = T F T
(b) (cid:82) atbt(cid:48)gt(cid:48)−tdtdt(cid:48) = 0
(c) (cid:82) atbt(cid:48)|gt(cid:48)−t|dtdt(cid:48) ≤ 2T F T
4. ∀t ∈ R, at(b (cid:63) (cid:101)g)t = 0, where (cid:101)gs = g−s.

18

Fourth cumulant We want here to compute (cid:82) κi,j,i,j
We remark that |gt(cid:48)−tgs(cid:48)−s| ≤ (||f ||∞(1 + 2HT /T ))2 ≤ 4||f ||2
∞.
(cid:19)2 (cid:90)

(cid:90)

t,t(cid:48),s,s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)(cid:12)
κi,j,i,j

(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)

1
T 2

t,t(cid:48),s,s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48).

dtat

dt(cid:48)bt(cid:48)

dsas

ds(cid:48)bs(cid:48)M ijij

t(cid:48)−t,s−t,s(cid:48)−t

(cid:90)

(cid:90)

(cid:90)

(cid:90)

dtat

dt(cid:48)bt(cid:48)

dsas

dwM ijij

t(cid:48)−t,s−t,w

dtat

M ijij

u,v,wdudvdw

(cid:90)

(cid:90)

(cid:90)

(cid:18) 2||f ||∞
T
(cid:18) 2||f ||∞
T
(cid:18) 2||f ||∞
T
4||f ||2
∞
T

≤

≤

≤

(cid:19)2 (cid:90)

(cid:19)2 (cid:90)

M ijij −→
T →∞

0

Third × First We have four terms, but only two different forms since the roles of (s, s(cid:48)) and (t, t(cid:48))
are symmetric.
First form

Second form

(cid:90)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
κi,j,j
t,t(cid:48),s(cid:48)ΛiGtdt
(cid:12)
(cid:12) =

(cid:90)

κi,j,i
t,t(cid:48),sΛjGtdt =

κi,j,i
t,t(cid:48),satbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)

(cid:90)

(cid:90)

Λj
T 2
Λj
T 2

=

= 0

κi,j,i
t,t(cid:48),satbt(cid:48)as(b (cid:63) (cid:101)g)sgt(cid:48)−tdtdt(cid:48)ds
since as(b (cid:63) (cid:101)g)s = 0

t,t(cid:48),s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)(cid:12)
κi,j,j
t,t(cid:48),s(cid:48)atbt(cid:48)gt(cid:48)−tbs(cid:48)(a (cid:63) g)s(cid:48)dtdt(cid:48)ds(cid:48)(cid:12)
κi,j,j

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:90)

(cid:90)

(cid:12)
(cid:12)
(cid:12)

=

Λi
T 2
Λi
(cid:12)
(cid:12)
(cid:12)
T 2
Λi
≤
T 2 2||f ||∞
≤ 4||f ||∞KijjΛi F T
T

(cid:90)

−→
T →∞

0

ds(cid:48)bs(cid:48)(a (cid:63) |g|)s(cid:48)

dtat

dt(cid:48)bt(cid:48)Kijj

t(cid:48)−s(cid:48),t−s(cid:48)

(cid:90)

(cid:90)

Second × Second
First form

(cid:90)

(cid:12)
(cid:12)
(cid:12)

t,sκj,j
κi,i

t(cid:48),s(cid:48)Gtdt

Second form

Second × First × First
First form

(cid:90)

t(cid:48)−s(cid:48)atbt(cid:48)|gt(cid:48)−t|asbs(cid:48)dtdt(cid:48)dsds(cid:48)

(cid:12)
(cid:12)
(cid:12) ≤

(cid:90)

t−sCjj
Cii
(cid:90)

2||f ||∞
T 2
2||f ||∞
T 2 CiiCjj
≤
≤ 4||f ||∞CiiCjj F T
T

atbt(cid:48)|gt(cid:48)−t|dtdt(cid:48)

−→
T →∞

0

(cid:90)

(cid:12)
(cid:12)
(cid:12)

t,s(cid:48)κi,j
κi,j

(cid:12)
(cid:12)
t(cid:48),sGtdt

(cid:12) ≤ 4||f ||∞(Cij)2 F T

−→
T →∞

0

T

κi,j
t,t(cid:48)ΛiΛjGtdt =

κi,j
t,t(cid:48)atbt(cid:48)gt(cid:48)−tdtdt(cid:48)

asbs(cid:48)gs(cid:48)−sdsds(cid:48) = 0

(cid:90)

(cid:90)

ΛiΛj
T 2

19

Second form

(cid:90)

κi,i
t,sΛjΛjGtdt =

(cid:19)2 (cid:90)

(cid:18) Λj
T

κi,i
t,satbt(cid:48)gt(cid:48)−tas(b (cid:63) (cid:101)g)sdtdt(cid:48)ds = 0

We have just proved that V( (cid:98)C
− C(cid:107)
and ﬁnally that (cid:107) (cid:98)C

(T )

(T )

P

)
P

−→ 0.

−→ 0. By Markov inequality, it ensures us that (cid:107) (cid:98)C

(T )

−E( (cid:98)C

(T )

P

)(cid:107)

−→ 0,

− Kc(cid:107)

Proof that (cid:107)(cid:100)Kc(T )
The scheme of the proof is similar to the previous one. The upper bounds of the integrals involve the same
kind of terms, plus the new term (F T )2/T that goes to zero thanks to the assumption 5 of the theorem.

−→ 0

P

5 Conclusion

In this paper, we introduce a simple nonparametric method (the NPHC algorithm) that leads to a fast and
robust estimation of the matrix G of the kernel integrals of a Multivariate Hawkes process that encodes
Granger causality between nodes. This method relies on the matching of the integrated order 2 and
order 3 empirical cumulants, which represent the simplest set of global observables containing sufﬁcient
information to recover the matrix G. Since this matrix fully accounts for the self- and cross- inﬂuences of
the process nodes (that can represent agents or users in applications), our approach can naturally be used
to quantify the degree of endogeneity of a system and to uncover the causality structure of a network.

By performing numerical experiments involving very different kernel shapes, we show that the
baselines, involving either parametric or non-parametric approaches are very sensible to model misspeci-
ﬁcation, do not lead to accurate estimation, and are numerically expensive, while NPHC provides fast,
robust and reliable results. This is conﬁrmed on the MemeTracker database, where we show that NPHC
outperforms classical approaches based on EM algorithms or the Wiener-Hopf equations. Finally, the
NPHC algorithm provided very satisfying results on ﬁnancial data, that are consistent with well-known
stylized facts in ﬁnance.

Acknowledgements

This work beneﬁted from the support of the chair “Changing markets”, CMAP École Polytechnique and
École Polytechnique fund raising - Data Science Initiative.

The authors want to thank Marcello Rambaldi for fruitful discussions on order book data’s experi-

ments.

20

References

M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.
arXiv preprint arXiv:1603.04467, 2016.

Y. Aït-Sahalia, J. Cacho-Diaz, and R. JA Laeven. Modeling ﬁnancial contagion using mutually exciting

jump processes. Technical report, National Bureau of Economic Research, 2010.

E. Bacry and J.-F. Muzy. First- and second-order statistics characterization of hawkes processes and

non-parametric estimation. IEEE Transactions on Information Theory, 62(4):2184–2202, 2016.

E. Bacry, I. Mastromatteo, and J.-F. Muzy. Hawkes processes in ﬁnance. Market Microstructure and

Liquidity, 1(01):1550005, 2015.

E. Bacry, T. Jaisson, and J.-F. Muzy. Estimation of slowly decreasing hawkes kernels: application to

high-frequency order book dynamics. Quantitative Finance, pages 1–23, 2016.

A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of multilayer

networks. In AISTATS, 2015.

R. Crane and D. Sornette. Robust dynamic classes revealed by measuring the response function of a

social system. Proceedings of the National Academy of Sciences, 105(41), 2008.

J. Da Fonseca and R. Zaatour. Hawkes process: Fast calibration, application to trade clustering, and

diffusive limit. Journal of Futures Markets, 34(6):548–579, 2014.

D. J. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes Volume I: Elementary

Theory and Methods. Springer Science & Business Media, 2003.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.

M. Eichler, R. Dahlhaus, and J. Dueck. Graphical modeling for multivariate hawkes processes with non-
parametric link functions. Journal of Time Series Analysis, pages n/a–n/a, 2016. ISSN 1467-9892. doi:
10.1111/jtsa.12213. URL http://dx.doi.org/10.1111/jtsa.12213. 10.1111/jtsa.12213.

M. Farajtabar, Y. Wang, M. Rodriguez, S. Li, H. Zha, and L. Song. Coevolve: A joint point process model
for information diffusion and network co-evolution. In Advances in Neural Information Processing
Systems, pages 1945–1953, 2015.

M. Gomez-Rodriguez, J. Leskovec, and B. Schölkopf. Modeling information propagation with survival

theory. Proceedings of the International Conference on Machine Learning, 2013.

C. W. J. Granger. Investigating causal relations by econometric models and cross-spectral methods.
Econometrica, 37(3):424–438, 1969. ISSN 00129682, 14680262. URL http://www.jstor.
org/stable/1912791.

A. R. Hall. Generalized Method of Moments. Oxford university press, 2005.

L. P. Hansen. Large sample properties of generalized method of moments estimators. Econometrica:

Journal of the Econometric Society, pages 1029–1054, 1982.

N. R. Hansen, P. Reynaud-Bouret, and V. Rivoirard. Lasso and probabilistic inequalities for multivariate

point processes. Bernoulli, 21(1):83–143, 2015.

21

S. J. Hardiman and J.-P. Bouchaud. Branching-ratio approximation for the self-exciting Hawkes process.

Phys. Rev. E, 90(6):062807, December 2014. doi: 10.1103/PhysRevE.90.062807.

A. G. Hawkes. Point spectra of some mutually exciting point processes. Journal of the Royal Statistical
Society. Series B (Methodological), 33(3):438–443, 1971. ISSN 00359246. URL http://www.
jstor.org/stable/2984686.

A. G. Hawkes and D. Oakes. A cluster process representation of a self-exciting process. Journal of

Applied Probability, pages 493–503, 1974.

S. Jovanovi´c, J. Hertz, and S. Rotter. Cumulants of Hawkes point processes. Phys. Rev. E, 91(4):042802,

April 2015. doi: 10.1103/PhysRevE.91.042802.

R. Lemonnier and N. Vayatis. Nonparametric markovian learning of triggering kernels for mutually
exciting and mutually inhibiting multivariate hawkes processes. In Machine Learning and Knowledge
Discovery in Databases, pages 161–176. Springer, 2014.

E. Lewis and G. Mohler. A nonparametric em algorithm for multiscale hawkes processes. Journal of

Nonparametric Statistics, 2011.

G. O. Mohler, M. B. Short, P. J. Brantingham, F. P. Schoenberg, and G. E. Tita. Self-exciting point

process modeling of crime. Journal of the American Statistical Association, 2011.

W. K Newey and D. McFadden. Large sample estimation and hypothesis testing. Handbook of economet-

rics, 4:2111–2245, 1994.

27(1):23–31, 1981.

Y. Ogata. On lewis’ simulation method for point processes. Information Theory, IEEE Transactions on,

Y. Ogata. Space-time point-process models for earthquake occurrences. Annals of the Institute of

Statistical Mathematics, 50(2):379–402, 1998.

A. Podosinnikova, F. Bach, and S. Lacoste-Julien. Rethinking lda: moment matching for discrete ica. In

Advances in Neural Information Processing Systems, pages 514–522, 2015.

P. Reynaud-Bouret and S. Schbath. Adaptive estimation for hawkes processes; application to genome

analysis. The Annals of Statistics, 38(5):2781–2822, 2010.

V.S. Subrahmanian, A. Azaria, S. Durst, V. Kagan, A. Galstyan, K. Lerman, L. Zhu, E. Ferrara, A. Flam-

mini, and F. Menczer. The darpa twitter bot challenge. Computer, 49(6):38–46, 2016.

H. Xu, M. Farajtabar, and H. Zha. Learning granger causality for hawkes processes. In Proceedings of

The 33rd International Conference on Machine Learning, pages 1717–1726, 2016.

S.-H. Yang and H. Zha. Mixture of mutually exciting processes for viral diffusion. In Proceedings of the

International Conference on Machine Learning, 2013.

K. Zhou, H. Zha, and L. Song. Learning triggering kernels for multi-dimensional hawkes processes. In

Proceedings of the International Conference on Machine Learning, pages 1301–1309, 2013a.

K. Zhou, H. Zha, and L. Song. Learning social infectivity in sparse low-rank networks using multi-

dimensional hawkes processes. AISTATS, 2013b.

22

7
1
0
2
 
y
a
M
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
3
3
6
0
.
7
0
6
1
:
v
i
X
r
a

Uncovering Causality from Multivariate Hawkes Integrated
Cumulants

Massil Achab∗1, Emmanuel Bacry1, Stéphane Gaiffas1, Iacopo Mastromatteo2, and
Jean-François Muzy1,3

1Centre de Mathématiques Appliquées, CNRS, Ecole Polytechnique, UMR 7641, 91128 Palaiseau, France
2Capital Fund Management, 23 rue de l’Université, 75007 Paris, France
3Laboratoire Sciences Pour l’Environnement, Université de Corse, 7 Avenue Jean Nicoli, 20250 Corte, France

May 31, 2017

Abstract

We design a new nonparametric method that allows one to estimate the matrix of integrated
kernels of a multivariate Hawkes process. This matrix not only encodes the mutual inﬂuences of each
node of the process, but also disentangles the causality relationships between them. Our approach is
the ﬁrst that leads to an estimation of this matrix without any parametric modeling and estimation of
the kernels themselves. As a consequence, it can give an estimation of causality relationships between
nodes (or users), based on their activity timestamps (on a social network for instance), without
knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment
matching method that ﬁts the second-order and the third-order integrated cumulants of the process. A
theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we
show, on numerical experiments, that our approach is indeed very robust with respect to the shape of
the kernels and gives appealing results on the MemeTracker database and on ﬁnancial order book data.

Keywords. Hawkes Process, Causality Inference, Cumulants, Generalized Method of Moments

1 Introduction

In many applications, one needs to deal with data containing a very large number of irregular timestamped
events that are recorded in continuous time. These events can reﬂect, for instance, the activity of users
on a social network, see Subrahmanian et al. (2016), the high-frequency variations of signals in ﬁnance,
see Bacry et al. (2015), the earthquakes and aftershocks in geophysics, see Ogata (1998), the crime
activity, see Mohler et al. (2011) or the position of genes in genomics, see Reynaud-Bouret and Schbath
(2010). The succession of the precise timestamps carries a great deal of information about the dynamics
of the underlying systems. In this context, multidimensional counting processes based models play a
paramount role. Within this framework, an important task is to recover the mutual inﬂuence of the nodes
(i.e., the different components of the counting process), by leveraging on their timestamp patterns, see,
for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou
et al. (2013a); Gomez-Rodriguez et al. (2013); Farajtabar et al. (2015); Xu et al. (2016).

Consider a set of nodes I = {1, . . . , d}. For each i ∈ I, we observe a set Zi of events, where each
τ ∈ Zi labels the occurrence time of an event related to the activity of i. The events of all nodes can

∗massil.achab@m4x.org

1

be represented as a vector of counting processes N t = [N 1
events of node i until time t ∈ R+, namely N i
λt = [λ1

t = (cid:80)
t ](cid:62) associated with the multivariate counting process N t is deﬁned as
P(N i

t ](cid:62), where N i

t counts the number of
τ ∈Zi 1{τ ≤t}. The vector of stochastic intensities

t · · · N d

t · · · λd

t = 1|Ft)

λi
t = lim
dt→0

t+dt − N i
dt

for i ∈ I, where the ﬁltration Ft encodes the information available up to time t. The coordinate λi
t gives
the expected instantaneous rate of event occurrence at time t for node i. The vector λt characterizes
the distribution of N t, see Daley and Vere-Jones (2003), and patterns in the events time-series can be
captured by structuring these intensities.

The Hawkes process introduced in Hawkes (1971) corresponds to an autoregressive structure of
the intensities in order to capture self-excitation and cross-excitation of nodes, which is a phenomenon
typically observed, for instance, in social networks, see for instance Crane and Sornette (2008). Namely,
N t is called a Hawkes point process if the stochastic intensities can be written as

t = µi +
λi

φij(t − t(cid:48))dN j
t(cid:48),

d
(cid:88)

(cid:90) t

j=1

0

where µi ∈ R+ is an exogenous intensity and φij are positive, integrable and causal (with support in R+)
functions called kernels encoding the impact of an action by node j on the activity of node i. Note that
when all kernels are zero, the process is a simple homogeneous multivariate Poisson process.

Most of the litterature uses a parametric approach for estimating the kernels. With no doubt, the
most popular parametrization form is the exponential kernel φij(t) = αijβije−βij t because it deﬁnitely
simpliﬁes the inference algorithm (e.g., the complexity needed for computing the likelihood is much
smaller). When d is large, in order to reduce the number of parameters, some authors choose to arbitrarily
share the kernel shapes across the different nodes. Thus, for instance, in Yang and Zha (2013); Zhou
et al. (2013b); Farajtabar et al. (2015), they choose φij(t) = αijh(t) with αij ∈ R+ quantiﬁes the
intensity of the inﬂuence of j on i and h(t) a (normalized) function that characterizes the time-proﬁle
of this inﬂuence and that is shared by all couples of nodes (i, j) (most often, it is chosen to be either
exponential h(t) = βe−βt or power law h(t) = βt−(β+1)). Both approaches are, most of the time,
highly non-realistic. On the one hand there is a priori no reason for assuming that the time-proﬁle of the
inﬂuence of a node j on a node i does not depend on the pair (i, j). On the other hand, assuming an
exponential shape or a power law shape for a kernel arbitrarily imposes an event impact that is always
instantly maximal and that can only decrease with time, while in practice, there may exist a latency
between an event and its maximal impact.

In order to have more ﬂexibility on the shape of the kernels, nonparametric estimation can be
considered. Expectation-Maximization algorithms can be found in Lewis and Mohler (2011) (for d = 1)
or in Zhou et al. (2013a) (d > 1). An alternative method is proposed in Bacry and Muzy (2016) where
the nonparametric estimation is formulated as a numerical solving of a Wiener-Hopf equation. Another
nonparametric strategy considers a decomposition of kernels on a dictionary of function h1, . . . , hK,
namely φij(t) = (cid:80)K
k are estimated, see Hansen et al. (2015);
Lemonnier and Vayatis (2014) and Xu et al. (2016), where group-lasso is used to induce a sparsity pattern
on the coefﬁcients aij

k hk(t), where the coefﬁcients aij

k that is shared across k = 1, . . . , K.

k=1 aij

Such methods are heavy when d is large, since they rely on likelihood maximization or least squares
minimization within an over-parametrized space in order to gain ﬂexibility on the shape of the kernels.
This is problematic, since the original motivation for the use of Hawkes processes is to estimate the
inﬂuence and causality of nodes, the knowledge of the full parametrization of the model being of little

2

interest for causality purpose.

Our paper solves this problem with a different and more direct approach. Instead of trying to estimate
the kernels φij, we focus on the direct estimation of their integrals. Namely, we want to estimate the
matrix G = [gij] where

gij =

φij(u) du ≥ 0 for 1 ≤ i, j ≤ d.

(1)

(cid:90) +∞

0

As it can be seen from the cluster representation of Hawkes processes (Hawkes and Oakes (1974)), this
integral represents the mean total number of events of type i directly triggered by an event of type j, and
then encodes a notion of causality. Actually, as detailed below (see Section 2.1), such integral can be
related to the Granger causality (Granger (1969)).

The main idea of the method we developed in this paper is to estimate the matrix G directly using a
matching cumulants (or moments) method. Apart from the mean, we shall use second and third-order
cumulants which correspond respectively to centered second and third-order moments. We ﬁrst compute
an estimation (cid:99)M of these centered moments M (G) (they are uniquely deﬁned by G). Then, we look for
a matrix (cid:98)G that minimizes the L2 error (cid:107)M ( (cid:98)G) − (cid:99)M (cid:107)2. Thus the integral matrix (cid:98)G is directly estimated
without making hardly any assumptions on the shape the involved kernels. As it will be shown, this
approach turns out to be particularly robust to the kernel shapes, which is not the case of all previous
Hawkes-based approaches that aim causality recovery. We call this method NPHC (Non Parametric
Hawkes Cumulant), since our approach is of nonparametric nature. We provide a theoretical analysis
that proves the consistency of the NPHC estimator. Our proof is based on ideas from the theory of
Generalized Method of Moments (GMM) but requires an original technical trick since our setting strongly
departs from the standard parametric statistics with i.i.d observations. Note that moment and cumulant
matching techniques proved particularly powerful for latent topic models, in particular Latent Dirichlet
Allocation, see Podosinnikova et al. (2015). A small set of previous works, namely Da Fonseca and
Zaatour (2014); Aït-Sahalia et al. (2010), already used method of moments with Hawkes processes, but
only in a parametric setting. Our work is the ﬁrst to consider such an approach for a nonparametric
counting processes framework.

The paper is organized as follows: in Section 2, we provide the background on the integrated kernels
and the integrated cumulants of the Hawkes process. We then introduce the method, investigate its
complexity and explain the consistency result we prove. In Section 3, we estimate the matrix of Hawkes
kernels’ integrals for various simulated datasets and for real datasets, namely the MemeTracker database
and ﬁnancial order book data. We then provide in Section 4 the technical details skipped in the previous
parts and the proof of our consistency result. Section 5 contains concluding remarks.

2 NPHC: The Non Parametric Hawkes Cumulant method

In this Section, we provide the background on integrals of Hawkes kernels and integrals of Hawkes
cumulants. We then explain how the NPHC method enables estimating G.

2.1 Branching structure and Granger causality

From the deﬁnition of Hawkes process as a Poisson cluster process, see Jovanovi´c et al. (2015) or Hawkes
and Oakes (1974), gij can be simply interpreted as the average total number of events of node i whose
direct ancestor is a given event of node j (by direct we mean that interactions mediated by any other
intermediate event are not counted). In that respect, G not only describes the mutual inﬂuences between

3

nodes, but it also quantiﬁes their direct causal relationships. Namely, introducing the counting function
N i←j
that counts the number of events of i whose direct ancestor is an event of j, we know from Bacry
t
et al. (2015) that

E[dN i←j
(2)
t
where we introduced Λi as the intensity expectation, namely satisfying E[dN i
t ] = Λidt. Note that Λi does
not depend on time by stationarity of N t, which is known to hold under the stability condition (cid:107)G(cid:107) < 1,
where (cid:107)G(cid:107) stands for the spectral norm of G. In particular, this condition implies the non-singularity of
Id − G.

t ] = gijΛjdt,

] = gijE[dN j

Since the question of a real causality is too complex in general, most econometricians agreed on
the simpler deﬁnition of Granger causality Granger (1969). Its mathematical formulation is a statistical
hypothesis test: X causes Y in the sense of Granger causality if forecasting future values of Y is more
successful while taking X past values into account. In Eichler et al. (2016), it is shown that for N t
a multivariate Hawkes process, N j
t w.r.t N t if and only if φij(u) = 0 for
u ∈ R+. Since the kernels take positive values, the latter condition is equivalent to (cid:82) ∞
0 φij(u)du = 0. In
the following, we’ll refer to learning the kernels’ integrals as uncovering causality since each integral
encodes the notion of Granger causality, and is also linked to the number of events directly caused from a
node to another node, as described above at Eq. (2).

t does not Granger-cause N i

2.2

Integrated cumulants of the Hawkes process

A general formula for the integral of the cumulants of a multivariate Hawkes process is provided
in Jovanovi´c et al. (2015). As explained below, for the purpose of our method, we only need to consider
cumulants up to the third order. Given 1 ≤ i, j, k ≤ d, the ﬁrst three integrated cumulants of the Hawkes
process can be deﬁned as follows thanks to stationarity:

Λidt = E(dN i
t )
(cid:90)
(cid:16)

Cijdt =

E(dN i

t dN j

t+τ ) − E(dN i

t )E(dN j

(cid:17)
t+τ )

τ ∈R

(cid:90) (cid:90)

(cid:16)

τ,τ (cid:48)∈R2
− E(dN i

Kijkdt =

E(dN i

t dN j

t+τ dN k

t+τ (cid:48)) + 2E(dN i

t )E(dN j

t+τ )E(dN k

t+τ (cid:48))

t dN j

t+τ )E(dN k

t+τ (cid:48)) − E(dN i

t dN k

t+τ (cid:48))E(dN j

t+τ ) − E(dN j

t+τ dN k

(cid:17)
t+τ (cid:48))E(dN i
t )

,

where Eq. (3) is the mean intensity of the Hawkes process, the second-order cumulant (4) refers to the
integrated covariance density matrix and the third-order cumulant (5) measures the skewness of N t. Using
the martingale representation from Bacry and Muzy (2016) or the Poisson cluster process representation
from Jovanovi´c et al. (2015), one can obtain an explicit relationship between these integrated cumulants
and the matrix G. If one sets

R = (Id − G)−1,

straightforward computations (see Section 4) lead to the following identities:

Λi =

Rimµm

Cij =

ΛmRimRjm

d
(cid:88)

m=1
d
(cid:88)

m=1
d
(cid:88)

m=1

4

Kijk =

(RimRjmCkm + RimCjmRkm + CimRjmRkm − 2ΛmRimRjmRkm).

(3)

(4)

(5)

(6)

(7)

(8)

(9)

Equations (8) and (9) are proved in Section 4. Our strategy is to use a convenient subset of Eqs. (3), (4)
and (5) to deﬁne M , while we use Eqs. (7), (8) and (9) in order to construct the operator that maps a
candidate matrix R to the corresponding cumulants M (R). By looking for (cid:98)R that minimizes R (cid:55)→
(cid:107)M (R) − (cid:99)M (cid:107)2, we obtain, as illustrated below, good recovery of the ground truth matrix G using
Equation (6).

The simplest case d = 1 has been considered in Hardiman and Bouchaud (2014), where it is shown
that one can choose M = {C11} in order to compute the kernel integral. Eq. (8) then reduces to a simple
second-order equation that has a unique solution in R (and consequently a unique G) that accounts for
the stability condition ((cid:107)G(cid:107) < 1).

Unfortunately, for d > 1, the choice M = {Cij}1≤i≤j≤d is not sufﬁcient to uniquely determine the
kernels integrals. In fact, the integrated covariance matrix provides d(d + 1)/2 independent coefﬁcients,
while d2 parameters are needed. It is straightforward to show that the remaining d(d − 1)/2 conditions
can be encoded in an orthogonal matrix O, reﬂecting the fact that Eq. (8) is invariant under the change
R → OR, so that the system is under-determined.

Our approach relies on using the third order cumulant tensor K = [Kijk] which contains (d3 + 3d2 +
2d)/6 > d2 independent coefﬁcients that are sufﬁcient to uniquely ﬁx the matrix G. This can be justiﬁed
intuitively as follows: while the integrated covariance only contains symmetric information, and is thus
unable to provide causal information, the skewness given by the third order cumulant in the estimation
procedure can break the symmetry between past and future so as to uniquely ﬁx G. Thus, our algorithm
consists of selecting d2 third-order cumulant components, namely M = {Kiij}1≤i,j≤d. In particular, we
deﬁne the estimator of R as (cid:98)R ∈ argminRL(R), where

L(R) = (1 − κ)(cid:107)Kc(R) − (cid:100)Kc(cid:107)2

2 + κ(cid:107)C(R) − (cid:98)C(cid:107)2
2,

(10)

where (cid:107) · (cid:107)2 stands for the Frobenius norm, Kc = {Kiij}1≤i,j≤d is the matrix obtained by the contraction
of the tensor K to d2 indices, C is the covariance matrix, while (cid:100)Kc and (cid:98)C are their respective estimators,
see Equations (12), (13) below. It is noteworthy that the above mean square error approach can be seen
as a peculiar Generalized Method of Moments (GMM), see Hall (2005). This framework allows us to
determine the optimal weighting matrix involved in the loss function. However, this approach is unusable
in practice, since the associated complexity is too high. Indeed, since we have d2 parameters, this matrix
has d4 coefﬁcients and GMM calls for computing its inverse leading to a O(d6) complexity. In this work,
we use the coefﬁcient κ to scale the two terms, as

see Section 4.4 for an explanation about the link between κ and the weighting matrix. Finally, the
estimator of G is straightforwardly obtained as

from the inversion of Eq. (6). Let us mention an important point: the matrix inversion in the previous
formula is not the bottleneck of the algorithm. Indeed, its has a complexity O(d3) that is cheap compared
to the computation of the cumulants when n = maxi |Zi| (cid:29) d, which is the typical scaling satisﬁed
in applications. Solving the considered problem on a larger scale, say d (cid:29) 103, is an open question,
even with state-of-the-art parametric and nonparametric approaches, see for instance Zhou et al. (2013a);
Xu et al. (2016); Zhou et al. (2013b); Bacry and Muzy (2016), where the number of components d in
experiments is always around 100 or smaller. Note that, actually, our approach leads to a much faster
algorithm than the considered state-of-the-art baselines, see Tables 1–4 from Section 3 below.

κ =

(cid:107)(cid:100)Kc(cid:107)2
2
2 + (cid:107) (cid:98)C(cid:107)2
2

,

(cid:107)(cid:100)Kc(cid:107)2

(cid:98)G = Id − (cid:98)R

−1

,

5

2.3 Estimation of the integrated cumulants

In this section we present explicit formulas to estimate the three moment-based quantities listed in the
previous section, namely, Λ, C and K. We ﬁrst assume there exists H > 0 such that the truncation
from (−∞, +∞) to [−H, H] of the domain of integration of the quantities appearing in Eqs. (4) and (5),
introduces only a small error. In practice, this amounts to neglecting border effects in the covariance
density and in the skewness density that is a good approximation if the support of the kernel φij(t) is
smaller than H and the spectral norm (cid:107)G(cid:107) satisﬁes (cid:107)G(cid:107) < 1.
In this case, given a realization of a stationary Hawkes process {N t : t ∈ [0, T ]}, as shown in Section 4,
we can write the estimators of the ﬁrst three cumulants (3), (4) and (5) as

(cid:98)Λi =

(cid:88)

1 =

N i
T
T

1
T

1
T

1
T

τ ∈Zi
(cid:88)

(cid:16)

τ ∈Zi
(cid:88)

(cid:16)

(cid:98)Cij =

(cid:98)Kijk =

τ ∈Zi
(cid:98)Λi
T

−

(cid:88)

(cid:88)

τ ∈Zj

τ (cid:48)∈Zk

N j

τ +H − N j

τ −H − 2H (cid:98)Λj(cid:17)

N j

τ +H − N j

τ −H − 2H (cid:98)Λj(cid:17)

·

(cid:16)

N k

τ +H − N k

τ −H − 2H (cid:98)Λk(cid:17)

(2H − |τ (cid:48) − τ |)+ + 4H 2 (cid:98)Λi (cid:98)Λj (cid:98)Λk.

(11)

(12)

(13)

Let us mention the following facts.

Bias. While the ﬁrst cumulant ˆΛi is an unbiased estimator of Λi, the other estimators (cid:98)Cij and (cid:98)Kijk
introduce a bias. However, as we will show, in practice this bias is small and hardly affects
numerical estimations (see Section 3). This is conﬁrmed by our theoretical analysis, which proves
that if H does not grow too fast compared to T , then these estimated cumulants are consistent
estimators of the theoretical cumulants (see Section 2.6).

Complexity. The computations of all the estimators of the ﬁrst, second and third-order cumulants
have complexity respectively O(nd), O(nd2) and O(nd3), where n = maxi |Zi|. However, our
algorithm requires a lot less than that: it computes only d2 third-order terms, of the form (cid:98)Kiij,
leaving us with only O(nd2) operations to perform.

Symmetry. While the values of Λi, Cij and Kijk are symmetric under permutation of the indices, their
estimators are generally not symmetric. We have thus chosen to symmetrize the estimators by
averaging their values over permutations of the indices. Worst case is for the estimator of Kc,
which involves only an extra factor of 2 in the complexity.

2.4 The NPHC algorithm

The objective to minimize in Equation (10) is non-convex. More precisely, the loss function is a
polynomial of R of degree 6. However, the expectations of cumulants Λ and C deﬁned in Eq. (4) and (5)
that appear in the deﬁnition of L(R) are unknown and should be replaced with (cid:98)Λ and (cid:98)C. We denote
(cid:101)L(R) the objective function, where the expectations of cumulants Λi and Cij have been replaced with
their estimators in the right-hand side of Eqs. (8) and (9):

(cid:101)L(R) = (1 − κ)(cid:107)R(cid:12)2 (cid:98)C

+ 2[R (cid:12) ( (cid:98)C − R (cid:98)L)]R(cid:62) − (cid:100)Kc(cid:107)2

2 + κ(cid:107)R (cid:98)LR(cid:62) − (cid:98)C(cid:107)2
2

(14)

(cid:62)

As explained in Choromanska et al. (2015), the loss function of a typical multilayer neural network
with simple nonlinearities can be expressed as a polynomial function of the weights in the network,

6

whose degree is the number of layers. Since the loss function of NPHC writes as a polynomial of
degree 6, we expect good results using optimization methods designed to train deep multilayer neural
networks. We used the AdaGrad from Duchi et al. (2011), a variant of the Stochastic Gradient Descent
with adaptive learning rates. AdaGrad scales the learning rates coordinate-wise using the online variance
of the previous gradients, in order to incorporate second-order information during training. The NPHC
method is summarized schematically in Algorithm 1.

Algorithm 1 Non Parametric Hawkes Cumulant method
Input: N t
Output: (cid:98)G
1: Estimate (cid:98)Λi, (cid:98)Cij, (cid:98)Kiij from Eqs. (11, 12, 13)
2: Design (cid:101)L(R) using the computed estimators.
3: Minimize numerically (cid:101)L(R) so as to obtain (cid:98)R
−1
4: Return (cid:98)G = Id − (cid:98)R

.

Our problem being non-convex, the choice of the starting point has a major effect on the convergence.
Here, the key is to notice that the matrices R that match Equation (8) writes C1/2OL−1/2, with
L = diag(Λ) and O an orthogonal matrix. Our starting point is then simply chosen by setting O = Id in
the previous formula, leading to nice convergence results. Even though our main concern is to retrieve the
matrix G, let us notice we can also obtain an estimation of the baseline intensities’ from Eq. (3), which
leads to (cid:98)µ = (cid:98)R
(cid:98)Λ. An efﬁcient implementation of this algorithm with TensorFlow, see Abadi et al.
(2016), is available on GitHub: https://github.com/achab/nphc.

−1

2.5 Complexity of the algorithm

Compared with existing state-of-the-art methods to estimate the kernel functions, e.g., the ordinary
differential equations-based (ODE) algorithm in Zhou et al. (2013a), the Granger Causality-based
algorithm in Xu et al. (2016), the ADM4 algorithm in Zhou et al. (2013b), and the Wiener-Hopf-based
algorithm in Bacry and Muzy (2016), our method has a very competitive complexity. This can be
understood by the fact that those methods estimate the kernel functions, while in NPHC we only estimate
their integrals. The ODE-based algorithm is an EM algorithm that parametrizes the kernel function
with M basis functions, each being discretized to L points. The basis functions are updated after
solving M Euler-Lagrange equations. If n denotes the maximum number of events per component (i.e.
n = max1≤i≤d |Zi|) then the complexity of one iteration of the algorithm is O(M n3d2 + M L(nd + n2)).
The Granger Causality-based algorithm is similar to the previous one, without the update of the basis
functions, that are Gaussian kernels. The complexity per iteration is O(M n3d2). The algorithm ADM4
is similar to the two algorithms above, as EM algorithm as well, with only one exponential kernel as
basis function. The complexity per iteration is then O(n3d2). The Wiener-Hopf-based algorithm is not
iterative, on the contrary to the previous ones. It ﬁrst computes the empirical conditional laws on many
points, and then invert the Wiener-Hopf system, leading to a O(nd2L + d4L3) computation. Similarly,
our method ﬁrst computes the integrated cumulants, then minimize the objective function with Niter
iterations, and invert the resulting matrix (cid:98)R to obtain (cid:98)G. In the end, the complexity of the NPHC method
is O(nd2 + Niterd3). According to this analysis, summarized in Table 1 below, one can see that in the
regime n (cid:29) d, the NPHC method outperforms all the other ones.

2.6 Theoretical guarantee: consistency

The NPHC method can be phrased using the framework of the Generalized Method of Moments (GMM).
GMM is a generic method for estimating parameters in statistical models. In order to apply GMM,

7

Table 1: Complexity of state-of-the-art methods. NPHC’s complexity is very low , especially in the
regime n (cid:29) d.

Method

ODE Zhou et al. (2013a)
GC Xu et al. (2016)
ADM4 Zhou et al. (2013b)
WH Bacry and Muzy (2016) O(nd2L + d4L3)
O(nd2 + Niterd3)
NPHC

Total complexity
O(NiterM (n3d2 + L(nd + n2)))
O(NiterM n3d2)
O(Nitern3d2)

we have to ﬁnd a vector-valued function g(X, θ) of the data, where X is distributed with respect to a
distribution Pθ0, which satisﬁes the moment condition: E[g(X, θ)] = 0 if and only if θ = θ0, where θ0 is
the “ground truth” value of the parameter. Based on i.i.d. observed copies x1, . . . , xn of X, the GMM
method minimizes the norm of the empirical mean over n samples, (cid:107) 1
i=1 g(xi, θ)(cid:107), as a function of
n
θ, to obtain an estimate of θ0.

(cid:80)n

In the theoretical analysis of NPHC, we use ideas from the consistency proof of the GMM, but the
proof actually relies on very different arguments. Indeed, the integrated cumulants estimators used in
NPHC are not unbiased, as the theory of GMM requires, but asymptotically unbiased. Moreover, the
setting considered here, where data consists of a single realization {N t} of a Hawkes process strongly
departs from the standard i.i.d setting. Our approach is therefore based on the GMM idea but the proof is
actually not using the theory of GMM.

In the following, we use the subscript T to refer to quantities that only depend on the process (Nt) in
the interval [0, T ] (e.g., the truncation term HT , the estimated integrated covariance (cid:98)CT or the estimated
kernel norm matrix (cid:98)GT ). In the next equation, (cid:12) stands for the Hadamard product and (cid:12)2 stands for
the entrywise square of a matrix. We denote G0 = Id − R−1
the true value of G, and the R2d×d valued
0
vector functions

g0(R) =

(cid:98)gT (R) =

(cid:20)

C − RLR(cid:62)
Kc − R(cid:12)2C(cid:62) − 2[R (cid:12) (C − RL)]R(cid:62)
(cid:34)

(cid:21)

(cid:98)CT − R (cid:98)LT R(cid:62)
(cid:62)
T − 2[R (cid:12) ( (cid:98)CT − R (cid:98)LT )]R(cid:62) .

(cid:35)

(cid:100)Kc

T − R(cid:12)2 (cid:98)C

Using these notations, (cid:101)LT (R) can be seen as the weighted squared Frobenius norm of (cid:98)gT (R). Moreover,
P
when T → +∞, one has (cid:98)gT (R)
→
stands for convergence in probability.

P
→ g0(R) under the conditions of the following theorem, where

Theorem 2.1 (Consistency of NPHC). Suppose that (Nt) is observed on R+ and assume that

1. g0(R) = 0 if and only if R = R0;

2. R ∈ Θ, where Θ is a compact set;

4. HT → ∞ and H 2

T /T → 0.

Then

3. the spectral radius of the kernel norm matrix satisﬁes (cid:107)G0(cid:107) < 1;

(cid:18)

(cid:98)GT = Id −

arg min
R∈Θ

(cid:101)LT (R)

(cid:19)−1

P
→ G0.

8

The proof of the Theorem is given in Section 4.5 below. Assumption 3 is mandatory for stability of
the Hawkes process, and Assumptions 3 and 4 are sufﬁcient to prove that the estimators of the integrated
cumulants deﬁned in Equations (11), (12) and (13) are asymptotically consistent. Assumption 2 is a
very mild standard technical assumption allowing to prove consistency for estimators based on moments.
Assumption 1 is a standard asymptotic moment condition, that allows to identity parameters from the
integrated cumulants.

3 Numerical Experiments

In this Section, we provide a comparison of NPHC with the state-of-the art, on simulated datasets with
different kernel shapes, the MemeTracker dataset (social networks) and the order book dynamics dataset
(ﬁnance).

Simulated datasets. We simulated several datasets with Ogata’s Thinning algorithm Ogata (1981)
using the open-source library tick1, each corresponding to a shape of kernel: rectangular, exponential
or power law kernel, see Figure 1 below.

φt
αβ

log φt
log αβγ

φt
αβ

slope ≈ −(1 + γ)

0 γ

γ + 1/β

t

− log β

log t

0

1/β

t

(a) Rectangular kernel
φt = αβ1[0,1/β](t − γ)

(b) Power law kernel on log-log scale
φt = αβγ(1 + βt)−(1+γ)

(c) Exponential kernel
φt = αβ exp(−βt)

Figure 1: The three different kernels used to simulate the datasets.

The integral of each kernel on its support equals α, 1/β can be regarded as a characteristic time-scale
and γ is the scaling exponent for the power law distribution and a delay parameter for the rectangular
one. We consider a non-symmetric block-matrix G to show that our method can effectively uncover
causality between the nodes, see Figure 3. The matrix G has constant entries α on the three blocks -
α = gij = 1/6 for dimension 10 and α = gij = 1/10 for dimension 100 -, and zero outside. The two
other parameters’ values are the same for dimensions 10 and 100. The parameter γ is set to 1/2 on the
three blocks as well, but we set three very different β0, β1 and β2 from one block to the other, with ratio
βi+1/βi = 10 and β0 = 0.1. The number of events is roughly equal to 105 on average over the nodes.
We ran the algorithm on three simulated datasets: a 10-dimensional process with rectangular kernels
named Rect10, a 10-dimensional process with power law kernels named PLaw10 and a 100-dimensional
process with exponential kernels named Exp100.

MemeTracker dataset. We use events of the most active sites from the MemeTracker dataset2. This
dataset contains the publication times of articles in many websites/blogs from August 2008 to April
2009, and hyperlinks between posts. We extract the top 100 media sites with the largest number of
documents, with about 7 million of events. We use the links to trace the ﬂow of information and establish
an estimated ground truth for the matrix G. Indeed, when an hyperlink j appears in a post in website i,
the link j can be regarded as a direct ancestor of the event. Then, Eq. (2) shows gij can be estimated by
N i←j
T

T = #{links j → i}/N j
T .

/N j

1https://github.com/X-DataInitiative/tick
2https://www.memetracker.org/data.html

9

Order book dynamics. We apply our method to ﬁnancial data, in order to understand the self and cross-
inﬂuencing dynamics of all event types in an order book. An order book is a list of buy and sell orders for
a speciﬁc ﬁnancial instrument, the list being updated in real-time throughout the day. This model has ﬁrst
been introduced in Bacry et al. (2016), and models the order book via the following 8-dimensional point
, C(a)
process: Nt = (P (a)
t ), where P (a) (resp. P (b)) counts the number
t
of upward (resp. downward) price moves, T (a) (resp. T (b)) counts the number of market orders at the
ask3 (resp. at the bid) that do not move the price, L(a) (resp. L(b)) counts the number of limit orders at
the ask4 (resp. at the bid) that do not move the price, and C(a) (resp. C(b)) counts the number of cancel
orders at the ask5 (resp. at the bid) that do not move the price. The ﬁnancial data has been provided by
QuantHouse EUROPE/ASIA, and consists of DAX future contracts between 01/01/2014 and 03/01/2014.

, T (a)
t

, P (b)
t

, L(a)
t

, T (b)
t

, L(b)
t

, C(b)

t

Baselines. We compare NPHC to state-of-the art baselines: the ODE-based algorithm (ODE) by Zhou
et al. (2013a), the Granger Causality-based algorithm (GC) by Xu et al. (2016), the ADM4 algorithm
(ADM4) by Zhou et al. (2013b), and the Wiener-Hopf-based algorithm (WH) by Bacry and Muzy (2016).

Metrics. We evaluate the performance of the proposed methods using the computing time, the Relative
Error

RelErr(A, B) =

{aij (cid:54)=0} + |bij|1
1

{aij =0}

1
d2

(cid:88)

i,j

|aij − bij|
|aij|

and the Mean Kendall Rank Correlation

MRankCorr(A, B) =

RankCorr([ai•], [bi•]),

1
d

d
(cid:88)

i=1

where RankCorr(x, y) = 2
d(d−1) (Nconcordant(x, y) − Ndiscordant(x, y)) with Nconcordant(x, y) the number
of pairs (i, j) satisfying xi > xj and yi > yj or xi < xj and yi < yj and Ndiscordant(x, y) the number of
pairs (i, j) for which the same condition is not satisﬁed.

Note that RankCorr score is a value between −1 and 1, representing rank matching, but can take

smaller values (in absolute value) if the entries of the vectors are not distinct.

Figure 2: On Exp100 dataset, estimated (cid:98)G with ADM4 (left), with NPHC (middle) and the ground-
truth matrix G (right). Both ADM4 and NPHC estimates recover the three blocks. However, ADM4
overestimates the integrals on two of the three blocks, while NPHC gives the same value on each blocks.

3i.e. buy orders that are executed and removed from the list
4i.e. buy orders added to the list
5i.e. the number of times a limit order at the ask is canceled: in our dataset, almost 95% of limit orders are canceled before

execution.

10

Figure 3: Estimated (cid:98)G via NPHC on DAX order book data.

Table 2: Metrics on Rect10: comparable rank correlation, strong improvement for relative error and
computing time.

Method

ODE GC

ADM4 WH

NPHC

RelErr
MRankCorr
Time (s)

0.007
0.33
846

0.15
0.02
768

0.10
0.21
709

0.005
0.34
933

0.001
0.34
20

Discussion. We perform the ADM4 estimation, with exponential kernel, by giving the exact value
β = β0 of one block. Let us stress that this helps a lot this baseline, in comparison to NPHC where
nothing is speciﬁed on the shape of the kernel functions. We used M = 10 basis functions for both ODE
and GC algorithms, and L = 50 quadrature points for WH. We did not run WH on the 100-dimensional
datasets, for computing time reasons, because its complexity scales with d4. We ran multi-processed
versions of the baseline methods on 56 cores, to decrease the computing time.

Our method consistently performs better than all baselines, on the three synthetic datasets, on
MemeTracker and on the ﬁnancial dataset, both in terms of Kendall rank correlation and estimation error.
Moreover, we observe that our algorithm is roughly 50 times faster than all the considered baselines.

On Rect10, PLaw10 and Exp100 our method gives very impressive results, despite the fact that it
does not uses any prior shape on the kernel functions, while for instance the ADM4 baseline do. On
Figure 3, we observe that the matrix (cid:98)G estimated with ADM4 recovers well the block for which β = β0,
i.e. the value we gave to the method, but does not perform well on the two other blocks, while the matrix
(cid:98)G estimated with NPHC approximately reaches the true value for each of the three blocks. On these
simulated datasets, NPHC obtains a comparable or slightly better Kendall rank correlation, but improves
a lot the relative error.

On MemeTracker, the baseline methods obtain a high relative error between 9% and 19% while our
method achieves a relative error of 7% which is a strong improvement. Moreover, NPHC reaches a much
better Kendall rank correlation, which proves that it leads to a much better recovery of the relative order
of estimated inﬂuences than all the baselines. Indeed, it has been shown in Zhou et al. (2013a) that kernels
of MemeTracker data are not exponential, nor power law. This partly explains why our approach behaves

11

Table 3: Metrics on PLaw10: comparable rank correlation, strong improvement for relative error and
computing time.

Table 4: Metrics on Exp100: comparable rank correlation, strong improvement for relative error and
computing time.

Method

ODE GC

ADM4 WH

NPHC

RelErr
MRankCorr
Time (s)

0.011
0.31
870

0.09
0.26
781

0.053
0.24
717

0.009
0.34
946

0.0048
0.33
18

Method

ODE GC

ADM4 NPHC

RelErr
MRankCorr
Time (s)

0.092
0.032
3215

0.112
0.009
2950

0.079
0.049
2411

0.008
0.041
47

better.

On the ﬁnancial data, the estimated kernel norm matrix obtained via NPHC, see Figure 3, gave some

interpretable results (see also Bacry et al. (2016)):

1. Any 2 × 2 sub-matrix with same kind of inputs (i.e. Prices changes, Trades, Limits or Cancels) is

symmetric. This shows empirically that ask and bid have symmetric roles.

2. The prices are mostly cross-excited, which means that a price increase is very likely to be followed
by a price decrease, and conversely. This is consistent with the wavy prices we observe on ﬁnancial
markets.

3. The market, limit and cancel orders are strongly self-excited. This can be explained by the
persistence of order ﬂows, and by the splitting of meta-orders into sequences of smaller orders.
Moreover, we observe that orders impact the price without changing it. For example, the increase
of cancel orders at the bid causes downward price moves.

We show in this section how to obtain the equations stated above, the estimators of the integrated
cumulants and the scaling coefﬁcient κ that appears in the objective function. We then prove the theorem
of the paper.

4 Technical details

4.1 Proof of Equation (8)

We denote ν(z) the matrix

νij(z) = Lz

t →

(cid:16)

E(dN i

u+t)

udN j
dudt

− ΛiΛj(cid:17)

,

where Lz(f ) is the Laplace transform of f , and ψt = (cid:80)
refers to the nth auto-
convolution of φt. Then we use the characterization of second-order statistics, ﬁrst formulated in Hawkes
(1971) and fully generalized in Bacry and Muzy (2016),

, where φ((cid:63)n)

n≥1 φ((cid:63)n)

t

t

ν(z) = (Id + L−z(Ψ))L(Id + Lz(Ψ))(cid:62),

12

Table 5: Metrics on MemeTracker: strong improvement in relative error, rank correlation and computing
time.

Method

ODE GC

ADM4 NPHC

RelErr
MRankCorr
Time (s)

0.162
0.07
2944

0.19
0.053
2780

0.092
0.081
2217

0.071
0.095
38

where Lij = Λiδij with δij the Kronecker symbol. Since Id + Lz(Ψ) = (Id − Lz(Φ))−1, taking z = 0
in the previous equation gives

ν(0) = (Id − G)−1L(Id − G(cid:62))−1,

C = RLR(cid:62),

which gives us the result since the entry (i, j) of the last equation gives Cij = (cid:80)

m ΛmRimRjm.

4.2 Proof of Equation (9)

We start from Jovanovi´c et al. (2015), cf. Eqs. (48) to (51), and group some terms:

Kijk =

(cid:88)

ΛmRimRjmRkm

+

+

+

m
(cid:88)

m
(cid:88)

m
(cid:88)

m

RimRjm

ΛnRknL0(ψmn)

RimRkm

ΛnRjnL0(ψmn)

RjmRkm

ΛnRinL0(ψmn).

(cid:88)

n
(cid:88)

n
(cid:88)

n

Using the relations L0(ψmn) = Rmn − δmn and Cij = (cid:80)

m ΛmRimRjm, proves Equation (9).

4.3

Integrated cumulants estimators

For H > 0 let us denote ∆H N i
domain to (−H, H) in Eqs. (4) and (5), one gets by permuting integrals and expectations:

t−H . Let us ﬁrst remark that, if one restricts the integration

t+H − N i

t = N i

(cid:16)

Cijdt = E

Λidt = E(dN i
t )
t (∆H N j
dN i
t (∆H N j
(cid:16)

Kijkdt = E

dN i

(cid:16)

(cid:17)
t − 2HΛj)

t − 2HΛj)(∆H N k

t − 2HΛk)

(cid:17)

− dtΛiE

(∆H N j

t − 2HΛj)(∆H N k

(cid:17)
t − 2HΛk)

.

The estimators (11) and (12) are then naturally obtained by replacing the expectations by their empirical
counterparts, notably

E(dN i
t f (t))
dt

→

1
T

(cid:88)

τ ∈Zi

f (τ ).

13

For the estimator (13), we shall also notice that

E((∆H N j
(cid:90) (cid:90)

=

=

(cid:90)

t − 2HΛj)(∆H N k

t − 2HΛk))

1[−H,H](t)1[−H,H](t(cid:48))Cjk

t−t(cid:48)dtdt(cid:48)

(2H − |t|)+Cjk

t dt.

We estimate the last integral with the remark above.

4.4 Choice of the scaling coefﬁcient κ

Following the theory of GMM, we denote m(X, θ) a function of the data, where X is distributed with
respect to a distribution Pθ0, which satisﬁes the moment conditions g(θ) = E[m(X, θ)] = 0 if and
only if θ = θ0, the parameter θ0 being the ground truth. For x1, . . . , xN observed copies of X, we
denote (cid:98)gi(θ) = m(xi, θ), the usual choice of weighting matrix is (cid:99)WN (θ) = 1
i=1 (cid:98)gi(θ)(cid:98)gi(θ)(cid:62), and
the objective to minimize is then

(cid:80)N

N

(cid:32)

1
N

N
(cid:88)

i=1

(cid:33)

(cid:16)

(cid:98)gi(θ)

(cid:99)WN (θ1)

(cid:32)

(cid:17)−1

(cid:33)

(cid:98)gi(θ)

,

1
N

N
(cid:88)

i=1

(15)

where θ1 is a constant vector. Instead of computing the inverse weighting matrix, we rather use its
projection on {αId : α ∈ R}. It can be shown that the projection choses α as the mean eigenvalue of
(cid:99)WN (θ1). We can easily compute the sum of its eigenvalues:

Tr((cid:99)WN (θ1)) =

Tr((cid:98)gi(θ1)(cid:98)gi(θ1)(cid:62)) =

Tr((cid:98)gi(θ1)(cid:62)

(cid:98)gi(θ1)) =

||(cid:98)gi(θ1)||2
2.

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

i=1

(cid:104)
vec[(cid:100)Kc − Kc(R)], vec[ (cid:98)C − C(R)]

In our case, (cid:98)g(R) =
. Considering a block-wise
weighting matrix, one block for (cid:100)Kc − Kc(R) and the other for (cid:98)C − C(R), the sum of the eigenvalues of
the ﬁrst block becomes (cid:107)(cid:100)Kc − Kc(R)(cid:107)2
2, and (cid:107) (cid:98)C − C(R)(cid:107)2
2 for the second. We compute the previous
terms with R1 = 0. All together, the objective function to minimize is

∈ R2d2

(cid:105)(cid:62)

1
(cid:107)(cid:100)Kc(cid:107)2
2

(cid:16)

(cid:107)Kc(R) − (cid:100)Kc(cid:107)2

2 +

(cid:107)C(R) − (cid:98)C(cid:107)2
2.

(16)

1
(cid:107) (cid:98)C(cid:107)2
2

(cid:17)−1

, and setting κ = (cid:107)(cid:100)Kc(cid:107)2

2/((cid:107)(cid:100)Kc(cid:107)2

2 + (cid:107) (cid:98)C(cid:107)2

2), we

2 + 1/(cid:107) (cid:98)C(cid:107)2
Dividing this function by
2
obtaind the loss function given in Equation (10).

1/(cid:107)(cid:100)Kc(cid:107)2

4.5 Proof of the Theorem

The main difference with the usual Generalized Method of Moments, see Hansen (1982), relies in the
relaxation of the moment conditions, since we have E[(cid:98)gT (θ0)] = mT (cid:54)= 0. We adapt the proof of
consistency given in Newey and McFadden (1994).

We can relate the integral of the Hawkes process’s kernels to the integrals of the cumulant densities,
from Jovanovi´c et al. (2015). Our cumulant matching method would fall into the usual GMM framework
if we could estimate - without bias - the integral of the covariance on R, and the integral of the skewness
on R2. Unfortunately, we can’t do that easily. We can however estimate without bias (cid:82) f T
t dt

t Cij

14

t Kijk

and (cid:82) f T
t dt with f T a compact supported function on [−HT , HT ] that weakly converges to 1,
t = 1[−HT ,HT ](t). Denoting (cid:98)Cij,(T ) the estimator of
with HT −→ ∞. In most cases we will take f T
(cid:82) f T
t Cij
t dt − Cij| can be considered a proxy to the distance to
the classical GMM. This distance has to go to zero to make the rest of GMM’s proof work: the estimator
(cid:98)Cij,(T ) is then asymptotically unbiased towards Cij when T goes to inﬁnity.

t dt, the term |E[ (cid:98)Cij,(T )] − Cij| = | (cid:82) f T

t Cij

4.5.1 Notations

We observe the multivariate point process (N t) on R+, with Zi the events of the ith component. We will
often write covariance / skewness instead of integrated covariance / skewness. In the rest of the document,
we use the following notations.

Hawkes kernels’ integrals Gtrue = (cid:82) Φtdt = ((cid:82) φij

t dt)ij = Id − (Rtrue)−1

Theoretical mean matrix L = diag(Λ1, . . . , Λd)

Theoretical covariance C = RtrueL(Rtrue)(cid:62)

Theoretical skewness Kc = (Kiij)ij = (Rtrue)(cid:12)2C(cid:62) + 2[Rtrue (cid:12) (C − RtrueL)](Rtrue)(cid:62)

Filtering function

f T ≥ 0

supp(f T ) ⊂ [−HT , HT ]

F T = (cid:82) f T

s ds

t = f T
(cid:101)f T
−t

Events sets Zi,T,1 = Zi ∩ [HT , T + HT ]

Zj,T,2 = Zj ∩ [0, T + 2HT ]

Estimators of the mean

(cid:98)Λi =

N i

T +HT
T

−N i

HT

Estimator of the covariance

(cid:98)Cij,(T ) = 1
T

(cid:80)

τ ∈Zi,T,1

τ (cid:48)∈Zj,T,2 fτ (cid:48)−τ − (cid:101)ΛjF T (cid:17)

Estimator of the skewness6

N j
T +2HT
T +2HT

(cid:101)Λj =
(cid:16)(cid:80)





(cid:98)Kijk,(T ) =

1
T





(cid:88)

(cid:88)

fτ (cid:48)−τ − (cid:101)ΛjF T





(cid:88)

fτ (cid:48)−τ − (cid:101)ΛkF T

τ ∈Zi,T,1

−

(cid:98)Λi
T + 2HT

τ (cid:48)∈Zj,T,2


(cid:88)



τ (cid:48)∈Zj,T,2

τ (cid:48)(cid:48)∈Zk,T,2

τ (cid:48)(cid:48)∈Zk,T,2

(cid:88)

(f T (cid:63) (cid:101)f T )τ (cid:48)−τ (cid:48)(cid:48) − (cid:101)Λk(F T )2









6When f T

t = 1[−HT ,HT ](t), we remind that (f T (cid:63) (cid:101)f T )t = (2HT − |t|)+. This leads to the estimator we showed in the

article.

15

GMM related notations

θ = R and

θ0 = Rtrue

(cid:20)

g0(θ) = vec

C − RLR(cid:62)

C(cid:62) − 2[R (cid:12) (C − RL)]R(cid:62)

(cid:21)

∈ R2d2

Kc − R(cid:12)2
(cid:34)

(T )

(cid:98)C
(T )

− R (cid:98)LR(cid:62)
)(cid:62) − 2[R (cid:12) ( (cid:98)C

(T )

− R (cid:98)L)]R(cid:62)

(cid:35)

∈ R2d2

(cid:98)gT (θ) = vec

(cid:100)Kc(T )

− R(cid:12)2

( (cid:98)C

Q0(θ) = g0(θ)(cid:62)W g0(θ)
(cid:98)QT (θ) = (cid:98)gT (θ)(cid:62)(cid:99)WT (cid:98)gT (θ)

4.5.2 Consistency

First, let’s remind a useful theorem for consistency in GMM from Newey and McFadden (1994).

Theorem 4.1. If there is a function Q0(θ) such that (i) Q0(θ) is uniquely maximized at θ0; (ii) Θ
is compact; (iii) Q0(θ) is continuous; (iv) (cid:98)QT (θ) converges uniformly in probability to Q0(θ), then
(cid:98)θT = arg max (cid:98)QT (θ)

−→ θ0.

P

We can now prove the consistency of our estimator.

Theorem 4.2. Suppose that (Nt) is observed on R+, (cid:99)WT

P

−→ W , and

1. W is positive semi-deﬁnite and W g0(θ) = 0 if and only if θ = θ0,

2. θ ∈ Θ, which is compact,

3. the spectral radius of the kernel norm matrix satisﬁes ||Φ||∗ < 1,
4. ∀i, j, k ∈ [d], (cid:82) f T

u du and (cid:82) f T

u du → (cid:82) Cij

u,v dudv → (cid:82) Kijk

u f T

v Kijk

u Cij

u,v dudv,

5. (F T )2/T

−→ 0 and ||f ||∞ = O(1).

P

Then

P

(cid:98)θT

−→ θ0.

Remark 1. In practice, we use a constant sequence of weighting matrices: (cid:99)WT = Id.

Proof. Proceed by verifying the hypotheses of Theorem 2.1 from Newey and McFadden (1994). Con-
dition 2.1(i) follows by (i) and by Q0(θ) = [W 1/2g0(θ)](cid:62)[W 1/2g0(θ)] > 0 = Q0(θ0). Indeed, there
exists a neighborhood N of θ0 such that θ ∈ N \{θ0} and g0(θ) (cid:54)= 0 since g0(θ) is a polynom. Condition
2.1(ii) follows by (ii). Condition 2.1(iii) is satisﬁed since Q0(θ) is a polynom. Condition 2.1(iv) is
harder to prove. First, since (cid:98)gT (θ) is a polynom of θ, we prove easily that E[supθ∈Θ |(cid:98)gT (θ)|] < ∞. Then,
by Θ compact, g0(θ) is bounded on Θ, and by the triangle and Cauchy-Schwarz inequalities,

(cid:12) (cid:98)QT (θ) − Q0(θ)(cid:12)
(cid:12)
(cid:12)

≤ (cid:12)
(cid:12)((cid:98)gT (θ) − g0(θ))(cid:62)(cid:99)WT ((cid:98)gT (θ) − g0(θ))(cid:12)
(cid:12)
(cid:12) + (cid:12)
T )((cid:98)gT (θ) − g0(θ))(cid:12)
+ (cid:12)
(cid:12)g0(θ)(cid:62)((cid:99)WT + (cid:99)W (cid:62)
≤ (cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)2(cid:107)(cid:99)WT (cid:107) + 2(cid:107)g0(θ)(cid:107)(cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)(cid:107)(cid:99)WT (cid:107) + (cid:107)g0(θ)(cid:107)2(cid:107)(cid:99)WT − W (cid:107).

(cid:12)g0(θ)(cid:62)((cid:99)WT − W )g0(θ)(cid:12)
(cid:12)

To prove supθ∈Θ
Θ compact, it is sufﬁcient to prove that (cid:107)(cid:98)L−L(cid:107)

P
−→ 0, we should now prove that supθ∈Θ(cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)
−C(cid:107)

−→ 0, and (cid:107)(cid:100)Kc(T )

(cid:12) (cid:98)QT (θ) − Q0(θ)(cid:12)
(cid:12)
(cid:12)

−→ 0, (cid:107) (cid:98)C

(T )

P

P

−Kc(cid:107)

−→ 0. By
P

−→ 0.

P

16

Proof that (cid:107) (cid:98)L − L(cid:107)

P

−→ 0

The estimator of L is unbiased so let’s focus on the variance of (cid:98)L.

E[((cid:98)Λi − Λi)2] = E

(cid:19)2(cid:35)

(dN i

t − Λidt)

E[(dN i

t − Λidt)(dN i

t(cid:48) − Λidt(cid:48))]

(cid:90) T +HT

(cid:34)(cid:18) 1
T

HT
(cid:90) T +HT

(cid:90) T +HT

HT

HT

(cid:90) T +HT

(cid:90) T +HT

HT

(cid:90) T +HT

HT

HT

Ciidt =

−→ 0

Cii

t(cid:48)−tdtdt(cid:48)

Cii
T

P

=

=

≤

1
T 2

1
T 2

1
T 2

By Markov inequality, we have just proved that (cid:107)(cid:98)L − L(cid:107)

−→ 0.

Proof that (cid:107) (cid:98)C

(T )

P

− C(cid:107)

−→ 0

First, let’s remind that E( (cid:98)C

) (cid:54)= C. Indeed,

E

(cid:16)

(cid:98)Cij,(T )(cid:17)

= E

(T )

(cid:18) 1
T
(cid:18) 1
T

(cid:90) T +HT

(cid:90) T +2HT

HT

(cid:90) T +HT

(cid:90) T +2HT −t

dN i
t

dN i
t

0

−t
(cid:16)

HT
(cid:90) T +HT

(cid:90) HT

HT
fsCij

−HT
s ds + (cid:15)ij,T,HT F T

= E

1
T
(cid:90)

=

=

Now,

dN j

t(cid:48)ft(cid:48)−t − (cid:98)Λi (cid:101)ΛjF T

(cid:19)

(cid:19)

dN j

t+sfs − ΛiΛjF T

+ (cid:15)ij,T,HT F T

fsE

dN i

t dN j

t+s − ΛiΛjds

+ (cid:15)ij,T,HT F T

(cid:17)

(cid:15)ij,T,HT = E

(cid:16)

ΛiΛj − (cid:98)Λi (cid:101)Λj(cid:17)
(cid:90) T +HT
1
T 2

0

HT

(cid:90) T +2HT

(cid:90) T +HT

(cid:90) T +2HT

= −

= −

1
T 2

1
T

HT
(cid:90) (cid:32)

= −

1 +

Cij
t−t(cid:48)dtdt(cid:48)
(cid:19)−(cid:33)+

Cij

t dt

0

(cid:18) HT − |t|
T

(cid:16)

E

dN i

t dN j

t(cid:48) − ΛiΛjdtdt(cid:48)(cid:17)

(T )

Since f satisﬁes F T = o(T ), we have E( (cid:98)C
E( (cid:98)C
Let’s now focus on the variance of (cid:98)Cij,(T ) : V( (cid:98)Cij,(T )) = E

) −→ C.

−→ 0.

)(cid:107)

P

(T )

It remains now to prove that (cid:107) (cid:98)C

(T )

−

(cid:16)

( (cid:98)Cij,(T ))2(cid:17)

− E( (cid:98)Cij,(T ))2.

17

Now,

(cid:16)

E

( (cid:98)Cij,(T ))2(cid:17)


1
T 2

1
T 2
(cid:90)

= E



(cid:32)

= E

=

1
T 2

And,

(cid:88)

(fτ (cid:48)−τ − F T /(T + 2HT ))(fη(cid:48)−η − F T /(T + 2HT ))

(τ,η,τ (cid:48),η(cid:48))∈(Zi,T,1)2×(Zj,T,2)2
(cid:90)

(cid:90)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)

(cid:90)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)∈[0,T +2HT ]

dN i

t dN j

t(cid:48)dN i

sdN j

s(cid:48)(ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))

(cid:16)

E

dN i

t dN j

t(cid:48)dN i

sdN j
s(cid:48)

(cid:17)

· (ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))





(cid:33)

E( (cid:98)Cij,(T ))2
1
T 2

=

(cid:90)

(cid:90)

(cid:16)

E

dN i

t dN j
t(cid:48)

(cid:17)

(cid:16)

E

dN i

sdN j
s(cid:48)

(cid:17)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)∈[0,T +2HT ]

· (ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))

Then, the variance involves the integration towards the difference of moments µr,s,t,u − µr,sµt,u. Let’s
write it as a sum of cumulants, since cumulants density are integrable.

µr,s,t,u − µr,sµt,u = κr,s,t,u + κr,s,tκu[4] + κr,sκt,u[3] + κr,sκtκu[6] + κrκsκtκu − (κr,s + κrκs)(κt,u + κtκu)

= κr,s,t,u
+ κr,s,tκu + κu,r,sκt + κt,u,rκs + κs,t,uκr
+ κr,tκs,u + κr,uκs,t
+ κr,tκsκu + κr,uκsκt + κs,tκrκu + κs,tκrκu

In the rest of the proof, we denote at = 1t∈[HT ,T +HT ], bt = 1t∈[0,T +2HT ], ct = 1t∈[−HT ,HT ], gt =
ft − 1
Before starting the integration of each term, let’s remark that:

T +2HT

F T

u,v (skewness density) and M ijkl

u,v,w (fourth cumulant density) are positive
· with positive coefﬁcients. The integrals of the singular parts are

1. Ψt = (cid:80)

n≥1 Φ((cid:63)n)

t ≥ 0 since Φt ≥ 0.

2. The regular parts of Cij

u , Kijk

3.

as polynoms of integrals of ψab
positive as well.
(a) (cid:82) atbt(cid:48)ft(cid:48)−tdtdt(cid:48) = T F T
(b) (cid:82) atbt(cid:48)gt(cid:48)−tdtdt(cid:48) = 0
(c) (cid:82) atbt(cid:48)|gt(cid:48)−t|dtdt(cid:48) ≤ 2T F T
4. ∀t ∈ R, at(b (cid:63) (cid:101)g)t = 0, where (cid:101)gs = g−s.

18

Fourth cumulant We want here to compute (cid:82) κi,j,i,j
We remark that |gt(cid:48)−tgs(cid:48)−s| ≤ (||f ||∞(1 + 2HT /T ))2 ≤ 4||f ||2
∞.
(cid:19)2 (cid:90)

(cid:90)

t,t(cid:48),s,s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)(cid:12)
κi,j,i,j

(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)

1
T 2

t,t(cid:48),s,s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48).

dtat

dt(cid:48)bt(cid:48)

dsas

ds(cid:48)bs(cid:48)M ijij

t(cid:48)−t,s−t,s(cid:48)−t

(cid:90)

(cid:90)

(cid:90)

(cid:90)

dtat

dt(cid:48)bt(cid:48)

dsas

dwM ijij

t(cid:48)−t,s−t,w

dtat

M ijij

u,v,wdudvdw

(cid:90)

(cid:90)

(cid:90)

(cid:18) 2||f ||∞
T
(cid:18) 2||f ||∞
T
(cid:18) 2||f ||∞
T
4||f ||2
∞
T

≤

≤

≤

(cid:19)2 (cid:90)

(cid:19)2 (cid:90)

M ijij −→
T →∞

0

Third × First We have four terms, but only two different forms since the roles of (s, s(cid:48)) and (t, t(cid:48))
are symmetric.
First form

Second form

(cid:90)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
κi,j,j
t,t(cid:48),s(cid:48)ΛiGtdt
(cid:12)
(cid:12) =

(cid:90)

κi,j,i
t,t(cid:48),sΛjGtdt =

κi,j,i
t,t(cid:48),satbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)

(cid:90)

(cid:90)

Λj
T 2
Λj
T 2

=

= 0

κi,j,i
t,t(cid:48),satbt(cid:48)as(b (cid:63) (cid:101)g)sgt(cid:48)−tdtdt(cid:48)ds
since as(b (cid:63) (cid:101)g)s = 0

t,t(cid:48),s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)(cid:12)
κi,j,j
t,t(cid:48),s(cid:48)atbt(cid:48)gt(cid:48)−tbs(cid:48)(a (cid:63) g)s(cid:48)dtdt(cid:48)ds(cid:48)(cid:12)
κi,j,j

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:90)

(cid:90)

(cid:12)
(cid:12)
(cid:12)

=

Λi
T 2
Λi
(cid:12)
(cid:12)
(cid:12)
T 2
Λi
≤
T 2 2||f ||∞
≤ 4||f ||∞KijjΛi F T
T

(cid:90)

−→
T →∞

0

ds(cid:48)bs(cid:48)(a (cid:63) |g|)s(cid:48)

dtat

dt(cid:48)bt(cid:48)Kijj

t(cid:48)−s(cid:48),t−s(cid:48)

(cid:90)

(cid:90)

Second × Second
First form

(cid:90)

(cid:12)
(cid:12)
(cid:12)

t,sκj,j
κi,i

t(cid:48),s(cid:48)Gtdt

Second form

Second × First × First
First form

(cid:90)

t(cid:48)−s(cid:48)atbt(cid:48)|gt(cid:48)−t|asbs(cid:48)dtdt(cid:48)dsds(cid:48)

(cid:12)
(cid:12)
(cid:12) ≤

(cid:90)

t−sCjj
Cii
(cid:90)

2||f ||∞
T 2
2||f ||∞
T 2 CiiCjj
≤
≤ 4||f ||∞CiiCjj F T
T

atbt(cid:48)|gt(cid:48)−t|dtdt(cid:48)

−→
T →∞

0

(cid:90)

(cid:12)
(cid:12)
(cid:12)

t,s(cid:48)κi,j
κi,j

(cid:12)
(cid:12)
t(cid:48),sGtdt

(cid:12) ≤ 4||f ||∞(Cij)2 F T

−→
T →∞

0

T

κi,j
t,t(cid:48)ΛiΛjGtdt =

κi,j
t,t(cid:48)atbt(cid:48)gt(cid:48)−tdtdt(cid:48)

asbs(cid:48)gs(cid:48)−sdsds(cid:48) = 0

(cid:90)

(cid:90)

ΛiΛj
T 2

19

Second form

(cid:90)

κi,i
t,sΛjΛjGtdt =

(cid:19)2 (cid:90)

(cid:18) Λj
T

κi,i
t,satbt(cid:48)gt(cid:48)−tas(b (cid:63) (cid:101)g)sdtdt(cid:48)ds = 0

We have just proved that V( (cid:98)C
− C(cid:107)
and ﬁnally that (cid:107) (cid:98)C

(T )

(T )

P

)
P

−→ 0.

−→ 0. By Markov inequality, it ensures us that (cid:107) (cid:98)C

(T )

−E( (cid:98)C

(T )

P

)(cid:107)

−→ 0,

− Kc(cid:107)

Proof that (cid:107)(cid:100)Kc(T )
The scheme of the proof is similar to the previous one. The upper bounds of the integrals involve the same
kind of terms, plus the new term (F T )2/T that goes to zero thanks to the assumption 5 of the theorem.

−→ 0

P

5 Conclusion

In this paper, we introduce a simple nonparametric method (the NPHC algorithm) that leads to a fast and
robust estimation of the matrix G of the kernel integrals of a Multivariate Hawkes process that encodes
Granger causality between nodes. This method relies on the matching of the integrated order 2 and
order 3 empirical cumulants, which represent the simplest set of global observables containing sufﬁcient
information to recover the matrix G. Since this matrix fully accounts for the self- and cross- inﬂuences of
the process nodes (that can represent agents or users in applications), our approach can naturally be used
to quantify the degree of endogeneity of a system and to uncover the causality structure of a network.

By performing numerical experiments involving very different kernel shapes, we show that the
baselines, involving either parametric or non-parametric approaches are very sensible to model misspeci-
ﬁcation, do not lead to accurate estimation, and are numerically expensive, while NPHC provides fast,
robust and reliable results. This is conﬁrmed on the MemeTracker database, where we show that NPHC
outperforms classical approaches based on EM algorithms or the Wiener-Hopf equations. Finally, the
NPHC algorithm provided very satisfying results on ﬁnancial data, that are consistent with well-known
stylized facts in ﬁnance.

Acknowledgements

This work beneﬁted from the support of the chair “Changing markets”, CMAP École Polytechnique and
École Polytechnique fund raising - Data Science Initiative.

The authors want to thank Marcello Rambaldi for fruitful discussions on order book data’s experi-

ments.

20

References

M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.
arXiv preprint arXiv:1603.04467, 2016.

Y. Aït-Sahalia, J. Cacho-Diaz, and R. JA Laeven. Modeling ﬁnancial contagion using mutually exciting

jump processes. Technical report, National Bureau of Economic Research, 2010.

E. Bacry and J.-F. Muzy. First- and second-order statistics characterization of hawkes processes and

non-parametric estimation. IEEE Transactions on Information Theory, 62(4):2184–2202, 2016.

E. Bacry, I. Mastromatteo, and J.-F. Muzy. Hawkes processes in ﬁnance. Market Microstructure and

Liquidity, 1(01):1550005, 2015.

E. Bacry, T. Jaisson, and J.-F. Muzy. Estimation of slowly decreasing hawkes kernels: application to

high-frequency order book dynamics. Quantitative Finance, pages 1–23, 2016.

A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of multilayer

networks. In AISTATS, 2015.

R. Crane and D. Sornette. Robust dynamic classes revealed by measuring the response function of a

social system. Proceedings of the National Academy of Sciences, 105(41), 2008.

J. Da Fonseca and R. Zaatour. Hawkes process: Fast calibration, application to trade clustering, and

diffusive limit. Journal of Futures Markets, 34(6):548–579, 2014.

D. J. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes Volume I: Elementary

Theory and Methods. Springer Science & Business Media, 2003.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.

M. Eichler, R. Dahlhaus, and J. Dueck. Graphical modeling for multivariate hawkes processes with non-
parametric link functions. Journal of Time Series Analysis, pages n/a–n/a, 2016. ISSN 1467-9892. doi:
10.1111/jtsa.12213. URL http://dx.doi.org/10.1111/jtsa.12213. 10.1111/jtsa.12213.

M. Farajtabar, Y. Wang, M. Rodriguez, S. Li, H. Zha, and L. Song. Coevolve: A joint point process model
for information diffusion and network co-evolution. In Advances in Neural Information Processing
Systems, pages 1945–1953, 2015.

M. Gomez-Rodriguez, J. Leskovec, and B. Schölkopf. Modeling information propagation with survival

theory. Proceedings of the International Conference on Machine Learning, 2013.

C. W. J. Granger. Investigating causal relations by econometric models and cross-spectral methods.
Econometrica, 37(3):424–438, 1969. ISSN 00129682, 14680262. URL http://www.jstor.
org/stable/1912791.

A. R. Hall. Generalized Method of Moments. Oxford university press, 2005.

L. P. Hansen. Large sample properties of generalized method of moments estimators. Econometrica:

Journal of the Econometric Society, pages 1029–1054, 1982.

N. R. Hansen, P. Reynaud-Bouret, and V. Rivoirard. Lasso and probabilistic inequalities for multivariate

point processes. Bernoulli, 21(1):83–143, 2015.

21

S. J. Hardiman and J.-P. Bouchaud. Branching-ratio approximation for the self-exciting Hawkes process.

Phys. Rev. E, 90(6):062807, December 2014. doi: 10.1103/PhysRevE.90.062807.

A. G. Hawkes. Point spectra of some mutually exciting point processes. Journal of the Royal Statistical
Society. Series B (Methodological), 33(3):438–443, 1971. ISSN 00359246. URL http://www.
jstor.org/stable/2984686.

A. G. Hawkes and D. Oakes. A cluster process representation of a self-exciting process. Journal of

Applied Probability, pages 493–503, 1974.

S. Jovanovi´c, J. Hertz, and S. Rotter. Cumulants of Hawkes point processes. Phys. Rev. E, 91(4):042802,

April 2015. doi: 10.1103/PhysRevE.91.042802.

R. Lemonnier and N. Vayatis. Nonparametric markovian learning of triggering kernels for mutually
exciting and mutually inhibiting multivariate hawkes processes. In Machine Learning and Knowledge
Discovery in Databases, pages 161–176. Springer, 2014.

E. Lewis and G. Mohler. A nonparametric em algorithm for multiscale hawkes processes. Journal of

Nonparametric Statistics, 2011.

G. O. Mohler, M. B. Short, P. J. Brantingham, F. P. Schoenberg, and G. E. Tita. Self-exciting point

process modeling of crime. Journal of the American Statistical Association, 2011.

W. K Newey and D. McFadden. Large sample estimation and hypothesis testing. Handbook of economet-

rics, 4:2111–2245, 1994.

27(1):23–31, 1981.

Y. Ogata. On lewis’ simulation method for point processes. Information Theory, IEEE Transactions on,

Y. Ogata. Space-time point-process models for earthquake occurrences. Annals of the Institute of

Statistical Mathematics, 50(2):379–402, 1998.

A. Podosinnikova, F. Bach, and S. Lacoste-Julien. Rethinking lda: moment matching for discrete ica. In

Advances in Neural Information Processing Systems, pages 514–522, 2015.

P. Reynaud-Bouret and S. Schbath. Adaptive estimation for hawkes processes; application to genome

analysis. The Annals of Statistics, 38(5):2781–2822, 2010.

V.S. Subrahmanian, A. Azaria, S. Durst, V. Kagan, A. Galstyan, K. Lerman, L. Zhu, E. Ferrara, A. Flam-

mini, and F. Menczer. The darpa twitter bot challenge. Computer, 49(6):38–46, 2016.

H. Xu, M. Farajtabar, and H. Zha. Learning granger causality for hawkes processes. In Proceedings of

The 33rd International Conference on Machine Learning, pages 1717–1726, 2016.

S.-H. Yang and H. Zha. Mixture of mutually exciting processes for viral diffusion. In Proceedings of the

International Conference on Machine Learning, 2013.

K. Zhou, H. Zha, and L. Song. Learning triggering kernels for multi-dimensional hawkes processes. In

Proceedings of the International Conference on Machine Learning, pages 1301–1309, 2013a.

K. Zhou, H. Zha, and L. Song. Learning social infectivity in sparse low-rank networks using multi-

dimensional hawkes processes. AISTATS, 2013b.

22

7
1
0
2
 
y
a
M
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
3
3
6
0
.
7
0
6
1
:
v
i
X
r
a

Uncovering Causality from Multivariate Hawkes Integrated
Cumulants

Massil Achab∗1, Emmanuel Bacry1, Stéphane Gaiffas1, Iacopo Mastromatteo2, and
Jean-François Muzy1,3

1Centre de Mathématiques Appliquées, CNRS, Ecole Polytechnique, UMR 7641, 91128 Palaiseau, France
2Capital Fund Management, 23 rue de l’Université, 75007 Paris, France
3Laboratoire Sciences Pour l’Environnement, Université de Corse, 7 Avenue Jean Nicoli, 20250 Corte, France

May 31, 2017

Abstract

We design a new nonparametric method that allows one to estimate the matrix of integrated
kernels of a multivariate Hawkes process. This matrix not only encodes the mutual inﬂuences of each
node of the process, but also disentangles the causality relationships between them. Our approach is
the ﬁrst that leads to an estimation of this matrix without any parametric modeling and estimation of
the kernels themselves. As a consequence, it can give an estimation of causality relationships between
nodes (or users), based on their activity timestamps (on a social network for instance), without
knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment
matching method that ﬁts the second-order and the third-order integrated cumulants of the process. A
theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we
show, on numerical experiments, that our approach is indeed very robust with respect to the shape of
the kernels and gives appealing results on the MemeTracker database and on ﬁnancial order book data.

Keywords. Hawkes Process, Causality Inference, Cumulants, Generalized Method of Moments

1 Introduction

In many applications, one needs to deal with data containing a very large number of irregular timestamped
events that are recorded in continuous time. These events can reﬂect, for instance, the activity of users
on a social network, see Subrahmanian et al. (2016), the high-frequency variations of signals in ﬁnance,
see Bacry et al. (2015), the earthquakes and aftershocks in geophysics, see Ogata (1998), the crime
activity, see Mohler et al. (2011) or the position of genes in genomics, see Reynaud-Bouret and Schbath
(2010). The succession of the precise timestamps carries a great deal of information about the dynamics
of the underlying systems. In this context, multidimensional counting processes based models play a
paramount role. Within this framework, an important task is to recover the mutual inﬂuence of the nodes
(i.e., the different components of the counting process), by leveraging on their timestamp patterns, see,
for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou
et al. (2013a); Gomez-Rodriguez et al. (2013); Farajtabar et al. (2015); Xu et al. (2016).

Consider a set of nodes I = {1, . . . , d}. For each i ∈ I, we observe a set Zi of events, where each
τ ∈ Zi labels the occurrence time of an event related to the activity of i. The events of all nodes can

∗massil.achab@m4x.org

1

be represented as a vector of counting processes N t = [N 1
events of node i until time t ∈ R+, namely N i
λt = [λ1

t = (cid:80)
t ](cid:62) associated with the multivariate counting process N t is deﬁned as
P(N i

t ](cid:62), where N i

t counts the number of
τ ∈Zi 1{τ ≤t}. The vector of stochastic intensities

t · · · N d

t · · · λd

t = 1|Ft)

λi
t = lim
dt→0

t+dt − N i
dt

for i ∈ I, where the ﬁltration Ft encodes the information available up to time t. The coordinate λi
t gives
the expected instantaneous rate of event occurrence at time t for node i. The vector λt characterizes
the distribution of N t, see Daley and Vere-Jones (2003), and patterns in the events time-series can be
captured by structuring these intensities.

The Hawkes process introduced in Hawkes (1971) corresponds to an autoregressive structure of
the intensities in order to capture self-excitation and cross-excitation of nodes, which is a phenomenon
typically observed, for instance, in social networks, see for instance Crane and Sornette (2008). Namely,
N t is called a Hawkes point process if the stochastic intensities can be written as

t = µi +
λi

φij(t − t(cid:48))dN j
t(cid:48),

d
(cid:88)

(cid:90) t

j=1

0

where µi ∈ R+ is an exogenous intensity and φij are positive, integrable and causal (with support in R+)
functions called kernels encoding the impact of an action by node j on the activity of node i. Note that
when all kernels are zero, the process is a simple homogeneous multivariate Poisson process.

Most of the litterature uses a parametric approach for estimating the kernels. With no doubt, the
most popular parametrization form is the exponential kernel φij(t) = αijβije−βij t because it deﬁnitely
simpliﬁes the inference algorithm (e.g., the complexity needed for computing the likelihood is much
smaller). When d is large, in order to reduce the number of parameters, some authors choose to arbitrarily
share the kernel shapes across the different nodes. Thus, for instance, in Yang and Zha (2013); Zhou
et al. (2013b); Farajtabar et al. (2015), they choose φij(t) = αijh(t) with αij ∈ R+ quantiﬁes the
intensity of the inﬂuence of j on i and h(t) a (normalized) function that characterizes the time-proﬁle
of this inﬂuence and that is shared by all couples of nodes (i, j) (most often, it is chosen to be either
exponential h(t) = βe−βt or power law h(t) = βt−(β+1)). Both approaches are, most of the time,
highly non-realistic. On the one hand there is a priori no reason for assuming that the time-proﬁle of the
inﬂuence of a node j on a node i does not depend on the pair (i, j). On the other hand, assuming an
exponential shape or a power law shape for a kernel arbitrarily imposes an event impact that is always
instantly maximal and that can only decrease with time, while in practice, there may exist a latency
between an event and its maximal impact.

In order to have more ﬂexibility on the shape of the kernels, nonparametric estimation can be
considered. Expectation-Maximization algorithms can be found in Lewis and Mohler (2011) (for d = 1)
or in Zhou et al. (2013a) (d > 1). An alternative method is proposed in Bacry and Muzy (2016) where
the nonparametric estimation is formulated as a numerical solving of a Wiener-Hopf equation. Another
nonparametric strategy considers a decomposition of kernels on a dictionary of function h1, . . . , hK,
namely φij(t) = (cid:80)K
k are estimated, see Hansen et al. (2015);
Lemonnier and Vayatis (2014) and Xu et al. (2016), where group-lasso is used to induce a sparsity pattern
on the coefﬁcients aij

k hk(t), where the coefﬁcients aij

k that is shared across k = 1, . . . , K.

k=1 aij

Such methods are heavy when d is large, since they rely on likelihood maximization or least squares
minimization within an over-parametrized space in order to gain ﬂexibility on the shape of the kernels.
This is problematic, since the original motivation for the use of Hawkes processes is to estimate the
inﬂuence and causality of nodes, the knowledge of the full parametrization of the model being of little

2

interest for causality purpose.

Our paper solves this problem with a different and more direct approach. Instead of trying to estimate
the kernels φij, we focus on the direct estimation of their integrals. Namely, we want to estimate the
matrix G = [gij] where

gij =

φij(u) du ≥ 0 for 1 ≤ i, j ≤ d.

(1)

(cid:90) +∞

0

As it can be seen from the cluster representation of Hawkes processes (Hawkes and Oakes (1974)), this
integral represents the mean total number of events of type i directly triggered by an event of type j, and
then encodes a notion of causality. Actually, as detailed below (see Section 2.1), such integral can be
related to the Granger causality (Granger (1969)).

The main idea of the method we developed in this paper is to estimate the matrix G directly using a
matching cumulants (or moments) method. Apart from the mean, we shall use second and third-order
cumulants which correspond respectively to centered second and third-order moments. We ﬁrst compute
an estimation (cid:99)M of these centered moments M (G) (they are uniquely deﬁned by G). Then, we look for
a matrix (cid:98)G that minimizes the L2 error (cid:107)M ( (cid:98)G) − (cid:99)M (cid:107)2. Thus the integral matrix (cid:98)G is directly estimated
without making hardly any assumptions on the shape the involved kernels. As it will be shown, this
approach turns out to be particularly robust to the kernel shapes, which is not the case of all previous
Hawkes-based approaches that aim causality recovery. We call this method NPHC (Non Parametric
Hawkes Cumulant), since our approach is of nonparametric nature. We provide a theoretical analysis
that proves the consistency of the NPHC estimator. Our proof is based on ideas from the theory of
Generalized Method of Moments (GMM) but requires an original technical trick since our setting strongly
departs from the standard parametric statistics with i.i.d observations. Note that moment and cumulant
matching techniques proved particularly powerful for latent topic models, in particular Latent Dirichlet
Allocation, see Podosinnikova et al. (2015). A small set of previous works, namely Da Fonseca and
Zaatour (2014); Aït-Sahalia et al. (2010), already used method of moments with Hawkes processes, but
only in a parametric setting. Our work is the ﬁrst to consider such an approach for a nonparametric
counting processes framework.

The paper is organized as follows: in Section 2, we provide the background on the integrated kernels
and the integrated cumulants of the Hawkes process. We then introduce the method, investigate its
complexity and explain the consistency result we prove. In Section 3, we estimate the matrix of Hawkes
kernels’ integrals for various simulated datasets and for real datasets, namely the MemeTracker database
and ﬁnancial order book data. We then provide in Section 4 the technical details skipped in the previous
parts and the proof of our consistency result. Section 5 contains concluding remarks.

2 NPHC: The Non Parametric Hawkes Cumulant method

In this Section, we provide the background on integrals of Hawkes kernels and integrals of Hawkes
cumulants. We then explain how the NPHC method enables estimating G.

2.1 Branching structure and Granger causality

From the deﬁnition of Hawkes process as a Poisson cluster process, see Jovanovi´c et al. (2015) or Hawkes
and Oakes (1974), gij can be simply interpreted as the average total number of events of node i whose
direct ancestor is a given event of node j (by direct we mean that interactions mediated by any other
intermediate event are not counted). In that respect, G not only describes the mutual inﬂuences between

3

nodes, but it also quantiﬁes their direct causal relationships. Namely, introducing the counting function
N i←j
that counts the number of events of i whose direct ancestor is an event of j, we know from Bacry
t
et al. (2015) that

E[dN i←j
(2)
t
where we introduced Λi as the intensity expectation, namely satisfying E[dN i
t ] = Λidt. Note that Λi does
not depend on time by stationarity of N t, which is known to hold under the stability condition (cid:107)G(cid:107) < 1,
where (cid:107)G(cid:107) stands for the spectral norm of G. In particular, this condition implies the non-singularity of
Id − G.

t ] = gijΛjdt,

] = gijE[dN j

Since the question of a real causality is too complex in general, most econometricians agreed on
the simpler deﬁnition of Granger causality Granger (1969). Its mathematical formulation is a statistical
hypothesis test: X causes Y in the sense of Granger causality if forecasting future values of Y is more
successful while taking X past values into account. In Eichler et al. (2016), it is shown that for N t
a multivariate Hawkes process, N j
t w.r.t N t if and only if φij(u) = 0 for
u ∈ R+. Since the kernels take positive values, the latter condition is equivalent to (cid:82) ∞
0 φij(u)du = 0. In
the following, we’ll refer to learning the kernels’ integrals as uncovering causality since each integral
encodes the notion of Granger causality, and is also linked to the number of events directly caused from a
node to another node, as described above at Eq. (2).

t does not Granger-cause N i

2.2

Integrated cumulants of the Hawkes process

A general formula for the integral of the cumulants of a multivariate Hawkes process is provided
in Jovanovi´c et al. (2015). As explained below, for the purpose of our method, we only need to consider
cumulants up to the third order. Given 1 ≤ i, j, k ≤ d, the ﬁrst three integrated cumulants of the Hawkes
process can be deﬁned as follows thanks to stationarity:

Λidt = E(dN i
t )
(cid:90)
(cid:16)

Cijdt =

E(dN i

t dN j

t+τ ) − E(dN i

t )E(dN j

(cid:17)
t+τ )

τ ∈R

(cid:90) (cid:90)

(cid:16)

τ,τ (cid:48)∈R2
− E(dN i

Kijkdt =

E(dN i

t dN j

t+τ dN k

t+τ (cid:48)) + 2E(dN i

t )E(dN j

t+τ )E(dN k

t+τ (cid:48))

t dN j

t+τ )E(dN k

t+τ (cid:48)) − E(dN i

t dN k

t+τ (cid:48))E(dN j

t+τ ) − E(dN j

t+τ dN k

(cid:17)
t+τ (cid:48))E(dN i
t )

,

where Eq. (3) is the mean intensity of the Hawkes process, the second-order cumulant (4) refers to the
integrated covariance density matrix and the third-order cumulant (5) measures the skewness of N t. Using
the martingale representation from Bacry and Muzy (2016) or the Poisson cluster process representation
from Jovanovi´c et al. (2015), one can obtain an explicit relationship between these integrated cumulants
and the matrix G. If one sets

R = (Id − G)−1,

straightforward computations (see Section 4) lead to the following identities:

Λi =

Rimµm

Cij =

ΛmRimRjm

d
(cid:88)

m=1
d
(cid:88)

m=1
d
(cid:88)

m=1

4

Kijk =

(RimRjmCkm + RimCjmRkm + CimRjmRkm − 2ΛmRimRjmRkm).

(3)

(4)

(5)

(6)

(7)

(8)

(9)

Equations (8) and (9) are proved in Section 4. Our strategy is to use a convenient subset of Eqs. (3), (4)
and (5) to deﬁne M , while we use Eqs. (7), (8) and (9) in order to construct the operator that maps a
candidate matrix R to the corresponding cumulants M (R). By looking for (cid:98)R that minimizes R (cid:55)→
(cid:107)M (R) − (cid:99)M (cid:107)2, we obtain, as illustrated below, good recovery of the ground truth matrix G using
Equation (6).

The simplest case d = 1 has been considered in Hardiman and Bouchaud (2014), where it is shown
that one can choose M = {C11} in order to compute the kernel integral. Eq. (8) then reduces to a simple
second-order equation that has a unique solution in R (and consequently a unique G) that accounts for
the stability condition ((cid:107)G(cid:107) < 1).

Unfortunately, for d > 1, the choice M = {Cij}1≤i≤j≤d is not sufﬁcient to uniquely determine the
kernels integrals. In fact, the integrated covariance matrix provides d(d + 1)/2 independent coefﬁcients,
while d2 parameters are needed. It is straightforward to show that the remaining d(d − 1)/2 conditions
can be encoded in an orthogonal matrix O, reﬂecting the fact that Eq. (8) is invariant under the change
R → OR, so that the system is under-determined.

Our approach relies on using the third order cumulant tensor K = [Kijk] which contains (d3 + 3d2 +
2d)/6 > d2 independent coefﬁcients that are sufﬁcient to uniquely ﬁx the matrix G. This can be justiﬁed
intuitively as follows: while the integrated covariance only contains symmetric information, and is thus
unable to provide causal information, the skewness given by the third order cumulant in the estimation
procedure can break the symmetry between past and future so as to uniquely ﬁx G. Thus, our algorithm
consists of selecting d2 third-order cumulant components, namely M = {Kiij}1≤i,j≤d. In particular, we
deﬁne the estimator of R as (cid:98)R ∈ argminRL(R), where

L(R) = (1 − κ)(cid:107)Kc(R) − (cid:100)Kc(cid:107)2

2 + κ(cid:107)C(R) − (cid:98)C(cid:107)2
2,

(10)

where (cid:107) · (cid:107)2 stands for the Frobenius norm, Kc = {Kiij}1≤i,j≤d is the matrix obtained by the contraction
of the tensor K to d2 indices, C is the covariance matrix, while (cid:100)Kc and (cid:98)C are their respective estimators,
see Equations (12), (13) below. It is noteworthy that the above mean square error approach can be seen
as a peculiar Generalized Method of Moments (GMM), see Hall (2005). This framework allows us to
determine the optimal weighting matrix involved in the loss function. However, this approach is unusable
in practice, since the associated complexity is too high. Indeed, since we have d2 parameters, this matrix
has d4 coefﬁcients and GMM calls for computing its inverse leading to a O(d6) complexity. In this work,
we use the coefﬁcient κ to scale the two terms, as

see Section 4.4 for an explanation about the link between κ and the weighting matrix. Finally, the
estimator of G is straightforwardly obtained as

from the inversion of Eq. (6). Let us mention an important point: the matrix inversion in the previous
formula is not the bottleneck of the algorithm. Indeed, its has a complexity O(d3) that is cheap compared
to the computation of the cumulants when n = maxi |Zi| (cid:29) d, which is the typical scaling satisﬁed
in applications. Solving the considered problem on a larger scale, say d (cid:29) 103, is an open question,
even with state-of-the-art parametric and nonparametric approaches, see for instance Zhou et al. (2013a);
Xu et al. (2016); Zhou et al. (2013b); Bacry and Muzy (2016), where the number of components d in
experiments is always around 100 or smaller. Note that, actually, our approach leads to a much faster
algorithm than the considered state-of-the-art baselines, see Tables 1–4 from Section 3 below.

κ =

(cid:107)(cid:100)Kc(cid:107)2
2
2 + (cid:107) (cid:98)C(cid:107)2
2

,

(cid:107)(cid:100)Kc(cid:107)2

(cid:98)G = Id − (cid:98)R

−1

,

5

2.3 Estimation of the integrated cumulants

In this section we present explicit formulas to estimate the three moment-based quantities listed in the
previous section, namely, Λ, C and K. We ﬁrst assume there exists H > 0 such that the truncation
from (−∞, +∞) to [−H, H] of the domain of integration of the quantities appearing in Eqs. (4) and (5),
introduces only a small error. In practice, this amounts to neglecting border effects in the covariance
density and in the skewness density that is a good approximation if the support of the kernel φij(t) is
smaller than H and the spectral norm (cid:107)G(cid:107) satisﬁes (cid:107)G(cid:107) < 1.
In this case, given a realization of a stationary Hawkes process {N t : t ∈ [0, T ]}, as shown in Section 4,
we can write the estimators of the ﬁrst three cumulants (3), (4) and (5) as

(cid:98)Λi =

(cid:88)

1 =

N i
T
T

1
T

1
T

1
T

τ ∈Zi
(cid:88)

(cid:16)

τ ∈Zi
(cid:88)

(cid:16)

(cid:98)Cij =

(cid:98)Kijk =

τ ∈Zi
(cid:98)Λi
T

−

(cid:88)

(cid:88)

τ ∈Zj

τ (cid:48)∈Zk

N j

τ +H − N j

τ −H − 2H (cid:98)Λj(cid:17)

N j

τ +H − N j

τ −H − 2H (cid:98)Λj(cid:17)

·

(cid:16)

N k

τ +H − N k

τ −H − 2H (cid:98)Λk(cid:17)

(2H − |τ (cid:48) − τ |)+ + 4H 2 (cid:98)Λi (cid:98)Λj (cid:98)Λk.

(11)

(12)

(13)

Let us mention the following facts.

Bias. While the ﬁrst cumulant ˆΛi is an unbiased estimator of Λi, the other estimators (cid:98)Cij and (cid:98)Kijk
introduce a bias. However, as we will show, in practice this bias is small and hardly affects
numerical estimations (see Section 3). This is conﬁrmed by our theoretical analysis, which proves
that if H does not grow too fast compared to T , then these estimated cumulants are consistent
estimators of the theoretical cumulants (see Section 2.6).

Complexity. The computations of all the estimators of the ﬁrst, second and third-order cumulants
have complexity respectively O(nd), O(nd2) and O(nd3), where n = maxi |Zi|. However, our
algorithm requires a lot less than that: it computes only d2 third-order terms, of the form (cid:98)Kiij,
leaving us with only O(nd2) operations to perform.

Symmetry. While the values of Λi, Cij and Kijk are symmetric under permutation of the indices, their
estimators are generally not symmetric. We have thus chosen to symmetrize the estimators by
averaging their values over permutations of the indices. Worst case is for the estimator of Kc,
which involves only an extra factor of 2 in the complexity.

2.4 The NPHC algorithm

The objective to minimize in Equation (10) is non-convex. More precisely, the loss function is a
polynomial of R of degree 6. However, the expectations of cumulants Λ and C deﬁned in Eq. (4) and (5)
that appear in the deﬁnition of L(R) are unknown and should be replaced with (cid:98)Λ and (cid:98)C. We denote
(cid:101)L(R) the objective function, where the expectations of cumulants Λi and Cij have been replaced with
their estimators in the right-hand side of Eqs. (8) and (9):

(cid:101)L(R) = (1 − κ)(cid:107)R(cid:12)2 (cid:98)C

+ 2[R (cid:12) ( (cid:98)C − R (cid:98)L)]R(cid:62) − (cid:100)Kc(cid:107)2

2 + κ(cid:107)R (cid:98)LR(cid:62) − (cid:98)C(cid:107)2
2

(14)

(cid:62)

As explained in Choromanska et al. (2015), the loss function of a typical multilayer neural network
with simple nonlinearities can be expressed as a polynomial function of the weights in the network,

6

whose degree is the number of layers. Since the loss function of NPHC writes as a polynomial of
degree 6, we expect good results using optimization methods designed to train deep multilayer neural
networks. We used the AdaGrad from Duchi et al. (2011), a variant of the Stochastic Gradient Descent
with adaptive learning rates. AdaGrad scales the learning rates coordinate-wise using the online variance
of the previous gradients, in order to incorporate second-order information during training. The NPHC
method is summarized schematically in Algorithm 1.

Algorithm 1 Non Parametric Hawkes Cumulant method
Input: N t
Output: (cid:98)G
1: Estimate (cid:98)Λi, (cid:98)Cij, (cid:98)Kiij from Eqs. (11, 12, 13)
2: Design (cid:101)L(R) using the computed estimators.
3: Minimize numerically (cid:101)L(R) so as to obtain (cid:98)R
−1
4: Return (cid:98)G = Id − (cid:98)R

.

Our problem being non-convex, the choice of the starting point has a major effect on the convergence.
Here, the key is to notice that the matrices R that match Equation (8) writes C1/2OL−1/2, with
L = diag(Λ) and O an orthogonal matrix. Our starting point is then simply chosen by setting O = Id in
the previous formula, leading to nice convergence results. Even though our main concern is to retrieve the
matrix G, let us notice we can also obtain an estimation of the baseline intensities’ from Eq. (3), which
leads to (cid:98)µ = (cid:98)R
(cid:98)Λ. An efﬁcient implementation of this algorithm with TensorFlow, see Abadi et al.
(2016), is available on GitHub: https://github.com/achab/nphc.

−1

2.5 Complexity of the algorithm

Compared with existing state-of-the-art methods to estimate the kernel functions, e.g., the ordinary
differential equations-based (ODE) algorithm in Zhou et al. (2013a), the Granger Causality-based
algorithm in Xu et al. (2016), the ADM4 algorithm in Zhou et al. (2013b), and the Wiener-Hopf-based
algorithm in Bacry and Muzy (2016), our method has a very competitive complexity. This can be
understood by the fact that those methods estimate the kernel functions, while in NPHC we only estimate
their integrals. The ODE-based algorithm is an EM algorithm that parametrizes the kernel function
with M basis functions, each being discretized to L points. The basis functions are updated after
solving M Euler-Lagrange equations. If n denotes the maximum number of events per component (i.e.
n = max1≤i≤d |Zi|) then the complexity of one iteration of the algorithm is O(M n3d2 + M L(nd + n2)).
The Granger Causality-based algorithm is similar to the previous one, without the update of the basis
functions, that are Gaussian kernels. The complexity per iteration is O(M n3d2). The algorithm ADM4
is similar to the two algorithms above, as EM algorithm as well, with only one exponential kernel as
basis function. The complexity per iteration is then O(n3d2). The Wiener-Hopf-based algorithm is not
iterative, on the contrary to the previous ones. It ﬁrst computes the empirical conditional laws on many
points, and then invert the Wiener-Hopf system, leading to a O(nd2L + d4L3) computation. Similarly,
our method ﬁrst computes the integrated cumulants, then minimize the objective function with Niter
iterations, and invert the resulting matrix (cid:98)R to obtain (cid:98)G. In the end, the complexity of the NPHC method
is O(nd2 + Niterd3). According to this analysis, summarized in Table 1 below, one can see that in the
regime n (cid:29) d, the NPHC method outperforms all the other ones.

2.6 Theoretical guarantee: consistency

The NPHC method can be phrased using the framework of the Generalized Method of Moments (GMM).
GMM is a generic method for estimating parameters in statistical models. In order to apply GMM,

7

Table 1: Complexity of state-of-the-art methods. NPHC’s complexity is very low , especially in the
regime n (cid:29) d.

Method

ODE Zhou et al. (2013a)
GC Xu et al. (2016)
ADM4 Zhou et al. (2013b)
WH Bacry and Muzy (2016) O(nd2L + d4L3)
O(nd2 + Niterd3)
NPHC

Total complexity
O(NiterM (n3d2 + L(nd + n2)))
O(NiterM n3d2)
O(Nitern3d2)

we have to ﬁnd a vector-valued function g(X, θ) of the data, where X is distributed with respect to a
distribution Pθ0, which satisﬁes the moment condition: E[g(X, θ)] = 0 if and only if θ = θ0, where θ0 is
the “ground truth” value of the parameter. Based on i.i.d. observed copies x1, . . . , xn of X, the GMM
method minimizes the norm of the empirical mean over n samples, (cid:107) 1
i=1 g(xi, θ)(cid:107), as a function of
n
θ, to obtain an estimate of θ0.

(cid:80)n

In the theoretical analysis of NPHC, we use ideas from the consistency proof of the GMM, but the
proof actually relies on very different arguments. Indeed, the integrated cumulants estimators used in
NPHC are not unbiased, as the theory of GMM requires, but asymptotically unbiased. Moreover, the
setting considered here, where data consists of a single realization {N t} of a Hawkes process strongly
departs from the standard i.i.d setting. Our approach is therefore based on the GMM idea but the proof is
actually not using the theory of GMM.

In the following, we use the subscript T to refer to quantities that only depend on the process (Nt) in
the interval [0, T ] (e.g., the truncation term HT , the estimated integrated covariance (cid:98)CT or the estimated
kernel norm matrix (cid:98)GT ). In the next equation, (cid:12) stands for the Hadamard product and (cid:12)2 stands for
the entrywise square of a matrix. We denote G0 = Id − R−1
the true value of G, and the R2d×d valued
0
vector functions

g0(R) =

(cid:98)gT (R) =

(cid:20)

C − RLR(cid:62)
Kc − R(cid:12)2C(cid:62) − 2[R (cid:12) (C − RL)]R(cid:62)
(cid:34)

(cid:21)

(cid:98)CT − R (cid:98)LT R(cid:62)
(cid:62)
T − 2[R (cid:12) ( (cid:98)CT − R (cid:98)LT )]R(cid:62) .

(cid:35)

(cid:100)Kc

T − R(cid:12)2 (cid:98)C

Using these notations, (cid:101)LT (R) can be seen as the weighted squared Frobenius norm of (cid:98)gT (R). Moreover,
P
when T → +∞, one has (cid:98)gT (R)
→
stands for convergence in probability.

P
→ g0(R) under the conditions of the following theorem, where

Theorem 2.1 (Consistency of NPHC). Suppose that (Nt) is observed on R+ and assume that

1. g0(R) = 0 if and only if R = R0;

2. R ∈ Θ, where Θ is a compact set;

4. HT → ∞ and H 2

T /T → 0.

Then

3. the spectral radius of the kernel norm matrix satisﬁes (cid:107)G0(cid:107) < 1;

(cid:18)

(cid:98)GT = Id −

arg min
R∈Θ

(cid:101)LT (R)

(cid:19)−1

P
→ G0.

8

The proof of the Theorem is given in Section 4.5 below. Assumption 3 is mandatory for stability of
the Hawkes process, and Assumptions 3 and 4 are sufﬁcient to prove that the estimators of the integrated
cumulants deﬁned in Equations (11), (12) and (13) are asymptotically consistent. Assumption 2 is a
very mild standard technical assumption allowing to prove consistency for estimators based on moments.
Assumption 1 is a standard asymptotic moment condition, that allows to identity parameters from the
integrated cumulants.

3 Numerical Experiments

In this Section, we provide a comparison of NPHC with the state-of-the art, on simulated datasets with
different kernel shapes, the MemeTracker dataset (social networks) and the order book dynamics dataset
(ﬁnance).

Simulated datasets. We simulated several datasets with Ogata’s Thinning algorithm Ogata (1981)
using the open-source library tick1, each corresponding to a shape of kernel: rectangular, exponential
or power law kernel, see Figure 1 below.

φt
αβ

log φt
log αβγ

φt
αβ

slope ≈ −(1 + γ)

0 γ

γ + 1/β

t

− log β

log t

0

1/β

t

(a) Rectangular kernel
φt = αβ1[0,1/β](t − γ)

(b) Power law kernel on log-log scale
φt = αβγ(1 + βt)−(1+γ)

(c) Exponential kernel
φt = αβ exp(−βt)

Figure 1: The three different kernels used to simulate the datasets.

The integral of each kernel on its support equals α, 1/β can be regarded as a characteristic time-scale
and γ is the scaling exponent for the power law distribution and a delay parameter for the rectangular
one. We consider a non-symmetric block-matrix G to show that our method can effectively uncover
causality between the nodes, see Figure 3. The matrix G has constant entries α on the three blocks -
α = gij = 1/6 for dimension 10 and α = gij = 1/10 for dimension 100 -, and zero outside. The two
other parameters’ values are the same for dimensions 10 and 100. The parameter γ is set to 1/2 on the
three blocks as well, but we set three very different β0, β1 and β2 from one block to the other, with ratio
βi+1/βi = 10 and β0 = 0.1. The number of events is roughly equal to 105 on average over the nodes.
We ran the algorithm on three simulated datasets: a 10-dimensional process with rectangular kernels
named Rect10, a 10-dimensional process with power law kernels named PLaw10 and a 100-dimensional
process with exponential kernels named Exp100.

MemeTracker dataset. We use events of the most active sites from the MemeTracker dataset2. This
dataset contains the publication times of articles in many websites/blogs from August 2008 to April
2009, and hyperlinks between posts. We extract the top 100 media sites with the largest number of
documents, with about 7 million of events. We use the links to trace the ﬂow of information and establish
an estimated ground truth for the matrix G. Indeed, when an hyperlink j appears in a post in website i,
the link j can be regarded as a direct ancestor of the event. Then, Eq. (2) shows gij can be estimated by
N i←j
T

T = #{links j → i}/N j
T .

/N j

1https://github.com/X-DataInitiative/tick
2https://www.memetracker.org/data.html

9

Order book dynamics. We apply our method to ﬁnancial data, in order to understand the self and cross-
inﬂuencing dynamics of all event types in an order book. An order book is a list of buy and sell orders for
a speciﬁc ﬁnancial instrument, the list being updated in real-time throughout the day. This model has ﬁrst
been introduced in Bacry et al. (2016), and models the order book via the following 8-dimensional point
, C(a)
process: Nt = (P (a)
t ), where P (a) (resp. P (b)) counts the number
t
of upward (resp. downward) price moves, T (a) (resp. T (b)) counts the number of market orders at the
ask3 (resp. at the bid) that do not move the price, L(a) (resp. L(b)) counts the number of limit orders at
the ask4 (resp. at the bid) that do not move the price, and C(a) (resp. C(b)) counts the number of cancel
orders at the ask5 (resp. at the bid) that do not move the price. The ﬁnancial data has been provided by
QuantHouse EUROPE/ASIA, and consists of DAX future contracts between 01/01/2014 and 03/01/2014.

, T (a)
t

, P (b)
t

, L(a)
t

, T (b)
t

, L(b)
t

, C(b)

t

Baselines. We compare NPHC to state-of-the art baselines: the ODE-based algorithm (ODE) by Zhou
et al. (2013a), the Granger Causality-based algorithm (GC) by Xu et al. (2016), the ADM4 algorithm
(ADM4) by Zhou et al. (2013b), and the Wiener-Hopf-based algorithm (WH) by Bacry and Muzy (2016).

Metrics. We evaluate the performance of the proposed methods using the computing time, the Relative
Error

RelErr(A, B) =

{aij (cid:54)=0} + |bij|1
1

{aij =0}

1
d2

(cid:88)

i,j

|aij − bij|
|aij|

and the Mean Kendall Rank Correlation

MRankCorr(A, B) =

RankCorr([ai•], [bi•]),

1
d

d
(cid:88)

i=1

where RankCorr(x, y) = 2
d(d−1) (Nconcordant(x, y) − Ndiscordant(x, y)) with Nconcordant(x, y) the number
of pairs (i, j) satisfying xi > xj and yi > yj or xi < xj and yi < yj and Ndiscordant(x, y) the number of
pairs (i, j) for which the same condition is not satisﬁed.

Note that RankCorr score is a value between −1 and 1, representing rank matching, but can take

smaller values (in absolute value) if the entries of the vectors are not distinct.

Figure 2: On Exp100 dataset, estimated (cid:98)G with ADM4 (left), with NPHC (middle) and the ground-
truth matrix G (right). Both ADM4 and NPHC estimates recover the three blocks. However, ADM4
overestimates the integrals on two of the three blocks, while NPHC gives the same value on each blocks.

3i.e. buy orders that are executed and removed from the list
4i.e. buy orders added to the list
5i.e. the number of times a limit order at the ask is canceled: in our dataset, almost 95% of limit orders are canceled before

execution.

10

Figure 3: Estimated (cid:98)G via NPHC on DAX order book data.

Table 2: Metrics on Rect10: comparable rank correlation, strong improvement for relative error and
computing time.

Method

ODE GC

ADM4 WH

NPHC

RelErr
MRankCorr
Time (s)

0.007
0.33
846

0.15
0.02
768

0.10
0.21
709

0.005
0.34
933

0.001
0.34
20

Discussion. We perform the ADM4 estimation, with exponential kernel, by giving the exact value
β = β0 of one block. Let us stress that this helps a lot this baseline, in comparison to NPHC where
nothing is speciﬁed on the shape of the kernel functions. We used M = 10 basis functions for both ODE
and GC algorithms, and L = 50 quadrature points for WH. We did not run WH on the 100-dimensional
datasets, for computing time reasons, because its complexity scales with d4. We ran multi-processed
versions of the baseline methods on 56 cores, to decrease the computing time.

Our method consistently performs better than all baselines, on the three synthetic datasets, on
MemeTracker and on the ﬁnancial dataset, both in terms of Kendall rank correlation and estimation error.
Moreover, we observe that our algorithm is roughly 50 times faster than all the considered baselines.

On Rect10, PLaw10 and Exp100 our method gives very impressive results, despite the fact that it
does not uses any prior shape on the kernel functions, while for instance the ADM4 baseline do. On
Figure 3, we observe that the matrix (cid:98)G estimated with ADM4 recovers well the block for which β = β0,
i.e. the value we gave to the method, but does not perform well on the two other blocks, while the matrix
(cid:98)G estimated with NPHC approximately reaches the true value for each of the three blocks. On these
simulated datasets, NPHC obtains a comparable or slightly better Kendall rank correlation, but improves
a lot the relative error.

On MemeTracker, the baseline methods obtain a high relative error between 9% and 19% while our
method achieves a relative error of 7% which is a strong improvement. Moreover, NPHC reaches a much
better Kendall rank correlation, which proves that it leads to a much better recovery of the relative order
of estimated inﬂuences than all the baselines. Indeed, it has been shown in Zhou et al. (2013a) that kernels
of MemeTracker data are not exponential, nor power law. This partly explains why our approach behaves

11

Table 3: Metrics on PLaw10: comparable rank correlation, strong improvement for relative error and
computing time.

Table 4: Metrics on Exp100: comparable rank correlation, strong improvement for relative error and
computing time.

Method

ODE GC

ADM4 WH

NPHC

RelErr
MRankCorr
Time (s)

0.011
0.31
870

0.09
0.26
781

0.053
0.24
717

0.009
0.34
946

0.0048
0.33
18

Method

ODE GC

ADM4 NPHC

RelErr
MRankCorr
Time (s)

0.092
0.032
3215

0.112
0.009
2950

0.079
0.049
2411

0.008
0.041
47

better.

On the ﬁnancial data, the estimated kernel norm matrix obtained via NPHC, see Figure 3, gave some

interpretable results (see also Bacry et al. (2016)):

1. Any 2 × 2 sub-matrix with same kind of inputs (i.e. Prices changes, Trades, Limits or Cancels) is

symmetric. This shows empirically that ask and bid have symmetric roles.

2. The prices are mostly cross-excited, which means that a price increase is very likely to be followed
by a price decrease, and conversely. This is consistent with the wavy prices we observe on ﬁnancial
markets.

3. The market, limit and cancel orders are strongly self-excited. This can be explained by the
persistence of order ﬂows, and by the splitting of meta-orders into sequences of smaller orders.
Moreover, we observe that orders impact the price without changing it. For example, the increase
of cancel orders at the bid causes downward price moves.

We show in this section how to obtain the equations stated above, the estimators of the integrated
cumulants and the scaling coefﬁcient κ that appears in the objective function. We then prove the theorem
of the paper.

4 Technical details

4.1 Proof of Equation (8)

We denote ν(z) the matrix

νij(z) = Lz

t →

(cid:16)

E(dN i

u+t)

udN j
dudt

− ΛiΛj(cid:17)

,

where Lz(f ) is the Laplace transform of f , and ψt = (cid:80)
refers to the nth auto-
convolution of φt. Then we use the characterization of second-order statistics, ﬁrst formulated in Hawkes
(1971) and fully generalized in Bacry and Muzy (2016),

, where φ((cid:63)n)

n≥1 φ((cid:63)n)

t

t

ν(z) = (Id + L−z(Ψ))L(Id + Lz(Ψ))(cid:62),

12

Table 5: Metrics on MemeTracker: strong improvement in relative error, rank correlation and computing
time.

Method

ODE GC

ADM4 NPHC

RelErr
MRankCorr
Time (s)

0.162
0.07
2944

0.19
0.053
2780

0.092
0.081
2217

0.071
0.095
38

where Lij = Λiδij with δij the Kronecker symbol. Since Id + Lz(Ψ) = (Id − Lz(Φ))−1, taking z = 0
in the previous equation gives

ν(0) = (Id − G)−1L(Id − G(cid:62))−1,

C = RLR(cid:62),

which gives us the result since the entry (i, j) of the last equation gives Cij = (cid:80)

m ΛmRimRjm.

4.2 Proof of Equation (9)

We start from Jovanovi´c et al. (2015), cf. Eqs. (48) to (51), and group some terms:

Kijk =

(cid:88)

ΛmRimRjmRkm

+

+

+

m
(cid:88)

m
(cid:88)

m
(cid:88)

m

RimRjm

ΛnRknL0(ψmn)

RimRkm

ΛnRjnL0(ψmn)

RjmRkm

ΛnRinL0(ψmn).

(cid:88)

n
(cid:88)

n
(cid:88)

n

Using the relations L0(ψmn) = Rmn − δmn and Cij = (cid:80)

m ΛmRimRjm, proves Equation (9).

4.3

Integrated cumulants estimators

For H > 0 let us denote ∆H N i
domain to (−H, H) in Eqs. (4) and (5), one gets by permuting integrals and expectations:

t−H . Let us ﬁrst remark that, if one restricts the integration

t+H − N i

t = N i

(cid:16)

Cijdt = E

Λidt = E(dN i
t )
t (∆H N j
dN i
t (∆H N j
(cid:16)

Kijkdt = E

dN i

(cid:16)

(cid:17)
t − 2HΛj)

t − 2HΛj)(∆H N k

t − 2HΛk)

(cid:17)

− dtΛiE

(∆H N j

t − 2HΛj)(∆H N k

(cid:17)
t − 2HΛk)

.

The estimators (11) and (12) are then naturally obtained by replacing the expectations by their empirical
counterparts, notably

E(dN i
t f (t))
dt

→

1
T

(cid:88)

τ ∈Zi

f (τ ).

13

For the estimator (13), we shall also notice that

E((∆H N j
(cid:90) (cid:90)

=

=

(cid:90)

t − 2HΛj)(∆H N k

t − 2HΛk))

1[−H,H](t)1[−H,H](t(cid:48))Cjk

t−t(cid:48)dtdt(cid:48)

(2H − |t|)+Cjk

t dt.

We estimate the last integral with the remark above.

4.4 Choice of the scaling coefﬁcient κ

Following the theory of GMM, we denote m(X, θ) a function of the data, where X is distributed with
respect to a distribution Pθ0, which satisﬁes the moment conditions g(θ) = E[m(X, θ)] = 0 if and
only if θ = θ0, the parameter θ0 being the ground truth. For x1, . . . , xN observed copies of X, we
denote (cid:98)gi(θ) = m(xi, θ), the usual choice of weighting matrix is (cid:99)WN (θ) = 1
i=1 (cid:98)gi(θ)(cid:98)gi(θ)(cid:62), and
the objective to minimize is then

(cid:80)N

N

(cid:32)

1
N

N
(cid:88)

i=1

(cid:33)

(cid:16)

(cid:98)gi(θ)

(cid:99)WN (θ1)

(cid:32)

(cid:17)−1

(cid:33)

(cid:98)gi(θ)

,

1
N

N
(cid:88)

i=1

(15)

where θ1 is a constant vector. Instead of computing the inverse weighting matrix, we rather use its
projection on {αId : α ∈ R}. It can be shown that the projection choses α as the mean eigenvalue of
(cid:99)WN (θ1). We can easily compute the sum of its eigenvalues:

Tr((cid:99)WN (θ1)) =

Tr((cid:98)gi(θ1)(cid:98)gi(θ1)(cid:62)) =

Tr((cid:98)gi(θ1)(cid:62)

(cid:98)gi(θ1)) =

||(cid:98)gi(θ1)||2
2.

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

i=1

(cid:104)
vec[(cid:100)Kc − Kc(R)], vec[ (cid:98)C − C(R)]

In our case, (cid:98)g(R) =
. Considering a block-wise
weighting matrix, one block for (cid:100)Kc − Kc(R) and the other for (cid:98)C − C(R), the sum of the eigenvalues of
the ﬁrst block becomes (cid:107)(cid:100)Kc − Kc(R)(cid:107)2
2, and (cid:107) (cid:98)C − C(R)(cid:107)2
2 for the second. We compute the previous
terms with R1 = 0. All together, the objective function to minimize is

∈ R2d2

(cid:105)(cid:62)

1
(cid:107)(cid:100)Kc(cid:107)2
2

(cid:16)

(cid:107)Kc(R) − (cid:100)Kc(cid:107)2

2 +

(cid:107)C(R) − (cid:98)C(cid:107)2
2.

(16)

1
(cid:107) (cid:98)C(cid:107)2
2

(cid:17)−1

, and setting κ = (cid:107)(cid:100)Kc(cid:107)2

2/((cid:107)(cid:100)Kc(cid:107)2

2 + (cid:107) (cid:98)C(cid:107)2

2), we

2 + 1/(cid:107) (cid:98)C(cid:107)2
Dividing this function by
2
obtaind the loss function given in Equation (10).

1/(cid:107)(cid:100)Kc(cid:107)2

4.5 Proof of the Theorem

The main difference with the usual Generalized Method of Moments, see Hansen (1982), relies in the
relaxation of the moment conditions, since we have E[(cid:98)gT (θ0)] = mT (cid:54)= 0. We adapt the proof of
consistency given in Newey and McFadden (1994).

We can relate the integral of the Hawkes process’s kernels to the integrals of the cumulant densities,
from Jovanovi´c et al. (2015). Our cumulant matching method would fall into the usual GMM framework
if we could estimate - without bias - the integral of the covariance on R, and the integral of the skewness
on R2. Unfortunately, we can’t do that easily. We can however estimate without bias (cid:82) f T
t dt

t Cij

14

t Kijk

and (cid:82) f T
t dt with f T a compact supported function on [−HT , HT ] that weakly converges to 1,
t = 1[−HT ,HT ](t). Denoting (cid:98)Cij,(T ) the estimator of
with HT −→ ∞. In most cases we will take f T
(cid:82) f T
t Cij
t dt − Cij| can be considered a proxy to the distance to
the classical GMM. This distance has to go to zero to make the rest of GMM’s proof work: the estimator
(cid:98)Cij,(T ) is then asymptotically unbiased towards Cij when T goes to inﬁnity.

t dt, the term |E[ (cid:98)Cij,(T )] − Cij| = | (cid:82) f T

t Cij

4.5.1 Notations

We observe the multivariate point process (N t) on R+, with Zi the events of the ith component. We will
often write covariance / skewness instead of integrated covariance / skewness. In the rest of the document,
we use the following notations.

Hawkes kernels’ integrals Gtrue = (cid:82) Φtdt = ((cid:82) φij

t dt)ij = Id − (Rtrue)−1

Theoretical mean matrix L = diag(Λ1, . . . , Λd)

Theoretical covariance C = RtrueL(Rtrue)(cid:62)

Theoretical skewness Kc = (Kiij)ij = (Rtrue)(cid:12)2C(cid:62) + 2[Rtrue (cid:12) (C − RtrueL)](Rtrue)(cid:62)

Filtering function

f T ≥ 0

supp(f T ) ⊂ [−HT , HT ]

F T = (cid:82) f T

s ds

t = f T
(cid:101)f T
−t

Events sets Zi,T,1 = Zi ∩ [HT , T + HT ]

Zj,T,2 = Zj ∩ [0, T + 2HT ]

Estimators of the mean

(cid:98)Λi =

N i

T +HT
T

−N i

HT

Estimator of the covariance

(cid:98)Cij,(T ) = 1
T

(cid:80)

τ ∈Zi,T,1

τ (cid:48)∈Zj,T,2 fτ (cid:48)−τ − (cid:101)ΛjF T (cid:17)

Estimator of the skewness6

N j
T +2HT
T +2HT

(cid:101)Λj =
(cid:16)(cid:80)





(cid:98)Kijk,(T ) =

1
T





(cid:88)

(cid:88)

fτ (cid:48)−τ − (cid:101)ΛjF T





(cid:88)

fτ (cid:48)−τ − (cid:101)ΛkF T

τ ∈Zi,T,1

−

(cid:98)Λi
T + 2HT

τ (cid:48)∈Zj,T,2


(cid:88)



τ (cid:48)∈Zj,T,2

τ (cid:48)(cid:48)∈Zk,T,2

τ (cid:48)(cid:48)∈Zk,T,2

(cid:88)

(f T (cid:63) (cid:101)f T )τ (cid:48)−τ (cid:48)(cid:48) − (cid:101)Λk(F T )2









6When f T

t = 1[−HT ,HT ](t), we remind that (f T (cid:63) (cid:101)f T )t = (2HT − |t|)+. This leads to the estimator we showed in the

article.

15

GMM related notations

θ = R and

θ0 = Rtrue

(cid:20)

g0(θ) = vec

C − RLR(cid:62)

C(cid:62) − 2[R (cid:12) (C − RL)]R(cid:62)

(cid:21)

∈ R2d2

Kc − R(cid:12)2
(cid:34)

(T )

(cid:98)C
(T )

− R (cid:98)LR(cid:62)
)(cid:62) − 2[R (cid:12) ( (cid:98)C

(T )

− R (cid:98)L)]R(cid:62)

(cid:35)

∈ R2d2

(cid:98)gT (θ) = vec

(cid:100)Kc(T )

− R(cid:12)2

( (cid:98)C

Q0(θ) = g0(θ)(cid:62)W g0(θ)
(cid:98)QT (θ) = (cid:98)gT (θ)(cid:62)(cid:99)WT (cid:98)gT (θ)

4.5.2 Consistency

First, let’s remind a useful theorem for consistency in GMM from Newey and McFadden (1994).

Theorem 4.1. If there is a function Q0(θ) such that (i) Q0(θ) is uniquely maximized at θ0; (ii) Θ
is compact; (iii) Q0(θ) is continuous; (iv) (cid:98)QT (θ) converges uniformly in probability to Q0(θ), then
(cid:98)θT = arg max (cid:98)QT (θ)

−→ θ0.

P

We can now prove the consistency of our estimator.

Theorem 4.2. Suppose that (Nt) is observed on R+, (cid:99)WT

P

−→ W , and

1. W is positive semi-deﬁnite and W g0(θ) = 0 if and only if θ = θ0,

2. θ ∈ Θ, which is compact,

3. the spectral radius of the kernel norm matrix satisﬁes ||Φ||∗ < 1,
4. ∀i, j, k ∈ [d], (cid:82) f T

u du and (cid:82) f T

u du → (cid:82) Cij

u,v dudv → (cid:82) Kijk

u f T

v Kijk

u Cij

u,v dudv,

5. (F T )2/T

−→ 0 and ||f ||∞ = O(1).

P

Then

P

(cid:98)θT

−→ θ0.

Remark 1. In practice, we use a constant sequence of weighting matrices: (cid:99)WT = Id.

Proof. Proceed by verifying the hypotheses of Theorem 2.1 from Newey and McFadden (1994). Con-
dition 2.1(i) follows by (i) and by Q0(θ) = [W 1/2g0(θ)](cid:62)[W 1/2g0(θ)] > 0 = Q0(θ0). Indeed, there
exists a neighborhood N of θ0 such that θ ∈ N \{θ0} and g0(θ) (cid:54)= 0 since g0(θ) is a polynom. Condition
2.1(ii) follows by (ii). Condition 2.1(iii) is satisﬁed since Q0(θ) is a polynom. Condition 2.1(iv) is
harder to prove. First, since (cid:98)gT (θ) is a polynom of θ, we prove easily that E[supθ∈Θ |(cid:98)gT (θ)|] < ∞. Then,
by Θ compact, g0(θ) is bounded on Θ, and by the triangle and Cauchy-Schwarz inequalities,

(cid:12) (cid:98)QT (θ) − Q0(θ)(cid:12)
(cid:12)
(cid:12)

≤ (cid:12)
(cid:12)((cid:98)gT (θ) − g0(θ))(cid:62)(cid:99)WT ((cid:98)gT (θ) − g0(θ))(cid:12)
(cid:12)
(cid:12) + (cid:12)
T )((cid:98)gT (θ) − g0(θ))(cid:12)
+ (cid:12)
(cid:12)g0(θ)(cid:62)((cid:99)WT + (cid:99)W (cid:62)
≤ (cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)2(cid:107)(cid:99)WT (cid:107) + 2(cid:107)g0(θ)(cid:107)(cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)(cid:107)(cid:99)WT (cid:107) + (cid:107)g0(θ)(cid:107)2(cid:107)(cid:99)WT − W (cid:107).

(cid:12)g0(θ)(cid:62)((cid:99)WT − W )g0(θ)(cid:12)
(cid:12)

To prove supθ∈Θ
Θ compact, it is sufﬁcient to prove that (cid:107)(cid:98)L−L(cid:107)

P
−→ 0, we should now prove that supθ∈Θ(cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)
−C(cid:107)

−→ 0, and (cid:107)(cid:100)Kc(T )

(cid:12) (cid:98)QT (θ) − Q0(θ)(cid:12)
(cid:12)
(cid:12)

−→ 0, (cid:107) (cid:98)C

(T )

P

P

−Kc(cid:107)

−→ 0. By
P

−→ 0.

P

16

Proof that (cid:107) (cid:98)L − L(cid:107)

P

−→ 0

The estimator of L is unbiased so let’s focus on the variance of (cid:98)L.

E[((cid:98)Λi − Λi)2] = E

(cid:19)2(cid:35)

(dN i

t − Λidt)

E[(dN i

t − Λidt)(dN i

t(cid:48) − Λidt(cid:48))]

(cid:90) T +HT

(cid:34)(cid:18) 1
T

HT
(cid:90) T +HT

(cid:90) T +HT

HT

HT

(cid:90) T +HT

(cid:90) T +HT

HT

(cid:90) T +HT

HT

HT

Ciidt =

−→ 0

Cii

t(cid:48)−tdtdt(cid:48)

Cii
T

P

=

=

≤

1
T 2

1
T 2

1
T 2

By Markov inequality, we have just proved that (cid:107)(cid:98)L − L(cid:107)

−→ 0.

Proof that (cid:107) (cid:98)C

(T )

P

− C(cid:107)

−→ 0

First, let’s remind that E( (cid:98)C

) (cid:54)= C. Indeed,

E

(cid:16)

(cid:98)Cij,(T )(cid:17)

= E

(T )

(cid:18) 1
T
(cid:18) 1
T

(cid:90) T +HT

(cid:90) T +2HT

HT

(cid:90) T +HT

(cid:90) T +2HT −t

dN i
t

dN i
t

0

−t
(cid:16)

HT
(cid:90) T +HT

(cid:90) HT

HT
fsCij

−HT
s ds + (cid:15)ij,T,HT F T

= E

1
T
(cid:90)

=

=

Now,

dN j

t(cid:48)ft(cid:48)−t − (cid:98)Λi (cid:101)ΛjF T

(cid:19)

(cid:19)

dN j

t+sfs − ΛiΛjF T

+ (cid:15)ij,T,HT F T

fsE

dN i

t dN j

t+s − ΛiΛjds

+ (cid:15)ij,T,HT F T

(cid:17)

(cid:15)ij,T,HT = E

(cid:16)

ΛiΛj − (cid:98)Λi (cid:101)Λj(cid:17)
(cid:90) T +HT
1
T 2

0

HT

(cid:90) T +2HT

(cid:90) T +HT

(cid:90) T +2HT

= −

= −

1
T 2

1
T

HT
(cid:90) (cid:32)

= −

1 +

Cij
t−t(cid:48)dtdt(cid:48)
(cid:19)−(cid:33)+

Cij

t dt

0

(cid:18) HT − |t|
T

(cid:16)

E

dN i

t dN j

t(cid:48) − ΛiΛjdtdt(cid:48)(cid:17)

(T )

Since f satisﬁes F T = o(T ), we have E( (cid:98)C
E( (cid:98)C
Let’s now focus on the variance of (cid:98)Cij,(T ) : V( (cid:98)Cij,(T )) = E

) −→ C.

−→ 0.

)(cid:107)

P

(T )

It remains now to prove that (cid:107) (cid:98)C

(T )

−

(cid:16)

( (cid:98)Cij,(T ))2(cid:17)

− E( (cid:98)Cij,(T ))2.

17

Now,

(cid:16)

E

( (cid:98)Cij,(T ))2(cid:17)


1
T 2

1
T 2
(cid:90)

= E



(cid:32)

= E

=

1
T 2

And,

(cid:88)

(fτ (cid:48)−τ − F T /(T + 2HT ))(fη(cid:48)−η − F T /(T + 2HT ))

(τ,η,τ (cid:48),η(cid:48))∈(Zi,T,1)2×(Zj,T,2)2
(cid:90)

(cid:90)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)

(cid:90)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)∈[0,T +2HT ]

dN i

t dN j

t(cid:48)dN i

sdN j

s(cid:48)(ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))

(cid:16)

E

dN i

t dN j

t(cid:48)dN i

sdN j
s(cid:48)

(cid:17)

· (ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))





(cid:33)

E( (cid:98)Cij,(T ))2
1
T 2

=

(cid:90)

(cid:90)

(cid:16)

E

dN i

t dN j
t(cid:48)

(cid:17)

(cid:16)

E

dN i

sdN j
s(cid:48)

(cid:17)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)∈[0,T +2HT ]

· (ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))

Then, the variance involves the integration towards the difference of moments µr,s,t,u − µr,sµt,u. Let’s
write it as a sum of cumulants, since cumulants density are integrable.

µr,s,t,u − µr,sµt,u = κr,s,t,u + κr,s,tκu[4] + κr,sκt,u[3] + κr,sκtκu[6] + κrκsκtκu − (κr,s + κrκs)(κt,u + κtκu)

= κr,s,t,u
+ κr,s,tκu + κu,r,sκt + κt,u,rκs + κs,t,uκr
+ κr,tκs,u + κr,uκs,t
+ κr,tκsκu + κr,uκsκt + κs,tκrκu + κs,tκrκu

In the rest of the proof, we denote at = 1t∈[HT ,T +HT ], bt = 1t∈[0,T +2HT ], ct = 1t∈[−HT ,HT ], gt =
ft − 1
Before starting the integration of each term, let’s remark that:

T +2HT

F T

u,v (skewness density) and M ijkl

u,v,w (fourth cumulant density) are positive
· with positive coefﬁcients. The integrals of the singular parts are

1. Ψt = (cid:80)

n≥1 Φ((cid:63)n)

t ≥ 0 since Φt ≥ 0.

2. The regular parts of Cij

u , Kijk

3.

as polynoms of integrals of ψab
positive as well.
(a) (cid:82) atbt(cid:48)ft(cid:48)−tdtdt(cid:48) = T F T
(b) (cid:82) atbt(cid:48)gt(cid:48)−tdtdt(cid:48) = 0
(c) (cid:82) atbt(cid:48)|gt(cid:48)−t|dtdt(cid:48) ≤ 2T F T
4. ∀t ∈ R, at(b (cid:63) (cid:101)g)t = 0, where (cid:101)gs = g−s.

18

Fourth cumulant We want here to compute (cid:82) κi,j,i,j
We remark that |gt(cid:48)−tgs(cid:48)−s| ≤ (||f ||∞(1 + 2HT /T ))2 ≤ 4||f ||2
∞.
(cid:19)2 (cid:90)

(cid:90)

t,t(cid:48),s,s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)(cid:12)
κi,j,i,j

(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)

1
T 2

t,t(cid:48),s,s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48).

dtat

dt(cid:48)bt(cid:48)

dsas

ds(cid:48)bs(cid:48)M ijij

t(cid:48)−t,s−t,s(cid:48)−t

(cid:90)

(cid:90)

(cid:90)

(cid:90)

dtat

dt(cid:48)bt(cid:48)

dsas

dwM ijij

t(cid:48)−t,s−t,w

dtat

M ijij

u,v,wdudvdw

(cid:90)

(cid:90)

(cid:90)

(cid:18) 2||f ||∞
T
(cid:18) 2||f ||∞
T
(cid:18) 2||f ||∞
T
4||f ||2
∞
T

≤

≤

≤

(cid:19)2 (cid:90)

(cid:19)2 (cid:90)

M ijij −→
T →∞

0

Third × First We have four terms, but only two different forms since the roles of (s, s(cid:48)) and (t, t(cid:48))
are symmetric.
First form

Second form

(cid:90)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
κi,j,j
t,t(cid:48),s(cid:48)ΛiGtdt
(cid:12)
(cid:12) =

(cid:90)

κi,j,i
t,t(cid:48),sΛjGtdt =

κi,j,i
t,t(cid:48),satbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)

(cid:90)

(cid:90)

Λj
T 2
Λj
T 2

=

= 0

κi,j,i
t,t(cid:48),satbt(cid:48)as(b (cid:63) (cid:101)g)sgt(cid:48)−tdtdt(cid:48)ds
since as(b (cid:63) (cid:101)g)s = 0

t,t(cid:48),s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)(cid:12)
κi,j,j
t,t(cid:48),s(cid:48)atbt(cid:48)gt(cid:48)−tbs(cid:48)(a (cid:63) g)s(cid:48)dtdt(cid:48)ds(cid:48)(cid:12)
κi,j,j

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:90)

(cid:90)

(cid:12)
(cid:12)
(cid:12)

=

Λi
T 2
Λi
(cid:12)
(cid:12)
(cid:12)
T 2
Λi
≤
T 2 2||f ||∞
≤ 4||f ||∞KijjΛi F T
T

(cid:90)

−→
T →∞

0

ds(cid:48)bs(cid:48)(a (cid:63) |g|)s(cid:48)

dtat

dt(cid:48)bt(cid:48)Kijj

t(cid:48)−s(cid:48),t−s(cid:48)

(cid:90)

(cid:90)

Second × Second
First form

(cid:90)

(cid:12)
(cid:12)
(cid:12)

t,sκj,j
κi,i

t(cid:48),s(cid:48)Gtdt

Second form

Second × First × First
First form

(cid:90)

t(cid:48)−s(cid:48)atbt(cid:48)|gt(cid:48)−t|asbs(cid:48)dtdt(cid:48)dsds(cid:48)

(cid:12)
(cid:12)
(cid:12) ≤

(cid:90)

t−sCjj
Cii
(cid:90)

2||f ||∞
T 2
2||f ||∞
T 2 CiiCjj
≤
≤ 4||f ||∞CiiCjj F T
T

atbt(cid:48)|gt(cid:48)−t|dtdt(cid:48)

−→
T →∞

0

(cid:90)

(cid:12)
(cid:12)
(cid:12)

t,s(cid:48)κi,j
κi,j

(cid:12)
(cid:12)
t(cid:48),sGtdt

(cid:12) ≤ 4||f ||∞(Cij)2 F T

−→
T →∞

0

T

κi,j
t,t(cid:48)ΛiΛjGtdt =

κi,j
t,t(cid:48)atbt(cid:48)gt(cid:48)−tdtdt(cid:48)

asbs(cid:48)gs(cid:48)−sdsds(cid:48) = 0

(cid:90)

(cid:90)

ΛiΛj
T 2

19

Second form

(cid:90)

κi,i
t,sΛjΛjGtdt =

(cid:19)2 (cid:90)

(cid:18) Λj
T

κi,i
t,satbt(cid:48)gt(cid:48)−tas(b (cid:63) (cid:101)g)sdtdt(cid:48)ds = 0

We have just proved that V( (cid:98)C
− C(cid:107)
and ﬁnally that (cid:107) (cid:98)C

(T )

(T )

P

)
P

−→ 0.

−→ 0. By Markov inequality, it ensures us that (cid:107) (cid:98)C

(T )

−E( (cid:98)C

(T )

P

)(cid:107)

−→ 0,

− Kc(cid:107)

Proof that (cid:107)(cid:100)Kc(T )
The scheme of the proof is similar to the previous one. The upper bounds of the integrals involve the same
kind of terms, plus the new term (F T )2/T that goes to zero thanks to the assumption 5 of the theorem.

−→ 0

P

5 Conclusion

In this paper, we introduce a simple nonparametric method (the NPHC algorithm) that leads to a fast and
robust estimation of the matrix G of the kernel integrals of a Multivariate Hawkes process that encodes
Granger causality between nodes. This method relies on the matching of the integrated order 2 and
order 3 empirical cumulants, which represent the simplest set of global observables containing sufﬁcient
information to recover the matrix G. Since this matrix fully accounts for the self- and cross- inﬂuences of
the process nodes (that can represent agents or users in applications), our approach can naturally be used
to quantify the degree of endogeneity of a system and to uncover the causality structure of a network.

By performing numerical experiments involving very different kernel shapes, we show that the
baselines, involving either parametric or non-parametric approaches are very sensible to model misspeci-
ﬁcation, do not lead to accurate estimation, and are numerically expensive, while NPHC provides fast,
robust and reliable results. This is conﬁrmed on the MemeTracker database, where we show that NPHC
outperforms classical approaches based on EM algorithms or the Wiener-Hopf equations. Finally, the
NPHC algorithm provided very satisfying results on ﬁnancial data, that are consistent with well-known
stylized facts in ﬁnance.

Acknowledgements

This work beneﬁted from the support of the chair “Changing markets”, CMAP École Polytechnique and
École Polytechnique fund raising - Data Science Initiative.

The authors want to thank Marcello Rambaldi for fruitful discussions on order book data’s experi-

ments.

20

References

M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.
arXiv preprint arXiv:1603.04467, 2016.

Y. Aït-Sahalia, J. Cacho-Diaz, and R. JA Laeven. Modeling ﬁnancial contagion using mutually exciting

jump processes. Technical report, National Bureau of Economic Research, 2010.

E. Bacry and J.-F. Muzy. First- and second-order statistics characterization of hawkes processes and

non-parametric estimation. IEEE Transactions on Information Theory, 62(4):2184–2202, 2016.

E. Bacry, I. Mastromatteo, and J.-F. Muzy. Hawkes processes in ﬁnance. Market Microstructure and

Liquidity, 1(01):1550005, 2015.

E. Bacry, T. Jaisson, and J.-F. Muzy. Estimation of slowly decreasing hawkes kernels: application to

high-frequency order book dynamics. Quantitative Finance, pages 1–23, 2016.

A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of multilayer

networks. In AISTATS, 2015.

R. Crane and D. Sornette. Robust dynamic classes revealed by measuring the response function of a

social system. Proceedings of the National Academy of Sciences, 105(41), 2008.

J. Da Fonseca and R. Zaatour. Hawkes process: Fast calibration, application to trade clustering, and

diffusive limit. Journal of Futures Markets, 34(6):548–579, 2014.

D. J. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes Volume I: Elementary

Theory and Methods. Springer Science & Business Media, 2003.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.

M. Eichler, R. Dahlhaus, and J. Dueck. Graphical modeling for multivariate hawkes processes with non-
parametric link functions. Journal of Time Series Analysis, pages n/a–n/a, 2016. ISSN 1467-9892. doi:
10.1111/jtsa.12213. URL http://dx.doi.org/10.1111/jtsa.12213. 10.1111/jtsa.12213.

M. Farajtabar, Y. Wang, M. Rodriguez, S. Li, H. Zha, and L. Song. Coevolve: A joint point process model
for information diffusion and network co-evolution. In Advances in Neural Information Processing
Systems, pages 1945–1953, 2015.

M. Gomez-Rodriguez, J. Leskovec, and B. Schölkopf. Modeling information propagation with survival

theory. Proceedings of the International Conference on Machine Learning, 2013.

C. W. J. Granger. Investigating causal relations by econometric models and cross-spectral methods.
Econometrica, 37(3):424–438, 1969. ISSN 00129682, 14680262. URL http://www.jstor.
org/stable/1912791.

A. R. Hall. Generalized Method of Moments. Oxford university press, 2005.

L. P. Hansen. Large sample properties of generalized method of moments estimators. Econometrica:

Journal of the Econometric Society, pages 1029–1054, 1982.

N. R. Hansen, P. Reynaud-Bouret, and V. Rivoirard. Lasso and probabilistic inequalities for multivariate

point processes. Bernoulli, 21(1):83–143, 2015.

21

S. J. Hardiman and J.-P. Bouchaud. Branching-ratio approximation for the self-exciting Hawkes process.

Phys. Rev. E, 90(6):062807, December 2014. doi: 10.1103/PhysRevE.90.062807.

A. G. Hawkes. Point spectra of some mutually exciting point processes. Journal of the Royal Statistical
Society. Series B (Methodological), 33(3):438–443, 1971. ISSN 00359246. URL http://www.
jstor.org/stable/2984686.

A. G. Hawkes and D. Oakes. A cluster process representation of a self-exciting process. Journal of

Applied Probability, pages 493–503, 1974.

S. Jovanovi´c, J. Hertz, and S. Rotter. Cumulants of Hawkes point processes. Phys. Rev. E, 91(4):042802,

April 2015. doi: 10.1103/PhysRevE.91.042802.

R. Lemonnier and N. Vayatis. Nonparametric markovian learning of triggering kernels for mutually
exciting and mutually inhibiting multivariate hawkes processes. In Machine Learning and Knowledge
Discovery in Databases, pages 161–176. Springer, 2014.

E. Lewis and G. Mohler. A nonparametric em algorithm for multiscale hawkes processes. Journal of

Nonparametric Statistics, 2011.

G. O. Mohler, M. B. Short, P. J. Brantingham, F. P. Schoenberg, and G. E. Tita. Self-exciting point

process modeling of crime. Journal of the American Statistical Association, 2011.

W. K Newey and D. McFadden. Large sample estimation and hypothesis testing. Handbook of economet-

rics, 4:2111–2245, 1994.

27(1):23–31, 1981.

Y. Ogata. On lewis’ simulation method for point processes. Information Theory, IEEE Transactions on,

Y. Ogata. Space-time point-process models for earthquake occurrences. Annals of the Institute of

Statistical Mathematics, 50(2):379–402, 1998.

A. Podosinnikova, F. Bach, and S. Lacoste-Julien. Rethinking lda: moment matching for discrete ica. In

Advances in Neural Information Processing Systems, pages 514–522, 2015.

P. Reynaud-Bouret and S. Schbath. Adaptive estimation for hawkes processes; application to genome

analysis. The Annals of Statistics, 38(5):2781–2822, 2010.

V.S. Subrahmanian, A. Azaria, S. Durst, V. Kagan, A. Galstyan, K. Lerman, L. Zhu, E. Ferrara, A. Flam-

mini, and F. Menczer. The darpa twitter bot challenge. Computer, 49(6):38–46, 2016.

H. Xu, M. Farajtabar, and H. Zha. Learning granger causality for hawkes processes. In Proceedings of

The 33rd International Conference on Machine Learning, pages 1717–1726, 2016.

S.-H. Yang and H. Zha. Mixture of mutually exciting processes for viral diffusion. In Proceedings of the

International Conference on Machine Learning, 2013.

K. Zhou, H. Zha, and L. Song. Learning triggering kernels for multi-dimensional hawkes processes. In

Proceedings of the International Conference on Machine Learning, pages 1301–1309, 2013a.

K. Zhou, H. Zha, and L. Song. Learning social infectivity in sparse low-rank networks using multi-

dimensional hawkes processes. AISTATS, 2013b.

22

7
1
0
2
 
y
a
M
 
0
3
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
3
3
3
6
0
.
7
0
6
1
:
v
i
X
r
a

Uncovering Causality from Multivariate Hawkes Integrated
Cumulants

Massil Achab∗1, Emmanuel Bacry1, Stéphane Gaiffas1, Iacopo Mastromatteo2, and
Jean-François Muzy1,3

1Centre de Mathématiques Appliquées, CNRS, Ecole Polytechnique, UMR 7641, 91128 Palaiseau, France
2Capital Fund Management, 23 rue de l’Université, 75007 Paris, France
3Laboratoire Sciences Pour l’Environnement, Université de Corse, 7 Avenue Jean Nicoli, 20250 Corte, France

May 31, 2017

Abstract

We design a new nonparametric method that allows one to estimate the matrix of integrated
kernels of a multivariate Hawkes process. This matrix not only encodes the mutual inﬂuences of each
node of the process, but also disentangles the causality relationships between them. Our approach is
the ﬁrst that leads to an estimation of this matrix without any parametric modeling and estimation of
the kernels themselves. As a consequence, it can give an estimation of causality relationships between
nodes (or users), based on their activity timestamps (on a social network for instance), without
knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment
matching method that ﬁts the second-order and the third-order integrated cumulants of the process. A
theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we
show, on numerical experiments, that our approach is indeed very robust with respect to the shape of
the kernels and gives appealing results on the MemeTracker database and on ﬁnancial order book data.

Keywords. Hawkes Process, Causality Inference, Cumulants, Generalized Method of Moments

1 Introduction

In many applications, one needs to deal with data containing a very large number of irregular timestamped
events that are recorded in continuous time. These events can reﬂect, for instance, the activity of users
on a social network, see Subrahmanian et al. (2016), the high-frequency variations of signals in ﬁnance,
see Bacry et al. (2015), the earthquakes and aftershocks in geophysics, see Ogata (1998), the crime
activity, see Mohler et al. (2011) or the position of genes in genomics, see Reynaud-Bouret and Schbath
(2010). The succession of the precise timestamps carries a great deal of information about the dynamics
of the underlying systems. In this context, multidimensional counting processes based models play a
paramount role. Within this framework, an important task is to recover the mutual inﬂuence of the nodes
(i.e., the different components of the counting process), by leveraging on their timestamp patterns, see,
for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou
et al. (2013a); Gomez-Rodriguez et al. (2013); Farajtabar et al. (2015); Xu et al. (2016).

Consider a set of nodes I = {1, . . . , d}. For each i ∈ I, we observe a set Zi of events, where each
τ ∈ Zi labels the occurrence time of an event related to the activity of i. The events of all nodes can

∗massil.achab@m4x.org

1

be represented as a vector of counting processes N t = [N 1
events of node i until time t ∈ R+, namely N i
λt = [λ1

t = (cid:80)
t ](cid:62) associated with the multivariate counting process N t is deﬁned as
P(N i

t ](cid:62), where N i

t counts the number of
τ ∈Zi 1{τ ≤t}. The vector of stochastic intensities

t · · · N d

t · · · λd

t = 1|Ft)

λi
t = lim
dt→0

t+dt − N i
dt

for i ∈ I, where the ﬁltration Ft encodes the information available up to time t. The coordinate λi
t gives
the expected instantaneous rate of event occurrence at time t for node i. The vector λt characterizes
the distribution of N t, see Daley and Vere-Jones (2003), and patterns in the events time-series can be
captured by structuring these intensities.

The Hawkes process introduced in Hawkes (1971) corresponds to an autoregressive structure of
the intensities in order to capture self-excitation and cross-excitation of nodes, which is a phenomenon
typically observed, for instance, in social networks, see for instance Crane and Sornette (2008). Namely,
N t is called a Hawkes point process if the stochastic intensities can be written as

t = µi +
λi

φij(t − t(cid:48))dN j
t(cid:48),

d
(cid:88)

(cid:90) t

j=1

0

where µi ∈ R+ is an exogenous intensity and φij are positive, integrable and causal (with support in R+)
functions called kernels encoding the impact of an action by node j on the activity of node i. Note that
when all kernels are zero, the process is a simple homogeneous multivariate Poisson process.

Most of the litterature uses a parametric approach for estimating the kernels. With no doubt, the
most popular parametrization form is the exponential kernel φij(t) = αijβije−βij t because it deﬁnitely
simpliﬁes the inference algorithm (e.g., the complexity needed for computing the likelihood is much
smaller). When d is large, in order to reduce the number of parameters, some authors choose to arbitrarily
share the kernel shapes across the different nodes. Thus, for instance, in Yang and Zha (2013); Zhou
et al. (2013b); Farajtabar et al. (2015), they choose φij(t) = αijh(t) with αij ∈ R+ quantiﬁes the
intensity of the inﬂuence of j on i and h(t) a (normalized) function that characterizes the time-proﬁle
of this inﬂuence and that is shared by all couples of nodes (i, j) (most often, it is chosen to be either
exponential h(t) = βe−βt or power law h(t) = βt−(β+1)). Both approaches are, most of the time,
highly non-realistic. On the one hand there is a priori no reason for assuming that the time-proﬁle of the
inﬂuence of a node j on a node i does not depend on the pair (i, j). On the other hand, assuming an
exponential shape or a power law shape for a kernel arbitrarily imposes an event impact that is always
instantly maximal and that can only decrease with time, while in practice, there may exist a latency
between an event and its maximal impact.

In order to have more ﬂexibility on the shape of the kernels, nonparametric estimation can be
considered. Expectation-Maximization algorithms can be found in Lewis and Mohler (2011) (for d = 1)
or in Zhou et al. (2013a) (d > 1). An alternative method is proposed in Bacry and Muzy (2016) where
the nonparametric estimation is formulated as a numerical solving of a Wiener-Hopf equation. Another
nonparametric strategy considers a decomposition of kernels on a dictionary of function h1, . . . , hK,
namely φij(t) = (cid:80)K
k are estimated, see Hansen et al. (2015);
Lemonnier and Vayatis (2014) and Xu et al. (2016), where group-lasso is used to induce a sparsity pattern
on the coefﬁcients aij

k hk(t), where the coefﬁcients aij

k that is shared across k = 1, . . . , K.

k=1 aij

Such methods are heavy when d is large, since they rely on likelihood maximization or least squares
minimization within an over-parametrized space in order to gain ﬂexibility on the shape of the kernels.
This is problematic, since the original motivation for the use of Hawkes processes is to estimate the
inﬂuence and causality of nodes, the knowledge of the full parametrization of the model being of little

2

interest for causality purpose.

Our paper solves this problem with a different and more direct approach. Instead of trying to estimate
the kernels φij, we focus on the direct estimation of their integrals. Namely, we want to estimate the
matrix G = [gij] where

gij =

φij(u) du ≥ 0 for 1 ≤ i, j ≤ d.

(1)

(cid:90) +∞

0

As it can be seen from the cluster representation of Hawkes processes (Hawkes and Oakes (1974)), this
integral represents the mean total number of events of type i directly triggered by an event of type j, and
then encodes a notion of causality. Actually, as detailed below (see Section 2.1), such integral can be
related to the Granger causality (Granger (1969)).

The main idea of the method we developed in this paper is to estimate the matrix G directly using a
matching cumulants (or moments) method. Apart from the mean, we shall use second and third-order
cumulants which correspond respectively to centered second and third-order moments. We ﬁrst compute
an estimation (cid:99)M of these centered moments M (G) (they are uniquely deﬁned by G). Then, we look for
a matrix (cid:98)G that minimizes the L2 error (cid:107)M ( (cid:98)G) − (cid:99)M (cid:107)2. Thus the integral matrix (cid:98)G is directly estimated
without making hardly any assumptions on the shape the involved kernels. As it will be shown, this
approach turns out to be particularly robust to the kernel shapes, which is not the case of all previous
Hawkes-based approaches that aim causality recovery. We call this method NPHC (Non Parametric
Hawkes Cumulant), since our approach is of nonparametric nature. We provide a theoretical analysis
that proves the consistency of the NPHC estimator. Our proof is based on ideas from the theory of
Generalized Method of Moments (GMM) but requires an original technical trick since our setting strongly
departs from the standard parametric statistics with i.i.d observations. Note that moment and cumulant
matching techniques proved particularly powerful for latent topic models, in particular Latent Dirichlet
Allocation, see Podosinnikova et al. (2015). A small set of previous works, namely Da Fonseca and
Zaatour (2014); Aït-Sahalia et al. (2010), already used method of moments with Hawkes processes, but
only in a parametric setting. Our work is the ﬁrst to consider such an approach for a nonparametric
counting processes framework.

The paper is organized as follows: in Section 2, we provide the background on the integrated kernels
and the integrated cumulants of the Hawkes process. We then introduce the method, investigate its
complexity and explain the consistency result we prove. In Section 3, we estimate the matrix of Hawkes
kernels’ integrals for various simulated datasets and for real datasets, namely the MemeTracker database
and ﬁnancial order book data. We then provide in Section 4 the technical details skipped in the previous
parts and the proof of our consistency result. Section 5 contains concluding remarks.

2 NPHC: The Non Parametric Hawkes Cumulant method

In this Section, we provide the background on integrals of Hawkes kernels and integrals of Hawkes
cumulants. We then explain how the NPHC method enables estimating G.

2.1 Branching structure and Granger causality

From the deﬁnition of Hawkes process as a Poisson cluster process, see Jovanovi´c et al. (2015) or Hawkes
and Oakes (1974), gij can be simply interpreted as the average total number of events of node i whose
direct ancestor is a given event of node j (by direct we mean that interactions mediated by any other
intermediate event are not counted). In that respect, G not only describes the mutual inﬂuences between

3

nodes, but it also quantiﬁes their direct causal relationships. Namely, introducing the counting function
N i←j
that counts the number of events of i whose direct ancestor is an event of j, we know from Bacry
t
et al. (2015) that

E[dN i←j
(2)
t
where we introduced Λi as the intensity expectation, namely satisfying E[dN i
t ] = Λidt. Note that Λi does
not depend on time by stationarity of N t, which is known to hold under the stability condition (cid:107)G(cid:107) < 1,
where (cid:107)G(cid:107) stands for the spectral norm of G. In particular, this condition implies the non-singularity of
Id − G.

t ] = gijΛjdt,

] = gijE[dN j

Since the question of a real causality is too complex in general, most econometricians agreed on
the simpler deﬁnition of Granger causality Granger (1969). Its mathematical formulation is a statistical
hypothesis test: X causes Y in the sense of Granger causality if forecasting future values of Y is more
successful while taking X past values into account. In Eichler et al. (2016), it is shown that for N t
a multivariate Hawkes process, N j
t w.r.t N t if and only if φij(u) = 0 for
u ∈ R+. Since the kernels take positive values, the latter condition is equivalent to (cid:82) ∞
0 φij(u)du = 0. In
the following, we’ll refer to learning the kernels’ integrals as uncovering causality since each integral
encodes the notion of Granger causality, and is also linked to the number of events directly caused from a
node to another node, as described above at Eq. (2).

t does not Granger-cause N i

2.2

Integrated cumulants of the Hawkes process

A general formula for the integral of the cumulants of a multivariate Hawkes process is provided
in Jovanovi´c et al. (2015). As explained below, for the purpose of our method, we only need to consider
cumulants up to the third order. Given 1 ≤ i, j, k ≤ d, the ﬁrst three integrated cumulants of the Hawkes
process can be deﬁned as follows thanks to stationarity:

Λidt = E(dN i
t )
(cid:90)
(cid:16)

Cijdt =

E(dN i

t dN j

t+τ ) − E(dN i

t )E(dN j

(cid:17)
t+τ )

τ ∈R

(cid:90) (cid:90)

(cid:16)

τ,τ (cid:48)∈R2
− E(dN i

Kijkdt =

E(dN i

t dN j

t+τ dN k

t+τ (cid:48)) + 2E(dN i

t )E(dN j

t+τ )E(dN k

t+τ (cid:48))

t dN j

t+τ )E(dN k

t+τ (cid:48)) − E(dN i

t dN k

t+τ (cid:48))E(dN j

t+τ ) − E(dN j

t+τ dN k

(cid:17)
t+τ (cid:48))E(dN i
t )

,

where Eq. (3) is the mean intensity of the Hawkes process, the second-order cumulant (4) refers to the
integrated covariance density matrix and the third-order cumulant (5) measures the skewness of N t. Using
the martingale representation from Bacry and Muzy (2016) or the Poisson cluster process representation
from Jovanovi´c et al. (2015), one can obtain an explicit relationship between these integrated cumulants
and the matrix G. If one sets

R = (Id − G)−1,

straightforward computations (see Section 4) lead to the following identities:

Λi =

Rimµm

Cij =

ΛmRimRjm

d
(cid:88)

m=1
d
(cid:88)

m=1
d
(cid:88)

m=1

4

Kijk =

(RimRjmCkm + RimCjmRkm + CimRjmRkm − 2ΛmRimRjmRkm).

(3)

(4)

(5)

(6)

(7)

(8)

(9)

Equations (8) and (9) are proved in Section 4. Our strategy is to use a convenient subset of Eqs. (3), (4)
and (5) to deﬁne M , while we use Eqs. (7), (8) and (9) in order to construct the operator that maps a
candidate matrix R to the corresponding cumulants M (R). By looking for (cid:98)R that minimizes R (cid:55)→
(cid:107)M (R) − (cid:99)M (cid:107)2, we obtain, as illustrated below, good recovery of the ground truth matrix G using
Equation (6).

The simplest case d = 1 has been considered in Hardiman and Bouchaud (2014), where it is shown
that one can choose M = {C11} in order to compute the kernel integral. Eq. (8) then reduces to a simple
second-order equation that has a unique solution in R (and consequently a unique G) that accounts for
the stability condition ((cid:107)G(cid:107) < 1).

Unfortunately, for d > 1, the choice M = {Cij}1≤i≤j≤d is not sufﬁcient to uniquely determine the
kernels integrals. In fact, the integrated covariance matrix provides d(d + 1)/2 independent coefﬁcients,
while d2 parameters are needed. It is straightforward to show that the remaining d(d − 1)/2 conditions
can be encoded in an orthogonal matrix O, reﬂecting the fact that Eq. (8) is invariant under the change
R → OR, so that the system is under-determined.

Our approach relies on using the third order cumulant tensor K = [Kijk] which contains (d3 + 3d2 +
2d)/6 > d2 independent coefﬁcients that are sufﬁcient to uniquely ﬁx the matrix G. This can be justiﬁed
intuitively as follows: while the integrated covariance only contains symmetric information, and is thus
unable to provide causal information, the skewness given by the third order cumulant in the estimation
procedure can break the symmetry between past and future so as to uniquely ﬁx G. Thus, our algorithm
consists of selecting d2 third-order cumulant components, namely M = {Kiij}1≤i,j≤d. In particular, we
deﬁne the estimator of R as (cid:98)R ∈ argminRL(R), where

L(R) = (1 − κ)(cid:107)Kc(R) − (cid:100)Kc(cid:107)2

2 + κ(cid:107)C(R) − (cid:98)C(cid:107)2
2,

(10)

where (cid:107) · (cid:107)2 stands for the Frobenius norm, Kc = {Kiij}1≤i,j≤d is the matrix obtained by the contraction
of the tensor K to d2 indices, C is the covariance matrix, while (cid:100)Kc and (cid:98)C are their respective estimators,
see Equations (12), (13) below. It is noteworthy that the above mean square error approach can be seen
as a peculiar Generalized Method of Moments (GMM), see Hall (2005). This framework allows us to
determine the optimal weighting matrix involved in the loss function. However, this approach is unusable
in practice, since the associated complexity is too high. Indeed, since we have d2 parameters, this matrix
has d4 coefﬁcients and GMM calls for computing its inverse leading to a O(d6) complexity. In this work,
we use the coefﬁcient κ to scale the two terms, as

see Section 4.4 for an explanation about the link between κ and the weighting matrix. Finally, the
estimator of G is straightforwardly obtained as

from the inversion of Eq. (6). Let us mention an important point: the matrix inversion in the previous
formula is not the bottleneck of the algorithm. Indeed, its has a complexity O(d3) that is cheap compared
to the computation of the cumulants when n = maxi |Zi| (cid:29) d, which is the typical scaling satisﬁed
in applications. Solving the considered problem on a larger scale, say d (cid:29) 103, is an open question,
even with state-of-the-art parametric and nonparametric approaches, see for instance Zhou et al. (2013a);
Xu et al. (2016); Zhou et al. (2013b); Bacry and Muzy (2016), where the number of components d in
experiments is always around 100 or smaller. Note that, actually, our approach leads to a much faster
algorithm than the considered state-of-the-art baselines, see Tables 1–4 from Section 3 below.

κ =

(cid:107)(cid:100)Kc(cid:107)2
2
2 + (cid:107) (cid:98)C(cid:107)2
2

,

(cid:107)(cid:100)Kc(cid:107)2

(cid:98)G = Id − (cid:98)R

−1

,

5

2.3 Estimation of the integrated cumulants

In this section we present explicit formulas to estimate the three moment-based quantities listed in the
previous section, namely, Λ, C and K. We ﬁrst assume there exists H > 0 such that the truncation
from (−∞, +∞) to [−H, H] of the domain of integration of the quantities appearing in Eqs. (4) and (5),
introduces only a small error. In practice, this amounts to neglecting border effects in the covariance
density and in the skewness density that is a good approximation if the support of the kernel φij(t) is
smaller than H and the spectral norm (cid:107)G(cid:107) satisﬁes (cid:107)G(cid:107) < 1.
In this case, given a realization of a stationary Hawkes process {N t : t ∈ [0, T ]}, as shown in Section 4,
we can write the estimators of the ﬁrst three cumulants (3), (4) and (5) as

(cid:98)Λi =

(cid:88)

1 =

N i
T
T

1
T

1
T

1
T

τ ∈Zi
(cid:88)

(cid:16)

τ ∈Zi
(cid:88)

(cid:16)

(cid:98)Cij =

(cid:98)Kijk =

τ ∈Zi
(cid:98)Λi
T

−

(cid:88)

(cid:88)

τ ∈Zj

τ (cid:48)∈Zk

N j

τ +H − N j

τ −H − 2H (cid:98)Λj(cid:17)

N j

τ +H − N j

τ −H − 2H (cid:98)Λj(cid:17)

·

(cid:16)

N k

τ +H − N k

τ −H − 2H (cid:98)Λk(cid:17)

(2H − |τ (cid:48) − τ |)+ + 4H 2 (cid:98)Λi (cid:98)Λj (cid:98)Λk.

(11)

(12)

(13)

Let us mention the following facts.

Bias. While the ﬁrst cumulant ˆΛi is an unbiased estimator of Λi, the other estimators (cid:98)Cij and (cid:98)Kijk
introduce a bias. However, as we will show, in practice this bias is small and hardly affects
numerical estimations (see Section 3). This is conﬁrmed by our theoretical analysis, which proves
that if H does not grow too fast compared to T , then these estimated cumulants are consistent
estimators of the theoretical cumulants (see Section 2.6).

Complexity. The computations of all the estimators of the ﬁrst, second and third-order cumulants
have complexity respectively O(nd), O(nd2) and O(nd3), where n = maxi |Zi|. However, our
algorithm requires a lot less than that: it computes only d2 third-order terms, of the form (cid:98)Kiij,
leaving us with only O(nd2) operations to perform.

Symmetry. While the values of Λi, Cij and Kijk are symmetric under permutation of the indices, their
estimators are generally not symmetric. We have thus chosen to symmetrize the estimators by
averaging their values over permutations of the indices. Worst case is for the estimator of Kc,
which involves only an extra factor of 2 in the complexity.

2.4 The NPHC algorithm

The objective to minimize in Equation (10) is non-convex. More precisely, the loss function is a
polynomial of R of degree 6. However, the expectations of cumulants Λ and C deﬁned in Eq. (4) and (5)
that appear in the deﬁnition of L(R) are unknown and should be replaced with (cid:98)Λ and (cid:98)C. We denote
(cid:101)L(R) the objective function, where the expectations of cumulants Λi and Cij have been replaced with
their estimators in the right-hand side of Eqs. (8) and (9):

(cid:101)L(R) = (1 − κ)(cid:107)R(cid:12)2 (cid:98)C

+ 2[R (cid:12) ( (cid:98)C − R (cid:98)L)]R(cid:62) − (cid:100)Kc(cid:107)2

2 + κ(cid:107)R (cid:98)LR(cid:62) − (cid:98)C(cid:107)2
2

(14)

(cid:62)

As explained in Choromanska et al. (2015), the loss function of a typical multilayer neural network
with simple nonlinearities can be expressed as a polynomial function of the weights in the network,

6

whose degree is the number of layers. Since the loss function of NPHC writes as a polynomial of
degree 6, we expect good results using optimization methods designed to train deep multilayer neural
networks. We used the AdaGrad from Duchi et al. (2011), a variant of the Stochastic Gradient Descent
with adaptive learning rates. AdaGrad scales the learning rates coordinate-wise using the online variance
of the previous gradients, in order to incorporate second-order information during training. The NPHC
method is summarized schematically in Algorithm 1.

Algorithm 1 Non Parametric Hawkes Cumulant method
Input: N t
Output: (cid:98)G
1: Estimate (cid:98)Λi, (cid:98)Cij, (cid:98)Kiij from Eqs. (11, 12, 13)
2: Design (cid:101)L(R) using the computed estimators.
3: Minimize numerically (cid:101)L(R) so as to obtain (cid:98)R
−1
4: Return (cid:98)G = Id − (cid:98)R

.

Our problem being non-convex, the choice of the starting point has a major effect on the convergence.
Here, the key is to notice that the matrices R that match Equation (8) writes C1/2OL−1/2, with
L = diag(Λ) and O an orthogonal matrix. Our starting point is then simply chosen by setting O = Id in
the previous formula, leading to nice convergence results. Even though our main concern is to retrieve the
matrix G, let us notice we can also obtain an estimation of the baseline intensities’ from Eq. (3), which
leads to (cid:98)µ = (cid:98)R
(cid:98)Λ. An efﬁcient implementation of this algorithm with TensorFlow, see Abadi et al.
(2016), is available on GitHub: https://github.com/achab/nphc.

−1

2.5 Complexity of the algorithm

Compared with existing state-of-the-art methods to estimate the kernel functions, e.g., the ordinary
differential equations-based (ODE) algorithm in Zhou et al. (2013a), the Granger Causality-based
algorithm in Xu et al. (2016), the ADM4 algorithm in Zhou et al. (2013b), and the Wiener-Hopf-based
algorithm in Bacry and Muzy (2016), our method has a very competitive complexity. This can be
understood by the fact that those methods estimate the kernel functions, while in NPHC we only estimate
their integrals. The ODE-based algorithm is an EM algorithm that parametrizes the kernel function
with M basis functions, each being discretized to L points. The basis functions are updated after
solving M Euler-Lagrange equations. If n denotes the maximum number of events per component (i.e.
n = max1≤i≤d |Zi|) then the complexity of one iteration of the algorithm is O(M n3d2 + M L(nd + n2)).
The Granger Causality-based algorithm is similar to the previous one, without the update of the basis
functions, that are Gaussian kernels. The complexity per iteration is O(M n3d2). The algorithm ADM4
is similar to the two algorithms above, as EM algorithm as well, with only one exponential kernel as
basis function. The complexity per iteration is then O(n3d2). The Wiener-Hopf-based algorithm is not
iterative, on the contrary to the previous ones. It ﬁrst computes the empirical conditional laws on many
points, and then invert the Wiener-Hopf system, leading to a O(nd2L + d4L3) computation. Similarly,
our method ﬁrst computes the integrated cumulants, then minimize the objective function with Niter
iterations, and invert the resulting matrix (cid:98)R to obtain (cid:98)G. In the end, the complexity of the NPHC method
is O(nd2 + Niterd3). According to this analysis, summarized in Table 1 below, one can see that in the
regime n (cid:29) d, the NPHC method outperforms all the other ones.

2.6 Theoretical guarantee: consistency

The NPHC method can be phrased using the framework of the Generalized Method of Moments (GMM).
GMM is a generic method for estimating parameters in statistical models. In order to apply GMM,

7

Table 1: Complexity of state-of-the-art methods. NPHC’s complexity is very low , especially in the
regime n (cid:29) d.

Method

ODE Zhou et al. (2013a)
GC Xu et al. (2016)
ADM4 Zhou et al. (2013b)
WH Bacry and Muzy (2016) O(nd2L + d4L3)
O(nd2 + Niterd3)
NPHC

Total complexity
O(NiterM (n3d2 + L(nd + n2)))
O(NiterM n3d2)
O(Nitern3d2)

we have to ﬁnd a vector-valued function g(X, θ) of the data, where X is distributed with respect to a
distribution Pθ0, which satisﬁes the moment condition: E[g(X, θ)] = 0 if and only if θ = θ0, where θ0 is
the “ground truth” value of the parameter. Based on i.i.d. observed copies x1, . . . , xn of X, the GMM
method minimizes the norm of the empirical mean over n samples, (cid:107) 1
i=1 g(xi, θ)(cid:107), as a function of
n
θ, to obtain an estimate of θ0.

(cid:80)n

In the theoretical analysis of NPHC, we use ideas from the consistency proof of the GMM, but the
proof actually relies on very different arguments. Indeed, the integrated cumulants estimators used in
NPHC are not unbiased, as the theory of GMM requires, but asymptotically unbiased. Moreover, the
setting considered here, where data consists of a single realization {N t} of a Hawkes process strongly
departs from the standard i.i.d setting. Our approach is therefore based on the GMM idea but the proof is
actually not using the theory of GMM.

In the following, we use the subscript T to refer to quantities that only depend on the process (Nt) in
the interval [0, T ] (e.g., the truncation term HT , the estimated integrated covariance (cid:98)CT or the estimated
kernel norm matrix (cid:98)GT ). In the next equation, (cid:12) stands for the Hadamard product and (cid:12)2 stands for
the entrywise square of a matrix. We denote G0 = Id − R−1
the true value of G, and the R2d×d valued
0
vector functions

g0(R) =

(cid:98)gT (R) =

(cid:20)

C − RLR(cid:62)
Kc − R(cid:12)2C(cid:62) − 2[R (cid:12) (C − RL)]R(cid:62)
(cid:34)

(cid:21)

(cid:98)CT − R (cid:98)LT R(cid:62)
(cid:62)
T − 2[R (cid:12) ( (cid:98)CT − R (cid:98)LT )]R(cid:62) .

(cid:35)

(cid:100)Kc

T − R(cid:12)2 (cid:98)C

Using these notations, (cid:101)LT (R) can be seen as the weighted squared Frobenius norm of (cid:98)gT (R). Moreover,
P
when T → +∞, one has (cid:98)gT (R)
→
stands for convergence in probability.

P
→ g0(R) under the conditions of the following theorem, where

Theorem 2.1 (Consistency of NPHC). Suppose that (Nt) is observed on R+ and assume that

1. g0(R) = 0 if and only if R = R0;

2. R ∈ Θ, where Θ is a compact set;

4. HT → ∞ and H 2

T /T → 0.

Then

3. the spectral radius of the kernel norm matrix satisﬁes (cid:107)G0(cid:107) < 1;

(cid:18)

(cid:98)GT = Id −

arg min
R∈Θ

(cid:101)LT (R)

(cid:19)−1

P
→ G0.

8

The proof of the Theorem is given in Section 4.5 below. Assumption 3 is mandatory for stability of
the Hawkes process, and Assumptions 3 and 4 are sufﬁcient to prove that the estimators of the integrated
cumulants deﬁned in Equations (11), (12) and (13) are asymptotically consistent. Assumption 2 is a
very mild standard technical assumption allowing to prove consistency for estimators based on moments.
Assumption 1 is a standard asymptotic moment condition, that allows to identity parameters from the
integrated cumulants.

3 Numerical Experiments

In this Section, we provide a comparison of NPHC with the state-of-the art, on simulated datasets with
different kernel shapes, the MemeTracker dataset (social networks) and the order book dynamics dataset
(ﬁnance).

Simulated datasets. We simulated several datasets with Ogata’s Thinning algorithm Ogata (1981)
using the open-source library tick1, each corresponding to a shape of kernel: rectangular, exponential
or power law kernel, see Figure 1 below.

φt
αβ

log φt
log αβγ

φt
αβ

slope ≈ −(1 + γ)

0 γ

γ + 1/β

t

− log β

log t

0

1/β

t

(a) Rectangular kernel
φt = αβ1[0,1/β](t − γ)

(b) Power law kernel on log-log scale
φt = αβγ(1 + βt)−(1+γ)

(c) Exponential kernel
φt = αβ exp(−βt)

Figure 1: The three different kernels used to simulate the datasets.

The integral of each kernel on its support equals α, 1/β can be regarded as a characteristic time-scale
and γ is the scaling exponent for the power law distribution and a delay parameter for the rectangular
one. We consider a non-symmetric block-matrix G to show that our method can effectively uncover
causality between the nodes, see Figure 3. The matrix G has constant entries α on the three blocks -
α = gij = 1/6 for dimension 10 and α = gij = 1/10 for dimension 100 -, and zero outside. The two
other parameters’ values are the same for dimensions 10 and 100. The parameter γ is set to 1/2 on the
three blocks as well, but we set three very different β0, β1 and β2 from one block to the other, with ratio
βi+1/βi = 10 and β0 = 0.1. The number of events is roughly equal to 105 on average over the nodes.
We ran the algorithm on three simulated datasets: a 10-dimensional process with rectangular kernels
named Rect10, a 10-dimensional process with power law kernels named PLaw10 and a 100-dimensional
process with exponential kernels named Exp100.

MemeTracker dataset. We use events of the most active sites from the MemeTracker dataset2. This
dataset contains the publication times of articles in many websites/blogs from August 2008 to April
2009, and hyperlinks between posts. We extract the top 100 media sites with the largest number of
documents, with about 7 million of events. We use the links to trace the ﬂow of information and establish
an estimated ground truth for the matrix G. Indeed, when an hyperlink j appears in a post in website i,
the link j can be regarded as a direct ancestor of the event. Then, Eq. (2) shows gij can be estimated by
N i←j
T

T = #{links j → i}/N j
T .

/N j

1https://github.com/X-DataInitiative/tick
2https://www.memetracker.org/data.html

9

Order book dynamics. We apply our method to ﬁnancial data, in order to understand the self and cross-
inﬂuencing dynamics of all event types in an order book. An order book is a list of buy and sell orders for
a speciﬁc ﬁnancial instrument, the list being updated in real-time throughout the day. This model has ﬁrst
been introduced in Bacry et al. (2016), and models the order book via the following 8-dimensional point
, C(a)
process: Nt = (P (a)
t ), where P (a) (resp. P (b)) counts the number
t
of upward (resp. downward) price moves, T (a) (resp. T (b)) counts the number of market orders at the
ask3 (resp. at the bid) that do not move the price, L(a) (resp. L(b)) counts the number of limit orders at
the ask4 (resp. at the bid) that do not move the price, and C(a) (resp. C(b)) counts the number of cancel
orders at the ask5 (resp. at the bid) that do not move the price. The ﬁnancial data has been provided by
QuantHouse EUROPE/ASIA, and consists of DAX future contracts between 01/01/2014 and 03/01/2014.

, T (a)
t

, P (b)
t

, L(a)
t

, T (b)
t

, L(b)
t

, C(b)

t

Baselines. We compare NPHC to state-of-the art baselines: the ODE-based algorithm (ODE) by Zhou
et al. (2013a), the Granger Causality-based algorithm (GC) by Xu et al. (2016), the ADM4 algorithm
(ADM4) by Zhou et al. (2013b), and the Wiener-Hopf-based algorithm (WH) by Bacry and Muzy (2016).

Metrics. We evaluate the performance of the proposed methods using the computing time, the Relative
Error

RelErr(A, B) =

{aij (cid:54)=0} + |bij|1
1

{aij =0}

1
d2

(cid:88)

i,j

|aij − bij|
|aij|

and the Mean Kendall Rank Correlation

MRankCorr(A, B) =

RankCorr([ai•], [bi•]),

1
d

d
(cid:88)

i=1

where RankCorr(x, y) = 2
d(d−1) (Nconcordant(x, y) − Ndiscordant(x, y)) with Nconcordant(x, y) the number
of pairs (i, j) satisfying xi > xj and yi > yj or xi < xj and yi < yj and Ndiscordant(x, y) the number of
pairs (i, j) for which the same condition is not satisﬁed.

Note that RankCorr score is a value between −1 and 1, representing rank matching, but can take

smaller values (in absolute value) if the entries of the vectors are not distinct.

Figure 2: On Exp100 dataset, estimated (cid:98)G with ADM4 (left), with NPHC (middle) and the ground-
truth matrix G (right). Both ADM4 and NPHC estimates recover the three blocks. However, ADM4
overestimates the integrals on two of the three blocks, while NPHC gives the same value on each blocks.

3i.e. buy orders that are executed and removed from the list
4i.e. buy orders added to the list
5i.e. the number of times a limit order at the ask is canceled: in our dataset, almost 95% of limit orders are canceled before

execution.

10

Figure 3: Estimated (cid:98)G via NPHC on DAX order book data.

Table 2: Metrics on Rect10: comparable rank correlation, strong improvement for relative error and
computing time.

Method

ODE GC

ADM4 WH

NPHC

RelErr
MRankCorr
Time (s)

0.007
0.33
846

0.15
0.02
768

0.10
0.21
709

0.005
0.34
933

0.001
0.34
20

Discussion. We perform the ADM4 estimation, with exponential kernel, by giving the exact value
β = β0 of one block. Let us stress that this helps a lot this baseline, in comparison to NPHC where
nothing is speciﬁed on the shape of the kernel functions. We used M = 10 basis functions for both ODE
and GC algorithms, and L = 50 quadrature points for WH. We did not run WH on the 100-dimensional
datasets, for computing time reasons, because its complexity scales with d4. We ran multi-processed
versions of the baseline methods on 56 cores, to decrease the computing time.

Our method consistently performs better than all baselines, on the three synthetic datasets, on
MemeTracker and on the ﬁnancial dataset, both in terms of Kendall rank correlation and estimation error.
Moreover, we observe that our algorithm is roughly 50 times faster than all the considered baselines.

On Rect10, PLaw10 and Exp100 our method gives very impressive results, despite the fact that it
does not uses any prior shape on the kernel functions, while for instance the ADM4 baseline do. On
Figure 3, we observe that the matrix (cid:98)G estimated with ADM4 recovers well the block for which β = β0,
i.e. the value we gave to the method, but does not perform well on the two other blocks, while the matrix
(cid:98)G estimated with NPHC approximately reaches the true value for each of the three blocks. On these
simulated datasets, NPHC obtains a comparable or slightly better Kendall rank correlation, but improves
a lot the relative error.

On MemeTracker, the baseline methods obtain a high relative error between 9% and 19% while our
method achieves a relative error of 7% which is a strong improvement. Moreover, NPHC reaches a much
better Kendall rank correlation, which proves that it leads to a much better recovery of the relative order
of estimated inﬂuences than all the baselines. Indeed, it has been shown in Zhou et al. (2013a) that kernels
of MemeTracker data are not exponential, nor power law. This partly explains why our approach behaves

11

Table 3: Metrics on PLaw10: comparable rank correlation, strong improvement for relative error and
computing time.

Table 4: Metrics on Exp100: comparable rank correlation, strong improvement for relative error and
computing time.

Method

ODE GC

ADM4 WH

NPHC

RelErr
MRankCorr
Time (s)

0.011
0.31
870

0.09
0.26
781

0.053
0.24
717

0.009
0.34
946

0.0048
0.33
18

Method

ODE GC

ADM4 NPHC

RelErr
MRankCorr
Time (s)

0.092
0.032
3215

0.112
0.009
2950

0.079
0.049
2411

0.008
0.041
47

better.

On the ﬁnancial data, the estimated kernel norm matrix obtained via NPHC, see Figure 3, gave some

interpretable results (see also Bacry et al. (2016)):

1. Any 2 × 2 sub-matrix with same kind of inputs (i.e. Prices changes, Trades, Limits or Cancels) is

symmetric. This shows empirically that ask and bid have symmetric roles.

2. The prices are mostly cross-excited, which means that a price increase is very likely to be followed
by a price decrease, and conversely. This is consistent with the wavy prices we observe on ﬁnancial
markets.

3. The market, limit and cancel orders are strongly self-excited. This can be explained by the
persistence of order ﬂows, and by the splitting of meta-orders into sequences of smaller orders.
Moreover, we observe that orders impact the price without changing it. For example, the increase
of cancel orders at the bid causes downward price moves.

We show in this section how to obtain the equations stated above, the estimators of the integrated
cumulants and the scaling coefﬁcient κ that appears in the objective function. We then prove the theorem
of the paper.

4 Technical details

4.1 Proof of Equation (8)

We denote ν(z) the matrix

νij(z) = Lz

t →

(cid:16)

E(dN i

u+t)

udN j
dudt

− ΛiΛj(cid:17)

,

where Lz(f ) is the Laplace transform of f , and ψt = (cid:80)
refers to the nth auto-
convolution of φt. Then we use the characterization of second-order statistics, ﬁrst formulated in Hawkes
(1971) and fully generalized in Bacry and Muzy (2016),

, where φ((cid:63)n)

n≥1 φ((cid:63)n)

t

t

ν(z) = (Id + L−z(Ψ))L(Id + Lz(Ψ))(cid:62),

12

Table 5: Metrics on MemeTracker: strong improvement in relative error, rank correlation and computing
time.

Method

ODE GC

ADM4 NPHC

RelErr
MRankCorr
Time (s)

0.162
0.07
2944

0.19
0.053
2780

0.092
0.081
2217

0.071
0.095
38

where Lij = Λiδij with δij the Kronecker symbol. Since Id + Lz(Ψ) = (Id − Lz(Φ))−1, taking z = 0
in the previous equation gives

ν(0) = (Id − G)−1L(Id − G(cid:62))−1,

C = RLR(cid:62),

which gives us the result since the entry (i, j) of the last equation gives Cij = (cid:80)

m ΛmRimRjm.

4.2 Proof of Equation (9)

We start from Jovanovi´c et al. (2015), cf. Eqs. (48) to (51), and group some terms:

Kijk =

(cid:88)

ΛmRimRjmRkm

+

+

+

m
(cid:88)

m
(cid:88)

m
(cid:88)

m

RimRjm

ΛnRknL0(ψmn)

RimRkm

ΛnRjnL0(ψmn)

RjmRkm

ΛnRinL0(ψmn).

(cid:88)

n
(cid:88)

n
(cid:88)

n

Using the relations L0(ψmn) = Rmn − δmn and Cij = (cid:80)

m ΛmRimRjm, proves Equation (9).

4.3

Integrated cumulants estimators

For H > 0 let us denote ∆H N i
domain to (−H, H) in Eqs. (4) and (5), one gets by permuting integrals and expectations:

t−H . Let us ﬁrst remark that, if one restricts the integration

t+H − N i

t = N i

(cid:16)

Cijdt = E

Λidt = E(dN i
t )
t (∆H N j
dN i
t (∆H N j
(cid:16)

Kijkdt = E

dN i

(cid:16)

(cid:17)
t − 2HΛj)

t − 2HΛj)(∆H N k

t − 2HΛk)

(cid:17)

− dtΛiE

(∆H N j

t − 2HΛj)(∆H N k

(cid:17)
t − 2HΛk)

.

The estimators (11) and (12) are then naturally obtained by replacing the expectations by their empirical
counterparts, notably

E(dN i
t f (t))
dt

→

1
T

(cid:88)

τ ∈Zi

f (τ ).

13

For the estimator (13), we shall also notice that

E((∆H N j
(cid:90) (cid:90)

=

=

(cid:90)

t − 2HΛj)(∆H N k

t − 2HΛk))

1[−H,H](t)1[−H,H](t(cid:48))Cjk

t−t(cid:48)dtdt(cid:48)

(2H − |t|)+Cjk

t dt.

We estimate the last integral with the remark above.

4.4 Choice of the scaling coefﬁcient κ

Following the theory of GMM, we denote m(X, θ) a function of the data, where X is distributed with
respect to a distribution Pθ0, which satisﬁes the moment conditions g(θ) = E[m(X, θ)] = 0 if and
only if θ = θ0, the parameter θ0 being the ground truth. For x1, . . . , xN observed copies of X, we
denote (cid:98)gi(θ) = m(xi, θ), the usual choice of weighting matrix is (cid:99)WN (θ) = 1
i=1 (cid:98)gi(θ)(cid:98)gi(θ)(cid:62), and
the objective to minimize is then

(cid:80)N

N

(cid:32)

1
N

N
(cid:88)

i=1

(cid:33)

(cid:16)

(cid:98)gi(θ)

(cid:99)WN (θ1)

(cid:32)

(cid:17)−1

(cid:33)

(cid:98)gi(θ)

,

1
N

N
(cid:88)

i=1

(15)

where θ1 is a constant vector. Instead of computing the inverse weighting matrix, we rather use its
projection on {αId : α ∈ R}. It can be shown that the projection choses α as the mean eigenvalue of
(cid:99)WN (θ1). We can easily compute the sum of its eigenvalues:

Tr((cid:99)WN (θ1)) =

Tr((cid:98)gi(θ1)(cid:98)gi(θ1)(cid:62)) =

Tr((cid:98)gi(θ1)(cid:62)

(cid:98)gi(θ1)) =

||(cid:98)gi(θ1)||2
2.

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

i=1

1
N

N
(cid:88)

i=1

(cid:104)
vec[(cid:100)Kc − Kc(R)], vec[ (cid:98)C − C(R)]

In our case, (cid:98)g(R) =
. Considering a block-wise
weighting matrix, one block for (cid:100)Kc − Kc(R) and the other for (cid:98)C − C(R), the sum of the eigenvalues of
the ﬁrst block becomes (cid:107)(cid:100)Kc − Kc(R)(cid:107)2
2, and (cid:107) (cid:98)C − C(R)(cid:107)2
2 for the second. We compute the previous
terms with R1 = 0. All together, the objective function to minimize is

∈ R2d2

(cid:105)(cid:62)

1
(cid:107)(cid:100)Kc(cid:107)2
2

(cid:16)

(cid:107)Kc(R) − (cid:100)Kc(cid:107)2

2 +

(cid:107)C(R) − (cid:98)C(cid:107)2
2.

(16)

1
(cid:107) (cid:98)C(cid:107)2
2

(cid:17)−1

, and setting κ = (cid:107)(cid:100)Kc(cid:107)2

2/((cid:107)(cid:100)Kc(cid:107)2

2 + (cid:107) (cid:98)C(cid:107)2

2), we

2 + 1/(cid:107) (cid:98)C(cid:107)2
Dividing this function by
2
obtaind the loss function given in Equation (10).

1/(cid:107)(cid:100)Kc(cid:107)2

4.5 Proof of the Theorem

The main difference with the usual Generalized Method of Moments, see Hansen (1982), relies in the
relaxation of the moment conditions, since we have E[(cid:98)gT (θ0)] = mT (cid:54)= 0. We adapt the proof of
consistency given in Newey and McFadden (1994).

We can relate the integral of the Hawkes process’s kernels to the integrals of the cumulant densities,
from Jovanovi´c et al. (2015). Our cumulant matching method would fall into the usual GMM framework
if we could estimate - without bias - the integral of the covariance on R, and the integral of the skewness
on R2. Unfortunately, we can’t do that easily. We can however estimate without bias (cid:82) f T
t dt

t Cij

14

t Kijk

and (cid:82) f T
t dt with f T a compact supported function on [−HT , HT ] that weakly converges to 1,
t = 1[−HT ,HT ](t). Denoting (cid:98)Cij,(T ) the estimator of
with HT −→ ∞. In most cases we will take f T
(cid:82) f T
t Cij
t dt − Cij| can be considered a proxy to the distance to
the classical GMM. This distance has to go to zero to make the rest of GMM’s proof work: the estimator
(cid:98)Cij,(T ) is then asymptotically unbiased towards Cij when T goes to inﬁnity.

t dt, the term |E[ (cid:98)Cij,(T )] − Cij| = | (cid:82) f T

t Cij

4.5.1 Notations

We observe the multivariate point process (N t) on R+, with Zi the events of the ith component. We will
often write covariance / skewness instead of integrated covariance / skewness. In the rest of the document,
we use the following notations.

Hawkes kernels’ integrals Gtrue = (cid:82) Φtdt = ((cid:82) φij

t dt)ij = Id − (Rtrue)−1

Theoretical mean matrix L = diag(Λ1, . . . , Λd)

Theoretical covariance C = RtrueL(Rtrue)(cid:62)

Theoretical skewness Kc = (Kiij)ij = (Rtrue)(cid:12)2C(cid:62) + 2[Rtrue (cid:12) (C − RtrueL)](Rtrue)(cid:62)

Filtering function

f T ≥ 0

supp(f T ) ⊂ [−HT , HT ]

F T = (cid:82) f T

s ds

t = f T
(cid:101)f T
−t

Events sets Zi,T,1 = Zi ∩ [HT , T + HT ]

Zj,T,2 = Zj ∩ [0, T + 2HT ]

Estimators of the mean

(cid:98)Λi =

N i

T +HT
T

−N i

HT

Estimator of the covariance

(cid:98)Cij,(T ) = 1
T

(cid:80)

τ ∈Zi,T,1

τ (cid:48)∈Zj,T,2 fτ (cid:48)−τ − (cid:101)ΛjF T (cid:17)

Estimator of the skewness6

N j
T +2HT
T +2HT

(cid:101)Λj =
(cid:16)(cid:80)





(cid:98)Kijk,(T ) =

1
T





(cid:88)

(cid:88)

fτ (cid:48)−τ − (cid:101)ΛjF T





(cid:88)

fτ (cid:48)−τ − (cid:101)ΛkF T

τ ∈Zi,T,1

−

(cid:98)Λi
T + 2HT

τ (cid:48)∈Zj,T,2


(cid:88)



τ (cid:48)∈Zj,T,2

τ (cid:48)(cid:48)∈Zk,T,2

τ (cid:48)(cid:48)∈Zk,T,2

(cid:88)

(f T (cid:63) (cid:101)f T )τ (cid:48)−τ (cid:48)(cid:48) − (cid:101)Λk(F T )2









6When f T

t = 1[−HT ,HT ](t), we remind that (f T (cid:63) (cid:101)f T )t = (2HT − |t|)+. This leads to the estimator we showed in the

article.

15

GMM related notations

θ = R and

θ0 = Rtrue

(cid:20)

g0(θ) = vec

C − RLR(cid:62)

C(cid:62) − 2[R (cid:12) (C − RL)]R(cid:62)

(cid:21)

∈ R2d2

Kc − R(cid:12)2
(cid:34)

(T )

(cid:98)C
(T )

− R (cid:98)LR(cid:62)
)(cid:62) − 2[R (cid:12) ( (cid:98)C

(T )

− R (cid:98)L)]R(cid:62)

(cid:35)

∈ R2d2

(cid:98)gT (θ) = vec

(cid:100)Kc(T )

− R(cid:12)2

( (cid:98)C

Q0(θ) = g0(θ)(cid:62)W g0(θ)
(cid:98)QT (θ) = (cid:98)gT (θ)(cid:62)(cid:99)WT (cid:98)gT (θ)

4.5.2 Consistency

First, let’s remind a useful theorem for consistency in GMM from Newey and McFadden (1994).

Theorem 4.1. If there is a function Q0(θ) such that (i) Q0(θ) is uniquely maximized at θ0; (ii) Θ
is compact; (iii) Q0(θ) is continuous; (iv) (cid:98)QT (θ) converges uniformly in probability to Q0(θ), then
(cid:98)θT = arg max (cid:98)QT (θ)

−→ θ0.

P

We can now prove the consistency of our estimator.

Theorem 4.2. Suppose that (Nt) is observed on R+, (cid:99)WT

P

−→ W , and

1. W is positive semi-deﬁnite and W g0(θ) = 0 if and only if θ = θ0,

2. θ ∈ Θ, which is compact,

3. the spectral radius of the kernel norm matrix satisﬁes ||Φ||∗ < 1,
4. ∀i, j, k ∈ [d], (cid:82) f T

u du and (cid:82) f T

u du → (cid:82) Cij

u,v dudv → (cid:82) Kijk

u f T

v Kijk

u Cij

u,v dudv,

5. (F T )2/T

−→ 0 and ||f ||∞ = O(1).

P

Then

P

(cid:98)θT

−→ θ0.

Remark 1. In practice, we use a constant sequence of weighting matrices: (cid:99)WT = Id.

Proof. Proceed by verifying the hypotheses of Theorem 2.1 from Newey and McFadden (1994). Con-
dition 2.1(i) follows by (i) and by Q0(θ) = [W 1/2g0(θ)](cid:62)[W 1/2g0(θ)] > 0 = Q0(θ0). Indeed, there
exists a neighborhood N of θ0 such that θ ∈ N \{θ0} and g0(θ) (cid:54)= 0 since g0(θ) is a polynom. Condition
2.1(ii) follows by (ii). Condition 2.1(iii) is satisﬁed since Q0(θ) is a polynom. Condition 2.1(iv) is
harder to prove. First, since (cid:98)gT (θ) is a polynom of θ, we prove easily that E[supθ∈Θ |(cid:98)gT (θ)|] < ∞. Then,
by Θ compact, g0(θ) is bounded on Θ, and by the triangle and Cauchy-Schwarz inequalities,

(cid:12) (cid:98)QT (θ) − Q0(θ)(cid:12)
(cid:12)
(cid:12)

≤ (cid:12)
(cid:12)((cid:98)gT (θ) − g0(θ))(cid:62)(cid:99)WT ((cid:98)gT (θ) − g0(θ))(cid:12)
(cid:12)
(cid:12) + (cid:12)
T )((cid:98)gT (θ) − g0(θ))(cid:12)
+ (cid:12)
(cid:12)g0(θ)(cid:62)((cid:99)WT + (cid:99)W (cid:62)
≤ (cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)2(cid:107)(cid:99)WT (cid:107) + 2(cid:107)g0(θ)(cid:107)(cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)(cid:107)(cid:99)WT (cid:107) + (cid:107)g0(θ)(cid:107)2(cid:107)(cid:99)WT − W (cid:107).

(cid:12)g0(θ)(cid:62)((cid:99)WT − W )g0(θ)(cid:12)
(cid:12)

To prove supθ∈Θ
Θ compact, it is sufﬁcient to prove that (cid:107)(cid:98)L−L(cid:107)

P
−→ 0, we should now prove that supθ∈Θ(cid:107)(cid:98)gT (θ) − g0(θ)(cid:107)
−C(cid:107)

−→ 0, and (cid:107)(cid:100)Kc(T )

(cid:12) (cid:98)QT (θ) − Q0(θ)(cid:12)
(cid:12)
(cid:12)

−→ 0, (cid:107) (cid:98)C

(T )

P

P

−Kc(cid:107)

−→ 0. By
P

−→ 0.

P

16

Proof that (cid:107) (cid:98)L − L(cid:107)

P

−→ 0

The estimator of L is unbiased so let’s focus on the variance of (cid:98)L.

E[((cid:98)Λi − Λi)2] = E

(cid:19)2(cid:35)

(dN i

t − Λidt)

E[(dN i

t − Λidt)(dN i

t(cid:48) − Λidt(cid:48))]

(cid:90) T +HT

(cid:34)(cid:18) 1
T

HT
(cid:90) T +HT

(cid:90) T +HT

HT

HT

(cid:90) T +HT

(cid:90) T +HT

HT

(cid:90) T +HT

HT

HT

Ciidt =

−→ 0

Cii

t(cid:48)−tdtdt(cid:48)

Cii
T

P

=

=

≤

1
T 2

1
T 2

1
T 2

By Markov inequality, we have just proved that (cid:107)(cid:98)L − L(cid:107)

−→ 0.

Proof that (cid:107) (cid:98)C

(T )

P

− C(cid:107)

−→ 0

First, let’s remind that E( (cid:98)C

) (cid:54)= C. Indeed,

E

(cid:16)

(cid:98)Cij,(T )(cid:17)

= E

(T )

(cid:18) 1
T
(cid:18) 1
T

(cid:90) T +HT

(cid:90) T +2HT

HT

(cid:90) T +HT

(cid:90) T +2HT −t

dN i
t

dN i
t

0

−t
(cid:16)

HT
(cid:90) T +HT

(cid:90) HT

HT
fsCij

−HT
s ds + (cid:15)ij,T,HT F T

= E

1
T
(cid:90)

=

=

Now,

dN j

t(cid:48)ft(cid:48)−t − (cid:98)Λi (cid:101)ΛjF T

(cid:19)

(cid:19)

dN j

t+sfs − ΛiΛjF T

+ (cid:15)ij,T,HT F T

fsE

dN i

t dN j

t+s − ΛiΛjds

+ (cid:15)ij,T,HT F T

(cid:17)

(cid:15)ij,T,HT = E

(cid:16)

ΛiΛj − (cid:98)Λi (cid:101)Λj(cid:17)
(cid:90) T +HT
1
T 2

0

HT

(cid:90) T +2HT

(cid:90) T +HT

(cid:90) T +2HT

= −

= −

1
T 2

1
T

HT
(cid:90) (cid:32)

= −

1 +

Cij
t−t(cid:48)dtdt(cid:48)
(cid:19)−(cid:33)+

Cij

t dt

0

(cid:18) HT − |t|
T

(cid:16)

E

dN i

t dN j

t(cid:48) − ΛiΛjdtdt(cid:48)(cid:17)

(T )

Since f satisﬁes F T = o(T ), we have E( (cid:98)C
E( (cid:98)C
Let’s now focus on the variance of (cid:98)Cij,(T ) : V( (cid:98)Cij,(T )) = E

) −→ C.

−→ 0.

)(cid:107)

P

(T )

It remains now to prove that (cid:107) (cid:98)C

(T )

−

(cid:16)

( (cid:98)Cij,(T ))2(cid:17)

− E( (cid:98)Cij,(T ))2.

17

Now,

(cid:16)

E

( (cid:98)Cij,(T ))2(cid:17)


1
T 2

1
T 2
(cid:90)

= E



(cid:32)

= E

=

1
T 2

And,

(cid:88)

(fτ (cid:48)−τ − F T /(T + 2HT ))(fη(cid:48)−η − F T /(T + 2HT ))

(τ,η,τ (cid:48),η(cid:48))∈(Zi,T,1)2×(Zj,T,2)2
(cid:90)

(cid:90)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)

(cid:90)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)∈[0,T +2HT ]

dN i

t dN j

t(cid:48)dN i

sdN j

s(cid:48)(ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))

(cid:16)

E

dN i

t dN j

t(cid:48)dN i

sdN j
s(cid:48)

(cid:17)

· (ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))





(cid:33)

E( (cid:98)Cij,(T ))2
1
T 2

=

(cid:90)

(cid:90)

(cid:16)

E

dN i

t dN j
t(cid:48)

(cid:17)

(cid:16)

E

dN i

sdN j
s(cid:48)

(cid:17)

t,s∈[HT ,T +HT ]

t(cid:48),s(cid:48)∈[0,T +2HT ]

· (ft(cid:48)−t − F T /(T + 2HT ))(fs(cid:48)−s − F T /(T + 2HT ))

Then, the variance involves the integration towards the difference of moments µr,s,t,u − µr,sµt,u. Let’s
write it as a sum of cumulants, since cumulants density are integrable.

µr,s,t,u − µr,sµt,u = κr,s,t,u + κr,s,tκu[4] + κr,sκt,u[3] + κr,sκtκu[6] + κrκsκtκu − (κr,s + κrκs)(κt,u + κtκu)

= κr,s,t,u
+ κr,s,tκu + κu,r,sκt + κt,u,rκs + κs,t,uκr
+ κr,tκs,u + κr,uκs,t
+ κr,tκsκu + κr,uκsκt + κs,tκrκu + κs,tκrκu

In the rest of the proof, we denote at = 1t∈[HT ,T +HT ], bt = 1t∈[0,T +2HT ], ct = 1t∈[−HT ,HT ], gt =
ft − 1
Before starting the integration of each term, let’s remark that:

T +2HT

F T

u,v (skewness density) and M ijkl

u,v,w (fourth cumulant density) are positive
· with positive coefﬁcients. The integrals of the singular parts are

1. Ψt = (cid:80)

n≥1 Φ((cid:63)n)

t ≥ 0 since Φt ≥ 0.

2. The regular parts of Cij

u , Kijk

3.

as polynoms of integrals of ψab
positive as well.
(a) (cid:82) atbt(cid:48)ft(cid:48)−tdtdt(cid:48) = T F T
(b) (cid:82) atbt(cid:48)gt(cid:48)−tdtdt(cid:48) = 0
(c) (cid:82) atbt(cid:48)|gt(cid:48)−t|dtdt(cid:48) ≤ 2T F T
4. ∀t ∈ R, at(b (cid:63) (cid:101)g)t = 0, where (cid:101)gs = g−s.

18

Fourth cumulant We want here to compute (cid:82) κi,j,i,j
We remark that |gt(cid:48)−tgs(cid:48)−s| ≤ (||f ||∞(1 + 2HT /T ))2 ≤ 4||f ||2
∞.
(cid:19)2 (cid:90)

(cid:90)

t,t(cid:48),s,s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)(cid:12)
κi,j,i,j

(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)

1
T 2

t,t(cid:48),s,s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48).

dtat

dt(cid:48)bt(cid:48)

dsas

ds(cid:48)bs(cid:48)M ijij

t(cid:48)−t,s−t,s(cid:48)−t

(cid:90)

(cid:90)

(cid:90)

(cid:90)

dtat

dt(cid:48)bt(cid:48)

dsas

dwM ijij

t(cid:48)−t,s−t,w

dtat

M ijij

u,v,wdudvdw

(cid:90)

(cid:90)

(cid:90)

(cid:18) 2||f ||∞
T
(cid:18) 2||f ||∞
T
(cid:18) 2||f ||∞
T
4||f ||2
∞
T

≤

≤

≤

(cid:19)2 (cid:90)

(cid:19)2 (cid:90)

M ijij −→
T →∞

0

Third × First We have four terms, but only two different forms since the roles of (s, s(cid:48)) and (t, t(cid:48))
are symmetric.
First form

Second form

(cid:90)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
κi,j,j
t,t(cid:48),s(cid:48)ΛiGtdt
(cid:12)
(cid:12) =

(cid:90)

κi,j,i
t,t(cid:48),sΛjGtdt =

κi,j,i
t,t(cid:48),satbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)

(cid:90)

(cid:90)

Λj
T 2
Λj
T 2

=

= 0

κi,j,i
t,t(cid:48),satbt(cid:48)as(b (cid:63) (cid:101)g)sgt(cid:48)−tdtdt(cid:48)ds
since as(b (cid:63) (cid:101)g)s = 0

t,t(cid:48),s(cid:48)atbt(cid:48)asbs(cid:48)gt(cid:48)−tgs(cid:48)−sdtdt(cid:48)dsds(cid:48)(cid:12)
κi,j,j
t,t(cid:48),s(cid:48)atbt(cid:48)gt(cid:48)−tbs(cid:48)(a (cid:63) g)s(cid:48)dtdt(cid:48)ds(cid:48)(cid:12)
κi,j,j

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:90)

(cid:90)

(cid:12)
(cid:12)
(cid:12)

=

Λi
T 2
Λi
(cid:12)
(cid:12)
(cid:12)
T 2
Λi
≤
T 2 2||f ||∞
≤ 4||f ||∞KijjΛi F T
T

(cid:90)

−→
T →∞

0

ds(cid:48)bs(cid:48)(a (cid:63) |g|)s(cid:48)

dtat

dt(cid:48)bt(cid:48)Kijj

t(cid:48)−s(cid:48),t−s(cid:48)

(cid:90)

(cid:90)

Second × Second
First form

(cid:90)

(cid:12)
(cid:12)
(cid:12)

t,sκj,j
κi,i

t(cid:48),s(cid:48)Gtdt

Second form

Second × First × First
First form

(cid:90)

t(cid:48)−s(cid:48)atbt(cid:48)|gt(cid:48)−t|asbs(cid:48)dtdt(cid:48)dsds(cid:48)

(cid:12)
(cid:12)
(cid:12) ≤

(cid:90)

t−sCjj
Cii
(cid:90)

2||f ||∞
T 2
2||f ||∞
T 2 CiiCjj
≤
≤ 4||f ||∞CiiCjj F T
T

atbt(cid:48)|gt(cid:48)−t|dtdt(cid:48)

−→
T →∞

0

(cid:90)

(cid:12)
(cid:12)
(cid:12)

t,s(cid:48)κi,j
κi,j

(cid:12)
(cid:12)
t(cid:48),sGtdt

(cid:12) ≤ 4||f ||∞(Cij)2 F T

−→
T →∞

0

T

κi,j
t,t(cid:48)ΛiΛjGtdt =

κi,j
t,t(cid:48)atbt(cid:48)gt(cid:48)−tdtdt(cid:48)

asbs(cid:48)gs(cid:48)−sdsds(cid:48) = 0

(cid:90)

(cid:90)

ΛiΛj
T 2

19

Second form

(cid:90)

κi,i
t,sΛjΛjGtdt =

(cid:19)2 (cid:90)

(cid:18) Λj
T

κi,i
t,satbt(cid:48)gt(cid:48)−tas(b (cid:63) (cid:101)g)sdtdt(cid:48)ds = 0

We have just proved that V( (cid:98)C
− C(cid:107)
and ﬁnally that (cid:107) (cid:98)C

(T )

(T )

P

)
P

−→ 0.

−→ 0. By Markov inequality, it ensures us that (cid:107) (cid:98)C

(T )

−E( (cid:98)C

(T )

P

)(cid:107)

−→ 0,

− Kc(cid:107)

Proof that (cid:107)(cid:100)Kc(T )
The scheme of the proof is similar to the previous one. The upper bounds of the integrals involve the same
kind of terms, plus the new term (F T )2/T that goes to zero thanks to the assumption 5 of the theorem.

−→ 0

P

5 Conclusion

In this paper, we introduce a simple nonparametric method (the NPHC algorithm) that leads to a fast and
robust estimation of the matrix G of the kernel integrals of a Multivariate Hawkes process that encodes
Granger causality between nodes. This method relies on the matching of the integrated order 2 and
order 3 empirical cumulants, which represent the simplest set of global observables containing sufﬁcient
information to recover the matrix G. Since this matrix fully accounts for the self- and cross- inﬂuences of
the process nodes (that can represent agents or users in applications), our approach can naturally be used
to quantify the degree of endogeneity of a system and to uncover the causality structure of a network.

By performing numerical experiments involving very different kernel shapes, we show that the
baselines, involving either parametric or non-parametric approaches are very sensible to model misspeci-
ﬁcation, do not lead to accurate estimation, and are numerically expensive, while NPHC provides fast,
robust and reliable results. This is conﬁrmed on the MemeTracker database, where we show that NPHC
outperforms classical approaches based on EM algorithms or the Wiener-Hopf equations. Finally, the
NPHC algorithm provided very satisfying results on ﬁnancial data, that are consistent with well-known
stylized facts in ﬁnance.

Acknowledgements

This work beneﬁted from the support of the chair “Changing markets”, CMAP École Polytechnique and
École Polytechnique fund raising - Data Science Initiative.

The authors want to thank Marcello Rambaldi for fruitful discussions on order book data’s experi-

ments.

20

References

M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, et al. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems.
arXiv preprint arXiv:1603.04467, 2016.

Y. Aït-Sahalia, J. Cacho-Diaz, and R. JA Laeven. Modeling ﬁnancial contagion using mutually exciting

jump processes. Technical report, National Bureau of Economic Research, 2010.

E. Bacry and J.-F. Muzy. First- and second-order statistics characterization of hawkes processes and

non-parametric estimation. IEEE Transactions on Information Theory, 62(4):2184–2202, 2016.

E. Bacry, I. Mastromatteo, and J.-F. Muzy. Hawkes processes in ﬁnance. Market Microstructure and

Liquidity, 1(01):1550005, 2015.

E. Bacry, T. Jaisson, and J.-F. Muzy. Estimation of slowly decreasing hawkes kernels: application to

high-frequency order book dynamics. Quantitative Finance, pages 1–23, 2016.

A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of multilayer

networks. In AISTATS, 2015.

R. Crane and D. Sornette. Robust dynamic classes revealed by measuring the response function of a

social system. Proceedings of the National Academy of Sciences, 105(41), 2008.

J. Da Fonseca and R. Zaatour. Hawkes process: Fast calibration, application to trade clustering, and

diffusive limit. Journal of Futures Markets, 34(6):548–579, 2014.

D. J. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes Volume I: Elementary

Theory and Methods. Springer Science & Business Media, 2003.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.

M. Eichler, R. Dahlhaus, and J. Dueck. Graphical modeling for multivariate hawkes processes with non-
parametric link functions. Journal of Time Series Analysis, pages n/a–n/a, 2016. ISSN 1467-9892. doi:
10.1111/jtsa.12213. URL http://dx.doi.org/10.1111/jtsa.12213. 10.1111/jtsa.12213.

M. Farajtabar, Y. Wang, M. Rodriguez, S. Li, H. Zha, and L. Song. Coevolve: A joint point process model
for information diffusion and network co-evolution. In Advances in Neural Information Processing
Systems, pages 1945–1953, 2015.

M. Gomez-Rodriguez, J. Leskovec, and B. Schölkopf. Modeling information propagation with survival

theory. Proceedings of the International Conference on Machine Learning, 2013.

C. W. J. Granger. Investigating causal relations by econometric models and cross-spectral methods.
Econometrica, 37(3):424–438, 1969. ISSN 00129682, 14680262. URL http://www.jstor.
org/stable/1912791.

A. R. Hall. Generalized Method of Moments. Oxford university press, 2005.

L. P. Hansen. Large sample properties of generalized method of moments estimators. Econometrica:

Journal of the Econometric Society, pages 1029–1054, 1982.

N. R. Hansen, P. Reynaud-Bouret, and V. Rivoirard. Lasso and probabilistic inequalities for multivariate

point processes. Bernoulli, 21(1):83–143, 2015.

21

S. J. Hardiman and J.-P. Bouchaud. Branching-ratio approximation for the self-exciting Hawkes process.

Phys. Rev. E, 90(6):062807, December 2014. doi: 10.1103/PhysRevE.90.062807.

A. G. Hawkes. Point spectra of some mutually exciting point processes. Journal of the Royal Statistical
Society. Series B (Methodological), 33(3):438–443, 1971. ISSN 00359246. URL http://www.
jstor.org/stable/2984686.

A. G. Hawkes and D. Oakes. A cluster process representation of a self-exciting process. Journal of

Applied Probability, pages 493–503, 1974.

S. Jovanovi´c, J. Hertz, and S. Rotter. Cumulants of Hawkes point processes. Phys. Rev. E, 91(4):042802,

April 2015. doi: 10.1103/PhysRevE.91.042802.

R. Lemonnier and N. Vayatis. Nonparametric markovian learning of triggering kernels for mutually
exciting and mutually inhibiting multivariate hawkes processes. In Machine Learning and Knowledge
Discovery in Databases, pages 161–176. Springer, 2014.

E. Lewis and G. Mohler. A nonparametric em algorithm for multiscale hawkes processes. Journal of

Nonparametric Statistics, 2011.

G. O. Mohler, M. B. Short, P. J. Brantingham, F. P. Schoenberg, and G. E. Tita. Self-exciting point

process modeling of crime. Journal of the American Statistical Association, 2011.

W. K Newey and D. McFadden. Large sample estimation and hypothesis testing. Handbook of economet-

rics, 4:2111–2245, 1994.

27(1):23–31, 1981.

Y. Ogata. On lewis’ simulation method for point processes. Information Theory, IEEE Transactions on,

Y. Ogata. Space-time point-process models for earthquake occurrences. Annals of the Institute of

Statistical Mathematics, 50(2):379–402, 1998.

A. Podosinnikova, F. Bach, and S. Lacoste-Julien. Rethinking lda: moment matching for discrete ica. In

Advances in Neural Information Processing Systems, pages 514–522, 2015.

P. Reynaud-Bouret and S. Schbath. Adaptive estimation for hawkes processes; application to genome

analysis. The Annals of Statistics, 38(5):2781–2822, 2010.

V.S. Subrahmanian, A. Azaria, S. Durst, V. Kagan, A. Galstyan, K. Lerman, L. Zhu, E. Ferrara, A. Flam-

mini, and F. Menczer. The darpa twitter bot challenge. Computer, 49(6):38–46, 2016.

H. Xu, M. Farajtabar, and H. Zha. Learning granger causality for hawkes processes. In Proceedings of

The 33rd International Conference on Machine Learning, pages 1717–1726, 2016.

S.-H. Yang and H. Zha. Mixture of mutually exciting processes for viral diffusion. In Proceedings of the

International Conference on Machine Learning, 2013.

K. Zhou, H. Zha, and L. Song. Learning triggering kernels for multi-dimensional hawkes processes. In

Proceedings of the International Conference on Machine Learning, pages 1301–1309, 2013a.

K. Zhou, H. Zha, and L. Song. Learning social infectivity in sparse low-rank networks using multi-

dimensional hawkes processes. AISTATS, 2013b.

22

