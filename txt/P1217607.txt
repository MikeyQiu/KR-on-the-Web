Two-Stream Convolutional Networks for Dynamic Texture Synthesis

Matthew Tesfaldet Marcus A. Brubaker
Department of Electrical Engineering and Computer Science
York University, Toronto
{mtesfald,mab}@eecs.yorku.ca

Konstantinos G. Derpanis
Department of Computer Science
Ryerson University, Toronto
kosta@scs.ryerson.ca

8
1
0
2
 
r
p
A
 
2
1
 
 
]

V
C
.
s
c
[
 
 
4
v
2
8
9
6
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

We introduce a two-stream model for dynamic texture
synthesis. Our model is based on pre-trained convolutional
networks (ConvNets) that target two independent tasks: (i)
object recognition, and (ii) optical ﬂow prediction. Given
an input dynamic texture, statistics of ﬁlter responses from
the object recognition ConvNet encapsulate the per-frame
appearance of the input texture, while statistics of ﬁlter re-
sponses from the optical ﬂow ConvNet model its dynamics.
To generate a novel texture, a randomly initialized input se-
quence is optimized to match the feature statistics from each
stream of an example texture. Inspired by recent work on
image style transfer and enabled by the two-stream model,
we also apply the synthesis approach to combine the texture
appearance from one texture with the dynamics of another
to generate entirely novel dynamic textures. We show that
our approach generates novel, high quality samples that
match both the framewise appearance and temporal evo-
lution of input texture. Finally, we quantitatively evaluate
our texture synthesis approach with a thorough user study.

1. Introduction

Many common temporal visual patterns are naturally de-
scribed by the ensemble of appearance and dynamics (i.e.,
temporal pattern variation) of their constituent elements.
Examples of such patterns include ﬁre, ﬂuttering vegetation,
and wavy water. Understanding and characterizing these
temporal patterns has long been a problem of interest in hu-
man perception, computer vision, and computer graphics.
These patterns have been previously studied under a variety
of names, including turbulent-ﬂow motion [17], temporal
textures [30], time-varying textures [3], dynamic textures
[8], textured motion [45] and spacetime textures [7]. Here,
we adopt the term “dynamic texture”. In this work, we pro-
pose a factored analysis of dynamic textures in terms of ap-
pearance and temporal dynamics. This factorization is then
used to enable dynamic texture synthesis which, based on

Figure 1: Dynamic texture synthesis. (left) Given an input
dynamic texture as the target, our two-stream model is able
to synthesize a novel dynamic texture that preserves the tar-
get’s appearance and dynamics characteristics. (right) Our
two-stream approach enables synthesis that combines the
texture appearance from one target with the dynamics from
another, resulting in a composition of the two.

example texture inputs, generates a novel dynamic texture
instance. It also enables a novel form of style transfer where
the target appearance and dynamics can be taken from dif-
ferent sources as shown in Fig. 1.

Our model is constructed from two convolutional net-
works (ConvNets), an appearance stream and a dynamics
stream, which have been pre-trained for object recognition
and optical ﬂow prediction, respectively. Similar to previ-
ous work on spatial textures [13, 19, 33], we summarize an
input dynamic texture in terms of a set of spatiotemporal
statistics of ﬁlter outputs from each stream. The appear-
ance stream models the per frame appearance of the input
texture, while the dynamics stream models its temporal dy-
namics. The synthesis process consists of optimizing a ran-
domly initialized noise pattern such that its spatiotemporal
statistics from each stream match those of the input tex-
ture. The architecture is inspired by insights from human
perception and neuroscience. In particular, psychophysical
studies [6] show that humans are able to perceive the struc-
ture of a dynamic texture even in the absence of appearance
cues, suggesting that the two streams are effectively inde-

pendent. Similarly, the two-stream hypothesis [16] models
the human visual cortex in terms of two pathways, the ven-
tral stream (involved with object recognition) and the dorsal
stream (involved with motion processing).

In this paper, our two-stream analysis of dynamic tex-
tures is applied to texture synthesis. We consider a range
of dynamic textures and show that our approach generates
novel, high quality samples that match both the frame-wise
appearance and temporal evolution of an input example.
Further, the factorization of appearance and dynamics en-
ables a novel form of style transfer, where dynamics of one
texture are combined with the appearance of a different one,
cf . [14]. This can even be done using a single image as
an appearance target, which allows static images to be an-
imated. Finally, we validate the perceived realism of our
generated textures through an extensive user study.

2. Related work

There are two general approaches that have dominated
the texture synthesis literature: non-parametric sampling
approaches that synthesize a texture by sampling pixels of
a given source texture [10, 26, 37, 47], and statistical para-
metric models. As our approach is an instance of a para-
metric model, here we focus on these approaches.

The statistical characterization of visual textures was in-
troduced in the seminal work of Julesz [23]. He conjectured
that particular statistics of pixel intensities were sufﬁcient
to partition spatial textures into metameric (i.e., perceptu-
ally indistinguishable) classes. Later work leveraged this
notion for texture synthesis [19, 33]. In particular, inspired
by models of the early stages of visual processing, statistics
of (handcrafted) multi-scale oriented ﬁlter responses were
used to optimize an initial noise pattern to match the ﬁlter
response statistics of an input texture. More recently, Gatys
et al. [13] demonstrated impressive results by replacing the
linear ﬁlter bank with a ConvNet that, in effect, served as
a proxy for the ventral visual processing stream. Textures
are modelled in terms of the correlations between ﬁlter re-
sponses within several layers of the network. In subsequent
work, this texture model was used in image style transfer
[14], where the style of one image was combined with the
image content of another to produce a new image. Ruder et
al. [36] extended this model to video by using optical ﬂow
to enforce temporal consistency of the resulting imagery.

Variants of linear autoregressive models have been stud-
ied [42, 8] that jointly model appearance and dynamics of
the spatiotemporal pattern. More recent work has consid-
ered ConvNets as a basis for modelling dynamic textures.
Xie et al. [48] proposed a spatiotemporal generative model
where each dynamic texture is modelled as a random ﬁeld
deﬁned by multiscale, spatiotemporal ConvNet ﬁlter re-
sponses and dynamic textures are realized by sampling the
model. Unlike our current work, which assumes pretrained

ﬁxed networks, this approach requires the ConvNet weights
to be trained using the input texture prior to synthesis.

A recent preprint [12] described preliminary results ex-
tending the framework of Gatys et al. [13] to model and syn-
thesize dynamic textures by computing a Gram matrix of
ﬁlter activations over a small temporal window. In contrast,
our two stream ﬁltering architecture is more expressive as
our dynamics stream is speciﬁcally tuned to spatiotemporal
dynamics. Moreover, as will be demonstrated, the factoriza-
tion in terms of appearance and dynamics enables a novel
form of style transfer, where the dynamics of one pattern
are transferred to the appearance of another to generate an
entirely new dynamic texture. To the best of our knowledge,
we are the ﬁrst to demonstrate this form of style transfer.

The recovery of optical ﬂow from temporal imagery
has long been studied in computer vision.
Tradition-
ally, it has been addressed by handcrafted approaches e.g.,
[20, 29, 35]. Recently, ConvNet approaches [9, 34, 21, 49]
have been demonstrated as viable alternatives. Most closely
related to our approach are energy models of visual motion
[2, 18, 39, 31, 7, 25] that have been motivated and studied
in a variety of contexts, including computer vision, visual
neuroscience, and visual psychology. Given an input image
sequence, these models consist of an alternating sequence
of linear and non-linear operations that yield a distributed
representation (i.e., implicitly coded) of pixelwise optical
ﬂow. Here, an energy model motivates the representation of
observed dynamics which is then encoded as a ConvNet.

3. Technical approach

Our proposed two-stream approach consists of an ap-
pearance stream, representing the static (texture) appear-
ance of each frame, and a dynamics stream, representing
temporal variations between frames. Each stream consists
of a ConvNet whose activation statistics are used to charac-
terize the dynamic texture. Synthesizing a dynamic texture
is formulated as an optimization problem with the objective
of matching the activation statistics. Our dynamic texture
synthesis approach is summarized in Fig. 2 and the individ-
ual pieces are described in turn in the following sections.

3.1. Texture model: Appearance stream

The appearance stream follows the spatial texture model
introduced by Gatys et al. [13] which we brieﬂy review
here. The key idea is that feature correlations in a Con-
vNet trained for object recognition capture texture appear-
ance. We use the same publicly available normalized VGG-
19 network [40] used by Gatys et al. [13].

To capture the appearance of an input dynamic texture,
we ﬁrst perform a forward pass with each frame of the im-
age sequence through the ConvNet and compute the feature
RNl×Ml , for various levels in the net-
activations, Alt
work, where Nl and Ml denote the number of ﬁlters and

∈

ture suited for computing optical ﬂow (e.g., [9, 21]) which
is naturally differentiable. However, with most such mod-
els it is unclear how invariant their layers are to appearance.
Instead, we propose a novel network architecture which is
motivated by the spacetime-oriented energy model [7, 39].
In motion energy models, the velocity of image content
(i.e., motion) is interpreted as a three-dimensional orienta-
tion in the x-y-t spatiotemporal domain [2, 11, 18, 39, 46].
In the frequency domain, the signal energy of a translating
pattern can be shown to lie on a plane through the origin
where the slant of the plane is deﬁned by the velocity of
the pattern. Thus, motion energy models attempt to identify
this orientation-plane (and hence the patterns velocity) via
a set of image ﬁltering operations. More generally the con-
stituent spacetime orientations for a spectrum of common
visual patterns (including translation and dynamic textures)
can serve as a basis for describing the temporal variation of
an image sequence [7]. This suggests that motion energy
models may form an ideal basis for our dynamics stream.

Speciﬁcally, we use the spacetime-oriented energy
model [7, 39] to motivate our network architecture which
we brieﬂy review here; see [7] for a more in-depth descrip-
tion. Given an input video, a bank of oriented 3D ﬁlters
are applied which are sensitive to a range of spatiotemporal
orientations. These ﬁlter activations are rectiﬁed (squared)
and pooled over local regions to make the responses robust
to the phase of the input signal, i.e., robust to the alignment
of the ﬁlter with the underlying image structure. Next, ﬁl-
ter activations consistent with the same spacetime orienta-
tion are summed. These responses provide a pixelwise dis-
tributed measure of which orientations (frequency domain
planes) are present in the input. However, these responses
are confounded by local image contrast that makes it dif-
ﬁcult to determine whether a high response is indicative
of the presence of a spacetime orientation or simply due
to high image contrast. To address this ambiguity, an L1
normalization is applied across orientation responses which
results in a representation that is robust to local appearance
variations but highly selective to spacetime orientation.

Using this model as our basis, we propose the follow-
ing fully convolutional network [38]. Our ConvNet in-
put is a pair of temporally consecutive greyscale images.
Each input pair is ﬁrst normalized to have zero-mean and
unit variance. This step provides a level of invariance to
overall brightness and contrast, i.e., global additive and
multiplicative signal variations. The ﬁrst layer consists of
32 3D spacetime convolution ﬁlters of size 11
2
width
(height
time). Next, a squaring activation function
and 5
5 spatial max-pooling (with a stride of one) is ap-
plied to make the responses robust to local signal phase. A
1 convolution layer follows with 64 ﬁlters that combines
1
energy measurements that are consistent with the same ori-
entation. Finally, to remove local contrast dependence, an

×
×

11

×

×

×

×

Figure 2: Two-stream dynamic texture generation. Sets of
Gram matrices represent a texture’s appearance and dynam-
ics. Matching these statistics allows for the generation of
novel textures as well as style transfer between textures.

(cid:80)T

∈
t=1

ij =

1
T NlMl

the number of spatial locations of layer l at time t, respec-
tively. The correlations of the ﬁlter responses in a particular
layer are averaged over the frames and encapsulated by a
RNl×Nl , whose entries are given by
Gram matrix, Gl
(cid:80)Ml
ikAlt
Gl
k=1 Alt
jk, where T denotes the
number of input frames and Alt
ik denotes the activation of
feature i at location k in layer l on the target frame t. The
synthesized texture appearance is similarly represented by
a Gram matrix, ˆGlt
RNl×Nl , whose activations are given
ˆAlt
ˆAlt
by ˆGlt
ij = 1
ik denotes the acti-
ik
vation of feature i at location k in layer l on the synthesized
frame t. The appearance loss,
appearance, is then deﬁned as
the temporal average of the mean squared error between the
Gram matrix of the input texture and that of the generated
texture computed at each frame:

jk, where ˆAlt

∈
(cid:80)Ml
k=1

NlMl

L

appearance =

L

1
LappTout

Tout(cid:88)

(cid:88)

t=1

l

Gl

(cid:107)

ˆGlt

2
F ,

(cid:107)

−

(1)

where Lapp is the number of layers used to compute Gram
matrices, Tout is the number of frames being generated in
the output, and
(cid:107) · (cid:107)F is the Frobenius norm. Consistent
with previous work [13], we compute Gram matrices on the
following layers: conv1 1, pool1, pool2, pool3, and pool4.

3.2. Texture model: Dynamics stream

There are three primary goals in designing our dynamics
stream. First, the activations of the network must represent
the temporal variation of the input pattern. Second, the acti-
vations should be largely invariant to the appearance of the
images which should be characterized by the appearance
stream described above. Finally, the representation must be
differentiable to enable synthesis. By analogy to the ap-
pearance stream, an obvious choice is a ConvNet architec-

in layer l on the target frames t and t + 1. The dynam-
ics of the synthesized texture is represented by a Gram ma-
trix of ﬁlter response correlations computed separately for
each pair of frames, ˆGlt
ij =
1
ik denotes the activation of
NlMl
feature i at location k in layer l on the synthesized frames t
and t + 1. The dynamics loss,
dynamics, is deﬁned as the av-
L
erage of the mean squared error between the Gram matrices
of the input texture and those of the generated texture:

RNl×Nl , with entries ˆGlt

∈
jk, where ˆDlt

(cid:80)Ml
k=1

ˆDlt
ik

ˆDlt

dynamics =

L

1
Ldyn(Tout

Tout−1
(cid:88)

(cid:88)

1)

−

t=1

l

Gl

(cid:107)

ˆGlt

2
F ,

(cid:107)

−

(2)

where Ldyn is the number of ConvNet layers being used in
the dynamics stream.

Here we propose to use the output of the concatenation
layer, where the multiscale distributed representation of ori-
entations is stored, as the layer to compute the Gram ma-
trix. While it is tempting to use the predicted ﬂow out-
put from the network, this generally yields poor results as
shown in our evaluation. Due to the complex, temporal vari-
ation present in dynamic textures, they contain a variety of
local spacetime orientations rather than a single dominant
orientation. As a result, the ﬂow estimates will tend to be
an average of the underlying orientation measurements and
consequently not descriptive. A comparison between the
texture synthesis results using the concatenation layer and
the predicted ﬂow output is provided in Sec. 4.

3.3. Texture generation

The overall dynamic texture loss consists of the combi-
nation of the appearance loss, Eq. (1), and the dynamics
loss, Eq. (2):

dynamic texture = α

appearance + β

dynamics,

(3)

L

L

L

where α and β are the weighting factors for the appearance
and dynamics content, respectively. Dynamic textures are
implicitly deﬁned as the (local) minima of this loss. Tex-
tures are generated by optimizing Eq. (3) with respect to
the spacetime volume, i.e., the pixels of the video. Vari-
ations in the resulting texture are found by initializing the
optimization process using IID Gaussian noise. Consistent
with previous work [13], we use L-BFGS [28] optimization.
Naive application of the outlined approach will consume
increasing amounts of memory as the temporal extent of the
dynamic texture grows; this makes it impractical to gener-
ate longer sequences. Instead, long sequences can be in-
crementally generated by separating the sequence into sub-
sequences and optimizing them sequentially. This is real-
ized by initializing the ﬁrst frame of a subsequence as the
last frame from the previous subsequence and keeping it
ﬁxed throughout the optimization. The remaining frames

Figure 3: Dynamics stream ConvNet. The ConvNet is based
on a spacetime-oriented energy model [7, 39] and is trained
for optical ﬂow prediction. Three scales are shown for il-
lustration; in practice ﬁve scales were used.

L1 divisive normalization is applied.

To capture spacetime orientations beyond those capable
with the limited receptive ﬁelds used in the initial layer,
we compute a ﬁve-level spatial Gaussian pyramid. Each
pyramid level is processed independently with the same
spacetime-oriented energy model and then bilinearly up-
sampled to the original resolution and concatenated.

Prior energy model instantiations (e.g., [2, 7, 39]) used
handcrafted ﬁlter weights. While a similar approach could
be followed here, we opt to learn the weights so that they
are better tuned to natural imagery. To train the network
weights, we add additional decoding layers that take the
concatenated distributed representation and apply a 3
3
convolution (with 64 ﬁlters), ReLU activation, and a 1
1
convolution (with 2 ﬁlters) that yields a two channel output
encoding the optical ﬂow directly. The proposed architec-
ture is illustrated in Fig. 3.

×
×

For training, we use the standard average endpoint er-
ror (aEPE) ﬂow metric (i.e., L2 norm) between the pre-
dicted ﬂow and the ground truth ﬂow as the loss. Since no
large-scale ﬂow dataset exists that captures natural imagery
with groundtruth ﬂow, we take an unlabeled video dataset
and apply an existing ﬂow estimator [35] to estimate opti-
cal ﬂow for training, cf . [43]. For training data, we used
videos from the UCF101 dataset [41] with geometric and
photometric data augmentations similar to those used by
FlowNet [9], and optimized the aEPE loss using Adam [24].
Inspection of the learned ﬁlters in the initial layer showed
evidence of spacetime-oriented ﬁlters, consistent with the
handcrafted ﬁlters used in previous work [7].

Similar to the appearance stream, ﬁlter response cor-
relations in a particular layer of the dynamics stream are
averaged over the number of image frame pairs and en-
RNl×Nl , whose en-
capsulated by a Gram matrix, Gl
tries are given by Gl
ikDlt
k=1 Dlt
jk,
where Dlt
ik denotes the activation of feature i at location k

1
(T −1)NlMl

∈
(cid:80)T −1
t=1

ij =

(cid:80)Ml

of the subsequence are initialized randomly and optimized
as above. This ensures temporal consistency across synthe-
sized subsequences and can be viewed as a form of coordi-
nate descent for the full sequence objective. The ﬂexibility
of this framework allows other texture generation problems
to be handled simply by altering the initialization of frames
and controlling which frames or frame regions are updated.

4. Experimental results

The goal of (dynamic) texture synthesis is to gener-
ate samples that are indistinguishable from the real input
target texture by a human observer.
In this section, we
present a variety of synthesis results including a user study
to quantitatively evaluate the realism of our results. Given
their temporal nature, our results are best viewed as videos.
Our two-stream architecture was implemented using Ten-
sorFlow [1]. Results were generated using an NVIDIA Ti-
tan X (Pascal) GPU and synthesis times ranged between
one to three hours to generate 12 frames with an image
resolution of 256
256. For our full synthesis results
and source code, please refer to the supplemental material
on the project website: ryersonvisionlab.github.
io/two-stream-projpage.

×

4.1. Dynamic texture synthesis

We applied our dynamic texture synthesis process to a
wide range of textures which were selected from the Dyn-
Tex [32] database and others we collected in the wild. In-
cluded in our supplemental material are synthesized results
of nearly 60 different textures that encapsulate a range of
phenomena, such as ﬂowing water, waves, clouds, ﬁre, rip-
pling ﬂags, waving plants, and schools of ﬁsh. Some sam-
ple frames are shown in Fig. 4 but we encourage readers to
view the videos to fully appreciate the results. In addition,
we performed a comparison with [12] and [48]. Generally,
we found our results to be qualitatively comparable or better
than these methods. See the supplemental for more details
on the comparisons with these methods.

We also generated dynamic textures incrementally, as
described in Sec. 3.3. The resulting textures were perceptu-
ally indistinguishable from those generated with the batch
process. Another extension that we explored were textures
with no discernible temporal seam between the last and ﬁrst
frames. Played as a loop, these textures appear to be tempo-
rally endless. This was achieved by assuming that the ﬁrst
frame follows the ﬁnal frame and adding an additional loss
for the dynamics stream evaluated on that pair of frames.

Example failure modes of our method are presented
in Fig. 6.
In general, we ﬁnd that most failures result
from inputs that violate the underlying assumption of a
dynamic texture, i.e., the appearance and/or dynamics are
not spatiotemporally homogeneous.
In the case of the
escalator example, the long edge structures in the ap-

pearance are not spatially homogeneous, and the dynam-
ics vary due to perspective effects that change the motion
from downward to outward. The resulting synthesized tex-
ture captures an overall downward motion but lacks the per-
spective effects and is unable to consistently reproduce the
long edge structures. This is consistent with previous ob-
servations on static texture synthesis [13] and suggests it is
a limitation of the appearance stream.

Another example is the flag sequence where the rip-
pling dynamics are relatively homogeneous across the pat-
tern but the appearance varies spatially. As expected, the
generated texture does not faithfully reproduce the appear-
ance; however, it does exhibit plausible rippling dynamics.
In the supplemental material, we include an additional fail-
ure case, cranberries, which consists of a swirling pat-
tern. Our model faithfully reproduces the appearance but is
unable to capture the spatially varying dynamics. Interest-
ingly, it still produces a result which is statistically indistin-
guishable from real in our user study discussed below.

Appearance vs. dynamics streams We sought to verify
that the appearance and dynamics streams were capturing
complementary information. To validate that the texture
generation of multiple frames would not induce dynamics
consistent with the input, we generated frames starting from
randomly generated noise but only using the appearance
statistics and corresponding loss, i.e., Eq. 1. As expected,
this produced frames that were valid textures but with no
coherent dynamics present. Results for a sequence contain-
ing a school of ﬁsh are shown in Fig. 5; to examine the
dynamics, see fish in the supplemental material.

Similarly, to validate that the dynamics stream did not in-
advertently include appearance information, we generated
videos using the dynamics loss only, i.e., Eq. 2. The re-
sulting frames had no visible appearance and had an ex-
tremely low dynamic range, i.e., the standard deviation of
pixel intensities was 10 for values in [0, 255]. This indi-
cates a general invariance to appearance and suggests that
our two-stream dynamic texture representation has factored
appearance and dynamics, as desired.

4.2. User study

Quantitative evaluation for texture synthesis is a partic-
ularly challenging task as there is no single correct output
when synthesizing new samples of a texture. Like in other
image generation tasks (e.g., rendering), human perception
is ultimately the most important measure. Thus, we per-
formed a user study to evaluate the perceived realism of our
synthesized textures.

Similar to previous image synthesis work (e.g., [5]), we
conducted a perceptual experiment with human observers
to quantitatively evaluate our synthesis results. We em-
ployed a forced-choice evaluation on Amazon Mechanical

fireplace 1

(original)

fireplace 1

(synthesized)

lava

(original)

lava

(synthesized)

smoke 1

(original)

smoke 1

(synthesized)

underwater

vegetation 1

(original)

underwater
vegetation 1

(synthesized)

water 3

(original)

water 3

(synthesized)

Figure 4: Dynamic texture synthesis success examples. Names correspond to ﬁles in the supplemental material.

Turk (AMT) with 200 different users. Each user performed
59 pairwise comparisons between a synthesized dynamic
texture and its target. Users were asked to choose which
appeared more realistic after viewing the textures for an ex-
posure time sampled randomly from discrete intervals be-
tween 0.3 and 4.8 seconds. Measures were taken to control
the experimental conditions and minimize the possibility of
low quality data. See the supplemental material for further
experimental details of our user study.

For comparison, we constructed a baseline by using the
ﬂow decode layer in the dynamics loss of Eq. 2. This corre-
sponds with attempting to mimic the optical ﬂow statistics
of the texture directly. Textures were synthesized with this
model and the user study was repeated with an additional
200 users. To differentiate between the models, we label

“Flow decode layer” and “Concat layer” in the ﬁgures to
describe our baseline and ﬁnal model, respectively.

The results of this study are summarized in Fig. 7 which
shows user accuracy in differentiating real versus generated
textures as a function of time for both methods. Over-
all, users are able to correctly identify the real texture
2.5% of the time for brief exposures of 0.3 sec-
66.1%
onds. This rises to 79.6%
1.1% with exposures of 1.2 sec-
onds and higher. Note that “perfect” synthesis results would
have an accuracy of 50%, indicating that users were unable
to differentiate between the real and generated textures and
higher accuracy indicating less convincing textures.

±

±

The results clearly show that the use of the concatenation
layer activations is far more effective than the ﬂow decode
layer. This is not surprising as optical ﬂow alone is known

target
(fish)

appearance
only

both
streams

escalator

(original)

escalator

(synthesized)

flag

(original)

flag

(synthesized)

(top row) Target texture.

Figure 5: Dynamic texture synthesis versus texture synthe-
sis.
(middle) Texture synthesis
without dynamics constraints shows consistent per-frame
appearance but no temporal coherence. (bottom) Including
both streams induces consistent appearance and dynamics.

Figure 6: Dynamic texture synthesis failure examples. In
these cases, the failures are attributed to either the appear-
ance or the dynamics not being homogeneous.

to be unreliable on many textures, particularly those with
transparency or chaotic motion (e.g., water, smoke, ﬂames,
etc.). Also evident in these results is the time-dependant
nature of perception for textures from both models. Users’
ability to identify the generated texture improved as expo-
sure times increased to 1.2 seconds and remained relatively
ﬂat for longer exposures.

To better understand the performance of our approach,
we grouped and analyzed the results in terms of appear-
ance and dynamics characteristics. For appearance we used
the taxonomy presented in [27] and grouped textures as
either regular/near-regular (e.g., periodic tiling and brick
wall), irregular (e.g., a ﬁeld of ﬂowers), or stochastic/near-
stochastic (e.g.,
For dynamics we
grouped textures as either spatially-consistent (e.g., closeup
of rippling sea water) or spatially-inconsistent (e.g., rippling
sea water juxtaposed with translating clouds in the sky). Re-

tv static or water).

Figure 7: Time-limited pairwise comparisons across all tex-
tures with 95% statistical conﬁdence intervals.

sults based on these groupings can be seen in Fig. 8.

1.6% and 90.8%

A full breakdown of the user study results by texture and
grouping can be found in the supplemental material. Here
we discuss some of the overall trends. Based on appear-
ance it is clear that textures with large-scale spatial consis-
tencies (regular, near-regular, and irregular textures) tend to
perform poorly. Examples being flag and fountain 2
with user accuracies of 98.9%
4.3%
averaged across all exposures, respectively. This is not un-
expected and is a fundamental limitation of the local na-
ture of the Gram matrix representation used in the appear-
ance stream which was observed in static texture synthe-
sis [13]. In contrast, stochastic and near-stochastic textures
performed signiﬁcantly better as their smaller-scale local
variations are well captured by the appearance stream, for
instance water 1 and lava which had average accuracies
of 53.8%
7.4%, respectively, making
them both statistically indistinguishable from real.

7.4% and 55.6%

±

±

±

±

±

(e.g.,

dynamics

7.4% and 63.2%

In terms of dynamics, we ﬁnd that

textures with
tv static,
spatially-consistent
water *, and calm water *) perform signiﬁcantly
better than those with spatially-inconsistent dynamics (e.g.,
candle flame, fountain 2, and snake *), where
the dynamics drastically differ across spatial locations.
For example, tv static and calm water 6 have
average accuracies of 48.6%
7.2%,
respectively, while candle flame and snake 5 have
average accuracies of 92.4%
4%,
respectively. Overall, our model is capable of reproducing
a full spectrum of spatially-consistent dynamics. However,
as the appearance shifts from containing small-scale spatial
consistencies,
to containing large-scale
consistencies
performance degrades. This was evident in the user study
where the best-performing textures typically consisted of
a stochastic or near-stochastic appearance with spatially-
In contrast the worst-performing
consistent dynamics.
textures consisted of regular, near-regular, or irregular
appearance with spatially-inconsistent dynamics.

4% and 92.1%

±

±

±

appearance
target

synthesized output

Figure 9: Dynamics style transfer. (top row) Appearance of
still water was used with the dynamics of a different water
dynamic texture (water 4). (bottom row) The appearance
of a painting of ﬁre was used with the dynamics of a real
ﬁre (fireplace 1). Animated results and additional ex-
amples are available in the supplemental material.

ance and dynamics. We applied this model to a variety of
dynamic texture synthesis tasks and showed that, so long
as the input textures are generally true dynamic textures,
i.e., have spatially invariant statistics and spatiotemporally
invariant dynamics, the resulting synthesized textures are
compelling. This was validated both qualitatively and quan-
titatively through a large user study. Further, we showed
that the two-stream model enabled dynamics style transfer,
where the appearance and dynamics information from dif-
ferent sources can be combined to generate a novel texture.
We have explored this model thoroughly and found a few
limitations which we leave as directions for future work.
First, much like has been reported in recent image style
transfer work [14], we have found that high frequency noise
and chromatic aberrations are a problem in generation. An-
other issue that arises is the model fails to capture textures
with spatially-variant appearance, (e.g., flag in Fig. 6) and
spatially-inconsistent dynamics (e.g., escalator in Fig.
6). By collapsing the local statistics into a Gram matrix,
the spatial and temporal organization is lost. Simple post-
processing methods may alleviate some of these issues but
we believe that they also point to a need for a better rep-
resentation. Beyond addressing these limitations, a natural
next step would be to extend the idea of a factorized rep-
resentation into feed-forward generative networks that have
found success in static image synthesis, e.g., [22, 44].

Acknowledgements MT is supported by a Natural Sci-
ences and Engineering Research Council of Canada
(NSERC) Canadian Graduate Scholarship. KGD and MAB
are supported by NSERC Discovery Grants. This research
was undertaken as part of the Vision: Science to Applica-
tions program, thanks in part to funding from the Canada
First Research Excellence Fund.

Figure 8: Time-limited pairwise comparisons across all tex-
tures, grouped by appearance (top) and dynamics (bottom).
Shown with 95% statistical conﬁdence intervals.

4.3. Dynamics style transfer

The underlying assumption of our model is that appear-
ance and dynamics of texture can be factorized. As such, it
should allow for the transfer of the dynamics of one texture
onto the appearance of another. This has been explored pre-
viously for artistic style transfer [4, 15] with static imagery.
We accomplish this with our model by performing the same
optimization as above, but with the target Gram matrices for
appearance and dynamics computed from different textures.
A dynamics style transfer result is shown in Fig. 9 (top),
using two real videos. Additional examples are available
in the supplemental material. We note that when perform-
ing dynamics style transfer it is important that the appear-
ance structure be similar in scale and semantics, otherwise,
the generated dynamic textures will look unnatural. For in-
stance, transferring the dynamics of a ﬂame onto a water
scene will generally produce implausible results.

We can also apply the dynamics of a texture to a static
input image, as the target Gram matrices for the appearance
loss can be computed on just a single frame. This allows us
to effectively animate regions of a static image. The result
of this process can be striking and is visualized in Fig. 9
(bottom), where the appearance is taken from a painting and
the dynamics from a real world video.

5. Discussion and summary

In this paper, we presented a novel, two-stream model of
dynamic textures using ConvNets to represent the appear-

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 5
[2] E. H. Adelson and J. R. Bergen. Spatiotemporal energy mod-
els for the perception of motion. JOSA–A, 2(2):284–299,
1985. 2, 3, 4

[3] Z. Bar-Joseph, R. El-Yaniv, D. Lischinski, and M. Werman.
Texture mixing and texture movie synthesis using statistical
learning. T-VCG, 7(2):120–135, 2001. 1

[4] A. J. Champandard. Semantic style transfer and turning two-
bit doodles into ﬁne artworks. arXiv:1603.01768, 2016. 8
[5] Q. Chen and V. Koltun. Photographic image synthesis with

cascaded reﬁnement networks. In ICCV, 2017. 5

[6] J. E. Cutting. Blowing in the wind: Perceiving structure in

trees and bushes. Cognition, 12(1):25 – 44, 1982. 1

[7] K. G. Derpanis and R. P. Wildes. Spacetime texture represen-
tation and recognition based on a spatiotemporal orientation
analysis. PAMI, 34(6):1193–1205, 2012. 1, 2, 3, 4

[8] G. Doretto, A. Chiuso, Y. N. Wu, and S. Soatto. Dynamic

textures. IJCV, 51(2):91–109, 2003. 1, 2

[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. H¨ausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
FlowNet: Learning optical ﬂow with convolutional net-
works. In ICCV, pages 2758–2766, 2015. 2, 3, 4

[10] A. A. Efros and T. K. Leung. Texture synthesis by non-

parametric sampling. In ICCV, pages 1033–1038, 1999. 2

[11] M. Fahle and T. Poggio. Visual hyperacuity: Spatiotemporal
interpolation in human vision. Proceedings of the Royal So-
ciety of London B: Biological Sciences, 213(1193):451–477,
1981. 3

[12] C. M. Funke, L. A. Gatys, A. S. Ecker, and M. Bethge. Syn-
thesising dynamic textures using convolutional neural net-
works. arXiv:1702.07006, 2017. 2, 5, 10, 11

[13] L. A. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis
using convolutional neural networks. In NIPS, pages 262–
270, 2015. 1, 2, 3, 4, 5, 7

[14] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks. In CVPR, pages 2414–
2423, 2016. 2, 8

[15] L. A. Gatys, A. S. Ecker, M. Bethge, A. Hertzmann, and
E. Shechtman. Controlling perceptual factors in neural style
transfer. In CVPR, 2017. 8

[16] M. A. Goodale and A. D. Milner. Separate visual path-
ways for perception and action. Trends in Neurosciences,
15(1):20–25, 1992. 2

[18] D. J. Heeger. Optical ﬂow using spatiotemporal ﬁlters. IJCV,

1(4):279–302, 1988. 2, 3

[19] D. J. Heeger and J. R. Bergen. Pyramid-based texture analy-
sis/synthesis. In SIGGRAPH, pages 229–238, 1995. 1, 2
[20] B. K. P. Horn and B. G. Schunck. Determining optical ﬂow.

A.I., 17:185–203, 1981. 2

[21] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet 2.0: Evolution of optical ﬂow estimation
with deep networks. In CVPR, 2017. 2, 3

[22] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution. In ECCV, pages
694–711, 2016. 8

[23] B. Julesz. Visual pattern discrimination. IRE Trans. Infor-

mation Theory, 8(2):84–92, 1962. 2

[24] D. P. Kingma and J. Ba. Adam: A Method for Stochastic

Optimization. arXiv:1412.6980, 2014. 4

[25] K. Konda, R. Memisevic, and V. Michalski. Learning to en-
code motion using spatio-temporal synchrony international
conference on learning representation. In ICLR, 2014. 2
[26] V. Kwatra, A. Sch¨odl, I. Essa, G. Turk, and A. Bobick.
Graphcut textures: Image and video synthesis using graph
cuts. In SIGGRAPH, pages 277–286, 2003. 2

[27] W.-C. Lin, J. Hays, C. Wu, Y. Liu, and V. Kwatra. Quantita-
tive evaluation of near regular texture synthesis algorithms.
In CVPR, volume 1, pages 427–434, 2006. 7

[28] D. C. Liu and J. Nocedal. On the limited memory method
for large scale optimization. Mathematical Programming,
45(3):503–528, 1989. 4

[29] B. D. Lucas and T. Kanade. An iterative image registra-
tion technique with an application to stereo vision. In IJCAI,
pages 674–679, 1981. 2

[30] R. Nelson and R. Polana. Qualitative recognition of motion

using temporal textures. CVGIP, 56(1), 1992. 1

[31] S. Nishimoto and J. L. Gallant. A three-dimensional spa-
tiotemporal receptive ﬁeld model explains responses of area
mt neurons to naturalistic movies. Journal of Neuroscience,
31(41):14551–14564, 2011. 2

[32] R. P´eteri, S. Fazekas, and M. J. Huiskes. DynTex: A Com-
prehensive Database of Dynamic Textures. PRL, 31(12),
2010. 5

[33] J. Portilla and E. P. Simoncelli. A parametric texture model
based on joint statistics of complex wavelet coefﬁcients.
IJCV, 40(1):49–70, 2000. 1, 2

[34] A. Ranjan and M. J. Black. Optical Flow Estimation using a

Spatial Pyramid Network. In CVPR, 2017. 2

[35] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-preserving interpolation of correspon-
dences for optical ﬂow. In CVPR, pages 1164–1172, 2015.
2, 4

[36] M. Ruder, A. Dosovitskiy, and T. Brox. Artistic style transfer

for videos. In GCPR, pages 26–36, 2016. 2

[37] A. Sch¨odl, R. Szeliski, D. Salesin, and I. A. Essa. Video

textures. In SIGGRAPH, pages 489–498, 2000. 2

[17] D. Heeger and A. Pentland. Seeing structure through chaos.
In IEEE Motion Workshop: Representation and Analysis,
pages 131–136, 1986. 1

[38] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional
networks for semantic segmentation. PAMI, 39(4):640–651,
2017. 3

[39] E. P. Simoncelli and D. J. Heeger. A model of neuronal re-
sponses in visual area MT. Vision Research, 38(5):743 – 761,
1998. 2, 3, 4

[40] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014. 2

[41] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A
dataset of 101 human actions classes from videos in the wild.
arXiv:1212.0402, 2012. 4

[42] M. Szummer and R. W. Picard. Temporal texture modeling.

In ICIP, pages 823–826, 1996. 2

[43] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and
M. Paluri. Deep end2end voxel2voxel prediction. In CVPR
Workshops, pages 402–409, 2016. 4

[44] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. S. Lempitsky.
Texture networks: Feed-forward synthesis of textures and
stylized images. In ICML, pages 1349–1357, 2016. 8
[45] Y. Wang and S. C. Zhu. Modeling textured motion: Particle,

wave and sketch. In ICCV, pages 213–220, 2003. 1

[46] A. B. Watson and A. J. Ahumada. A look at motion in the
frequency domain. In Motion workshop: Perception and rep-
resentation, pages 1–10, 1983. 3

[47] L. Wei and M. Levoy. Fast texture synthesis using tree-
structured vector quantization. In SIGGRAPH, pages 479–
488, 2000. 2

[48] J. Xie, S.-C. Zhu, and Y. N. Wu. Synthesizing dynamic
patterns by spatial-temporal generative convnet. In CVPR,
2017. 2, 5, 11

[49] J. J. Yu, A. W. Harley, and K. G. Derpanis. Back to Ba-
sics: Unsupervised Learning of Optical Flow via Brightness
Constancy and Motion Smoothness. In ECCVW, 2016. 2

A. Experimental procedure

Here we provide further experimental details of our user study
using Amazon Mechanical Turk (AMT). Experimental trials were
grouped into batches of Human Intelligence Tasks (HITs) for users
to complete. Each HIT consisted of 59 pairwise comparisons be-
tween a synthesized dynamic texture and its target. Users were
asked to choose which texture appeared more realistic after view-
ing each texture independently for an exposure time (in seconds)
sampled randomly from the set {0.3, 0.4, 0.6, 1.2, 2.4, 3.6, 4.8}.
Note that 12 frames of the dynamic texture corresponds to 1.2 sec-
onds, i.e., 10 frames per second. Before viewing a dynamic tex-
ture, a centred dot is ﬂashed twice to indicate to the user where
to look (left or right). To prepare users for the task, the ﬁrst three
comparisons were used for warm-up, exposing them to the short-
est (0.3s), median (1.2s), and longest (4.8s) durations. To prevent
spamming and bias, we constrained the experiment as follows:
users could make a choice only after both dynamic textures were
shown; the next texture comparison could only be made after a
decision was made for the current comparison; a choice could not
be changed after the next pair of dynamic textures were shown;
and users were each restricted to a single HIT. Obvious unrealistic
dynamic textures were synthesized by terminating synthesis early
(100 iterations) and were used as sentinel tests. Three of the 59
pairwise comparisons were sentinels and results from users which
gave incorrect answers on any of the sentinel comparisons were

not used. The left-right order of textures within a pair, display
order within a pair, and order of pairs within a HIT, were random-
ized. An example of a HIT is shown in a video included with the
supplemental on the project page: HIT example.mp4.

Users were paid $2 USD per HIT, and were required to have
at least a 98% HIT approval rating, greater than or equal to 5000
HITs approved, and to be residing in the US. We collected results
from 200 unique users to evaluate our ﬁnal model and another 200
to evaluate our baseline model.

B. Qualitative results

We provide videos showcasing the qualitative results of our
two-stream model, including the experiments mentioned in the
main manuscript, on our project page: ryersonvisionlab.
github.io/two-stream-projpage. The videos are in
MP4 format (H.264 codec) and are best viewed in a loop. They
are enclosed in the following folders:

• target textures: This folder contains the 59 dynamic

textures used as targets for synthesis.

• dynamic texture synthesis: This folder contains
synthesized dynamic textures where the appearance and dy-
namics targets are the same.

• using concatenation layer: This folder contains
synthesized dynamic textures where the concatenation layer
was used for computing the Gramian on the dynamics
stream. These are the results from our ﬁnal model.

• using flow decode layer: This folder contains syn-
thesized dynamic textures where the predicted ﬂow output
is used for computing the Gramian on the dynamics stream.
These are the results from our baseline.

• full synthesis:

This

folder

contains

synthesized dynamic textures,
generated, nor temporally-endless, etc.

i.e., not

regularly-
incrementally-

• appearance stream only: This folder contains dy-
namic textures synthesized using only the appearance stream
of our two-stream model. The dynamics stream is not used.

• incrementally generated: This folder contains dy-
namic textures synthesized using the incremental process
outlined in Section 3.3 in the main manuscript.

• temporally endless: This folder contains a synthe-
sized dynamic texture (smoke plume 1) where there is no
discernible temporal seam between the last and ﬁrst frames.
Played as a loop, it appears to be temporally endless, thus, it
is presented in animated GIF format.

• dynamics style transfer: This folder contains syn-
thesized dynamic textures where the appearance and dynam-
ics targets are different. Also included are videos where the
synthesized dynamic texture is “pasted” back onto the origi-
nal image it was cropped from, showing a proof-of-concept
of dynamics style transfer as an artistic tool.

• comparisons/funke: This folder contains four dy-
namic texture synthesis comparisons between our model and
a recent (unpublished) approach [12]. The dynamic textures

(labeled “Xie et al. (ST)”) designed for dynamic textures with both
spatial and temporal homogeneity, and their temporal model (la-
beled “Xie et al. (FC)”) designed for dynamic textures with only
temporal homogeneity.

Overall, we demonstrate that our results appear qualitatively
better, showing more temporal coherence and similarity in dy-
namics and fewer artifacts, e.g., blur and ﬂicker. This may be a
natural consequence of their limited representation of dynamics.
Although the spatiotemporal model of Xie et al. [48] is able to
synthesize dynamic textures that lack spatial homogeneity (e.g.,
bamboo and escalator), we note that their method can not
synthesize novel dynamic textures, i.e., it appears to faithfully re-
produce the target texture, reducing the applicability of their ap-
proach.

As a consequence of jointly modelling appearance and dynam-
ics, the methods of [12, 48] are not capable of the novel form of
style transfer we demonstrated. This was enabled by the factored
representation of dynamics and appearance. Furthermore, the spa-
tiotemporal extent of the output sequence generated by Xie et al.’s
[48] method is limited to being equal to the input. The proposed
approach does not share this limitation.

[12] which ex-
chosen are those reported by Funke et al.
hibit spatiotemporal homogeneity. For ease of comparison,
we have concatenated the results from both models with their
corresponding targets.

• comparisons/xie and funke: This folder contains
nine dynamic texture synthesis comparisons between our
model, Funke et al.’s [12], and Xie et al.’s [48]. The dynamic
textures chosen cover the full range of our appearance and
dynamics groupings. For ease of comparison, we have con-
catenated the results from all models with their correspond-
ing targets.

C. Full user study results

Figures 10a and 10b show histograms of the average user ac-
curacy on each texture, averaged over a range of exposure times.
The histogram bars are ordered from lowest to highest accuracy,
based on the results when using our ﬁnal model.

Tables 1 and 2 show the average user accuracy on each texture
when using our ﬁnal model. The results are averaged over expo-
sure times. Similarly, Tables 3 and 4 show the results when using
our baseline.

Tables 5 and 6 show the average user accuracy on texture ap-
pearance groups when using our ﬁnal model. The results are av-
eraged over exposure times. Similarly, Tables 7 and 8 show the
results when using our baseline.

Tables 9 and 10 show the average user accuracy on texture dy-
namics groups when using our ﬁnal model. The results are aver-
aged over exposure times. Similarly, Tables 11 and 12 show the
results when using our baseline.

Tables 13 and 14 show the average user accuracy over all tex-
tures when using our ﬁnal model. The results are averaged over
exposure times. Similarly, Tables 15 and 16 show the results when
using our baseline.

D. Qualitative comparisons

We qualitatively compare our results to those of Funke
et al. [12] and Xie et al. [48].
Note that Funke et al.
[12] provided results on only ﬁve textures and of those only
four are dynamic textures in the sense that
their appear-
ance and dynamics are spatiotemporally coherent. Their re-
sults on these sequences (cranberries, flames, leaves,
and water 5) are included in the folder funke under
dynamic texture synthesis/comparisons. Our re-
sults are included as well.

We also compare our results to [12, 48] on nine dynamic tex-
tures chosen to cover the full range of our dynamics and appear-
ance groupings. We use their publicly available code and follow
the parameters used in their experiments. For Funke et al.’s model
[12], the parameters used are ∆t = 4 and T = 12 (recall that
target dynamic textures consist of 12 frames). For the spatiotem-
poral and temporal models from Xie et al. [48], the parameters
used are T = 1200 and ˜M = 3. A comparison between our
results, Funke et al.’s [12], and Xie et al’s [48] on the nine dy-
namic textures are included in the folder xie and funke un-
der dynamic texture synthesis/comparisons. Note
for Xie et al. [48], we compare with their spatiotemporal model

.
)
s

m
0
0
6
-
0
0
3
(

s
e
m

i
t

e
r
u
s
o
p
x
e

t
r
o
h
S
)
a
(

.
)
s

m
0
0
8
4
-
0
0
2
1
(

s
e
m

i
t

e
r
u
s
o
p
x
e
g
n
o
L
)
b
(

.
e
c
n
e
d
ﬁ
n
o
c

l
a
c
i
t
s
i
t
a
t
s

%
5
9
a
h
t
i

w

r
o
r
r
e

f
o
n
i
g
r
a
m
a

s
e
d
u
l
c
n
i

y
c
a
r
u
c
c
a

e
r
u
t
x
e
t

h
c
a
E

.
s
e
m

i
t

e
r
u
s
o
p
x
e

r
e
v
o

d
e
g
a
r
e
v
a

s
e
i
c
a
r
u
c
c
a

e
r
u
t
x
e
t
-
r
e
P

:
0
1

e
r
u
g
i
F

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

±
±
±
0.0

±
±
±
0.248

0.203
0.095
0.175
0.135
0.217

±
±
±
±
±
0.175

0.205
±
0.196
0.162
0.12
0.212

0.194
±
0.162
±
0.199
±
0.139
±
0.252
±
0.181
±
0.195
0.135
0.186
0.199

300 ms.
0.625
0.769
0.609
0.806
0.533
0.607
0.44
±
0.813
0.727
0.609
0.6
±
0.139
0.806
±
0.168
0.81
±
0.5
0.219
±
0.591
0.48
±
0.792
0.909
0.571
1.0
±
0.069
0.964
±
0.176
0.72
±
0.5
0.209
±
0.435
0.929
0.452
0.813
0.632
0.8
±
0.476
0.458
0.632
0.6
±
0.542
0.517
0.767
0.667
0.792
0.538
0.478
0.769
0.724
0.862
0.72
±
0.643
0.643
0.826
0.538
0.656
0.556
0.375
0.632
0.545
0.688
0.571
0.444

0.199
±
0.182
±
0.151
±
0.202
±
0.162
±
0.192
±
0.204
±
0.162
±
0.163
±
0.126
±
0.176
0.177
0.177
0.155
0.192
0.165
0.23
0.237
0.217
0.208
0.161
0.183
0.187

±
±
±
0.192

0.214
0.199
0.217

±
±
±
±
±
±
±
±
±
±
±
±

400 ms.
0.161
0.333
±
0.215
0.786
±
0.152
0.786
±
0.127
0.88
±
0.164
0.842
±
0.212
0.571
±
0.621
0.177
±
0.245
0.5
±
0.183
0.654
±
0.175
0.773
±
0.175
0.773
±
0.212
0.75
±
0.129
0.839
±
0.429
0.212
±
0.168
0.81
±
0.195
0.318
±
0.158
0.733
±
0.952
0.091
±
0.209
0.65
±
0.0
1.0
±
1.0
0.0
±
0.909
0.12
±
0.565
0.203
±
0.688
0.227
±
0.826
0.155
±
0.538
0.192
±
0.778
0.192
±
0.667
0.202
±
0.903
0.104
±
0.714
0.167
±
0.346
0.183
±
0.667
0.202
±
0.769
0.162
±
0.625
0.168
±
0.741
0.165
±
0.903
0.104
±
0.737
0.198
±
0.938
0.119
±
0.731
0.17
±
0.727
0.186
±
0.833
0.149
±
0.783
0.169
±
0.704
0.172
±
0.708
0.182
±
0.773
0.175
±
0.815
0.147
±
0.947
0.1
±
0.63
0.182
±
0.231
0.5
±
0.183
0.32
±
0.586
0.179
±
0.188
0.64
±
0.165
0.741
±
0.218
0.667
±
0.179
0.586
±
0.201
0.364
±

0.193
0.164
0.187
0.196

±
±
±
±
0.164

±
±
±
±
±
±
±
±
0.0

0.139
0.186
0.155
0.185
0.188
0.111
0.165
0.069

600 ms.
0.714
0.842
0.615
0.846
0.7
±
0.187
0.615
±
0.156
0.622
±
0.169
0.667
±
0.209
0.65
±
0.205
0.591
±
0.643
0.177
±
1.0
0.0
±
0.788
0.727
0.826
0.593
0.696
0.897
0.656
0.964
1.0
±
0.115
0.913
±
0.181
0.552
±
0.151
0.808
±
0.147
0.815
±
0.177
0.621
±
0.202
0.667
±
0.151
0.767
±
0.096
0.95
±
0.173
0.679
±
0.23
0.556
±
0.195
0.652
±
0.155
0.826
±
0.174
0.581
±
0.175
0.8
±
0.16
0.75
±
0.613
0.171
±
0.058
0.97
±
0.165
0.741
±
0.6
0.215
±
0.119
0.938
±
0.168
0.81
±
0.155
0.826
±
0.191
0.813
±
0.111
0.917
±
0.193
0.714
±
0.103
0.889
±
0.19
0.423
±
0.222
0.579
±
0.169
0.667
±
0.652
0.195
±
0.196
0.52
±
0.173
0.75
±
0.179
0.586
±
0.227
0.688
±
0.197
0.583
±

0.12

±
0.131

±
±
0.2

0.151
0.199

1200 ms.
0.185
0.536
±
0.101
0.906
±
0.199
0.542
±
0.193
0.714
±
0.138
0.87
±
0.636
0.164
±
0.201
0.7
±
0.201
0.7
±
0.767
0.609
0.5
±
0.909
0.9
±
0.164
0.636
±
0.147
0.815
±
0.188
0.64
±
0.064
0.967
±
0.111
0.917
±
0.195
0.652
±
0.062
0.968
±
0.102
0.923
±
0.119
0.889
±
0.118
0.871
±
0.149
0.833
±
1.0
0.0
±
0.15
0.75
±
0.792
0.162
±
0.127
0.88
±
0.08
0.958
±
0.163
0.724
±
0.158
0.733
±
0.151
0.767
±
0.955
0.087
±
0.173
0.75
±
0.609
0.199
±
1.0
0.0
±
0.176
0.72
±
0.083
0.957
±
0.237
0.471
±
0.176
0.72
±
0.142
0.821
±
0.963
0.071
±
0.127
0.88
±
0.08
0.958
±
0.87
0.138
±
1.0
0.0
±
0.875
0.615
0.821
0.727
0.826
0.739
0.833
0.759
0.792
0.75

0.132
±
0.187
±
0.142
±
0.186
±
0.155
±
0.179
±
0.149
±
0.156
±
0.162
±
0.16
±

0.188

0.201

0.122

0.195
0.181
0.132
0.182
0.188

0.119
0.195
0.175
0.175
0.126

0.119

0.139
0.126
0.198
0.148

±
0.0
0.0

±
±
±
±
±
0.0

2400 ms.
0.636
±
0.95
0.096
±
0.867
±
0.058
0.97
±
0.731
0.17
±
0.75
0.19
±
0.652
±
0.824
±
0.875
±
0.708
±
0.519
±
1.0
0.0
±
0.938
0.652
0.773
0.548
0.933
1.0
±
0.696
1.0
±
1.0
±
0.889
±
0.92
0.106
±
0.788
±
0.905
±
0.737
±
0.735
±
0.0
1.0
±
0.0
1.0
±
0.808
0.593
0.806
0.857
0.75
0.9
±
0.952
±
0.652
±
0.92
0.106
±
0.895
±
0.5
0.173
±
0.931
±
0.84
0.144
±
0.905
±
0.852
±
0.913
±
0.917
±
0.923
±
0.227
±
0.813
±
0.571
±
0.706
±
0.667
±
0.771
±
0.65
0.209
±
0.696
±
0.182
0.37
±

±
±
±
±
0.19
±
0.131

0.091
0.195

0.138

0.092

0.126
0.134
0.115
0.111
0.102
0.175
0.191
0.212
0.153
0.202
0.139

0.151
0.185
0.139
0.15

0.188

0.15
±
0.084
±
0.195
±
0.077
0.134
±
0.182
±
0.175
±
0.182
0.122
0.163
0.202

0.071
0.163
0.123
0.188
0.099
0.074
0.177

0.132
±
0.111
±
0.189
±
0.064
±
0.225
±
0.138
±
0.127
0.106
0.169
±
0.204
±
0.15
±
0.069
±
0.252
±
0.151
±
0.138
0.259
±
0.119
±
0.167
0.163
0.062
0.157

±
±
±
0.0
0.107
0.0

0.119

±
±
±
0.0

±
±
±
±
±
±
±
0.0
0.0

3600 ms.
0.857
0.938
0.682
0.96
±
0.852
0.762
0.773
0.63
±
0.848
0.724
0.765
1.0
±
0.963
0.724
0.885
0.519
0.926
0.962
0.692
1.0
±
1.0
±
0.875
0.917
0.667
0.967
0.526
0.895
0.88
±
0.92
±
0.783
0.522
0.857
0.964
0.533
0.767
0.87
±
0.571
0.889
0.76
±
0.724
0.968
0.778
1.0
±
0.9
±
1.0
±
0.889
1.0
±
0.619
0.733
0.583
0.818
0.724
0.652
0.652
0.731
0.632

±
±
±
±
±
±
±
±
±

±
0.0

0.208
0.158
0.197
0.161
0.163
0.195
0.195
0.17
0.217

±
±
0.0

0.066
0.133

±
±
±
±
0.0

0.172
0.099
0.192
0.071

0.182
0.217
0.143
0.195
0.152
0.151
0.062
0.091
0.165
0.137
0.214
0.147

±
±
±
±
±
±
±
±
±
±
±
±
0.0
0.179
0.0

4800 ms.
0.704
0.926
0.778
0.963
1.0
±
0.762
0.706
0.781
0.682
0.786
0.658
0.968
0.952
0.741
0.828
0.524
0.815
1.0
±
0.5
±
1.0
±
0.966
0.833
1.0
±
0.151
0.808
±
0.089
0.933
±
0.218
0.667
±
0.155
0.826
±
0.135
0.813
±
0.119
0.889
±
0.138
0.87
±
0.195
0.652
±
0.077
0.96
±
0.127
0.88
±
0.151
0.808
±
0.195
0.652
±
0.145
0.889
±
0.15
0.714
±
0.074
0.962
±
0.165
0.588
±
0.182
0.63
±
0.0
1.0
±
0.87
0.138
±
0.0
1.0
±
0.127
0.88
±
0.069
0.964
0.852
0.134
1.0
±
0.333
0.821
0.394
0.917
0.7
±
0.682
0.667
0.833
0.452

±
±
±
±
0.164

0.195
0.189
0.133
0.175

0.178
0.142
0.167
0.111

±
±
0.0

±
±
±
±

Table 1: Per-texture accuracies averaged over exposure times, using the concatenation layer. Each texture accuracy includes
a margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

±
±
±
±
±
0.111

Short (300-600 ms.)
0.111
0.526
0.103
0.797
0.105
0.675
0.086
0.841
0.703
0.112
0.6
±
0.102
0.571
±
0.102
0.692
±
0.111
0.676
±
0.114
0.657
±
0.114
0.677
±
0.08
0.861
±
0.083
0.812
±
0.123
0.556
±
0.106
0.742
±
0.114
0.473
±
0.098
0.74
±
0.917
0.064
±
0.111
0.63
±
0.025
0.987
±
0.03
0.985
±
0.085
0.843
±
0.114
0.541
±
0.116
0.646
±
0.077
0.859
±
0.105
0.535
±
0.761
0.099
±
0.7
0.107
±
0.074
0.887
±
0.107
0.636
±
0.118
0.441
±
0.118
0.651
±
0.101
0.73
±
0.103
0.586
±
0.106
0.671
±
0.082
0.809
±
0.11
0.662
±
0.068
0.904
±
0.104
0.671
±
0.119
0.6
±
0.09
0.833
±
0.097
0.767
±
0.089
0.797
±
0.107
0.738
±
0.096
0.77
±
0.101
0.724
±
0.071
0.885
±
0.11
0.532
±
0.116
0.594
±
0.115
0.521
±
0.118
0.559
±
0.116
0.594
±
0.107
0.685
±
0.105
0.646
±
0.112
0.603
±
0.114
0.466
±

Long (1200-4800 ms.)
0.093
0.673
±
0.048
0.928
±
0.09
0.723
±
0.053
0.915
±
0.066
0.864
±
0.091
0.716
±
0.098
0.707
±
0.089
0.729
±
0.075
0.798
±
0.087
0.712
±
0.604
0.093
±
0.033
0.97
±
0.051
0.94
±
0.086
0.688
±
0.073
0.827
±
0.095
0.558
±
0.057
0.909
±
0.971
0.032
±
0.094
0.627
±
0.02
0.99
±
0.971
0.032
±
0.063
0.87
±
0.054
0.918
±
0.079
0.776
±
0.045
0.947
±
0.682
0.097
±
0.8
0.078
±
0.064
0.88
±
0.046
0.941
±
0.079
0.792
±
0.093
0.631
±
0.069
0.841
±
0.055
0.917
±
0.094
0.729
±
0.729
0.089
±
0.054
0.93
±
0.093
0.68
±
0.05
0.931
±
0.094
0.674
±
0.089
0.637
±
0.049
0.927
±
0.067
0.863
±
0.947
0.045
±
0.058
0.896
±
0.047
0.94
±
0.903
0.06
±
0.043
0.95
±
0.448
0.099
±
0.078
0.794
±
0.098
0.55
±
0.076
0.806
±
0.709
0.088
±
0.084
0.74
±
0.093
0.688
±
0.082
0.767
±
0.095
0.543
±

All (300-4800 ms.)
0.072
0.608
±
0.048
0.882
±
0.069
0.702
±
0.047
0.886
±
0.06
0.802
±
0.071
0.665
±
0.072
0.636
±
0.067
0.713
±
0.064
0.751
±
0.069
0.69
±
0.072
0.632
±
0.04
0.924
±
0.876
0.05
±
0.071
0.64
±
0.061
0.794
±
0.073
0.522
±
0.055
0.835
±
0.033
0.949
±
0.072
0.629
±
0.016
0.989
±
0.976
0.023
±
0.051
0.86
±
0.064
0.756
±
0.067
0.727
±
0.043
0.908
±
0.073
0.609
±
0.062
0.784
±
0.059
0.806
±
0.041
0.919
±
0.066
0.725
±
0.074
0.556
±
0.063
0.771
±
0.056
0.835
±
0.071
0.657
±
0.068
0.703
±
0.05
0.869
±
0.673
0.071
±
0.04
0.92
±
0.07
0.672
0.071
0.624
0.046
0.892
0.057
0.823
0.049
0.879
0.055
0.836
0.05
0.868
0.058
0.822
0.04
0.921
0.074
0.486
0.068
0.713
0.074
0.538
0.068
0.708
0.071
0.663
0.066
0.718
0.07
0.669
0.068
0.699
0.073
0.511

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±

Table 2: Per-texture accuracies averaged over a range of exposure times, using the concatenation layer. Each texture accuracy
includes a margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

0.126

0.138
0.196
0.151
0.135

0.162

0.115
±
0.106
±
0.202
±
0.143
±
0.144
0.212
0.1
±
0.126
±
0.089
±
0.162
±
0.08
±
0.189
±
0.112
±
0.199
±
0.096
0.161
0.091

0.1
0.091

±
±
0.131

0.145
±
0.156
0.151
0.112

±
0.0

±
0.0

±
±
±
±
0.0
0.0

300 ms.
0.933
1.0
±
0.895
0.846
0.808
0.929
1.0
±
1.0
±
0.875
1.0
±
0.913
0.944
0.765
0.864
0.84
±
0.75
±
0.947
0.905
0.933
0.875
0.958
0.667
0.941
0.609
0.95
±
0.818
0.952
1.0
±
1.0
±
0.947
0.952
0.9
±
0.889
0.85
±
0.808
0.941
1.0
±
0.941
0.867
0.667
1.0
±
1.0
±
0.941
0.958
0.957
1.0
±
0.909
0.684
0.857
0.929
0.778
0.867
0.737
1.0
±
0.947
0.941

±
±
±
±
±
±
±
0.0

±
±
±
0.0
0.0

±
±
0.0
0.0

±
±
±
0.0

±
±
0.0

±
±

0.112
0.172
0.239

0.112
0.08
0.083

0.12
0.209
0.183
0.135
0.272
0.172
0.198

0.1
0.112

0.186

0.1
0.111

0.202
0.083

±
±
±
±
±
0.0
0.0

0.106
0.195
0.138
0.119
0.071

±
±
0.0
0.0
0.138
0.132
0.083
0.111

400 ms.
0.9
±
0.944
0.652
0.895
0.889
0.963
1.0
±
1.0
±
0.947
0.897
1.0
±
1.0
±
0.87
±
0.875
±
0.957
±
0.917
±
1.0
0.0
±
0.765
±
0.957
±
1.0
0.0
±
1.0
0.0
±
0.75
0.19
±
0.88
0.127
±
0.209
0.65
±
0.0
1.0
±
0.096
0.95
±
0.938
0.119
±
0.92
0.106
±
0.0
1.0
±
0.962
1.0
±
1.0
±
1.0
±
0.857
0.8
±
0.857
1.0
±
1.0
±
0.773
0.957
1.0
±
0.958
1.0
±
0.917
1.0
±
0.947
1.0
±
0.588
0.958
0.778
1.0
±
1.0
±
0.905
0.944
0.933
0.88

0.126
±
0.106
±
0.126
±
0.127
±

0.234
0.08
0.192

±
0.175
0.15

±
±
±
0.0
0.0

0.175
0.083

±
0.0
0.0
0.0

±
0.0
0.0

±
±
0.0

0.183

0.074

0.111

±
0.0

±
0.0

±
0.0

0.08

0.1

0.08

±
0.0

±
0.0

±
0.0

0.119

0.099

0.115

0.111

±
0.0
0.0

±
±
±
0.0

±
±
0.0
0.0

±
±
0.0
0.0

0.066
0.083

0.102
0.106

±
±
±
±
0.0

0.126
0.083
0.193

0.207
0.186
0.229
0.1

600 ms.
0.913
1.0
±
0.933
0.957
0.714
1.0
±
0.966
0.957
1.0
±
1.0
±
0.958
1.0
±
0.938
1.0
±
1.0
±
0.926
1.0
±
0.923
0.944
1.0
±
1.0
±
0.722
0.727
0.769
0.947
1.0
±
0.917
1.0
±
0.08
0.958
±
0.077
0.96
±
0.941
0.112
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.75
±
0.923
±
0.947
±
0.941
±
0.846
±
0.0
1.0
±
1.0
0.0
±
0.964
1.0
±
0.962
1.0
±
1.0
±
1.0
±
0.64
±
0.952
±
0.952
±
0.889
±
0.962
±
0.938
±
1.0
0.0
±
0.952
0.947

0.188
0.091
0.091
0.119
0.074
0.119

0.19
0.102
0.1
0.112
0.139

0.091
0.1

±
0.0
0.0
0.0

0.069

0.074

±
0.0

±
0.0

±
±

0.08

0.15

±
0.0

±
0.0

±
0.0

±
0.0

0.112

0.071

0.126

0.183

±
0.0
0.0

±
0.0
0.0

±
±
±
0.0

0.115
0.091
0.087

1200 ms.
0.963
1.0
±
0.1
0.947
±
0.077
0.96
±
0.096
0.95
±
0.962
0.074
±
1.0
0.0
±
0.941
1.0
±
0.857
1.0
±
1.0
±
0.905
1.0
±
1.0
±
0.958
1.0
±
0.172
0.867
±
0.138
0.87
±
0.958
0.08
±
1.0
0.0
±
0.789
1.0
±
0.913
0.952
0.955
1.0
±
0.913
0.923
1.0
±
1.0
±
1.0
±
1.0
±
0.087
0.955
±
0.127
0.88
±
0.929
0.135
±
0.0
1.0
±
1.0
0.0
±
0.889
1.0
±
0.96
1.0
±
1.0
±
1.0
±
1.0
±
0.957
1.0
±
0.778
1.0
±
0.929
1.0
±
1.0
±
0.897
1.0
±
0.85
1.0

±
±
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

0.115
0.145

±
0.0
0.0

±
0.0

0.145

0.077

0.111

0.083

0.192

0.095

0.156

±
0.0

±
0.0

±
0.0

±
0.0

±

0.183
0.091

0.087

0.139

0.112
0.172

0.095

0.155

0.182

±
0.157

±
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.096
±
0.0
0.0

2400 ms.
0.0
1.0
±
0.0
1.0
±
0.131
0.9
±
0.92
0.106
±
0.857
±
0.952
±
1.0
0.0
±
0.955
1.0
±
1.0
±
1.0
±
1.0
±
0.846
±
0.96
0.077
±
0.941
±
0.867
±
1.0
0.0
±
0.929
1.0
±
1.0
±
1.0
±
0.826
0.8
±
0.762
1.0
±
1.0
±
1.0
±
0.95
1.0
±
1.0
±
0.906
1.0
±
1.0
±
1.0
±
0.8
±
1.0
±
1.0
±
0.933
0.944
0.947
1.0
±
0.955
1.0
±
1.0
±
1.0
±
0.95
1.0
±
0.55
1.0
±
0.889
1.0
±
1.0
±
1.0
±
1.0
±
0.929
0.773

±
0.0
0.0
0.0
0.096
±
0.0
0.218
±
0.0

±
0.0
0.0
0.0
0.202
0.0
0.0

±
0.0
0.0
0.0
0.0

±
±
±
0.0

±
±

0.126
0.106
0.1

0.087

0.095
0.175

0.101

0.145

0.1

0.08

0.106

±
0.0
0.0
0.0

±
±
±
0.0
0.0

0.115
0.091
0.145

0.111
0.101
0.161

±
0.0
0.0
0.168
0.091

3600 ms.
0.0
1.0
±
1.0
0.0
±
0.913
±
0.952
±
0.889
±
0.0
1.0
±
0.0
1.0
±
0.077
0.96
±
1.0
0.0
±
0.944
1.0
±
1.0
±
0.81
±
0.952
±
0.0
1.0
±
0.95
0.096
±
1.0
0.0
±
0.947
1.0
±
1.0
±
1.0
±
0.917
0.906
0.818
1.0
±
1.0
±
0.958
1.0
±
1.0
±
1.0
±
1.0
±
0.08
0.958
±
0.08
0.958
±
0.062
0.968
±
0.077
0.96
±
0.0
1.0
±
0.0
1.0
±
0.077
0.96
±
0.095
0.929
1.0
±
1.0
±
1.0
±
1.0
±
1.0
±
0.905
±
0.0
1.0
±
0.0
1.0
±
0.76
0.167
±
0.0
1.0
±
1.0
0.0
±
0.909
0.955
0.875
0.933
0.926
0.905

0.12
0.087
0.132
0.089
0.099
0.126

±
0.0
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

0.126

±
±
±
±
±
±

0.1
0.119
0.164
0.172
0.138

0.123

0.066

0.112

±
0.0

±
±
±
±
±
0.0
0.0
0.0

4800 ms.
0.885
1.0
±
0.966
±
0.0
1.0
±
0.92
0.106
±
1.0
0.0
±
0.941
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.157
0.8
±
0.95
0.096
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.947
0.938
0.842
0.867
0.895
1.0
±
1.0
±
1.0
±
0.895
1.0
±
0.813
0.95
±
0.958
0.889
1.0
±
0.9
±
1.0
±
1.0
±
1.0
±
0.95
±
0.909
1.0
±
1.0
±
1.0
±
0.955
1.0
±
1.0
±
1.0
±
0.783
±
0.0
1.0
±
0.88
0.127
±
0.0
1.0
±
1.0
0.0
±
0.909
0.962
1.0
0.9

±
±
0.0
0.131
0.0
0.0
0.0
0.096
0.12

±
±
0.0
0.131

±
0.0
0.0
0.0

±
0.0
0.0
0.0

±
0.0

±
±

0.087

0.169

0.12
0.074

0.138

0.191
±
0.096
0.08
0.145

Table 3: Per-texture accuracies averaged over exposure times, using the ﬂow decode layer. Each texture accuracy includes a
margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

Short (300-600 ms.)
0.078
0.917
±
0.034
0.983
±
0.102
0.807
±
0.076
0.909
±
0.089
0.811
±
0.05
0.963
±
0.027
0.986
±
0.985
0.029
±
0.055
0.95
±
0.051
0.954
±
0.049
0.956
±
0.028
0.986
±
0.092
0.857
±
0.067
0.912
±
0.057
0.932
±
0.881
0.078
±
0.038
0.98
±
0.081
0.875
±
0.054
0.944
±
0.05
0.963
±
0.987
0.025
±
0.113
0.71
±
0.844
0.089
±
0.124
0.661
±
0.054
0.96
±
0.07
0.917
±
0.062
0.934
±
0.042
0.969
±
0.034
0.983
±
0.047
0.957
±
0.046
0.966
±
0.049
0.964
±
0.044
0.968
±
0.082
0.902
±
0.099
0.788
±
0.071
0.906
±
0.029
0.985
±
0.966
0.046
±
0.094
0.825
±
0.91
0.068
±
1.0
0.0
±
0.038
0.972
±
0.032
0.983
±
0.052
0.946
±
0.026
0.986
±
0.03
0.984
±
0.049
0.964
±
0.121
0.639
±
0.064
0.932
±
0.085
0.887
±
0.077
0.907
±
0.055
0.95
±
0.092
0.857
±
0.037
0.981
±
0.06
0.945
±
0.069
0.918
±

0.05

±
0.0

±
0.0
0.0

±
±
±
±
0.0

±
±
±
±
±
±
0.0

0.075
0.042
0.029
0.046

0.051
0.044
0.064
0.028
0.024
0.04

0.051
0.043
0.029
0.029
0.077
0.068
0.077
0.021
0.023
0.023
0.052
0.022
0.041
0.043
0.03
0.039
0.029
0.065
0.023

Long (1200-4800 ms.)
0.959
0.039
1.0
±
0.934
0.955
0.909
0.979
0.988
0.964
1.0
±
0.948
1.0
±
1.0
±
0.839
0.963
0.985
0.952
1.0
±
0.94
±
0.961
±
0.979
±
0.985
±
0.847
±
0.884
±
0.847
±
0.989
±
0.988
±
0.988
±
0.939
±
0.989
±
0.963
±
0.956
±
0.978
±
0.965
±
0.979
±
0.894
±
0.988
±
1.0
0.0
±
0.978
0.929
0.964
0.989
0.986
1.0
±
0.986
0.973
0.975
1.0
±
0.721
1.0
±
0.921
0.978
0.988
0.914
0.969
0.921
0.897

0.056
0.03
0.023
0.057
0.035
0.056
0.064

0.031
0.055
0.04
0.022
0.026

±
±
±
±
±
0.0

0.027
0.037
0.034

±
±
±
0.0

0.095

±
0.0

±
±
±
±
±
±
±

All (300-4800 ms.)
0.037
0.945
±
0.013
0.993
±
0.051
0.885
±
0.04
0.937
±
0.055
0.861
±
0.026
0.974
±
0.018
0.987
±
0.026
0.974
±
0.023
0.979
±
0.951
0.036
±
0.023
0.98
±
0.014
0.993
±
0.058
0.846
±
0.039
0.939
±
0.957
0.033
±
0.043
0.92
±
0.014
0.993
±
0.046
0.912
±
0.034
0.953
±
0.026
0.973
±
0.019
0.986
±
0.066
0.789
±
0.054
0.867
±
0.069
0.773
±
0.023
0.979
±
0.033
0.958
±
0.03
0.966
±
0.034
0.952
±
0.019
0.986
±
0.031
0.96
±
0.032
0.96
±
0.027
0.972
±
0.029
0.966
±
0.035
0.952
±
0.057
0.848
±
0.034
0.952
±
0.014
0.993
±
0.026
0.973
±
0.052
0.884
±
0.038
0.94
±
0.013
0.993
±
0.023
0.979
±
0.013
0.993
±
0.03
0.966
±
0.023
0.98
±
0.023
0.979
0.019
0.986
0.075
0.687
0.027
0.972
0.047
0.908
0.035
0.952
0.027
0.972
0.05
0.893
0.026
0.973
0.042
0.931
0.047
0.905

±
±
±
±
±
±
±
±
±
±
±

Table 4: Per-texture accuracies averaged over a range of exposure times, using the ﬂow decode layer. Each texture accuracy
includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

300 ms.
0.702
0.806
0.616

±
±
±

0.098
0.046
0.03

400 ms.
0.74
±
0.853
±
0.658
±

0.101

0.044
0.029

600 ms.
0.838
0.837
0.687

±
±
±

0.088
0.043
0.028

1200 ms.
0.84
±
0.903
0.76

0.083
0.036
±
0.026

±

2400 ms.
0.954
0.909
0.751

±
±
±

0.051
0.037
0.026

3600 ms.
0.878
0.919
0.776

±
±
±

0.074
0.031
0.026

4800 ms.
0.827
0.902
0.762

±
±
±

0.082
0.035
0.025

Table 5: Accuracies of textures grouped by appearances, averaged over exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

Short (300-600 ms.)
0.056
0.756
0.026
0.831
0.017
0.654

±
±
±

Long (1200-4800 ms.)
0.038
0.871
0.017
0.908
0.013
0.762

±
±
±

All (300-4800 ms.)
0.033
0.821
0.015
0.875
0.01
0.717

±
±
±

Table 6: Accuracies of textures grouped by appearances, averaged over a range of exposure times, using the concatenation
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

300 ms.
0.078
0.889
±
0.041
0.89
±
0.021
0.901
±

400 ms.
0.933
0.942
0.916

±
±
±

0.063
0.031
0.018

600 ms.
0.921
0.957
0.937

±
±
±

0.067
0.026
0.016

1200 ms.
0.961
0.953
0.957

±
±
±

0.043
0.028
0.014

2400 ms.
0.057
0.948
±
0.025
0.96
±
0.015
0.945
±

3600 ms.
0.984
0.968
0.955

±
±
±

0.031
0.022
0.013

4800 ms.
0.964
0.947
0.96

±
±
0.013
±

0.049
0.029

Table 7: Accuracies of textures grouped by appearances, averaged over exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

Short (300-600 ms.)
0.914
0.04
±
0.93
0.019
±
0.919
±

0.011

Long (1200-4800 ms.)
0.023
0.964
0.013
0.957
0.007
0.954

±
±
±

All (300-4800 ms.)
0.022
0.943
0.011
0.946
0.006
0.939

±
±
±

Table 8: Accuracies of textures grouped by appearances, averaged over a range of exposure times, using the ﬂow decode
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

300 ms.
0.625
0.721

±
±

0.032
0.039

400 ms.
0.664
0.763

±
±

0.032
0.039

600 ms.
0.698
0.777

±
±

0.03
0.037

1200 ms.
0.741
0.885

0.028
0.028

±
±

2400 ms.
0.753
0.854

0.028
0.032

±
±

3600 ms.
0.762
0.902

0.028
0.026

±
±

4800 ms.
0.755
0.861

0.028
0.029

±
±

Table 9: Accuracies of textures grouped by dynamics, averaged over exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

Short (300-600 ms.)
0.018
0.663
0.022
0.753

±
±

Long (1200-4800 ms.)
0.014
0.753
0.015
0.876

±
±

All (300-4800 ms.)
0.011
0.715
0.013
0.823

±
±

Table 10: Accuracies of textures grouped by dynamics, averaged over a range of exposure times, using the concatenation
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

300 ms.
0.886
0.92

0.024

±
0.027

±

400 ms.
0.911
0.942

±
±

0.02
0.023

600 ms.
0.934
0.949

±
±

0.018
0.021

1200 ms.
0.947
0.974

0.016
0.016

±
±

2400 ms.
0.945
0.954

0.016
0.02

±
±

3600 ms.
0.955
0.966

0.014
0.017

±
±

4800 ms.
0.954
0.964

0.015
0.018

±
±

Table 11: Accuracies of textures grouped by dynamics, averaged over exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

Short (300-600 ms.)
0.012
0.911
0.013
0.937

±
±

Long (1200-4800 ms.)
0.95
±
0.964
±

0.008
0.009

All (300-4800 ms.)
0.007
0.934
0.008
0.953

±
±

Table 12: Accuracies of textures grouped by dynamics, averaged over a range of exposure times, using the ﬂow decode layer.
Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

300 ms.
0.661

±

400 ms.
0.699

±

600 ms.
0.726

±

1200 ms.
0.791

0.021

±

2400 ms.
0.788

0.022

±

3600 ms.
0.812

0.021

±

4800 ms.
0.793

0.021

±

0.025

0.025

0.023

Table 13: Average accuracy over all textures, averaged over exposure times, using the concatenation layer. Each texture
accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

Short (300-600 ms.)
0.014
0.695

Long (1200-4800 ms.)
0.011
0.796

All (300-4800 ms.)
0.009
0.754

±

±

±

Table 14: Average accuracy over all textures, averaged over a range of exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

300 ms.
0.898

±

0.018

0.015

400 ms.
0.922

±

600 ms.
0.94

±

0.013

1200 ms.
0.956

0.012

±

2400 ms.
0.948

0.013

±

3600 ms.
0.959

0.011

±

4800 ms.
0.957

0.012

±

Table 15: Average accuracy over all textures, averaged over exposure times, using the ﬂow decode layer. Each texture
accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

Short (300-600 ms.)
0.009
0.921

Long (1200-4800 ms.)
0.006
0.955

All (300-4800 ms.)
0.005
0.941

±

±

±

Table 16: Average accuracy over all textures, averaged over a range of exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Two-Stream Convolutional Networks for Dynamic Texture Synthesis

Matthew Tesfaldet Marcus A. Brubaker
Department of Electrical Engineering and Computer Science
York University, Toronto
{mtesfald,mab}@eecs.yorku.ca

Konstantinos G. Derpanis
Department of Computer Science
Ryerson University, Toronto
kosta@scs.ryerson.ca

8
1
0
2
 
r
p
A
 
2
1
 
 
]

V
C
.
s
c
[
 
 
4
v
2
8
9
6
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

We introduce a two-stream model for dynamic texture
synthesis. Our model is based on pre-trained convolutional
networks (ConvNets) that target two independent tasks: (i)
object recognition, and (ii) optical ﬂow prediction. Given
an input dynamic texture, statistics of ﬁlter responses from
the object recognition ConvNet encapsulate the per-frame
appearance of the input texture, while statistics of ﬁlter re-
sponses from the optical ﬂow ConvNet model its dynamics.
To generate a novel texture, a randomly initialized input se-
quence is optimized to match the feature statistics from each
stream of an example texture. Inspired by recent work on
image style transfer and enabled by the two-stream model,
we also apply the synthesis approach to combine the texture
appearance from one texture with the dynamics of another
to generate entirely novel dynamic textures. We show that
our approach generates novel, high quality samples that
match both the framewise appearance and temporal evo-
lution of input texture. Finally, we quantitatively evaluate
our texture synthesis approach with a thorough user study.

1. Introduction

Many common temporal visual patterns are naturally de-
scribed by the ensemble of appearance and dynamics (i.e.,
temporal pattern variation) of their constituent elements.
Examples of such patterns include ﬁre, ﬂuttering vegetation,
and wavy water. Understanding and characterizing these
temporal patterns has long been a problem of interest in hu-
man perception, computer vision, and computer graphics.
These patterns have been previously studied under a variety
of names, including turbulent-ﬂow motion [17], temporal
textures [30], time-varying textures [3], dynamic textures
[8], textured motion [45] and spacetime textures [7]. Here,
we adopt the term “dynamic texture”. In this work, we pro-
pose a factored analysis of dynamic textures in terms of ap-
pearance and temporal dynamics. This factorization is then
used to enable dynamic texture synthesis which, based on

Figure 1: Dynamic texture synthesis. (left) Given an input
dynamic texture as the target, our two-stream model is able
to synthesize a novel dynamic texture that preserves the tar-
get’s appearance and dynamics characteristics. (right) Our
two-stream approach enables synthesis that combines the
texture appearance from one target with the dynamics from
another, resulting in a composition of the two.

example texture inputs, generates a novel dynamic texture
instance. It also enables a novel form of style transfer where
the target appearance and dynamics can be taken from dif-
ferent sources as shown in Fig. 1.

Our model is constructed from two convolutional net-
works (ConvNets), an appearance stream and a dynamics
stream, which have been pre-trained for object recognition
and optical ﬂow prediction, respectively. Similar to previ-
ous work on spatial textures [13, 19, 33], we summarize an
input dynamic texture in terms of a set of spatiotemporal
statistics of ﬁlter outputs from each stream. The appear-
ance stream models the per frame appearance of the input
texture, while the dynamics stream models its temporal dy-
namics. The synthesis process consists of optimizing a ran-
domly initialized noise pattern such that its spatiotemporal
statistics from each stream match those of the input tex-
ture. The architecture is inspired by insights from human
perception and neuroscience. In particular, psychophysical
studies [6] show that humans are able to perceive the struc-
ture of a dynamic texture even in the absence of appearance
cues, suggesting that the two streams are effectively inde-

pendent. Similarly, the two-stream hypothesis [16] models
the human visual cortex in terms of two pathways, the ven-
tral stream (involved with object recognition) and the dorsal
stream (involved with motion processing).

In this paper, our two-stream analysis of dynamic tex-
tures is applied to texture synthesis. We consider a range
of dynamic textures and show that our approach generates
novel, high quality samples that match both the frame-wise
appearance and temporal evolution of an input example.
Further, the factorization of appearance and dynamics en-
ables a novel form of style transfer, where dynamics of one
texture are combined with the appearance of a different one,
cf . [14]. This can even be done using a single image as
an appearance target, which allows static images to be an-
imated. Finally, we validate the perceived realism of our
generated textures through an extensive user study.

2. Related work

There are two general approaches that have dominated
the texture synthesis literature: non-parametric sampling
approaches that synthesize a texture by sampling pixels of
a given source texture [10, 26, 37, 47], and statistical para-
metric models. As our approach is an instance of a para-
metric model, here we focus on these approaches.

The statistical characterization of visual textures was in-
troduced in the seminal work of Julesz [23]. He conjectured
that particular statistics of pixel intensities were sufﬁcient
to partition spatial textures into metameric (i.e., perceptu-
ally indistinguishable) classes. Later work leveraged this
notion for texture synthesis [19, 33]. In particular, inspired
by models of the early stages of visual processing, statistics
of (handcrafted) multi-scale oriented ﬁlter responses were
used to optimize an initial noise pattern to match the ﬁlter
response statistics of an input texture. More recently, Gatys
et al. [13] demonstrated impressive results by replacing the
linear ﬁlter bank with a ConvNet that, in effect, served as
a proxy for the ventral visual processing stream. Textures
are modelled in terms of the correlations between ﬁlter re-
sponses within several layers of the network. In subsequent
work, this texture model was used in image style transfer
[14], where the style of one image was combined with the
image content of another to produce a new image. Ruder et
al. [36] extended this model to video by using optical ﬂow
to enforce temporal consistency of the resulting imagery.

Variants of linear autoregressive models have been stud-
ied [42, 8] that jointly model appearance and dynamics of
the spatiotemporal pattern. More recent work has consid-
ered ConvNets as a basis for modelling dynamic textures.
Xie et al. [48] proposed a spatiotemporal generative model
where each dynamic texture is modelled as a random ﬁeld
deﬁned by multiscale, spatiotemporal ConvNet ﬁlter re-
sponses and dynamic textures are realized by sampling the
model. Unlike our current work, which assumes pretrained

ﬁxed networks, this approach requires the ConvNet weights
to be trained using the input texture prior to synthesis.

A recent preprint [12] described preliminary results ex-
tending the framework of Gatys et al. [13] to model and syn-
thesize dynamic textures by computing a Gram matrix of
ﬁlter activations over a small temporal window. In contrast,
our two stream ﬁltering architecture is more expressive as
our dynamics stream is speciﬁcally tuned to spatiotemporal
dynamics. Moreover, as will be demonstrated, the factoriza-
tion in terms of appearance and dynamics enables a novel
form of style transfer, where the dynamics of one pattern
are transferred to the appearance of another to generate an
entirely new dynamic texture. To the best of our knowledge,
we are the ﬁrst to demonstrate this form of style transfer.

The recovery of optical ﬂow from temporal imagery
has long been studied in computer vision.
Tradition-
ally, it has been addressed by handcrafted approaches e.g.,
[20, 29, 35]. Recently, ConvNet approaches [9, 34, 21, 49]
have been demonstrated as viable alternatives. Most closely
related to our approach are energy models of visual motion
[2, 18, 39, 31, 7, 25] that have been motivated and studied
in a variety of contexts, including computer vision, visual
neuroscience, and visual psychology. Given an input image
sequence, these models consist of an alternating sequence
of linear and non-linear operations that yield a distributed
representation (i.e., implicitly coded) of pixelwise optical
ﬂow. Here, an energy model motivates the representation of
observed dynamics which is then encoded as a ConvNet.

3. Technical approach

Our proposed two-stream approach consists of an ap-
pearance stream, representing the static (texture) appear-
ance of each frame, and a dynamics stream, representing
temporal variations between frames. Each stream consists
of a ConvNet whose activation statistics are used to charac-
terize the dynamic texture. Synthesizing a dynamic texture
is formulated as an optimization problem with the objective
of matching the activation statistics. Our dynamic texture
synthesis approach is summarized in Fig. 2 and the individ-
ual pieces are described in turn in the following sections.

3.1. Texture model: Appearance stream

The appearance stream follows the spatial texture model
introduced by Gatys et al. [13] which we brieﬂy review
here. The key idea is that feature correlations in a Con-
vNet trained for object recognition capture texture appear-
ance. We use the same publicly available normalized VGG-
19 network [40] used by Gatys et al. [13].

To capture the appearance of an input dynamic texture,
we ﬁrst perform a forward pass with each frame of the im-
age sequence through the ConvNet and compute the feature
RNl×Ml , for various levels in the net-
activations, Alt
work, where Nl and Ml denote the number of ﬁlters and

∈

ture suited for computing optical ﬂow (e.g., [9, 21]) which
is naturally differentiable. However, with most such mod-
els it is unclear how invariant their layers are to appearance.
Instead, we propose a novel network architecture which is
motivated by the spacetime-oriented energy model [7, 39].
In motion energy models, the velocity of image content
(i.e., motion) is interpreted as a three-dimensional orienta-
tion in the x-y-t spatiotemporal domain [2, 11, 18, 39, 46].
In the frequency domain, the signal energy of a translating
pattern can be shown to lie on a plane through the origin
where the slant of the plane is deﬁned by the velocity of
the pattern. Thus, motion energy models attempt to identify
this orientation-plane (and hence the patterns velocity) via
a set of image ﬁltering operations. More generally the con-
stituent spacetime orientations for a spectrum of common
visual patterns (including translation and dynamic textures)
can serve as a basis for describing the temporal variation of
an image sequence [7]. This suggests that motion energy
models may form an ideal basis for our dynamics stream.

Speciﬁcally, we use the spacetime-oriented energy
model [7, 39] to motivate our network architecture which
we brieﬂy review here; see [7] for a more in-depth descrip-
tion. Given an input video, a bank of oriented 3D ﬁlters
are applied which are sensitive to a range of spatiotemporal
orientations. These ﬁlter activations are rectiﬁed (squared)
and pooled over local regions to make the responses robust
to the phase of the input signal, i.e., robust to the alignment
of the ﬁlter with the underlying image structure. Next, ﬁl-
ter activations consistent with the same spacetime orienta-
tion are summed. These responses provide a pixelwise dis-
tributed measure of which orientations (frequency domain
planes) are present in the input. However, these responses
are confounded by local image contrast that makes it dif-
ﬁcult to determine whether a high response is indicative
of the presence of a spacetime orientation or simply due
to high image contrast. To address this ambiguity, an L1
normalization is applied across orientation responses which
results in a representation that is robust to local appearance
variations but highly selective to spacetime orientation.

Using this model as our basis, we propose the follow-
ing fully convolutional network [38]. Our ConvNet in-
put is a pair of temporally consecutive greyscale images.
Each input pair is ﬁrst normalized to have zero-mean and
unit variance. This step provides a level of invariance to
overall brightness and contrast, i.e., global additive and
multiplicative signal variations. The ﬁrst layer consists of
32 3D spacetime convolution ﬁlters of size 11
2
width
(height
time). Next, a squaring activation function
and 5
5 spatial max-pooling (with a stride of one) is ap-
plied to make the responses robust to local signal phase. A
1 convolution layer follows with 64 ﬁlters that combines
1
energy measurements that are consistent with the same ori-
entation. Finally, to remove local contrast dependence, an

×
×

11

×

×

×

×

Figure 2: Two-stream dynamic texture generation. Sets of
Gram matrices represent a texture’s appearance and dynam-
ics. Matching these statistics allows for the generation of
novel textures as well as style transfer between textures.

(cid:80)T

∈
t=1

ij =

1
T NlMl

the number of spatial locations of layer l at time t, respec-
tively. The correlations of the ﬁlter responses in a particular
layer are averaged over the frames and encapsulated by a
RNl×Nl , whose entries are given by
Gram matrix, Gl
(cid:80)Ml
ikAlt
Gl
k=1 Alt
jk, where T denotes the
number of input frames and Alt
ik denotes the activation of
feature i at location k in layer l on the target frame t. The
synthesized texture appearance is similarly represented by
a Gram matrix, ˆGlt
RNl×Nl , whose activations are given
ˆAlt
ˆAlt
by ˆGlt
ij = 1
ik denotes the acti-
ik
vation of feature i at location k in layer l on the synthesized
frame t. The appearance loss,
appearance, is then deﬁned as
the temporal average of the mean squared error between the
Gram matrix of the input texture and that of the generated
texture computed at each frame:

jk, where ˆAlt

∈
(cid:80)Ml
k=1

NlMl

L

appearance =

L

1
LappTout

Tout(cid:88)

(cid:88)

t=1

l

Gl

(cid:107)

ˆGlt

2
F ,

(cid:107)

−

(1)

where Lapp is the number of layers used to compute Gram
matrices, Tout is the number of frames being generated in
the output, and
(cid:107) · (cid:107)F is the Frobenius norm. Consistent
with previous work [13], we compute Gram matrices on the
following layers: conv1 1, pool1, pool2, pool3, and pool4.

3.2. Texture model: Dynamics stream

There are three primary goals in designing our dynamics
stream. First, the activations of the network must represent
the temporal variation of the input pattern. Second, the acti-
vations should be largely invariant to the appearance of the
images which should be characterized by the appearance
stream described above. Finally, the representation must be
differentiable to enable synthesis. By analogy to the ap-
pearance stream, an obvious choice is a ConvNet architec-

in layer l on the target frames t and t + 1. The dynam-
ics of the synthesized texture is represented by a Gram ma-
trix of ﬁlter response correlations computed separately for
each pair of frames, ˆGlt
ij =
1
ik denotes the activation of
NlMl
feature i at location k in layer l on the synthesized frames t
and t + 1. The dynamics loss,
dynamics, is deﬁned as the av-
L
erage of the mean squared error between the Gram matrices
of the input texture and those of the generated texture:

RNl×Nl , with entries ˆGlt

∈
jk, where ˆDlt

(cid:80)Ml
k=1

ˆDlt
ik

ˆDlt

dynamics =

L

1
Ldyn(Tout

Tout−1
(cid:88)

(cid:88)

1)

−

t=1

l

Gl

(cid:107)

ˆGlt

2
F ,

(cid:107)

−

(2)

where Ldyn is the number of ConvNet layers being used in
the dynamics stream.

Here we propose to use the output of the concatenation
layer, where the multiscale distributed representation of ori-
entations is stored, as the layer to compute the Gram ma-
trix. While it is tempting to use the predicted ﬂow out-
put from the network, this generally yields poor results as
shown in our evaluation. Due to the complex, temporal vari-
ation present in dynamic textures, they contain a variety of
local spacetime orientations rather than a single dominant
orientation. As a result, the ﬂow estimates will tend to be
an average of the underlying orientation measurements and
consequently not descriptive. A comparison between the
texture synthesis results using the concatenation layer and
the predicted ﬂow output is provided in Sec. 4.

3.3. Texture generation

The overall dynamic texture loss consists of the combi-
nation of the appearance loss, Eq. (1), and the dynamics
loss, Eq. (2):

dynamic texture = α

appearance + β

dynamics,

(3)

L

L

L

where α and β are the weighting factors for the appearance
and dynamics content, respectively. Dynamic textures are
implicitly deﬁned as the (local) minima of this loss. Tex-
tures are generated by optimizing Eq. (3) with respect to
the spacetime volume, i.e., the pixels of the video. Vari-
ations in the resulting texture are found by initializing the
optimization process using IID Gaussian noise. Consistent
with previous work [13], we use L-BFGS [28] optimization.
Naive application of the outlined approach will consume
increasing amounts of memory as the temporal extent of the
dynamic texture grows; this makes it impractical to gener-
ate longer sequences. Instead, long sequences can be in-
crementally generated by separating the sequence into sub-
sequences and optimizing them sequentially. This is real-
ized by initializing the ﬁrst frame of a subsequence as the
last frame from the previous subsequence and keeping it
ﬁxed throughout the optimization. The remaining frames

Figure 3: Dynamics stream ConvNet. The ConvNet is based
on a spacetime-oriented energy model [7, 39] and is trained
for optical ﬂow prediction. Three scales are shown for il-
lustration; in practice ﬁve scales were used.

L1 divisive normalization is applied.

To capture spacetime orientations beyond those capable
with the limited receptive ﬁelds used in the initial layer,
we compute a ﬁve-level spatial Gaussian pyramid. Each
pyramid level is processed independently with the same
spacetime-oriented energy model and then bilinearly up-
sampled to the original resolution and concatenated.

Prior energy model instantiations (e.g., [2, 7, 39]) used
handcrafted ﬁlter weights. While a similar approach could
be followed here, we opt to learn the weights so that they
are better tuned to natural imagery. To train the network
weights, we add additional decoding layers that take the
concatenated distributed representation and apply a 3
3
convolution (with 64 ﬁlters), ReLU activation, and a 1
1
convolution (with 2 ﬁlters) that yields a two channel output
encoding the optical ﬂow directly. The proposed architec-
ture is illustrated in Fig. 3.

×
×

For training, we use the standard average endpoint er-
ror (aEPE) ﬂow metric (i.e., L2 norm) between the pre-
dicted ﬂow and the ground truth ﬂow as the loss. Since no
large-scale ﬂow dataset exists that captures natural imagery
with groundtruth ﬂow, we take an unlabeled video dataset
and apply an existing ﬂow estimator [35] to estimate opti-
cal ﬂow for training, cf . [43]. For training data, we used
videos from the UCF101 dataset [41] with geometric and
photometric data augmentations similar to those used by
FlowNet [9], and optimized the aEPE loss using Adam [24].
Inspection of the learned ﬁlters in the initial layer showed
evidence of spacetime-oriented ﬁlters, consistent with the
handcrafted ﬁlters used in previous work [7].

Similar to the appearance stream, ﬁlter response cor-
relations in a particular layer of the dynamics stream are
averaged over the number of image frame pairs and en-
RNl×Nl , whose en-
capsulated by a Gram matrix, Gl
tries are given by Gl
ikDlt
k=1 Dlt
jk,
where Dlt
ik denotes the activation of feature i at location k

1
(T −1)NlMl

∈
(cid:80)T −1
t=1

ij =

(cid:80)Ml

of the subsequence are initialized randomly and optimized
as above. This ensures temporal consistency across synthe-
sized subsequences and can be viewed as a form of coordi-
nate descent for the full sequence objective. The ﬂexibility
of this framework allows other texture generation problems
to be handled simply by altering the initialization of frames
and controlling which frames or frame regions are updated.

4. Experimental results

The goal of (dynamic) texture synthesis is to gener-
ate samples that are indistinguishable from the real input
target texture by a human observer.
In this section, we
present a variety of synthesis results including a user study
to quantitatively evaluate the realism of our results. Given
their temporal nature, our results are best viewed as videos.
Our two-stream architecture was implemented using Ten-
sorFlow [1]. Results were generated using an NVIDIA Ti-
tan X (Pascal) GPU and synthesis times ranged between
one to three hours to generate 12 frames with an image
resolution of 256
256. For our full synthesis results
and source code, please refer to the supplemental material
on the project website: ryersonvisionlab.github.
io/two-stream-projpage.

×

4.1. Dynamic texture synthesis

We applied our dynamic texture synthesis process to a
wide range of textures which were selected from the Dyn-
Tex [32] database and others we collected in the wild. In-
cluded in our supplemental material are synthesized results
of nearly 60 different textures that encapsulate a range of
phenomena, such as ﬂowing water, waves, clouds, ﬁre, rip-
pling ﬂags, waving plants, and schools of ﬁsh. Some sam-
ple frames are shown in Fig. 4 but we encourage readers to
view the videos to fully appreciate the results. In addition,
we performed a comparison with [12] and [48]. Generally,
we found our results to be qualitatively comparable or better
than these methods. See the supplemental for more details
on the comparisons with these methods.

We also generated dynamic textures incrementally, as
described in Sec. 3.3. The resulting textures were perceptu-
ally indistinguishable from those generated with the batch
process. Another extension that we explored were textures
with no discernible temporal seam between the last and ﬁrst
frames. Played as a loop, these textures appear to be tempo-
rally endless. This was achieved by assuming that the ﬁrst
frame follows the ﬁnal frame and adding an additional loss
for the dynamics stream evaluated on that pair of frames.

Example failure modes of our method are presented
in Fig. 6.
In general, we ﬁnd that most failures result
from inputs that violate the underlying assumption of a
dynamic texture, i.e., the appearance and/or dynamics are
not spatiotemporally homogeneous.
In the case of the
escalator example, the long edge structures in the ap-

pearance are not spatially homogeneous, and the dynam-
ics vary due to perspective effects that change the motion
from downward to outward. The resulting synthesized tex-
ture captures an overall downward motion but lacks the per-
spective effects and is unable to consistently reproduce the
long edge structures. This is consistent with previous ob-
servations on static texture synthesis [13] and suggests it is
a limitation of the appearance stream.

Another example is the flag sequence where the rip-
pling dynamics are relatively homogeneous across the pat-
tern but the appearance varies spatially. As expected, the
generated texture does not faithfully reproduce the appear-
ance; however, it does exhibit plausible rippling dynamics.
In the supplemental material, we include an additional fail-
ure case, cranberries, which consists of a swirling pat-
tern. Our model faithfully reproduces the appearance but is
unable to capture the spatially varying dynamics. Interest-
ingly, it still produces a result which is statistically indistin-
guishable from real in our user study discussed below.

Appearance vs. dynamics streams We sought to verify
that the appearance and dynamics streams were capturing
complementary information. To validate that the texture
generation of multiple frames would not induce dynamics
consistent with the input, we generated frames starting from
randomly generated noise but only using the appearance
statistics and corresponding loss, i.e., Eq. 1. As expected,
this produced frames that were valid textures but with no
coherent dynamics present. Results for a sequence contain-
ing a school of ﬁsh are shown in Fig. 5; to examine the
dynamics, see fish in the supplemental material.

Similarly, to validate that the dynamics stream did not in-
advertently include appearance information, we generated
videos using the dynamics loss only, i.e., Eq. 2. The re-
sulting frames had no visible appearance and had an ex-
tremely low dynamic range, i.e., the standard deviation of
pixel intensities was 10 for values in [0, 255]. This indi-
cates a general invariance to appearance and suggests that
our two-stream dynamic texture representation has factored
appearance and dynamics, as desired.

4.2. User study

Quantitative evaluation for texture synthesis is a partic-
ularly challenging task as there is no single correct output
when synthesizing new samples of a texture. Like in other
image generation tasks (e.g., rendering), human perception
is ultimately the most important measure. Thus, we per-
formed a user study to evaluate the perceived realism of our
synthesized textures.

Similar to previous image synthesis work (e.g., [5]), we
conducted a perceptual experiment with human observers
to quantitatively evaluate our synthesis results. We em-
ployed a forced-choice evaluation on Amazon Mechanical

fireplace 1

(original)

fireplace 1

(synthesized)

lava

(original)

lava

(synthesized)

smoke 1

(original)

smoke 1

(synthesized)

underwater

vegetation 1

(original)

underwater
vegetation 1

(synthesized)

water 3

(original)

water 3

(synthesized)

Figure 4: Dynamic texture synthesis success examples. Names correspond to ﬁles in the supplemental material.

Turk (AMT) with 200 different users. Each user performed
59 pairwise comparisons between a synthesized dynamic
texture and its target. Users were asked to choose which
appeared more realistic after viewing the textures for an ex-
posure time sampled randomly from discrete intervals be-
tween 0.3 and 4.8 seconds. Measures were taken to control
the experimental conditions and minimize the possibility of
low quality data. See the supplemental material for further
experimental details of our user study.

For comparison, we constructed a baseline by using the
ﬂow decode layer in the dynamics loss of Eq. 2. This corre-
sponds with attempting to mimic the optical ﬂow statistics
of the texture directly. Textures were synthesized with this
model and the user study was repeated with an additional
200 users. To differentiate between the models, we label

“Flow decode layer” and “Concat layer” in the ﬁgures to
describe our baseline and ﬁnal model, respectively.

The results of this study are summarized in Fig. 7 which
shows user accuracy in differentiating real versus generated
textures as a function of time for both methods. Over-
all, users are able to correctly identify the real texture
2.5% of the time for brief exposures of 0.3 sec-
66.1%
onds. This rises to 79.6%
1.1% with exposures of 1.2 sec-
onds and higher. Note that “perfect” synthesis results would
have an accuracy of 50%, indicating that users were unable
to differentiate between the real and generated textures and
higher accuracy indicating less convincing textures.

±

±

The results clearly show that the use of the concatenation
layer activations is far more effective than the ﬂow decode
layer. This is not surprising as optical ﬂow alone is known

target
(fish)

appearance
only

both
streams

escalator

(original)

escalator

(synthesized)

flag

(original)

flag

(synthesized)

(top row) Target texture.

Figure 5: Dynamic texture synthesis versus texture synthe-
sis.
(middle) Texture synthesis
without dynamics constraints shows consistent per-frame
appearance but no temporal coherence. (bottom) Including
both streams induces consistent appearance and dynamics.

Figure 6: Dynamic texture synthesis failure examples. In
these cases, the failures are attributed to either the appear-
ance or the dynamics not being homogeneous.

to be unreliable on many textures, particularly those with
transparency or chaotic motion (e.g., water, smoke, ﬂames,
etc.). Also evident in these results is the time-dependant
nature of perception for textures from both models. Users’
ability to identify the generated texture improved as expo-
sure times increased to 1.2 seconds and remained relatively
ﬂat for longer exposures.

To better understand the performance of our approach,
we grouped and analyzed the results in terms of appear-
ance and dynamics characteristics. For appearance we used
the taxonomy presented in [27] and grouped textures as
either regular/near-regular (e.g., periodic tiling and brick
wall), irregular (e.g., a ﬁeld of ﬂowers), or stochastic/near-
stochastic (e.g.,
For dynamics we
grouped textures as either spatially-consistent (e.g., closeup
of rippling sea water) or spatially-inconsistent (e.g., rippling
sea water juxtaposed with translating clouds in the sky). Re-

tv static or water).

Figure 7: Time-limited pairwise comparisons across all tex-
tures with 95% statistical conﬁdence intervals.

sults based on these groupings can be seen in Fig. 8.

1.6% and 90.8%

A full breakdown of the user study results by texture and
grouping can be found in the supplemental material. Here
we discuss some of the overall trends. Based on appear-
ance it is clear that textures with large-scale spatial consis-
tencies (regular, near-regular, and irregular textures) tend to
perform poorly. Examples being flag and fountain 2
with user accuracies of 98.9%
4.3%
averaged across all exposures, respectively. This is not un-
expected and is a fundamental limitation of the local na-
ture of the Gram matrix representation used in the appear-
ance stream which was observed in static texture synthe-
sis [13]. In contrast, stochastic and near-stochastic textures
performed signiﬁcantly better as their smaller-scale local
variations are well captured by the appearance stream, for
instance water 1 and lava which had average accuracies
of 53.8%
7.4%, respectively, making
them both statistically indistinguishable from real.

7.4% and 55.6%

±

±

±

±

±

(e.g.,

dynamics

7.4% and 63.2%

In terms of dynamics, we ﬁnd that

textures with
tv static,
spatially-consistent
water *, and calm water *) perform signiﬁcantly
better than those with spatially-inconsistent dynamics (e.g.,
candle flame, fountain 2, and snake *), where
the dynamics drastically differ across spatial locations.
For example, tv static and calm water 6 have
average accuracies of 48.6%
7.2%,
respectively, while candle flame and snake 5 have
average accuracies of 92.4%
4%,
respectively. Overall, our model is capable of reproducing
a full spectrum of spatially-consistent dynamics. However,
as the appearance shifts from containing small-scale spatial
consistencies,
to containing large-scale
consistencies
performance degrades. This was evident in the user study
where the best-performing textures typically consisted of
a stochastic or near-stochastic appearance with spatially-
In contrast the worst-performing
consistent dynamics.
textures consisted of regular, near-regular, or irregular
appearance with spatially-inconsistent dynamics.

4% and 92.1%

±

±

±

appearance
target

synthesized output

Figure 9: Dynamics style transfer. (top row) Appearance of
still water was used with the dynamics of a different water
dynamic texture (water 4). (bottom row) The appearance
of a painting of ﬁre was used with the dynamics of a real
ﬁre (fireplace 1). Animated results and additional ex-
amples are available in the supplemental material.

ance and dynamics. We applied this model to a variety of
dynamic texture synthesis tasks and showed that, so long
as the input textures are generally true dynamic textures,
i.e., have spatially invariant statistics and spatiotemporally
invariant dynamics, the resulting synthesized textures are
compelling. This was validated both qualitatively and quan-
titatively through a large user study. Further, we showed
that the two-stream model enabled dynamics style transfer,
where the appearance and dynamics information from dif-
ferent sources can be combined to generate a novel texture.
We have explored this model thoroughly and found a few
limitations which we leave as directions for future work.
First, much like has been reported in recent image style
transfer work [14], we have found that high frequency noise
and chromatic aberrations are a problem in generation. An-
other issue that arises is the model fails to capture textures
with spatially-variant appearance, (e.g., flag in Fig. 6) and
spatially-inconsistent dynamics (e.g., escalator in Fig.
6). By collapsing the local statistics into a Gram matrix,
the spatial and temporal organization is lost. Simple post-
processing methods may alleviate some of these issues but
we believe that they also point to a need for a better rep-
resentation. Beyond addressing these limitations, a natural
next step would be to extend the idea of a factorized rep-
resentation into feed-forward generative networks that have
found success in static image synthesis, e.g., [22, 44].

Acknowledgements MT is supported by a Natural Sci-
ences and Engineering Research Council of Canada
(NSERC) Canadian Graduate Scholarship. KGD and MAB
are supported by NSERC Discovery Grants. This research
was undertaken as part of the Vision: Science to Applica-
tions program, thanks in part to funding from the Canada
First Research Excellence Fund.

Figure 8: Time-limited pairwise comparisons across all tex-
tures, grouped by appearance (top) and dynamics (bottom).
Shown with 95% statistical conﬁdence intervals.

4.3. Dynamics style transfer

The underlying assumption of our model is that appear-
ance and dynamics of texture can be factorized. As such, it
should allow for the transfer of the dynamics of one texture
onto the appearance of another. This has been explored pre-
viously for artistic style transfer [4, 15] with static imagery.
We accomplish this with our model by performing the same
optimization as above, but with the target Gram matrices for
appearance and dynamics computed from different textures.
A dynamics style transfer result is shown in Fig. 9 (top),
using two real videos. Additional examples are available
in the supplemental material. We note that when perform-
ing dynamics style transfer it is important that the appear-
ance structure be similar in scale and semantics, otherwise,
the generated dynamic textures will look unnatural. For in-
stance, transferring the dynamics of a ﬂame onto a water
scene will generally produce implausible results.

We can also apply the dynamics of a texture to a static
input image, as the target Gram matrices for the appearance
loss can be computed on just a single frame. This allows us
to effectively animate regions of a static image. The result
of this process can be striking and is visualized in Fig. 9
(bottom), where the appearance is taken from a painting and
the dynamics from a real world video.

5. Discussion and summary

In this paper, we presented a novel, two-stream model of
dynamic textures using ConvNets to represent the appear-

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 5
[2] E. H. Adelson and J. R. Bergen. Spatiotemporal energy mod-
els for the perception of motion. JOSA–A, 2(2):284–299,
1985. 2, 3, 4

[3] Z. Bar-Joseph, R. El-Yaniv, D. Lischinski, and M. Werman.
Texture mixing and texture movie synthesis using statistical
learning. T-VCG, 7(2):120–135, 2001. 1

[4] A. J. Champandard. Semantic style transfer and turning two-
bit doodles into ﬁne artworks. arXiv:1603.01768, 2016. 8
[5] Q. Chen and V. Koltun. Photographic image synthesis with

cascaded reﬁnement networks. In ICCV, 2017. 5

[6] J. E. Cutting. Blowing in the wind: Perceiving structure in

trees and bushes. Cognition, 12(1):25 – 44, 1982. 1

[7] K. G. Derpanis and R. P. Wildes. Spacetime texture represen-
tation and recognition based on a spatiotemporal orientation
analysis. PAMI, 34(6):1193–1205, 2012. 1, 2, 3, 4

[8] G. Doretto, A. Chiuso, Y. N. Wu, and S. Soatto. Dynamic

textures. IJCV, 51(2):91–109, 2003. 1, 2

[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. H¨ausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
FlowNet: Learning optical ﬂow with convolutional net-
works. In ICCV, pages 2758–2766, 2015. 2, 3, 4

[10] A. A. Efros and T. K. Leung. Texture synthesis by non-

parametric sampling. In ICCV, pages 1033–1038, 1999. 2

[11] M. Fahle and T. Poggio. Visual hyperacuity: Spatiotemporal
interpolation in human vision. Proceedings of the Royal So-
ciety of London B: Biological Sciences, 213(1193):451–477,
1981. 3

[12] C. M. Funke, L. A. Gatys, A. S. Ecker, and M. Bethge. Syn-
thesising dynamic textures using convolutional neural net-
works. arXiv:1702.07006, 2017. 2, 5, 10, 11

[13] L. A. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis
using convolutional neural networks. In NIPS, pages 262–
270, 2015. 1, 2, 3, 4, 5, 7

[14] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks. In CVPR, pages 2414–
2423, 2016. 2, 8

[15] L. A. Gatys, A. S. Ecker, M. Bethge, A. Hertzmann, and
E. Shechtman. Controlling perceptual factors in neural style
transfer. In CVPR, 2017. 8

[16] M. A. Goodale and A. D. Milner. Separate visual path-
ways for perception and action. Trends in Neurosciences,
15(1):20–25, 1992. 2

[18] D. J. Heeger. Optical ﬂow using spatiotemporal ﬁlters. IJCV,

1(4):279–302, 1988. 2, 3

[19] D. J. Heeger and J. R. Bergen. Pyramid-based texture analy-
sis/synthesis. In SIGGRAPH, pages 229–238, 1995. 1, 2
[20] B. K. P. Horn and B. G. Schunck. Determining optical ﬂow.

A.I., 17:185–203, 1981. 2

[21] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet 2.0: Evolution of optical ﬂow estimation
with deep networks. In CVPR, 2017. 2, 3

[22] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution. In ECCV, pages
694–711, 2016. 8

[23] B. Julesz. Visual pattern discrimination. IRE Trans. Infor-

mation Theory, 8(2):84–92, 1962. 2

[24] D. P. Kingma and J. Ba. Adam: A Method for Stochastic

Optimization. arXiv:1412.6980, 2014. 4

[25] K. Konda, R. Memisevic, and V. Michalski. Learning to en-
code motion using spatio-temporal synchrony international
conference on learning representation. In ICLR, 2014. 2
[26] V. Kwatra, A. Sch¨odl, I. Essa, G. Turk, and A. Bobick.
Graphcut textures: Image and video synthesis using graph
cuts. In SIGGRAPH, pages 277–286, 2003. 2

[27] W.-C. Lin, J. Hays, C. Wu, Y. Liu, and V. Kwatra. Quantita-
tive evaluation of near regular texture synthesis algorithms.
In CVPR, volume 1, pages 427–434, 2006. 7

[28] D. C. Liu and J. Nocedal. On the limited memory method
for large scale optimization. Mathematical Programming,
45(3):503–528, 1989. 4

[29] B. D. Lucas and T. Kanade. An iterative image registra-
tion technique with an application to stereo vision. In IJCAI,
pages 674–679, 1981. 2

[30] R. Nelson and R. Polana. Qualitative recognition of motion

using temporal textures. CVGIP, 56(1), 1992. 1

[31] S. Nishimoto and J. L. Gallant. A three-dimensional spa-
tiotemporal receptive ﬁeld model explains responses of area
mt neurons to naturalistic movies. Journal of Neuroscience,
31(41):14551–14564, 2011. 2

[32] R. P´eteri, S. Fazekas, and M. J. Huiskes. DynTex: A Com-
prehensive Database of Dynamic Textures. PRL, 31(12),
2010. 5

[33] J. Portilla and E. P. Simoncelli. A parametric texture model
based on joint statistics of complex wavelet coefﬁcients.
IJCV, 40(1):49–70, 2000. 1, 2

[34] A. Ranjan and M. J. Black. Optical Flow Estimation using a

Spatial Pyramid Network. In CVPR, 2017. 2

[35] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-preserving interpolation of correspon-
dences for optical ﬂow. In CVPR, pages 1164–1172, 2015.
2, 4

[36] M. Ruder, A. Dosovitskiy, and T. Brox. Artistic style transfer

for videos. In GCPR, pages 26–36, 2016. 2

[37] A. Sch¨odl, R. Szeliski, D. Salesin, and I. A. Essa. Video

textures. In SIGGRAPH, pages 489–498, 2000. 2

[17] D. Heeger and A. Pentland. Seeing structure through chaos.
In IEEE Motion Workshop: Representation and Analysis,
pages 131–136, 1986. 1

[38] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional
networks for semantic segmentation. PAMI, 39(4):640–651,
2017. 3

[39] E. P. Simoncelli and D. J. Heeger. A model of neuronal re-
sponses in visual area MT. Vision Research, 38(5):743 – 761,
1998. 2, 3, 4

[40] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014. 2

[41] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A
dataset of 101 human actions classes from videos in the wild.
arXiv:1212.0402, 2012. 4

[42] M. Szummer and R. W. Picard. Temporal texture modeling.

In ICIP, pages 823–826, 1996. 2

[43] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and
M. Paluri. Deep end2end voxel2voxel prediction. In CVPR
Workshops, pages 402–409, 2016. 4

[44] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. S. Lempitsky.
Texture networks: Feed-forward synthesis of textures and
stylized images. In ICML, pages 1349–1357, 2016. 8
[45] Y. Wang and S. C. Zhu. Modeling textured motion: Particle,

wave and sketch. In ICCV, pages 213–220, 2003. 1

[46] A. B. Watson and A. J. Ahumada. A look at motion in the
frequency domain. In Motion workshop: Perception and rep-
resentation, pages 1–10, 1983. 3

[47] L. Wei and M. Levoy. Fast texture synthesis using tree-
structured vector quantization. In SIGGRAPH, pages 479–
488, 2000. 2

[48] J. Xie, S.-C. Zhu, and Y. N. Wu. Synthesizing dynamic
patterns by spatial-temporal generative convnet. In CVPR,
2017. 2, 5, 11

[49] J. J. Yu, A. W. Harley, and K. G. Derpanis. Back to Ba-
sics: Unsupervised Learning of Optical Flow via Brightness
Constancy and Motion Smoothness. In ECCVW, 2016. 2

A. Experimental procedure

Here we provide further experimental details of our user study
using Amazon Mechanical Turk (AMT). Experimental trials were
grouped into batches of Human Intelligence Tasks (HITs) for users
to complete. Each HIT consisted of 59 pairwise comparisons be-
tween a synthesized dynamic texture and its target. Users were
asked to choose which texture appeared more realistic after view-
ing each texture independently for an exposure time (in seconds)
sampled randomly from the set {0.3, 0.4, 0.6, 1.2, 2.4, 3.6, 4.8}.
Note that 12 frames of the dynamic texture corresponds to 1.2 sec-
onds, i.e., 10 frames per second. Before viewing a dynamic tex-
ture, a centred dot is ﬂashed twice to indicate to the user where
to look (left or right). To prepare users for the task, the ﬁrst three
comparisons were used for warm-up, exposing them to the short-
est (0.3s), median (1.2s), and longest (4.8s) durations. To prevent
spamming and bias, we constrained the experiment as follows:
users could make a choice only after both dynamic textures were
shown; the next texture comparison could only be made after a
decision was made for the current comparison; a choice could not
be changed after the next pair of dynamic textures were shown;
and users were each restricted to a single HIT. Obvious unrealistic
dynamic textures were synthesized by terminating synthesis early
(100 iterations) and were used as sentinel tests. Three of the 59
pairwise comparisons were sentinels and results from users which
gave incorrect answers on any of the sentinel comparisons were

not used. The left-right order of textures within a pair, display
order within a pair, and order of pairs within a HIT, were random-
ized. An example of a HIT is shown in a video included with the
supplemental on the project page: HIT example.mp4.

Users were paid $2 USD per HIT, and were required to have
at least a 98% HIT approval rating, greater than or equal to 5000
HITs approved, and to be residing in the US. We collected results
from 200 unique users to evaluate our ﬁnal model and another 200
to evaluate our baseline model.

B. Qualitative results

We provide videos showcasing the qualitative results of our
two-stream model, including the experiments mentioned in the
main manuscript, on our project page: ryersonvisionlab.
github.io/two-stream-projpage. The videos are in
MP4 format (H.264 codec) and are best viewed in a loop. They
are enclosed in the following folders:

• target textures: This folder contains the 59 dynamic

textures used as targets for synthesis.

• dynamic texture synthesis: This folder contains
synthesized dynamic textures where the appearance and dy-
namics targets are the same.

• using concatenation layer: This folder contains
synthesized dynamic textures where the concatenation layer
was used for computing the Gramian on the dynamics
stream. These are the results from our ﬁnal model.

• using flow decode layer: This folder contains syn-
thesized dynamic textures where the predicted ﬂow output
is used for computing the Gramian on the dynamics stream.
These are the results from our baseline.

• full synthesis:

This

folder

contains

synthesized dynamic textures,
generated, nor temporally-endless, etc.

i.e., not

regularly-
incrementally-

• appearance stream only: This folder contains dy-
namic textures synthesized using only the appearance stream
of our two-stream model. The dynamics stream is not used.

• incrementally generated: This folder contains dy-
namic textures synthesized using the incremental process
outlined in Section 3.3 in the main manuscript.

• temporally endless: This folder contains a synthe-
sized dynamic texture (smoke plume 1) where there is no
discernible temporal seam between the last and ﬁrst frames.
Played as a loop, it appears to be temporally endless, thus, it
is presented in animated GIF format.

• dynamics style transfer: This folder contains syn-
thesized dynamic textures where the appearance and dynam-
ics targets are different. Also included are videos where the
synthesized dynamic texture is “pasted” back onto the origi-
nal image it was cropped from, showing a proof-of-concept
of dynamics style transfer as an artistic tool.

• comparisons/funke: This folder contains four dy-
namic texture synthesis comparisons between our model and
a recent (unpublished) approach [12]. The dynamic textures

(labeled “Xie et al. (ST)”) designed for dynamic textures with both
spatial and temporal homogeneity, and their temporal model (la-
beled “Xie et al. (FC)”) designed for dynamic textures with only
temporal homogeneity.

Overall, we demonstrate that our results appear qualitatively
better, showing more temporal coherence and similarity in dy-
namics and fewer artifacts, e.g., blur and ﬂicker. This may be a
natural consequence of their limited representation of dynamics.
Although the spatiotemporal model of Xie et al. [48] is able to
synthesize dynamic textures that lack spatial homogeneity (e.g.,
bamboo and escalator), we note that their method can not
synthesize novel dynamic textures, i.e., it appears to faithfully re-
produce the target texture, reducing the applicability of their ap-
proach.

As a consequence of jointly modelling appearance and dynam-
ics, the methods of [12, 48] are not capable of the novel form of
style transfer we demonstrated. This was enabled by the factored
representation of dynamics and appearance. Furthermore, the spa-
tiotemporal extent of the output sequence generated by Xie et al.’s
[48] method is limited to being equal to the input. The proposed
approach does not share this limitation.

[12] which ex-
chosen are those reported by Funke et al.
hibit spatiotemporal homogeneity. For ease of comparison,
we have concatenated the results from both models with their
corresponding targets.

• comparisons/xie and funke: This folder contains
nine dynamic texture synthesis comparisons between our
model, Funke et al.’s [12], and Xie et al.’s [48]. The dynamic
textures chosen cover the full range of our appearance and
dynamics groupings. For ease of comparison, we have con-
catenated the results from all models with their correspond-
ing targets.

C. Full user study results

Figures 10a and 10b show histograms of the average user ac-
curacy on each texture, averaged over a range of exposure times.
The histogram bars are ordered from lowest to highest accuracy,
based on the results when using our ﬁnal model.

Tables 1 and 2 show the average user accuracy on each texture
when using our ﬁnal model. The results are averaged over expo-
sure times. Similarly, Tables 3 and 4 show the results when using
our baseline.

Tables 5 and 6 show the average user accuracy on texture ap-
pearance groups when using our ﬁnal model. The results are av-
eraged over exposure times. Similarly, Tables 7 and 8 show the
results when using our baseline.

Tables 9 and 10 show the average user accuracy on texture dy-
namics groups when using our ﬁnal model. The results are aver-
aged over exposure times. Similarly, Tables 11 and 12 show the
results when using our baseline.

Tables 13 and 14 show the average user accuracy over all tex-
tures when using our ﬁnal model. The results are averaged over
exposure times. Similarly, Tables 15 and 16 show the results when
using our baseline.

D. Qualitative comparisons

We qualitatively compare our results to those of Funke
et al. [12] and Xie et al. [48].
Note that Funke et al.
[12] provided results on only ﬁve textures and of those only
four are dynamic textures in the sense that
their appear-
ance and dynamics are spatiotemporally coherent. Their re-
sults on these sequences (cranberries, flames, leaves,
and water 5) are included in the folder funke under
dynamic texture synthesis/comparisons. Our re-
sults are included as well.

We also compare our results to [12, 48] on nine dynamic tex-
tures chosen to cover the full range of our dynamics and appear-
ance groupings. We use their publicly available code and follow
the parameters used in their experiments. For Funke et al.’s model
[12], the parameters used are ∆t = 4 and T = 12 (recall that
target dynamic textures consist of 12 frames). For the spatiotem-
poral and temporal models from Xie et al. [48], the parameters
used are T = 1200 and ˜M = 3. A comparison between our
results, Funke et al.’s [12], and Xie et al’s [48] on the nine dy-
namic textures are included in the folder xie and funke un-
der dynamic texture synthesis/comparisons. Note
for Xie et al. [48], we compare with their spatiotemporal model

.
)
s

m
0
0
6
-
0
0
3
(

s
e
m

i
t

e
r
u
s
o
p
x
e

t
r
o
h
S
)
a
(

.
)
s

m
0
0
8
4
-
0
0
2
1
(

s
e
m

i
t

e
r
u
s
o
p
x
e
g
n
o
L
)
b
(

.
e
c
n
e
d
ﬁ
n
o
c

l
a
c
i
t
s
i
t
a
t
s

%
5
9
a
h
t
i

w

r
o
r
r
e

f
o
n
i
g
r
a
m
a

s
e
d
u
l
c
n
i

y
c
a
r
u
c
c
a

e
r
u
t
x
e
t

h
c
a
E

.
s
e
m

i
t

e
r
u
s
o
p
x
e

r
e
v
o

d
e
g
a
r
e
v
a

s
e
i
c
a
r
u
c
c
a

e
r
u
t
x
e
t
-
r
e
P

:
0
1

e
r
u
g
i
F

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

±
±
±
0.0

±
±
±
0.248

0.203
0.095
0.175
0.135
0.217

±
±
±
±
±
0.175

0.205
±
0.196
0.162
0.12
0.212

0.194
±
0.162
±
0.199
±
0.139
±
0.252
±
0.181
±
0.195
0.135
0.186
0.199

300 ms.
0.625
0.769
0.609
0.806
0.533
0.607
0.44
±
0.813
0.727
0.609
0.6
±
0.139
0.806
±
0.168
0.81
±
0.5
0.219
±
0.591
0.48
±
0.792
0.909
0.571
1.0
±
0.069
0.964
±
0.176
0.72
±
0.5
0.209
±
0.435
0.929
0.452
0.813
0.632
0.8
±
0.476
0.458
0.632
0.6
±
0.542
0.517
0.767
0.667
0.792
0.538
0.478
0.769
0.724
0.862
0.72
±
0.643
0.643
0.826
0.538
0.656
0.556
0.375
0.632
0.545
0.688
0.571
0.444

0.199
±
0.182
±
0.151
±
0.202
±
0.162
±
0.192
±
0.204
±
0.162
±
0.163
±
0.126
±
0.176
0.177
0.177
0.155
0.192
0.165
0.23
0.237
0.217
0.208
0.161
0.183
0.187

±
±
±
0.192

0.214
0.199
0.217

±
±
±
±
±
±
±
±
±
±
±
±

400 ms.
0.161
0.333
±
0.215
0.786
±
0.152
0.786
±
0.127
0.88
±
0.164
0.842
±
0.212
0.571
±
0.621
0.177
±
0.245
0.5
±
0.183
0.654
±
0.175
0.773
±
0.175
0.773
±
0.212
0.75
±
0.129
0.839
±
0.429
0.212
±
0.168
0.81
±
0.195
0.318
±
0.158
0.733
±
0.952
0.091
±
0.209
0.65
±
0.0
1.0
±
1.0
0.0
±
0.909
0.12
±
0.565
0.203
±
0.688
0.227
±
0.826
0.155
±
0.538
0.192
±
0.778
0.192
±
0.667
0.202
±
0.903
0.104
±
0.714
0.167
±
0.346
0.183
±
0.667
0.202
±
0.769
0.162
±
0.625
0.168
±
0.741
0.165
±
0.903
0.104
±
0.737
0.198
±
0.938
0.119
±
0.731
0.17
±
0.727
0.186
±
0.833
0.149
±
0.783
0.169
±
0.704
0.172
±
0.708
0.182
±
0.773
0.175
±
0.815
0.147
±
0.947
0.1
±
0.63
0.182
±
0.231
0.5
±
0.183
0.32
±
0.586
0.179
±
0.188
0.64
±
0.165
0.741
±
0.218
0.667
±
0.179
0.586
±
0.201
0.364
±

0.193
0.164
0.187
0.196

±
±
±
±
0.164

±
±
±
±
±
±
±
±
0.0

0.139
0.186
0.155
0.185
0.188
0.111
0.165
0.069

600 ms.
0.714
0.842
0.615
0.846
0.7
±
0.187
0.615
±
0.156
0.622
±
0.169
0.667
±
0.209
0.65
±
0.205
0.591
±
0.643
0.177
±
1.0
0.0
±
0.788
0.727
0.826
0.593
0.696
0.897
0.656
0.964
1.0
±
0.115
0.913
±
0.181
0.552
±
0.151
0.808
±
0.147
0.815
±
0.177
0.621
±
0.202
0.667
±
0.151
0.767
±
0.096
0.95
±
0.173
0.679
±
0.23
0.556
±
0.195
0.652
±
0.155
0.826
±
0.174
0.581
±
0.175
0.8
±
0.16
0.75
±
0.613
0.171
±
0.058
0.97
±
0.165
0.741
±
0.6
0.215
±
0.119
0.938
±
0.168
0.81
±
0.155
0.826
±
0.191
0.813
±
0.111
0.917
±
0.193
0.714
±
0.103
0.889
±
0.19
0.423
±
0.222
0.579
±
0.169
0.667
±
0.652
0.195
±
0.196
0.52
±
0.173
0.75
±
0.179
0.586
±
0.227
0.688
±
0.197
0.583
±

0.12

±
0.131

±
±
0.2

0.151
0.199

1200 ms.
0.185
0.536
±
0.101
0.906
±
0.199
0.542
±
0.193
0.714
±
0.138
0.87
±
0.636
0.164
±
0.201
0.7
±
0.201
0.7
±
0.767
0.609
0.5
±
0.909
0.9
±
0.164
0.636
±
0.147
0.815
±
0.188
0.64
±
0.064
0.967
±
0.111
0.917
±
0.195
0.652
±
0.062
0.968
±
0.102
0.923
±
0.119
0.889
±
0.118
0.871
±
0.149
0.833
±
1.0
0.0
±
0.15
0.75
±
0.792
0.162
±
0.127
0.88
±
0.08
0.958
±
0.163
0.724
±
0.158
0.733
±
0.151
0.767
±
0.955
0.087
±
0.173
0.75
±
0.609
0.199
±
1.0
0.0
±
0.176
0.72
±
0.083
0.957
±
0.237
0.471
±
0.176
0.72
±
0.142
0.821
±
0.963
0.071
±
0.127
0.88
±
0.08
0.958
±
0.87
0.138
±
1.0
0.0
±
0.875
0.615
0.821
0.727
0.826
0.739
0.833
0.759
0.792
0.75

0.132
±
0.187
±
0.142
±
0.186
±
0.155
±
0.179
±
0.149
±
0.156
±
0.162
±
0.16
±

0.188

0.201

0.122

0.195
0.181
0.132
0.182
0.188

0.119
0.195
0.175
0.175
0.126

0.119

0.139
0.126
0.198
0.148

±
0.0
0.0

±
±
±
±
±
0.0

2400 ms.
0.636
±
0.95
0.096
±
0.867
±
0.058
0.97
±
0.731
0.17
±
0.75
0.19
±
0.652
±
0.824
±
0.875
±
0.708
±
0.519
±
1.0
0.0
±
0.938
0.652
0.773
0.548
0.933
1.0
±
0.696
1.0
±
1.0
±
0.889
±
0.92
0.106
±
0.788
±
0.905
±
0.737
±
0.735
±
0.0
1.0
±
0.0
1.0
±
0.808
0.593
0.806
0.857
0.75
0.9
±
0.952
±
0.652
±
0.92
0.106
±
0.895
±
0.5
0.173
±
0.931
±
0.84
0.144
±
0.905
±
0.852
±
0.913
±
0.917
±
0.923
±
0.227
±
0.813
±
0.571
±
0.706
±
0.667
±
0.771
±
0.65
0.209
±
0.696
±
0.182
0.37
±

±
±
±
±
0.19
±
0.131

0.091
0.195

0.138

0.092

0.126
0.134
0.115
0.111
0.102
0.175
0.191
0.212
0.153
0.202
0.139

0.151
0.185
0.139
0.15

0.188

0.15
±
0.084
±
0.195
±
0.077
0.134
±
0.182
±
0.175
±
0.182
0.122
0.163
0.202

0.071
0.163
0.123
0.188
0.099
0.074
0.177

0.132
±
0.111
±
0.189
±
0.064
±
0.225
±
0.138
±
0.127
0.106
0.169
±
0.204
±
0.15
±
0.069
±
0.252
±
0.151
±
0.138
0.259
±
0.119
±
0.167
0.163
0.062
0.157

±
±
±
0.0
0.107
0.0

0.119

±
±
±
0.0

±
±
±
±
±
±
±
0.0
0.0

3600 ms.
0.857
0.938
0.682
0.96
±
0.852
0.762
0.773
0.63
±
0.848
0.724
0.765
1.0
±
0.963
0.724
0.885
0.519
0.926
0.962
0.692
1.0
±
1.0
±
0.875
0.917
0.667
0.967
0.526
0.895
0.88
±
0.92
±
0.783
0.522
0.857
0.964
0.533
0.767
0.87
±
0.571
0.889
0.76
±
0.724
0.968
0.778
1.0
±
0.9
±
1.0
±
0.889
1.0
±
0.619
0.733
0.583
0.818
0.724
0.652
0.652
0.731
0.632

±
±
±
±
±
±
±
±
±

±
0.0

0.208
0.158
0.197
0.161
0.163
0.195
0.195
0.17
0.217

±
±
0.0

0.066
0.133

±
±
±
±
0.0

0.172
0.099
0.192
0.071

0.182
0.217
0.143
0.195
0.152
0.151
0.062
0.091
0.165
0.137
0.214
0.147

±
±
±
±
±
±
±
±
±
±
±
±
0.0
0.179
0.0

4800 ms.
0.704
0.926
0.778
0.963
1.0
±
0.762
0.706
0.781
0.682
0.786
0.658
0.968
0.952
0.741
0.828
0.524
0.815
1.0
±
0.5
±
1.0
±
0.966
0.833
1.0
±
0.151
0.808
±
0.089
0.933
±
0.218
0.667
±
0.155
0.826
±
0.135
0.813
±
0.119
0.889
±
0.138
0.87
±
0.195
0.652
±
0.077
0.96
±
0.127
0.88
±
0.151
0.808
±
0.195
0.652
±
0.145
0.889
±
0.15
0.714
±
0.074
0.962
±
0.165
0.588
±
0.182
0.63
±
0.0
1.0
±
0.87
0.138
±
0.0
1.0
±
0.127
0.88
±
0.069
0.964
0.852
0.134
1.0
±
0.333
0.821
0.394
0.917
0.7
±
0.682
0.667
0.833
0.452

±
±
±
±
0.164

0.195
0.189
0.133
0.175

0.178
0.142
0.167
0.111

±
±
0.0

±
±
±
±

Table 1: Per-texture accuracies averaged over exposure times, using the concatenation layer. Each texture accuracy includes
a margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

±
±
±
±
±
0.111

Short (300-600 ms.)
0.111
0.526
0.103
0.797
0.105
0.675
0.086
0.841
0.703
0.112
0.6
±
0.102
0.571
±
0.102
0.692
±
0.111
0.676
±
0.114
0.657
±
0.114
0.677
±
0.08
0.861
±
0.083
0.812
±
0.123
0.556
±
0.106
0.742
±
0.114
0.473
±
0.098
0.74
±
0.917
0.064
±
0.111
0.63
±
0.025
0.987
±
0.03
0.985
±
0.085
0.843
±
0.114
0.541
±
0.116
0.646
±
0.077
0.859
±
0.105
0.535
±
0.761
0.099
±
0.7
0.107
±
0.074
0.887
±
0.107
0.636
±
0.118
0.441
±
0.118
0.651
±
0.101
0.73
±
0.103
0.586
±
0.106
0.671
±
0.082
0.809
±
0.11
0.662
±
0.068
0.904
±
0.104
0.671
±
0.119
0.6
±
0.09
0.833
±
0.097
0.767
±
0.089
0.797
±
0.107
0.738
±
0.096
0.77
±
0.101
0.724
±
0.071
0.885
±
0.11
0.532
±
0.116
0.594
±
0.115
0.521
±
0.118
0.559
±
0.116
0.594
±
0.107
0.685
±
0.105
0.646
±
0.112
0.603
±
0.114
0.466
±

Long (1200-4800 ms.)
0.093
0.673
±
0.048
0.928
±
0.09
0.723
±
0.053
0.915
±
0.066
0.864
±
0.091
0.716
±
0.098
0.707
±
0.089
0.729
±
0.075
0.798
±
0.087
0.712
±
0.604
0.093
±
0.033
0.97
±
0.051
0.94
±
0.086
0.688
±
0.073
0.827
±
0.095
0.558
±
0.057
0.909
±
0.971
0.032
±
0.094
0.627
±
0.02
0.99
±
0.971
0.032
±
0.063
0.87
±
0.054
0.918
±
0.079
0.776
±
0.045
0.947
±
0.682
0.097
±
0.8
0.078
±
0.064
0.88
±
0.046
0.941
±
0.079
0.792
±
0.093
0.631
±
0.069
0.841
±
0.055
0.917
±
0.094
0.729
±
0.729
0.089
±
0.054
0.93
±
0.093
0.68
±
0.05
0.931
±
0.094
0.674
±
0.089
0.637
±
0.049
0.927
±
0.067
0.863
±
0.947
0.045
±
0.058
0.896
±
0.047
0.94
±
0.903
0.06
±
0.043
0.95
±
0.448
0.099
±
0.078
0.794
±
0.098
0.55
±
0.076
0.806
±
0.709
0.088
±
0.084
0.74
±
0.093
0.688
±
0.082
0.767
±
0.095
0.543
±

All (300-4800 ms.)
0.072
0.608
±
0.048
0.882
±
0.069
0.702
±
0.047
0.886
±
0.06
0.802
±
0.071
0.665
±
0.072
0.636
±
0.067
0.713
±
0.064
0.751
±
0.069
0.69
±
0.072
0.632
±
0.04
0.924
±
0.876
0.05
±
0.071
0.64
±
0.061
0.794
±
0.073
0.522
±
0.055
0.835
±
0.033
0.949
±
0.072
0.629
±
0.016
0.989
±
0.976
0.023
±
0.051
0.86
±
0.064
0.756
±
0.067
0.727
±
0.043
0.908
±
0.073
0.609
±
0.062
0.784
±
0.059
0.806
±
0.041
0.919
±
0.066
0.725
±
0.074
0.556
±
0.063
0.771
±
0.056
0.835
±
0.071
0.657
±
0.068
0.703
±
0.05
0.869
±
0.673
0.071
±
0.04
0.92
±
0.07
0.672
0.071
0.624
0.046
0.892
0.057
0.823
0.049
0.879
0.055
0.836
0.05
0.868
0.058
0.822
0.04
0.921
0.074
0.486
0.068
0.713
0.074
0.538
0.068
0.708
0.071
0.663
0.066
0.718
0.07
0.669
0.068
0.699
0.073
0.511

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±

Table 2: Per-texture accuracies averaged over a range of exposure times, using the concatenation layer. Each texture accuracy
includes a margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

0.126

0.138
0.196
0.151
0.135

0.162

0.115
±
0.106
±
0.202
±
0.143
±
0.144
0.212
0.1
±
0.126
±
0.089
±
0.162
±
0.08
±
0.189
±
0.112
±
0.199
±
0.096
0.161
0.091

0.1
0.091

±
±
0.131

0.145
±
0.156
0.151
0.112

±
0.0

±
0.0

±
±
±
±
0.0
0.0

300 ms.
0.933
1.0
±
0.895
0.846
0.808
0.929
1.0
±
1.0
±
0.875
1.0
±
0.913
0.944
0.765
0.864
0.84
±
0.75
±
0.947
0.905
0.933
0.875
0.958
0.667
0.941
0.609
0.95
±
0.818
0.952
1.0
±
1.0
±
0.947
0.952
0.9
±
0.889
0.85
±
0.808
0.941
1.0
±
0.941
0.867
0.667
1.0
±
1.0
±
0.941
0.958
0.957
1.0
±
0.909
0.684
0.857
0.929
0.778
0.867
0.737
1.0
±
0.947
0.941

±
±
±
±
±
±
±
0.0

±
±
±
0.0
0.0

±
±
0.0
0.0

±
±
±
0.0

±
±
0.0

±
±

0.112
0.172
0.239

0.112
0.08
0.083

0.12
0.209
0.183
0.135
0.272
0.172
0.198

0.1
0.112

0.186

0.1
0.111

0.202
0.083

±
±
±
±
±
0.0
0.0

0.106
0.195
0.138
0.119
0.071

±
±
0.0
0.0
0.138
0.132
0.083
0.111

400 ms.
0.9
±
0.944
0.652
0.895
0.889
0.963
1.0
±
1.0
±
0.947
0.897
1.0
±
1.0
±
0.87
±
0.875
±
0.957
±
0.917
±
1.0
0.0
±
0.765
±
0.957
±
1.0
0.0
±
1.0
0.0
±
0.75
0.19
±
0.88
0.127
±
0.209
0.65
±
0.0
1.0
±
0.096
0.95
±
0.938
0.119
±
0.92
0.106
±
0.0
1.0
±
0.962
1.0
±
1.0
±
1.0
±
0.857
0.8
±
0.857
1.0
±
1.0
±
0.773
0.957
1.0
±
0.958
1.0
±
0.917
1.0
±
0.947
1.0
±
0.588
0.958
0.778
1.0
±
1.0
±
0.905
0.944
0.933
0.88

0.126
±
0.106
±
0.126
±
0.127
±

0.234
0.08
0.192

±
0.175
0.15

±
±
±
0.0
0.0

0.175
0.083

±
0.0
0.0
0.0

±
0.0
0.0

±
±
0.0

0.183

0.074

0.111

±
0.0

±
0.0

±
0.0

0.08

0.1

0.08

±
0.0

±
0.0

±
0.0

0.119

0.099

0.115

0.111

±
0.0
0.0

±
±
±
0.0

±
±
0.0
0.0

±
±
0.0
0.0

0.066
0.083

0.102
0.106

±
±
±
±
0.0

0.126
0.083
0.193

0.207
0.186
0.229
0.1

600 ms.
0.913
1.0
±
0.933
0.957
0.714
1.0
±
0.966
0.957
1.0
±
1.0
±
0.958
1.0
±
0.938
1.0
±
1.0
±
0.926
1.0
±
0.923
0.944
1.0
±
1.0
±
0.722
0.727
0.769
0.947
1.0
±
0.917
1.0
±
0.08
0.958
±
0.077
0.96
±
0.941
0.112
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.75
±
0.923
±
0.947
±
0.941
±
0.846
±
0.0
1.0
±
1.0
0.0
±
0.964
1.0
±
0.962
1.0
±
1.0
±
1.0
±
0.64
±
0.952
±
0.952
±
0.889
±
0.962
±
0.938
±
1.0
0.0
±
0.952
0.947

0.188
0.091
0.091
0.119
0.074
0.119

0.19
0.102
0.1
0.112
0.139

0.091
0.1

±
0.0
0.0
0.0

0.069

0.074

±
0.0

±
0.0

±
±

0.08

0.15

±
0.0

±
0.0

±
0.0

±
0.0

0.126

0.112

0.071

0.183

±
0.0
0.0

±
0.0
0.0

±
±
±
0.0

0.115
0.091
0.087

1200 ms.
0.963
1.0
±
0.1
0.947
±
0.077
0.96
±
0.096
0.95
±
0.962
0.074
±
1.0
0.0
±
0.941
1.0
±
0.857
1.0
±
1.0
±
0.905
1.0
±
1.0
±
0.958
1.0
±
0.172
0.867
±
0.138
0.87
±
0.958
0.08
±
1.0
0.0
±
0.789
1.0
±
0.913
0.952
0.955
1.0
±
0.913
0.923
1.0
±
1.0
±
1.0
±
1.0
±
0.087
0.955
±
0.127
0.88
±
0.929
0.135
±
0.0
1.0
±
1.0
0.0
±
0.889
1.0
±
0.96
1.0
±
1.0
±
1.0
±
1.0
±
0.957
1.0
±
0.778
1.0
±
0.929
1.0
±
1.0
±
0.897
1.0
±
0.85
1.0

±
±
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

0.115
0.145

±
0.0
0.0

±
0.0

0.145

0.077

0.111

0.083

0.192

0.095

0.156

±
0.0

±
0.0

±
0.0

±
0.0

±

0.183
0.091

0.087

0.139

0.112
0.172

0.095

0.155

0.182

±
0.157

±
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.096
±
0.0
0.0

2400 ms.
0.0
1.0
±
0.0
1.0
±
0.131
0.9
±
0.92
0.106
±
0.857
±
0.952
±
1.0
0.0
±
0.955
1.0
±
1.0
±
1.0
±
1.0
±
0.846
±
0.96
0.077
±
0.941
±
0.867
±
1.0
0.0
±
0.929
1.0
±
1.0
±
1.0
±
0.826
0.8
±
0.762
1.0
±
1.0
±
1.0
±
0.95
1.0
±
1.0
±
0.906
1.0
±
1.0
±
1.0
±
0.8
±
1.0
±
1.0
±
0.933
0.944
0.947
1.0
±
0.955
1.0
±
1.0
±
1.0
±
0.95
1.0
±
0.55
1.0
±
0.889
1.0
±
1.0
±
1.0
±
1.0
±
0.929
0.773

±
0.0
0.0
0.0
0.096
±
0.0
0.218
±
0.0

±
0.0
0.0
0.0
0.202
0.0
0.0

±
0.0
0.0
0.0
0.0

±
±
±
0.0

±
±

0.126
0.106
0.1

0.087

0.095
0.175

0.101

0.145

0.1

0.08

0.106

±
0.0
0.0
0.0

±
±
±
0.0
0.0

0.115
0.091
0.145

0.111
0.101
0.161

±
0.0
0.0
0.168
0.091

3600 ms.
0.0
1.0
±
1.0
0.0
±
0.913
±
0.952
±
0.889
±
0.0
1.0
±
0.0
1.0
±
0.077
0.96
±
1.0
0.0
±
0.944
1.0
±
1.0
±
0.81
±
0.952
±
0.0
1.0
±
0.95
0.096
±
1.0
0.0
±
0.947
1.0
±
1.0
±
1.0
±
0.917
0.906
0.818
1.0
±
1.0
±
0.958
1.0
±
1.0
±
1.0
±
1.0
±
0.08
0.958
±
0.08
0.958
±
0.062
0.968
±
0.077
0.96
±
0.0
1.0
±
0.0
1.0
±
0.077
0.96
±
0.095
0.929
1.0
±
1.0
±
1.0
±
1.0
±
1.0
±
0.905
±
0.0
1.0
±
0.0
1.0
±
0.76
0.167
±
0.0
1.0
±
1.0
0.0
±
0.909
0.955
0.875
0.933
0.926
0.905

0.12
0.087
0.132
0.089
0.099
0.126

±
0.0
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

0.126

±
±
±
±
±
±

0.1
0.119
0.164
0.172
0.138

0.123

0.066

0.112

±
0.0

±
±
±
±
±
0.0
0.0
0.0

4800 ms.
0.885
1.0
±
0.966
±
0.0
1.0
±
0.92
0.106
±
1.0
0.0
±
0.941
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.157
0.8
±
0.95
0.096
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.947
0.938
0.842
0.867
0.895
1.0
±
1.0
±
1.0
±
0.895
1.0
±
0.813
0.95
±
0.958
0.889
1.0
±
0.9
±
1.0
±
1.0
±
1.0
±
0.95
±
0.909
1.0
±
1.0
±
1.0
±
0.955
1.0
±
1.0
±
1.0
±
0.783
±
0.0
1.0
±
0.88
0.127
±
0.0
1.0
±
1.0
0.0
±
0.909
0.962
1.0
0.9

±
±
0.0
0.131
0.0
0.0
0.0
0.096
0.12

±
±
0.0
0.131

±
0.0
0.0
0.0

±
0.0
0.0
0.0

±
0.0

±
±

0.087

0.169

0.12
0.074

0.138

0.191
±
0.096
0.08
0.145

Table 3: Per-texture accuracies averaged over exposure times, using the ﬂow decode layer. Each texture accuracy includes a
margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

Short (300-600 ms.)
0.078
0.917
±
0.034
0.983
±
0.102
0.807
±
0.076
0.909
±
0.089
0.811
±
0.05
0.963
±
0.027
0.986
±
0.985
0.029
±
0.055
0.95
±
0.051
0.954
±
0.049
0.956
±
0.028
0.986
±
0.092
0.857
±
0.067
0.912
±
0.057
0.932
±
0.881
0.078
±
0.038
0.98
±
0.081
0.875
±
0.054
0.944
±
0.05
0.963
±
0.987
0.025
±
0.113
0.71
±
0.844
0.089
±
0.124
0.661
±
0.054
0.96
±
0.07
0.917
±
0.062
0.934
±
0.042
0.969
±
0.034
0.983
±
0.047
0.957
±
0.046
0.966
±
0.049
0.964
±
0.044
0.968
±
0.082
0.902
±
0.099
0.788
±
0.071
0.906
±
0.029
0.985
±
0.966
0.046
±
0.094
0.825
±
0.91
0.068
±
1.0
0.0
±
0.038
0.972
±
0.032
0.983
±
0.052
0.946
±
0.026
0.986
±
0.03
0.984
±
0.049
0.964
±
0.121
0.639
±
0.064
0.932
±
0.085
0.887
±
0.077
0.907
±
0.055
0.95
±
0.092
0.857
±
0.037
0.981
±
0.06
0.945
±
0.069
0.918
±

0.05

±
0.0

±
0.0
0.0

±
±
±
±
0.0

±
±
±
±
±
±
0.0

0.075
0.042
0.029
0.046

0.051
0.044
0.064
0.028
0.024
0.04

0.051
0.043
0.029
0.029
0.077
0.068
0.077
0.021
0.023
0.023
0.052
0.022
0.041
0.043
0.03
0.039
0.029
0.065
0.023

Long (1200-4800 ms.)
0.959
0.039
1.0
±
0.934
0.955
0.909
0.979
0.988
0.964
1.0
±
0.948
1.0
±
1.0
±
0.839
0.963
0.985
0.952
1.0
±
0.94
±
0.961
±
0.979
±
0.985
±
0.847
±
0.884
±
0.847
±
0.989
±
0.988
±
0.988
±
0.939
±
0.989
±
0.963
±
0.956
±
0.978
±
0.965
±
0.979
±
0.894
±
0.988
±
1.0
0.0
±
0.978
0.929
0.964
0.989
0.986
1.0
±
0.986
0.973
0.975
1.0
±
0.721
1.0
±
0.921
0.978
0.988
0.914
0.969
0.921
0.897

0.056
0.03
0.023
0.057
0.035
0.056
0.064

0.031
0.055
0.04
0.022
0.026

±
±
±
±
±
0.0

0.027
0.037
0.034

±
±
±
0.0

0.095

±
0.0

±
±
±
±
±
±
±

All (300-4800 ms.)
0.037
0.945
±
0.013
0.993
±
0.051
0.885
±
0.04
0.937
±
0.055
0.861
±
0.026
0.974
±
0.018
0.987
±
0.026
0.974
±
0.023
0.979
±
0.951
0.036
±
0.023
0.98
±
0.014
0.993
±
0.058
0.846
±
0.039
0.939
±
0.957
0.033
±
0.043
0.92
±
0.014
0.993
±
0.046
0.912
±
0.034
0.953
±
0.026
0.973
±
0.019
0.986
±
0.066
0.789
±
0.054
0.867
±
0.069
0.773
±
0.023
0.979
±
0.033
0.958
±
0.03
0.966
±
0.034
0.952
±
0.019
0.986
±
0.031
0.96
±
0.032
0.96
±
0.027
0.972
±
0.029
0.966
±
0.035
0.952
±
0.057
0.848
±
0.034
0.952
±
0.014
0.993
±
0.026
0.973
±
0.052
0.884
±
0.038
0.94
±
0.013
0.993
±
0.023
0.979
±
0.013
0.993
±
0.03
0.966
±
0.023
0.98
±
0.023
0.979
0.019
0.986
0.075
0.687
0.027
0.972
0.047
0.908
0.035
0.952
0.027
0.972
0.05
0.893
0.026
0.973
0.042
0.931
0.047
0.905

±
±
±
±
±
±
±
±
±
±
±

Table 4: Per-texture accuracies averaged over a range of exposure times, using the ﬂow decode layer. Each texture accuracy
includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

300 ms.
0.702
0.806
0.616

±
±
±

0.098
0.046
0.03

400 ms.
0.74
±
0.853
±
0.658
±

0.101

0.044
0.029

600 ms.
0.838
0.837
0.687

±
±
±

0.088
0.043
0.028

1200 ms.
0.84
±
0.903
0.76

0.083
0.036
±
0.026

±

2400 ms.
0.954
0.909
0.751

±
±
±

0.051
0.037
0.026

3600 ms.
0.878
0.919
0.776

±
±
±

0.074
0.031
0.026

4800 ms.
0.827
0.902
0.762

±
±
±

0.082
0.035
0.025

Table 5: Accuracies of textures grouped by appearances, averaged over exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

Short (300-600 ms.)
0.056
0.756
0.026
0.831
0.017
0.654

±
±
±

Long (1200-4800 ms.)
0.038
0.871
0.017
0.908
0.013
0.762

±
±
±

All (300-4800 ms.)
0.033
0.821
0.015
0.875
0.01
0.717

±
±
±

Table 6: Accuracies of textures grouped by appearances, averaged over a range of exposure times, using the concatenation
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

300 ms.
0.078
0.889
±
0.041
0.89
±
0.021
0.901
±

400 ms.
0.933
0.942
0.916

±
±
±

0.063
0.031
0.018

600 ms.
0.921
0.957
0.937

±
±
±

0.067
0.026
0.016

1200 ms.
0.961
0.953
0.957

±
±
±

0.043
0.028
0.014

2400 ms.
0.057
0.948
±
0.025
0.96
±
0.015
0.945
±

3600 ms.
0.984
0.968
0.955

±
±
±

0.031
0.022
0.013

4800 ms.
0.964
0.947
0.96

±
±
0.013
±

0.049
0.029

Table 7: Accuracies of textures grouped by appearances, averaged over exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

Short (300-600 ms.)
0.914
0.04
±
0.93
0.019
±
0.919
±

0.011

Long (1200-4800 ms.)
0.023
0.964
0.013
0.957
0.007
0.954

±
±
±

All (300-4800 ms.)
0.022
0.943
0.011
0.946
0.006
0.939

±
±
±

Table 8: Accuracies of textures grouped by appearances, averaged over a range of exposure times, using the ﬂow decode
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

300 ms.
0.625
0.721

±
±

0.032
0.039

400 ms.
0.664
0.763

±
±

0.032
0.039

600 ms.
0.698
0.777

±
±

0.03
0.037

1200 ms.
0.741
0.885

0.028
0.028

±
±

2400 ms.
0.753
0.854

0.028
0.032

±
±

3600 ms.
0.762
0.902

0.028
0.026

±
±

4800 ms.
0.755
0.861

0.028
0.029

±
±

Table 9: Accuracies of textures grouped by dynamics, averaged over exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

Short (300-600 ms.)
0.018
0.663
0.022
0.753

±
±

Long (1200-4800 ms.)
0.014
0.753
0.015
0.876

±
±

All (300-4800 ms.)
0.011
0.715
0.013
0.823

±
±

Table 10: Accuracies of textures grouped by dynamics, averaged over a range of exposure times, using the concatenation
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

300 ms.
0.886
0.92

0.024

±
0.027

±

400 ms.
0.911
0.942

±
±

0.02
0.023

600 ms.
0.934
0.949

±
±

0.018
0.021

1200 ms.
0.947
0.974

0.016
0.016

±
±

2400 ms.
0.945
0.954

0.016
0.02

±
±

3600 ms.
0.955
0.966

0.014
0.017

±
±

4800 ms.
0.954
0.964

0.015
0.018

±
±

Table 11: Accuracies of textures grouped by dynamics, averaged over exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

Short (300-600 ms.)
0.012
0.911
0.013
0.937

±
±

Long (1200-4800 ms.)
0.95
±
0.964
±

0.008
0.009

All (300-4800 ms.)
0.007
0.934
0.008
0.953

±
±

Table 12: Accuracies of textures grouped by dynamics, averaged over a range of exposure times, using the ﬂow decode layer.
Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

300 ms.
0.661

±

400 ms.
0.699

±

600 ms.
0.726

±

1200 ms.
0.791

0.021

±

2400 ms.
0.788

0.022

±

3600 ms.
0.812

0.021

±

4800 ms.
0.793

0.021

±

0.025

0.025

0.023

Table 13: Average accuracy over all textures, averaged over exposure times, using the concatenation layer. Each texture
accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

Short (300-600 ms.)
0.014
0.695

Long (1200-4800 ms.)
0.011
0.796

All (300-4800 ms.)
0.009
0.754

±

±

±

Table 14: Average accuracy over all textures, averaged over a range of exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

300 ms.
0.898

±

0.018

0.015

400 ms.
0.922

±

600 ms.
0.94

±

0.013

1200 ms.
0.956

0.012

±

2400 ms.
0.948

0.013

±

3600 ms.
0.959

0.011

±

4800 ms.
0.957

0.012

±

Table 15: Average accuracy over all textures, averaged over exposure times, using the ﬂow decode layer. Each texture
accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

Short (300-600 ms.)
0.009
0.921

Long (1200-4800 ms.)
0.006
0.955

All (300-4800 ms.)
0.005
0.941

±

±

±

Table 16: Average accuracy over all textures, averaged over a range of exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Two-Stream Convolutional Networks for Dynamic Texture Synthesis

Matthew Tesfaldet Marcus A. Brubaker
Department of Electrical Engineering and Computer Science
York University, Toronto
{mtesfald,mab}@eecs.yorku.ca

Konstantinos G. Derpanis
Department of Computer Science
Ryerson University, Toronto
kosta@scs.ryerson.ca

8
1
0
2
 
r
p
A
 
2
1
 
 
]

V
C
.
s
c
[
 
 
4
v
2
8
9
6
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

We introduce a two-stream model for dynamic texture
synthesis. Our model is based on pre-trained convolutional
networks (ConvNets) that target two independent tasks: (i)
object recognition, and (ii) optical ﬂow prediction. Given
an input dynamic texture, statistics of ﬁlter responses from
the object recognition ConvNet encapsulate the per-frame
appearance of the input texture, while statistics of ﬁlter re-
sponses from the optical ﬂow ConvNet model its dynamics.
To generate a novel texture, a randomly initialized input se-
quence is optimized to match the feature statistics from each
stream of an example texture. Inspired by recent work on
image style transfer and enabled by the two-stream model,
we also apply the synthesis approach to combine the texture
appearance from one texture with the dynamics of another
to generate entirely novel dynamic textures. We show that
our approach generates novel, high quality samples that
match both the framewise appearance and temporal evo-
lution of input texture. Finally, we quantitatively evaluate
our texture synthesis approach with a thorough user study.

1. Introduction

Many common temporal visual patterns are naturally de-
scribed by the ensemble of appearance and dynamics (i.e.,
temporal pattern variation) of their constituent elements.
Examples of such patterns include ﬁre, ﬂuttering vegetation,
and wavy water. Understanding and characterizing these
temporal patterns has long been a problem of interest in hu-
man perception, computer vision, and computer graphics.
These patterns have been previously studied under a variety
of names, including turbulent-ﬂow motion [17], temporal
textures [30], time-varying textures [3], dynamic textures
[8], textured motion [45] and spacetime textures [7]. Here,
we adopt the term “dynamic texture”. In this work, we pro-
pose a factored analysis of dynamic textures in terms of ap-
pearance and temporal dynamics. This factorization is then
used to enable dynamic texture synthesis which, based on

Figure 1: Dynamic texture synthesis. (left) Given an input
dynamic texture as the target, our two-stream model is able
to synthesize a novel dynamic texture that preserves the tar-
get’s appearance and dynamics characteristics. (right) Our
two-stream approach enables synthesis that combines the
texture appearance from one target with the dynamics from
another, resulting in a composition of the two.

example texture inputs, generates a novel dynamic texture
instance. It also enables a novel form of style transfer where
the target appearance and dynamics can be taken from dif-
ferent sources as shown in Fig. 1.

Our model is constructed from two convolutional net-
works (ConvNets), an appearance stream and a dynamics
stream, which have been pre-trained for object recognition
and optical ﬂow prediction, respectively. Similar to previ-
ous work on spatial textures [13, 19, 33], we summarize an
input dynamic texture in terms of a set of spatiotemporal
statistics of ﬁlter outputs from each stream. The appear-
ance stream models the per frame appearance of the input
texture, while the dynamics stream models its temporal dy-
namics. The synthesis process consists of optimizing a ran-
domly initialized noise pattern such that its spatiotemporal
statistics from each stream match those of the input tex-
ture. The architecture is inspired by insights from human
perception and neuroscience. In particular, psychophysical
studies [6] show that humans are able to perceive the struc-
ture of a dynamic texture even in the absence of appearance
cues, suggesting that the two streams are effectively inde-

pendent. Similarly, the two-stream hypothesis [16] models
the human visual cortex in terms of two pathways, the ven-
tral stream (involved with object recognition) and the dorsal
stream (involved with motion processing).

In this paper, our two-stream analysis of dynamic tex-
tures is applied to texture synthesis. We consider a range
of dynamic textures and show that our approach generates
novel, high quality samples that match both the frame-wise
appearance and temporal evolution of an input example.
Further, the factorization of appearance and dynamics en-
ables a novel form of style transfer, where dynamics of one
texture are combined with the appearance of a different one,
cf . [14]. This can even be done using a single image as
an appearance target, which allows static images to be an-
imated. Finally, we validate the perceived realism of our
generated textures through an extensive user study.

2. Related work

There are two general approaches that have dominated
the texture synthesis literature: non-parametric sampling
approaches that synthesize a texture by sampling pixels of
a given source texture [10, 26, 37, 47], and statistical para-
metric models. As our approach is an instance of a para-
metric model, here we focus on these approaches.

The statistical characterization of visual textures was in-
troduced in the seminal work of Julesz [23]. He conjectured
that particular statistics of pixel intensities were sufﬁcient
to partition spatial textures into metameric (i.e., perceptu-
ally indistinguishable) classes. Later work leveraged this
notion for texture synthesis [19, 33]. In particular, inspired
by models of the early stages of visual processing, statistics
of (handcrafted) multi-scale oriented ﬁlter responses were
used to optimize an initial noise pattern to match the ﬁlter
response statistics of an input texture. More recently, Gatys
et al. [13] demonstrated impressive results by replacing the
linear ﬁlter bank with a ConvNet that, in effect, served as
a proxy for the ventral visual processing stream. Textures
are modelled in terms of the correlations between ﬁlter re-
sponses within several layers of the network. In subsequent
work, this texture model was used in image style transfer
[14], where the style of one image was combined with the
image content of another to produce a new image. Ruder et
al. [36] extended this model to video by using optical ﬂow
to enforce temporal consistency of the resulting imagery.

Variants of linear autoregressive models have been stud-
ied [42, 8] that jointly model appearance and dynamics of
the spatiotemporal pattern. More recent work has consid-
ered ConvNets as a basis for modelling dynamic textures.
Xie et al. [48] proposed a spatiotemporal generative model
where each dynamic texture is modelled as a random ﬁeld
deﬁned by multiscale, spatiotemporal ConvNet ﬁlter re-
sponses and dynamic textures are realized by sampling the
model. Unlike our current work, which assumes pretrained

ﬁxed networks, this approach requires the ConvNet weights
to be trained using the input texture prior to synthesis.

A recent preprint [12] described preliminary results ex-
tending the framework of Gatys et al. [13] to model and syn-
thesize dynamic textures by computing a Gram matrix of
ﬁlter activations over a small temporal window. In contrast,
our two stream ﬁltering architecture is more expressive as
our dynamics stream is speciﬁcally tuned to spatiotemporal
dynamics. Moreover, as will be demonstrated, the factoriza-
tion in terms of appearance and dynamics enables a novel
form of style transfer, where the dynamics of one pattern
are transferred to the appearance of another to generate an
entirely new dynamic texture. To the best of our knowledge,
we are the ﬁrst to demonstrate this form of style transfer.

The recovery of optical ﬂow from temporal imagery
has long been studied in computer vision.
Tradition-
ally, it has been addressed by handcrafted approaches e.g.,
[20, 29, 35]. Recently, ConvNet approaches [9, 34, 21, 49]
have been demonstrated as viable alternatives. Most closely
related to our approach are energy models of visual motion
[2, 18, 39, 31, 7, 25] that have been motivated and studied
in a variety of contexts, including computer vision, visual
neuroscience, and visual psychology. Given an input image
sequence, these models consist of an alternating sequence
of linear and non-linear operations that yield a distributed
representation (i.e., implicitly coded) of pixelwise optical
ﬂow. Here, an energy model motivates the representation of
observed dynamics which is then encoded as a ConvNet.

3. Technical approach

Our proposed two-stream approach consists of an ap-
pearance stream, representing the static (texture) appear-
ance of each frame, and a dynamics stream, representing
temporal variations between frames. Each stream consists
of a ConvNet whose activation statistics are used to charac-
terize the dynamic texture. Synthesizing a dynamic texture
is formulated as an optimization problem with the objective
of matching the activation statistics. Our dynamic texture
synthesis approach is summarized in Fig. 2 and the individ-
ual pieces are described in turn in the following sections.

3.1. Texture model: Appearance stream

The appearance stream follows the spatial texture model
introduced by Gatys et al. [13] which we brieﬂy review
here. The key idea is that feature correlations in a Con-
vNet trained for object recognition capture texture appear-
ance. We use the same publicly available normalized VGG-
19 network [40] used by Gatys et al. [13].

To capture the appearance of an input dynamic texture,
we ﬁrst perform a forward pass with each frame of the im-
age sequence through the ConvNet and compute the feature
RNl×Ml , for various levels in the net-
activations, Alt
work, where Nl and Ml denote the number of ﬁlters and

∈

ture suited for computing optical ﬂow (e.g., [9, 21]) which
is naturally differentiable. However, with most such mod-
els it is unclear how invariant their layers are to appearance.
Instead, we propose a novel network architecture which is
motivated by the spacetime-oriented energy model [7, 39].
In motion energy models, the velocity of image content
(i.e., motion) is interpreted as a three-dimensional orienta-
tion in the x-y-t spatiotemporal domain [2, 11, 18, 39, 46].
In the frequency domain, the signal energy of a translating
pattern can be shown to lie on a plane through the origin
where the slant of the plane is deﬁned by the velocity of
the pattern. Thus, motion energy models attempt to identify
this orientation-plane (and hence the patterns velocity) via
a set of image ﬁltering operations. More generally the con-
stituent spacetime orientations for a spectrum of common
visual patterns (including translation and dynamic textures)
can serve as a basis for describing the temporal variation of
an image sequence [7]. This suggests that motion energy
models may form an ideal basis for our dynamics stream.

Speciﬁcally, we use the spacetime-oriented energy
model [7, 39] to motivate our network architecture which
we brieﬂy review here; see [7] for a more in-depth descrip-
tion. Given an input video, a bank of oriented 3D ﬁlters
are applied which are sensitive to a range of spatiotemporal
orientations. These ﬁlter activations are rectiﬁed (squared)
and pooled over local regions to make the responses robust
to the phase of the input signal, i.e., robust to the alignment
of the ﬁlter with the underlying image structure. Next, ﬁl-
ter activations consistent with the same spacetime orienta-
tion are summed. These responses provide a pixelwise dis-
tributed measure of which orientations (frequency domain
planes) are present in the input. However, these responses
are confounded by local image contrast that makes it dif-
ﬁcult to determine whether a high response is indicative
of the presence of a spacetime orientation or simply due
to high image contrast. To address this ambiguity, an L1
normalization is applied across orientation responses which
results in a representation that is robust to local appearance
variations but highly selective to spacetime orientation.

Using this model as our basis, we propose the follow-
ing fully convolutional network [38]. Our ConvNet in-
put is a pair of temporally consecutive greyscale images.
Each input pair is ﬁrst normalized to have zero-mean and
unit variance. This step provides a level of invariance to
overall brightness and contrast, i.e., global additive and
multiplicative signal variations. The ﬁrst layer consists of
32 3D spacetime convolution ﬁlters of size 11
2
width
(height
time). Next, a squaring activation function
and 5
5 spatial max-pooling (with a stride of one) is ap-
plied to make the responses robust to local signal phase. A
1 convolution layer follows with 64 ﬁlters that combines
1
energy measurements that are consistent with the same ori-
entation. Finally, to remove local contrast dependence, an

×
×

11

×

×

×

×

Figure 2: Two-stream dynamic texture generation. Sets of
Gram matrices represent a texture’s appearance and dynam-
ics. Matching these statistics allows for the generation of
novel textures as well as style transfer between textures.

(cid:80)T

∈
t=1

ij =

1
T NlMl

the number of spatial locations of layer l at time t, respec-
tively. The correlations of the ﬁlter responses in a particular
layer are averaged over the frames and encapsulated by a
RNl×Nl , whose entries are given by
Gram matrix, Gl
(cid:80)Ml
ikAlt
Gl
k=1 Alt
jk, where T denotes the
number of input frames and Alt
ik denotes the activation of
feature i at location k in layer l on the target frame t. The
synthesized texture appearance is similarly represented by
a Gram matrix, ˆGlt
RNl×Nl , whose activations are given
ˆAlt
ˆAlt
by ˆGlt
ij = 1
ik denotes the acti-
ik
vation of feature i at location k in layer l on the synthesized
frame t. The appearance loss,
appearance, is then deﬁned as
the temporal average of the mean squared error between the
Gram matrix of the input texture and that of the generated
texture computed at each frame:

jk, where ˆAlt

∈
(cid:80)Ml
k=1

NlMl

L

appearance =

L

1
LappTout

Tout(cid:88)

(cid:88)

t=1

l

Gl

(cid:107)

ˆGlt

2
F ,

(cid:107)

−

(1)

where Lapp is the number of layers used to compute Gram
matrices, Tout is the number of frames being generated in
the output, and
(cid:107) · (cid:107)F is the Frobenius norm. Consistent
with previous work [13], we compute Gram matrices on the
following layers: conv1 1, pool1, pool2, pool3, and pool4.

3.2. Texture model: Dynamics stream

There are three primary goals in designing our dynamics
stream. First, the activations of the network must represent
the temporal variation of the input pattern. Second, the acti-
vations should be largely invariant to the appearance of the
images which should be characterized by the appearance
stream described above. Finally, the representation must be
differentiable to enable synthesis. By analogy to the ap-
pearance stream, an obvious choice is a ConvNet architec-

in layer l on the target frames t and t + 1. The dynam-
ics of the synthesized texture is represented by a Gram ma-
trix of ﬁlter response correlations computed separately for
each pair of frames, ˆGlt
ij =
1
ik denotes the activation of
NlMl
feature i at location k in layer l on the synthesized frames t
and t + 1. The dynamics loss,
dynamics, is deﬁned as the av-
L
erage of the mean squared error between the Gram matrices
of the input texture and those of the generated texture:

RNl×Nl , with entries ˆGlt

∈
jk, where ˆDlt

(cid:80)Ml
k=1

ˆDlt
ik

ˆDlt

dynamics =

L

1
Ldyn(Tout

Tout−1
(cid:88)

(cid:88)

1)

−

t=1

l

Gl

(cid:107)

ˆGlt

2
F ,

(cid:107)

−

(2)

where Ldyn is the number of ConvNet layers being used in
the dynamics stream.

Here we propose to use the output of the concatenation
layer, where the multiscale distributed representation of ori-
entations is stored, as the layer to compute the Gram ma-
trix. While it is tempting to use the predicted ﬂow out-
put from the network, this generally yields poor results as
shown in our evaluation. Due to the complex, temporal vari-
ation present in dynamic textures, they contain a variety of
local spacetime orientations rather than a single dominant
orientation. As a result, the ﬂow estimates will tend to be
an average of the underlying orientation measurements and
consequently not descriptive. A comparison between the
texture synthesis results using the concatenation layer and
the predicted ﬂow output is provided in Sec. 4.

3.3. Texture generation

The overall dynamic texture loss consists of the combi-
nation of the appearance loss, Eq. (1), and the dynamics
loss, Eq. (2):

dynamic texture = α

appearance + β

dynamics,

(3)

L

L

L

where α and β are the weighting factors for the appearance
and dynamics content, respectively. Dynamic textures are
implicitly deﬁned as the (local) minima of this loss. Tex-
tures are generated by optimizing Eq. (3) with respect to
the spacetime volume, i.e., the pixels of the video. Vari-
ations in the resulting texture are found by initializing the
optimization process using IID Gaussian noise. Consistent
with previous work [13], we use L-BFGS [28] optimization.
Naive application of the outlined approach will consume
increasing amounts of memory as the temporal extent of the
dynamic texture grows; this makes it impractical to gener-
ate longer sequences. Instead, long sequences can be in-
crementally generated by separating the sequence into sub-
sequences and optimizing them sequentially. This is real-
ized by initializing the ﬁrst frame of a subsequence as the
last frame from the previous subsequence and keeping it
ﬁxed throughout the optimization. The remaining frames

Figure 3: Dynamics stream ConvNet. The ConvNet is based
on a spacetime-oriented energy model [7, 39] and is trained
for optical ﬂow prediction. Three scales are shown for il-
lustration; in practice ﬁve scales were used.

L1 divisive normalization is applied.

To capture spacetime orientations beyond those capable
with the limited receptive ﬁelds used in the initial layer,
we compute a ﬁve-level spatial Gaussian pyramid. Each
pyramid level is processed independently with the same
spacetime-oriented energy model and then bilinearly up-
sampled to the original resolution and concatenated.

Prior energy model instantiations (e.g., [2, 7, 39]) used
handcrafted ﬁlter weights. While a similar approach could
be followed here, we opt to learn the weights so that they
are better tuned to natural imagery. To train the network
weights, we add additional decoding layers that take the
concatenated distributed representation and apply a 3
3
convolution (with 64 ﬁlters), ReLU activation, and a 1
1
convolution (with 2 ﬁlters) that yields a two channel output
encoding the optical ﬂow directly. The proposed architec-
ture is illustrated in Fig. 3.

×
×

For training, we use the standard average endpoint er-
ror (aEPE) ﬂow metric (i.e., L2 norm) between the pre-
dicted ﬂow and the ground truth ﬂow as the loss. Since no
large-scale ﬂow dataset exists that captures natural imagery
with groundtruth ﬂow, we take an unlabeled video dataset
and apply an existing ﬂow estimator [35] to estimate opti-
cal ﬂow for training, cf . [43]. For training data, we used
videos from the UCF101 dataset [41] with geometric and
photometric data augmentations similar to those used by
FlowNet [9], and optimized the aEPE loss using Adam [24].
Inspection of the learned ﬁlters in the initial layer showed
evidence of spacetime-oriented ﬁlters, consistent with the
handcrafted ﬁlters used in previous work [7].

Similar to the appearance stream, ﬁlter response cor-
relations in a particular layer of the dynamics stream are
averaged over the number of image frame pairs and en-
RNl×Nl , whose en-
capsulated by a Gram matrix, Gl
tries are given by Gl
ikDlt
k=1 Dlt
jk,
where Dlt
ik denotes the activation of feature i at location k

1
(T −1)NlMl

∈
(cid:80)T −1
t=1

ij =

(cid:80)Ml

of the subsequence are initialized randomly and optimized
as above. This ensures temporal consistency across synthe-
sized subsequences and can be viewed as a form of coordi-
nate descent for the full sequence objective. The ﬂexibility
of this framework allows other texture generation problems
to be handled simply by altering the initialization of frames
and controlling which frames or frame regions are updated.

4. Experimental results

The goal of (dynamic) texture synthesis is to gener-
ate samples that are indistinguishable from the real input
target texture by a human observer.
In this section, we
present a variety of synthesis results including a user study
to quantitatively evaluate the realism of our results. Given
their temporal nature, our results are best viewed as videos.
Our two-stream architecture was implemented using Ten-
sorFlow [1]. Results were generated using an NVIDIA Ti-
tan X (Pascal) GPU and synthesis times ranged between
one to three hours to generate 12 frames with an image
resolution of 256
256. For our full synthesis results
and source code, please refer to the supplemental material
on the project website: ryersonvisionlab.github.
io/two-stream-projpage.

×

4.1. Dynamic texture synthesis

We applied our dynamic texture synthesis process to a
wide range of textures which were selected from the Dyn-
Tex [32] database and others we collected in the wild. In-
cluded in our supplemental material are synthesized results
of nearly 60 different textures that encapsulate a range of
phenomena, such as ﬂowing water, waves, clouds, ﬁre, rip-
pling ﬂags, waving plants, and schools of ﬁsh. Some sam-
ple frames are shown in Fig. 4 but we encourage readers to
view the videos to fully appreciate the results. In addition,
we performed a comparison with [12] and [48]. Generally,
we found our results to be qualitatively comparable or better
than these methods. See the supplemental for more details
on the comparisons with these methods.

We also generated dynamic textures incrementally, as
described in Sec. 3.3. The resulting textures were perceptu-
ally indistinguishable from those generated with the batch
process. Another extension that we explored were textures
with no discernible temporal seam between the last and ﬁrst
frames. Played as a loop, these textures appear to be tempo-
rally endless. This was achieved by assuming that the ﬁrst
frame follows the ﬁnal frame and adding an additional loss
for the dynamics stream evaluated on that pair of frames.

Example failure modes of our method are presented
in Fig. 6.
In general, we ﬁnd that most failures result
from inputs that violate the underlying assumption of a
dynamic texture, i.e., the appearance and/or dynamics are
not spatiotemporally homogeneous.
In the case of the
escalator example, the long edge structures in the ap-

pearance are not spatially homogeneous, and the dynam-
ics vary due to perspective effects that change the motion
from downward to outward. The resulting synthesized tex-
ture captures an overall downward motion but lacks the per-
spective effects and is unable to consistently reproduce the
long edge structures. This is consistent with previous ob-
servations on static texture synthesis [13] and suggests it is
a limitation of the appearance stream.

Another example is the flag sequence where the rip-
pling dynamics are relatively homogeneous across the pat-
tern but the appearance varies spatially. As expected, the
generated texture does not faithfully reproduce the appear-
ance; however, it does exhibit plausible rippling dynamics.
In the supplemental material, we include an additional fail-
ure case, cranberries, which consists of a swirling pat-
tern. Our model faithfully reproduces the appearance but is
unable to capture the spatially varying dynamics. Interest-
ingly, it still produces a result which is statistically indistin-
guishable from real in our user study discussed below.

Appearance vs. dynamics streams We sought to verify
that the appearance and dynamics streams were capturing
complementary information. To validate that the texture
generation of multiple frames would not induce dynamics
consistent with the input, we generated frames starting from
randomly generated noise but only using the appearance
statistics and corresponding loss, i.e., Eq. 1. As expected,
this produced frames that were valid textures but with no
coherent dynamics present. Results for a sequence contain-
ing a school of ﬁsh are shown in Fig. 5; to examine the
dynamics, see fish in the supplemental material.

Similarly, to validate that the dynamics stream did not in-
advertently include appearance information, we generated
videos using the dynamics loss only, i.e., Eq. 2. The re-
sulting frames had no visible appearance and had an ex-
tremely low dynamic range, i.e., the standard deviation of
pixel intensities was 10 for values in [0, 255]. This indi-
cates a general invariance to appearance and suggests that
our two-stream dynamic texture representation has factored
appearance and dynamics, as desired.

4.2. User study

Quantitative evaluation for texture synthesis is a partic-
ularly challenging task as there is no single correct output
when synthesizing new samples of a texture. Like in other
image generation tasks (e.g., rendering), human perception
is ultimately the most important measure. Thus, we per-
formed a user study to evaluate the perceived realism of our
synthesized textures.

Similar to previous image synthesis work (e.g., [5]), we
conducted a perceptual experiment with human observers
to quantitatively evaluate our synthesis results. We em-
ployed a forced-choice evaluation on Amazon Mechanical

fireplace 1

(original)

fireplace 1

(synthesized)

lava

(original)

lava

(synthesized)

smoke 1

(original)

smoke 1

(synthesized)

underwater

vegetation 1

(original)

underwater
vegetation 1

(synthesized)

water 3

(original)

water 3

(synthesized)

Figure 4: Dynamic texture synthesis success examples. Names correspond to ﬁles in the supplemental material.

Turk (AMT) with 200 different users. Each user performed
59 pairwise comparisons between a synthesized dynamic
texture and its target. Users were asked to choose which
appeared more realistic after viewing the textures for an ex-
posure time sampled randomly from discrete intervals be-
tween 0.3 and 4.8 seconds. Measures were taken to control
the experimental conditions and minimize the possibility of
low quality data. See the supplemental material for further
experimental details of our user study.

For comparison, we constructed a baseline by using the
ﬂow decode layer in the dynamics loss of Eq. 2. This corre-
sponds with attempting to mimic the optical ﬂow statistics
of the texture directly. Textures were synthesized with this
model and the user study was repeated with an additional
200 users. To differentiate between the models, we label

“Flow decode layer” and “Concat layer” in the ﬁgures to
describe our baseline and ﬁnal model, respectively.

The results of this study are summarized in Fig. 7 which
shows user accuracy in differentiating real versus generated
textures as a function of time for both methods. Over-
all, users are able to correctly identify the real texture
2.5% of the time for brief exposures of 0.3 sec-
66.1%
onds. This rises to 79.6%
1.1% with exposures of 1.2 sec-
onds and higher. Note that “perfect” synthesis results would
have an accuracy of 50%, indicating that users were unable
to differentiate between the real and generated textures and
higher accuracy indicating less convincing textures.

±

±

The results clearly show that the use of the concatenation
layer activations is far more effective than the ﬂow decode
layer. This is not surprising as optical ﬂow alone is known

target
(fish)

appearance
only

both
streams

escalator

(original)

escalator

(synthesized)

flag

(original)

flag

(synthesized)

(top row) Target texture.

Figure 5: Dynamic texture synthesis versus texture synthe-
sis.
(middle) Texture synthesis
without dynamics constraints shows consistent per-frame
appearance but no temporal coherence. (bottom) Including
both streams induces consistent appearance and dynamics.

Figure 6: Dynamic texture synthesis failure examples. In
these cases, the failures are attributed to either the appear-
ance or the dynamics not being homogeneous.

to be unreliable on many textures, particularly those with
transparency or chaotic motion (e.g., water, smoke, ﬂames,
etc.). Also evident in these results is the time-dependant
nature of perception for textures from both models. Users’
ability to identify the generated texture improved as expo-
sure times increased to 1.2 seconds and remained relatively
ﬂat for longer exposures.

To better understand the performance of our approach,
we grouped and analyzed the results in terms of appear-
ance and dynamics characteristics. For appearance we used
the taxonomy presented in [27] and grouped textures as
either regular/near-regular (e.g., periodic tiling and brick
wall), irregular (e.g., a ﬁeld of ﬂowers), or stochastic/near-
stochastic (e.g.,
For dynamics we
grouped textures as either spatially-consistent (e.g., closeup
of rippling sea water) or spatially-inconsistent (e.g., rippling
sea water juxtaposed with translating clouds in the sky). Re-

tv static or water).

Figure 7: Time-limited pairwise comparisons across all tex-
tures with 95% statistical conﬁdence intervals.

sults based on these groupings can be seen in Fig. 8.

1.6% and 90.8%

A full breakdown of the user study results by texture and
grouping can be found in the supplemental material. Here
we discuss some of the overall trends. Based on appear-
ance it is clear that textures with large-scale spatial consis-
tencies (regular, near-regular, and irregular textures) tend to
perform poorly. Examples being flag and fountain 2
with user accuracies of 98.9%
4.3%
averaged across all exposures, respectively. This is not un-
expected and is a fundamental limitation of the local na-
ture of the Gram matrix representation used in the appear-
ance stream which was observed in static texture synthe-
sis [13]. In contrast, stochastic and near-stochastic textures
performed signiﬁcantly better as their smaller-scale local
variations are well captured by the appearance stream, for
instance water 1 and lava which had average accuracies
of 53.8%
7.4%, respectively, making
them both statistically indistinguishable from real.

7.4% and 55.6%

±

±

±

±

±

(e.g.,

dynamics

7.4% and 63.2%

In terms of dynamics, we ﬁnd that

textures with
tv static,
spatially-consistent
water *, and calm water *) perform signiﬁcantly
better than those with spatially-inconsistent dynamics (e.g.,
candle flame, fountain 2, and snake *), where
the dynamics drastically differ across spatial locations.
For example, tv static and calm water 6 have
average accuracies of 48.6%
7.2%,
respectively, while candle flame and snake 5 have
average accuracies of 92.4%
4%,
respectively. Overall, our model is capable of reproducing
a full spectrum of spatially-consistent dynamics. However,
as the appearance shifts from containing small-scale spatial
consistencies,
to containing large-scale
consistencies
performance degrades. This was evident in the user study
where the best-performing textures typically consisted of
a stochastic or near-stochastic appearance with spatially-
In contrast the worst-performing
consistent dynamics.
textures consisted of regular, near-regular, or irregular
appearance with spatially-inconsistent dynamics.

4% and 92.1%

±

±

±

appearance
target

synthesized output

Figure 9: Dynamics style transfer. (top row) Appearance of
still water was used with the dynamics of a different water
dynamic texture (water 4). (bottom row) The appearance
of a painting of ﬁre was used with the dynamics of a real
ﬁre (fireplace 1). Animated results and additional ex-
amples are available in the supplemental material.

ance and dynamics. We applied this model to a variety of
dynamic texture synthesis tasks and showed that, so long
as the input textures are generally true dynamic textures,
i.e., have spatially invariant statistics and spatiotemporally
invariant dynamics, the resulting synthesized textures are
compelling. This was validated both qualitatively and quan-
titatively through a large user study. Further, we showed
that the two-stream model enabled dynamics style transfer,
where the appearance and dynamics information from dif-
ferent sources can be combined to generate a novel texture.
We have explored this model thoroughly and found a few
limitations which we leave as directions for future work.
First, much like has been reported in recent image style
transfer work [14], we have found that high frequency noise
and chromatic aberrations are a problem in generation. An-
other issue that arises is the model fails to capture textures
with spatially-variant appearance, (e.g., flag in Fig. 6) and
spatially-inconsistent dynamics (e.g., escalator in Fig.
6). By collapsing the local statistics into a Gram matrix,
the spatial and temporal organization is lost. Simple post-
processing methods may alleviate some of these issues but
we believe that they also point to a need for a better rep-
resentation. Beyond addressing these limitations, a natural
next step would be to extend the idea of a factorized rep-
resentation into feed-forward generative networks that have
found success in static image synthesis, e.g., [22, 44].

Acknowledgements MT is supported by a Natural Sci-
ences and Engineering Research Council of Canada
(NSERC) Canadian Graduate Scholarship. KGD and MAB
are supported by NSERC Discovery Grants. This research
was undertaken as part of the Vision: Science to Applica-
tions program, thanks in part to funding from the Canada
First Research Excellence Fund.

Figure 8: Time-limited pairwise comparisons across all tex-
tures, grouped by appearance (top) and dynamics (bottom).
Shown with 95% statistical conﬁdence intervals.

4.3. Dynamics style transfer

The underlying assumption of our model is that appear-
ance and dynamics of texture can be factorized. As such, it
should allow for the transfer of the dynamics of one texture
onto the appearance of another. This has been explored pre-
viously for artistic style transfer [4, 15] with static imagery.
We accomplish this with our model by performing the same
optimization as above, but with the target Gram matrices for
appearance and dynamics computed from different textures.
A dynamics style transfer result is shown in Fig. 9 (top),
using two real videos. Additional examples are available
in the supplemental material. We note that when perform-
ing dynamics style transfer it is important that the appear-
ance structure be similar in scale and semantics, otherwise,
the generated dynamic textures will look unnatural. For in-
stance, transferring the dynamics of a ﬂame onto a water
scene will generally produce implausible results.

We can also apply the dynamics of a texture to a static
input image, as the target Gram matrices for the appearance
loss can be computed on just a single frame. This allows us
to effectively animate regions of a static image. The result
of this process can be striking and is visualized in Fig. 9
(bottom), where the appearance is taken from a painting and
the dynamics from a real world video.

5. Discussion and summary

In this paper, we presented a novel, two-stream model of
dynamic textures using ConvNets to represent the appear-

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 5
[2] E. H. Adelson and J. R. Bergen. Spatiotemporal energy mod-
els for the perception of motion. JOSA–A, 2(2):284–299,
1985. 2, 3, 4

[3] Z. Bar-Joseph, R. El-Yaniv, D. Lischinski, and M. Werman.
Texture mixing and texture movie synthesis using statistical
learning. T-VCG, 7(2):120–135, 2001. 1

[4] A. J. Champandard. Semantic style transfer and turning two-
bit doodles into ﬁne artworks. arXiv:1603.01768, 2016. 8
[5] Q. Chen and V. Koltun. Photographic image synthesis with

cascaded reﬁnement networks. In ICCV, 2017. 5

[6] J. E. Cutting. Blowing in the wind: Perceiving structure in

trees and bushes. Cognition, 12(1):25 – 44, 1982. 1

[7] K. G. Derpanis and R. P. Wildes. Spacetime texture represen-
tation and recognition based on a spatiotemporal orientation
analysis. PAMI, 34(6):1193–1205, 2012. 1, 2, 3, 4

[8] G. Doretto, A. Chiuso, Y. N. Wu, and S. Soatto. Dynamic

textures. IJCV, 51(2):91–109, 2003. 1, 2

[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. H¨ausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
FlowNet: Learning optical ﬂow with convolutional net-
works. In ICCV, pages 2758–2766, 2015. 2, 3, 4

[10] A. A. Efros and T. K. Leung. Texture synthesis by non-

parametric sampling. In ICCV, pages 1033–1038, 1999. 2

[11] M. Fahle and T. Poggio. Visual hyperacuity: Spatiotemporal
interpolation in human vision. Proceedings of the Royal So-
ciety of London B: Biological Sciences, 213(1193):451–477,
1981. 3

[12] C. M. Funke, L. A. Gatys, A. S. Ecker, and M. Bethge. Syn-
thesising dynamic textures using convolutional neural net-
works. arXiv:1702.07006, 2017. 2, 5, 10, 11

[13] L. A. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis
using convolutional neural networks. In NIPS, pages 262–
270, 2015. 1, 2, 3, 4, 5, 7

[14] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks. In CVPR, pages 2414–
2423, 2016. 2, 8

[15] L. A. Gatys, A. S. Ecker, M. Bethge, A. Hertzmann, and
E. Shechtman. Controlling perceptual factors in neural style
transfer. In CVPR, 2017. 8

[16] M. A. Goodale and A. D. Milner. Separate visual path-
ways for perception and action. Trends in Neurosciences,
15(1):20–25, 1992. 2

[18] D. J. Heeger. Optical ﬂow using spatiotemporal ﬁlters. IJCV,

1(4):279–302, 1988. 2, 3

[19] D. J. Heeger and J. R. Bergen. Pyramid-based texture analy-
sis/synthesis. In SIGGRAPH, pages 229–238, 1995. 1, 2
[20] B. K. P. Horn and B. G. Schunck. Determining optical ﬂow.

A.I., 17:185–203, 1981. 2

[21] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet 2.0: Evolution of optical ﬂow estimation
with deep networks. In CVPR, 2017. 2, 3

[22] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution. In ECCV, pages
694–711, 2016. 8

[23] B. Julesz. Visual pattern discrimination. IRE Trans. Infor-

mation Theory, 8(2):84–92, 1962. 2

[24] D. P. Kingma and J. Ba. Adam: A Method for Stochastic

Optimization. arXiv:1412.6980, 2014. 4

[25] K. Konda, R. Memisevic, and V. Michalski. Learning to en-
code motion using spatio-temporal synchrony international
conference on learning representation. In ICLR, 2014. 2
[26] V. Kwatra, A. Sch¨odl, I. Essa, G. Turk, and A. Bobick.
Graphcut textures: Image and video synthesis using graph
cuts. In SIGGRAPH, pages 277–286, 2003. 2

[27] W.-C. Lin, J. Hays, C. Wu, Y. Liu, and V. Kwatra. Quantita-
tive evaluation of near regular texture synthesis algorithms.
In CVPR, volume 1, pages 427–434, 2006. 7

[28] D. C. Liu and J. Nocedal. On the limited memory method
for large scale optimization. Mathematical Programming,
45(3):503–528, 1989. 4

[29] B. D. Lucas and T. Kanade. An iterative image registra-
tion technique with an application to stereo vision. In IJCAI,
pages 674–679, 1981. 2

[30] R. Nelson and R. Polana. Qualitative recognition of motion

using temporal textures. CVGIP, 56(1), 1992. 1

[31] S. Nishimoto and J. L. Gallant. A three-dimensional spa-
tiotemporal receptive ﬁeld model explains responses of area
mt neurons to naturalistic movies. Journal of Neuroscience,
31(41):14551–14564, 2011. 2

[32] R. P´eteri, S. Fazekas, and M. J. Huiskes. DynTex: A Com-
prehensive Database of Dynamic Textures. PRL, 31(12),
2010. 5

[33] J. Portilla and E. P. Simoncelli. A parametric texture model
based on joint statistics of complex wavelet coefﬁcients.
IJCV, 40(1):49–70, 2000. 1, 2

[34] A. Ranjan and M. J. Black. Optical Flow Estimation using a

Spatial Pyramid Network. In CVPR, 2017. 2

[35] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-preserving interpolation of correspon-
dences for optical ﬂow. In CVPR, pages 1164–1172, 2015.
2, 4

[36] M. Ruder, A. Dosovitskiy, and T. Brox. Artistic style transfer

for videos. In GCPR, pages 26–36, 2016. 2

[37] A. Sch¨odl, R. Szeliski, D. Salesin, and I. A. Essa. Video

textures. In SIGGRAPH, pages 489–498, 2000. 2

[17] D. Heeger and A. Pentland. Seeing structure through chaos.
In IEEE Motion Workshop: Representation and Analysis,
pages 131–136, 1986. 1

[38] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional
networks for semantic segmentation. PAMI, 39(4):640–651,
2017. 3

[39] E. P. Simoncelli and D. J. Heeger. A model of neuronal re-
sponses in visual area MT. Vision Research, 38(5):743 – 761,
1998. 2, 3, 4

[40] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014. 2

[41] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A
dataset of 101 human actions classes from videos in the wild.
arXiv:1212.0402, 2012. 4

[42] M. Szummer and R. W. Picard. Temporal texture modeling.

In ICIP, pages 823–826, 1996. 2

[43] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and
M. Paluri. Deep end2end voxel2voxel prediction. In CVPR
Workshops, pages 402–409, 2016. 4

[44] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. S. Lempitsky.
Texture networks: Feed-forward synthesis of textures and
stylized images. In ICML, pages 1349–1357, 2016. 8
[45] Y. Wang and S. C. Zhu. Modeling textured motion: Particle,

wave and sketch. In ICCV, pages 213–220, 2003. 1

[46] A. B. Watson and A. J. Ahumada. A look at motion in the
frequency domain. In Motion workshop: Perception and rep-
resentation, pages 1–10, 1983. 3

[47] L. Wei and M. Levoy. Fast texture synthesis using tree-
structured vector quantization. In SIGGRAPH, pages 479–
488, 2000. 2

[48] J. Xie, S.-C. Zhu, and Y. N. Wu. Synthesizing dynamic
patterns by spatial-temporal generative convnet. In CVPR,
2017. 2, 5, 11

[49] J. J. Yu, A. W. Harley, and K. G. Derpanis. Back to Ba-
sics: Unsupervised Learning of Optical Flow via Brightness
Constancy and Motion Smoothness. In ECCVW, 2016. 2

A. Experimental procedure

Here we provide further experimental details of our user study
using Amazon Mechanical Turk (AMT). Experimental trials were
grouped into batches of Human Intelligence Tasks (HITs) for users
to complete. Each HIT consisted of 59 pairwise comparisons be-
tween a synthesized dynamic texture and its target. Users were
asked to choose which texture appeared more realistic after view-
ing each texture independently for an exposure time (in seconds)
sampled randomly from the set {0.3, 0.4, 0.6, 1.2, 2.4, 3.6, 4.8}.
Note that 12 frames of the dynamic texture corresponds to 1.2 sec-
onds, i.e., 10 frames per second. Before viewing a dynamic tex-
ture, a centred dot is ﬂashed twice to indicate to the user where
to look (left or right). To prepare users for the task, the ﬁrst three
comparisons were used for warm-up, exposing them to the short-
est (0.3s), median (1.2s), and longest (4.8s) durations. To prevent
spamming and bias, we constrained the experiment as follows:
users could make a choice only after both dynamic textures were
shown; the next texture comparison could only be made after a
decision was made for the current comparison; a choice could not
be changed after the next pair of dynamic textures were shown;
and users were each restricted to a single HIT. Obvious unrealistic
dynamic textures were synthesized by terminating synthesis early
(100 iterations) and were used as sentinel tests. Three of the 59
pairwise comparisons were sentinels and results from users which
gave incorrect answers on any of the sentinel comparisons were

not used. The left-right order of textures within a pair, display
order within a pair, and order of pairs within a HIT, were random-
ized. An example of a HIT is shown in a video included with the
supplemental on the project page: HIT example.mp4.

Users were paid $2 USD per HIT, and were required to have
at least a 98% HIT approval rating, greater than or equal to 5000
HITs approved, and to be residing in the US. We collected results
from 200 unique users to evaluate our ﬁnal model and another 200
to evaluate our baseline model.

B. Qualitative results

We provide videos showcasing the qualitative results of our
two-stream model, including the experiments mentioned in the
main manuscript, on our project page: ryersonvisionlab.
github.io/two-stream-projpage. The videos are in
MP4 format (H.264 codec) and are best viewed in a loop. They
are enclosed in the following folders:

• target textures: This folder contains the 59 dynamic

textures used as targets for synthesis.

• dynamic texture synthesis: This folder contains
synthesized dynamic textures where the appearance and dy-
namics targets are the same.

• using concatenation layer: This folder contains
synthesized dynamic textures where the concatenation layer
was used for computing the Gramian on the dynamics
stream. These are the results from our ﬁnal model.

• using flow decode layer: This folder contains syn-
thesized dynamic textures where the predicted ﬂow output
is used for computing the Gramian on the dynamics stream.
These are the results from our baseline.

• full synthesis:

This

folder

contains

synthesized dynamic textures,
generated, nor temporally-endless, etc.

i.e., not

regularly-
incrementally-

• appearance stream only: This folder contains dy-
namic textures synthesized using only the appearance stream
of our two-stream model. The dynamics stream is not used.

• incrementally generated: This folder contains dy-
namic textures synthesized using the incremental process
outlined in Section 3.3 in the main manuscript.

• temporally endless: This folder contains a synthe-
sized dynamic texture (smoke plume 1) where there is no
discernible temporal seam between the last and ﬁrst frames.
Played as a loop, it appears to be temporally endless, thus, it
is presented in animated GIF format.

• dynamics style transfer: This folder contains syn-
thesized dynamic textures where the appearance and dynam-
ics targets are different. Also included are videos where the
synthesized dynamic texture is “pasted” back onto the origi-
nal image it was cropped from, showing a proof-of-concept
of dynamics style transfer as an artistic tool.

• comparisons/funke: This folder contains four dy-
namic texture synthesis comparisons between our model and
a recent (unpublished) approach [12]. The dynamic textures

(labeled “Xie et al. (ST)”) designed for dynamic textures with both
spatial and temporal homogeneity, and their temporal model (la-
beled “Xie et al. (FC)”) designed for dynamic textures with only
temporal homogeneity.

Overall, we demonstrate that our results appear qualitatively
better, showing more temporal coherence and similarity in dy-
namics and fewer artifacts, e.g., blur and ﬂicker. This may be a
natural consequence of their limited representation of dynamics.
Although the spatiotemporal model of Xie et al. [48] is able to
synthesize dynamic textures that lack spatial homogeneity (e.g.,
bamboo and escalator), we note that their method can not
synthesize novel dynamic textures, i.e., it appears to faithfully re-
produce the target texture, reducing the applicability of their ap-
proach.

As a consequence of jointly modelling appearance and dynam-
ics, the methods of [12, 48] are not capable of the novel form of
style transfer we demonstrated. This was enabled by the factored
representation of dynamics and appearance. Furthermore, the spa-
tiotemporal extent of the output sequence generated by Xie et al.’s
[48] method is limited to being equal to the input. The proposed
approach does not share this limitation.

[12] which ex-
chosen are those reported by Funke et al.
hibit spatiotemporal homogeneity. For ease of comparison,
we have concatenated the results from both models with their
corresponding targets.

• comparisons/xie and funke: This folder contains
nine dynamic texture synthesis comparisons between our
model, Funke et al.’s [12], and Xie et al.’s [48]. The dynamic
textures chosen cover the full range of our appearance and
dynamics groupings. For ease of comparison, we have con-
catenated the results from all models with their correspond-
ing targets.

C. Full user study results

Figures 10a and 10b show histograms of the average user ac-
curacy on each texture, averaged over a range of exposure times.
The histogram bars are ordered from lowest to highest accuracy,
based on the results when using our ﬁnal model.

Tables 1 and 2 show the average user accuracy on each texture
when using our ﬁnal model. The results are averaged over expo-
sure times. Similarly, Tables 3 and 4 show the results when using
our baseline.

Tables 5 and 6 show the average user accuracy on texture ap-
pearance groups when using our ﬁnal model. The results are av-
eraged over exposure times. Similarly, Tables 7 and 8 show the
results when using our baseline.

Tables 9 and 10 show the average user accuracy on texture dy-
namics groups when using our ﬁnal model. The results are aver-
aged over exposure times. Similarly, Tables 11 and 12 show the
results when using our baseline.

Tables 13 and 14 show the average user accuracy over all tex-
tures when using our ﬁnal model. The results are averaged over
exposure times. Similarly, Tables 15 and 16 show the results when
using our baseline.

D. Qualitative comparisons

We qualitatively compare our results to those of Funke
et al. [12] and Xie et al. [48].
Note that Funke et al.
[12] provided results on only ﬁve textures and of those only
four are dynamic textures in the sense that
their appear-
ance and dynamics are spatiotemporally coherent. Their re-
sults on these sequences (cranberries, flames, leaves,
and water 5) are included in the folder funke under
dynamic texture synthesis/comparisons. Our re-
sults are included as well.

We also compare our results to [12, 48] on nine dynamic tex-
tures chosen to cover the full range of our dynamics and appear-
ance groupings. We use their publicly available code and follow
the parameters used in their experiments. For Funke et al.’s model
[12], the parameters used are ∆t = 4 and T = 12 (recall that
target dynamic textures consist of 12 frames). For the spatiotem-
poral and temporal models from Xie et al. [48], the parameters
used are T = 1200 and ˜M = 3. A comparison between our
results, Funke et al.’s [12], and Xie et al’s [48] on the nine dy-
namic textures are included in the folder xie and funke un-
der dynamic texture synthesis/comparisons. Note
for Xie et al. [48], we compare with their spatiotemporal model

.
)
s

m
0
0
6
-
0
0
3
(

s
e
m

i
t

e
r
u
s
o
p
x
e

t
r
o
h
S
)
a
(

.
)
s

m
0
0
8
4
-
0
0
2
1
(

s
e
m

i
t

e
r
u
s
o
p
x
e
g
n
o
L
)
b
(

.
e
c
n
e
d
ﬁ
n
o
c

l
a
c
i
t
s
i
t
a
t
s

%
5
9
a
h
t
i

w

r
o
r
r
e

f
o
n
i
g
r
a
m
a

s
e
d
u
l
c
n
i

y
c
a
r
u
c
c
a

e
r
u
t
x
e
t

h
c
a
E

.
s
e
m

i
t

e
r
u
s
o
p
x
e

r
e
v
o

d
e
g
a
r
e
v
a

s
e
i
c
a
r
u
c
c
a

e
r
u
t
x
e
t
-
r
e
P

:
0
1

e
r
u
g
i
F

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

±
±
±
0.0

±
±
±
0.248

0.203
0.095
0.175
0.135
0.217

±
±
±
±
±
0.175

0.205
±
0.196
0.162
0.12
0.212

0.194
±
0.162
±
0.199
±
0.139
±
0.252
±
0.181
±
0.195
0.135
0.186
0.199

300 ms.
0.625
0.769
0.609
0.806
0.533
0.607
0.44
±
0.813
0.727
0.609
0.6
±
0.139
0.806
±
0.168
0.81
±
0.5
0.219
±
0.591
0.48
±
0.792
0.909
0.571
1.0
±
0.069
0.964
±
0.176
0.72
±
0.5
0.209
±
0.435
0.929
0.452
0.813
0.632
0.8
±
0.476
0.458
0.632
0.6
±
0.542
0.517
0.767
0.667
0.792
0.538
0.478
0.769
0.724
0.862
0.72
±
0.643
0.643
0.826
0.538
0.656
0.556
0.375
0.632
0.545
0.688
0.571
0.444

0.199
±
0.182
±
0.151
±
0.202
±
0.162
±
0.192
±
0.204
±
0.162
±
0.163
±
0.126
±
0.176
0.177
0.177
0.155
0.192
0.165
0.23
0.237
0.217
0.208
0.161
0.183
0.187

±
±
±
0.192

0.214
0.199
0.217

±
±
±
±
±
±
±
±
±
±
±
±

400 ms.
0.161
0.333
±
0.215
0.786
±
0.152
0.786
±
0.127
0.88
±
0.164
0.842
±
0.212
0.571
±
0.621
0.177
±
0.245
0.5
±
0.183
0.654
±
0.175
0.773
±
0.175
0.773
±
0.212
0.75
±
0.129
0.839
±
0.429
0.212
±
0.168
0.81
±
0.195
0.318
±
0.158
0.733
±
0.952
0.091
±
0.209
0.65
±
0.0
1.0
±
1.0
0.0
±
0.909
0.12
±
0.565
0.203
±
0.688
0.227
±
0.826
0.155
±
0.538
0.192
±
0.778
0.192
±
0.667
0.202
±
0.903
0.104
±
0.714
0.167
±
0.346
0.183
±
0.667
0.202
±
0.769
0.162
±
0.625
0.168
±
0.741
0.165
±
0.903
0.104
±
0.737
0.198
±
0.938
0.119
±
0.731
0.17
±
0.727
0.186
±
0.833
0.149
±
0.783
0.169
±
0.704
0.172
±
0.708
0.182
±
0.773
0.175
±
0.815
0.147
±
0.947
0.1
±
0.63
0.182
±
0.231
0.5
±
0.183
0.32
±
0.586
0.179
±
0.188
0.64
±
0.165
0.741
±
0.218
0.667
±
0.179
0.586
±
0.201
0.364
±

0.193
0.164
0.187
0.196

±
±
±
±
0.164

±
±
±
±
±
±
±
±
0.0

0.139
0.186
0.155
0.185
0.188
0.111
0.165
0.069

600 ms.
0.714
0.842
0.615
0.846
0.7
±
0.187
0.615
±
0.156
0.622
±
0.169
0.667
±
0.209
0.65
±
0.205
0.591
±
0.643
0.177
±
1.0
0.0
±
0.788
0.727
0.826
0.593
0.696
0.897
0.656
0.964
1.0
±
0.115
0.913
±
0.181
0.552
±
0.151
0.808
±
0.147
0.815
±
0.177
0.621
±
0.202
0.667
±
0.151
0.767
±
0.096
0.95
±
0.173
0.679
±
0.23
0.556
±
0.195
0.652
±
0.155
0.826
±
0.174
0.581
±
0.175
0.8
±
0.16
0.75
±
0.613
0.171
±
0.058
0.97
±
0.165
0.741
±
0.6
0.215
±
0.119
0.938
±
0.168
0.81
±
0.155
0.826
±
0.191
0.813
±
0.111
0.917
±
0.193
0.714
±
0.103
0.889
±
0.19
0.423
±
0.222
0.579
±
0.169
0.667
±
0.652
0.195
±
0.196
0.52
±
0.173
0.75
±
0.179
0.586
±
0.227
0.688
±
0.197
0.583
±

0.12

±
0.131

±
±
0.2

0.151
0.199

1200 ms.
0.185
0.536
±
0.101
0.906
±
0.199
0.542
±
0.193
0.714
±
0.138
0.87
±
0.636
0.164
±
0.201
0.7
±
0.201
0.7
±
0.767
0.609
0.5
±
0.909
0.9
±
0.164
0.636
±
0.147
0.815
±
0.188
0.64
±
0.064
0.967
±
0.111
0.917
±
0.195
0.652
±
0.062
0.968
±
0.102
0.923
±
0.119
0.889
±
0.118
0.871
±
0.149
0.833
±
1.0
0.0
±
0.15
0.75
±
0.792
0.162
±
0.127
0.88
±
0.08
0.958
±
0.163
0.724
±
0.158
0.733
±
0.151
0.767
±
0.955
0.087
±
0.173
0.75
±
0.609
0.199
±
1.0
0.0
±
0.176
0.72
±
0.083
0.957
±
0.237
0.471
±
0.176
0.72
±
0.142
0.821
±
0.963
0.071
±
0.127
0.88
±
0.08
0.958
±
0.87
0.138
±
1.0
0.0
±
0.875
0.615
0.821
0.727
0.826
0.739
0.833
0.759
0.792
0.75

0.132
±
0.187
±
0.142
±
0.186
±
0.155
±
0.179
±
0.149
±
0.156
±
0.162
±
0.16
±

0.188

0.201

0.122

0.195
0.181
0.132
0.182
0.188

0.119
0.195
0.175
0.175
0.126

0.119

0.139
0.126
0.198
0.148

±
0.0
0.0

±
±
±
±
±
0.0

2400 ms.
0.636
±
0.95
0.096
±
0.867
±
0.058
0.97
±
0.731
0.17
±
0.75
0.19
±
0.652
±
0.824
±
0.875
±
0.708
±
0.519
±
1.0
0.0
±
0.938
0.652
0.773
0.548
0.933
1.0
±
0.696
1.0
±
1.0
±
0.889
±
0.92
0.106
±
0.788
±
0.905
±
0.737
±
0.735
±
0.0
1.0
±
0.0
1.0
±
0.808
0.593
0.806
0.857
0.75
0.9
±
0.952
±
0.652
±
0.92
0.106
±
0.895
±
0.5
0.173
±
0.931
±
0.84
0.144
±
0.905
±
0.852
±
0.913
±
0.917
±
0.923
±
0.227
±
0.813
±
0.571
±
0.706
±
0.667
±
0.771
±
0.65
0.209
±
0.696
±
0.182
0.37
±

±
±
±
±
0.19
±
0.131

0.091
0.195

0.138

0.092

0.126
0.134
0.115
0.111
0.102
0.175
0.191
0.212
0.153
0.202
0.139

0.151
0.185
0.139
0.15

0.188

0.15
±
0.084
±
0.195
±
0.077
0.134
±
0.182
±
0.175
±
0.182
0.122
0.163
0.202

0.071
0.163
0.123
0.188
0.099
0.074
0.177

0.132
±
0.111
±
0.189
±
0.064
±
0.225
±
0.138
±
0.127
0.106
0.169
±
0.204
±
0.15
±
0.069
±
0.252
±
0.151
±
0.138
0.259
±
0.119
±
0.167
0.163
0.062
0.157

±
±
±
0.0
0.107
0.0

0.119

±
±
±
0.0

±
±
±
±
±
±
±
0.0
0.0

3600 ms.
0.857
0.938
0.682
0.96
±
0.852
0.762
0.773
0.63
±
0.848
0.724
0.765
1.0
±
0.963
0.724
0.885
0.519
0.926
0.962
0.692
1.0
±
1.0
±
0.875
0.917
0.667
0.967
0.526
0.895
0.88
±
0.92
±
0.783
0.522
0.857
0.964
0.533
0.767
0.87
±
0.571
0.889
0.76
±
0.724
0.968
0.778
1.0
±
0.9
±
1.0
±
0.889
1.0
±
0.619
0.733
0.583
0.818
0.724
0.652
0.652
0.731
0.632

±
±
±
±
±
±
±
±
±

±
0.0

0.208
0.158
0.197
0.161
0.163
0.195
0.195
0.17
0.217

±
±
0.0

0.066
0.133

±
±
±
±
0.0

0.172
0.099
0.192
0.071

0.182
0.217
0.143
0.195
0.152
0.151
0.062
0.091
0.165
0.137
0.214
0.147

±
±
±
±
±
±
±
±
±
±
±
±
0.0
0.179
0.0

4800 ms.
0.704
0.926
0.778
0.963
1.0
±
0.762
0.706
0.781
0.682
0.786
0.658
0.968
0.952
0.741
0.828
0.524
0.815
1.0
±
0.5
±
1.0
±
0.966
0.833
1.0
±
0.151
0.808
±
0.089
0.933
±
0.218
0.667
±
0.155
0.826
±
0.135
0.813
±
0.119
0.889
±
0.138
0.87
±
0.195
0.652
±
0.077
0.96
±
0.127
0.88
±
0.151
0.808
±
0.195
0.652
±
0.145
0.889
±
0.15
0.714
±
0.074
0.962
±
0.165
0.588
±
0.182
0.63
±
0.0
1.0
±
0.87
0.138
±
0.0
1.0
±
0.127
0.88
±
0.069
0.964
0.852
0.134
1.0
±
0.333
0.821
0.394
0.917
0.7
±
0.682
0.667
0.833
0.452

±
±
±
±
0.164

0.195
0.189
0.133
0.175

0.178
0.142
0.167
0.111

±
±
0.0

±
±
±
±

Table 1: Per-texture accuracies averaged over exposure times, using the concatenation layer. Each texture accuracy includes
a margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

±
±
±
±
±
0.111

Short (300-600 ms.)
0.111
0.526
0.103
0.797
0.105
0.675
0.086
0.841
0.703
0.112
0.6
±
0.102
0.571
±
0.102
0.692
±
0.111
0.676
±
0.114
0.657
±
0.114
0.677
±
0.08
0.861
±
0.083
0.812
±
0.123
0.556
±
0.106
0.742
±
0.114
0.473
±
0.098
0.74
±
0.917
0.064
±
0.111
0.63
±
0.025
0.987
±
0.03
0.985
±
0.085
0.843
±
0.114
0.541
±
0.116
0.646
±
0.077
0.859
±
0.105
0.535
±
0.761
0.099
±
0.7
0.107
±
0.074
0.887
±
0.107
0.636
±
0.118
0.441
±
0.118
0.651
±
0.101
0.73
±
0.103
0.586
±
0.106
0.671
±
0.082
0.809
±
0.11
0.662
±
0.068
0.904
±
0.104
0.671
±
0.119
0.6
±
0.09
0.833
±
0.097
0.767
±
0.089
0.797
±
0.107
0.738
±
0.096
0.77
±
0.101
0.724
±
0.071
0.885
±
0.11
0.532
±
0.116
0.594
±
0.115
0.521
±
0.118
0.559
±
0.116
0.594
±
0.107
0.685
±
0.105
0.646
±
0.112
0.603
±
0.114
0.466
±

Long (1200-4800 ms.)
0.093
0.673
±
0.048
0.928
±
0.09
0.723
±
0.053
0.915
±
0.066
0.864
±
0.091
0.716
±
0.098
0.707
±
0.089
0.729
±
0.075
0.798
±
0.087
0.712
±
0.604
0.093
±
0.033
0.97
±
0.051
0.94
±
0.086
0.688
±
0.073
0.827
±
0.095
0.558
±
0.057
0.909
±
0.971
0.032
±
0.094
0.627
±
0.02
0.99
±
0.971
0.032
±
0.063
0.87
±
0.054
0.918
±
0.079
0.776
±
0.045
0.947
±
0.682
0.097
±
0.8
0.078
±
0.064
0.88
±
0.046
0.941
±
0.079
0.792
±
0.093
0.631
±
0.069
0.841
±
0.055
0.917
±
0.094
0.729
±
0.729
0.089
±
0.054
0.93
±
0.093
0.68
±
0.05
0.931
±
0.094
0.674
±
0.089
0.637
±
0.049
0.927
±
0.067
0.863
±
0.947
0.045
±
0.058
0.896
±
0.047
0.94
±
0.903
0.06
±
0.043
0.95
±
0.448
0.099
±
0.078
0.794
±
0.098
0.55
±
0.076
0.806
±
0.709
0.088
±
0.084
0.74
±
0.093
0.688
±
0.082
0.767
±
0.095
0.543
±

All (300-4800 ms.)
0.072
0.608
±
0.048
0.882
±
0.069
0.702
±
0.047
0.886
±
0.06
0.802
±
0.071
0.665
±
0.072
0.636
±
0.067
0.713
±
0.064
0.751
±
0.069
0.69
±
0.072
0.632
±
0.04
0.924
±
0.876
0.05
±
0.071
0.64
±
0.061
0.794
±
0.073
0.522
±
0.055
0.835
±
0.033
0.949
±
0.072
0.629
±
0.016
0.989
±
0.976
0.023
±
0.051
0.86
±
0.064
0.756
±
0.067
0.727
±
0.043
0.908
±
0.073
0.609
±
0.062
0.784
±
0.059
0.806
±
0.041
0.919
±
0.066
0.725
±
0.074
0.556
±
0.063
0.771
±
0.056
0.835
±
0.071
0.657
±
0.068
0.703
±
0.05
0.869
±
0.673
0.071
±
0.04
0.92
±
0.07
0.672
0.071
0.624
0.046
0.892
0.057
0.823
0.049
0.879
0.055
0.836
0.05
0.868
0.058
0.822
0.04
0.921
0.074
0.486
0.068
0.713
0.074
0.538
0.068
0.708
0.071
0.663
0.066
0.718
0.07
0.669
0.068
0.699
0.073
0.511

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±

Table 2: Per-texture accuracies averaged over a range of exposure times, using the concatenation layer. Each texture accuracy
includes a margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

0.126

0.138
0.196
0.151
0.135

0.162

0.115
±
0.106
±
0.202
±
0.143
±
0.144
0.212
0.1
±
0.126
±
0.089
±
0.162
±
0.08
±
0.189
±
0.112
±
0.199
±
0.096
0.161
0.091

0.1
0.091

±
±
0.131

0.145
±
0.156
0.151
0.112

±
0.0

±
0.0

±
±
±
±
0.0
0.0

300 ms.
0.933
1.0
±
0.895
0.846
0.808
0.929
1.0
±
1.0
±
0.875
1.0
±
0.913
0.944
0.765
0.864
0.84
±
0.75
±
0.947
0.905
0.933
0.875
0.958
0.667
0.941
0.609
0.95
±
0.818
0.952
1.0
±
1.0
±
0.947
0.952
0.9
±
0.889
0.85
±
0.808
0.941
1.0
±
0.941
0.867
0.667
1.0
±
1.0
±
0.941
0.958
0.957
1.0
±
0.909
0.684
0.857
0.929
0.778
0.867
0.737
1.0
±
0.947
0.941

±
±
±
±
±
±
±
0.0

±
±
±
0.0
0.0

±
±
0.0
0.0

±
±
±
0.0

±
±
0.0

±
±

0.112
0.172
0.239

0.112
0.08
0.083

0.12
0.209
0.183
0.135
0.272
0.172
0.198

0.1
0.112

0.186

0.1
0.111

0.202
0.083

±
±
±
±
±
0.0
0.0

0.106
0.195
0.138
0.119
0.071

±
±
0.0
0.0
0.138
0.132
0.083
0.111

400 ms.
0.9
±
0.944
0.652
0.895
0.889
0.963
1.0
±
1.0
±
0.947
0.897
1.0
±
1.0
±
0.87
±
0.875
±
0.957
±
0.917
±
1.0
0.0
±
0.765
±
0.957
±
1.0
0.0
±
1.0
0.0
±
0.75
0.19
±
0.88
0.127
±
0.209
0.65
±
0.0
1.0
±
0.096
0.95
±
0.938
0.119
±
0.92
0.106
±
0.0
1.0
±
0.962
1.0
±
1.0
±
1.0
±
0.857
0.8
±
0.857
1.0
±
1.0
±
0.773
0.957
1.0
±
0.958
1.0
±
0.917
1.0
±
0.947
1.0
±
0.588
0.958
0.778
1.0
±
1.0
±
0.905
0.944
0.933
0.88

0.126
±
0.106
±
0.126
±
0.127
±

0.234
0.08
0.192

±
0.175
0.15

±
±
±
0.0
0.0

0.175
0.083

±
0.0
0.0
0.0

±
0.0
0.0

±
±
0.0

0.074

0.183

0.111

±
0.0

±
0.0

±
0.0

0.08

0.1

0.08

±
0.0

±
0.0

±
0.0

0.119

0.099

0.115

0.111

±
0.0
0.0

±
±
±
0.0

±
±
0.0
0.0

±
±
0.0
0.0

0.066
0.083

0.102
0.106

±
±
±
±
0.0

0.126
0.083
0.193

0.207
0.186
0.229
0.1

600 ms.
0.913
1.0
±
0.933
0.957
0.714
1.0
±
0.966
0.957
1.0
±
1.0
±
0.958
1.0
±
0.938
1.0
±
1.0
±
0.926
1.0
±
0.923
0.944
1.0
±
1.0
±
0.722
0.727
0.769
0.947
1.0
±
0.917
1.0
±
0.08
0.958
±
0.077
0.96
±
0.941
0.112
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.75
±
0.923
±
0.947
±
0.941
±
0.846
±
0.0
1.0
±
1.0
0.0
±
0.964
1.0
±
0.962
1.0
±
1.0
±
1.0
±
0.64
±
0.952
±
0.952
±
0.889
±
0.962
±
0.938
±
1.0
0.0
±
0.952
0.947

0.188
0.091
0.091
0.119
0.074
0.119

0.19
0.102
0.1
0.112
0.139

0.091
0.1

±
0.0
0.0
0.0

0.069

0.074

±
0.0

±
0.0

±
±

0.08

0.15

±
0.0

±
0.0

±
0.0

±
0.0

0.112

0.071

0.126

0.183

±
0.0
0.0

±
0.0
0.0

±
±
±
0.0

0.115
0.091
0.087

1200 ms.
0.963
1.0
±
0.1
0.947
±
0.077
0.96
±
0.096
0.95
±
0.962
0.074
±
1.0
0.0
±
0.941
1.0
±
0.857
1.0
±
1.0
±
0.905
1.0
±
1.0
±
0.958
1.0
±
0.172
0.867
±
0.138
0.87
±
0.958
0.08
±
1.0
0.0
±
0.789
1.0
±
0.913
0.952
0.955
1.0
±
0.913
0.923
1.0
±
1.0
±
1.0
±
1.0
±
0.087
0.955
±
0.127
0.88
±
0.929
0.135
±
0.0
1.0
±
1.0
0.0
±
0.889
1.0
±
0.96
1.0
±
1.0
±
1.0
±
1.0
±
0.957
1.0
±
0.778
1.0
±
0.929
1.0
±
1.0
±
0.897
1.0
±
0.85
1.0

±
±
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

0.115
0.145

±
0.0
0.0

±
0.0

0.145

0.077

0.111

0.083

0.192

0.095

0.156

±
0.0

±
0.0

±
0.0

±
0.0

±

0.183
0.091

0.087

0.139

0.112
0.172

0.095

0.155

0.182

±
0.157

±
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.096
±
0.0
0.0

2400 ms.
0.0
1.0
±
0.0
1.0
±
0.131
0.9
±
0.92
0.106
±
0.857
±
0.952
±
1.0
0.0
±
0.955
1.0
±
1.0
±
1.0
±
1.0
±
0.846
±
0.96
0.077
±
0.941
±
0.867
±
1.0
0.0
±
0.929
1.0
±
1.0
±
1.0
±
0.826
0.8
±
0.762
1.0
±
1.0
±
1.0
±
0.95
1.0
±
1.0
±
0.906
1.0
±
1.0
±
1.0
±
0.8
±
1.0
±
1.0
±
0.933
0.944
0.947
1.0
±
0.955
1.0
±
1.0
±
1.0
±
0.95
1.0
±
0.55
1.0
±
0.889
1.0
±
1.0
±
1.0
±
1.0
±
0.929
0.773

±
0.0
0.0
0.0
0.096
±
0.0
0.218
±
0.0

±
0.0
0.0
0.0
0.202
0.0
0.0

±
0.0
0.0
0.0
0.0

±
±
±
0.0

±
±

0.126
0.106
0.1

0.087

0.095
0.175

0.101

0.145

0.1

0.08

0.106

±
0.0
0.0
0.0

±
±
±
0.0
0.0

0.115
0.091
0.145

0.111
0.101
0.161

±
0.0
0.0
0.168
0.091

3600 ms.
0.0
1.0
±
1.0
0.0
±
0.913
±
0.952
±
0.889
±
0.0
1.0
±
0.0
1.0
±
0.077
0.96
±
1.0
0.0
±
0.944
1.0
±
1.0
±
0.81
±
0.952
±
0.0
1.0
±
0.95
0.096
±
1.0
0.0
±
0.947
1.0
±
1.0
±
1.0
±
0.917
0.906
0.818
1.0
±
1.0
±
0.958
1.0
±
1.0
±
1.0
±
1.0
±
0.08
0.958
±
0.08
0.958
±
0.062
0.968
±
0.077
0.96
±
0.0
1.0
±
0.0
1.0
±
0.077
0.96
±
0.095
0.929
1.0
±
1.0
±
1.0
±
1.0
±
1.0
±
0.905
±
0.0
1.0
±
0.0
1.0
±
0.76
0.167
±
0.0
1.0
±
1.0
0.0
±
0.909
0.955
0.875
0.933
0.926
0.905

0.12
0.087
0.132
0.089
0.099
0.126

±
0.0
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

0.126

±
±
±
±
±
±

0.1
0.119
0.164
0.172
0.138

0.123

0.066

0.112

±
0.0

±
±
±
±
±
0.0
0.0
0.0

4800 ms.
0.885
1.0
±
0.966
±
0.0
1.0
±
0.92
0.106
±
1.0
0.0
±
0.941
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.157
0.8
±
0.95
0.096
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.947
0.938
0.842
0.867
0.895
1.0
±
1.0
±
1.0
±
0.895
1.0
±
0.813
0.95
±
0.958
0.889
1.0
±
0.9
±
1.0
±
1.0
±
1.0
±
0.95
±
0.909
1.0
±
1.0
±
1.0
±
0.955
1.0
±
1.0
±
1.0
±
0.783
±
0.0
1.0
±
0.88
0.127
±
0.0
1.0
±
1.0
0.0
±
0.909
0.962
1.0
0.9

±
±
0.0
0.131
0.0
0.0
0.0
0.096
0.12

±
±
0.0
0.131

±
0.0
0.0
0.0

±
0.0
0.0
0.0

±
0.0

±
±

0.087

0.169

0.12
0.074

0.138

0.191
±
0.096
0.08
0.145

Table 3: Per-texture accuracies averaged over exposure times, using the ﬂow decode layer. Each texture accuracy includes a
margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

Short (300-600 ms.)
0.078
0.917
±
0.034
0.983
±
0.102
0.807
±
0.076
0.909
±
0.089
0.811
±
0.05
0.963
±
0.027
0.986
±
0.985
0.029
±
0.055
0.95
±
0.051
0.954
±
0.049
0.956
±
0.028
0.986
±
0.092
0.857
±
0.067
0.912
±
0.057
0.932
±
0.881
0.078
±
0.038
0.98
±
0.081
0.875
±
0.054
0.944
±
0.05
0.963
±
0.987
0.025
±
0.113
0.71
±
0.844
0.089
±
0.124
0.661
±
0.054
0.96
±
0.07
0.917
±
0.062
0.934
±
0.042
0.969
±
0.034
0.983
±
0.047
0.957
±
0.046
0.966
±
0.049
0.964
±
0.044
0.968
±
0.082
0.902
±
0.099
0.788
±
0.071
0.906
±
0.029
0.985
±
0.966
0.046
±
0.094
0.825
±
0.91
0.068
±
1.0
0.0
±
0.038
0.972
±
0.032
0.983
±
0.052
0.946
±
0.026
0.986
±
0.03
0.984
±
0.049
0.964
±
0.121
0.639
±
0.064
0.932
±
0.085
0.887
±
0.077
0.907
±
0.055
0.95
±
0.092
0.857
±
0.037
0.981
±
0.06
0.945
±
0.069
0.918
±

0.05

±
0.0

±
0.0
0.0

±
±
±
±
0.0

±
±
±
±
±
±
0.0

0.075
0.042
0.029
0.046

0.051
0.044
0.064
0.028
0.024
0.04

0.051
0.043
0.029
0.029
0.077
0.068
0.077
0.021
0.023
0.023
0.052
0.022
0.041
0.043
0.03
0.039
0.029
0.065
0.023

Long (1200-4800 ms.)
0.959
0.039
1.0
±
0.934
0.955
0.909
0.979
0.988
0.964
1.0
±
0.948
1.0
±
1.0
±
0.839
0.963
0.985
0.952
1.0
±
0.94
±
0.961
±
0.979
±
0.985
±
0.847
±
0.884
±
0.847
±
0.989
±
0.988
±
0.988
±
0.939
±
0.989
±
0.963
±
0.956
±
0.978
±
0.965
±
0.979
±
0.894
±
0.988
±
1.0
0.0
±
0.978
0.929
0.964
0.989
0.986
1.0
±
0.986
0.973
0.975
1.0
±
0.721
1.0
±
0.921
0.978
0.988
0.914
0.969
0.921
0.897

0.056
0.03
0.023
0.057
0.035
0.056
0.064

0.031
0.055
0.04
0.022
0.026

±
±
±
±
±
0.0

0.027
0.037
0.034

±
±
±
0.0

0.095

±
0.0

±
±
±
±
±
±
±

All (300-4800 ms.)
0.037
0.945
±
0.013
0.993
±
0.051
0.885
±
0.04
0.937
±
0.055
0.861
±
0.026
0.974
±
0.018
0.987
±
0.026
0.974
±
0.023
0.979
±
0.951
0.036
±
0.023
0.98
±
0.014
0.993
±
0.058
0.846
±
0.039
0.939
±
0.957
0.033
±
0.043
0.92
±
0.014
0.993
±
0.046
0.912
±
0.034
0.953
±
0.026
0.973
±
0.019
0.986
±
0.066
0.789
±
0.054
0.867
±
0.069
0.773
±
0.023
0.979
±
0.033
0.958
±
0.03
0.966
±
0.034
0.952
±
0.019
0.986
±
0.031
0.96
±
0.032
0.96
±
0.027
0.972
±
0.029
0.966
±
0.035
0.952
±
0.057
0.848
±
0.034
0.952
±
0.014
0.993
±
0.026
0.973
±
0.052
0.884
±
0.038
0.94
±
0.013
0.993
±
0.023
0.979
±
0.013
0.993
±
0.03
0.966
±
0.023
0.98
±
0.023
0.979
0.019
0.986
0.075
0.687
0.027
0.972
0.047
0.908
0.035
0.952
0.027
0.972
0.05
0.893
0.026
0.973
0.042
0.931
0.047
0.905

±
±
±
±
±
±
±
±
±
±
±

Table 4: Per-texture accuracies averaged over a range of exposure times, using the ﬂow decode layer. Each texture accuracy
includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

300 ms.
0.702
0.806
0.616

±
±
±

0.098
0.046
0.03

400 ms.
0.74
±
0.853
±
0.658
±

0.101

0.044
0.029

600 ms.
0.838
0.837
0.687

±
±
±

0.088
0.043
0.028

1200 ms.
0.84
±
0.903
0.76

0.083
0.036
±
0.026

±

2400 ms.
0.954
0.909
0.751

±
±
±

0.051
0.037
0.026

3600 ms.
0.878
0.919
0.776

±
±
±

0.074
0.031
0.026

4800 ms.
0.827
0.902
0.762

±
±
±

0.082
0.035
0.025

Table 5: Accuracies of textures grouped by appearances, averaged over exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

Short (300-600 ms.)
0.056
0.756
0.026
0.831
0.017
0.654

±
±
±

Long (1200-4800 ms.)
0.038
0.871
0.017
0.908
0.013
0.762

±
±
±

All (300-4800 ms.)
0.033
0.821
0.015
0.875
0.01
0.717

±
±
±

Table 6: Accuracies of textures grouped by appearances, averaged over a range of exposure times, using the concatenation
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

300 ms.
0.078
0.889
±
0.041
0.89
±
0.021
0.901
±

400 ms.
0.933
0.942
0.916

±
±
±

0.063
0.031
0.018

600 ms.
0.921
0.957
0.937

±
±
±

0.067
0.026
0.016

1200 ms.
0.961
0.953
0.957

±
±
±

0.043
0.028
0.014

2400 ms.
0.057
0.948
±
0.025
0.96
±
0.015
0.945
±

3600 ms.
0.984
0.968
0.955

±
±
±

0.031
0.022
0.013

4800 ms.
0.964
0.947
0.96

±
±
0.013
±

0.049
0.029

Table 7: Accuracies of textures grouped by appearances, averaged over exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

Short (300-600 ms.)
0.914
0.04
±
0.93
0.019
±
0.919
±

0.011

Long (1200-4800 ms.)
0.023
0.964
0.013
0.957
0.007
0.954

±
±
±

All (300-4800 ms.)
0.022
0.943
0.011
0.946
0.006
0.939

±
±
±

Table 8: Accuracies of textures grouped by appearances, averaged over a range of exposure times, using the ﬂow decode
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

300 ms.
0.625
0.721

±
±

0.032
0.039

400 ms.
0.664
0.763

±
±

0.032
0.039

600 ms.
0.698
0.777

±
±

0.03
0.037

1200 ms.
0.741
0.885

0.028
0.028

±
±

2400 ms.
0.753
0.854

0.028
0.032

±
±

3600 ms.
0.762
0.902

0.028
0.026

±
±

4800 ms.
0.755
0.861

0.028
0.029

±
±

Table 9: Accuracies of textures grouped by dynamics, averaged over exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

Short (300-600 ms.)
0.018
0.663
0.022
0.753

±
±

Long (1200-4800 ms.)
0.014
0.753
0.015
0.876

±
±

All (300-4800 ms.)
0.011
0.715
0.013
0.823

±
±

Table 10: Accuracies of textures grouped by dynamics, averaged over a range of exposure times, using the concatenation
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

300 ms.
0.886
0.92

0.024

±
0.027

±

400 ms.
0.911
0.942

±
±

0.02
0.023

600 ms.
0.934
0.949

±
±

0.018
0.021

1200 ms.
0.947
0.974

0.016
0.016

±
±

2400 ms.
0.945
0.954

0.016
0.02

±
±

3600 ms.
0.955
0.966

0.014
0.017

±
±

4800 ms.
0.954
0.964

0.015
0.018

±
±

Table 11: Accuracies of textures grouped by dynamics, averaged over exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

Short (300-600 ms.)
0.012
0.911
0.013
0.937

±
±

Long (1200-4800 ms.)
0.95
±
0.964
±

0.008
0.009

All (300-4800 ms.)
0.007
0.934
0.008
0.953

±
±

Table 12: Accuracies of textures grouped by dynamics, averaged over a range of exposure times, using the ﬂow decode layer.
Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

300 ms.
0.661

±

400 ms.
0.699

±

600 ms.
0.726

±

1200 ms.
0.791

0.021

±

2400 ms.
0.788

0.022

±

3600 ms.
0.812

0.021

±

4800 ms.
0.793

0.021

±

0.025

0.025

0.023

Table 13: Average accuracy over all textures, averaged over exposure times, using the concatenation layer. Each texture
accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

Short (300-600 ms.)
0.014
0.695

Long (1200-4800 ms.)
0.011
0.796

All (300-4800 ms.)
0.009
0.754

±

±

±

Table 14: Average accuracy over all textures, averaged over a range of exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

300 ms.
0.898

±

0.018

0.015

400 ms.
0.922

±

600 ms.
0.94

±

0.013

1200 ms.
0.956

0.012

±

2400 ms.
0.948

0.013

±

3600 ms.
0.959

0.011

±

4800 ms.
0.957

0.012

±

Table 15: Average accuracy over all textures, averaged over exposure times, using the ﬂow decode layer. Each texture
accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

Short (300-600 ms.)
0.009
0.921

Long (1200-4800 ms.)
0.006
0.955

All (300-4800 ms.)
0.005
0.941

±

±

±

Table 16: Average accuracy over all textures, averaged over a range of exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Two-Stream Convolutional Networks for Dynamic Texture Synthesis

Matthew Tesfaldet Marcus A. Brubaker
Department of Electrical Engineering and Computer Science
York University, Toronto
{mtesfald,mab}@eecs.yorku.ca

Konstantinos G. Derpanis
Department of Computer Science
Ryerson University, Toronto
kosta@scs.ryerson.ca

8
1
0
2
 
r
p
A
 
2
1
 
 
]

V
C
.
s
c
[
 
 
4
v
2
8
9
6
0
.
6
0
7
1
:
v
i
X
r
a

Abstract

We introduce a two-stream model for dynamic texture
synthesis. Our model is based on pre-trained convolutional
networks (ConvNets) that target two independent tasks: (i)
object recognition, and (ii) optical ﬂow prediction. Given
an input dynamic texture, statistics of ﬁlter responses from
the object recognition ConvNet encapsulate the per-frame
appearance of the input texture, while statistics of ﬁlter re-
sponses from the optical ﬂow ConvNet model its dynamics.
To generate a novel texture, a randomly initialized input se-
quence is optimized to match the feature statistics from each
stream of an example texture. Inspired by recent work on
image style transfer and enabled by the two-stream model,
we also apply the synthesis approach to combine the texture
appearance from one texture with the dynamics of another
to generate entirely novel dynamic textures. We show that
our approach generates novel, high quality samples that
match both the framewise appearance and temporal evo-
lution of input texture. Finally, we quantitatively evaluate
our texture synthesis approach with a thorough user study.

1. Introduction

Many common temporal visual patterns are naturally de-
scribed by the ensemble of appearance and dynamics (i.e.,
temporal pattern variation) of their constituent elements.
Examples of such patterns include ﬁre, ﬂuttering vegetation,
and wavy water. Understanding and characterizing these
temporal patterns has long been a problem of interest in hu-
man perception, computer vision, and computer graphics.
These patterns have been previously studied under a variety
of names, including turbulent-ﬂow motion [17], temporal
textures [30], time-varying textures [3], dynamic textures
[8], textured motion [45] and spacetime textures [7]. Here,
we adopt the term “dynamic texture”. In this work, we pro-
pose a factored analysis of dynamic textures in terms of ap-
pearance and temporal dynamics. This factorization is then
used to enable dynamic texture synthesis which, based on

Figure 1: Dynamic texture synthesis. (left) Given an input
dynamic texture as the target, our two-stream model is able
to synthesize a novel dynamic texture that preserves the tar-
get’s appearance and dynamics characteristics. (right) Our
two-stream approach enables synthesis that combines the
texture appearance from one target with the dynamics from
another, resulting in a composition of the two.

example texture inputs, generates a novel dynamic texture
instance. It also enables a novel form of style transfer where
the target appearance and dynamics can be taken from dif-
ferent sources as shown in Fig. 1.

Our model is constructed from two convolutional net-
works (ConvNets), an appearance stream and a dynamics
stream, which have been pre-trained for object recognition
and optical ﬂow prediction, respectively. Similar to previ-
ous work on spatial textures [13, 19, 33], we summarize an
input dynamic texture in terms of a set of spatiotemporal
statistics of ﬁlter outputs from each stream. The appear-
ance stream models the per frame appearance of the input
texture, while the dynamics stream models its temporal dy-
namics. The synthesis process consists of optimizing a ran-
domly initialized noise pattern such that its spatiotemporal
statistics from each stream match those of the input tex-
ture. The architecture is inspired by insights from human
perception and neuroscience. In particular, psychophysical
studies [6] show that humans are able to perceive the struc-
ture of a dynamic texture even in the absence of appearance
cues, suggesting that the two streams are effectively inde-

pendent. Similarly, the two-stream hypothesis [16] models
the human visual cortex in terms of two pathways, the ven-
tral stream (involved with object recognition) and the dorsal
stream (involved with motion processing).

In this paper, our two-stream analysis of dynamic tex-
tures is applied to texture synthesis. We consider a range
of dynamic textures and show that our approach generates
novel, high quality samples that match both the frame-wise
appearance and temporal evolution of an input example.
Further, the factorization of appearance and dynamics en-
ables a novel form of style transfer, where dynamics of one
texture are combined with the appearance of a different one,
cf . [14]. This can even be done using a single image as
an appearance target, which allows static images to be an-
imated. Finally, we validate the perceived realism of our
generated textures through an extensive user study.

2. Related work

There are two general approaches that have dominated
the texture synthesis literature: non-parametric sampling
approaches that synthesize a texture by sampling pixels of
a given source texture [10, 26, 37, 47], and statistical para-
metric models. As our approach is an instance of a para-
metric model, here we focus on these approaches.

The statistical characterization of visual textures was in-
troduced in the seminal work of Julesz [23]. He conjectured
that particular statistics of pixel intensities were sufﬁcient
to partition spatial textures into metameric (i.e., perceptu-
ally indistinguishable) classes. Later work leveraged this
notion for texture synthesis [19, 33]. In particular, inspired
by models of the early stages of visual processing, statistics
of (handcrafted) multi-scale oriented ﬁlter responses were
used to optimize an initial noise pattern to match the ﬁlter
response statistics of an input texture. More recently, Gatys
et al. [13] demonstrated impressive results by replacing the
linear ﬁlter bank with a ConvNet that, in effect, served as
a proxy for the ventral visual processing stream. Textures
are modelled in terms of the correlations between ﬁlter re-
sponses within several layers of the network. In subsequent
work, this texture model was used in image style transfer
[14], where the style of one image was combined with the
image content of another to produce a new image. Ruder et
al. [36] extended this model to video by using optical ﬂow
to enforce temporal consistency of the resulting imagery.

Variants of linear autoregressive models have been stud-
ied [42, 8] that jointly model appearance and dynamics of
the spatiotemporal pattern. More recent work has consid-
ered ConvNets as a basis for modelling dynamic textures.
Xie et al. [48] proposed a spatiotemporal generative model
where each dynamic texture is modelled as a random ﬁeld
deﬁned by multiscale, spatiotemporal ConvNet ﬁlter re-
sponses and dynamic textures are realized by sampling the
model. Unlike our current work, which assumes pretrained

ﬁxed networks, this approach requires the ConvNet weights
to be trained using the input texture prior to synthesis.

A recent preprint [12] described preliminary results ex-
tending the framework of Gatys et al. [13] to model and syn-
thesize dynamic textures by computing a Gram matrix of
ﬁlter activations over a small temporal window. In contrast,
our two stream ﬁltering architecture is more expressive as
our dynamics stream is speciﬁcally tuned to spatiotemporal
dynamics. Moreover, as will be demonstrated, the factoriza-
tion in terms of appearance and dynamics enables a novel
form of style transfer, where the dynamics of one pattern
are transferred to the appearance of another to generate an
entirely new dynamic texture. To the best of our knowledge,
we are the ﬁrst to demonstrate this form of style transfer.

The recovery of optical ﬂow from temporal imagery
has long been studied in computer vision.
Tradition-
ally, it has been addressed by handcrafted approaches e.g.,
[20, 29, 35]. Recently, ConvNet approaches [9, 34, 21, 49]
have been demonstrated as viable alternatives. Most closely
related to our approach are energy models of visual motion
[2, 18, 39, 31, 7, 25] that have been motivated and studied
in a variety of contexts, including computer vision, visual
neuroscience, and visual psychology. Given an input image
sequence, these models consist of an alternating sequence
of linear and non-linear operations that yield a distributed
representation (i.e., implicitly coded) of pixelwise optical
ﬂow. Here, an energy model motivates the representation of
observed dynamics which is then encoded as a ConvNet.

3. Technical approach

Our proposed two-stream approach consists of an ap-
pearance stream, representing the static (texture) appear-
ance of each frame, and a dynamics stream, representing
temporal variations between frames. Each stream consists
of a ConvNet whose activation statistics are used to charac-
terize the dynamic texture. Synthesizing a dynamic texture
is formulated as an optimization problem with the objective
of matching the activation statistics. Our dynamic texture
synthesis approach is summarized in Fig. 2 and the individ-
ual pieces are described in turn in the following sections.

3.1. Texture model: Appearance stream

The appearance stream follows the spatial texture model
introduced by Gatys et al. [13] which we brieﬂy review
here. The key idea is that feature correlations in a Con-
vNet trained for object recognition capture texture appear-
ance. We use the same publicly available normalized VGG-
19 network [40] used by Gatys et al. [13].

To capture the appearance of an input dynamic texture,
we ﬁrst perform a forward pass with each frame of the im-
age sequence through the ConvNet and compute the feature
RNl×Ml , for various levels in the net-
activations, Alt
work, where Nl and Ml denote the number of ﬁlters and

∈

ture suited for computing optical ﬂow (e.g., [9, 21]) which
is naturally differentiable. However, with most such mod-
els it is unclear how invariant their layers are to appearance.
Instead, we propose a novel network architecture which is
motivated by the spacetime-oriented energy model [7, 39].
In motion energy models, the velocity of image content
(i.e., motion) is interpreted as a three-dimensional orienta-
tion in the x-y-t spatiotemporal domain [2, 11, 18, 39, 46].
In the frequency domain, the signal energy of a translating
pattern can be shown to lie on a plane through the origin
where the slant of the plane is deﬁned by the velocity of
the pattern. Thus, motion energy models attempt to identify
this orientation-plane (and hence the patterns velocity) via
a set of image ﬁltering operations. More generally the con-
stituent spacetime orientations for a spectrum of common
visual patterns (including translation and dynamic textures)
can serve as a basis for describing the temporal variation of
an image sequence [7]. This suggests that motion energy
models may form an ideal basis for our dynamics stream.

Speciﬁcally, we use the spacetime-oriented energy
model [7, 39] to motivate our network architecture which
we brieﬂy review here; see [7] for a more in-depth descrip-
tion. Given an input video, a bank of oriented 3D ﬁlters
are applied which are sensitive to a range of spatiotemporal
orientations. These ﬁlter activations are rectiﬁed (squared)
and pooled over local regions to make the responses robust
to the phase of the input signal, i.e., robust to the alignment
of the ﬁlter with the underlying image structure. Next, ﬁl-
ter activations consistent with the same spacetime orienta-
tion are summed. These responses provide a pixelwise dis-
tributed measure of which orientations (frequency domain
planes) are present in the input. However, these responses
are confounded by local image contrast that makes it dif-
ﬁcult to determine whether a high response is indicative
of the presence of a spacetime orientation or simply due
to high image contrast. To address this ambiguity, an L1
normalization is applied across orientation responses which
results in a representation that is robust to local appearance
variations but highly selective to spacetime orientation.

Using this model as our basis, we propose the follow-
ing fully convolutional network [38]. Our ConvNet in-
put is a pair of temporally consecutive greyscale images.
Each input pair is ﬁrst normalized to have zero-mean and
unit variance. This step provides a level of invariance to
overall brightness and contrast, i.e., global additive and
multiplicative signal variations. The ﬁrst layer consists of
32 3D spacetime convolution ﬁlters of size 11
2
width
(height
time). Next, a squaring activation function
and 5
5 spatial max-pooling (with a stride of one) is ap-
plied to make the responses robust to local signal phase. A
1 convolution layer follows with 64 ﬁlters that combines
1
energy measurements that are consistent with the same ori-
entation. Finally, to remove local contrast dependence, an

×
×

11

×

×

×

×

Figure 2: Two-stream dynamic texture generation. Sets of
Gram matrices represent a texture’s appearance and dynam-
ics. Matching these statistics allows for the generation of
novel textures as well as style transfer between textures.

(cid:80)T

∈
t=1

ij =

1
T NlMl

the number of spatial locations of layer l at time t, respec-
tively. The correlations of the ﬁlter responses in a particular
layer are averaged over the frames and encapsulated by a
RNl×Nl , whose entries are given by
Gram matrix, Gl
(cid:80)Ml
ikAlt
Gl
k=1 Alt
jk, where T denotes the
number of input frames and Alt
ik denotes the activation of
feature i at location k in layer l on the target frame t. The
synthesized texture appearance is similarly represented by
a Gram matrix, ˆGlt
RNl×Nl , whose activations are given
ˆAlt
ˆAlt
by ˆGlt
ij = 1
ik denotes the acti-
ik
vation of feature i at location k in layer l on the synthesized
frame t. The appearance loss,
appearance, is then deﬁned as
the temporal average of the mean squared error between the
Gram matrix of the input texture and that of the generated
texture computed at each frame:

jk, where ˆAlt

∈
(cid:80)Ml
k=1

NlMl

L

appearance =

L

1
LappTout

Tout(cid:88)

(cid:88)

t=1

l

Gl

(cid:107)

ˆGlt

2
F ,

(cid:107)

−

(1)

where Lapp is the number of layers used to compute Gram
matrices, Tout is the number of frames being generated in
the output, and
(cid:107) · (cid:107)F is the Frobenius norm. Consistent
with previous work [13], we compute Gram matrices on the
following layers: conv1 1, pool1, pool2, pool3, and pool4.

3.2. Texture model: Dynamics stream

There are three primary goals in designing our dynamics
stream. First, the activations of the network must represent
the temporal variation of the input pattern. Second, the acti-
vations should be largely invariant to the appearance of the
images which should be characterized by the appearance
stream described above. Finally, the representation must be
differentiable to enable synthesis. By analogy to the ap-
pearance stream, an obvious choice is a ConvNet architec-

in layer l on the target frames t and t + 1. The dynam-
ics of the synthesized texture is represented by a Gram ma-
trix of ﬁlter response correlations computed separately for
each pair of frames, ˆGlt
ij =
1
ik denotes the activation of
NlMl
feature i at location k in layer l on the synthesized frames t
and t + 1. The dynamics loss,
dynamics, is deﬁned as the av-
L
erage of the mean squared error between the Gram matrices
of the input texture and those of the generated texture:

RNl×Nl , with entries ˆGlt

∈
jk, where ˆDlt

(cid:80)Ml
k=1

ˆDlt
ik

ˆDlt

dynamics =

L

1
Ldyn(Tout

Tout−1
(cid:88)

(cid:88)

1)

−

t=1

l

Gl

(cid:107)

ˆGlt

2
F ,

(cid:107)

−

(2)

where Ldyn is the number of ConvNet layers being used in
the dynamics stream.

Here we propose to use the output of the concatenation
layer, where the multiscale distributed representation of ori-
entations is stored, as the layer to compute the Gram ma-
trix. While it is tempting to use the predicted ﬂow out-
put from the network, this generally yields poor results as
shown in our evaluation. Due to the complex, temporal vari-
ation present in dynamic textures, they contain a variety of
local spacetime orientations rather than a single dominant
orientation. As a result, the ﬂow estimates will tend to be
an average of the underlying orientation measurements and
consequently not descriptive. A comparison between the
texture synthesis results using the concatenation layer and
the predicted ﬂow output is provided in Sec. 4.

3.3. Texture generation

The overall dynamic texture loss consists of the combi-
nation of the appearance loss, Eq. (1), and the dynamics
loss, Eq. (2):

dynamic texture = α

appearance + β

dynamics,

(3)

L

L

L

where α and β are the weighting factors for the appearance
and dynamics content, respectively. Dynamic textures are
implicitly deﬁned as the (local) minima of this loss. Tex-
tures are generated by optimizing Eq. (3) with respect to
the spacetime volume, i.e., the pixels of the video. Vari-
ations in the resulting texture are found by initializing the
optimization process using IID Gaussian noise. Consistent
with previous work [13], we use L-BFGS [28] optimization.
Naive application of the outlined approach will consume
increasing amounts of memory as the temporal extent of the
dynamic texture grows; this makes it impractical to gener-
ate longer sequences. Instead, long sequences can be in-
crementally generated by separating the sequence into sub-
sequences and optimizing them sequentially. This is real-
ized by initializing the ﬁrst frame of a subsequence as the
last frame from the previous subsequence and keeping it
ﬁxed throughout the optimization. The remaining frames

Figure 3: Dynamics stream ConvNet. The ConvNet is based
on a spacetime-oriented energy model [7, 39] and is trained
for optical ﬂow prediction. Three scales are shown for il-
lustration; in practice ﬁve scales were used.

L1 divisive normalization is applied.

To capture spacetime orientations beyond those capable
with the limited receptive ﬁelds used in the initial layer,
we compute a ﬁve-level spatial Gaussian pyramid. Each
pyramid level is processed independently with the same
spacetime-oriented energy model and then bilinearly up-
sampled to the original resolution and concatenated.

Prior energy model instantiations (e.g., [2, 7, 39]) used
handcrafted ﬁlter weights. While a similar approach could
be followed here, we opt to learn the weights so that they
are better tuned to natural imagery. To train the network
weights, we add additional decoding layers that take the
concatenated distributed representation and apply a 3
3
convolution (with 64 ﬁlters), ReLU activation, and a 1
1
convolution (with 2 ﬁlters) that yields a two channel output
encoding the optical ﬂow directly. The proposed architec-
ture is illustrated in Fig. 3.

×
×

For training, we use the standard average endpoint er-
ror (aEPE) ﬂow metric (i.e., L2 norm) between the pre-
dicted ﬂow and the ground truth ﬂow as the loss. Since no
large-scale ﬂow dataset exists that captures natural imagery
with groundtruth ﬂow, we take an unlabeled video dataset
and apply an existing ﬂow estimator [35] to estimate opti-
cal ﬂow for training, cf . [43]. For training data, we used
videos from the UCF101 dataset [41] with geometric and
photometric data augmentations similar to those used by
FlowNet [9], and optimized the aEPE loss using Adam [24].
Inspection of the learned ﬁlters in the initial layer showed
evidence of spacetime-oriented ﬁlters, consistent with the
handcrafted ﬁlters used in previous work [7].

Similar to the appearance stream, ﬁlter response cor-
relations in a particular layer of the dynamics stream are
averaged over the number of image frame pairs and en-
RNl×Nl , whose en-
capsulated by a Gram matrix, Gl
tries are given by Gl
ikDlt
k=1 Dlt
jk,
where Dlt
ik denotes the activation of feature i at location k

1
(T −1)NlMl

∈
(cid:80)T −1
t=1

ij =

(cid:80)Ml

of the subsequence are initialized randomly and optimized
as above. This ensures temporal consistency across synthe-
sized subsequences and can be viewed as a form of coordi-
nate descent for the full sequence objective. The ﬂexibility
of this framework allows other texture generation problems
to be handled simply by altering the initialization of frames
and controlling which frames or frame regions are updated.

4. Experimental results

The goal of (dynamic) texture synthesis is to gener-
ate samples that are indistinguishable from the real input
target texture by a human observer.
In this section, we
present a variety of synthesis results including a user study
to quantitatively evaluate the realism of our results. Given
their temporal nature, our results are best viewed as videos.
Our two-stream architecture was implemented using Ten-
sorFlow [1]. Results were generated using an NVIDIA Ti-
tan X (Pascal) GPU and synthesis times ranged between
one to three hours to generate 12 frames with an image
resolution of 256
256. For our full synthesis results
and source code, please refer to the supplemental material
on the project website: ryersonvisionlab.github.
io/two-stream-projpage.

×

4.1. Dynamic texture synthesis

We applied our dynamic texture synthesis process to a
wide range of textures which were selected from the Dyn-
Tex [32] database and others we collected in the wild. In-
cluded in our supplemental material are synthesized results
of nearly 60 different textures that encapsulate a range of
phenomena, such as ﬂowing water, waves, clouds, ﬁre, rip-
pling ﬂags, waving plants, and schools of ﬁsh. Some sam-
ple frames are shown in Fig. 4 but we encourage readers to
view the videos to fully appreciate the results. In addition,
we performed a comparison with [12] and [48]. Generally,
we found our results to be qualitatively comparable or better
than these methods. See the supplemental for more details
on the comparisons with these methods.

We also generated dynamic textures incrementally, as
described in Sec. 3.3. The resulting textures were perceptu-
ally indistinguishable from those generated with the batch
process. Another extension that we explored were textures
with no discernible temporal seam between the last and ﬁrst
frames. Played as a loop, these textures appear to be tempo-
rally endless. This was achieved by assuming that the ﬁrst
frame follows the ﬁnal frame and adding an additional loss
for the dynamics stream evaluated on that pair of frames.

Example failure modes of our method are presented
in Fig. 6.
In general, we ﬁnd that most failures result
from inputs that violate the underlying assumption of a
dynamic texture, i.e., the appearance and/or dynamics are
not spatiotemporally homogeneous.
In the case of the
escalator example, the long edge structures in the ap-

pearance are not spatially homogeneous, and the dynam-
ics vary due to perspective effects that change the motion
from downward to outward. The resulting synthesized tex-
ture captures an overall downward motion but lacks the per-
spective effects and is unable to consistently reproduce the
long edge structures. This is consistent with previous ob-
servations on static texture synthesis [13] and suggests it is
a limitation of the appearance stream.

Another example is the flag sequence where the rip-
pling dynamics are relatively homogeneous across the pat-
tern but the appearance varies spatially. As expected, the
generated texture does not faithfully reproduce the appear-
ance; however, it does exhibit plausible rippling dynamics.
In the supplemental material, we include an additional fail-
ure case, cranberries, which consists of a swirling pat-
tern. Our model faithfully reproduces the appearance but is
unable to capture the spatially varying dynamics. Interest-
ingly, it still produces a result which is statistically indistin-
guishable from real in our user study discussed below.

Appearance vs. dynamics streams We sought to verify
that the appearance and dynamics streams were capturing
complementary information. To validate that the texture
generation of multiple frames would not induce dynamics
consistent with the input, we generated frames starting from
randomly generated noise but only using the appearance
statistics and corresponding loss, i.e., Eq. 1. As expected,
this produced frames that were valid textures but with no
coherent dynamics present. Results for a sequence contain-
ing a school of ﬁsh are shown in Fig. 5; to examine the
dynamics, see fish in the supplemental material.

Similarly, to validate that the dynamics stream did not in-
advertently include appearance information, we generated
videos using the dynamics loss only, i.e., Eq. 2. The re-
sulting frames had no visible appearance and had an ex-
tremely low dynamic range, i.e., the standard deviation of
pixel intensities was 10 for values in [0, 255]. This indi-
cates a general invariance to appearance and suggests that
our two-stream dynamic texture representation has factored
appearance and dynamics, as desired.

4.2. User study

Quantitative evaluation for texture synthesis is a partic-
ularly challenging task as there is no single correct output
when synthesizing new samples of a texture. Like in other
image generation tasks (e.g., rendering), human perception
is ultimately the most important measure. Thus, we per-
formed a user study to evaluate the perceived realism of our
synthesized textures.

Similar to previous image synthesis work (e.g., [5]), we
conducted a perceptual experiment with human observers
to quantitatively evaluate our synthesis results. We em-
ployed a forced-choice evaluation on Amazon Mechanical

fireplace 1

(original)

fireplace 1

(synthesized)

lava

(original)

lava

(synthesized)

smoke 1

(original)

smoke 1

(synthesized)

underwater

vegetation 1

(original)

underwater
vegetation 1

(synthesized)

water 3

(original)

water 3

(synthesized)

Figure 4: Dynamic texture synthesis success examples. Names correspond to ﬁles in the supplemental material.

Turk (AMT) with 200 different users. Each user performed
59 pairwise comparisons between a synthesized dynamic
texture and its target. Users were asked to choose which
appeared more realistic after viewing the textures for an ex-
posure time sampled randomly from discrete intervals be-
tween 0.3 and 4.8 seconds. Measures were taken to control
the experimental conditions and minimize the possibility of
low quality data. See the supplemental material for further
experimental details of our user study.

For comparison, we constructed a baseline by using the
ﬂow decode layer in the dynamics loss of Eq. 2. This corre-
sponds with attempting to mimic the optical ﬂow statistics
of the texture directly. Textures were synthesized with this
model and the user study was repeated with an additional
200 users. To differentiate between the models, we label

“Flow decode layer” and “Concat layer” in the ﬁgures to
describe our baseline and ﬁnal model, respectively.

The results of this study are summarized in Fig. 7 which
shows user accuracy in differentiating real versus generated
textures as a function of time for both methods. Over-
all, users are able to correctly identify the real texture
2.5% of the time for brief exposures of 0.3 sec-
66.1%
onds. This rises to 79.6%
1.1% with exposures of 1.2 sec-
onds and higher. Note that “perfect” synthesis results would
have an accuracy of 50%, indicating that users were unable
to differentiate between the real and generated textures and
higher accuracy indicating less convincing textures.

±

±

The results clearly show that the use of the concatenation
layer activations is far more effective than the ﬂow decode
layer. This is not surprising as optical ﬂow alone is known

target
(fish)

appearance
only

both
streams

escalator

(original)

escalator

(synthesized)

flag

(original)

flag

(synthesized)

(top row) Target texture.

Figure 5: Dynamic texture synthesis versus texture synthe-
sis.
(middle) Texture synthesis
without dynamics constraints shows consistent per-frame
appearance but no temporal coherence. (bottom) Including
both streams induces consistent appearance and dynamics.

Figure 6: Dynamic texture synthesis failure examples. In
these cases, the failures are attributed to either the appear-
ance or the dynamics not being homogeneous.

to be unreliable on many textures, particularly those with
transparency or chaotic motion (e.g., water, smoke, ﬂames,
etc.). Also evident in these results is the time-dependant
nature of perception for textures from both models. Users’
ability to identify the generated texture improved as expo-
sure times increased to 1.2 seconds and remained relatively
ﬂat for longer exposures.

To better understand the performance of our approach,
we grouped and analyzed the results in terms of appear-
ance and dynamics characteristics. For appearance we used
the taxonomy presented in [27] and grouped textures as
either regular/near-regular (e.g., periodic tiling and brick
wall), irregular (e.g., a ﬁeld of ﬂowers), or stochastic/near-
stochastic (e.g.,
For dynamics we
grouped textures as either spatially-consistent (e.g., closeup
of rippling sea water) or spatially-inconsistent (e.g., rippling
sea water juxtaposed with translating clouds in the sky). Re-

tv static or water).

Figure 7: Time-limited pairwise comparisons across all tex-
tures with 95% statistical conﬁdence intervals.

sults based on these groupings can be seen in Fig. 8.

1.6% and 90.8%

A full breakdown of the user study results by texture and
grouping can be found in the supplemental material. Here
we discuss some of the overall trends. Based on appear-
ance it is clear that textures with large-scale spatial consis-
tencies (regular, near-regular, and irregular textures) tend to
perform poorly. Examples being flag and fountain 2
with user accuracies of 98.9%
4.3%
averaged across all exposures, respectively. This is not un-
expected and is a fundamental limitation of the local na-
ture of the Gram matrix representation used in the appear-
ance stream which was observed in static texture synthe-
sis [13]. In contrast, stochastic and near-stochastic textures
performed signiﬁcantly better as their smaller-scale local
variations are well captured by the appearance stream, for
instance water 1 and lava which had average accuracies
of 53.8%
7.4%, respectively, making
them both statistically indistinguishable from real.

7.4% and 55.6%

±

±

±

±

±

(e.g.,

dynamics

7.4% and 63.2%

In terms of dynamics, we ﬁnd that

textures with
tv static,
spatially-consistent
water *, and calm water *) perform signiﬁcantly
better than those with spatially-inconsistent dynamics (e.g.,
candle flame, fountain 2, and snake *), where
the dynamics drastically differ across spatial locations.
For example, tv static and calm water 6 have
average accuracies of 48.6%
7.2%,
respectively, while candle flame and snake 5 have
average accuracies of 92.4%
4%,
respectively. Overall, our model is capable of reproducing
a full spectrum of spatially-consistent dynamics. However,
as the appearance shifts from containing small-scale spatial
consistencies,
to containing large-scale
consistencies
performance degrades. This was evident in the user study
where the best-performing textures typically consisted of
a stochastic or near-stochastic appearance with spatially-
In contrast the worst-performing
consistent dynamics.
textures consisted of regular, near-regular, or irregular
appearance with spatially-inconsistent dynamics.

4% and 92.1%

±

±

±

appearance
target

synthesized output

Figure 9: Dynamics style transfer. (top row) Appearance of
still water was used with the dynamics of a different water
dynamic texture (water 4). (bottom row) The appearance
of a painting of ﬁre was used with the dynamics of a real
ﬁre (fireplace 1). Animated results and additional ex-
amples are available in the supplemental material.

ance and dynamics. We applied this model to a variety of
dynamic texture synthesis tasks and showed that, so long
as the input textures are generally true dynamic textures,
i.e., have spatially invariant statistics and spatiotemporally
invariant dynamics, the resulting synthesized textures are
compelling. This was validated both qualitatively and quan-
titatively through a large user study. Further, we showed
that the two-stream model enabled dynamics style transfer,
where the appearance and dynamics information from dif-
ferent sources can be combined to generate a novel texture.
We have explored this model thoroughly and found a few
limitations which we leave as directions for future work.
First, much like has been reported in recent image style
transfer work [14], we have found that high frequency noise
and chromatic aberrations are a problem in generation. An-
other issue that arises is the model fails to capture textures
with spatially-variant appearance, (e.g., flag in Fig. 6) and
spatially-inconsistent dynamics (e.g., escalator in Fig.
6). By collapsing the local statistics into a Gram matrix,
the spatial and temporal organization is lost. Simple post-
processing methods may alleviate some of these issues but
we believe that they also point to a need for a better rep-
resentation. Beyond addressing these limitations, a natural
next step would be to extend the idea of a factorized rep-
resentation into feed-forward generative networks that have
found success in static image synthesis, e.g., [22, 44].

Acknowledgements MT is supported by a Natural Sci-
ences and Engineering Research Council of Canada
(NSERC) Canadian Graduate Scholarship. KGD and MAB
are supported by NSERC Discovery Grants. This research
was undertaken as part of the Vision: Science to Applica-
tions program, thanks in part to funding from the Canada
First Research Excellence Fund.

Figure 8: Time-limited pairwise comparisons across all tex-
tures, grouped by appearance (top) and dynamics (bottom).
Shown with 95% statistical conﬁdence intervals.

4.3. Dynamics style transfer

The underlying assumption of our model is that appear-
ance and dynamics of texture can be factorized. As such, it
should allow for the transfer of the dynamics of one texture
onto the appearance of another. This has been explored pre-
viously for artistic style transfer [4, 15] with static imagery.
We accomplish this with our model by performing the same
optimization as above, but with the target Gram matrices for
appearance and dynamics computed from different textures.
A dynamics style transfer result is shown in Fig. 9 (top),
using two real videos. Additional examples are available
in the supplemental material. We note that when perform-
ing dynamics style transfer it is important that the appear-
ance structure be similar in scale and semantics, otherwise,
the generated dynamic textures will look unnatural. For in-
stance, transferring the dynamics of a ﬂame onto a water
scene will generally produce implausible results.

We can also apply the dynamics of a texture to a static
input image, as the target Gram matrices for the appearance
loss can be computed on just a single frame. This allows us
to effectively animate regions of a static image. The result
of this process can be striking and is visualized in Fig. 9
(bottom), where the appearance is taken from a painting and
the dynamics from a real world video.

5. Discussion and summary

In this paper, we presented a novel, two-stream model of
dynamic textures using ConvNets to represent the appear-

References

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-
mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,
R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man´e,
R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. War-
den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 5
[2] E. H. Adelson and J. R. Bergen. Spatiotemporal energy mod-
els for the perception of motion. JOSA–A, 2(2):284–299,
1985. 2, 3, 4

[3] Z. Bar-Joseph, R. El-Yaniv, D. Lischinski, and M. Werman.
Texture mixing and texture movie synthesis using statistical
learning. T-VCG, 7(2):120–135, 2001. 1

[4] A. J. Champandard. Semantic style transfer and turning two-
bit doodles into ﬁne artworks. arXiv:1603.01768, 2016. 8
[5] Q. Chen and V. Koltun. Photographic image synthesis with

cascaded reﬁnement networks. In ICCV, 2017. 5

[6] J. E. Cutting. Blowing in the wind: Perceiving structure in

trees and bushes. Cognition, 12(1):25 – 44, 1982. 1

[7] K. G. Derpanis and R. P. Wildes. Spacetime texture represen-
tation and recognition based on a spatiotemporal orientation
analysis. PAMI, 34(6):1193–1205, 2012. 1, 2, 3, 4

[8] G. Doretto, A. Chiuso, Y. N. Wu, and S. Soatto. Dynamic

textures. IJCV, 51(2):91–109, 2003. 1, 2

[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. H¨ausser, C. Hazirbas,
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox.
FlowNet: Learning optical ﬂow with convolutional net-
works. In ICCV, pages 2758–2766, 2015. 2, 3, 4

[10] A. A. Efros and T. K. Leung. Texture synthesis by non-

parametric sampling. In ICCV, pages 1033–1038, 1999. 2

[11] M. Fahle and T. Poggio. Visual hyperacuity: Spatiotemporal
interpolation in human vision. Proceedings of the Royal So-
ciety of London B: Biological Sciences, 213(1193):451–477,
1981. 3

[12] C. M. Funke, L. A. Gatys, A. S. Ecker, and M. Bethge. Syn-
thesising dynamic textures using convolutional neural net-
works. arXiv:1702.07006, 2017. 2, 5, 10, 11

[13] L. A. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis
using convolutional neural networks. In NIPS, pages 262–
270, 2015. 1, 2, 3, 4, 5, 7

[14] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks. In CVPR, pages 2414–
2423, 2016. 2, 8

[15] L. A. Gatys, A. S. Ecker, M. Bethge, A. Hertzmann, and
E. Shechtman. Controlling perceptual factors in neural style
transfer. In CVPR, 2017. 8

[16] M. A. Goodale and A. D. Milner. Separate visual path-
ways for perception and action. Trends in Neurosciences,
15(1):20–25, 1992. 2

[18] D. J. Heeger. Optical ﬂow using spatiotemporal ﬁlters. IJCV,

1(4):279–302, 1988. 2, 3

[19] D. J. Heeger and J. R. Bergen. Pyramid-based texture analy-
sis/synthesis. In SIGGRAPH, pages 229–238, 1995. 1, 2
[20] B. K. P. Horn and B. G. Schunck. Determining optical ﬂow.

A.I., 17:185–203, 1981. 2

[21] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet 2.0: Evolution of optical ﬂow estimation
with deep networks. In CVPR, 2017. 2, 3

[22] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution. In ECCV, pages
694–711, 2016. 8

[23] B. Julesz. Visual pattern discrimination. IRE Trans. Infor-

mation Theory, 8(2):84–92, 1962. 2

[24] D. P. Kingma and J. Ba. Adam: A Method for Stochastic

Optimization. arXiv:1412.6980, 2014. 4

[25] K. Konda, R. Memisevic, and V. Michalski. Learning to en-
code motion using spatio-temporal synchrony international
conference on learning representation. In ICLR, 2014. 2
[26] V. Kwatra, A. Sch¨odl, I. Essa, G. Turk, and A. Bobick.
Graphcut textures: Image and video synthesis using graph
cuts. In SIGGRAPH, pages 277–286, 2003. 2

[27] W.-C. Lin, J. Hays, C. Wu, Y. Liu, and V. Kwatra. Quantita-
tive evaluation of near regular texture synthesis algorithms.
In CVPR, volume 1, pages 427–434, 2006. 7

[28] D. C. Liu and J. Nocedal. On the limited memory method
for large scale optimization. Mathematical Programming,
45(3):503–528, 1989. 4

[29] B. D. Lucas and T. Kanade. An iterative image registra-
tion technique with an application to stereo vision. In IJCAI,
pages 674–679, 1981. 2

[30] R. Nelson and R. Polana. Qualitative recognition of motion

using temporal textures. CVGIP, 56(1), 1992. 1

[31] S. Nishimoto and J. L. Gallant. A three-dimensional spa-
tiotemporal receptive ﬁeld model explains responses of area
mt neurons to naturalistic movies. Journal of Neuroscience,
31(41):14551–14564, 2011. 2

[32] R. P´eteri, S. Fazekas, and M. J. Huiskes. DynTex: A Com-
prehensive Database of Dynamic Textures. PRL, 31(12),
2010. 5

[33] J. Portilla and E. P. Simoncelli. A parametric texture model
based on joint statistics of complex wavelet coefﬁcients.
IJCV, 40(1):49–70, 2000. 1, 2

[34] A. Ranjan and M. J. Black. Optical Flow Estimation using a

Spatial Pyramid Network. In CVPR, 2017. 2

[35] J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid.
EpicFlow: Edge-preserving interpolation of correspon-
dences for optical ﬂow. In CVPR, pages 1164–1172, 2015.
2, 4

[36] M. Ruder, A. Dosovitskiy, and T. Brox. Artistic style transfer

for videos. In GCPR, pages 26–36, 2016. 2

[37] A. Sch¨odl, R. Szeliski, D. Salesin, and I. A. Essa. Video

textures. In SIGGRAPH, pages 489–498, 2000. 2

[17] D. Heeger and A. Pentland. Seeing structure through chaos.
In IEEE Motion Workshop: Representation and Analysis,
pages 131–136, 1986. 1

[38] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional
networks for semantic segmentation. PAMI, 39(4):640–651,
2017. 3

[39] E. P. Simoncelli and D. J. Heeger. A model of neuronal re-
sponses in visual area MT. Vision Research, 38(5):743 – 761,
1998. 2, 3, 4

[40] K. Simonyan and A. Zisserman.

Very deep con-
large-scale image recognition.

volutional networks for
arXiv:1409.1556, 2014. 2

[41] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A
dataset of 101 human actions classes from videos in the wild.
arXiv:1212.0402, 2012. 4

[42] M. Szummer and R. W. Picard. Temporal texture modeling.

In ICIP, pages 823–826, 1996. 2

[43] D. Tran, L. D. Bourdev, R. Fergus, L. Torresani, and
M. Paluri. Deep end2end voxel2voxel prediction. In CVPR
Workshops, pages 402–409, 2016. 4

[44] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. S. Lempitsky.
Texture networks: Feed-forward synthesis of textures and
stylized images. In ICML, pages 1349–1357, 2016. 8
[45] Y. Wang and S. C. Zhu. Modeling textured motion: Particle,

wave and sketch. In ICCV, pages 213–220, 2003. 1

[46] A. B. Watson and A. J. Ahumada. A look at motion in the
frequency domain. In Motion workshop: Perception and rep-
resentation, pages 1–10, 1983. 3

[47] L. Wei and M. Levoy. Fast texture synthesis using tree-
structured vector quantization. In SIGGRAPH, pages 479–
488, 2000. 2

[48] J. Xie, S.-C. Zhu, and Y. N. Wu. Synthesizing dynamic
patterns by spatial-temporal generative convnet. In CVPR,
2017. 2, 5, 11

[49] J. J. Yu, A. W. Harley, and K. G. Derpanis. Back to Ba-
sics: Unsupervised Learning of Optical Flow via Brightness
Constancy and Motion Smoothness. In ECCVW, 2016. 2

A. Experimental procedure

Here we provide further experimental details of our user study
using Amazon Mechanical Turk (AMT). Experimental trials were
grouped into batches of Human Intelligence Tasks (HITs) for users
to complete. Each HIT consisted of 59 pairwise comparisons be-
tween a synthesized dynamic texture and its target. Users were
asked to choose which texture appeared more realistic after view-
ing each texture independently for an exposure time (in seconds)
sampled randomly from the set {0.3, 0.4, 0.6, 1.2, 2.4, 3.6, 4.8}.
Note that 12 frames of the dynamic texture corresponds to 1.2 sec-
onds, i.e., 10 frames per second. Before viewing a dynamic tex-
ture, a centred dot is ﬂashed twice to indicate to the user where
to look (left or right). To prepare users for the task, the ﬁrst three
comparisons were used for warm-up, exposing them to the short-
est (0.3s), median (1.2s), and longest (4.8s) durations. To prevent
spamming and bias, we constrained the experiment as follows:
users could make a choice only after both dynamic textures were
shown; the next texture comparison could only be made after a
decision was made for the current comparison; a choice could not
be changed after the next pair of dynamic textures were shown;
and users were each restricted to a single HIT. Obvious unrealistic
dynamic textures were synthesized by terminating synthesis early
(100 iterations) and were used as sentinel tests. Three of the 59
pairwise comparisons were sentinels and results from users which
gave incorrect answers on any of the sentinel comparisons were

not used. The left-right order of textures within a pair, display
order within a pair, and order of pairs within a HIT, were random-
ized. An example of a HIT is shown in a video included with the
supplemental on the project page: HIT example.mp4.

Users were paid $2 USD per HIT, and were required to have
at least a 98% HIT approval rating, greater than or equal to 5000
HITs approved, and to be residing in the US. We collected results
from 200 unique users to evaluate our ﬁnal model and another 200
to evaluate our baseline model.

B. Qualitative results

We provide videos showcasing the qualitative results of our
two-stream model, including the experiments mentioned in the
main manuscript, on our project page: ryersonvisionlab.
github.io/two-stream-projpage. The videos are in
MP4 format (H.264 codec) and are best viewed in a loop. They
are enclosed in the following folders:

• target textures: This folder contains the 59 dynamic

textures used as targets for synthesis.

• dynamic texture synthesis: This folder contains
synthesized dynamic textures where the appearance and dy-
namics targets are the same.

• using concatenation layer: This folder contains
synthesized dynamic textures where the concatenation layer
was used for computing the Gramian on the dynamics
stream. These are the results from our ﬁnal model.

• using flow decode layer: This folder contains syn-
thesized dynamic textures where the predicted ﬂow output
is used for computing the Gramian on the dynamics stream.
These are the results from our baseline.

• full synthesis:

This

folder

contains

synthesized dynamic textures,
generated, nor temporally-endless, etc.

i.e., not

regularly-
incrementally-

• appearance stream only: This folder contains dy-
namic textures synthesized using only the appearance stream
of our two-stream model. The dynamics stream is not used.

• incrementally generated: This folder contains dy-
namic textures synthesized using the incremental process
outlined in Section 3.3 in the main manuscript.

• temporally endless: This folder contains a synthe-
sized dynamic texture (smoke plume 1) where there is no
discernible temporal seam between the last and ﬁrst frames.
Played as a loop, it appears to be temporally endless, thus, it
is presented in animated GIF format.

• dynamics style transfer: This folder contains syn-
thesized dynamic textures where the appearance and dynam-
ics targets are different. Also included are videos where the
synthesized dynamic texture is “pasted” back onto the origi-
nal image it was cropped from, showing a proof-of-concept
of dynamics style transfer as an artistic tool.

• comparisons/funke: This folder contains four dy-
namic texture synthesis comparisons between our model and
a recent (unpublished) approach [12]. The dynamic textures

(labeled “Xie et al. (ST)”) designed for dynamic textures with both
spatial and temporal homogeneity, and their temporal model (la-
beled “Xie et al. (FC)”) designed for dynamic textures with only
temporal homogeneity.

Overall, we demonstrate that our results appear qualitatively
better, showing more temporal coherence and similarity in dy-
namics and fewer artifacts, e.g., blur and ﬂicker. This may be a
natural consequence of their limited representation of dynamics.
Although the spatiotemporal model of Xie et al. [48] is able to
synthesize dynamic textures that lack spatial homogeneity (e.g.,
bamboo and escalator), we note that their method can not
synthesize novel dynamic textures, i.e., it appears to faithfully re-
produce the target texture, reducing the applicability of their ap-
proach.

As a consequence of jointly modelling appearance and dynam-
ics, the methods of [12, 48] are not capable of the novel form of
style transfer we demonstrated. This was enabled by the factored
representation of dynamics and appearance. Furthermore, the spa-
tiotemporal extent of the output sequence generated by Xie et al.’s
[48] method is limited to being equal to the input. The proposed
approach does not share this limitation.

[12] which ex-
chosen are those reported by Funke et al.
hibit spatiotemporal homogeneity. For ease of comparison,
we have concatenated the results from both models with their
corresponding targets.

• comparisons/xie and funke: This folder contains
nine dynamic texture synthesis comparisons between our
model, Funke et al.’s [12], and Xie et al.’s [48]. The dynamic
textures chosen cover the full range of our appearance and
dynamics groupings. For ease of comparison, we have con-
catenated the results from all models with their correspond-
ing targets.

C. Full user study results

Figures 10a and 10b show histograms of the average user ac-
curacy on each texture, averaged over a range of exposure times.
The histogram bars are ordered from lowest to highest accuracy,
based on the results when using our ﬁnal model.

Tables 1 and 2 show the average user accuracy on each texture
when using our ﬁnal model. The results are averaged over expo-
sure times. Similarly, Tables 3 and 4 show the results when using
our baseline.

Tables 5 and 6 show the average user accuracy on texture ap-
pearance groups when using our ﬁnal model. The results are av-
eraged over exposure times. Similarly, Tables 7 and 8 show the
results when using our baseline.

Tables 9 and 10 show the average user accuracy on texture dy-
namics groups when using our ﬁnal model. The results are aver-
aged over exposure times. Similarly, Tables 11 and 12 show the
results when using our baseline.

Tables 13 and 14 show the average user accuracy over all tex-
tures when using our ﬁnal model. The results are averaged over
exposure times. Similarly, Tables 15 and 16 show the results when
using our baseline.

D. Qualitative comparisons

We qualitatively compare our results to those of Funke
et al. [12] and Xie et al. [48].
Note that Funke et al.
[12] provided results on only ﬁve textures and of those only
four are dynamic textures in the sense that
their appear-
ance and dynamics are spatiotemporally coherent. Their re-
sults on these sequences (cranberries, flames, leaves,
and water 5) are included in the folder funke under
dynamic texture synthesis/comparisons. Our re-
sults are included as well.

We also compare our results to [12, 48] on nine dynamic tex-
tures chosen to cover the full range of our dynamics and appear-
ance groupings. We use their publicly available code and follow
the parameters used in their experiments. For Funke et al.’s model
[12], the parameters used are ∆t = 4 and T = 12 (recall that
target dynamic textures consist of 12 frames). For the spatiotem-
poral and temporal models from Xie et al. [48], the parameters
used are T = 1200 and ˜M = 3. A comparison between our
results, Funke et al.’s [12], and Xie et al’s [48] on the nine dy-
namic textures are included in the folder xie and funke un-
der dynamic texture synthesis/comparisons. Note
for Xie et al. [48], we compare with their spatiotemporal model

.
)
s

m
0
0
6
-
0
0
3
(

s
e
m

i
t

e
r
u
s
o
p
x
e

t
r
o
h
S
)
a
(

.
)
s

m
0
0
8
4
-
0
0
2
1
(

s
e
m

i
t

e
r
u
s
o
p
x
e
g
n
o
L
)
b
(

.
e
c
n
e
d
ﬁ
n
o
c

l
a
c
i
t
s
i
t
a
t
s

%
5
9
a
h
t
i

w

r
o
r
r
e

f
o
n
i
g
r
a
m
a

s
e
d
u
l
c
n
i

y
c
a
r
u
c
c
a

e
r
u
t
x
e
t

h
c
a
E

.
s
e
m

i
t

e
r
u
s
o
p
x
e

r
e
v
o

d
e
g
a
r
e
v
a

s
e
i
c
a
r
u
c
c
a

e
r
u
t
x
e
t
-
r
e
P

:
0
1

e
r
u
g
i
F

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

±
±
±
0.0

±
±
±
0.248

0.203
0.095
0.175
0.135
0.217

±
±
±
±
±
0.175

0.205
±
0.196
0.162
0.12
0.212

0.194
±
0.162
±
0.199
±
0.139
±
0.252
±
0.181
±
0.195
0.135
0.186
0.199

300 ms.
0.625
0.769
0.609
0.806
0.533
0.607
0.44
±
0.813
0.727
0.609
0.6
±
0.139
0.806
±
0.168
0.81
±
0.5
0.219
±
0.591
0.48
±
0.792
0.909
0.571
1.0
±
0.069
0.964
±
0.176
0.72
±
0.5
0.209
±
0.435
0.929
0.452
0.813
0.632
0.8
±
0.476
0.458
0.632
0.6
±
0.542
0.517
0.767
0.667
0.792
0.538
0.478
0.769
0.724
0.862
0.72
±
0.643
0.643
0.826
0.538
0.656
0.556
0.375
0.632
0.545
0.688
0.571
0.444

0.199
±
0.182
±
0.151
±
0.202
±
0.162
±
0.192
±
0.204
±
0.162
±
0.163
±
0.126
±
0.176
0.177
0.177
0.155
0.192
0.165
0.23
0.237
0.217
0.208
0.161
0.183
0.187

±
±
±
0.192

0.214
0.199
0.217

±
±
±
±
±
±
±
±
±
±
±
±

400 ms.
0.161
0.333
±
0.215
0.786
±
0.152
0.786
±
0.127
0.88
±
0.164
0.842
±
0.212
0.571
±
0.621
0.177
±
0.245
0.5
±
0.183
0.654
±
0.175
0.773
±
0.175
0.773
±
0.212
0.75
±
0.129
0.839
±
0.429
0.212
±
0.168
0.81
±
0.195
0.318
±
0.158
0.733
±
0.952
0.091
±
0.209
0.65
±
0.0
1.0
±
1.0
0.0
±
0.909
0.12
±
0.565
0.203
±
0.688
0.227
±
0.826
0.155
±
0.538
0.192
±
0.778
0.192
±
0.667
0.202
±
0.903
0.104
±
0.714
0.167
±
0.346
0.183
±
0.667
0.202
±
0.769
0.162
±
0.625
0.168
±
0.741
0.165
±
0.903
0.104
±
0.737
0.198
±
0.938
0.119
±
0.731
0.17
±
0.727
0.186
±
0.833
0.149
±
0.783
0.169
±
0.704
0.172
±
0.708
0.182
±
0.773
0.175
±
0.815
0.147
±
0.947
0.1
±
0.63
0.182
±
0.231
0.5
±
0.183
0.32
±
0.586
0.179
±
0.188
0.64
±
0.165
0.741
±
0.218
0.667
±
0.179
0.586
±
0.201
0.364
±

0.193
0.164
0.187
0.196

±
±
±
±
0.164

±
±
±
±
±
±
±
±
0.0

0.139
0.186
0.155
0.185
0.188
0.111
0.165
0.069

600 ms.
0.714
0.842
0.615
0.846
0.7
±
0.187
0.615
±
0.156
0.622
±
0.169
0.667
±
0.209
0.65
±
0.205
0.591
±
0.643
0.177
±
1.0
0.0
±
0.788
0.727
0.826
0.593
0.696
0.897
0.656
0.964
1.0
±
0.115
0.913
±
0.181
0.552
±
0.151
0.808
±
0.147
0.815
±
0.177
0.621
±
0.202
0.667
±
0.151
0.767
±
0.096
0.95
±
0.173
0.679
±
0.23
0.556
±
0.195
0.652
±
0.155
0.826
±
0.174
0.581
±
0.175
0.8
±
0.16
0.75
±
0.613
0.171
±
0.058
0.97
±
0.165
0.741
±
0.6
0.215
±
0.119
0.938
±
0.168
0.81
±
0.155
0.826
±
0.191
0.813
±
0.111
0.917
±
0.193
0.714
±
0.103
0.889
±
0.19
0.423
±
0.222
0.579
±
0.169
0.667
±
0.652
0.195
±
0.196
0.52
±
0.173
0.75
±
0.179
0.586
±
0.227
0.688
±
0.197
0.583
±

0.12

±
0.131

±
±
0.2

0.151
0.199

1200 ms.
0.185
0.536
±
0.101
0.906
±
0.199
0.542
±
0.193
0.714
±
0.138
0.87
±
0.636
0.164
±
0.201
0.7
±
0.201
0.7
±
0.767
0.609
0.5
±
0.909
0.9
±
0.164
0.636
±
0.147
0.815
±
0.188
0.64
±
0.064
0.967
±
0.111
0.917
±
0.195
0.652
±
0.062
0.968
±
0.102
0.923
±
0.119
0.889
±
0.118
0.871
±
0.149
0.833
±
1.0
0.0
±
0.15
0.75
±
0.792
0.162
±
0.127
0.88
±
0.08
0.958
±
0.163
0.724
±
0.158
0.733
±
0.151
0.767
±
0.955
0.087
±
0.173
0.75
±
0.609
0.199
±
1.0
0.0
±
0.176
0.72
±
0.083
0.957
±
0.237
0.471
±
0.176
0.72
±
0.142
0.821
±
0.963
0.071
±
0.127
0.88
±
0.08
0.958
±
0.87
0.138
±
1.0
0.0
±
0.875
0.615
0.821
0.727
0.826
0.739
0.833
0.759
0.792
0.75

0.132
±
0.187
±
0.142
±
0.186
±
0.155
±
0.179
±
0.149
±
0.156
±
0.162
±
0.16
±

0.188

0.201

0.122

0.195
0.181
0.132
0.182
0.188

0.119
0.195
0.175
0.175
0.126

0.119

0.139
0.126
0.198
0.148

±
0.0
0.0

±
±
±
±
±
0.0

2400 ms.
0.636
±
0.95
0.096
±
0.867
±
0.058
0.97
±
0.731
0.17
±
0.75
0.19
±
0.652
±
0.824
±
0.875
±
0.708
±
0.519
±
1.0
0.0
±
0.938
0.652
0.773
0.548
0.933
1.0
±
0.696
1.0
±
1.0
±
0.889
±
0.92
0.106
±
0.788
±
0.905
±
0.737
±
0.735
±
0.0
1.0
±
0.0
1.0
±
0.808
0.593
0.806
0.857
0.75
0.9
±
0.952
±
0.652
±
0.92
0.106
±
0.895
±
0.5
0.173
±
0.931
±
0.84
0.144
±
0.905
±
0.852
±
0.913
±
0.917
±
0.923
±
0.227
±
0.813
±
0.571
±
0.706
±
0.667
±
0.771
±
0.65
0.209
±
0.696
±
0.182
0.37
±

±
±
±
±
0.19
±
0.131

0.091
0.195

0.138

0.092

0.126
0.134
0.115
0.111
0.102
0.175
0.191
0.212
0.153
0.202
0.139

0.151
0.185
0.139
0.15

0.188

0.15
±
0.084
±
0.195
±
0.077
0.134
±
0.182
±
0.175
±
0.182
0.122
0.163
0.202

0.071
0.163
0.123
0.188
0.099
0.074
0.177

0.132
±
0.111
±
0.189
±
0.064
±
0.225
±
0.138
±
0.127
0.106
0.169
±
0.204
±
0.15
±
0.069
±
0.252
±
0.151
±
0.138
0.259
±
0.119
±
0.167
0.163
0.062
0.157

±
±
±
0.0
0.107
0.0

0.119

±
±
±
0.0

±
±
±
±
±
±
±
0.0
0.0

3600 ms.
0.857
0.938
0.682
0.96
±
0.852
0.762
0.773
0.63
±
0.848
0.724
0.765
1.0
±
0.963
0.724
0.885
0.519
0.926
0.962
0.692
1.0
±
1.0
±
0.875
0.917
0.667
0.967
0.526
0.895
0.88
±
0.92
±
0.783
0.522
0.857
0.964
0.533
0.767
0.87
±
0.571
0.889
0.76
±
0.724
0.968
0.778
1.0
±
0.9
±
1.0
±
0.889
1.0
±
0.619
0.733
0.583
0.818
0.724
0.652
0.652
0.731
0.632

±
±
±
±
±
±
±
±
±

±
0.0

0.208
0.158
0.197
0.161
0.163
0.195
0.195
0.17
0.217

±
±
0.0

0.066
0.133

±
±
±
±
0.0

0.172
0.099
0.192
0.071

0.182
0.217
0.143
0.195
0.152
0.151
0.062
0.091
0.165
0.137
0.214
0.147

±
±
±
±
±
±
±
±
±
±
±
±
0.0
0.179
0.0

4800 ms.
0.704
0.926
0.778
0.963
1.0
±
0.762
0.706
0.781
0.682
0.786
0.658
0.968
0.952
0.741
0.828
0.524
0.815
1.0
±
0.5
±
1.0
±
0.966
0.833
1.0
±
0.151
0.808
±
0.089
0.933
±
0.218
0.667
±
0.155
0.826
±
0.135
0.813
±
0.119
0.889
±
0.138
0.87
±
0.195
0.652
±
0.077
0.96
±
0.127
0.88
±
0.151
0.808
±
0.195
0.652
±
0.145
0.889
±
0.15
0.714
±
0.074
0.962
±
0.165
0.588
±
0.182
0.63
±
0.0
1.0
±
0.87
0.138
±
0.0
1.0
±
0.127
0.88
±
0.069
0.964
0.852
0.134
1.0
±
0.333
0.821
0.394
0.917
0.7
±
0.682
0.667
0.833
0.452

±
±
±
±
0.164

0.195
0.189
0.133
0.175

0.178
0.142
0.167
0.111

±
±
0.0

±
±
±
±

Table 1: Per-texture accuracies averaged over exposure times, using the concatenation layer. Each texture accuracy includes
a margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

±
±
±
±
±
0.111

Short (300-600 ms.)
0.111
0.526
0.103
0.797
0.105
0.675
0.086
0.841
0.703
0.112
0.6
±
0.102
0.571
±
0.102
0.692
±
0.111
0.676
±
0.114
0.657
±
0.114
0.677
±
0.08
0.861
±
0.083
0.812
±
0.123
0.556
±
0.106
0.742
±
0.114
0.473
±
0.098
0.74
±
0.917
0.064
±
0.111
0.63
±
0.025
0.987
±
0.03
0.985
±
0.085
0.843
±
0.114
0.541
±
0.116
0.646
±
0.077
0.859
±
0.105
0.535
±
0.761
0.099
±
0.7
0.107
±
0.074
0.887
±
0.107
0.636
±
0.118
0.441
±
0.118
0.651
±
0.101
0.73
±
0.103
0.586
±
0.106
0.671
±
0.082
0.809
±
0.11
0.662
±
0.068
0.904
±
0.104
0.671
±
0.119
0.6
±
0.09
0.833
±
0.097
0.767
±
0.089
0.797
±
0.107
0.738
±
0.096
0.77
±
0.101
0.724
±
0.071
0.885
±
0.11
0.532
±
0.116
0.594
±
0.115
0.521
±
0.118
0.559
±
0.116
0.594
±
0.107
0.685
±
0.105
0.646
±
0.112
0.603
±
0.114
0.466
±

Long (1200-4800 ms.)
0.093
0.673
±
0.048
0.928
±
0.09
0.723
±
0.053
0.915
±
0.066
0.864
±
0.091
0.716
±
0.098
0.707
±
0.089
0.729
±
0.075
0.798
±
0.087
0.712
±
0.604
0.093
±
0.033
0.97
±
0.051
0.94
±
0.086
0.688
±
0.073
0.827
±
0.095
0.558
±
0.057
0.909
±
0.971
0.032
±
0.094
0.627
±
0.02
0.99
±
0.971
0.032
±
0.063
0.87
±
0.054
0.918
±
0.079
0.776
±
0.045
0.947
±
0.682
0.097
±
0.8
0.078
±
0.064
0.88
±
0.046
0.941
±
0.079
0.792
±
0.093
0.631
±
0.069
0.841
±
0.055
0.917
±
0.094
0.729
±
0.729
0.089
±
0.054
0.93
±
0.093
0.68
±
0.05
0.931
±
0.094
0.674
±
0.089
0.637
±
0.049
0.927
±
0.067
0.863
±
0.947
0.045
±
0.058
0.896
±
0.047
0.94
±
0.903
0.06
±
0.043
0.95
±
0.448
0.099
±
0.078
0.794
±
0.098
0.55
±
0.076
0.806
±
0.709
0.088
±
0.084
0.74
±
0.093
0.688
±
0.082
0.767
±
0.095
0.543
±

All (300-4800 ms.)
0.072
0.608
±
0.048
0.882
±
0.069
0.702
±
0.047
0.886
±
0.06
0.802
±
0.071
0.665
±
0.072
0.636
±
0.067
0.713
±
0.064
0.751
±
0.069
0.69
±
0.072
0.632
±
0.04
0.924
±
0.876
0.05
±
0.071
0.64
±
0.061
0.794
±
0.073
0.522
±
0.055
0.835
±
0.033
0.949
±
0.072
0.629
±
0.016
0.989
±
0.976
0.023
±
0.051
0.86
±
0.064
0.756
±
0.067
0.727
±
0.043
0.908
±
0.073
0.609
±
0.062
0.784
±
0.059
0.806
±
0.041
0.919
±
0.066
0.725
±
0.074
0.556
±
0.063
0.771
±
0.056
0.835
±
0.071
0.657
±
0.068
0.703
±
0.05
0.869
±
0.673
0.071
±
0.04
0.92
±
0.07
0.672
0.071
0.624
0.046
0.892
0.057
0.823
0.049
0.879
0.055
0.836
0.05
0.868
0.058
0.822
0.04
0.921
0.074
0.486
0.068
0.713
0.074
0.538
0.068
0.708
0.071
0.663
0.066
0.718
0.07
0.669
0.068
0.699
0.073
0.511

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±

Table 2: Per-texture accuracies averaged over a range of exposure times, using the concatenation layer. Each texture accuracy
includes a margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

0.126

0.138
0.196
0.151
0.135

0.162

0.115
±
0.106
±
0.202
±
0.143
±
0.144
0.212
0.1
±
0.126
±
0.089
±
0.162
±
0.08
±
0.189
±
0.112
±
0.199
±
0.096
0.161
0.091

0.1
0.091

±
±
0.131

0.145
±
0.156
0.151
0.112

±
0.0

±
0.0

±
±
±
±
0.0
0.0

300 ms.
0.933
1.0
±
0.895
0.846
0.808
0.929
1.0
±
1.0
±
0.875
1.0
±
0.913
0.944
0.765
0.864
0.84
±
0.75
±
0.947
0.905
0.933
0.875
0.958
0.667
0.941
0.609
0.95
±
0.818
0.952
1.0
±
1.0
±
0.947
0.952
0.9
±
0.889
0.85
±
0.808
0.941
1.0
±
0.941
0.867
0.667
1.0
±
1.0
±
0.941
0.958
0.957
1.0
±
0.909
0.684
0.857
0.929
0.778
0.867
0.737
1.0
±
0.947
0.941

±
±
±
±
±
±
±
0.0

±
±
±
0.0
0.0

±
±
0.0
0.0

±
±
±
0.0

±
±
0.0

±
±

0.112
0.172
0.239

0.112
0.08
0.083

0.12
0.209
0.183
0.135
0.272
0.172
0.198

0.1
0.112

0.186

0.1
0.111

0.202
0.083

±
±
±
±
±
0.0
0.0

0.106
0.195
0.138
0.119
0.071

±
±
0.0
0.0
0.138
0.132
0.083
0.111

400 ms.
0.9
±
0.944
0.652
0.895
0.889
0.963
1.0
±
1.0
±
0.947
0.897
1.0
±
1.0
±
0.87
±
0.875
±
0.957
±
0.917
±
1.0
0.0
±
0.765
±
0.957
±
1.0
0.0
±
1.0
0.0
±
0.75
0.19
±
0.88
0.127
±
0.209
0.65
±
0.0
1.0
±
0.096
0.95
±
0.938
0.119
±
0.92
0.106
±
0.0
1.0
±
0.962
1.0
±
1.0
±
1.0
±
0.857
0.8
±
0.857
1.0
±
1.0
±
0.773
0.957
1.0
±
0.958
1.0
±
0.917
1.0
±
0.947
1.0
±
0.588
0.958
0.778
1.0
±
1.0
±
0.905
0.944
0.933
0.88

0.126
±
0.106
±
0.126
±
0.127
±

0.234
0.08
0.192

±
0.175
0.15

±
±
±
0.0
0.0

0.175
0.083

±
0.0
0.0
0.0

±
0.0
0.0

±
±
0.0

0.183

0.074

0.111

±
0.0

±
0.0

±
0.0

0.08

0.1

0.08

±
0.0

±
0.0

±
0.0

0.119

0.099

0.115

0.111

±
0.0
0.0

±
±
±
0.0

±
±
0.0
0.0

±
±
0.0
0.0

0.066
0.083

0.102
0.106

±
±
±
±
0.0

0.126
0.083
0.193

0.207
0.186
0.229
0.1

600 ms.
0.913
1.0
±
0.933
0.957
0.714
1.0
±
0.966
0.957
1.0
±
1.0
±
0.958
1.0
±
0.938
1.0
±
1.0
±
0.926
1.0
±
0.923
0.944
1.0
±
1.0
±
0.722
0.727
0.769
0.947
1.0
±
0.917
1.0
±
0.08
0.958
±
0.077
0.96
±
0.941
0.112
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.75
±
0.923
±
0.947
±
0.941
±
0.846
±
0.0
1.0
±
1.0
0.0
±
0.964
1.0
±
0.962
1.0
±
1.0
±
1.0
±
0.64
±
0.952
±
0.952
±
0.889
±
0.962
±
0.938
±
1.0
0.0
±
0.952
0.947

0.188
0.091
0.091
0.119
0.074
0.119

0.19
0.102
0.1
0.112
0.139

0.091
0.1

±
0.0
0.0
0.0

0.069

0.074

±
0.0

±
0.0

±
±

0.08

0.15

±
0.0

±
0.0

±
0.0

±
0.0

0.112

0.071

0.126

0.183

±
0.0
0.0

±
0.0
0.0

±
±
±
0.0

0.115
0.091
0.087

1200 ms.
0.963
1.0
±
0.1
0.947
±
0.077
0.96
±
0.096
0.95
±
0.962
0.074
±
1.0
0.0
±
0.941
1.0
±
0.857
1.0
±
1.0
±
0.905
1.0
±
1.0
±
0.958
1.0
±
0.172
0.867
±
0.138
0.87
±
0.958
0.08
±
1.0
0.0
±
0.789
1.0
±
0.913
0.952
0.955
1.0
±
0.913
0.923
1.0
±
1.0
±
1.0
±
1.0
±
0.087
0.955
±
0.127
0.88
±
0.929
0.135
±
0.0
1.0
±
1.0
0.0
±
0.889
1.0
±
0.96
1.0
±
1.0
±
1.0
±
1.0
±
0.957
1.0
±
0.778
1.0
±
0.929
1.0
±
1.0
±
0.897
1.0
±
0.85
1.0

±
±
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

0.115
0.145

±
0.0
0.0

±
0.0

0.145

0.077

0.111

0.083

0.192

0.095

0.156

±
0.0

±
0.0

±
0.0

±
0.0

±

0.183
0.091

0.087

0.139

0.112
0.172

0.095

0.155

0.182

±
0.157

±
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.096
±
0.0
0.0

2400 ms.
0.0
1.0
±
0.0
1.0
±
0.131
0.9
±
0.92
0.106
±
0.857
±
0.952
±
1.0
0.0
±
0.955
1.0
±
1.0
±
1.0
±
1.0
±
0.846
±
0.96
0.077
±
0.941
±
0.867
±
1.0
0.0
±
0.929
1.0
±
1.0
±
1.0
±
0.826
0.8
±
0.762
1.0
±
1.0
±
1.0
±
0.95
1.0
±
1.0
±
0.906
1.0
±
1.0
±
1.0
±
0.8
±
1.0
±
1.0
±
0.933
0.944
0.947
1.0
±
0.955
1.0
±
1.0
±
1.0
±
0.95
1.0
±
0.55
1.0
±
0.889
1.0
±
1.0
±
1.0
±
1.0
±
0.929
0.773

±
0.0
0.0
0.0
0.096
±
0.0
0.218
±
0.0

±
0.0
0.0
0.0
0.202
0.0
0.0

±
0.0
0.0
0.0
0.0

±
±
±
0.0

±
±

0.126
0.106
0.1

0.087

0.095
0.175

0.101

0.145

0.1

0.08

0.106

±
0.0
0.0
0.0

±
±
±
0.0
0.0

0.115
0.091
0.145

0.111
0.101
0.161

±
0.0
0.0
0.168
0.091

3600 ms.
0.0
1.0
±
1.0
0.0
±
0.913
±
0.952
±
0.889
±
0.0
1.0
±
0.0
1.0
±
0.077
0.96
±
1.0
0.0
±
0.944
1.0
±
1.0
±
0.81
±
0.952
±
0.0
1.0
±
0.95
0.096
±
1.0
0.0
±
0.947
1.0
±
1.0
±
1.0
±
0.917
0.906
0.818
1.0
±
1.0
±
0.958
1.0
±
1.0
±
1.0
±
1.0
±
0.08
0.958
±
0.08
0.958
±
0.062
0.968
±
0.077
0.96
±
0.0
1.0
±
0.0
1.0
±
0.077
0.96
±
0.095
0.929
1.0
±
1.0
±
1.0
±
1.0
±
1.0
±
0.905
±
0.0
1.0
±
0.0
1.0
±
0.76
0.167
±
0.0
1.0
±
1.0
0.0
±
0.909
0.955
0.875
0.933
0.926
0.905

0.12
0.087
0.132
0.089
0.099
0.126

±
0.0
0.0
0.0
0.0
0.0

±
0.0
0.0
0.0
0.0

0.126

±
±
±
±
±
±

0.1
0.119
0.164
0.172
0.138

0.123

0.066

0.112

±
0.0

±
±
±
±
±
0.0
0.0
0.0

4800 ms.
0.885
1.0
±
0.966
±
0.0
1.0
±
0.92
0.106
±
1.0
0.0
±
0.941
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.157
0.8
±
0.95
0.096
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.0
1.0
±
0.947
0.938
0.842
0.867
0.895
1.0
±
1.0
±
1.0
±
0.895
1.0
±
0.813
0.95
±
0.958
0.889
1.0
±
0.9
±
1.0
±
1.0
±
1.0
±
0.95
±
0.909
1.0
±
1.0
±
1.0
±
0.955
1.0
±
1.0
±
1.0
±
0.783
±
0.0
1.0
±
0.88
0.127
±
0.0
1.0
±
1.0
0.0
±
0.909
0.962
1.0
0.9

±
±
0.0
0.131
0.0
0.0
0.0
0.096
0.12

±
±
0.0
0.131

±
0.0
0.0
0.0

±
0.0
0.0
0.0

±
0.0

±
±

0.087

0.169

0.12
0.074

0.138

0.191
±
0.096
0.08
0.145

Table 3: Per-texture accuracies averaged over exposure times, using the ﬂow decode layer. Each texture accuracy includes a
margin of error with a 95% statistical conﬁdence.

Dynamic texture
ants
bamboo
birds
boiling water 1
boiling water 2
calm water
calm water 2
calm water 3
calm water 4
calm water 5
calm water 6
candle ﬂame
candy 1
candy 2
coral
cranberries
escalator
ﬁreplace 1
ﬁsh
ﬂag
ﬂag 2
ﬂames
ﬂushing water
fountain 1
fountain 2
fur
grass 1
grass 2
grass 3
ink
lava
plants
sea 1
sea 2
shiny circles
shower water 1
sky clouds 1
sky clouds 2
smoke 1
smoke 2
smoke 3
smoke plume 1
snake 1
snake 2
snake 3
snake 4
snake 5
tv static
underwater vegetation 1
water 1
water 4
water 2
water 3
water 5
waterfall
waterfall 2

Short (300-600 ms.)
0.078
0.917
±
0.034
0.983
±
0.102
0.807
±
0.076
0.909
±
0.089
0.811
±
0.05
0.963
±
0.027
0.986
±
0.985
0.029
±
0.055
0.95
±
0.051
0.954
±
0.049
0.956
±
0.028
0.986
±
0.092
0.857
±
0.067
0.912
±
0.057
0.932
±
0.881
0.078
±
0.038
0.98
±
0.081
0.875
±
0.054
0.944
±
0.05
0.963
±
0.987
0.025
±
0.113
0.71
±
0.844
0.089
±
0.124
0.661
±
0.054
0.96
±
0.07
0.917
±
0.062
0.934
±
0.042
0.969
±
0.034
0.983
±
0.047
0.957
±
0.046
0.966
±
0.049
0.964
±
0.044
0.968
±
0.082
0.902
±
0.099
0.788
±
0.071
0.906
±
0.029
0.985
±
0.966
0.046
±
0.094
0.825
±
0.91
0.068
±
1.0
0.0
±
0.038
0.972
±
0.032
0.983
±
0.052
0.946
±
0.026
0.986
±
0.03
0.984
±
0.049
0.964
±
0.121
0.639
±
0.064
0.932
±
0.085
0.887
±
0.077
0.907
±
0.055
0.95
±
0.092
0.857
±
0.037
0.981
±
0.06
0.945
±
0.069
0.918
±

0.05

±
0.0

±
0.0
0.0

±
±
±
±
0.0

±
±
±
±
±
±
0.0

0.075
0.042
0.029
0.046

0.051
0.044
0.064
0.028
0.024
0.04

0.051
0.043
0.029
0.029
0.077
0.068
0.077
0.021
0.023
0.023
0.052
0.022
0.041
0.043
0.03
0.039
0.029
0.065
0.023

Long (1200-4800 ms.)
0.959
0.039
1.0
±
0.934
0.955
0.909
0.979
0.988
0.964
1.0
±
0.948
1.0
±
1.0
±
0.839
0.963
0.985
0.952
1.0
±
0.94
±
0.961
±
0.979
±
0.985
±
0.847
±
0.884
±
0.847
±
0.989
±
0.988
±
0.988
±
0.939
±
0.989
±
0.963
±
0.956
±
0.978
±
0.965
±
0.979
±
0.894
±
0.988
±
1.0
0.0
±
0.978
0.929
0.964
0.989
0.986
1.0
±
0.986
0.973
0.975
1.0
±
0.721
1.0
±
0.921
0.978
0.988
0.914
0.969
0.921
0.897

0.056
0.03
0.023
0.057
0.035
0.056
0.064

0.031
0.055
0.04
0.022
0.026

±
±
±
±
±
0.0

0.027
0.037
0.034

±
±
±
0.0

0.095

±
0.0

±
±
±
±
±
±
±

All (300-4800 ms.)
0.037
0.945
±
0.013
0.993
±
0.051
0.885
±
0.04
0.937
±
0.055
0.861
±
0.026
0.974
±
0.018
0.987
±
0.026
0.974
±
0.023
0.979
±
0.951
0.036
±
0.023
0.98
±
0.014
0.993
±
0.058
0.846
±
0.039
0.939
±
0.957
0.033
±
0.043
0.92
±
0.014
0.993
±
0.046
0.912
±
0.034
0.953
±
0.026
0.973
±
0.019
0.986
±
0.066
0.789
±
0.054
0.867
±
0.069
0.773
±
0.023
0.979
±
0.033
0.958
±
0.03
0.966
±
0.034
0.952
±
0.019
0.986
±
0.031
0.96
±
0.032
0.96
±
0.027
0.972
±
0.029
0.966
±
0.035
0.952
±
0.057
0.848
±
0.034
0.952
±
0.014
0.993
±
0.026
0.973
±
0.052
0.884
±
0.038
0.94
±
0.013
0.993
±
0.023
0.979
±
0.013
0.993
±
0.03
0.966
±
0.023
0.98
±
0.023
0.979
0.019
0.986
0.075
0.687
0.027
0.972
0.047
0.908
0.035
0.952
0.027
0.972
0.05
0.893
0.026
0.973
0.042
0.931
0.047
0.905

±
±
±
±
±
±
±
±
±
±
±

Table 4: Per-texture accuracies averaged over a range of exposure times, using the ﬂow decode layer. Each texture accuracy
includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

300 ms.
0.702
0.806
0.616

±
±
±

0.098
0.046
0.03

400 ms.
0.74
±
0.853
±
0.658
±

0.101

0.044
0.029

600 ms.
0.838
0.837
0.687

±
±
±

0.088
0.043
0.028

1200 ms.
0.84
±
0.903
0.76

0.083
0.036
±
0.026

±

2400 ms.
0.954
0.909
0.751

±
±
±

0.051
0.037
0.026

3600 ms.
0.878
0.919
0.776

±
±
±

0.074
0.031
0.026

4800 ms.
0.827
0.902
0.762

±
±
±

0.082
0.035
0.025

Table 5: Accuracies of textures grouped by appearances, averaged over exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

Short (300-600 ms.)
0.056
0.756
0.026
0.831
0.017
0.654

±
±
±

Long (1200-4800 ms.)
0.038
0.871
0.017
0.908
0.013
0.762

±
±
±

All (300-4800 ms.)
0.033
0.821
0.015
0.875
0.01
0.717

±
±
±

Table 6: Accuracies of textures grouped by appearances, averaged over a range of exposure times, using the concatenation
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

300 ms.
0.078
0.889
±
0.041
0.89
±
0.021
0.901
±

400 ms.
0.933
0.942
0.916

±
±
±

0.063
0.031
0.018

600 ms.
0.921
0.957
0.937

±
±
±

0.067
0.026
0.016

1200 ms.
0.961
0.953
0.957

±
±
±

0.043
0.028
0.014

2400 ms.
0.057
0.948
±
0.025
0.96
±
0.015
0.945
±

3600 ms.
0.984
0.968
0.955

±
±
±

0.031
0.022
0.013

4800 ms.
0.964
0.947
0.96

±
±
0.013
±

0.049
0.029

Table 7: Accuracies of textures grouped by appearances, averaged over exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Appearance group
Regular & Near-regular
Irregular
Stochastic & Near-stochastic

Short (300-600 ms.)
0.914
0.04
±
0.93
0.019
±
0.919
±

0.011

Long (1200-4800 ms.)
0.023
0.964
0.013
0.957
0.007
0.954

±
±
±

All (300-4800 ms.)
0.022
0.943
0.011
0.946
0.006
0.939

±
±
±

Table 8: Accuracies of textures grouped by appearances, averaged over a range of exposure times, using the ﬂow decode
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

300 ms.
0.625
0.721

±
±

0.032
0.039

400 ms.
0.664
0.763

±
±

0.032
0.039

600 ms.
0.698
0.777

±
±

0.03
0.037

1200 ms.
0.741
0.885

0.028
0.028

±
±

2400 ms.
0.753
0.854

0.028
0.032

±
±

3600 ms.
0.762
0.902

0.028
0.026

±
±

4800 ms.
0.755
0.861

0.028
0.029

±
±

Table 9: Accuracies of textures grouped by dynamics, averaged over exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

Short (300-600 ms.)
0.018
0.663
0.022
0.753

±
±

Long (1200-4800 ms.)
0.014
0.753
0.015
0.876

±
±

All (300-4800 ms.)
0.011
0.715
0.013
0.823

±
±

Table 10: Accuracies of textures grouped by dynamics, averaged over a range of exposure times, using the concatenation
layer. Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

300 ms.
0.886
0.92

0.024

±
0.027

±

400 ms.
0.911
0.942

±
±

0.02
0.023

600 ms.
0.934
0.949

±
±

0.018
0.021

1200 ms.
0.947
0.974

0.016
0.016

±
±

2400 ms.
0.945
0.954

0.016
0.02

±
±

3600 ms.
0.955
0.966

0.014
0.017

±
±

4800 ms.
0.954
0.964

0.015
0.018

±
±

Table 11: Accuracies of textures grouped by dynamics, averaged over exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Dynamics group
Spatially-consistent
Spatially-inconsistent

Short (300-600 ms.)
0.012
0.911
0.013
0.937

±
±

Long (1200-4800 ms.)
0.95
±
0.964
±

0.008
0.009

All (300-4800 ms.)
0.007
0.934
0.008
0.953

±
±

Table 12: Accuracies of textures grouped by dynamics, averaged over a range of exposure times, using the ﬂow decode layer.
Each texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

300 ms.
0.661

±

400 ms.
0.699

±

600 ms.
0.726

±

1200 ms.
0.791

0.021

±

2400 ms.
0.788

0.022

±

3600 ms.
0.812

0.021

±

4800 ms.
0.793

0.021

±

0.025

0.025

0.023

Table 13: Average accuracy over all textures, averaged over exposure times, using the concatenation layer. Each texture
accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

Short (300-600 ms.)
0.014
0.695

Long (1200-4800 ms.)
0.011
0.796

All (300-4800 ms.)
0.009
0.754

±

±

±

Table 14: Average accuracy over all textures, averaged over a range of exposure times, using the concatenation layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

300 ms.
0.898

±

0.018

0.015

400 ms.
0.922

±

600 ms.
0.94

±

0.013

1200 ms.
0.956

0.012

±

2400 ms.
0.948

0.013

±

3600 ms.
0.959

0.011

±

4800 ms.
0.957

0.012

±

Table 15: Average accuracy over all textures, averaged over exposure times, using the ﬂow decode layer. Each texture
accuracy includes a margin of error with a 95% statistical conﬁdence.

Group
All textures

Short (300-600 ms.)
0.009
0.921

Long (1200-4800 ms.)
0.006
0.955

All (300-4800 ms.)
0.005
0.941

±

±

±

Table 16: Average accuracy over all textures, averaged over a range of exposure times, using the ﬂow decode layer. Each
texture accuracy includes a margin of error with a 95% statistical conﬁdence.

