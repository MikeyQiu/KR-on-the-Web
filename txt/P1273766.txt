Understanding Measures of Uncertainty for Adversarial Example Detection

Lewis Smith
Department of Engineering Science
University of Oxford
Oxford, United Kingdom

Yarin Gal
Department of Computer Science
University of Oxford
Oxford, United Kingdom

8
1
0
2
 
r
a

M
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
3
3
5
8
0
.
3
0
8
1
:
v
i
X
r
a

Abstract

Measuring uncertainty is a promising technique
for detecting adversarial examples, crafted in-
puts on which the model predicts an incorrect
class with high conﬁdence. But many measures
of uncertainty exist, including predictive en-
tropy and mutual information, each capturing
different types of uncertainty. We study these
measures, and shed light on why mutual infor-
mation seems to be effective at the task of adver-
sarial example detection. We highlight failure
modes for MC dropout, a widely used approach
for estimating uncertainty in deep models. This
leads to an improved understanding of the draw-
backs of current methods, and a proposal to im-
prove the quality of uncertainty estimates using
probabilistic model ensembles. We give illustra-
tive experiments using MNIST to demonstrate
the intuition underlying the different measures
of uncertainty, as well as experiments on a real-
world Kaggle dogs vs cats classiﬁcation dataset.

1

Introduction

Deep neural networks are state of the art models for rep-
resenting complex, high dimensional data such as natural
images. However, neural networks are not robust: it is
possible to ﬁnd small perturbations to the input of the net-
work that produce erroneous and over-conﬁdent classiﬁca-
tion results. Such perturbed inputs, known as adversarial
examples (Szegedy et al., 2013), are a major hurdle for
the use of deep networks in safety-critical applications, or
those for which security is a concern.

One possible hypothesis for the existence of adversarial
examples is that such images lie off the manifold of nat-
ural images, occupying regions where the model makes
unconstrained extrapolations. If this hypothesis were to
hold true, then one would be able to detect adversarial

perturbation by measuring the distance of the perturbed
input to the image manifold.

Hypothetically, such distances could be measured using
nearest neighbour approaches, or by assessing the proba-
bility of the input under a density model on image space.
However, approaches based on geometric distance are a
poor choice for images, as pixel-wise distance is a poor
metric for perceptual similarity; similarly, density mod-
elling is difﬁcult to scale to the very high dimensional
spaces found in image recognition.

Instead, we may consider proxies to the distance from the
image manifold. For example, the model uncertainty of a
discriminative Bayesian classiﬁcation model will be high

Figure 1: Uncertainty of a standard dropout network
trained on MNIST, as measured by mutual information,
visualized in the latent space obtained from a variational
autoencoder. Colours are classes for each encoded train-
ing image. In white is uncertainty, calculated by decoding
each latent point into image space, and evaluating the
mutual information between the decoded image and the
model parameters. A lighter background corresponds to
higher uncertainty.

for points far away from the training data, in theory. High
capacity Bayesian neural networks, for example, could
then be used to obtain such measures of uncertainty. Un-
der the hypothesis that adversarial examples lie far from
the image manifold, i.e. the training data, such uncertainty
could be used to identify an input as adversarial.

The uncertainty of such models is not straightforward to
obtain. Numerical methods for integrating the posterior,
such as Markov Chain Monte Carlo, are difﬁcult to scale
to large datasets (Gal, 2016). As a result, approximations
have been studied extensively. For example, approximate
inference in Bayesian neural networks using dropout is a
computationally tractable technique (Gal & Ghahramani,
2016) which has been successfully used in the literature
(Leibig et al., 2017; Gal, 2016). Further, it has been shown
that dropout based model uncertainty can form a good
proxy for the detection of adversarial examples (Li & Gal,
2017; Feinman et al., 2017; Rawat et al., 2017).

However, existing research has mostly overlooked the ef-
fect of the chosen measure for uncertainty quantiﬁcation.
Many such measures exist, including mutual information,
predictive entropy, softmax variance, expected entropy,
and the entropy of deterministic deep networks. (Li &
Gal, 2017) for example use expected entropy, (Rawat et
al., 2017) use mutual information, whereas (Feinman et
al., 2017) estimate the variance of multiple draws from
the predictive distribution (obtained using dropout). Fur-
ther, to date, research for the identiﬁcation of adversarial
examples using model uncertainty has concentrated on
toy problems such as MNIST, and has not been shown
to extend to more realistic data distributions and larger
models such as ResNet (He et al., 2015).

In this paper we examine the differences between the vari-
ous measures of uncertainty used for adversarial example
detection, and in the process provide further evidence for
the hypothesis that model uncertainty could be used to
identify an input as adversarial. More speciﬁcally, we
illustrate the differences between the measures by project-
ing the uncertainty onto lower dimensional spaces (see
for example Fig. 1). We show that the softmax variance
can be seen as an approximation to the mutual informa-
tion (section 3.2), explaining the effectiveness of this
rather ad-hoc technique. We show that non-adversarial
off-manifold images (for example image interpolations)
are treated in the same way as adversarial inputs by some
measures of uncertainty (inputs which were not assessed
in previous literature using such measures). We highlight
ways in which dropout fails to capture the full Bayesian
uncertainty by visualizing gaps in model uncertainty in
the latent space (Section 4.2), and use this insight to pro-
pose a simple extension to dropout schemes to be studied
in future research. We ﬁnish by demonstrating the effec-

tiveness of dropout on the real-world ASIRRA (Elson
et al., 2007) cats and dogs classiﬁcation dataset (Section
4.3). Code for the experiments described in this paper is
available online 1.

2 Background

We begin with a brief review of required literature on
Bayesian neural networks and adversarial examples.

2.1 Bayesian Deep Learning

A deep neural network (with a given architecture) de-
ﬁnes a function f : X (cid:55)→ Y parametrised by a set of
weights and biases ω = {Wl, bl}L
l=1. These parameters
are generally chosen to minimize some energy function
E : Y × Y (cid:55)→ R on the model outputs and the target
outputs over some dataset D = {xi, yi}N
i=1 with x ∈ X
and y ∈ Y. Since neural networks are highly ﬂexible
models with many degrees of freedom, modiﬁcations to
this loss to regularize the solution are often necessary, so
that the weights are chosen as

ˆω = arg min

E(f (xi; ω), y) + λ

||Wl||2

(1)

(cid:88)

ω

i

(cid:88)

l

for the common choice of an L2 regulariser with weight
decay λ.

Rather than thinking of the weights as ﬁxed parameters to
be optimized over, the Bayesian approach is to treat them
as random variables, and so we place a prior distribution
p(ω) over the weights of the network. If we also have a
likelihood function p(y | x, ω) that gives the probability
of y ∈ Y given a set of parameter values and an input
to the network, we can then conduct inference given a
dataset by marginalizing the parameters. Such models are
known as Bayesian neural networks.

If, as is often the case, the prior on the weights is a zero
mean Gaussian with diagonal covariance, and the en-
ergy of the network is the negative log likelihood (so
p(y | ω, x) = e−E(f (x),y)) then the optimized solution
in equation 1 corresponds to a mode of the posterior over
the weights.

Ideally we would integrate out our uncertainty by tak-
ing the expectation of the predictions over the posterior,
rather than using this point estimate. For neural networks
this can only be done approximately. Here we discuss
one practical approximation, variational inference with
dropout approximating distributions.

2.2 Variational Inference and Dropout

Variational inference is a general technique for approx-
imating complex probability distributions. The idea is

1https://github.com/lsgos/uncertainty-adversarial-paper

to approximate the intractable posterior p(ω | D) with a
simpler approximating distribution qθ(ω). By applying
Jensen’s inequality to the Kullback-Leibler divergence
between the approximating distribution and the true pos-
terior, we can obtain the log-evidence lower bound LV I

(cid:90)

LV I :=

qθ(ω) log p(D | ω)dω − DKL(qθ || p(ω)).

Since the model evidence is a constant that does not de-
pend on the parameters of qθ, maximizing LV I with re-
spect to θ will minimize the KL divergence between q
and the model posterior. The key advantage of this from
a computational point of view is that we have replaced
an integration problem with the optimization problem
of maximising a parametrised function, which can be
approached by standard gradient based techniques.

For neural networks, a common approximating distribu-
tion is dropout (Srivastava et al., 2014) and it’s variants.
In the variational framework, this means the weights are
drawn from
Wl = Ml · diag([zl,j]Kl

j=1)

where zl,j ∼ Bernoulli(pl), l = 1..L, j = 1..Kl−1
for a network with L layers, where the dimension of
each layer is Ki × Ki−1, and the parameters of q are
θ = {Ml, pl | l = [1..L]}. Informally, this corresponds
to randomly setting the outputs of units in the network to
zero (or zeroing the rows of the ﬁxed matrix Ml). Often
the layer dropout probabilities pi are chosen as constant
and not varied as part of the variational framework, but
it is possible to learn these parameters as well (Gal et al.,
2017). Using variational inference, the expectation over
the posterior can be evaluated by replacing the true pos-
terior with the approximating distribution. The dropout
distribution is still challenging to marginalize, but it is
readily sampled from, so expectations can be approxi-
mated using the Monte Carlo estimator

Ep(ω|D)[f ω(x)] =

p(ω|D)f ω(x)dω

(cid:90)

(cid:90)

(cid:39)

(cid:39)

1
T

T
(cid:88)

i=1

qθ(ω)f ω(x)dω

f ωi (x), ω1..T ∼ qθ(ω).

(2)

2.3 Adversarial Examples

Works by (Szegedy et al., 2013) and others, demonstrating
that state-of-the-art deep image classiﬁers can be fooled
by small perturbations to input images, have initiated a
great deal of interest in both understanding the reasons for
why such adversarial examples occur with deep models,
as well as devising methods to resist and detect adversarial
attacks. So far, attacking has proven more successful
than defence; a recent survey of defences by (Carlini &

Wagner, 2017a) found that, with the partial exception
of the method based on dropout uncertainty analysed
by (Feinman et al., 2017), all other investigated defence
methods failed.

There is no precise deﬁnition of when an example quali-
ﬁes as ‘adversarial’. The most common deﬁnition used
is of an input xadv which is close to a real data point x
as measured by some Lp norm, but is classiﬁed wrongly
by the network with high score. Speaking more loosely,
an adversarially perturbed input is one which a human
observer would assign a certain class, but for which the
network would predict a different class with a high score.

It is notable that there exists a second, related, type of im-
ages which have troubling implications for the robustness
of deep models, namely meaningless images which are
nevertheless classiﬁed conﬁdently as belonging to a partic-
ular class (see, for example, (Nguyen et al., 2015)). These
are often produced more for the visualization of network
features than in the interests of producing an adversarial
attack, yet they are still an interesting shortcoming of
neural networks from the point of view of uncertainty,
since they are points that are far from all training data
by any reasonable metric (based on either pixel-wise or
perceptual distance), yet the model gives conﬁdent (low
predictive entropy) predictions for. We refer to these as
‘rubbish class examples’ or ‘fooling images’ following
(Nguyen et al., 2015) and (Goodfellow et al., 2014).

Several possible explanations for the existence of ad-
versarial examples have been proposed in the literature
(Akhtar & Mian, 2018). These include the idea, proposed
in the original paper by (Szegedy et al., 2013), that the
set of adversarial examples are a dense, low probability
set like the rational numbers on R, with the discontinuous
boundary somehow due to the strong non-linearity of neu-
ral networks. Contrary to that, (Goodfellow et al., 2014)
proposed that adversarial examples are partially due of
the intrinsically linear response of neural network layers
to their inputs. (Tanay & Grifﬁn, 2016) have proposed
that adversarial examples are possible when the decision
boundaries are strongly tilted with respect to the vector
separating the means of the class clusters.

Many of these ideas are consistent with the idea that
the training data of the model lies on a low dimensional
manifold in image space, the hypothesis we build upon
and provide evidence for in this paper.

2.4 Measures of Uncertainty

There are two major sources of uncertainty a model may
have:

1. epistemic uncertainty is uncertainty due to our lack
of knowledge; we are uncertain because we lack

understanding. In terms of machine learning, this
corresponds to a situation where our model parame-
ters are poorly determined due to a lack of data, so
our posterior over parameters is broad.

2. aleatoric uncertainty is due to genuine stochastic-
ity in the data. In this situation, an uncertain pre-
diction is the best possible prediction. This corre-
sponds to noisy data; no matter how much data the
model has seen, if there is inherent noise then the
best prediction possible may be a high entropy one
(for example, if we train a model to predict coin ﬂips,
the best prediction is the max-entropy distribution
P (heads) = P (tails)).

In the classiﬁcation setting, where the output of a model
is a conditional probability distribution P (y|x) over some
discrete set of outcomes Y, one straight-forward measure
of uncertainty is the entropy of the predictive distribution

H[P (y|x)] = −

P (y|x) log P (y|x).

(3)

(cid:88)

y∈Y

However, the predictive entropy is not an entirely satisfac-
tory measure of uncertainty, since it does not distinguish
between epistemic and aleatoric uncertainties. However,
it may be of interest to do so; in particular, we want to
capture when an input lies in a region of data space where
the model is poorly constrained, and distinguish this from
inputs near the data distribution with noisy labels.

An attractive measure of uncertainty able to distinguish
epistemic from aleatoric examples is the information gain
between the model parameters and the data. Recall that
the mutual information (MI) between two random vari-
ables X and Y is given by

I(X, Y ) = H[P (X)] − EP (y)H[P (X | Y )]
= H[P (Y )] − EP (x)H[P (Y | X)].

The amount of information we would gain about the
model parameters if we were to receive a label y for
a new point x, given the dataset D is then given by
I(ω, y | D, x) = H[p(y | x, D)] − Ep(ω|D)H[p(y | x, ω)]
(4)
Being uncertain about an input point x implies that if we
knew the label at that point we would gain information.
Conversely, if the parameters at a point are already well
determined, then we would gain little information from
obtaining the label. Thus, the MI is a measurement of the
model’s epistemic uncertainty.

In the form presented above, it is also readily approxi-
mated using the Bayesian interpretation of dropout. The
ﬁrst term we will refer to as the ‘predictive entropy’; this
is just the entropy of the predictive distribution, which we
have already discussed. The second term is the mean of
the entropy of the predictions given the parameters over

(5)

(6)

(7)

(8)

the posterior distribution p(ω | D), and we thus refer to it
as the expected entropy.

These quantities are not tractable analytically for deep
nets, but using the dropout approximation and equation
(2), the predictive distribution, entropy and the MI are
readily computable (Gal, 2016):

p(y | D, x) (cid:39)

p(y | ωi, x)

T
(cid:88)

1
T

i=1
:= pM C(y | x)

H[p(y | D, x)] (cid:39) H[pM C(y | D, x)]
I(ω, y | D, x) (cid:39) H[pM C(y | D, x)]

−

1
T

T
(cid:88)

i=1

H[p(y | ωi, x)]

where ωi ∼ q(ω | D) are samples from the dropout
distribution.

Other, more ad-hoc, measures of uncertainty include the
variance of the softmax probabilities p(y = c | ωi, x)
(with the variance calculated over i), and variation ratios
(Gal, 2016), with the former commonly used in previous
research.

In the next section we shed light on the properties of
these measures of uncertainty in the context of adversarial
example detection. We relate the more ad-hoc softmax
variance measure to the principled mutual information,
and give intuition into why some measures are more suit-
able for adversarial example detection than others. This
is followed by empirical evaluation of these ideas.

3 Understanding Measures of Uncertainty
for Adversarial Example Detection

We start by explaining the type of uncertainty relevant for
adversarial example detection under the hypothesis that
adversarial images lie off the manifold of natural images,
occupying regions where the model makes unconstrained
extrapolations. We continue by relating the fairly ad-
hoc softmax variance measure of uncertainty to mutual
information.

3.1 Adversarial Examples and Uncertainty

Both the MI and predictive entropy should increase on
inputs which lie far from the image manifold. Under our
hypothesis, we expect both to be effective in highlight-
ing such inputs. However, predictive entropy could also
be high near the image manifold, on inputs which have
inherent ambiguity. Such inputs could be ambiguous im-
ages (for example, with MNIST, an image that could be
either of class 1 or 7), or more generally interpolations
between classes. Such inputs would have high predictive
probability for more than one class, yielding high pre-

dictive entropy (but low MI). Such inputs are clearly not
adversarial, but will falsely trigger an automatic detection
system2. We demonstrate this experimentally in the next
section.

Further, adversarial example crafting algorithms seek to
create an example image with a different class to the orig-
inal, typically by either minimising the predicted prob-
ability of the current class for an untargeted attack, or
maximising the predicted probability of a target class.
This has the side-effect that adversarial examples seek to
minimise the entropy of the predictions, a simple conse-
quence of the normalisation of the probability. It is inter-
esting to highlight that this also affects the uncertainty as
measured by MI; since both the mutual information and
entropy are strictly positive, the mutual information is
bounded above by the predictive entropy (see equation 4).
Therefore, the model giving low entropy predictions at a
point is a sufﬁcient condition for the mutual information
to be low as well. Equally, the mutual information bounds
the entropy from below; it is not possible for a model to
give low entropy predictions when the MI is high.

3.2 Mutual Information and Softmax Variance

Some works in the literature estimate the epistemic un-
certainty of a dropout model using the estimated variance
of the MC samples, rather than the mutual information
(Leibig et al., 2017; Feinman et al., 2017; Carlini & Wag-
ner, 2017a). This is somewhat ad-hoc, but seems to work
fairly well in practice. We suggest a possible explanation
to the effectiveness of this ad-hoc measure, arguing that
the softmax variance can be seen as a proxy to the mutual
information.

One way to see the relation between the two measures of
uncertainty is to observe that the variance is the leading
term in the series expansion of the mutual information
which we demonstrate below. For brevity, we denote the
sampled distributions p(y | ωi, x) as pi and the mean
predictive distribution pM C(y | x) as ˆp. These are in
general distribution over C classes, and we denote the
probability of the jth class as ˆpj and pij for the mean and
ith sampled distribution respectively. The variance score
is the mean variance across the classes

ˆσ2 =

(pij − ˆpj)2

(9)

1
C

1
C

1
T

T
(cid:88)

i=1
(cid:32)

C
(cid:88)

j=1




C
(cid:88)

j=1

1
T

T
(cid:88)

i=1

=

(cid:33)



p2
ij

− ˆp2
j



And the mutual information score is

ˆI = H(ˆp) −

1
T

(cid:88)

i

H(pi)

(cid:32)

(cid:88)

=

j

1
T

(cid:88)

i

(cid:33)

pij log pij

− ˆpj log ˆpj

Using a Taylor expansion of the logarithm,

(cid:88)

ˆI =

(cid:88)

pij(pij − 1)

− ˆpj(ˆpj − 1) + ...

(cid:33)

(cid:32)

(cid:32)

1
T

1
T

1
T

j

j

j

i

i

i

(cid:88)

=

(cid:88)

p2
ij

− ˆp2

j −

(cid:32)

C
(cid:88)

T
(cid:88)

=

p2
ij

− ˆp2

j + ...

(cid:33)

(cid:33)

(cid:33)

(cid:32)

1
T

(cid:88)

i

pij

+ ˆpj + ...

(10)

we see that the ﬁrst term in the series is identical up
to a multiplicative constant to the mean variance of the
samples.

This relation between the softmax variance and the mutual
information measure could explain the effectiveness of the
variance in detecting adversarial examples encountered
by (Feinman et al., 2017). MI increases on images far
from the image manifold and not on image interpolations
(on which the predictive variance increases as well), with
the variance following similar trends.

We next give an empirical study of these various measures
of uncertainty, and demonstrate experimentally the claims
above.

4 Empirical Evaluation

In the next section we demonstrate the effectiveness of
various measures of uncertainty as proxies to distance
from the image manifold. We demonstrate the difference
in behaviour between the predictive entropy and mutual
information on image interpolations, both for interpola-
tions in the latent space as well as interpolations in image
space. We continue by visualising the various measures of
uncertainty, highlighting the differences discussed above.
This is further developed by highlighting shortcomings
with current approaches for uncertainty estimation, to
which we suggest initial ideas on how to overcome and
suggest new ideas for attacks (to be explored further in fu-
ture research). We ﬁnish by assessing the ideas discussed
in this paper on a real world dataset of cats vs dogs image
classiﬁcation.

4.1 Measures of Uncertainty on Image Interpola-

tions

2We speculate that previous research using predictive en-
tropy has not encountered this phenomenon due to insufﬁcient
coverage of the test cases.

We start by assessing the behaviour of the measures of
uncertainty on image interpolations, with interpolations

Figure 2: Entropy (middle) and the MI (bottom) vary
along a convex interpolation between two images in latent
space and image space (top). The entropy is high for
regions along both interpolations, wherever the class of
the image is ambiguous. In contrast, the MI is roughly
constant along the interpolation in latent space, since these
images have aleatoric uncertainty (they are ambiguous),
but the model has seen data that resembles them. On the
other hand, the MI has a clear peak as the pixel space
interpolation produces out-of-sample images between the
classes

done both in latent space, as well as we in image space.
That model uncertainty can capture what we want in prac-
tice is demonstrated in Figures 2 and 3, both showing
experiments comparing interpolations, which goes off
the manifold of natural images, to those in the latent
space of an auto-encoder, which we assume does a rea-
sonably good job of capturing the manifold. We see that
the MI distinguishes between these on-manifold and off-
manifold images, whereas the entropy fails to do so. This
is necessary for the hypothesis proposed in the introduc-
tion; if we are able to accurately capture the MI, it would
serve well as a proxy for whether an images belongs to
the learned manifold or not.

4.2 Visualization in Latent Space and Dropout Fail-

ures

We wish to gain intuition into how the different mea-
sures of uncertainty behaves. In order to do so, we use
a variational autoencoder (Kingma & Welling, 2013) to
compress the MNIST latent space. By choosing a latent
space of two dimensions, we can plot the encodings of
the dataset in two dimensions. By decoding the image
that corresponds to a point in latent space, we can classify
the decoded image and evaluate the network uncertainty,
thus providing a two dimensional map of the input space.
Figure 1 shows the latent space with the MI measure of
uncertainty, calculated using dropout. Similarly, Figure
4 shows the predictive entropy measure of uncertainty.
Note the differences in uncertainty near the class clus-
ters boundaries (corresponding to image interpolations) –
the MI has low uncertainty in these regions, whereas the

Figure 3: The entropy (top) and mutual information
(bottom) of the interpolation halfway between 3000 ran-
dom points of different classes in the MNIST test set,
showing the intuition in Figure 2 that the two modes of
interpolation have very different statistical properties with
respect to the model uncertainty, while the entropy cannot
distinguish between them.

Figure 4: The predictive entropy of the same network as
in ﬁgure 1. Note the differences with the MI, which is low
everywhere close to the data in the centre of the plot, but
the entropy is high between the classes here. These points
correspond to images which resemble digits, but which
are inherently ambiguous. Note however that there are
large regions of latent space where the predictive entropy
is high and the MI low, despite being far from any training
data.
predictive entropy is high.

Another question of interest in this context is how well
the dropout approximation captures uncertainty. The ap-
proximating distribution is fairly crude, and variational
inference schemes are known to underestimate the uncer-
tainty of the posterior, tending to ﬁt an approximation to
a local mode rather than capturing the full posterior3.

3There are two reasons for this behaviour: ﬁrstly, that the

are mistakenly overconﬁdent, which adversarial attack al-
gorithms can exploit. This intuition suggests a simple
ﬁx; since a single dropout model averages over a single
mode of the posterior, we can capture the posterior using
an ensemble of dropout models using different initializa-
tions, assuming that these will converge to different local
modes. We ﬁnd that even a small ensemble can qualita-
tively improve this behaviour (Figure 6). We leave further
developments of these insights, into both new adversarial
example crafting techniques using uncertainty gaps, as
well as into new mitigation techniques using ensembles,
for future research.

Lastly, it should be noted that there is no guarantee that
an ensemble of dropout models captures the true poste-
rior. It will approximate it well only if the true poste-
rior is concentrated in many local modes, all of roughly
equal likelihood (since all the models in the ensemble are
weighted equally), and a randomly initialized variational
dropout net trained with some variant of gradient descent
will converge to all of these modes with roughly equal
probability.

4.3 Evaluation on Cats and Dogs Dataset

It has been observed by (Carlini & Wagner, 2017a) that
many proposed defences against adversarial examples

Figure 5: A typical garbage class example generated by
searching latent space. This is classiﬁed as a 2 with high
conﬁdence.
As seen from the ﬁgures, the network does a reasonable
job of capturing uncertainty close to the data. However,
the network’s uncertainty has ‘holes’– regions where the
predictions of the model are very conﬁdent, despite the
images generated by the decoder here being essentially
nonsense (see Figure 5). This is somewhat troubling; we
want our models to be uncertain on data that does not
resemble what they saw during training. It also suggests
that, while the uncertainty estimates generated by MC
dropout are useful, they do not capture the full posterior,
instead capturing local behaviour near one of it’s modes.

This may offer an explanation as to why MC dropout nets
are still vulnerable to adversarial attack; since they do not
capture the full posterior, there are still regions where they

approximating distribution q may not have sufﬁcient capacity
to represent the full posterior, and secondly, the asymmetry of
the KL divergence, which penalizes q placing probability mass
where the support of p is small far more heavily than the reverse.

Figure 6:
The MI calculated using an ensemble of
dropout models, treating all of their predictions as Monte
Carlo samples from the posterior. This mitigates some of
the spuriously conﬁdent regions in latent space

Figure 7: Example adversarial images generated by the
Momentum iterative method at (cid:15) = 10, with original
images on the left, adversarial images on the determin-
istic model in the second column, and those for the MC
dropout model in the fourth column. The difference be-
tween the adversarial image and the original is shown on
the right of each image.

Table 1: The AUC for the adversarial discrimination task described in the experiments section. Fields marked with (S)
denote this quantity evaluated on a version of the dataset with unsuccessful adversarial examples (that do not change the
label) removed. The success rate of each attack in changing the label is given as a measure of each attacks effectiveness
on this dataset.

ENTROPY MI

ENTROPY (S) MI (S)

SUCCESS RATE

BIM (cid:15) = 5
DETERMINISTIC
MC
FGM (cid:15) = 5
DETERMINISTIC MODEL
MC MODEL
MIM (cid:15) = 5
DETERMINISTIC MODEL
MC MODEL
BIM (cid:15) = 10
DETERMINISTIC MODEL
MC MODEL
FGM (cid:15) = 10
DETERMINISTIC MODEL
MC MODEL
MIM (cid:15) = 10
DETERMINISTIC MODEL
MC MODEL

0.322
0.0712

0.439
0.426

0.347
0.0476

0.302
0.0686

0.502
0.480

0.350
0.0527

N.A
0.728

N.A
0.557

N.A
0.657

N.A
0.708

N.A
0.529

N.A
0.661

0.293
0.0617

0.490
0.465

0.319
0.0410

0.285
0.0719

0.550
0.514

0.319
0.0442

N.A
0.733

N.A
0.497

N.A
0.669

N.A
0.723

N.A
0.491

N.A
0.665

0.757
0.900

0.517
0.563

0.743
0.917

0.753
0.917

0.487
0.547

0.763
0.907

fail to generalize from MNIST. Therefore, we also evalu-
ate the various uncertainty measures on a more realistic
dataset; the ASSIRA cats and dogs dataset (see Figure
7 for example images). The task is to distinguish pic-
tures of cats and dogs. While this is not a state of the art
problem, these are realistic, high resolution images. We
ﬁnetune a ResNet model (He et al., 2015), pre-trained on
Imagenet, replacing the ﬁnal layer with a dropout layer
followed by a new fully connected layer. We use 20 for-
ward passes for the Monte Carlo dropout estimates. We
use dropout only on the layers we retrain, treating the
pre-trained convolutions as deterministic.

We compare the receiver operating characteristic (ROC)
of the predictive entropy of the deterministic network,
the predictive entropy of the dropout network (equation
7), and the MI of the dropout network (the MI is always
zero if the model is deterministic; this corresponds to the
approximating distribution q being a delta function). Note
that we compare with the same set of weights (trained
with dropout) – the only difference is whether we use
dropout at test time. For each measure of uncertainty we
generate the ROC plot by thresholding the uncertainty at
different values, using the threshold to decide whether an
input is adversarial or not.

The receiver operating characteristic is evaluated on a
synthetic dataset consisting of images drawn at random

from the test set and images from the test set corrupted by
Gaussian noise, which comprise the negative examples,
as well as adversarial examples generated with the Basic
Iterative Method (Kurakin et al., 2016), Fast Gradient
method (Goodfellow et al., 2014), and Momentum Iter-
ative Method (Dong et al., 2017). We test with the ﬁnal
attack because it is notably strong, winning the recent
NIPS adversarial attack competition, and is simpler to
adapt to stochastic models than the other strong attacks in
the literature, such as that of Carlini and Wagner (Carlini
& Wagner, 2017b).

We ﬁnd that only the mutual information gets a useful
AUC on adversarial examples. In fact, most other mea-
sures of uncertainty seem to be worse than random guess-
ing; this suggests that this dataset has a lot of examples
the model considers to be ambiguous (high aleatoric un-
certainty), which mean that the entropy has a high false
positive rate. The fact the AUC of the entropy is low
suggests that the model is actually more conﬁdent about
adversarial examples than natural ones under this mea-
sure.

An interesting quirk of this particular model is that the ac-
curacy of using Monte Carlo estimation is actually lower
than the point estimates, even though the uncertainty esti-
mates are sensible. Possibly this is because the dropout
probability is quite high; only a subset of the features

Figure 8: BIM with (cid:15) = 5

Figure 9: FGM with (cid:15) = 5

Figure 10: MIM with (cid:15) = 5

Figure 11: BIM with (cid:15) = 10

Figure 12: FGM with (cid:15) = 10

Figure 13: MIM with (cid:15) = 10

Figure 14: ROC (precision recall) plots for adversarial example detection with different measures of uncertainty and
different attacks. From left to right: basic iterative method (BIM), fast gradient method (FGM), and momentum iterative
method (MIM). Top row uses (cid:15) of 5, bottom row uses (cid:15) of 10. All use inﬁnity norm. (succ) denotes the quantity
evaluated only for successful adversarial examples. We suspect that the low FGM attack success rate is related to the
difﬁcultly we observe in identifying these using model uncertainty, however further investigation is required.

in the later layers of a convnet are relevant to cat and
dog discrimination, so this may be a relic of our transfer
learning procedure; dropout does not normally have an
adverse effect on the accuracy of fully trained models.

5 Discussion & Conclusion

While security considerations for practical applications
are clearly a concern, fundamentally, we are interested
in the question of whether adversarial examples are an
intrinsic property of neural networks, in the sense that
they are somehow a property of the function class, or
whether they are an artefact of our training procedure,
that can be addressed through better training and infer-
ence. We believe that the results in this paper are evidence
in favour of the latter; even approximate marginalization
produces an improvement in terms of robustness to adver-
sarial examples. It is notable that fundamentally, these
techniques can be derived without any explicit reference
to the adversarial setting, and no assumptions are made

about the distribution of adversarial examples. Rather,
looking for better uncertainty results in better robustness
automatically.

We do not claim that dropout alone provides a very con-
vincing defence against adversarial attack. Our results (in
agreement with previous literature on the subject) show
that dropout networks are more difﬁcult to attack than their
deterministic counterparts, but attacks against them can
still succeed while remaining imperceptible to the human
eye, at least in the white-box setting we investigated.

However, we think that this is likely to be because dropout
is a fairly crude approximation that underestimates the un-
certainty signiﬁcantly, as suggested by our visualisations
in latent space. These insights suggest that the best way
to combat adversarial examples is from ﬁrst principles; by
improving model robustness and dealing with uncertainty
properly, models become harder to fool as a side effect.
Looking for scalable ways to improve on the uncertainty
quality captured by dropout is an important avenue for

Li, Y., & Gal, Y. (2017). Dropout inference in bayesian
neural networks with alpha-divergences. arXiv preprint
arXiv:1703.02914.

Nguyen, A., Yosinski, J., & Clune, J.

(2015). Deep
neural networks are easily fooled: High conﬁdence
predictions for unrecognizable images. In Proceedings
of the ieee conference on computer vision and pattern
recognition (pp. 427–436).

Rawat, A., Wistuba, M., & Nicolae, M.-I. (2017). Ad-
versarial phenomenon in the eyes of bayesian deep
learning. arXiv preprint arXiv:1711.08244.

Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever,
I., & Salakhutdinov, R. (2014). Dropout: a simple way
to prevent neural networks from overﬁtting. Journal of
machine learning research, 15(1), 1929–1958.

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Er-
In-
han, D., Goodfellow, I., & Fergus, R.
triguing properties of neural networks. arXiv preprint
arXiv:1312.6199.

(2013).

Tanay, T., & Grifﬁn, L.

(2016). A boundary tilting
persepective on the phenomenon of adversarial exam-
ples. arXiv preprint arXiv:1608.07690.

future research in this area.

References

Akhtar, N., & Mian, A. (2018). Threat of adversarial
attacks on deep learning in computer vision: A survey.
arXiv preprint arXiv:1801.00553.

Carlini, N., & Wagner, D. (2017a). Adversarial exam-
ples are not easily detected: Bypassing ten detection
methods. arXiv preprint arXiv:1705.07263.

Carlini, N., & Wagner, D. (2017b). Towards evaluating
In Security and

the robustness of neural networks.
privacy (sp), 2017 ieee symposium on (pp. 39–57).

Dong, Y., Liao, F., Pang, T., Su, H., Hu, X., Li, J., & Zhu,
J. (2017). Boosting adversarial attacks with momentum.
arXiv preprint arXiv:1710.06081.

Elson, J., Douceur, J. J., Howell, J., & Saul, J. (2007, Oc-
tober). Asirra: A captcha that exploits interest-aligned
manual image categorization. In Proceedings of 14th
acm conference on computer and communications se-
curity (ccs). Association for Computing Machinery,
Inc.

Feinman, R., Curtin, R. R., Shintre, S., & Gardner, A. B.
(2017). Detecting adversarial samples from artifacts.
arXiv preprint arXiv:1703.00410.

Gal, Y. (2016). Uncertainty in deep learning (Unpub-
lished doctoral dissertation). PhD thesis, University of
Cambridge.

Gal, Y., & Ghahramani, Z. (2016). Dropout as a bayesian
approximation: Representing model uncertainty in
deep learning. In international conference on machine
learning (pp. 1050–1059).

Gal, Y., Hron, J., & Kendall, A. (2017). Concrete dropout.

arXiv preprint arXiv:1705.07832.

Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Ex-
plaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572.

He, K., Zhang, X., Ren, S., & Sun, J.

(2015).
Deep residual learning for image recognition. corr
abs/1512.03385 (2015).

Kingma, D. P., & Welling, M. (2013). Auto-encoding
variational bayes. arXiv preprint arXiv:1312.6114.

Kurakin, A., Goodfellow, I., & Bengio, S. (2016). Adver-
sarial examples in the physical world. arXiv preprint
arXiv:1607.02533.

Leibig, C., Allken, V., Ayhan, M. S., Berens, P., & Wahl,
S. (2017). Leveraging uncertainty information from
deep neural networks for disease detection. Scientiﬁc
Reports, 7(1), 17816. Retrieved from https://doi
.org/10.1038/s41598-017-17876-z
doi:
10.1038/s41598-017-17876-z

