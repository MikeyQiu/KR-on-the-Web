7
1
0
2
 
v
o
N
 
5
 
 
]

C
O
.
h
t
a
m

[
 
 
3
v
8
6
4
6
0
.
7
0
7
1
:
v
i
X
r
a

Breaking the Nonsmooth Barrier: A Scalable Parallel
Method for Composite Optimization

Fabian Pedregosa
INRIA/ENS∗
Paris, France

R´emi Leblond
INRIA/ENS∗
Paris, France

Simon Lacoste-Julien
MILA and DIRO
Universit´e de Montr´eal, Canada

Abstract

Due to their simplicity and excellent performance, parallel asynchronous variants
of stochastic gradient descent have become popular methods to solve a wide range
of large-scale optimization problems on multi-core architectures. Yet, despite their
practical success, support for nonsmooth objectives is still lacking, making them
unsuitable for many problems of interest in machine learning, such as the Lasso,
group Lasso or empirical risk minimization with convex constraints. In this work,
we propose and analyze PROXASAGA, a fully asynchronous sparse method in-
spired by SAGA, a variance reduced incremental gradient algorithm. The proposed
method is easy to implement and signiﬁcantly outperforms the state of the art on
several nonsmooth, large-scale problems. We prove that our method achieves a
theoretical linear speedup with respect to the sequential version under assump-
tions on the sparsity of gradients and block-separability of the proximal term.
Empirical benchmarks on a multi-core architecture illustrate practical speedups of
up to 12x on a 20-core machine.

1

Introduction

The widespread availability of multi-core computers motivates the development of parallel methods
adapted for these architectures. One of the most popular approaches is HOGWILD (Niu et al., 2011),
an asynchronous variant of stochastic gradient descent (SGD). In this algorithm, multiple threads run
the update rule of SGD asynchronously in parallel. As SGD, it only requires visiting a small batch
of random examples per iteration, which makes it ideally suited for large scale machine learning
problems. Due to its simplicity and excellent performance, this parallelization approach has recently
been extended to other variants of SGD with better convergence properties, such as SVRG (Johnson
& Zhang, 2013) and SAGA (Defazio et al., 2014).

Despite their practical success, existing parallel asynchronous variants of SGD are limited to smooth
objectives, making them inapplicable to many problems in machine learning and signal processing.
In this work, we develop a sparse variant of the SAGA algorithm and consider its parallel asyn-
chronous variants for general composite optimization problems of the form:

arg min
x∈Rp

f (x) + h(x)

, with f (x) := 1
n

(cid:80)n

i=1 fi(x)

,

(OPT)

where each fi is convex with L-Lipschitz gradient, the average function f is µ-strongly convex and
h is convex but potentially nonsmooth. We further assume that h is “simple” in the sense that we
have access to its proximal operator, and that it is block-separable, that is, it can be decomposed
block coordinate-wise as h(x) = (cid:80)
B∈BhB([x]B), where B is a partition of the coefﬁcients into

∗DI ´Ecole normale sup´erieure, CNRS, PSL Research University

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

subsets which will call blocks and hB only depends on coordinates in block B. Note that there is
no loss of generality in this last assumption as a unique block covering all coordinates is a valid
partition, though in this case, our sparse variant of the SAGA algorithm reduces to the original SAGA
algorithm and no gain from sparsity is obtained.

This template models a broad range of problems arising in machine learning and signal processing:
the ﬁnite-sum structure of f includes the least squares or logistic loss functions; the proximal term
h includes penalties such as the (cid:96)1 or group lasso penalty. Furthermore, this term can be extended-
valued, thus allowing for convex constraints through the indicator function.

Contributions. This work presents two main contributions. First, in §2 we describe Sparse Proxi-
mal SAGA, a novel variant of the SAGA algorithm which features a reduced cost per iteration in the
presence of sparse gradients and a block-separable penalty. Like other variance reduced methods, it
enjoys a linear convergence rate under strong convexity. Second, in §3 we present PROXASAGA, a
lock-free asynchronous parallel version of the aforementioned algorithm that does not require con-
sistent reads. Our main results states that PROXASAGA obtains (under assumptions) a theoretical
linear speedup with respect to its sequential version. Empirical benchmarks reported in §4 show that
this method dramatically outperforms state-of-the-art alternatives on large sparse datasets, while the
empirical speedup analysis illustrates the practical gains as well as its limitations.

1.1 Related work

Asynchronous coordinate-descent. For composite objective functions of the form (OPT), most of
the existing literature on asynchronous optimization has focused on variants of coordinate descent.
Liu & Wright (2015) proposed an asynchronous variant of (proximal) coordinate descent and proved
a near-linear speedup in the number of cores used, given a suitable step size. This approach has been
recently extended to general block-coordinate schemes by Peng et al. (2016), to greedy coordinate-
descent schemes by You et al. (2016) and to non-convex problems by Davis et al. (2016). However,
as illustrated by our experiments, in the large sample regime coordinate descent compares poorly
against incremental gradient methods like SAGA.

Variance reduced incremental gradient and their asynchronous variants.
Initially proposed in
the context of smooth optimization by Le Roux et al. (2012), variance reduced incremental gradient
methods have since been extended to minimize composite problems of the form (OPT) (see table
below). Smooth variants of these methods have also recently been extended to the asynchronous set-
ting, where multiple threads run the update rule asynchronously and in parallel. Interestingly, none
of these methods achieve both simultaneously, i.e. asynchronous optimization of composite prob-
lems. Since variance reduced incremental gradient methods have shown state of the art performance
in both settings, this generalization is of key practical interest.

Objective

Smooth

Composite

Sequential Algorithm
SVRG (Johnson & Zhang, 2013)
SDCA (Shalev-Shwartz & Zhang, 2013)
SAGA (Defazio et al., 2014)
PROXSDCA (Shalev-Shwartz et al., 2012)
SAGA (Defazio et al., 2014)
ProxSVRG (Xiao & Zhang, 2014)

Asynchronous Algorithm

SVRG (Reddi et al., 2015)
PASSCODE (Hsieh et al., 2015, SDCA variant)
ASAGA (Leblond et al., 2017, SAGA variant)

This work: PROXASAGA

On the difﬁculty of a composite extension. Two key issues explain the paucity in the develop-
ment of asynchronous incremental gradient methods for composite optimization. The ﬁrst issue
is related to the design of such algorithms. Asynchronous variants of SGD are most competitive
when the updates are sparse and have a small overlap, that is, when each update modiﬁes a small
and different subset of the coefﬁcients. This is typically achieved by updating only coefﬁcients for
which the partial gradient at a given iteration is nonzero,2 but existing schemes such as the lagged
updates technique (Schmidt et al., 2016) are not applicable in the asynchronous setting. The second

2Although some regularizers are sparsity inducing, large scale datasets are often extremely sparse and lever-

aging this property is crucial for the efﬁciency of the method.

2

difﬁculty is related to the analysis of such algorithms. All convergence proofs crucially use the Lip-
schitz condition on the gradient to bound the noise terms derived from asynchrony. However, in the
composite case, the gradient mapping term (Beck & Teboulle, 2009), which replaces the gradient
in proximal-gradient methods, does not have a bounded Lipschitz constant. Hence, the traditional
proof technique breaks down in this scenario.

Other approaches. Recently, Meng et al. (2017); Gu et al. (2016) independently proposed a dou-
bly stochastic method to solve the problem at hand. Following Meng et al. (2017) we refer to it
as Async-PROXSVRCD. This method performs coordinate descent-like updates in which the true
gradient is replaced by its SVRG approximation. It hence features a doubly-stochastic loop: at each
iteration we select a random coordinate and a random sample. Because the selected coordinate block
is uncorrelated with the chosen sample, the algorithm can be orders of magnitude slower than SAGA
in the presence of sparse gradients. Appendix F contains a comparison of these methods.

1.2 Deﬁnitions and notations

By convention, we denote vectors and vector-valued functions in lowercase boldface (e.g. x) and
matrices in uppercase boldface (e.g. D). The proximal operator of a convex lower semicontinuous
function h is deﬁned as proxh(x) := arg minz∈Rp {h(z) + 1
2 (cid:107)x − z(cid:107)2}. A function f is said to be
L-smooth if it is differentiable and its gradient is L-Lipschitz continuous. A function f is said to be
µ-strongly convex if f − µ
2 (cid:107) · (cid:107)2 is convex. We use the notation κ := L/µ to denote the condition
number for an L-smooth and µ-strongly convex function.3
I p denotes the p-dimensional identity matrix, 1{cond} the characteristic function, which is 1 if cond
evaluates to true and 0 otherwise. The average of a vector or matrix is denoted α := 1
i=1 αi.
n
We use (cid:107) · (cid:107) for the Euclidean norm. For a positive semi-deﬁnite matrix D, we deﬁne its associated
distance as (cid:107)x(cid:107)2
D := (cid:104)x, Dx(cid:105). We denote by [ x ]b the b-th coordinate in x. This notation is
overloaded so that for a collection of blocks T = {B1, B2, . . .}, [x]T denotes the vector x restricted
to the coordinates in the blocks of T . For convenience, when T consists of a single block B we use
[x]B as a shortcut of [x]{B}. Finally, we distinguish E, the full expectation taken with respect to
all the randomness in the system, from E, the conditional expectation of a random it (the random
feature sampled at each iteration by SGD-like algorithms) conditioned on all the “past”, which the
context will clarify.

(cid:80)n

2 Sparse Proximal SAGA

Original SAGA algorithm. The original SAGA algorithm (Defazio et al., 2014) maintains two
moving quantities: the current iterate x and a table (memory) of historical gradients (αi)n
i=1. At
every iteration, it samples an index i ∈ {1, . . . , n} uniformly at random, and computes the next
iterate (x+, α+) according to the following recursion:
ui = ∇fi(x) − αi + α ; x+ = proxγh

(1)
On each iteration, this update rule requires to visit all coefﬁcients even if the partial gradients ∇fi are
sparse. Sparse partial gradients arise in a variety of practical scenarios: for example, in generalized
linear models the partial gradients inherit the sparsity pattern of the dataset. Given that large-scale
datasets are often sparse,4 leveraging this sparsity is crucial for the success of the optimizer.

i = ∇fi(x) .

(cid:0)x − γui

(cid:1); α+

Sparse Proximal SAGA algorithm. We will now describe an algorithm that leverages sparsity
in the partial gradients by only updating those blocks that intersect with the support of the partial
gradients. Since in this update scheme some blocks might appear more frequently than others, we
will need to counterbalance this undersirable effect with a well-chosen block-wise reweighting of
the average gradient and the proximal term.

In order to make precise this block-wise reweighting, we deﬁne the following quantities. We denote
by Ti the extended support of ∇fi, which is the set of blocks that intersect the support of ∇fi,

3Since we have assumed that each individual fi is L-smooth, f itself is L-smooth – but it could have a

smaller smoothness constant. Our rates are in terms of this bigger L/µ, as is standard in the SAGA literature.

4For example, in the LibSVM datasets suite, 8 out of the 11 datasets (as of May 2017) with more than a

million samples have a density between 10−4 and 10−6.

3

i

formally deﬁned as Ti := {B : supp(∇fi) ∩ B (cid:54)= ∅, B ∈ B}. For totally separable penalties such
as the (cid:96)1 norm, the blocks are individual coordinates and so the extended support covers the same
coordinates as the support. Let dB := n/nB, where nB := (cid:80)
1{B ∈ Ti} is the number of times
that B ∈ Ti. For simplicity we assume nB > 0, as otherwise the problem can be reformulated
without block B. The update rule in (1) requires computing the proximal operator of h, which
involves a full pass on the coordinates. In our proposed algorithm, we replace h in (1) with the
function ϕi(x) := (cid:80)
dBhB(x), whose form is justiﬁed by the following three properties.
First, this function is zero outside Ti, allowing for sparse updates. Second, because of the block-wise
reweighting dB, the function ϕi is an unbiased estimator of h (i.e., E ϕi = h), property which will
be crucial to prove the convergence of the method. Third, ϕi inherits the block-wise structure of h
and its proximal operator can be computed from that of h as [proxγϕi(x)]B = [prox(dB γ)hB (x)]B
if B ∈ Ti and [proxγϕi(x)]B = [x]B otherwise. Following Leblond et al. (2017), we will also
replace the dense gradient estimate ui by the sparse estimate vi := ∇fi(x) − αi + Diα, where
Di is the diagonal matrix deﬁned block-wise as [Di]B,B = dB1{B ∈ Ti}I |B|. It is easy to verify
that the vector Diα is a weighted projection onto the support of Ti and E Diα = α, making vi an
unbiased estimate of the gradient.

B∈Ti

We now have all necessary elements to describe the Sparse Proximal SAGA algorithm. As the
original SAGA algorithm, it maintains two moving quantities: the current iterate x ∈ Rp and a
i=1, αi ∈ Rp. At each iteration, the algorithm samples an index
table of historical gradients (αi)n
i ∈ {1, . . . , n} and computes the next iterate (x+, α+) as:

vi = ∇fi(x) − αi + Diα ; x+ = proxγϕi

(cid:0)x − γvi

(cid:1) ; α+

i = ∇fi(x) ,

(SPS)

where in a practical implementation the vector α is updated incrementally at each iteration.

The above algorithm is sparse in the sense that it only requires to visit and update blocks in the
extended support: if B /∈ Ti, by the sparsity of vi and proxϕi, we have [x+]B = [x]B. Hence,
when the extended support Ti is sparse, this algorithm can be orders of magnitude faster than the
naive SAGA algorithm. The extended support is sparse for example when the partial gradients are
sparse and the penalty is separable, as is the case of the (cid:96)1 norm or the indicator function over a
hypercube, or when the the penalty is block-separable in a way such that only a small subset of the
blocks overlap with the support of the partial gradients. Initialization of variables and a reduced
storage scheme for the memory are discussed in the implementation details section of Appendix E.

Relationship with existing methods. This algorithm can be seen as a generalization of both the
Standard SAGA algorithm and the Sparse SAGA algorithm of Leblond et al. (2017). When the
proximal term is not block-separable, then dB = 1 (for a unique block B) and the algorithm defaults
to the Standard (dense) SAGA algorithm. In the smooth case (i.e., h = 0), the algorithm defaults to
the Sparse SAGA method. Hence we note that the sparse gradient estimate vi in our algorithm is the
same as the one proposed in Leblond et al. (2017). However, we emphasize that a straightforward
combination of this sparse update rule with the proximal update from the Standard SAGA algorithm
results in a nonconvergent algorithm: the block-wise reweighting of h is a surprisingly simple but
crucial change. We now give the convergence guarantees for this algorithm.
Theorem 1. Let γ = a
SAGA converges geometrically in expectation with a rate factor of at least ρ = 1
is, for xt obtained after t updates, we have the following bound:

5L for any a ≤ 1 and f be µ-strongly convex (µ > 0). Then Sparse Proximal
κ }. That

5 min{ 1

n , a 1

E(cid:107)xt − x∗(cid:107)2 ≤ (1 − ρ)tC0 , with C0 := (cid:107)x0 − x∗(cid:107)2 + 1
5L2

(cid:80)n

i=1 (cid:107)α0

i − ∇fi(x∗)(cid:107)2

.

Remark. For the step size γ = 1/5L, the convergence rate is (1 − 1/5 min{1/n, 1/κ}). We can thus
identify two regimes: the “big data” regime, n ≥ κ, in which the rate factor is bounded by 1/5n, and
the “ill-conditioned” regime, κ ≥ n, in which the rate factor is bounded by 1/5κ. This rate roughly
matches the rate obtained by Defazio et al. (2014). While the step size bound of 1/5L is slightly
smaller than the 1/3L one obtained in that work, this can be explained by their stronger assumptions:
each fi is strongly convex whereas they are strongly convex only on average in this work. All proofs
for this section can be found in Appendix B.

4

i=1

j=1[ ˆαj ]Ti

ˆx = inconsistent read of x
ˆα = inconsistent read of α
Sample i uniformly in {1, ..., n}
Si := support of ∇fi
Ti := extended support of ∇fi in B
[ α ]Ti = 1/n (cid:80)n
[ δα ]Si = [∇fi(ˆx)]Si − [ ˆαi]Si
[ ˆv ]Ti = [ δα ]Ti + [Diα ]Ti
[ δx ]Ti = [proxγϕi(ˆx − γ ˆv)]Ti − [ˆx]Ti
for B in Ti do
for b ∈ B do

Algorithm 1 PROXASAGA (analyzed)
1: Initialize shared variables x and (αi)n
2: keep doing in parallel
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end parallel loop

end for
// (‘←’ denotes shared memory update.)

[ x ]b ← [ x ]b + [ δx ]b
if b ∈ Si then

[αi]b ← [∇fi(ˆx)]b

end if
end for

(cid:46) atomic

i=1, α

Sample i uniformly in {1, ..., n}
Si := support of ∇fi
Ti := extended support of ∇fi in B
[ ˆx ]Ti = inconsistent read of x on Ti
ˆαi = inconsistent read of αi
[ α ]Ti = inconsistent read of α on Ti
[ δα ]Si = [∇fi(ˆx)]Si − [ ˆαi]Si
[ ˆv ]Ti = [δα ]Ti + [ Diα ]Ti
[ δx ]Ti = [proxγϕi(ˆx − γ ˆv)]Ti − [ˆx]Ti
for B in Ti do
for b in B do

Algorithm 2 PROXASAGA (implemented)
1: Initialize shared variables x, (αi)n
2: keep doing in parallel
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
end for
19:
20: αi ← ∇fi(ˆx)
21: end parallel loop

[ x ]b ← [ x ]b + [ δx ]b
if b ∈ Si then

end if
end for

[ α ]b ← [α]b + 1/n[δα]b (cid:46) atomic

(scalar update) (cid:46) atomic

(cid:46) atomic

3 Asynchronous Sparse Proximal SAGA

We introduce PROXASAGA – the asynchronous parallel variant of Sparse Proximal SAGA. In this
algorithm, multiple cores update a central parameter vector using the Sparse Proximal SAGA intro-
duced in the previous section, and updates are performed asynchronously. The algorithm parameters
are read and written without vector locks, i.e., the vector content of the shared memory can poten-
tially change while a core is reading or writing to main memory coordinate by coordinate. These
operations are typically called inconsistent (at the vector level).

The full algorithm is described in Algorithm 1 for its theoretical version (on which our analysis
is built) and in Algorithm 2 for its practical implementation. The practical implementation differs
from the analyzed agorithm in three points. First, in the implemented algorithm, index i is sampled
before reading the coefﬁcients to minimize memory access since only the extended support needs to
be read. Second, since our implementation targets generalized linear models, the memory αi can be
compressed into a single scalar in L20 (see Appendix E). Third, α is stored in memory and updated
incrementally instead of recomputed at each iteration.

The rest of the section is structured as follows: we start by describing our framework of analysis; we
then derive essential properties of PROXASAGA along with a classical delay assumption. Finally,
we state our main convergence and speedup result.

3.1 Analysis framework

As in most of the recent asynchronous optimization literature, we build on the hardware model in-
troduced by Niu et al. (2011), with multiple cores reading and writing to a shared memory parameter
vector. These operations are asynchronous (lock-free) and inconsistent:5 ˆxt, the local copy of the
parameters of a given core, does not necessarily correspond to a consistent iterate in memory.

“Perturbed” iterates. To handle this additional difﬁculty, contrary to most contributions in this
ﬁeld, we choose the “perturbed iterate framework” proposed by Mania et al. (2017) and reﬁned
by Leblond et al. (2017). This framework can analyze variants of SGD which obey the update rule:

xt+1 = xt − γv(xt, it) , where v veriﬁes the unbiasedness condition E v(x, it) = ∇f (x)

5This is an extension of the framework of Niu et al. (2011), where consistent updates were assumed.

5

and the expectation is computed with respect to it. In the asynchronous parallel setting, cores are
reading inconsistent iterates from memory, which we denote ˆxt. As these inconsistent iterates are
affected by various delays induced by asynchrony, they cannot easily be written as a function of
their previous iterates. To alleviate this issue, Mania et al. (2017) choose to introduce an additional
quantity for the purpose of the analysis:
xt+1 := xt − γv(ˆxt, it) ,

(2)
Note that this equation is the deﬁnition of this new quantity xt. This virtual iterate is useful for the
convergence analysis and makes for much easier proofs than in the related literature.

the “virtual iterate” – which is never actually computed .

“After read” labeling. How we choose to deﬁne the iteration counter t to label an iterate xt
matters in the analysis.
In this paper, we follow the “after read” labeling proposed in Leblond
et al. (2017), in which we update our iterate counter, t, as each core ﬁnishes reading its copy of
the parameters (in the speciﬁc case of PROXASAGA, this includes both ˆxt and ˆαt). This means
that ˆxt is the (t + 1)th fully completed read. One key advantage of this approach compared to the
classical choice of Niu et al. (2011) – where t is increasing after each successful update – is that
it guarantees both that the it are uniformly distributed and that it and ˆxt are independent. This
property is not veriﬁed when using the “after write” labeling of Niu et al. (2011), although it is still
implicitly assumed in the papers using this approach, see Leblond et al. (2017, Section 3.2) for a
discussion of issues related to the different labeling schemes.

Generalization to composite optimization. Although the perturbed iterate framework was de-
signed for gradient-based updates, we can extend it to proximal methods by remarking that in the
sequential setting, proximal stochastic gradient descent and its variants can be characterized by the
following similar update rule:

xt+1 = xt − γg(xt, vit, it) , with g(x, v, i) := 1
γ

(cid:0)x − proxγϕi(x − γv)(cid:1) ,

where as before v veriﬁes the unbiasedness condition E v = ∇f (x). The Proximal Sparse SAGA
iteration can be easily written within this template by using ϕi and vi as deﬁned in §2. Using this
deﬁnition of g, we can deﬁne PROXASAGA virtual iterates as:

xt+1 := xt − γg(ˆxt, ˆvt
it

, it) , with ˆvt
it

= ∇fit (ˆxt) − ˆαt
it

+ Dit αt

,

(3)

(4)

where as in the sequential case, the memory terms are updated as ˆαt
it
analysis of PROXASAGA will be based on this deﬁnition of the virtual iterate xt+1.

= ∇fit(ˆxt). Our theoretical

3.2 Properties and assumptions

Now that we have introduced the “after read” labeling for proximal methods in Eq. (4), we can
leverage the framework of Leblond et al. (2017, Section 3.3) to derive essential properties for the
analysis of PROXASAGA. We describe below three useful properties arising from the deﬁnition
of Algorithm 1, and then state a central (but standard) assumption that the delays induced by the
asynchrony are uniformly bounded.

Independence: Due to the “after read” global ordering, ir is independent of ˆxt for all r ≥ t. We
enforce the independence for r = t by having the cores read all the shared parameters before their
iterations.
Unbiasedness: The term ˆvt
consequence of the independence between it and ˆxt.

it is an unbiased estimator of the gradient of f at ˆxt. This property is a

Atomicity: The shared parameter coordinate update of [x]b on Line 14 is atomic. This means that
there are no overwrites for a single coordinate even if several cores compete for the same resources.
Most modern processors have support for atomic operations with minimal overhead.

Bounded overlap assumption. We assume that there exists a uniform bound, τ , on the maximum
number of overlapping iterations. This means that every coordinate update from iteration t is suc-
cessfully written to memory before iteration t + τ + 1 starts. Our result will give us conditions on τ
to obtain linear speedups.

Bounding ˆxt − xt. The delay assumption of the previous paragraph allows to express the difference
between real and virtual iterate using the gradient mapping gu := g(ˆxu, ˆvu
iu
ˆxt−xt = γ (cid:80)t−1

u are p × p diagonal matrices with terms in {0, +1}. (5)

ugu , where Gt

, iu) as:

Gt

u=(t−τ )+

6

0 represents instances where both ˆxu and xu have received the corresponding updates. +1, on
the contrary, represents instances where ˆxu has not yet received an update that is already in xu by
deﬁnition. This bound will prove essential to our analysis.

3.3 Analysis

In this section, we state our convergence and speedup results for PROXASAGA. The full details
of the analysis can be found in Appendix C. Following Niu et al. (2011), we introduce a sparsity
measure (generalized to the composite setting) that will appear in our results.
Deﬁnition 1. Let ∆ := maxB∈B |{i : Ti (cid:51) B}|/n. This is the normalized maximum number of
times that a block appears in the extended support. For example, if a block is present in all Ti, then
∆ = 1. If no two Ti share the same block, then ∆ = 1/n. We always have 1/n ≤ ∆ ≤ 1.
Theorem 2 (Convergence guarantee of PROXASAGA). Suppose τ ≤ 1
√
10
γ = a
in expectation at a geometric rate factor of at least: ρ(a) = 1
(1 − ρ)t ˜C0, where ˜C0 is a constant independent of t (≈ nκ

. For any step size
τ }, the inconsistent read iterates of Algorithm 1 converge
(cid:9), i.e. E(cid:107)ˆxt − x∗(cid:107)2 ≤

L with a ≤ a∗(τ ) := 1

a C0 with C0 as deﬁned in Theorem 1).

36 min{1, 6κ

5 min (cid:8) 1

n , a 1

∆

κ

√

This last result is similar to the original SAGA convergence result and our own Theorem 1, with both
an extra condition on τ and on the maximum allowable step size. In the best sparsity case, ∆ = 1/n
and we get the condition τ ≤
n/10. We now compare the geometric rate above to the one of Sparse
Proximal SAGA to derive the necessary conditions under which PROXASAGA is linearly faster.
Corollary 1 (Speedup). Suppose τ ≤ 1
. If κ ≥ n, then using the step size γ = 1/36L, PROXAS-
√
10
AGA converges geometrically with rate factor Ω( 1
κ ). If κ < n, then using the step size γ = 1/36nµ,
PROXASAGA converges geometrically with rate factor Ω( 1
n ). In both cases, the convergence rate
is the same as Sparse Proximal SAGA. Thus PROXASAGA is linearly faster than its sequential
counterpart up to a constant factor. Note that in both cases the step size does not depend on τ .

∆

Furthermore, if τ ≤ 6κ, we can use a universal step size of Θ(1/L) to get a similar rate for PROX-
ASAGA than Sparse Proximal SAGA, thus making it adaptive to local strong convexity since the
knowledge of κ is not required.

These speedup regimes are comparable with the best ones obtained in the smooth case, including Niu
et al. (2011); Reddi et al. (2015), even though unlike these papers, we support inconsistent reads
and nonsmooth objective functions. The one exception is Leblond et al. (2017), where the authors
prove that their algorithm, ASAGA, can obtain a linear speedup even without sparsity in the well-
conditioned regime. In contrast, PROXASAGA always requires some sparsity. Whether this property
for smooth objective functions could be extended to the composite case remains an open problem.

√

Relative to ASYSPCD, in the best case scenario (where the components of the gradient are uncorre-
√
lated, a somewhat unrealistic setting), ASYSPCD can get a near-linear speedup for τ as big as 4
p.
∆) is necessary for a linear speedup. This means in case ∆ ≤ 1/√
Our result states that τ = O(1/
p
our bound is better than the one obtained for ASYSPCD. Recalling that 1/n ≤ ∆ ≤ 1, it appears
that PROXASAGA is favored when n is bigger than
p whereas ASYSPCD may have a better bound
otherwise, though this comparison should be taken with a grain of salt given the assumptions we
had to make to arrive at comparable quantities. An extended comparison with the related work can
be found in Appendix D.

√

4 Experiments

In this section, we compare PROXASAGA with related methods on different datasets. Although
PROXASAGA can be applied more broadly, we focus on (cid:96)1 + (cid:96)2-regularized logistic regression, a
model of particular practical importance. The objective function takes the form

1
n

n
(cid:88)

i=1

log (cid:0)1 + exp(−bia

(cid:124)

i x)(cid:1) + λ1

2 (cid:107)x(cid:107)2

2 + λ2(cid:107)x(cid:107)1

,

(6)

where ai ∈ Rp and bi ∈ {−1, +1} are the data samples. Following Defazio et al. (2014), we set
λ1 = 1/n. The amount of (cid:96)1 regularization (λ2) is selected to give an approximate 1/10 nonzero

7

Table 1: Description of datasets.

Dataset

n

p

KDD 2010 (Yu et al., 2010)
KDD 2012 (Juan et al., 2016)
Criteo (Juan et al., 2016)

19,264,097
149,639,105
45,840,617

1,163,024
54,686,452
1,000,000

density
10−6
2 × 10−7
4 × 10−5

L

28.12
1.25
1.25

∆

0.15
0.85
0.89

Figure 1: Convergence for asynchronous stochastic methods for (cid:96)1 + (cid:96)2-regularized logistic
regression. Top: Suboptimality as a function of time for different asynchronous methods using 1
and 10 cores. Bottom: Running time speedup as function of the number of cores. PROXASAGA
achieves signiﬁcant speedups over its sequential version while being orders of magnitude faster than
competing methods. ASYSPCD achieves the highest speedups but it also the slowest overall method.

coefﬁcients. Implementation details are available in Appendix E. We chose the 3 datasets described
in Table 1

Results. We compare three parallel asynchronous methods on the aforementioned datasets: PROX-
ASAGA (this work),6 ASYSPCD, the asynchronous proximal coordinate descent method of Liu &
Wright (2015) and the (synchronous) FISTA algorithm (Beck & Teboulle, 2009), in which the gra-
dient computation is parallelized by splitting the dataset into equal batches. We aim to benchmark
these methods in the most realistic scenario possible; to this end we use the following step size:
1/2L for PROXASAGA, 1/Lc for ASYSPCD, where Lc is the coordinate-wise Lipschitz constant
of the gradient, while FISTA uses backtracking line-search. The results can be seen in Figure 1
(top) with both one (thus sequential) and ten processors. Two main observations can be made from
this ﬁgure. First, PROXASAGA is signiﬁcantly faster on these problems. Second, its asynchronous
version offers a signiﬁcant speedup over its sequential counterpart.

In Figure 1 (bottom) we present speedup with respect to the number of cores, where speedup is
computed as the time to achieve a suboptimality of 10−10 with one core divided by the time to
achieve the same suboptimality using several cores. While our theoretical speedups (with respect
to the number of iterations) are almost linear as our theory predicts (see Appendix F), we observe
a different story for our running time speedups. This can be attributed to memory access overhead,
which our model does not take into account. As predicted by our theoretical results, we observe

6A reference C++/Python implementation of is available at https://github.com/fabianp/ProxASAGA

8

a high correlation between the ∆ dataset sparsity measure and the empirical speedup: KDD 2010
(∆ = 0.15) achieves a 11x speedup, while in Criteo (∆ = 0.89) the speedup is never above 6x.

Note that although competitor methods exhibit similar or sometimes better speedups, they remain
orders of magnitude slower than PROXASAGA in running time for large sparse problems. In fact,
our method is between 5x and 80x times faster (in time to reach 10−10 suboptimality) than FISTA
and between 13x and 290x times faster than ASYSPCD (see Appendix F.3).

5 Conclusion and future work

In this work, we have described PROXASAGA, an asynchronous variance reduced algorithm with
support for composite objective functions. This method builds upon a novel sparse variant of the
(proximal) SAGA algorithm that takes advantage of sparsity in the individual gradients. We have
proven that this algorithm is linearly convergent under a condition on the step size and that it is
linearly faster than its sequential counterpart given a bound on the delay. Empirical benchmarks
show that PROXASAGA is orders of magnitude faster than existing state-of-the-art methods.

This work can be extended in several ways. First, we have focused on the SAGA method as the basic
iteration loop, but this approach can likely be extended to other proximal incremental schemes such
as SGD or ProxSVRG. Second, as mentioned in §3.3, it is an open question whether it is possible to
obtain convergence guarantees without any sparsity assumption, as was done for ASAGA.

Acknowledgements

The authors would like to thank our colleagues Damien Garreau, Robert Gower, Thomas Ker-
dreux, Geoffrey Negiar and Konstantin Mishchenko for their feedback on this manuscript, and Jean-
Baptiste Alayrac for support managing the computational resources.

This work was partially supported by a Google Research Award. FP acknowledges support from the
chaire ´Economie des nouvelles donn´ees with the data science joint research initiative with the fonds
AXA pour la recherche.

References

Bauschke, Heinz and Combettes, Patrick L. Convex analysis and monotone operator theory in

Hilbert spaces. Springer, 2011.

Beck, Amir and Teboulle, Marc. Gradient-based algorithms with applications to signal recovery.

Convex Optimization in Signal Processing and Communications, 2009.

Davis, Damek, Edmunds, Brent, and Udell, Madeleine. The sound of APALM clapping: faster
nonsmooth nonconvex optimization with stochastic asynchronous PALM. In Advances in Neural
Information Processing Systems 29, 2016.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural Infor-
mation Processing Systems, 2014.

Gu, Bin, Huo, Zhouyuan, and Huang, Heng. Asynchronous stochastic block coordinate descent

with variance reduction. arXiv preprint arXiv:1610.09447v3, 2016.

Hsieh, Cho-Jui, Yu, Hsiang-Fu, and Dhillon, Inderjit S. PASSCoDe: parallel asynchronous stochas-

tic dual coordinate descent. In ICML, 2015.

Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems, 2013.

Juan, Yuchin, Zhuang, Yong, Chin, Wei-Sheng, and Lin, Chih-Jen. Field-aware factorization ma-
chines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Sys-
tems. ACM, 2016.

9

Le Roux, Nicolas, Schmidt, Mark, and Bach, Francis R. A stochastic gradient method with an ex-
ponential convergence rate for ﬁnite training sets. In Advances in Neural Information Processing
Systems, 2012.

Leblond, R´emi, Pedregosa, Fabian, and Lacoste-Julien, Simon. ASAGA: asynchronous parallel
SAGA. Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS 2017), 2017.

Liu, Ji and Wright, Stephen J. Asynchronous stochastic coordinate descent: Parallelism and conver-

gence properties. SIAM Journal on Optimization, 2015.

Mania, Horia, Pan, Xinghao, Papailiopoulos, Dimitris, Recht, Benjamin, Ramchandran, Kannan,
and Jordan, Michael I. Perturbed iterate analysis for asynchronous stochastic optimization. SIAM
Journal on Optimization, 2017.

Meng, Qi, Chen, Wei, Yu, Jingcheng, Wang, Taifeng, Ma, Zhi-Ming, and Liu, Tie-Yan. Asyn-
chronous stochastic proximal optimization algorithms with variance reduction. In AAAI, 2017.

Nesterov, Yurii. Introductory lectures on convex optimization. Springer Science & Business Media,

Nesterov, Yurii. Gradient methods for minimizing composite functions. Mathematical Program-

2004.

ming, 2013.

Niu, Feng, Recht, Benjamin, Re, Christopher, and Wright, Stephen. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Sys-
tems, 2011.

Peng, Zhimin, Xu, Yangyang, Yan, Ming, and Yin, Wotao. ARock: an algorithmic framework for

asynchronous parallel coordinate updates. SIAM Journal on Scientiﬁc Computing, 2016.

Reddi, Sashank J, Hefny, Ahmed, Sra, Suvrit, Poczos, Barnabas, and Smola, Alexander J. On
variance reduction in stochastic gradient descent and its asynchronous variants. In Advances in
Neural Information Processing Systems, 2015.

Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Minimizing ﬁnite sums with the stochastic

average gradient. Mathematical Programming, 2016.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research, 2013.

Shalev-Shwartz, Shai et al.
arXiv:1211.2717, 2012.

Proximal stochastic dual coordinate ascent.

arXiv preprint

Xiao, Lin and Zhang, Tong. A proximal stochastic gradient method with progressive variance re-

duction. SIAM Journal on Optimization, 2014.

You, Yang, Lian, Xiangru, Liu, Ji, Yu, Hsiang-Fu, Dhillon, Inderjit S, Demmel, James, and Hsieh,
Cho-Jui. Asynchronous parallel greedy coordinate descent. In Advances In Neural Information
Processing Systems, 2016.

Yu, Hsiang-Fu, Lo, Hung-Yi, Hsieh, Hsun-Ping, Lou, Jing-Kai, McKenzie, Todd G, Chou, Jung-
Wei, Chung, Po-Han, Ho, Chia-Hua, Chang, Chun-Fu, Wei, Yin-Hsuan, et al. Feature engineering
and classiﬁer ensemble for KDD cup 2010. In KDD Cup, 2010.

Zhao, Tuo, Yu, Mo, Wang, Yiming, Arora, Raman, and Liu, Han. Accelerated mini-batch random-
ized block coordinate descent method. In Advances in neural information processing systems,
2014.

10

Breaking the Nonsmooth Barrier: A Scalable Parallel
Method for Composite Optimization

Supplementary material

Notations. Throughout the supplementary material we use the following extra notation. We denote
by (cid:104)·, ·(cid:105)(i) (resp. (cid:107) · (cid:107)(i)) the scalar product (resp. norm) restricted to blocks in Ti, i.e., (cid:104)x, y(cid:105)(i) :=
(cid:104)[x]B, [y]B(cid:105) and (cid:107)x(cid:107)(i) := (cid:112)(cid:104)x, x(cid:105)(i). We will also use the following deﬁnitions: ϕ :=
(cid:80)
(cid:80)

B∈Ti
B∈B dBhB(x) and D is the diagonal matrix deﬁned block-wise as [D]B,B = dBI |B|.

The Bregman divergence associated with a convex function f for points x, y in its domain is
deﬁned as:

Bf (x, y) := f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105) .

Note that this is always positive due to the convexity of f .

Appendix A Basic properties

Lemma 1. For any µ-strongly convex function f we have the following inequality:

(cid:104)∇f (y) − ∇f (x), y − x(cid:105) ≥

(cid:107)y − x(cid:107)2 + Bf (x, y) .

µ
2

Proof. By strong convexity, f veriﬁes the inequality:

f (y) ≤ f (x) + (cid:104)∇f (y), y − x(cid:105) −

(cid:107)y − x(cid:107)2 ,

µ
2

for any x, y in the domain (see e.g. (Nesterov, 2004)). We then have the equivalences:

f (x) ≤ f (y) + (cid:104)∇f (x), x − y(cid:105) −

(cid:107)x − y(cid:107)2

µ
2

⇐⇒

(cid:107)x − y(cid:107)2 + f (x) − f (y) ≤ (cid:104)∇f (x), x − y(cid:105)

⇐⇒

(cid:107)x − y(cid:107)2 + f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105)
(cid:125)

(cid:124)

(cid:123)(cid:122)
Bf (x,y)

µ
2
µ
2

≤ (cid:104)∇f (x) − ∇f (y), x − y(cid:105) ,

(10)

where in the last line we have subtracted (cid:104)∇f (y), x − y(cid:105) from both sides of the inequality.

Lemma 2. Let the fi be L-smooth and convex functions. Then it is veriﬁed that:

1
n

n
(cid:88)

i=1

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ 2LBf (x, y) .

Proof. Since each fi is L-smooth, it is veriﬁed (see e.g. Nesterov (2004, Theorem 2.1.5)) that

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ 2L(cid:0)fi(x) − fi(y) − (cid:104)∇fi(y), x − y(cid:105)(cid:1) .

The result is obtained by averaging over i.

(7)

(8)

(9)

(11)

(12)

Lemma 3 (Characterization of the proximal operator). Let h be convex lower semicontinuous. Then
we have the following characterization of the proximal operator:

z = proxγh(x) ⇐⇒

(x − z) ∈ ∂h(z) .

(13)

1
γ

11

Proof. This is a direct consequence of the ﬁrst order optimality conditions on the deﬁnition of
proximal operator, see e.g. (Beck & Teboulle, 2009; Nesterov, 2013).

Lemma 4 (Firm non-expansiveness). Let x, ˜x be two arbitrary elements in the domain of ϕi and
z, ˜z be deﬁned as z := proxϕi

(˜x). Then it is veriﬁed that:

(x), ˜z := proxϕi

(cid:104)z − ˜z, x − ˜x(cid:105)(i) ≥ (cid:107)z − ˜z(cid:107)2

(i) .

Proof. By the block-separability of ϕi, the proximal operator is the concatenation of the proximal
operators of the blocks. In other words, for any block B ∈ Ti we have:

[z]B = proxγϕB ([x]B) ,

[˜z]B = proxγϕB ([˜x]B) ,

where ϕB is the restriction of ϕi to B. By ﬁrm non-expansiveness of the proximal operator (see
e.g. Bauschke & Combettes (2011, Proposition 4.2)) we have that:

(cid:104)[z]B − [˜z]B, [x]B − [˜x]B(cid:105) ≥ (cid:107)[z]B − [˜z]B(cid:107)2 .

Summing over the blocks in Ti yields the desired result.

(14)

(15)

12

Appendix B Sparse Proximal SAGA

This Appendix contains all proofs for Section 2. The main result of this section is Theorem 1, whose
proof is structured as follows:

• We start by proving four auxiliary results that will be used later on in the proofs of both
synchronous and asynchronous variants. The ﬁrst is the unbiasedness of key quantities used
in the algorithm. The second is a characterization of the solutions of (OPT) in terms of f
and ϕ (deﬁned below) in Lemma 6. The third is a key inequality in Lemma 7 that relates
the gradient mapping to other terms that arise in the optimization. The fourth is an upper
bound on the variance terms of the gradient estimator, relating it to the Bregman divergence
of f and the past gradient estimator terms.

• In Lemma 9, we deﬁne an upper bound on the iterates (cid:107)xt − x∗(cid:107)2, called a Lyapunov
function, and prove an inequality that relates this Lyapunov function value at the current
iterate with its value at the previous iterate.

• Finally, in the proof of Theorem 1 we use the previous inequality in terms of the Lyapunov

function to prove a geometric convergence of the iterates.

We start by proving the following unbiasedness result, mentioned in §2.

Lemma 5. Let Di and ϕi be deﬁned as in §2. Then it is veriﬁed that EDi = I p and E ϕi = h.

Proof. Let B ∈ B an arbitrary block. We have the following sequence of equalities:

where the last equality comes from the deﬁnition of nB. EDi = I p then follows from the arbitrari-
ness of B.

Similarly, for ϕi we have:

E[Di]B,B =

[Di]B,B =

dB1{B ∈ Ti}I |B|

1
n

n
(cid:88)

i=1

(cid:33)

1{B ∈ Ti}I |B|

1
n

1
n
(cid:32)

n
(cid:88)

i=1
n
(cid:88)

i=1

1
nB

n
nB

n
(cid:88)

i=1

=

=

1{B ∈ Ti}

I |B| = I |B| ,

Eϕi([x]B) =

dB1{B ∈ Ti}hB([x]B)

1
n

1
n
(cid:32)

n
(cid:88)

i=1
n
(cid:88)

i=1

1
nB

n
nB

n
(cid:88)

i=1

=

=

1{B ∈ Ti}hB([x]B)

(cid:33)

1{B ∈ Ti}

hB([x]B) = hB([x]B) ,

Finally, the result E ϕi = h comes from adding over all blocks.

Lemma 6. x∗ is a solution to (OPT) if and only if the following condition is veriﬁed:

x∗ = proxγϕ

(cid:0)x∗ − γD∇f (x∗)(cid:1) .

Proof. By the ﬁrst order optimality conditions, the solutions to (OPT) are characterized by the sub-
differential inclusion −∇f (x∗) ∈ ∂h(x∗). We can then write the following sequence of equiva-

13

(16)

(17)

(18)

(19)

(20)

(21)

(22)

lences:

−∇f (x∗) ∈ ∂h(x∗) ⇐⇒ −D∇f (x∗) ∈ D∂h(x∗)

(multiplying by D, equivalence since diagonals are nonzero)

⇐⇒ −D∇f (x∗) ∈ ∂ϕ(x∗)
(by deﬁnition of ϕ)

⇐⇒

(x∗ − γD∇f (x∗) − x∗) ∈ ∂ϕ(x∗)

1
γ

(adding and subtracting x∗)

⇐⇒ x∗ = proxγϕ(x∗ − γD∇f (x∗)) .

(by Lemma 3)

(23)

Since all steps are equivalences, we have the desired result.

The following lemma will be key in the proof of convergence for both the sequential and the parallel
versions of the algorithm. With this result, we will be able to bound the product between the gradient
mapping and the iterate suboptimality by:

• First, the negative norm of the gradient mapping, which will be key in the parallel setting

to cancel out the terms arising from the asynchrony.

• Second, variance terms in (cid:107)vi−Di∇f (x∗)(cid:107)2 that we will be able to bound by the Bregman

divergence using Lemma 2.

• Third and last, a product with terms in (cid:104)vi − Di∇f (x∗), x − x∗(cid:105), which taken in expec-
tation gives (cid:104)∇f (x) − ∇f (x∗), x − x∗(cid:105) and will allow us to apply Lemma 1 to obtain the
contraction terms needed to obtain a geometric rate of convergence.

Lemma 7 (Gradient mapping inequality). Let x be an arbitrary vector, x∗ a solution to (OPT), vi
as deﬁned in (SPS) and g = g(x, vi, i) the gradient mapping deﬁned in (3). Then the following
inequality is veriﬁed for any β > 0:

(cid:104)g, x − x∗(cid:105) ≥ −

(β − 2)(cid:107)g(cid:107)2 −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + (cid:104)vi − Di∇f (x∗), x − x∗(cid:105) .

(24)

γ
2

γ
2β

Proof. By ﬁrm non-expansiveness of the proximal operator (Lemma 4) applied to z = proxγϕi(x−
γvi) and ˜z = proxγϕi(x∗ − γD∇f (x∗)) we have:

(cid:107)z − ˜z(cid:107)2

(i) − (cid:104)z − ˜z, x − γvi − x∗ + γD∇f (x∗)(cid:105)(i) ≤ 0 .

(25)

By the (SPS) iteration we have x+ = z and by Lemma 3 we have that [z]Ti = [x∗]Ti, hence the
above can be rewritten as

(cid:107)x+ − x∗(cid:107)2

(i) − (cid:104)x+ − x∗, x − γvi − x∗ + γD∇f (x∗)(cid:105)(i) ≤ 0 .

(26)

14

We can now write the following sequence of inequalities

(cid:104)γg, x − x∗(cid:105) = (cid:104)x − x+, x − x∗(cid:105)(i)

(by deﬁnition and sparsity of g)

(cid:16)

≥

1 −

(cid:17)

β
2

(cid:16)

≥

1 −

(cid:16)

=

1 −

(cid:17)

(cid:17)

β
2

β
2

= (cid:104)x − x+ + x∗ − x∗, x − x∗(cid:105)(i)
= (cid:107)x − x∗(cid:107)2
≥ (cid:107)x − x∗(cid:107)2

(i) − (cid:104)x+ − x∗, x − x∗(cid:105)(i)
(i) − (cid:104)x+ − x∗, 2x − γvi − 2x∗ + γD∇f (x∗)(cid:105)(i) + (cid:107)x+ − x∗(cid:107)2
(i)

(27)

(adding Eq. (26))

= (cid:107)x − x+(cid:107)2
= (cid:107)x − x+(cid:107)2

(i) + (cid:104)x+ − x∗, γvi − γD∇f (x∗)(cid:105)(i)
(i) + (cid:104)x − x∗, γvi − γD∇f (x∗)(cid:105)(i) − (cid:104)x − x+, γvi − γD∇f (x∗)(cid:105)(i)

(completing the square)

(adding and substracting x)

(cid:107)x − x+(cid:107)2

(i) −

(cid:107)vi − D∇f (x∗)(cid:107)2

(i) + γ(cid:104)vi − D∇f (x∗), x − x∗(cid:105)(i)

(Young’s inequality 2(cid:104)a, b(cid:105) ≤

+ β(cid:107)b(cid:107)2, valid for arbitrary β > 0)

γ2
2β

γ2
2β

(cid:107)a(cid:107)2
β

(cid:107)x − x+(cid:107)2

(i) −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + γ(cid:104)vi − Di∇f (x∗), x − x∗(cid:105)

(by deﬁnition of Di and using the fact that vi is Ti-sparse)

(cid:107)γg(cid:107)2 −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + γ(cid:104)vi − D∇f (x∗), x − x∗(cid:105) ,

(28)

γ2
2β

where in the last inequality we have used the fact that g is Ti-sparse. Finally, dividing by γ both
sides yields the desired result.

Lemma 8 (Upper bound on the gradient estimator variance). For arbitrary vectors x, (αi)n
vi as deﬁned in (SPS) we have:

i=0, and

E(cid:107)vi − Di∇f (x∗)(cid:107)2 ≤ 4LBf (x, x∗) + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(29)

Proof. We will now bound the variance terms. For this we have:

E(cid:107)vi − D∇f (x∗)(cid:107)2

(i) = E(cid:107)∇fi(x) − ∇fi(x∗) + ∇fi(x∗) − αi + Diα − D∇f (x∗)(cid:107)2
(i)

≤ 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)∇fi(x∗) − αi − (D∇f (x∗) − Dα)(cid:107)2
(i)

(by inequality (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2)
= 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)∇fi(x∗) − αi(cid:107)2

− 4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)(i) + 2E(cid:107)D∇f (x∗) − Dα(cid:107)2
(developing the square)

(i) .

(30)

We will now simplify the last two terms in the above expression. For the ﬁrst of the two last terms
we have:

− 4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)(i) = −4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)

(31)

(support of ﬁrst term)

= −4(cid:104)∇f (x∗) − α, D∇f (x∗) − Dα(cid:105)
= −4(cid:107)∇f (x∗) − α(cid:107)2

D .

Similarly, for the last term we have:

2E(cid:107)D∇f (x∗) − Dα(cid:107)2

(i) = 2E(cid:104)Di∇f (x∗) − Diα, D∇f (x∗) − Dα(cid:105)

= 2(cid:104)∇f (x∗) − α, D∇f (x∗) − Dα(cid:105)

(32)

(33)

(using Lemma 5)
D .

= 2(cid:107)∇f (x∗) − α(cid:107)2

15

and so the addition of these terms is negative and can be dropped. In all, for the variance terms we
have

E(cid:107)vi − D∇f (x∗)(cid:107)2

(i) ≤ 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2

≤ 4LBf (x, x∗) + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(by Lemma 2)

(34)

c
n

n
(cid:88)

i=1

(cid:16)

We now deﬁne an upper bound on the quantity that we would like to bound, often called a Lyapunov
function, and establish a recursive inequality on this Lyapunov function.

Lemma 9 (Lyapunov inequality). Let L be the following c-parametrized function:

L(x, α) := (cid:107)x − x∗(cid:107)2 +

(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(35)

Let x+ and α+ be obtained from the Sparse Proximal SAGA updates (SPS). Then we have:

EL(x+, α+) − L(x, α) ≤ − γµ(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:17)

c
n

(cid:16)

+

2γ2 −

(cid:17)

c
n

E(cid:107)αi − ∇fi(x)(cid:107)2 .

Proof. For the ﬁrst term of L we have:

(cid:107)x+ − x∗(cid:107)2 = (cid:107)x − γg − x∗(cid:107)2

(g := g(x, vi, i))

= (cid:107)x − x∗(cid:107)2 − 2γ(cid:104)g, x − x∗(cid:105) + (cid:107)γg(cid:107)2
≤ (cid:107)x − x∗(cid:107)2 + γ2(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γ(cid:104)vi − Di∇f (x∗), x − x∗(cid:105)

(by Lemma 7 with β = 1)

Since vi is an unbiased estimator of the gradient and EDi = I p, taking expectations we have:

E(cid:107)x+ − x∗(cid:107)2 ≤ (cid:107)x − x∗(cid:107)2 + γ2E(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γ(cid:104)∇f (x) − ∇f (x∗), x − x∗(cid:105)

≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + γ2E(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γBf (x, x∗) .

(38)

(by Lemma 1)

By using the variance terms bound (Lemma 8) in the previous equation we have:

E(cid:107)x+ − x∗(cid:107)2 ≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + (4Lγ2 − 2γ)Bf (x, x∗)
+ 2γ2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

We will now bound the second term of the Lyapunov function. We have:

1
n

n
(cid:88)

i=1

(cid:107)α+

i − ∇fi(x∗)(cid:107)2 =

1 −

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2

(cid:18)

(cid:19)

1
n

1
n

(by deﬁnition of α+)

(cid:18)

(cid:19)

≤

1 −

1
n

2
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

LBf (x, x∗) .

(by Lemma 2)

(36)

(37)

(39)

(40)

(41)

16

Combining Eq. (39) and (40) we have:

EL(x+, α+) ≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + (4Lγ2 − 2γ)Bf (x, x∗) + 2γ2E(cid:107)αi − ∇fi(x∗)(cid:107)2

(cid:20)(cid:18)

+ c

1 −

(cid:19)

1
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

(cid:21)
2LBf (x, x∗)

= (1 − γµ)(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:16)

1
n
c
n

(cid:17)

(cid:16)

+

2γ2 −

(cid:17)

c
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 + cE(cid:107)αi − ∇fi(x∗)(cid:107)2

= L(x, α) − γµ(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:16)

(cid:17)

c
n

c
n
Finally, subtracting L(x, α) from both sides yields the desired result.

E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

2γ2 −

+

(cid:16)

(cid:17)

(42)

Theorem 1. Let γ = a
converges geometrically in expectation with a rate factor of at least ρ = 1
xt obtained after t updates and x∗ the solution to (OPT), we have the bound:
i=1 (cid:107)α0

5L for any a ≤ 1 and f be µ-strongly convex. Then Sparse Proximal SAGA
κ }. That is, for

with C0 := (cid:107)x0 − x∗(cid:107)2 + 1
5L2

E(cid:107)xt − x∗(cid:107)2 ≤ (1 − ρ)tC0 ,

i − ∇fi(x∗)(cid:107)2

5 min{ 1

n , a 1

(cid:80)n

.

Proof. Let H := 1
n

(cid:80)

ELt+1 − (1 − ρ)Lt ≤ ρLt − γµ(cid:107)xt − x∗(cid:107)2 +

i (cid:107)αi − ∇fi(x∗)(cid:107)2. By the Lyapunov inequality from Lemma 9, we have:
(cid:17)
c
4Lγ2 − 2γ + 2L
n

c
n

(cid:17)

(cid:16)

(cid:16)

H

= (ρ − γµ) (cid:107)xt − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (xt, x∗) +

(cid:17)

c
n

(cid:17)

c
n

Bf (xt, x∗) +
(cid:20)
2γ2 + c

ρ −

(cid:18)

2γ2 −
(cid:19)(cid:21)

H

1
n

(cid:18)

(cid:19)

H

2c
3n

≤ (ρ − γµ) (cid:107)xt − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (xt, x∗) +

2γ2 −

(cid:16)

(cid:16)

(by deﬁnition of Lt)

(choosing ρ ≤

1
3n

)

= (ρ − γµ) (cid:107)xt − x∗(cid:107)2 + (cid:0)10Lγ2 − 2γ(cid:1) Bf (xt, x∗)

(choosing

= 3γ2)

c
n

(cid:16)

(cid:17)

aµ
5L

≤

ρ −

(cid:107)xt − x∗(cid:107)2
µ
a
L
5
And so we have the bound:

(for ρ ≤

≤ 0 .

·

)

(for all γ =

, a ≤ 1)

a
5L

(cid:18)

ELt+1 ≤

1 − min

(cid:110) 1
3n

,

a
5

·

1
κ

(cid:111)(cid:19)

(cid:18)

Lt ≤

min

, a ·

(cid:110) 1
n

(cid:111)(cid:19)

1
κ

Lt ,

1 −

1
5
3n ≤ 1

5n merely for clarity of exposition.

where in the last inequality we have used the trivial bound 1
Chaining expectations from t to 0 we have:
(cid:111)(cid:19)t+1

(cid:18)

ELt+1 ≤

1 −

min

, a ·

1
5

1
5

1
5

(cid:110) 1
n

(cid:110) 1
n

(cid:110) 1
n

L0
(cid:111)(cid:19)t+1 (cid:32)

(cid:111)(cid:19)t+1 (cid:32)

1
κ

1
κ

1
κ

(cid:18)

(cid:18)

(since a ≤ 1 and 3/5 ≤ 1) .

=

1 −

min

, a ·

(cid:107)x0 − x∗(cid:107)2 +

(cid:107)α0

i − ∇fi(x∗)(cid:107)2

≤

1 −

min

, a ·

(cid:107)x0 − x∗(cid:107)2 +

(cid:107)α0

i − ∇fi(x∗)(cid:107)2

(45)

3a2
52L2

1
5L2

n
(cid:88)

i=1
n
(cid:88)

i=1

The fact that Lt is a majorizer of (cid:107)xt − x∗(cid:107)2 completes the proof.

(43)

(44)

(cid:33)

(cid:33)

17

Appendix C ProxASAGA

In this Appendix we provide the proofs for results from Section 3, that is Theorem 2 (the convergence
theorem for PROXASAGA) and Corollary 1 (its speedup result).

Notation. Through this section, we use the following shorthand for the gradient mapping: gt :=
g(ˆxt, ˆvt
it

, it).

Appendix C.1 Proof outline.

As in the smooth case (h = 0), we start by using the deﬁnition of xt+1 in Eq. (4) to relate the
distance to the optimum in terms of its previous iterates:

(cid:107)xt+1 − x∗(cid:107)2 =(cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)ˆxt − xt, gt(cid:105) + γ2(cid:107)gt(cid:107)2 − 2γ(cid:104)ˆxt − x∗, gt(cid:105) .

(46)

However, in this case gt is not a gradient estimator but a gradient mapping, so we cannot continue
as is customary – by using the unbiasedness of the gradient in the (cid:104)ˆxt − x∗, gt(cid:105) term together with
the strong convexity of f (see Leblond et al. (2017, Section 3.5)).

To circumvent this difﬁculty, we derive a tailored inequality for the gradient mapping (Lemma 7
in Appendix B), which in turn allows us to use the classical unbiasedness and strong convexity
arguments to get the following inequality:

at+1 ≤ (1 −

γµ
2

)at + γ2E(cid:107)gt(cid:107)2 − 2γEBf (ˆxt, x∗) + γµE(cid:107)ˆxt − x(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105)
(cid:125)
(cid:124)

(cid:123)(cid:122)
additional asynchrony terms

(47)

+γ2(β − 2)E(cid:107)gt(cid:107)2 +
(cid:124)

γ2
β
(cid:123)(cid:122)
additional proximal and variance terms

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2
(cid:125)

,

where at := E(cid:107)xt − x∗(cid:107)2. Note that since f is strongly convex, Bf (ˆxt, x∗) ≥ µ
In the smooth setting, one ﬁrst expresses the additional asynchrony terms as linear combinations
of past gradient variance terms (E(cid:107)gu(cid:107)2)0≤u≤t. Then one crucially uses the negative Bregman
divergence term to control the variance terms. However, in our current setting, we cannot relate the
norm of the gradient mapping E(cid:107)gt(cid:107)2 to the Bregman divergence (from which h is absent). Instead,
we use the negative term γ2(β − 1)E(cid:107)gt(cid:107)2 to control all the (E(cid:107)gu(cid:107)2)0≤u≤t terms that arise from
asynchrony.

2 (cid:107)ˆxt − x∗(cid:107)2.

The rest of the proof consists in:
i) expressing the additional asynchrony terms as linear combinations of (E(cid:107)gu(cid:107)2)0≤u≤t, follow-
ing Leblond et al. (2017, Lemma 1);
ii) expressing the last variance term, (cid:107)ˆvt
it
divergences (Lemma 8 in Appendix B and Lemma 2 from Leblond et al. (2017));
iii) deﬁning a Lyapunov function, Lt := (cid:80)t
contraction given conditions on the maximum step size and delay.

u=0(1 − ρ)t−uau, and proving that it is bounded by a

− Dit∇f (x∗)(cid:107)2, as a linear combination of past Bregman

Appendix C.2 Detailed proof

Theorem 2 (Convergence guarantee and rate of PROXASAGA). Suppose τ ≤ 1
√
10
36 min{1, 6κ
size γ = a
expectation at a geometric rate factor of at least: ρ(a) = 1
(1 − ρ)t ˜C0, where ˜C0 is a constant independent of t (≈ nκ

. For any step
τ }, the inconsistent read iterates of Algorithm 1 converge in
(cid:9), i.e. E(cid:107)ˆxt − x∗(cid:107)2 ≤
5 min (cid:8) 1

a C0 with C0 as deﬁned in Theorem 1).

L with a ≤ 1

n , a 1

∆

κ

Proof. In order to get an initial recursive inequality, we ﬁrst unroll the (virtual) update:
(cid:107)xt+1 − x∗(cid:107)2 = (cid:107)xt − γgt − x∗(cid:107)2 = (cid:107)xt − x∗(cid:107)2 + (cid:107)γgt(cid:107)2 − 2γ(cid:104)gt, xt − x∗(cid:105)

= (cid:107)xt − x∗(cid:107)2 + (cid:107)γgt(cid:107)2 − 2γ(cid:104)gt, ˆxt − x∗(cid:105) + 2γ(cid:104)gt, ˆxt − xt(cid:105) ,

(48)

18

and then apply Lemma 7 with x = ˆxt and v = ˆvt
(cid:104)·(cid:105)(i) = (cid:104)·(cid:105)(it).

it. Note that in this case we have g = gt and

(cid:107)xt+1 − x∗(cid:107)2 ≤ (cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)gt, ˆxt − xt(cid:105) + γ2(cid:107)gt(cid:107)2 + γ2(β − 2)(cid:107)gt(cid:107)2

+

γ2
β

(cid:107)ˆvt
it

− D∇f (x∗)(cid:107)2

(it) − 2γ(cid:104)ˆvt
it

− D∇f (x∗), ˆxt − x∗(cid:105)(it)

= (cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)(cid:107)gt(cid:107)2

− Dit∇f (x∗)(cid:107)2 − 2γ(cid:104)ˆvt
it

− Dit∇f (x∗), ˆxt − x∗(cid:105).

(49)

+

(cid:107)ˆvt
it

γ2
β
(as [ˆvt
it

]Tit

= ˆvt

it)

We now use the property that it is independent of ˆxt (which we enforce by reading ˆxt before picking
it, see Section 3), together with the unbiasedness of the gradient update ˆvt
= ∇f (ˆxt)) and
the deﬁnition of D to simplify the following expression as follows:

it (Eˆvt
it

E(cid:104)ˆvt
it

− Dit∇f (x∗), ˆxt − x∗(cid:105) = (cid:104)∇f (ˆxt) − ∇f (x∗), ˆxt − x∗(cid:105)
µ
(cid:107)ˆxt − x∗(cid:107)2 + Bf (ˆxt, x∗) ,
2

≥

where the last inequality comes from Lemma 1. Taking conditional expectations on (49) we get:

E(cid:107)xt+1 − x∗(cid:107)2 ≤ (cid:107)xt − x∗(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)E(cid:107)gt(cid:107)2

+

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2 − γµ(cid:107)ˆxt − x∗(cid:107)2 − 2γBf (ˆxt, x∗)

≤ (1 −

)(cid:107)xt − x∗(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)E(cid:107)gt(cid:107)2

+

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2 + γµ(cid:107)ˆxt − xt(cid:107)2 − 2γBf (ˆxt, x∗)

(using (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2 on (cid:107)xt − ˆxt + ˆxt − x∗(cid:107)2)

γ2
β
γµ
2
γ2
β

γµ
2

≤ (1 −

)(cid:107)xt − x∗(cid:107)2 + γ2(β − 1)E(cid:107)gt(cid:107)2 + γµ(cid:107)ˆxt − xt(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105)
2γ2
β

− ∇fit (x∗)(cid:107)2 .

Bf (ˆxt, x∗) +

4γ2L
β

E(cid:107) ˆαt
it

(52)

− 2γBf (ˆxt, x∗) +

(using Lemma 8 on the variance terms)

Since we also have:

ˆxt − xt = γ

Gt

ug(ˆxu, ˆαu, iu),

t−1
(cid:88)

u=(t−τ )+

the effect of asynchrony for the perturbed iterate updates was already derived in a very similar setup
in Leblond et al. (2017). We re-use the following bounds from their Appendix C.4:7

E(cid:107)ˆxt − xt(cid:107)2 ≤ γ2(1 +

∆τ )

E(cid:107)gu(cid:107)2 ,

√

t−1
(cid:88)

u=(t−τ )+

Leblond et al. (2017, Eq. (48))

E(cid:104)gt, ˆxt − xt(cid:105) ≤

E(cid:107)gu(cid:107)2 +

E(cid:107)gt(cid:107)2 . Leblond et al. (2017, Eq. (46)).

√

γ

∆

2

t−1
(cid:88)

u=(t−τ )+

√

γ

∆τ
2

7The appearance of the sparsity constant ∆ is coming from the crucial property that E(cid:107)x(cid:107)2
∀x ∈ Rp (see Eq. (39) in Leblond et al. (2017), where they use the notation (cid:107) · (cid:107)i for our (cid:107) · (cid:107)(i)).

(i) ≤ ∆(cid:107)x(cid:107)2

19

(50)

(51)

(53)

(54)

(55)

Because the updates on α are the same for PROXASAGA as for ASAGA, we can re-use the same
argument arising in the proof of Leblond et al. (2017, Lemma 2) to get the following bound on
E(cid:107) ˆαt
it

− ∇fit(x∗)(cid:107)2:

E(cid:107) ˆαt
it

− ∇fit(x∗)(cid:107)2 ≤

(1 −

)(t−2τ −u−1)+EBf (ˆxu, x∗)

+2L(1 −

)(t−τ )+ ˜e0 ,

(56)

1
n

2L
n

t−1
(cid:88)

u=1
(cid:124)

1
n

(cid:123)(cid:122)
Henceforth denoted Ht

(cid:125)

E(cid:107)α0

i − f (cid:48)

where ˜e0 := 1
i (x∗)(cid:107)2. This bound is obtained by analyzing which gradient could be
2L
the source of αit in the past (taking in consideration the inconsistent writes), and then applying
Lemma 2 on the E(cid:107)∇f (ˆxu) − ∇f (x∗)(cid:107)2 terms, explaining the presence of Bf (ˆxu, x∗) terms.8
The inequality (56) corresponds to Eq. (56) and (57) in Leblond et al. (2017).

By taking the full expectation of (52) and plugging the above inequalities back, we obtain an in-
equality similar to Leblond et al. (2017, Master inequality (28)) which describes how the error terms
at := E(cid:107)xt − x∗(cid:107)2 of the virtual iterates are related:

at+1 ≤(1 −

)at +

(1 −

)(t−τ )+ ˜e0

γµ
2
+ γ2 (cid:104)

4γ2L
β
√

1
n

β − 1 +

∆τ

(cid:105)

E(cid:107)gt(cid:107)2 +

√

(cid:104)

γ2

∆ + γ3µ(1 +

√

(cid:105)
∆τ )

t
(cid:88)

E(cid:107)gu(cid:107)2

(57)

u=(t−τ )+

− 2γEBf (ˆxt, x∗) +

EBf (ˆxt, x∗) +

4γ2L
β

4γ2L
βn

Ht .

We now have a promising inequality with a contractive term and several quantities that we need to
bound. In order to achieve our ﬁnal result, we introduce the same Lyapunov function as in Leblond
et al. (2017):

Lt :=

(1 − ρ)t−uau ,

t
(cid:88)

u=0

where ρ is a target rate factor for which we will provide a value later on. Proving that this Lyapunov
function is bounded by a contraction will ﬁnish our proof. We have:

Lt+1 =

(1 − ρ)t+1−uau = (1 − ρ)t+1a0 +

(1 − ρ)t+1−uau

t+1
(cid:88)

u=0

= (1 − ρ)t+1a0 +

(1 − ρ)t−uau+1 .

(58)

t+1
(cid:88)

u=1
t
(cid:88)

u=0

We now plug our new bound on at+1, (57):
(1 − ρ)t−u(cid:104)

Lt+1 ≤ (1 − ρ)t+1a0 +

t
(cid:88)

u=0

)(u−τ )+ ˜e0

(1 −

)au +

γµ
2
+ γ2(cid:0)β − 1 +
√

+ (cid:0)γ2

(1 −

4γ2L
1
β
n
∆τ (cid:1)E(cid:107)gu(cid:107)2
√

√

∆ + γ3µ(1 +

∆τ )(cid:1)

u
(cid:88)

E(cid:107)gv(cid:107)2

v=(u−τ )+

− 2γEBf (ˆxu, x∗) +

EBf (ˆxu, x∗) +

4γ2L
β

4γ2L
βn

(cid:105)

Hu

.

(59)

After regrouping similar terms, we get:

Lt+1 ≤ (1 − ρ)t+1(a0 + A˜e0) + (1 −

)Lt +

st
u

E(cid:107)gu(cid:107)2 +

rt
u

EBf (ˆxu, x∗) .

(60)

γµ
2

t
(cid:88)

u=0

t
(cid:88)

u=1

8Note that Leblond et al. (2017) analyzed the unconstrained scenario, and so Bf (ˆxu, x∗) is replaced by the

simpler f (ˆxu) − f (x∗) in their bound.

20

Now, provided that we can prove that under certain conditions the st
u terms are all negative
(and that the A term is not too big), we can drop them from the right-hand side of (60) which will
allow us to ﬁnish the proof.
Let us compute these terms. Let q := 1−1/n

1−ρ and we assume in the rest that ρ < 1/n.

u and rt

Computing A. We have:

4γ2L
β

t
(cid:88)

u=0

(1 − ρ)t−u(1 −

)(u−τ )+ ≤

(1 − ρ)t(1 − ρ)−τ (τ + 1 +

1
n

4γ2L
β

1
1 − q

)

from Leblond et al. (2017, Eq (75))

= (1 − ρ)t+1 4γ2L
β

(cid:124)

(1 − ρ)−τ −1(τ + 1 +

)

.

(61)

1
1 − q

(cid:125)

(cid:123)(cid:122)
:=A

E(cid:107)gu(cid:107)2 ≤ τ (1 − ρ)−τ

(1 − ρ)t−uE(cid:107)gu(cid:107)2 ,

(62)

Computing st

u. Since we have:
t
(cid:88)

(1 − ρ)t−u

u−1
(cid:88)

u=0

v=(u−τ )+

we have for all 0 ≤ u ≤ t:
u ≤ (1 − ρ)t−u(cid:104)
st

t
(cid:88)

u=0

√

γ2(cid:0)β − 1 +

∆τ ) + τ (1 − ρ)−τ (cid:0)γ2

∆ + γ3µ(1 +

√

√

∆τ )(cid:1)(cid:105)

.

(63)

u. To analyze these quantities, we need to compute: (cid:80)t

Computing rt
v=1 (1 −
1
n )(u−2τ −v−1)+. Fortunately, this is already done in Leblond et al. (2017, Eq (66)), and thus we
know that for all 1 ≤ u ≤ t:

u=0(1 − ρ)t−u (cid:80)u−1

u ≤ (1 − ρ)t−u
rt

−2γ +

(cid:20)

4γ2L
β

+

4Lγ2
nβ

(1 − ρ)−2τ −1(cid:16)

2τ +

(cid:17)(cid:21)

,

1
1 − q

(64)

recalling that q := 1−1/n

1−ρ and that we assumed ρ < 1
n .

We now need some assumptions to further analyze these quantities. We make simple choices for
simplicity, though a tighter analysis is possible. To get manageable (and simple) constants, we
follow Leblond et al. (2017, Eq. (82) and (83)) and assume:

ρ ≤

τ ≤

1
4n

;

n
10

.

This tells us:

1
1 − q

≤

4n
3
4
3

(1 − ρ)−kτ −1 ≤

for 0 ≤ k ≤ 2 .

(using Bernouilli’s inequality)

Additionally, we set β = 1

2 . Equation (63) thus becomes:
(cid:0)√

√

(cid:20)

−

+

∆τ +

u ≤ γ2(1 − ρ)t−u
st

4
3

1
2

∆τ + γµτ (1 +

√

(cid:21)

∆τ )(cid:1)

.

We see that for st
get:

u to be negative, we need τ = O( 1√
∆

). Let us assume that τ ≤ 1
√
10

∆

. We then

u ≤ γ2(1 − ρ)t−u
st

(cid:20)

−

+

+

+ γµτ

1
2

1
10

4
30

(cid:21)

.

4
3

11
10

Thus, the condition under which all st

(65)

(66)

(67)

(68)

u are negative boils down to:
2
11

γµτ ≤

.

21

Now looking at the rt

u terms given our assumptions, the inequality (64) becomes:

u ≤ (1 − ρ)t−u
rt

(cid:20)
−2γ + 8γ2L +

8γ2L
n

4
3

(cid:0) n
5

+

(cid:21)

(cid:1)

4n
3

≤ (1 − ρ)t−u(cid:0) − 2γ + 36γ2L(cid:1) .

The condition for all rt

u to be negative then can be simpliﬁed down to:

γ ≤

1
18L

.

γ ≤

1
36L

.

We now have a promising inequality for proving that our Lyapunov function is bounded by a con-
traction. However we have deﬁned Lt in terms of the virtual iterate xt, which means that our result
would only hold for a given T ﬁxed in advance, as is the case in Mania et al. (2017). Fortunately,
we can use the same trick as in Leblond et al. (2017, Eq. (97)): we simply add γBf (ˆxt, x∗) to both
sides in (60). rt
t + γ, which makes for a slightly worse bound on γ to ensure linear
convergence:

t is replaced by rt

For this small cost, we get a contraction bound on Bf (ˆxt, x∗), and thus by the strong convexity of
f (see (9)) we get a contraction bound for E(cid:107)ˆxt − x∗(cid:107)2.
Recap. Let us use ρ = 1
to:

L . Then the conditions (68) and (71) on the step size γ reduce

4n and γ := a

Moreover, the condition:

a ≤

min{1,

1
36

72
11

κ
τ

}.

τ ≤

1
√

∆
is sufﬁcient to also ensure that (65) is satisﬁed as ∆ ∈ [ 1

10

Thus under the conditions (72) and (73), we have that all st
rewrite the recurrent step of our Lyapunov function as:

√

n ≤ n.

1√
∆

≤

n , 1], and thus
u and rt

u terms are negative and we can

Lt+1 ≤ γEBf (ˆxt) + Lt+1 ≤ (1 − ρ)t+1(a0 + A˜e0) + (1 −

)Lt .

(74)

γµ
2

By unrolling the recursion (74), we can carefully combine the effect of the geometric term (1 − ρ)
with the one of (1 − γµ
2 ). This was already done in Leblond et al. (2017, Apx C.9, Eq. (101) to
(103)), with a trick to handle various boundary cases, yielding the overall rate:

where ρ∗ = min{ 1
To get the ﬁnal constant, we need to bound A. We have:

5κ } (that we simpliﬁed to ρ∗ = 1

5n , a 2

EBf (ˆxt, x∗) ≤ (1 − ρ∗)t+1 ˆC0,
5 min{ 1

n , a 1

κ } in the theorem statement).

A =

(1 − ρ)−τ −1(τ + 1 +

1
1 − q

)

4γ2L
β

n
10

(

≤ 8γ2L

4
3
≤ 26γ2Ln
≤ γn .

+ 1 +

4n
3

)

This is the same bound on A that was used by Leblond et al. (2017) and so we obtain the same
constant as their Eq. (104):

ˆC0 :=

21n
γ

((cid:107)x0 − x∗(cid:107)2 + γ

E(cid:107)α0

i − ∇fi(x∗)(cid:107)2).

n
2L

Note that ˆC0 = O( n

γ C0) with C0 deﬁned as in Theorem 1.

22

(69)

(70)

(71)

(72)

(73)

(75)

(76)

(77)

Now, using the strong convexity of f via (9), we get:

E(cid:107)ˆxt − x∗(cid:107)2 ≤

EBf (ˆxt, x∗) ≤ (1 − ρ∗)t+1 ˜C0,

(78)

2
µ

where ˜C0 = O( nκ
a C0).
This ﬁnishes the proof for Theorem 2.

Corollary 3 (Speedup). Suppose τ ≤ 1
. If κ ≥ n, then using the step size γ = 1/36L, PROXAS-
√
10
AGA converges geometrically with rate factor Ω( 1
κ ). If κ < n, then using the step size γ = 1/36nµ,
PROXASAGA converges geometrically with rate factor Ω( 1
n ). In both cases, the convergence rate
is the same as Sparse Proximal SAGA and PROXASAGA is thus linearly faster than its sequential
counterpart up to a constant factor. Note that in both cases the step size does not depend on τ .

∆

Furthermore, if τ ≤ 6κ, we can use a universal step size of Θ(1/L) to get a similar rate for PROX-
ASAGA than Sparse Proximal SAGA, thus making it adaptive to local strong convexity since the
knowledge of κ is not required.

Proof. If κ ≥ n, the rate factor of Sparse Proximal SAGA is 1/κ. To get the same rate factor, we
need to choose a = Ω(1), which we can fortunately do since κ ≥ n ≥

≥ 10τ .

√

n ≥ 10 1
√
10

∆

If κ < n, then the rate factor of Sparse Proximal SAGA is 1/n. Any choice of a bigger than Ω(κ/n)
gives us the same rate factor for PROXASAGA. Since τ ≤
n/10 we can pick such an a without
violating the condition of Theorem 2.

√

23

Appendix D Comparison with related work

In this section, we relate our theoretical results and proof technique with the related literature.

Speedups. Our speedup regimes are comparable with the best ones obtained in the smooth case,
including Niu et al. (2011); Reddi et al. (2015), even though unlike these papers, we support incon-
sistent reads and nonsmooth objective functions. The one exception is Leblond et al. (2017), where
the authors prove that their algorithm, ASAGA, can obtain a linear speedup even without sparsity in
the well-conditioned regime. In contrast, PROXASAGA always requires some sparsity. Whether this
property for smooth objective functions could be extended to the composite case remains an open
problem.

Coordinate Descent. We compare our approach for composite objective functions to its most
natural competitor: ASYSPCD (Liu & Wright, 2015), an asynchronous stochastic coordinate descent
algorithm. While ASYSPCD also exhibits linear speedups, subject to a condition on τ , one has to be
especially careful when trying to compare these conditions.

First, while in theory the iterations of both algorithms have the same cost, in practice various tricks
are introduced to save on computation, yielding different costs per updates.9 Second, the bound on
τ for the coordinate descent algorithm depends on p, the dimensionality of the problem, whereas
ours involves n, the number of data points. Third, a more subtle issue is that τ is not affected by
the same quantities for both algorithms.10 See Appendix D.1 for a more detailed explanation of the
differences between the bounds.

√

∆) is necessary for a linear speedup. This means in case ∆ ≤ 1/√

In the best case scenario (where the components of the gradient are uncorrelated, a somewhat un-
√
realistic setting), ASYSPCD can get a near-linear speedup for τ as big as 4
p. Our result states that
τ = O(1/
p our bound is better
than the one obtained for ASYSPCD. Recalling that 1/n ≤ ∆ ≤ 1, it appears that PROXASAGA is
favored when n is bigger than
p whereas ASYSPCD may have a better bound otherwise, though
this comparison should be taken with a grain of salt given the assumptions we had to make to arrive
at comparable quantities.

√

Furthermore, one has to note that while Liu & Wright (2015) use the classical labeling scheme
inherited from Niu et al. (2011), they still assume in their proof that the it are uniformly distributed
and that their gradient estimators are conditionally unbiased – though neither property is veriﬁed in
the general asynchronous setting. Finally, we note that ASYSPCD (as well as its incremental variant
Async-PROXSVRCD) assumes that the computation and assignment of the proximal operator is an
atomic step, while we do not make such assumption.

SVRG. The Async-ProxSVRG algorithm of Meng et al. (2017) also exhibits theoretical linear
speedups subject to the same condition as ours. However, the analyzed algorithm uses dense updates
and consistent read and writes. Although they make the analysis easier, these two factors introduce
costly bottlenecks and prevent linear speedups in running time. Furthermore, here again the classical
labeling scheme is used together with the unveriﬁed conditional unbiasedness condition.

Doubly stochastic algorithms. The Async-PROXSVRCD algorithm from Meng et al. (2017); Gu
et al. (2016) has a maximum allowable stepsize11 that is in O(1/pL), whereas the maximum step
size for PROXASAGA is in Ω(1/L), so can be up to p times bigger. Consequently, PROXASAGA
enjoys much faster theoretical convergence rates. Unfortunately, we could not ﬁnd a condition for
linear speedups to compare to. We also note that their algorithm is not appropriate in a sparse
features setting. This is illustrated in an empirical comparison in Appendix F where we see that

9For PROXASAGA the relevant quantity becomes the average number of features per data point. For
In both cases the tricks involved are

ASYSPCD it is rather the average number of data points per feature.
not covered by the theory.

10To make sure τ is the same quantity for both algorithms, we have to assume that the iteration costs are

homogeneous.

11To the best of our understanding, noting that extracting an interpretable bound from the given theoretical
results was difﬁcult. Furthermore, it appears that the proof technique may still have signiﬁcant issues: for
example, the “fully lock-free” assumption of Gu et al. (2016) allows for overwrites, and is thus incompatible
with their framework of analysis, in particular their Eq. (8).

24

their convergence in number of iterations is orders of magnitude slower than appropriate algorithms
like SAGA or PROXASAGA.

Appendix D.1 Comparison of bounds with Liu & Wright (2015)

Iteration costs. For both PROXASAGA and ASYSPCD, the average cost of an iteration is O(nS)
(where S is the average support size).
In the case of PROXASAGA (see Algorithm 1), at each
iteration the most costly operation is the computation of α, while in the general case we need to
compute a full gradient for ASYSPCD.

In order to reduce these prohibitive computation costs, several tricks are introduced. Although they
lead to much improved empirical performance, it should be noted that in both cases these tricks are
not covered by the theory. In particular, the unbiasedness condition can be violated.

In the case of PROXASAGA, we store the average gradient term α in shared memory. The cost
of each iteration then becomes the size of the extended support of the partial gradient selected at
random at this iteration, hence it is in O(∆l), where ∆l := maxi=1..n |Ti|.

For ASYSPCD, following Peng et al. (2016) we can store intermediary quantities for speciﬁc losses
(e.g. (cid:96)1-regularized logistic regression). The cost of an iteration then becomes the number of data
points whose extended support includes the coordinate selected at random at this iteration, hence it
is in O(n∆).

The relative difference in update cost of both algorithms then depends heavily on the data matrix:
if the partial gradients usually have a extended support but coordinates belong to few of them (this
can be the case if n (cid:28) p for example), then the iterations of ASYSPCD can be cheaper than those of
PROXASAGA. Conversely, if data points usually have small extended support but coordinates belong
to many of them (which can happen when p (cid:28) n for example), then the updates of PROXASAGA
are the cheaper ones.

Dependency of τ on the data matrix.
In the case of PROXASAGA the sizes of the extended
support of each data point are important – they are directly linked to the cost of each iteration.
Identical iteration costs for each data point do not inﬂuence τ , whereas heterogeneous costs may
cause τ to increase substantially. In contrast, in the case of ASYSPCD, the relevant parts of the data
matrix are the number of data points each dimension touches – for much the same reason. In the
bipartite graph between data points and dimensions, either the left or the right degrees matter for τ ,
depending on which algorithm you choose.

In order to compare their respective bounds, we have to make the assumption that the iteration costs
are homogeneous, which means that each data point has the same support size and each dimension is
active in the same number of data points. This implies that τ is the same quantity for both algorithms.

√

Best case scenario bound for AsySPCD. The result obtained in Liu & Wright (2015) states that
if τ 2Λ = O(
p), ASYSPCD can get a near-linear speedup (where Λ is a measure of the interactions
p). In the best possible scenario where
between the components of the gradient, with 1 ≤ Λ ≤
Λ = 1 (which means that the coordinates of the gradients are completely uncorrelated), τ can be as
√
big as 4

√

p.

25

Appendix E Implementation details

Initialization.
In the Sparse Proximal SAGA algorithm and its asynchronous variant, PROXAS-
AGA, the vector x can be initialized arbitrarily. The memory terms αi can be initialized to any
vector that veriﬁes supp(αi) = supp(∇fi). In practice we found that the initialization αi = 0 is
very fast to set up and often outperforms more costly initializations.

With this initialization, the gradient approximation before the ﬁrst update of the memory terms be-
comes ∇fi(x) + Diα. Since most of the values in α are zero, α will tend to be small compared to
∇fi(x), and so the gradient estimate is very close to the SGD estimate ∇fi(x). The SGD approx-
imation is known to have a very fast initial convergence (which, in light of Figure 1, our method
inherits) and has even been used as a heuristic to use during the ﬁrst epoch of variance reduced
methods (Schmidt et al., 2016).

The initialization of coefﬁcients x0 was always set to zero.

Exact regularization. Computing the gradient of a smooth regularization such as the squared (cid:96)2
penalty of Eq. (6) is independent of n and so we can use the exact regularizer in the update of
the coefﬁcients instead of storing it in α, which would also destroy the compressed storage of the
memory terms described below. In practice we use this “exact regularization”, multiplied by Di to
preserve the sparsity pattern.
Assuming a squared (cid:96)2 regularization term of the form λ
(note the extra λx)

2 , the gradient estimate in (SPS) becomes

vi = ∇fi(x) − αi + Di(α + λx) .

(79)

Storage of memory terms. The storage requirements for this method is in the worst case a table
of size n × p. However, as for SAG and SAGA, for linearly parametrized loss functions of the form
fi(x) = (cid:96)(aT
i=1 are samples associated with
the learning problem, this can be reduced to a table of size n (Schmidt et al., 2016, §4.1). This
includes popular linear models such as least squares or logistic regression with (cid:96) the squared or
logistic function, respectively.

i x), where (cid:96) is some real-valued function and (ai)n

The reduce storage comes from the fact that in this case the partial gradients have the structure

∇fi(x) = ai (cid:96)(cid:48)(aT

.

i x)
(cid:124) (cid:123)(cid:122) (cid:125)
scalar

(80)

Since ai is independent of x, we only need to store the scalar (cid:96)(cid:48)(aT
explains why ∇fi inherits the sparsity pattern of ai.

i x). This decomposition also

Atomic updates. Most modern processors have support for atomic operations with minimal over-
head. In our case, we implemented a double-precision atomic type using the C++11 atomic features
(std::atomic<double>). This type implements atomic operations through the compare and swap
semantics.

Empirically, we have found it necessary to implement atomic operations at least in the vector α and
α to reach arbitrary precision. If non-atomic operations are used, the method converges only to a
limited precision (around normalized function suboptimality of 10−3), which might be sufﬁcient for
some machine learning applications but which we found not satisfying from an optimization point
of view.

AsySPCD. Following (Peng et al., 2016) we keep the vector (aT
at each iteration using atomic updates.

i x)n

i=1 in memory and update it

Hardware and software. All experiments were run on a Dell PowerEdge 920 machine with 4 Intel
Xeon E7-4830v2 processors with 10 2.2GHz cores each and 384GB 1600 Mhz RAM. The PROX-
ASAGAand ASYSPCD code was implemented on C++ and binded in Python. The FISTA code is
implemented in pure Python using NumPY and SciPy for matrix computations (in this case the bot-
tleneck is in large sparse matrix-vector operations for which efﬁcient BLAS routines were used). Our
PROXASAGA implementation can be downloaded from http://github.com/fabianp/ProxASAGA.

26

Appendix F Experiments

All datasets used for the experiments were downloaded from the LibSVM dataset suite.12

Appendix F.1 Comparison of ProxASAGA with other sequential methods

We provide a comparison between the Sparse Proximal SAGA and related methods in the sequential
case. We compare against two methods: the MRBCD method of Zhao et al. (2014) (which forms
the basis of Async-PROXSVRCD) and the vanilla implementation of SAGA (Defazio et al., 2014),
which does not have the ability to perform sparse updates. We compare in terms of both passes
through the data (epochs) and time. We use the same step size for all methods (1/3L). Due to
the slow convergence of some methods, we use a smaller dataset than the ones used in §4. Dataset
RCV1 has n = 697, 641, d = 47, 236 and a density of 0.15, while Covtype is a dense dataset with
n = 581, 012, d = 54.

Figure 2: Suboptimality of different sequential algorithms. Each marker represents one pass through
the dataset.

We observe that for the convergence behavior in terms of number of passes, Sparse Proximal SAGA
performs as well as vanilla SAGA, though the latter requires dense updates at every iteration (Fig. 2
top left). On the other hand, in terms of running time, our implementation of Sparse Proximal SAGA
is much more efﬁcient than the other methods for sparse input (Fig. 2 top right). In the case of dense
input (Fig. 2 bottom), the three methods perform similarly.
A note on the performance of MRBCD.
It may appear surprising that Sparse Proximal SAGA
outperforms MRBCD so dramatically on sparse datasets. However, one should note that MRBCD is
a doubly stochastic algorithm where both a random data point and a random coordinate are sampled
for each iteration. If the data matrix is very sparse, then the probability that the sampled coordinate
is in the support of the sampled data point becomes very low. This means that the gradient estimator
term only contains the reference gradient term of SVRG, which only changes once per epoch. As a
result, this estimator becomes very coarse and produces a slower empirical convergence.

This is reﬂected in the theoretical results given in Zhao et al. (2014), where the epoch size needed to
get linear convergence are k times bigger than the ones required by plain SVRG, where k is the size
of the set of blocks of coordinates.

12https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/

27

Appendix F.2 Theoretical speedups.

In the experimental section, we have shown experimental speedup results where suboptimality was
a function of the running time. This measure encompasses both theoretical algorithmic optimization
properties and hardware overheads (such as contention of shared memory) which are not taken into
account in our analysis.

In order to isolate these two effects, we now plot our speedup results in Figure 3 where suboptimality
is a function of the number of iterations; thus, we abstract away any potential hardware overhead. To
do so, we implement a global counter which is sparsely updated (every 100 iterations for example)
in order not to modify the asynchrony of the system. This counter is used only for plotting purposes
and is not needed otherwise. Speciﬁcally, we deﬁne the theoretical speedup as:

theoretical speedup := (number of cores)

number of iterations for sequential algorithm
total number of iterations for parallel algorithm

.

Figure 3: Theoretical optimization speedups for (cid:96)1+(cid:96)2-regularized logistic regression. Speedup
as measured by the number of iterations required to reach 10−5 suboptimality for PROXASAGA
and ASYSPCD. In FISTA the iterates are the same with different cores and so matches the “ideal”
speedup.

We see clearly that the theoretical speedups obtained by both PROXASAGAand ASYSPCD are linear
(i.e. ideal). As we observe worse results in running time, this means that the hardware overheads of
asynchronous methods are quite signiﬁcant.

Appendix F.3 Timing benchmarks

We now provide the time it takes for the different methods with 10 cores to reach a suboptimality of
10−10. All results are in hours.

Dataset

PROXASAGA ASYSPCD

FISTA

KDD 2010
KDD 2012
Criteo

1.01
0.09
0.14

13.3
26.6
33.3

5.2
8.3
6.6

Appendix F.4 Hyperparameters

The (cid:96)1-regularization parameter λ2 was chosen as to give around 10% of non-zero features. The
exact chosen values are the following: λ2 = 10−11 for KDD 2010, λ2 = 10−16 for KDD 2012 and
λ2 = 4 × 10−12 for Criteo.

28

7
1
0
2
 
v
o
N
 
5
 
 
]

C
O
.
h
t
a
m

[
 
 
3
v
8
6
4
6
0
.
7
0
7
1
:
v
i
X
r
a

Breaking the Nonsmooth Barrier: A Scalable Parallel
Method for Composite Optimization

Fabian Pedregosa
INRIA/ENS∗
Paris, France

R´emi Leblond
INRIA/ENS∗
Paris, France

Simon Lacoste-Julien
MILA and DIRO
Universit´e de Montr´eal, Canada

Abstract

Due to their simplicity and excellent performance, parallel asynchronous variants
of stochastic gradient descent have become popular methods to solve a wide range
of large-scale optimization problems on multi-core architectures. Yet, despite their
practical success, support for nonsmooth objectives is still lacking, making them
unsuitable for many problems of interest in machine learning, such as the Lasso,
group Lasso or empirical risk minimization with convex constraints. In this work,
we propose and analyze PROXASAGA, a fully asynchronous sparse method in-
spired by SAGA, a variance reduced incremental gradient algorithm. The proposed
method is easy to implement and signiﬁcantly outperforms the state of the art on
several nonsmooth, large-scale problems. We prove that our method achieves a
theoretical linear speedup with respect to the sequential version under assump-
tions on the sparsity of gradients and block-separability of the proximal term.
Empirical benchmarks on a multi-core architecture illustrate practical speedups of
up to 12x on a 20-core machine.

1

Introduction

The widespread availability of multi-core computers motivates the development of parallel methods
adapted for these architectures. One of the most popular approaches is HOGWILD (Niu et al., 2011),
an asynchronous variant of stochastic gradient descent (SGD). In this algorithm, multiple threads run
the update rule of SGD asynchronously in parallel. As SGD, it only requires visiting a small batch
of random examples per iteration, which makes it ideally suited for large scale machine learning
problems. Due to its simplicity and excellent performance, this parallelization approach has recently
been extended to other variants of SGD with better convergence properties, such as SVRG (Johnson
& Zhang, 2013) and SAGA (Defazio et al., 2014).

Despite their practical success, existing parallel asynchronous variants of SGD are limited to smooth
objectives, making them inapplicable to many problems in machine learning and signal processing.
In this work, we develop a sparse variant of the SAGA algorithm and consider its parallel asyn-
chronous variants for general composite optimization problems of the form:

arg min
x∈Rp

f (x) + h(x)

, with f (x) := 1
n

(cid:80)n

i=1 fi(x)

,

(OPT)

where each fi is convex with L-Lipschitz gradient, the average function f is µ-strongly convex and
h is convex but potentially nonsmooth. We further assume that h is “simple” in the sense that we
have access to its proximal operator, and that it is block-separable, that is, it can be decomposed
block coordinate-wise as h(x) = (cid:80)
B∈BhB([x]B), where B is a partition of the coefﬁcients into

∗DI ´Ecole normale sup´erieure, CNRS, PSL Research University

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

subsets which will call blocks and hB only depends on coordinates in block B. Note that there is
no loss of generality in this last assumption as a unique block covering all coordinates is a valid
partition, though in this case, our sparse variant of the SAGA algorithm reduces to the original SAGA
algorithm and no gain from sparsity is obtained.

This template models a broad range of problems arising in machine learning and signal processing:
the ﬁnite-sum structure of f includes the least squares or logistic loss functions; the proximal term
h includes penalties such as the (cid:96)1 or group lasso penalty. Furthermore, this term can be extended-
valued, thus allowing for convex constraints through the indicator function.

Contributions. This work presents two main contributions. First, in §2 we describe Sparse Proxi-
mal SAGA, a novel variant of the SAGA algorithm which features a reduced cost per iteration in the
presence of sparse gradients and a block-separable penalty. Like other variance reduced methods, it
enjoys a linear convergence rate under strong convexity. Second, in §3 we present PROXASAGA, a
lock-free asynchronous parallel version of the aforementioned algorithm that does not require con-
sistent reads. Our main results states that PROXASAGA obtains (under assumptions) a theoretical
linear speedup with respect to its sequential version. Empirical benchmarks reported in §4 show that
this method dramatically outperforms state-of-the-art alternatives on large sparse datasets, while the
empirical speedup analysis illustrates the practical gains as well as its limitations.

1.1 Related work

Asynchronous coordinate-descent. For composite objective functions of the form (OPT), most of
the existing literature on asynchronous optimization has focused on variants of coordinate descent.
Liu & Wright (2015) proposed an asynchronous variant of (proximal) coordinate descent and proved
a near-linear speedup in the number of cores used, given a suitable step size. This approach has been
recently extended to general block-coordinate schemes by Peng et al. (2016), to greedy coordinate-
descent schemes by You et al. (2016) and to non-convex problems by Davis et al. (2016). However,
as illustrated by our experiments, in the large sample regime coordinate descent compares poorly
against incremental gradient methods like SAGA.

Variance reduced incremental gradient and their asynchronous variants.
Initially proposed in
the context of smooth optimization by Le Roux et al. (2012), variance reduced incremental gradient
methods have since been extended to minimize composite problems of the form (OPT) (see table
below). Smooth variants of these methods have also recently been extended to the asynchronous set-
ting, where multiple threads run the update rule asynchronously and in parallel. Interestingly, none
of these methods achieve both simultaneously, i.e. asynchronous optimization of composite prob-
lems. Since variance reduced incremental gradient methods have shown state of the art performance
in both settings, this generalization is of key practical interest.

Objective

Smooth

Composite

Sequential Algorithm
SVRG (Johnson & Zhang, 2013)
SDCA (Shalev-Shwartz & Zhang, 2013)
SAGA (Defazio et al., 2014)
PROXSDCA (Shalev-Shwartz et al., 2012)
SAGA (Defazio et al., 2014)
ProxSVRG (Xiao & Zhang, 2014)

Asynchronous Algorithm

SVRG (Reddi et al., 2015)
PASSCODE (Hsieh et al., 2015, SDCA variant)
ASAGA (Leblond et al., 2017, SAGA variant)

This work: PROXASAGA

On the difﬁculty of a composite extension. Two key issues explain the paucity in the develop-
ment of asynchronous incremental gradient methods for composite optimization. The ﬁrst issue
is related to the design of such algorithms. Asynchronous variants of SGD are most competitive
when the updates are sparse and have a small overlap, that is, when each update modiﬁes a small
and different subset of the coefﬁcients. This is typically achieved by updating only coefﬁcients for
which the partial gradient at a given iteration is nonzero,2 but existing schemes such as the lagged
updates technique (Schmidt et al., 2016) are not applicable in the asynchronous setting. The second

2Although some regularizers are sparsity inducing, large scale datasets are often extremely sparse and lever-

aging this property is crucial for the efﬁciency of the method.

2

difﬁculty is related to the analysis of such algorithms. All convergence proofs crucially use the Lip-
schitz condition on the gradient to bound the noise terms derived from asynchrony. However, in the
composite case, the gradient mapping term (Beck & Teboulle, 2009), which replaces the gradient
in proximal-gradient methods, does not have a bounded Lipschitz constant. Hence, the traditional
proof technique breaks down in this scenario.

Other approaches. Recently, Meng et al. (2017); Gu et al. (2016) independently proposed a dou-
bly stochastic method to solve the problem at hand. Following Meng et al. (2017) we refer to it
as Async-PROXSVRCD. This method performs coordinate descent-like updates in which the true
gradient is replaced by its SVRG approximation. It hence features a doubly-stochastic loop: at each
iteration we select a random coordinate and a random sample. Because the selected coordinate block
is uncorrelated with the chosen sample, the algorithm can be orders of magnitude slower than SAGA
in the presence of sparse gradients. Appendix F contains a comparison of these methods.

1.2 Deﬁnitions and notations

By convention, we denote vectors and vector-valued functions in lowercase boldface (e.g. x) and
matrices in uppercase boldface (e.g. D). The proximal operator of a convex lower semicontinuous
function h is deﬁned as proxh(x) := arg minz∈Rp {h(z) + 1
2 (cid:107)x − z(cid:107)2}. A function f is said to be
L-smooth if it is differentiable and its gradient is L-Lipschitz continuous. A function f is said to be
µ-strongly convex if f − µ
2 (cid:107) · (cid:107)2 is convex. We use the notation κ := L/µ to denote the condition
number for an L-smooth and µ-strongly convex function.3
I p denotes the p-dimensional identity matrix, 1{cond} the characteristic function, which is 1 if cond
evaluates to true and 0 otherwise. The average of a vector or matrix is denoted α := 1
i=1 αi.
n
We use (cid:107) · (cid:107) for the Euclidean norm. For a positive semi-deﬁnite matrix D, we deﬁne its associated
distance as (cid:107)x(cid:107)2
D := (cid:104)x, Dx(cid:105). We denote by [ x ]b the b-th coordinate in x. This notation is
overloaded so that for a collection of blocks T = {B1, B2, . . .}, [x]T denotes the vector x restricted
to the coordinates in the blocks of T . For convenience, when T consists of a single block B we use
[x]B as a shortcut of [x]{B}. Finally, we distinguish E, the full expectation taken with respect to
all the randomness in the system, from E, the conditional expectation of a random it (the random
feature sampled at each iteration by SGD-like algorithms) conditioned on all the “past”, which the
context will clarify.

(cid:80)n

2 Sparse Proximal SAGA

Original SAGA algorithm. The original SAGA algorithm (Defazio et al., 2014) maintains two
moving quantities: the current iterate x and a table (memory) of historical gradients (αi)n
i=1. At
every iteration, it samples an index i ∈ {1, . . . , n} uniformly at random, and computes the next
iterate (x+, α+) according to the following recursion:
ui = ∇fi(x) − αi + α ; x+ = proxγh

(1)
On each iteration, this update rule requires to visit all coefﬁcients even if the partial gradients ∇fi are
sparse. Sparse partial gradients arise in a variety of practical scenarios: for example, in generalized
linear models the partial gradients inherit the sparsity pattern of the dataset. Given that large-scale
datasets are often sparse,4 leveraging this sparsity is crucial for the success of the optimizer.

i = ∇fi(x) .

(cid:0)x − γui

(cid:1); α+

Sparse Proximal SAGA algorithm. We will now describe an algorithm that leverages sparsity
in the partial gradients by only updating those blocks that intersect with the support of the partial
gradients. Since in this update scheme some blocks might appear more frequently than others, we
will need to counterbalance this undersirable effect with a well-chosen block-wise reweighting of
the average gradient and the proximal term.

In order to make precise this block-wise reweighting, we deﬁne the following quantities. We denote
by Ti the extended support of ∇fi, which is the set of blocks that intersect the support of ∇fi,

3Since we have assumed that each individual fi is L-smooth, f itself is L-smooth – but it could have a

smaller smoothness constant. Our rates are in terms of this bigger L/µ, as is standard in the SAGA literature.

4For example, in the LibSVM datasets suite, 8 out of the 11 datasets (as of May 2017) with more than a

million samples have a density between 10−4 and 10−6.

3

i

formally deﬁned as Ti := {B : supp(∇fi) ∩ B (cid:54)= ∅, B ∈ B}. For totally separable penalties such
as the (cid:96)1 norm, the blocks are individual coordinates and so the extended support covers the same
coordinates as the support. Let dB := n/nB, where nB := (cid:80)
1{B ∈ Ti} is the number of times
that B ∈ Ti. For simplicity we assume nB > 0, as otherwise the problem can be reformulated
without block B. The update rule in (1) requires computing the proximal operator of h, which
involves a full pass on the coordinates. In our proposed algorithm, we replace h in (1) with the
function ϕi(x) := (cid:80)
dBhB(x), whose form is justiﬁed by the following three properties.
First, this function is zero outside Ti, allowing for sparse updates. Second, because of the block-wise
reweighting dB, the function ϕi is an unbiased estimator of h (i.e., E ϕi = h), property which will
be crucial to prove the convergence of the method. Third, ϕi inherits the block-wise structure of h
and its proximal operator can be computed from that of h as [proxγϕi(x)]B = [prox(dB γ)hB (x)]B
if B ∈ Ti and [proxγϕi(x)]B = [x]B otherwise. Following Leblond et al. (2017), we will also
replace the dense gradient estimate ui by the sparse estimate vi := ∇fi(x) − αi + Diα, where
Di is the diagonal matrix deﬁned block-wise as [Di]B,B = dB1{B ∈ Ti}I |B|. It is easy to verify
that the vector Diα is a weighted projection onto the support of Ti and E Diα = α, making vi an
unbiased estimate of the gradient.

B∈Ti

We now have all necessary elements to describe the Sparse Proximal SAGA algorithm. As the
original SAGA algorithm, it maintains two moving quantities: the current iterate x ∈ Rp and a
i=1, αi ∈ Rp. At each iteration, the algorithm samples an index
table of historical gradients (αi)n
i ∈ {1, . . . , n} and computes the next iterate (x+, α+) as:

vi = ∇fi(x) − αi + Diα ; x+ = proxγϕi

(cid:0)x − γvi

(cid:1) ; α+

i = ∇fi(x) ,

(SPS)

where in a practical implementation the vector α is updated incrementally at each iteration.

The above algorithm is sparse in the sense that it only requires to visit and update blocks in the
extended support: if B /∈ Ti, by the sparsity of vi and proxϕi, we have [x+]B = [x]B. Hence,
when the extended support Ti is sparse, this algorithm can be orders of magnitude faster than the
naive SAGA algorithm. The extended support is sparse for example when the partial gradients are
sparse and the penalty is separable, as is the case of the (cid:96)1 norm or the indicator function over a
hypercube, or when the the penalty is block-separable in a way such that only a small subset of the
blocks overlap with the support of the partial gradients. Initialization of variables and a reduced
storage scheme for the memory are discussed in the implementation details section of Appendix E.

Relationship with existing methods. This algorithm can be seen as a generalization of both the
Standard SAGA algorithm and the Sparse SAGA algorithm of Leblond et al. (2017). When the
proximal term is not block-separable, then dB = 1 (for a unique block B) and the algorithm defaults
to the Standard (dense) SAGA algorithm. In the smooth case (i.e., h = 0), the algorithm defaults to
the Sparse SAGA method. Hence we note that the sparse gradient estimate vi in our algorithm is the
same as the one proposed in Leblond et al. (2017). However, we emphasize that a straightforward
combination of this sparse update rule with the proximal update from the Standard SAGA algorithm
results in a nonconvergent algorithm: the block-wise reweighting of h is a surprisingly simple but
crucial change. We now give the convergence guarantees for this algorithm.
Theorem 1. Let γ = a
SAGA converges geometrically in expectation with a rate factor of at least ρ = 1
is, for xt obtained after t updates, we have the following bound:

5L for any a ≤ 1 and f be µ-strongly convex (µ > 0). Then Sparse Proximal
κ }. That

5 min{ 1

n , a 1

E(cid:107)xt − x∗(cid:107)2 ≤ (1 − ρ)tC0 , with C0 := (cid:107)x0 − x∗(cid:107)2 + 1
5L2

(cid:80)n

i=1 (cid:107)α0

i − ∇fi(x∗)(cid:107)2

.

Remark. For the step size γ = 1/5L, the convergence rate is (1 − 1/5 min{1/n, 1/κ}). We can thus
identify two regimes: the “big data” regime, n ≥ κ, in which the rate factor is bounded by 1/5n, and
the “ill-conditioned” regime, κ ≥ n, in which the rate factor is bounded by 1/5κ. This rate roughly
matches the rate obtained by Defazio et al. (2014). While the step size bound of 1/5L is slightly
smaller than the 1/3L one obtained in that work, this can be explained by their stronger assumptions:
each fi is strongly convex whereas they are strongly convex only on average in this work. All proofs
for this section can be found in Appendix B.

4

i=1

j=1[ ˆαj ]Ti

ˆx = inconsistent read of x
ˆα = inconsistent read of α
Sample i uniformly in {1, ..., n}
Si := support of ∇fi
Ti := extended support of ∇fi in B
[ α ]Ti = 1/n (cid:80)n
[ δα ]Si = [∇fi(ˆx)]Si − [ ˆαi]Si
[ ˆv ]Ti = [ δα ]Ti + [Diα ]Ti
[ δx ]Ti = [proxγϕi(ˆx − γ ˆv)]Ti − [ˆx]Ti
for B in Ti do
for b ∈ B do

Algorithm 1 PROXASAGA (analyzed)
1: Initialize shared variables x and (αi)n
2: keep doing in parallel
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end parallel loop

end for
// (‘←’ denotes shared memory update.)

[ x ]b ← [ x ]b + [ δx ]b
if b ∈ Si then

[αi]b ← [∇fi(ˆx)]b

end if
end for

(cid:46) atomic

i=1, α

Sample i uniformly in {1, ..., n}
Si := support of ∇fi
Ti := extended support of ∇fi in B
[ ˆx ]Ti = inconsistent read of x on Ti
ˆαi = inconsistent read of αi
[ α ]Ti = inconsistent read of α on Ti
[ δα ]Si = [∇fi(ˆx)]Si − [ ˆαi]Si
[ ˆv ]Ti = [δα ]Ti + [ Diα ]Ti
[ δx ]Ti = [proxγϕi(ˆx − γ ˆv)]Ti − [ˆx]Ti
for B in Ti do
for b in B do

Algorithm 2 PROXASAGA (implemented)
1: Initialize shared variables x, (αi)n
2: keep doing in parallel
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
end for
19:
20: αi ← ∇fi(ˆx)
21: end parallel loop

[ x ]b ← [ x ]b + [ δx ]b
if b ∈ Si then

end if
end for

[ α ]b ← [α]b + 1/n[δα]b (cid:46) atomic

(scalar update) (cid:46) atomic

(cid:46) atomic

3 Asynchronous Sparse Proximal SAGA

We introduce PROXASAGA – the asynchronous parallel variant of Sparse Proximal SAGA. In this
algorithm, multiple cores update a central parameter vector using the Sparse Proximal SAGA intro-
duced in the previous section, and updates are performed asynchronously. The algorithm parameters
are read and written without vector locks, i.e., the vector content of the shared memory can poten-
tially change while a core is reading or writing to main memory coordinate by coordinate. These
operations are typically called inconsistent (at the vector level).

The full algorithm is described in Algorithm 1 for its theoretical version (on which our analysis
is built) and in Algorithm 2 for its practical implementation. The practical implementation differs
from the analyzed agorithm in three points. First, in the implemented algorithm, index i is sampled
before reading the coefﬁcients to minimize memory access since only the extended support needs to
be read. Second, since our implementation targets generalized linear models, the memory αi can be
compressed into a single scalar in L20 (see Appendix E). Third, α is stored in memory and updated
incrementally instead of recomputed at each iteration.

The rest of the section is structured as follows: we start by describing our framework of analysis; we
then derive essential properties of PROXASAGA along with a classical delay assumption. Finally,
we state our main convergence and speedup result.

3.1 Analysis framework

As in most of the recent asynchronous optimization literature, we build on the hardware model in-
troduced by Niu et al. (2011), with multiple cores reading and writing to a shared memory parameter
vector. These operations are asynchronous (lock-free) and inconsistent:5 ˆxt, the local copy of the
parameters of a given core, does not necessarily correspond to a consistent iterate in memory.

“Perturbed” iterates. To handle this additional difﬁculty, contrary to most contributions in this
ﬁeld, we choose the “perturbed iterate framework” proposed by Mania et al. (2017) and reﬁned
by Leblond et al. (2017). This framework can analyze variants of SGD which obey the update rule:

xt+1 = xt − γv(xt, it) , where v veriﬁes the unbiasedness condition E v(x, it) = ∇f (x)

5This is an extension of the framework of Niu et al. (2011), where consistent updates were assumed.

5

and the expectation is computed with respect to it. In the asynchronous parallel setting, cores are
reading inconsistent iterates from memory, which we denote ˆxt. As these inconsistent iterates are
affected by various delays induced by asynchrony, they cannot easily be written as a function of
their previous iterates. To alleviate this issue, Mania et al. (2017) choose to introduce an additional
quantity for the purpose of the analysis:
xt+1 := xt − γv(ˆxt, it) ,

(2)
Note that this equation is the deﬁnition of this new quantity xt. This virtual iterate is useful for the
convergence analysis and makes for much easier proofs than in the related literature.

the “virtual iterate” – which is never actually computed .

“After read” labeling. How we choose to deﬁne the iteration counter t to label an iterate xt
matters in the analysis.
In this paper, we follow the “after read” labeling proposed in Leblond
et al. (2017), in which we update our iterate counter, t, as each core ﬁnishes reading its copy of
the parameters (in the speciﬁc case of PROXASAGA, this includes both ˆxt and ˆαt). This means
that ˆxt is the (t + 1)th fully completed read. One key advantage of this approach compared to the
classical choice of Niu et al. (2011) – where t is increasing after each successful update – is that
it guarantees both that the it are uniformly distributed and that it and ˆxt are independent. This
property is not veriﬁed when using the “after write” labeling of Niu et al. (2011), although it is still
implicitly assumed in the papers using this approach, see Leblond et al. (2017, Section 3.2) for a
discussion of issues related to the different labeling schemes.

Generalization to composite optimization. Although the perturbed iterate framework was de-
signed for gradient-based updates, we can extend it to proximal methods by remarking that in the
sequential setting, proximal stochastic gradient descent and its variants can be characterized by the
following similar update rule:

xt+1 = xt − γg(xt, vit, it) , with g(x, v, i) := 1
γ

(cid:0)x − proxγϕi(x − γv)(cid:1) ,

where as before v veriﬁes the unbiasedness condition E v = ∇f (x). The Proximal Sparse SAGA
iteration can be easily written within this template by using ϕi and vi as deﬁned in §2. Using this
deﬁnition of g, we can deﬁne PROXASAGA virtual iterates as:

xt+1 := xt − γg(ˆxt, ˆvt
it

, it) , with ˆvt
it

= ∇fit (ˆxt) − ˆαt
it

+ Dit αt

,

(3)

(4)

where as in the sequential case, the memory terms are updated as ˆαt
it
analysis of PROXASAGA will be based on this deﬁnition of the virtual iterate xt+1.

= ∇fit(ˆxt). Our theoretical

3.2 Properties and assumptions

Now that we have introduced the “after read” labeling for proximal methods in Eq. (4), we can
leverage the framework of Leblond et al. (2017, Section 3.3) to derive essential properties for the
analysis of PROXASAGA. We describe below three useful properties arising from the deﬁnition
of Algorithm 1, and then state a central (but standard) assumption that the delays induced by the
asynchrony are uniformly bounded.

Independence: Due to the “after read” global ordering, ir is independent of ˆxt for all r ≥ t. We
enforce the independence for r = t by having the cores read all the shared parameters before their
iterations.
Unbiasedness: The term ˆvt
consequence of the independence between it and ˆxt.

it is an unbiased estimator of the gradient of f at ˆxt. This property is a

Atomicity: The shared parameter coordinate update of [x]b on Line 14 is atomic. This means that
there are no overwrites for a single coordinate even if several cores compete for the same resources.
Most modern processors have support for atomic operations with minimal overhead.

Bounded overlap assumption. We assume that there exists a uniform bound, τ , on the maximum
number of overlapping iterations. This means that every coordinate update from iteration t is suc-
cessfully written to memory before iteration t + τ + 1 starts. Our result will give us conditions on τ
to obtain linear speedups.

Bounding ˆxt − xt. The delay assumption of the previous paragraph allows to express the difference
between real and virtual iterate using the gradient mapping gu := g(ˆxu, ˆvu
iu
ˆxt−xt = γ (cid:80)t−1

u are p × p diagonal matrices with terms in {0, +1}. (5)

ugu , where Gt

, iu) as:

Gt

u=(t−τ )+

6

0 represents instances where both ˆxu and xu have received the corresponding updates. +1, on
the contrary, represents instances where ˆxu has not yet received an update that is already in xu by
deﬁnition. This bound will prove essential to our analysis.

3.3 Analysis

In this section, we state our convergence and speedup results for PROXASAGA. The full details
of the analysis can be found in Appendix C. Following Niu et al. (2011), we introduce a sparsity
measure (generalized to the composite setting) that will appear in our results.
Deﬁnition 1. Let ∆ := maxB∈B |{i : Ti (cid:51) B}|/n. This is the normalized maximum number of
times that a block appears in the extended support. For example, if a block is present in all Ti, then
∆ = 1. If no two Ti share the same block, then ∆ = 1/n. We always have 1/n ≤ ∆ ≤ 1.
Theorem 2 (Convergence guarantee of PROXASAGA). Suppose τ ≤ 1
√
10
γ = a
in expectation at a geometric rate factor of at least: ρ(a) = 1
(1 − ρ)t ˜C0, where ˜C0 is a constant independent of t (≈ nκ

. For any step size
τ }, the inconsistent read iterates of Algorithm 1 converge
(cid:9), i.e. E(cid:107)ˆxt − x∗(cid:107)2 ≤

L with a ≤ a∗(τ ) := 1

a C0 with C0 as deﬁned in Theorem 1).

36 min{1, 6κ

5 min (cid:8) 1

n , a 1

∆

κ

√

This last result is similar to the original SAGA convergence result and our own Theorem 1, with both
an extra condition on τ and on the maximum allowable step size. In the best sparsity case, ∆ = 1/n
and we get the condition τ ≤
n/10. We now compare the geometric rate above to the one of Sparse
Proximal SAGA to derive the necessary conditions under which PROXASAGA is linearly faster.
Corollary 1 (Speedup). Suppose τ ≤ 1
. If κ ≥ n, then using the step size γ = 1/36L, PROXAS-
√
10
AGA converges geometrically with rate factor Ω( 1
κ ). If κ < n, then using the step size γ = 1/36nµ,
PROXASAGA converges geometrically with rate factor Ω( 1
n ). In both cases, the convergence rate
is the same as Sparse Proximal SAGA. Thus PROXASAGA is linearly faster than its sequential
counterpart up to a constant factor. Note that in both cases the step size does not depend on τ .

∆

Furthermore, if τ ≤ 6κ, we can use a universal step size of Θ(1/L) to get a similar rate for PROX-
ASAGA than Sparse Proximal SAGA, thus making it adaptive to local strong convexity since the
knowledge of κ is not required.

These speedup regimes are comparable with the best ones obtained in the smooth case, including Niu
et al. (2011); Reddi et al. (2015), even though unlike these papers, we support inconsistent reads
and nonsmooth objective functions. The one exception is Leblond et al. (2017), where the authors
prove that their algorithm, ASAGA, can obtain a linear speedup even without sparsity in the well-
conditioned regime. In contrast, PROXASAGA always requires some sparsity. Whether this property
for smooth objective functions could be extended to the composite case remains an open problem.

√

Relative to ASYSPCD, in the best case scenario (where the components of the gradient are uncorre-
√
lated, a somewhat unrealistic setting), ASYSPCD can get a near-linear speedup for τ as big as 4
p.
∆) is necessary for a linear speedup. This means in case ∆ ≤ 1/√
Our result states that τ = O(1/
p
our bound is better than the one obtained for ASYSPCD. Recalling that 1/n ≤ ∆ ≤ 1, it appears
that PROXASAGA is favored when n is bigger than
p whereas ASYSPCD may have a better bound
otherwise, though this comparison should be taken with a grain of salt given the assumptions we
had to make to arrive at comparable quantities. An extended comparison with the related work can
be found in Appendix D.

√

4 Experiments

In this section, we compare PROXASAGA with related methods on different datasets. Although
PROXASAGA can be applied more broadly, we focus on (cid:96)1 + (cid:96)2-regularized logistic regression, a
model of particular practical importance. The objective function takes the form

1
n

n
(cid:88)

i=1

log (cid:0)1 + exp(−bia

(cid:124)

i x)(cid:1) + λ1

2 (cid:107)x(cid:107)2

2 + λ2(cid:107)x(cid:107)1

,

(6)

where ai ∈ Rp and bi ∈ {−1, +1} are the data samples. Following Defazio et al. (2014), we set
λ1 = 1/n. The amount of (cid:96)1 regularization (λ2) is selected to give an approximate 1/10 nonzero

7

Table 1: Description of datasets.

Dataset

n

p

KDD 2010 (Yu et al., 2010)
KDD 2012 (Juan et al., 2016)
Criteo (Juan et al., 2016)

19,264,097
149,639,105
45,840,617

1,163,024
54,686,452
1,000,000

density
10−6
2 × 10−7
4 × 10−5

L

28.12
1.25
1.25

∆

0.15
0.85
0.89

Figure 1: Convergence for asynchronous stochastic methods for (cid:96)1 + (cid:96)2-regularized logistic
regression. Top: Suboptimality as a function of time for different asynchronous methods using 1
and 10 cores. Bottom: Running time speedup as function of the number of cores. PROXASAGA
achieves signiﬁcant speedups over its sequential version while being orders of magnitude faster than
competing methods. ASYSPCD achieves the highest speedups but it also the slowest overall method.

coefﬁcients. Implementation details are available in Appendix E. We chose the 3 datasets described
in Table 1

Results. We compare three parallel asynchronous methods on the aforementioned datasets: PROX-
ASAGA (this work),6 ASYSPCD, the asynchronous proximal coordinate descent method of Liu &
Wright (2015) and the (synchronous) FISTA algorithm (Beck & Teboulle, 2009), in which the gra-
dient computation is parallelized by splitting the dataset into equal batches. We aim to benchmark
these methods in the most realistic scenario possible; to this end we use the following step size:
1/2L for PROXASAGA, 1/Lc for ASYSPCD, where Lc is the coordinate-wise Lipschitz constant
of the gradient, while FISTA uses backtracking line-search. The results can be seen in Figure 1
(top) with both one (thus sequential) and ten processors. Two main observations can be made from
this ﬁgure. First, PROXASAGA is signiﬁcantly faster on these problems. Second, its asynchronous
version offers a signiﬁcant speedup over its sequential counterpart.

In Figure 1 (bottom) we present speedup with respect to the number of cores, where speedup is
computed as the time to achieve a suboptimality of 10−10 with one core divided by the time to
achieve the same suboptimality using several cores. While our theoretical speedups (with respect
to the number of iterations) are almost linear as our theory predicts (see Appendix F), we observe
a different story for our running time speedups. This can be attributed to memory access overhead,
which our model does not take into account. As predicted by our theoretical results, we observe

6A reference C++/Python implementation of is available at https://github.com/fabianp/ProxASAGA

8

a high correlation between the ∆ dataset sparsity measure and the empirical speedup: KDD 2010
(∆ = 0.15) achieves a 11x speedup, while in Criteo (∆ = 0.89) the speedup is never above 6x.

Note that although competitor methods exhibit similar or sometimes better speedups, they remain
orders of magnitude slower than PROXASAGA in running time for large sparse problems. In fact,
our method is between 5x and 80x times faster (in time to reach 10−10 suboptimality) than FISTA
and between 13x and 290x times faster than ASYSPCD (see Appendix F.3).

5 Conclusion and future work

In this work, we have described PROXASAGA, an asynchronous variance reduced algorithm with
support for composite objective functions. This method builds upon a novel sparse variant of the
(proximal) SAGA algorithm that takes advantage of sparsity in the individual gradients. We have
proven that this algorithm is linearly convergent under a condition on the step size and that it is
linearly faster than its sequential counterpart given a bound on the delay. Empirical benchmarks
show that PROXASAGA is orders of magnitude faster than existing state-of-the-art methods.

This work can be extended in several ways. First, we have focused on the SAGA method as the basic
iteration loop, but this approach can likely be extended to other proximal incremental schemes such
as SGD or ProxSVRG. Second, as mentioned in §3.3, it is an open question whether it is possible to
obtain convergence guarantees without any sparsity assumption, as was done for ASAGA.

Acknowledgements

The authors would like to thank our colleagues Damien Garreau, Robert Gower, Thomas Ker-
dreux, Geoffrey Negiar and Konstantin Mishchenko for their feedback on this manuscript, and Jean-
Baptiste Alayrac for support managing the computational resources.

This work was partially supported by a Google Research Award. FP acknowledges support from the
chaire ´Economie des nouvelles donn´ees with the data science joint research initiative with the fonds
AXA pour la recherche.

References

Bauschke, Heinz and Combettes, Patrick L. Convex analysis and monotone operator theory in

Hilbert spaces. Springer, 2011.

Beck, Amir and Teboulle, Marc. Gradient-based algorithms with applications to signal recovery.

Convex Optimization in Signal Processing and Communications, 2009.

Davis, Damek, Edmunds, Brent, and Udell, Madeleine. The sound of APALM clapping: faster
nonsmooth nonconvex optimization with stochastic asynchronous PALM. In Advances in Neural
Information Processing Systems 29, 2016.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural Infor-
mation Processing Systems, 2014.

Gu, Bin, Huo, Zhouyuan, and Huang, Heng. Asynchronous stochastic block coordinate descent

with variance reduction. arXiv preprint arXiv:1610.09447v3, 2016.

Hsieh, Cho-Jui, Yu, Hsiang-Fu, and Dhillon, Inderjit S. PASSCoDe: parallel asynchronous stochas-

tic dual coordinate descent. In ICML, 2015.

Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems, 2013.

Juan, Yuchin, Zhuang, Yong, Chin, Wei-Sheng, and Lin, Chih-Jen. Field-aware factorization ma-
chines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Sys-
tems. ACM, 2016.

9

Le Roux, Nicolas, Schmidt, Mark, and Bach, Francis R. A stochastic gradient method with an ex-
ponential convergence rate for ﬁnite training sets. In Advances in Neural Information Processing
Systems, 2012.

Leblond, R´emi, Pedregosa, Fabian, and Lacoste-Julien, Simon. ASAGA: asynchronous parallel
SAGA. Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS 2017), 2017.

Liu, Ji and Wright, Stephen J. Asynchronous stochastic coordinate descent: Parallelism and conver-

gence properties. SIAM Journal on Optimization, 2015.

Mania, Horia, Pan, Xinghao, Papailiopoulos, Dimitris, Recht, Benjamin, Ramchandran, Kannan,
and Jordan, Michael I. Perturbed iterate analysis for asynchronous stochastic optimization. SIAM
Journal on Optimization, 2017.

Meng, Qi, Chen, Wei, Yu, Jingcheng, Wang, Taifeng, Ma, Zhi-Ming, and Liu, Tie-Yan. Asyn-
chronous stochastic proximal optimization algorithms with variance reduction. In AAAI, 2017.

Nesterov, Yurii. Introductory lectures on convex optimization. Springer Science & Business Media,

Nesterov, Yurii. Gradient methods for minimizing composite functions. Mathematical Program-

2004.

ming, 2013.

Niu, Feng, Recht, Benjamin, Re, Christopher, and Wright, Stephen. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Sys-
tems, 2011.

Peng, Zhimin, Xu, Yangyang, Yan, Ming, and Yin, Wotao. ARock: an algorithmic framework for

asynchronous parallel coordinate updates. SIAM Journal on Scientiﬁc Computing, 2016.

Reddi, Sashank J, Hefny, Ahmed, Sra, Suvrit, Poczos, Barnabas, and Smola, Alexander J. On
variance reduction in stochastic gradient descent and its asynchronous variants. In Advances in
Neural Information Processing Systems, 2015.

Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Minimizing ﬁnite sums with the stochastic

average gradient. Mathematical Programming, 2016.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research, 2013.

Shalev-Shwartz, Shai et al.
arXiv:1211.2717, 2012.

Proximal stochastic dual coordinate ascent.

arXiv preprint

Xiao, Lin and Zhang, Tong. A proximal stochastic gradient method with progressive variance re-

duction. SIAM Journal on Optimization, 2014.

You, Yang, Lian, Xiangru, Liu, Ji, Yu, Hsiang-Fu, Dhillon, Inderjit S, Demmel, James, and Hsieh,
Cho-Jui. Asynchronous parallel greedy coordinate descent. In Advances In Neural Information
Processing Systems, 2016.

Yu, Hsiang-Fu, Lo, Hung-Yi, Hsieh, Hsun-Ping, Lou, Jing-Kai, McKenzie, Todd G, Chou, Jung-
Wei, Chung, Po-Han, Ho, Chia-Hua, Chang, Chun-Fu, Wei, Yin-Hsuan, et al. Feature engineering
and classiﬁer ensemble for KDD cup 2010. In KDD Cup, 2010.

Zhao, Tuo, Yu, Mo, Wang, Yiming, Arora, Raman, and Liu, Han. Accelerated mini-batch random-
ized block coordinate descent method. In Advances in neural information processing systems,
2014.

10

Breaking the Nonsmooth Barrier: A Scalable Parallel
Method for Composite Optimization

Supplementary material

Notations. Throughout the supplementary material we use the following extra notation. We denote
by (cid:104)·, ·(cid:105)(i) (resp. (cid:107) · (cid:107)(i)) the scalar product (resp. norm) restricted to blocks in Ti, i.e., (cid:104)x, y(cid:105)(i) :=
(cid:104)[x]B, [y]B(cid:105) and (cid:107)x(cid:107)(i) := (cid:112)(cid:104)x, x(cid:105)(i). We will also use the following deﬁnitions: ϕ :=
(cid:80)
(cid:80)

B∈Ti
B∈B dBhB(x) and D is the diagonal matrix deﬁned block-wise as [D]B,B = dBI |B|.

The Bregman divergence associated with a convex function f for points x, y in its domain is
deﬁned as:

Bf (x, y) := f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105) .

Note that this is always positive due to the convexity of f .

Appendix A Basic properties

Lemma 1. For any µ-strongly convex function f we have the following inequality:

(cid:104)∇f (y) − ∇f (x), y − x(cid:105) ≥

(cid:107)y − x(cid:107)2 + Bf (x, y) .

µ
2

Proof. By strong convexity, f veriﬁes the inequality:

f (y) ≤ f (x) + (cid:104)∇f (y), y − x(cid:105) −

(cid:107)y − x(cid:107)2 ,

µ
2

for any x, y in the domain (see e.g. (Nesterov, 2004)). We then have the equivalences:

f (x) ≤ f (y) + (cid:104)∇f (x), x − y(cid:105) −

(cid:107)x − y(cid:107)2

µ
2

⇐⇒

(cid:107)x − y(cid:107)2 + f (x) − f (y) ≤ (cid:104)∇f (x), x − y(cid:105)

⇐⇒

(cid:107)x − y(cid:107)2 + f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105)
(cid:125)

(cid:124)

(cid:123)(cid:122)
Bf (x,y)

µ
2
µ
2

≤ (cid:104)∇f (x) − ∇f (y), x − y(cid:105) ,

(10)

where in the last line we have subtracted (cid:104)∇f (y), x − y(cid:105) from both sides of the inequality.

Lemma 2. Let the fi be L-smooth and convex functions. Then it is veriﬁed that:

1
n

n
(cid:88)

i=1

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ 2LBf (x, y) .

Proof. Since each fi is L-smooth, it is veriﬁed (see e.g. Nesterov (2004, Theorem 2.1.5)) that

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ 2L(cid:0)fi(x) − fi(y) − (cid:104)∇fi(y), x − y(cid:105)(cid:1) .

The result is obtained by averaging over i.

(7)

(8)

(9)

(11)

(12)

Lemma 3 (Characterization of the proximal operator). Let h be convex lower semicontinuous. Then
we have the following characterization of the proximal operator:

z = proxγh(x) ⇐⇒

(x − z) ∈ ∂h(z) .

(13)

1
γ

11

Proof. This is a direct consequence of the ﬁrst order optimality conditions on the deﬁnition of
proximal operator, see e.g. (Beck & Teboulle, 2009; Nesterov, 2013).

Lemma 4 (Firm non-expansiveness). Let x, ˜x be two arbitrary elements in the domain of ϕi and
z, ˜z be deﬁned as z := proxϕi

(˜x). Then it is veriﬁed that:

(x), ˜z := proxϕi

(cid:104)z − ˜z, x − ˜x(cid:105)(i) ≥ (cid:107)z − ˜z(cid:107)2

(i) .

Proof. By the block-separability of ϕi, the proximal operator is the concatenation of the proximal
operators of the blocks. In other words, for any block B ∈ Ti we have:

[z]B = proxγϕB ([x]B) ,

[˜z]B = proxγϕB ([˜x]B) ,

where ϕB is the restriction of ϕi to B. By ﬁrm non-expansiveness of the proximal operator (see
e.g. Bauschke & Combettes (2011, Proposition 4.2)) we have that:

(cid:104)[z]B − [˜z]B, [x]B − [˜x]B(cid:105) ≥ (cid:107)[z]B − [˜z]B(cid:107)2 .

Summing over the blocks in Ti yields the desired result.

(14)

(15)

12

Appendix B Sparse Proximal SAGA

This Appendix contains all proofs for Section 2. The main result of this section is Theorem 1, whose
proof is structured as follows:

• We start by proving four auxiliary results that will be used later on in the proofs of both
synchronous and asynchronous variants. The ﬁrst is the unbiasedness of key quantities used
in the algorithm. The second is a characterization of the solutions of (OPT) in terms of f
and ϕ (deﬁned below) in Lemma 6. The third is a key inequality in Lemma 7 that relates
the gradient mapping to other terms that arise in the optimization. The fourth is an upper
bound on the variance terms of the gradient estimator, relating it to the Bregman divergence
of f and the past gradient estimator terms.

• In Lemma 9, we deﬁne an upper bound on the iterates (cid:107)xt − x∗(cid:107)2, called a Lyapunov
function, and prove an inequality that relates this Lyapunov function value at the current
iterate with its value at the previous iterate.

• Finally, in the proof of Theorem 1 we use the previous inequality in terms of the Lyapunov

function to prove a geometric convergence of the iterates.

We start by proving the following unbiasedness result, mentioned in §2.

Lemma 5. Let Di and ϕi be deﬁned as in §2. Then it is veriﬁed that EDi = I p and E ϕi = h.

Proof. Let B ∈ B an arbitrary block. We have the following sequence of equalities:

where the last equality comes from the deﬁnition of nB. EDi = I p then follows from the arbitrari-
ness of B.

Similarly, for ϕi we have:

E[Di]B,B =

[Di]B,B =

dB1{B ∈ Ti}I |B|

1
n

n
(cid:88)

i=1

(cid:33)

1{B ∈ Ti}I |B|

1
n

1
n
(cid:32)

n
(cid:88)

i=1
n
(cid:88)

i=1

1
nB

n
nB

n
(cid:88)

i=1

=

=

1{B ∈ Ti}

I |B| = I |B| ,

Eϕi([x]B) =

dB1{B ∈ Ti}hB([x]B)

1
n

1
n
(cid:32)

n
(cid:88)

i=1
n
(cid:88)

i=1

1
nB

n
nB

n
(cid:88)

i=1

=

=

1{B ∈ Ti}hB([x]B)

(cid:33)

1{B ∈ Ti}

hB([x]B) = hB([x]B) ,

Finally, the result E ϕi = h comes from adding over all blocks.

Lemma 6. x∗ is a solution to (OPT) if and only if the following condition is veriﬁed:

x∗ = proxγϕ

(cid:0)x∗ − γD∇f (x∗)(cid:1) .

Proof. By the ﬁrst order optimality conditions, the solutions to (OPT) are characterized by the sub-
differential inclusion −∇f (x∗) ∈ ∂h(x∗). We can then write the following sequence of equiva-

13

(16)

(17)

(18)

(19)

(20)

(21)

(22)

lences:

−∇f (x∗) ∈ ∂h(x∗) ⇐⇒ −D∇f (x∗) ∈ D∂h(x∗)

(multiplying by D, equivalence since diagonals are nonzero)

⇐⇒ −D∇f (x∗) ∈ ∂ϕ(x∗)
(by deﬁnition of ϕ)

⇐⇒

(x∗ − γD∇f (x∗) − x∗) ∈ ∂ϕ(x∗)

1
γ

(adding and subtracting x∗)

⇐⇒ x∗ = proxγϕ(x∗ − γD∇f (x∗)) .

(by Lemma 3)

(23)

Since all steps are equivalences, we have the desired result.

The following lemma will be key in the proof of convergence for both the sequential and the parallel
versions of the algorithm. With this result, we will be able to bound the product between the gradient
mapping and the iterate suboptimality by:

• First, the negative norm of the gradient mapping, which will be key in the parallel setting

to cancel out the terms arising from the asynchrony.

• Second, variance terms in (cid:107)vi−Di∇f (x∗)(cid:107)2 that we will be able to bound by the Bregman

divergence using Lemma 2.

• Third and last, a product with terms in (cid:104)vi − Di∇f (x∗), x − x∗(cid:105), which taken in expec-
tation gives (cid:104)∇f (x) − ∇f (x∗), x − x∗(cid:105) and will allow us to apply Lemma 1 to obtain the
contraction terms needed to obtain a geometric rate of convergence.

Lemma 7 (Gradient mapping inequality). Let x be an arbitrary vector, x∗ a solution to (OPT), vi
as deﬁned in (SPS) and g = g(x, vi, i) the gradient mapping deﬁned in (3). Then the following
inequality is veriﬁed for any β > 0:

(cid:104)g, x − x∗(cid:105) ≥ −

(β − 2)(cid:107)g(cid:107)2 −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + (cid:104)vi − Di∇f (x∗), x − x∗(cid:105) .

(24)

γ
2

γ
2β

Proof. By ﬁrm non-expansiveness of the proximal operator (Lemma 4) applied to z = proxγϕi(x−
γvi) and ˜z = proxγϕi(x∗ − γD∇f (x∗)) we have:

(cid:107)z − ˜z(cid:107)2

(i) − (cid:104)z − ˜z, x − γvi − x∗ + γD∇f (x∗)(cid:105)(i) ≤ 0 .

(25)

By the (SPS) iteration we have x+ = z and by Lemma 3 we have that [z]Ti = [x∗]Ti, hence the
above can be rewritten as

(cid:107)x+ − x∗(cid:107)2

(i) − (cid:104)x+ − x∗, x − γvi − x∗ + γD∇f (x∗)(cid:105)(i) ≤ 0 .

(26)

14

We can now write the following sequence of inequalities

(cid:104)γg, x − x∗(cid:105) = (cid:104)x − x+, x − x∗(cid:105)(i)

(by deﬁnition and sparsity of g)

(cid:16)

≥

1 −

(cid:17)

β
2

(cid:16)

≥

1 −

(cid:16)

=

1 −

(cid:17)

(cid:17)

β
2

β
2

= (cid:104)x − x+ + x∗ − x∗, x − x∗(cid:105)(i)
= (cid:107)x − x∗(cid:107)2
≥ (cid:107)x − x∗(cid:107)2

(i) − (cid:104)x+ − x∗, x − x∗(cid:105)(i)
(i) − (cid:104)x+ − x∗, 2x − γvi − 2x∗ + γD∇f (x∗)(cid:105)(i) + (cid:107)x+ − x∗(cid:107)2
(i)

(27)

(adding Eq. (26))

= (cid:107)x − x+(cid:107)2
= (cid:107)x − x+(cid:107)2

(i) + (cid:104)x+ − x∗, γvi − γD∇f (x∗)(cid:105)(i)
(i) + (cid:104)x − x∗, γvi − γD∇f (x∗)(cid:105)(i) − (cid:104)x − x+, γvi − γD∇f (x∗)(cid:105)(i)

(completing the square)

(adding and substracting x)

(cid:107)x − x+(cid:107)2

(i) −

(cid:107)vi − D∇f (x∗)(cid:107)2

(i) + γ(cid:104)vi − D∇f (x∗), x − x∗(cid:105)(i)

(Young’s inequality 2(cid:104)a, b(cid:105) ≤

+ β(cid:107)b(cid:107)2, valid for arbitrary β > 0)

γ2
2β

γ2
2β

(cid:107)a(cid:107)2
β

(cid:107)x − x+(cid:107)2

(i) −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + γ(cid:104)vi − Di∇f (x∗), x − x∗(cid:105)

(by deﬁnition of Di and using the fact that vi is Ti-sparse)

(cid:107)γg(cid:107)2 −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + γ(cid:104)vi − D∇f (x∗), x − x∗(cid:105) ,

(28)

γ2
2β

where in the last inequality we have used the fact that g is Ti-sparse. Finally, dividing by γ both
sides yields the desired result.

Lemma 8 (Upper bound on the gradient estimator variance). For arbitrary vectors x, (αi)n
vi as deﬁned in (SPS) we have:

i=0, and

E(cid:107)vi − Di∇f (x∗)(cid:107)2 ≤ 4LBf (x, x∗) + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(29)

Proof. We will now bound the variance terms. For this we have:

E(cid:107)vi − D∇f (x∗)(cid:107)2

(i) = E(cid:107)∇fi(x) − ∇fi(x∗) + ∇fi(x∗) − αi + Diα − D∇f (x∗)(cid:107)2
(i)

≤ 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)∇fi(x∗) − αi − (D∇f (x∗) − Dα)(cid:107)2
(i)

(by inequality (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2)
= 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)∇fi(x∗) − αi(cid:107)2

− 4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)(i) + 2E(cid:107)D∇f (x∗) − Dα(cid:107)2
(developing the square)

(i) .

(30)

We will now simplify the last two terms in the above expression. For the ﬁrst of the two last terms
we have:

− 4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)(i) = −4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)

(31)

(support of ﬁrst term)

= −4(cid:104)∇f (x∗) − α, D∇f (x∗) − Dα(cid:105)
= −4(cid:107)∇f (x∗) − α(cid:107)2

D .

Similarly, for the last term we have:

2E(cid:107)D∇f (x∗) − Dα(cid:107)2

(i) = 2E(cid:104)Di∇f (x∗) − Diα, D∇f (x∗) − Dα(cid:105)

= 2(cid:104)∇f (x∗) − α, D∇f (x∗) − Dα(cid:105)

(32)

(33)

(using Lemma 5)
D .

= 2(cid:107)∇f (x∗) − α(cid:107)2

15

and so the addition of these terms is negative and can be dropped. In all, for the variance terms we
have

E(cid:107)vi − D∇f (x∗)(cid:107)2

(i) ≤ 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2

≤ 4LBf (x, x∗) + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(by Lemma 2)

(34)

c
n

n
(cid:88)

i=1

(cid:16)

We now deﬁne an upper bound on the quantity that we would like to bound, often called a Lyapunov
function, and establish a recursive inequality on this Lyapunov function.

Lemma 9 (Lyapunov inequality). Let L be the following c-parametrized function:

L(x, α) := (cid:107)x − x∗(cid:107)2 +

(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(35)

Let x+ and α+ be obtained from the Sparse Proximal SAGA updates (SPS). Then we have:

EL(x+, α+) − L(x, α) ≤ − γµ(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:17)

c
n

(cid:16)

+

2γ2 −

(cid:17)

c
n

E(cid:107)αi − ∇fi(x)(cid:107)2 .

Proof. For the ﬁrst term of L we have:

(cid:107)x+ − x∗(cid:107)2 = (cid:107)x − γg − x∗(cid:107)2

(g := g(x, vi, i))

= (cid:107)x − x∗(cid:107)2 − 2γ(cid:104)g, x − x∗(cid:105) + (cid:107)γg(cid:107)2
≤ (cid:107)x − x∗(cid:107)2 + γ2(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γ(cid:104)vi − Di∇f (x∗), x − x∗(cid:105)

(by Lemma 7 with β = 1)

Since vi is an unbiased estimator of the gradient and EDi = I p, taking expectations we have:

E(cid:107)x+ − x∗(cid:107)2 ≤ (cid:107)x − x∗(cid:107)2 + γ2E(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γ(cid:104)∇f (x) − ∇f (x∗), x − x∗(cid:105)

≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + γ2E(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γBf (x, x∗) .

(38)

(by Lemma 1)

By using the variance terms bound (Lemma 8) in the previous equation we have:

E(cid:107)x+ − x∗(cid:107)2 ≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + (4Lγ2 − 2γ)Bf (x, x∗)
+ 2γ2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

We will now bound the second term of the Lyapunov function. We have:

1
n

n
(cid:88)

i=1

(cid:107)α+

i − ∇fi(x∗)(cid:107)2 =

1 −

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2

(cid:18)

(cid:19)

1
n

1
n

(by deﬁnition of α+)

(cid:18)

(cid:19)

≤

1 −

1
n

2
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

LBf (x, x∗) .

(by Lemma 2)

(36)

(37)

(39)

(40)

(41)

16

Combining Eq. (39) and (40) we have:

EL(x+, α+) ≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + (4Lγ2 − 2γ)Bf (x, x∗) + 2γ2E(cid:107)αi − ∇fi(x∗)(cid:107)2

(cid:20)(cid:18)

+ c

1 −

(cid:19)

1
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

(cid:21)
2LBf (x, x∗)

= (1 − γµ)(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:16)

1
n
c
n

(cid:17)

(cid:16)

+

2γ2 −

(cid:17)

c
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 + cE(cid:107)αi − ∇fi(x∗)(cid:107)2

= L(x, α) − γµ(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:16)

(cid:17)

c
n

c
n
Finally, subtracting L(x, α) from both sides yields the desired result.

E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

2γ2 −

+

(cid:17)

(cid:16)

(42)

Theorem 1. Let γ = a
converges geometrically in expectation with a rate factor of at least ρ = 1
xt obtained after t updates and x∗ the solution to (OPT), we have the bound:
i=1 (cid:107)α0

5L for any a ≤ 1 and f be µ-strongly convex. Then Sparse Proximal SAGA
κ }. That is, for

with C0 := (cid:107)x0 − x∗(cid:107)2 + 1
5L2

E(cid:107)xt − x∗(cid:107)2 ≤ (1 − ρ)tC0 ,

i − ∇fi(x∗)(cid:107)2

5 min{ 1

n , a 1

(cid:80)n

.

Proof. Let H := 1
n

(cid:80)

ELt+1 − (1 − ρ)Lt ≤ ρLt − γµ(cid:107)xt − x∗(cid:107)2 +

i (cid:107)αi − ∇fi(x∗)(cid:107)2. By the Lyapunov inequality from Lemma 9, we have:
(cid:17)
c
4Lγ2 − 2γ + 2L
n

c
n

(cid:16)

(cid:17)

(cid:16)

H

= (ρ − γµ) (cid:107)xt − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (xt, x∗) +

(cid:17)

c
n

(cid:17)

c
n

Bf (xt, x∗) +
(cid:20)
2γ2 + c

ρ −

(cid:18)

2γ2 −
(cid:19)(cid:21)

H

1
n

(cid:18)

(cid:19)

H

2c
3n

≤ (ρ − γµ) (cid:107)xt − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (xt, x∗) +

2γ2 −

(cid:16)

(cid:16)

(by deﬁnition of Lt)

(choosing ρ ≤

1
3n

)

= (ρ − γµ) (cid:107)xt − x∗(cid:107)2 + (cid:0)10Lγ2 − 2γ(cid:1) Bf (xt, x∗)

(choosing

= 3γ2)

c
n

(cid:16)

(cid:17)

aµ
5L

≤

ρ −

(cid:107)xt − x∗(cid:107)2
µ
a
L
5
And so we have the bound:

(for ρ ≤

≤ 0 .

·

)

(for all γ =

, a ≤ 1)

a
5L

(cid:18)

ELt+1 ≤

1 − min

(cid:110) 1
3n

,

a
5

·

1
κ

(cid:111)(cid:19)

(cid:18)

Lt ≤

min

, a ·

(cid:110) 1
n

(cid:111)(cid:19)

1
κ

Lt ,

1 −

1
5
3n ≤ 1

5n merely for clarity of exposition.

where in the last inequality we have used the trivial bound 1
Chaining expectations from t to 0 we have:
(cid:111)(cid:19)t+1

(cid:18)

ELt+1 ≤

1 −

min

, a ·

1
5

1
5

1
5

(cid:110) 1
n

(cid:110) 1
n

(cid:110) 1
n

L0
(cid:111)(cid:19)t+1 (cid:32)

(cid:111)(cid:19)t+1 (cid:32)

1
κ

1
κ

1
κ

(cid:18)

(cid:18)

(since a ≤ 1 and 3/5 ≤ 1) .

=

1 −

min

, a ·

(cid:107)x0 − x∗(cid:107)2 +

(cid:107)α0

i − ∇fi(x∗)(cid:107)2

≤

1 −

min

, a ·

(cid:107)x0 − x∗(cid:107)2 +

(cid:107)α0

i − ∇fi(x∗)(cid:107)2

(45)

3a2
52L2

1
5L2

n
(cid:88)

i=1
n
(cid:88)

i=1

The fact that Lt is a majorizer of (cid:107)xt − x∗(cid:107)2 completes the proof.

(43)

(44)

(cid:33)

(cid:33)

17

Appendix C ProxASAGA

In this Appendix we provide the proofs for results from Section 3, that is Theorem 2 (the convergence
theorem for PROXASAGA) and Corollary 1 (its speedup result).

Notation. Through this section, we use the following shorthand for the gradient mapping: gt :=
g(ˆxt, ˆvt
it

, it).

Appendix C.1 Proof outline.

As in the smooth case (h = 0), we start by using the deﬁnition of xt+1 in Eq. (4) to relate the
distance to the optimum in terms of its previous iterates:

(cid:107)xt+1 − x∗(cid:107)2 =(cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)ˆxt − xt, gt(cid:105) + γ2(cid:107)gt(cid:107)2 − 2γ(cid:104)ˆxt − x∗, gt(cid:105) .

(46)

However, in this case gt is not a gradient estimator but a gradient mapping, so we cannot continue
as is customary – by using the unbiasedness of the gradient in the (cid:104)ˆxt − x∗, gt(cid:105) term together with
the strong convexity of f (see Leblond et al. (2017, Section 3.5)).

To circumvent this difﬁculty, we derive a tailored inequality for the gradient mapping (Lemma 7
in Appendix B), which in turn allows us to use the classical unbiasedness and strong convexity
arguments to get the following inequality:

at+1 ≤ (1 −

γµ
2

)at + γ2E(cid:107)gt(cid:107)2 − 2γEBf (ˆxt, x∗) + γµE(cid:107)ˆxt − x(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105)
(cid:125)
(cid:124)

(cid:123)(cid:122)
additional asynchrony terms

(47)

+γ2(β − 2)E(cid:107)gt(cid:107)2 +
(cid:124)

γ2
β
(cid:123)(cid:122)
additional proximal and variance terms

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2
(cid:125)

,

where at := E(cid:107)xt − x∗(cid:107)2. Note that since f is strongly convex, Bf (ˆxt, x∗) ≥ µ
In the smooth setting, one ﬁrst expresses the additional asynchrony terms as linear combinations
of past gradient variance terms (E(cid:107)gu(cid:107)2)0≤u≤t. Then one crucially uses the negative Bregman
divergence term to control the variance terms. However, in our current setting, we cannot relate the
norm of the gradient mapping E(cid:107)gt(cid:107)2 to the Bregman divergence (from which h is absent). Instead,
we use the negative term γ2(β − 1)E(cid:107)gt(cid:107)2 to control all the (E(cid:107)gu(cid:107)2)0≤u≤t terms that arise from
asynchrony.

2 (cid:107)ˆxt − x∗(cid:107)2.

The rest of the proof consists in:
i) expressing the additional asynchrony terms as linear combinations of (E(cid:107)gu(cid:107)2)0≤u≤t, follow-
ing Leblond et al. (2017, Lemma 1);
ii) expressing the last variance term, (cid:107)ˆvt
it
divergences (Lemma 8 in Appendix B and Lemma 2 from Leblond et al. (2017));
iii) deﬁning a Lyapunov function, Lt := (cid:80)t
contraction given conditions on the maximum step size and delay.

u=0(1 − ρ)t−uau, and proving that it is bounded by a

− Dit∇f (x∗)(cid:107)2, as a linear combination of past Bregman

Appendix C.2 Detailed proof

Theorem 2 (Convergence guarantee and rate of PROXASAGA). Suppose τ ≤ 1
√
10
36 min{1, 6κ
size γ = a
expectation at a geometric rate factor of at least: ρ(a) = 1
(1 − ρ)t ˜C0, where ˜C0 is a constant independent of t (≈ nκ

. For any step
τ }, the inconsistent read iterates of Algorithm 1 converge in
(cid:9), i.e. E(cid:107)ˆxt − x∗(cid:107)2 ≤
5 min (cid:8) 1

a C0 with C0 as deﬁned in Theorem 1).

L with a ≤ 1

n , a 1

∆

κ

Proof. In order to get an initial recursive inequality, we ﬁrst unroll the (virtual) update:
(cid:107)xt+1 − x∗(cid:107)2 = (cid:107)xt − γgt − x∗(cid:107)2 = (cid:107)xt − x∗(cid:107)2 + (cid:107)γgt(cid:107)2 − 2γ(cid:104)gt, xt − x∗(cid:105)

= (cid:107)xt − x∗(cid:107)2 + (cid:107)γgt(cid:107)2 − 2γ(cid:104)gt, ˆxt − x∗(cid:105) + 2γ(cid:104)gt, ˆxt − xt(cid:105) ,

(48)

18

and then apply Lemma 7 with x = ˆxt and v = ˆvt
(cid:104)·(cid:105)(i) = (cid:104)·(cid:105)(it).

it. Note that in this case we have g = gt and

(cid:107)xt+1 − x∗(cid:107)2 ≤ (cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)gt, ˆxt − xt(cid:105) + γ2(cid:107)gt(cid:107)2 + γ2(β − 2)(cid:107)gt(cid:107)2

+

γ2
β

(cid:107)ˆvt
it

− D∇f (x∗)(cid:107)2

(it) − 2γ(cid:104)ˆvt
it

− D∇f (x∗), ˆxt − x∗(cid:105)(it)

= (cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)(cid:107)gt(cid:107)2

− Dit∇f (x∗)(cid:107)2 − 2γ(cid:104)ˆvt
it

− Dit∇f (x∗), ˆxt − x∗(cid:105).

(49)

+

(cid:107)ˆvt
it

γ2
β
(as [ˆvt
it

]Tit

= ˆvt

it)

We now use the property that it is independent of ˆxt (which we enforce by reading ˆxt before picking
it, see Section 3), together with the unbiasedness of the gradient update ˆvt
= ∇f (ˆxt)) and
the deﬁnition of D to simplify the following expression as follows:

it (Eˆvt
it

E(cid:104)ˆvt
it

− Dit∇f (x∗), ˆxt − x∗(cid:105) = (cid:104)∇f (ˆxt) − ∇f (x∗), ˆxt − x∗(cid:105)
µ
(cid:107)ˆxt − x∗(cid:107)2 + Bf (ˆxt, x∗) ,
2

≥

where the last inequality comes from Lemma 1. Taking conditional expectations on (49) we get:

E(cid:107)xt+1 − x∗(cid:107)2 ≤ (cid:107)xt − x∗(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)E(cid:107)gt(cid:107)2

+

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2 − γµ(cid:107)ˆxt − x∗(cid:107)2 − 2γBf (ˆxt, x∗)

≤ (1 −

)(cid:107)xt − x∗(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)E(cid:107)gt(cid:107)2

+

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2 + γµ(cid:107)ˆxt − xt(cid:107)2 − 2γBf (ˆxt, x∗)

(using (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2 on (cid:107)xt − ˆxt + ˆxt − x∗(cid:107)2)

γ2
β
γµ
2
γ2
β

γµ
2

≤ (1 −

)(cid:107)xt − x∗(cid:107)2 + γ2(β − 1)E(cid:107)gt(cid:107)2 + γµ(cid:107)ˆxt − xt(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105)
2γ2
β

− ∇fit (x∗)(cid:107)2 .

Bf (ˆxt, x∗) +

4γ2L
β

E(cid:107) ˆαt
it

(52)

− 2γBf (ˆxt, x∗) +

(using Lemma 8 on the variance terms)

Since we also have:

ˆxt − xt = γ

Gt

ug(ˆxu, ˆαu, iu),

t−1
(cid:88)

u=(t−τ )+

the effect of asynchrony for the perturbed iterate updates was already derived in a very similar setup
in Leblond et al. (2017). We re-use the following bounds from their Appendix C.4:7

E(cid:107)ˆxt − xt(cid:107)2 ≤ γ2(1 +

∆τ )

E(cid:107)gu(cid:107)2 ,

√

t−1
(cid:88)

u=(t−τ )+

Leblond et al. (2017, Eq. (48))

E(cid:104)gt, ˆxt − xt(cid:105) ≤

E(cid:107)gu(cid:107)2 +

E(cid:107)gt(cid:107)2 . Leblond et al. (2017, Eq. (46)).

√

γ

∆

2

t−1
(cid:88)

u=(t−τ )+

√

γ

∆τ
2

7The appearance of the sparsity constant ∆ is coming from the crucial property that E(cid:107)x(cid:107)2
∀x ∈ Rp (see Eq. (39) in Leblond et al. (2017), where they use the notation (cid:107) · (cid:107)i for our (cid:107) · (cid:107)(i)).

(i) ≤ ∆(cid:107)x(cid:107)2

19

(50)

(51)

(53)

(54)

(55)

Because the updates on α are the same for PROXASAGA as for ASAGA, we can re-use the same
argument arising in the proof of Leblond et al. (2017, Lemma 2) to get the following bound on
E(cid:107) ˆαt
it

− ∇fit(x∗)(cid:107)2:

E(cid:107) ˆαt
it

− ∇fit(x∗)(cid:107)2 ≤

(1 −

)(t−2τ −u−1)+EBf (ˆxu, x∗)

+2L(1 −

)(t−τ )+ ˜e0 ,

(56)

1
n

2L
n

t−1
(cid:88)

u=1
(cid:124)

1
n

(cid:123)(cid:122)
Henceforth denoted Ht

(cid:125)

E(cid:107)α0

i − f (cid:48)

where ˜e0 := 1
i (x∗)(cid:107)2. This bound is obtained by analyzing which gradient could be
2L
the source of αit in the past (taking in consideration the inconsistent writes), and then applying
Lemma 2 on the E(cid:107)∇f (ˆxu) − ∇f (x∗)(cid:107)2 terms, explaining the presence of Bf (ˆxu, x∗) terms.8
The inequality (56) corresponds to Eq. (56) and (57) in Leblond et al. (2017).

By taking the full expectation of (52) and plugging the above inequalities back, we obtain an in-
equality similar to Leblond et al. (2017, Master inequality (28)) which describes how the error terms
at := E(cid:107)xt − x∗(cid:107)2 of the virtual iterates are related:

at+1 ≤(1 −

)at +

(1 −

)(t−τ )+ ˜e0

γµ
2
+ γ2 (cid:104)

4γ2L
β
√

1
n

β − 1 +

∆τ

(cid:105)

E(cid:107)gt(cid:107)2 +

√

(cid:104)

γ2

∆ + γ3µ(1 +

√

(cid:105)
∆τ )

t
(cid:88)

E(cid:107)gu(cid:107)2

(57)

u=(t−τ )+

− 2γEBf (ˆxt, x∗) +

EBf (ˆxt, x∗) +

4γ2L
β

4γ2L
βn

Ht .

We now have a promising inequality with a contractive term and several quantities that we need to
bound. In order to achieve our ﬁnal result, we introduce the same Lyapunov function as in Leblond
et al. (2017):

Lt :=

(1 − ρ)t−uau ,

t
(cid:88)

u=0

where ρ is a target rate factor for which we will provide a value later on. Proving that this Lyapunov
function is bounded by a contraction will ﬁnish our proof. We have:

Lt+1 =

(1 − ρ)t+1−uau = (1 − ρ)t+1a0 +

(1 − ρ)t+1−uau

t+1
(cid:88)

u=0

= (1 − ρ)t+1a0 +

(1 − ρ)t−uau+1 .

(58)

t+1
(cid:88)

u=1
t
(cid:88)

u=0

We now plug our new bound on at+1, (57):
(1 − ρ)t−u(cid:104)

Lt+1 ≤ (1 − ρ)t+1a0 +

t
(cid:88)

u=0

)(u−τ )+ ˜e0

(1 −

)au +

γµ
2
+ γ2(cid:0)β − 1 +
√

+ (cid:0)γ2

(1 −

4γ2L
1
β
n
∆τ (cid:1)E(cid:107)gu(cid:107)2
√

√

∆ + γ3µ(1 +

∆τ )(cid:1)

u
(cid:88)

E(cid:107)gv(cid:107)2

v=(u−τ )+

− 2γEBf (ˆxu, x∗) +

EBf (ˆxu, x∗) +

4γ2L
β

4γ2L
βn

(cid:105)

Hu

.

(59)

After regrouping similar terms, we get:

Lt+1 ≤ (1 − ρ)t+1(a0 + A˜e0) + (1 −

)Lt +

st
u

E(cid:107)gu(cid:107)2 +

rt
u

EBf (ˆxu, x∗) .

(60)

γµ
2

t
(cid:88)

u=0

t
(cid:88)

u=1

8Note that Leblond et al. (2017) analyzed the unconstrained scenario, and so Bf (ˆxu, x∗) is replaced by the

simpler f (ˆxu) − f (x∗) in their bound.

20

Now, provided that we can prove that under certain conditions the st
u terms are all negative
(and that the A term is not too big), we can drop them from the right-hand side of (60) which will
allow us to ﬁnish the proof.
Let us compute these terms. Let q := 1−1/n

1−ρ and we assume in the rest that ρ < 1/n.

u and rt

Computing A. We have:

4γ2L
β

t
(cid:88)

u=0

(1 − ρ)t−u(1 −

)(u−τ )+ ≤

(1 − ρ)t(1 − ρ)−τ (τ + 1 +

1
n

4γ2L
β

1
1 − q

)

from Leblond et al. (2017, Eq (75))

= (1 − ρ)t+1 4γ2L
β

(cid:124)

(1 − ρ)−τ −1(τ + 1 +

)

.

(61)

1
1 − q

(cid:125)

(cid:123)(cid:122)
:=A

E(cid:107)gu(cid:107)2 ≤ τ (1 − ρ)−τ

(1 − ρ)t−uE(cid:107)gu(cid:107)2 ,

(62)

Computing st

u. Since we have:
t
(cid:88)

(1 − ρ)t−u

u−1
(cid:88)

u=0

v=(u−τ )+

we have for all 0 ≤ u ≤ t:
u ≤ (1 − ρ)t−u(cid:104)
st

t
(cid:88)

u=0

√

γ2(cid:0)β − 1 +

∆τ ) + τ (1 − ρ)−τ (cid:0)γ2

∆ + γ3µ(1 +

√

√

∆τ )(cid:1)(cid:105)

.

(63)

u. To analyze these quantities, we need to compute: (cid:80)t

Computing rt
v=1 (1 −
1
n )(u−2τ −v−1)+. Fortunately, this is already done in Leblond et al. (2017, Eq (66)), and thus we
know that for all 1 ≤ u ≤ t:

u=0(1 − ρ)t−u (cid:80)u−1

u ≤ (1 − ρ)t−u
rt

−2γ +

(cid:20)

4γ2L
β

+

4Lγ2
nβ

(1 − ρ)−2τ −1(cid:16)

2τ +

(cid:17)(cid:21)

,

1
1 − q

(64)

recalling that q := 1−1/n

1−ρ and that we assumed ρ < 1
n .

We now need some assumptions to further analyze these quantities. We make simple choices for
simplicity, though a tighter analysis is possible. To get manageable (and simple) constants, we
follow Leblond et al. (2017, Eq. (82) and (83)) and assume:

ρ ≤

τ ≤

1
4n

;

n
10

.

This tells us:

1
1 − q

≤

4n
3
4
3

(1 − ρ)−kτ −1 ≤

for 0 ≤ k ≤ 2 .

(using Bernouilli’s inequality)

Additionally, we set β = 1

2 . Equation (63) thus becomes:
(cid:0)√

√

(cid:20)

−

+

∆τ +

u ≤ γ2(1 − ρ)t−u
st

4
3

1
2

∆τ + γµτ (1 +

√

(cid:21)

∆τ )(cid:1)

.

We see that for st
get:

u to be negative, we need τ = O( 1√
∆

). Let us assume that τ ≤ 1
√
10

∆

. We then

u ≤ γ2(1 − ρ)t−u
st

(cid:20)

−

+

+

+ γµτ

1
2

1
10

4
30

(cid:21)

.

4
3

11
10

Thus, the condition under which all st

(65)

(66)

(67)

(68)

u are negative boils down to:
2
11

γµτ ≤

.

21

Now looking at the rt

u terms given our assumptions, the inequality (64) becomes:

u ≤ (1 − ρ)t−u
rt

(cid:20)
−2γ + 8γ2L +

8γ2L
n

4
3

(cid:0) n
5

+

(cid:21)

(cid:1)

4n
3

≤ (1 − ρ)t−u(cid:0) − 2γ + 36γ2L(cid:1) .

The condition for all rt

u to be negative then can be simpliﬁed down to:

γ ≤

1
18L

.

γ ≤

1
36L

.

We now have a promising inequality for proving that our Lyapunov function is bounded by a con-
traction. However we have deﬁned Lt in terms of the virtual iterate xt, which means that our result
would only hold for a given T ﬁxed in advance, as is the case in Mania et al. (2017). Fortunately,
we can use the same trick as in Leblond et al. (2017, Eq. (97)): we simply add γBf (ˆxt, x∗) to both
sides in (60). rt
t + γ, which makes for a slightly worse bound on γ to ensure linear
convergence:

t is replaced by rt

For this small cost, we get a contraction bound on Bf (ˆxt, x∗), and thus by the strong convexity of
f (see (9)) we get a contraction bound for E(cid:107)ˆxt − x∗(cid:107)2.
Recap. Let us use ρ = 1
to:

L . Then the conditions (68) and (71) on the step size γ reduce

4n and γ := a

Moreover, the condition:

a ≤

min{1,

1
36

72
11

κ
τ

}.

τ ≤

1
√

∆
is sufﬁcient to also ensure that (65) is satisﬁed as ∆ ∈ [ 1

10

Thus under the conditions (72) and (73), we have that all st
rewrite the recurrent step of our Lyapunov function as:

√

n ≤ n.

1√
∆

≤

n , 1], and thus
u and rt

u terms are negative and we can

Lt+1 ≤ γEBf (ˆxt) + Lt+1 ≤ (1 − ρ)t+1(a0 + A˜e0) + (1 −

)Lt .

(74)

γµ
2

By unrolling the recursion (74), we can carefully combine the effect of the geometric term (1 − ρ)
with the one of (1 − γµ
2 ). This was already done in Leblond et al. (2017, Apx C.9, Eq. (101) to
(103)), with a trick to handle various boundary cases, yielding the overall rate:

where ρ∗ = min{ 1
To get the ﬁnal constant, we need to bound A. We have:

5κ } (that we simpliﬁed to ρ∗ = 1

5n , a 2

EBf (ˆxt, x∗) ≤ (1 − ρ∗)t+1 ˆC0,
5 min{ 1

n , a 1

κ } in the theorem statement).

A =

(1 − ρ)−τ −1(τ + 1 +

1
1 − q

)

4γ2L
β

n
10

(

≤ 8γ2L

4
3
≤ 26γ2Ln
≤ γn .

+ 1 +

4n
3

)

This is the same bound on A that was used by Leblond et al. (2017) and so we obtain the same
constant as their Eq. (104):

ˆC0 :=

21n
γ

((cid:107)x0 − x∗(cid:107)2 + γ

E(cid:107)α0

i − ∇fi(x∗)(cid:107)2).

n
2L

Note that ˆC0 = O( n

γ C0) with C0 deﬁned as in Theorem 1.

22

(69)

(70)

(71)

(72)

(73)

(75)

(76)

(77)

Now, using the strong convexity of f via (9), we get:

E(cid:107)ˆxt − x∗(cid:107)2 ≤

EBf (ˆxt, x∗) ≤ (1 − ρ∗)t+1 ˜C0,

(78)

2
µ

where ˜C0 = O( nκ
a C0).
This ﬁnishes the proof for Theorem 2.

Corollary 3 (Speedup). Suppose τ ≤ 1
. If κ ≥ n, then using the step size γ = 1/36L, PROXAS-
√
10
AGA converges geometrically with rate factor Ω( 1
κ ). If κ < n, then using the step size γ = 1/36nµ,
PROXASAGA converges geometrically with rate factor Ω( 1
n ). In both cases, the convergence rate
is the same as Sparse Proximal SAGA and PROXASAGA is thus linearly faster than its sequential
counterpart up to a constant factor. Note that in both cases the step size does not depend on τ .

∆

Furthermore, if τ ≤ 6κ, we can use a universal step size of Θ(1/L) to get a similar rate for PROX-
ASAGA than Sparse Proximal SAGA, thus making it adaptive to local strong convexity since the
knowledge of κ is not required.

Proof. If κ ≥ n, the rate factor of Sparse Proximal SAGA is 1/κ. To get the same rate factor, we
need to choose a = Ω(1), which we can fortunately do since κ ≥ n ≥

≥ 10τ .

√

n ≥ 10 1
√
10

∆

If κ < n, then the rate factor of Sparse Proximal SAGA is 1/n. Any choice of a bigger than Ω(κ/n)
gives us the same rate factor for PROXASAGA. Since τ ≤
n/10 we can pick such an a without
violating the condition of Theorem 2.

√

23

Appendix D Comparison with related work

In this section, we relate our theoretical results and proof technique with the related literature.

Speedups. Our speedup regimes are comparable with the best ones obtained in the smooth case,
including Niu et al. (2011); Reddi et al. (2015), even though unlike these papers, we support incon-
sistent reads and nonsmooth objective functions. The one exception is Leblond et al. (2017), where
the authors prove that their algorithm, ASAGA, can obtain a linear speedup even without sparsity in
the well-conditioned regime. In contrast, PROXASAGA always requires some sparsity. Whether this
property for smooth objective functions could be extended to the composite case remains an open
problem.

Coordinate Descent. We compare our approach for composite objective functions to its most
natural competitor: ASYSPCD (Liu & Wright, 2015), an asynchronous stochastic coordinate descent
algorithm. While ASYSPCD also exhibits linear speedups, subject to a condition on τ , one has to be
especially careful when trying to compare these conditions.

First, while in theory the iterations of both algorithms have the same cost, in practice various tricks
are introduced to save on computation, yielding different costs per updates.9 Second, the bound on
τ for the coordinate descent algorithm depends on p, the dimensionality of the problem, whereas
ours involves n, the number of data points. Third, a more subtle issue is that τ is not affected by
the same quantities for both algorithms.10 See Appendix D.1 for a more detailed explanation of the
differences between the bounds.

√

∆) is necessary for a linear speedup. This means in case ∆ ≤ 1/√

In the best case scenario (where the components of the gradient are uncorrelated, a somewhat un-
√
realistic setting), ASYSPCD can get a near-linear speedup for τ as big as 4
p. Our result states that
τ = O(1/
p our bound is better
than the one obtained for ASYSPCD. Recalling that 1/n ≤ ∆ ≤ 1, it appears that PROXASAGA is
favored when n is bigger than
p whereas ASYSPCD may have a better bound otherwise, though
this comparison should be taken with a grain of salt given the assumptions we had to make to arrive
at comparable quantities.

√

Furthermore, one has to note that while Liu & Wright (2015) use the classical labeling scheme
inherited from Niu et al. (2011), they still assume in their proof that the it are uniformly distributed
and that their gradient estimators are conditionally unbiased – though neither property is veriﬁed in
the general asynchronous setting. Finally, we note that ASYSPCD (as well as its incremental variant
Async-PROXSVRCD) assumes that the computation and assignment of the proximal operator is an
atomic step, while we do not make such assumption.

SVRG. The Async-ProxSVRG algorithm of Meng et al. (2017) also exhibits theoretical linear
speedups subject to the same condition as ours. However, the analyzed algorithm uses dense updates
and consistent read and writes. Although they make the analysis easier, these two factors introduce
costly bottlenecks and prevent linear speedups in running time. Furthermore, here again the classical
labeling scheme is used together with the unveriﬁed conditional unbiasedness condition.

Doubly stochastic algorithms. The Async-PROXSVRCD algorithm from Meng et al. (2017); Gu
et al. (2016) has a maximum allowable stepsize11 that is in O(1/pL), whereas the maximum step
size for PROXASAGA is in Ω(1/L), so can be up to p times bigger. Consequently, PROXASAGA
enjoys much faster theoretical convergence rates. Unfortunately, we could not ﬁnd a condition for
linear speedups to compare to. We also note that their algorithm is not appropriate in a sparse
features setting. This is illustrated in an empirical comparison in Appendix F where we see that

9For PROXASAGA the relevant quantity becomes the average number of features per data point. For
In both cases the tricks involved are

ASYSPCD it is rather the average number of data points per feature.
not covered by the theory.

10To make sure τ is the same quantity for both algorithms, we have to assume that the iteration costs are

homogeneous.

11To the best of our understanding, noting that extracting an interpretable bound from the given theoretical
results was difﬁcult. Furthermore, it appears that the proof technique may still have signiﬁcant issues: for
example, the “fully lock-free” assumption of Gu et al. (2016) allows for overwrites, and is thus incompatible
with their framework of analysis, in particular their Eq. (8).

24

their convergence in number of iterations is orders of magnitude slower than appropriate algorithms
like SAGA or PROXASAGA.

Appendix D.1 Comparison of bounds with Liu & Wright (2015)

Iteration costs. For both PROXASAGA and ASYSPCD, the average cost of an iteration is O(nS)
(where S is the average support size).
In the case of PROXASAGA (see Algorithm 1), at each
iteration the most costly operation is the computation of α, while in the general case we need to
compute a full gradient for ASYSPCD.

In order to reduce these prohibitive computation costs, several tricks are introduced. Although they
lead to much improved empirical performance, it should be noted that in both cases these tricks are
not covered by the theory. In particular, the unbiasedness condition can be violated.

In the case of PROXASAGA, we store the average gradient term α in shared memory. The cost
of each iteration then becomes the size of the extended support of the partial gradient selected at
random at this iteration, hence it is in O(∆l), where ∆l := maxi=1..n |Ti|.

For ASYSPCD, following Peng et al. (2016) we can store intermediary quantities for speciﬁc losses
(e.g. (cid:96)1-regularized logistic regression). The cost of an iteration then becomes the number of data
points whose extended support includes the coordinate selected at random at this iteration, hence it
is in O(n∆).

The relative difference in update cost of both algorithms then depends heavily on the data matrix:
if the partial gradients usually have a extended support but coordinates belong to few of them (this
can be the case if n (cid:28) p for example), then the iterations of ASYSPCD can be cheaper than those of
PROXASAGA. Conversely, if data points usually have small extended support but coordinates belong
to many of them (which can happen when p (cid:28) n for example), then the updates of PROXASAGA
are the cheaper ones.

Dependency of τ on the data matrix.
In the case of PROXASAGA the sizes of the extended
support of each data point are important – they are directly linked to the cost of each iteration.
Identical iteration costs for each data point do not inﬂuence τ , whereas heterogeneous costs may
cause τ to increase substantially. In contrast, in the case of ASYSPCD, the relevant parts of the data
matrix are the number of data points each dimension touches – for much the same reason. In the
bipartite graph between data points and dimensions, either the left or the right degrees matter for τ ,
depending on which algorithm you choose.

In order to compare their respective bounds, we have to make the assumption that the iteration costs
are homogeneous, which means that each data point has the same support size and each dimension is
active in the same number of data points. This implies that τ is the same quantity for both algorithms.

√

Best case scenario bound for AsySPCD. The result obtained in Liu & Wright (2015) states that
if τ 2Λ = O(
p), ASYSPCD can get a near-linear speedup (where Λ is a measure of the interactions
p). In the best possible scenario where
between the components of the gradient, with 1 ≤ Λ ≤
Λ = 1 (which means that the coordinates of the gradients are completely uncorrelated), τ can be as
√
big as 4

√

p.

25

Appendix E Implementation details

Initialization.
In the Sparse Proximal SAGA algorithm and its asynchronous variant, PROXAS-
AGA, the vector x can be initialized arbitrarily. The memory terms αi can be initialized to any
vector that veriﬁes supp(αi) = supp(∇fi). In practice we found that the initialization αi = 0 is
very fast to set up and often outperforms more costly initializations.

With this initialization, the gradient approximation before the ﬁrst update of the memory terms be-
comes ∇fi(x) + Diα. Since most of the values in α are zero, α will tend to be small compared to
∇fi(x), and so the gradient estimate is very close to the SGD estimate ∇fi(x). The SGD approx-
imation is known to have a very fast initial convergence (which, in light of Figure 1, our method
inherits) and has even been used as a heuristic to use during the ﬁrst epoch of variance reduced
methods (Schmidt et al., 2016).

The initialization of coefﬁcients x0 was always set to zero.

Exact regularization. Computing the gradient of a smooth regularization such as the squared (cid:96)2
penalty of Eq. (6) is independent of n and so we can use the exact regularizer in the update of
the coefﬁcients instead of storing it in α, which would also destroy the compressed storage of the
memory terms described below. In practice we use this “exact regularization”, multiplied by Di to
preserve the sparsity pattern.
Assuming a squared (cid:96)2 regularization term of the form λ
(note the extra λx)

2 , the gradient estimate in (SPS) becomes

vi = ∇fi(x) − αi + Di(α + λx) .

(79)

Storage of memory terms. The storage requirements for this method is in the worst case a table
of size n × p. However, as for SAG and SAGA, for linearly parametrized loss functions of the form
fi(x) = (cid:96)(aT
i=1 are samples associated with
the learning problem, this can be reduced to a table of size n (Schmidt et al., 2016, §4.1). This
includes popular linear models such as least squares or logistic regression with (cid:96) the squared or
logistic function, respectively.

i x), where (cid:96) is some real-valued function and (ai)n

The reduce storage comes from the fact that in this case the partial gradients have the structure

∇fi(x) = ai (cid:96)(cid:48)(aT

.

i x)
(cid:124) (cid:123)(cid:122) (cid:125)
scalar

(80)

Since ai is independent of x, we only need to store the scalar (cid:96)(cid:48)(aT
explains why ∇fi inherits the sparsity pattern of ai.

i x). This decomposition also

Atomic updates. Most modern processors have support for atomic operations with minimal over-
head. In our case, we implemented a double-precision atomic type using the C++11 atomic features
(std::atomic<double>). This type implements atomic operations through the compare and swap
semantics.

Empirically, we have found it necessary to implement atomic operations at least in the vector α and
α to reach arbitrary precision. If non-atomic operations are used, the method converges only to a
limited precision (around normalized function suboptimality of 10−3), which might be sufﬁcient for
some machine learning applications but which we found not satisfying from an optimization point
of view.

AsySPCD. Following (Peng et al., 2016) we keep the vector (aT
at each iteration using atomic updates.

i x)n

i=1 in memory and update it

Hardware and software. All experiments were run on a Dell PowerEdge 920 machine with 4 Intel
Xeon E7-4830v2 processors with 10 2.2GHz cores each and 384GB 1600 Mhz RAM. The PROX-
ASAGAand ASYSPCD code was implemented on C++ and binded in Python. The FISTA code is
implemented in pure Python using NumPY and SciPy for matrix computations (in this case the bot-
tleneck is in large sparse matrix-vector operations for which efﬁcient BLAS routines were used). Our
PROXASAGA implementation can be downloaded from http://github.com/fabianp/ProxASAGA.

26

Appendix F Experiments

All datasets used for the experiments were downloaded from the LibSVM dataset suite.12

Appendix F.1 Comparison of ProxASAGA with other sequential methods

We provide a comparison between the Sparse Proximal SAGA and related methods in the sequential
case. We compare against two methods: the MRBCD method of Zhao et al. (2014) (which forms
the basis of Async-PROXSVRCD) and the vanilla implementation of SAGA (Defazio et al., 2014),
which does not have the ability to perform sparse updates. We compare in terms of both passes
through the data (epochs) and time. We use the same step size for all methods (1/3L). Due to
the slow convergence of some methods, we use a smaller dataset than the ones used in §4. Dataset
RCV1 has n = 697, 641, d = 47, 236 and a density of 0.15, while Covtype is a dense dataset with
n = 581, 012, d = 54.

Figure 2: Suboptimality of different sequential algorithms. Each marker represents one pass through
the dataset.

We observe that for the convergence behavior in terms of number of passes, Sparse Proximal SAGA
performs as well as vanilla SAGA, though the latter requires dense updates at every iteration (Fig. 2
top left). On the other hand, in terms of running time, our implementation of Sparse Proximal SAGA
is much more efﬁcient than the other methods for sparse input (Fig. 2 top right). In the case of dense
input (Fig. 2 bottom), the three methods perform similarly.
A note on the performance of MRBCD.
It may appear surprising that Sparse Proximal SAGA
outperforms MRBCD so dramatically on sparse datasets. However, one should note that MRBCD is
a doubly stochastic algorithm where both a random data point and a random coordinate are sampled
for each iteration. If the data matrix is very sparse, then the probability that the sampled coordinate
is in the support of the sampled data point becomes very low. This means that the gradient estimator
term only contains the reference gradient term of SVRG, which only changes once per epoch. As a
result, this estimator becomes very coarse and produces a slower empirical convergence.

This is reﬂected in the theoretical results given in Zhao et al. (2014), where the epoch size needed to
get linear convergence are k times bigger than the ones required by plain SVRG, where k is the size
of the set of blocks of coordinates.

12https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/

27

Appendix F.2 Theoretical speedups.

In the experimental section, we have shown experimental speedup results where suboptimality was
a function of the running time. This measure encompasses both theoretical algorithmic optimization
properties and hardware overheads (such as contention of shared memory) which are not taken into
account in our analysis.

In order to isolate these two effects, we now plot our speedup results in Figure 3 where suboptimality
is a function of the number of iterations; thus, we abstract away any potential hardware overhead. To
do so, we implement a global counter which is sparsely updated (every 100 iterations for example)
in order not to modify the asynchrony of the system. This counter is used only for plotting purposes
and is not needed otherwise. Speciﬁcally, we deﬁne the theoretical speedup as:

theoretical speedup := (number of cores)

number of iterations for sequential algorithm
total number of iterations for parallel algorithm

.

Figure 3: Theoretical optimization speedups for (cid:96)1+(cid:96)2-regularized logistic regression. Speedup
as measured by the number of iterations required to reach 10−5 suboptimality for PROXASAGA
and ASYSPCD. In FISTA the iterates are the same with different cores and so matches the “ideal”
speedup.

We see clearly that the theoretical speedups obtained by both PROXASAGAand ASYSPCD are linear
(i.e. ideal). As we observe worse results in running time, this means that the hardware overheads of
asynchronous methods are quite signiﬁcant.

Appendix F.3 Timing benchmarks

We now provide the time it takes for the different methods with 10 cores to reach a suboptimality of
10−10. All results are in hours.

Dataset

PROXASAGA ASYSPCD

FISTA

KDD 2010
KDD 2012
Criteo

1.01
0.09
0.14

13.3
26.6
33.3

5.2
8.3
6.6

Appendix F.4 Hyperparameters

The (cid:96)1-regularization parameter λ2 was chosen as to give around 10% of non-zero features. The
exact chosen values are the following: λ2 = 10−11 for KDD 2010, λ2 = 10−16 for KDD 2012 and
λ2 = 4 × 10−12 for Criteo.

28

7
1
0
2
 
v
o
N
 
5
 
 
]

C
O
.
h
t
a
m

[
 
 
3
v
8
6
4
6
0
.
7
0
7
1
:
v
i
X
r
a

Breaking the Nonsmooth Barrier: A Scalable Parallel
Method for Composite Optimization

Fabian Pedregosa
INRIA/ENS∗
Paris, France

R´emi Leblond
INRIA/ENS∗
Paris, France

Simon Lacoste-Julien
MILA and DIRO
Universit´e de Montr´eal, Canada

Abstract

Due to their simplicity and excellent performance, parallel asynchronous variants
of stochastic gradient descent have become popular methods to solve a wide range
of large-scale optimization problems on multi-core architectures. Yet, despite their
practical success, support for nonsmooth objectives is still lacking, making them
unsuitable for many problems of interest in machine learning, such as the Lasso,
group Lasso or empirical risk minimization with convex constraints. In this work,
we propose and analyze PROXASAGA, a fully asynchronous sparse method in-
spired by SAGA, a variance reduced incremental gradient algorithm. The proposed
method is easy to implement and signiﬁcantly outperforms the state of the art on
several nonsmooth, large-scale problems. We prove that our method achieves a
theoretical linear speedup with respect to the sequential version under assump-
tions on the sparsity of gradients and block-separability of the proximal term.
Empirical benchmarks on a multi-core architecture illustrate practical speedups of
up to 12x on a 20-core machine.

1

Introduction

The widespread availability of multi-core computers motivates the development of parallel methods
adapted for these architectures. One of the most popular approaches is HOGWILD (Niu et al., 2011),
an asynchronous variant of stochastic gradient descent (SGD). In this algorithm, multiple threads run
the update rule of SGD asynchronously in parallel. As SGD, it only requires visiting a small batch
of random examples per iteration, which makes it ideally suited for large scale machine learning
problems. Due to its simplicity and excellent performance, this parallelization approach has recently
been extended to other variants of SGD with better convergence properties, such as SVRG (Johnson
& Zhang, 2013) and SAGA (Defazio et al., 2014).

Despite their practical success, existing parallel asynchronous variants of SGD are limited to smooth
objectives, making them inapplicable to many problems in machine learning and signal processing.
In this work, we develop a sparse variant of the SAGA algorithm and consider its parallel asyn-
chronous variants for general composite optimization problems of the form:

arg min
x∈Rp

f (x) + h(x)

, with f (x) := 1
n

(cid:80)n

i=1 fi(x)

,

(OPT)

where each fi is convex with L-Lipschitz gradient, the average function f is µ-strongly convex and
h is convex but potentially nonsmooth. We further assume that h is “simple” in the sense that we
have access to its proximal operator, and that it is block-separable, that is, it can be decomposed
block coordinate-wise as h(x) = (cid:80)
B∈BhB([x]B), where B is a partition of the coefﬁcients into

∗DI ´Ecole normale sup´erieure, CNRS, PSL Research University

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

subsets which will call blocks and hB only depends on coordinates in block B. Note that there is
no loss of generality in this last assumption as a unique block covering all coordinates is a valid
partition, though in this case, our sparse variant of the SAGA algorithm reduces to the original SAGA
algorithm and no gain from sparsity is obtained.

This template models a broad range of problems arising in machine learning and signal processing:
the ﬁnite-sum structure of f includes the least squares or logistic loss functions; the proximal term
h includes penalties such as the (cid:96)1 or group lasso penalty. Furthermore, this term can be extended-
valued, thus allowing for convex constraints through the indicator function.

Contributions. This work presents two main contributions. First, in §2 we describe Sparse Proxi-
mal SAGA, a novel variant of the SAGA algorithm which features a reduced cost per iteration in the
presence of sparse gradients and a block-separable penalty. Like other variance reduced methods, it
enjoys a linear convergence rate under strong convexity. Second, in §3 we present PROXASAGA, a
lock-free asynchronous parallel version of the aforementioned algorithm that does not require con-
sistent reads. Our main results states that PROXASAGA obtains (under assumptions) a theoretical
linear speedup with respect to its sequential version. Empirical benchmarks reported in §4 show that
this method dramatically outperforms state-of-the-art alternatives on large sparse datasets, while the
empirical speedup analysis illustrates the practical gains as well as its limitations.

1.1 Related work

Asynchronous coordinate-descent. For composite objective functions of the form (OPT), most of
the existing literature on asynchronous optimization has focused on variants of coordinate descent.
Liu & Wright (2015) proposed an asynchronous variant of (proximal) coordinate descent and proved
a near-linear speedup in the number of cores used, given a suitable step size. This approach has been
recently extended to general block-coordinate schemes by Peng et al. (2016), to greedy coordinate-
descent schemes by You et al. (2016) and to non-convex problems by Davis et al. (2016). However,
as illustrated by our experiments, in the large sample regime coordinate descent compares poorly
against incremental gradient methods like SAGA.

Variance reduced incremental gradient and their asynchronous variants.
Initially proposed in
the context of smooth optimization by Le Roux et al. (2012), variance reduced incremental gradient
methods have since been extended to minimize composite problems of the form (OPT) (see table
below). Smooth variants of these methods have also recently been extended to the asynchronous set-
ting, where multiple threads run the update rule asynchronously and in parallel. Interestingly, none
of these methods achieve both simultaneously, i.e. asynchronous optimization of composite prob-
lems. Since variance reduced incremental gradient methods have shown state of the art performance
in both settings, this generalization is of key practical interest.

Objective

Smooth

Composite

Sequential Algorithm
SVRG (Johnson & Zhang, 2013)
SDCA (Shalev-Shwartz & Zhang, 2013)
SAGA (Defazio et al., 2014)
PROXSDCA (Shalev-Shwartz et al., 2012)
SAGA (Defazio et al., 2014)
ProxSVRG (Xiao & Zhang, 2014)

Asynchronous Algorithm

SVRG (Reddi et al., 2015)
PASSCODE (Hsieh et al., 2015, SDCA variant)
ASAGA (Leblond et al., 2017, SAGA variant)

This work: PROXASAGA

On the difﬁculty of a composite extension. Two key issues explain the paucity in the develop-
ment of asynchronous incremental gradient methods for composite optimization. The ﬁrst issue
is related to the design of such algorithms. Asynchronous variants of SGD are most competitive
when the updates are sparse and have a small overlap, that is, when each update modiﬁes a small
and different subset of the coefﬁcients. This is typically achieved by updating only coefﬁcients for
which the partial gradient at a given iteration is nonzero,2 but existing schemes such as the lagged
updates technique (Schmidt et al., 2016) are not applicable in the asynchronous setting. The second

2Although some regularizers are sparsity inducing, large scale datasets are often extremely sparse and lever-

aging this property is crucial for the efﬁciency of the method.

2

difﬁculty is related to the analysis of such algorithms. All convergence proofs crucially use the Lip-
schitz condition on the gradient to bound the noise terms derived from asynchrony. However, in the
composite case, the gradient mapping term (Beck & Teboulle, 2009), which replaces the gradient
in proximal-gradient methods, does not have a bounded Lipschitz constant. Hence, the traditional
proof technique breaks down in this scenario.

Other approaches. Recently, Meng et al. (2017); Gu et al. (2016) independently proposed a dou-
bly stochastic method to solve the problem at hand. Following Meng et al. (2017) we refer to it
as Async-PROXSVRCD. This method performs coordinate descent-like updates in which the true
gradient is replaced by its SVRG approximation. It hence features a doubly-stochastic loop: at each
iteration we select a random coordinate and a random sample. Because the selected coordinate block
is uncorrelated with the chosen sample, the algorithm can be orders of magnitude slower than SAGA
in the presence of sparse gradients. Appendix F contains a comparison of these methods.

1.2 Deﬁnitions and notations

By convention, we denote vectors and vector-valued functions in lowercase boldface (e.g. x) and
matrices in uppercase boldface (e.g. D). The proximal operator of a convex lower semicontinuous
function h is deﬁned as proxh(x) := arg minz∈Rp {h(z) + 1
2 (cid:107)x − z(cid:107)2}. A function f is said to be
L-smooth if it is differentiable and its gradient is L-Lipschitz continuous. A function f is said to be
µ-strongly convex if f − µ
2 (cid:107) · (cid:107)2 is convex. We use the notation κ := L/µ to denote the condition
number for an L-smooth and µ-strongly convex function.3
I p denotes the p-dimensional identity matrix, 1{cond} the characteristic function, which is 1 if cond
evaluates to true and 0 otherwise. The average of a vector or matrix is denoted α := 1
i=1 αi.
n
We use (cid:107) · (cid:107) for the Euclidean norm. For a positive semi-deﬁnite matrix D, we deﬁne its associated
distance as (cid:107)x(cid:107)2
D := (cid:104)x, Dx(cid:105). We denote by [ x ]b the b-th coordinate in x. This notation is
overloaded so that for a collection of blocks T = {B1, B2, . . .}, [x]T denotes the vector x restricted
to the coordinates in the blocks of T . For convenience, when T consists of a single block B we use
[x]B as a shortcut of [x]{B}. Finally, we distinguish E, the full expectation taken with respect to
all the randomness in the system, from E, the conditional expectation of a random it (the random
feature sampled at each iteration by SGD-like algorithms) conditioned on all the “past”, which the
context will clarify.

(cid:80)n

2 Sparse Proximal SAGA

Original SAGA algorithm. The original SAGA algorithm (Defazio et al., 2014) maintains two
moving quantities: the current iterate x and a table (memory) of historical gradients (αi)n
i=1. At
every iteration, it samples an index i ∈ {1, . . . , n} uniformly at random, and computes the next
iterate (x+, α+) according to the following recursion:
ui = ∇fi(x) − αi + α ; x+ = proxγh

(1)
On each iteration, this update rule requires to visit all coefﬁcients even if the partial gradients ∇fi are
sparse. Sparse partial gradients arise in a variety of practical scenarios: for example, in generalized
linear models the partial gradients inherit the sparsity pattern of the dataset. Given that large-scale
datasets are often sparse,4 leveraging this sparsity is crucial for the success of the optimizer.

i = ∇fi(x) .

(cid:0)x − γui

(cid:1); α+

Sparse Proximal SAGA algorithm. We will now describe an algorithm that leverages sparsity
in the partial gradients by only updating those blocks that intersect with the support of the partial
gradients. Since in this update scheme some blocks might appear more frequently than others, we
will need to counterbalance this undersirable effect with a well-chosen block-wise reweighting of
the average gradient and the proximal term.

In order to make precise this block-wise reweighting, we deﬁne the following quantities. We denote
by Ti the extended support of ∇fi, which is the set of blocks that intersect the support of ∇fi,

3Since we have assumed that each individual fi is L-smooth, f itself is L-smooth – but it could have a

smaller smoothness constant. Our rates are in terms of this bigger L/µ, as is standard in the SAGA literature.

4For example, in the LibSVM datasets suite, 8 out of the 11 datasets (as of May 2017) with more than a

million samples have a density between 10−4 and 10−6.

3

i

formally deﬁned as Ti := {B : supp(∇fi) ∩ B (cid:54)= ∅, B ∈ B}. For totally separable penalties such
as the (cid:96)1 norm, the blocks are individual coordinates and so the extended support covers the same
coordinates as the support. Let dB := n/nB, where nB := (cid:80)
1{B ∈ Ti} is the number of times
that B ∈ Ti. For simplicity we assume nB > 0, as otherwise the problem can be reformulated
without block B. The update rule in (1) requires computing the proximal operator of h, which
involves a full pass on the coordinates. In our proposed algorithm, we replace h in (1) with the
function ϕi(x) := (cid:80)
dBhB(x), whose form is justiﬁed by the following three properties.
First, this function is zero outside Ti, allowing for sparse updates. Second, because of the block-wise
reweighting dB, the function ϕi is an unbiased estimator of h (i.e., E ϕi = h), property which will
be crucial to prove the convergence of the method. Third, ϕi inherits the block-wise structure of h
and its proximal operator can be computed from that of h as [proxγϕi(x)]B = [prox(dB γ)hB (x)]B
if B ∈ Ti and [proxγϕi(x)]B = [x]B otherwise. Following Leblond et al. (2017), we will also
replace the dense gradient estimate ui by the sparse estimate vi := ∇fi(x) − αi + Diα, where
Di is the diagonal matrix deﬁned block-wise as [Di]B,B = dB1{B ∈ Ti}I |B|. It is easy to verify
that the vector Diα is a weighted projection onto the support of Ti and E Diα = α, making vi an
unbiased estimate of the gradient.

B∈Ti

We now have all necessary elements to describe the Sparse Proximal SAGA algorithm. As the
original SAGA algorithm, it maintains two moving quantities: the current iterate x ∈ Rp and a
i=1, αi ∈ Rp. At each iteration, the algorithm samples an index
table of historical gradients (αi)n
i ∈ {1, . . . , n} and computes the next iterate (x+, α+) as:

vi = ∇fi(x) − αi + Diα ; x+ = proxγϕi

(cid:0)x − γvi

(cid:1) ; α+

i = ∇fi(x) ,

(SPS)

where in a practical implementation the vector α is updated incrementally at each iteration.

The above algorithm is sparse in the sense that it only requires to visit and update blocks in the
extended support: if B /∈ Ti, by the sparsity of vi and proxϕi, we have [x+]B = [x]B. Hence,
when the extended support Ti is sparse, this algorithm can be orders of magnitude faster than the
naive SAGA algorithm. The extended support is sparse for example when the partial gradients are
sparse and the penalty is separable, as is the case of the (cid:96)1 norm or the indicator function over a
hypercube, or when the the penalty is block-separable in a way such that only a small subset of the
blocks overlap with the support of the partial gradients. Initialization of variables and a reduced
storage scheme for the memory are discussed in the implementation details section of Appendix E.

Relationship with existing methods. This algorithm can be seen as a generalization of both the
Standard SAGA algorithm and the Sparse SAGA algorithm of Leblond et al. (2017). When the
proximal term is not block-separable, then dB = 1 (for a unique block B) and the algorithm defaults
to the Standard (dense) SAGA algorithm. In the smooth case (i.e., h = 0), the algorithm defaults to
the Sparse SAGA method. Hence we note that the sparse gradient estimate vi in our algorithm is the
same as the one proposed in Leblond et al. (2017). However, we emphasize that a straightforward
combination of this sparse update rule with the proximal update from the Standard SAGA algorithm
results in a nonconvergent algorithm: the block-wise reweighting of h is a surprisingly simple but
crucial change. We now give the convergence guarantees for this algorithm.
Theorem 1. Let γ = a
SAGA converges geometrically in expectation with a rate factor of at least ρ = 1
is, for xt obtained after t updates, we have the following bound:

5L for any a ≤ 1 and f be µ-strongly convex (µ > 0). Then Sparse Proximal
κ }. That

5 min{ 1

n , a 1

E(cid:107)xt − x∗(cid:107)2 ≤ (1 − ρ)tC0 , with C0 := (cid:107)x0 − x∗(cid:107)2 + 1
5L2

(cid:80)n

i=1 (cid:107)α0

i − ∇fi(x∗)(cid:107)2

.

Remark. For the step size γ = 1/5L, the convergence rate is (1 − 1/5 min{1/n, 1/κ}). We can thus
identify two regimes: the “big data” regime, n ≥ κ, in which the rate factor is bounded by 1/5n, and
the “ill-conditioned” regime, κ ≥ n, in which the rate factor is bounded by 1/5κ. This rate roughly
matches the rate obtained by Defazio et al. (2014). While the step size bound of 1/5L is slightly
smaller than the 1/3L one obtained in that work, this can be explained by their stronger assumptions:
each fi is strongly convex whereas they are strongly convex only on average in this work. All proofs
for this section can be found in Appendix B.

4

i=1

j=1[ ˆαj ]Ti

ˆx = inconsistent read of x
ˆα = inconsistent read of α
Sample i uniformly in {1, ..., n}
Si := support of ∇fi
Ti := extended support of ∇fi in B
[ α ]Ti = 1/n (cid:80)n
[ δα ]Si = [∇fi(ˆx)]Si − [ ˆαi]Si
[ ˆv ]Ti = [ δα ]Ti + [Diα ]Ti
[ δx ]Ti = [proxγϕi(ˆx − γ ˆv)]Ti − [ˆx]Ti
for B in Ti do
for b ∈ B do

Algorithm 1 PROXASAGA (analyzed)
1: Initialize shared variables x and (αi)n
2: keep doing in parallel
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end parallel loop

end for
// (‘←’ denotes shared memory update.)

[ x ]b ← [ x ]b + [ δx ]b
if b ∈ Si then

[αi]b ← [∇fi(ˆx)]b

end if
end for

(cid:46) atomic

i=1, α

Sample i uniformly in {1, ..., n}
Si := support of ∇fi
Ti := extended support of ∇fi in B
[ ˆx ]Ti = inconsistent read of x on Ti
ˆαi = inconsistent read of αi
[ α ]Ti = inconsistent read of α on Ti
[ δα ]Si = [∇fi(ˆx)]Si − [ ˆαi]Si
[ ˆv ]Ti = [δα ]Ti + [ Diα ]Ti
[ δx ]Ti = [proxγϕi(ˆx − γ ˆv)]Ti − [ˆx]Ti
for B in Ti do
for b in B do

Algorithm 2 PROXASAGA (implemented)
1: Initialize shared variables x, (αi)n
2: keep doing in parallel
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
end for
19:
20: αi ← ∇fi(ˆx)
21: end parallel loop

[ x ]b ← [ x ]b + [ δx ]b
if b ∈ Si then

end if
end for

[ α ]b ← [α]b + 1/n[δα]b (cid:46) atomic

(scalar update) (cid:46) atomic

(cid:46) atomic

3 Asynchronous Sparse Proximal SAGA

We introduce PROXASAGA – the asynchronous parallel variant of Sparse Proximal SAGA. In this
algorithm, multiple cores update a central parameter vector using the Sparse Proximal SAGA intro-
duced in the previous section, and updates are performed asynchronously. The algorithm parameters
are read and written without vector locks, i.e., the vector content of the shared memory can poten-
tially change while a core is reading or writing to main memory coordinate by coordinate. These
operations are typically called inconsistent (at the vector level).

The full algorithm is described in Algorithm 1 for its theoretical version (on which our analysis
is built) and in Algorithm 2 for its practical implementation. The practical implementation differs
from the analyzed agorithm in three points. First, in the implemented algorithm, index i is sampled
before reading the coefﬁcients to minimize memory access since only the extended support needs to
be read. Second, since our implementation targets generalized linear models, the memory αi can be
compressed into a single scalar in L20 (see Appendix E). Third, α is stored in memory and updated
incrementally instead of recomputed at each iteration.

The rest of the section is structured as follows: we start by describing our framework of analysis; we
then derive essential properties of PROXASAGA along with a classical delay assumption. Finally,
we state our main convergence and speedup result.

3.1 Analysis framework

As in most of the recent asynchronous optimization literature, we build on the hardware model in-
troduced by Niu et al. (2011), with multiple cores reading and writing to a shared memory parameter
vector. These operations are asynchronous (lock-free) and inconsistent:5 ˆxt, the local copy of the
parameters of a given core, does not necessarily correspond to a consistent iterate in memory.

“Perturbed” iterates. To handle this additional difﬁculty, contrary to most contributions in this
ﬁeld, we choose the “perturbed iterate framework” proposed by Mania et al. (2017) and reﬁned
by Leblond et al. (2017). This framework can analyze variants of SGD which obey the update rule:

xt+1 = xt − γv(xt, it) , where v veriﬁes the unbiasedness condition E v(x, it) = ∇f (x)

5This is an extension of the framework of Niu et al. (2011), where consistent updates were assumed.

5

and the expectation is computed with respect to it. In the asynchronous parallel setting, cores are
reading inconsistent iterates from memory, which we denote ˆxt. As these inconsistent iterates are
affected by various delays induced by asynchrony, they cannot easily be written as a function of
their previous iterates. To alleviate this issue, Mania et al. (2017) choose to introduce an additional
quantity for the purpose of the analysis:
xt+1 := xt − γv(ˆxt, it) ,

(2)
Note that this equation is the deﬁnition of this new quantity xt. This virtual iterate is useful for the
convergence analysis and makes for much easier proofs than in the related literature.

the “virtual iterate” – which is never actually computed .

“After read” labeling. How we choose to deﬁne the iteration counter t to label an iterate xt
matters in the analysis.
In this paper, we follow the “after read” labeling proposed in Leblond
et al. (2017), in which we update our iterate counter, t, as each core ﬁnishes reading its copy of
the parameters (in the speciﬁc case of PROXASAGA, this includes both ˆxt and ˆαt). This means
that ˆxt is the (t + 1)th fully completed read. One key advantage of this approach compared to the
classical choice of Niu et al. (2011) – where t is increasing after each successful update – is that
it guarantees both that the it are uniformly distributed and that it and ˆxt are independent. This
property is not veriﬁed when using the “after write” labeling of Niu et al. (2011), although it is still
implicitly assumed in the papers using this approach, see Leblond et al. (2017, Section 3.2) for a
discussion of issues related to the different labeling schemes.

Generalization to composite optimization. Although the perturbed iterate framework was de-
signed for gradient-based updates, we can extend it to proximal methods by remarking that in the
sequential setting, proximal stochastic gradient descent and its variants can be characterized by the
following similar update rule:

xt+1 = xt − γg(xt, vit, it) , with g(x, v, i) := 1
γ

(cid:0)x − proxγϕi(x − γv)(cid:1) ,

where as before v veriﬁes the unbiasedness condition E v = ∇f (x). The Proximal Sparse SAGA
iteration can be easily written within this template by using ϕi and vi as deﬁned in §2. Using this
deﬁnition of g, we can deﬁne PROXASAGA virtual iterates as:

xt+1 := xt − γg(ˆxt, ˆvt
it

, it) , with ˆvt
it

= ∇fit (ˆxt) − ˆαt
it

+ Dit αt

,

(3)

(4)

where as in the sequential case, the memory terms are updated as ˆαt
it
analysis of PROXASAGA will be based on this deﬁnition of the virtual iterate xt+1.

= ∇fit(ˆxt). Our theoretical

3.2 Properties and assumptions

Now that we have introduced the “after read” labeling for proximal methods in Eq. (4), we can
leverage the framework of Leblond et al. (2017, Section 3.3) to derive essential properties for the
analysis of PROXASAGA. We describe below three useful properties arising from the deﬁnition
of Algorithm 1, and then state a central (but standard) assumption that the delays induced by the
asynchrony are uniformly bounded.

Independence: Due to the “after read” global ordering, ir is independent of ˆxt for all r ≥ t. We
enforce the independence for r = t by having the cores read all the shared parameters before their
iterations.
Unbiasedness: The term ˆvt
consequence of the independence between it and ˆxt.

it is an unbiased estimator of the gradient of f at ˆxt. This property is a

Atomicity: The shared parameter coordinate update of [x]b on Line 14 is atomic. This means that
there are no overwrites for a single coordinate even if several cores compete for the same resources.
Most modern processors have support for atomic operations with minimal overhead.

Bounded overlap assumption. We assume that there exists a uniform bound, τ , on the maximum
number of overlapping iterations. This means that every coordinate update from iteration t is suc-
cessfully written to memory before iteration t + τ + 1 starts. Our result will give us conditions on τ
to obtain linear speedups.

Bounding ˆxt − xt. The delay assumption of the previous paragraph allows to express the difference
between real and virtual iterate using the gradient mapping gu := g(ˆxu, ˆvu
iu
ˆxt−xt = γ (cid:80)t−1

u are p × p diagonal matrices with terms in {0, +1}. (5)

ugu , where Gt

, iu) as:

Gt

u=(t−τ )+

6

0 represents instances where both ˆxu and xu have received the corresponding updates. +1, on
the contrary, represents instances where ˆxu has not yet received an update that is already in xu by
deﬁnition. This bound will prove essential to our analysis.

3.3 Analysis

In this section, we state our convergence and speedup results for PROXASAGA. The full details
of the analysis can be found in Appendix C. Following Niu et al. (2011), we introduce a sparsity
measure (generalized to the composite setting) that will appear in our results.
Deﬁnition 1. Let ∆ := maxB∈B |{i : Ti (cid:51) B}|/n. This is the normalized maximum number of
times that a block appears in the extended support. For example, if a block is present in all Ti, then
∆ = 1. If no two Ti share the same block, then ∆ = 1/n. We always have 1/n ≤ ∆ ≤ 1.
Theorem 2 (Convergence guarantee of PROXASAGA). Suppose τ ≤ 1
√
10
γ = a
in expectation at a geometric rate factor of at least: ρ(a) = 1
(1 − ρ)t ˜C0, where ˜C0 is a constant independent of t (≈ nκ

. For any step size
τ }, the inconsistent read iterates of Algorithm 1 converge
(cid:9), i.e. E(cid:107)ˆxt − x∗(cid:107)2 ≤

L with a ≤ a∗(τ ) := 1

a C0 with C0 as deﬁned in Theorem 1).

36 min{1, 6κ

5 min (cid:8) 1

n , a 1

∆

κ

√

This last result is similar to the original SAGA convergence result and our own Theorem 1, with both
an extra condition on τ and on the maximum allowable step size. In the best sparsity case, ∆ = 1/n
and we get the condition τ ≤
n/10. We now compare the geometric rate above to the one of Sparse
Proximal SAGA to derive the necessary conditions under which PROXASAGA is linearly faster.
Corollary 1 (Speedup). Suppose τ ≤ 1
. If κ ≥ n, then using the step size γ = 1/36L, PROXAS-
√
10
AGA converges geometrically with rate factor Ω( 1
κ ). If κ < n, then using the step size γ = 1/36nµ,
PROXASAGA converges geometrically with rate factor Ω( 1
n ). In both cases, the convergence rate
is the same as Sparse Proximal SAGA. Thus PROXASAGA is linearly faster than its sequential
counterpart up to a constant factor. Note that in both cases the step size does not depend on τ .

∆

Furthermore, if τ ≤ 6κ, we can use a universal step size of Θ(1/L) to get a similar rate for PROX-
ASAGA than Sparse Proximal SAGA, thus making it adaptive to local strong convexity since the
knowledge of κ is not required.

These speedup regimes are comparable with the best ones obtained in the smooth case, including Niu
et al. (2011); Reddi et al. (2015), even though unlike these papers, we support inconsistent reads
and nonsmooth objective functions. The one exception is Leblond et al. (2017), where the authors
prove that their algorithm, ASAGA, can obtain a linear speedup even without sparsity in the well-
conditioned regime. In contrast, PROXASAGA always requires some sparsity. Whether this property
for smooth objective functions could be extended to the composite case remains an open problem.

√

Relative to ASYSPCD, in the best case scenario (where the components of the gradient are uncorre-
√
lated, a somewhat unrealistic setting), ASYSPCD can get a near-linear speedup for τ as big as 4
p.
∆) is necessary for a linear speedup. This means in case ∆ ≤ 1/√
Our result states that τ = O(1/
p
our bound is better than the one obtained for ASYSPCD. Recalling that 1/n ≤ ∆ ≤ 1, it appears
that PROXASAGA is favored when n is bigger than
p whereas ASYSPCD may have a better bound
otherwise, though this comparison should be taken with a grain of salt given the assumptions we
had to make to arrive at comparable quantities. An extended comparison with the related work can
be found in Appendix D.

√

4 Experiments

In this section, we compare PROXASAGA with related methods on different datasets. Although
PROXASAGA can be applied more broadly, we focus on (cid:96)1 + (cid:96)2-regularized logistic regression, a
model of particular practical importance. The objective function takes the form

1
n

n
(cid:88)

i=1

log (cid:0)1 + exp(−bia

(cid:124)

i x)(cid:1) + λ1

2 (cid:107)x(cid:107)2

2 + λ2(cid:107)x(cid:107)1

,

(6)

where ai ∈ Rp and bi ∈ {−1, +1} are the data samples. Following Defazio et al. (2014), we set
λ1 = 1/n. The amount of (cid:96)1 regularization (λ2) is selected to give an approximate 1/10 nonzero

7

Table 1: Description of datasets.

Dataset

n

p

KDD 2010 (Yu et al., 2010)
KDD 2012 (Juan et al., 2016)
Criteo (Juan et al., 2016)

19,264,097
149,639,105
45,840,617

1,163,024
54,686,452
1,000,000

density
10−6
2 × 10−7
4 × 10−5

L

28.12
1.25
1.25

∆

0.15
0.85
0.89

Figure 1: Convergence for asynchronous stochastic methods for (cid:96)1 + (cid:96)2-regularized logistic
regression. Top: Suboptimality as a function of time for different asynchronous methods using 1
and 10 cores. Bottom: Running time speedup as function of the number of cores. PROXASAGA
achieves signiﬁcant speedups over its sequential version while being orders of magnitude faster than
competing methods. ASYSPCD achieves the highest speedups but it also the slowest overall method.

coefﬁcients. Implementation details are available in Appendix E. We chose the 3 datasets described
in Table 1

Results. We compare three parallel asynchronous methods on the aforementioned datasets: PROX-
ASAGA (this work),6 ASYSPCD, the asynchronous proximal coordinate descent method of Liu &
Wright (2015) and the (synchronous) FISTA algorithm (Beck & Teboulle, 2009), in which the gra-
dient computation is parallelized by splitting the dataset into equal batches. We aim to benchmark
these methods in the most realistic scenario possible; to this end we use the following step size:
1/2L for PROXASAGA, 1/Lc for ASYSPCD, where Lc is the coordinate-wise Lipschitz constant
of the gradient, while FISTA uses backtracking line-search. The results can be seen in Figure 1
(top) with both one (thus sequential) and ten processors. Two main observations can be made from
this ﬁgure. First, PROXASAGA is signiﬁcantly faster on these problems. Second, its asynchronous
version offers a signiﬁcant speedup over its sequential counterpart.

In Figure 1 (bottom) we present speedup with respect to the number of cores, where speedup is
computed as the time to achieve a suboptimality of 10−10 with one core divided by the time to
achieve the same suboptimality using several cores. While our theoretical speedups (with respect
to the number of iterations) are almost linear as our theory predicts (see Appendix F), we observe
a different story for our running time speedups. This can be attributed to memory access overhead,
which our model does not take into account. As predicted by our theoretical results, we observe

6A reference C++/Python implementation of is available at https://github.com/fabianp/ProxASAGA

8

a high correlation between the ∆ dataset sparsity measure and the empirical speedup: KDD 2010
(∆ = 0.15) achieves a 11x speedup, while in Criteo (∆ = 0.89) the speedup is never above 6x.

Note that although competitor methods exhibit similar or sometimes better speedups, they remain
orders of magnitude slower than PROXASAGA in running time for large sparse problems. In fact,
our method is between 5x and 80x times faster (in time to reach 10−10 suboptimality) than FISTA
and between 13x and 290x times faster than ASYSPCD (see Appendix F.3).

5 Conclusion and future work

In this work, we have described PROXASAGA, an asynchronous variance reduced algorithm with
support for composite objective functions. This method builds upon a novel sparse variant of the
(proximal) SAGA algorithm that takes advantage of sparsity in the individual gradients. We have
proven that this algorithm is linearly convergent under a condition on the step size and that it is
linearly faster than its sequential counterpart given a bound on the delay. Empirical benchmarks
show that PROXASAGA is orders of magnitude faster than existing state-of-the-art methods.

This work can be extended in several ways. First, we have focused on the SAGA method as the basic
iteration loop, but this approach can likely be extended to other proximal incremental schemes such
as SGD or ProxSVRG. Second, as mentioned in §3.3, it is an open question whether it is possible to
obtain convergence guarantees without any sparsity assumption, as was done for ASAGA.

Acknowledgements

The authors would like to thank our colleagues Damien Garreau, Robert Gower, Thomas Ker-
dreux, Geoffrey Negiar and Konstantin Mishchenko for their feedback on this manuscript, and Jean-
Baptiste Alayrac for support managing the computational resources.

This work was partially supported by a Google Research Award. FP acknowledges support from the
chaire ´Economie des nouvelles donn´ees with the data science joint research initiative with the fonds
AXA pour la recherche.

References

Bauschke, Heinz and Combettes, Patrick L. Convex analysis and monotone operator theory in

Hilbert spaces. Springer, 2011.

Beck, Amir and Teboulle, Marc. Gradient-based algorithms with applications to signal recovery.

Convex Optimization in Signal Processing and Communications, 2009.

Davis, Damek, Edmunds, Brent, and Udell, Madeleine. The sound of APALM clapping: faster
nonsmooth nonconvex optimization with stochastic asynchronous PALM. In Advances in Neural
Information Processing Systems 29, 2016.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural Infor-
mation Processing Systems, 2014.

Gu, Bin, Huo, Zhouyuan, and Huang, Heng. Asynchronous stochastic block coordinate descent

with variance reduction. arXiv preprint arXiv:1610.09447v3, 2016.

Hsieh, Cho-Jui, Yu, Hsiang-Fu, and Dhillon, Inderjit S. PASSCoDe: parallel asynchronous stochas-

tic dual coordinate descent. In ICML, 2015.

Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems, 2013.

Juan, Yuchin, Zhuang, Yong, Chin, Wei-Sheng, and Lin, Chih-Jen. Field-aware factorization ma-
chines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Sys-
tems. ACM, 2016.

9

Le Roux, Nicolas, Schmidt, Mark, and Bach, Francis R. A stochastic gradient method with an ex-
ponential convergence rate for ﬁnite training sets. In Advances in Neural Information Processing
Systems, 2012.

Leblond, R´emi, Pedregosa, Fabian, and Lacoste-Julien, Simon. ASAGA: asynchronous parallel
SAGA. Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS 2017), 2017.

Liu, Ji and Wright, Stephen J. Asynchronous stochastic coordinate descent: Parallelism and conver-

gence properties. SIAM Journal on Optimization, 2015.

Mania, Horia, Pan, Xinghao, Papailiopoulos, Dimitris, Recht, Benjamin, Ramchandran, Kannan,
and Jordan, Michael I. Perturbed iterate analysis for asynchronous stochastic optimization. SIAM
Journal on Optimization, 2017.

Meng, Qi, Chen, Wei, Yu, Jingcheng, Wang, Taifeng, Ma, Zhi-Ming, and Liu, Tie-Yan. Asyn-
chronous stochastic proximal optimization algorithms with variance reduction. In AAAI, 2017.

Nesterov, Yurii. Introductory lectures on convex optimization. Springer Science & Business Media,

Nesterov, Yurii. Gradient methods for minimizing composite functions. Mathematical Program-

2004.

ming, 2013.

Niu, Feng, Recht, Benjamin, Re, Christopher, and Wright, Stephen. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Sys-
tems, 2011.

Peng, Zhimin, Xu, Yangyang, Yan, Ming, and Yin, Wotao. ARock: an algorithmic framework for

asynchronous parallel coordinate updates. SIAM Journal on Scientiﬁc Computing, 2016.

Reddi, Sashank J, Hefny, Ahmed, Sra, Suvrit, Poczos, Barnabas, and Smola, Alexander J. On
variance reduction in stochastic gradient descent and its asynchronous variants. In Advances in
Neural Information Processing Systems, 2015.

Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Minimizing ﬁnite sums with the stochastic

average gradient. Mathematical Programming, 2016.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research, 2013.

Shalev-Shwartz, Shai et al.
arXiv:1211.2717, 2012.

Proximal stochastic dual coordinate ascent.

arXiv preprint

Xiao, Lin and Zhang, Tong. A proximal stochastic gradient method with progressive variance re-

duction. SIAM Journal on Optimization, 2014.

You, Yang, Lian, Xiangru, Liu, Ji, Yu, Hsiang-Fu, Dhillon, Inderjit S, Demmel, James, and Hsieh,
Cho-Jui. Asynchronous parallel greedy coordinate descent. In Advances In Neural Information
Processing Systems, 2016.

Yu, Hsiang-Fu, Lo, Hung-Yi, Hsieh, Hsun-Ping, Lou, Jing-Kai, McKenzie, Todd G, Chou, Jung-
Wei, Chung, Po-Han, Ho, Chia-Hua, Chang, Chun-Fu, Wei, Yin-Hsuan, et al. Feature engineering
and classiﬁer ensemble for KDD cup 2010. In KDD Cup, 2010.

Zhao, Tuo, Yu, Mo, Wang, Yiming, Arora, Raman, and Liu, Han. Accelerated mini-batch random-
ized block coordinate descent method. In Advances in neural information processing systems,
2014.

10

Breaking the Nonsmooth Barrier: A Scalable Parallel
Method for Composite Optimization

Supplementary material

Notations. Throughout the supplementary material we use the following extra notation. We denote
by (cid:104)·, ·(cid:105)(i) (resp. (cid:107) · (cid:107)(i)) the scalar product (resp. norm) restricted to blocks in Ti, i.e., (cid:104)x, y(cid:105)(i) :=
(cid:104)[x]B, [y]B(cid:105) and (cid:107)x(cid:107)(i) := (cid:112)(cid:104)x, x(cid:105)(i). We will also use the following deﬁnitions: ϕ :=
(cid:80)
(cid:80)

B∈Ti
B∈B dBhB(x) and D is the diagonal matrix deﬁned block-wise as [D]B,B = dBI |B|.

The Bregman divergence associated with a convex function f for points x, y in its domain is
deﬁned as:

Bf (x, y) := f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105) .

Note that this is always positive due to the convexity of f .

Appendix A Basic properties

Lemma 1. For any µ-strongly convex function f we have the following inequality:

(cid:104)∇f (y) − ∇f (x), y − x(cid:105) ≥

(cid:107)y − x(cid:107)2 + Bf (x, y) .

µ
2

Proof. By strong convexity, f veriﬁes the inequality:

f (y) ≤ f (x) + (cid:104)∇f (y), y − x(cid:105) −

(cid:107)y − x(cid:107)2 ,

µ
2

for any x, y in the domain (see e.g. (Nesterov, 2004)). We then have the equivalences:

f (x) ≤ f (y) + (cid:104)∇f (x), x − y(cid:105) −

(cid:107)x − y(cid:107)2

µ
2

⇐⇒

(cid:107)x − y(cid:107)2 + f (x) − f (y) ≤ (cid:104)∇f (x), x − y(cid:105)

⇐⇒

(cid:107)x − y(cid:107)2 + f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105)
(cid:125)

(cid:124)

(cid:123)(cid:122)
Bf (x,y)

µ
2
µ
2

≤ (cid:104)∇f (x) − ∇f (y), x − y(cid:105) ,

(10)

where in the last line we have subtracted (cid:104)∇f (y), x − y(cid:105) from both sides of the inequality.

Lemma 2. Let the fi be L-smooth and convex functions. Then it is veriﬁed that:

1
n

n
(cid:88)

i=1

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ 2LBf (x, y) .

Proof. Since each fi is L-smooth, it is veriﬁed (see e.g. Nesterov (2004, Theorem 2.1.5)) that

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ 2L(cid:0)fi(x) − fi(y) − (cid:104)∇fi(y), x − y(cid:105)(cid:1) .

The result is obtained by averaging over i.

(7)

(8)

(9)

(11)

(12)

Lemma 3 (Characterization of the proximal operator). Let h be convex lower semicontinuous. Then
we have the following characterization of the proximal operator:

z = proxγh(x) ⇐⇒

(x − z) ∈ ∂h(z) .

(13)

1
γ

11

Proof. This is a direct consequence of the ﬁrst order optimality conditions on the deﬁnition of
proximal operator, see e.g. (Beck & Teboulle, 2009; Nesterov, 2013).

Lemma 4 (Firm non-expansiveness). Let x, ˜x be two arbitrary elements in the domain of ϕi and
z, ˜z be deﬁned as z := proxϕi

(˜x). Then it is veriﬁed that:

(x), ˜z := proxϕi

(cid:104)z − ˜z, x − ˜x(cid:105)(i) ≥ (cid:107)z − ˜z(cid:107)2

(i) .

Proof. By the block-separability of ϕi, the proximal operator is the concatenation of the proximal
operators of the blocks. In other words, for any block B ∈ Ti we have:

[z]B = proxγϕB ([x]B) ,

[˜z]B = proxγϕB ([˜x]B) ,

where ϕB is the restriction of ϕi to B. By ﬁrm non-expansiveness of the proximal operator (see
e.g. Bauschke & Combettes (2011, Proposition 4.2)) we have that:

(cid:104)[z]B − [˜z]B, [x]B − [˜x]B(cid:105) ≥ (cid:107)[z]B − [˜z]B(cid:107)2 .

Summing over the blocks in Ti yields the desired result.

(14)

(15)

12

Appendix B Sparse Proximal SAGA

This Appendix contains all proofs for Section 2. The main result of this section is Theorem 1, whose
proof is structured as follows:

• We start by proving four auxiliary results that will be used later on in the proofs of both
synchronous and asynchronous variants. The ﬁrst is the unbiasedness of key quantities used
in the algorithm. The second is a characterization of the solutions of (OPT) in terms of f
and ϕ (deﬁned below) in Lemma 6. The third is a key inequality in Lemma 7 that relates
the gradient mapping to other terms that arise in the optimization. The fourth is an upper
bound on the variance terms of the gradient estimator, relating it to the Bregman divergence
of f and the past gradient estimator terms.

• In Lemma 9, we deﬁne an upper bound on the iterates (cid:107)xt − x∗(cid:107)2, called a Lyapunov
function, and prove an inequality that relates this Lyapunov function value at the current
iterate with its value at the previous iterate.

• Finally, in the proof of Theorem 1 we use the previous inequality in terms of the Lyapunov

function to prove a geometric convergence of the iterates.

We start by proving the following unbiasedness result, mentioned in §2.

Lemma 5. Let Di and ϕi be deﬁned as in §2. Then it is veriﬁed that EDi = I p and E ϕi = h.

Proof. Let B ∈ B an arbitrary block. We have the following sequence of equalities:

where the last equality comes from the deﬁnition of nB. EDi = I p then follows from the arbitrari-
ness of B.

Similarly, for ϕi we have:

E[Di]B,B =

[Di]B,B =

dB1{B ∈ Ti}I |B|

1
n

n
(cid:88)

i=1

(cid:33)

1{B ∈ Ti}I |B|

1
n

1
n
(cid:32)

n
(cid:88)

i=1
n
(cid:88)

i=1

1
nB

n
nB

n
(cid:88)

i=1

=

=

1{B ∈ Ti}

I |B| = I |B| ,

Eϕi([x]B) =

dB1{B ∈ Ti}hB([x]B)

1
n

1
n
(cid:32)

n
(cid:88)

i=1
n
(cid:88)

i=1

1
nB

n
nB

n
(cid:88)

i=1

=

=

1{B ∈ Ti}hB([x]B)

(cid:33)

1{B ∈ Ti}

hB([x]B) = hB([x]B) ,

Finally, the result E ϕi = h comes from adding over all blocks.

Lemma 6. x∗ is a solution to (OPT) if and only if the following condition is veriﬁed:

x∗ = proxγϕ

(cid:0)x∗ − γD∇f (x∗)(cid:1) .

Proof. By the ﬁrst order optimality conditions, the solutions to (OPT) are characterized by the sub-
differential inclusion −∇f (x∗) ∈ ∂h(x∗). We can then write the following sequence of equiva-

13

(16)

(17)

(18)

(19)

(20)

(21)

(22)

lences:

−∇f (x∗) ∈ ∂h(x∗) ⇐⇒ −D∇f (x∗) ∈ D∂h(x∗)

(multiplying by D, equivalence since diagonals are nonzero)

⇐⇒ −D∇f (x∗) ∈ ∂ϕ(x∗)
(by deﬁnition of ϕ)

⇐⇒

(x∗ − γD∇f (x∗) − x∗) ∈ ∂ϕ(x∗)

1
γ

(adding and subtracting x∗)

⇐⇒ x∗ = proxγϕ(x∗ − γD∇f (x∗)) .

(by Lemma 3)

(23)

Since all steps are equivalences, we have the desired result.

The following lemma will be key in the proof of convergence for both the sequential and the parallel
versions of the algorithm. With this result, we will be able to bound the product between the gradient
mapping and the iterate suboptimality by:

• First, the negative norm of the gradient mapping, which will be key in the parallel setting

to cancel out the terms arising from the asynchrony.

• Second, variance terms in (cid:107)vi−Di∇f (x∗)(cid:107)2 that we will be able to bound by the Bregman

divergence using Lemma 2.

• Third and last, a product with terms in (cid:104)vi − Di∇f (x∗), x − x∗(cid:105), which taken in expec-
tation gives (cid:104)∇f (x) − ∇f (x∗), x − x∗(cid:105) and will allow us to apply Lemma 1 to obtain the
contraction terms needed to obtain a geometric rate of convergence.

Lemma 7 (Gradient mapping inequality). Let x be an arbitrary vector, x∗ a solution to (OPT), vi
as deﬁned in (SPS) and g = g(x, vi, i) the gradient mapping deﬁned in (3). Then the following
inequality is veriﬁed for any β > 0:

(cid:104)g, x − x∗(cid:105) ≥ −

(β − 2)(cid:107)g(cid:107)2 −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + (cid:104)vi − Di∇f (x∗), x − x∗(cid:105) .

(24)

γ
2

γ
2β

Proof. By ﬁrm non-expansiveness of the proximal operator (Lemma 4) applied to z = proxγϕi(x−
γvi) and ˜z = proxγϕi(x∗ − γD∇f (x∗)) we have:

(cid:107)z − ˜z(cid:107)2

(i) − (cid:104)z − ˜z, x − γvi − x∗ + γD∇f (x∗)(cid:105)(i) ≤ 0 .

(25)

By the (SPS) iteration we have x+ = z and by Lemma 3 we have that [z]Ti = [x∗]Ti, hence the
above can be rewritten as

(cid:107)x+ − x∗(cid:107)2

(i) − (cid:104)x+ − x∗, x − γvi − x∗ + γD∇f (x∗)(cid:105)(i) ≤ 0 .

(26)

14

We can now write the following sequence of inequalities

(cid:104)γg, x − x∗(cid:105) = (cid:104)x − x+, x − x∗(cid:105)(i)

(by deﬁnition and sparsity of g)

(cid:16)

≥

1 −

(cid:17)

β
2

(cid:16)

≥

1 −

(cid:16)

=

1 −

(cid:17)

(cid:17)

β
2

β
2

= (cid:104)x − x+ + x∗ − x∗, x − x∗(cid:105)(i)
= (cid:107)x − x∗(cid:107)2
≥ (cid:107)x − x∗(cid:107)2

(i) − (cid:104)x+ − x∗, x − x∗(cid:105)(i)
(i) − (cid:104)x+ − x∗, 2x − γvi − 2x∗ + γD∇f (x∗)(cid:105)(i) + (cid:107)x+ − x∗(cid:107)2
(i)

(27)

(adding Eq. (26))

= (cid:107)x − x+(cid:107)2
= (cid:107)x − x+(cid:107)2

(i) + (cid:104)x+ − x∗, γvi − γD∇f (x∗)(cid:105)(i)
(i) + (cid:104)x − x∗, γvi − γD∇f (x∗)(cid:105)(i) − (cid:104)x − x+, γvi − γD∇f (x∗)(cid:105)(i)

(completing the square)

(adding and substracting x)

(cid:107)x − x+(cid:107)2

(i) −

(cid:107)vi − D∇f (x∗)(cid:107)2

(i) + γ(cid:104)vi − D∇f (x∗), x − x∗(cid:105)(i)

(Young’s inequality 2(cid:104)a, b(cid:105) ≤

+ β(cid:107)b(cid:107)2, valid for arbitrary β > 0)

γ2
2β

γ2
2β

(cid:107)a(cid:107)2
β

(cid:107)x − x+(cid:107)2

(i) −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + γ(cid:104)vi − Di∇f (x∗), x − x∗(cid:105)

(by deﬁnition of Di and using the fact that vi is Ti-sparse)

(cid:107)γg(cid:107)2 −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + γ(cid:104)vi − D∇f (x∗), x − x∗(cid:105) ,

(28)

γ2
2β

where in the last inequality we have used the fact that g is Ti-sparse. Finally, dividing by γ both
sides yields the desired result.

Lemma 8 (Upper bound on the gradient estimator variance). For arbitrary vectors x, (αi)n
vi as deﬁned in (SPS) we have:

i=0, and

E(cid:107)vi − Di∇f (x∗)(cid:107)2 ≤ 4LBf (x, x∗) + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(29)

Proof. We will now bound the variance terms. For this we have:

E(cid:107)vi − D∇f (x∗)(cid:107)2

(i) = E(cid:107)∇fi(x) − ∇fi(x∗) + ∇fi(x∗) − αi + Diα − D∇f (x∗)(cid:107)2
(i)

≤ 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)∇fi(x∗) − αi − (D∇f (x∗) − Dα)(cid:107)2
(i)

(by inequality (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2)
= 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)∇fi(x∗) − αi(cid:107)2

− 4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)(i) + 2E(cid:107)D∇f (x∗) − Dα(cid:107)2
(developing the square)

(i) .

(30)

We will now simplify the last two terms in the above expression. For the ﬁrst of the two last terms
we have:

− 4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)(i) = −4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)

(31)

(support of ﬁrst term)

= −4(cid:104)∇f (x∗) − α, D∇f (x∗) − Dα(cid:105)
= −4(cid:107)∇f (x∗) − α(cid:107)2

D .

Similarly, for the last term we have:

2E(cid:107)D∇f (x∗) − Dα(cid:107)2

(i) = 2E(cid:104)Di∇f (x∗) − Diα, D∇f (x∗) − Dα(cid:105)

= 2(cid:104)∇f (x∗) − α, D∇f (x∗) − Dα(cid:105)

(32)

(33)

(using Lemma 5)
D .

= 2(cid:107)∇f (x∗) − α(cid:107)2

15

and so the addition of these terms is negative and can be dropped. In all, for the variance terms we
have

E(cid:107)vi − D∇f (x∗)(cid:107)2

(i) ≤ 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2

≤ 4LBf (x, x∗) + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(by Lemma 2)

(34)

c
n

n
(cid:88)

i=1

(cid:16)

We now deﬁne an upper bound on the quantity that we would like to bound, often called a Lyapunov
function, and establish a recursive inequality on this Lyapunov function.

Lemma 9 (Lyapunov inequality). Let L be the following c-parametrized function:

L(x, α) := (cid:107)x − x∗(cid:107)2 +

(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(35)

Let x+ and α+ be obtained from the Sparse Proximal SAGA updates (SPS). Then we have:

EL(x+, α+) − L(x, α) ≤ − γµ(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:17)

c
n

(cid:16)

+

2γ2 −

(cid:17)

c
n

E(cid:107)αi − ∇fi(x)(cid:107)2 .

Proof. For the ﬁrst term of L we have:

(cid:107)x+ − x∗(cid:107)2 = (cid:107)x − γg − x∗(cid:107)2

(g := g(x, vi, i))

= (cid:107)x − x∗(cid:107)2 − 2γ(cid:104)g, x − x∗(cid:105) + (cid:107)γg(cid:107)2
≤ (cid:107)x − x∗(cid:107)2 + γ2(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γ(cid:104)vi − Di∇f (x∗), x − x∗(cid:105)

(by Lemma 7 with β = 1)

Since vi is an unbiased estimator of the gradient and EDi = I p, taking expectations we have:

E(cid:107)x+ − x∗(cid:107)2 ≤ (cid:107)x − x∗(cid:107)2 + γ2E(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γ(cid:104)∇f (x) − ∇f (x∗), x − x∗(cid:105)

≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + γ2E(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γBf (x, x∗) .

(38)

(by Lemma 1)

By using the variance terms bound (Lemma 8) in the previous equation we have:

E(cid:107)x+ − x∗(cid:107)2 ≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + (4Lγ2 − 2γ)Bf (x, x∗)
+ 2γ2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

We will now bound the second term of the Lyapunov function. We have:

1
n

n
(cid:88)

i=1

(cid:107)α+

i − ∇fi(x∗)(cid:107)2 =

1 −

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2

(cid:18)

(cid:19)

1
n

1
n

(by deﬁnition of α+)

(cid:18)

(cid:19)

≤

1 −

1
n

2
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

LBf (x, x∗) .

(by Lemma 2)

(36)

(37)

(39)

(40)

(41)

16

Combining Eq. (39) and (40) we have:

EL(x+, α+) ≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + (4Lγ2 − 2γ)Bf (x, x∗) + 2γ2E(cid:107)αi − ∇fi(x∗)(cid:107)2

(cid:20)(cid:18)

+ c

1 −

(cid:19)

1
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

(cid:21)
2LBf (x, x∗)

= (1 − γµ)(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:16)

1
n
c
n

(cid:17)

(cid:16)

+

2γ2 −

(cid:17)

c
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 + cE(cid:107)αi − ∇fi(x∗)(cid:107)2

= L(x, α) − γµ(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:16)

(cid:17)

c
n

c
n
Finally, subtracting L(x, α) from both sides yields the desired result.

E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

2γ2 −

+

(cid:17)

(cid:16)

(42)

Theorem 1. Let γ = a
converges geometrically in expectation with a rate factor of at least ρ = 1
xt obtained after t updates and x∗ the solution to (OPT), we have the bound:
i=1 (cid:107)α0

5L for any a ≤ 1 and f be µ-strongly convex. Then Sparse Proximal SAGA
κ }. That is, for

with C0 := (cid:107)x0 − x∗(cid:107)2 + 1
5L2

E(cid:107)xt − x∗(cid:107)2 ≤ (1 − ρ)tC0 ,

i − ∇fi(x∗)(cid:107)2

5 min{ 1

n , a 1

(cid:80)n

.

Proof. Let H := 1
n

(cid:80)

ELt+1 − (1 − ρ)Lt ≤ ρLt − γµ(cid:107)xt − x∗(cid:107)2 +

i (cid:107)αi − ∇fi(x∗)(cid:107)2. By the Lyapunov inequality from Lemma 9, we have:
(cid:17)
c
4Lγ2 − 2γ + 2L
n

c
n

(cid:17)

(cid:16)

(cid:16)

H

= (ρ − γµ) (cid:107)xt − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (xt, x∗) +

(cid:17)

c
n

(cid:17)

c
n

Bf (xt, x∗) +
(cid:20)
2γ2 + c

ρ −

(cid:18)

2γ2 −
(cid:19)(cid:21)

H

1
n

(cid:18)

(cid:19)

H

2c
3n

≤ (ρ − γµ) (cid:107)xt − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (xt, x∗) +

2γ2 −

(cid:16)

(cid:16)

(by deﬁnition of Lt)

(choosing ρ ≤

1
3n

)

= (ρ − γµ) (cid:107)xt − x∗(cid:107)2 + (cid:0)10Lγ2 − 2γ(cid:1) Bf (xt, x∗)

(choosing

= 3γ2)

c
n

(cid:16)

(cid:17)

aµ
5L

≤

ρ −

(cid:107)xt − x∗(cid:107)2
µ
a
L
5
And so we have the bound:

(for ρ ≤

≤ 0 .

·

)

(for all γ =

, a ≤ 1)

a
5L

(cid:18)

ELt+1 ≤

1 − min

(cid:110) 1
3n

,

a
5

·

1
κ

(cid:111)(cid:19)

(cid:18)

Lt ≤

min

, a ·

(cid:110) 1
n

(cid:111)(cid:19)

1
κ

Lt ,

1 −

1
5
3n ≤ 1

5n merely for clarity of exposition.

where in the last inequality we have used the trivial bound 1
Chaining expectations from t to 0 we have:
(cid:111)(cid:19)t+1

(cid:18)

ELt+1 ≤

1 −

min

, a ·

1
5

1
5

1
5

(cid:110) 1
n

(cid:110) 1
n

(cid:110) 1
n

L0
(cid:111)(cid:19)t+1 (cid:32)

(cid:111)(cid:19)t+1 (cid:32)

1
κ

1
κ

1
κ

(cid:18)

(cid:18)

(since a ≤ 1 and 3/5 ≤ 1) .

=

1 −

min

, a ·

(cid:107)x0 − x∗(cid:107)2 +

(cid:107)α0

i − ∇fi(x∗)(cid:107)2

≤

1 −

min

, a ·

(cid:107)x0 − x∗(cid:107)2 +

(cid:107)α0

i − ∇fi(x∗)(cid:107)2

(45)

3a2
52L2

1
5L2

n
(cid:88)

i=1
n
(cid:88)

i=1

The fact that Lt is a majorizer of (cid:107)xt − x∗(cid:107)2 completes the proof.

(43)

(44)

(cid:33)

(cid:33)

17

Appendix C ProxASAGA

In this Appendix we provide the proofs for results from Section 3, that is Theorem 2 (the convergence
theorem for PROXASAGA) and Corollary 1 (its speedup result).

Notation. Through this section, we use the following shorthand for the gradient mapping: gt :=
g(ˆxt, ˆvt
it

, it).

Appendix C.1 Proof outline.

As in the smooth case (h = 0), we start by using the deﬁnition of xt+1 in Eq. (4) to relate the
distance to the optimum in terms of its previous iterates:

(cid:107)xt+1 − x∗(cid:107)2 =(cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)ˆxt − xt, gt(cid:105) + γ2(cid:107)gt(cid:107)2 − 2γ(cid:104)ˆxt − x∗, gt(cid:105) .

(46)

However, in this case gt is not a gradient estimator but a gradient mapping, so we cannot continue
as is customary – by using the unbiasedness of the gradient in the (cid:104)ˆxt − x∗, gt(cid:105) term together with
the strong convexity of f (see Leblond et al. (2017, Section 3.5)).

To circumvent this difﬁculty, we derive a tailored inequality for the gradient mapping (Lemma 7
in Appendix B), which in turn allows us to use the classical unbiasedness and strong convexity
arguments to get the following inequality:

at+1 ≤ (1 −

γµ
2

)at + γ2E(cid:107)gt(cid:107)2 − 2γEBf (ˆxt, x∗) + γµE(cid:107)ˆxt − x(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105)
(cid:125)
(cid:124)

(cid:123)(cid:122)
additional asynchrony terms

(47)

+γ2(β − 2)E(cid:107)gt(cid:107)2 +
(cid:124)

γ2
β
(cid:123)(cid:122)
additional proximal and variance terms

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2
(cid:125)

,

where at := E(cid:107)xt − x∗(cid:107)2. Note that since f is strongly convex, Bf (ˆxt, x∗) ≥ µ
In the smooth setting, one ﬁrst expresses the additional asynchrony terms as linear combinations
of past gradient variance terms (E(cid:107)gu(cid:107)2)0≤u≤t. Then one crucially uses the negative Bregman
divergence term to control the variance terms. However, in our current setting, we cannot relate the
norm of the gradient mapping E(cid:107)gt(cid:107)2 to the Bregman divergence (from which h is absent). Instead,
we use the negative term γ2(β − 1)E(cid:107)gt(cid:107)2 to control all the (E(cid:107)gu(cid:107)2)0≤u≤t terms that arise from
asynchrony.

2 (cid:107)ˆxt − x∗(cid:107)2.

The rest of the proof consists in:
i) expressing the additional asynchrony terms as linear combinations of (E(cid:107)gu(cid:107)2)0≤u≤t, follow-
ing Leblond et al. (2017, Lemma 1);
ii) expressing the last variance term, (cid:107)ˆvt
it
divergences (Lemma 8 in Appendix B and Lemma 2 from Leblond et al. (2017));
iii) deﬁning a Lyapunov function, Lt := (cid:80)t
contraction given conditions on the maximum step size and delay.

u=0(1 − ρ)t−uau, and proving that it is bounded by a

− Dit∇f (x∗)(cid:107)2, as a linear combination of past Bregman

Appendix C.2 Detailed proof

Theorem 2 (Convergence guarantee and rate of PROXASAGA). Suppose τ ≤ 1
√
10
36 min{1, 6κ
size γ = a
expectation at a geometric rate factor of at least: ρ(a) = 1
(1 − ρ)t ˜C0, where ˜C0 is a constant independent of t (≈ nκ

. For any step
τ }, the inconsistent read iterates of Algorithm 1 converge in
(cid:9), i.e. E(cid:107)ˆxt − x∗(cid:107)2 ≤
5 min (cid:8) 1

a C0 with C0 as deﬁned in Theorem 1).

L with a ≤ 1

n , a 1

∆

κ

Proof. In order to get an initial recursive inequality, we ﬁrst unroll the (virtual) update:
(cid:107)xt+1 − x∗(cid:107)2 = (cid:107)xt − γgt − x∗(cid:107)2 = (cid:107)xt − x∗(cid:107)2 + (cid:107)γgt(cid:107)2 − 2γ(cid:104)gt, xt − x∗(cid:105)

= (cid:107)xt − x∗(cid:107)2 + (cid:107)γgt(cid:107)2 − 2γ(cid:104)gt, ˆxt − x∗(cid:105) + 2γ(cid:104)gt, ˆxt − xt(cid:105) ,

(48)

18

and then apply Lemma 7 with x = ˆxt and v = ˆvt
(cid:104)·(cid:105)(i) = (cid:104)·(cid:105)(it).

it. Note that in this case we have g = gt and

(cid:107)xt+1 − x∗(cid:107)2 ≤ (cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)gt, ˆxt − xt(cid:105) + γ2(cid:107)gt(cid:107)2 + γ2(β − 2)(cid:107)gt(cid:107)2

+

γ2
β

(cid:107)ˆvt
it

− D∇f (x∗)(cid:107)2

(it) − 2γ(cid:104)ˆvt
it

− D∇f (x∗), ˆxt − x∗(cid:105)(it)

= (cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)(cid:107)gt(cid:107)2

− Dit∇f (x∗)(cid:107)2 − 2γ(cid:104)ˆvt
it

− Dit∇f (x∗), ˆxt − x∗(cid:105).

(49)

+

(cid:107)ˆvt
it

γ2
β
(as [ˆvt
it

]Tit

= ˆvt

it)

We now use the property that it is independent of ˆxt (which we enforce by reading ˆxt before picking
it, see Section 3), together with the unbiasedness of the gradient update ˆvt
= ∇f (ˆxt)) and
the deﬁnition of D to simplify the following expression as follows:

it (Eˆvt
it

E(cid:104)ˆvt
it

− Dit∇f (x∗), ˆxt − x∗(cid:105) = (cid:104)∇f (ˆxt) − ∇f (x∗), ˆxt − x∗(cid:105)
µ
(cid:107)ˆxt − x∗(cid:107)2 + Bf (ˆxt, x∗) ,
2

≥

where the last inequality comes from Lemma 1. Taking conditional expectations on (49) we get:

E(cid:107)xt+1 − x∗(cid:107)2 ≤ (cid:107)xt − x∗(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)E(cid:107)gt(cid:107)2

+

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2 − γµ(cid:107)ˆxt − x∗(cid:107)2 − 2γBf (ˆxt, x∗)

≤ (1 −

)(cid:107)xt − x∗(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)E(cid:107)gt(cid:107)2

+

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2 + γµ(cid:107)ˆxt − xt(cid:107)2 − 2γBf (ˆxt, x∗)

(using (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2 on (cid:107)xt − ˆxt + ˆxt − x∗(cid:107)2)

γ2
β
γµ
2
γ2
β

γµ
2

≤ (1 −

)(cid:107)xt − x∗(cid:107)2 + γ2(β − 1)E(cid:107)gt(cid:107)2 + γµ(cid:107)ˆxt − xt(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105)
2γ2
β

− ∇fit (x∗)(cid:107)2 .

Bf (ˆxt, x∗) +

4γ2L
β

E(cid:107) ˆαt
it

(52)

− 2γBf (ˆxt, x∗) +

(using Lemma 8 on the variance terms)

Since we also have:

ˆxt − xt = γ

Gt

ug(ˆxu, ˆαu, iu),

t−1
(cid:88)

u=(t−τ )+

the effect of asynchrony for the perturbed iterate updates was already derived in a very similar setup
in Leblond et al. (2017). We re-use the following bounds from their Appendix C.4:7

E(cid:107)ˆxt − xt(cid:107)2 ≤ γ2(1 +

∆τ )

E(cid:107)gu(cid:107)2 ,

√

t−1
(cid:88)

u=(t−τ )+

Leblond et al. (2017, Eq. (48))

E(cid:104)gt, ˆxt − xt(cid:105) ≤

E(cid:107)gu(cid:107)2 +

E(cid:107)gt(cid:107)2 . Leblond et al. (2017, Eq. (46)).

√

γ

∆

2

t−1
(cid:88)

u=(t−τ )+

√

γ

∆τ
2

7The appearance of the sparsity constant ∆ is coming from the crucial property that E(cid:107)x(cid:107)2
∀x ∈ Rp (see Eq. (39) in Leblond et al. (2017), where they use the notation (cid:107) · (cid:107)i for our (cid:107) · (cid:107)(i)).

(i) ≤ ∆(cid:107)x(cid:107)2

19

(50)

(51)

(53)

(54)

(55)

Because the updates on α are the same for PROXASAGA as for ASAGA, we can re-use the same
argument arising in the proof of Leblond et al. (2017, Lemma 2) to get the following bound on
E(cid:107) ˆαt
it

− ∇fit(x∗)(cid:107)2:

E(cid:107) ˆαt
it

− ∇fit(x∗)(cid:107)2 ≤

(1 −

)(t−2τ −u−1)+EBf (ˆxu, x∗)

+2L(1 −

)(t−τ )+ ˜e0 ,

(56)

1
n

2L
n

t−1
(cid:88)

u=1
(cid:124)

1
n

(cid:123)(cid:122)
Henceforth denoted Ht

(cid:125)

E(cid:107)α0

i − f (cid:48)

where ˜e0 := 1
i (x∗)(cid:107)2. This bound is obtained by analyzing which gradient could be
2L
the source of αit in the past (taking in consideration the inconsistent writes), and then applying
Lemma 2 on the E(cid:107)∇f (ˆxu) − ∇f (x∗)(cid:107)2 terms, explaining the presence of Bf (ˆxu, x∗) terms.8
The inequality (56) corresponds to Eq. (56) and (57) in Leblond et al. (2017).

By taking the full expectation of (52) and plugging the above inequalities back, we obtain an in-
equality similar to Leblond et al. (2017, Master inequality (28)) which describes how the error terms
at := E(cid:107)xt − x∗(cid:107)2 of the virtual iterates are related:

at+1 ≤(1 −

)at +

(1 −

)(t−τ )+ ˜e0

γµ
2
+ γ2 (cid:104)

4γ2L
β
√

1
n

β − 1 +

∆τ

(cid:105)

E(cid:107)gt(cid:107)2 +

√

(cid:104)

γ2

∆ + γ3µ(1 +

√

(cid:105)
∆τ )

t
(cid:88)

E(cid:107)gu(cid:107)2

(57)

u=(t−τ )+

− 2γEBf (ˆxt, x∗) +

EBf (ˆxt, x∗) +

4γ2L
β

4γ2L
βn

Ht .

We now have a promising inequality with a contractive term and several quantities that we need to
bound. In order to achieve our ﬁnal result, we introduce the same Lyapunov function as in Leblond
et al. (2017):

Lt :=

(1 − ρ)t−uau ,

t
(cid:88)

u=0

where ρ is a target rate factor for which we will provide a value later on. Proving that this Lyapunov
function is bounded by a contraction will ﬁnish our proof. We have:

Lt+1 =

(1 − ρ)t+1−uau = (1 − ρ)t+1a0 +

(1 − ρ)t+1−uau

t+1
(cid:88)

u=0

= (1 − ρ)t+1a0 +

(1 − ρ)t−uau+1 .

(58)

t+1
(cid:88)

u=1
t
(cid:88)

u=0

We now plug our new bound on at+1, (57):
(1 − ρ)t−u(cid:104)

Lt+1 ≤ (1 − ρ)t+1a0 +

t
(cid:88)

u=0

)(u−τ )+ ˜e0

(1 −

)au +

γµ
2
+ γ2(cid:0)β − 1 +
√

+ (cid:0)γ2

(1 −

4γ2L
1
β
n
∆τ (cid:1)E(cid:107)gu(cid:107)2
√

√

∆ + γ3µ(1 +

∆τ )(cid:1)

u
(cid:88)

E(cid:107)gv(cid:107)2

v=(u−τ )+

− 2γEBf (ˆxu, x∗) +

EBf (ˆxu, x∗) +

4γ2L
β

4γ2L
βn

(cid:105)

Hu

.

(59)

After regrouping similar terms, we get:

Lt+1 ≤ (1 − ρ)t+1(a0 + A˜e0) + (1 −

)Lt +

st
u

E(cid:107)gu(cid:107)2 +

rt
u

EBf (ˆxu, x∗) .

(60)

γµ
2

t
(cid:88)

u=0

t
(cid:88)

u=1

8Note that Leblond et al. (2017) analyzed the unconstrained scenario, and so Bf (ˆxu, x∗) is replaced by the

simpler f (ˆxu) − f (x∗) in their bound.

20

Now, provided that we can prove that under certain conditions the st
u terms are all negative
(and that the A term is not too big), we can drop them from the right-hand side of (60) which will
allow us to ﬁnish the proof.
Let us compute these terms. Let q := 1−1/n

1−ρ and we assume in the rest that ρ < 1/n.

u and rt

Computing A. We have:

4γ2L
β

t
(cid:88)

u=0

(1 − ρ)t−u(1 −

)(u−τ )+ ≤

(1 − ρ)t(1 − ρ)−τ (τ + 1 +

1
n

4γ2L
β

1
1 − q

)

from Leblond et al. (2017, Eq (75))

= (1 − ρ)t+1 4γ2L
β

(cid:124)

(1 − ρ)−τ −1(τ + 1 +

)

.

(61)

1
1 − q

(cid:125)

(cid:123)(cid:122)
:=A

E(cid:107)gu(cid:107)2 ≤ τ (1 − ρ)−τ

(1 − ρ)t−uE(cid:107)gu(cid:107)2 ,

(62)

Computing st

u. Since we have:
t
(cid:88)

(1 − ρ)t−u

u−1
(cid:88)

u=0

v=(u−τ )+

we have for all 0 ≤ u ≤ t:
u ≤ (1 − ρ)t−u(cid:104)
st

t
(cid:88)

u=0

√

γ2(cid:0)β − 1 +

∆τ ) + τ (1 − ρ)−τ (cid:0)γ2

∆ + γ3µ(1 +

√

√

∆τ )(cid:1)(cid:105)

.

(63)

u. To analyze these quantities, we need to compute: (cid:80)t

Computing rt
v=1 (1 −
1
n )(u−2τ −v−1)+. Fortunately, this is already done in Leblond et al. (2017, Eq (66)), and thus we
know that for all 1 ≤ u ≤ t:

u=0(1 − ρ)t−u (cid:80)u−1

u ≤ (1 − ρ)t−u
rt

−2γ +

(cid:20)

4γ2L
β

+

4Lγ2
nβ

(1 − ρ)−2τ −1(cid:16)

2τ +

(cid:17)(cid:21)

,

1
1 − q

(64)

recalling that q := 1−1/n

1−ρ and that we assumed ρ < 1
n .

We now need some assumptions to further analyze these quantities. We make simple choices for
simplicity, though a tighter analysis is possible. To get manageable (and simple) constants, we
follow Leblond et al. (2017, Eq. (82) and (83)) and assume:

ρ ≤

τ ≤

1
4n

;

n
10

.

This tells us:

1
1 − q

≤

4n
3
4
3

(1 − ρ)−kτ −1 ≤

for 0 ≤ k ≤ 2 .

(using Bernouilli’s inequality)

Additionally, we set β = 1

2 . Equation (63) thus becomes:
(cid:0)√

√

(cid:20)

−

+

∆τ +

u ≤ γ2(1 − ρ)t−u
st

4
3

1
2

∆τ + γµτ (1 +

√

(cid:21)

∆τ )(cid:1)

.

We see that for st
get:

u to be negative, we need τ = O( 1√
∆

). Let us assume that τ ≤ 1
√
10

∆

. We then

u ≤ γ2(1 − ρ)t−u
st

(cid:20)

−

+

+

+ γµτ

1
2

1
10

4
30

(cid:21)

.

4
3

11
10

Thus, the condition under which all st

(65)

(66)

(67)

(68)

u are negative boils down to:
2
11

γµτ ≤

.

21

Now looking at the rt

u terms given our assumptions, the inequality (64) becomes:

u ≤ (1 − ρ)t−u
rt

(cid:20)
−2γ + 8γ2L +

8γ2L
n

4
3

(cid:0) n
5

+

(cid:21)

(cid:1)

4n
3

≤ (1 − ρ)t−u(cid:0) − 2γ + 36γ2L(cid:1) .

The condition for all rt

u to be negative then can be simpliﬁed down to:

γ ≤

1
18L

.

γ ≤

1
36L

.

We now have a promising inequality for proving that our Lyapunov function is bounded by a con-
traction. However we have deﬁned Lt in terms of the virtual iterate xt, which means that our result
would only hold for a given T ﬁxed in advance, as is the case in Mania et al. (2017). Fortunately,
we can use the same trick as in Leblond et al. (2017, Eq. (97)): we simply add γBf (ˆxt, x∗) to both
sides in (60). rt
t + γ, which makes for a slightly worse bound on γ to ensure linear
convergence:

t is replaced by rt

For this small cost, we get a contraction bound on Bf (ˆxt, x∗), and thus by the strong convexity of
f (see (9)) we get a contraction bound for E(cid:107)ˆxt − x∗(cid:107)2.
Recap. Let us use ρ = 1
to:

L . Then the conditions (68) and (71) on the step size γ reduce

4n and γ := a

Moreover, the condition:

a ≤

min{1,

1
36

72
11

κ
τ

}.

τ ≤

1
√

∆
is sufﬁcient to also ensure that (65) is satisﬁed as ∆ ∈ [ 1

10

Thus under the conditions (72) and (73), we have that all st
rewrite the recurrent step of our Lyapunov function as:

√

n ≤ n.

1√
∆

≤

n , 1], and thus
u and rt

u terms are negative and we can

Lt+1 ≤ γEBf (ˆxt) + Lt+1 ≤ (1 − ρ)t+1(a0 + A˜e0) + (1 −

)Lt .

(74)

γµ
2

By unrolling the recursion (74), we can carefully combine the effect of the geometric term (1 − ρ)
with the one of (1 − γµ
2 ). This was already done in Leblond et al. (2017, Apx C.9, Eq. (101) to
(103)), with a trick to handle various boundary cases, yielding the overall rate:

where ρ∗ = min{ 1
To get the ﬁnal constant, we need to bound A. We have:

5κ } (that we simpliﬁed to ρ∗ = 1

5n , a 2

EBf (ˆxt, x∗) ≤ (1 − ρ∗)t+1 ˆC0,
5 min{ 1

n , a 1

κ } in the theorem statement).

A =

(1 − ρ)−τ −1(τ + 1 +

1
1 − q

)

4γ2L
β

n
10

(

≤ 8γ2L

4
3
≤ 26γ2Ln
≤ γn .

+ 1 +

4n
3

)

This is the same bound on A that was used by Leblond et al. (2017) and so we obtain the same
constant as their Eq. (104):

ˆC0 :=

21n
γ

((cid:107)x0 − x∗(cid:107)2 + γ

E(cid:107)α0

i − ∇fi(x∗)(cid:107)2).

n
2L

Note that ˆC0 = O( n

γ C0) with C0 deﬁned as in Theorem 1.

22

(69)

(70)

(71)

(72)

(73)

(75)

(76)

(77)

Now, using the strong convexity of f via (9), we get:

E(cid:107)ˆxt − x∗(cid:107)2 ≤

EBf (ˆxt, x∗) ≤ (1 − ρ∗)t+1 ˜C0,

(78)

2
µ

where ˜C0 = O( nκ
a C0).
This ﬁnishes the proof for Theorem 2.

Corollary 3 (Speedup). Suppose τ ≤ 1
. If κ ≥ n, then using the step size γ = 1/36L, PROXAS-
√
10
AGA converges geometrically with rate factor Ω( 1
κ ). If κ < n, then using the step size γ = 1/36nµ,
PROXASAGA converges geometrically with rate factor Ω( 1
n ). In both cases, the convergence rate
is the same as Sparse Proximal SAGA and PROXASAGA is thus linearly faster than its sequential
counterpart up to a constant factor. Note that in both cases the step size does not depend on τ .

∆

Furthermore, if τ ≤ 6κ, we can use a universal step size of Θ(1/L) to get a similar rate for PROX-
ASAGA than Sparse Proximal SAGA, thus making it adaptive to local strong convexity since the
knowledge of κ is not required.

Proof. If κ ≥ n, the rate factor of Sparse Proximal SAGA is 1/κ. To get the same rate factor, we
need to choose a = Ω(1), which we can fortunately do since κ ≥ n ≥

≥ 10τ .

√

n ≥ 10 1
√
10

∆

If κ < n, then the rate factor of Sparse Proximal SAGA is 1/n. Any choice of a bigger than Ω(κ/n)
gives us the same rate factor for PROXASAGA. Since τ ≤
n/10 we can pick such an a without
violating the condition of Theorem 2.

√

23

Appendix D Comparison with related work

In this section, we relate our theoretical results and proof technique with the related literature.

Speedups. Our speedup regimes are comparable with the best ones obtained in the smooth case,
including Niu et al. (2011); Reddi et al. (2015), even though unlike these papers, we support incon-
sistent reads and nonsmooth objective functions. The one exception is Leblond et al. (2017), where
the authors prove that their algorithm, ASAGA, can obtain a linear speedup even without sparsity in
the well-conditioned regime. In contrast, PROXASAGA always requires some sparsity. Whether this
property for smooth objective functions could be extended to the composite case remains an open
problem.

Coordinate Descent. We compare our approach for composite objective functions to its most
natural competitor: ASYSPCD (Liu & Wright, 2015), an asynchronous stochastic coordinate descent
algorithm. While ASYSPCD also exhibits linear speedups, subject to a condition on τ , one has to be
especially careful when trying to compare these conditions.

First, while in theory the iterations of both algorithms have the same cost, in practice various tricks
are introduced to save on computation, yielding different costs per updates.9 Second, the bound on
τ for the coordinate descent algorithm depends on p, the dimensionality of the problem, whereas
ours involves n, the number of data points. Third, a more subtle issue is that τ is not affected by
the same quantities for both algorithms.10 See Appendix D.1 for a more detailed explanation of the
differences between the bounds.

√

∆) is necessary for a linear speedup. This means in case ∆ ≤ 1/√

In the best case scenario (where the components of the gradient are uncorrelated, a somewhat un-
√
realistic setting), ASYSPCD can get a near-linear speedup for τ as big as 4
p. Our result states that
τ = O(1/
p our bound is better
than the one obtained for ASYSPCD. Recalling that 1/n ≤ ∆ ≤ 1, it appears that PROXASAGA is
favored when n is bigger than
p whereas ASYSPCD may have a better bound otherwise, though
this comparison should be taken with a grain of salt given the assumptions we had to make to arrive
at comparable quantities.

√

Furthermore, one has to note that while Liu & Wright (2015) use the classical labeling scheme
inherited from Niu et al. (2011), they still assume in their proof that the it are uniformly distributed
and that their gradient estimators are conditionally unbiased – though neither property is veriﬁed in
the general asynchronous setting. Finally, we note that ASYSPCD (as well as its incremental variant
Async-PROXSVRCD) assumes that the computation and assignment of the proximal operator is an
atomic step, while we do not make such assumption.

SVRG. The Async-ProxSVRG algorithm of Meng et al. (2017) also exhibits theoretical linear
speedups subject to the same condition as ours. However, the analyzed algorithm uses dense updates
and consistent read and writes. Although they make the analysis easier, these two factors introduce
costly bottlenecks and prevent linear speedups in running time. Furthermore, here again the classical
labeling scheme is used together with the unveriﬁed conditional unbiasedness condition.

Doubly stochastic algorithms. The Async-PROXSVRCD algorithm from Meng et al. (2017); Gu
et al. (2016) has a maximum allowable stepsize11 that is in O(1/pL), whereas the maximum step
size for PROXASAGA is in Ω(1/L), so can be up to p times bigger. Consequently, PROXASAGA
enjoys much faster theoretical convergence rates. Unfortunately, we could not ﬁnd a condition for
linear speedups to compare to. We also note that their algorithm is not appropriate in a sparse
features setting. This is illustrated in an empirical comparison in Appendix F where we see that

9For PROXASAGA the relevant quantity becomes the average number of features per data point. For
In both cases the tricks involved are

ASYSPCD it is rather the average number of data points per feature.
not covered by the theory.

10To make sure τ is the same quantity for both algorithms, we have to assume that the iteration costs are

homogeneous.

11To the best of our understanding, noting that extracting an interpretable bound from the given theoretical
results was difﬁcult. Furthermore, it appears that the proof technique may still have signiﬁcant issues: for
example, the “fully lock-free” assumption of Gu et al. (2016) allows for overwrites, and is thus incompatible
with their framework of analysis, in particular their Eq. (8).

24

their convergence in number of iterations is orders of magnitude slower than appropriate algorithms
like SAGA or PROXASAGA.

Appendix D.1 Comparison of bounds with Liu & Wright (2015)

Iteration costs. For both PROXASAGA and ASYSPCD, the average cost of an iteration is O(nS)
(where S is the average support size).
In the case of PROXASAGA (see Algorithm 1), at each
iteration the most costly operation is the computation of α, while in the general case we need to
compute a full gradient for ASYSPCD.

In order to reduce these prohibitive computation costs, several tricks are introduced. Although they
lead to much improved empirical performance, it should be noted that in both cases these tricks are
not covered by the theory. In particular, the unbiasedness condition can be violated.

In the case of PROXASAGA, we store the average gradient term α in shared memory. The cost
of each iteration then becomes the size of the extended support of the partial gradient selected at
random at this iteration, hence it is in O(∆l), where ∆l := maxi=1..n |Ti|.

For ASYSPCD, following Peng et al. (2016) we can store intermediary quantities for speciﬁc losses
(e.g. (cid:96)1-regularized logistic regression). The cost of an iteration then becomes the number of data
points whose extended support includes the coordinate selected at random at this iteration, hence it
is in O(n∆).

The relative difference in update cost of both algorithms then depends heavily on the data matrix:
if the partial gradients usually have a extended support but coordinates belong to few of them (this
can be the case if n (cid:28) p for example), then the iterations of ASYSPCD can be cheaper than those of
PROXASAGA. Conversely, if data points usually have small extended support but coordinates belong
to many of them (which can happen when p (cid:28) n for example), then the updates of PROXASAGA
are the cheaper ones.

Dependency of τ on the data matrix.
In the case of PROXASAGA the sizes of the extended
support of each data point are important – they are directly linked to the cost of each iteration.
Identical iteration costs for each data point do not inﬂuence τ , whereas heterogeneous costs may
cause τ to increase substantially. In contrast, in the case of ASYSPCD, the relevant parts of the data
matrix are the number of data points each dimension touches – for much the same reason. In the
bipartite graph between data points and dimensions, either the left or the right degrees matter for τ ,
depending on which algorithm you choose.

In order to compare their respective bounds, we have to make the assumption that the iteration costs
are homogeneous, which means that each data point has the same support size and each dimension is
active in the same number of data points. This implies that τ is the same quantity for both algorithms.

√

Best case scenario bound for AsySPCD. The result obtained in Liu & Wright (2015) states that
if τ 2Λ = O(
p), ASYSPCD can get a near-linear speedup (where Λ is a measure of the interactions
p). In the best possible scenario where
between the components of the gradient, with 1 ≤ Λ ≤
Λ = 1 (which means that the coordinates of the gradients are completely uncorrelated), τ can be as
√
big as 4

√

p.

25

Appendix E Implementation details

Initialization.
In the Sparse Proximal SAGA algorithm and its asynchronous variant, PROXAS-
AGA, the vector x can be initialized arbitrarily. The memory terms αi can be initialized to any
vector that veriﬁes supp(αi) = supp(∇fi). In practice we found that the initialization αi = 0 is
very fast to set up and often outperforms more costly initializations.

With this initialization, the gradient approximation before the ﬁrst update of the memory terms be-
comes ∇fi(x) + Diα. Since most of the values in α are zero, α will tend to be small compared to
∇fi(x), and so the gradient estimate is very close to the SGD estimate ∇fi(x). The SGD approx-
imation is known to have a very fast initial convergence (which, in light of Figure 1, our method
inherits) and has even been used as a heuristic to use during the ﬁrst epoch of variance reduced
methods (Schmidt et al., 2016).

The initialization of coefﬁcients x0 was always set to zero.

Exact regularization. Computing the gradient of a smooth regularization such as the squared (cid:96)2
penalty of Eq. (6) is independent of n and so we can use the exact regularizer in the update of
the coefﬁcients instead of storing it in α, which would also destroy the compressed storage of the
memory terms described below. In practice we use this “exact regularization”, multiplied by Di to
preserve the sparsity pattern.
Assuming a squared (cid:96)2 regularization term of the form λ
(note the extra λx)

2 , the gradient estimate in (SPS) becomes

vi = ∇fi(x) − αi + Di(α + λx) .

(79)

Storage of memory terms. The storage requirements for this method is in the worst case a table
of size n × p. However, as for SAG and SAGA, for linearly parametrized loss functions of the form
fi(x) = (cid:96)(aT
i=1 are samples associated with
the learning problem, this can be reduced to a table of size n (Schmidt et al., 2016, §4.1). This
includes popular linear models such as least squares or logistic regression with (cid:96) the squared or
logistic function, respectively.

i x), where (cid:96) is some real-valued function and (ai)n

The reduce storage comes from the fact that in this case the partial gradients have the structure

∇fi(x) = ai (cid:96)(cid:48)(aT

.

i x)
(cid:124) (cid:123)(cid:122) (cid:125)
scalar

(80)

Since ai is independent of x, we only need to store the scalar (cid:96)(cid:48)(aT
explains why ∇fi inherits the sparsity pattern of ai.

i x). This decomposition also

Atomic updates. Most modern processors have support for atomic operations with minimal over-
head. In our case, we implemented a double-precision atomic type using the C++11 atomic features
(std::atomic<double>). This type implements atomic operations through the compare and swap
semantics.

Empirically, we have found it necessary to implement atomic operations at least in the vector α and
α to reach arbitrary precision. If non-atomic operations are used, the method converges only to a
limited precision (around normalized function suboptimality of 10−3), which might be sufﬁcient for
some machine learning applications but which we found not satisfying from an optimization point
of view.

AsySPCD. Following (Peng et al., 2016) we keep the vector (aT
at each iteration using atomic updates.

i x)n

i=1 in memory and update it

Hardware and software. All experiments were run on a Dell PowerEdge 920 machine with 4 Intel
Xeon E7-4830v2 processors with 10 2.2GHz cores each and 384GB 1600 Mhz RAM. The PROX-
ASAGAand ASYSPCD code was implemented on C++ and binded in Python. The FISTA code is
implemented in pure Python using NumPY and SciPy for matrix computations (in this case the bot-
tleneck is in large sparse matrix-vector operations for which efﬁcient BLAS routines were used). Our
PROXASAGA implementation can be downloaded from http://github.com/fabianp/ProxASAGA.

26

Appendix F Experiments

All datasets used for the experiments were downloaded from the LibSVM dataset suite.12

Appendix F.1 Comparison of ProxASAGA with other sequential methods

We provide a comparison between the Sparse Proximal SAGA and related methods in the sequential
case. We compare against two methods: the MRBCD method of Zhao et al. (2014) (which forms
the basis of Async-PROXSVRCD) and the vanilla implementation of SAGA (Defazio et al., 2014),
which does not have the ability to perform sparse updates. We compare in terms of both passes
through the data (epochs) and time. We use the same step size for all methods (1/3L). Due to
the slow convergence of some methods, we use a smaller dataset than the ones used in §4. Dataset
RCV1 has n = 697, 641, d = 47, 236 and a density of 0.15, while Covtype is a dense dataset with
n = 581, 012, d = 54.

Figure 2: Suboptimality of different sequential algorithms. Each marker represents one pass through
the dataset.

We observe that for the convergence behavior in terms of number of passes, Sparse Proximal SAGA
performs as well as vanilla SAGA, though the latter requires dense updates at every iteration (Fig. 2
top left). On the other hand, in terms of running time, our implementation of Sparse Proximal SAGA
is much more efﬁcient than the other methods for sparse input (Fig. 2 top right). In the case of dense
input (Fig. 2 bottom), the three methods perform similarly.
A note on the performance of MRBCD.
It may appear surprising that Sparse Proximal SAGA
outperforms MRBCD so dramatically on sparse datasets. However, one should note that MRBCD is
a doubly stochastic algorithm where both a random data point and a random coordinate are sampled
for each iteration. If the data matrix is very sparse, then the probability that the sampled coordinate
is in the support of the sampled data point becomes very low. This means that the gradient estimator
term only contains the reference gradient term of SVRG, which only changes once per epoch. As a
result, this estimator becomes very coarse and produces a slower empirical convergence.

This is reﬂected in the theoretical results given in Zhao et al. (2014), where the epoch size needed to
get linear convergence are k times bigger than the ones required by plain SVRG, where k is the size
of the set of blocks of coordinates.

12https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/

27

Appendix F.2 Theoretical speedups.

In the experimental section, we have shown experimental speedup results where suboptimality was
a function of the running time. This measure encompasses both theoretical algorithmic optimization
properties and hardware overheads (such as contention of shared memory) which are not taken into
account in our analysis.

In order to isolate these two effects, we now plot our speedup results in Figure 3 where suboptimality
is a function of the number of iterations; thus, we abstract away any potential hardware overhead. To
do so, we implement a global counter which is sparsely updated (every 100 iterations for example)
in order not to modify the asynchrony of the system. This counter is used only for plotting purposes
and is not needed otherwise. Speciﬁcally, we deﬁne the theoretical speedup as:

theoretical speedup := (number of cores)

number of iterations for sequential algorithm
total number of iterations for parallel algorithm

.

Figure 3: Theoretical optimization speedups for (cid:96)1+(cid:96)2-regularized logistic regression. Speedup
as measured by the number of iterations required to reach 10−5 suboptimality for PROXASAGA
and ASYSPCD. In FISTA the iterates are the same with different cores and so matches the “ideal”
speedup.

We see clearly that the theoretical speedups obtained by both PROXASAGAand ASYSPCD are linear
(i.e. ideal). As we observe worse results in running time, this means that the hardware overheads of
asynchronous methods are quite signiﬁcant.

Appendix F.3 Timing benchmarks

We now provide the time it takes for the different methods with 10 cores to reach a suboptimality of
10−10. All results are in hours.

Dataset

PROXASAGA ASYSPCD

FISTA

KDD 2010
KDD 2012
Criteo

1.01
0.09
0.14

13.3
26.6
33.3

5.2
8.3
6.6

Appendix F.4 Hyperparameters

The (cid:96)1-regularization parameter λ2 was chosen as to give around 10% of non-zero features. The
exact chosen values are the following: λ2 = 10−11 for KDD 2010, λ2 = 10−16 for KDD 2012 and
λ2 = 4 × 10−12 for Criteo.

28

7
1
0
2
 
v
o
N
 
5
 
 
]

C
O
.
h
t
a
m

[
 
 
3
v
8
6
4
6
0
.
7
0
7
1
:
v
i
X
r
a

Breaking the Nonsmooth Barrier: A Scalable Parallel
Method for Composite Optimization

Fabian Pedregosa
INRIA/ENS∗
Paris, France

R´emi Leblond
INRIA/ENS∗
Paris, France

Simon Lacoste-Julien
MILA and DIRO
Universit´e de Montr´eal, Canada

Abstract

Due to their simplicity and excellent performance, parallel asynchronous variants
of stochastic gradient descent have become popular methods to solve a wide range
of large-scale optimization problems on multi-core architectures. Yet, despite their
practical success, support for nonsmooth objectives is still lacking, making them
unsuitable for many problems of interest in machine learning, such as the Lasso,
group Lasso or empirical risk minimization with convex constraints. In this work,
we propose and analyze PROXASAGA, a fully asynchronous sparse method in-
spired by SAGA, a variance reduced incremental gradient algorithm. The proposed
method is easy to implement and signiﬁcantly outperforms the state of the art on
several nonsmooth, large-scale problems. We prove that our method achieves a
theoretical linear speedup with respect to the sequential version under assump-
tions on the sparsity of gradients and block-separability of the proximal term.
Empirical benchmarks on a multi-core architecture illustrate practical speedups of
up to 12x on a 20-core machine.

1

Introduction

The widespread availability of multi-core computers motivates the development of parallel methods
adapted for these architectures. One of the most popular approaches is HOGWILD (Niu et al., 2011),
an asynchronous variant of stochastic gradient descent (SGD). In this algorithm, multiple threads run
the update rule of SGD asynchronously in parallel. As SGD, it only requires visiting a small batch
of random examples per iteration, which makes it ideally suited for large scale machine learning
problems. Due to its simplicity and excellent performance, this parallelization approach has recently
been extended to other variants of SGD with better convergence properties, such as SVRG (Johnson
& Zhang, 2013) and SAGA (Defazio et al., 2014).

Despite their practical success, existing parallel asynchronous variants of SGD are limited to smooth
objectives, making them inapplicable to many problems in machine learning and signal processing.
In this work, we develop a sparse variant of the SAGA algorithm and consider its parallel asyn-
chronous variants for general composite optimization problems of the form:

arg min
x∈Rp

f (x) + h(x)

, with f (x) := 1
n

(cid:80)n

i=1 fi(x)

,

(OPT)

where each fi is convex with L-Lipschitz gradient, the average function f is µ-strongly convex and
h is convex but potentially nonsmooth. We further assume that h is “simple” in the sense that we
have access to its proximal operator, and that it is block-separable, that is, it can be decomposed
block coordinate-wise as h(x) = (cid:80)
B∈BhB([x]B), where B is a partition of the coefﬁcients into

∗DI ´Ecole normale sup´erieure, CNRS, PSL Research University

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

subsets which will call blocks and hB only depends on coordinates in block B. Note that there is
no loss of generality in this last assumption as a unique block covering all coordinates is a valid
partition, though in this case, our sparse variant of the SAGA algorithm reduces to the original SAGA
algorithm and no gain from sparsity is obtained.

This template models a broad range of problems arising in machine learning and signal processing:
the ﬁnite-sum structure of f includes the least squares or logistic loss functions; the proximal term
h includes penalties such as the (cid:96)1 or group lasso penalty. Furthermore, this term can be extended-
valued, thus allowing for convex constraints through the indicator function.

Contributions. This work presents two main contributions. First, in §2 we describe Sparse Proxi-
mal SAGA, a novel variant of the SAGA algorithm which features a reduced cost per iteration in the
presence of sparse gradients and a block-separable penalty. Like other variance reduced methods, it
enjoys a linear convergence rate under strong convexity. Second, in §3 we present PROXASAGA, a
lock-free asynchronous parallel version of the aforementioned algorithm that does not require con-
sistent reads. Our main results states that PROXASAGA obtains (under assumptions) a theoretical
linear speedup with respect to its sequential version. Empirical benchmarks reported in §4 show that
this method dramatically outperforms state-of-the-art alternatives on large sparse datasets, while the
empirical speedup analysis illustrates the practical gains as well as its limitations.

1.1 Related work

Asynchronous coordinate-descent. For composite objective functions of the form (OPT), most of
the existing literature on asynchronous optimization has focused on variants of coordinate descent.
Liu & Wright (2015) proposed an asynchronous variant of (proximal) coordinate descent and proved
a near-linear speedup in the number of cores used, given a suitable step size. This approach has been
recently extended to general block-coordinate schemes by Peng et al. (2016), to greedy coordinate-
descent schemes by You et al. (2016) and to non-convex problems by Davis et al. (2016). However,
as illustrated by our experiments, in the large sample regime coordinate descent compares poorly
against incremental gradient methods like SAGA.

Variance reduced incremental gradient and their asynchronous variants.
Initially proposed in
the context of smooth optimization by Le Roux et al. (2012), variance reduced incremental gradient
methods have since been extended to minimize composite problems of the form (OPT) (see table
below). Smooth variants of these methods have also recently been extended to the asynchronous set-
ting, where multiple threads run the update rule asynchronously and in parallel. Interestingly, none
of these methods achieve both simultaneously, i.e. asynchronous optimization of composite prob-
lems. Since variance reduced incremental gradient methods have shown state of the art performance
in both settings, this generalization is of key practical interest.

Objective

Smooth

Composite

Sequential Algorithm
SVRG (Johnson & Zhang, 2013)
SDCA (Shalev-Shwartz & Zhang, 2013)
SAGA (Defazio et al., 2014)
PROXSDCA (Shalev-Shwartz et al., 2012)
SAGA (Defazio et al., 2014)
ProxSVRG (Xiao & Zhang, 2014)

Asynchronous Algorithm

SVRG (Reddi et al., 2015)
PASSCODE (Hsieh et al., 2015, SDCA variant)
ASAGA (Leblond et al., 2017, SAGA variant)

This work: PROXASAGA

On the difﬁculty of a composite extension. Two key issues explain the paucity in the develop-
ment of asynchronous incremental gradient methods for composite optimization. The ﬁrst issue
is related to the design of such algorithms. Asynchronous variants of SGD are most competitive
when the updates are sparse and have a small overlap, that is, when each update modiﬁes a small
and different subset of the coefﬁcients. This is typically achieved by updating only coefﬁcients for
which the partial gradient at a given iteration is nonzero,2 but existing schemes such as the lagged
updates technique (Schmidt et al., 2016) are not applicable in the asynchronous setting. The second

2Although some regularizers are sparsity inducing, large scale datasets are often extremely sparse and lever-

aging this property is crucial for the efﬁciency of the method.

2

difﬁculty is related to the analysis of such algorithms. All convergence proofs crucially use the Lip-
schitz condition on the gradient to bound the noise terms derived from asynchrony. However, in the
composite case, the gradient mapping term (Beck & Teboulle, 2009), which replaces the gradient
in proximal-gradient methods, does not have a bounded Lipschitz constant. Hence, the traditional
proof technique breaks down in this scenario.

Other approaches. Recently, Meng et al. (2017); Gu et al. (2016) independently proposed a dou-
bly stochastic method to solve the problem at hand. Following Meng et al. (2017) we refer to it
as Async-PROXSVRCD. This method performs coordinate descent-like updates in which the true
gradient is replaced by its SVRG approximation. It hence features a doubly-stochastic loop: at each
iteration we select a random coordinate and a random sample. Because the selected coordinate block
is uncorrelated with the chosen sample, the algorithm can be orders of magnitude slower than SAGA
in the presence of sparse gradients. Appendix F contains a comparison of these methods.

1.2 Deﬁnitions and notations

By convention, we denote vectors and vector-valued functions in lowercase boldface (e.g. x) and
matrices in uppercase boldface (e.g. D). The proximal operator of a convex lower semicontinuous
function h is deﬁned as proxh(x) := arg minz∈Rp {h(z) + 1
2 (cid:107)x − z(cid:107)2}. A function f is said to be
L-smooth if it is differentiable and its gradient is L-Lipschitz continuous. A function f is said to be
µ-strongly convex if f − µ
2 (cid:107) · (cid:107)2 is convex. We use the notation κ := L/µ to denote the condition
number for an L-smooth and µ-strongly convex function.3
I p denotes the p-dimensional identity matrix, 1{cond} the characteristic function, which is 1 if cond
evaluates to true and 0 otherwise. The average of a vector or matrix is denoted α := 1
i=1 αi.
n
We use (cid:107) · (cid:107) for the Euclidean norm. For a positive semi-deﬁnite matrix D, we deﬁne its associated
distance as (cid:107)x(cid:107)2
D := (cid:104)x, Dx(cid:105). We denote by [ x ]b the b-th coordinate in x. This notation is
overloaded so that for a collection of blocks T = {B1, B2, . . .}, [x]T denotes the vector x restricted
to the coordinates in the blocks of T . For convenience, when T consists of a single block B we use
[x]B as a shortcut of [x]{B}. Finally, we distinguish E, the full expectation taken with respect to
all the randomness in the system, from E, the conditional expectation of a random it (the random
feature sampled at each iteration by SGD-like algorithms) conditioned on all the “past”, which the
context will clarify.

(cid:80)n

2 Sparse Proximal SAGA

Original SAGA algorithm. The original SAGA algorithm (Defazio et al., 2014) maintains two
moving quantities: the current iterate x and a table (memory) of historical gradients (αi)n
i=1. At
every iteration, it samples an index i ∈ {1, . . . , n} uniformly at random, and computes the next
iterate (x+, α+) according to the following recursion:
ui = ∇fi(x) − αi + α ; x+ = proxγh

(1)
On each iteration, this update rule requires to visit all coefﬁcients even if the partial gradients ∇fi are
sparse. Sparse partial gradients arise in a variety of practical scenarios: for example, in generalized
linear models the partial gradients inherit the sparsity pattern of the dataset. Given that large-scale
datasets are often sparse,4 leveraging this sparsity is crucial for the success of the optimizer.

i = ∇fi(x) .

(cid:0)x − γui

(cid:1); α+

Sparse Proximal SAGA algorithm. We will now describe an algorithm that leverages sparsity
in the partial gradients by only updating those blocks that intersect with the support of the partial
gradients. Since in this update scheme some blocks might appear more frequently than others, we
will need to counterbalance this undersirable effect with a well-chosen block-wise reweighting of
the average gradient and the proximal term.

In order to make precise this block-wise reweighting, we deﬁne the following quantities. We denote
by Ti the extended support of ∇fi, which is the set of blocks that intersect the support of ∇fi,

3Since we have assumed that each individual fi is L-smooth, f itself is L-smooth – but it could have a

smaller smoothness constant. Our rates are in terms of this bigger L/µ, as is standard in the SAGA literature.

4For example, in the LibSVM datasets suite, 8 out of the 11 datasets (as of May 2017) with more than a

million samples have a density between 10−4 and 10−6.

3

i

formally deﬁned as Ti := {B : supp(∇fi) ∩ B (cid:54)= ∅, B ∈ B}. For totally separable penalties such
as the (cid:96)1 norm, the blocks are individual coordinates and so the extended support covers the same
coordinates as the support. Let dB := n/nB, where nB := (cid:80)
1{B ∈ Ti} is the number of times
that B ∈ Ti. For simplicity we assume nB > 0, as otherwise the problem can be reformulated
without block B. The update rule in (1) requires computing the proximal operator of h, which
involves a full pass on the coordinates. In our proposed algorithm, we replace h in (1) with the
function ϕi(x) := (cid:80)
dBhB(x), whose form is justiﬁed by the following three properties.
First, this function is zero outside Ti, allowing for sparse updates. Second, because of the block-wise
reweighting dB, the function ϕi is an unbiased estimator of h (i.e., E ϕi = h), property which will
be crucial to prove the convergence of the method. Third, ϕi inherits the block-wise structure of h
and its proximal operator can be computed from that of h as [proxγϕi(x)]B = [prox(dB γ)hB (x)]B
if B ∈ Ti and [proxγϕi(x)]B = [x]B otherwise. Following Leblond et al. (2017), we will also
replace the dense gradient estimate ui by the sparse estimate vi := ∇fi(x) − αi + Diα, where
Di is the diagonal matrix deﬁned block-wise as [Di]B,B = dB1{B ∈ Ti}I |B|. It is easy to verify
that the vector Diα is a weighted projection onto the support of Ti and E Diα = α, making vi an
unbiased estimate of the gradient.

B∈Ti

We now have all necessary elements to describe the Sparse Proximal SAGA algorithm. As the
original SAGA algorithm, it maintains two moving quantities: the current iterate x ∈ Rp and a
i=1, αi ∈ Rp. At each iteration, the algorithm samples an index
table of historical gradients (αi)n
i ∈ {1, . . . , n} and computes the next iterate (x+, α+) as:

vi = ∇fi(x) − αi + Diα ; x+ = proxγϕi

(cid:0)x − γvi

(cid:1) ; α+

i = ∇fi(x) ,

(SPS)

where in a practical implementation the vector α is updated incrementally at each iteration.

The above algorithm is sparse in the sense that it only requires to visit and update blocks in the
extended support: if B /∈ Ti, by the sparsity of vi and proxϕi, we have [x+]B = [x]B. Hence,
when the extended support Ti is sparse, this algorithm can be orders of magnitude faster than the
naive SAGA algorithm. The extended support is sparse for example when the partial gradients are
sparse and the penalty is separable, as is the case of the (cid:96)1 norm or the indicator function over a
hypercube, or when the the penalty is block-separable in a way such that only a small subset of the
blocks overlap with the support of the partial gradients. Initialization of variables and a reduced
storage scheme for the memory are discussed in the implementation details section of Appendix E.

Relationship with existing methods. This algorithm can be seen as a generalization of both the
Standard SAGA algorithm and the Sparse SAGA algorithm of Leblond et al. (2017). When the
proximal term is not block-separable, then dB = 1 (for a unique block B) and the algorithm defaults
to the Standard (dense) SAGA algorithm. In the smooth case (i.e., h = 0), the algorithm defaults to
the Sparse SAGA method. Hence we note that the sparse gradient estimate vi in our algorithm is the
same as the one proposed in Leblond et al. (2017). However, we emphasize that a straightforward
combination of this sparse update rule with the proximal update from the Standard SAGA algorithm
results in a nonconvergent algorithm: the block-wise reweighting of h is a surprisingly simple but
crucial change. We now give the convergence guarantees for this algorithm.
Theorem 1. Let γ = a
SAGA converges geometrically in expectation with a rate factor of at least ρ = 1
is, for xt obtained after t updates, we have the following bound:

5L for any a ≤ 1 and f be µ-strongly convex (µ > 0). Then Sparse Proximal
κ }. That

5 min{ 1

n , a 1

E(cid:107)xt − x∗(cid:107)2 ≤ (1 − ρ)tC0 , with C0 := (cid:107)x0 − x∗(cid:107)2 + 1
5L2

(cid:80)n

i=1 (cid:107)α0

i − ∇fi(x∗)(cid:107)2

.

Remark. For the step size γ = 1/5L, the convergence rate is (1 − 1/5 min{1/n, 1/κ}). We can thus
identify two regimes: the “big data” regime, n ≥ κ, in which the rate factor is bounded by 1/5n, and
the “ill-conditioned” regime, κ ≥ n, in which the rate factor is bounded by 1/5κ. This rate roughly
matches the rate obtained by Defazio et al. (2014). While the step size bound of 1/5L is slightly
smaller than the 1/3L one obtained in that work, this can be explained by their stronger assumptions:
each fi is strongly convex whereas they are strongly convex only on average in this work. All proofs
for this section can be found in Appendix B.

4

i=1

j=1[ ˆαj ]Ti

ˆx = inconsistent read of x
ˆα = inconsistent read of α
Sample i uniformly in {1, ..., n}
Si := support of ∇fi
Ti := extended support of ∇fi in B
[ α ]Ti = 1/n (cid:80)n
[ δα ]Si = [∇fi(ˆx)]Si − [ ˆαi]Si
[ ˆv ]Ti = [ δα ]Ti + [Diα ]Ti
[ δx ]Ti = [proxγϕi(ˆx − γ ˆv)]Ti − [ˆx]Ti
for B in Ti do
for b ∈ B do

Algorithm 1 PROXASAGA (analyzed)
1: Initialize shared variables x and (αi)n
2: keep doing in parallel
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end parallel loop

end for
// (‘←’ denotes shared memory update.)

[ x ]b ← [ x ]b + [ δx ]b
if b ∈ Si then

[αi]b ← [∇fi(ˆx)]b

end if
end for

(cid:46) atomic

i=1, α

Sample i uniformly in {1, ..., n}
Si := support of ∇fi
Ti := extended support of ∇fi in B
[ ˆx ]Ti = inconsistent read of x on Ti
ˆαi = inconsistent read of αi
[ α ]Ti = inconsistent read of α on Ti
[ δα ]Si = [∇fi(ˆx)]Si − [ ˆαi]Si
[ ˆv ]Ti = [δα ]Ti + [ Diα ]Ti
[ δx ]Ti = [proxγϕi(ˆx − γ ˆv)]Ti − [ˆx]Ti
for B in Ti do
for b in B do

Algorithm 2 PROXASAGA (implemented)
1: Initialize shared variables x, (αi)n
2: keep doing in parallel
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
end for
19:
20: αi ← ∇fi(ˆx)
21: end parallel loop

[ x ]b ← [ x ]b + [ δx ]b
if b ∈ Si then

end if
end for

[ α ]b ← [α]b + 1/n[δα]b (cid:46) atomic

(scalar update) (cid:46) atomic

(cid:46) atomic

3 Asynchronous Sparse Proximal SAGA

We introduce PROXASAGA – the asynchronous parallel variant of Sparse Proximal SAGA. In this
algorithm, multiple cores update a central parameter vector using the Sparse Proximal SAGA intro-
duced in the previous section, and updates are performed asynchronously. The algorithm parameters
are read and written without vector locks, i.e., the vector content of the shared memory can poten-
tially change while a core is reading or writing to main memory coordinate by coordinate. These
operations are typically called inconsistent (at the vector level).

The full algorithm is described in Algorithm 1 for its theoretical version (on which our analysis
is built) and in Algorithm 2 for its practical implementation. The practical implementation differs
from the analyzed agorithm in three points. First, in the implemented algorithm, index i is sampled
before reading the coefﬁcients to minimize memory access since only the extended support needs to
be read. Second, since our implementation targets generalized linear models, the memory αi can be
compressed into a single scalar in L20 (see Appendix E). Third, α is stored in memory and updated
incrementally instead of recomputed at each iteration.

The rest of the section is structured as follows: we start by describing our framework of analysis; we
then derive essential properties of PROXASAGA along with a classical delay assumption. Finally,
we state our main convergence and speedup result.

3.1 Analysis framework

As in most of the recent asynchronous optimization literature, we build on the hardware model in-
troduced by Niu et al. (2011), with multiple cores reading and writing to a shared memory parameter
vector. These operations are asynchronous (lock-free) and inconsistent:5 ˆxt, the local copy of the
parameters of a given core, does not necessarily correspond to a consistent iterate in memory.

“Perturbed” iterates. To handle this additional difﬁculty, contrary to most contributions in this
ﬁeld, we choose the “perturbed iterate framework” proposed by Mania et al. (2017) and reﬁned
by Leblond et al. (2017). This framework can analyze variants of SGD which obey the update rule:

xt+1 = xt − γv(xt, it) , where v veriﬁes the unbiasedness condition E v(x, it) = ∇f (x)

5This is an extension of the framework of Niu et al. (2011), where consistent updates were assumed.

5

and the expectation is computed with respect to it. In the asynchronous parallel setting, cores are
reading inconsistent iterates from memory, which we denote ˆxt. As these inconsistent iterates are
affected by various delays induced by asynchrony, they cannot easily be written as a function of
their previous iterates. To alleviate this issue, Mania et al. (2017) choose to introduce an additional
quantity for the purpose of the analysis:
xt+1 := xt − γv(ˆxt, it) ,

(2)
Note that this equation is the deﬁnition of this new quantity xt. This virtual iterate is useful for the
convergence analysis and makes for much easier proofs than in the related literature.

the “virtual iterate” – which is never actually computed .

“After read” labeling. How we choose to deﬁne the iteration counter t to label an iterate xt
matters in the analysis.
In this paper, we follow the “after read” labeling proposed in Leblond
et al. (2017), in which we update our iterate counter, t, as each core ﬁnishes reading its copy of
the parameters (in the speciﬁc case of PROXASAGA, this includes both ˆxt and ˆαt). This means
that ˆxt is the (t + 1)th fully completed read. One key advantage of this approach compared to the
classical choice of Niu et al. (2011) – where t is increasing after each successful update – is that
it guarantees both that the it are uniformly distributed and that it and ˆxt are independent. This
property is not veriﬁed when using the “after write” labeling of Niu et al. (2011), although it is still
implicitly assumed in the papers using this approach, see Leblond et al. (2017, Section 3.2) for a
discussion of issues related to the different labeling schemes.

Generalization to composite optimization. Although the perturbed iterate framework was de-
signed for gradient-based updates, we can extend it to proximal methods by remarking that in the
sequential setting, proximal stochastic gradient descent and its variants can be characterized by the
following similar update rule:

xt+1 = xt − γg(xt, vit, it) , with g(x, v, i) := 1
γ

(cid:0)x − proxγϕi(x − γv)(cid:1) ,

where as before v veriﬁes the unbiasedness condition E v = ∇f (x). The Proximal Sparse SAGA
iteration can be easily written within this template by using ϕi and vi as deﬁned in §2. Using this
deﬁnition of g, we can deﬁne PROXASAGA virtual iterates as:

xt+1 := xt − γg(ˆxt, ˆvt
it

, it) , with ˆvt
it

= ∇fit (ˆxt) − ˆαt
it

+ Dit αt

,

(3)

(4)

where as in the sequential case, the memory terms are updated as ˆαt
it
analysis of PROXASAGA will be based on this deﬁnition of the virtual iterate xt+1.

= ∇fit(ˆxt). Our theoretical

3.2 Properties and assumptions

Now that we have introduced the “after read” labeling for proximal methods in Eq. (4), we can
leverage the framework of Leblond et al. (2017, Section 3.3) to derive essential properties for the
analysis of PROXASAGA. We describe below three useful properties arising from the deﬁnition
of Algorithm 1, and then state a central (but standard) assumption that the delays induced by the
asynchrony are uniformly bounded.

Independence: Due to the “after read” global ordering, ir is independent of ˆxt for all r ≥ t. We
enforce the independence for r = t by having the cores read all the shared parameters before their
iterations.
Unbiasedness: The term ˆvt
consequence of the independence between it and ˆxt.

it is an unbiased estimator of the gradient of f at ˆxt. This property is a

Atomicity: The shared parameter coordinate update of [x]b on Line 14 is atomic. This means that
there are no overwrites for a single coordinate even if several cores compete for the same resources.
Most modern processors have support for atomic operations with minimal overhead.

Bounded overlap assumption. We assume that there exists a uniform bound, τ , on the maximum
number of overlapping iterations. This means that every coordinate update from iteration t is suc-
cessfully written to memory before iteration t + τ + 1 starts. Our result will give us conditions on τ
to obtain linear speedups.

Bounding ˆxt − xt. The delay assumption of the previous paragraph allows to express the difference
between real and virtual iterate using the gradient mapping gu := g(ˆxu, ˆvu
iu
ˆxt−xt = γ (cid:80)t−1

u are p × p diagonal matrices with terms in {0, +1}. (5)

ugu , where Gt

, iu) as:

Gt

u=(t−τ )+

6

0 represents instances where both ˆxu and xu have received the corresponding updates. +1, on
the contrary, represents instances where ˆxu has not yet received an update that is already in xu by
deﬁnition. This bound will prove essential to our analysis.

3.3 Analysis

In this section, we state our convergence and speedup results for PROXASAGA. The full details
of the analysis can be found in Appendix C. Following Niu et al. (2011), we introduce a sparsity
measure (generalized to the composite setting) that will appear in our results.
Deﬁnition 1. Let ∆ := maxB∈B |{i : Ti (cid:51) B}|/n. This is the normalized maximum number of
times that a block appears in the extended support. For example, if a block is present in all Ti, then
∆ = 1. If no two Ti share the same block, then ∆ = 1/n. We always have 1/n ≤ ∆ ≤ 1.
Theorem 2 (Convergence guarantee of PROXASAGA). Suppose τ ≤ 1
√
10
γ = a
in expectation at a geometric rate factor of at least: ρ(a) = 1
(1 − ρ)t ˜C0, where ˜C0 is a constant independent of t (≈ nκ

. For any step size
τ }, the inconsistent read iterates of Algorithm 1 converge
(cid:9), i.e. E(cid:107)ˆxt − x∗(cid:107)2 ≤

L with a ≤ a∗(τ ) := 1

a C0 with C0 as deﬁned in Theorem 1).

36 min{1, 6κ

5 min (cid:8) 1

n , a 1

∆

κ

√

This last result is similar to the original SAGA convergence result and our own Theorem 1, with both
an extra condition on τ and on the maximum allowable step size. In the best sparsity case, ∆ = 1/n
and we get the condition τ ≤
n/10. We now compare the geometric rate above to the one of Sparse
Proximal SAGA to derive the necessary conditions under which PROXASAGA is linearly faster.
Corollary 1 (Speedup). Suppose τ ≤ 1
. If κ ≥ n, then using the step size γ = 1/36L, PROXAS-
√
10
AGA converges geometrically with rate factor Ω( 1
κ ). If κ < n, then using the step size γ = 1/36nµ,
PROXASAGA converges geometrically with rate factor Ω( 1
n ). In both cases, the convergence rate
is the same as Sparse Proximal SAGA. Thus PROXASAGA is linearly faster than its sequential
counterpart up to a constant factor. Note that in both cases the step size does not depend on τ .

∆

Furthermore, if τ ≤ 6κ, we can use a universal step size of Θ(1/L) to get a similar rate for PROX-
ASAGA than Sparse Proximal SAGA, thus making it adaptive to local strong convexity since the
knowledge of κ is not required.

These speedup regimes are comparable with the best ones obtained in the smooth case, including Niu
et al. (2011); Reddi et al. (2015), even though unlike these papers, we support inconsistent reads
and nonsmooth objective functions. The one exception is Leblond et al. (2017), where the authors
prove that their algorithm, ASAGA, can obtain a linear speedup even without sparsity in the well-
conditioned regime. In contrast, PROXASAGA always requires some sparsity. Whether this property
for smooth objective functions could be extended to the composite case remains an open problem.

√

Relative to ASYSPCD, in the best case scenario (where the components of the gradient are uncorre-
√
lated, a somewhat unrealistic setting), ASYSPCD can get a near-linear speedup for τ as big as 4
p.
∆) is necessary for a linear speedup. This means in case ∆ ≤ 1/√
Our result states that τ = O(1/
p
our bound is better than the one obtained for ASYSPCD. Recalling that 1/n ≤ ∆ ≤ 1, it appears
that PROXASAGA is favored when n is bigger than
p whereas ASYSPCD may have a better bound
otherwise, though this comparison should be taken with a grain of salt given the assumptions we
had to make to arrive at comparable quantities. An extended comparison with the related work can
be found in Appendix D.

√

4 Experiments

In this section, we compare PROXASAGA with related methods on different datasets. Although
PROXASAGA can be applied more broadly, we focus on (cid:96)1 + (cid:96)2-regularized logistic regression, a
model of particular practical importance. The objective function takes the form

1
n

n
(cid:88)

i=1

log (cid:0)1 + exp(−bia

(cid:124)

i x)(cid:1) + λ1

2 (cid:107)x(cid:107)2

2 + λ2(cid:107)x(cid:107)1

,

(6)

where ai ∈ Rp and bi ∈ {−1, +1} are the data samples. Following Defazio et al. (2014), we set
λ1 = 1/n. The amount of (cid:96)1 regularization (λ2) is selected to give an approximate 1/10 nonzero

7

Table 1: Description of datasets.

Dataset

n

p

KDD 2010 (Yu et al., 2010)
KDD 2012 (Juan et al., 2016)
Criteo (Juan et al., 2016)

19,264,097
149,639,105
45,840,617

1,163,024
54,686,452
1,000,000

density
10−6
2 × 10−7
4 × 10−5

L

28.12
1.25
1.25

∆

0.15
0.85
0.89

Figure 1: Convergence for asynchronous stochastic methods for (cid:96)1 + (cid:96)2-regularized logistic
regression. Top: Suboptimality as a function of time for different asynchronous methods using 1
and 10 cores. Bottom: Running time speedup as function of the number of cores. PROXASAGA
achieves signiﬁcant speedups over its sequential version while being orders of magnitude faster than
competing methods. ASYSPCD achieves the highest speedups but it also the slowest overall method.

coefﬁcients. Implementation details are available in Appendix E. We chose the 3 datasets described
in Table 1

Results. We compare three parallel asynchronous methods on the aforementioned datasets: PROX-
ASAGA (this work),6 ASYSPCD, the asynchronous proximal coordinate descent method of Liu &
Wright (2015) and the (synchronous) FISTA algorithm (Beck & Teboulle, 2009), in which the gra-
dient computation is parallelized by splitting the dataset into equal batches. We aim to benchmark
these methods in the most realistic scenario possible; to this end we use the following step size:
1/2L for PROXASAGA, 1/Lc for ASYSPCD, where Lc is the coordinate-wise Lipschitz constant
of the gradient, while FISTA uses backtracking line-search. The results can be seen in Figure 1
(top) with both one (thus sequential) and ten processors. Two main observations can be made from
this ﬁgure. First, PROXASAGA is signiﬁcantly faster on these problems. Second, its asynchronous
version offers a signiﬁcant speedup over its sequential counterpart.

In Figure 1 (bottom) we present speedup with respect to the number of cores, where speedup is
computed as the time to achieve a suboptimality of 10−10 with one core divided by the time to
achieve the same suboptimality using several cores. While our theoretical speedups (with respect
to the number of iterations) are almost linear as our theory predicts (see Appendix F), we observe
a different story for our running time speedups. This can be attributed to memory access overhead,
which our model does not take into account. As predicted by our theoretical results, we observe

6A reference C++/Python implementation of is available at https://github.com/fabianp/ProxASAGA

8

a high correlation between the ∆ dataset sparsity measure and the empirical speedup: KDD 2010
(∆ = 0.15) achieves a 11x speedup, while in Criteo (∆ = 0.89) the speedup is never above 6x.

Note that although competitor methods exhibit similar or sometimes better speedups, they remain
orders of magnitude slower than PROXASAGA in running time for large sparse problems. In fact,
our method is between 5x and 80x times faster (in time to reach 10−10 suboptimality) than FISTA
and between 13x and 290x times faster than ASYSPCD (see Appendix F.3).

5 Conclusion and future work

In this work, we have described PROXASAGA, an asynchronous variance reduced algorithm with
support for composite objective functions. This method builds upon a novel sparse variant of the
(proximal) SAGA algorithm that takes advantage of sparsity in the individual gradients. We have
proven that this algorithm is linearly convergent under a condition on the step size and that it is
linearly faster than its sequential counterpart given a bound on the delay. Empirical benchmarks
show that PROXASAGA is orders of magnitude faster than existing state-of-the-art methods.

This work can be extended in several ways. First, we have focused on the SAGA method as the basic
iteration loop, but this approach can likely be extended to other proximal incremental schemes such
as SGD or ProxSVRG. Second, as mentioned in §3.3, it is an open question whether it is possible to
obtain convergence guarantees without any sparsity assumption, as was done for ASAGA.

Acknowledgements

The authors would like to thank our colleagues Damien Garreau, Robert Gower, Thomas Ker-
dreux, Geoffrey Negiar and Konstantin Mishchenko for their feedback on this manuscript, and Jean-
Baptiste Alayrac for support managing the computational resources.

This work was partially supported by a Google Research Award. FP acknowledges support from the
chaire ´Economie des nouvelles donn´ees with the data science joint research initiative with the fonds
AXA pour la recherche.

References

Bauschke, Heinz and Combettes, Patrick L. Convex analysis and monotone operator theory in

Hilbert spaces. Springer, 2011.

Beck, Amir and Teboulle, Marc. Gradient-based algorithms with applications to signal recovery.

Convex Optimization in Signal Processing and Communications, 2009.

Davis, Damek, Edmunds, Brent, and Udell, Madeleine. The sound of APALM clapping: faster
nonsmooth nonconvex optimization with stochastic asynchronous PALM. In Advances in Neural
Information Processing Systems 29, 2016.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural Infor-
mation Processing Systems, 2014.

Gu, Bin, Huo, Zhouyuan, and Huang, Heng. Asynchronous stochastic block coordinate descent

with variance reduction. arXiv preprint arXiv:1610.09447v3, 2016.

Hsieh, Cho-Jui, Yu, Hsiang-Fu, and Dhillon, Inderjit S. PASSCoDe: parallel asynchronous stochas-

tic dual coordinate descent. In ICML, 2015.

Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems, 2013.

Juan, Yuchin, Zhuang, Yong, Chin, Wei-Sheng, and Lin, Chih-Jen. Field-aware factorization ma-
chines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Sys-
tems. ACM, 2016.

9

Le Roux, Nicolas, Schmidt, Mark, and Bach, Francis R. A stochastic gradient method with an ex-
ponential convergence rate for ﬁnite training sets. In Advances in Neural Information Processing
Systems, 2012.

Leblond, R´emi, Pedregosa, Fabian, and Lacoste-Julien, Simon. ASAGA: asynchronous parallel
SAGA. Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS 2017), 2017.

Liu, Ji and Wright, Stephen J. Asynchronous stochastic coordinate descent: Parallelism and conver-

gence properties. SIAM Journal on Optimization, 2015.

Mania, Horia, Pan, Xinghao, Papailiopoulos, Dimitris, Recht, Benjamin, Ramchandran, Kannan,
and Jordan, Michael I. Perturbed iterate analysis for asynchronous stochastic optimization. SIAM
Journal on Optimization, 2017.

Meng, Qi, Chen, Wei, Yu, Jingcheng, Wang, Taifeng, Ma, Zhi-Ming, and Liu, Tie-Yan. Asyn-
chronous stochastic proximal optimization algorithms with variance reduction. In AAAI, 2017.

Nesterov, Yurii. Introductory lectures on convex optimization. Springer Science & Business Media,

Nesterov, Yurii. Gradient methods for minimizing composite functions. Mathematical Program-

2004.

ming, 2013.

Niu, Feng, Recht, Benjamin, Re, Christopher, and Wright, Stephen. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Sys-
tems, 2011.

Peng, Zhimin, Xu, Yangyang, Yan, Ming, and Yin, Wotao. ARock: an algorithmic framework for

asynchronous parallel coordinate updates. SIAM Journal on Scientiﬁc Computing, 2016.

Reddi, Sashank J, Hefny, Ahmed, Sra, Suvrit, Poczos, Barnabas, and Smola, Alexander J. On
variance reduction in stochastic gradient descent and its asynchronous variants. In Advances in
Neural Information Processing Systems, 2015.

Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Minimizing ﬁnite sums with the stochastic

average gradient. Mathematical Programming, 2016.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research, 2013.

Shalev-Shwartz, Shai et al.
arXiv:1211.2717, 2012.

Proximal stochastic dual coordinate ascent.

arXiv preprint

Xiao, Lin and Zhang, Tong. A proximal stochastic gradient method with progressive variance re-

duction. SIAM Journal on Optimization, 2014.

You, Yang, Lian, Xiangru, Liu, Ji, Yu, Hsiang-Fu, Dhillon, Inderjit S, Demmel, James, and Hsieh,
Cho-Jui. Asynchronous parallel greedy coordinate descent. In Advances In Neural Information
Processing Systems, 2016.

Yu, Hsiang-Fu, Lo, Hung-Yi, Hsieh, Hsun-Ping, Lou, Jing-Kai, McKenzie, Todd G, Chou, Jung-
Wei, Chung, Po-Han, Ho, Chia-Hua, Chang, Chun-Fu, Wei, Yin-Hsuan, et al. Feature engineering
and classiﬁer ensemble for KDD cup 2010. In KDD Cup, 2010.

Zhao, Tuo, Yu, Mo, Wang, Yiming, Arora, Raman, and Liu, Han. Accelerated mini-batch random-
ized block coordinate descent method. In Advances in neural information processing systems,
2014.

10

Breaking the Nonsmooth Barrier: A Scalable Parallel
Method for Composite Optimization

Supplementary material

Notations. Throughout the supplementary material we use the following extra notation. We denote
by (cid:104)·, ·(cid:105)(i) (resp. (cid:107) · (cid:107)(i)) the scalar product (resp. norm) restricted to blocks in Ti, i.e., (cid:104)x, y(cid:105)(i) :=
(cid:104)[x]B, [y]B(cid:105) and (cid:107)x(cid:107)(i) := (cid:112)(cid:104)x, x(cid:105)(i). We will also use the following deﬁnitions: ϕ :=
(cid:80)
(cid:80)

B∈Ti
B∈B dBhB(x) and D is the diagonal matrix deﬁned block-wise as [D]B,B = dBI |B|.

The Bregman divergence associated with a convex function f for points x, y in its domain is
deﬁned as:

Bf (x, y) := f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105) .

Note that this is always positive due to the convexity of f .

Appendix A Basic properties

Lemma 1. For any µ-strongly convex function f we have the following inequality:

(cid:104)∇f (y) − ∇f (x), y − x(cid:105) ≥

(cid:107)y − x(cid:107)2 + Bf (x, y) .

µ
2

Proof. By strong convexity, f veriﬁes the inequality:

f (y) ≤ f (x) + (cid:104)∇f (y), y − x(cid:105) −

(cid:107)y − x(cid:107)2 ,

µ
2

for any x, y in the domain (see e.g. (Nesterov, 2004)). We then have the equivalences:

f (x) ≤ f (y) + (cid:104)∇f (x), x − y(cid:105) −

(cid:107)x − y(cid:107)2

µ
2

⇐⇒

(cid:107)x − y(cid:107)2 + f (x) − f (y) ≤ (cid:104)∇f (x), x − y(cid:105)

⇐⇒

(cid:107)x − y(cid:107)2 + f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105)
(cid:125)

(cid:124)

(cid:123)(cid:122)
Bf (x,y)

µ
2
µ
2

≤ (cid:104)∇f (x) − ∇f (y), x − y(cid:105) ,

(10)

where in the last line we have subtracted (cid:104)∇f (y), x − y(cid:105) from both sides of the inequality.

Lemma 2. Let the fi be L-smooth and convex functions. Then it is veriﬁed that:

1
n

n
(cid:88)

i=1

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ 2LBf (x, y) .

Proof. Since each fi is L-smooth, it is veriﬁed (see e.g. Nesterov (2004, Theorem 2.1.5)) that

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ 2L(cid:0)fi(x) − fi(y) − (cid:104)∇fi(y), x − y(cid:105)(cid:1) .

The result is obtained by averaging over i.

(7)

(8)

(9)

(11)

(12)

Lemma 3 (Characterization of the proximal operator). Let h be convex lower semicontinuous. Then
we have the following characterization of the proximal operator:

z = proxγh(x) ⇐⇒

(x − z) ∈ ∂h(z) .

(13)

1
γ

11

Proof. This is a direct consequence of the ﬁrst order optimality conditions on the deﬁnition of
proximal operator, see e.g. (Beck & Teboulle, 2009; Nesterov, 2013).

Lemma 4 (Firm non-expansiveness). Let x, ˜x be two arbitrary elements in the domain of ϕi and
z, ˜z be deﬁned as z := proxϕi

(˜x). Then it is veriﬁed that:

(x), ˜z := proxϕi

(cid:104)z − ˜z, x − ˜x(cid:105)(i) ≥ (cid:107)z − ˜z(cid:107)2

(i) .

Proof. By the block-separability of ϕi, the proximal operator is the concatenation of the proximal
operators of the blocks. In other words, for any block B ∈ Ti we have:

[z]B = proxγϕB ([x]B) ,

[˜z]B = proxγϕB ([˜x]B) ,

where ϕB is the restriction of ϕi to B. By ﬁrm non-expansiveness of the proximal operator (see
e.g. Bauschke & Combettes (2011, Proposition 4.2)) we have that:

(cid:104)[z]B − [˜z]B, [x]B − [˜x]B(cid:105) ≥ (cid:107)[z]B − [˜z]B(cid:107)2 .

Summing over the blocks in Ti yields the desired result.

(14)

(15)

12

Appendix B Sparse Proximal SAGA

This Appendix contains all proofs for Section 2. The main result of this section is Theorem 1, whose
proof is structured as follows:

• We start by proving four auxiliary results that will be used later on in the proofs of both
synchronous and asynchronous variants. The ﬁrst is the unbiasedness of key quantities used
in the algorithm. The second is a characterization of the solutions of (OPT) in terms of f
and ϕ (deﬁned below) in Lemma 6. The third is a key inequality in Lemma 7 that relates
the gradient mapping to other terms that arise in the optimization. The fourth is an upper
bound on the variance terms of the gradient estimator, relating it to the Bregman divergence
of f and the past gradient estimator terms.

• In Lemma 9, we deﬁne an upper bound on the iterates (cid:107)xt − x∗(cid:107)2, called a Lyapunov
function, and prove an inequality that relates this Lyapunov function value at the current
iterate with its value at the previous iterate.

• Finally, in the proof of Theorem 1 we use the previous inequality in terms of the Lyapunov

function to prove a geometric convergence of the iterates.

We start by proving the following unbiasedness result, mentioned in §2.

Lemma 5. Let Di and ϕi be deﬁned as in §2. Then it is veriﬁed that EDi = I p and E ϕi = h.

Proof. Let B ∈ B an arbitrary block. We have the following sequence of equalities:

where the last equality comes from the deﬁnition of nB. EDi = I p then follows from the arbitrari-
ness of B.

Similarly, for ϕi we have:

E[Di]B,B =

[Di]B,B =

dB1{B ∈ Ti}I |B|

1
n

n
(cid:88)

i=1

(cid:33)

1{B ∈ Ti}I |B|

1
n

1
n
(cid:32)

n
(cid:88)

i=1
n
(cid:88)

i=1

1
nB

n
nB

n
(cid:88)

i=1

=

=

1{B ∈ Ti}

I |B| = I |B| ,

Eϕi([x]B) =

dB1{B ∈ Ti}hB([x]B)

1
n

1
n
(cid:32)

n
(cid:88)

i=1
n
(cid:88)

i=1

1
nB

n
nB

n
(cid:88)

i=1

=

=

1{B ∈ Ti}hB([x]B)

(cid:33)

1{B ∈ Ti}

hB([x]B) = hB([x]B) ,

Finally, the result E ϕi = h comes from adding over all blocks.

Lemma 6. x∗ is a solution to (OPT) if and only if the following condition is veriﬁed:

x∗ = proxγϕ

(cid:0)x∗ − γD∇f (x∗)(cid:1) .

Proof. By the ﬁrst order optimality conditions, the solutions to (OPT) are characterized by the sub-
differential inclusion −∇f (x∗) ∈ ∂h(x∗). We can then write the following sequence of equiva-

13

(16)

(17)

(18)

(19)

(20)

(21)

(22)

lences:

−∇f (x∗) ∈ ∂h(x∗) ⇐⇒ −D∇f (x∗) ∈ D∂h(x∗)

(multiplying by D, equivalence since diagonals are nonzero)

⇐⇒ −D∇f (x∗) ∈ ∂ϕ(x∗)
(by deﬁnition of ϕ)

⇐⇒

(x∗ − γD∇f (x∗) − x∗) ∈ ∂ϕ(x∗)

1
γ

(adding and subtracting x∗)

⇐⇒ x∗ = proxγϕ(x∗ − γD∇f (x∗)) .

(by Lemma 3)

(23)

Since all steps are equivalences, we have the desired result.

The following lemma will be key in the proof of convergence for both the sequential and the parallel
versions of the algorithm. With this result, we will be able to bound the product between the gradient
mapping and the iterate suboptimality by:

• First, the negative norm of the gradient mapping, which will be key in the parallel setting

to cancel out the terms arising from the asynchrony.

• Second, variance terms in (cid:107)vi−Di∇f (x∗)(cid:107)2 that we will be able to bound by the Bregman

divergence using Lemma 2.

• Third and last, a product with terms in (cid:104)vi − Di∇f (x∗), x − x∗(cid:105), which taken in expec-
tation gives (cid:104)∇f (x) − ∇f (x∗), x − x∗(cid:105) and will allow us to apply Lemma 1 to obtain the
contraction terms needed to obtain a geometric rate of convergence.

Lemma 7 (Gradient mapping inequality). Let x be an arbitrary vector, x∗ a solution to (OPT), vi
as deﬁned in (SPS) and g = g(x, vi, i) the gradient mapping deﬁned in (3). Then the following
inequality is veriﬁed for any β > 0:

(cid:104)g, x − x∗(cid:105) ≥ −

(β − 2)(cid:107)g(cid:107)2 −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + (cid:104)vi − Di∇f (x∗), x − x∗(cid:105) .

(24)

γ
2

γ
2β

Proof. By ﬁrm non-expansiveness of the proximal operator (Lemma 4) applied to z = proxγϕi(x−
γvi) and ˜z = proxγϕi(x∗ − γD∇f (x∗)) we have:

(cid:107)z − ˜z(cid:107)2

(i) − (cid:104)z − ˜z, x − γvi − x∗ + γD∇f (x∗)(cid:105)(i) ≤ 0 .

(25)

By the (SPS) iteration we have x+ = z and by Lemma 3 we have that [z]Ti = [x∗]Ti, hence the
above can be rewritten as

(cid:107)x+ − x∗(cid:107)2

(i) − (cid:104)x+ − x∗, x − γvi − x∗ + γD∇f (x∗)(cid:105)(i) ≤ 0 .

(26)

14

We can now write the following sequence of inequalities

(cid:104)γg, x − x∗(cid:105) = (cid:104)x − x+, x − x∗(cid:105)(i)

(by deﬁnition and sparsity of g)

(cid:16)

≥

1 −

(cid:17)

β
2

(cid:16)

≥

1 −

(cid:16)

=

1 −

(cid:17)

(cid:17)

β
2

β
2

= (cid:104)x − x+ + x∗ − x∗, x − x∗(cid:105)(i)
= (cid:107)x − x∗(cid:107)2
≥ (cid:107)x − x∗(cid:107)2

(i) − (cid:104)x+ − x∗, x − x∗(cid:105)(i)
(i) − (cid:104)x+ − x∗, 2x − γvi − 2x∗ + γD∇f (x∗)(cid:105)(i) + (cid:107)x+ − x∗(cid:107)2
(i)

(27)

(adding Eq. (26))

= (cid:107)x − x+(cid:107)2
= (cid:107)x − x+(cid:107)2

(i) + (cid:104)x+ − x∗, γvi − γD∇f (x∗)(cid:105)(i)
(i) + (cid:104)x − x∗, γvi − γD∇f (x∗)(cid:105)(i) − (cid:104)x − x+, γvi − γD∇f (x∗)(cid:105)(i)

(completing the square)

(adding and substracting x)

(cid:107)x − x+(cid:107)2

(i) −

(cid:107)vi − D∇f (x∗)(cid:107)2

(i) + γ(cid:104)vi − D∇f (x∗), x − x∗(cid:105)(i)

(Young’s inequality 2(cid:104)a, b(cid:105) ≤

+ β(cid:107)b(cid:107)2, valid for arbitrary β > 0)

γ2
2β

γ2
2β

(cid:107)a(cid:107)2
β

(cid:107)x − x+(cid:107)2

(i) −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + γ(cid:104)vi − Di∇f (x∗), x − x∗(cid:105)

(by deﬁnition of Di and using the fact that vi is Ti-sparse)

(cid:107)γg(cid:107)2 −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + γ(cid:104)vi − D∇f (x∗), x − x∗(cid:105) ,

(28)

γ2
2β

where in the last inequality we have used the fact that g is Ti-sparse. Finally, dividing by γ both
sides yields the desired result.

Lemma 8 (Upper bound on the gradient estimator variance). For arbitrary vectors x, (αi)n
vi as deﬁned in (SPS) we have:

i=0, and

E(cid:107)vi − Di∇f (x∗)(cid:107)2 ≤ 4LBf (x, x∗) + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(29)

Proof. We will now bound the variance terms. For this we have:

E(cid:107)vi − D∇f (x∗)(cid:107)2

(i) = E(cid:107)∇fi(x) − ∇fi(x∗) + ∇fi(x∗) − αi + Diα − D∇f (x∗)(cid:107)2
(i)

≤ 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)∇fi(x∗) − αi − (D∇f (x∗) − Dα)(cid:107)2
(i)

(by inequality (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2)
= 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)∇fi(x∗) − αi(cid:107)2

− 4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)(i) + 2E(cid:107)D∇f (x∗) − Dα(cid:107)2
(developing the square)

(i) .

(30)

We will now simplify the last two terms in the above expression. For the ﬁrst of the two last terms
we have:

− 4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)(i) = −4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)

(31)

(support of ﬁrst term)

= −4(cid:104)∇f (x∗) − α, D∇f (x∗) − Dα(cid:105)
= −4(cid:107)∇f (x∗) − α(cid:107)2

D .

Similarly, for the last term we have:

2E(cid:107)D∇f (x∗) − Dα(cid:107)2

(i) = 2E(cid:104)Di∇f (x∗) − Diα, D∇f (x∗) − Dα(cid:105)

= 2(cid:104)∇f (x∗) − α, D∇f (x∗) − Dα(cid:105)

(32)

(33)

(using Lemma 5)
D .

= 2(cid:107)∇f (x∗) − α(cid:107)2

15

and so the addition of these terms is negative and can be dropped. In all, for the variance terms we
have

E(cid:107)vi − D∇f (x∗)(cid:107)2

(i) ≤ 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2

≤ 4LBf (x, x∗) + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(by Lemma 2)

(34)

c
n

n
(cid:88)

i=1

(cid:16)

We now deﬁne an upper bound on the quantity that we would like to bound, often called a Lyapunov
function, and establish a recursive inequality on this Lyapunov function.

Lemma 9 (Lyapunov inequality). Let L be the following c-parametrized function:

L(x, α) := (cid:107)x − x∗(cid:107)2 +

(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(35)

Let x+ and α+ be obtained from the Sparse Proximal SAGA updates (SPS). Then we have:

EL(x+, α+) − L(x, α) ≤ − γµ(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:17)

c
n

(cid:16)

+

2γ2 −

(cid:17)

c
n

E(cid:107)αi − ∇fi(x)(cid:107)2 .

Proof. For the ﬁrst term of L we have:

(cid:107)x+ − x∗(cid:107)2 = (cid:107)x − γg − x∗(cid:107)2

(g := g(x, vi, i))

= (cid:107)x − x∗(cid:107)2 − 2γ(cid:104)g, x − x∗(cid:105) + (cid:107)γg(cid:107)2
≤ (cid:107)x − x∗(cid:107)2 + γ2(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γ(cid:104)vi − Di∇f (x∗), x − x∗(cid:105)

(by Lemma 7 with β = 1)

Since vi is an unbiased estimator of the gradient and EDi = I p, taking expectations we have:

E(cid:107)x+ − x∗(cid:107)2 ≤ (cid:107)x − x∗(cid:107)2 + γ2E(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γ(cid:104)∇f (x) − ∇f (x∗), x − x∗(cid:105)

≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + γ2E(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γBf (x, x∗) .

(38)

(by Lemma 1)

By using the variance terms bound (Lemma 8) in the previous equation we have:

E(cid:107)x+ − x∗(cid:107)2 ≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + (4Lγ2 − 2γ)Bf (x, x∗)
+ 2γ2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

We will now bound the second term of the Lyapunov function. We have:

1
n

n
(cid:88)

i=1

(cid:107)α+

i − ∇fi(x∗)(cid:107)2 =

1 −

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2

(cid:18)

(cid:19)

1
n

1
n

(by deﬁnition of α+)

(cid:18)

(cid:19)

≤

1 −

1
n

2
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

LBf (x, x∗) .

(by Lemma 2)

(36)

(37)

(39)

(40)

(41)

16

Combining Eq. (39) and (40) we have:

EL(x+, α+) ≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + (4Lγ2 − 2γ)Bf (x, x∗) + 2γ2E(cid:107)αi − ∇fi(x∗)(cid:107)2

(cid:20)(cid:18)

+ c

1 −

(cid:19)

1
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

(cid:21)
2LBf (x, x∗)

= (1 − γµ)(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:16)

1
n
c
n

(cid:17)

(cid:16)

+

2γ2 −

(cid:17)

c
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 + cE(cid:107)αi − ∇fi(x∗)(cid:107)2

= L(x, α) − γµ(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:16)

(cid:17)

c
n

c
n
Finally, subtracting L(x, α) from both sides yields the desired result.

E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

2γ2 −

+

(cid:16)

(cid:17)

(42)

Theorem 1. Let γ = a
converges geometrically in expectation with a rate factor of at least ρ = 1
xt obtained after t updates and x∗ the solution to (OPT), we have the bound:
i=1 (cid:107)α0

5L for any a ≤ 1 and f be µ-strongly convex. Then Sparse Proximal SAGA
κ }. That is, for

with C0 := (cid:107)x0 − x∗(cid:107)2 + 1
5L2

E(cid:107)xt − x∗(cid:107)2 ≤ (1 − ρ)tC0 ,

i − ∇fi(x∗)(cid:107)2

5 min{ 1

n , a 1

(cid:80)n

.

Proof. Let H := 1
n

(cid:80)

ELt+1 − (1 − ρ)Lt ≤ ρLt − γµ(cid:107)xt − x∗(cid:107)2 +

i (cid:107)αi − ∇fi(x∗)(cid:107)2. By the Lyapunov inequality from Lemma 9, we have:
(cid:17)
c
4Lγ2 − 2γ + 2L
n

c
n

(cid:16)

(cid:17)

(cid:16)

H

= (ρ − γµ) (cid:107)xt − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (xt, x∗) +

(cid:17)

c
n

(cid:17)

c
n

Bf (xt, x∗) +
(cid:20)
2γ2 + c

ρ −

(cid:18)

2γ2 −
(cid:19)(cid:21)

H

1
n

(cid:18)

(cid:19)

H

2c
3n

≤ (ρ − γµ) (cid:107)xt − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (xt, x∗) +

2γ2 −

(cid:16)

(cid:16)

(by deﬁnition of Lt)

(choosing ρ ≤

1
3n

)

= (ρ − γµ) (cid:107)xt − x∗(cid:107)2 + (cid:0)10Lγ2 − 2γ(cid:1) Bf (xt, x∗)

(choosing

= 3γ2)

c
n

(cid:16)

(cid:17)

aµ
5L

≤

ρ −

(cid:107)xt − x∗(cid:107)2
µ
a
L
5
And so we have the bound:

(for ρ ≤

≤ 0 .

·

)

(for all γ =

, a ≤ 1)

a
5L

(cid:18)

ELt+1 ≤

1 − min

(cid:110) 1
3n

,

a
5

·

1
κ

(cid:111)(cid:19)

(cid:18)

Lt ≤

min

, a ·

(cid:110) 1
n

(cid:111)(cid:19)

1
κ

Lt ,

1 −

1
5
3n ≤ 1

5n merely for clarity of exposition.

where in the last inequality we have used the trivial bound 1
Chaining expectations from t to 0 we have:
(cid:111)(cid:19)t+1

(cid:18)

ELt+1 ≤

1 −

min

, a ·

1
5

1
5

1
5

(cid:110) 1
n

(cid:110) 1
n

(cid:110) 1
n

L0
(cid:111)(cid:19)t+1 (cid:32)

(cid:111)(cid:19)t+1 (cid:32)

1
κ

1
κ

1
κ

(cid:18)

(cid:18)

(since a ≤ 1 and 3/5 ≤ 1) .

=

1 −

min

, a ·

(cid:107)x0 − x∗(cid:107)2 +

(cid:107)α0

i − ∇fi(x∗)(cid:107)2

≤

1 −

min

, a ·

(cid:107)x0 − x∗(cid:107)2 +

(cid:107)α0

i − ∇fi(x∗)(cid:107)2

(45)

3a2
52L2

1
5L2

n
(cid:88)

i=1
n
(cid:88)

i=1

The fact that Lt is a majorizer of (cid:107)xt − x∗(cid:107)2 completes the proof.

(43)

(44)

(cid:33)

(cid:33)

17

Appendix C ProxASAGA

In this Appendix we provide the proofs for results from Section 3, that is Theorem 2 (the convergence
theorem for PROXASAGA) and Corollary 1 (its speedup result).

Notation. Through this section, we use the following shorthand for the gradient mapping: gt :=
g(ˆxt, ˆvt
it

, it).

Appendix C.1 Proof outline.

As in the smooth case (h = 0), we start by using the deﬁnition of xt+1 in Eq. (4) to relate the
distance to the optimum in terms of its previous iterates:

(cid:107)xt+1 − x∗(cid:107)2 =(cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)ˆxt − xt, gt(cid:105) + γ2(cid:107)gt(cid:107)2 − 2γ(cid:104)ˆxt − x∗, gt(cid:105) .

(46)

However, in this case gt is not a gradient estimator but a gradient mapping, so we cannot continue
as is customary – by using the unbiasedness of the gradient in the (cid:104)ˆxt − x∗, gt(cid:105) term together with
the strong convexity of f (see Leblond et al. (2017, Section 3.5)).

To circumvent this difﬁculty, we derive a tailored inequality for the gradient mapping (Lemma 7
in Appendix B), which in turn allows us to use the classical unbiasedness and strong convexity
arguments to get the following inequality:

at+1 ≤ (1 −

γµ
2

)at + γ2E(cid:107)gt(cid:107)2 − 2γEBf (ˆxt, x∗) + γµE(cid:107)ˆxt − x(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105)
(cid:125)
(cid:124)

(cid:123)(cid:122)
additional asynchrony terms

(47)

+γ2(β − 2)E(cid:107)gt(cid:107)2 +
(cid:124)

γ2
β
(cid:123)(cid:122)
additional proximal and variance terms

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2
(cid:125)

,

where at := E(cid:107)xt − x∗(cid:107)2. Note that since f is strongly convex, Bf (ˆxt, x∗) ≥ µ
In the smooth setting, one ﬁrst expresses the additional asynchrony terms as linear combinations
of past gradient variance terms (E(cid:107)gu(cid:107)2)0≤u≤t. Then one crucially uses the negative Bregman
divergence term to control the variance terms. However, in our current setting, we cannot relate the
norm of the gradient mapping E(cid:107)gt(cid:107)2 to the Bregman divergence (from which h is absent). Instead,
we use the negative term γ2(β − 1)E(cid:107)gt(cid:107)2 to control all the (E(cid:107)gu(cid:107)2)0≤u≤t terms that arise from
asynchrony.

2 (cid:107)ˆxt − x∗(cid:107)2.

The rest of the proof consists in:
i) expressing the additional asynchrony terms as linear combinations of (E(cid:107)gu(cid:107)2)0≤u≤t, follow-
ing Leblond et al. (2017, Lemma 1);
ii) expressing the last variance term, (cid:107)ˆvt
it
divergences (Lemma 8 in Appendix B and Lemma 2 from Leblond et al. (2017));
iii) deﬁning a Lyapunov function, Lt := (cid:80)t
contraction given conditions on the maximum step size and delay.

u=0(1 − ρ)t−uau, and proving that it is bounded by a

− Dit∇f (x∗)(cid:107)2, as a linear combination of past Bregman

Appendix C.2 Detailed proof

Theorem 2 (Convergence guarantee and rate of PROXASAGA). Suppose τ ≤ 1
√
10
36 min{1, 6κ
size γ = a
expectation at a geometric rate factor of at least: ρ(a) = 1
(1 − ρ)t ˜C0, where ˜C0 is a constant independent of t (≈ nκ

. For any step
τ }, the inconsistent read iterates of Algorithm 1 converge in
(cid:9), i.e. E(cid:107)ˆxt − x∗(cid:107)2 ≤
5 min (cid:8) 1

a C0 with C0 as deﬁned in Theorem 1).

L with a ≤ 1

n , a 1

∆

κ

Proof. In order to get an initial recursive inequality, we ﬁrst unroll the (virtual) update:
(cid:107)xt+1 − x∗(cid:107)2 = (cid:107)xt − γgt − x∗(cid:107)2 = (cid:107)xt − x∗(cid:107)2 + (cid:107)γgt(cid:107)2 − 2γ(cid:104)gt, xt − x∗(cid:105)

= (cid:107)xt − x∗(cid:107)2 + (cid:107)γgt(cid:107)2 − 2γ(cid:104)gt, ˆxt − x∗(cid:105) + 2γ(cid:104)gt, ˆxt − xt(cid:105) ,

(48)

18

and then apply Lemma 7 with x = ˆxt and v = ˆvt
(cid:104)·(cid:105)(i) = (cid:104)·(cid:105)(it).

it. Note that in this case we have g = gt and

(cid:107)xt+1 − x∗(cid:107)2 ≤ (cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)gt, ˆxt − xt(cid:105) + γ2(cid:107)gt(cid:107)2 + γ2(β − 2)(cid:107)gt(cid:107)2

+

γ2
β

(cid:107)ˆvt
it

− D∇f (x∗)(cid:107)2

(it) − 2γ(cid:104)ˆvt
it

− D∇f (x∗), ˆxt − x∗(cid:105)(it)

= (cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)(cid:107)gt(cid:107)2

− Dit∇f (x∗)(cid:107)2 − 2γ(cid:104)ˆvt
it

− Dit∇f (x∗), ˆxt − x∗(cid:105).

(49)

+

(cid:107)ˆvt
it

γ2
β
(as [ˆvt
it

]Tit

= ˆvt

it)

We now use the property that it is independent of ˆxt (which we enforce by reading ˆxt before picking
it, see Section 3), together with the unbiasedness of the gradient update ˆvt
= ∇f (ˆxt)) and
the deﬁnition of D to simplify the following expression as follows:

it (Eˆvt
it

E(cid:104)ˆvt
it

− Dit∇f (x∗), ˆxt − x∗(cid:105) = (cid:104)∇f (ˆxt) − ∇f (x∗), ˆxt − x∗(cid:105)
µ
(cid:107)ˆxt − x∗(cid:107)2 + Bf (ˆxt, x∗) ,
2

≥

where the last inequality comes from Lemma 1. Taking conditional expectations on (49) we get:

E(cid:107)xt+1 − x∗(cid:107)2 ≤ (cid:107)xt − x∗(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)E(cid:107)gt(cid:107)2

+

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2 − γµ(cid:107)ˆxt − x∗(cid:107)2 − 2γBf (ˆxt, x∗)

≤ (1 −

)(cid:107)xt − x∗(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)E(cid:107)gt(cid:107)2

+

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2 + γµ(cid:107)ˆxt − xt(cid:107)2 − 2γBf (ˆxt, x∗)

(using (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2 on (cid:107)xt − ˆxt + ˆxt − x∗(cid:107)2)

γ2
β
γµ
2
γ2
β

γµ
2

≤ (1 −

)(cid:107)xt − x∗(cid:107)2 + γ2(β − 1)E(cid:107)gt(cid:107)2 + γµ(cid:107)ˆxt − xt(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105)
2γ2
β

− ∇fit (x∗)(cid:107)2 .

Bf (ˆxt, x∗) +

4γ2L
β

E(cid:107) ˆαt
it

(52)

− 2γBf (ˆxt, x∗) +

(using Lemma 8 on the variance terms)

Since we also have:

ˆxt − xt = γ

Gt

ug(ˆxu, ˆαu, iu),

t−1
(cid:88)

u=(t−τ )+

the effect of asynchrony for the perturbed iterate updates was already derived in a very similar setup
in Leblond et al. (2017). We re-use the following bounds from their Appendix C.4:7

E(cid:107)ˆxt − xt(cid:107)2 ≤ γ2(1 +

∆τ )

E(cid:107)gu(cid:107)2 ,

√

t−1
(cid:88)

u=(t−τ )+

Leblond et al. (2017, Eq. (48))

E(cid:104)gt, ˆxt − xt(cid:105) ≤

E(cid:107)gu(cid:107)2 +

E(cid:107)gt(cid:107)2 . Leblond et al. (2017, Eq. (46)).

√

γ

∆

2

t−1
(cid:88)

u=(t−τ )+

√

γ

∆τ
2

7The appearance of the sparsity constant ∆ is coming from the crucial property that E(cid:107)x(cid:107)2
∀x ∈ Rp (see Eq. (39) in Leblond et al. (2017), where they use the notation (cid:107) · (cid:107)i for our (cid:107) · (cid:107)(i)).

(i) ≤ ∆(cid:107)x(cid:107)2

19

(50)

(51)

(53)

(54)

(55)

Because the updates on α are the same for PROXASAGA as for ASAGA, we can re-use the same
argument arising in the proof of Leblond et al. (2017, Lemma 2) to get the following bound on
E(cid:107) ˆαt
it

− ∇fit(x∗)(cid:107)2:

E(cid:107) ˆαt
it

− ∇fit(x∗)(cid:107)2 ≤

(1 −

)(t−2τ −u−1)+EBf (ˆxu, x∗)

+2L(1 −

)(t−τ )+ ˜e0 ,

(56)

1
n

2L
n

t−1
(cid:88)

u=1
(cid:124)

1
n

(cid:123)(cid:122)
Henceforth denoted Ht

(cid:125)

E(cid:107)α0

i − f (cid:48)

where ˜e0 := 1
i (x∗)(cid:107)2. This bound is obtained by analyzing which gradient could be
2L
the source of αit in the past (taking in consideration the inconsistent writes), and then applying
Lemma 2 on the E(cid:107)∇f (ˆxu) − ∇f (x∗)(cid:107)2 terms, explaining the presence of Bf (ˆxu, x∗) terms.8
The inequality (56) corresponds to Eq. (56) and (57) in Leblond et al. (2017).

By taking the full expectation of (52) and plugging the above inequalities back, we obtain an in-
equality similar to Leblond et al. (2017, Master inequality (28)) which describes how the error terms
at := E(cid:107)xt − x∗(cid:107)2 of the virtual iterates are related:

at+1 ≤(1 −

)at +

(1 −

)(t−τ )+ ˜e0

γµ
2
+ γ2 (cid:104)

4γ2L
β
√

1
n

β − 1 +

∆τ

(cid:105)

E(cid:107)gt(cid:107)2 +

√

(cid:104)

γ2

∆ + γ3µ(1 +

√

(cid:105)
∆τ )

t
(cid:88)

E(cid:107)gu(cid:107)2

(57)

u=(t−τ )+

− 2γEBf (ˆxt, x∗) +

EBf (ˆxt, x∗) +

4γ2L
β

4γ2L
βn

Ht .

We now have a promising inequality with a contractive term and several quantities that we need to
bound. In order to achieve our ﬁnal result, we introduce the same Lyapunov function as in Leblond
et al. (2017):

Lt :=

(1 − ρ)t−uau ,

t
(cid:88)

u=0

where ρ is a target rate factor for which we will provide a value later on. Proving that this Lyapunov
function is bounded by a contraction will ﬁnish our proof. We have:

Lt+1 =

(1 − ρ)t+1−uau = (1 − ρ)t+1a0 +

(1 − ρ)t+1−uau

t+1
(cid:88)

u=0

= (1 − ρ)t+1a0 +

(1 − ρ)t−uau+1 .

(58)

t+1
(cid:88)

u=1
t
(cid:88)

u=0

We now plug our new bound on at+1, (57):
(1 − ρ)t−u(cid:104)

Lt+1 ≤ (1 − ρ)t+1a0 +

t
(cid:88)

u=0

)(u−τ )+ ˜e0

(1 −

)au +

γµ
2
+ γ2(cid:0)β − 1 +
√

+ (cid:0)γ2

(1 −

4γ2L
1
β
n
∆τ (cid:1)E(cid:107)gu(cid:107)2
√

√

∆ + γ3µ(1 +

∆τ )(cid:1)

u
(cid:88)

E(cid:107)gv(cid:107)2

v=(u−τ )+

− 2γEBf (ˆxu, x∗) +

EBf (ˆxu, x∗) +

4γ2L
β

4γ2L
βn

(cid:105)

Hu

.

(59)

After regrouping similar terms, we get:

Lt+1 ≤ (1 − ρ)t+1(a0 + A˜e0) + (1 −

)Lt +

st
u

E(cid:107)gu(cid:107)2 +

rt
u

EBf (ˆxu, x∗) .

(60)

γµ
2

t
(cid:88)

u=0

t
(cid:88)

u=1

8Note that Leblond et al. (2017) analyzed the unconstrained scenario, and so Bf (ˆxu, x∗) is replaced by the

simpler f (ˆxu) − f (x∗) in their bound.

20

Now, provided that we can prove that under certain conditions the st
u terms are all negative
(and that the A term is not too big), we can drop them from the right-hand side of (60) which will
allow us to ﬁnish the proof.
Let us compute these terms. Let q := 1−1/n

1−ρ and we assume in the rest that ρ < 1/n.

u and rt

Computing A. We have:

4γ2L
β

t
(cid:88)

u=0

(1 − ρ)t−u(1 −

)(u−τ )+ ≤

(1 − ρ)t(1 − ρ)−τ (τ + 1 +

1
n

4γ2L
β

1
1 − q

)

from Leblond et al. (2017, Eq (75))

= (1 − ρ)t+1 4γ2L
β

(cid:124)

(1 − ρ)−τ −1(τ + 1 +

)

.

(61)

1
1 − q

(cid:125)

(cid:123)(cid:122)
:=A

E(cid:107)gu(cid:107)2 ≤ τ (1 − ρ)−τ

(1 − ρ)t−uE(cid:107)gu(cid:107)2 ,

(62)

Computing st

u. Since we have:
t
(cid:88)

(1 − ρ)t−u

u−1
(cid:88)

u=0

v=(u−τ )+

we have for all 0 ≤ u ≤ t:
u ≤ (1 − ρ)t−u(cid:104)
st

t
(cid:88)

u=0

√

γ2(cid:0)β − 1 +

∆τ ) + τ (1 − ρ)−τ (cid:0)γ2

∆ + γ3µ(1 +

√

√

∆τ )(cid:1)(cid:105)

.

(63)

u. To analyze these quantities, we need to compute: (cid:80)t

Computing rt
v=1 (1 −
1
n )(u−2τ −v−1)+. Fortunately, this is already done in Leblond et al. (2017, Eq (66)), and thus we
know that for all 1 ≤ u ≤ t:

u=0(1 − ρ)t−u (cid:80)u−1

u ≤ (1 − ρ)t−u
rt

−2γ +

(cid:20)

4γ2L
β

+

4Lγ2
nβ

(1 − ρ)−2τ −1(cid:16)

2τ +

(cid:17)(cid:21)

,

1
1 − q

(64)

recalling that q := 1−1/n

1−ρ and that we assumed ρ < 1
n .

We now need some assumptions to further analyze these quantities. We make simple choices for
simplicity, though a tighter analysis is possible. To get manageable (and simple) constants, we
follow Leblond et al. (2017, Eq. (82) and (83)) and assume:

ρ ≤

τ ≤

1
4n

;

n
10

.

This tells us:

1
1 − q

≤

4n
3
4
3

(1 − ρ)−kτ −1 ≤

for 0 ≤ k ≤ 2 .

(using Bernouilli’s inequality)

Additionally, we set β = 1

2 . Equation (63) thus becomes:
(cid:0)√

√

(cid:20)

−

+

∆τ +

u ≤ γ2(1 − ρ)t−u
st

1
2

4
3

∆τ + γµτ (1 +

√

(cid:21)

∆τ )(cid:1)

.

We see that for st
get:

u to be negative, we need τ = O( 1√
∆

). Let us assume that τ ≤ 1
√
10

∆

. We then

u ≤ γ2(1 − ρ)t−u
st

(cid:20)

−

+

+

+ γµτ

1
2

1
10

4
30

(cid:21)

.

4
3

11
10

Thus, the condition under which all st

(65)

(66)

(67)

(68)

u are negative boils down to:
2
11

γµτ ≤

.

21

Now looking at the rt

u terms given our assumptions, the inequality (64) becomes:

u ≤ (1 − ρ)t−u
rt

(cid:20)
−2γ + 8γ2L +

8γ2L
n

4
3

(cid:0) n
5

+

(cid:21)

(cid:1)

4n
3

≤ (1 − ρ)t−u(cid:0) − 2γ + 36γ2L(cid:1) .

The condition for all rt

u to be negative then can be simpliﬁed down to:

γ ≤

1
18L

.

γ ≤

1
36L

.

We now have a promising inequality for proving that our Lyapunov function is bounded by a con-
traction. However we have deﬁned Lt in terms of the virtual iterate xt, which means that our result
would only hold for a given T ﬁxed in advance, as is the case in Mania et al. (2017). Fortunately,
we can use the same trick as in Leblond et al. (2017, Eq. (97)): we simply add γBf (ˆxt, x∗) to both
sides in (60). rt
t + γ, which makes for a slightly worse bound on γ to ensure linear
convergence:

t is replaced by rt

For this small cost, we get a contraction bound on Bf (ˆxt, x∗), and thus by the strong convexity of
f (see (9)) we get a contraction bound for E(cid:107)ˆxt − x∗(cid:107)2.
Recap. Let us use ρ = 1
to:

L . Then the conditions (68) and (71) on the step size γ reduce

4n and γ := a

Moreover, the condition:

a ≤

min{1,

1
36

72
11

κ
τ

}.

τ ≤

1
√

∆
is sufﬁcient to also ensure that (65) is satisﬁed as ∆ ∈ [ 1

10

Thus under the conditions (72) and (73), we have that all st
rewrite the recurrent step of our Lyapunov function as:

√

n ≤ n.

1√
∆

≤

n , 1], and thus
u and rt

u terms are negative and we can

Lt+1 ≤ γEBf (ˆxt) + Lt+1 ≤ (1 − ρ)t+1(a0 + A˜e0) + (1 −

)Lt .

(74)

γµ
2

By unrolling the recursion (74), we can carefully combine the effect of the geometric term (1 − ρ)
with the one of (1 − γµ
2 ). This was already done in Leblond et al. (2017, Apx C.9, Eq. (101) to
(103)), with a trick to handle various boundary cases, yielding the overall rate:

where ρ∗ = min{ 1
To get the ﬁnal constant, we need to bound A. We have:

5κ } (that we simpliﬁed to ρ∗ = 1

5n , a 2

EBf (ˆxt, x∗) ≤ (1 − ρ∗)t+1 ˆC0,
5 min{ 1

n , a 1

κ } in the theorem statement).

A =

(1 − ρ)−τ −1(τ + 1 +

1
1 − q

)

4γ2L
β

n
10

(

≤ 8γ2L

4
3
≤ 26γ2Ln
≤ γn .

+ 1 +

4n
3

)

This is the same bound on A that was used by Leblond et al. (2017) and so we obtain the same
constant as their Eq. (104):

ˆC0 :=

21n
γ

((cid:107)x0 − x∗(cid:107)2 + γ

E(cid:107)α0

i − ∇fi(x∗)(cid:107)2).

n
2L

Note that ˆC0 = O( n

γ C0) with C0 deﬁned as in Theorem 1.

22

(69)

(70)

(71)

(72)

(73)

(75)

(76)

(77)

Now, using the strong convexity of f via (9), we get:

E(cid:107)ˆxt − x∗(cid:107)2 ≤

EBf (ˆxt, x∗) ≤ (1 − ρ∗)t+1 ˜C0,

(78)

2
µ

where ˜C0 = O( nκ
a C0).
This ﬁnishes the proof for Theorem 2.

Corollary 3 (Speedup). Suppose τ ≤ 1
. If κ ≥ n, then using the step size γ = 1/36L, PROXAS-
√
10
AGA converges geometrically with rate factor Ω( 1
κ ). If κ < n, then using the step size γ = 1/36nµ,
PROXASAGA converges geometrically with rate factor Ω( 1
n ). In both cases, the convergence rate
is the same as Sparse Proximal SAGA and PROXASAGA is thus linearly faster than its sequential
counterpart up to a constant factor. Note that in both cases the step size does not depend on τ .

∆

Furthermore, if τ ≤ 6κ, we can use a universal step size of Θ(1/L) to get a similar rate for PROX-
ASAGA than Sparse Proximal SAGA, thus making it adaptive to local strong convexity since the
knowledge of κ is not required.

Proof. If κ ≥ n, the rate factor of Sparse Proximal SAGA is 1/κ. To get the same rate factor, we
need to choose a = Ω(1), which we can fortunately do since κ ≥ n ≥

≥ 10τ .

√

n ≥ 10 1
√
10

∆

If κ < n, then the rate factor of Sparse Proximal SAGA is 1/n. Any choice of a bigger than Ω(κ/n)
gives us the same rate factor for PROXASAGA. Since τ ≤
n/10 we can pick such an a without
violating the condition of Theorem 2.

√

23

Appendix D Comparison with related work

In this section, we relate our theoretical results and proof technique with the related literature.

Speedups. Our speedup regimes are comparable with the best ones obtained in the smooth case,
including Niu et al. (2011); Reddi et al. (2015), even though unlike these papers, we support incon-
sistent reads and nonsmooth objective functions. The one exception is Leblond et al. (2017), where
the authors prove that their algorithm, ASAGA, can obtain a linear speedup even without sparsity in
the well-conditioned regime. In contrast, PROXASAGA always requires some sparsity. Whether this
property for smooth objective functions could be extended to the composite case remains an open
problem.

Coordinate Descent. We compare our approach for composite objective functions to its most
natural competitor: ASYSPCD (Liu & Wright, 2015), an asynchronous stochastic coordinate descent
algorithm. While ASYSPCD also exhibits linear speedups, subject to a condition on τ , one has to be
especially careful when trying to compare these conditions.

First, while in theory the iterations of both algorithms have the same cost, in practice various tricks
are introduced to save on computation, yielding different costs per updates.9 Second, the bound on
τ for the coordinate descent algorithm depends on p, the dimensionality of the problem, whereas
ours involves n, the number of data points. Third, a more subtle issue is that τ is not affected by
the same quantities for both algorithms.10 See Appendix D.1 for a more detailed explanation of the
differences between the bounds.

√

∆) is necessary for a linear speedup. This means in case ∆ ≤ 1/√

In the best case scenario (where the components of the gradient are uncorrelated, a somewhat un-
√
realistic setting), ASYSPCD can get a near-linear speedup for τ as big as 4
p. Our result states that
τ = O(1/
p our bound is better
than the one obtained for ASYSPCD. Recalling that 1/n ≤ ∆ ≤ 1, it appears that PROXASAGA is
favored when n is bigger than
p whereas ASYSPCD may have a better bound otherwise, though
this comparison should be taken with a grain of salt given the assumptions we had to make to arrive
at comparable quantities.

√

Furthermore, one has to note that while Liu & Wright (2015) use the classical labeling scheme
inherited from Niu et al. (2011), they still assume in their proof that the it are uniformly distributed
and that their gradient estimators are conditionally unbiased – though neither property is veriﬁed in
the general asynchronous setting. Finally, we note that ASYSPCD (as well as its incremental variant
Async-PROXSVRCD) assumes that the computation and assignment of the proximal operator is an
atomic step, while we do not make such assumption.

SVRG. The Async-ProxSVRG algorithm of Meng et al. (2017) also exhibits theoretical linear
speedups subject to the same condition as ours. However, the analyzed algorithm uses dense updates
and consistent read and writes. Although they make the analysis easier, these two factors introduce
costly bottlenecks and prevent linear speedups in running time. Furthermore, here again the classical
labeling scheme is used together with the unveriﬁed conditional unbiasedness condition.

Doubly stochastic algorithms. The Async-PROXSVRCD algorithm from Meng et al. (2017); Gu
et al. (2016) has a maximum allowable stepsize11 that is in O(1/pL), whereas the maximum step
size for PROXASAGA is in Ω(1/L), so can be up to p times bigger. Consequently, PROXASAGA
enjoys much faster theoretical convergence rates. Unfortunately, we could not ﬁnd a condition for
linear speedups to compare to. We also note that their algorithm is not appropriate in a sparse
features setting. This is illustrated in an empirical comparison in Appendix F where we see that

9For PROXASAGA the relevant quantity becomes the average number of features per data point. For
In both cases the tricks involved are

ASYSPCD it is rather the average number of data points per feature.
not covered by the theory.

10To make sure τ is the same quantity for both algorithms, we have to assume that the iteration costs are

homogeneous.

11To the best of our understanding, noting that extracting an interpretable bound from the given theoretical
results was difﬁcult. Furthermore, it appears that the proof technique may still have signiﬁcant issues: for
example, the “fully lock-free” assumption of Gu et al. (2016) allows for overwrites, and is thus incompatible
with their framework of analysis, in particular their Eq. (8).

24

their convergence in number of iterations is orders of magnitude slower than appropriate algorithms
like SAGA or PROXASAGA.

Appendix D.1 Comparison of bounds with Liu & Wright (2015)

Iteration costs. For both PROXASAGA and ASYSPCD, the average cost of an iteration is O(nS)
(where S is the average support size).
In the case of PROXASAGA (see Algorithm 1), at each
iteration the most costly operation is the computation of α, while in the general case we need to
compute a full gradient for ASYSPCD.

In order to reduce these prohibitive computation costs, several tricks are introduced. Although they
lead to much improved empirical performance, it should be noted that in both cases these tricks are
not covered by the theory. In particular, the unbiasedness condition can be violated.

In the case of PROXASAGA, we store the average gradient term α in shared memory. The cost
of each iteration then becomes the size of the extended support of the partial gradient selected at
random at this iteration, hence it is in O(∆l), where ∆l := maxi=1..n |Ti|.

For ASYSPCD, following Peng et al. (2016) we can store intermediary quantities for speciﬁc losses
(e.g. (cid:96)1-regularized logistic regression). The cost of an iteration then becomes the number of data
points whose extended support includes the coordinate selected at random at this iteration, hence it
is in O(n∆).

The relative difference in update cost of both algorithms then depends heavily on the data matrix:
if the partial gradients usually have a extended support but coordinates belong to few of them (this
can be the case if n (cid:28) p for example), then the iterations of ASYSPCD can be cheaper than those of
PROXASAGA. Conversely, if data points usually have small extended support but coordinates belong
to many of them (which can happen when p (cid:28) n for example), then the updates of PROXASAGA
are the cheaper ones.

Dependency of τ on the data matrix.
In the case of PROXASAGA the sizes of the extended
support of each data point are important – they are directly linked to the cost of each iteration.
Identical iteration costs for each data point do not inﬂuence τ , whereas heterogeneous costs may
cause τ to increase substantially. In contrast, in the case of ASYSPCD, the relevant parts of the data
matrix are the number of data points each dimension touches – for much the same reason. In the
bipartite graph between data points and dimensions, either the left or the right degrees matter for τ ,
depending on which algorithm you choose.

In order to compare their respective bounds, we have to make the assumption that the iteration costs
are homogeneous, which means that each data point has the same support size and each dimension is
active in the same number of data points. This implies that τ is the same quantity for both algorithms.

√

Best case scenario bound for AsySPCD. The result obtained in Liu & Wright (2015) states that
if τ 2Λ = O(
p), ASYSPCD can get a near-linear speedup (where Λ is a measure of the interactions
p). In the best possible scenario where
between the components of the gradient, with 1 ≤ Λ ≤
Λ = 1 (which means that the coordinates of the gradients are completely uncorrelated), τ can be as
√
big as 4

√

p.

25

Appendix E Implementation details

Initialization.
In the Sparse Proximal SAGA algorithm and its asynchronous variant, PROXAS-
AGA, the vector x can be initialized arbitrarily. The memory terms αi can be initialized to any
vector that veriﬁes supp(αi) = supp(∇fi). In practice we found that the initialization αi = 0 is
very fast to set up and often outperforms more costly initializations.

With this initialization, the gradient approximation before the ﬁrst update of the memory terms be-
comes ∇fi(x) + Diα. Since most of the values in α are zero, α will tend to be small compared to
∇fi(x), and so the gradient estimate is very close to the SGD estimate ∇fi(x). The SGD approx-
imation is known to have a very fast initial convergence (which, in light of Figure 1, our method
inherits) and has even been used as a heuristic to use during the ﬁrst epoch of variance reduced
methods (Schmidt et al., 2016).

The initialization of coefﬁcients x0 was always set to zero.

Exact regularization. Computing the gradient of a smooth regularization such as the squared (cid:96)2
penalty of Eq. (6) is independent of n and so we can use the exact regularizer in the update of
the coefﬁcients instead of storing it in α, which would also destroy the compressed storage of the
memory terms described below. In practice we use this “exact regularization”, multiplied by Di to
preserve the sparsity pattern.
Assuming a squared (cid:96)2 regularization term of the form λ
(note the extra λx)

2 , the gradient estimate in (SPS) becomes

vi = ∇fi(x) − αi + Di(α + λx) .

(79)

Storage of memory terms. The storage requirements for this method is in the worst case a table
of size n × p. However, as for SAG and SAGA, for linearly parametrized loss functions of the form
fi(x) = (cid:96)(aT
i=1 are samples associated with
the learning problem, this can be reduced to a table of size n (Schmidt et al., 2016, §4.1). This
includes popular linear models such as least squares or logistic regression with (cid:96) the squared or
logistic function, respectively.

i x), where (cid:96) is some real-valued function and (ai)n

The reduce storage comes from the fact that in this case the partial gradients have the structure

∇fi(x) = ai (cid:96)(cid:48)(aT

.

i x)
(cid:124) (cid:123)(cid:122) (cid:125)
scalar

(80)

Since ai is independent of x, we only need to store the scalar (cid:96)(cid:48)(aT
explains why ∇fi inherits the sparsity pattern of ai.

i x). This decomposition also

Atomic updates. Most modern processors have support for atomic operations with minimal over-
head. In our case, we implemented a double-precision atomic type using the C++11 atomic features
(std::atomic<double>). This type implements atomic operations through the compare and swap
semantics.

Empirically, we have found it necessary to implement atomic operations at least in the vector α and
α to reach arbitrary precision. If non-atomic operations are used, the method converges only to a
limited precision (around normalized function suboptimality of 10−3), which might be sufﬁcient for
some machine learning applications but which we found not satisfying from an optimization point
of view.

AsySPCD. Following (Peng et al., 2016) we keep the vector (aT
at each iteration using atomic updates.

i x)n

i=1 in memory and update it

Hardware and software. All experiments were run on a Dell PowerEdge 920 machine with 4 Intel
Xeon E7-4830v2 processors with 10 2.2GHz cores each and 384GB 1600 Mhz RAM. The PROX-
ASAGAand ASYSPCD code was implemented on C++ and binded in Python. The FISTA code is
implemented in pure Python using NumPY and SciPy for matrix computations (in this case the bot-
tleneck is in large sparse matrix-vector operations for which efﬁcient BLAS routines were used). Our
PROXASAGA implementation can be downloaded from http://github.com/fabianp/ProxASAGA.

26

Appendix F Experiments

All datasets used for the experiments were downloaded from the LibSVM dataset suite.12

Appendix F.1 Comparison of ProxASAGA with other sequential methods

We provide a comparison between the Sparse Proximal SAGA and related methods in the sequential
case. We compare against two methods: the MRBCD method of Zhao et al. (2014) (which forms
the basis of Async-PROXSVRCD) and the vanilla implementation of SAGA (Defazio et al., 2014),
which does not have the ability to perform sparse updates. We compare in terms of both passes
through the data (epochs) and time. We use the same step size for all methods (1/3L). Due to
the slow convergence of some methods, we use a smaller dataset than the ones used in §4. Dataset
RCV1 has n = 697, 641, d = 47, 236 and a density of 0.15, while Covtype is a dense dataset with
n = 581, 012, d = 54.

Figure 2: Suboptimality of different sequential algorithms. Each marker represents one pass through
the dataset.

We observe that for the convergence behavior in terms of number of passes, Sparse Proximal SAGA
performs as well as vanilla SAGA, though the latter requires dense updates at every iteration (Fig. 2
top left). On the other hand, in terms of running time, our implementation of Sparse Proximal SAGA
is much more efﬁcient than the other methods for sparse input (Fig. 2 top right). In the case of dense
input (Fig. 2 bottom), the three methods perform similarly.
A note on the performance of MRBCD.
It may appear surprising that Sparse Proximal SAGA
outperforms MRBCD so dramatically on sparse datasets. However, one should note that MRBCD is
a doubly stochastic algorithm where both a random data point and a random coordinate are sampled
for each iteration. If the data matrix is very sparse, then the probability that the sampled coordinate
is in the support of the sampled data point becomes very low. This means that the gradient estimator
term only contains the reference gradient term of SVRG, which only changes once per epoch. As a
result, this estimator becomes very coarse and produces a slower empirical convergence.

This is reﬂected in the theoretical results given in Zhao et al. (2014), where the epoch size needed to
get linear convergence are k times bigger than the ones required by plain SVRG, where k is the size
of the set of blocks of coordinates.

12https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/

27

Appendix F.2 Theoretical speedups.

In the experimental section, we have shown experimental speedup results where suboptimality was
a function of the running time. This measure encompasses both theoretical algorithmic optimization
properties and hardware overheads (such as contention of shared memory) which are not taken into
account in our analysis.

In order to isolate these two effects, we now plot our speedup results in Figure 3 where suboptimality
is a function of the number of iterations; thus, we abstract away any potential hardware overhead. To
do so, we implement a global counter which is sparsely updated (every 100 iterations for example)
in order not to modify the asynchrony of the system. This counter is used only for plotting purposes
and is not needed otherwise. Speciﬁcally, we deﬁne the theoretical speedup as:

theoretical speedup := (number of cores)

number of iterations for sequential algorithm
total number of iterations for parallel algorithm

.

Figure 3: Theoretical optimization speedups for (cid:96)1+(cid:96)2-regularized logistic regression. Speedup
as measured by the number of iterations required to reach 10−5 suboptimality for PROXASAGA
and ASYSPCD. In FISTA the iterates are the same with different cores and so matches the “ideal”
speedup.

We see clearly that the theoretical speedups obtained by both PROXASAGAand ASYSPCD are linear
(i.e. ideal). As we observe worse results in running time, this means that the hardware overheads of
asynchronous methods are quite signiﬁcant.

Appendix F.3 Timing benchmarks

We now provide the time it takes for the different methods with 10 cores to reach a suboptimality of
10−10. All results are in hours.

Dataset

PROXASAGA ASYSPCD

FISTA

KDD 2010
KDD 2012
Criteo

1.01
0.09
0.14

13.3
26.6
33.3

5.2
8.3
6.6

Appendix F.4 Hyperparameters

The (cid:96)1-regularization parameter λ2 was chosen as to give around 10% of non-zero features. The
exact chosen values are the following: λ2 = 10−11 for KDD 2010, λ2 = 10−16 for KDD 2012 and
λ2 = 4 × 10−12 for Criteo.

28

7
1
0
2
 
v
o
N
 
5
 
 
]

C
O
.
h
t
a
m

[
 
 
3
v
8
6
4
6
0
.
7
0
7
1
:
v
i
X
r
a

Breaking the Nonsmooth Barrier: A Scalable Parallel
Method for Composite Optimization

Fabian Pedregosa
INRIA/ENS∗
Paris, France

R´emi Leblond
INRIA/ENS∗
Paris, France

Simon Lacoste-Julien
MILA and DIRO
Universit´e de Montr´eal, Canada

Abstract

Due to their simplicity and excellent performance, parallel asynchronous variants
of stochastic gradient descent have become popular methods to solve a wide range
of large-scale optimization problems on multi-core architectures. Yet, despite their
practical success, support for nonsmooth objectives is still lacking, making them
unsuitable for many problems of interest in machine learning, such as the Lasso,
group Lasso or empirical risk minimization with convex constraints. In this work,
we propose and analyze PROXASAGA, a fully asynchronous sparse method in-
spired by SAGA, a variance reduced incremental gradient algorithm. The proposed
method is easy to implement and signiﬁcantly outperforms the state of the art on
several nonsmooth, large-scale problems. We prove that our method achieves a
theoretical linear speedup with respect to the sequential version under assump-
tions on the sparsity of gradients and block-separability of the proximal term.
Empirical benchmarks on a multi-core architecture illustrate practical speedups of
up to 12x on a 20-core machine.

1

Introduction

The widespread availability of multi-core computers motivates the development of parallel methods
adapted for these architectures. One of the most popular approaches is HOGWILD (Niu et al., 2011),
an asynchronous variant of stochastic gradient descent (SGD). In this algorithm, multiple threads run
the update rule of SGD asynchronously in parallel. As SGD, it only requires visiting a small batch
of random examples per iteration, which makes it ideally suited for large scale machine learning
problems. Due to its simplicity and excellent performance, this parallelization approach has recently
been extended to other variants of SGD with better convergence properties, such as SVRG (Johnson
& Zhang, 2013) and SAGA (Defazio et al., 2014).

Despite their practical success, existing parallel asynchronous variants of SGD are limited to smooth
objectives, making them inapplicable to many problems in machine learning and signal processing.
In this work, we develop a sparse variant of the SAGA algorithm and consider its parallel asyn-
chronous variants for general composite optimization problems of the form:

arg min
x∈Rp

f (x) + h(x)

, with f (x) := 1
n

(cid:80)n

i=1 fi(x)

,

(OPT)

where each fi is convex with L-Lipschitz gradient, the average function f is µ-strongly convex and
h is convex but potentially nonsmooth. We further assume that h is “simple” in the sense that we
have access to its proximal operator, and that it is block-separable, that is, it can be decomposed
block coordinate-wise as h(x) = (cid:80)
B∈BhB([x]B), where B is a partition of the coefﬁcients into

∗DI ´Ecole normale sup´erieure, CNRS, PSL Research University

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

subsets which will call blocks and hB only depends on coordinates in block B. Note that there is
no loss of generality in this last assumption as a unique block covering all coordinates is a valid
partition, though in this case, our sparse variant of the SAGA algorithm reduces to the original SAGA
algorithm and no gain from sparsity is obtained.

This template models a broad range of problems arising in machine learning and signal processing:
the ﬁnite-sum structure of f includes the least squares or logistic loss functions; the proximal term
h includes penalties such as the (cid:96)1 or group lasso penalty. Furthermore, this term can be extended-
valued, thus allowing for convex constraints through the indicator function.

Contributions. This work presents two main contributions. First, in §2 we describe Sparse Proxi-
mal SAGA, a novel variant of the SAGA algorithm which features a reduced cost per iteration in the
presence of sparse gradients and a block-separable penalty. Like other variance reduced methods, it
enjoys a linear convergence rate under strong convexity. Second, in §3 we present PROXASAGA, a
lock-free asynchronous parallel version of the aforementioned algorithm that does not require con-
sistent reads. Our main results states that PROXASAGA obtains (under assumptions) a theoretical
linear speedup with respect to its sequential version. Empirical benchmarks reported in §4 show that
this method dramatically outperforms state-of-the-art alternatives on large sparse datasets, while the
empirical speedup analysis illustrates the practical gains as well as its limitations.

1.1 Related work

Asynchronous coordinate-descent. For composite objective functions of the form (OPT), most of
the existing literature on asynchronous optimization has focused on variants of coordinate descent.
Liu & Wright (2015) proposed an asynchronous variant of (proximal) coordinate descent and proved
a near-linear speedup in the number of cores used, given a suitable step size. This approach has been
recently extended to general block-coordinate schemes by Peng et al. (2016), to greedy coordinate-
descent schemes by You et al. (2016) and to non-convex problems by Davis et al. (2016). However,
as illustrated by our experiments, in the large sample regime coordinate descent compares poorly
against incremental gradient methods like SAGA.

Variance reduced incremental gradient and their asynchronous variants.
Initially proposed in
the context of smooth optimization by Le Roux et al. (2012), variance reduced incremental gradient
methods have since been extended to minimize composite problems of the form (OPT) (see table
below). Smooth variants of these methods have also recently been extended to the asynchronous set-
ting, where multiple threads run the update rule asynchronously and in parallel. Interestingly, none
of these methods achieve both simultaneously, i.e. asynchronous optimization of composite prob-
lems. Since variance reduced incremental gradient methods have shown state of the art performance
in both settings, this generalization is of key practical interest.

Objective

Smooth

Composite

Sequential Algorithm
SVRG (Johnson & Zhang, 2013)
SDCA (Shalev-Shwartz & Zhang, 2013)
SAGA (Defazio et al., 2014)
PROXSDCA (Shalev-Shwartz et al., 2012)
SAGA (Defazio et al., 2014)
ProxSVRG (Xiao & Zhang, 2014)

Asynchronous Algorithm

SVRG (Reddi et al., 2015)
PASSCODE (Hsieh et al., 2015, SDCA variant)
ASAGA (Leblond et al., 2017, SAGA variant)

This work: PROXASAGA

On the difﬁculty of a composite extension. Two key issues explain the paucity in the develop-
ment of asynchronous incremental gradient methods for composite optimization. The ﬁrst issue
is related to the design of such algorithms. Asynchronous variants of SGD are most competitive
when the updates are sparse and have a small overlap, that is, when each update modiﬁes a small
and different subset of the coefﬁcients. This is typically achieved by updating only coefﬁcients for
which the partial gradient at a given iteration is nonzero,2 but existing schemes such as the lagged
updates technique (Schmidt et al., 2016) are not applicable in the asynchronous setting. The second

2Although some regularizers are sparsity inducing, large scale datasets are often extremely sparse and lever-

aging this property is crucial for the efﬁciency of the method.

2

difﬁculty is related to the analysis of such algorithms. All convergence proofs crucially use the Lip-
schitz condition on the gradient to bound the noise terms derived from asynchrony. However, in the
composite case, the gradient mapping term (Beck & Teboulle, 2009), which replaces the gradient
in proximal-gradient methods, does not have a bounded Lipschitz constant. Hence, the traditional
proof technique breaks down in this scenario.

Other approaches. Recently, Meng et al. (2017); Gu et al. (2016) independently proposed a dou-
bly stochastic method to solve the problem at hand. Following Meng et al. (2017) we refer to it
as Async-PROXSVRCD. This method performs coordinate descent-like updates in which the true
gradient is replaced by its SVRG approximation. It hence features a doubly-stochastic loop: at each
iteration we select a random coordinate and a random sample. Because the selected coordinate block
is uncorrelated with the chosen sample, the algorithm can be orders of magnitude slower than SAGA
in the presence of sparse gradients. Appendix F contains a comparison of these methods.

1.2 Deﬁnitions and notations

By convention, we denote vectors and vector-valued functions in lowercase boldface (e.g. x) and
matrices in uppercase boldface (e.g. D). The proximal operator of a convex lower semicontinuous
function h is deﬁned as proxh(x) := arg minz∈Rp {h(z) + 1
2 (cid:107)x − z(cid:107)2}. A function f is said to be
L-smooth if it is differentiable and its gradient is L-Lipschitz continuous. A function f is said to be
µ-strongly convex if f − µ
2 (cid:107) · (cid:107)2 is convex. We use the notation κ := L/µ to denote the condition
number for an L-smooth and µ-strongly convex function.3
I p denotes the p-dimensional identity matrix, 1{cond} the characteristic function, which is 1 if cond
evaluates to true and 0 otherwise. The average of a vector or matrix is denoted α := 1
i=1 αi.
n
We use (cid:107) · (cid:107) for the Euclidean norm. For a positive semi-deﬁnite matrix D, we deﬁne its associated
distance as (cid:107)x(cid:107)2
D := (cid:104)x, Dx(cid:105). We denote by [ x ]b the b-th coordinate in x. This notation is
overloaded so that for a collection of blocks T = {B1, B2, . . .}, [x]T denotes the vector x restricted
to the coordinates in the blocks of T . For convenience, when T consists of a single block B we use
[x]B as a shortcut of [x]{B}. Finally, we distinguish E, the full expectation taken with respect to
all the randomness in the system, from E, the conditional expectation of a random it (the random
feature sampled at each iteration by SGD-like algorithms) conditioned on all the “past”, which the
context will clarify.

(cid:80)n

2 Sparse Proximal SAGA

Original SAGA algorithm. The original SAGA algorithm (Defazio et al., 2014) maintains two
moving quantities: the current iterate x and a table (memory) of historical gradients (αi)n
i=1. At
every iteration, it samples an index i ∈ {1, . . . , n} uniformly at random, and computes the next
iterate (x+, α+) according to the following recursion:
ui = ∇fi(x) − αi + α ; x+ = proxγh

(1)
On each iteration, this update rule requires to visit all coefﬁcients even if the partial gradients ∇fi are
sparse. Sparse partial gradients arise in a variety of practical scenarios: for example, in generalized
linear models the partial gradients inherit the sparsity pattern of the dataset. Given that large-scale
datasets are often sparse,4 leveraging this sparsity is crucial for the success of the optimizer.

i = ∇fi(x) .

(cid:0)x − γui

(cid:1); α+

Sparse Proximal SAGA algorithm. We will now describe an algorithm that leverages sparsity
in the partial gradients by only updating those blocks that intersect with the support of the partial
gradients. Since in this update scheme some blocks might appear more frequently than others, we
will need to counterbalance this undersirable effect with a well-chosen block-wise reweighting of
the average gradient and the proximal term.

In order to make precise this block-wise reweighting, we deﬁne the following quantities. We denote
by Ti the extended support of ∇fi, which is the set of blocks that intersect the support of ∇fi,

3Since we have assumed that each individual fi is L-smooth, f itself is L-smooth – but it could have a

smaller smoothness constant. Our rates are in terms of this bigger L/µ, as is standard in the SAGA literature.

4For example, in the LibSVM datasets suite, 8 out of the 11 datasets (as of May 2017) with more than a

million samples have a density between 10−4 and 10−6.

3

i

formally deﬁned as Ti := {B : supp(∇fi) ∩ B (cid:54)= ∅, B ∈ B}. For totally separable penalties such
as the (cid:96)1 norm, the blocks are individual coordinates and so the extended support covers the same
coordinates as the support. Let dB := n/nB, where nB := (cid:80)
1{B ∈ Ti} is the number of times
that B ∈ Ti. For simplicity we assume nB > 0, as otherwise the problem can be reformulated
without block B. The update rule in (1) requires computing the proximal operator of h, which
involves a full pass on the coordinates. In our proposed algorithm, we replace h in (1) with the
function ϕi(x) := (cid:80)
dBhB(x), whose form is justiﬁed by the following three properties.
First, this function is zero outside Ti, allowing for sparse updates. Second, because of the block-wise
reweighting dB, the function ϕi is an unbiased estimator of h (i.e., E ϕi = h), property which will
be crucial to prove the convergence of the method. Third, ϕi inherits the block-wise structure of h
and its proximal operator can be computed from that of h as [proxγϕi(x)]B = [prox(dB γ)hB (x)]B
if B ∈ Ti and [proxγϕi(x)]B = [x]B otherwise. Following Leblond et al. (2017), we will also
replace the dense gradient estimate ui by the sparse estimate vi := ∇fi(x) − αi + Diα, where
Di is the diagonal matrix deﬁned block-wise as [Di]B,B = dB1{B ∈ Ti}I |B|. It is easy to verify
that the vector Diα is a weighted projection onto the support of Ti and E Diα = α, making vi an
unbiased estimate of the gradient.

B∈Ti

We now have all necessary elements to describe the Sparse Proximal SAGA algorithm. As the
original SAGA algorithm, it maintains two moving quantities: the current iterate x ∈ Rp and a
i=1, αi ∈ Rp. At each iteration, the algorithm samples an index
table of historical gradients (αi)n
i ∈ {1, . . . , n} and computes the next iterate (x+, α+) as:

vi = ∇fi(x) − αi + Diα ; x+ = proxγϕi

(cid:0)x − γvi

(cid:1) ; α+

i = ∇fi(x) ,

(SPS)

where in a practical implementation the vector α is updated incrementally at each iteration.

The above algorithm is sparse in the sense that it only requires to visit and update blocks in the
extended support: if B /∈ Ti, by the sparsity of vi and proxϕi, we have [x+]B = [x]B. Hence,
when the extended support Ti is sparse, this algorithm can be orders of magnitude faster than the
naive SAGA algorithm. The extended support is sparse for example when the partial gradients are
sparse and the penalty is separable, as is the case of the (cid:96)1 norm or the indicator function over a
hypercube, or when the the penalty is block-separable in a way such that only a small subset of the
blocks overlap with the support of the partial gradients. Initialization of variables and a reduced
storage scheme for the memory are discussed in the implementation details section of Appendix E.

Relationship with existing methods. This algorithm can be seen as a generalization of both the
Standard SAGA algorithm and the Sparse SAGA algorithm of Leblond et al. (2017). When the
proximal term is not block-separable, then dB = 1 (for a unique block B) and the algorithm defaults
to the Standard (dense) SAGA algorithm. In the smooth case (i.e., h = 0), the algorithm defaults to
the Sparse SAGA method. Hence we note that the sparse gradient estimate vi in our algorithm is the
same as the one proposed in Leblond et al. (2017). However, we emphasize that a straightforward
combination of this sparse update rule with the proximal update from the Standard SAGA algorithm
results in a nonconvergent algorithm: the block-wise reweighting of h is a surprisingly simple but
crucial change. We now give the convergence guarantees for this algorithm.
Theorem 1. Let γ = a
SAGA converges geometrically in expectation with a rate factor of at least ρ = 1
is, for xt obtained after t updates, we have the following bound:

5L for any a ≤ 1 and f be µ-strongly convex (µ > 0). Then Sparse Proximal
κ }. That

5 min{ 1

n , a 1

E(cid:107)xt − x∗(cid:107)2 ≤ (1 − ρ)tC0 , with C0 := (cid:107)x0 − x∗(cid:107)2 + 1
5L2

(cid:80)n

i=1 (cid:107)α0

i − ∇fi(x∗)(cid:107)2

.

Remark. For the step size γ = 1/5L, the convergence rate is (1 − 1/5 min{1/n, 1/κ}). We can thus
identify two regimes: the “big data” regime, n ≥ κ, in which the rate factor is bounded by 1/5n, and
the “ill-conditioned” regime, κ ≥ n, in which the rate factor is bounded by 1/5κ. This rate roughly
matches the rate obtained by Defazio et al. (2014). While the step size bound of 1/5L is slightly
smaller than the 1/3L one obtained in that work, this can be explained by their stronger assumptions:
each fi is strongly convex whereas they are strongly convex only on average in this work. All proofs
for this section can be found in Appendix B.

4

i=1

j=1[ ˆαj ]Ti

ˆx = inconsistent read of x
ˆα = inconsistent read of α
Sample i uniformly in {1, ..., n}
Si := support of ∇fi
Ti := extended support of ∇fi in B
[ α ]Ti = 1/n (cid:80)n
[ δα ]Si = [∇fi(ˆx)]Si − [ ˆαi]Si
[ ˆv ]Ti = [ δα ]Ti + [Diα ]Ti
[ δx ]Ti = [proxγϕi(ˆx − γ ˆv)]Ti − [ˆx]Ti
for B in Ti do
for b ∈ B do

Algorithm 1 PROXASAGA (analyzed)
1: Initialize shared variables x and (αi)n
2: keep doing in parallel
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end parallel loop

end for
// (‘←’ denotes shared memory update.)

[ x ]b ← [ x ]b + [ δx ]b
if b ∈ Si then

[αi]b ← [∇fi(ˆx)]b

end if
end for

(cid:46) atomic

i=1, α

Sample i uniformly in {1, ..., n}
Si := support of ∇fi
Ti := extended support of ∇fi in B
[ ˆx ]Ti = inconsistent read of x on Ti
ˆαi = inconsistent read of αi
[ α ]Ti = inconsistent read of α on Ti
[ δα ]Si = [∇fi(ˆx)]Si − [ ˆαi]Si
[ ˆv ]Ti = [δα ]Ti + [ Diα ]Ti
[ δx ]Ti = [proxγϕi(ˆx − γ ˆv)]Ti − [ˆx]Ti
for B in Ti do
for b in B do

Algorithm 2 PROXASAGA (implemented)
1: Initialize shared variables x, (αi)n
2: keep doing in parallel
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
end for
19:
20: αi ← ∇fi(ˆx)
21: end parallel loop

[ x ]b ← [ x ]b + [ δx ]b
if b ∈ Si then

end if
end for

[ α ]b ← [α]b + 1/n[δα]b (cid:46) atomic

(scalar update) (cid:46) atomic

(cid:46) atomic

3 Asynchronous Sparse Proximal SAGA

We introduce PROXASAGA – the asynchronous parallel variant of Sparse Proximal SAGA. In this
algorithm, multiple cores update a central parameter vector using the Sparse Proximal SAGA intro-
duced in the previous section, and updates are performed asynchronously. The algorithm parameters
are read and written without vector locks, i.e., the vector content of the shared memory can poten-
tially change while a core is reading or writing to main memory coordinate by coordinate. These
operations are typically called inconsistent (at the vector level).

The full algorithm is described in Algorithm 1 for its theoretical version (on which our analysis
is built) and in Algorithm 2 for its practical implementation. The practical implementation differs
from the analyzed agorithm in three points. First, in the implemented algorithm, index i is sampled
before reading the coefﬁcients to minimize memory access since only the extended support needs to
be read. Second, since our implementation targets generalized linear models, the memory αi can be
compressed into a single scalar in L20 (see Appendix E). Third, α is stored in memory and updated
incrementally instead of recomputed at each iteration.

The rest of the section is structured as follows: we start by describing our framework of analysis; we
then derive essential properties of PROXASAGA along with a classical delay assumption. Finally,
we state our main convergence and speedup result.

3.1 Analysis framework

As in most of the recent asynchronous optimization literature, we build on the hardware model in-
troduced by Niu et al. (2011), with multiple cores reading and writing to a shared memory parameter
vector. These operations are asynchronous (lock-free) and inconsistent:5 ˆxt, the local copy of the
parameters of a given core, does not necessarily correspond to a consistent iterate in memory.

“Perturbed” iterates. To handle this additional difﬁculty, contrary to most contributions in this
ﬁeld, we choose the “perturbed iterate framework” proposed by Mania et al. (2017) and reﬁned
by Leblond et al. (2017). This framework can analyze variants of SGD which obey the update rule:

xt+1 = xt − γv(xt, it) , where v veriﬁes the unbiasedness condition E v(x, it) = ∇f (x)

5This is an extension of the framework of Niu et al. (2011), where consistent updates were assumed.

5

and the expectation is computed with respect to it. In the asynchronous parallel setting, cores are
reading inconsistent iterates from memory, which we denote ˆxt. As these inconsistent iterates are
affected by various delays induced by asynchrony, they cannot easily be written as a function of
their previous iterates. To alleviate this issue, Mania et al. (2017) choose to introduce an additional
quantity for the purpose of the analysis:
xt+1 := xt − γv(ˆxt, it) ,

(2)
Note that this equation is the deﬁnition of this new quantity xt. This virtual iterate is useful for the
convergence analysis and makes for much easier proofs than in the related literature.

the “virtual iterate” – which is never actually computed .

“After read” labeling. How we choose to deﬁne the iteration counter t to label an iterate xt
matters in the analysis.
In this paper, we follow the “after read” labeling proposed in Leblond
et al. (2017), in which we update our iterate counter, t, as each core ﬁnishes reading its copy of
the parameters (in the speciﬁc case of PROXASAGA, this includes both ˆxt and ˆαt). This means
that ˆxt is the (t + 1)th fully completed read. One key advantage of this approach compared to the
classical choice of Niu et al. (2011) – where t is increasing after each successful update – is that
it guarantees both that the it are uniformly distributed and that it and ˆxt are independent. This
property is not veriﬁed when using the “after write” labeling of Niu et al. (2011), although it is still
implicitly assumed in the papers using this approach, see Leblond et al. (2017, Section 3.2) for a
discussion of issues related to the different labeling schemes.

Generalization to composite optimization. Although the perturbed iterate framework was de-
signed for gradient-based updates, we can extend it to proximal methods by remarking that in the
sequential setting, proximal stochastic gradient descent and its variants can be characterized by the
following similar update rule:

xt+1 = xt − γg(xt, vit, it) , with g(x, v, i) := 1
γ

(cid:0)x − proxγϕi(x − γv)(cid:1) ,

where as before v veriﬁes the unbiasedness condition E v = ∇f (x). The Proximal Sparse SAGA
iteration can be easily written within this template by using ϕi and vi as deﬁned in §2. Using this
deﬁnition of g, we can deﬁne PROXASAGA virtual iterates as:

xt+1 := xt − γg(ˆxt, ˆvt
it

, it) , with ˆvt
it

= ∇fit (ˆxt) − ˆαt
it

+ Dit αt

,

(3)

(4)

where as in the sequential case, the memory terms are updated as ˆαt
it
analysis of PROXASAGA will be based on this deﬁnition of the virtual iterate xt+1.

= ∇fit(ˆxt). Our theoretical

3.2 Properties and assumptions

Now that we have introduced the “after read” labeling for proximal methods in Eq. (4), we can
leverage the framework of Leblond et al. (2017, Section 3.3) to derive essential properties for the
analysis of PROXASAGA. We describe below three useful properties arising from the deﬁnition
of Algorithm 1, and then state a central (but standard) assumption that the delays induced by the
asynchrony are uniformly bounded.

Independence: Due to the “after read” global ordering, ir is independent of ˆxt for all r ≥ t. We
enforce the independence for r = t by having the cores read all the shared parameters before their
iterations.
Unbiasedness: The term ˆvt
consequence of the independence between it and ˆxt.

it is an unbiased estimator of the gradient of f at ˆxt. This property is a

Atomicity: The shared parameter coordinate update of [x]b on Line 14 is atomic. This means that
there are no overwrites for a single coordinate even if several cores compete for the same resources.
Most modern processors have support for atomic operations with minimal overhead.

Bounded overlap assumption. We assume that there exists a uniform bound, τ , on the maximum
number of overlapping iterations. This means that every coordinate update from iteration t is suc-
cessfully written to memory before iteration t + τ + 1 starts. Our result will give us conditions on τ
to obtain linear speedups.

Bounding ˆxt − xt. The delay assumption of the previous paragraph allows to express the difference
between real and virtual iterate using the gradient mapping gu := g(ˆxu, ˆvu
iu
ˆxt−xt = γ (cid:80)t−1

u are p × p diagonal matrices with terms in {0, +1}. (5)

ugu , where Gt

, iu) as:

Gt

u=(t−τ )+

6

0 represents instances where both ˆxu and xu have received the corresponding updates. +1, on
the contrary, represents instances where ˆxu has not yet received an update that is already in xu by
deﬁnition. This bound will prove essential to our analysis.

3.3 Analysis

In this section, we state our convergence and speedup results for PROXASAGA. The full details
of the analysis can be found in Appendix C. Following Niu et al. (2011), we introduce a sparsity
measure (generalized to the composite setting) that will appear in our results.
Deﬁnition 1. Let ∆ := maxB∈B |{i : Ti (cid:51) B}|/n. This is the normalized maximum number of
times that a block appears in the extended support. For example, if a block is present in all Ti, then
∆ = 1. If no two Ti share the same block, then ∆ = 1/n. We always have 1/n ≤ ∆ ≤ 1.
Theorem 2 (Convergence guarantee of PROXASAGA). Suppose τ ≤ 1
√
10
γ = a
in expectation at a geometric rate factor of at least: ρ(a) = 1
(1 − ρ)t ˜C0, where ˜C0 is a constant independent of t (≈ nκ

. For any step size
τ }, the inconsistent read iterates of Algorithm 1 converge
(cid:9), i.e. E(cid:107)ˆxt − x∗(cid:107)2 ≤

L with a ≤ a∗(τ ) := 1

a C0 with C0 as deﬁned in Theorem 1).

36 min{1, 6κ

5 min (cid:8) 1

n , a 1

∆

κ

√

This last result is similar to the original SAGA convergence result and our own Theorem 1, with both
an extra condition on τ and on the maximum allowable step size. In the best sparsity case, ∆ = 1/n
and we get the condition τ ≤
n/10. We now compare the geometric rate above to the one of Sparse
Proximal SAGA to derive the necessary conditions under which PROXASAGA is linearly faster.
Corollary 1 (Speedup). Suppose τ ≤ 1
. If κ ≥ n, then using the step size γ = 1/36L, PROXAS-
√
10
AGA converges geometrically with rate factor Ω( 1
κ ). If κ < n, then using the step size γ = 1/36nµ,
PROXASAGA converges geometrically with rate factor Ω( 1
n ). In both cases, the convergence rate
is the same as Sparse Proximal SAGA. Thus PROXASAGA is linearly faster than its sequential
counterpart up to a constant factor. Note that in both cases the step size does not depend on τ .

∆

Furthermore, if τ ≤ 6κ, we can use a universal step size of Θ(1/L) to get a similar rate for PROX-
ASAGA than Sparse Proximal SAGA, thus making it adaptive to local strong convexity since the
knowledge of κ is not required.

These speedup regimes are comparable with the best ones obtained in the smooth case, including Niu
et al. (2011); Reddi et al. (2015), even though unlike these papers, we support inconsistent reads
and nonsmooth objective functions. The one exception is Leblond et al. (2017), where the authors
prove that their algorithm, ASAGA, can obtain a linear speedup even without sparsity in the well-
conditioned regime. In contrast, PROXASAGA always requires some sparsity. Whether this property
for smooth objective functions could be extended to the composite case remains an open problem.

√

Relative to ASYSPCD, in the best case scenario (where the components of the gradient are uncorre-
√
lated, a somewhat unrealistic setting), ASYSPCD can get a near-linear speedup for τ as big as 4
p.
∆) is necessary for a linear speedup. This means in case ∆ ≤ 1/√
Our result states that τ = O(1/
p
our bound is better than the one obtained for ASYSPCD. Recalling that 1/n ≤ ∆ ≤ 1, it appears
that PROXASAGA is favored when n is bigger than
p whereas ASYSPCD may have a better bound
otherwise, though this comparison should be taken with a grain of salt given the assumptions we
had to make to arrive at comparable quantities. An extended comparison with the related work can
be found in Appendix D.

√

4 Experiments

In this section, we compare PROXASAGA with related methods on different datasets. Although
PROXASAGA can be applied more broadly, we focus on (cid:96)1 + (cid:96)2-regularized logistic regression, a
model of particular practical importance. The objective function takes the form

1
n

n
(cid:88)

i=1

log (cid:0)1 + exp(−bia

(cid:124)

i x)(cid:1) + λ1

2 (cid:107)x(cid:107)2

2 + λ2(cid:107)x(cid:107)1

,

(6)

where ai ∈ Rp and bi ∈ {−1, +1} are the data samples. Following Defazio et al. (2014), we set
λ1 = 1/n. The amount of (cid:96)1 regularization (λ2) is selected to give an approximate 1/10 nonzero

7

Table 1: Description of datasets.

Dataset

n

p

KDD 2010 (Yu et al., 2010)
KDD 2012 (Juan et al., 2016)
Criteo (Juan et al., 2016)

19,264,097
149,639,105
45,840,617

1,163,024
54,686,452
1,000,000

density
10−6
2 × 10−7
4 × 10−5

L

28.12
1.25
1.25

∆

0.15
0.85
0.89

Figure 1: Convergence for asynchronous stochastic methods for (cid:96)1 + (cid:96)2-regularized logistic
regression. Top: Suboptimality as a function of time for different asynchronous methods using 1
and 10 cores. Bottom: Running time speedup as function of the number of cores. PROXASAGA
achieves signiﬁcant speedups over its sequential version while being orders of magnitude faster than
competing methods. ASYSPCD achieves the highest speedups but it also the slowest overall method.

coefﬁcients. Implementation details are available in Appendix E. We chose the 3 datasets described
in Table 1

Results. We compare three parallel asynchronous methods on the aforementioned datasets: PROX-
ASAGA (this work),6 ASYSPCD, the asynchronous proximal coordinate descent method of Liu &
Wright (2015) and the (synchronous) FISTA algorithm (Beck & Teboulle, 2009), in which the gra-
dient computation is parallelized by splitting the dataset into equal batches. We aim to benchmark
these methods in the most realistic scenario possible; to this end we use the following step size:
1/2L for PROXASAGA, 1/Lc for ASYSPCD, where Lc is the coordinate-wise Lipschitz constant
of the gradient, while FISTA uses backtracking line-search. The results can be seen in Figure 1
(top) with both one (thus sequential) and ten processors. Two main observations can be made from
this ﬁgure. First, PROXASAGA is signiﬁcantly faster on these problems. Second, its asynchronous
version offers a signiﬁcant speedup over its sequential counterpart.

In Figure 1 (bottom) we present speedup with respect to the number of cores, where speedup is
computed as the time to achieve a suboptimality of 10−10 with one core divided by the time to
achieve the same suboptimality using several cores. While our theoretical speedups (with respect
to the number of iterations) are almost linear as our theory predicts (see Appendix F), we observe
a different story for our running time speedups. This can be attributed to memory access overhead,
which our model does not take into account. As predicted by our theoretical results, we observe

6A reference C++/Python implementation of is available at https://github.com/fabianp/ProxASAGA

8

a high correlation between the ∆ dataset sparsity measure and the empirical speedup: KDD 2010
(∆ = 0.15) achieves a 11x speedup, while in Criteo (∆ = 0.89) the speedup is never above 6x.

Note that although competitor methods exhibit similar or sometimes better speedups, they remain
orders of magnitude slower than PROXASAGA in running time for large sparse problems. In fact,
our method is between 5x and 80x times faster (in time to reach 10−10 suboptimality) than FISTA
and between 13x and 290x times faster than ASYSPCD (see Appendix F.3).

5 Conclusion and future work

In this work, we have described PROXASAGA, an asynchronous variance reduced algorithm with
support for composite objective functions. This method builds upon a novel sparse variant of the
(proximal) SAGA algorithm that takes advantage of sparsity in the individual gradients. We have
proven that this algorithm is linearly convergent under a condition on the step size and that it is
linearly faster than its sequential counterpart given a bound on the delay. Empirical benchmarks
show that PROXASAGA is orders of magnitude faster than existing state-of-the-art methods.

This work can be extended in several ways. First, we have focused on the SAGA method as the basic
iteration loop, but this approach can likely be extended to other proximal incremental schemes such
as SGD or ProxSVRG. Second, as mentioned in §3.3, it is an open question whether it is possible to
obtain convergence guarantees without any sparsity assumption, as was done for ASAGA.

Acknowledgements

The authors would like to thank our colleagues Damien Garreau, Robert Gower, Thomas Ker-
dreux, Geoffrey Negiar and Konstantin Mishchenko for their feedback on this manuscript, and Jean-
Baptiste Alayrac for support managing the computational resources.

This work was partially supported by a Google Research Award. FP acknowledges support from the
chaire ´Economie des nouvelles donn´ees with the data science joint research initiative with the fonds
AXA pour la recherche.

References

Bauschke, Heinz and Combettes, Patrick L. Convex analysis and monotone operator theory in

Hilbert spaces. Springer, 2011.

Beck, Amir and Teboulle, Marc. Gradient-based algorithms with applications to signal recovery.

Convex Optimization in Signal Processing and Communications, 2009.

Davis, Damek, Edmunds, Brent, and Udell, Madeleine. The sound of APALM clapping: faster
nonsmooth nonconvex optimization with stochastic asynchronous PALM. In Advances in Neural
Information Processing Systems 29, 2016.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural Infor-
mation Processing Systems, 2014.

Gu, Bin, Huo, Zhouyuan, and Huang, Heng. Asynchronous stochastic block coordinate descent

with variance reduction. arXiv preprint arXiv:1610.09447v3, 2016.

Hsieh, Cho-Jui, Yu, Hsiang-Fu, and Dhillon, Inderjit S. PASSCoDe: parallel asynchronous stochas-

tic dual coordinate descent. In ICML, 2015.

Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems, 2013.

Juan, Yuchin, Zhuang, Yong, Chin, Wei-Sheng, and Lin, Chih-Jen. Field-aware factorization ma-
chines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Sys-
tems. ACM, 2016.

9

Le Roux, Nicolas, Schmidt, Mark, and Bach, Francis R. A stochastic gradient method with an ex-
ponential convergence rate for ﬁnite training sets. In Advances in Neural Information Processing
Systems, 2012.

Leblond, R´emi, Pedregosa, Fabian, and Lacoste-Julien, Simon. ASAGA: asynchronous parallel
SAGA. Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS 2017), 2017.

Liu, Ji and Wright, Stephen J. Asynchronous stochastic coordinate descent: Parallelism and conver-

gence properties. SIAM Journal on Optimization, 2015.

Mania, Horia, Pan, Xinghao, Papailiopoulos, Dimitris, Recht, Benjamin, Ramchandran, Kannan,
and Jordan, Michael I. Perturbed iterate analysis for asynchronous stochastic optimization. SIAM
Journal on Optimization, 2017.

Meng, Qi, Chen, Wei, Yu, Jingcheng, Wang, Taifeng, Ma, Zhi-Ming, and Liu, Tie-Yan. Asyn-
chronous stochastic proximal optimization algorithms with variance reduction. In AAAI, 2017.

Nesterov, Yurii. Introductory lectures on convex optimization. Springer Science & Business Media,

Nesterov, Yurii. Gradient methods for minimizing composite functions. Mathematical Program-

2004.

ming, 2013.

Niu, Feng, Recht, Benjamin, Re, Christopher, and Wright, Stephen. Hogwild: A lock-free approach
to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Sys-
tems, 2011.

Peng, Zhimin, Xu, Yangyang, Yan, Ming, and Yin, Wotao. ARock: an algorithmic framework for

asynchronous parallel coordinate updates. SIAM Journal on Scientiﬁc Computing, 2016.

Reddi, Sashank J, Hefny, Ahmed, Sra, Suvrit, Poczos, Barnabas, and Smola, Alexander J. On
variance reduction in stochastic gradient descent and its asynchronous variants. In Advances in
Neural Information Processing Systems, 2015.

Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Minimizing ﬁnite sums with the stochastic

average gradient. Mathematical Programming, 2016.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research, 2013.

Shalev-Shwartz, Shai et al.
arXiv:1211.2717, 2012.

Proximal stochastic dual coordinate ascent.

arXiv preprint

Xiao, Lin and Zhang, Tong. A proximal stochastic gradient method with progressive variance re-

duction. SIAM Journal on Optimization, 2014.

You, Yang, Lian, Xiangru, Liu, Ji, Yu, Hsiang-Fu, Dhillon, Inderjit S, Demmel, James, and Hsieh,
Cho-Jui. Asynchronous parallel greedy coordinate descent. In Advances In Neural Information
Processing Systems, 2016.

Yu, Hsiang-Fu, Lo, Hung-Yi, Hsieh, Hsun-Ping, Lou, Jing-Kai, McKenzie, Todd G, Chou, Jung-
Wei, Chung, Po-Han, Ho, Chia-Hua, Chang, Chun-Fu, Wei, Yin-Hsuan, et al. Feature engineering
and classiﬁer ensemble for KDD cup 2010. In KDD Cup, 2010.

Zhao, Tuo, Yu, Mo, Wang, Yiming, Arora, Raman, and Liu, Han. Accelerated mini-batch random-
ized block coordinate descent method. In Advances in neural information processing systems,
2014.

10

Breaking the Nonsmooth Barrier: A Scalable Parallel
Method for Composite Optimization

Supplementary material

Notations. Throughout the supplementary material we use the following extra notation. We denote
by (cid:104)·, ·(cid:105)(i) (resp. (cid:107) · (cid:107)(i)) the scalar product (resp. norm) restricted to blocks in Ti, i.e., (cid:104)x, y(cid:105)(i) :=
(cid:104)[x]B, [y]B(cid:105) and (cid:107)x(cid:107)(i) := (cid:112)(cid:104)x, x(cid:105)(i). We will also use the following deﬁnitions: ϕ :=
(cid:80)
(cid:80)

B∈Ti
B∈B dBhB(x) and D is the diagonal matrix deﬁned block-wise as [D]B,B = dBI |B|.

The Bregman divergence associated with a convex function f for points x, y in its domain is
deﬁned as:

Bf (x, y) := f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105) .

Note that this is always positive due to the convexity of f .

Appendix A Basic properties

Lemma 1. For any µ-strongly convex function f we have the following inequality:

(cid:104)∇f (y) − ∇f (x), y − x(cid:105) ≥

(cid:107)y − x(cid:107)2 + Bf (x, y) .

µ
2

Proof. By strong convexity, f veriﬁes the inequality:

f (y) ≤ f (x) + (cid:104)∇f (y), y − x(cid:105) −

(cid:107)y − x(cid:107)2 ,

µ
2

for any x, y in the domain (see e.g. (Nesterov, 2004)). We then have the equivalences:

f (x) ≤ f (y) + (cid:104)∇f (x), x − y(cid:105) −

(cid:107)x − y(cid:107)2

µ
2

⇐⇒

(cid:107)x − y(cid:107)2 + f (x) − f (y) ≤ (cid:104)∇f (x), x − y(cid:105)

⇐⇒

(cid:107)x − y(cid:107)2 + f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105)
(cid:125)

(cid:124)

(cid:123)(cid:122)
Bf (x,y)

µ
2
µ
2

≤ (cid:104)∇f (x) − ∇f (y), x − y(cid:105) ,

(10)

where in the last line we have subtracted (cid:104)∇f (y), x − y(cid:105) from both sides of the inequality.

Lemma 2. Let the fi be L-smooth and convex functions. Then it is veriﬁed that:

1
n

n
(cid:88)

i=1

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ 2LBf (x, y) .

Proof. Since each fi is L-smooth, it is veriﬁed (see e.g. Nesterov (2004, Theorem 2.1.5)) that

(cid:107)∇fi(x) − ∇fi(y)(cid:107)2 ≤ 2L(cid:0)fi(x) − fi(y) − (cid:104)∇fi(y), x − y(cid:105)(cid:1) .

The result is obtained by averaging over i.

(7)

(8)

(9)

(11)

(12)

Lemma 3 (Characterization of the proximal operator). Let h be convex lower semicontinuous. Then
we have the following characterization of the proximal operator:

z = proxγh(x) ⇐⇒

(x − z) ∈ ∂h(z) .

(13)

1
γ

11

Proof. This is a direct consequence of the ﬁrst order optimality conditions on the deﬁnition of
proximal operator, see e.g. (Beck & Teboulle, 2009; Nesterov, 2013).

Lemma 4 (Firm non-expansiveness). Let x, ˜x be two arbitrary elements in the domain of ϕi and
z, ˜z be deﬁned as z := proxϕi

(˜x). Then it is veriﬁed that:

(x), ˜z := proxϕi

(cid:104)z − ˜z, x − ˜x(cid:105)(i) ≥ (cid:107)z − ˜z(cid:107)2

(i) .

Proof. By the block-separability of ϕi, the proximal operator is the concatenation of the proximal
operators of the blocks. In other words, for any block B ∈ Ti we have:

[z]B = proxγϕB ([x]B) ,

[˜z]B = proxγϕB ([˜x]B) ,

where ϕB is the restriction of ϕi to B. By ﬁrm non-expansiveness of the proximal operator (see
e.g. Bauschke & Combettes (2011, Proposition 4.2)) we have that:

(cid:104)[z]B − [˜z]B, [x]B − [˜x]B(cid:105) ≥ (cid:107)[z]B − [˜z]B(cid:107)2 .

Summing over the blocks in Ti yields the desired result.

(14)

(15)

12

Appendix B Sparse Proximal SAGA

This Appendix contains all proofs for Section 2. The main result of this section is Theorem 1, whose
proof is structured as follows:

• We start by proving four auxiliary results that will be used later on in the proofs of both
synchronous and asynchronous variants. The ﬁrst is the unbiasedness of key quantities used
in the algorithm. The second is a characterization of the solutions of (OPT) in terms of f
and ϕ (deﬁned below) in Lemma 6. The third is a key inequality in Lemma 7 that relates
the gradient mapping to other terms that arise in the optimization. The fourth is an upper
bound on the variance terms of the gradient estimator, relating it to the Bregman divergence
of f and the past gradient estimator terms.

• In Lemma 9, we deﬁne an upper bound on the iterates (cid:107)xt − x∗(cid:107)2, called a Lyapunov
function, and prove an inequality that relates this Lyapunov function value at the current
iterate with its value at the previous iterate.

• Finally, in the proof of Theorem 1 we use the previous inequality in terms of the Lyapunov

function to prove a geometric convergence of the iterates.

We start by proving the following unbiasedness result, mentioned in §2.

Lemma 5. Let Di and ϕi be deﬁned as in §2. Then it is veriﬁed that EDi = I p and E ϕi = h.

Proof. Let B ∈ B an arbitrary block. We have the following sequence of equalities:

where the last equality comes from the deﬁnition of nB. EDi = I p then follows from the arbitrari-
ness of B.

Similarly, for ϕi we have:

E[Di]B,B =

[Di]B,B =

dB1{B ∈ Ti}I |B|

1
n

n
(cid:88)

i=1

(cid:33)

1{B ∈ Ti}I |B|

1
n

1
n
(cid:32)

n
(cid:88)

i=1
n
(cid:88)

i=1

1
nB

n
nB

n
(cid:88)

i=1

=

=

1{B ∈ Ti}

I |B| = I |B| ,

Eϕi([x]B) =

dB1{B ∈ Ti}hB([x]B)

1
n

1
n
(cid:32)

n
(cid:88)

i=1
n
(cid:88)

i=1

1
nB

n
nB

n
(cid:88)

i=1

=

=

1{B ∈ Ti}hB([x]B)

(cid:33)

1{B ∈ Ti}

hB([x]B) = hB([x]B) ,

Finally, the result E ϕi = h comes from adding over all blocks.

Lemma 6. x∗ is a solution to (OPT) if and only if the following condition is veriﬁed:

x∗ = proxγϕ

(cid:0)x∗ − γD∇f (x∗)(cid:1) .

Proof. By the ﬁrst order optimality conditions, the solutions to (OPT) are characterized by the sub-
differential inclusion −∇f (x∗) ∈ ∂h(x∗). We can then write the following sequence of equiva-

13

(16)

(17)

(18)

(19)

(20)

(21)

(22)

lences:

−∇f (x∗) ∈ ∂h(x∗) ⇐⇒ −D∇f (x∗) ∈ D∂h(x∗)

(multiplying by D, equivalence since diagonals are nonzero)

⇐⇒ −D∇f (x∗) ∈ ∂ϕ(x∗)
(by deﬁnition of ϕ)

⇐⇒

(x∗ − γD∇f (x∗) − x∗) ∈ ∂ϕ(x∗)

1
γ

(adding and subtracting x∗)

⇐⇒ x∗ = proxγϕ(x∗ − γD∇f (x∗)) .

(by Lemma 3)

(23)

Since all steps are equivalences, we have the desired result.

The following lemma will be key in the proof of convergence for both the sequential and the parallel
versions of the algorithm. With this result, we will be able to bound the product between the gradient
mapping and the iterate suboptimality by:

• First, the negative norm of the gradient mapping, which will be key in the parallel setting

to cancel out the terms arising from the asynchrony.

• Second, variance terms in (cid:107)vi−Di∇f (x∗)(cid:107)2 that we will be able to bound by the Bregman

divergence using Lemma 2.

• Third and last, a product with terms in (cid:104)vi − Di∇f (x∗), x − x∗(cid:105), which taken in expec-
tation gives (cid:104)∇f (x) − ∇f (x∗), x − x∗(cid:105) and will allow us to apply Lemma 1 to obtain the
contraction terms needed to obtain a geometric rate of convergence.

Lemma 7 (Gradient mapping inequality). Let x be an arbitrary vector, x∗ a solution to (OPT), vi
as deﬁned in (SPS) and g = g(x, vi, i) the gradient mapping deﬁned in (3). Then the following
inequality is veriﬁed for any β > 0:

(cid:104)g, x − x∗(cid:105) ≥ −

(β − 2)(cid:107)g(cid:107)2 −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + (cid:104)vi − Di∇f (x∗), x − x∗(cid:105) .

(24)

γ
2

γ
2β

Proof. By ﬁrm non-expansiveness of the proximal operator (Lemma 4) applied to z = proxγϕi(x−
γvi) and ˜z = proxγϕi(x∗ − γD∇f (x∗)) we have:

(cid:107)z − ˜z(cid:107)2

(i) − (cid:104)z − ˜z, x − γvi − x∗ + γD∇f (x∗)(cid:105)(i) ≤ 0 .

(25)

By the (SPS) iteration we have x+ = z and by Lemma 3 we have that [z]Ti = [x∗]Ti, hence the
above can be rewritten as

(cid:107)x+ − x∗(cid:107)2

(i) − (cid:104)x+ − x∗, x − γvi − x∗ + γD∇f (x∗)(cid:105)(i) ≤ 0 .

(26)

14

We can now write the following sequence of inequalities

(cid:104)γg, x − x∗(cid:105) = (cid:104)x − x+, x − x∗(cid:105)(i)

(by deﬁnition and sparsity of g)

(cid:16)

≥

1 −

(cid:17)

β
2

(cid:16)

≥

1 −

(cid:16)

=

1 −

(cid:17)

(cid:17)

β
2

β
2

= (cid:104)x − x+ + x∗ − x∗, x − x∗(cid:105)(i)
= (cid:107)x − x∗(cid:107)2
≥ (cid:107)x − x∗(cid:107)2

(i) − (cid:104)x+ − x∗, x − x∗(cid:105)(i)
(i) − (cid:104)x+ − x∗, 2x − γvi − 2x∗ + γD∇f (x∗)(cid:105)(i) + (cid:107)x+ − x∗(cid:107)2
(i)

(27)

(adding Eq. (26))

= (cid:107)x − x+(cid:107)2
= (cid:107)x − x+(cid:107)2

(i) + (cid:104)x+ − x∗, γvi − γD∇f (x∗)(cid:105)(i)
(i) + (cid:104)x − x∗, γvi − γD∇f (x∗)(cid:105)(i) − (cid:104)x − x+, γvi − γD∇f (x∗)(cid:105)(i)

(completing the square)

(adding and substracting x)

(cid:107)x − x+(cid:107)2

(i) −

(cid:107)vi − D∇f (x∗)(cid:107)2

(i) + γ(cid:104)vi − D∇f (x∗), x − x∗(cid:105)(i)

(Young’s inequality 2(cid:104)a, b(cid:105) ≤

+ β(cid:107)b(cid:107)2, valid for arbitrary β > 0)

γ2
2β

γ2
2β

(cid:107)a(cid:107)2
β

(cid:107)x − x+(cid:107)2

(i) −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + γ(cid:104)vi − Di∇f (x∗), x − x∗(cid:105)

(by deﬁnition of Di and using the fact that vi is Ti-sparse)

(cid:107)γg(cid:107)2 −

(cid:107)vi − Di∇f (x∗)(cid:107)2 + γ(cid:104)vi − D∇f (x∗), x − x∗(cid:105) ,

(28)

γ2
2β

where in the last inequality we have used the fact that g is Ti-sparse. Finally, dividing by γ both
sides yields the desired result.

Lemma 8 (Upper bound on the gradient estimator variance). For arbitrary vectors x, (αi)n
vi as deﬁned in (SPS) we have:

i=0, and

E(cid:107)vi − Di∇f (x∗)(cid:107)2 ≤ 4LBf (x, x∗) + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(29)

Proof. We will now bound the variance terms. For this we have:

E(cid:107)vi − D∇f (x∗)(cid:107)2

(i) = E(cid:107)∇fi(x) − ∇fi(x∗) + ∇fi(x∗) − αi + Diα − D∇f (x∗)(cid:107)2
(i)

≤ 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)∇fi(x∗) − αi − (D∇f (x∗) − Dα)(cid:107)2
(i)

(by inequality (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2)
= 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)∇fi(x∗) − αi(cid:107)2

− 4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)(i) + 2E(cid:107)D∇f (x∗) − Dα(cid:107)2
(developing the square)

(i) .

(30)

We will now simplify the last two terms in the above expression. For the ﬁrst of the two last terms
we have:

− 4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)(i) = −4E(cid:104)∇fi(x∗) − αi, D∇f (x∗) − Dα(cid:105)

(31)

(support of ﬁrst term)

= −4(cid:104)∇f (x∗) − α, D∇f (x∗) − Dα(cid:105)
= −4(cid:107)∇f (x∗) − α(cid:107)2

D .

Similarly, for the last term we have:

2E(cid:107)D∇f (x∗) − Dα(cid:107)2

(i) = 2E(cid:104)Di∇f (x∗) − Diα, D∇f (x∗) − Dα(cid:105)

= 2(cid:104)∇f (x∗) − α, D∇f (x∗) − Dα(cid:105)

(32)

(33)

(using Lemma 5)
D .

= 2(cid:107)∇f (x∗) − α(cid:107)2

15

and so the addition of these terms is negative and can be dropped. In all, for the variance terms we
have

E(cid:107)vi − D∇f (x∗)(cid:107)2

(i) ≤ 2E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2

≤ 4LBf (x, x∗) + 2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(by Lemma 2)

(34)

c
n

n
(cid:88)

i=1

(cid:16)

We now deﬁne an upper bound on the quantity that we would like to bound, often called a Lyapunov
function, and establish a recursive inequality on this Lyapunov function.

Lemma 9 (Lyapunov inequality). Let L be the following c-parametrized function:

L(x, α) := (cid:107)x − x∗(cid:107)2 +

(cid:107)αi − ∇fi(x∗)(cid:107)2 .

(35)

Let x+ and α+ be obtained from the Sparse Proximal SAGA updates (SPS). Then we have:

EL(x+, α+) − L(x, α) ≤ − γµ(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:17)

c
n

(cid:16)

+

2γ2 −

(cid:17)

c
n

E(cid:107)αi − ∇fi(x)(cid:107)2 .

Proof. For the ﬁrst term of L we have:

(cid:107)x+ − x∗(cid:107)2 = (cid:107)x − γg − x∗(cid:107)2

(g := g(x, vi, i))

= (cid:107)x − x∗(cid:107)2 − 2γ(cid:104)g, x − x∗(cid:105) + (cid:107)γg(cid:107)2
≤ (cid:107)x − x∗(cid:107)2 + γ2(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γ(cid:104)vi − Di∇f (x∗), x − x∗(cid:105)

(by Lemma 7 with β = 1)

Since vi is an unbiased estimator of the gradient and EDi = I p, taking expectations we have:

E(cid:107)x+ − x∗(cid:107)2 ≤ (cid:107)x − x∗(cid:107)2 + γ2E(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γ(cid:104)∇f (x) − ∇f (x∗), x − x∗(cid:105)

≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + γ2E(cid:107)vi − Di∇f (x∗)(cid:107)2 − 2γBf (x, x∗) .

(38)

(by Lemma 1)

By using the variance terms bound (Lemma 8) in the previous equation we have:

E(cid:107)x+ − x∗(cid:107)2 ≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + (4Lγ2 − 2γ)Bf (x, x∗)
+ 2γ2E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

We will now bound the second term of the Lyapunov function. We have:

1
n

n
(cid:88)

i=1

(cid:107)α+

i − ∇fi(x∗)(cid:107)2 =

1 −

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

E(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2

(cid:18)

(cid:19)

1
n

1
n

(by deﬁnition of α+)

(cid:18)

(cid:19)

≤

1 −

1
n

2
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

LBf (x, x∗) .

(by Lemma 2)

(36)

(37)

(39)

(40)

(41)

16

Combining Eq. (39) and (40) we have:

EL(x+, α+) ≤ (1 − γµ)(cid:107)x − x∗(cid:107)2 + (4Lγ2 − 2γ)Bf (x, x∗) + 2γ2E(cid:107)αi − ∇fi(x∗)(cid:107)2

(cid:20)(cid:18)

+ c

1 −

(cid:19)

1
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 +

(cid:21)
2LBf (x, x∗)

= (1 − γµ)(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:16)

1
n
c
n

(cid:17)

(cid:16)

+

2γ2 −

(cid:17)

c
n

E(cid:107)αi − ∇fi(x∗)(cid:107)2 + cE(cid:107)αi − ∇fi(x∗)(cid:107)2

= L(x, α) − γµ(cid:107)x − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (x, x∗)

(cid:16)

(cid:17)

c
n

c
n
Finally, subtracting L(x, α) from both sides yields the desired result.

E(cid:107)αi − ∇fi(x∗)(cid:107)2 .

2γ2 −

+

(cid:17)

(cid:16)

(42)

Theorem 1. Let γ = a
converges geometrically in expectation with a rate factor of at least ρ = 1
xt obtained after t updates and x∗ the solution to (OPT), we have the bound:
i=1 (cid:107)α0

5L for any a ≤ 1 and f be µ-strongly convex. Then Sparse Proximal SAGA
κ }. That is, for

with C0 := (cid:107)x0 − x∗(cid:107)2 + 1
5L2

E(cid:107)xt − x∗(cid:107)2 ≤ (1 − ρ)tC0 ,

i − ∇fi(x∗)(cid:107)2

5 min{ 1

n , a 1

(cid:80)n

.

Proof. Let H := 1
n

(cid:80)

ELt+1 − (1 − ρ)Lt ≤ ρLt − γµ(cid:107)xt − x∗(cid:107)2 +

i (cid:107)αi − ∇fi(x∗)(cid:107)2. By the Lyapunov inequality from Lemma 9, we have:
(cid:17)
c
4Lγ2 − 2γ + 2L
n

c
n

(cid:16)

(cid:17)

(cid:16)

H

= (ρ − γµ) (cid:107)xt − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (xt, x∗) +

(cid:17)

c
n

(cid:17)

c
n

Bf (xt, x∗) +
(cid:20)
2γ2 + c

ρ −

(cid:18)

2γ2 −
(cid:19)(cid:21)

H

1
n

(cid:18)

(cid:19)

H

2c
3n

≤ (ρ − γµ) (cid:107)xt − x∗(cid:107)2 +

4Lγ2 − 2γ + 2L

Bf (xt, x∗) +

2γ2 −

(cid:16)

(cid:16)

(by deﬁnition of Lt)

(choosing ρ ≤

1
3n

)

= (ρ − γµ) (cid:107)xt − x∗(cid:107)2 + (cid:0)10Lγ2 − 2γ(cid:1) Bf (xt, x∗)

(choosing

= 3γ2)

c
n

(cid:16)

(cid:17)

aµ
5L

≤

ρ −

(cid:107)xt − x∗(cid:107)2
µ
a
L
5
And so we have the bound:

(for ρ ≤

≤ 0 .

·

)

(for all γ =

, a ≤ 1)

a
5L

(cid:18)

ELt+1 ≤

1 − min

(cid:110) 1
3n

,

a
5

·

1
κ

(cid:111)(cid:19)

(cid:18)

Lt ≤

min

, a ·

(cid:110) 1
n

(cid:111)(cid:19)

1
κ

Lt ,

1 −

1
5
3n ≤ 1

5n merely for clarity of exposition.

where in the last inequality we have used the trivial bound 1
Chaining expectations from t to 0 we have:
(cid:111)(cid:19)t+1

(cid:18)

ELt+1 ≤

1 −

min

, a ·

1
5

1
5

1
5

(cid:110) 1
n

(cid:110) 1
n

(cid:110) 1
n

L0
(cid:111)(cid:19)t+1 (cid:32)

(cid:111)(cid:19)t+1 (cid:32)

1
κ

1
κ

1
κ

(cid:18)

(cid:18)

(since a ≤ 1 and 3/5 ≤ 1) .

=

1 −

min

, a ·

(cid:107)x0 − x∗(cid:107)2 +

(cid:107)α0

i − ∇fi(x∗)(cid:107)2

≤

1 −

min

, a ·

(cid:107)x0 − x∗(cid:107)2 +

(cid:107)α0

i − ∇fi(x∗)(cid:107)2

(45)

3a2
52L2

1
5L2

n
(cid:88)

i=1
n
(cid:88)

i=1

The fact that Lt is a majorizer of (cid:107)xt − x∗(cid:107)2 completes the proof.

(43)

(44)

(cid:33)

(cid:33)

17

Appendix C ProxASAGA

In this Appendix we provide the proofs for results from Section 3, that is Theorem 2 (the convergence
theorem for PROXASAGA) and Corollary 1 (its speedup result).

Notation. Through this section, we use the following shorthand for the gradient mapping: gt :=
g(ˆxt, ˆvt
it

, it).

Appendix C.1 Proof outline.

As in the smooth case (h = 0), we start by using the deﬁnition of xt+1 in Eq. (4) to relate the
distance to the optimum in terms of its previous iterates:

(cid:107)xt+1 − x∗(cid:107)2 =(cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)ˆxt − xt, gt(cid:105) + γ2(cid:107)gt(cid:107)2 − 2γ(cid:104)ˆxt − x∗, gt(cid:105) .

(46)

However, in this case gt is not a gradient estimator but a gradient mapping, so we cannot continue
as is customary – by using the unbiasedness of the gradient in the (cid:104)ˆxt − x∗, gt(cid:105) term together with
the strong convexity of f (see Leblond et al. (2017, Section 3.5)).

To circumvent this difﬁculty, we derive a tailored inequality for the gradient mapping (Lemma 7
in Appendix B), which in turn allows us to use the classical unbiasedness and strong convexity
arguments to get the following inequality:

at+1 ≤ (1 −

γµ
2

)at + γ2E(cid:107)gt(cid:107)2 − 2γEBf (ˆxt, x∗) + γµE(cid:107)ˆxt − x(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105)
(cid:125)
(cid:124)

(cid:123)(cid:122)
additional asynchrony terms

(47)

+γ2(β − 2)E(cid:107)gt(cid:107)2 +
(cid:124)

γ2
β
(cid:123)(cid:122)
additional proximal and variance terms

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2
(cid:125)

,

where at := E(cid:107)xt − x∗(cid:107)2. Note that since f is strongly convex, Bf (ˆxt, x∗) ≥ µ
In the smooth setting, one ﬁrst expresses the additional asynchrony terms as linear combinations
of past gradient variance terms (E(cid:107)gu(cid:107)2)0≤u≤t. Then one crucially uses the negative Bregman
divergence term to control the variance terms. However, in our current setting, we cannot relate the
norm of the gradient mapping E(cid:107)gt(cid:107)2 to the Bregman divergence (from which h is absent). Instead,
we use the negative term γ2(β − 1)E(cid:107)gt(cid:107)2 to control all the (E(cid:107)gu(cid:107)2)0≤u≤t terms that arise from
asynchrony.

2 (cid:107)ˆxt − x∗(cid:107)2.

The rest of the proof consists in:
i) expressing the additional asynchrony terms as linear combinations of (E(cid:107)gu(cid:107)2)0≤u≤t, follow-
ing Leblond et al. (2017, Lemma 1);
ii) expressing the last variance term, (cid:107)ˆvt
it
divergences (Lemma 8 in Appendix B and Lemma 2 from Leblond et al. (2017));
iii) deﬁning a Lyapunov function, Lt := (cid:80)t
contraction given conditions on the maximum step size and delay.

u=0(1 − ρ)t−uau, and proving that it is bounded by a

− Dit∇f (x∗)(cid:107)2, as a linear combination of past Bregman

Appendix C.2 Detailed proof

Theorem 2 (Convergence guarantee and rate of PROXASAGA). Suppose τ ≤ 1
√
10
36 min{1, 6κ
size γ = a
expectation at a geometric rate factor of at least: ρ(a) = 1
(1 − ρ)t ˜C0, where ˜C0 is a constant independent of t (≈ nκ

. For any step
τ }, the inconsistent read iterates of Algorithm 1 converge in
(cid:9), i.e. E(cid:107)ˆxt − x∗(cid:107)2 ≤
5 min (cid:8) 1

a C0 with C0 as deﬁned in Theorem 1).

L with a ≤ 1

n , a 1

∆

κ

Proof. In order to get an initial recursive inequality, we ﬁrst unroll the (virtual) update:
(cid:107)xt+1 − x∗(cid:107)2 = (cid:107)xt − γgt − x∗(cid:107)2 = (cid:107)xt − x∗(cid:107)2 + (cid:107)γgt(cid:107)2 − 2γ(cid:104)gt, xt − x∗(cid:105)

= (cid:107)xt − x∗(cid:107)2 + (cid:107)γgt(cid:107)2 − 2γ(cid:104)gt, ˆxt − x∗(cid:105) + 2γ(cid:104)gt, ˆxt − xt(cid:105) ,

(48)

18

and then apply Lemma 7 with x = ˆxt and v = ˆvt
(cid:104)·(cid:105)(i) = (cid:104)·(cid:105)(it).

it. Note that in this case we have g = gt and

(cid:107)xt+1 − x∗(cid:107)2 ≤ (cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)gt, ˆxt − xt(cid:105) + γ2(cid:107)gt(cid:107)2 + γ2(β − 2)(cid:107)gt(cid:107)2

+

γ2
β

(cid:107)ˆvt
it

− D∇f (x∗)(cid:107)2

(it) − 2γ(cid:104)ˆvt
it

− D∇f (x∗), ˆxt − x∗(cid:105)(it)

= (cid:107)xt − x∗(cid:107)2 + 2γ(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)(cid:107)gt(cid:107)2

− Dit∇f (x∗)(cid:107)2 − 2γ(cid:104)ˆvt
it

− Dit∇f (x∗), ˆxt − x∗(cid:105).

(49)

+

(cid:107)ˆvt
it

γ2
β
(as [ˆvt
it

]Tit

= ˆvt

it)

We now use the property that it is independent of ˆxt (which we enforce by reading ˆxt before picking
it, see Section 3), together with the unbiasedness of the gradient update ˆvt
= ∇f (ˆxt)) and
the deﬁnition of D to simplify the following expression as follows:

it (Eˆvt
it

E(cid:104)ˆvt
it

− Dit∇f (x∗), ˆxt − x∗(cid:105) = (cid:104)∇f (ˆxt) − ∇f (x∗), ˆxt − x∗(cid:105)
µ
(cid:107)ˆxt − x∗(cid:107)2 + Bf (ˆxt, x∗) ,
2

≥

where the last inequality comes from Lemma 1. Taking conditional expectations on (49) we get:

E(cid:107)xt+1 − x∗(cid:107)2 ≤ (cid:107)xt − x∗(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)E(cid:107)gt(cid:107)2

+

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2 − γµ(cid:107)ˆxt − x∗(cid:107)2 − 2γBf (ˆxt, x∗)

≤ (1 −

)(cid:107)xt − x∗(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105) + γ2(β − 1)E(cid:107)gt(cid:107)2

+

E(cid:107)ˆvt
it

− Dit∇f (x∗)(cid:107)2 + γµ(cid:107)ˆxt − xt(cid:107)2 − 2γBf (ˆxt, x∗)

(using (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2 on (cid:107)xt − ˆxt + ˆxt − x∗(cid:107)2)

γ2
β
γµ
2
γ2
β

γµ
2

≤ (1 −

)(cid:107)xt − x∗(cid:107)2 + γ2(β − 1)E(cid:107)gt(cid:107)2 + γµ(cid:107)ˆxt − xt(cid:107)2 + 2γE(cid:104)gt, ˆxt − xt(cid:105)
2γ2
β

− ∇fit (x∗)(cid:107)2 .

Bf (ˆxt, x∗) +

4γ2L
β

E(cid:107) ˆαt
it

(52)

− 2γBf (ˆxt, x∗) +

(using Lemma 8 on the variance terms)

Since we also have:

ˆxt − xt = γ

Gt

ug(ˆxu, ˆαu, iu),

t−1
(cid:88)

u=(t−τ )+

the effect of asynchrony for the perturbed iterate updates was already derived in a very similar setup
in Leblond et al. (2017). We re-use the following bounds from their Appendix C.4:7

E(cid:107)ˆxt − xt(cid:107)2 ≤ γ2(1 +

∆τ )

E(cid:107)gu(cid:107)2 ,

√

t−1
(cid:88)

u=(t−τ )+

Leblond et al. (2017, Eq. (48))

E(cid:104)gt, ˆxt − xt(cid:105) ≤

E(cid:107)gu(cid:107)2 +

E(cid:107)gt(cid:107)2 . Leblond et al. (2017, Eq. (46)).

√

γ

∆

2

t−1
(cid:88)

u=(t−τ )+

√

γ

∆τ
2

7The appearance of the sparsity constant ∆ is coming from the crucial property that E(cid:107)x(cid:107)2
∀x ∈ Rp (see Eq. (39) in Leblond et al. (2017), where they use the notation (cid:107) · (cid:107)i for our (cid:107) · (cid:107)(i)).

(i) ≤ ∆(cid:107)x(cid:107)2

19

(50)

(51)

(53)

(54)

(55)

Because the updates on α are the same for PROXASAGA as for ASAGA, we can re-use the same
argument arising in the proof of Leblond et al. (2017, Lemma 2) to get the following bound on
E(cid:107) ˆαt
it

− ∇fit(x∗)(cid:107)2:

E(cid:107) ˆαt
it

− ∇fit(x∗)(cid:107)2 ≤

(1 −

)(t−2τ −u−1)+EBf (ˆxu, x∗)

+2L(1 −

)(t−τ )+ ˜e0 ,

(56)

1
n

2L
n

t−1
(cid:88)

u=1
(cid:124)

1
n

(cid:123)(cid:122)
Henceforth denoted Ht

(cid:125)

E(cid:107)α0

i − f (cid:48)

where ˜e0 := 1
i (x∗)(cid:107)2. This bound is obtained by analyzing which gradient could be
2L
the source of αit in the past (taking in consideration the inconsistent writes), and then applying
Lemma 2 on the E(cid:107)∇f (ˆxu) − ∇f (x∗)(cid:107)2 terms, explaining the presence of Bf (ˆxu, x∗) terms.8
The inequality (56) corresponds to Eq. (56) and (57) in Leblond et al. (2017).

By taking the full expectation of (52) and plugging the above inequalities back, we obtain an in-
equality similar to Leblond et al. (2017, Master inequality (28)) which describes how the error terms
at := E(cid:107)xt − x∗(cid:107)2 of the virtual iterates are related:

at+1 ≤(1 −

)at +

(1 −

)(t−τ )+ ˜e0

γµ
2
+ γ2 (cid:104)

4γ2L
β
√

1
n

β − 1 +

∆τ

(cid:105)

E(cid:107)gt(cid:107)2 +

√

(cid:104)

γ2

∆ + γ3µ(1 +

√

(cid:105)
∆τ )

t
(cid:88)

E(cid:107)gu(cid:107)2

(57)

u=(t−τ )+

− 2γEBf (ˆxt, x∗) +

EBf (ˆxt, x∗) +

4γ2L
β

4γ2L
βn

Ht .

We now have a promising inequality with a contractive term and several quantities that we need to
bound. In order to achieve our ﬁnal result, we introduce the same Lyapunov function as in Leblond
et al. (2017):

Lt :=

(1 − ρ)t−uau ,

t
(cid:88)

u=0

where ρ is a target rate factor for which we will provide a value later on. Proving that this Lyapunov
function is bounded by a contraction will ﬁnish our proof. We have:

Lt+1 =

(1 − ρ)t+1−uau = (1 − ρ)t+1a0 +

(1 − ρ)t+1−uau

t+1
(cid:88)

u=0

= (1 − ρ)t+1a0 +

(1 − ρ)t−uau+1 .

(58)

t+1
(cid:88)

u=1
t
(cid:88)

u=0

We now plug our new bound on at+1, (57):
(1 − ρ)t−u(cid:104)

Lt+1 ≤ (1 − ρ)t+1a0 +

t
(cid:88)

u=0

)(u−τ )+ ˜e0

(1 −

)au +

γµ
2
+ γ2(cid:0)β − 1 +
√

+ (cid:0)γ2

(1 −

4γ2L
1
β
n
∆τ (cid:1)E(cid:107)gu(cid:107)2
√

√

∆ + γ3µ(1 +

∆τ )(cid:1)

u
(cid:88)

E(cid:107)gv(cid:107)2

v=(u−τ )+

− 2γEBf (ˆxu, x∗) +

EBf (ˆxu, x∗) +

4γ2L
β

4γ2L
βn

(cid:105)

Hu

.

(59)

After regrouping similar terms, we get:

Lt+1 ≤ (1 − ρ)t+1(a0 + A˜e0) + (1 −

)Lt +

st
u

E(cid:107)gu(cid:107)2 +

rt
u

EBf (ˆxu, x∗) .

(60)

γµ
2

t
(cid:88)

u=0

t
(cid:88)

u=1

8Note that Leblond et al. (2017) analyzed the unconstrained scenario, and so Bf (ˆxu, x∗) is replaced by the

simpler f (ˆxu) − f (x∗) in their bound.

20

Now, provided that we can prove that under certain conditions the st
u terms are all negative
(and that the A term is not too big), we can drop them from the right-hand side of (60) which will
allow us to ﬁnish the proof.
Let us compute these terms. Let q := 1−1/n

1−ρ and we assume in the rest that ρ < 1/n.

u and rt

Computing A. We have:

4γ2L
β

t
(cid:88)

u=0

(1 − ρ)t−u(1 −

)(u−τ )+ ≤

(1 − ρ)t(1 − ρ)−τ (τ + 1 +

1
n

4γ2L
β

1
1 − q

)

from Leblond et al. (2017, Eq (75))

= (1 − ρ)t+1 4γ2L
β

(cid:124)

(1 − ρ)−τ −1(τ + 1 +

)

.

(61)

1
1 − q

(cid:125)

(cid:123)(cid:122)
:=A

E(cid:107)gu(cid:107)2 ≤ τ (1 − ρ)−τ

(1 − ρ)t−uE(cid:107)gu(cid:107)2 ,

(62)

Computing st

u. Since we have:
t
(cid:88)

(1 − ρ)t−u

u−1
(cid:88)

u=0

v=(u−τ )+

we have for all 0 ≤ u ≤ t:
u ≤ (1 − ρ)t−u(cid:104)
st

t
(cid:88)

u=0

√

γ2(cid:0)β − 1 +

∆τ ) + τ (1 − ρ)−τ (cid:0)γ2

∆ + γ3µ(1 +

√

√

∆τ )(cid:1)(cid:105)

.

(63)

u. To analyze these quantities, we need to compute: (cid:80)t

Computing rt
v=1 (1 −
1
n )(u−2τ −v−1)+. Fortunately, this is already done in Leblond et al. (2017, Eq (66)), and thus we
know that for all 1 ≤ u ≤ t:

u=0(1 − ρ)t−u (cid:80)u−1

u ≤ (1 − ρ)t−u
rt

−2γ +

(cid:20)

4γ2L
β

+

4Lγ2
nβ

(1 − ρ)−2τ −1(cid:16)

2τ +

(cid:17)(cid:21)

,

1
1 − q

(64)

recalling that q := 1−1/n

1−ρ and that we assumed ρ < 1
n .

We now need some assumptions to further analyze these quantities. We make simple choices for
simplicity, though a tighter analysis is possible. To get manageable (and simple) constants, we
follow Leblond et al. (2017, Eq. (82) and (83)) and assume:

ρ ≤

τ ≤

1
4n

;

n
10

.

This tells us:

1
1 − q

≤

4n
3
4
3

(1 − ρ)−kτ −1 ≤

for 0 ≤ k ≤ 2 .

(using Bernouilli’s inequality)

Additionally, we set β = 1

2 . Equation (63) thus becomes:
(cid:0)√

√

(cid:20)

−

+

∆τ +

u ≤ γ2(1 − ρ)t−u
st

4
3

1
2

∆τ + γµτ (1 +

√

(cid:21)

∆τ )(cid:1)

.

We see that for st
get:

u to be negative, we need τ = O( 1√
∆

). Let us assume that τ ≤ 1
√
10

∆

. We then

u ≤ γ2(1 − ρ)t−u
st

(cid:20)

−

+

+

+ γµτ

1
2

1
10

4
30

(cid:21)

.

4
3

11
10

Thus, the condition under which all st

(65)

(66)

(67)

(68)

u are negative boils down to:
2
11

γµτ ≤

.

21

Now looking at the rt

u terms given our assumptions, the inequality (64) becomes:

u ≤ (1 − ρ)t−u
rt

(cid:20)
−2γ + 8γ2L +

8γ2L
n

4
3

(cid:0) n
5

+

(cid:21)

(cid:1)

4n
3

≤ (1 − ρ)t−u(cid:0) − 2γ + 36γ2L(cid:1) .

The condition for all rt

u to be negative then can be simpliﬁed down to:

γ ≤

1
18L

.

γ ≤

1
36L

.

We now have a promising inequality for proving that our Lyapunov function is bounded by a con-
traction. However we have deﬁned Lt in terms of the virtual iterate xt, which means that our result
would only hold for a given T ﬁxed in advance, as is the case in Mania et al. (2017). Fortunately,
we can use the same trick as in Leblond et al. (2017, Eq. (97)): we simply add γBf (ˆxt, x∗) to both
sides in (60). rt
t + γ, which makes for a slightly worse bound on γ to ensure linear
convergence:

t is replaced by rt

For this small cost, we get a contraction bound on Bf (ˆxt, x∗), and thus by the strong convexity of
f (see (9)) we get a contraction bound for E(cid:107)ˆxt − x∗(cid:107)2.
Recap. Let us use ρ = 1
to:

L . Then the conditions (68) and (71) on the step size γ reduce

4n and γ := a

Moreover, the condition:

a ≤

min{1,

1
36

72
11

κ
τ

}.

τ ≤

1
√

∆
is sufﬁcient to also ensure that (65) is satisﬁed as ∆ ∈ [ 1

10

Thus under the conditions (72) and (73), we have that all st
rewrite the recurrent step of our Lyapunov function as:

√

n ≤ n.

1√
∆

≤

n , 1], and thus
u and rt

u terms are negative and we can

Lt+1 ≤ γEBf (ˆxt) + Lt+1 ≤ (1 − ρ)t+1(a0 + A˜e0) + (1 −

)Lt .

(74)

γµ
2

By unrolling the recursion (74), we can carefully combine the effect of the geometric term (1 − ρ)
with the one of (1 − γµ
2 ). This was already done in Leblond et al. (2017, Apx C.9, Eq. (101) to
(103)), with a trick to handle various boundary cases, yielding the overall rate:

where ρ∗ = min{ 1
To get the ﬁnal constant, we need to bound A. We have:

5κ } (that we simpliﬁed to ρ∗ = 1

5n , a 2

EBf (ˆxt, x∗) ≤ (1 − ρ∗)t+1 ˆC0,
5 min{ 1

n , a 1

κ } in the theorem statement).

A =

(1 − ρ)−τ −1(τ + 1 +

1
1 − q

)

4γ2L
β

n
10

(

≤ 8γ2L

4
3
≤ 26γ2Ln
≤ γn .

+ 1 +

4n
3

)

This is the same bound on A that was used by Leblond et al. (2017) and so we obtain the same
constant as their Eq. (104):

ˆC0 :=

21n
γ

((cid:107)x0 − x∗(cid:107)2 + γ

E(cid:107)α0

i − ∇fi(x∗)(cid:107)2).

n
2L

Note that ˆC0 = O( n

γ C0) with C0 deﬁned as in Theorem 1.

22

(69)

(70)

(71)

(72)

(73)

(75)

(76)

(77)

Now, using the strong convexity of f via (9), we get:

E(cid:107)ˆxt − x∗(cid:107)2 ≤

EBf (ˆxt, x∗) ≤ (1 − ρ∗)t+1 ˜C0,

(78)

2
µ

where ˜C0 = O( nκ
a C0).
This ﬁnishes the proof for Theorem 2.

Corollary 3 (Speedup). Suppose τ ≤ 1
. If κ ≥ n, then using the step size γ = 1/36L, PROXAS-
√
10
AGA converges geometrically with rate factor Ω( 1
κ ). If κ < n, then using the step size γ = 1/36nµ,
PROXASAGA converges geometrically with rate factor Ω( 1
n ). In both cases, the convergence rate
is the same as Sparse Proximal SAGA and PROXASAGA is thus linearly faster than its sequential
counterpart up to a constant factor. Note that in both cases the step size does not depend on τ .

∆

Furthermore, if τ ≤ 6κ, we can use a universal step size of Θ(1/L) to get a similar rate for PROX-
ASAGA than Sparse Proximal SAGA, thus making it adaptive to local strong convexity since the
knowledge of κ is not required.

Proof. If κ ≥ n, the rate factor of Sparse Proximal SAGA is 1/κ. To get the same rate factor, we
need to choose a = Ω(1), which we can fortunately do since κ ≥ n ≥

≥ 10τ .

√

n ≥ 10 1
√
10

∆

If κ < n, then the rate factor of Sparse Proximal SAGA is 1/n. Any choice of a bigger than Ω(κ/n)
gives us the same rate factor for PROXASAGA. Since τ ≤
n/10 we can pick such an a without
violating the condition of Theorem 2.

√

23

Appendix D Comparison with related work

In this section, we relate our theoretical results and proof technique with the related literature.

Speedups. Our speedup regimes are comparable with the best ones obtained in the smooth case,
including Niu et al. (2011); Reddi et al. (2015), even though unlike these papers, we support incon-
sistent reads and nonsmooth objective functions. The one exception is Leblond et al. (2017), where
the authors prove that their algorithm, ASAGA, can obtain a linear speedup even without sparsity in
the well-conditioned regime. In contrast, PROXASAGA always requires some sparsity. Whether this
property for smooth objective functions could be extended to the composite case remains an open
problem.

Coordinate Descent. We compare our approach for composite objective functions to its most
natural competitor: ASYSPCD (Liu & Wright, 2015), an asynchronous stochastic coordinate descent
algorithm. While ASYSPCD also exhibits linear speedups, subject to a condition on τ , one has to be
especially careful when trying to compare these conditions.

First, while in theory the iterations of both algorithms have the same cost, in practice various tricks
are introduced to save on computation, yielding different costs per updates.9 Second, the bound on
τ for the coordinate descent algorithm depends on p, the dimensionality of the problem, whereas
ours involves n, the number of data points. Third, a more subtle issue is that τ is not affected by
the same quantities for both algorithms.10 See Appendix D.1 for a more detailed explanation of the
differences between the bounds.

√

∆) is necessary for a linear speedup. This means in case ∆ ≤ 1/√

In the best case scenario (where the components of the gradient are uncorrelated, a somewhat un-
√
realistic setting), ASYSPCD can get a near-linear speedup for τ as big as 4
p. Our result states that
τ = O(1/
p our bound is better
than the one obtained for ASYSPCD. Recalling that 1/n ≤ ∆ ≤ 1, it appears that PROXASAGA is
favored when n is bigger than
p whereas ASYSPCD may have a better bound otherwise, though
this comparison should be taken with a grain of salt given the assumptions we had to make to arrive
at comparable quantities.

√

Furthermore, one has to note that while Liu & Wright (2015) use the classical labeling scheme
inherited from Niu et al. (2011), they still assume in their proof that the it are uniformly distributed
and that their gradient estimators are conditionally unbiased – though neither property is veriﬁed in
the general asynchronous setting. Finally, we note that ASYSPCD (as well as its incremental variant
Async-PROXSVRCD) assumes that the computation and assignment of the proximal operator is an
atomic step, while we do not make such assumption.

SVRG. The Async-ProxSVRG algorithm of Meng et al. (2017) also exhibits theoretical linear
speedups subject to the same condition as ours. However, the analyzed algorithm uses dense updates
and consistent read and writes. Although they make the analysis easier, these two factors introduce
costly bottlenecks and prevent linear speedups in running time. Furthermore, here again the classical
labeling scheme is used together with the unveriﬁed conditional unbiasedness condition.

Doubly stochastic algorithms. The Async-PROXSVRCD algorithm from Meng et al. (2017); Gu
et al. (2016) has a maximum allowable stepsize11 that is in O(1/pL), whereas the maximum step
size for PROXASAGA is in Ω(1/L), so can be up to p times bigger. Consequently, PROXASAGA
enjoys much faster theoretical convergence rates. Unfortunately, we could not ﬁnd a condition for
linear speedups to compare to. We also note that their algorithm is not appropriate in a sparse
features setting. This is illustrated in an empirical comparison in Appendix F where we see that

9For PROXASAGA the relevant quantity becomes the average number of features per data point. For
In both cases the tricks involved are

ASYSPCD it is rather the average number of data points per feature.
not covered by the theory.

10To make sure τ is the same quantity for both algorithms, we have to assume that the iteration costs are

homogeneous.

11To the best of our understanding, noting that extracting an interpretable bound from the given theoretical
results was difﬁcult. Furthermore, it appears that the proof technique may still have signiﬁcant issues: for
example, the “fully lock-free” assumption of Gu et al. (2016) allows for overwrites, and is thus incompatible
with their framework of analysis, in particular their Eq. (8).

24

their convergence in number of iterations is orders of magnitude slower than appropriate algorithms
like SAGA or PROXASAGA.

Appendix D.1 Comparison of bounds with Liu & Wright (2015)

Iteration costs. For both PROXASAGA and ASYSPCD, the average cost of an iteration is O(nS)
(where S is the average support size).
In the case of PROXASAGA (see Algorithm 1), at each
iteration the most costly operation is the computation of α, while in the general case we need to
compute a full gradient for ASYSPCD.

In order to reduce these prohibitive computation costs, several tricks are introduced. Although they
lead to much improved empirical performance, it should be noted that in both cases these tricks are
not covered by the theory. In particular, the unbiasedness condition can be violated.

In the case of PROXASAGA, we store the average gradient term α in shared memory. The cost
of each iteration then becomes the size of the extended support of the partial gradient selected at
random at this iteration, hence it is in O(∆l), where ∆l := maxi=1..n |Ti|.

For ASYSPCD, following Peng et al. (2016) we can store intermediary quantities for speciﬁc losses
(e.g. (cid:96)1-regularized logistic regression). The cost of an iteration then becomes the number of data
points whose extended support includes the coordinate selected at random at this iteration, hence it
is in O(n∆).

The relative difference in update cost of both algorithms then depends heavily on the data matrix:
if the partial gradients usually have a extended support but coordinates belong to few of them (this
can be the case if n (cid:28) p for example), then the iterations of ASYSPCD can be cheaper than those of
PROXASAGA. Conversely, if data points usually have small extended support but coordinates belong
to many of them (which can happen when p (cid:28) n for example), then the updates of PROXASAGA
are the cheaper ones.

Dependency of τ on the data matrix.
In the case of PROXASAGA the sizes of the extended
support of each data point are important – they are directly linked to the cost of each iteration.
Identical iteration costs for each data point do not inﬂuence τ , whereas heterogeneous costs may
cause τ to increase substantially. In contrast, in the case of ASYSPCD, the relevant parts of the data
matrix are the number of data points each dimension touches – for much the same reason. In the
bipartite graph between data points and dimensions, either the left or the right degrees matter for τ ,
depending on which algorithm you choose.

In order to compare their respective bounds, we have to make the assumption that the iteration costs
are homogeneous, which means that each data point has the same support size and each dimension is
active in the same number of data points. This implies that τ is the same quantity for both algorithms.

√

Best case scenario bound for AsySPCD. The result obtained in Liu & Wright (2015) states that
if τ 2Λ = O(
p), ASYSPCD can get a near-linear speedup (where Λ is a measure of the interactions
p). In the best possible scenario where
between the components of the gradient, with 1 ≤ Λ ≤
Λ = 1 (which means that the coordinates of the gradients are completely uncorrelated), τ can be as
√
big as 4

√

p.

25

Appendix E Implementation details

Initialization.
In the Sparse Proximal SAGA algorithm and its asynchronous variant, PROXAS-
AGA, the vector x can be initialized arbitrarily. The memory terms αi can be initialized to any
vector that veriﬁes supp(αi) = supp(∇fi). In practice we found that the initialization αi = 0 is
very fast to set up and often outperforms more costly initializations.

With this initialization, the gradient approximation before the ﬁrst update of the memory terms be-
comes ∇fi(x) + Diα. Since most of the values in α are zero, α will tend to be small compared to
∇fi(x), and so the gradient estimate is very close to the SGD estimate ∇fi(x). The SGD approx-
imation is known to have a very fast initial convergence (which, in light of Figure 1, our method
inherits) and has even been used as a heuristic to use during the ﬁrst epoch of variance reduced
methods (Schmidt et al., 2016).

The initialization of coefﬁcients x0 was always set to zero.

Exact regularization. Computing the gradient of a smooth regularization such as the squared (cid:96)2
penalty of Eq. (6) is independent of n and so we can use the exact regularizer in the update of
the coefﬁcients instead of storing it in α, which would also destroy the compressed storage of the
memory terms described below. In practice we use this “exact regularization”, multiplied by Di to
preserve the sparsity pattern.
Assuming a squared (cid:96)2 regularization term of the form λ
(note the extra λx)

2 , the gradient estimate in (SPS) becomes

vi = ∇fi(x) − αi + Di(α + λx) .

(79)

Storage of memory terms. The storage requirements for this method is in the worst case a table
of size n × p. However, as for SAG and SAGA, for linearly parametrized loss functions of the form
fi(x) = (cid:96)(aT
i=1 are samples associated with
the learning problem, this can be reduced to a table of size n (Schmidt et al., 2016, §4.1). This
includes popular linear models such as least squares or logistic regression with (cid:96) the squared or
logistic function, respectively.

i x), where (cid:96) is some real-valued function and (ai)n

The reduce storage comes from the fact that in this case the partial gradients have the structure

∇fi(x) = ai (cid:96)(cid:48)(aT

.

i x)
(cid:124) (cid:123)(cid:122) (cid:125)
scalar

(80)

Since ai is independent of x, we only need to store the scalar (cid:96)(cid:48)(aT
explains why ∇fi inherits the sparsity pattern of ai.

i x). This decomposition also

Atomic updates. Most modern processors have support for atomic operations with minimal over-
head. In our case, we implemented a double-precision atomic type using the C++11 atomic features
(std::atomic<double>). This type implements atomic operations through the compare and swap
semantics.

Empirically, we have found it necessary to implement atomic operations at least in the vector α and
α to reach arbitrary precision. If non-atomic operations are used, the method converges only to a
limited precision (around normalized function suboptimality of 10−3), which might be sufﬁcient for
some machine learning applications but which we found not satisfying from an optimization point
of view.

AsySPCD. Following (Peng et al., 2016) we keep the vector (aT
at each iteration using atomic updates.

i x)n

i=1 in memory and update it

Hardware and software. All experiments were run on a Dell PowerEdge 920 machine with 4 Intel
Xeon E7-4830v2 processors with 10 2.2GHz cores each and 384GB 1600 Mhz RAM. The PROX-
ASAGAand ASYSPCD code was implemented on C++ and binded in Python. The FISTA code is
implemented in pure Python using NumPY and SciPy for matrix computations (in this case the bot-
tleneck is in large sparse matrix-vector operations for which efﬁcient BLAS routines were used). Our
PROXASAGA implementation can be downloaded from http://github.com/fabianp/ProxASAGA.

26

Appendix F Experiments

All datasets used for the experiments were downloaded from the LibSVM dataset suite.12

Appendix F.1 Comparison of ProxASAGA with other sequential methods

We provide a comparison between the Sparse Proximal SAGA and related methods in the sequential
case. We compare against two methods: the MRBCD method of Zhao et al. (2014) (which forms
the basis of Async-PROXSVRCD) and the vanilla implementation of SAGA (Defazio et al., 2014),
which does not have the ability to perform sparse updates. We compare in terms of both passes
through the data (epochs) and time. We use the same step size for all methods (1/3L). Due to
the slow convergence of some methods, we use a smaller dataset than the ones used in §4. Dataset
RCV1 has n = 697, 641, d = 47, 236 and a density of 0.15, while Covtype is a dense dataset with
n = 581, 012, d = 54.

Figure 2: Suboptimality of different sequential algorithms. Each marker represents one pass through
the dataset.

We observe that for the convergence behavior in terms of number of passes, Sparse Proximal SAGA
performs as well as vanilla SAGA, though the latter requires dense updates at every iteration (Fig. 2
top left). On the other hand, in terms of running time, our implementation of Sparse Proximal SAGA
is much more efﬁcient than the other methods for sparse input (Fig. 2 top right). In the case of dense
input (Fig. 2 bottom), the three methods perform similarly.
A note on the performance of MRBCD.
It may appear surprising that Sparse Proximal SAGA
outperforms MRBCD so dramatically on sparse datasets. However, one should note that MRBCD is
a doubly stochastic algorithm where both a random data point and a random coordinate are sampled
for each iteration. If the data matrix is very sparse, then the probability that the sampled coordinate
is in the support of the sampled data point becomes very low. This means that the gradient estimator
term only contains the reference gradient term of SVRG, which only changes once per epoch. As a
result, this estimator becomes very coarse and produces a slower empirical convergence.

This is reﬂected in the theoretical results given in Zhao et al. (2014), where the epoch size needed to
get linear convergence are k times bigger than the ones required by plain SVRG, where k is the size
of the set of blocks of coordinates.

12https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/

27

Appendix F.2 Theoretical speedups.

In the experimental section, we have shown experimental speedup results where suboptimality was
a function of the running time. This measure encompasses both theoretical algorithmic optimization
properties and hardware overheads (such as contention of shared memory) which are not taken into
account in our analysis.

In order to isolate these two effects, we now plot our speedup results in Figure 3 where suboptimality
is a function of the number of iterations; thus, we abstract away any potential hardware overhead. To
do so, we implement a global counter which is sparsely updated (every 100 iterations for example)
in order not to modify the asynchrony of the system. This counter is used only for plotting purposes
and is not needed otherwise. Speciﬁcally, we deﬁne the theoretical speedup as:

theoretical speedup := (number of cores)

number of iterations for sequential algorithm
total number of iterations for parallel algorithm

.

Figure 3: Theoretical optimization speedups for (cid:96)1+(cid:96)2-regularized logistic regression. Speedup
as measured by the number of iterations required to reach 10−5 suboptimality for PROXASAGA
and ASYSPCD. In FISTA the iterates are the same with different cores and so matches the “ideal”
speedup.

We see clearly that the theoretical speedups obtained by both PROXASAGAand ASYSPCD are linear
(i.e. ideal). As we observe worse results in running time, this means that the hardware overheads of
asynchronous methods are quite signiﬁcant.

Appendix F.3 Timing benchmarks

We now provide the time it takes for the different methods with 10 cores to reach a suboptimality of
10−10. All results are in hours.

Dataset

PROXASAGA ASYSPCD

FISTA

KDD 2010
KDD 2012
Criteo

1.01
0.09
0.14

13.3
26.6
33.3

5.2
8.3
6.6

Appendix F.4 Hyperparameters

The (cid:96)1-regularization parameter λ2 was chosen as to give around 10% of non-zero features. The
exact chosen values are the following: λ2 = 10−11 for KDD 2010, λ2 = 10−16 for KDD 2012 and
λ2 = 4 × 10−12 for Criteo.

28

