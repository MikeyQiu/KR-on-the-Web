6
1
0
2
 
r
a

M
 
1
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
0
4
3
6
0
.
3
0
6
1
:
v
i
X
r
a

1

Data Augmentation via L´evy Processes

Stefan Wager
Stanford University
Stanford, USA

William Fithian
University of California, Berkeley
Berkeley, USA

Percy Liang
Stanford University
Stanford, USA

swager@stanford.edu

wfithian@berkeley.edu

pliang@cs.stanford.edu

If a document is about travel, we may expect that short snippets of the document
should also be about travel. We introduce a general framework for incorporating
these types of invariances into a discriminative classiﬁer. The framework imagines
data as being drawn from a slice of a L´evy process. If we slice the L´evy process at
an earlier point in time, we obtain additional pseudo-examples, which can be used
to train the classiﬁer. We show that this scheme has two desirable properties: it
preserves the Bayes decision boundary, and it is equivalent to ﬁtting a generative
model in the limit where we rewind time back to 0. Our construction captures popular
schemes such as Gaussian feature noising and dropout training, as well as admitting
new generalizations.

1.1 Introduction

Black-box discriminative classiﬁers such as logistic regression, neural networks, and
SVMs are the go-to solution in machine learning: they are simple to apply and
often perform well. However, an expert may have additional knowledge to exploit,
often taking the form of a certain family of transformations that should usually
leave labels ﬁxed. For example, in object recognition, an image of a cat rotated,
translated, and peppered with a small amount of noise is probably still a cat.
Likewise, in document classiﬁcation, the ﬁrst paragraph of an article about travel

2

Data Augmentation via L´evy Processes

Figure 1.1: Two examples of transforming an original input X into a noisy, less
informative input (cid:101)X. The new inputs clearly have the same label but contain less
information and thus are harder to classify.

is most likely still about travel. In both cases, the “expert knowledge” amounts
to a belief that a certain transform of the features should generally not aﬀect an
example’s label.

One popular strategy for encoding such a belief is data augmentation: generat-
ing additional pseudo-examples or “hints” by applying label-invariant transforma-
tions to training examples’ features (Abu-Mostafa, 1990; Sch¨olkopf et al., 1997;
Simard et al., 1998). That is, each example (X (i), Y (i)) is replaced by many pairs
( (cid:101)X (i,b), Y (i)) for b = 1, . . . , B, where each (cid:101)X (i,b) is a transformed version of X (i).
This strategy is simple and modular: after generating the pseudo-examples, we can
simply apply any supervised learning algorithm to the augmented dataset. Fig-
ure 1.1 illustrates two examples of this approach, an image transformed to a noisy
image and a text caption, transformed by deleting words.

Dropout training (Srivastava et al., 2014) is an instance of data augmentation
that, when applied to an input feature vector, zeros out a subset of the features ran-
domly. Intuitively, dropout implies a certain amount of signal redundancy across
features—that an input with about half the features masked should usually be
classiﬁed the same way as a fully-observed input. In the setting of document clas-

1.1 Introduction

3

siﬁcation, dropout can be seen as creating pseudo-examples by randomly omitting
some information (i.e., words) from each training example. Building on this in-
terpretation, Wager et al. (2014) show that learning with such artiﬁcially diﬃcult
examples can substantially improve the generalization performance of a classiﬁer.
To study dropout, Wager et al. (2014) assume that documents can be summarized
as Poisson word counts. Speciﬁcally, assume that each document has an underlying
topic associated with a word frequency distribution π on the p-dimensional simplex
and an expected length T ≥ 0, and that, given π and T , the word counts Xj
(cid:12)
are independently generated as Xj
(cid:12) T, π ∼ Pois(T πj). The analysis of Wager
et al. (2014) then builds on a duality between dropout and the above generative
model. Consider the example given in Figure 1.1, where dropout creates pseudo-
documents (cid:101)X by deleting half the words at random from the original document
X. As explained in detail in Section 1.2.1, if X itself is drawn from the above
Poisson model, then the dropout pseudo-examples (cid:101)X are marginally distributed
(cid:12)
as (cid:101)Xj
(cid:12) T, π ∼ Pois(0.5 T πj). Thus, in the context of this Poisson generative
model, dropout enables us to create new, shorter pseudo-examples that preserve
the generative structure of the problem.

The above interpretation of dropout raises the following question: if feature
deletion is a natural way to create information-poor pseudo-examples for document
classiﬁcation, are there natural analogous feature noising schemes that can be
applied to other problems? In this chapter, we seek to address this question, and
study a more general family of data augmentation methods generalizing dropout,
based on L´evy processes: We propose an abstract L´evy thinning scheme that reduces
to dropout in the Poisson generative model considered by Wager et al. (2014). Our
framework further suggests new methods for feature noising such as Gamma noising
based on alternative generative models, all while allowing for a uniﬁed theoretical
analysis.

In the above discussion,
From generative modeling to data augmentation.
we treated the expected document length T as ﬁxed. More generally, we could
imagine the document as growing in length over time, with the observed document
X merely a “snapshot” of what the document looks like at time T . Formally, we
(cid:12)
(cid:12) π ∼
can imagine a latent Poisson process (At)t≥0, with ﬁxed-t marginals (At)j
Pois(t πj), and set X = AT . In this notation, dropout amounts to “rewinding”
the process At to obtain short pseudo-examples. By setting (cid:101)X = AαT , we have
P[ (cid:101)X = ˜x (cid:12)

(cid:12) AT = x], for thinning parameter α ∈ (0, 1).

(cid:12) X = x] = P[AαT = ˜x (cid:12)

The main result of this chapter is that the analytic tools developed by Wager
et al. (2014) are not restricted to the case where (At) is a Poisson process, and in
fact hold whenever (At) is a L´evy process. In other words, their analysis applies
to any classiﬁcation problem where the features X can be understood as time-T
snapshots of a process (At), i.e., X = AT .

Recall that a L´evy process (At)t≥0 is a stochastic process with A0 = 0 that
has independent and stationary increments: {Ati − Ati−1} are independent for
d= At−s for and s < t. Common examples
0 = t0 < t1 < t2 < · · · , and At − As

4

Data Augmentation via L´evy Processes

Figure 1.2: Graphical model depicting our generative assumptions; note that we
are not ﬁtting this generative model. Given class Y , we draw a topic θ, which
governs the parameters of the L´evy process (At). We slice at time T to get the
original input X = AT and at an earlier time ˜T to get the thinned or noised input
˜X = A ˜T . We show that given X, we can sample ˜X without knowledge of θ.

of L´evy processes include Brownian motion and Poisson processes.

In any such L´evy setup, we show that it is possible to devise an analogue to
dropout that creates pseudo-examples by rewinding the process back to some earlier
time (cid:101)T ≤ T . Our generative model is depicted in Figure 1.2: (At), the information
relevant to classifying Y , is governed by a latent topic θ ∈ Rp. L´evy thinning then
seeks to rewind (At)—importantly as we shall see, without having access to θ.

We should think of (At) as representing an ever-accumulating amount of infor-
mation concerning the topic θ: In the case of document classiﬁcation, (At) are the
word counts associated with a document that grows longer as t increases. In other
examples that we discuss in Section 1.3, At will represent the sum of t independent
noisy sensor readings. The independence of increments property assures that as we
progress in time, we are always obtaining new information. The stopping time T
thus represents the information content in input X about topic θ. L´evy thinning
seeks to improve classiﬁcation accuracy by turning a few information-rich examples
X into many information-poor examples (cid:101)X.

We emphasize that, although our approach uses generative modeling to motivate
a data augmentation scheme, we do not in fact ﬁt a generative model. This
presents a contrast to the prevailing practice: two classical approaches to multiclass
classiﬁcation are to either directly train a discriminative model by running, e.g.,
multiclass logistic regression on the n original training examples; or, at the other
extreme, to specify and ﬁt a simple parametric version of the above generative
model, e.g., naive Bayes, and then use Bayes’ rule for classiﬁcation. It is well
known that the latter approach is usually more eﬃcient if it has access to a
correctly speciﬁed generative model, but may be badly biased in case of model
misspeciﬁcation (Efron, 1975; Ng and Jordan, 2002; Liang and Jordan, 2008). Here,
we ﬁrst seek to devise a noising scheme X → (cid:101)X and then to train a discriminative
model on the pseudo-examples ( (cid:101)X, Y ) instead of the original examples (X, Y ). Note

1.1 Introduction

5

Figure 1.3: We model each input X as a slice of a L´evy process at time T .
We generate noised examples (cid:101)X by “stepping back in time” to ˜T . Note that the
examples of the two classes are closer together now, thus forcing the classiﬁer to
work harder.

that even if the generative model is incorrect, this approach will incur limited bias as
long as the noising scheme roughly preserves class boundaries — for example, even
if the Poisson document model is incorrect, we may still be justiﬁed in classifying a
subsampled travel document as a travel document. As a result, this approach can
take advantage of an abstract generative structure while remaining more robust to
model misspeciﬁcation than parametric generative modeling.

Overview of results. We consider the multiclass classiﬁcation setting where we
seek to estimate a mapping from input X to class label Y . We imagine that each
X is generated by a mixture of L´evy process, where we ﬁrst draw a random topic θ
given the class Y , and then run a L´evy process (At) depending on θ to time T . In
order to train a classiﬁer, we pick a thinning parameter α ∈ (0, 1), and then create
(cid:12)
pseudo examples by rewinding the original X back to time α T , i.e., (cid:101)X ∼ AαT
(cid:12) AT .
We show three main results in this chapter. Our ﬁrst result is that we can generate
such pseudo-examples (cid:101)X without knowledge of the parameters θ governing the
generative L´evy process. In other words, while our method posits the existence of
a generative model, our algorithm does not actually need to estimate it. Instead,
it enables us to give hints about a potentially complex generative structure to a
discriminative model such as logistic regression.

Second, under assumptions that our generative model is correct, we show that

6

Data Augmentation via L´evy Processes

feature noising preserves the Bayes decision boundary: P[Y | X = x] = P[Y | (cid:101)X =
x]. This means that feature noising does not introduce any bias in the inﬁnite data
limit.

Third, we consider the limit of rewinding to the beginning of time (α → 0). Here,
we establish conditions given which, even with ﬁnite data, the decision boundary
obtained by ﬁtting a linear classiﬁer on the pseudo-examples is equivalent to that
induced by a simpliﬁed generative model. When this latter result holds, we can
interpret α-thinning as providing a semi-generative regularization path for logistic
regression, with a simple generative procedure at one end and unregularized logistic
regression at the other.

Related work. The trade-oﬀ between generative models and discriminative
models has been explored extensively. Rubinstein and Hastie (1997) empirically
compare discriminative and generative classiﬁers models with respect to bias and
variance, Efron (1975) and Ng and Jordan (2002) provide a more formal discussion
of the bias-variance trade-oﬀ between logistic regression and naive Bayes. Liang
and Jordan (2008) perform an asymptotic analysis for general exponential families.
A number of papers study hybrid loss functions that combine both a joint and
conditional likelihood (Raina et al., 2004; Bouchard and Triggs, 2004; Lasserre et al.,
2006; McCallum et al., 2006; Liang and Jordan, 2008). The data augmentation
approach we advocate in this chapter is fundamentally diﬀerent, in that we are
merely using the structural assumptions implied by the generative models to
generate more data, and are not explicitly ﬁtting a full generative model.

The present work was initially motivated by understanding dropout training
(Srivastava et al., 2014), which was introduced in the context of regularizing
deep neural networks, and has had much empirical success (Ba and Frey, 2013;
Goodfellow et al., 2013; Krizhevsky et al., 2012; Wan et al., 2013). Many of the
regularization beneﬁts of dropout can be found in logistic regression and other
single-layer models, where it is also known as “blankout noise” (Globerson and
Roweis, 2006; van der Maaten et al., 2013) and has been successful in natural
language tasks such as document classiﬁcation and named entity recognition (Wager
et al., 2013; Wang and Manning, 2013; Wang et al., 2013). There are a number of
theoretical analyses of dropout: using PAC-Bayes framework (McAllester, 2013),
comparing dropout to “altitude training” (Wager et al., 2014), and interpreting
dropout as a form of adaptive regularization (Baldi and Sadowski, 2014; Bishop,
1995; Helmbold and Long, 2015; Josse and Wager, 2014; Wager et al., 2013).

1.2 L´evy Thinning

We begin by brieﬂy reviewing the results of Wager et al. (2014), who study dropout
training for document classiﬁcation from the perspective of thinning documents
(Section 1.2.1). Then, in Section 1.2.2, we generalize these results to the setting of

1.2 L´evy Thinning

7

generic L´evy generative models.

1.2.1 Motivating Example: Thinning Poisson Documents

Suppose we want to classify documents according to their subject, e.g., sports,
politics, or travel. As discussed in the introduction, common sense intuition about
the nature of documents suggests that a short snippet of a sports document
should also be classiﬁed as a sports document. If so, we can generate many new
training examples by cutting up the original documents in our dataset into shorter
subdocuments and labeling each subdocument with the same label as the original
document it came from. By training a classiﬁer on all of the pseudo-examples we
generate in this way, we should be able to obtain a better classiﬁer.

In order to formalize this intuition, we can represent a document as a sequence
of words from a dictionary {1, . . . , d}, with the word count Xj denoting the
number of occurrences of word j in the document. Given this representation, we
can easily create “subdocuments” by binomially downsampling the word counts Xj
independently. That is, for some ﬁxed downsampling fraction α ∈ (0, 1), we draw

(cid:101)Xj | Xj ∼ Binom(Xj, α).

In other words, we keep each occurrence of word j independently with probability
α.

Wager et al. (2014) study this downsampling scheme in the context of a Poisson
mixture model for the inputs X that obeys the structure of Figure 1.2: ﬁrst, we
draw a class Y ∈ {1, . . . , K} (e.g., travel) and a “topic” θ ∈ Rd (e.g., corresponding
to travel in Norway). The topic θ speciﬁes a distribution over words,

(1.1)

(1.2)

µj(θ) = eθj ,

where, without loss of generality, we assume that (cid:80)d
j=1 eθj = 1. We then draw a
Pois(T ) number of words, where T is the expected document length, and generate
each word independently according to θ. Equivalently, each word count is an
independent Poisson random variable, Xj ∼ Pois(T µj(θ)). The following is an
example draw of a document:

Y = travel

norway
(cid:122)(cid:125)(cid:124)(cid:123)
0.5 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
0.5 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
1.2 ,

skyscraper
(cid:122) (cid:125)(cid:124) (cid:123)
−2.7 , . . . ]

θ = [

norway
(cid:122)(cid:125)(cid:124)(cid:123)

2 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
1 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
3 ,

skyscraper
(cid:122)(cid:125)(cid:124)(cid:123)
0

, . . . ]

X = [

norway
(cid:122)(cid:125)(cid:124)(cid:123)

1 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
0 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
1 ,

skyscraper
(cid:122)(cid:125)(cid:124)(cid:123)
0

, . . . ]

(cid:101)X = [

Let us now try to understand the downsampling scheme (cid:101)X | X in the context of the

8

Data Augmentation via L´evy Processes

Poisson topic model over X. For each word j, recall that (cid:101)Xj | Xj ∼ Binom(Xj, α).
If we marginalize over X, then we have:

(cid:101)Xj | T, θ ∼ Pois (αT µj(θ)) .

(1.3)

(1.4)

As a result, the distribution of (cid:101)X is exactly the distribution of X if we replaced T
with (cid:101)T = αT .

We can understand this thinning by embedding the document X in a multivariate
Poisson process (At)t≥0, where the marginal distribution of At ∈ {0, 1, 2, . . . }d is
deﬁned to be the distribution over counts when the expected document length is t.
Then, we can write

X = AT ,

(cid:101)X = A

(cid:101)T .

Thus, under the Poisson topic model, the binomial thinning procedure does not
alter the structure of the problem other than by shifting the expected document
length from T to (cid:101)T . Figure 1.4 illustrates one realization of L´evy thinning in the
Poisson case with a three-word dictionary. Note that in this case we can sample
(cid:101)X = AαT given X = AT without knowledge of θ.

This perspective lies at the heart of the analysis in Wager et al. (2014), who
show under the Poisson model that, when the overall document length (cid:107)X(cid:107)1
is independent of the topic θ, thinning does not perturb the optimal decision
boundary. Indeed, the conditional distribution over class labels is identical for the
original features and the thinned features:

P[Y | X = x] = P[Y | (cid:101)X = x].

(1.5)

This chapter extends the result to general L´evy processes (see Theorem 1.2).

This last result (1.5) may appear quite counterintuitive: for example, if A60 is
more informative than A40, how can it be that downsampling does not perturb
the conditional class probabilities? Suppose x is a 40-word document ((cid:107)x(cid:107)1 = 40).
When t = 60, most of the documents will be longer than 40 words, and thus x will be
less likely under t = 60 than under t = 40. However, (1.5) is about the distribution
of Y conditioned on a particular realization x. The claim is that, having observed
x, we obtain the same information about Y regardless of whether t, the expected
document length, is 40 or 60.

1.2.2 Thinning L´evy Processes

The goal of this section is to extend the Poisson topic model from Section 1.2.1
and construct general thinning schemes with the invariance property of (1.5). We
will see that L´evy processes provide a natural vehicle for such a generalization: The
Poisson process used to generate documents is a speciﬁc L´evy process, and binomial
sampling corresponds to “rewinding” the L´evy process back in time.

Consider the multiclass classiﬁcation problem of predicting a discrete class Y ∈
{1, . . . , K} given an input vector X ∈ Rd. Let us assume that the joint distribution

1.2 L´evy Thinning

9

over (X, Y ) is governed by the following generative model:

1. Choose Y ∼ Mult(π), where π is on the K-dimensional simplex.

2. Draw a topic θ | Y , representing a subpopulation of class Y .
3. Construct a L´evy process (At)t≥0 | θ, where At ∈ Rd is a potential input vector
at time t.

4. Observe the input vector X = AT at a ﬁxed time T .

While the L´evy process imposes a fair amount of structure, we make no assump-
tions about the number of topics, which could be uncountably inﬁnite, or about
their distribution, which could be arbitrary. Of course, in such an unconstrained
non-parametric setting, it would be extremely diﬃcult to adequately ﬁt the genera-
tive model. Therefore, we take a diﬀerent tack: We will use the structure endowed by
the L´evy process to generate pseudo-examples for consumption by a discriminative
classiﬁer. These pseudo-examples implicitly encode our generative assumptions.

The natural way to generate a pseudo-example ( (cid:101)X, Y ) is to “rewind” the L´evy
process (At) backwards from time T (recall X = AT ) to an earlier time (cid:101)T = αT
(cid:101)T . In practice, (At) is
for some α ∈ (0, 1) and deﬁne the thinned input as (cid:101)X = A
unobserved, so we draw (cid:101)X conditioned on the original input X = AT and topic θ.
In fact, we can draw many realizations of (cid:101)X | X, θ.

Our hope is that a single full example (X, Y ) is rich enough to generate many
diﬀerent pseudo-examples ( (cid:101)X, Y ), thus increasing the eﬀective sample size. More-
over, Wager et al. (2014) show that training with such pseudo-examples can also
lead to a somewhat surprising “altitude training” phenomenon whereby thinning
yields an improvement in generalization performance because the pseudo-examples
are more diﬃcult to classify than the original examples, and thus force the learning
algorithm to work harder and learn a more robust model.

A technical diﬃculty is that generating (cid:101)X | X, θ seemingly requires knowledge of
the topic θ driving the underlying L´evy process (At). In order to get around this
issue, we establish the following condition under which the observed input X = AT
alone is suﬃcient—that is, P[ (cid:101)X | X, θ] does not actually depend on θ.
Assumption 1.1 (exponential family structure). The L´evy process (At) (cid:12)
(cid:12) θ is
drawn according to an exponential family model whose marginal density at time
t is

f (t)
θ (x) = exp [θ · x − tψ (θ)] h(t) (x) for every t ∈ R.

(1.6)

Here, the topic θ ∈ Rd is an unknown parameter vector, and h(t)(x) is a family of
carrier densities indexed by t ∈ R.

The above assumption is a natural extension of a standard exponential family
assumption that holds for a single value of t. Speciﬁcally, suppose that h(t)(x),
t > 0, denotes the t-marginal densities of a L´evy process, and that f (1)
(x) =
exp [θ · x − ψ (θ)] h(1)(x) is an exponential family through h(1)(x) indexed by θ ∈
Rd. Then, we can verify that the densities speciﬁed in (1.6) induce a family of

θ

10

Data Augmentation via L´evy Processes

Figure 1.4: Illustration of our Poisson process document model with a three-word
dictionary and µ(θ) = (0.25, 0.3, 0.45). The word counts of the original document,
X = (8, 7, 16), represents the trivariate Poisson process At, sliced at T = 28. The
thinned pseudo-document (cid:101)X = (2, 4, 9) represents At sliced at (cid:101)T = 14.

L´evy processes indexed by θ. The key observation in establishing this result is that,
because h(t)(x) is the t-marginal of a L´evy process, the L´evy–Khintchine formula
implies that

(cid:90)

eθ·xh(t)(x) dx =

eθ·xh(1)(x) dx

= et ψ(θ),

(cid:18)(cid:90)

(cid:19)t

and so the densities in (1.6) are properly normalized.

We also note that, given this assumption and as T → ∞, we have that AT /T
converges almost surely to µ(θ) def= E [A1]. Thus, the topic θ can be understood as
a description of an inﬁnitely informative input. For ﬁnite values of T , X represents
a noisy observation of the topic θ.

Now, given this structure, we show that the distribution of (cid:101)X = AαT conditional
on X = AT does not depend on θ. Thus, feature thinning is possible without
knowledge of θ using the L´evy thinning procedure deﬁned below. We note that,
in our setting, the carrier distributions h(t)(x) are always known; in Section 1.3,
we discuss how to eﬃciently sample from the induced distribution g(αT ) for some
speciﬁc cases of interest.

Theorem 1.1 (L´evy thinning). Assume that (At) satisﬁes the exponential family
structure in (1.6), and let α ∈ (0, 1) be the thinning parameter. Then, given an
input X = AT and conditioned on any θ, the thinned input (cid:101)X = AαT has the

1.2 L´evy Thinning

11

(1.7)

(1.8)

following density:

g(αT )(˜x; X) =

h(αT )(˜x) h((1−α)T )(X − ˜x)
h(T )(X)

,

which importantly does not depend on θ.

Proof. Because the L´evy process (At) has independent and stationary increments,
we have that AαT ∼ f (αT )
and AT − AαT ∼ f ((1−α)T )
are independent. Therefore,
we can write the conditional density of AαT given AT as the joint density over
(AαT , AT ) (equivalently, the reparametrization (AαT , AT − AαT )) divided by the
marginal density over AT :

θ

θ

g(αT )(˜x; X) =

f (αT )
θ

(X − ˜x)

(˜x)f ((1−α)T )
θ
f (T )
(X)
θ
exp [θ · ˜x − αT ψ(θ)] h(αT )(˜x)

(cid:16)

(cid:17)

=

×

×

(cid:16)

(cid:16)

exp [θ · (X − ˜x) − (1 − α)T ψ(θ)] h((1−α)T )(X − ˜x)

(cid:17)

exp [θ · X − T ψ(θ)] h(T )(X)

(cid:17)−1

,

where the last step expands everything (1.6). Algebraic cancellation, which removes
all dependence on θ, completes the proof.

Note that while Theorem 1.1 guarantees we can carry out feature thinning
without knowing the topic θ, it does not guarantee that we can do it without
knowing the information content T . For Poisson processes, the binomial thinning
mechanism depends only on α and not on the original T . This is a convenient
property in the Poisson case but does not carry over to all L´evy processes — for
example, if Bt is a standard Brownian motion, then the distribution of B2 given
B4 = 0 is N(0, 1), while the distribution of B200 given B400 = 0 is N(0, 100).
As we will see in Section 1.3, thinning in the Gaussian and Gamma families
does require knowing T , which will correspond to a “sample size” or “precision.”
Likewise, Theorem 1.1 does not guarantee that sampling from (1.7) can be carried
out eﬃciently; however, in all the examples we present here, sampling can be carried
out easily in closed form.

1.2.3 Learning with Thinned Features

Having shown how to thin the input X to (cid:101)X without knowledge of θ, we can
proceed to deﬁning our full data augmentation strategy. We are given n training
examples {(X (i), Y (i))}n
i=1. For each original input X (i), we generate B thinned
versions (cid:101)X (i,1), . . . , (cid:101)X (i,B) by sampling from (1.7). We then pair these B examples
up with Y (i) and train any discriminative classiﬁer on these Bn examples. Algo-
rithm 1 describes the full procedure where we specialize to logistic regression. If
one is implementing this procedure using stochastic gradient descent, one can also

12

Data Augmentation via L´evy Processes

Procedure 1. Logistic Regression with L´evy Regularization
Input: n training examples (X (i), Y (i)), a thinning parameter α ∈ (0, 1), and a feature
map φ : Rd (cid:55)→ Rp.

1. For each training example X (i), generate B thinned versions ( (cid:101)X (i,b))B
to (1.7).

b=1 according

2. Train logistic regression on the resulting pseudo-examples:

(cid:40) n
(cid:88)

B
(cid:88)

(cid:41)
β; (cid:101)X (i,b), Y (i)(cid:17)
(cid:16)

,

(cid:96)

ˆβ def= argmin
β∈Rp×K

i=1

b=1

where the multi-class logistic loss with feature map φ is

(cid:96)(β; x, y) def= log

eβ(k)·φ(x)

− β(y) · φ(x).

(cid:33)

(cid:32) K
(cid:88)

k=1

3. Classify new examples according to

ˆy(x) = argmin

k∈{1, ..., K}

(cid:110)

ˆc(k) − ˆβ(k) · φ(x)

(cid:111)

,

(1.9)

(1.10)

(1.11)

where the ˆck ∈ R are optional class-speciﬁc calibration parameters for k = 1, . . . , K.

generate a fresh thinned input (cid:101)X whenever we sample an input X on the ﬂy, which
is the usual implementation of dropout training (Srivastava et al., 2014).

In the ﬁnal step (1.11) of Algorithm 1, we also allow for class-speciﬁc calibration
parameters . After the ˆβ(k) have been determined by logistic regression with L´evy
regularization, these parameters ˆc(k) can be chosen by optimizing the logistic loss on
the original uncorrupted training data. As discussed in Section 1.2.5, re-calibrating
the model is recommended, especially when α is small.

1.2.4 Thinning Preserves the Bayes Decision Boundary

We can easily implement the thinning procedure, but how will it aﬀect the accuracy
of the classiﬁer? The following result gives us a ﬁrst promising piece of the answer
by establishing conditions under which thinning does not aﬀect the Bayes decision
boundary.

At a high level, our results rely on the fact that under our generative model, the
“amount of information” contained in the input vector X is itself uninformative
about the class label Y .

Assumption 1.2 (Equal information content across topics). Assume there exists
a constant ψ0 such that ψ(θ) = ψ0 with probability 1, over random θ.

For example, in our Poisson topic model, we imposed the restriction that ψ(θ) =

1.2 L´evy Thinning

13

(1.12)

(1.13)

(1.14)

(1.15)

(cid:80)d

j=1 eθj = 1, which ensures that the document length (cid:107)At(cid:107)1 has the same

distribution (which has expectation ψ(θ) in this case) for all possible θ.

Theorem 1.2. Under Assumption 1.2, the posterior class probabilities are invari-
ant under thinning (1.7):

(cid:104)
Y = y (cid:12)

P

(cid:12) (cid:101)X = x

(cid:105)

= P (cid:2)Y = y (cid:12)

(cid:12) X = x(cid:3)

for all y ∈ {1, . . . , K} and x ∈ X.

Proof. Given Assumption 1.2, the density of At | θ is given by:

f (t)
θ (x) = eθ·xe−tψ0h(t)(x),

which importantly splits into two factors, one depending on (θ, x), and the other
depending on (t, x). Now, let us compute the posterior distribution:

P (cid:2)Y = y (cid:12)

(cid:12) At = x(cid:3) ∝ P [Y = y]

P [θ | Y ] f (t)

θ (x)dθ

∝ P [Y = y]

P [θ | Y ] eθ·xdθ,

(cid:90)

(cid:90)

which does not depend on t, as e−tψ0 h(t)(x) can be folded into the normalization
constant. Recall that X = AT and (cid:101)X = A
(cid:101)T . Substitute t = T and t = (cid:101)T to conclude
(1.12).

To see the importance of Assumption 1.2, consider the case where we have two
labels (Y ∈ {1, 2}), each with a single topic (Y yields topic θY ). Suppose that
ψ(θ2) = 2ψ(θ1)—that is, documents in class 2 are on average twice as long as those
in class 1. Then, we would be able to make class 2 documents look like class 1
documents by thinning them with α = 0.5.

Remark 1.1. If we also condition on the information content T , then an analogue
to Theorem 1.2 holds even without Assumption 1.2:

(cid:104)
Y = y (cid:12)

P

(cid:12) (cid:101)X = x, (cid:101)T = t

= P (cid:2)Y = y (cid:12)

(cid:12) X = x, T = t(cid:3) .

(cid:105)

(1.16)

This is because, after conditioning on T , the e−tψ(θ) term factors out of the
likelihood.

The upshot of Theorem 1.2 is that thinning will not induce asymptotic bias
whenever an estimator produces P (cid:2)Y = y (cid:12)
(cid:12) X = x(cid:3) in the limit of inﬁnite data
(n → ∞), i.e., if the logistic regression (Algorithm 1) is well-speciﬁed. Speciﬁcally,
training either on original examples or thinned examples will both converge to the
true class-conditional distribution. The following result assumes that the feature
space X is discrete; the proof can easily be generalized to the case of continuous
features.

Corollary 1.3. Suppose that Assumption 1.2 holds, and that the above multi-class

14

Data Augmentation via L´evy Processes

logistic regression model is well-speciﬁed, i.e., P (cid:2)Y = y (cid:12)
(cid:12) X = x(cid:3) ∝ eβ(y)·φ(x) for
some β and all y = 1, ..., K. Then, assuming that P [At = x] > 0 for all x ∈ X and
t > 0, Algorithm 1 is consistent, i.e., the learned classiﬁcation rule converges to the
Bayes classiﬁer as n → ∞.
Proof. At a ﬁxed x, the population loss E (cid:2)(cid:96) (cid:0)β; X, Y (cid:12)
any choice of β satisfying:
exp (cid:2)β(y) · φ(x)(cid:3)
k=1 exp (cid:2)β(k) · φ(x)(cid:3) = P (cid:2)Y = y (cid:12)

(cid:12) X = x(cid:1)(cid:3) is minimized by

(cid:12) X = x(cid:3)

(1.17)

(cid:80)K

for all y = 1, ..., K. Since the model is well-speciﬁed and by assumption P[ (cid:101)X =
x] > 0 for all x ∈ X, we conclude that weight vector ˆβ learned using Algorithm 1
must satisfy asymptotically (1.17) for all x ∈ X as n → ∞.

1.2.5 The End of the Path

As seen above, if we have a correctly speciﬁed logistic regression model, then
L´evy thinning regularizes it without introducing any bias. However, if the logistic
regression model is misspeciﬁed, thinning will in general induce bias, and the
amount of thinning presents a bias-variance trade-oﬀ. The reason for this bias is that
although thinning preserves the Bayes decision boundary, it changes the marginal
distribution of the covariates X, which in turn aﬀects logistic regression’s linear
approximation to the decision boundary. Figure 1.5 illustrates this phenomenon in
the case where At is a Brownian motion, corresponding to Gaussian feature noising;
Wager et al. (2014) provides a similar example for the Poisson topic model.

Fully characterizing the bias of L´evy thinning is beyond the scope of this paper.
However, we can gain some helpful insights about this bias by studying “strong
thinning”—i.e., L´evy thinning in the limit as the thinning parameter α → 0:

ˆβ0+

def= lim
α→0

lim
B→∞

ˆβ(α, B),

(1.18)

where ˆβ(α, B) is deﬁned as in (1.9) with the explicit dependence on α and B.
For each α, we take B → ∞ perturbed points for each of the original n data
points. As we show in this section, this limiting classiﬁer is well-deﬁned under weak
conditions; moreover, in some cases of interest, it can be interpreted as a simple
generative classiﬁer. The result below concerns the existence of ˆβ0+, and establishes
that it is the empirical minimizer of a convex loss function.

Theorem 1.4. Assume the setting of Procedure 1, and let the feature map be
φ(x) = x. Assume that the generative L´evy process (At) has ﬁnitely many jumps in
expectation over the interval [0, T ]. Then, the limit ˆβ0+ is well-deﬁned and can be
written as

ˆβ0+ = argmin
β∈Rp×K

(cid:40) n
(cid:88)

i=1

(cid:16)

β; X (i), Y (i)(cid:17)

ρ

(cid:41)

,

(1.19)

1.2 L´evy Thinning

15

(cid:12) θ, T ∼ N (cid:0)T θ, σ2T Ip×p

Figure 1.5: The eﬀect of L´evy thinning with data generated from a Gaussian
model of the form X (cid:12)
(cid:1), as described in Section 1.3.1.
The outer circle depicts the distribution of θ conditional on the color Y : blue
points all have θ ∝ (cos(0.75 π/2), sin(0.75 π/2)), whereas the red points have
θ ∝ (cos(ω π/2), sin(ω π/2)) where ω is uniform between 0 and 2/3. Inside this
circle, we see 3 clusters of points generated with T = 0.1, 0.4, and 1, along with
logistic regression decision boundaries obtained from each cluster. The dashed line
shows the Bayes decision boundary separating the blue and red points, which is the
same for all T (Theorem 1.2). Note that the logistic regression boundaries learned
from data with diﬀerent T are not the same. This issue arises because the Bayes
decision boundary is curved, and the best linear approximation to a curved Bayes
boundary changes with T .

for some convex function ρ(·; x, y).

The proof of Theorem 1.4 is provided in the appendix. Here, we begin by
establishing notation that lets us write down an expression for the limiting loss
ρ. First, note that Assumption 1.1 implicitly requires that the process (At) has
ﬁnite moments. Thus, by the L´evy–It¯o decomposition, we can uniquely write this
process as

At = bt + Wt + Nt,

(1.20)

where b ∈ Rp, Wt is a Wiener process with covariance Σ, and Nt is a compound
Poisson process which, by hypothesis, has a ﬁnite jump intensity.

Now, by an argument analogous to that in the proof of Theorem 1.1, we see that

16

Data Augmentation via L´evy Processes

the joint distribution of WT and NT conditional on AT does not depend on θ. Thus,
we can deﬁne the following quantities without ambiguity:

(cid:12)
µT (x) = bT + E (cid:2)WT
(cid:12) AT = x(cid:3) ,
λT (x) = E (cid:2)number of jumps in (At) for t ∈ [0, T ] (cid:12)
νT (z; x) = lim
t→0

(cid:12) Nt (cid:54)= 0, AT = x(cid:3) .

P (cid:2)Nt = z (cid:12)

(cid:12) AT = x(cid:3) ,

(1.21)

(1.22)

(1.23)

More prosaically, νT (·; x) can be described as the distribution of the ﬁrst jump
of (cid:101)Nt, a thinned version of the jump process Nt. In the degenerate case where
P (cid:2)NT = 0 (cid:12)
(cid:12) AT = x(cid:3) = 0, we set νT (·; x) to be a point mass at z = 0.

Given this notation, we can write the eﬀective loss function ρ for strong thinning

as

ρ (β; x, y) = −µT (x) · β(y) +

β(k)(cid:62)Σβ(k)

(1.24)

T
2

1
K

K
(cid:88)

k=1

(cid:90)

+ λT (x)

(cid:96) (β; z, y) dνT (z; x),

provided we require without loss of generality that (cid:80)K
k=1 β(k) = 0. In other words,
the limiting loss can be described entirely in terms of the distribution of the ﬁrst
jump of (cid:101)Nt, and continuous part Wt of the L´evy process. The reason for this
phenomenon is that, in the strong thinning limit, the pseudo-examples (cid:101)X ∼ AαT
can all be characterized using either 0 or 1 jumps.

Aggregating over all the training examples, we can equivalently write this strong

thinning loss as

(cid:16)

β; X (i), Y (i)(cid:17)

ρ

=

n
(cid:88)

i=1

1
2T

n
(cid:88)

i=1

γ−1
Y (i)

(cid:13)
(cid:13)
(cid:13)γY (i) µT

X (i)(cid:17)
(cid:16)

− T Σβ(Y (i))(cid:13)
2
(cid:13)
(cid:13)

Σ−1

λT (X (i))

(cid:90)

(cid:16)

β; z, Y (i)(cid:17)

(cid:96)

dνT (z; X (i)),

(1.25)

+

n
(cid:88)

i=1

2 terms that do not depend on β. Here, 1

up to (cid:107)µT (cid:107)2
2 v(cid:48)Σ−1v corresponds
to the Gaussian log-likelihood with covariance Σ (up to constants), and γy =
K (cid:12)
(cid:12) /n measures the over-representation of class y relative to other
(cid:12)
classes.

(cid:8)i : Y (i) = y(cid:9)(cid:12)

Σ−1 = 1

2 (cid:107)v(cid:107)2

(cid:13)
(cid:13)

(cid:80)n

(cid:13)µT (X (i)) − T Σβ(Y (i))(cid:13)

In the case where we have the same number of training examples from
each class (and so γy = 1 for all y = 1, ..., K), the strong thinning loss
can be understood in terms of a generative model. The ﬁrst term, namely
1
, is the loss function for linear classiﬁcation
2T
in a Gaussian mixture with observations µT (X (i)), while the second term is the
logistic loss obtained by classifying single jumps. Thus, strong thinning is eﬀec-
tively seeking the best linear classiﬁer for a generative model that is a mixture of
Gaussians and single jumps.

2
(cid:13)
(cid:13)

Σ−1

i=1

In the pure jump case (Σ = 0), we also note that strong thinning is closely related

1.3 Examples

17

1.3 Examples

to naive Bayes classiﬁcation. In fact, if the jump measure of Nt has a ﬁnite number
of atoms that are all linearly independent, then we can verify that the parameters
ˆβ0+ learned by strong thinning are equivalent to those learned via naive Bayes,
although the calibration constants c(k) may be diﬀerent.

At a high level, by elucidating the generative model that strong thinning pushes
us towards, these results can help us better understand the behavior of L´evy
thinning for intermediate value of α, e.g., α = 1/2. They also suggest caution with
respect to calibration: For both the diﬀusion and jump terms, we saw above that
L´evy thinning gives helpful guidance for the angle of β(k), but does not in general
(cid:13)β(k)(cid:13)
elegantly account for signal strength (cid:13)
(cid:13)2 or relative class weights. Thus, we
recommend re-calibrating the class decision boundaries obtained by L´evy thinning,
as in Algorithm 1.

So far, we have developed our theory of L´evy thinning using the Poisson topic
model as a motivating example, which corresponds to dropping out words
from a document. In this section, we present two models based on other L´evy
processes—multivariate Brownian motion (Section 1.3.1) and Gamma processes
(Section 1.3.2)— exploring the consequences of L´evy thinning.

1.3.1 Multivariate Brownian Motion

Consider a classiﬁcation problem where the input vector is the aggregation of
multiple noisy, independent measurements of some underlying object. For example,
in a biomedical application, we might want to predict a patient’s disease status
based on a set of biomarkers such as gene expression levels or brain activity.
A measurement is typically obtained through a noisy experiment involving an
microarray or fMRI, so multiple experiments might be performed and aggregated.
More formally, suppose that patient i has disease status Y (i) and expression level
µi ∈ Rd for d genes, with the distribution of µi diﬀerent for each disease status.
Given µi, suppose the t-th measurement for patient i is distributed as

Zi,t ∼ N(µi, Σ),

(1.26)

where Σ ∈ Rd×d is assumed to be a known, ﬁxed matrix. Let the observed input be
X (i) = (cid:80)Ti
t=1 Zi,t, the sum of the noisy measurements. If we could take inﬁnitely
many measurements (Ti → ∞), we would have X (i)/Ti → µi almost surely; that
is, we would observe gene expression noiselessly. For ﬁnitely many measurements,
X (i) is a noisy proxy for the unobserved µi.

We can model the process of accumulating measurements with a multivariate

18

Data Augmentation via L´evy Processes

Brownian motion (At):

At = tµ + Σ1/2Bt,

where Bt is a d-dimensional white Brownian motion.1 For integer values of t, At
represents the sum of the ﬁrst t measurements, but At is also deﬁned for fractional
values of t. The distribution of the features X at a given time T is thus

(1.27)

(1.28)

(1.29)

(1.30)

(1.31)

X | µ, T ∼ N(T µ, T Σ),

leading to density

f (t)
µ (x) =

exp (cid:2) 1

2 (x − tµ)(cid:62)(tΣ)−1(x − tµ)(cid:3)

(cid:20)

(2π)d/2 det(Σ)
t
2

(cid:21)
µ(cid:62)Σ−1µ

x(cid:62)Σ−1µ −

= exp

h(t)(x),

where

h(t)(x) =

exp (cid:2)− 1
2t x(cid:62)Σ−1x(cid:3)
(2π)d/2 det(Σ)1/2

.

We can recover the form of (1.6) by setting θ = Σ−1µ, a one-to-one mapping
provided Σ is positive-deﬁnite.

Thinning. The distribution of (cid:101)X = AαT given X = AT is that of a Brownian
bridge process with the following marginals:

(cid:101)X | X ∼ N (αX, α(1 − α)T Σ) .

In this case, “thinning” corresponds exactly to adding zero-mean, additive Gaus-
sian noise to the scaled features αX. Note that in this model, unlike in the Poisson
topic model, sampling (cid:101)X from X does require observing T —for example, knowing
how many observations were taken. The larger T is, the more noise we need to
inject to achieve the same downsampling ratio.

In the Poisson topic model, the features (Xi,1, . . . , Xi,d) were independent of
each other given the topic θi and expected length Ti. By contrast, in the Brownian
motion model the features are correlated (unless Σ is the identity matrix). This
serves to illustrate that independence or dependence of the features is irrelevant to
our general framework; what is important is that the increments Zt = At − At−1
are independent of each other, the key property of a L´evy process.

Assumption 1.2 requires that µ(cid:62)Σ−1µ is constant across topics; i.e., that the
true gene expression levels are equally sized in the Mahalanobis norm deﬁned
by Σ. Clearly, this assumption is overly stringent in real situations. Fortunately,
Assumption 1.2 is not required (see Remark 1.1) as long as T is observed—as it

1. By deﬁnition of Brownian motion, we have marginally that Bt ∼ N(0, tI).

1.3 Examples

19

must be anyway if we want to be able to carry out L´evy thinning.

Thinning X in this case is very similar to subsampling. Indeed, for integer values
of (cid:101)T , instead of formally carrying out L´evy thinning as detailed above, we could
simply resample (cid:101)T values of Zi,t without replacement, and add them together to
obtain (cid:101)X. If there are relatively few repeats, however, the resampling scheme can
(cid:1) pseudo-examples (e.g. 6 pseudo-examples if T = 4 and (cid:101)T = 2),
lead to only (cid:0)T
(cid:101)T
whereas the thinning approach leads to inﬁnitely many possible pseudo-examples
we can use to augment the regression. Moreover, if T = 4 then subsampling leaves
us with only four choices of α; there would be no way to thin using α = 0.1, for
instance.

1.3.2 Gamma Process

As another example, suppose again that we are predicting a patient’s disease status
based on repeated measurements of a biomarker such as gene expression or brain
activity. But now, instead of (or in addition to) the average signal, we want our
features to represent the variance or covariance of the signals across the diﬀerent
measurements.

Assume ﬁrst that the signals at diﬀerent genes or brain locations are independent;

that is, the t-th measurement for patient i and gene j has distribution

Zi,j,t ∼ N(µi,j, σ2

i,j).

(1.32)

Here, the variances σ2
subscript i, after T + 1 measurements we can compute

i,1, . . . , σ2

i = (σ2

i,d) parameterize the “topic.” Suppressing the

Xj,T =

(Zi,j,t − ¯Zi,j,T +1)2,

where

¯Zi,j,T +1 =

Zi,j,t. (1.33)

1
T + 1

T +1
(cid:88)

t=1

T +1
(cid:88)

t=1

Then Xj,T ∼ σ2
j χ2
scale parameter 2σ2
more and more observations (increasing T ), we will have XT /T → (σ2
almost surely.

T , which is a Gamma distribution with shape parameter T /2 and
j (there is no dependence on µi). Once again, as we accumulate
1, . . . , σ2
d)

We can embed Xj,T in a multivariate Gamma process with d independent

coordinates and scale parameters σ2
j :

(At)j ∼ Gamma(t/2, 2σ2

j ).

The density of At given σ2 is

f (t)
σ2 (x) =

d
(cid:89)

j=1

j

e−xj /2σ2
xt/2−1
j
Γ(t/2)2t/2σ2(t/2)


j

= exp

−

xj/2σ2

j − (t/2)

log σ2
j


 h(t)(x),

d
(cid:88)

j=1

d
(cid:88)

j=1

(1.34)

(1.35)

20

Data Augmentation via L´evy Processes

where

h(t)(x) =

(cid:81)

j xt/2−1
j
Γ(t/2)d2dt/2

.

We can recover the form of (1.6) by setting θj = −1/2σ2

j , a one-to-one mapping.

Thinning. Because (cid:101)Xj ∼ Gamma(αT /2, 2σ2
Xj − (cid:101)Xj ∼ Gamma((1 − α)T /2, 2σ2

j ), we have

j ) is independent of the increment

(cid:101)Xj
Xj

| Xj ∼ Beta (αT /2, (1 − α)T /2) .

In other words, we create a noisy (cid:101)X by generating for each coordinate an indepen-
dent multiplicative noise factor

mj ∼ Beta (αT, (1 − α)T )

and setting (cid:101)Xj = mjXj. Once again, we can downsample without knowing σ2
j , but
we do need to observe T . Assumption 1.2 would require that (cid:81)
j is identical for
all topics. This is an unrealistic assumption, but once again it is unnecessary as
long as we observe T .

j σ2

General covariance. More generally, the signals at diﬀerent brain locations, or
expressions for diﬀerent genes, will typically be correlated with each other, and these
correlations could be important predictors. To model this, let the measurements be
distributed as:

Zi,t ∼ N(µi, Σi),

where Σ represents the unknown “topic”—some covariance matrix that is charac-
teristic of a certain subcategory of a disease status.

After observing T + 1 observations we can construct the matrix-valued features:

XT =

(Zi,t − ¯Zi,T +1)(Zi,t − ¯Zi,T +1)(cid:62).

T +1
(cid:88)

t=1

Now XT has a Wishart distribution: XT ∼ Wishd(Σ, T ). When T ≥ d, the density
of At given Σ is

f (t)
Σ (x) = exp

−

tr(Σ−1x) −

log det(Σ)

h(t)(x),

(1.41)

(cid:26)

1
2

(cid:27)

t
2

(1.36)

(1.37)

(1.38)

(1.39)

(1.40)

1.4 Simulation Experiments

where

21

(1.42)

(1.43)

h(t)(x) =

td
2 det(x)

t−d−2

2 Γd

(cid:18)
2

(cid:19)(cid:19)−1

,

(cid:18) t
2

Γd

(cid:19)

(cid:18) t
2

d(d−1)
4

= π

d
(cid:89)

j=1

(cid:18) t
2

Γ

+

1 − j
2

(cid:19)

,

supported on positive-deﬁnite symmetric matrices. If X = AT and αT ≥ d as well,
we can sample a “thinned” observation (cid:101)X from density proportional to

h(αT )(˜x)h(T −αT )(X − ˜x) ∝ det(˜x)

2+d−αT
2

det(X − ˜x)

2+d−(1−α)T
2

,

(1.44)

or after the aﬃne change of variables (cid:101)X = X 1/2M X 1/2, we sample M from density
proportional to det(m)
, a matrix beta distribution.
Here, M may be interpreted as matrix-valued multiplicative noise.

det(Id − m)

2+d−(1−α)t
2

2+d−αT
2

1.4 Simulation Experiments

In this section, we perform several simulations to illustrate the utility of L´evy
thinning. In particular, we will highlight the modularity between L´evy thinning
(which provides pseudo-examples) and the discriminative learner (which ingests
these pseudo-examples). We treat the discriminative learner as a black box, com-
plete with its own internal cross-validation scheme that optimizes accuracy on
pseudo-examples. Nonetheless, we show that accuracy on the original examples
improves when we train on thinned examples.

More speciﬁcally, given a set of training examples {(X, Y )}, we ﬁrst use L´evy
thinning to generate a set of pseudo-examples {( (cid:101)X, Y )}. Then we feed these
examples to the R function cv.glmnet to learn a linear classiﬁer on these pseudo-
examples (Friedman et al., 2010). We emphasize that cv.glmnet seeks to choose
its regularization parameter λ to maximize its accuracy on the pseudo-examples
( (cid:101)X, Y ) rather than on the original data (X, Y ). Thus, we are using cross-validation
as a black box instead of trying to adapt the procedure to the context of L´evy
thinning. In principle, we might be concerned that cross-validating on the pseudo-
examples would yield a highly suboptimal choice of λ, but our experiments will
show that the procedure in fact works quite well.

The two extremes of the path correspond to naive Bayes generative modeling at
one end (α = 0), and plain ridge-regularized logistic regression at the other (α = 1).
All methods were calibrated on the training data as follows: Given original weight
vectors ˆβ, we ﬁrst compute un-calibrated predictions ˆµ = X ˆβ for the log-odds of
P (cid:2)Y = 1 (cid:12)
(cid:12) X(cid:3), and then run a second univariate logistic regression Y ∼ ˆµ to adjust
both the intercept and the magnitude of the original coeﬃcients. Moreover, when us-
ing cross-validation on pseudo-examples ( (cid:101)X, Y ), we ensure that all pseudo-examples
induced by a given example (X, Y ) are in the same cross-validation fold. Code for

22

Data Augmentation via L´evy Processes

Figure 1.6: Performance of L´evy thinning with cross-validated ridge-regularized
logistic regression, on a random Gaussian design described in (1.45). The curves
depict the relationship between thinning α and classiﬁcation error as the number of
training examples grows: n = 30, 50, 75, 100, 150, 200, 400, and 600. We see that
naive Bayes improves over ridge logistic regression in very small samples, while in
moderately small samples L´evy thinning does better than either end of the path.

reproducing our results is available at https://github.com/swager/levythin.

Gaussian example. We generate data from the following hierarchical model:

Y ∼ Binomial (0.5) , µ (cid:12)

(cid:12) Y ∼ LY , X (cid:12)

(cid:12) µ ∼ N (µ, Id×d) ,

(1.45)

1

, ..., µ(Y )

where µ, X ∈ Rd and d = 100. The distribution LY associated with each label
Y consists of 10 atoms µ(Y )
10 . These atoms themselves are all randomly
generated such that their ﬁrst 20 coordinates are independent draws of 1.1 T4 where
T4 follows Student’s t-distribution with 4 degrees of freedom; meanwhile, the last
80 coordinates of µ are all 0. The results in Figure 1.6 are marginalized over the
randomness in LY ; i.e., diﬀerent simulation realizations have diﬀerent conditional
laws for µ given Y . Figure 1.6 shows the results.

Poisson example. We generate data from the following hierarchical model:

Y ∼ Binomial (0.5) , θ (cid:12)

(cid:12) Y ∼ LY , Xj

(cid:12)
(cid:12) θ ∼ Pois

1000

,

(1.46)

(cid:32)

(cid:33)

eθj
j=1 eθj

(cid:80)d

1.4 Simulation Experiments

23

Figure 1.7: Performance of L´evy thinning with cross-validated ridge-regularized
logistic regression, on a random Poisson design described in (1.46). The curves
depict the relationship between thinning α and classiﬁcation accuracy for n =
30, 50, 100, 150, 200, 400, 800, and 1600. Here, aggressive L´evy thinning with
small but non-zero α does substantially better than naive Bayes (α = 0) as soon as
n is moderately large.

where θ ∈ Rd, X ∈ Nd, and d = 500. This time, however, LY is deterministic: If
Y = 0, then θ is just 7 ones followed by 493 zeros, whereas





θ (cid:12)
(cid:12) Y = 1 ∼

0, ..., 0
(cid:124) (cid:123)(cid:122) (cid:125)
7

(cid:12)
(cid:12) τ, ..., τ
(cid:124) (cid:123)(cid:122) (cid:125)
7

(cid:12)
(cid:12) 0, ..., 0
(cid:124) (cid:123)(cid:122) (cid:125)
486

 , with τ ∼ Exp(3).

This generative model was also used in simulations by Wager et al. (2014); the
diﬀerence is that they applied thinning to plain logistic regression, whereas here
we verify that L´evy thinning is also helpful when paired with cross-validated ridge
logistic regression. Figure 1.7 shows the results.

These experiments suggest that it is reasonable to pair L´evy thinning with a well-
tuned black box learner on the pseudo-examples ( (cid:101)X, Y ), without worrying about
potential interactions between L´evy thinning and the tuning of the discriminative
model.

24

Data Augmentation via L´evy Processes

1.5 Discussion

In this chapter, we have explored a general framework for performing data augmen-
tation: apply L´evy thinning and train a discriminative classiﬁer on the resulting
pseudo-examples. The exact thinning scheme reﬂects our generative modeling as-
sumptions. We emphasize that the generative assumptions are non-parametric and
of a structural nature; in particular, we never ﬁt an actual generative model, but
rather encode the generative hints implicitly in the pseudo-examples.

A key result is that under the generative assumptions, thinning preserves the
Bayes decision boundary, which suggests that a well-speciﬁed classiﬁer incurs
no asymptotic bias. Similarly, we would expect that a misspeciﬁed but powerful
classiﬁer should incur little bias. We showed that in limit of maximum thinning,
the resulting procedure corresponds to ﬁtting a generative model. The exact bias-
variance trade-oﬀ for moderate levels of thinning is an interesting subject for further
study.

While L´evy processes provide a general framework for thinning examples, we
recognize that there are many other forms of coarsening that could lead to the
same intuitions. For instance, suppose X | θ is a Markov process over words in
a document. We might expect that short contiguous subsequences of X could
serve as good pseudo-examples. More broadly, there are many forms of data
augmentation that do not have the intuition of coarsening an input. For example,
rotating or shearing an image to generate pseudo-images appeals to other forms
of transformational invariance. It would be enlightening to establish a generative
framework in which data augmentation with these other forms of invariance arise
naturally.

To establish the desired result, we show that for a single training example (X, Y ),
the following limit is well-deﬁned for any β ∈ Rp×K:

ρ (β; X, Y ) = lim
α→0

(cid:16)

(cid:101)E

(cid:16)

(cid:104)
(cid:96)

1
α

= −β(Y ) · X + lim
α→0

β; (cid:101)X, Y
(cid:34)

(cid:32)

1
α

(cid:101)E

log

(cid:17)(cid:105)

(cid:17)

− log (K)

(cid:33)(cid:35)

eβ(k)· (cid:101)X

,

1
K

K
(cid:88)

k=1

(1.47)

where on the second line we wrote down the logistic loss explicitly and exploited
linearity of the term involving Y as in Wager et al. (2013). Here (cid:101)E denotes
expectation with respect to the thinning process and reﬂects the B → ∞ limit.
Because (cid:96) is convex, ρ must also be convex; and by equicontinuity ˆβ(α) must also
converge to its minimizer.

Our argument relies on the decomposition At = bt+Wt+Nt from (1.20). Without
loss of generality, we can generate the pseudo-features (cid:101)X as (cid:101)X = bt + (cid:102)WαT + (cid:101)NαT ,

1.6 Appendix: Proof of Theorem 1.4

1.6 Appendix: Proof of Theorem 1.4

25

where (cid:102)WαT and (cid:101)NαT have the same marginal distribution as WαT and NαT . Given
this notation,

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:33)(cid:35)

(cid:33)

(cid:34)

(cid:32)

log

1
K

1
α

(cid:101)E

(cid:34)

log

=

1
α

(cid:101)E

+

1
α

(cid:101)E

K
(cid:88)

k=1
(cid:32)

1
K

(cid:34)

log

K
(cid:88)

k=1
(cid:32)

1
K

K
(cid:88)

k=1

eβ(k)·(αbT +(cid:102)WαT )

(cid:12)
(cid:12) (cid:101)NαT = 0

P

(cid:104)

(cid:105)
(cid:101)NαT = 0

(cid:35)

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:33)

(cid:35)

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

P

(cid:104)

(cid:105)
(cid:101)NαT (cid:54)= 0

.

We now characterize these terms individually. First, because Nt has a ﬁnite jump
intensity, we can verify that, almost surely,

lim
α→0

1
α

P

(cid:104)

(cid:105)
(cid:101)NαT (cid:54)= 0

= λT (X),

where λT (X) is as deﬁned in (1.22). Next, because (cid:102)WαT concentrates at 0 as α → 0,
we can check that
(cid:32)

(cid:33)

(cid:34)

(cid:35)

lim
α→0

(cid:101)E

log

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

1
K
(cid:34)

K
(cid:88)

k=1
(cid:32)

(cid:33)

(cid:35)

eβ(k)· (cid:101)NαT

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

1
K

K
(cid:88)

k=1

(cid:33)

= lim
α→0

(cid:101)E

log

(cid:32)

(cid:90)

=

log

1
K

K
(cid:88)

k=1

eβ(k)·z

dνT (z; X)

where νT (·; X) (1.23) is the ﬁrst jump measure conditional on X.

Meanwhile, in order to control the remaining term, we note that we can write

(cid:102)WαT = α(cid:102)WT + (cid:101)BαT ,

where (cid:101)Bt is a Brownian bridge from 0 to T that is independent from (cid:102)WT . Thus,
noting that limα→0 P
= 1, we ﬁnd that

(cid:105)
(cid:101)NαT = 0

(cid:104)

eβ(k)·(αbT +(cid:102)WαT )

(cid:33)

(cid:35)

(cid:12)
(cid:12) (cid:101)NαT = 0

P

(cid:104)

(cid:105)
(cid:101)NαT = 0

lim
α→0

1
α

(cid:101)E

(cid:34)

(cid:32)

log

1
K

K
(cid:88)

k=1
(cid:34)

= lim
α→0

1
α

(cid:101)E

log

eβ(k)·(α(bT +(cid:102)WT ))+ (cid:101)BαT

(cid:33)(cid:35)

(cid:33)

(cid:32)

1
K
(cid:32)

K
(cid:88)

k=1
K
(cid:88)

k=1

1
K

T
2

= ¯β · µT (X) +

β(k)(cid:62)Σ β(k) − ¯β(cid:62)Σ ¯β

,

where µT (X) is as deﬁned in (1.21) and ¯β = K −1 (cid:80)K
k=1 β(k). The last equality
follows from Taylor expanding the log((cid:80) exp) term and noting that 3rd- and higher-

26

Data Augmentation via L´evy Processes

1.7 References

order terms vanish in the limit.

that ¯β = 0, we ﬁnally conclude that

Bringing back the linear term form (1.47), and assuming without loss of generality

ρ (β; X, Y ) = −β(Y ) · X +

β(k)(cid:62)Σ β(k)

T
2

1
K

K
(cid:88)

k=1

+ λT (X)

log

(cid:32)

1
K

K
(cid:88)

k=1

(cid:33)

eβ(k)·z

dνT (z; X)

= −β(Y ) · µT (X) +

β(k)(cid:62)Σ β(k)

(cid:90)

(cid:90)

T
2

K
(cid:88)

1
K
(cid:32) K
(cid:88)

k=1

k=1

(cid:33)

eβ(k)·z

+ λT (X)

−β(Y ) · z + log

− log(K) dνT (z; X),

where for the second equality we used the fact that X = µT (X)+λT (X) (cid:82) z dνT (z; X).
Finally, this expression only diﬀers from (1.24) by terms that do not include β; thus,
they yield the same minimizer.

Y. S. Abu-Mostafa. Learning from hints in neural networks. Journal of Complexity,

6(2):192–198, 1990.

J. Ba and B. Frey. Adaptive dropout for training deep neural networks. In Advances

in Neural Information Processing Systems (NIPS), pages 3084–3092, 2013.

P. Baldi and P. Sadowski. The dropout learning algorithm. Artiﬁcial intelligence,

210:78–122, 2014.

C. M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural

computation, 7(1):108–116, 1995.

G. Bouchard and B. Triggs. The trade-oﬀ between generative and discriminative
classiﬁers. In International Conference on Computational Statistics, pages 721–
728, 2004.

B. Efron. The eﬃciency of logistic regression compared to normal discriminant
analysis. Journal of the American Statistical Association (JASA), 70(352):892–
898, 1975.

J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized
linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22,
2010.

A. Globerson and S. Roweis. Nightmare at test time: robust learning by feature
deletion. In International Conference on Machine Learning (ICML), pages 353–
360, 2006.

I. Goodfellow, D. Warde-farley, M. Mirza, A. Courville, and Y. Bengio. Maxout
In International Conference on Machine Learning (ICML), pages

networks.
1319–1327, 2013.

D. P. Helmbold and P. M. Long. On the inductive bias of dropout. Journal of

1.7 References

27

Machine Learning Research (JMLR), 16:3403–3454, 2015.

J. Josse and S. Wager. Stable autoencoding: A ﬂexible framework for regularized

low-rank matrix estimation. arXiv preprint arXiv:1410.8275, 2014.

A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep
In Advances in Neural Information Processing

convolutional neural networks.
Systems (NIPS), pages 1097–1105, 2012.

J. A. Lasserre, C. M. Bishop, and T. P. Minka. Principled hybrids of generative and
discriminative models. In Computer Vision and Pattern Recognition (CVPR),
pages 87–94, 2006.

P. Liang and M. I. Jordan. An asymptotic analysis of generative, discriminative, and
pseudolikelihood estimators. In International Conference on Machine Learning
(ICML), pages 584–591, 2008.

D. McAllester. A PAC-Bayesian tutorial with a dropout bound. arXiv preprint

arXiv:1307.2118, 2013.

A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Gener-
ative/discriminative training for clustering and classiﬁcation. In Association for
the Advancement of Artiﬁcial Intelligence (AAAI), 2006.

A. Y. Ng and M. I. Jordan. On discriminative vs. generative classiﬁers: A compar-
ison of logistic regression and naive Bayes. In Advances in Neural Information
Processing Systems (NIPS), 2002.

R. Raina, Y. Shen, A. Ng, and A. McCallum. Classiﬁcation with hybrid gen-
In Advances in Neural Information Processing

erative/discriminative models.
Systems (NIPS), 2004.

Y. D. Rubinstein and T. Hastie. Discriminative vs informative learning.

In
International Conference on Knowledge Discovery and Data Mining (KDD),
volume 5, pages 49–53, 1997.

S. P. Sch¨olkopf, P. Simard, V. Vapnik, and A. Smola. Improving the accuracy and
speed of support vector machines. In Advances in Neural Information Processing
Systems (NIPS), pages 375–381, 1997.

P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. Transformation
Invariance in Pattern Recognition—Tangent Distance and Tangent Propagation.
Neural networks: Tricks of the trade Springer, 1998.

N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. Journal
of Machine Learning Research (JMLR), 15(1):1929–1958, 2014.

L. van der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger. Learning with
marginalized corrupted features. In International Conference on Machine Learn-
ing (ICML), pages 410–418, 2013.

S. Wager, S. I. Wang, and P. Liang. Dropout training as adaptive regularization.

In Advances in Neural Information Processing Systems (NIPS), 2013.

S. Wager, W. Fithian, S. I. Wang, and P. Liang. Altitude training: Strong bounds
for single-layer dropout. In Advances in Neural Information Processing Systems
(NIPS), 2014.

L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Regularization of neural
networks using dropconnect. In International Conference on Machine Learning
(ICML), pages 1058–1066, 2013.

S. I. Wang and C. Manning. Fast dropout training. In International Conference

28

Data Augmentation via L´evy Processes

on Machine Learning (ICML), pages 118–126, 2013.

S. I. Wang, M. Wang, S. Wager, P. Liang, and C. Manning. Feature noising
for log-linear structured prediction. In Empirical Methods in Natural Language
Processing (EMNLP), 2013.

6
1
0
2
 
r
a

M
 
1
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
0
4
3
6
0
.
3
0
6
1
:
v
i
X
r
a

1

Data Augmentation via L´evy Processes

Stefan Wager
Stanford University
Stanford, USA

William Fithian
University of California, Berkeley
Berkeley, USA

Percy Liang
Stanford University
Stanford, USA

swager@stanford.edu

wfithian@berkeley.edu

pliang@cs.stanford.edu

If a document is about travel, we may expect that short snippets of the document
should also be about travel. We introduce a general framework for incorporating
these types of invariances into a discriminative classiﬁer. The framework imagines
data as being drawn from a slice of a L´evy process. If we slice the L´evy process at
an earlier point in time, we obtain additional pseudo-examples, which can be used
to train the classiﬁer. We show that this scheme has two desirable properties: it
preserves the Bayes decision boundary, and it is equivalent to ﬁtting a generative
model in the limit where we rewind time back to 0. Our construction captures popular
schemes such as Gaussian feature noising and dropout training, as well as admitting
new generalizations.

1.1 Introduction

Black-box discriminative classiﬁers such as logistic regression, neural networks, and
SVMs are the go-to solution in machine learning: they are simple to apply and
often perform well. However, an expert may have additional knowledge to exploit,
often taking the form of a certain family of transformations that should usually
leave labels ﬁxed. For example, in object recognition, an image of a cat rotated,
translated, and peppered with a small amount of noise is probably still a cat.
Likewise, in document classiﬁcation, the ﬁrst paragraph of an article about travel

2

Data Augmentation via L´evy Processes

Figure 1.1: Two examples of transforming an original input X into a noisy, less
informative input (cid:101)X. The new inputs clearly have the same label but contain less
information and thus are harder to classify.

is most likely still about travel. In both cases, the “expert knowledge” amounts
to a belief that a certain transform of the features should generally not aﬀect an
example’s label.

One popular strategy for encoding such a belief is data augmentation: generat-
ing additional pseudo-examples or “hints” by applying label-invariant transforma-
tions to training examples’ features (Abu-Mostafa, 1990; Sch¨olkopf et al., 1997;
Simard et al., 1998). That is, each example (X (i), Y (i)) is replaced by many pairs
( (cid:101)X (i,b), Y (i)) for b = 1, . . . , B, where each (cid:101)X (i,b) is a transformed version of X (i).
This strategy is simple and modular: after generating the pseudo-examples, we can
simply apply any supervised learning algorithm to the augmented dataset. Fig-
ure 1.1 illustrates two examples of this approach, an image transformed to a noisy
image and a text caption, transformed by deleting words.

Dropout training (Srivastava et al., 2014) is an instance of data augmentation
that, when applied to an input feature vector, zeros out a subset of the features ran-
domly. Intuitively, dropout implies a certain amount of signal redundancy across
features—that an input with about half the features masked should usually be
classiﬁed the same way as a fully-observed input. In the setting of document clas-

1.1 Introduction

3

siﬁcation, dropout can be seen as creating pseudo-examples by randomly omitting
some information (i.e., words) from each training example. Building on this in-
terpretation, Wager et al. (2014) show that learning with such artiﬁcially diﬃcult
examples can substantially improve the generalization performance of a classiﬁer.
To study dropout, Wager et al. (2014) assume that documents can be summarized
as Poisson word counts. Speciﬁcally, assume that each document has an underlying
topic associated with a word frequency distribution π on the p-dimensional simplex
and an expected length T ≥ 0, and that, given π and T , the word counts Xj
(cid:12)
are independently generated as Xj
(cid:12) T, π ∼ Pois(T πj). The analysis of Wager
et al. (2014) then builds on a duality between dropout and the above generative
model. Consider the example given in Figure 1.1, where dropout creates pseudo-
documents (cid:101)X by deleting half the words at random from the original document
X. As explained in detail in Section 1.2.1, if X itself is drawn from the above
Poisson model, then the dropout pseudo-examples (cid:101)X are marginally distributed
(cid:12)
as (cid:101)Xj
(cid:12) T, π ∼ Pois(0.5 T πj). Thus, in the context of this Poisson generative
model, dropout enables us to create new, shorter pseudo-examples that preserve
the generative structure of the problem.

The above interpretation of dropout raises the following question: if feature
deletion is a natural way to create information-poor pseudo-examples for document
classiﬁcation, are there natural analogous feature noising schemes that can be
applied to other problems? In this chapter, we seek to address this question, and
study a more general family of data augmentation methods generalizing dropout,
based on L´evy processes: We propose an abstract L´evy thinning scheme that reduces
to dropout in the Poisson generative model considered by Wager et al. (2014). Our
framework further suggests new methods for feature noising such as Gamma noising
based on alternative generative models, all while allowing for a uniﬁed theoretical
analysis.

In the above discussion,
From generative modeling to data augmentation.
we treated the expected document length T as ﬁxed. More generally, we could
imagine the document as growing in length over time, with the observed document
X merely a “snapshot” of what the document looks like at time T . Formally, we
(cid:12)
(cid:12) π ∼
can imagine a latent Poisson process (At)t≥0, with ﬁxed-t marginals (At)j
Pois(t πj), and set X = AT . In this notation, dropout amounts to “rewinding”
the process At to obtain short pseudo-examples. By setting (cid:101)X = AαT , we have
P[ (cid:101)X = ˜x (cid:12)

(cid:12) AT = x], for thinning parameter α ∈ (0, 1).

(cid:12) X = x] = P[AαT = ˜x (cid:12)

The main result of this chapter is that the analytic tools developed by Wager
et al. (2014) are not restricted to the case where (At) is a Poisson process, and in
fact hold whenever (At) is a L´evy process. In other words, their analysis applies
to any classiﬁcation problem where the features X can be understood as time-T
snapshots of a process (At), i.e., X = AT .

Recall that a L´evy process (At)t≥0 is a stochastic process with A0 = 0 that
has independent and stationary increments: {Ati − Ati−1} are independent for
d= At−s for and s < t. Common examples
0 = t0 < t1 < t2 < · · · , and At − As

4

Data Augmentation via L´evy Processes

Figure 1.2: Graphical model depicting our generative assumptions; note that we
are not ﬁtting this generative model. Given class Y , we draw a topic θ, which
governs the parameters of the L´evy process (At). We slice at time T to get the
original input X = AT and at an earlier time ˜T to get the thinned or noised input
˜X = A ˜T . We show that given X, we can sample ˜X without knowledge of θ.

of L´evy processes include Brownian motion and Poisson processes.

In any such L´evy setup, we show that it is possible to devise an analogue to
dropout that creates pseudo-examples by rewinding the process back to some earlier
time (cid:101)T ≤ T . Our generative model is depicted in Figure 1.2: (At), the information
relevant to classifying Y , is governed by a latent topic θ ∈ Rp. L´evy thinning then
seeks to rewind (At)—importantly as we shall see, without having access to θ.

We should think of (At) as representing an ever-accumulating amount of infor-
mation concerning the topic θ: In the case of document classiﬁcation, (At) are the
word counts associated with a document that grows longer as t increases. In other
examples that we discuss in Section 1.3, At will represent the sum of t independent
noisy sensor readings. The independence of increments property assures that as we
progress in time, we are always obtaining new information. The stopping time T
thus represents the information content in input X about topic θ. L´evy thinning
seeks to improve classiﬁcation accuracy by turning a few information-rich examples
X into many information-poor examples (cid:101)X.

We emphasize that, although our approach uses generative modeling to motivate
a data augmentation scheme, we do not in fact ﬁt a generative model. This
presents a contrast to the prevailing practice: two classical approaches to multiclass
classiﬁcation are to either directly train a discriminative model by running, e.g.,
multiclass logistic regression on the n original training examples; or, at the other
extreme, to specify and ﬁt a simple parametric version of the above generative
model, e.g., naive Bayes, and then use Bayes’ rule for classiﬁcation. It is well
known that the latter approach is usually more eﬃcient if it has access to a
correctly speciﬁed generative model, but may be badly biased in case of model
misspeciﬁcation (Efron, 1975; Ng and Jordan, 2002; Liang and Jordan, 2008). Here,
we ﬁrst seek to devise a noising scheme X → (cid:101)X and then to train a discriminative
model on the pseudo-examples ( (cid:101)X, Y ) instead of the original examples (X, Y ). Note

1.1 Introduction

5

Figure 1.3: We model each input X as a slice of a L´evy process at time T .
We generate noised examples (cid:101)X by “stepping back in time” to ˜T . Note that the
examples of the two classes are closer together now, thus forcing the classiﬁer to
work harder.

that even if the generative model is incorrect, this approach will incur limited bias as
long as the noising scheme roughly preserves class boundaries — for example, even
if the Poisson document model is incorrect, we may still be justiﬁed in classifying a
subsampled travel document as a travel document. As a result, this approach can
take advantage of an abstract generative structure while remaining more robust to
model misspeciﬁcation than parametric generative modeling.

Overview of results. We consider the multiclass classiﬁcation setting where we
seek to estimate a mapping from input X to class label Y . We imagine that each
X is generated by a mixture of L´evy process, where we ﬁrst draw a random topic θ
given the class Y , and then run a L´evy process (At) depending on θ to time T . In
order to train a classiﬁer, we pick a thinning parameter α ∈ (0, 1), and then create
(cid:12)
pseudo examples by rewinding the original X back to time α T , i.e., (cid:101)X ∼ AαT
(cid:12) AT .
We show three main results in this chapter. Our ﬁrst result is that we can generate
such pseudo-examples (cid:101)X without knowledge of the parameters θ governing the
generative L´evy process. In other words, while our method posits the existence of
a generative model, our algorithm does not actually need to estimate it. Instead,
it enables us to give hints about a potentially complex generative structure to a
discriminative model such as logistic regression.

Second, under assumptions that our generative model is correct, we show that

6

Data Augmentation via L´evy Processes

feature noising preserves the Bayes decision boundary: P[Y | X = x] = P[Y | (cid:101)X =
x]. This means that feature noising does not introduce any bias in the inﬁnite data
limit.

Third, we consider the limit of rewinding to the beginning of time (α → 0). Here,
we establish conditions given which, even with ﬁnite data, the decision boundary
obtained by ﬁtting a linear classiﬁer on the pseudo-examples is equivalent to that
induced by a simpliﬁed generative model. When this latter result holds, we can
interpret α-thinning as providing a semi-generative regularization path for logistic
regression, with a simple generative procedure at one end and unregularized logistic
regression at the other.

Related work. The trade-oﬀ between generative models and discriminative
models has been explored extensively. Rubinstein and Hastie (1997) empirically
compare discriminative and generative classiﬁers models with respect to bias and
variance, Efron (1975) and Ng and Jordan (2002) provide a more formal discussion
of the bias-variance trade-oﬀ between logistic regression and naive Bayes. Liang
and Jordan (2008) perform an asymptotic analysis for general exponential families.
A number of papers study hybrid loss functions that combine both a joint and
conditional likelihood (Raina et al., 2004; Bouchard and Triggs, 2004; Lasserre et al.,
2006; McCallum et al., 2006; Liang and Jordan, 2008). The data augmentation
approach we advocate in this chapter is fundamentally diﬀerent, in that we are
merely using the structural assumptions implied by the generative models to
generate more data, and are not explicitly ﬁtting a full generative model.

The present work was initially motivated by understanding dropout training
(Srivastava et al., 2014), which was introduced in the context of regularizing
deep neural networks, and has had much empirical success (Ba and Frey, 2013;
Goodfellow et al., 2013; Krizhevsky et al., 2012; Wan et al., 2013). Many of the
regularization beneﬁts of dropout can be found in logistic regression and other
single-layer models, where it is also known as “blankout noise” (Globerson and
Roweis, 2006; van der Maaten et al., 2013) and has been successful in natural
language tasks such as document classiﬁcation and named entity recognition (Wager
et al., 2013; Wang and Manning, 2013; Wang et al., 2013). There are a number of
theoretical analyses of dropout: using PAC-Bayes framework (McAllester, 2013),
comparing dropout to “altitude training” (Wager et al., 2014), and interpreting
dropout as a form of adaptive regularization (Baldi and Sadowski, 2014; Bishop,
1995; Helmbold and Long, 2015; Josse and Wager, 2014; Wager et al., 2013).

1.2 L´evy Thinning

We begin by brieﬂy reviewing the results of Wager et al. (2014), who study dropout
training for document classiﬁcation from the perspective of thinning documents
(Section 1.2.1). Then, in Section 1.2.2, we generalize these results to the setting of

1.2 L´evy Thinning

7

generic L´evy generative models.

1.2.1 Motivating Example: Thinning Poisson Documents

Suppose we want to classify documents according to their subject, e.g., sports,
politics, or travel. As discussed in the introduction, common sense intuition about
the nature of documents suggests that a short snippet of a sports document
should also be classiﬁed as a sports document. If so, we can generate many new
training examples by cutting up the original documents in our dataset into shorter
subdocuments and labeling each subdocument with the same label as the original
document it came from. By training a classiﬁer on all of the pseudo-examples we
generate in this way, we should be able to obtain a better classiﬁer.

In order to formalize this intuition, we can represent a document as a sequence
of words from a dictionary {1, . . . , d}, with the word count Xj denoting the
number of occurrences of word j in the document. Given this representation, we
can easily create “subdocuments” by binomially downsampling the word counts Xj
independently. That is, for some ﬁxed downsampling fraction α ∈ (0, 1), we draw

(cid:101)Xj | Xj ∼ Binom(Xj, α).

In other words, we keep each occurrence of word j independently with probability
α.

Wager et al. (2014) study this downsampling scheme in the context of a Poisson
mixture model for the inputs X that obeys the structure of Figure 1.2: ﬁrst, we
draw a class Y ∈ {1, . . . , K} (e.g., travel) and a “topic” θ ∈ Rd (e.g., corresponding
to travel in Norway). The topic θ speciﬁes a distribution over words,

(1.1)

(1.2)

µj(θ) = eθj ,

where, without loss of generality, we assume that (cid:80)d
j=1 eθj = 1. We then draw a
Pois(T ) number of words, where T is the expected document length, and generate
each word independently according to θ. Equivalently, each word count is an
independent Poisson random variable, Xj ∼ Pois(T µj(θ)). The following is an
example draw of a document:

Y = travel

norway
(cid:122)(cid:125)(cid:124)(cid:123)
0.5 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
0.5 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
1.2 ,

skyscraper
(cid:122) (cid:125)(cid:124) (cid:123)
−2.7 , . . . ]

θ = [

norway
(cid:122)(cid:125)(cid:124)(cid:123)

2 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
1 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
3 ,

skyscraper
(cid:122)(cid:125)(cid:124)(cid:123)
0

, . . . ]

X = [

norway
(cid:122)(cid:125)(cid:124)(cid:123)

1 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
0 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
1 ,

skyscraper
(cid:122)(cid:125)(cid:124)(cid:123)
0

, . . . ]

(cid:101)X = [

Let us now try to understand the downsampling scheme (cid:101)X | X in the context of the

8

Data Augmentation via L´evy Processes

Poisson topic model over X. For each word j, recall that (cid:101)Xj | Xj ∼ Binom(Xj, α).
If we marginalize over X, then we have:

(cid:101)Xj | T, θ ∼ Pois (αT µj(θ)) .

(1.3)

(1.4)

As a result, the distribution of (cid:101)X is exactly the distribution of X if we replaced T
with (cid:101)T = αT .

We can understand this thinning by embedding the document X in a multivariate
Poisson process (At)t≥0, where the marginal distribution of At ∈ {0, 1, 2, . . . }d is
deﬁned to be the distribution over counts when the expected document length is t.
Then, we can write

X = AT ,

(cid:101)X = A

(cid:101)T .

Thus, under the Poisson topic model, the binomial thinning procedure does not
alter the structure of the problem other than by shifting the expected document
length from T to (cid:101)T . Figure 1.4 illustrates one realization of L´evy thinning in the
Poisson case with a three-word dictionary. Note that in this case we can sample
(cid:101)X = AαT given X = AT without knowledge of θ.

This perspective lies at the heart of the analysis in Wager et al. (2014), who
show under the Poisson model that, when the overall document length (cid:107)X(cid:107)1
is independent of the topic θ, thinning does not perturb the optimal decision
boundary. Indeed, the conditional distribution over class labels is identical for the
original features and the thinned features:

P[Y | X = x] = P[Y | (cid:101)X = x].

(1.5)

This chapter extends the result to general L´evy processes (see Theorem 1.2).

This last result (1.5) may appear quite counterintuitive: for example, if A60 is
more informative than A40, how can it be that downsampling does not perturb
the conditional class probabilities? Suppose x is a 40-word document ((cid:107)x(cid:107)1 = 40).
When t = 60, most of the documents will be longer than 40 words, and thus x will be
less likely under t = 60 than under t = 40. However, (1.5) is about the distribution
of Y conditioned on a particular realization x. The claim is that, having observed
x, we obtain the same information about Y regardless of whether t, the expected
document length, is 40 or 60.

1.2.2 Thinning L´evy Processes

The goal of this section is to extend the Poisson topic model from Section 1.2.1
and construct general thinning schemes with the invariance property of (1.5). We
will see that L´evy processes provide a natural vehicle for such a generalization: The
Poisson process used to generate documents is a speciﬁc L´evy process, and binomial
sampling corresponds to “rewinding” the L´evy process back in time.

Consider the multiclass classiﬁcation problem of predicting a discrete class Y ∈
{1, . . . , K} given an input vector X ∈ Rd. Let us assume that the joint distribution

1.2 L´evy Thinning

9

over (X, Y ) is governed by the following generative model:

1. Choose Y ∼ Mult(π), where π is on the K-dimensional simplex.

2. Draw a topic θ | Y , representing a subpopulation of class Y .
3. Construct a L´evy process (At)t≥0 | θ, where At ∈ Rd is a potential input vector
at time t.

4. Observe the input vector X = AT at a ﬁxed time T .

While the L´evy process imposes a fair amount of structure, we make no assump-
tions about the number of topics, which could be uncountably inﬁnite, or about
their distribution, which could be arbitrary. Of course, in such an unconstrained
non-parametric setting, it would be extremely diﬃcult to adequately ﬁt the genera-
tive model. Therefore, we take a diﬀerent tack: We will use the structure endowed by
the L´evy process to generate pseudo-examples for consumption by a discriminative
classiﬁer. These pseudo-examples implicitly encode our generative assumptions.

The natural way to generate a pseudo-example ( (cid:101)X, Y ) is to “rewind” the L´evy
process (At) backwards from time T (recall X = AT ) to an earlier time (cid:101)T = αT
(cid:101)T . In practice, (At) is
for some α ∈ (0, 1) and deﬁne the thinned input as (cid:101)X = A
unobserved, so we draw (cid:101)X conditioned on the original input X = AT and topic θ.
In fact, we can draw many realizations of (cid:101)X | X, θ.

Our hope is that a single full example (X, Y ) is rich enough to generate many
diﬀerent pseudo-examples ( (cid:101)X, Y ), thus increasing the eﬀective sample size. More-
over, Wager et al. (2014) show that training with such pseudo-examples can also
lead to a somewhat surprising “altitude training” phenomenon whereby thinning
yields an improvement in generalization performance because the pseudo-examples
are more diﬃcult to classify than the original examples, and thus force the learning
algorithm to work harder and learn a more robust model.

A technical diﬃculty is that generating (cid:101)X | X, θ seemingly requires knowledge of
the topic θ driving the underlying L´evy process (At). In order to get around this
issue, we establish the following condition under which the observed input X = AT
alone is suﬃcient—that is, P[ (cid:101)X | X, θ] does not actually depend on θ.
Assumption 1.1 (exponential family structure). The L´evy process (At) (cid:12)
(cid:12) θ is
drawn according to an exponential family model whose marginal density at time
t is

f (t)
θ (x) = exp [θ · x − tψ (θ)] h(t) (x) for every t ∈ R.

(1.6)

Here, the topic θ ∈ Rd is an unknown parameter vector, and h(t)(x) is a family of
carrier densities indexed by t ∈ R.

The above assumption is a natural extension of a standard exponential family
assumption that holds for a single value of t. Speciﬁcally, suppose that h(t)(x),
t > 0, denotes the t-marginal densities of a L´evy process, and that f (1)
(x) =
exp [θ · x − ψ (θ)] h(1)(x) is an exponential family through h(1)(x) indexed by θ ∈
Rd. Then, we can verify that the densities speciﬁed in (1.6) induce a family of

θ

10

Data Augmentation via L´evy Processes

Figure 1.4: Illustration of our Poisson process document model with a three-word
dictionary and µ(θ) = (0.25, 0.3, 0.45). The word counts of the original document,
X = (8, 7, 16), represents the trivariate Poisson process At, sliced at T = 28. The
thinned pseudo-document (cid:101)X = (2, 4, 9) represents At sliced at (cid:101)T = 14.

L´evy processes indexed by θ. The key observation in establishing this result is that,
because h(t)(x) is the t-marginal of a L´evy process, the L´evy–Khintchine formula
implies that

(cid:90)

eθ·xh(t)(x) dx =

eθ·xh(1)(x) dx

= et ψ(θ),

(cid:18)(cid:90)

(cid:19)t

and so the densities in (1.6) are properly normalized.

We also note that, given this assumption and as T → ∞, we have that AT /T
converges almost surely to µ(θ) def= E [A1]. Thus, the topic θ can be understood as
a description of an inﬁnitely informative input. For ﬁnite values of T , X represents
a noisy observation of the topic θ.

Now, given this structure, we show that the distribution of (cid:101)X = AαT conditional
on X = AT does not depend on θ. Thus, feature thinning is possible without
knowledge of θ using the L´evy thinning procedure deﬁned below. We note that,
in our setting, the carrier distributions h(t)(x) are always known; in Section 1.3,
we discuss how to eﬃciently sample from the induced distribution g(αT ) for some
speciﬁc cases of interest.

Theorem 1.1 (L´evy thinning). Assume that (At) satisﬁes the exponential family
structure in (1.6), and let α ∈ (0, 1) be the thinning parameter. Then, given an
input X = AT and conditioned on any θ, the thinned input (cid:101)X = AαT has the

1.2 L´evy Thinning

11

(1.7)

(1.8)

following density:

g(αT )(˜x; X) =

h(αT )(˜x) h((1−α)T )(X − ˜x)
h(T )(X)

,

which importantly does not depend on θ.

Proof. Because the L´evy process (At) has independent and stationary increments,
we have that AαT ∼ f (αT )
and AT − AαT ∼ f ((1−α)T )
are independent. Therefore,
we can write the conditional density of AαT given AT as the joint density over
(AαT , AT ) (equivalently, the reparametrization (AαT , AT − AαT )) divided by the
marginal density over AT :

θ

θ

g(αT )(˜x; X) =

f (αT )
θ

(X − ˜x)

(˜x)f ((1−α)T )
θ
f (T )
(X)
θ
exp [θ · ˜x − αT ψ(θ)] h(αT )(˜x)

(cid:16)

(cid:17)

=

×

×

(cid:16)

(cid:16)

exp [θ · (X − ˜x) − (1 − α)T ψ(θ)] h((1−α)T )(X − ˜x)

(cid:17)

exp [θ · X − T ψ(θ)] h(T )(X)

(cid:17)−1

,

where the last step expands everything (1.6). Algebraic cancellation, which removes
all dependence on θ, completes the proof.

Note that while Theorem 1.1 guarantees we can carry out feature thinning
without knowing the topic θ, it does not guarantee that we can do it without
knowing the information content T . For Poisson processes, the binomial thinning
mechanism depends only on α and not on the original T . This is a convenient
property in the Poisson case but does not carry over to all L´evy processes — for
example, if Bt is a standard Brownian motion, then the distribution of B2 given
B4 = 0 is N(0, 1), while the distribution of B200 given B400 = 0 is N(0, 100).
As we will see in Section 1.3, thinning in the Gaussian and Gamma families
does require knowing T , which will correspond to a “sample size” or “precision.”
Likewise, Theorem 1.1 does not guarantee that sampling from (1.7) can be carried
out eﬃciently; however, in all the examples we present here, sampling can be carried
out easily in closed form.

1.2.3 Learning with Thinned Features

Having shown how to thin the input X to (cid:101)X without knowledge of θ, we can
proceed to deﬁning our full data augmentation strategy. We are given n training
examples {(X (i), Y (i))}n
i=1. For each original input X (i), we generate B thinned
versions (cid:101)X (i,1), . . . , (cid:101)X (i,B) by sampling from (1.7). We then pair these B examples
up with Y (i) and train any discriminative classiﬁer on these Bn examples. Algo-
rithm 1 describes the full procedure where we specialize to logistic regression. If
one is implementing this procedure using stochastic gradient descent, one can also

12

Data Augmentation via L´evy Processes

Procedure 1. Logistic Regression with L´evy Regularization
Input: n training examples (X (i), Y (i)), a thinning parameter α ∈ (0, 1), and a feature
map φ : Rd (cid:55)→ Rp.

1. For each training example X (i), generate B thinned versions ( (cid:101)X (i,b))B
to (1.7).

b=1 according

2. Train logistic regression on the resulting pseudo-examples:

(cid:40) n
(cid:88)

B
(cid:88)

(cid:41)
β; (cid:101)X (i,b), Y (i)(cid:17)
(cid:16)

,

(cid:96)

ˆβ def= argmin
β∈Rp×K

i=1

b=1

where the multi-class logistic loss with feature map φ is

(cid:96)(β; x, y) def= log

eβ(k)·φ(x)

− β(y) · φ(x).

(cid:33)

(cid:32) K
(cid:88)

k=1

3. Classify new examples according to

ˆy(x) = argmin

k∈{1, ..., K}

(cid:110)

ˆc(k) − ˆβ(k) · φ(x)

(cid:111)

,

(1.9)

(1.10)

(1.11)

where the ˆck ∈ R are optional class-speciﬁc calibration parameters for k = 1, . . . , K.

generate a fresh thinned input (cid:101)X whenever we sample an input X on the ﬂy, which
is the usual implementation of dropout training (Srivastava et al., 2014).

In the ﬁnal step (1.11) of Algorithm 1, we also allow for class-speciﬁc calibration
parameters . After the ˆβ(k) have been determined by logistic regression with L´evy
regularization, these parameters ˆc(k) can be chosen by optimizing the logistic loss on
the original uncorrupted training data. As discussed in Section 1.2.5, re-calibrating
the model is recommended, especially when α is small.

1.2.4 Thinning Preserves the Bayes Decision Boundary

We can easily implement the thinning procedure, but how will it aﬀect the accuracy
of the classiﬁer? The following result gives us a ﬁrst promising piece of the answer
by establishing conditions under which thinning does not aﬀect the Bayes decision
boundary.

At a high level, our results rely on the fact that under our generative model, the
“amount of information” contained in the input vector X is itself uninformative
about the class label Y .

Assumption 1.2 (Equal information content across topics). Assume there exists
a constant ψ0 such that ψ(θ) = ψ0 with probability 1, over random θ.

For example, in our Poisson topic model, we imposed the restriction that ψ(θ) =

1.2 L´evy Thinning

13

(1.12)

(1.13)

(1.14)

(1.15)

(cid:80)d

j=1 eθj = 1, which ensures that the document length (cid:107)At(cid:107)1 has the same

distribution (which has expectation ψ(θ) in this case) for all possible θ.

Theorem 1.2. Under Assumption 1.2, the posterior class probabilities are invari-
ant under thinning (1.7):

(cid:104)
Y = y (cid:12)

P

(cid:12) (cid:101)X = x

(cid:105)

= P (cid:2)Y = y (cid:12)

(cid:12) X = x(cid:3)

for all y ∈ {1, . . . , K} and x ∈ X.

Proof. Given Assumption 1.2, the density of At | θ is given by:

f (t)
θ (x) = eθ·xe−tψ0h(t)(x),

which importantly splits into two factors, one depending on (θ, x), and the other
depending on (t, x). Now, let us compute the posterior distribution:

P (cid:2)Y = y (cid:12)

(cid:12) At = x(cid:3) ∝ P [Y = y]

P [θ | Y ] f (t)

θ (x)dθ

∝ P [Y = y]

P [θ | Y ] eθ·xdθ,

(cid:90)

(cid:90)

which does not depend on t, as e−tψ0 h(t)(x) can be folded into the normalization
constant. Recall that X = AT and (cid:101)X = A
(cid:101)T . Substitute t = T and t = (cid:101)T to conclude
(1.12).

To see the importance of Assumption 1.2, consider the case where we have two
labels (Y ∈ {1, 2}), each with a single topic (Y yields topic θY ). Suppose that
ψ(θ2) = 2ψ(θ1)—that is, documents in class 2 are on average twice as long as those
in class 1. Then, we would be able to make class 2 documents look like class 1
documents by thinning them with α = 0.5.

Remark 1.1. If we also condition on the information content T , then an analogue
to Theorem 1.2 holds even without Assumption 1.2:

(cid:104)
Y = y (cid:12)

P

(cid:12) (cid:101)X = x, (cid:101)T = t

= P (cid:2)Y = y (cid:12)

(cid:12) X = x, T = t(cid:3) .

(cid:105)

(1.16)

This is because, after conditioning on T , the e−tψ(θ) term factors out of the
likelihood.

The upshot of Theorem 1.2 is that thinning will not induce asymptotic bias
whenever an estimator produces P (cid:2)Y = y (cid:12)
(cid:12) X = x(cid:3) in the limit of inﬁnite data
(n → ∞), i.e., if the logistic regression (Algorithm 1) is well-speciﬁed. Speciﬁcally,
training either on original examples or thinned examples will both converge to the
true class-conditional distribution. The following result assumes that the feature
space X is discrete; the proof can easily be generalized to the case of continuous
features.

Corollary 1.3. Suppose that Assumption 1.2 holds, and that the above multi-class

14

Data Augmentation via L´evy Processes

logistic regression model is well-speciﬁed, i.e., P (cid:2)Y = y (cid:12)
(cid:12) X = x(cid:3) ∝ eβ(y)·φ(x) for
some β and all y = 1, ..., K. Then, assuming that P [At = x] > 0 for all x ∈ X and
t > 0, Algorithm 1 is consistent, i.e., the learned classiﬁcation rule converges to the
Bayes classiﬁer as n → ∞.
Proof. At a ﬁxed x, the population loss E (cid:2)(cid:96) (cid:0)β; X, Y (cid:12)
any choice of β satisfying:
exp (cid:2)β(y) · φ(x)(cid:3)
k=1 exp (cid:2)β(k) · φ(x)(cid:3) = P (cid:2)Y = y (cid:12)

(cid:12) X = x(cid:1)(cid:3) is minimized by

(cid:12) X = x(cid:3)

(1.17)

(cid:80)K

for all y = 1, ..., K. Since the model is well-speciﬁed and by assumption P[ (cid:101)X =
x] > 0 for all x ∈ X, we conclude that weight vector ˆβ learned using Algorithm 1
must satisfy asymptotically (1.17) for all x ∈ X as n → ∞.

1.2.5 The End of the Path

As seen above, if we have a correctly speciﬁed logistic regression model, then
L´evy thinning regularizes it without introducing any bias. However, if the logistic
regression model is misspeciﬁed, thinning will in general induce bias, and the
amount of thinning presents a bias-variance trade-oﬀ. The reason for this bias is that
although thinning preserves the Bayes decision boundary, it changes the marginal
distribution of the covariates X, which in turn aﬀects logistic regression’s linear
approximation to the decision boundary. Figure 1.5 illustrates this phenomenon in
the case where At is a Brownian motion, corresponding to Gaussian feature noising;
Wager et al. (2014) provides a similar example for the Poisson topic model.

Fully characterizing the bias of L´evy thinning is beyond the scope of this paper.
However, we can gain some helpful insights about this bias by studying “strong
thinning”—i.e., L´evy thinning in the limit as the thinning parameter α → 0:

ˆβ0+

def= lim
α→0

lim
B→∞

ˆβ(α, B),

(1.18)

where ˆβ(α, B) is deﬁned as in (1.9) with the explicit dependence on α and B.
For each α, we take B → ∞ perturbed points for each of the original n data
points. As we show in this section, this limiting classiﬁer is well-deﬁned under weak
conditions; moreover, in some cases of interest, it can be interpreted as a simple
generative classiﬁer. The result below concerns the existence of ˆβ0+, and establishes
that it is the empirical minimizer of a convex loss function.

Theorem 1.4. Assume the setting of Procedure 1, and let the feature map be
φ(x) = x. Assume that the generative L´evy process (At) has ﬁnitely many jumps in
expectation over the interval [0, T ]. Then, the limit ˆβ0+ is well-deﬁned and can be
written as

ˆβ0+ = argmin
β∈Rp×K

(cid:40) n
(cid:88)

i=1

(cid:16)

β; X (i), Y (i)(cid:17)

ρ

(cid:41)

,

(1.19)

1.2 L´evy Thinning

15

(cid:12) θ, T ∼ N (cid:0)T θ, σ2T Ip×p

Figure 1.5: The eﬀect of L´evy thinning with data generated from a Gaussian
model of the form X (cid:12)
(cid:1), as described in Section 1.3.1.
The outer circle depicts the distribution of θ conditional on the color Y : blue
points all have θ ∝ (cos(0.75 π/2), sin(0.75 π/2)), whereas the red points have
θ ∝ (cos(ω π/2), sin(ω π/2)) where ω is uniform between 0 and 2/3. Inside this
circle, we see 3 clusters of points generated with T = 0.1, 0.4, and 1, along with
logistic regression decision boundaries obtained from each cluster. The dashed line
shows the Bayes decision boundary separating the blue and red points, which is the
same for all T (Theorem 1.2). Note that the logistic regression boundaries learned
from data with diﬀerent T are not the same. This issue arises because the Bayes
decision boundary is curved, and the best linear approximation to a curved Bayes
boundary changes with T .

for some convex function ρ(·; x, y).

The proof of Theorem 1.4 is provided in the appendix. Here, we begin by
establishing notation that lets us write down an expression for the limiting loss
ρ. First, note that Assumption 1.1 implicitly requires that the process (At) has
ﬁnite moments. Thus, by the L´evy–It¯o decomposition, we can uniquely write this
process as

At = bt + Wt + Nt,

(1.20)

where b ∈ Rp, Wt is a Wiener process with covariance Σ, and Nt is a compound
Poisson process which, by hypothesis, has a ﬁnite jump intensity.

Now, by an argument analogous to that in the proof of Theorem 1.1, we see that

16

Data Augmentation via L´evy Processes

the joint distribution of WT and NT conditional on AT does not depend on θ. Thus,
we can deﬁne the following quantities without ambiguity:

(cid:12)
µT (x) = bT + E (cid:2)WT
(cid:12) AT = x(cid:3) ,
λT (x) = E (cid:2)number of jumps in (At) for t ∈ [0, T ] (cid:12)
νT (z; x) = lim
t→0

(cid:12) Nt (cid:54)= 0, AT = x(cid:3) .

P (cid:2)Nt = z (cid:12)

(cid:12) AT = x(cid:3) ,

(1.21)

(1.22)

(1.23)

More prosaically, νT (·; x) can be described as the distribution of the ﬁrst jump
of (cid:101)Nt, a thinned version of the jump process Nt. In the degenerate case where
P (cid:2)NT = 0 (cid:12)
(cid:12) AT = x(cid:3) = 0, we set νT (·; x) to be a point mass at z = 0.

Given this notation, we can write the eﬀective loss function ρ for strong thinning

as

ρ (β; x, y) = −µT (x) · β(y) +

β(k)(cid:62)Σβ(k)

(1.24)

T
2

1
K

K
(cid:88)

k=1

(cid:90)

+ λT (x)

(cid:96) (β; z, y) dνT (z; x),

provided we require without loss of generality that (cid:80)K
k=1 β(k) = 0. In other words,
the limiting loss can be described entirely in terms of the distribution of the ﬁrst
jump of (cid:101)Nt, and continuous part Wt of the L´evy process. The reason for this
phenomenon is that, in the strong thinning limit, the pseudo-examples (cid:101)X ∼ AαT
can all be characterized using either 0 or 1 jumps.

Aggregating over all the training examples, we can equivalently write this strong

thinning loss as

(cid:16)

β; X (i), Y (i)(cid:17)

ρ

=

n
(cid:88)

i=1

1
2T

n
(cid:88)

i=1

γ−1
Y (i)

(cid:13)
(cid:13)
(cid:13)γY (i) µT

X (i)(cid:17)
(cid:16)

− T Σβ(Y (i))(cid:13)
2
(cid:13)
(cid:13)

Σ−1

λT (X (i))

(cid:90)

(cid:16)

β; z, Y (i)(cid:17)

(cid:96)

dνT (z; X (i)),

(1.25)

+

n
(cid:88)

i=1

2 terms that do not depend on β. Here, 1

up to (cid:107)µT (cid:107)2
2 v(cid:48)Σ−1v corresponds
to the Gaussian log-likelihood with covariance Σ (up to constants), and γy =
K (cid:12)
(cid:12) /n measures the over-representation of class y relative to other
(cid:12)
classes.

(cid:8)i : Y (i) = y(cid:9)(cid:12)

Σ−1 = 1

2 (cid:107)v(cid:107)2

(cid:13)
(cid:13)

(cid:80)n

(cid:13)µT (X (i)) − T Σβ(Y (i))(cid:13)

In the case where we have the same number of training examples from
each class (and so γy = 1 for all y = 1, ..., K), the strong thinning loss
can be understood in terms of a generative model. The ﬁrst term, namely
1
, is the loss function for linear classiﬁcation
2T
in a Gaussian mixture with observations µT (X (i)), while the second term is the
logistic loss obtained by classifying single jumps. Thus, strong thinning is eﬀec-
tively seeking the best linear classiﬁer for a generative model that is a mixture of
Gaussians and single jumps.

2
(cid:13)
(cid:13)

Σ−1

i=1

In the pure jump case (Σ = 0), we also note that strong thinning is closely related

1.3 Examples

17

1.3 Examples

to naive Bayes classiﬁcation. In fact, if the jump measure of Nt has a ﬁnite number
of atoms that are all linearly independent, then we can verify that the parameters
ˆβ0+ learned by strong thinning are equivalent to those learned via naive Bayes,
although the calibration constants c(k) may be diﬀerent.

At a high level, by elucidating the generative model that strong thinning pushes
us towards, these results can help us better understand the behavior of L´evy
thinning for intermediate value of α, e.g., α = 1/2. They also suggest caution with
respect to calibration: For both the diﬀusion and jump terms, we saw above that
L´evy thinning gives helpful guidance for the angle of β(k), but does not in general
(cid:13)β(k)(cid:13)
elegantly account for signal strength (cid:13)
(cid:13)2 or relative class weights. Thus, we
recommend re-calibrating the class decision boundaries obtained by L´evy thinning,
as in Algorithm 1.

So far, we have developed our theory of L´evy thinning using the Poisson topic
model as a motivating example, which corresponds to dropping out words
from a document. In this section, we present two models based on other L´evy
processes—multivariate Brownian motion (Section 1.3.1) and Gamma processes
(Section 1.3.2)— exploring the consequences of L´evy thinning.

1.3.1 Multivariate Brownian Motion

Consider a classiﬁcation problem where the input vector is the aggregation of
multiple noisy, independent measurements of some underlying object. For example,
in a biomedical application, we might want to predict a patient’s disease status
based on a set of biomarkers such as gene expression levels or brain activity.
A measurement is typically obtained through a noisy experiment involving an
microarray or fMRI, so multiple experiments might be performed and aggregated.
More formally, suppose that patient i has disease status Y (i) and expression level
µi ∈ Rd for d genes, with the distribution of µi diﬀerent for each disease status.
Given µi, suppose the t-th measurement for patient i is distributed as

Zi,t ∼ N(µi, Σ),

(1.26)

where Σ ∈ Rd×d is assumed to be a known, ﬁxed matrix. Let the observed input be
X (i) = (cid:80)Ti
t=1 Zi,t, the sum of the noisy measurements. If we could take inﬁnitely
many measurements (Ti → ∞), we would have X (i)/Ti → µi almost surely; that
is, we would observe gene expression noiselessly. For ﬁnitely many measurements,
X (i) is a noisy proxy for the unobserved µi.

We can model the process of accumulating measurements with a multivariate

18

Data Augmentation via L´evy Processes

Brownian motion (At):

At = tµ + Σ1/2Bt,

where Bt is a d-dimensional white Brownian motion.1 For integer values of t, At
represents the sum of the ﬁrst t measurements, but At is also deﬁned for fractional
values of t. The distribution of the features X at a given time T is thus

(1.27)

(1.28)

(1.29)

(1.30)

(1.31)

X | µ, T ∼ N(T µ, T Σ),

leading to density

f (t)
µ (x) =

exp (cid:2) 1

2 (x − tµ)(cid:62)(tΣ)−1(x − tµ)(cid:3)

(cid:20)

(2π)d/2 det(Σ)
t
2

(cid:21)
µ(cid:62)Σ−1µ

x(cid:62)Σ−1µ −

= exp

h(t)(x),

where

h(t)(x) =

exp (cid:2)− 1
2t x(cid:62)Σ−1x(cid:3)
(2π)d/2 det(Σ)1/2

.

We can recover the form of (1.6) by setting θ = Σ−1µ, a one-to-one mapping
provided Σ is positive-deﬁnite.

Thinning. The distribution of (cid:101)X = AαT given X = AT is that of a Brownian
bridge process with the following marginals:

(cid:101)X | X ∼ N (αX, α(1 − α)T Σ) .

In this case, “thinning” corresponds exactly to adding zero-mean, additive Gaus-
sian noise to the scaled features αX. Note that in this model, unlike in the Poisson
topic model, sampling (cid:101)X from X does require observing T —for example, knowing
how many observations were taken. The larger T is, the more noise we need to
inject to achieve the same downsampling ratio.

In the Poisson topic model, the features (Xi,1, . . . , Xi,d) were independent of
each other given the topic θi and expected length Ti. By contrast, in the Brownian
motion model the features are correlated (unless Σ is the identity matrix). This
serves to illustrate that independence or dependence of the features is irrelevant to
our general framework; what is important is that the increments Zt = At − At−1
are independent of each other, the key property of a L´evy process.

Assumption 1.2 requires that µ(cid:62)Σ−1µ is constant across topics; i.e., that the
true gene expression levels are equally sized in the Mahalanobis norm deﬁned
by Σ. Clearly, this assumption is overly stringent in real situations. Fortunately,
Assumption 1.2 is not required (see Remark 1.1) as long as T is observed—as it

1. By deﬁnition of Brownian motion, we have marginally that Bt ∼ N(0, tI).

1.3 Examples

19

must be anyway if we want to be able to carry out L´evy thinning.

Thinning X in this case is very similar to subsampling. Indeed, for integer values
of (cid:101)T , instead of formally carrying out L´evy thinning as detailed above, we could
simply resample (cid:101)T values of Zi,t without replacement, and add them together to
obtain (cid:101)X. If there are relatively few repeats, however, the resampling scheme can
(cid:1) pseudo-examples (e.g. 6 pseudo-examples if T = 4 and (cid:101)T = 2),
lead to only (cid:0)T
(cid:101)T
whereas the thinning approach leads to inﬁnitely many possible pseudo-examples
we can use to augment the regression. Moreover, if T = 4 then subsampling leaves
us with only four choices of α; there would be no way to thin using α = 0.1, for
instance.

1.3.2 Gamma Process

As another example, suppose again that we are predicting a patient’s disease status
based on repeated measurements of a biomarker such as gene expression or brain
activity. But now, instead of (or in addition to) the average signal, we want our
features to represent the variance or covariance of the signals across the diﬀerent
measurements.

Assume ﬁrst that the signals at diﬀerent genes or brain locations are independent;

that is, the t-th measurement for patient i and gene j has distribution

Zi,j,t ∼ N(µi,j, σ2

i,j).

(1.32)

Here, the variances σ2
subscript i, after T + 1 measurements we can compute

i,1, . . . , σ2

i = (σ2

i,d) parameterize the “topic.” Suppressing the

Xj,T =

(Zi,j,t − ¯Zi,j,T +1)2,

where

¯Zi,j,T +1 =

Zi,j,t. (1.33)

1
T + 1

T +1
(cid:88)

t=1

T +1
(cid:88)

t=1

Then Xj,T ∼ σ2
j χ2
scale parameter 2σ2
more and more observations (increasing T ), we will have XT /T → (σ2
almost surely.

T , which is a Gamma distribution with shape parameter T /2 and
j (there is no dependence on µi). Once again, as we accumulate
1, . . . , σ2
d)

We can embed Xj,T in a multivariate Gamma process with d independent

coordinates and scale parameters σ2
j :

(At)j ∼ Gamma(t/2, 2σ2

j ).

The density of At given σ2 is

f (t)
σ2 (x) =

d
(cid:89)

j=1

j

e−xj /2σ2
xt/2−1
j
Γ(t/2)2t/2σ2(t/2)


j

= exp

−

xj/2σ2

j − (t/2)

log σ2
j


 h(t)(x),

d
(cid:88)

j=1

d
(cid:88)

j=1

(1.34)

(1.35)

20

Data Augmentation via L´evy Processes

where

h(t)(x) =

(cid:81)

j xt/2−1
j
Γ(t/2)d2dt/2

.

We can recover the form of (1.6) by setting θj = −1/2σ2

j , a one-to-one mapping.

Thinning. Because (cid:101)Xj ∼ Gamma(αT /2, 2σ2
Xj − (cid:101)Xj ∼ Gamma((1 − α)T /2, 2σ2

j ), we have

j ) is independent of the increment

(cid:101)Xj
Xj

| Xj ∼ Beta (αT /2, (1 − α)T /2) .

In other words, we create a noisy (cid:101)X by generating for each coordinate an indepen-
dent multiplicative noise factor

mj ∼ Beta (αT, (1 − α)T )

and setting (cid:101)Xj = mjXj. Once again, we can downsample without knowing σ2
j , but
we do need to observe T . Assumption 1.2 would require that (cid:81)
j is identical for
all topics. This is an unrealistic assumption, but once again it is unnecessary as
long as we observe T .

j σ2

General covariance. More generally, the signals at diﬀerent brain locations, or
expressions for diﬀerent genes, will typically be correlated with each other, and these
correlations could be important predictors. To model this, let the measurements be
distributed as:

Zi,t ∼ N(µi, Σi),

where Σ represents the unknown “topic”—some covariance matrix that is charac-
teristic of a certain subcategory of a disease status.

After observing T + 1 observations we can construct the matrix-valued features:

XT =

(Zi,t − ¯Zi,T +1)(Zi,t − ¯Zi,T +1)(cid:62).

T +1
(cid:88)

t=1

Now XT has a Wishart distribution: XT ∼ Wishd(Σ, T ). When T ≥ d, the density
of At given Σ is

f (t)
Σ (x) = exp

−

tr(Σ−1x) −

log det(Σ)

h(t)(x),

(1.41)

(cid:26)

1
2

(cid:27)

t
2

(1.36)

(1.37)

(1.38)

(1.39)

(1.40)

1.4 Simulation Experiments

where

21

(1.42)

(1.43)

h(t)(x) =

td
2 det(x)

t−d−2

2 Γd

(cid:18)
2

(cid:19)(cid:19)−1

,

(cid:18) t
2

Γd

(cid:19)

(cid:18) t
2

d(d−1)
4

= π

d
(cid:89)

j=1

(cid:18) t
2

Γ

+

1 − j
2

(cid:19)

,

supported on positive-deﬁnite symmetric matrices. If X = AT and αT ≥ d as well,
we can sample a “thinned” observation (cid:101)X from density proportional to

h(αT )(˜x)h(T −αT )(X − ˜x) ∝ det(˜x)

2+d−αT
2

det(X − ˜x)

2+d−(1−α)T
2

,

(1.44)

or after the aﬃne change of variables (cid:101)X = X 1/2M X 1/2, we sample M from density
proportional to det(m)
, a matrix beta distribution.
Here, M may be interpreted as matrix-valued multiplicative noise.

det(Id − m)

2+d−(1−α)t
2

2+d−αT
2

1.4 Simulation Experiments

In this section, we perform several simulations to illustrate the utility of L´evy
thinning. In particular, we will highlight the modularity between L´evy thinning
(which provides pseudo-examples) and the discriminative learner (which ingests
these pseudo-examples). We treat the discriminative learner as a black box, com-
plete with its own internal cross-validation scheme that optimizes accuracy on
pseudo-examples. Nonetheless, we show that accuracy on the original examples
improves when we train on thinned examples.

More speciﬁcally, given a set of training examples {(X, Y )}, we ﬁrst use L´evy
thinning to generate a set of pseudo-examples {( (cid:101)X, Y )}. Then we feed these
examples to the R function cv.glmnet to learn a linear classiﬁer on these pseudo-
examples (Friedman et al., 2010). We emphasize that cv.glmnet seeks to choose
its regularization parameter λ to maximize its accuracy on the pseudo-examples
( (cid:101)X, Y ) rather than on the original data (X, Y ). Thus, we are using cross-validation
as a black box instead of trying to adapt the procedure to the context of L´evy
thinning. In principle, we might be concerned that cross-validating on the pseudo-
examples would yield a highly suboptimal choice of λ, but our experiments will
show that the procedure in fact works quite well.

The two extremes of the path correspond to naive Bayes generative modeling at
one end (α = 0), and plain ridge-regularized logistic regression at the other (α = 1).
All methods were calibrated on the training data as follows: Given original weight
vectors ˆβ, we ﬁrst compute un-calibrated predictions ˆµ = X ˆβ for the log-odds of
P (cid:2)Y = 1 (cid:12)
(cid:12) X(cid:3), and then run a second univariate logistic regression Y ∼ ˆµ to adjust
both the intercept and the magnitude of the original coeﬃcients. Moreover, when us-
ing cross-validation on pseudo-examples ( (cid:101)X, Y ), we ensure that all pseudo-examples
induced by a given example (X, Y ) are in the same cross-validation fold. Code for

22

Data Augmentation via L´evy Processes

Figure 1.6: Performance of L´evy thinning with cross-validated ridge-regularized
logistic regression, on a random Gaussian design described in (1.45). The curves
depict the relationship between thinning α and classiﬁcation error as the number of
training examples grows: n = 30, 50, 75, 100, 150, 200, 400, and 600. We see that
naive Bayes improves over ridge logistic regression in very small samples, while in
moderately small samples L´evy thinning does better than either end of the path.

reproducing our results is available at https://github.com/swager/levythin.

Gaussian example. We generate data from the following hierarchical model:

Y ∼ Binomial (0.5) , µ (cid:12)

(cid:12) Y ∼ LY , X (cid:12)

(cid:12) µ ∼ N (µ, Id×d) ,

(1.45)

1

, ..., µ(Y )

where µ, X ∈ Rd and d = 100. The distribution LY associated with each label
Y consists of 10 atoms µ(Y )
10 . These atoms themselves are all randomly
generated such that their ﬁrst 20 coordinates are independent draws of 1.1 T4 where
T4 follows Student’s t-distribution with 4 degrees of freedom; meanwhile, the last
80 coordinates of µ are all 0. The results in Figure 1.6 are marginalized over the
randomness in LY ; i.e., diﬀerent simulation realizations have diﬀerent conditional
laws for µ given Y . Figure 1.6 shows the results.

Poisson example. We generate data from the following hierarchical model:

Y ∼ Binomial (0.5) , θ (cid:12)

(cid:12) Y ∼ LY , Xj

(cid:12)
(cid:12) θ ∼ Pois

1000

,

(1.46)

(cid:32)

(cid:33)

eθj
j=1 eθj

(cid:80)d

1.4 Simulation Experiments

23

Figure 1.7: Performance of L´evy thinning with cross-validated ridge-regularized
logistic regression, on a random Poisson design described in (1.46). The curves
depict the relationship between thinning α and classiﬁcation accuracy for n =
30, 50, 100, 150, 200, 400, 800, and 1600. Here, aggressive L´evy thinning with
small but non-zero α does substantially better than naive Bayes (α = 0) as soon as
n is moderately large.

where θ ∈ Rd, X ∈ Nd, and d = 500. This time, however, LY is deterministic: If
Y = 0, then θ is just 7 ones followed by 493 zeros, whereas





θ (cid:12)
(cid:12) Y = 1 ∼

0, ..., 0
(cid:124) (cid:123)(cid:122) (cid:125)
7

(cid:12)
(cid:12) τ, ..., τ
(cid:124) (cid:123)(cid:122) (cid:125)
7

(cid:12)
(cid:12) 0, ..., 0
(cid:124) (cid:123)(cid:122) (cid:125)
486

 , with τ ∼ Exp(3).

This generative model was also used in simulations by Wager et al. (2014); the
diﬀerence is that they applied thinning to plain logistic regression, whereas here
we verify that L´evy thinning is also helpful when paired with cross-validated ridge
logistic regression. Figure 1.7 shows the results.

These experiments suggest that it is reasonable to pair L´evy thinning with a well-
tuned black box learner on the pseudo-examples ( (cid:101)X, Y ), without worrying about
potential interactions between L´evy thinning and the tuning of the discriminative
model.

24

Data Augmentation via L´evy Processes

1.5 Discussion

In this chapter, we have explored a general framework for performing data augmen-
tation: apply L´evy thinning and train a discriminative classiﬁer on the resulting
pseudo-examples. The exact thinning scheme reﬂects our generative modeling as-
sumptions. We emphasize that the generative assumptions are non-parametric and
of a structural nature; in particular, we never ﬁt an actual generative model, but
rather encode the generative hints implicitly in the pseudo-examples.

A key result is that under the generative assumptions, thinning preserves the
Bayes decision boundary, which suggests that a well-speciﬁed classiﬁer incurs
no asymptotic bias. Similarly, we would expect that a misspeciﬁed but powerful
classiﬁer should incur little bias. We showed that in limit of maximum thinning,
the resulting procedure corresponds to ﬁtting a generative model. The exact bias-
variance trade-oﬀ for moderate levels of thinning is an interesting subject for further
study.

While L´evy processes provide a general framework for thinning examples, we
recognize that there are many other forms of coarsening that could lead to the
same intuitions. For instance, suppose X | θ is a Markov process over words in
a document. We might expect that short contiguous subsequences of X could
serve as good pseudo-examples. More broadly, there are many forms of data
augmentation that do not have the intuition of coarsening an input. For example,
rotating or shearing an image to generate pseudo-images appeals to other forms
of transformational invariance. It would be enlightening to establish a generative
framework in which data augmentation with these other forms of invariance arise
naturally.

To establish the desired result, we show that for a single training example (X, Y ),
the following limit is well-deﬁned for any β ∈ Rp×K:

ρ (β; X, Y ) = lim
α→0

(cid:16)

(cid:101)E

(cid:16)

(cid:104)
(cid:96)

1
α

= −β(Y ) · X + lim
α→0

β; (cid:101)X, Y
(cid:34)

(cid:32)

1
α

(cid:101)E

log

(cid:17)(cid:105)

(cid:17)

− log (K)

(cid:33)(cid:35)

eβ(k)· (cid:101)X

,

1
K

K
(cid:88)

k=1

(1.47)

where on the second line we wrote down the logistic loss explicitly and exploited
linearity of the term involving Y as in Wager et al. (2013). Here (cid:101)E denotes
expectation with respect to the thinning process and reﬂects the B → ∞ limit.
Because (cid:96) is convex, ρ must also be convex; and by equicontinuity ˆβ(α) must also
converge to its minimizer.

Our argument relies on the decomposition At = bt+Wt+Nt from (1.20). Without
loss of generality, we can generate the pseudo-features (cid:101)X as (cid:101)X = bt + (cid:102)WαT + (cid:101)NαT ,

1.6 Appendix: Proof of Theorem 1.4

1.6 Appendix: Proof of Theorem 1.4

25

where (cid:102)WαT and (cid:101)NαT have the same marginal distribution as WαT and NαT . Given
this notation,

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:33)(cid:35)

(cid:33)

(cid:34)

(cid:32)

log

1
K

1
α

(cid:101)E

(cid:34)

log

=

1
α

(cid:101)E

+

1
α

(cid:101)E

K
(cid:88)

k=1
(cid:32)

1
K

(cid:34)

log

K
(cid:88)

k=1
(cid:32)

1
K

K
(cid:88)

k=1

eβ(k)·(αbT +(cid:102)WαT )

(cid:12)
(cid:12) (cid:101)NαT = 0

P

(cid:104)

(cid:105)
(cid:101)NαT = 0

(cid:35)

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:33)

(cid:35)

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

P

(cid:104)

(cid:105)
(cid:101)NαT (cid:54)= 0

.

We now characterize these terms individually. First, because Nt has a ﬁnite jump
intensity, we can verify that, almost surely,

lim
α→0

1
α

P

(cid:104)

(cid:105)
(cid:101)NαT (cid:54)= 0

= λT (X),

where λT (X) is as deﬁned in (1.22). Next, because (cid:102)WαT concentrates at 0 as α → 0,
we can check that
(cid:32)

(cid:33)

(cid:34)

(cid:35)

lim
α→0

(cid:101)E

log

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

1
K
(cid:34)

K
(cid:88)

k=1
(cid:32)

(cid:33)

(cid:35)

eβ(k)· (cid:101)NαT

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

1
K

K
(cid:88)

k=1

(cid:33)

= lim
α→0

(cid:101)E

log

(cid:32)

(cid:90)

=

log

1
K

K
(cid:88)

k=1

eβ(k)·z

dνT (z; X)

where νT (·; X) (1.23) is the ﬁrst jump measure conditional on X.

Meanwhile, in order to control the remaining term, we note that we can write

(cid:102)WαT = α(cid:102)WT + (cid:101)BαT ,

where (cid:101)Bt is a Brownian bridge from 0 to T that is independent from (cid:102)WT . Thus,
noting that limα→0 P
= 1, we ﬁnd that

(cid:105)
(cid:101)NαT = 0

(cid:104)

eβ(k)·(αbT +(cid:102)WαT )

(cid:33)

(cid:35)

(cid:12)
(cid:12) (cid:101)NαT = 0

P

(cid:104)

(cid:105)
(cid:101)NαT = 0

lim
α→0

1
α

(cid:101)E

(cid:34)

(cid:32)

log

1
K

K
(cid:88)

k=1
(cid:34)

= lim
α→0

1
α

(cid:101)E

log

eβ(k)·(α(bT +(cid:102)WT ))+ (cid:101)BαT

(cid:33)(cid:35)

(cid:33)

(cid:32)

1
K
(cid:32)

K
(cid:88)

k=1
K
(cid:88)

k=1

1
K

T
2

= ¯β · µT (X) +

β(k)(cid:62)Σ β(k) − ¯β(cid:62)Σ ¯β

,

where µT (X) is as deﬁned in (1.21) and ¯β = K −1 (cid:80)K
k=1 β(k). The last equality
follows from Taylor expanding the log((cid:80) exp) term and noting that 3rd- and higher-

26

Data Augmentation via L´evy Processes

1.7 References

order terms vanish in the limit.

that ¯β = 0, we ﬁnally conclude that

Bringing back the linear term form (1.47), and assuming without loss of generality

ρ (β; X, Y ) = −β(Y ) · X +

β(k)(cid:62)Σ β(k)

T
2

1
K

K
(cid:88)

k=1

+ λT (X)

log

(cid:32)

1
K

K
(cid:88)

k=1

(cid:33)

eβ(k)·z

dνT (z; X)

= −β(Y ) · µT (X) +

β(k)(cid:62)Σ β(k)

(cid:90)

(cid:90)

T
2

K
(cid:88)

1
K
(cid:32) K
(cid:88)

k=1

k=1

(cid:33)

eβ(k)·z

+ λT (X)

−β(Y ) · z + log

− log(K) dνT (z; X),

where for the second equality we used the fact that X = µT (X)+λT (X) (cid:82) z dνT (z; X).
Finally, this expression only diﬀers from (1.24) by terms that do not include β; thus,
they yield the same minimizer.

Y. S. Abu-Mostafa. Learning from hints in neural networks. Journal of Complexity,

6(2):192–198, 1990.

J. Ba and B. Frey. Adaptive dropout for training deep neural networks. In Advances

in Neural Information Processing Systems (NIPS), pages 3084–3092, 2013.

P. Baldi and P. Sadowski. The dropout learning algorithm. Artiﬁcial intelligence,

210:78–122, 2014.

C. M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural

computation, 7(1):108–116, 1995.

G. Bouchard and B. Triggs. The trade-oﬀ between generative and discriminative
classiﬁers. In International Conference on Computational Statistics, pages 721–
728, 2004.

B. Efron. The eﬃciency of logistic regression compared to normal discriminant
analysis. Journal of the American Statistical Association (JASA), 70(352):892–
898, 1975.

J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized
linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22,
2010.

A. Globerson and S. Roweis. Nightmare at test time: robust learning by feature
deletion. In International Conference on Machine Learning (ICML), pages 353–
360, 2006.

I. Goodfellow, D. Warde-farley, M. Mirza, A. Courville, and Y. Bengio. Maxout
In International Conference on Machine Learning (ICML), pages

networks.
1319–1327, 2013.

D. P. Helmbold and P. M. Long. On the inductive bias of dropout. Journal of

1.7 References

27

Machine Learning Research (JMLR), 16:3403–3454, 2015.

J. Josse and S. Wager. Stable autoencoding: A ﬂexible framework for regularized

low-rank matrix estimation. arXiv preprint arXiv:1410.8275, 2014.

A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep
In Advances in Neural Information Processing

convolutional neural networks.
Systems (NIPS), pages 1097–1105, 2012.

J. A. Lasserre, C. M. Bishop, and T. P. Minka. Principled hybrids of generative and
discriminative models. In Computer Vision and Pattern Recognition (CVPR),
pages 87–94, 2006.

P. Liang and M. I. Jordan. An asymptotic analysis of generative, discriminative, and
pseudolikelihood estimators. In International Conference on Machine Learning
(ICML), pages 584–591, 2008.

D. McAllester. A PAC-Bayesian tutorial with a dropout bound. arXiv preprint

arXiv:1307.2118, 2013.

A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Gener-
ative/discriminative training for clustering and classiﬁcation. In Association for
the Advancement of Artiﬁcial Intelligence (AAAI), 2006.

A. Y. Ng and M. I. Jordan. On discriminative vs. generative classiﬁers: A compar-
ison of logistic regression and naive Bayes. In Advances in Neural Information
Processing Systems (NIPS), 2002.

R. Raina, Y. Shen, A. Ng, and A. McCallum. Classiﬁcation with hybrid gen-
In Advances in Neural Information Processing

erative/discriminative models.
Systems (NIPS), 2004.

Y. D. Rubinstein and T. Hastie. Discriminative vs informative learning.

In
International Conference on Knowledge Discovery and Data Mining (KDD),
volume 5, pages 49–53, 1997.

S. P. Sch¨olkopf, P. Simard, V. Vapnik, and A. Smola. Improving the accuracy and
speed of support vector machines. In Advances in Neural Information Processing
Systems (NIPS), pages 375–381, 1997.

P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. Transformation
Invariance in Pattern Recognition—Tangent Distance and Tangent Propagation.
Neural networks: Tricks of the trade Springer, 1998.

N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. Journal
of Machine Learning Research (JMLR), 15(1):1929–1958, 2014.

L. van der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger. Learning with
marginalized corrupted features. In International Conference on Machine Learn-
ing (ICML), pages 410–418, 2013.

S. Wager, S. I. Wang, and P. Liang. Dropout training as adaptive regularization.

In Advances in Neural Information Processing Systems (NIPS), 2013.

S. Wager, W. Fithian, S. I. Wang, and P. Liang. Altitude training: Strong bounds
for single-layer dropout. In Advances in Neural Information Processing Systems
(NIPS), 2014.

L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Regularization of neural
networks using dropconnect. In International Conference on Machine Learning
(ICML), pages 1058–1066, 2013.

S. I. Wang and C. Manning. Fast dropout training. In International Conference

28

Data Augmentation via L´evy Processes

on Machine Learning (ICML), pages 118–126, 2013.

S. I. Wang, M. Wang, S. Wager, P. Liang, and C. Manning. Feature noising
for log-linear structured prediction. In Empirical Methods in Natural Language
Processing (EMNLP), 2013.

6
1
0
2
 
r
a

M
 
1
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
0
4
3
6
0
.
3
0
6
1
:
v
i
X
r
a

1

Data Augmentation via L´evy Processes

Stefan Wager
Stanford University
Stanford, USA

William Fithian
University of California, Berkeley
Berkeley, USA

Percy Liang
Stanford University
Stanford, USA

swager@stanford.edu

wfithian@berkeley.edu

pliang@cs.stanford.edu

If a document is about travel, we may expect that short snippets of the document
should also be about travel. We introduce a general framework for incorporating
these types of invariances into a discriminative classiﬁer. The framework imagines
data as being drawn from a slice of a L´evy process. If we slice the L´evy process at
an earlier point in time, we obtain additional pseudo-examples, which can be used
to train the classiﬁer. We show that this scheme has two desirable properties: it
preserves the Bayes decision boundary, and it is equivalent to ﬁtting a generative
model in the limit where we rewind time back to 0. Our construction captures popular
schemes such as Gaussian feature noising and dropout training, as well as admitting
new generalizations.

1.1 Introduction

Black-box discriminative classiﬁers such as logistic regression, neural networks, and
SVMs are the go-to solution in machine learning: they are simple to apply and
often perform well. However, an expert may have additional knowledge to exploit,
often taking the form of a certain family of transformations that should usually
leave labels ﬁxed. For example, in object recognition, an image of a cat rotated,
translated, and peppered with a small amount of noise is probably still a cat.
Likewise, in document classiﬁcation, the ﬁrst paragraph of an article about travel

2

Data Augmentation via L´evy Processes

Figure 1.1: Two examples of transforming an original input X into a noisy, less
informative input (cid:101)X. The new inputs clearly have the same label but contain less
information and thus are harder to classify.

is most likely still about travel. In both cases, the “expert knowledge” amounts
to a belief that a certain transform of the features should generally not aﬀect an
example’s label.

One popular strategy for encoding such a belief is data augmentation: generat-
ing additional pseudo-examples or “hints” by applying label-invariant transforma-
tions to training examples’ features (Abu-Mostafa, 1990; Sch¨olkopf et al., 1997;
Simard et al., 1998). That is, each example (X (i), Y (i)) is replaced by many pairs
( (cid:101)X (i,b), Y (i)) for b = 1, . . . , B, where each (cid:101)X (i,b) is a transformed version of X (i).
This strategy is simple and modular: after generating the pseudo-examples, we can
simply apply any supervised learning algorithm to the augmented dataset. Fig-
ure 1.1 illustrates two examples of this approach, an image transformed to a noisy
image and a text caption, transformed by deleting words.

Dropout training (Srivastava et al., 2014) is an instance of data augmentation
that, when applied to an input feature vector, zeros out a subset of the features ran-
domly. Intuitively, dropout implies a certain amount of signal redundancy across
features—that an input with about half the features masked should usually be
classiﬁed the same way as a fully-observed input. In the setting of document clas-

1.1 Introduction

3

siﬁcation, dropout can be seen as creating pseudo-examples by randomly omitting
some information (i.e., words) from each training example. Building on this in-
terpretation, Wager et al. (2014) show that learning with such artiﬁcially diﬃcult
examples can substantially improve the generalization performance of a classiﬁer.
To study dropout, Wager et al. (2014) assume that documents can be summarized
as Poisson word counts. Speciﬁcally, assume that each document has an underlying
topic associated with a word frequency distribution π on the p-dimensional simplex
and an expected length T ≥ 0, and that, given π and T , the word counts Xj
(cid:12)
are independently generated as Xj
(cid:12) T, π ∼ Pois(T πj). The analysis of Wager
et al. (2014) then builds on a duality between dropout and the above generative
model. Consider the example given in Figure 1.1, where dropout creates pseudo-
documents (cid:101)X by deleting half the words at random from the original document
X. As explained in detail in Section 1.2.1, if X itself is drawn from the above
Poisson model, then the dropout pseudo-examples (cid:101)X are marginally distributed
(cid:12)
as (cid:101)Xj
(cid:12) T, π ∼ Pois(0.5 T πj). Thus, in the context of this Poisson generative
model, dropout enables us to create new, shorter pseudo-examples that preserve
the generative structure of the problem.

The above interpretation of dropout raises the following question: if feature
deletion is a natural way to create information-poor pseudo-examples for document
classiﬁcation, are there natural analogous feature noising schemes that can be
applied to other problems? In this chapter, we seek to address this question, and
study a more general family of data augmentation methods generalizing dropout,
based on L´evy processes: We propose an abstract L´evy thinning scheme that reduces
to dropout in the Poisson generative model considered by Wager et al. (2014). Our
framework further suggests new methods for feature noising such as Gamma noising
based on alternative generative models, all while allowing for a uniﬁed theoretical
analysis.

In the above discussion,
From generative modeling to data augmentation.
we treated the expected document length T as ﬁxed. More generally, we could
imagine the document as growing in length over time, with the observed document
X merely a “snapshot” of what the document looks like at time T . Formally, we
(cid:12)
(cid:12) π ∼
can imagine a latent Poisson process (At)t≥0, with ﬁxed-t marginals (At)j
Pois(t πj), and set X = AT . In this notation, dropout amounts to “rewinding”
the process At to obtain short pseudo-examples. By setting (cid:101)X = AαT , we have
P[ (cid:101)X = ˜x (cid:12)

(cid:12) AT = x], for thinning parameter α ∈ (0, 1).

(cid:12) X = x] = P[AαT = ˜x (cid:12)

The main result of this chapter is that the analytic tools developed by Wager
et al. (2014) are not restricted to the case where (At) is a Poisson process, and in
fact hold whenever (At) is a L´evy process. In other words, their analysis applies
to any classiﬁcation problem where the features X can be understood as time-T
snapshots of a process (At), i.e., X = AT .

Recall that a L´evy process (At)t≥0 is a stochastic process with A0 = 0 that
has independent and stationary increments: {Ati − Ati−1} are independent for
d= At−s for and s < t. Common examples
0 = t0 < t1 < t2 < · · · , and At − As

4

Data Augmentation via L´evy Processes

Figure 1.2: Graphical model depicting our generative assumptions; note that we
are not ﬁtting this generative model. Given class Y , we draw a topic θ, which
governs the parameters of the L´evy process (At). We slice at time T to get the
original input X = AT and at an earlier time ˜T to get the thinned or noised input
˜X = A ˜T . We show that given X, we can sample ˜X without knowledge of θ.

of L´evy processes include Brownian motion and Poisson processes.

In any such L´evy setup, we show that it is possible to devise an analogue to
dropout that creates pseudo-examples by rewinding the process back to some earlier
time (cid:101)T ≤ T . Our generative model is depicted in Figure 1.2: (At), the information
relevant to classifying Y , is governed by a latent topic θ ∈ Rp. L´evy thinning then
seeks to rewind (At)—importantly as we shall see, without having access to θ.

We should think of (At) as representing an ever-accumulating amount of infor-
mation concerning the topic θ: In the case of document classiﬁcation, (At) are the
word counts associated with a document that grows longer as t increases. In other
examples that we discuss in Section 1.3, At will represent the sum of t independent
noisy sensor readings. The independence of increments property assures that as we
progress in time, we are always obtaining new information. The stopping time T
thus represents the information content in input X about topic θ. L´evy thinning
seeks to improve classiﬁcation accuracy by turning a few information-rich examples
X into many information-poor examples (cid:101)X.

We emphasize that, although our approach uses generative modeling to motivate
a data augmentation scheme, we do not in fact ﬁt a generative model. This
presents a contrast to the prevailing practice: two classical approaches to multiclass
classiﬁcation are to either directly train a discriminative model by running, e.g.,
multiclass logistic regression on the n original training examples; or, at the other
extreme, to specify and ﬁt a simple parametric version of the above generative
model, e.g., naive Bayes, and then use Bayes’ rule for classiﬁcation. It is well
known that the latter approach is usually more eﬃcient if it has access to a
correctly speciﬁed generative model, but may be badly biased in case of model
misspeciﬁcation (Efron, 1975; Ng and Jordan, 2002; Liang and Jordan, 2008). Here,
we ﬁrst seek to devise a noising scheme X → (cid:101)X and then to train a discriminative
model on the pseudo-examples ( (cid:101)X, Y ) instead of the original examples (X, Y ). Note

1.1 Introduction

5

Figure 1.3: We model each input X as a slice of a L´evy process at time T .
We generate noised examples (cid:101)X by “stepping back in time” to ˜T . Note that the
examples of the two classes are closer together now, thus forcing the classiﬁer to
work harder.

that even if the generative model is incorrect, this approach will incur limited bias as
long as the noising scheme roughly preserves class boundaries — for example, even
if the Poisson document model is incorrect, we may still be justiﬁed in classifying a
subsampled travel document as a travel document. As a result, this approach can
take advantage of an abstract generative structure while remaining more robust to
model misspeciﬁcation than parametric generative modeling.

Overview of results. We consider the multiclass classiﬁcation setting where we
seek to estimate a mapping from input X to class label Y . We imagine that each
X is generated by a mixture of L´evy process, where we ﬁrst draw a random topic θ
given the class Y , and then run a L´evy process (At) depending on θ to time T . In
order to train a classiﬁer, we pick a thinning parameter α ∈ (0, 1), and then create
(cid:12)
pseudo examples by rewinding the original X back to time α T , i.e., (cid:101)X ∼ AαT
(cid:12) AT .
We show three main results in this chapter. Our ﬁrst result is that we can generate
such pseudo-examples (cid:101)X without knowledge of the parameters θ governing the
generative L´evy process. In other words, while our method posits the existence of
a generative model, our algorithm does not actually need to estimate it. Instead,
it enables us to give hints about a potentially complex generative structure to a
discriminative model such as logistic regression.

Second, under assumptions that our generative model is correct, we show that

6

Data Augmentation via L´evy Processes

feature noising preserves the Bayes decision boundary: P[Y | X = x] = P[Y | (cid:101)X =
x]. This means that feature noising does not introduce any bias in the inﬁnite data
limit.

Third, we consider the limit of rewinding to the beginning of time (α → 0). Here,
we establish conditions given which, even with ﬁnite data, the decision boundary
obtained by ﬁtting a linear classiﬁer on the pseudo-examples is equivalent to that
induced by a simpliﬁed generative model. When this latter result holds, we can
interpret α-thinning as providing a semi-generative regularization path for logistic
regression, with a simple generative procedure at one end and unregularized logistic
regression at the other.

Related work. The trade-oﬀ between generative models and discriminative
models has been explored extensively. Rubinstein and Hastie (1997) empirically
compare discriminative and generative classiﬁers models with respect to bias and
variance, Efron (1975) and Ng and Jordan (2002) provide a more formal discussion
of the bias-variance trade-oﬀ between logistic regression and naive Bayes. Liang
and Jordan (2008) perform an asymptotic analysis for general exponential families.
A number of papers study hybrid loss functions that combine both a joint and
conditional likelihood (Raina et al., 2004; Bouchard and Triggs, 2004; Lasserre et al.,
2006; McCallum et al., 2006; Liang and Jordan, 2008). The data augmentation
approach we advocate in this chapter is fundamentally diﬀerent, in that we are
merely using the structural assumptions implied by the generative models to
generate more data, and are not explicitly ﬁtting a full generative model.

The present work was initially motivated by understanding dropout training
(Srivastava et al., 2014), which was introduced in the context of regularizing
deep neural networks, and has had much empirical success (Ba and Frey, 2013;
Goodfellow et al., 2013; Krizhevsky et al., 2012; Wan et al., 2013). Many of the
regularization beneﬁts of dropout can be found in logistic regression and other
single-layer models, where it is also known as “blankout noise” (Globerson and
Roweis, 2006; van der Maaten et al., 2013) and has been successful in natural
language tasks such as document classiﬁcation and named entity recognition (Wager
et al., 2013; Wang and Manning, 2013; Wang et al., 2013). There are a number of
theoretical analyses of dropout: using PAC-Bayes framework (McAllester, 2013),
comparing dropout to “altitude training” (Wager et al., 2014), and interpreting
dropout as a form of adaptive regularization (Baldi and Sadowski, 2014; Bishop,
1995; Helmbold and Long, 2015; Josse and Wager, 2014; Wager et al., 2013).

1.2 L´evy Thinning

We begin by brieﬂy reviewing the results of Wager et al. (2014), who study dropout
training for document classiﬁcation from the perspective of thinning documents
(Section 1.2.1). Then, in Section 1.2.2, we generalize these results to the setting of

1.2 L´evy Thinning

7

generic L´evy generative models.

1.2.1 Motivating Example: Thinning Poisson Documents

Suppose we want to classify documents according to their subject, e.g., sports,
politics, or travel. As discussed in the introduction, common sense intuition about
the nature of documents suggests that a short snippet of a sports document
should also be classiﬁed as a sports document. If so, we can generate many new
training examples by cutting up the original documents in our dataset into shorter
subdocuments and labeling each subdocument with the same label as the original
document it came from. By training a classiﬁer on all of the pseudo-examples we
generate in this way, we should be able to obtain a better classiﬁer.

In order to formalize this intuition, we can represent a document as a sequence
of words from a dictionary {1, . . . , d}, with the word count Xj denoting the
number of occurrences of word j in the document. Given this representation, we
can easily create “subdocuments” by binomially downsampling the word counts Xj
independently. That is, for some ﬁxed downsampling fraction α ∈ (0, 1), we draw

(cid:101)Xj | Xj ∼ Binom(Xj, α).

In other words, we keep each occurrence of word j independently with probability
α.

Wager et al. (2014) study this downsampling scheme in the context of a Poisson
mixture model for the inputs X that obeys the structure of Figure 1.2: ﬁrst, we
draw a class Y ∈ {1, . . . , K} (e.g., travel) and a “topic” θ ∈ Rd (e.g., corresponding
to travel in Norway). The topic θ speciﬁes a distribution over words,

(1.1)

(1.2)

µj(θ) = eθj ,

where, without loss of generality, we assume that (cid:80)d
j=1 eθj = 1. We then draw a
Pois(T ) number of words, where T is the expected document length, and generate
each word independently according to θ. Equivalently, each word count is an
independent Poisson random variable, Xj ∼ Pois(T µj(θ)). The following is an
example draw of a document:

Y = travel

norway
(cid:122)(cid:125)(cid:124)(cid:123)
0.5 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
0.5 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
1.2 ,

skyscraper
(cid:122) (cid:125)(cid:124) (cid:123)
−2.7 , . . . ]

θ = [

norway
(cid:122)(cid:125)(cid:124)(cid:123)

2 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
1 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
3 ,

skyscraper
(cid:122)(cid:125)(cid:124)(cid:123)
0

, . . . ]

X = [

norway
(cid:122)(cid:125)(cid:124)(cid:123)

1 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
0 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
1 ,

skyscraper
(cid:122)(cid:125)(cid:124)(cid:123)
0

, . . . ]

(cid:101)X = [

Let us now try to understand the downsampling scheme (cid:101)X | X in the context of the

8

Data Augmentation via L´evy Processes

Poisson topic model over X. For each word j, recall that (cid:101)Xj | Xj ∼ Binom(Xj, α).
If we marginalize over X, then we have:

(cid:101)Xj | T, θ ∼ Pois (αT µj(θ)) .

(1.3)

(1.4)

As a result, the distribution of (cid:101)X is exactly the distribution of X if we replaced T
with (cid:101)T = αT .

We can understand this thinning by embedding the document X in a multivariate
Poisson process (At)t≥0, where the marginal distribution of At ∈ {0, 1, 2, . . . }d is
deﬁned to be the distribution over counts when the expected document length is t.
Then, we can write

X = AT ,

(cid:101)X = A

(cid:101)T .

Thus, under the Poisson topic model, the binomial thinning procedure does not
alter the structure of the problem other than by shifting the expected document
length from T to (cid:101)T . Figure 1.4 illustrates one realization of L´evy thinning in the
Poisson case with a three-word dictionary. Note that in this case we can sample
(cid:101)X = AαT given X = AT without knowledge of θ.

This perspective lies at the heart of the analysis in Wager et al. (2014), who
show under the Poisson model that, when the overall document length (cid:107)X(cid:107)1
is independent of the topic θ, thinning does not perturb the optimal decision
boundary. Indeed, the conditional distribution over class labels is identical for the
original features and the thinned features:

P[Y | X = x] = P[Y | (cid:101)X = x].

(1.5)

This chapter extends the result to general L´evy processes (see Theorem 1.2).

This last result (1.5) may appear quite counterintuitive: for example, if A60 is
more informative than A40, how can it be that downsampling does not perturb
the conditional class probabilities? Suppose x is a 40-word document ((cid:107)x(cid:107)1 = 40).
When t = 60, most of the documents will be longer than 40 words, and thus x will be
less likely under t = 60 than under t = 40. However, (1.5) is about the distribution
of Y conditioned on a particular realization x. The claim is that, having observed
x, we obtain the same information about Y regardless of whether t, the expected
document length, is 40 or 60.

1.2.2 Thinning L´evy Processes

The goal of this section is to extend the Poisson topic model from Section 1.2.1
and construct general thinning schemes with the invariance property of (1.5). We
will see that L´evy processes provide a natural vehicle for such a generalization: The
Poisson process used to generate documents is a speciﬁc L´evy process, and binomial
sampling corresponds to “rewinding” the L´evy process back in time.

Consider the multiclass classiﬁcation problem of predicting a discrete class Y ∈
{1, . . . , K} given an input vector X ∈ Rd. Let us assume that the joint distribution

1.2 L´evy Thinning

9

over (X, Y ) is governed by the following generative model:

1. Choose Y ∼ Mult(π), where π is on the K-dimensional simplex.

2. Draw a topic θ | Y , representing a subpopulation of class Y .
3. Construct a L´evy process (At)t≥0 | θ, where At ∈ Rd is a potential input vector
at time t.

4. Observe the input vector X = AT at a ﬁxed time T .

While the L´evy process imposes a fair amount of structure, we make no assump-
tions about the number of topics, which could be uncountably inﬁnite, or about
their distribution, which could be arbitrary. Of course, in such an unconstrained
non-parametric setting, it would be extremely diﬃcult to adequately ﬁt the genera-
tive model. Therefore, we take a diﬀerent tack: We will use the structure endowed by
the L´evy process to generate pseudo-examples for consumption by a discriminative
classiﬁer. These pseudo-examples implicitly encode our generative assumptions.

The natural way to generate a pseudo-example ( (cid:101)X, Y ) is to “rewind” the L´evy
process (At) backwards from time T (recall X = AT ) to an earlier time (cid:101)T = αT
(cid:101)T . In practice, (At) is
for some α ∈ (0, 1) and deﬁne the thinned input as (cid:101)X = A
unobserved, so we draw (cid:101)X conditioned on the original input X = AT and topic θ.
In fact, we can draw many realizations of (cid:101)X | X, θ.

Our hope is that a single full example (X, Y ) is rich enough to generate many
diﬀerent pseudo-examples ( (cid:101)X, Y ), thus increasing the eﬀective sample size. More-
over, Wager et al. (2014) show that training with such pseudo-examples can also
lead to a somewhat surprising “altitude training” phenomenon whereby thinning
yields an improvement in generalization performance because the pseudo-examples
are more diﬃcult to classify than the original examples, and thus force the learning
algorithm to work harder and learn a more robust model.

A technical diﬃculty is that generating (cid:101)X | X, θ seemingly requires knowledge of
the topic θ driving the underlying L´evy process (At). In order to get around this
issue, we establish the following condition under which the observed input X = AT
alone is suﬃcient—that is, P[ (cid:101)X | X, θ] does not actually depend on θ.
Assumption 1.1 (exponential family structure). The L´evy process (At) (cid:12)
(cid:12) θ is
drawn according to an exponential family model whose marginal density at time
t is

f (t)
θ (x) = exp [θ · x − tψ (θ)] h(t) (x) for every t ∈ R.

(1.6)

Here, the topic θ ∈ Rd is an unknown parameter vector, and h(t)(x) is a family of
carrier densities indexed by t ∈ R.

The above assumption is a natural extension of a standard exponential family
assumption that holds for a single value of t. Speciﬁcally, suppose that h(t)(x),
t > 0, denotes the t-marginal densities of a L´evy process, and that f (1)
(x) =
exp [θ · x − ψ (θ)] h(1)(x) is an exponential family through h(1)(x) indexed by θ ∈
Rd. Then, we can verify that the densities speciﬁed in (1.6) induce a family of

θ

10

Data Augmentation via L´evy Processes

Figure 1.4: Illustration of our Poisson process document model with a three-word
dictionary and µ(θ) = (0.25, 0.3, 0.45). The word counts of the original document,
X = (8, 7, 16), represents the trivariate Poisson process At, sliced at T = 28. The
thinned pseudo-document (cid:101)X = (2, 4, 9) represents At sliced at (cid:101)T = 14.

L´evy processes indexed by θ. The key observation in establishing this result is that,
because h(t)(x) is the t-marginal of a L´evy process, the L´evy–Khintchine formula
implies that

(cid:90)

eθ·xh(t)(x) dx =

eθ·xh(1)(x) dx

= et ψ(θ),

(cid:18)(cid:90)

(cid:19)t

and so the densities in (1.6) are properly normalized.

We also note that, given this assumption and as T → ∞, we have that AT /T
converges almost surely to µ(θ) def= E [A1]. Thus, the topic θ can be understood as
a description of an inﬁnitely informative input. For ﬁnite values of T , X represents
a noisy observation of the topic θ.

Now, given this structure, we show that the distribution of (cid:101)X = AαT conditional
on X = AT does not depend on θ. Thus, feature thinning is possible without
knowledge of θ using the L´evy thinning procedure deﬁned below. We note that,
in our setting, the carrier distributions h(t)(x) are always known; in Section 1.3,
we discuss how to eﬃciently sample from the induced distribution g(αT ) for some
speciﬁc cases of interest.

Theorem 1.1 (L´evy thinning). Assume that (At) satisﬁes the exponential family
structure in (1.6), and let α ∈ (0, 1) be the thinning parameter. Then, given an
input X = AT and conditioned on any θ, the thinned input (cid:101)X = AαT has the

1.2 L´evy Thinning

11

(1.7)

(1.8)

following density:

g(αT )(˜x; X) =

h(αT )(˜x) h((1−α)T )(X − ˜x)
h(T )(X)

,

which importantly does not depend on θ.

Proof. Because the L´evy process (At) has independent and stationary increments,
we have that AαT ∼ f (αT )
and AT − AαT ∼ f ((1−α)T )
are independent. Therefore,
we can write the conditional density of AαT given AT as the joint density over
(AαT , AT ) (equivalently, the reparametrization (AαT , AT − AαT )) divided by the
marginal density over AT :

θ

θ

g(αT )(˜x; X) =

f (αT )
θ

(X − ˜x)

(˜x)f ((1−α)T )
θ
f (T )
(X)
θ
exp [θ · ˜x − αT ψ(θ)] h(αT )(˜x)

(cid:16)

(cid:17)

=

×

×

(cid:16)

(cid:16)

exp [θ · (X − ˜x) − (1 − α)T ψ(θ)] h((1−α)T )(X − ˜x)

(cid:17)

exp [θ · X − T ψ(θ)] h(T )(X)

(cid:17)−1

,

where the last step expands everything (1.6). Algebraic cancellation, which removes
all dependence on θ, completes the proof.

Note that while Theorem 1.1 guarantees we can carry out feature thinning
without knowing the topic θ, it does not guarantee that we can do it without
knowing the information content T . For Poisson processes, the binomial thinning
mechanism depends only on α and not on the original T . This is a convenient
property in the Poisson case but does not carry over to all L´evy processes — for
example, if Bt is a standard Brownian motion, then the distribution of B2 given
B4 = 0 is N(0, 1), while the distribution of B200 given B400 = 0 is N(0, 100).
As we will see in Section 1.3, thinning in the Gaussian and Gamma families
does require knowing T , which will correspond to a “sample size” or “precision.”
Likewise, Theorem 1.1 does not guarantee that sampling from (1.7) can be carried
out eﬃciently; however, in all the examples we present here, sampling can be carried
out easily in closed form.

1.2.3 Learning with Thinned Features

Having shown how to thin the input X to (cid:101)X without knowledge of θ, we can
proceed to deﬁning our full data augmentation strategy. We are given n training
examples {(X (i), Y (i))}n
i=1. For each original input X (i), we generate B thinned
versions (cid:101)X (i,1), . . . , (cid:101)X (i,B) by sampling from (1.7). We then pair these B examples
up with Y (i) and train any discriminative classiﬁer on these Bn examples. Algo-
rithm 1 describes the full procedure where we specialize to logistic regression. If
one is implementing this procedure using stochastic gradient descent, one can also

12

Data Augmentation via L´evy Processes

Procedure 1. Logistic Regression with L´evy Regularization
Input: n training examples (X (i), Y (i)), a thinning parameter α ∈ (0, 1), and a feature
map φ : Rd (cid:55)→ Rp.

1. For each training example X (i), generate B thinned versions ( (cid:101)X (i,b))B
to (1.7).

b=1 according

2. Train logistic regression on the resulting pseudo-examples:

(cid:40) n
(cid:88)

B
(cid:88)

(cid:41)
β; (cid:101)X (i,b), Y (i)(cid:17)
(cid:16)

,

(cid:96)

ˆβ def= argmin
β∈Rp×K

i=1

b=1

where the multi-class logistic loss with feature map φ is

(cid:96)(β; x, y) def= log

eβ(k)·φ(x)

− β(y) · φ(x).

(cid:33)

(cid:32) K
(cid:88)

k=1

3. Classify new examples according to

ˆy(x) = argmin

k∈{1, ..., K}

(cid:110)

ˆc(k) − ˆβ(k) · φ(x)

(cid:111)

,

(1.9)

(1.10)

(1.11)

where the ˆck ∈ R are optional class-speciﬁc calibration parameters for k = 1, . . . , K.

generate a fresh thinned input (cid:101)X whenever we sample an input X on the ﬂy, which
is the usual implementation of dropout training (Srivastava et al., 2014).

In the ﬁnal step (1.11) of Algorithm 1, we also allow for class-speciﬁc calibration
parameters . After the ˆβ(k) have been determined by logistic regression with L´evy
regularization, these parameters ˆc(k) can be chosen by optimizing the logistic loss on
the original uncorrupted training data. As discussed in Section 1.2.5, re-calibrating
the model is recommended, especially when α is small.

1.2.4 Thinning Preserves the Bayes Decision Boundary

We can easily implement the thinning procedure, but how will it aﬀect the accuracy
of the classiﬁer? The following result gives us a ﬁrst promising piece of the answer
by establishing conditions under which thinning does not aﬀect the Bayes decision
boundary.

At a high level, our results rely on the fact that under our generative model, the
“amount of information” contained in the input vector X is itself uninformative
about the class label Y .

Assumption 1.2 (Equal information content across topics). Assume there exists
a constant ψ0 such that ψ(θ) = ψ0 with probability 1, over random θ.

For example, in our Poisson topic model, we imposed the restriction that ψ(θ) =

1.2 L´evy Thinning

13

(1.12)

(1.13)

(1.14)

(1.15)

(cid:80)d

j=1 eθj = 1, which ensures that the document length (cid:107)At(cid:107)1 has the same

distribution (which has expectation ψ(θ) in this case) for all possible θ.

Theorem 1.2. Under Assumption 1.2, the posterior class probabilities are invari-
ant under thinning (1.7):

(cid:104)
Y = y (cid:12)

P

(cid:12) (cid:101)X = x

(cid:105)

= P (cid:2)Y = y (cid:12)

(cid:12) X = x(cid:3)

for all y ∈ {1, . . . , K} and x ∈ X.

Proof. Given Assumption 1.2, the density of At | θ is given by:

f (t)
θ (x) = eθ·xe−tψ0h(t)(x),

which importantly splits into two factors, one depending on (θ, x), and the other
depending on (t, x). Now, let us compute the posterior distribution:

P (cid:2)Y = y (cid:12)

(cid:12) At = x(cid:3) ∝ P [Y = y]

P [θ | Y ] f (t)

θ (x)dθ

∝ P [Y = y]

P [θ | Y ] eθ·xdθ,

(cid:90)

(cid:90)

which does not depend on t, as e−tψ0 h(t)(x) can be folded into the normalization
constant. Recall that X = AT and (cid:101)X = A
(cid:101)T . Substitute t = T and t = (cid:101)T to conclude
(1.12).

To see the importance of Assumption 1.2, consider the case where we have two
labels (Y ∈ {1, 2}), each with a single topic (Y yields topic θY ). Suppose that
ψ(θ2) = 2ψ(θ1)—that is, documents in class 2 are on average twice as long as those
in class 1. Then, we would be able to make class 2 documents look like class 1
documents by thinning them with α = 0.5.

Remark 1.1. If we also condition on the information content T , then an analogue
to Theorem 1.2 holds even without Assumption 1.2:

(cid:104)
Y = y (cid:12)

P

(cid:12) (cid:101)X = x, (cid:101)T = t

= P (cid:2)Y = y (cid:12)

(cid:12) X = x, T = t(cid:3) .

(cid:105)

(1.16)

This is because, after conditioning on T , the e−tψ(θ) term factors out of the
likelihood.

The upshot of Theorem 1.2 is that thinning will not induce asymptotic bias
whenever an estimator produces P (cid:2)Y = y (cid:12)
(cid:12) X = x(cid:3) in the limit of inﬁnite data
(n → ∞), i.e., if the logistic regression (Algorithm 1) is well-speciﬁed. Speciﬁcally,
training either on original examples or thinned examples will both converge to the
true class-conditional distribution. The following result assumes that the feature
space X is discrete; the proof can easily be generalized to the case of continuous
features.

Corollary 1.3. Suppose that Assumption 1.2 holds, and that the above multi-class

14

Data Augmentation via L´evy Processes

logistic regression model is well-speciﬁed, i.e., P (cid:2)Y = y (cid:12)
(cid:12) X = x(cid:3) ∝ eβ(y)·φ(x) for
some β and all y = 1, ..., K. Then, assuming that P [At = x] > 0 for all x ∈ X and
t > 0, Algorithm 1 is consistent, i.e., the learned classiﬁcation rule converges to the
Bayes classiﬁer as n → ∞.
Proof. At a ﬁxed x, the population loss E (cid:2)(cid:96) (cid:0)β; X, Y (cid:12)
any choice of β satisfying:
exp (cid:2)β(y) · φ(x)(cid:3)
k=1 exp (cid:2)β(k) · φ(x)(cid:3) = P (cid:2)Y = y (cid:12)

(cid:12) X = x(cid:1)(cid:3) is minimized by

(cid:12) X = x(cid:3)

(1.17)

(cid:80)K

for all y = 1, ..., K. Since the model is well-speciﬁed and by assumption P[ (cid:101)X =
x] > 0 for all x ∈ X, we conclude that weight vector ˆβ learned using Algorithm 1
must satisfy asymptotically (1.17) for all x ∈ X as n → ∞.

1.2.5 The End of the Path

As seen above, if we have a correctly speciﬁed logistic regression model, then
L´evy thinning regularizes it without introducing any bias. However, if the logistic
regression model is misspeciﬁed, thinning will in general induce bias, and the
amount of thinning presents a bias-variance trade-oﬀ. The reason for this bias is that
although thinning preserves the Bayes decision boundary, it changes the marginal
distribution of the covariates X, which in turn aﬀects logistic regression’s linear
approximation to the decision boundary. Figure 1.5 illustrates this phenomenon in
the case where At is a Brownian motion, corresponding to Gaussian feature noising;
Wager et al. (2014) provides a similar example for the Poisson topic model.

Fully characterizing the bias of L´evy thinning is beyond the scope of this paper.
However, we can gain some helpful insights about this bias by studying “strong
thinning”—i.e., L´evy thinning in the limit as the thinning parameter α → 0:

ˆβ0+

def= lim
α→0

lim
B→∞

ˆβ(α, B),

(1.18)

where ˆβ(α, B) is deﬁned as in (1.9) with the explicit dependence on α and B.
For each α, we take B → ∞ perturbed points for each of the original n data
points. As we show in this section, this limiting classiﬁer is well-deﬁned under weak
conditions; moreover, in some cases of interest, it can be interpreted as a simple
generative classiﬁer. The result below concerns the existence of ˆβ0+, and establishes
that it is the empirical minimizer of a convex loss function.

Theorem 1.4. Assume the setting of Procedure 1, and let the feature map be
φ(x) = x. Assume that the generative L´evy process (At) has ﬁnitely many jumps in
expectation over the interval [0, T ]. Then, the limit ˆβ0+ is well-deﬁned and can be
written as

ˆβ0+ = argmin
β∈Rp×K

(cid:40) n
(cid:88)

i=1

(cid:16)

β; X (i), Y (i)(cid:17)

ρ

(cid:41)

,

(1.19)

1.2 L´evy Thinning

15

(cid:12) θ, T ∼ N (cid:0)T θ, σ2T Ip×p

Figure 1.5: The eﬀect of L´evy thinning with data generated from a Gaussian
model of the form X (cid:12)
(cid:1), as described in Section 1.3.1.
The outer circle depicts the distribution of θ conditional on the color Y : blue
points all have θ ∝ (cos(0.75 π/2), sin(0.75 π/2)), whereas the red points have
θ ∝ (cos(ω π/2), sin(ω π/2)) where ω is uniform between 0 and 2/3. Inside this
circle, we see 3 clusters of points generated with T = 0.1, 0.4, and 1, along with
logistic regression decision boundaries obtained from each cluster. The dashed line
shows the Bayes decision boundary separating the blue and red points, which is the
same for all T (Theorem 1.2). Note that the logistic regression boundaries learned
from data with diﬀerent T are not the same. This issue arises because the Bayes
decision boundary is curved, and the best linear approximation to a curved Bayes
boundary changes with T .

for some convex function ρ(·; x, y).

The proof of Theorem 1.4 is provided in the appendix. Here, we begin by
establishing notation that lets us write down an expression for the limiting loss
ρ. First, note that Assumption 1.1 implicitly requires that the process (At) has
ﬁnite moments. Thus, by the L´evy–It¯o decomposition, we can uniquely write this
process as

At = bt + Wt + Nt,

(1.20)

where b ∈ Rp, Wt is a Wiener process with covariance Σ, and Nt is a compound
Poisson process which, by hypothesis, has a ﬁnite jump intensity.

Now, by an argument analogous to that in the proof of Theorem 1.1, we see that

16

Data Augmentation via L´evy Processes

the joint distribution of WT and NT conditional on AT does not depend on θ. Thus,
we can deﬁne the following quantities without ambiguity:

(cid:12)
µT (x) = bT + E (cid:2)WT
(cid:12) AT = x(cid:3) ,
λT (x) = E (cid:2)number of jumps in (At) for t ∈ [0, T ] (cid:12)
νT (z; x) = lim
t→0

(cid:12) Nt (cid:54)= 0, AT = x(cid:3) .

P (cid:2)Nt = z (cid:12)

(cid:12) AT = x(cid:3) ,

(1.21)

(1.22)

(1.23)

More prosaically, νT (·; x) can be described as the distribution of the ﬁrst jump
of (cid:101)Nt, a thinned version of the jump process Nt. In the degenerate case where
P (cid:2)NT = 0 (cid:12)
(cid:12) AT = x(cid:3) = 0, we set νT (·; x) to be a point mass at z = 0.

Given this notation, we can write the eﬀective loss function ρ for strong thinning

as

ρ (β; x, y) = −µT (x) · β(y) +

β(k)(cid:62)Σβ(k)

(1.24)

T
2

1
K

K
(cid:88)

k=1

(cid:90)

+ λT (x)

(cid:96) (β; z, y) dνT (z; x),

provided we require without loss of generality that (cid:80)K
k=1 β(k) = 0. In other words,
the limiting loss can be described entirely in terms of the distribution of the ﬁrst
jump of (cid:101)Nt, and continuous part Wt of the L´evy process. The reason for this
phenomenon is that, in the strong thinning limit, the pseudo-examples (cid:101)X ∼ AαT
can all be characterized using either 0 or 1 jumps.

Aggregating over all the training examples, we can equivalently write this strong

thinning loss as

(cid:16)

β; X (i), Y (i)(cid:17)

ρ

=

n
(cid:88)

i=1

1
2T

n
(cid:88)

i=1

γ−1
Y (i)

(cid:13)
(cid:13)
(cid:13)γY (i) µT

X (i)(cid:17)
(cid:16)

− T Σβ(Y (i))(cid:13)
2
(cid:13)
(cid:13)

Σ−1

λT (X (i))

(cid:90)

(cid:16)

β; z, Y (i)(cid:17)

(cid:96)

dνT (z; X (i)),

(1.25)

+

n
(cid:88)

i=1

2 terms that do not depend on β. Here, 1

up to (cid:107)µT (cid:107)2
2 v(cid:48)Σ−1v corresponds
to the Gaussian log-likelihood with covariance Σ (up to constants), and γy =
K (cid:12)
(cid:12) /n measures the over-representation of class y relative to other
(cid:12)
classes.

(cid:8)i : Y (i) = y(cid:9)(cid:12)

Σ−1 = 1

2 (cid:107)v(cid:107)2

(cid:13)
(cid:13)

(cid:80)n

(cid:13)µT (X (i)) − T Σβ(Y (i))(cid:13)

In the case where we have the same number of training examples from
each class (and so γy = 1 for all y = 1, ..., K), the strong thinning loss
can be understood in terms of a generative model. The ﬁrst term, namely
1
, is the loss function for linear classiﬁcation
2T
in a Gaussian mixture with observations µT (X (i)), while the second term is the
logistic loss obtained by classifying single jumps. Thus, strong thinning is eﬀec-
tively seeking the best linear classiﬁer for a generative model that is a mixture of
Gaussians and single jumps.

2
(cid:13)
(cid:13)

Σ−1

i=1

In the pure jump case (Σ = 0), we also note that strong thinning is closely related

1.3 Examples

17

1.3 Examples

to naive Bayes classiﬁcation. In fact, if the jump measure of Nt has a ﬁnite number
of atoms that are all linearly independent, then we can verify that the parameters
ˆβ0+ learned by strong thinning are equivalent to those learned via naive Bayes,
although the calibration constants c(k) may be diﬀerent.

At a high level, by elucidating the generative model that strong thinning pushes
us towards, these results can help us better understand the behavior of L´evy
thinning for intermediate value of α, e.g., α = 1/2. They also suggest caution with
respect to calibration: For both the diﬀusion and jump terms, we saw above that
L´evy thinning gives helpful guidance for the angle of β(k), but does not in general
(cid:13)β(k)(cid:13)
elegantly account for signal strength (cid:13)
(cid:13)2 or relative class weights. Thus, we
recommend re-calibrating the class decision boundaries obtained by L´evy thinning,
as in Algorithm 1.

So far, we have developed our theory of L´evy thinning using the Poisson topic
model as a motivating example, which corresponds to dropping out words
from a document. In this section, we present two models based on other L´evy
processes—multivariate Brownian motion (Section 1.3.1) and Gamma processes
(Section 1.3.2)— exploring the consequences of L´evy thinning.

1.3.1 Multivariate Brownian Motion

Consider a classiﬁcation problem where the input vector is the aggregation of
multiple noisy, independent measurements of some underlying object. For example,
in a biomedical application, we might want to predict a patient’s disease status
based on a set of biomarkers such as gene expression levels or brain activity.
A measurement is typically obtained through a noisy experiment involving an
microarray or fMRI, so multiple experiments might be performed and aggregated.
More formally, suppose that patient i has disease status Y (i) and expression level
µi ∈ Rd for d genes, with the distribution of µi diﬀerent for each disease status.
Given µi, suppose the t-th measurement for patient i is distributed as

Zi,t ∼ N(µi, Σ),

(1.26)

where Σ ∈ Rd×d is assumed to be a known, ﬁxed matrix. Let the observed input be
X (i) = (cid:80)Ti
t=1 Zi,t, the sum of the noisy measurements. If we could take inﬁnitely
many measurements (Ti → ∞), we would have X (i)/Ti → µi almost surely; that
is, we would observe gene expression noiselessly. For ﬁnitely many measurements,
X (i) is a noisy proxy for the unobserved µi.

We can model the process of accumulating measurements with a multivariate

18

Data Augmentation via L´evy Processes

Brownian motion (At):

At = tµ + Σ1/2Bt,

where Bt is a d-dimensional white Brownian motion.1 For integer values of t, At
represents the sum of the ﬁrst t measurements, but At is also deﬁned for fractional
values of t. The distribution of the features X at a given time T is thus

(1.27)

(1.28)

(1.29)

(1.30)

(1.31)

X | µ, T ∼ N(T µ, T Σ),

leading to density

f (t)
µ (x) =

exp (cid:2) 1

2 (x − tµ)(cid:62)(tΣ)−1(x − tµ)(cid:3)

(cid:20)

(2π)d/2 det(Σ)
t
2

(cid:21)
µ(cid:62)Σ−1µ

x(cid:62)Σ−1µ −

= exp

h(t)(x),

where

h(t)(x) =

exp (cid:2)− 1
2t x(cid:62)Σ−1x(cid:3)
(2π)d/2 det(Σ)1/2

.

We can recover the form of (1.6) by setting θ = Σ−1µ, a one-to-one mapping
provided Σ is positive-deﬁnite.

Thinning. The distribution of (cid:101)X = AαT given X = AT is that of a Brownian
bridge process with the following marginals:

(cid:101)X | X ∼ N (αX, α(1 − α)T Σ) .

In this case, “thinning” corresponds exactly to adding zero-mean, additive Gaus-
sian noise to the scaled features αX. Note that in this model, unlike in the Poisson
topic model, sampling (cid:101)X from X does require observing T —for example, knowing
how many observations were taken. The larger T is, the more noise we need to
inject to achieve the same downsampling ratio.

In the Poisson topic model, the features (Xi,1, . . . , Xi,d) were independent of
each other given the topic θi and expected length Ti. By contrast, in the Brownian
motion model the features are correlated (unless Σ is the identity matrix). This
serves to illustrate that independence or dependence of the features is irrelevant to
our general framework; what is important is that the increments Zt = At − At−1
are independent of each other, the key property of a L´evy process.

Assumption 1.2 requires that µ(cid:62)Σ−1µ is constant across topics; i.e., that the
true gene expression levels are equally sized in the Mahalanobis norm deﬁned
by Σ. Clearly, this assumption is overly stringent in real situations. Fortunately,
Assumption 1.2 is not required (see Remark 1.1) as long as T is observed—as it

1. By deﬁnition of Brownian motion, we have marginally that Bt ∼ N(0, tI).

1.3 Examples

19

must be anyway if we want to be able to carry out L´evy thinning.

Thinning X in this case is very similar to subsampling. Indeed, for integer values
of (cid:101)T , instead of formally carrying out L´evy thinning as detailed above, we could
simply resample (cid:101)T values of Zi,t without replacement, and add them together to
obtain (cid:101)X. If there are relatively few repeats, however, the resampling scheme can
(cid:1) pseudo-examples (e.g. 6 pseudo-examples if T = 4 and (cid:101)T = 2),
lead to only (cid:0)T
(cid:101)T
whereas the thinning approach leads to inﬁnitely many possible pseudo-examples
we can use to augment the regression. Moreover, if T = 4 then subsampling leaves
us with only four choices of α; there would be no way to thin using α = 0.1, for
instance.

1.3.2 Gamma Process

As another example, suppose again that we are predicting a patient’s disease status
based on repeated measurements of a biomarker such as gene expression or brain
activity. But now, instead of (or in addition to) the average signal, we want our
features to represent the variance or covariance of the signals across the diﬀerent
measurements.

Assume ﬁrst that the signals at diﬀerent genes or brain locations are independent;

that is, the t-th measurement for patient i and gene j has distribution

Zi,j,t ∼ N(µi,j, σ2

i,j).

(1.32)

Here, the variances σ2
subscript i, after T + 1 measurements we can compute

i,1, . . . , σ2

i = (σ2

i,d) parameterize the “topic.” Suppressing the

Xj,T =

(Zi,j,t − ¯Zi,j,T +1)2,

where

¯Zi,j,T +1 =

Zi,j,t. (1.33)

1
T + 1

T +1
(cid:88)

t=1

T +1
(cid:88)

t=1

Then Xj,T ∼ σ2
j χ2
scale parameter 2σ2
more and more observations (increasing T ), we will have XT /T → (σ2
almost surely.

T , which is a Gamma distribution with shape parameter T /2 and
j (there is no dependence on µi). Once again, as we accumulate
1, . . . , σ2
d)

We can embed Xj,T in a multivariate Gamma process with d independent

coordinates and scale parameters σ2
j :

(At)j ∼ Gamma(t/2, 2σ2

j ).

The density of At given σ2 is

f (t)
σ2 (x) =

d
(cid:89)

j=1

j

e−xj /2σ2
xt/2−1
j
Γ(t/2)2t/2σ2(t/2)


j

= exp

−

xj/2σ2

j − (t/2)

log σ2
j


 h(t)(x),

d
(cid:88)

j=1

d
(cid:88)

j=1

(1.34)

(1.35)

20

Data Augmentation via L´evy Processes

where

h(t)(x) =

(cid:81)

j xt/2−1
j
Γ(t/2)d2dt/2

.

We can recover the form of (1.6) by setting θj = −1/2σ2

j , a one-to-one mapping.

Thinning. Because (cid:101)Xj ∼ Gamma(αT /2, 2σ2
Xj − (cid:101)Xj ∼ Gamma((1 − α)T /2, 2σ2

j ), we have

j ) is independent of the increment

(cid:101)Xj
Xj

| Xj ∼ Beta (αT /2, (1 − α)T /2) .

In other words, we create a noisy (cid:101)X by generating for each coordinate an indepen-
dent multiplicative noise factor

mj ∼ Beta (αT, (1 − α)T )

and setting (cid:101)Xj = mjXj. Once again, we can downsample without knowing σ2
j , but
we do need to observe T . Assumption 1.2 would require that (cid:81)
j is identical for
all topics. This is an unrealistic assumption, but once again it is unnecessary as
long as we observe T .

j σ2

General covariance. More generally, the signals at diﬀerent brain locations, or
expressions for diﬀerent genes, will typically be correlated with each other, and these
correlations could be important predictors. To model this, let the measurements be
distributed as:

Zi,t ∼ N(µi, Σi),

where Σ represents the unknown “topic”—some covariance matrix that is charac-
teristic of a certain subcategory of a disease status.

After observing T + 1 observations we can construct the matrix-valued features:

XT =

(Zi,t − ¯Zi,T +1)(Zi,t − ¯Zi,T +1)(cid:62).

T +1
(cid:88)

t=1

Now XT has a Wishart distribution: XT ∼ Wishd(Σ, T ). When T ≥ d, the density
of At given Σ is

f (t)
Σ (x) = exp

−

tr(Σ−1x) −

log det(Σ)

h(t)(x),

(1.41)

(cid:26)

1
2

(cid:27)

t
2

(1.36)

(1.37)

(1.38)

(1.39)

(1.40)

1.4 Simulation Experiments

where

21

(1.42)

(1.43)

h(t)(x) =

td
2 det(x)

t−d−2

2 Γd

(cid:18)
2

(cid:19)(cid:19)−1

,

(cid:18) t
2

Γd

(cid:19)

(cid:18) t
2

d(d−1)
4

= π

d
(cid:89)

j=1

(cid:18) t
2

Γ

+

1 − j
2

(cid:19)

,

supported on positive-deﬁnite symmetric matrices. If X = AT and αT ≥ d as well,
we can sample a “thinned” observation (cid:101)X from density proportional to

h(αT )(˜x)h(T −αT )(X − ˜x) ∝ det(˜x)

2+d−αT
2

det(X − ˜x)

2+d−(1−α)T
2

,

(1.44)

or after the aﬃne change of variables (cid:101)X = X 1/2M X 1/2, we sample M from density
proportional to det(m)
, a matrix beta distribution.
Here, M may be interpreted as matrix-valued multiplicative noise.

det(Id − m)

2+d−(1−α)t
2

2+d−αT
2

1.4 Simulation Experiments

In this section, we perform several simulations to illustrate the utility of L´evy
thinning. In particular, we will highlight the modularity between L´evy thinning
(which provides pseudo-examples) and the discriminative learner (which ingests
these pseudo-examples). We treat the discriminative learner as a black box, com-
plete with its own internal cross-validation scheme that optimizes accuracy on
pseudo-examples. Nonetheless, we show that accuracy on the original examples
improves when we train on thinned examples.

More speciﬁcally, given a set of training examples {(X, Y )}, we ﬁrst use L´evy
thinning to generate a set of pseudo-examples {( (cid:101)X, Y )}. Then we feed these
examples to the R function cv.glmnet to learn a linear classiﬁer on these pseudo-
examples (Friedman et al., 2010). We emphasize that cv.glmnet seeks to choose
its regularization parameter λ to maximize its accuracy on the pseudo-examples
( (cid:101)X, Y ) rather than on the original data (X, Y ). Thus, we are using cross-validation
as a black box instead of trying to adapt the procedure to the context of L´evy
thinning. In principle, we might be concerned that cross-validating on the pseudo-
examples would yield a highly suboptimal choice of λ, but our experiments will
show that the procedure in fact works quite well.

The two extremes of the path correspond to naive Bayes generative modeling at
one end (α = 0), and plain ridge-regularized logistic regression at the other (α = 1).
All methods were calibrated on the training data as follows: Given original weight
vectors ˆβ, we ﬁrst compute un-calibrated predictions ˆµ = X ˆβ for the log-odds of
P (cid:2)Y = 1 (cid:12)
(cid:12) X(cid:3), and then run a second univariate logistic regression Y ∼ ˆµ to adjust
both the intercept and the magnitude of the original coeﬃcients. Moreover, when us-
ing cross-validation on pseudo-examples ( (cid:101)X, Y ), we ensure that all pseudo-examples
induced by a given example (X, Y ) are in the same cross-validation fold. Code for

22

Data Augmentation via L´evy Processes

Figure 1.6: Performance of L´evy thinning with cross-validated ridge-regularized
logistic regression, on a random Gaussian design described in (1.45). The curves
depict the relationship between thinning α and classiﬁcation error as the number of
training examples grows: n = 30, 50, 75, 100, 150, 200, 400, and 600. We see that
naive Bayes improves over ridge logistic regression in very small samples, while in
moderately small samples L´evy thinning does better than either end of the path.

reproducing our results is available at https://github.com/swager/levythin.

Gaussian example. We generate data from the following hierarchical model:

Y ∼ Binomial (0.5) , µ (cid:12)

(cid:12) Y ∼ LY , X (cid:12)

(cid:12) µ ∼ N (µ, Id×d) ,

(1.45)

1

, ..., µ(Y )

where µ, X ∈ Rd and d = 100. The distribution LY associated with each label
Y consists of 10 atoms µ(Y )
10 . These atoms themselves are all randomly
generated such that their ﬁrst 20 coordinates are independent draws of 1.1 T4 where
T4 follows Student’s t-distribution with 4 degrees of freedom; meanwhile, the last
80 coordinates of µ are all 0. The results in Figure 1.6 are marginalized over the
randomness in LY ; i.e., diﬀerent simulation realizations have diﬀerent conditional
laws for µ given Y . Figure 1.6 shows the results.

Poisson example. We generate data from the following hierarchical model:

Y ∼ Binomial (0.5) , θ (cid:12)

(cid:12) Y ∼ LY , Xj

(cid:12)
(cid:12) θ ∼ Pois

1000

,

(1.46)

(cid:32)

(cid:33)

eθj
j=1 eθj

(cid:80)d

1.4 Simulation Experiments

23

Figure 1.7: Performance of L´evy thinning with cross-validated ridge-regularized
logistic regression, on a random Poisson design described in (1.46). The curves
depict the relationship between thinning α and classiﬁcation accuracy for n =
30, 50, 100, 150, 200, 400, 800, and 1600. Here, aggressive L´evy thinning with
small but non-zero α does substantially better than naive Bayes (α = 0) as soon as
n is moderately large.

where θ ∈ Rd, X ∈ Nd, and d = 500. This time, however, LY is deterministic: If
Y = 0, then θ is just 7 ones followed by 493 zeros, whereas





θ (cid:12)
(cid:12) Y = 1 ∼

0, ..., 0
(cid:124) (cid:123)(cid:122) (cid:125)
7

(cid:12)
(cid:12) τ, ..., τ
(cid:124) (cid:123)(cid:122) (cid:125)
7

(cid:12)
(cid:12) 0, ..., 0
(cid:124) (cid:123)(cid:122) (cid:125)
486

 , with τ ∼ Exp(3).

This generative model was also used in simulations by Wager et al. (2014); the
diﬀerence is that they applied thinning to plain logistic regression, whereas here
we verify that L´evy thinning is also helpful when paired with cross-validated ridge
logistic regression. Figure 1.7 shows the results.

These experiments suggest that it is reasonable to pair L´evy thinning with a well-
tuned black box learner on the pseudo-examples ( (cid:101)X, Y ), without worrying about
potential interactions between L´evy thinning and the tuning of the discriminative
model.

24

Data Augmentation via L´evy Processes

1.5 Discussion

In this chapter, we have explored a general framework for performing data augmen-
tation: apply L´evy thinning and train a discriminative classiﬁer on the resulting
pseudo-examples. The exact thinning scheme reﬂects our generative modeling as-
sumptions. We emphasize that the generative assumptions are non-parametric and
of a structural nature; in particular, we never ﬁt an actual generative model, but
rather encode the generative hints implicitly in the pseudo-examples.

A key result is that under the generative assumptions, thinning preserves the
Bayes decision boundary, which suggests that a well-speciﬁed classiﬁer incurs
no asymptotic bias. Similarly, we would expect that a misspeciﬁed but powerful
classiﬁer should incur little bias. We showed that in limit of maximum thinning,
the resulting procedure corresponds to ﬁtting a generative model. The exact bias-
variance trade-oﬀ for moderate levels of thinning is an interesting subject for further
study.

While L´evy processes provide a general framework for thinning examples, we
recognize that there are many other forms of coarsening that could lead to the
same intuitions. For instance, suppose X | θ is a Markov process over words in
a document. We might expect that short contiguous subsequences of X could
serve as good pseudo-examples. More broadly, there are many forms of data
augmentation that do not have the intuition of coarsening an input. For example,
rotating or shearing an image to generate pseudo-images appeals to other forms
of transformational invariance. It would be enlightening to establish a generative
framework in which data augmentation with these other forms of invariance arise
naturally.

To establish the desired result, we show that for a single training example (X, Y ),
the following limit is well-deﬁned for any β ∈ Rp×K:

ρ (β; X, Y ) = lim
α→0

(cid:16)

(cid:101)E

(cid:16)

(cid:104)
(cid:96)

1
α

= −β(Y ) · X + lim
α→0

β; (cid:101)X, Y
(cid:34)

(cid:32)

1
α

(cid:101)E

log

(cid:17)(cid:105)

(cid:17)

− log (K)

(cid:33)(cid:35)

eβ(k)· (cid:101)X

,

1
K

K
(cid:88)

k=1

(1.47)

where on the second line we wrote down the logistic loss explicitly and exploited
linearity of the term involving Y as in Wager et al. (2013). Here (cid:101)E denotes
expectation with respect to the thinning process and reﬂects the B → ∞ limit.
Because (cid:96) is convex, ρ must also be convex; and by equicontinuity ˆβ(α) must also
converge to its minimizer.

Our argument relies on the decomposition At = bt+Wt+Nt from (1.20). Without
loss of generality, we can generate the pseudo-features (cid:101)X as (cid:101)X = bt + (cid:102)WαT + (cid:101)NαT ,

1.6 Appendix: Proof of Theorem 1.4

1.6 Appendix: Proof of Theorem 1.4

25

where (cid:102)WαT and (cid:101)NαT have the same marginal distribution as WαT and NαT . Given
this notation,

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:33)(cid:35)

(cid:33)

(cid:34)

(cid:32)

log

1
K

1
α

(cid:101)E

(cid:34)

log

=

1
α

(cid:101)E

+

1
α

(cid:101)E

K
(cid:88)

k=1
(cid:32)

1
K

(cid:34)

log

K
(cid:88)

k=1
(cid:32)

1
K

K
(cid:88)

k=1

eβ(k)·(αbT +(cid:102)WαT )

(cid:12)
(cid:12) (cid:101)NαT = 0

P

(cid:104)

(cid:105)
(cid:101)NαT = 0

(cid:35)

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:33)

(cid:35)

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

P

(cid:104)

(cid:105)
(cid:101)NαT (cid:54)= 0

.

We now characterize these terms individually. First, because Nt has a ﬁnite jump
intensity, we can verify that, almost surely,

lim
α→0

1
α

P

(cid:104)

(cid:105)
(cid:101)NαT (cid:54)= 0

= λT (X),

where λT (X) is as deﬁned in (1.22). Next, because (cid:102)WαT concentrates at 0 as α → 0,
we can check that
(cid:32)

(cid:33)

(cid:34)

(cid:35)

lim
α→0

(cid:101)E

log

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

1
K
(cid:34)

K
(cid:88)

k=1
(cid:32)

(cid:33)

(cid:35)

eβ(k)· (cid:101)NαT

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

1
K

K
(cid:88)

k=1

(cid:33)

= lim
α→0

(cid:101)E

log

(cid:32)

(cid:90)

=

log

1
K

K
(cid:88)

k=1

eβ(k)·z

dνT (z; X)

where νT (·; X) (1.23) is the ﬁrst jump measure conditional on X.

Meanwhile, in order to control the remaining term, we note that we can write

(cid:102)WαT = α(cid:102)WT + (cid:101)BαT ,

where (cid:101)Bt is a Brownian bridge from 0 to T that is independent from (cid:102)WT . Thus,
noting that limα→0 P
= 1, we ﬁnd that

(cid:105)
(cid:101)NαT = 0

(cid:104)

eβ(k)·(αbT +(cid:102)WαT )

(cid:33)

(cid:35)

(cid:12)
(cid:12) (cid:101)NαT = 0

P

(cid:104)

(cid:105)
(cid:101)NαT = 0

lim
α→0

1
α

(cid:101)E

(cid:34)

(cid:32)

log

1
K

K
(cid:88)

k=1
(cid:34)

= lim
α→0

1
α

(cid:101)E

log

eβ(k)·(α(bT +(cid:102)WT ))+ (cid:101)BαT

(cid:33)(cid:35)

(cid:33)

(cid:32)

1
K
(cid:32)

K
(cid:88)

k=1
K
(cid:88)

k=1

1
K

T
2

= ¯β · µT (X) +

β(k)(cid:62)Σ β(k) − ¯β(cid:62)Σ ¯β

,

where µT (X) is as deﬁned in (1.21) and ¯β = K −1 (cid:80)K
k=1 β(k). The last equality
follows from Taylor expanding the log((cid:80) exp) term and noting that 3rd- and higher-

26

Data Augmentation via L´evy Processes

1.7 References

order terms vanish in the limit.

that ¯β = 0, we ﬁnally conclude that

Bringing back the linear term form (1.47), and assuming without loss of generality

ρ (β; X, Y ) = −β(Y ) · X +

β(k)(cid:62)Σ β(k)

T
2

1
K

K
(cid:88)

k=1

+ λT (X)

log

(cid:32)

1
K

K
(cid:88)

k=1

(cid:33)

eβ(k)·z

dνT (z; X)

= −β(Y ) · µT (X) +

β(k)(cid:62)Σ β(k)

(cid:90)

(cid:90)

T
2

K
(cid:88)

1
K
(cid:32) K
(cid:88)

k=1

k=1

(cid:33)

eβ(k)·z

+ λT (X)

−β(Y ) · z + log

− log(K) dνT (z; X),

where for the second equality we used the fact that X = µT (X)+λT (X) (cid:82) z dνT (z; X).
Finally, this expression only diﬀers from (1.24) by terms that do not include β; thus,
they yield the same minimizer.

Y. S. Abu-Mostafa. Learning from hints in neural networks. Journal of Complexity,

6(2):192–198, 1990.

J. Ba and B. Frey. Adaptive dropout for training deep neural networks. In Advances

in Neural Information Processing Systems (NIPS), pages 3084–3092, 2013.

P. Baldi and P. Sadowski. The dropout learning algorithm. Artiﬁcial intelligence,

210:78–122, 2014.

C. M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural

computation, 7(1):108–116, 1995.

G. Bouchard and B. Triggs. The trade-oﬀ between generative and discriminative
classiﬁers. In International Conference on Computational Statistics, pages 721–
728, 2004.

B. Efron. The eﬃciency of logistic regression compared to normal discriminant
analysis. Journal of the American Statistical Association (JASA), 70(352):892–
898, 1975.

J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized
linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22,
2010.

A. Globerson and S. Roweis. Nightmare at test time: robust learning by feature
deletion. In International Conference on Machine Learning (ICML), pages 353–
360, 2006.

I. Goodfellow, D. Warde-farley, M. Mirza, A. Courville, and Y. Bengio. Maxout
In International Conference on Machine Learning (ICML), pages

networks.
1319–1327, 2013.

D. P. Helmbold and P. M. Long. On the inductive bias of dropout. Journal of

1.7 References

27

Machine Learning Research (JMLR), 16:3403–3454, 2015.

J. Josse and S. Wager. Stable autoencoding: A ﬂexible framework for regularized

low-rank matrix estimation. arXiv preprint arXiv:1410.8275, 2014.

A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep
In Advances in Neural Information Processing

convolutional neural networks.
Systems (NIPS), pages 1097–1105, 2012.

J. A. Lasserre, C. M. Bishop, and T. P. Minka. Principled hybrids of generative and
discriminative models. In Computer Vision and Pattern Recognition (CVPR),
pages 87–94, 2006.

P. Liang and M. I. Jordan. An asymptotic analysis of generative, discriminative, and
pseudolikelihood estimators. In International Conference on Machine Learning
(ICML), pages 584–591, 2008.

D. McAllester. A PAC-Bayesian tutorial with a dropout bound. arXiv preprint

arXiv:1307.2118, 2013.

A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Gener-
ative/discriminative training for clustering and classiﬁcation. In Association for
the Advancement of Artiﬁcial Intelligence (AAAI), 2006.

A. Y. Ng and M. I. Jordan. On discriminative vs. generative classiﬁers: A compar-
ison of logistic regression and naive Bayes. In Advances in Neural Information
Processing Systems (NIPS), 2002.

R. Raina, Y. Shen, A. Ng, and A. McCallum. Classiﬁcation with hybrid gen-
In Advances in Neural Information Processing

erative/discriminative models.
Systems (NIPS), 2004.

Y. D. Rubinstein and T. Hastie. Discriminative vs informative learning.

In
International Conference on Knowledge Discovery and Data Mining (KDD),
volume 5, pages 49–53, 1997.

S. P. Sch¨olkopf, P. Simard, V. Vapnik, and A. Smola. Improving the accuracy and
speed of support vector machines. In Advances in Neural Information Processing
Systems (NIPS), pages 375–381, 1997.

P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. Transformation
Invariance in Pattern Recognition—Tangent Distance and Tangent Propagation.
Neural networks: Tricks of the trade Springer, 1998.

N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. Journal
of Machine Learning Research (JMLR), 15(1):1929–1958, 2014.

L. van der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger. Learning with
marginalized corrupted features. In International Conference on Machine Learn-
ing (ICML), pages 410–418, 2013.

S. Wager, S. I. Wang, and P. Liang. Dropout training as adaptive regularization.

In Advances in Neural Information Processing Systems (NIPS), 2013.

S. Wager, W. Fithian, S. I. Wang, and P. Liang. Altitude training: Strong bounds
for single-layer dropout. In Advances in Neural Information Processing Systems
(NIPS), 2014.

L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Regularization of neural
networks using dropconnect. In International Conference on Machine Learning
(ICML), pages 1058–1066, 2013.

S. I. Wang and C. Manning. Fast dropout training. In International Conference

28

Data Augmentation via L´evy Processes

on Machine Learning (ICML), pages 118–126, 2013.

S. I. Wang, M. Wang, S. Wager, P. Liang, and C. Manning. Feature noising
for log-linear structured prediction. In Empirical Methods in Natural Language
Processing (EMNLP), 2013.

6
1
0
2
 
r
a

M
 
1
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
0
4
3
6
0
.
3
0
6
1
:
v
i
X
r
a

1

Data Augmentation via L´evy Processes

Stefan Wager
Stanford University
Stanford, USA

William Fithian
University of California, Berkeley
Berkeley, USA

Percy Liang
Stanford University
Stanford, USA

swager@stanford.edu

wfithian@berkeley.edu

pliang@cs.stanford.edu

If a document is about travel, we may expect that short snippets of the document
should also be about travel. We introduce a general framework for incorporating
these types of invariances into a discriminative classiﬁer. The framework imagines
data as being drawn from a slice of a L´evy process. If we slice the L´evy process at
an earlier point in time, we obtain additional pseudo-examples, which can be used
to train the classiﬁer. We show that this scheme has two desirable properties: it
preserves the Bayes decision boundary, and it is equivalent to ﬁtting a generative
model in the limit where we rewind time back to 0. Our construction captures popular
schemes such as Gaussian feature noising and dropout training, as well as admitting
new generalizations.

1.1 Introduction

Black-box discriminative classiﬁers such as logistic regression, neural networks, and
SVMs are the go-to solution in machine learning: they are simple to apply and
often perform well. However, an expert may have additional knowledge to exploit,
often taking the form of a certain family of transformations that should usually
leave labels ﬁxed. For example, in object recognition, an image of a cat rotated,
translated, and peppered with a small amount of noise is probably still a cat.
Likewise, in document classiﬁcation, the ﬁrst paragraph of an article about travel

2

Data Augmentation via L´evy Processes

Figure 1.1: Two examples of transforming an original input X into a noisy, less
informative input (cid:101)X. The new inputs clearly have the same label but contain less
information and thus are harder to classify.

is most likely still about travel. In both cases, the “expert knowledge” amounts
to a belief that a certain transform of the features should generally not aﬀect an
example’s label.

One popular strategy for encoding such a belief is data augmentation: generat-
ing additional pseudo-examples or “hints” by applying label-invariant transforma-
tions to training examples’ features (Abu-Mostafa, 1990; Sch¨olkopf et al., 1997;
Simard et al., 1998). That is, each example (X (i), Y (i)) is replaced by many pairs
( (cid:101)X (i,b), Y (i)) for b = 1, . . . , B, where each (cid:101)X (i,b) is a transformed version of X (i).
This strategy is simple and modular: after generating the pseudo-examples, we can
simply apply any supervised learning algorithm to the augmented dataset. Fig-
ure 1.1 illustrates two examples of this approach, an image transformed to a noisy
image and a text caption, transformed by deleting words.

Dropout training (Srivastava et al., 2014) is an instance of data augmentation
that, when applied to an input feature vector, zeros out a subset of the features ran-
domly. Intuitively, dropout implies a certain amount of signal redundancy across
features—that an input with about half the features masked should usually be
classiﬁed the same way as a fully-observed input. In the setting of document clas-

1.1 Introduction

3

siﬁcation, dropout can be seen as creating pseudo-examples by randomly omitting
some information (i.e., words) from each training example. Building on this in-
terpretation, Wager et al. (2014) show that learning with such artiﬁcially diﬃcult
examples can substantially improve the generalization performance of a classiﬁer.
To study dropout, Wager et al. (2014) assume that documents can be summarized
as Poisson word counts. Speciﬁcally, assume that each document has an underlying
topic associated with a word frequency distribution π on the p-dimensional simplex
and an expected length T ≥ 0, and that, given π and T , the word counts Xj
(cid:12)
are independently generated as Xj
(cid:12) T, π ∼ Pois(T πj). The analysis of Wager
et al. (2014) then builds on a duality between dropout and the above generative
model. Consider the example given in Figure 1.1, where dropout creates pseudo-
documents (cid:101)X by deleting half the words at random from the original document
X. As explained in detail in Section 1.2.1, if X itself is drawn from the above
Poisson model, then the dropout pseudo-examples (cid:101)X are marginally distributed
(cid:12)
as (cid:101)Xj
(cid:12) T, π ∼ Pois(0.5 T πj). Thus, in the context of this Poisson generative
model, dropout enables us to create new, shorter pseudo-examples that preserve
the generative structure of the problem.

The above interpretation of dropout raises the following question: if feature
deletion is a natural way to create information-poor pseudo-examples for document
classiﬁcation, are there natural analogous feature noising schemes that can be
applied to other problems? In this chapter, we seek to address this question, and
study a more general family of data augmentation methods generalizing dropout,
based on L´evy processes: We propose an abstract L´evy thinning scheme that reduces
to dropout in the Poisson generative model considered by Wager et al. (2014). Our
framework further suggests new methods for feature noising such as Gamma noising
based on alternative generative models, all while allowing for a uniﬁed theoretical
analysis.

In the above discussion,
From generative modeling to data augmentation.
we treated the expected document length T as ﬁxed. More generally, we could
imagine the document as growing in length over time, with the observed document
X merely a “snapshot” of what the document looks like at time T . Formally, we
(cid:12)
(cid:12) π ∼
can imagine a latent Poisson process (At)t≥0, with ﬁxed-t marginals (At)j
Pois(t πj), and set X = AT . In this notation, dropout amounts to “rewinding”
the process At to obtain short pseudo-examples. By setting (cid:101)X = AαT , we have
P[ (cid:101)X = ˜x (cid:12)

(cid:12) AT = x], for thinning parameter α ∈ (0, 1).

(cid:12) X = x] = P[AαT = ˜x (cid:12)

The main result of this chapter is that the analytic tools developed by Wager
et al. (2014) are not restricted to the case where (At) is a Poisson process, and in
fact hold whenever (At) is a L´evy process. In other words, their analysis applies
to any classiﬁcation problem where the features X can be understood as time-T
snapshots of a process (At), i.e., X = AT .

Recall that a L´evy process (At)t≥0 is a stochastic process with A0 = 0 that
has independent and stationary increments: {Ati − Ati−1} are independent for
d= At−s for and s < t. Common examples
0 = t0 < t1 < t2 < · · · , and At − As

4

Data Augmentation via L´evy Processes

Figure 1.2: Graphical model depicting our generative assumptions; note that we
are not ﬁtting this generative model. Given class Y , we draw a topic θ, which
governs the parameters of the L´evy process (At). We slice at time T to get the
original input X = AT and at an earlier time ˜T to get the thinned or noised input
˜X = A ˜T . We show that given X, we can sample ˜X without knowledge of θ.

of L´evy processes include Brownian motion and Poisson processes.

In any such L´evy setup, we show that it is possible to devise an analogue to
dropout that creates pseudo-examples by rewinding the process back to some earlier
time (cid:101)T ≤ T . Our generative model is depicted in Figure 1.2: (At), the information
relevant to classifying Y , is governed by a latent topic θ ∈ Rp. L´evy thinning then
seeks to rewind (At)—importantly as we shall see, without having access to θ.

We should think of (At) as representing an ever-accumulating amount of infor-
mation concerning the topic θ: In the case of document classiﬁcation, (At) are the
word counts associated with a document that grows longer as t increases. In other
examples that we discuss in Section 1.3, At will represent the sum of t independent
noisy sensor readings. The independence of increments property assures that as we
progress in time, we are always obtaining new information. The stopping time T
thus represents the information content in input X about topic θ. L´evy thinning
seeks to improve classiﬁcation accuracy by turning a few information-rich examples
X into many information-poor examples (cid:101)X.

We emphasize that, although our approach uses generative modeling to motivate
a data augmentation scheme, we do not in fact ﬁt a generative model. This
presents a contrast to the prevailing practice: two classical approaches to multiclass
classiﬁcation are to either directly train a discriminative model by running, e.g.,
multiclass logistic regression on the n original training examples; or, at the other
extreme, to specify and ﬁt a simple parametric version of the above generative
model, e.g., naive Bayes, and then use Bayes’ rule for classiﬁcation. It is well
known that the latter approach is usually more eﬃcient if it has access to a
correctly speciﬁed generative model, but may be badly biased in case of model
misspeciﬁcation (Efron, 1975; Ng and Jordan, 2002; Liang and Jordan, 2008). Here,
we ﬁrst seek to devise a noising scheme X → (cid:101)X and then to train a discriminative
model on the pseudo-examples ( (cid:101)X, Y ) instead of the original examples (X, Y ). Note

1.1 Introduction

5

Figure 1.3: We model each input X as a slice of a L´evy process at time T .
We generate noised examples (cid:101)X by “stepping back in time” to ˜T . Note that the
examples of the two classes are closer together now, thus forcing the classiﬁer to
work harder.

that even if the generative model is incorrect, this approach will incur limited bias as
long as the noising scheme roughly preserves class boundaries — for example, even
if the Poisson document model is incorrect, we may still be justiﬁed in classifying a
subsampled travel document as a travel document. As a result, this approach can
take advantage of an abstract generative structure while remaining more robust to
model misspeciﬁcation than parametric generative modeling.

Overview of results. We consider the multiclass classiﬁcation setting where we
seek to estimate a mapping from input X to class label Y . We imagine that each
X is generated by a mixture of L´evy process, where we ﬁrst draw a random topic θ
given the class Y , and then run a L´evy process (At) depending on θ to time T . In
order to train a classiﬁer, we pick a thinning parameter α ∈ (0, 1), and then create
(cid:12)
pseudo examples by rewinding the original X back to time α T , i.e., (cid:101)X ∼ AαT
(cid:12) AT .
We show three main results in this chapter. Our ﬁrst result is that we can generate
such pseudo-examples (cid:101)X without knowledge of the parameters θ governing the
generative L´evy process. In other words, while our method posits the existence of
a generative model, our algorithm does not actually need to estimate it. Instead,
it enables us to give hints about a potentially complex generative structure to a
discriminative model such as logistic regression.

Second, under assumptions that our generative model is correct, we show that

6

Data Augmentation via L´evy Processes

feature noising preserves the Bayes decision boundary: P[Y | X = x] = P[Y | (cid:101)X =
x]. This means that feature noising does not introduce any bias in the inﬁnite data
limit.

Third, we consider the limit of rewinding to the beginning of time (α → 0). Here,
we establish conditions given which, even with ﬁnite data, the decision boundary
obtained by ﬁtting a linear classiﬁer on the pseudo-examples is equivalent to that
induced by a simpliﬁed generative model. When this latter result holds, we can
interpret α-thinning as providing a semi-generative regularization path for logistic
regression, with a simple generative procedure at one end and unregularized logistic
regression at the other.

Related work. The trade-oﬀ between generative models and discriminative
models has been explored extensively. Rubinstein and Hastie (1997) empirically
compare discriminative and generative classiﬁers models with respect to bias and
variance, Efron (1975) and Ng and Jordan (2002) provide a more formal discussion
of the bias-variance trade-oﬀ between logistic regression and naive Bayes. Liang
and Jordan (2008) perform an asymptotic analysis for general exponential families.
A number of papers study hybrid loss functions that combine both a joint and
conditional likelihood (Raina et al., 2004; Bouchard and Triggs, 2004; Lasserre et al.,
2006; McCallum et al., 2006; Liang and Jordan, 2008). The data augmentation
approach we advocate in this chapter is fundamentally diﬀerent, in that we are
merely using the structural assumptions implied by the generative models to
generate more data, and are not explicitly ﬁtting a full generative model.

The present work was initially motivated by understanding dropout training
(Srivastava et al., 2014), which was introduced in the context of regularizing
deep neural networks, and has had much empirical success (Ba and Frey, 2013;
Goodfellow et al., 2013; Krizhevsky et al., 2012; Wan et al., 2013). Many of the
regularization beneﬁts of dropout can be found in logistic regression and other
single-layer models, where it is also known as “blankout noise” (Globerson and
Roweis, 2006; van der Maaten et al., 2013) and has been successful in natural
language tasks such as document classiﬁcation and named entity recognition (Wager
et al., 2013; Wang and Manning, 2013; Wang et al., 2013). There are a number of
theoretical analyses of dropout: using PAC-Bayes framework (McAllester, 2013),
comparing dropout to “altitude training” (Wager et al., 2014), and interpreting
dropout as a form of adaptive regularization (Baldi and Sadowski, 2014; Bishop,
1995; Helmbold and Long, 2015; Josse and Wager, 2014; Wager et al., 2013).

1.2 L´evy Thinning

We begin by brieﬂy reviewing the results of Wager et al. (2014), who study dropout
training for document classiﬁcation from the perspective of thinning documents
(Section 1.2.1). Then, in Section 1.2.2, we generalize these results to the setting of

1.2 L´evy Thinning

7

generic L´evy generative models.

1.2.1 Motivating Example: Thinning Poisson Documents

Suppose we want to classify documents according to their subject, e.g., sports,
politics, or travel. As discussed in the introduction, common sense intuition about
the nature of documents suggests that a short snippet of a sports document
should also be classiﬁed as a sports document. If so, we can generate many new
training examples by cutting up the original documents in our dataset into shorter
subdocuments and labeling each subdocument with the same label as the original
document it came from. By training a classiﬁer on all of the pseudo-examples we
generate in this way, we should be able to obtain a better classiﬁer.

In order to formalize this intuition, we can represent a document as a sequence
of words from a dictionary {1, . . . , d}, with the word count Xj denoting the
number of occurrences of word j in the document. Given this representation, we
can easily create “subdocuments” by binomially downsampling the word counts Xj
independently. That is, for some ﬁxed downsampling fraction α ∈ (0, 1), we draw

(cid:101)Xj | Xj ∼ Binom(Xj, α).

In other words, we keep each occurrence of word j independently with probability
α.

Wager et al. (2014) study this downsampling scheme in the context of a Poisson
mixture model for the inputs X that obeys the structure of Figure 1.2: ﬁrst, we
draw a class Y ∈ {1, . . . , K} (e.g., travel) and a “topic” θ ∈ Rd (e.g., corresponding
to travel in Norway). The topic θ speciﬁes a distribution over words,

(1.1)

(1.2)

µj(θ) = eθj ,

where, without loss of generality, we assume that (cid:80)d
j=1 eθj = 1. We then draw a
Pois(T ) number of words, where T is the expected document length, and generate
each word independently according to θ. Equivalently, each word count is an
independent Poisson random variable, Xj ∼ Pois(T µj(θ)). The following is an
example draw of a document:

Y = travel

norway
(cid:122)(cid:125)(cid:124)(cid:123)
0.5 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
0.5 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
1.2 ,

skyscraper
(cid:122) (cid:125)(cid:124) (cid:123)
−2.7 , . . . ]

θ = [

norway
(cid:122)(cid:125)(cid:124)(cid:123)

2 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
1 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
3 ,

skyscraper
(cid:122)(cid:125)(cid:124)(cid:123)
0

, . . . ]

X = [

norway
(cid:122)(cid:125)(cid:124)(cid:123)

1 ,

fjord
(cid:122)(cid:125)(cid:124)(cid:123)
0 ,

the
(cid:122)(cid:125)(cid:124)(cid:123)
1 ,

skyscraper
(cid:122)(cid:125)(cid:124)(cid:123)
0

, . . . ]

(cid:101)X = [

Let us now try to understand the downsampling scheme (cid:101)X | X in the context of the

8

Data Augmentation via L´evy Processes

Poisson topic model over X. For each word j, recall that (cid:101)Xj | Xj ∼ Binom(Xj, α).
If we marginalize over X, then we have:

(cid:101)Xj | T, θ ∼ Pois (αT µj(θ)) .

(1.3)

(1.4)

As a result, the distribution of (cid:101)X is exactly the distribution of X if we replaced T
with (cid:101)T = αT .

We can understand this thinning by embedding the document X in a multivariate
Poisson process (At)t≥0, where the marginal distribution of At ∈ {0, 1, 2, . . . }d is
deﬁned to be the distribution over counts when the expected document length is t.
Then, we can write

X = AT ,

(cid:101)X = A

(cid:101)T .

Thus, under the Poisson topic model, the binomial thinning procedure does not
alter the structure of the problem other than by shifting the expected document
length from T to (cid:101)T . Figure 1.4 illustrates one realization of L´evy thinning in the
Poisson case with a three-word dictionary. Note that in this case we can sample
(cid:101)X = AαT given X = AT without knowledge of θ.

This perspective lies at the heart of the analysis in Wager et al. (2014), who
show under the Poisson model that, when the overall document length (cid:107)X(cid:107)1
is independent of the topic θ, thinning does not perturb the optimal decision
boundary. Indeed, the conditional distribution over class labels is identical for the
original features and the thinned features:

P[Y | X = x] = P[Y | (cid:101)X = x].

(1.5)

This chapter extends the result to general L´evy processes (see Theorem 1.2).

This last result (1.5) may appear quite counterintuitive: for example, if A60 is
more informative than A40, how can it be that downsampling does not perturb
the conditional class probabilities? Suppose x is a 40-word document ((cid:107)x(cid:107)1 = 40).
When t = 60, most of the documents will be longer than 40 words, and thus x will be
less likely under t = 60 than under t = 40. However, (1.5) is about the distribution
of Y conditioned on a particular realization x. The claim is that, having observed
x, we obtain the same information about Y regardless of whether t, the expected
document length, is 40 or 60.

1.2.2 Thinning L´evy Processes

The goal of this section is to extend the Poisson topic model from Section 1.2.1
and construct general thinning schemes with the invariance property of (1.5). We
will see that L´evy processes provide a natural vehicle for such a generalization: The
Poisson process used to generate documents is a speciﬁc L´evy process, and binomial
sampling corresponds to “rewinding” the L´evy process back in time.

Consider the multiclass classiﬁcation problem of predicting a discrete class Y ∈
{1, . . . , K} given an input vector X ∈ Rd. Let us assume that the joint distribution

1.2 L´evy Thinning

9

over (X, Y ) is governed by the following generative model:

1. Choose Y ∼ Mult(π), where π is on the K-dimensional simplex.

2. Draw a topic θ | Y , representing a subpopulation of class Y .
3. Construct a L´evy process (At)t≥0 | θ, where At ∈ Rd is a potential input vector
at time t.

4. Observe the input vector X = AT at a ﬁxed time T .

While the L´evy process imposes a fair amount of structure, we make no assump-
tions about the number of topics, which could be uncountably inﬁnite, or about
their distribution, which could be arbitrary. Of course, in such an unconstrained
non-parametric setting, it would be extremely diﬃcult to adequately ﬁt the genera-
tive model. Therefore, we take a diﬀerent tack: We will use the structure endowed by
the L´evy process to generate pseudo-examples for consumption by a discriminative
classiﬁer. These pseudo-examples implicitly encode our generative assumptions.

The natural way to generate a pseudo-example ( (cid:101)X, Y ) is to “rewind” the L´evy
process (At) backwards from time T (recall X = AT ) to an earlier time (cid:101)T = αT
(cid:101)T . In practice, (At) is
for some α ∈ (0, 1) and deﬁne the thinned input as (cid:101)X = A
unobserved, so we draw (cid:101)X conditioned on the original input X = AT and topic θ.
In fact, we can draw many realizations of (cid:101)X | X, θ.

Our hope is that a single full example (X, Y ) is rich enough to generate many
diﬀerent pseudo-examples ( (cid:101)X, Y ), thus increasing the eﬀective sample size. More-
over, Wager et al. (2014) show that training with such pseudo-examples can also
lead to a somewhat surprising “altitude training” phenomenon whereby thinning
yields an improvement in generalization performance because the pseudo-examples
are more diﬃcult to classify than the original examples, and thus force the learning
algorithm to work harder and learn a more robust model.

A technical diﬃculty is that generating (cid:101)X | X, θ seemingly requires knowledge of
the topic θ driving the underlying L´evy process (At). In order to get around this
issue, we establish the following condition under which the observed input X = AT
alone is suﬃcient—that is, P[ (cid:101)X | X, θ] does not actually depend on θ.
Assumption 1.1 (exponential family structure). The L´evy process (At) (cid:12)
(cid:12) θ is
drawn according to an exponential family model whose marginal density at time
t is

f (t)
θ (x) = exp [θ · x − tψ (θ)] h(t) (x) for every t ∈ R.

(1.6)

Here, the topic θ ∈ Rd is an unknown parameter vector, and h(t)(x) is a family of
carrier densities indexed by t ∈ R.

The above assumption is a natural extension of a standard exponential family
assumption that holds for a single value of t. Speciﬁcally, suppose that h(t)(x),
t > 0, denotes the t-marginal densities of a L´evy process, and that f (1)
(x) =
exp [θ · x − ψ (θ)] h(1)(x) is an exponential family through h(1)(x) indexed by θ ∈
Rd. Then, we can verify that the densities speciﬁed in (1.6) induce a family of

θ

10

Data Augmentation via L´evy Processes

Figure 1.4: Illustration of our Poisson process document model with a three-word
dictionary and µ(θ) = (0.25, 0.3, 0.45). The word counts of the original document,
X = (8, 7, 16), represents the trivariate Poisson process At, sliced at T = 28. The
thinned pseudo-document (cid:101)X = (2, 4, 9) represents At sliced at (cid:101)T = 14.

L´evy processes indexed by θ. The key observation in establishing this result is that,
because h(t)(x) is the t-marginal of a L´evy process, the L´evy–Khintchine formula
implies that

(cid:90)

eθ·xh(t)(x) dx =

eθ·xh(1)(x) dx

= et ψ(θ),

(cid:18)(cid:90)

(cid:19)t

and so the densities in (1.6) are properly normalized.

We also note that, given this assumption and as T → ∞, we have that AT /T
converges almost surely to µ(θ) def= E [A1]. Thus, the topic θ can be understood as
a description of an inﬁnitely informative input. For ﬁnite values of T , X represents
a noisy observation of the topic θ.

Now, given this structure, we show that the distribution of (cid:101)X = AαT conditional
on X = AT does not depend on θ. Thus, feature thinning is possible without
knowledge of θ using the L´evy thinning procedure deﬁned below. We note that,
in our setting, the carrier distributions h(t)(x) are always known; in Section 1.3,
we discuss how to eﬃciently sample from the induced distribution g(αT ) for some
speciﬁc cases of interest.

Theorem 1.1 (L´evy thinning). Assume that (At) satisﬁes the exponential family
structure in (1.6), and let α ∈ (0, 1) be the thinning parameter. Then, given an
input X = AT and conditioned on any θ, the thinned input (cid:101)X = AαT has the

1.2 L´evy Thinning

11

(1.7)

(1.8)

following density:

g(αT )(˜x; X) =

h(αT )(˜x) h((1−α)T )(X − ˜x)
h(T )(X)

,

which importantly does not depend on θ.

Proof. Because the L´evy process (At) has independent and stationary increments,
we have that AαT ∼ f (αT )
and AT − AαT ∼ f ((1−α)T )
are independent. Therefore,
we can write the conditional density of AαT given AT as the joint density over
(AαT , AT ) (equivalently, the reparametrization (AαT , AT − AαT )) divided by the
marginal density over AT :

θ

θ

g(αT )(˜x; X) =

f (αT )
θ

(X − ˜x)

(˜x)f ((1−α)T )
θ
f (T )
(X)
θ
exp [θ · ˜x − αT ψ(θ)] h(αT )(˜x)

(cid:16)

(cid:17)

=

×

×

(cid:16)

(cid:16)

exp [θ · (X − ˜x) − (1 − α)T ψ(θ)] h((1−α)T )(X − ˜x)

(cid:17)

exp [θ · X − T ψ(θ)] h(T )(X)

(cid:17)−1

,

where the last step expands everything (1.6). Algebraic cancellation, which removes
all dependence on θ, completes the proof.

Note that while Theorem 1.1 guarantees we can carry out feature thinning
without knowing the topic θ, it does not guarantee that we can do it without
knowing the information content T . For Poisson processes, the binomial thinning
mechanism depends only on α and not on the original T . This is a convenient
property in the Poisson case but does not carry over to all L´evy processes — for
example, if Bt is a standard Brownian motion, then the distribution of B2 given
B4 = 0 is N(0, 1), while the distribution of B200 given B400 = 0 is N(0, 100).
As we will see in Section 1.3, thinning in the Gaussian and Gamma families
does require knowing T , which will correspond to a “sample size” or “precision.”
Likewise, Theorem 1.1 does not guarantee that sampling from (1.7) can be carried
out eﬃciently; however, in all the examples we present here, sampling can be carried
out easily in closed form.

1.2.3 Learning with Thinned Features

Having shown how to thin the input X to (cid:101)X without knowledge of θ, we can
proceed to deﬁning our full data augmentation strategy. We are given n training
examples {(X (i), Y (i))}n
i=1. For each original input X (i), we generate B thinned
versions (cid:101)X (i,1), . . . , (cid:101)X (i,B) by sampling from (1.7). We then pair these B examples
up with Y (i) and train any discriminative classiﬁer on these Bn examples. Algo-
rithm 1 describes the full procedure where we specialize to logistic regression. If
one is implementing this procedure using stochastic gradient descent, one can also

12

Data Augmentation via L´evy Processes

Procedure 1. Logistic Regression with L´evy Regularization
Input: n training examples (X (i), Y (i)), a thinning parameter α ∈ (0, 1), and a feature
map φ : Rd (cid:55)→ Rp.

1. For each training example X (i), generate B thinned versions ( (cid:101)X (i,b))B
to (1.7).

b=1 according

2. Train logistic regression on the resulting pseudo-examples:

(cid:40) n
(cid:88)

B
(cid:88)

(cid:41)
β; (cid:101)X (i,b), Y (i)(cid:17)
(cid:16)

,

(cid:96)

ˆβ def= argmin
β∈Rp×K

i=1

b=1

where the multi-class logistic loss with feature map φ is

(cid:96)(β; x, y) def= log

eβ(k)·φ(x)

− β(y) · φ(x).

(cid:33)

(cid:32) K
(cid:88)

k=1

3. Classify new examples according to

ˆy(x) = argmin

k∈{1, ..., K}

(cid:110)

ˆc(k) − ˆβ(k) · φ(x)

(cid:111)

,

(1.9)

(1.10)

(1.11)

where the ˆck ∈ R are optional class-speciﬁc calibration parameters for k = 1, . . . , K.

generate a fresh thinned input (cid:101)X whenever we sample an input X on the ﬂy, which
is the usual implementation of dropout training (Srivastava et al., 2014).

In the ﬁnal step (1.11) of Algorithm 1, we also allow for class-speciﬁc calibration
parameters . After the ˆβ(k) have been determined by logistic regression with L´evy
regularization, these parameters ˆc(k) can be chosen by optimizing the logistic loss on
the original uncorrupted training data. As discussed in Section 1.2.5, re-calibrating
the model is recommended, especially when α is small.

1.2.4 Thinning Preserves the Bayes Decision Boundary

We can easily implement the thinning procedure, but how will it aﬀect the accuracy
of the classiﬁer? The following result gives us a ﬁrst promising piece of the answer
by establishing conditions under which thinning does not aﬀect the Bayes decision
boundary.

At a high level, our results rely on the fact that under our generative model, the
“amount of information” contained in the input vector X is itself uninformative
about the class label Y .

Assumption 1.2 (Equal information content across topics). Assume there exists
a constant ψ0 such that ψ(θ) = ψ0 with probability 1, over random θ.

For example, in our Poisson topic model, we imposed the restriction that ψ(θ) =

1.2 L´evy Thinning

13

(1.12)

(1.13)

(1.14)

(1.15)

(cid:80)d

j=1 eθj = 1, which ensures that the document length (cid:107)At(cid:107)1 has the same

distribution (which has expectation ψ(θ) in this case) for all possible θ.

Theorem 1.2. Under Assumption 1.2, the posterior class probabilities are invari-
ant under thinning (1.7):

(cid:104)
Y = y (cid:12)

P

(cid:12) (cid:101)X = x

(cid:105)

= P (cid:2)Y = y (cid:12)

(cid:12) X = x(cid:3)

for all y ∈ {1, . . . , K} and x ∈ X.

Proof. Given Assumption 1.2, the density of At | θ is given by:

f (t)
θ (x) = eθ·xe−tψ0h(t)(x),

which importantly splits into two factors, one depending on (θ, x), and the other
depending on (t, x). Now, let us compute the posterior distribution:

P (cid:2)Y = y (cid:12)

(cid:12) At = x(cid:3) ∝ P [Y = y]

P [θ | Y ] f (t)

θ (x)dθ

∝ P [Y = y]

P [θ | Y ] eθ·xdθ,

(cid:90)

(cid:90)

which does not depend on t, as e−tψ0 h(t)(x) can be folded into the normalization
constant. Recall that X = AT and (cid:101)X = A
(cid:101)T . Substitute t = T and t = (cid:101)T to conclude
(1.12).

To see the importance of Assumption 1.2, consider the case where we have two
labels (Y ∈ {1, 2}), each with a single topic (Y yields topic θY ). Suppose that
ψ(θ2) = 2ψ(θ1)—that is, documents in class 2 are on average twice as long as those
in class 1. Then, we would be able to make class 2 documents look like class 1
documents by thinning them with α = 0.5.

Remark 1.1. If we also condition on the information content T , then an analogue
to Theorem 1.2 holds even without Assumption 1.2:

(cid:104)
Y = y (cid:12)

P

(cid:12) (cid:101)X = x, (cid:101)T = t

= P (cid:2)Y = y (cid:12)

(cid:12) X = x, T = t(cid:3) .

(cid:105)

(1.16)

This is because, after conditioning on T , the e−tψ(θ) term factors out of the
likelihood.

The upshot of Theorem 1.2 is that thinning will not induce asymptotic bias
whenever an estimator produces P (cid:2)Y = y (cid:12)
(cid:12) X = x(cid:3) in the limit of inﬁnite data
(n → ∞), i.e., if the logistic regression (Algorithm 1) is well-speciﬁed. Speciﬁcally,
training either on original examples or thinned examples will both converge to the
true class-conditional distribution. The following result assumes that the feature
space X is discrete; the proof can easily be generalized to the case of continuous
features.

Corollary 1.3. Suppose that Assumption 1.2 holds, and that the above multi-class

14

Data Augmentation via L´evy Processes

logistic regression model is well-speciﬁed, i.e., P (cid:2)Y = y (cid:12)
(cid:12) X = x(cid:3) ∝ eβ(y)·φ(x) for
some β and all y = 1, ..., K. Then, assuming that P [At = x] > 0 for all x ∈ X and
t > 0, Algorithm 1 is consistent, i.e., the learned classiﬁcation rule converges to the
Bayes classiﬁer as n → ∞.
Proof. At a ﬁxed x, the population loss E (cid:2)(cid:96) (cid:0)β; X, Y (cid:12)
any choice of β satisfying:
exp (cid:2)β(y) · φ(x)(cid:3)
k=1 exp (cid:2)β(k) · φ(x)(cid:3) = P (cid:2)Y = y (cid:12)

(cid:12) X = x(cid:1)(cid:3) is minimized by

(cid:12) X = x(cid:3)

(1.17)

(cid:80)K

for all y = 1, ..., K. Since the model is well-speciﬁed and by assumption P[ (cid:101)X =
x] > 0 for all x ∈ X, we conclude that weight vector ˆβ learned using Algorithm 1
must satisfy asymptotically (1.17) for all x ∈ X as n → ∞.

1.2.5 The End of the Path

As seen above, if we have a correctly speciﬁed logistic regression model, then
L´evy thinning regularizes it without introducing any bias. However, if the logistic
regression model is misspeciﬁed, thinning will in general induce bias, and the
amount of thinning presents a bias-variance trade-oﬀ. The reason for this bias is that
although thinning preserves the Bayes decision boundary, it changes the marginal
distribution of the covariates X, which in turn aﬀects logistic regression’s linear
approximation to the decision boundary. Figure 1.5 illustrates this phenomenon in
the case where At is a Brownian motion, corresponding to Gaussian feature noising;
Wager et al. (2014) provides a similar example for the Poisson topic model.

Fully characterizing the bias of L´evy thinning is beyond the scope of this paper.
However, we can gain some helpful insights about this bias by studying “strong
thinning”—i.e., L´evy thinning in the limit as the thinning parameter α → 0:

ˆβ0+

def= lim
α→0

lim
B→∞

ˆβ(α, B),

(1.18)

where ˆβ(α, B) is deﬁned as in (1.9) with the explicit dependence on α and B.
For each α, we take B → ∞ perturbed points for each of the original n data
points. As we show in this section, this limiting classiﬁer is well-deﬁned under weak
conditions; moreover, in some cases of interest, it can be interpreted as a simple
generative classiﬁer. The result below concerns the existence of ˆβ0+, and establishes
that it is the empirical minimizer of a convex loss function.

Theorem 1.4. Assume the setting of Procedure 1, and let the feature map be
φ(x) = x. Assume that the generative L´evy process (At) has ﬁnitely many jumps in
expectation over the interval [0, T ]. Then, the limit ˆβ0+ is well-deﬁned and can be
written as

ˆβ0+ = argmin
β∈Rp×K

(cid:40) n
(cid:88)

i=1

(cid:16)

β; X (i), Y (i)(cid:17)

ρ

(cid:41)

,

(1.19)

1.2 L´evy Thinning

15

(cid:12) θ, T ∼ N (cid:0)T θ, σ2T Ip×p

Figure 1.5: The eﬀect of L´evy thinning with data generated from a Gaussian
model of the form X (cid:12)
(cid:1), as described in Section 1.3.1.
The outer circle depicts the distribution of θ conditional on the color Y : blue
points all have θ ∝ (cos(0.75 π/2), sin(0.75 π/2)), whereas the red points have
θ ∝ (cos(ω π/2), sin(ω π/2)) where ω is uniform between 0 and 2/3. Inside this
circle, we see 3 clusters of points generated with T = 0.1, 0.4, and 1, along with
logistic regression decision boundaries obtained from each cluster. The dashed line
shows the Bayes decision boundary separating the blue and red points, which is the
same for all T (Theorem 1.2). Note that the logistic regression boundaries learned
from data with diﬀerent T are not the same. This issue arises because the Bayes
decision boundary is curved, and the best linear approximation to a curved Bayes
boundary changes with T .

for some convex function ρ(·; x, y).

The proof of Theorem 1.4 is provided in the appendix. Here, we begin by
establishing notation that lets us write down an expression for the limiting loss
ρ. First, note that Assumption 1.1 implicitly requires that the process (At) has
ﬁnite moments. Thus, by the L´evy–It¯o decomposition, we can uniquely write this
process as

At = bt + Wt + Nt,

(1.20)

where b ∈ Rp, Wt is a Wiener process with covariance Σ, and Nt is a compound
Poisson process which, by hypothesis, has a ﬁnite jump intensity.

Now, by an argument analogous to that in the proof of Theorem 1.1, we see that

16

Data Augmentation via L´evy Processes

the joint distribution of WT and NT conditional on AT does not depend on θ. Thus,
we can deﬁne the following quantities without ambiguity:

(cid:12)
µT (x) = bT + E (cid:2)WT
(cid:12) AT = x(cid:3) ,
λT (x) = E (cid:2)number of jumps in (At) for t ∈ [0, T ] (cid:12)
νT (z; x) = lim
t→0

(cid:12) Nt (cid:54)= 0, AT = x(cid:3) .

P (cid:2)Nt = z (cid:12)

(cid:12) AT = x(cid:3) ,

(1.21)

(1.22)

(1.23)

More prosaically, νT (·; x) can be described as the distribution of the ﬁrst jump
of (cid:101)Nt, a thinned version of the jump process Nt. In the degenerate case where
P (cid:2)NT = 0 (cid:12)
(cid:12) AT = x(cid:3) = 0, we set νT (·; x) to be a point mass at z = 0.

Given this notation, we can write the eﬀective loss function ρ for strong thinning

as

ρ (β; x, y) = −µT (x) · β(y) +

β(k)(cid:62)Σβ(k)

(1.24)

T
2

1
K

K
(cid:88)

k=1

(cid:90)

+ λT (x)

(cid:96) (β; z, y) dνT (z; x),

provided we require without loss of generality that (cid:80)K
k=1 β(k) = 0. In other words,
the limiting loss can be described entirely in terms of the distribution of the ﬁrst
jump of (cid:101)Nt, and continuous part Wt of the L´evy process. The reason for this
phenomenon is that, in the strong thinning limit, the pseudo-examples (cid:101)X ∼ AαT
can all be characterized using either 0 or 1 jumps.

Aggregating over all the training examples, we can equivalently write this strong

thinning loss as

(cid:16)

β; X (i), Y (i)(cid:17)

ρ

=

n
(cid:88)

i=1

1
2T

n
(cid:88)

i=1

γ−1
Y (i)

(cid:13)
(cid:13)
(cid:13)γY (i) µT

X (i)(cid:17)
(cid:16)

− T Σβ(Y (i))(cid:13)
2
(cid:13)
(cid:13)

Σ−1

λT (X (i))

(cid:90)

(cid:16)

β; z, Y (i)(cid:17)

(cid:96)

dνT (z; X (i)),

(1.25)

+

n
(cid:88)

i=1

2 terms that do not depend on β. Here, 1

up to (cid:107)µT (cid:107)2
2 v(cid:48)Σ−1v corresponds
to the Gaussian log-likelihood with covariance Σ (up to constants), and γy =
K (cid:12)
(cid:12) /n measures the over-representation of class y relative to other
(cid:12)
classes.

(cid:8)i : Y (i) = y(cid:9)(cid:12)

Σ−1 = 1

2 (cid:107)v(cid:107)2

(cid:13)
(cid:13)

(cid:80)n

(cid:13)µT (X (i)) − T Σβ(Y (i))(cid:13)

In the case where we have the same number of training examples from
each class (and so γy = 1 for all y = 1, ..., K), the strong thinning loss
can be understood in terms of a generative model. The ﬁrst term, namely
1
, is the loss function for linear classiﬁcation
2T
in a Gaussian mixture with observations µT (X (i)), while the second term is the
logistic loss obtained by classifying single jumps. Thus, strong thinning is eﬀec-
tively seeking the best linear classiﬁer for a generative model that is a mixture of
Gaussians and single jumps.

2
(cid:13)
(cid:13)

Σ−1

i=1

In the pure jump case (Σ = 0), we also note that strong thinning is closely related

1.3 Examples

17

1.3 Examples

to naive Bayes classiﬁcation. In fact, if the jump measure of Nt has a ﬁnite number
of atoms that are all linearly independent, then we can verify that the parameters
ˆβ0+ learned by strong thinning are equivalent to those learned via naive Bayes,
although the calibration constants c(k) may be diﬀerent.

At a high level, by elucidating the generative model that strong thinning pushes
us towards, these results can help us better understand the behavior of L´evy
thinning for intermediate value of α, e.g., α = 1/2. They also suggest caution with
respect to calibration: For both the diﬀusion and jump terms, we saw above that
L´evy thinning gives helpful guidance for the angle of β(k), but does not in general
(cid:13)β(k)(cid:13)
elegantly account for signal strength (cid:13)
(cid:13)2 or relative class weights. Thus, we
recommend re-calibrating the class decision boundaries obtained by L´evy thinning,
as in Algorithm 1.

So far, we have developed our theory of L´evy thinning using the Poisson topic
model as a motivating example, which corresponds to dropping out words
from a document. In this section, we present two models based on other L´evy
processes—multivariate Brownian motion (Section 1.3.1) and Gamma processes
(Section 1.3.2)— exploring the consequences of L´evy thinning.

1.3.1 Multivariate Brownian Motion

Consider a classiﬁcation problem where the input vector is the aggregation of
multiple noisy, independent measurements of some underlying object. For example,
in a biomedical application, we might want to predict a patient’s disease status
based on a set of biomarkers such as gene expression levels or brain activity.
A measurement is typically obtained through a noisy experiment involving an
microarray or fMRI, so multiple experiments might be performed and aggregated.
More formally, suppose that patient i has disease status Y (i) and expression level
µi ∈ Rd for d genes, with the distribution of µi diﬀerent for each disease status.
Given µi, suppose the t-th measurement for patient i is distributed as

Zi,t ∼ N(µi, Σ),

(1.26)

where Σ ∈ Rd×d is assumed to be a known, ﬁxed matrix. Let the observed input be
X (i) = (cid:80)Ti
t=1 Zi,t, the sum of the noisy measurements. If we could take inﬁnitely
many measurements (Ti → ∞), we would have X (i)/Ti → µi almost surely; that
is, we would observe gene expression noiselessly. For ﬁnitely many measurements,
X (i) is a noisy proxy for the unobserved µi.

We can model the process of accumulating measurements with a multivariate

18

Data Augmentation via L´evy Processes

Brownian motion (At):

At = tµ + Σ1/2Bt,

where Bt is a d-dimensional white Brownian motion.1 For integer values of t, At
represents the sum of the ﬁrst t measurements, but At is also deﬁned for fractional
values of t. The distribution of the features X at a given time T is thus

(1.27)

(1.28)

(1.29)

(1.30)

(1.31)

X | µ, T ∼ N(T µ, T Σ),

leading to density

f (t)
µ (x) =

exp (cid:2) 1

2 (x − tµ)(cid:62)(tΣ)−1(x − tµ)(cid:3)

(cid:20)

(2π)d/2 det(Σ)
t
2

(cid:21)
µ(cid:62)Σ−1µ

x(cid:62)Σ−1µ −

= exp

h(t)(x),

where

h(t)(x) =

exp (cid:2)− 1
2t x(cid:62)Σ−1x(cid:3)
(2π)d/2 det(Σ)1/2

.

We can recover the form of (1.6) by setting θ = Σ−1µ, a one-to-one mapping
provided Σ is positive-deﬁnite.

Thinning. The distribution of (cid:101)X = AαT given X = AT is that of a Brownian
bridge process with the following marginals:

(cid:101)X | X ∼ N (αX, α(1 − α)T Σ) .

In this case, “thinning” corresponds exactly to adding zero-mean, additive Gaus-
sian noise to the scaled features αX. Note that in this model, unlike in the Poisson
topic model, sampling (cid:101)X from X does require observing T —for example, knowing
how many observations were taken. The larger T is, the more noise we need to
inject to achieve the same downsampling ratio.

In the Poisson topic model, the features (Xi,1, . . . , Xi,d) were independent of
each other given the topic θi and expected length Ti. By contrast, in the Brownian
motion model the features are correlated (unless Σ is the identity matrix). This
serves to illustrate that independence or dependence of the features is irrelevant to
our general framework; what is important is that the increments Zt = At − At−1
are independent of each other, the key property of a L´evy process.

Assumption 1.2 requires that µ(cid:62)Σ−1µ is constant across topics; i.e., that the
true gene expression levels are equally sized in the Mahalanobis norm deﬁned
by Σ. Clearly, this assumption is overly stringent in real situations. Fortunately,
Assumption 1.2 is not required (see Remark 1.1) as long as T is observed—as it

1. By deﬁnition of Brownian motion, we have marginally that Bt ∼ N(0, tI).

1.3 Examples

19

must be anyway if we want to be able to carry out L´evy thinning.

Thinning X in this case is very similar to subsampling. Indeed, for integer values
of (cid:101)T , instead of formally carrying out L´evy thinning as detailed above, we could
simply resample (cid:101)T values of Zi,t without replacement, and add them together to
obtain (cid:101)X. If there are relatively few repeats, however, the resampling scheme can
(cid:1) pseudo-examples (e.g. 6 pseudo-examples if T = 4 and (cid:101)T = 2),
lead to only (cid:0)T
(cid:101)T
whereas the thinning approach leads to inﬁnitely many possible pseudo-examples
we can use to augment the regression. Moreover, if T = 4 then subsampling leaves
us with only four choices of α; there would be no way to thin using α = 0.1, for
instance.

1.3.2 Gamma Process

As another example, suppose again that we are predicting a patient’s disease status
based on repeated measurements of a biomarker such as gene expression or brain
activity. But now, instead of (or in addition to) the average signal, we want our
features to represent the variance or covariance of the signals across the diﬀerent
measurements.

Assume ﬁrst that the signals at diﬀerent genes or brain locations are independent;

that is, the t-th measurement for patient i and gene j has distribution

Zi,j,t ∼ N(µi,j, σ2

i,j).

(1.32)

Here, the variances σ2
subscript i, after T + 1 measurements we can compute

i,1, . . . , σ2

i = (σ2

i,d) parameterize the “topic.” Suppressing the

Xj,T =

(Zi,j,t − ¯Zi,j,T +1)2,

where

¯Zi,j,T +1 =

Zi,j,t. (1.33)

1
T + 1

T +1
(cid:88)

t=1

T +1
(cid:88)

t=1

Then Xj,T ∼ σ2
j χ2
scale parameter 2σ2
more and more observations (increasing T ), we will have XT /T → (σ2
almost surely.

T , which is a Gamma distribution with shape parameter T /2 and
j (there is no dependence on µi). Once again, as we accumulate
1, . . . , σ2
d)

We can embed Xj,T in a multivariate Gamma process with d independent

coordinates and scale parameters σ2
j :

(At)j ∼ Gamma(t/2, 2σ2

j ).

The density of At given σ2 is

f (t)
σ2 (x) =

d
(cid:89)

j=1

j

e−xj /2σ2
xt/2−1
j
Γ(t/2)2t/2σ2(t/2)


j

= exp

−

xj/2σ2

j − (t/2)

log σ2
j


 h(t)(x),

d
(cid:88)

j=1

d
(cid:88)

j=1

(1.34)

(1.35)

20

Data Augmentation via L´evy Processes

where

h(t)(x) =

(cid:81)

j xt/2−1
j
Γ(t/2)d2dt/2

.

We can recover the form of (1.6) by setting θj = −1/2σ2

j , a one-to-one mapping.

Thinning. Because (cid:101)Xj ∼ Gamma(αT /2, 2σ2
Xj − (cid:101)Xj ∼ Gamma((1 − α)T /2, 2σ2

j ), we have

j ) is independent of the increment

(cid:101)Xj
Xj

| Xj ∼ Beta (αT /2, (1 − α)T /2) .

In other words, we create a noisy (cid:101)X by generating for each coordinate an indepen-
dent multiplicative noise factor

mj ∼ Beta (αT, (1 − α)T )

and setting (cid:101)Xj = mjXj. Once again, we can downsample without knowing σ2
j , but
we do need to observe T . Assumption 1.2 would require that (cid:81)
j is identical for
all topics. This is an unrealistic assumption, but once again it is unnecessary as
long as we observe T .

j σ2

General covariance. More generally, the signals at diﬀerent brain locations, or
expressions for diﬀerent genes, will typically be correlated with each other, and these
correlations could be important predictors. To model this, let the measurements be
distributed as:

Zi,t ∼ N(µi, Σi),

where Σ represents the unknown “topic”—some covariance matrix that is charac-
teristic of a certain subcategory of a disease status.

After observing T + 1 observations we can construct the matrix-valued features:

XT =

(Zi,t − ¯Zi,T +1)(Zi,t − ¯Zi,T +1)(cid:62).

T +1
(cid:88)

t=1

Now XT has a Wishart distribution: XT ∼ Wishd(Σ, T ). When T ≥ d, the density
of At given Σ is

f (t)
Σ (x) = exp

−

tr(Σ−1x) −

log det(Σ)

h(t)(x),

(1.41)

(cid:26)

1
2

(cid:27)

t
2

(1.36)

(1.37)

(1.38)

(1.39)

(1.40)

1.4 Simulation Experiments

where

21

(1.42)

(1.43)

h(t)(x) =

td
2 det(x)

t−d−2

2 Γd

(cid:18)
2

(cid:19)(cid:19)−1

,

(cid:18) t
2

Γd

(cid:19)

(cid:18) t
2

d(d−1)
4

= π

d
(cid:89)

j=1

(cid:18) t
2

Γ

+

1 − j
2

(cid:19)

,

supported on positive-deﬁnite symmetric matrices. If X = AT and αT ≥ d as well,
we can sample a “thinned” observation (cid:101)X from density proportional to

h(αT )(˜x)h(T −αT )(X − ˜x) ∝ det(˜x)

2+d−αT
2

det(X − ˜x)

2+d−(1−α)T
2

,

(1.44)

or after the aﬃne change of variables (cid:101)X = X 1/2M X 1/2, we sample M from density
proportional to det(m)
, a matrix beta distribution.
Here, M may be interpreted as matrix-valued multiplicative noise.

det(Id − m)

2+d−(1−α)t
2

2+d−αT
2

1.4 Simulation Experiments

In this section, we perform several simulations to illustrate the utility of L´evy
thinning. In particular, we will highlight the modularity between L´evy thinning
(which provides pseudo-examples) and the discriminative learner (which ingests
these pseudo-examples). We treat the discriminative learner as a black box, com-
plete with its own internal cross-validation scheme that optimizes accuracy on
pseudo-examples. Nonetheless, we show that accuracy on the original examples
improves when we train on thinned examples.

More speciﬁcally, given a set of training examples {(X, Y )}, we ﬁrst use L´evy
thinning to generate a set of pseudo-examples {( (cid:101)X, Y )}. Then we feed these
examples to the R function cv.glmnet to learn a linear classiﬁer on these pseudo-
examples (Friedman et al., 2010). We emphasize that cv.glmnet seeks to choose
its regularization parameter λ to maximize its accuracy on the pseudo-examples
( (cid:101)X, Y ) rather than on the original data (X, Y ). Thus, we are using cross-validation
as a black box instead of trying to adapt the procedure to the context of L´evy
thinning. In principle, we might be concerned that cross-validating on the pseudo-
examples would yield a highly suboptimal choice of λ, but our experiments will
show that the procedure in fact works quite well.

The two extremes of the path correspond to naive Bayes generative modeling at
one end (α = 0), and plain ridge-regularized logistic regression at the other (α = 1).
All methods were calibrated on the training data as follows: Given original weight
vectors ˆβ, we ﬁrst compute un-calibrated predictions ˆµ = X ˆβ for the log-odds of
P (cid:2)Y = 1 (cid:12)
(cid:12) X(cid:3), and then run a second univariate logistic regression Y ∼ ˆµ to adjust
both the intercept and the magnitude of the original coeﬃcients. Moreover, when us-
ing cross-validation on pseudo-examples ( (cid:101)X, Y ), we ensure that all pseudo-examples
induced by a given example (X, Y ) are in the same cross-validation fold. Code for

22

Data Augmentation via L´evy Processes

Figure 1.6: Performance of L´evy thinning with cross-validated ridge-regularized
logistic regression, on a random Gaussian design described in (1.45). The curves
depict the relationship between thinning α and classiﬁcation error as the number of
training examples grows: n = 30, 50, 75, 100, 150, 200, 400, and 600. We see that
naive Bayes improves over ridge logistic regression in very small samples, while in
moderately small samples L´evy thinning does better than either end of the path.

reproducing our results is available at https://github.com/swager/levythin.

Gaussian example. We generate data from the following hierarchical model:

Y ∼ Binomial (0.5) , µ (cid:12)

(cid:12) Y ∼ LY , X (cid:12)

(cid:12) µ ∼ N (µ, Id×d) ,

(1.45)

1

, ..., µ(Y )

where µ, X ∈ Rd and d = 100. The distribution LY associated with each label
Y consists of 10 atoms µ(Y )
10 . These atoms themselves are all randomly
generated such that their ﬁrst 20 coordinates are independent draws of 1.1 T4 where
T4 follows Student’s t-distribution with 4 degrees of freedom; meanwhile, the last
80 coordinates of µ are all 0. The results in Figure 1.6 are marginalized over the
randomness in LY ; i.e., diﬀerent simulation realizations have diﬀerent conditional
laws for µ given Y . Figure 1.6 shows the results.

Poisson example. We generate data from the following hierarchical model:

Y ∼ Binomial (0.5) , θ (cid:12)

(cid:12) Y ∼ LY , Xj

(cid:12)
(cid:12) θ ∼ Pois

1000

,

(1.46)

(cid:32)

(cid:33)

eθj
j=1 eθj

(cid:80)d

1.4 Simulation Experiments

23

Figure 1.7: Performance of L´evy thinning with cross-validated ridge-regularized
logistic regression, on a random Poisson design described in (1.46). The curves
depict the relationship between thinning α and classiﬁcation accuracy for n =
30, 50, 100, 150, 200, 400, 800, and 1600. Here, aggressive L´evy thinning with
small but non-zero α does substantially better than naive Bayes (α = 0) as soon as
n is moderately large.

where θ ∈ Rd, X ∈ Nd, and d = 500. This time, however, LY is deterministic: If
Y = 0, then θ is just 7 ones followed by 493 zeros, whereas





θ (cid:12)
(cid:12) Y = 1 ∼

0, ..., 0
(cid:124) (cid:123)(cid:122) (cid:125)
7

(cid:12)
(cid:12) τ, ..., τ
(cid:124) (cid:123)(cid:122) (cid:125)
7

(cid:12)
(cid:12) 0, ..., 0
(cid:124) (cid:123)(cid:122) (cid:125)
486

 , with τ ∼ Exp(3).

This generative model was also used in simulations by Wager et al. (2014); the
diﬀerence is that they applied thinning to plain logistic regression, whereas here
we verify that L´evy thinning is also helpful when paired with cross-validated ridge
logistic regression. Figure 1.7 shows the results.

These experiments suggest that it is reasonable to pair L´evy thinning with a well-
tuned black box learner on the pseudo-examples ( (cid:101)X, Y ), without worrying about
potential interactions between L´evy thinning and the tuning of the discriminative
model.

24

Data Augmentation via L´evy Processes

1.5 Discussion

In this chapter, we have explored a general framework for performing data augmen-
tation: apply L´evy thinning and train a discriminative classiﬁer on the resulting
pseudo-examples. The exact thinning scheme reﬂects our generative modeling as-
sumptions. We emphasize that the generative assumptions are non-parametric and
of a structural nature; in particular, we never ﬁt an actual generative model, but
rather encode the generative hints implicitly in the pseudo-examples.

A key result is that under the generative assumptions, thinning preserves the
Bayes decision boundary, which suggests that a well-speciﬁed classiﬁer incurs
no asymptotic bias. Similarly, we would expect that a misspeciﬁed but powerful
classiﬁer should incur little bias. We showed that in limit of maximum thinning,
the resulting procedure corresponds to ﬁtting a generative model. The exact bias-
variance trade-oﬀ for moderate levels of thinning is an interesting subject for further
study.

While L´evy processes provide a general framework for thinning examples, we
recognize that there are many other forms of coarsening that could lead to the
same intuitions. For instance, suppose X | θ is a Markov process over words in
a document. We might expect that short contiguous subsequences of X could
serve as good pseudo-examples. More broadly, there are many forms of data
augmentation that do not have the intuition of coarsening an input. For example,
rotating or shearing an image to generate pseudo-images appeals to other forms
of transformational invariance. It would be enlightening to establish a generative
framework in which data augmentation with these other forms of invariance arise
naturally.

To establish the desired result, we show that for a single training example (X, Y ),
the following limit is well-deﬁned for any β ∈ Rp×K:

ρ (β; X, Y ) = lim
α→0

(cid:16)

(cid:101)E

(cid:16)

(cid:104)
(cid:96)

1
α

= −β(Y ) · X + lim
α→0

β; (cid:101)X, Y
(cid:34)

(cid:32)

1
α

(cid:101)E

log

(cid:17)(cid:105)

(cid:17)

− log (K)

(cid:33)(cid:35)

eβ(k)· (cid:101)X

,

1
K

K
(cid:88)

k=1

(1.47)

where on the second line we wrote down the logistic loss explicitly and exploited
linearity of the term involving Y as in Wager et al. (2013). Here (cid:101)E denotes
expectation with respect to the thinning process and reﬂects the B → ∞ limit.
Because (cid:96) is convex, ρ must also be convex; and by equicontinuity ˆβ(α) must also
converge to its minimizer.

Our argument relies on the decomposition At = bt+Wt+Nt from (1.20). Without
loss of generality, we can generate the pseudo-features (cid:101)X as (cid:101)X = bt + (cid:102)WαT + (cid:101)NαT ,

1.6 Appendix: Proof of Theorem 1.4

1.6 Appendix: Proof of Theorem 1.4

25

where (cid:102)WαT and (cid:101)NαT have the same marginal distribution as WαT and NαT . Given
this notation,

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:33)(cid:35)

(cid:33)

(cid:34)

(cid:32)

log

1
K

1
α

(cid:101)E

(cid:34)

log

=

1
α

(cid:101)E

+

1
α

(cid:101)E

K
(cid:88)

k=1
(cid:32)

1
K

(cid:34)

log

K
(cid:88)

k=1
(cid:32)

1
K

K
(cid:88)

k=1

eβ(k)·(αbT +(cid:102)WαT )

(cid:12)
(cid:12) (cid:101)NαT = 0

P

(cid:104)

(cid:105)
(cid:101)NαT = 0

(cid:35)

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:33)

(cid:35)

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

P

(cid:104)

(cid:105)
(cid:101)NαT (cid:54)= 0

.

We now characterize these terms individually. First, because Nt has a ﬁnite jump
intensity, we can verify that, almost surely,

lim
α→0

1
α

P

(cid:104)

(cid:105)
(cid:101)NαT (cid:54)= 0

= λT (X),

where λT (X) is as deﬁned in (1.22). Next, because (cid:102)WαT concentrates at 0 as α → 0,
we can check that
(cid:32)

(cid:33)

(cid:34)

(cid:35)

lim
α→0

(cid:101)E

log

eβ(k)·(αbT +(cid:102)WαT + (cid:101)NαT )

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

1
K
(cid:34)

K
(cid:88)

k=1
(cid:32)

(cid:33)

(cid:35)

eβ(k)· (cid:101)NαT

(cid:12)
(cid:12) (cid:101)NαT (cid:54)= 0

1
K

K
(cid:88)

k=1

(cid:33)

= lim
α→0

(cid:101)E

log

(cid:32)

(cid:90)

=

log

1
K

K
(cid:88)

k=1

eβ(k)·z

dνT (z; X)

where νT (·; X) (1.23) is the ﬁrst jump measure conditional on X.

Meanwhile, in order to control the remaining term, we note that we can write

(cid:102)WαT = α(cid:102)WT + (cid:101)BαT ,

where (cid:101)Bt is a Brownian bridge from 0 to T that is independent from (cid:102)WT . Thus,
noting that limα→0 P
= 1, we ﬁnd that

(cid:105)
(cid:101)NαT = 0

(cid:104)

eβ(k)·(αbT +(cid:102)WαT )

(cid:33)

(cid:35)

(cid:12)
(cid:12) (cid:101)NαT = 0

P

(cid:104)

(cid:105)
(cid:101)NαT = 0

lim
α→0

1
α

(cid:101)E

(cid:34)

(cid:32)

log

1
K

K
(cid:88)

k=1
(cid:34)

= lim
α→0

1
α

(cid:101)E

log

eβ(k)·(α(bT +(cid:102)WT ))+ (cid:101)BαT

(cid:33)(cid:35)

(cid:33)

(cid:32)

1
K
(cid:32)

K
(cid:88)

k=1
K
(cid:88)

k=1

1
K

T
2

= ¯β · µT (X) +

β(k)(cid:62)Σ β(k) − ¯β(cid:62)Σ ¯β

,

where µT (X) is as deﬁned in (1.21) and ¯β = K −1 (cid:80)K
k=1 β(k). The last equality
follows from Taylor expanding the log((cid:80) exp) term and noting that 3rd- and higher-

26

Data Augmentation via L´evy Processes

1.7 References

order terms vanish in the limit.

that ¯β = 0, we ﬁnally conclude that

Bringing back the linear term form (1.47), and assuming without loss of generality

ρ (β; X, Y ) = −β(Y ) · X +

β(k)(cid:62)Σ β(k)

T
2

1
K

K
(cid:88)

k=1

+ λT (X)

log

(cid:32)

1
K

K
(cid:88)

k=1

(cid:33)

eβ(k)·z

dνT (z; X)

= −β(Y ) · µT (X) +

β(k)(cid:62)Σ β(k)

(cid:90)

(cid:90)

T
2

K
(cid:88)

1
K
(cid:32) K
(cid:88)

k=1

k=1

(cid:33)

eβ(k)·z

+ λT (X)

−β(Y ) · z + log

− log(K) dνT (z; X),

where for the second equality we used the fact that X = µT (X)+λT (X) (cid:82) z dνT (z; X).
Finally, this expression only diﬀers from (1.24) by terms that do not include β; thus,
they yield the same minimizer.

Y. S. Abu-Mostafa. Learning from hints in neural networks. Journal of Complexity,

6(2):192–198, 1990.

J. Ba and B. Frey. Adaptive dropout for training deep neural networks. In Advances

in Neural Information Processing Systems (NIPS), pages 3084–3092, 2013.

P. Baldi and P. Sadowski. The dropout learning algorithm. Artiﬁcial intelligence,

210:78–122, 2014.

C. M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural

computation, 7(1):108–116, 1995.

G. Bouchard and B. Triggs. The trade-oﬀ between generative and discriminative
classiﬁers. In International Conference on Computational Statistics, pages 721–
728, 2004.

B. Efron. The eﬃciency of logistic regression compared to normal discriminant
analysis. Journal of the American Statistical Association (JASA), 70(352):892–
898, 1975.

J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized
linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22,
2010.

A. Globerson and S. Roweis. Nightmare at test time: robust learning by feature
deletion. In International Conference on Machine Learning (ICML), pages 353–
360, 2006.

I. Goodfellow, D. Warde-farley, M. Mirza, A. Courville, and Y. Bengio. Maxout
In International Conference on Machine Learning (ICML), pages

networks.
1319–1327, 2013.

D. P. Helmbold and P. M. Long. On the inductive bias of dropout. Journal of

1.7 References

27

Machine Learning Research (JMLR), 16:3403–3454, 2015.

J. Josse and S. Wager. Stable autoencoding: A ﬂexible framework for regularized

low-rank matrix estimation. arXiv preprint arXiv:1410.8275, 2014.

A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep
In Advances in Neural Information Processing

convolutional neural networks.
Systems (NIPS), pages 1097–1105, 2012.

J. A. Lasserre, C. M. Bishop, and T. P. Minka. Principled hybrids of generative and
discriminative models. In Computer Vision and Pattern Recognition (CVPR),
pages 87–94, 2006.

P. Liang and M. I. Jordan. An asymptotic analysis of generative, discriminative, and
pseudolikelihood estimators. In International Conference on Machine Learning
(ICML), pages 584–591, 2008.

D. McAllester. A PAC-Bayesian tutorial with a dropout bound. arXiv preprint

arXiv:1307.2118, 2013.

A. McCallum, C. Pal, G. Druck, and X. Wang. Multi-conditional learning: Gener-
ative/discriminative training for clustering and classiﬁcation. In Association for
the Advancement of Artiﬁcial Intelligence (AAAI), 2006.

A. Y. Ng and M. I. Jordan. On discriminative vs. generative classiﬁers: A compar-
ison of logistic regression and naive Bayes. In Advances in Neural Information
Processing Systems (NIPS), 2002.

R. Raina, Y. Shen, A. Ng, and A. McCallum. Classiﬁcation with hybrid gen-
In Advances in Neural Information Processing

erative/discriminative models.
Systems (NIPS), 2004.

Y. D. Rubinstein and T. Hastie. Discriminative vs informative learning.

In
International Conference on Knowledge Discovery and Data Mining (KDD),
volume 5, pages 49–53, 1997.

S. P. Sch¨olkopf, P. Simard, V. Vapnik, and A. Smola. Improving the accuracy and
speed of support vector machines. In Advances in Neural Information Processing
Systems (NIPS), pages 375–381, 1997.

P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. Transformation
Invariance in Pattern Recognition—Tangent Distance and Tangent Propagation.
Neural networks: Tricks of the trade Springer, 1998.

N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
Dropout: A simple way to prevent neural networks from overﬁtting. Journal
of Machine Learning Research (JMLR), 15(1):1929–1958, 2014.

L. van der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger. Learning with
marginalized corrupted features. In International Conference on Machine Learn-
ing (ICML), pages 410–418, 2013.

S. Wager, S. I. Wang, and P. Liang. Dropout training as adaptive regularization.

In Advances in Neural Information Processing Systems (NIPS), 2013.

S. Wager, W. Fithian, S. I. Wang, and P. Liang. Altitude training: Strong bounds
for single-layer dropout. In Advances in Neural Information Processing Systems
(NIPS), 2014.

L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Regularization of neural
networks using dropconnect. In International Conference on Machine Learning
(ICML), pages 1058–1066, 2013.

S. I. Wang and C. Manning. Fast dropout training. In International Conference

28

Data Augmentation via L´evy Processes

on Machine Learning (ICML), pages 118–126, 2013.

S. I. Wang, M. Wang, S. Wager, P. Liang, and C. Manning. Feature noising
for log-linear structured prediction. In Empirical Methods in Natural Language
Processing (EMNLP), 2013.

