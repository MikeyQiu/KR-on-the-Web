Neural Models for Documents with Metadata

Dallas Card1 Chenhao Tan2 Noah A. Smith3
1Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, 15213, USA
2Department of Computer Science, University of Colorado, Boulder, CO, 80309, USA
3Paul G. Allen School of CSE, University of Washington, Seattle, WA, 98195, USA

dcard@cmu.edu

chenhao.tan@colorado.edu

nasmith@cs.washington.edu

Abstract

Most real-world document collections in-
volve various types of metadata, such as
author, source, and date, and yet the most
commonly-used approaches to modeling
text corpora ignore this information. While
specialized models have been developed
for particular applications, few are widely
used in practice, as customization typically
requires derivation of a custom inference
algorithm. In this paper, we build on recent
advances in variational inference methods
and propose a general neural framework,
based on topic models, to enable ﬂexible in-
corporation of metadata and allow for rapid
exploration of alternative models. Our ap-
proach achieves strong performance, with
a manageable tradeoff between perplex-
ity, coherence, and sparsity. Finally, we
demonstrate the potential of our framework
through an exploration of a corpus of arti-
cles about US immigration.

1

Introduction

Topic models comprise a family of methods for
uncovering latent structure in text corpora, and are
widely used tools in the digital humanities, political
science, and other related ﬁelds (Boyd-Graber et al.,
2017). Latent Dirichlet allocation (LDA; Blei et al.,
2003) is often used when there is no prior knowl-
edge about a corpus. In the real world, however,
most documents have non-textual attributes such
as author (Rosen-Zvi et al., 2004), timestamp (Blei
and Lafferty, 2006), rating (McAuliffe and Blei,
2008), or ideology (Eisenstein et al., 2011; Nguyen
et al., 2015b), which we refer to as metadata.

Many customizations of LDA have been devel-
oped to incorporate document metadata. Two mod-
els of note are supervised LDA (SLDA; McAuliffe

and Blei, 2008), which jointly models words and
labels (e.g., ratings) as being generated from a la-
tent representation, and sparse additive genera-
tive models (SAGE; Eisenstein et al., 2011), which
assumes that observed covariates (e.g., author ide-
ology) have a sparse effect on the relative proba-
bilities of words given topics. The structural topic
model (STM; Roberts et al., 2014), which adds cor-
relations between topics to SAGE, is also widely
used, but like SAGE it is limited in the types of
metadata it can efﬁciently make use of, and how
that metadata is used. Note that in this work we
will distinguish labels (metadata that are gener-
ated jointly with words from latent topic represen-
tations) from covariates (observed metadata that
inﬂuence the distribution of labels and words).

The ability to create variations of LDA such as
those listed above has been limited by the expertise
needed to develop custom inference algorithms for
each model. As a result, it is rare to see such varia-
tions being widely used in practice. In this work,
we take advantage of recent advances in variational
methods (Kingma and Welling, 2014; Rezende
et al., 2014; Miao et al., 2016; Srivastava and Sut-
ton, 2017) to facilitate approximate Bayesian infer-
ence without requiring model-speciﬁc derivations,
and propose a general neural framework for topic
models with metadata, SCHOLAR.1

SCHOLAR combines the abilities of SAGE and
SLDA, and allows for easy exploration of the fol-
lowing options for customization:

1. Covariates: as in SAGE and STM, we incorpo-
rate explicit deviations for observed covariates,
as well as effects for interactions with topics.

2. Supervision: as in SLDA, we can use metadata
as labels to help infer topics that are relevant in
predicting those labels.

1Sparse Contextual Hidden and Observed Language

AutoencodeR.

8
1
0
2
 
t
c
O
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
9
2
9
0
.
5
0
7
1
:
v
i
X
r
a

3. Rich encoder network: we use the encoding
network of a variational autoencoder (VAE) to
incorporate additional prior knowledge in the
form of word embeddings, and/or to provide
interpretable embeddings of covariates.

4. Sparsity: as in SAGE, a sparsity-inducing prior
can be used to encourage more interpretable
topics, represented as sparse deviations from a
background log-frequency.

We begin with the necessary background and
motivation (§2), and then describe our basic frame-
work and its extensions (§3), followed by a series
of experiments (§4). In an unsupervised setting,
we can customize the model to trade off between
perplexity, coherence, and sparsity, with improved
coherence through the introduction of word vec-
tors. Alternatively, by incorporating metadata we
can either learn topics that are more predictive of
labels than SLDA, or learn explicit deviations for
particular parts of the metadata. Finally, by com-
bining all parts of our model we can meaningfully
incorporate metadata in multiple ways, which we
demonstrate through an exploration of a corpus of
news articles about US immigration.

In presenting this particular model, we empha-
size not only its ability to adapt to the characteris-
tics of the data, but the extent to which the VAE
approach to inference provides a powerful frame-
work for latent variable modeling that suggests the
possibility of many further extensions. Our im-
plementation is available at https://github.
com/dallascard/scholar.

2 Background and Motivation

LDA can be understood as a non-negative
Bayesian matrix factorization model: the observed
document-word frequency matrix, X ∈ ZD×V
(D is the number of documents, V is the vocab-
ulary size) is factored into two low-rank matri-
ces, ΘD×K and BK×V , where each row of Θ,
θi ∈ ∆K is a latent variable representing a distri-
bution over topics in document i, and each row of
B, βk ∈ ∆V , represents a single topic, i.e., a dis-
tribution over words in the vocabulary.2 While it is
possible to factor the count data into unconstrained

2Z denotes nonnegative integers, and ∆K denotes the set
of K-length nonnegative vectors that sum to one. For a proper
probabilistic interpretation, the matrix to be factored is actually
the matrix of latent mean parameters of the assumed data
generating process, Xij ∼ Poisson(Λij). See Cemgil (2009)
or Paisley et al. (2014) for details.

matrices, the particular priors assumed by LDA
are important for interpretability (Wallach et al.,
2009). For example, the neural variational docu-
ment model (NVDM; Miao et al., 2016) allows
θi ∈ RK and achieves normalization by taking
the softmax of θ(cid:62)
i B. However, the experiments
in Srivastava and Sutton (2017) found the perfor-
mance of the NVDM to be slightly worse than LDA
in terms of perplexity, and dramatically worse in
terms of topic coherence.

The topics discovered by LDA tend to be parsi-
monious and coherent groupings of words which
are readily identiﬁable to humans as being related
to each other (Chang et al., 2009), and the resulting
mode of the matrix Θ provides a representation of
each document which can be treated as a measure-
ment for downstream tasks, such as classiﬁcation
or answering social scientiﬁc questions (Wallach,
2016). LDA does not require — and cannot make
use of — additional prior knowledge. As such, the
topics that are discovered may bear little connec-
tion to metadata of a corpus that is of interest to a
researcher, such as sentiment, ideology, or time.

In this paper, we take inspiration from two mod-
els which have sought to alleviate this problem.
The ﬁrst, supervised LDA (SLDA; McAuliffe and
Blei, 2008), assumes that documents have labels y
which are generated conditional on the correspond-
ing latent representation, i.e., yi ∼ p(y | θi).3 By
incorporating labels into the model, it is forced to
learn topics which allow documents to be repre-
sented in a way that is useful for the classiﬁcation
task. Such models can be used inductively as text
classiﬁers (Balasubramanyan et al., 2012).

SAGE (Eisenstein et al., 2011), by contrast, is
an exponential-family model, where the key inno-
vation was to replace topics with sparse deviations
from the background log-frequency of words (d),
i.e., p(word | softmax(d + θ(cid:62)
i B)). SAGE can also
incorporate deviations for observed covariates, as
well as interactions between topics and covariates,
by including additional terms inside the softmax.
In principle, this allows for inferring, for example,
the effect on an author’s ideology on their choice
of words, as well as ideological variations on each
underlying topic. Unlike the NVDM, SAGE still
constrains θi to lie on the simplex, as in LDA.

SLDA and SAGE provide two different ways
that users might wish to incorporate prior knowl-

3Technically, the model conditions on the mean of the per-
word latent variables, but we elide this detail in the interest of
concision.

edge as a way of guiding the discovery of topics
in a corpus: SLDA incorporates labels through a
distribution conditional on topics; SAGE includes
explicit sparse deviations for each unique value of
a covariate, in addition to topics.4

Because of the Dirichlet-multinomial conjugacy
in the original model, efﬁcient inference algorithms
exist for LDA. Each variation of LDA, however,
has required the derivation of a custom inference
algorithm, which is a time-consuming and error-
prone process. In SLDA, for example, each type of
distribution we might assume for p(y | θ) would
require a modiﬁcation of the inference algorithm.
SAGE breaks conjugacy, and as such, the authors
adopted L-BFGS for optimizing the variational
bound. Moreover, in order to maintain compu-
tational efﬁciency, it assumed that covariates were
limited to a single categorical label.

More recently,

the variational autoencoder
(VAE) was introduced as a way to perform approxi-
mate posterior inference on models with otherwise
intractable posteriors (Kingma and Welling, 2014;
Rezende et al., 2014). This approach has previously
been applied to models of text by Miao et al. (2016)
and Srivastava and Sutton (2017). We build on their
work and show how this framework can be adapted
to seamlessly incorporate the ideas of both SAGE
and SLDA, while allowing for greater ﬂexibility in
the use of metadata. Moreover, by exploiting au-
tomatic differentiation, we allow for modiﬁcation
of the model without requiring any change to the
inference procedure. The result is not only a highly
adaptable family of models with scalable inference
and efﬁcient prediction; it also points the way to
incorporation of many ideas found in the literature,
such as a gradual evolution of topics (Blei and Laf-
ferty, 2006), and hierarchical models (Blei et al.,
2010; Nguyen et al., 2013, 2015b).

3

SCHOLAR: A Neural Topic Model with
Covariates, Supervision, and Sparsity

We begin by presenting the generative story for our
model, and explain how it generalizes both SLDA
and SAGE (§3.1). We then provide a general expla-
nation of inference using VAEs and how it applies
to our model (§3.2), as well as how to infer docu-

4A third way of incorporating metadata is the approach
used by various “upstream” models, such as Dirichlet-
multinomial regression (Mimno and McCallum, 2008), which
uses observed metadata to inform the document prior. We hy-
pothesize that this approach could be productively combined
with our framework, but we leave this as future work.

ment representations and predict labels at test time
(§3.3). Finally, we discuss how we can incorporate
additional prior knowledge (§3.4).

3.1 Generative Story

Consider a corpus of D documents, where docu-
ment i is a list of Ni words, wi, with V words in
the vocabulary. For each document, we may have
observed covariates ci (e.g., year of publication),
and/or one or more labels, yi (e.g., sentiment).

Our model builds on the generative story of LDA,
but optionally incorporates labels and covariates,
and replaces the matrix product of Θ and B with a
more ﬂexible generative network, fg, followed by
a softmax transform. Instead of using a Dirichlet
prior as in LDA, we employ a logistic normal prior
on θ as in Srivastava and Sutton (2017) to facilitate
inference (§3.2): we draw a latent variable, r,5
from a multivariate normal, and transform it to lie
on the simplex using a softmax transform.6

The generative story is shown in Figure 1a and

described in equations below:

For each document i of length Ni:

# Draw a latent representation on the sim-
plex from a logistic normal prior:
ri ∼ N (r | µ0(α), diag(σ2
0(α)))
θi = softmax(ri)
# Generate words, incorporating covariates:
ηi = fg(θi, ci)
For each word j in document i:
wij ∼ p(w | softmax(ηi))

# Similarly generate labels:
yi ∼ p(y | fy(θi, ci)),

where p(w | softmax(ηi)) is a multinomial distri-
bution and p(y | fy(θi, ci)) is a distribution appro-
priate to the data (e.g., multinomial for categorical
labels). fg is a model-speciﬁc combination of latent
variables and covariates, fy is a multi-layer neural
network, and µ0(α) and σ2
0(α) are the mean and
diagonal covariance terms of a multivariate nor-
mal prior. To approximate a symmetric Dirichlet

5r is equivalent to z in the original VAE. To avoid con-
fusion with topic assignment of words in the topic modeling
literature, we use r instead of z.

6Unlike the correlated topic model (CTM; Lafferty and
Blei, 2006), which also uses a logistic-normal prior, we ﬁx the
parameters of the prior and use a diagonal covariance matrix,
rather than trying to infer correlations among topics. However,
it would be a straightforward extension of our framework to
place a richer prior on the latent document representations,
and learn correlations by updating the parameters of this prior
after each epoch, analogously to the variational EM approach
used for the CTM.

prior with hyperparameter α, these are given by the
Laplace approximation (Hennig et al., 2012) to be
µ0,k(α) = 0 and σ2

0,k = (K − 1)/(αK).

If we were to ignore covariates, place a Dirichlet
prior on B, and let η = θ(cid:62)
i B, this model is equiv-
alent to SLDA with a logistic normal prior. Sim-
ilarly, we can recover a model that is like SAGE,
but lacks sparsity, if we ignore labels, and let

ηi = d + θ(cid:62)

i B + c(cid:62)

i Bcov + (θi ⊗ ci)(cid:62)Bint, (1)

where d is the V -dimensional background term
(representing the log of the overall word frequency),
θi ⊗ ci is a vector of interactions between topics
and covariates, and Bcov and Bint are additional
weight (deviation) matrices. The background is
included to account for common words with ap-
proximately the same frequency across documents,
meaning that the B∗ weights now represent both
positive and negative deviations from this back-
ground. This is the form of fg which we will use
in our experiments.

To recover the full SAGE model, we can place
a sparsity-inducing prior on each B∗. As in Eisen-
stein et al. (2011), we make use of the compound
normal-exponential prior for each element of the
weight matrices, B∗

m,n, with hyperparameter γ,7

τm,n ∼ Exponential(γ),
B∗
m,n ∼ N (0, τm,n).

(2)

(3)

We can choose to ignore various parts of this
model, if, for example, we don’t have any labels
or observed covariates, or we don’t wish to use
interactions or sparsity.8 Other generator networks
could also be considered, with additional layers to
represent more complex interactions, although this
might involve some loss of interpretability.

In the absence of metadata, and without sparsity,
our model is equivalent to the ProdLDA model
of Srivastava and Sutton (2017) with an explicit
background term, and ProdLDA is, in turn, a

7To avoid having to tune γ, we employ an improper Jef-
fery’s prior, p(τm,n) ∝ 1/τm,n, as in SAGE. Although this
causes difﬁculties in posterior inference for the variance terms,
τ , in practice, we resort to a variational EM approach, with
MAP-estimation for the weights, B, and thus alternate be-
tween computing expectations of the τ parameters, and up-
dating all other parameters using some variant of stochastic
gradient descent. For this, we only require the expectation of
each τmn for each E-step, which is given by 1/B2
m,n. We re-
fer the reader to Eisenstein et al. (2011) for additional details.
8We could also ignore latent topics, in which case we
would get a naïve Bayes-like model of text with deviations for
each covariate p(wij | ci) ∝ exp(d + c(cid:62)

i Bcov).

α

r

θ

η

w

Ni

D

B

c

y

y

w

c

i

n

π
ar l
e
lin
µ

σ

e

a

r

r

(cid:15)

D

(a) Generative model

(b) Inference model

Figure 1: Figure 1a presents the generative story of
our model. Figure 1b illustrates the inference net-
work using the reparametrization trick to perform
variational inference on our model. Shaded nodes
are observed; double circles indicate deterministic
transformations of parent nodes.

special case of SAGE, without background log-
frequencies, sparsity, covariates, or labels. In the
next section we generalize the inference method
used for ProdLDA; in our experiments we validate
its performance and explore the effects of regular-
ization and word-vector initialization (§3.4). The
NVDM (Miao et al., 2016) uses the same approach
to inference, but does not not restrict document
representations to the simplex.

3.2 Learning and Inference

As in past work, each document i is assumed to
have a latent representation ri, which can be in-
terpreted as its relative membership in each topic
(after exponentiating and normalizing). In order
to infer an approximate posterior distribution over
ri, we adopt the sampling-based VAE framework
developed in previous work (Kingma and Welling,
2014; Rezende et al., 2014).

As in conventional variational inference, we
assume a variational approximation to the poste-
rior, qΦ(ri | wi, ci, yi), and seek to minimize the
KL divergence between it and the true posterior,
p(ri | wi, ci, yi), where Φ is the set of variational
parameters to be deﬁned below. After some ma-
nipulations (given in supplementary materials), we
obtain the evidence lower bound (ELBO) for a sin-

gle document,

ple10 of (cid:15) (and thereby of r):

L(wi) = EqΦ(ri|wi,ci,yi)

log p(wij | ri, ci)


Ni(cid:88)






L(wi)
Ni(cid:88)

≈

j=1
+ EqΦ(ri|wi,ci,yi) [log p(yi | ri, ci)]
− DKL [qΦ(ri | wi, ci, yi) || p(ri | α)] .
(4)

log p(wij | r(s)

, ci) + log p(yi | r(s)

, ci)

i

i

j=1
− DKL [qΦ(ri | wi, ci, yi) || p(ri | α)] .

(8)

We can now optimize this sampling-based approxi-
mation of the variational bound with respect to Φ,
B∗, and all parameters of fg and fy using stochas-
tic gradient descent. Moreover, because of this
stochastic approach to inference, we are not re-
stricted to covariates with a small number of unique
values, which was a limitation of SAGE. Finally,
the KL divergence term in Equation 8 can be com-
puted in closed form (see supplementary materials).

3.3 Prediction on Held-out Data

In addition to inferring latent topics, our model
can both infer latent representations for new docu-
ments and predict their labels, the latter of which
was the motivation for SLDA. In traditional vari-
ational inference, inference at test time requires
ﬁxing global parameters (topics), and optimizing
the per-document variational parameters for the
test set. With the VAE framework, by contrast,
the encoder network (Equations 5–7) can be used
to directly estimate the posterior distribution for
each test document, using only a forward pass (no
iterative optimization or sampling).

If not using labels, we can use this approach di-
rectly, passing the word counts of new documents
through the encoder to get a posterior qΦ(ri
|
wi, ci). When we also include labels to be pre-
dicted, we can ﬁrst train a fully-observed model, as
above, then ﬁx the decoder, and retrain the encoder
without labels. In practice, however, if we train
the encoder network using one-hot encodings of
document labels, it is sufﬁcient to provide a vector
of all zeros for the labels of test documents; this is
what we adopt for our experiments (§4.2), and we
still obtain good predictive performance.

The label network, fy, is a ﬂexible component
which can be used to predict a wide range of out-
comes, from categorical labels (such as star ratings;
McAuliffe and Blei, 2008) to real-valued outputs
(such as number of citations or box-ofﬁce returns;

10Alternatively, one can average over multiple samples.

As in the original VAE, we will encode the pa-
rameters of our variational distributions using a
shared multi-layer neural network. Because we
have assumed a diagonal normal prior on r, this
will take the form of a network which outputs a
mean vector, µi = fµ(wi, ci, yi) and diagonal of a
covariance matrix, σ2
i = fσ(wi, ci, yi), such that
qΦ(ri | wi, ci, yi) = N (µi, σ2
i ). Incorporating
labels and covariates to the inference network used
by Miao et al. (2016) and Srivastava and Sutton
(2017), we use:

πi = fe([Wxxi; Wcci; Wyyi]),
µi = Wµπi + bµ,
i = Wσπi + bσ,

log σ2

(5)

(6)

(7)

where xi is a V -dimensional vector representing
the counts of words in wi, and fe is a multilayer
perceptron. The full set of encoder parameters, Φ,
thus includes the parameters of fe and all weight
matrices and bias vectors in Equations 5–7 (see
Figure 1b).

This approach means that the expectations in
Equation 4 are intractable, but we can approximate
them using sampling. In order to maintain differen-
tiability with respect to Φ, even after sampling, we
make use of the reparameterization trick (Kingma
and Welling, 2014),9 which allows us to reparame-
terize samples from qΦ(r | wi, ci, yi) in terms of
samples from an independent source of noise, i.e.,

(cid:15)(s) ∼ N (0, I),

r(s)
i = gΦ(wi, ci, yi, (cid:15)(s)) = µi + σi · (cid:15)(s).

We thus replace the bound in Equation 4 with
a Monte Carlo approximation using a single sam-

9 The Dirichlet distribution cannot be directly reparame-
terized in this way, which is why we use the logistic normal
prior on θ to approximate the Dirichlet prior used in LDA.

Yogatama et al., 2011). For categorical labels, pre-
dictions are given by

ˆyi = argmax

p(y | ri, ci).

(9)

y ∈ Y

Alternatively, when dealing with a small set of
categorical labels, it is also possible to treat them as
observed categorical covariates during training. At
test time, we can then consider all possible one-hot
vectors, e, in place of ci, and predict the label that
maximizes the probability of the words, i.e.,

ˆyi = argmax

log p(wij | ri, ey).

(10)

Ni(cid:88)

j=1

y ∈ Y

This approach works well in practice (as we show
in §4.2), but does not scale to large numbers of
labels, or other types of prediction problems, such
as multi-class classiﬁcation or regression.

The choice to include metadata as covariates, la-
bels, or both, depends on the data. The key point
is that we can incorporate metadata in two very
different ways, depending on what we want from
the model. Labels guide the model to infer topics
that are relevant to those labels, whereas covari-
ates induce explicit deviations, leaving the latent
variables to account for the rest of the content.

3.4 Additional Prior Information

A ﬁnal advantage of the VAE framework is that
the encoder network provides a way to incorporate
additional prior information in the form of word
vectors. Although we can learn all parameters start-
ing from a random initialization, it is also possible
to initialize and ﬁx the initial embeddings of words
in the model, Wx, in Equation 5. This leverages
word similarities derived from large amounts of un-
labeled data, and may promote greater coherence
in inferred topics. The same could also be done
for some covariates; for example, we could embed
the source of a news article based on its place on
the ideological spectrum. Conversely, if we choose
to learn these parameters, the learned values (Wy
and Wc) may provide meaningful embeddings of
these metadata (see section §4.3).

Other variants on topic models have also pro-
posed incorporating word vectors, both as a par-
allel part of the generative process (Nguyen et al.,
2015a), and as an alternative parameterization of
topic distributions (Das et al., 2015), but inference
is not scalable in either of these models. Because
of the generality of the VAE framework, we could

also modify the generative story so that word em-
beddings are emitted (rather than tokens); we leave
this for future work.

4 Experiments and Results

To evaluate and demonstrate the potential of this
model, we present a series of experiments below.
We ﬁrst test SCHOLAR without observed meta-
data, and explore the effects of using regulariza-
tion and/or word vector initialization, compared to
LDA, SAGE, and NVDM (§4.1). We then evaluate
our model in terms of predictive performance, in
comparison to SLDA and an l2-regularized logistic
regression baseline (§4.2). Finally, we demonstrate
the ability to incorporate covariates and/or labels
in an exploratory data analysis (§4.3).

The scores we report are generalization to held-
out data, measured in terms of perplexity; coher-
ence, measured in terms of non-negative point-wise
mutual information (NPMI; Chang et al., 2009;
Newman et al., 2010), and classiﬁcation accuracy
on test data. For coherence we evaluate NPMI us-
ing the top 10 words of each topic, both internally
(using test data), and externally, using a decade of
articles from the English Gigaword dataset (Graff
and Cieri, 2003). Since our model employs varia-
tional methods, the reported perplexity is an upper
bound based on the ELBO.

As datasets we use the familiar 20 newsgroups,
the IMDB corpus of 50,000 movie reviews (Maas
et al., 2011), and the UIUC Yahoo answers dataset
with 150,000 documents in 15 categories (Chang
et al., 2008). For further exploration, we also
make use of a corpus of approximately 4,000 time-
stamped news articles about US immigration, each
annotated with pro- or anti-immigration tone (Card
et al., 2015). We use the original author-provided
implementations of SAGE11 and SLDA,12 while
for LDA we use Mallet.13. Our implementation
of SCHOLAR is in TensorFlow, but we have also
provided a preliminary PyTorch implementation
of the core of our model.14 For additional details
about datasets and implementation, please refer to
the supplementary material.

It is challenging to fairly evaluate the relative
computational efﬁciency of our approach compared
to past work (due to the stochastic nature of our ap-

11github.com/jacobeisenstein/SAGE
12github.com/blei-lab/class-slda
13mallet.cs.umass.edu
14github.com/dallascard/scholar

proach to inference, choices about hyperparameters
such as tolerance, and because of differences in im-
plementation). Nevertheless, in practice, the perfor-
mance of our approach is highly appealing. For all
experiments in this paper, our implementation was
much faster than SLDA or SAGE (implemented in
C and Matlab, respectively), and competitive with
Mallet.

4.1 Unsupervised Evaluation

Although the emphasis of this work is on incorpo-
rating observed labels and/or covariates, we brieﬂy
report on experiments in the unsupervised setting.
Recall that, without metadata, SCHOLAR equates to
ProdLDA, but with an explicit background term.15
We therefore use the same experimental setup as
Srivastava and Sutton (2017) (learning rate, mo-
mentum, batch size, and number of epochs) and
ﬁnd the same general patterns as they reported (see
Table 1 and supplementary material): our model
returns more coherent topics than LDA, but at the
cost of worse perplexity. SAGE, by contrast, attains
very high levels of sparsity, but at the cost of worse
perplexity and coherence than LDA. As expected,
the NVDM produces relatively low perplexity, but
very poor coherence, due to its lack of constraints
on θ.

Further experimentation revealed that the VAE
framework involves a tradeoff among the scores;
running for more epochs tends to result in bet-
ter perplexity on held-out data, but at the cost of
worse coherence. Adding regularization to encour-
age sparse topics has a similar effect as in SAGE,
leading to worse perplexity and coherence, but it
does create sparse topics. Interestingly, initializing
the encoder with pretrained word2vec embeddings,
and not updating them returned a model with the
best internal coherence of any model we considered
for IMDB and Yahoo answers, and the second-best
for 20 newsgroups.

The background term in our model does not have
much effect on perplexity, but plays an important
role in producing coherent topics; as in SAGE, the
background can account for common words, so
they are mostly absent among the most heavily
weighted words in the topics. For instance, words
like ﬁlm and movie in the IMDB corpus are rel-
atively unimportant in the topics learned by our

15Note, however, that a batchnorm layer in ProdLDA may
play a similar role to a background term, and there are small
differences in implementation; please see supplementary ma-
terial for more discussion of this.

Model

LDA
SAGE
NVDM
SCHOLAR − B.G.
SCHOLAR
SCHOLAR + W.V.
SCHOLAR + REG.

Ppl.
↓

1508
1767
1748
1889
1905
1991
2185

NPMI
(int.) ↑

NPMI
(ext.) ↑

Sparsity
↑

0.13
0.12
0.06
0.09
0.14
0.18
0.10

0.14
0.12
0.04
0.13
0.13
0.17
0.12

0
0.79
0
0
0
0
0.58

Table 1: Performance of our various models in
an unsupervised setting (i.e., without labels or co-
variates) on the IMDB dataset using a 5,000-word
vocabulary and 50 topics. The supplementary ma-
terials contain additional results for 20 newsgroups
and Yahoo answers.

model, but would be much more heavily weighted
without the background term, as they are in topics
learned by LDA.

4.2 Text Classiﬁcation

We next consider the utility of our model in the
context of categorical labels, and consider them
alternately as observed covariates and as labels
generated conditional on the latent representation.
We use the same setup as above, but tune number of
training epochs for our model using a random 20%
of training data as a development set, and similarly
tune regularization for logistic regression.

Table 2 summarizes the accuracy of various mod-
els on three datasets, revealing that our model offers
competitive performance, both as a joint model of
words and labels (Eq. 9), and a model which condi-
tions on covariates (Eq. 10). Although SCHOLAR
is comparable to the logistic regression baseline,
our purpose here is not to attain state-of-the-art per-
formance on text classiﬁcation. Rather, the high
accuracies we obtain demonstrate that we are learn-
ing low-dimensional representations of documents
that are relevant to the label of interest, outperform-
ing SLDA, and have the same attractive properties
as topic models. Further, any neural network that
is successful for text classiﬁcation could be incor-
porated into fy and trained end-to-end along with
topic discovery.

4.3 Exploratory Study

We demonstrate how our model might be used to
explore an annotated corpus of articles about immi-
gration, and adapt to different assumptions about
the data. We only use a small number of topics in
this part (K = 8) for compact presentation.

Vocabulary size
Number of topics

20news
2000
50

IMDB Yahoo
5000
5000
250
50

SLDA
SCHOLAR (labels)
SCHOLAR (covariates)
Logistic regression

0.60
0.67
0.71
0.70

0.64
0.86
0.87
0.87

0.65
0.73
0.72
0.76

Table 2: Accuracy of various models on three
datasets with categorical labels.

Tone as a label. We ﬁrst consider using the an-
notations as a label, and train a joint model to infer
topics relevant to the tone of the article (pro- or
anti-immigration). Figure 2 shows a set of top-
ics learned in this way, along with the predicted
probability of an article being pro-immigration con-
ditioned on the given topic. All topics are coherent,
and the predicted probabilities have strong face
validity, e.g., “arrested charged charges agents op-
eration” is least associated with pro-immigration.

Tone as a covariate. Next we consider using
tone as a covariate, and build a model using both
tone and tone-topic interactions. Table 3 shows
a set of topics learned from the immigration data,
along with the most highly-weighted words in the
corresponding tone-topic interaction terms. As can
be seen, these interaction terms tend to capture dif-
ferent frames (e.g., “criminal” vs. “detainees”, and
“illegals” vs. “newcomers”, etc).

Combined model with temporal metadata. Fi-
nally, we incorporate both the tone annotations and
the year of publication of each article, treating the
former as a label and the latter as a covariate. In
this model, we also include an embedding matrix,
Wc, to project the one-hot year vectors down to a
two-dimensional continuous space, with a learned
deviation for each dimension. We omit the topics
in the interest of space, but Figure 3 shows the
learned embedding for each year, along with the
top terms of the corresponding deviations. As can
be seen, the model learns that adjacent years tend
to produce similar deviations, even though we have
not explicitly encoded this information. The left-
right dimension roughly tracks a temporal trend
with positive deviations shifting from the years of
Clinton and INS on the left, to Obama and ICE on
the right.16 Meanwhile, the events of 9/11 dom-
inate the vertical direction, with the words sept,

16The Immigration and Naturalization Service (INS) was
transformed into Immigration and Customs Enforcement
(ICE) and other agencies in 2003.

Figure 2: Topics inferred by a joint model of words
and tone, and the corresponding probability of pro-
immigration tone for each topic. A topic is repre-
sented by the top words sorted by word probability
throughout the paper.

Figure 3:
Learned embeddings of year-of-
publication (treated as a covariate) from combined
model of news articles about immigration.

hijackers, and attacks increasing in probability as
we move up in the space. If we wanted to look at
each year individually, we could drop the embed-
ding of years, and learn a sparse set of topic-year
interactions, similar to tone in Table 3.

5 Additional Related Work

The literature on topic models is vast; in addition
to papers cited throughout, other efforts to incorpo-
rate metadata into topic models include Dirichlet-
multinomial regression (DMR; Mimno and McCal-
lum, 2008), Labeled LDA (Ramage et al., 2009),
and MedLDA (Zhu et al., 2009). A recent paper
also extended DMR by using deep neural networks
to embed metadata into a richer document prior
(Benton and Dredze, 2018).

A separate line of work has pursued parame-
terizing unsupervised models of documents us-
ing neural networks (Hinton and Salakhutdinov,

Base topics (each row is a topic)
ice customs agency enforcement homeland
population born percent americans english
judge case court guilty appeals attorney
patrol border miles coast desert boat guard
licenses drivers card visa cards applicants
island story chinese ellis international
guest worker workers bush labor bill
beneﬁts bill welfare republican state senate

Anti-immigration interactions
criminal customs arrested
jobs million illegals taxpayers
guilty charges man charged
patrol border agents boat
foreign sept visas system
smuggling federal charges
bill border house senate
republican california gov state

Pro-immigration interactions
detainees detention center agency
english newcomers hispanic city
asylum court judge case appeals
died authorities desert border bodies
green citizenship card citizen apply
island school ellis english story
workers tech skilled farm labor
law welfare students tuition

Table 3: Top words for topics (left) and the corresponding anti-immigration (middle) and pro-immigration
(right) variations when treating tone as a covariate, with interactions.

2009; Larochelle and Lauly, 2012), including non-
Bayesian approaches (Cao et al., 2015). More re-
cently, Lau et al. (2017) proposed a neural language
model that incorporated topics, and He et al. (2017)
developed a scalable alternative to the correlated
topic model by simultaneously learning topic em-
beddings.

Others have attempted to extend the reparameter-
ization trick to the Dirichlet and Gamma distribu-
tions, either through transformations (Kucukelbir
et al., 2016) or a generalization of reparameteriza-
tion (Ruiz et al., 2016). Black-box and VAE-style
inference have been implemented in at least two
general purpose tools designed to allow rapid explo-
ration and evaluation of models (Kucukelbir et al.,
2015; Tran et al., 2016).

6 Conclusion

We have presented a neural framework for general-
ized topic models to enable ﬂexible incorporation
of metadata with a variety of options. We take
advantage of stochastic variational inference to de-
velop a general algorithm for our framework such
that variations do not require any model-speciﬁc
algorithm derivations. Our model demonstrates
the tradeoff between perplexity, coherence, and
sparsity, and outperforms SLDA in predicting doc-
ument labels. Furthermore, the ﬂexibility of our
model enables intriguing exploration of a text cor-
pus on US immigration. We believe that our model
and code will facilitate rapid exploration of docu-
ment collections with metadata.

Acknowledgments

We would like to thank Charles Sutton, anonymous
reviewers, and all members of Noah’s ARK for
helpful discussions and feedback. This work was
made possible by a University of Washington Inno-
vation award and computing resources provided by
XSEDE.

References

Ramnath Balasubramanyan, William W. Cohen, Doug
Pierce, and David P. Redlawsk. 2012. Modeling po-
larizing topics: When do different political commu-
nities respond differently to the same news? In Pro-
ceedings of ICWSM.

Adrian Benton and Mark Dredze. 2018. Deep Dirichlet
multinomial regression. In Proceedings of NAACL.

David M. Blei, Thomas L. Grifﬁths, and Michael I. Jor-
dan. 2010. The nested Chinese restaurant process
and Bayesian nonparametric inference of topic hier-
archies. JACM, 57(2).

David M. Blei and John D. Lafferty. 2006. Dynamic

topic models. In Proceedings of ICML.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. JMLR, 3:993–
1022.

Jordan Boyd-Graber, Yuening Hu, and David Mimno.
2017. Applications of topic models. Foundations
and Trends in Information Retrieval, 11(2-3):143–
296.

Ziqiang Cao, Sujian Li, Yang Liu, Wenjie Li, and Heng
Ji. 2015. A novel neural topic model and its super-
vised extension. In Proceedings of AAAI.

Dallas Card, Amber E. Boydstun, Justin H. Gross,
Philip Resnik, and Noah A. Smith. 2015. The media
frames corpus: Annotations of frames across issues.
In Proceedings of ACL.

Ali Taylan Cemgil. 2009. Bayesian inference for
nonnegative matrix factorisation models. Compu-
tational Intelligence and Neuroscience, pages 4:1–
4:17.

Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L
Boyd-graber, and David M Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of NIPS.

Ming-Wei Chang, Lev-Arie Ratinov, Dan Roth, and
Vivek Srikumar. 2008. Importance of semantic rep-
resentation: Dataless classiﬁcation. In Proceedings
of AAAI.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian LDA for topic models with word embed-
dings. In Proceedings of ACL.

Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
In Pro-

Sparse additive generative models of text.
ceedings of ICML.

David Graff and C Cieri. 2003. English gigaword cor-

pus. Linguistic Data Consortium.

Junxian He, Zhiting Hu, Taylor Berg-Kirkpatrick, Ying
Huang, and Eric P. Xing. 2017. Efﬁcient correlated
topic modeling with topic embedding. In Proceed-
ings of KDD.

Philipp Hennig, David Stern, Ralf Herbrich, and Thore
Graepel. 2012. Kernel topic models. In Proceedings
of AISTATS.

Geoffrey E Hinton and Ruslan R Salakhutdinov. 2009.
Replicated softmax: An undirected topic model. In
Proceedings of NIPS.

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In Proceedings of ICLR.

Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman,
and David Blei. 2015. Automatic variational infer-
ence in stan. In Proceedings of NIPS.

Alp Kucukelbir, Dustin Tran, Rajesh Ranganath,
and David M. Blei. 2016.
inference.

Andrew Gelman,
Automatic differentiation variational
ArXiv:1603.00788.

John D. Lafferty and David M. Blei. 2006. Correlated

topic models. In Proceedings of NIPS.

Hugo Larochelle and Stanislas Lauly. 2012. A neural
autoregressive topic model. In Proceedings of NIPS.

Jey Han Lau, Timothy Baldwin, and Trevor Cohn.
2017. Topically driven neural language model. In
Proceedings of ACL.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of ACL.

Jon D. McAuliffe and David M. Blei. 2008. Supervised

topic models. In Proceedings of NIPS.

Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
and Kristina Miler. 2015b. Tea party in the house: A
hierarchical ideal point topic model and its applica-
tion to Republican legislators in the 112th congress.
In Proceedings of ACL.

Viet-An Nguyen, Jordan L. Boyd-Graber, and Philip
Resnik. 2013. Lexical and hierarchical topic regres-
sion. In Proceedings of NIPS.

John William Paisley, David M. Blei, and Michael I.
Jordan. 2014. Bayesian nonnegative matrix fac-
torization with stochastic variational inference.
In
Handbook of Mixed Membership Models and Their
Applications, pages 205–224.

Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A su-
pervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of EMNLP.

Danilo J. Rezende, Shakir Mohamed, and Daan Wier-
stra. 2014. Stochastic backpropagation and approx-
imate inference in deep generative models. In Pro-
ceedings of ICML.

Molly Roberts, Brandon Stewart, Dustin Tingley,
Christopher Lucas, Jetson Leder-Luis, Shana Gadar-
ian, Bethany Albertson, and David Rand. 2014.
Structural topic models for open ended survey re-
sponses. American Journal of Political Science,
58:1064–1082.

Michal Rosen-Zvi, Thomas Grifﬁths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model
for authors and documents. In Proceedings of UAI.

Francisco J R Ruiz, Michalis K Titsias, and David M
Blei. 2016. The generalized reparameterization gra-
dient. In Proceedings of NIPS.

Akash Srivastava and Charles Sutton. 2017. Autoen-
coding variational inference for topic models.
In
Proceedings of ICLR.

Dustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja
Rudolph, Dawen Liang, and David M. Blei. 2016.
Edward: A library for probabilistic modeling, infer-
ence, and criticism. ArXiv:1610.09787.

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neural
variational inference for text processing. In Proceed-
ings of ICML.

Hanna Wallach. 2016.

Interpretability and measure-
ment. EMNLP Workshop on Natural Language Pro-
cessing and Computational Social Science.

David Mimno and Andrew McCallum. 2008. Topic
models conditioned on arbitrary features with
Dirichlet-multinomial regression. In Proceedings of
UAI.

David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In Proceedings of ACL.

Hanna Wallach, David M. Mimno, and Andrew McCal-
lum. 2009. Rethinking LDA: Why priors matter. In
Proceedings of NIPS.

Dani Yogatama, Michael Heilman, Brendan O’Connor,
Chris Dyer, Bryan R Routledge, and Noah A Smith.
2011. Predicting a scientiﬁc community’s response
to an article. In Proceedings of EMNLP.

Dat Quoc Nguyen, Richard Billingsley, Lan Du, and
Mark Johnson. 2015a. Improving topic models with
latent feature word representations. In Proceedings
of ACL.

Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009.
MedLDA: Maximum margin supervised topic mod-
els for regression and classiﬁcation. In Proceedings
of ICML.

Supplementary Material

A Deriving the ELBO

The derivation of the ELBO for our model is given
below, dropping explicit reference to Φ and α for
simplicity. For document i,

log p(wi, yi | ci) = log

p(wi, yi, ri | ci)dri

(cid:90)

ri

(cid:90)

ri

(cid:18)

= log

p(wi, yi, ri |, ci)

q(ri | wi, ci, yi)
q(ri | wi, ci, yi)

= log

Eq(ri|wi,ci,yi)

(cid:20) p(wi, yi, ri | ci)
q(ri | wi, ci, yi)

(cid:21)(cid:19)

≥ Eq(ri|wi,ci,yi) [log p(wi, yi, ri | ci)]
−Eq(ri|wi,ci,yi) [log q(ri | wi, ci, yi)]
= Eq(ri|wi,ci,yi) [log p(wi, yi | ri, ci)]
+Eq(ri|wi,ci,yi) [log p(ri)]
−Eq(ri|wi,ci,yi) [log q(ri | wi, ci, yi)]

Ni(cid:88)


= Eq(ri|wi,ci,yi)

log p(wij | ri, ci)





j=1

+Eq(ri|wi,ci,yi) [log p(yi | ri, ci)]

−DKL [q(ri | wi, ci, yi) || p(ri)]

(11)

dri

(12)

(13)

(14)

(15)

(16)

DKL[qΦ(ri | wi, ci, yi)(cid:107)p(ri)] =

+ (µi − µ0)(cid:62)Σ−1

0 (µi − µ0) − K + log

(cid:18)

1
2

tr(Σ−1

0 Σi)
(cid:19)

|Σ0|
|Σi|

(17)

where Σi = diag(σ2
diag(σ2

0(α)).

i (wi, ci, yi)), and Σ0 =

C Practicalities and Implementation

As observed in past work, inference using a VAE
can suffer from component collapse, which trans-
lates into excessive redundancy in topics (i.e., many
topics containing the same set of words). To mit-
igate this problem, we borrow the approach used
by Srivastava and Sutton (2017), and make use of

the Adam optimizer with a high momentum, com-
bined with batchnorm layers to avoid divergence.
Speciﬁcally, we add batchnorm layers following
the computation of µ, log σ2, and η.

This effectively solves the problem of mode col-
lapse, but the batchnorm layer on η introduces an
additional problem, not previously reported. At test
time, the batchnorm layer will shift the input based
on the learned population mean of the training data;
this effectively encodes information about the dis-
tribution of words in this model that is not captured
by the topic weights and background distribution.
As such, although reconstruction error will be low,
the document representation θ, will not necessarily
be a useful representation of the topical content
of each document. In order to alleviate this prob-
lem, we reconstruct η as a convex combination
of two copies of the output of the generator net-
work, one passed through a batchnorm layer, and
one not. During training, we then gradually anneal
the model from relying entirely on the component
passed through the batchnorm layer, to relying en-
tirely on the one that is not. This ensures that the
the ﬁnal weights and document representations will
be properly interpretable.

Note that although ProdLDA (Srivastava and Sut-
ton, 2017) does not explicitly include a background
term, it is possible that the batchnorm layer applied
to η has a similar effect, albeit one that is not as
easily interpretable. This annealing process avoids
that ambiguity.

All datasets were preprocessed by tokenizing, con-
verting to lower case, removing punctuation, and
dropping all tokens that included numbers, all to-
kens less than 3 characters, and all words on the
stopword list from the snowball sampler.17 The
vocabulary was then formed by keeping the words
that occurred in the most documents (including
train and test), up to the desired size (2000 for 20
newsgroups, 5000 for the others). Note that these
small preprocessing decisions can make a large
difference to perplexity. We therefore include our
preprocessing scripts as part of our implementation
so as to facilitate easy future comparison.

For the UIUC Yahoo answers dataset, we down-
loaded the documents from the project webpage.18

17snowball.tartarus.org/algorithms/

english/stop.txt

18cogcomp.org/page/resource_view/89

B Model details

The KL divergence term in the variational bound
can be computed as

D Data

However, the ﬁle that is available does not com-
pletely match the description on the website. We
dropped Cars and Transportation and Social Sci-
ence which had less than the expected number of
documents, and merged Arts and Arts and Humani-
ties, which appeared to be the same category, pro-
ducing 15 categories, each with 10,000 documents.

E Experimental Details

For all experiments we use a model with 300-
dimensional embeddings of words, and we take
fe to be only the element-wise softplus nonlinear-
ity (followed by the linear transformations for µ
and log σ2). Similarly, fy is a linear transformation
of θ, followed by a softplus layer, followed by a
linear transformation to the size of the output (the
number of classes). During training, we set S (the
number of samples of (cid:15)) to be 1; for estimating the
ELBO at on test documents, we set S = 20.

For the unsupervised results, we use the same
set up as (Srivastava and Sutton, 2017): Adam op-
timizer with β1 = 0.99, learning rate = 0.002,
batch size of 200, and training for 200 epochs. The
setup was the same for all datasets, except we only
trained for 150 epochs on Yahoo answers because
it is much larger. For LDA, we updated the hyper-
parameters every 10 epochs.

For the external evaluation of NPMI, we use the
co-occurrence statistics from all New York Times
articles in the English Gigaword published from
the start of 2000 to the end of 2009, processed in
the same way as our data.

For the text classiﬁcation experiments, we use
the scikit-learn implementation of logistic
regression. We give it access to the same input
data as our model (using the same vocabulary), and
tune the strength of l2 regularization using cross-
validation. For our model, we only tune the number
of epochs, evaluating on development data. Our
models for this task did not use regularization or
word vectors.

F Additional Experimental Results

In this section we include additional experimental
results in the unsupervised setting.

Table 4 shows results on the 20 newsgroups
dataset, using 20 topics with a 2,000-word vocab-
ulary. Note that these results are not necessarily
directly comparable to previously published results,
as preprocessing decisions can have a large impact
on perplexity. These results show a similar pat-
tern to those on the IMDB data provided in the

Model
LDA
SAGE
NVDM
SCHOLAR - B.G.
SCHOLAR
SCHOLAR + W.V.
SCHOLAR + REG.

Ppl.
↓
810
867
1067
928
921
955
1053

NPMI
(int.) ↑
0.20
0.27
0.18
0.17
0.35
0.29
0.25

NPMI
(ext.) ↑
0.11
0.15
0.11
0.09
0.16
0.17
0.13

Sparsity
↑
0
0.71
0
0
0
0
0.43

Table 4: Performance of various models on the
20 newsgroups dataset with 20 topics and a 2,000-
word vocabulary.

Model
LDA
NVDM
SCHOLAR - B.G.
SCHOLAR
SCHOLAR + W.V.
SCHOLAR + REG.

Ppl.
↓
1035
4588
1589
1596
1780
1840

NPMI
(int.) ↑
0.29
0.20
0.27
0.33
0.37
0.34

NPMI
(ext.) ↑
0.15
0.09
0.16
0.13
0.15
0.13

Sparsity
↑
0
0
0
0
0
0.44

Table 5: Performance of various models on the
Yahoo answers dataset with 250 topics and a 5,000-
word vocabulary. SAGE did not ﬁnish in 72 hours
so we omit it from this table.

main paper, except that word vectors do not im-
prove internal coherence on this dataset, perhaps
because of the presence of relatively more names
and specialized terminology. Also, although the
NVDM still has worse perplexity than LDA, the
effects are not as dramatic as reported in (Srivas-
tava and Sutton, 2017). Regularization is also more
beneﬁcial for this data, with both SAGE and our
regularized model obtaining better coherence than
LDA. The topics from SCHOLAR for this dataset
are also shown in Table 6.

Table 5 shows the equivalent results for the Ya-
hoo answers dataset, using 250 topics, and a 5,000-
word vocabulary. These results closely match
those for the IMDB dataset, with our model having
higher perplexity but also higher internal coherence
than LDA. As with IMDB, the use of word vectors
improves coherence, both internally and externally,
but again at the cost of worse perplexity. Surpris-
ingly, our model without a background term actu-
ally has the best external coherence on this dataset,
but as described in the main paper, these tend to
give high weight primarily to common words, and
are more repetitive as a result.

NPMI Topic

turks armenian armenia turkish roads escape soviet muslim mountain soul
escrow clipper encryption wiretap crypto keys secure chip nsa key
jesus christ sin bible heaven christians church faith god doctrine
fbi waco batf clinton children koresh compound atf went ﬁre
players teams player team season baseball game fans roger league
guns gun weapons criminals criminal shooting police armed crime defend
playoff rangers detroit cup wings playoffs montreal toronto minnesota games
ftp images directory library available format archive graphics package macintosh
user server faq archive users ftp unix applications mailing directory
bike car cars riding ride engine rear bmw driving miles
study percent sexual medicine gay studies april percentage treatment published
israeli israel arab peace rights policy islamic civil adam citizens

0.77
0.52
0.49
0.43
0.41
0.39
0.37
0.36
0.33
0.32
0.32
0.32
0.30 morality atheist moral belief existence christianity truth exist god objective
space henry spencer international earth nasa orbit shuttle development vehicle
0.28
0.27
bus motherboard mhz ram controller port drive card apple mac
0.25 windows screen ﬁles button size program error mouse colors microsoft
0.24
0.21
0.19
0.04

sale shipping offer brand condition sell printer monitor items asking
driver drivers card video max advance vga thanks windows appreciated
cleveland advance thanks reserve ohio looking nntp western host usa
banks gordon univ keith soon pittsburgh michael computer article ryan

Table 6: Topics from the unsupervised SCHOLAR on the 20 newsgroups dataset, and the corresponding
internal coherence values.

Neural Models for Documents with Metadata

Dallas Card1 Chenhao Tan2 Noah A. Smith3
1Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, 15213, USA
2Department of Computer Science, University of Colorado, Boulder, CO, 80309, USA
3Paul G. Allen School of CSE, University of Washington, Seattle, WA, 98195, USA

dcard@cmu.edu

chenhao.tan@colorado.edu

nasmith@cs.washington.edu

Abstract

Most real-world document collections in-
volve various types of metadata, such as
author, source, and date, and yet the most
commonly-used approaches to modeling
text corpora ignore this information. While
specialized models have been developed
for particular applications, few are widely
used in practice, as customization typically
requires derivation of a custom inference
algorithm. In this paper, we build on recent
advances in variational inference methods
and propose a general neural framework,
based on topic models, to enable ﬂexible in-
corporation of metadata and allow for rapid
exploration of alternative models. Our ap-
proach achieves strong performance, with
a manageable tradeoff between perplex-
ity, coherence, and sparsity. Finally, we
demonstrate the potential of our framework
through an exploration of a corpus of arti-
cles about US immigration.

1

Introduction

Topic models comprise a family of methods for
uncovering latent structure in text corpora, and are
widely used tools in the digital humanities, political
science, and other related ﬁelds (Boyd-Graber et al.,
2017). Latent Dirichlet allocation (LDA; Blei et al.,
2003) is often used when there is no prior knowl-
edge about a corpus. In the real world, however,
most documents have non-textual attributes such
as author (Rosen-Zvi et al., 2004), timestamp (Blei
and Lafferty, 2006), rating (McAuliffe and Blei,
2008), or ideology (Eisenstein et al., 2011; Nguyen
et al., 2015b), which we refer to as metadata.

Many customizations of LDA have been devel-
oped to incorporate document metadata. Two mod-
els of note are supervised LDA (SLDA; McAuliffe

and Blei, 2008), which jointly models words and
labels (e.g., ratings) as being generated from a la-
tent representation, and sparse additive genera-
tive models (SAGE; Eisenstein et al., 2011), which
assumes that observed covariates (e.g., author ide-
ology) have a sparse effect on the relative proba-
bilities of words given topics. The structural topic
model (STM; Roberts et al., 2014), which adds cor-
relations between topics to SAGE, is also widely
used, but like SAGE it is limited in the types of
metadata it can efﬁciently make use of, and how
that metadata is used. Note that in this work we
will distinguish labels (metadata that are gener-
ated jointly with words from latent topic represen-
tations) from covariates (observed metadata that
inﬂuence the distribution of labels and words).

The ability to create variations of LDA such as
those listed above has been limited by the expertise
needed to develop custom inference algorithms for
each model. As a result, it is rare to see such varia-
tions being widely used in practice. In this work,
we take advantage of recent advances in variational
methods (Kingma and Welling, 2014; Rezende
et al., 2014; Miao et al., 2016; Srivastava and Sut-
ton, 2017) to facilitate approximate Bayesian infer-
ence without requiring model-speciﬁc derivations,
and propose a general neural framework for topic
models with metadata, SCHOLAR.1

SCHOLAR combines the abilities of SAGE and
SLDA, and allows for easy exploration of the fol-
lowing options for customization:

1. Covariates: as in SAGE and STM, we incorpo-
rate explicit deviations for observed covariates,
as well as effects for interactions with topics.

2. Supervision: as in SLDA, we can use metadata
as labels to help infer topics that are relevant in
predicting those labels.

1Sparse Contextual Hidden and Observed Language

AutoencodeR.

8
1
0
2
 
t
c
O
 
3
2
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
6
9
2
9
0
.
5
0
7
1
:
v
i
X
r
a

3. Rich encoder network: we use the encoding
network of a variational autoencoder (VAE) to
incorporate additional prior knowledge in the
form of word embeddings, and/or to provide
interpretable embeddings of covariates.

4. Sparsity: as in SAGE, a sparsity-inducing prior
can be used to encourage more interpretable
topics, represented as sparse deviations from a
background log-frequency.

We begin with the necessary background and
motivation (§2), and then describe our basic frame-
work and its extensions (§3), followed by a series
of experiments (§4). In an unsupervised setting,
we can customize the model to trade off between
perplexity, coherence, and sparsity, with improved
coherence through the introduction of word vec-
tors. Alternatively, by incorporating metadata we
can either learn topics that are more predictive of
labels than SLDA, or learn explicit deviations for
particular parts of the metadata. Finally, by com-
bining all parts of our model we can meaningfully
incorporate metadata in multiple ways, which we
demonstrate through an exploration of a corpus of
news articles about US immigration.

In presenting this particular model, we empha-
size not only its ability to adapt to the characteris-
tics of the data, but the extent to which the VAE
approach to inference provides a powerful frame-
work for latent variable modeling that suggests the
possibility of many further extensions. Our im-
plementation is available at https://github.
com/dallascard/scholar.

2 Background and Motivation

LDA can be understood as a non-negative
Bayesian matrix factorization model: the observed
document-word frequency matrix, X ∈ ZD×V
(D is the number of documents, V is the vocab-
ulary size) is factored into two low-rank matri-
ces, ΘD×K and BK×V , where each row of Θ,
θi ∈ ∆K is a latent variable representing a distri-
bution over topics in document i, and each row of
B, βk ∈ ∆V , represents a single topic, i.e., a dis-
tribution over words in the vocabulary.2 While it is
possible to factor the count data into unconstrained

2Z denotes nonnegative integers, and ∆K denotes the set
of K-length nonnegative vectors that sum to one. For a proper
probabilistic interpretation, the matrix to be factored is actually
the matrix of latent mean parameters of the assumed data
generating process, Xij ∼ Poisson(Λij). See Cemgil (2009)
or Paisley et al. (2014) for details.

matrices, the particular priors assumed by LDA
are important for interpretability (Wallach et al.,
2009). For example, the neural variational docu-
ment model (NVDM; Miao et al., 2016) allows
θi ∈ RK and achieves normalization by taking
the softmax of θ(cid:62)
i B. However, the experiments
in Srivastava and Sutton (2017) found the perfor-
mance of the NVDM to be slightly worse than LDA
in terms of perplexity, and dramatically worse in
terms of topic coherence.

The topics discovered by LDA tend to be parsi-
monious and coherent groupings of words which
are readily identiﬁable to humans as being related
to each other (Chang et al., 2009), and the resulting
mode of the matrix Θ provides a representation of
each document which can be treated as a measure-
ment for downstream tasks, such as classiﬁcation
or answering social scientiﬁc questions (Wallach,
2016). LDA does not require — and cannot make
use of — additional prior knowledge. As such, the
topics that are discovered may bear little connec-
tion to metadata of a corpus that is of interest to a
researcher, such as sentiment, ideology, or time.

In this paper, we take inspiration from two mod-
els which have sought to alleviate this problem.
The ﬁrst, supervised LDA (SLDA; McAuliffe and
Blei, 2008), assumes that documents have labels y
which are generated conditional on the correspond-
ing latent representation, i.e., yi ∼ p(y | θi).3 By
incorporating labels into the model, it is forced to
learn topics which allow documents to be repre-
sented in a way that is useful for the classiﬁcation
task. Such models can be used inductively as text
classiﬁers (Balasubramanyan et al., 2012).

SAGE (Eisenstein et al., 2011), by contrast, is
an exponential-family model, where the key inno-
vation was to replace topics with sparse deviations
from the background log-frequency of words (d),
i.e., p(word | softmax(d + θ(cid:62)
i B)). SAGE can also
incorporate deviations for observed covariates, as
well as interactions between topics and covariates,
by including additional terms inside the softmax.
In principle, this allows for inferring, for example,
the effect on an author’s ideology on their choice
of words, as well as ideological variations on each
underlying topic. Unlike the NVDM, SAGE still
constrains θi to lie on the simplex, as in LDA.

SLDA and SAGE provide two different ways
that users might wish to incorporate prior knowl-

3Technically, the model conditions on the mean of the per-
word latent variables, but we elide this detail in the interest of
concision.

edge as a way of guiding the discovery of topics
in a corpus: SLDA incorporates labels through a
distribution conditional on topics; SAGE includes
explicit sparse deviations for each unique value of
a covariate, in addition to topics.4

Because of the Dirichlet-multinomial conjugacy
in the original model, efﬁcient inference algorithms
exist for LDA. Each variation of LDA, however,
has required the derivation of a custom inference
algorithm, which is a time-consuming and error-
prone process. In SLDA, for example, each type of
distribution we might assume for p(y | θ) would
require a modiﬁcation of the inference algorithm.
SAGE breaks conjugacy, and as such, the authors
adopted L-BFGS for optimizing the variational
bound. Moreover, in order to maintain compu-
tational efﬁciency, it assumed that covariates were
limited to a single categorical label.

More recently,

the variational autoencoder
(VAE) was introduced as a way to perform approxi-
mate posterior inference on models with otherwise
intractable posteriors (Kingma and Welling, 2014;
Rezende et al., 2014). This approach has previously
been applied to models of text by Miao et al. (2016)
and Srivastava and Sutton (2017). We build on their
work and show how this framework can be adapted
to seamlessly incorporate the ideas of both SAGE
and SLDA, while allowing for greater ﬂexibility in
the use of metadata. Moreover, by exploiting au-
tomatic differentiation, we allow for modiﬁcation
of the model without requiring any change to the
inference procedure. The result is not only a highly
adaptable family of models with scalable inference
and efﬁcient prediction; it also points the way to
incorporation of many ideas found in the literature,
such as a gradual evolution of topics (Blei and Laf-
ferty, 2006), and hierarchical models (Blei et al.,
2010; Nguyen et al., 2013, 2015b).

3

SCHOLAR: A Neural Topic Model with
Covariates, Supervision, and Sparsity

We begin by presenting the generative story for our
model, and explain how it generalizes both SLDA
and SAGE (§3.1). We then provide a general expla-
nation of inference using VAEs and how it applies
to our model (§3.2), as well as how to infer docu-

4A third way of incorporating metadata is the approach
used by various “upstream” models, such as Dirichlet-
multinomial regression (Mimno and McCallum, 2008), which
uses observed metadata to inform the document prior. We hy-
pothesize that this approach could be productively combined
with our framework, but we leave this as future work.

ment representations and predict labels at test time
(§3.3). Finally, we discuss how we can incorporate
additional prior knowledge (§3.4).

3.1 Generative Story

Consider a corpus of D documents, where docu-
ment i is a list of Ni words, wi, with V words in
the vocabulary. For each document, we may have
observed covariates ci (e.g., year of publication),
and/or one or more labels, yi (e.g., sentiment).

Our model builds on the generative story of LDA,
but optionally incorporates labels and covariates,
and replaces the matrix product of Θ and B with a
more ﬂexible generative network, fg, followed by
a softmax transform. Instead of using a Dirichlet
prior as in LDA, we employ a logistic normal prior
on θ as in Srivastava and Sutton (2017) to facilitate
inference (§3.2): we draw a latent variable, r,5
from a multivariate normal, and transform it to lie
on the simplex using a softmax transform.6

The generative story is shown in Figure 1a and

described in equations below:

For each document i of length Ni:

# Draw a latent representation on the sim-
plex from a logistic normal prior:
ri ∼ N (r | µ0(α), diag(σ2
0(α)))
θi = softmax(ri)
# Generate words, incorporating covariates:
ηi = fg(θi, ci)
For each word j in document i:
wij ∼ p(w | softmax(ηi))

# Similarly generate labels:
yi ∼ p(y | fy(θi, ci)),

where p(w | softmax(ηi)) is a multinomial distri-
bution and p(y | fy(θi, ci)) is a distribution appro-
priate to the data (e.g., multinomial for categorical
labels). fg is a model-speciﬁc combination of latent
variables and covariates, fy is a multi-layer neural
network, and µ0(α) and σ2
0(α) are the mean and
diagonal covariance terms of a multivariate nor-
mal prior. To approximate a symmetric Dirichlet

5r is equivalent to z in the original VAE. To avoid con-
fusion with topic assignment of words in the topic modeling
literature, we use r instead of z.

6Unlike the correlated topic model (CTM; Lafferty and
Blei, 2006), which also uses a logistic-normal prior, we ﬁx the
parameters of the prior and use a diagonal covariance matrix,
rather than trying to infer correlations among topics. However,
it would be a straightforward extension of our framework to
place a richer prior on the latent document representations,
and learn correlations by updating the parameters of this prior
after each epoch, analogously to the variational EM approach
used for the CTM.

prior with hyperparameter α, these are given by the
Laplace approximation (Hennig et al., 2012) to be
µ0,k(α) = 0 and σ2

0,k = (K − 1)/(αK).

If we were to ignore covariates, place a Dirichlet
prior on B, and let η = θ(cid:62)
i B, this model is equiv-
alent to SLDA with a logistic normal prior. Sim-
ilarly, we can recover a model that is like SAGE,
but lacks sparsity, if we ignore labels, and let

ηi = d + θ(cid:62)

i B + c(cid:62)

i Bcov + (θi ⊗ ci)(cid:62)Bint, (1)

where d is the V -dimensional background term
(representing the log of the overall word frequency),
θi ⊗ ci is a vector of interactions between topics
and covariates, and Bcov and Bint are additional
weight (deviation) matrices. The background is
included to account for common words with ap-
proximately the same frequency across documents,
meaning that the B∗ weights now represent both
positive and negative deviations from this back-
ground. This is the form of fg which we will use
in our experiments.

To recover the full SAGE model, we can place
a sparsity-inducing prior on each B∗. As in Eisen-
stein et al. (2011), we make use of the compound
normal-exponential prior for each element of the
weight matrices, B∗

m,n, with hyperparameter γ,7

τm,n ∼ Exponential(γ),
B∗
m,n ∼ N (0, τm,n).

(2)

(3)

We can choose to ignore various parts of this
model, if, for example, we don’t have any labels
or observed covariates, or we don’t wish to use
interactions or sparsity.8 Other generator networks
could also be considered, with additional layers to
represent more complex interactions, although this
might involve some loss of interpretability.

In the absence of metadata, and without sparsity,
our model is equivalent to the ProdLDA model
of Srivastava and Sutton (2017) with an explicit
background term, and ProdLDA is, in turn, a

7To avoid having to tune γ, we employ an improper Jef-
fery’s prior, p(τm,n) ∝ 1/τm,n, as in SAGE. Although this
causes difﬁculties in posterior inference for the variance terms,
τ , in practice, we resort to a variational EM approach, with
MAP-estimation for the weights, B, and thus alternate be-
tween computing expectations of the τ parameters, and up-
dating all other parameters using some variant of stochastic
gradient descent. For this, we only require the expectation of
each τmn for each E-step, which is given by 1/B2
m,n. We re-
fer the reader to Eisenstein et al. (2011) for additional details.
8We could also ignore latent topics, in which case we
would get a naïve Bayes-like model of text with deviations for
each covariate p(wij | ci) ∝ exp(d + c(cid:62)

i Bcov).

α

r

θ

η

w

Ni

D

B

c

y

y

w

c

i

n

π
ar l
e
lin
µ

σ

e

a

r

r

(cid:15)

D

(a) Generative model

(b) Inference model

Figure 1: Figure 1a presents the generative story of
our model. Figure 1b illustrates the inference net-
work using the reparametrization trick to perform
variational inference on our model. Shaded nodes
are observed; double circles indicate deterministic
transformations of parent nodes.

special case of SAGE, without background log-
frequencies, sparsity, covariates, or labels. In the
next section we generalize the inference method
used for ProdLDA; in our experiments we validate
its performance and explore the effects of regular-
ization and word-vector initialization (§3.4). The
NVDM (Miao et al., 2016) uses the same approach
to inference, but does not not restrict document
representations to the simplex.

3.2 Learning and Inference

As in past work, each document i is assumed to
have a latent representation ri, which can be in-
terpreted as its relative membership in each topic
(after exponentiating and normalizing). In order
to infer an approximate posterior distribution over
ri, we adopt the sampling-based VAE framework
developed in previous work (Kingma and Welling,
2014; Rezende et al., 2014).

As in conventional variational inference, we
assume a variational approximation to the poste-
rior, qΦ(ri | wi, ci, yi), and seek to minimize the
KL divergence between it and the true posterior,
p(ri | wi, ci, yi), where Φ is the set of variational
parameters to be deﬁned below. After some ma-
nipulations (given in supplementary materials), we
obtain the evidence lower bound (ELBO) for a sin-

gle document,

ple10 of (cid:15) (and thereby of r):

L(wi) = EqΦ(ri|wi,ci,yi)

log p(wij | ri, ci)


Ni(cid:88)






L(wi)
Ni(cid:88)

≈

j=1
+ EqΦ(ri|wi,ci,yi) [log p(yi | ri, ci)]
− DKL [qΦ(ri | wi, ci, yi) || p(ri | α)] .
(4)

log p(wij | r(s)

, ci) + log p(yi | r(s)

, ci)

i

i

j=1
− DKL [qΦ(ri | wi, ci, yi) || p(ri | α)] .

(8)

We can now optimize this sampling-based approxi-
mation of the variational bound with respect to Φ,
B∗, and all parameters of fg and fy using stochas-
tic gradient descent. Moreover, because of this
stochastic approach to inference, we are not re-
stricted to covariates with a small number of unique
values, which was a limitation of SAGE. Finally,
the KL divergence term in Equation 8 can be com-
puted in closed form (see supplementary materials).

3.3 Prediction on Held-out Data

In addition to inferring latent topics, our model
can both infer latent representations for new docu-
ments and predict their labels, the latter of which
was the motivation for SLDA. In traditional vari-
ational inference, inference at test time requires
ﬁxing global parameters (topics), and optimizing
the per-document variational parameters for the
test set. With the VAE framework, by contrast,
the encoder network (Equations 5–7) can be used
to directly estimate the posterior distribution for
each test document, using only a forward pass (no
iterative optimization or sampling).

If not using labels, we can use this approach di-
rectly, passing the word counts of new documents
through the encoder to get a posterior qΦ(ri
|
wi, ci). When we also include labels to be pre-
dicted, we can ﬁrst train a fully-observed model, as
above, then ﬁx the decoder, and retrain the encoder
without labels. In practice, however, if we train
the encoder network using one-hot encodings of
document labels, it is sufﬁcient to provide a vector
of all zeros for the labels of test documents; this is
what we adopt for our experiments (§4.2), and we
still obtain good predictive performance.

The label network, fy, is a ﬂexible component
which can be used to predict a wide range of out-
comes, from categorical labels (such as star ratings;
McAuliffe and Blei, 2008) to real-valued outputs
(such as number of citations or box-ofﬁce returns;

10Alternatively, one can average over multiple samples.

As in the original VAE, we will encode the pa-
rameters of our variational distributions using a
shared multi-layer neural network. Because we
have assumed a diagonal normal prior on r, this
will take the form of a network which outputs a
mean vector, µi = fµ(wi, ci, yi) and diagonal of a
covariance matrix, σ2
i = fσ(wi, ci, yi), such that
qΦ(ri | wi, ci, yi) = N (µi, σ2
i ). Incorporating
labels and covariates to the inference network used
by Miao et al. (2016) and Srivastava and Sutton
(2017), we use:

πi = fe([Wxxi; Wcci; Wyyi]),
µi = Wµπi + bµ,
i = Wσπi + bσ,

log σ2

(5)

(6)

(7)

where xi is a V -dimensional vector representing
the counts of words in wi, and fe is a multilayer
perceptron. The full set of encoder parameters, Φ,
thus includes the parameters of fe and all weight
matrices and bias vectors in Equations 5–7 (see
Figure 1b).

This approach means that the expectations in
Equation 4 are intractable, but we can approximate
them using sampling. In order to maintain differen-
tiability with respect to Φ, even after sampling, we
make use of the reparameterization trick (Kingma
and Welling, 2014),9 which allows us to reparame-
terize samples from qΦ(r | wi, ci, yi) in terms of
samples from an independent source of noise, i.e.,

(cid:15)(s) ∼ N (0, I),

r(s)
i = gΦ(wi, ci, yi, (cid:15)(s)) = µi + σi · (cid:15)(s).

We thus replace the bound in Equation 4 with
a Monte Carlo approximation using a single sam-

9 The Dirichlet distribution cannot be directly reparame-
terized in this way, which is why we use the logistic normal
prior on θ to approximate the Dirichlet prior used in LDA.

Yogatama et al., 2011). For categorical labels, pre-
dictions are given by

ˆyi = argmax

p(y | ri, ci).

(9)

y ∈ Y

Alternatively, when dealing with a small set of
categorical labels, it is also possible to treat them as
observed categorical covariates during training. At
test time, we can then consider all possible one-hot
vectors, e, in place of ci, and predict the label that
maximizes the probability of the words, i.e.,

ˆyi = argmax

log p(wij | ri, ey).

(10)

Ni(cid:88)

j=1

y ∈ Y

This approach works well in practice (as we show
in §4.2), but does not scale to large numbers of
labels, or other types of prediction problems, such
as multi-class classiﬁcation or regression.

The choice to include metadata as covariates, la-
bels, or both, depends on the data. The key point
is that we can incorporate metadata in two very
different ways, depending on what we want from
the model. Labels guide the model to infer topics
that are relevant to those labels, whereas covari-
ates induce explicit deviations, leaving the latent
variables to account for the rest of the content.

3.4 Additional Prior Information

A ﬁnal advantage of the VAE framework is that
the encoder network provides a way to incorporate
additional prior information in the form of word
vectors. Although we can learn all parameters start-
ing from a random initialization, it is also possible
to initialize and ﬁx the initial embeddings of words
in the model, Wx, in Equation 5. This leverages
word similarities derived from large amounts of un-
labeled data, and may promote greater coherence
in inferred topics. The same could also be done
for some covariates; for example, we could embed
the source of a news article based on its place on
the ideological spectrum. Conversely, if we choose
to learn these parameters, the learned values (Wy
and Wc) may provide meaningful embeddings of
these metadata (see section §4.3).

Other variants on topic models have also pro-
posed incorporating word vectors, both as a par-
allel part of the generative process (Nguyen et al.,
2015a), and as an alternative parameterization of
topic distributions (Das et al., 2015), but inference
is not scalable in either of these models. Because
of the generality of the VAE framework, we could

also modify the generative story so that word em-
beddings are emitted (rather than tokens); we leave
this for future work.

4 Experiments and Results

To evaluate and demonstrate the potential of this
model, we present a series of experiments below.
We ﬁrst test SCHOLAR without observed meta-
data, and explore the effects of using regulariza-
tion and/or word vector initialization, compared to
LDA, SAGE, and NVDM (§4.1). We then evaluate
our model in terms of predictive performance, in
comparison to SLDA and an l2-regularized logistic
regression baseline (§4.2). Finally, we demonstrate
the ability to incorporate covariates and/or labels
in an exploratory data analysis (§4.3).

The scores we report are generalization to held-
out data, measured in terms of perplexity; coher-
ence, measured in terms of non-negative point-wise
mutual information (NPMI; Chang et al., 2009;
Newman et al., 2010), and classiﬁcation accuracy
on test data. For coherence we evaluate NPMI us-
ing the top 10 words of each topic, both internally
(using test data), and externally, using a decade of
articles from the English Gigaword dataset (Graff
and Cieri, 2003). Since our model employs varia-
tional methods, the reported perplexity is an upper
bound based on the ELBO.

As datasets we use the familiar 20 newsgroups,
the IMDB corpus of 50,000 movie reviews (Maas
et al., 2011), and the UIUC Yahoo answers dataset
with 150,000 documents in 15 categories (Chang
et al., 2008). For further exploration, we also
make use of a corpus of approximately 4,000 time-
stamped news articles about US immigration, each
annotated with pro- or anti-immigration tone (Card
et al., 2015). We use the original author-provided
implementations of SAGE11 and SLDA,12 while
for LDA we use Mallet.13. Our implementation
of SCHOLAR is in TensorFlow, but we have also
provided a preliminary PyTorch implementation
of the core of our model.14 For additional details
about datasets and implementation, please refer to
the supplementary material.

It is challenging to fairly evaluate the relative
computational efﬁciency of our approach compared
to past work (due to the stochastic nature of our ap-

11github.com/jacobeisenstein/SAGE
12github.com/blei-lab/class-slda
13mallet.cs.umass.edu
14github.com/dallascard/scholar

proach to inference, choices about hyperparameters
such as tolerance, and because of differences in im-
plementation). Nevertheless, in practice, the perfor-
mance of our approach is highly appealing. For all
experiments in this paper, our implementation was
much faster than SLDA or SAGE (implemented in
C and Matlab, respectively), and competitive with
Mallet.

4.1 Unsupervised Evaluation

Although the emphasis of this work is on incorpo-
rating observed labels and/or covariates, we brieﬂy
report on experiments in the unsupervised setting.
Recall that, without metadata, SCHOLAR equates to
ProdLDA, but with an explicit background term.15
We therefore use the same experimental setup as
Srivastava and Sutton (2017) (learning rate, mo-
mentum, batch size, and number of epochs) and
ﬁnd the same general patterns as they reported (see
Table 1 and supplementary material): our model
returns more coherent topics than LDA, but at the
cost of worse perplexity. SAGE, by contrast, attains
very high levels of sparsity, but at the cost of worse
perplexity and coherence than LDA. As expected,
the NVDM produces relatively low perplexity, but
very poor coherence, due to its lack of constraints
on θ.

Further experimentation revealed that the VAE
framework involves a tradeoff among the scores;
running for more epochs tends to result in bet-
ter perplexity on held-out data, but at the cost of
worse coherence. Adding regularization to encour-
age sparse topics has a similar effect as in SAGE,
leading to worse perplexity and coherence, but it
does create sparse topics. Interestingly, initializing
the encoder with pretrained word2vec embeddings,
and not updating them returned a model with the
best internal coherence of any model we considered
for IMDB and Yahoo answers, and the second-best
for 20 newsgroups.

The background term in our model does not have
much effect on perplexity, but plays an important
role in producing coherent topics; as in SAGE, the
background can account for common words, so
they are mostly absent among the most heavily
weighted words in the topics. For instance, words
like ﬁlm and movie in the IMDB corpus are rel-
atively unimportant in the topics learned by our

15Note, however, that a batchnorm layer in ProdLDA may
play a similar role to a background term, and there are small
differences in implementation; please see supplementary ma-
terial for more discussion of this.

Model

LDA
SAGE
NVDM
SCHOLAR − B.G.
SCHOLAR
SCHOLAR + W.V.
SCHOLAR + REG.

Ppl.
↓

1508
1767
1748
1889
1905
1991
2185

NPMI
(int.) ↑

NPMI
(ext.) ↑

Sparsity
↑

0.13
0.12
0.06
0.09
0.14
0.18
0.10

0.14
0.12
0.04
0.13
0.13
0.17
0.12

0
0.79
0
0
0
0
0.58

Table 1: Performance of our various models in
an unsupervised setting (i.e., without labels or co-
variates) on the IMDB dataset using a 5,000-word
vocabulary and 50 topics. The supplementary ma-
terials contain additional results for 20 newsgroups
and Yahoo answers.

model, but would be much more heavily weighted
without the background term, as they are in topics
learned by LDA.

4.2 Text Classiﬁcation

We next consider the utility of our model in the
context of categorical labels, and consider them
alternately as observed covariates and as labels
generated conditional on the latent representation.
We use the same setup as above, but tune number of
training epochs for our model using a random 20%
of training data as a development set, and similarly
tune regularization for logistic regression.

Table 2 summarizes the accuracy of various mod-
els on three datasets, revealing that our model offers
competitive performance, both as a joint model of
words and labels (Eq. 9), and a model which condi-
tions on covariates (Eq. 10). Although SCHOLAR
is comparable to the logistic regression baseline,
our purpose here is not to attain state-of-the-art per-
formance on text classiﬁcation. Rather, the high
accuracies we obtain demonstrate that we are learn-
ing low-dimensional representations of documents
that are relevant to the label of interest, outperform-
ing SLDA, and have the same attractive properties
as topic models. Further, any neural network that
is successful for text classiﬁcation could be incor-
porated into fy and trained end-to-end along with
topic discovery.

4.3 Exploratory Study

We demonstrate how our model might be used to
explore an annotated corpus of articles about immi-
gration, and adapt to different assumptions about
the data. We only use a small number of topics in
this part (K = 8) for compact presentation.

Vocabulary size
Number of topics

20news
2000
50

IMDB Yahoo
5000
5000
250
50

SLDA
SCHOLAR (labels)
SCHOLAR (covariates)
Logistic regression

0.60
0.67
0.71
0.70

0.64
0.86
0.87
0.87

0.65
0.73
0.72
0.76

Table 2: Accuracy of various models on three
datasets with categorical labels.

Tone as a label. We ﬁrst consider using the an-
notations as a label, and train a joint model to infer
topics relevant to the tone of the article (pro- or
anti-immigration). Figure 2 shows a set of top-
ics learned in this way, along with the predicted
probability of an article being pro-immigration con-
ditioned on the given topic. All topics are coherent,
and the predicted probabilities have strong face
validity, e.g., “arrested charged charges agents op-
eration” is least associated with pro-immigration.

Tone as a covariate. Next we consider using
tone as a covariate, and build a model using both
tone and tone-topic interactions. Table 3 shows
a set of topics learned from the immigration data,
along with the most highly-weighted words in the
corresponding tone-topic interaction terms. As can
be seen, these interaction terms tend to capture dif-
ferent frames (e.g., “criminal” vs. “detainees”, and
“illegals” vs. “newcomers”, etc).

Combined model with temporal metadata. Fi-
nally, we incorporate both the tone annotations and
the year of publication of each article, treating the
former as a label and the latter as a covariate. In
this model, we also include an embedding matrix,
Wc, to project the one-hot year vectors down to a
two-dimensional continuous space, with a learned
deviation for each dimension. We omit the topics
in the interest of space, but Figure 3 shows the
learned embedding for each year, along with the
top terms of the corresponding deviations. As can
be seen, the model learns that adjacent years tend
to produce similar deviations, even though we have
not explicitly encoded this information. The left-
right dimension roughly tracks a temporal trend
with positive deviations shifting from the years of
Clinton and INS on the left, to Obama and ICE on
the right.16 Meanwhile, the events of 9/11 dom-
inate the vertical direction, with the words sept,

16The Immigration and Naturalization Service (INS) was
transformed into Immigration and Customs Enforcement
(ICE) and other agencies in 2003.

Figure 2: Topics inferred by a joint model of words
and tone, and the corresponding probability of pro-
immigration tone for each topic. A topic is repre-
sented by the top words sorted by word probability
throughout the paper.

Figure 3:
Learned embeddings of year-of-
publication (treated as a covariate) from combined
model of news articles about immigration.

hijackers, and attacks increasing in probability as
we move up in the space. If we wanted to look at
each year individually, we could drop the embed-
ding of years, and learn a sparse set of topic-year
interactions, similar to tone in Table 3.

5 Additional Related Work

The literature on topic models is vast; in addition
to papers cited throughout, other efforts to incorpo-
rate metadata into topic models include Dirichlet-
multinomial regression (DMR; Mimno and McCal-
lum, 2008), Labeled LDA (Ramage et al., 2009),
and MedLDA (Zhu et al., 2009). A recent paper
also extended DMR by using deep neural networks
to embed metadata into a richer document prior
(Benton and Dredze, 2018).

A separate line of work has pursued parame-
terizing unsupervised models of documents us-
ing neural networks (Hinton and Salakhutdinov,

Base topics (each row is a topic)
ice customs agency enforcement homeland
population born percent americans english
judge case court guilty appeals attorney
patrol border miles coast desert boat guard
licenses drivers card visa cards applicants
island story chinese ellis international
guest worker workers bush labor bill
beneﬁts bill welfare republican state senate

Anti-immigration interactions
criminal customs arrested
jobs million illegals taxpayers
guilty charges man charged
patrol border agents boat
foreign sept visas system
smuggling federal charges
bill border house senate
republican california gov state

Pro-immigration interactions
detainees detention center agency
english newcomers hispanic city
asylum court judge case appeals
died authorities desert border bodies
green citizenship card citizen apply
island school ellis english story
workers tech skilled farm labor
law welfare students tuition

Table 3: Top words for topics (left) and the corresponding anti-immigration (middle) and pro-immigration
(right) variations when treating tone as a covariate, with interactions.

2009; Larochelle and Lauly, 2012), including non-
Bayesian approaches (Cao et al., 2015). More re-
cently, Lau et al. (2017) proposed a neural language
model that incorporated topics, and He et al. (2017)
developed a scalable alternative to the correlated
topic model by simultaneously learning topic em-
beddings.

Others have attempted to extend the reparameter-
ization trick to the Dirichlet and Gamma distribu-
tions, either through transformations (Kucukelbir
et al., 2016) or a generalization of reparameteriza-
tion (Ruiz et al., 2016). Black-box and VAE-style
inference have been implemented in at least two
general purpose tools designed to allow rapid explo-
ration and evaluation of models (Kucukelbir et al.,
2015; Tran et al., 2016).

6 Conclusion

We have presented a neural framework for general-
ized topic models to enable ﬂexible incorporation
of metadata with a variety of options. We take
advantage of stochastic variational inference to de-
velop a general algorithm for our framework such
that variations do not require any model-speciﬁc
algorithm derivations. Our model demonstrates
the tradeoff between perplexity, coherence, and
sparsity, and outperforms SLDA in predicting doc-
ument labels. Furthermore, the ﬂexibility of our
model enables intriguing exploration of a text cor-
pus on US immigration. We believe that our model
and code will facilitate rapid exploration of docu-
ment collections with metadata.

Acknowledgments

We would like to thank Charles Sutton, anonymous
reviewers, and all members of Noah’s ARK for
helpful discussions and feedback. This work was
made possible by a University of Washington Inno-
vation award and computing resources provided by
XSEDE.

References

Ramnath Balasubramanyan, William W. Cohen, Doug
Pierce, and David P. Redlawsk. 2012. Modeling po-
larizing topics: When do different political commu-
nities respond differently to the same news? In Pro-
ceedings of ICWSM.

Adrian Benton and Mark Dredze. 2018. Deep Dirichlet
multinomial regression. In Proceedings of NAACL.

David M. Blei, Thomas L. Grifﬁths, and Michael I. Jor-
dan. 2010. The nested Chinese restaurant process
and Bayesian nonparametric inference of topic hier-
archies. JACM, 57(2).

David M. Blei and John D. Lafferty. 2006. Dynamic

topic models. In Proceedings of ICML.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. JMLR, 3:993–
1022.

Jordan Boyd-Graber, Yuening Hu, and David Mimno.
2017. Applications of topic models. Foundations
and Trends in Information Retrieval, 11(2-3):143–
296.

Ziqiang Cao, Sujian Li, Yang Liu, Wenjie Li, and Heng
Ji. 2015. A novel neural topic model and its super-
vised extension. In Proceedings of AAAI.

Dallas Card, Amber E. Boydstun, Justin H. Gross,
Philip Resnik, and Noah A. Smith. 2015. The media
frames corpus: Annotations of frames across issues.
In Proceedings of ACL.

Ali Taylan Cemgil. 2009. Bayesian inference for
nonnegative matrix factorisation models. Compu-
tational Intelligence and Neuroscience, pages 4:1–
4:17.

Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L
Boyd-graber, and David M Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Pro-
ceedings of NIPS.

Ming-Wei Chang, Lev-Arie Ratinov, Dan Roth, and
Vivek Srikumar. 2008. Importance of semantic rep-
resentation: Dataless classiﬁcation. In Proceedings
of AAAI.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian LDA for topic models with word embed-
dings. In Proceedings of ACL.

Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
In Pro-

Sparse additive generative models of text.
ceedings of ICML.

David Graff and C Cieri. 2003. English gigaword cor-

pus. Linguistic Data Consortium.

Junxian He, Zhiting Hu, Taylor Berg-Kirkpatrick, Ying
Huang, and Eric P. Xing. 2017. Efﬁcient correlated
topic modeling with topic embedding. In Proceed-
ings of KDD.

Philipp Hennig, David Stern, Ralf Herbrich, and Thore
Graepel. 2012. Kernel topic models. In Proceedings
of AISTATS.

Geoffrey E Hinton and Ruslan R Salakhutdinov. 2009.
Replicated softmax: An undirected topic model. In
Proceedings of NIPS.

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In Proceedings of ICLR.

Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman,
and David Blei. 2015. Automatic variational infer-
ence in stan. In Proceedings of NIPS.

Alp Kucukelbir, Dustin Tran, Rajesh Ranganath,
and David M. Blei. 2016.
inference.

Andrew Gelman,
Automatic differentiation variational
ArXiv:1603.00788.

John D. Lafferty and David M. Blei. 2006. Correlated

topic models. In Proceedings of NIPS.

Hugo Larochelle and Stanislas Lauly. 2012. A neural
autoregressive topic model. In Proceedings of NIPS.

Jey Han Lau, Timothy Baldwin, and Trevor Cohn.
2017. Topically driven neural language model. In
Proceedings of ACL.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of ACL.

Jon D. McAuliffe and David M. Blei. 2008. Supervised

topic models. In Proceedings of NIPS.

Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
and Kristina Miler. 2015b. Tea party in the house: A
hierarchical ideal point topic model and its applica-
tion to Republican legislators in the 112th congress.
In Proceedings of ACL.

Viet-An Nguyen, Jordan L. Boyd-Graber, and Philip
Resnik. 2013. Lexical and hierarchical topic regres-
sion. In Proceedings of NIPS.

John William Paisley, David M. Blei, and Michael I.
Jordan. 2014. Bayesian nonnegative matrix fac-
torization with stochastic variational inference.
In
Handbook of Mixed Membership Models and Their
Applications, pages 205–224.

Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A su-
pervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of EMNLP.

Danilo J. Rezende, Shakir Mohamed, and Daan Wier-
stra. 2014. Stochastic backpropagation and approx-
imate inference in deep generative models. In Pro-
ceedings of ICML.

Molly Roberts, Brandon Stewart, Dustin Tingley,
Christopher Lucas, Jetson Leder-Luis, Shana Gadar-
ian, Bethany Albertson, and David Rand. 2014.
Structural topic models for open ended survey re-
sponses. American Journal of Political Science,
58:1064–1082.

Michal Rosen-Zvi, Thomas Grifﬁths, Mark Steyvers,
and Padhraic Smyth. 2004. The author-topic model
for authors and documents. In Proceedings of UAI.

Francisco J R Ruiz, Michalis K Titsias, and David M
Blei. 2016. The generalized reparameterization gra-
dient. In Proceedings of NIPS.

Akash Srivastava and Charles Sutton. 2017. Autoen-
coding variational inference for topic models.
In
Proceedings of ICLR.

Dustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja
Rudolph, Dawen Liang, and David M. Blei. 2016.
Edward: A library for probabilistic modeling, infer-
ence, and criticism. ArXiv:1610.09787.

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neural
variational inference for text processing. In Proceed-
ings of ICML.

Hanna Wallach. 2016.

Interpretability and measure-
ment. EMNLP Workshop on Natural Language Pro-
cessing and Computational Social Science.

David Mimno and Andrew McCallum. 2008. Topic
models conditioned on arbitrary features with
Dirichlet-multinomial regression. In Proceedings of
UAI.

David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In Proceedings of ACL.

Hanna Wallach, David M. Mimno, and Andrew McCal-
lum. 2009. Rethinking LDA: Why priors matter. In
Proceedings of NIPS.

Dani Yogatama, Michael Heilman, Brendan O’Connor,
Chris Dyer, Bryan R Routledge, and Noah A Smith.
2011. Predicting a scientiﬁc community’s response
to an article. In Proceedings of EMNLP.

Dat Quoc Nguyen, Richard Billingsley, Lan Du, and
Mark Johnson. 2015a. Improving topic models with
latent feature word representations. In Proceedings
of ACL.

Jun Zhu, Amr Ahmed, and Eric P. Xing. 2009.
MedLDA: Maximum margin supervised topic mod-
els for regression and classiﬁcation. In Proceedings
of ICML.

Supplementary Material

A Deriving the ELBO

The derivation of the ELBO for our model is given
below, dropping explicit reference to Φ and α for
simplicity. For document i,

log p(wi, yi | ci) = log

p(wi, yi, ri | ci)dri

(cid:90)

ri

(cid:90)

ri

(cid:18)

= log

p(wi, yi, ri |, ci)

q(ri | wi, ci, yi)
q(ri | wi, ci, yi)

= log

Eq(ri|wi,ci,yi)

(cid:20) p(wi, yi, ri | ci)
q(ri | wi, ci, yi)

(cid:21)(cid:19)

≥ Eq(ri|wi,ci,yi) [log p(wi, yi, ri | ci)]
−Eq(ri|wi,ci,yi) [log q(ri | wi, ci, yi)]
= Eq(ri|wi,ci,yi) [log p(wi, yi | ri, ci)]
+Eq(ri|wi,ci,yi) [log p(ri)]
−Eq(ri|wi,ci,yi) [log q(ri | wi, ci, yi)]

Ni(cid:88)


= Eq(ri|wi,ci,yi)

log p(wij | ri, ci)





j=1

+Eq(ri|wi,ci,yi) [log p(yi | ri, ci)]

−DKL [q(ri | wi, ci, yi) || p(ri)]

(11)

dri

(12)

(13)

(14)

(15)

(16)

DKL[qΦ(ri | wi, ci, yi)(cid:107)p(ri)] =

+ (µi − µ0)(cid:62)Σ−1

0 (µi − µ0) − K + log

(cid:18)

1
2

tr(Σ−1

0 Σi)
(cid:19)

|Σ0|
|Σi|

(17)

where Σi = diag(σ2
diag(σ2

0(α)).

i (wi, ci, yi)), and Σ0 =

C Practicalities and Implementation

As observed in past work, inference using a VAE
can suffer from component collapse, which trans-
lates into excessive redundancy in topics (i.e., many
topics containing the same set of words). To mit-
igate this problem, we borrow the approach used
by Srivastava and Sutton (2017), and make use of

the Adam optimizer with a high momentum, com-
bined with batchnorm layers to avoid divergence.
Speciﬁcally, we add batchnorm layers following
the computation of µ, log σ2, and η.

This effectively solves the problem of mode col-
lapse, but the batchnorm layer on η introduces an
additional problem, not previously reported. At test
time, the batchnorm layer will shift the input based
on the learned population mean of the training data;
this effectively encodes information about the dis-
tribution of words in this model that is not captured
by the topic weights and background distribution.
As such, although reconstruction error will be low,
the document representation θ, will not necessarily
be a useful representation of the topical content
of each document. In order to alleviate this prob-
lem, we reconstruct η as a convex combination
of two copies of the output of the generator net-
work, one passed through a batchnorm layer, and
one not. During training, we then gradually anneal
the model from relying entirely on the component
passed through the batchnorm layer, to relying en-
tirely on the one that is not. This ensures that the
the ﬁnal weights and document representations will
be properly interpretable.

Note that although ProdLDA (Srivastava and Sut-
ton, 2017) does not explicitly include a background
term, it is possible that the batchnorm layer applied
to η has a similar effect, albeit one that is not as
easily interpretable. This annealing process avoids
that ambiguity.

All datasets were preprocessed by tokenizing, con-
verting to lower case, removing punctuation, and
dropping all tokens that included numbers, all to-
kens less than 3 characters, and all words on the
stopword list from the snowball sampler.17 The
vocabulary was then formed by keeping the words
that occurred in the most documents (including
train and test), up to the desired size (2000 for 20
newsgroups, 5000 for the others). Note that these
small preprocessing decisions can make a large
difference to perplexity. We therefore include our
preprocessing scripts as part of our implementation
so as to facilitate easy future comparison.

For the UIUC Yahoo answers dataset, we down-
loaded the documents from the project webpage.18

17snowball.tartarus.org/algorithms/

english/stop.txt

18cogcomp.org/page/resource_view/89

B Model details

The KL divergence term in the variational bound
can be computed as

D Data

However, the ﬁle that is available does not com-
pletely match the description on the website. We
dropped Cars and Transportation and Social Sci-
ence which had less than the expected number of
documents, and merged Arts and Arts and Humani-
ties, which appeared to be the same category, pro-
ducing 15 categories, each with 10,000 documents.

E Experimental Details

For all experiments we use a model with 300-
dimensional embeddings of words, and we take
fe to be only the element-wise softplus nonlinear-
ity (followed by the linear transformations for µ
and log σ2). Similarly, fy is a linear transformation
of θ, followed by a softplus layer, followed by a
linear transformation to the size of the output (the
number of classes). During training, we set S (the
number of samples of (cid:15)) to be 1; for estimating the
ELBO at on test documents, we set S = 20.

For the unsupervised results, we use the same
set up as (Srivastava and Sutton, 2017): Adam op-
timizer with β1 = 0.99, learning rate = 0.002,
batch size of 200, and training for 200 epochs. The
setup was the same for all datasets, except we only
trained for 150 epochs on Yahoo answers because
it is much larger. For LDA, we updated the hyper-
parameters every 10 epochs.

For the external evaluation of NPMI, we use the
co-occurrence statistics from all New York Times
articles in the English Gigaword published from
the start of 2000 to the end of 2009, processed in
the same way as our data.

For the text classiﬁcation experiments, we use
the scikit-learn implementation of logistic
regression. We give it access to the same input
data as our model (using the same vocabulary), and
tune the strength of l2 regularization using cross-
validation. For our model, we only tune the number
of epochs, evaluating on development data. Our
models for this task did not use regularization or
word vectors.

F Additional Experimental Results

In this section we include additional experimental
results in the unsupervised setting.

Table 4 shows results on the 20 newsgroups
dataset, using 20 topics with a 2,000-word vocab-
ulary. Note that these results are not necessarily
directly comparable to previously published results,
as preprocessing decisions can have a large impact
on perplexity. These results show a similar pat-
tern to those on the IMDB data provided in the

Model
LDA
SAGE
NVDM
SCHOLAR - B.G.
SCHOLAR
SCHOLAR + W.V.
SCHOLAR + REG.

Ppl.
↓
810
867
1067
928
921
955
1053

NPMI
(int.) ↑
0.20
0.27
0.18
0.17
0.35
0.29
0.25

NPMI
(ext.) ↑
0.11
0.15
0.11
0.09
0.16
0.17
0.13

Sparsity
↑
0
0.71
0
0
0
0
0.43

Table 4: Performance of various models on the
20 newsgroups dataset with 20 topics and a 2,000-
word vocabulary.

Model
LDA
NVDM
SCHOLAR - B.G.
SCHOLAR
SCHOLAR + W.V.
SCHOLAR + REG.

Ppl.
↓
1035
4588
1589
1596
1780
1840

NPMI
(int.) ↑
0.29
0.20
0.27
0.33
0.37
0.34

NPMI
(ext.) ↑
0.15
0.09
0.16
0.13
0.15
0.13

Sparsity
↑
0
0
0
0
0
0.44

Table 5: Performance of various models on the
Yahoo answers dataset with 250 topics and a 5,000-
word vocabulary. SAGE did not ﬁnish in 72 hours
so we omit it from this table.

main paper, except that word vectors do not im-
prove internal coherence on this dataset, perhaps
because of the presence of relatively more names
and specialized terminology. Also, although the
NVDM still has worse perplexity than LDA, the
effects are not as dramatic as reported in (Srivas-
tava and Sutton, 2017). Regularization is also more
beneﬁcial for this data, with both SAGE and our
regularized model obtaining better coherence than
LDA. The topics from SCHOLAR for this dataset
are also shown in Table 6.

Table 5 shows the equivalent results for the Ya-
hoo answers dataset, using 250 topics, and a 5,000-
word vocabulary. These results closely match
those for the IMDB dataset, with our model having
higher perplexity but also higher internal coherence
than LDA. As with IMDB, the use of word vectors
improves coherence, both internally and externally,
but again at the cost of worse perplexity. Surpris-
ingly, our model without a background term actu-
ally has the best external coherence on this dataset,
but as described in the main paper, these tend to
give high weight primarily to common words, and
are more repetitive as a result.

NPMI Topic

turks armenian armenia turkish roads escape soviet muslim mountain soul
escrow clipper encryption wiretap crypto keys secure chip nsa key
jesus christ sin bible heaven christians church faith god doctrine
fbi waco batf clinton children koresh compound atf went ﬁre
players teams player team season baseball game fans roger league
guns gun weapons criminals criminal shooting police armed crime defend
playoff rangers detroit cup wings playoffs montreal toronto minnesota games
ftp images directory library available format archive graphics package macintosh
user server faq archive users ftp unix applications mailing directory
bike car cars riding ride engine rear bmw driving miles
study percent sexual medicine gay studies april percentage treatment published
israeli israel arab peace rights policy islamic civil adam citizens

0.77
0.52
0.49
0.43
0.41
0.39
0.37
0.36
0.33
0.32
0.32
0.32
0.30 morality atheist moral belief existence christianity truth exist god objective
space henry spencer international earth nasa orbit shuttle development vehicle
0.28
0.27
bus motherboard mhz ram controller port drive card apple mac
0.25 windows screen ﬁles button size program error mouse colors microsoft
0.24
0.21
0.19
0.04

sale shipping offer brand condition sell printer monitor items asking
driver drivers card video max advance vga thanks windows appreciated
cleveland advance thanks reserve ohio looking nntp western host usa
banks gordon univ keith soon pittsburgh michael computer article ryan

Table 6: Topics from the unsupervised SCHOLAR on the 20 newsgroups dataset, and the corresponding
internal coherence values.

