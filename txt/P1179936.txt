Using Distributed Representations to Disambiguate Biomedical and
Clinical Concepts

St´ephan Tulkens(cid:70) and Simon ˇSuster(cid:70)♠ and Walter Daelemans(cid:70)

(cid:70)CLiPS
University of Antwerp
Prinsstraat 13
2000 Antwerpen
Belgium
{ﬁrstname.lastname}@uantwerpen.be

♠CLCG
University of Groningen
Oude Kijk in ’t Jatstr. 26
9700AS Groningen
The Netherlands

Abstract

In this paper, we report a knowledge-based
method for Word Sense Disambiguation
in the domains of biomedical and clini-
cal text. We combine word representa-
tions created on large corpora with a small
number of deﬁnitions from the UMLS to
create concept representations, which we
then compare to representations of the con-
text of ambiguous terms. Using no re-
lational information, we obtain compara-
ble performance to previous approaches
on the MSH-WSD dataset, which is a
well-known dataset in the biomedical do-
main. Additionally, our method is fast
and easy to set up and extend to other do-
mains. Supplementary materials, includ-
ing source code, can be found at https:
//github.com/clips/yarn

1

Introduction

Word Sense Disambiguation (WSD) is a procedure
in which an ambiguous term or concept is assigned
a single sense appropriate for that context, and is
an important step in the creation of a semantic rep-
resentation of a document (Ide and V´eronis, 1998).
While performing WSD will beneﬁt most natural
language processing applications, disambiguation
of concepts is a critical component of applications
operating on clinical and biomedical text, in which
the same word can denote differing concepts, and
may thus elicit radically different responses.

Compounding this problem of ambiguity is the
fact that clinical text, in general, is noisier than
other domains, and contains a large variety of ab-
breviations, some of which may be speciﬁc to a
single hospital or physician. Additionally, there is a
marked absence of large volumes of annotated clini-
cal text, even for English, which presents a problem

for supervised approaches to Word Sense Disam-
biguation. For other languages, such as Dutch,
there exist no freely available annotated corpora of
clinical text.

A ﬁrst step towards solving this problem could
be the use of distributed representations. Where a
more traditional word representation, such as a TF-
IDF bag-of-words (BoW) representation, carries
frequency information, distributed representations
encode semantic information. A big advantage to
using these representations is that they can be gen-
erated from large corpora of unlabeled text, and
can be trained on very large corpora in a reason-
able amount of time. These representations, es-
pecially when trained using neural architectures
such as word2vec (Mikolov et al., 2013), have
been shown to improve performance on a variety
of tasks when compared to more traditional BoW
representations.

We hypothesize that these kinds of distributional
representations are well-suited for WSD in the clin-
ical and biomedical domain because of the lack
of training data, and the large terminological va-
riety. We present a knowledge-based approach to
Word Sense Disambiguation which creates concept
representations by combining deﬁnitions from the
Uniﬁed Medical Language System (UMLS) with
distributed representations. We test our hypothesis
on the MSH-WSD, which is a well-known dataset
for WSD in the biomedical domain.

2 Related Research

All knowledge-based methods we review use
the Uniﬁed Medical Language System® (UMLS)
Metathesaurus® (Bodenreider, 2004) as a knowl-
edge base, possibly augmented with external
sources, such as MeSH®-indexed abstracts. Gen-
erally speaking, the UMLS contains two separate
information sources that are suitable for use in dis-

6
1
0
2
 
g
u
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
0
6
5
0
.
8
0
6
1
:
v
i
X
r
a

ambiguation: the concept unique identiﬁer (CUI),
which is a unique label for each concept, and the
semantic type (ST), which is a set of 135 broad
labels such as “Animal” or “Chemical”. In gen-
eral, a word is only considered disambiguated if
the correct CUI can be selected; hence, as McInnes
and Pedersen (2013) note, approaches based on se-
mantic types are not able to disambiguate between
approximately 12% of concepts, as some concepts
with the same surface form have an identical ST,
but a different CUI.

In terms of approaches using ST, Humphrey et
al. (2006) create one vector for each semantic type
by creating a BoW representation of all words that
denote that semantic type. For each ambiguous
term, a target word vector is created by taking a
window of words from the right and left of the term.
The concept which is associated with the ST with
the lowest cosine distance is then taken to be the
correct sense of the term. Similarly, Alexopoulou
et al. (2009) create a method which ﬁnds the closest
concept based on a combination of co-occurrence
with other semantic types and ontological similarity
through is-a relationships.

Closest to our approach is the machine read-
able dictionary (MRD) approach (McInnes, 2008;
Jimeno-Yepes et al., 2011), which uses deﬁnitions
from the UMLS to create concept vectors by creat-
ing BoW representations of concepts using all deﬁ-
nitions of the concept and those of related concepts.
This BoW representation contains TF-IDF values
where D is the number of concepts in which a word
appears, thereby reducing the inﬂuence of general
words which occur in many concepts. These rep-
resentations are then compared to the vectorized
contexts of the ambiguous terms using cosine dis-
tance. A reﬁnement of MRD, called second-order
co-occurrence MRD (2-MRD) (McInnes, 2008), re-
places each word in a deﬁnition by a vector which
contains TF-IDF values of co-occurrence counts,
thereby associating each word with a context.

McInnes

introduce
and Pedersen (2013)
UMLS::SenseRelate, an approach which is based
on Pedersen et al. (2004)’s WordNet::SenseRelate.
In this system, each possible sense for an ambigu-
ous term is assigned a distance-weighted score
based on the concepts of the terms surrounding it,
where the concepts of the surrounding terms are
determined using UMLS::Similarity (McInnes et
al., 2009).

Corpus size
Vocabulary
Dimension

Medline Mimic-III Bioasq
920,081 13,097,844 -
196,960 71,663
320

1,701,632
200

320

Table 1: The number of words in the corpus, the
resulting vocabulary size, and the dimension of the
resulting vectors.

called step models, which calculate the probabil-
ity of a word occurring with a certain concept by
considering the number of times a word occurs
in the deﬁnitions of that concept and its related
concepts. It then steps through the UMLS-deﬁned
ontology of concepts, and reﬁnes the probabilities
for each word and each concept based on the rela-
tions within the ontology.

Finally, Chen et al. (2014) present an approach
for general WSD which uses word embeddings
coupled with WordNet (Fellbaum, 1998) as a re-
source to perform sense disambiguation, and which
creates sense-speciﬁc word embeddings from these
sense-disambiguated word representations.

3 Materials

3.1 Test Corpus

We use the MSH-WSD corpus (Jimeno-Yepes et
al., 2011), which consists of a set of 203 ambigu-
ous terms, each associated with multiple concepts,
to evaluate our approach. Of the 203 terms in the
corpus, 106 are regular terms, 88 are acronyms, and
9 can be acronyms and regular terms. For each of
these concepts, up to 100 MeSH abstracts were re-
trieved, resulting in a set of 37,888 abstracts. In our
approach, all abstracts were pre-processed using
the tokenizer from the Pattern package (De Smedt
and Daelemans, 2012), and all stop words were
removed using the English stop word list from
scikit-learn (Pedregosa et al., 2011).

3.2 Word vectors

We evaluate our approach using three sets of vec-
tors: The ﬁrst set was trained on a small set of
Medline abstracts1, and a second set of vectors
created on the entirety of the MIMIC-III corpus
of clinical notes (Johnson et al., 2016). For both
sets, we used the word2vec implementation from
gensim ( ˇReh˚uˇrek and Sojka, 2010), using skip-
gram with negative sampling, a frequency cutoff

1The speciﬁc IDs of these abstracts are available in the

Jimeno-Yepes and Berlanga (2015) present so-

online appendix.

of 5 and a negative sampling of 15. Additionally,
we used a third set of vectors, available from the
BioASQ organisers2, which was trained on a much
larger set of Medline abstracts.3 The model statis-
tics are visualized in Table 1.

4 Approach

Similar to the 2-MRD approach detailed above, our
approach creates concept vectors by replacing each
word in every deﬁnition by the vector representa-
tion of that word. This creates an M × n matrix for
each deﬁnition, where M is the dimensionality of
the word vectors, and n the number of words con-
tained in that deﬁnition. Following this, for each
deﬁnition, we then obtain a single vector of dimen-
sionality M by applying a compositional function
to the matrix, thereby obtaining so-called deﬁnition
vectors, which represent the entire meaning of the
deﬁnition in one vector. Each concept can then be
represented by a M × d matrix, where d is the num-
ber of deﬁnitions that a concept has in the UMLS.
Finally, we apply a second composition function
to this matrix, thereby obtaining a single vector of
dimensionality M which represents the combined
meaning of all deﬁnitions for that concept, i.e. a
concept vector.

For each abstract in the test corpus, we ﬁrst lo-
cate each ambiguous term through a simple lookup.
For each located term in the abstract we create a
vector representation by retrieving all words in a
window of size w surrounding the ambiguous term,
and replacing the words by their vectors. Note
that this window does not include the ambiguous
term itself. These collections of vectors are then
combined into M -dimensional vectors using the
same composition function as above. This is done
separately for each term occurrence within a single
document, creating a M × x matrix, where x is
the number of times the ambiguous term occurs
in a single document. These are then combined
in an M -dimensional term vector using the same
composition we used for the concepts, above. A
schematic representation of our model is given in
Figure 1.

Because all concept and term vectors are created
using the same distributed vectors and composi-
tional functions, the vector space in which they are

2Available on the BioASQ website.
3While we concede that the BioASQ corpora might contain
abstracts from the MSH dataset, it does not contain any explicit
labeled information that might be used in disambiguation.

Figure 1: Our model represents a concept by re-
placing all words W in a deﬁnition D by their
vectors, and then composing these into a deﬁnition
vector with a function f (x). For each concept, all
deﬁnition vectors D are then composed into a con-
cept vector C using a second composition function
g(x).

placed is also comparable. Hence, for each am-
biguous word we encounter, we can use the cosine
distance between the abstract vector of the am-
biguous utterance and each possible sense of that
word to determine the correct sense. This makes
our approach very similar to the Lesk family of
approaches (Lesk, 1986).

In terms of composition function we experi-
mented with elementwise multiplication, averaging
and summation, all of which are unordered com-
positional functions (Mitchell and Lapata, 2008).
In addition, it is worth noting that there’s still a
lively debate whether ordered composition actually
leads to better results for estimating document-,
or sentence-level meaning, when compared to un-
ordered composition (Iyyer et al., 2015; Socher et
al., 2013).

5 Results

The accuracy scores obtained by our models using
the different word vectors are displayed in Table
2. med, mim and bio denote the vectors created
on the small Medline corpus, the Mimic-III corpus
and the BioASQ vectors, respectively. We consider
both a constrained and an unconstrained version
of the task. For each word, the constrained ver-
sion of the task only considers the senses present

med mim bio MRD 2-MRD 0-step

Accuracy C 0.80 0.69 0.84 0.81 0.78
Accuracy U 0.72 0.63 0.75 -

-

0.82
-

2-step
0.86
-

r-step UMLS::SenseRelate
0.89
-

0.75
-

Table 2: Results using constrained (C) and unconstrained (U) terms.

Term Accuracy

DE 0.31
Hemlock 0.4
0.46
0.46
0.47

Brucella Abortus
WT1
Murine Sarcoma Virus

Table 3: The 5 lowest-performing terms.

in the MSH-WSD dataset as possible targets. The
unconstrained version considers all concepts which
are denoted by the ambiguous term in the 2015AB
version of the UMLS as possible targets. The term
cortex, for example, only has 2 concepts asso-
ciated with it in the MSH-WSD dataset, while in
the 2015AB UMLS release it can denote 5 separate
concepts. Because the unconstrained version of the
task considers all words, it therefore gives a better
indication of real-life performance.

Accuracy C and U denote that the scores were ob-
tained in the constrained settings and unconstrained
setting, respectively. All reported scores use a win-
dow size of 6, which was optimized on a randomly
selected set of 20 terms from the MSH-WSD set.
Varying the window size had negligible results: all
window sizes over 6 had comparable results, and
increasing the window size over 30 causes a (small)
decline in results. This is in line with McInnes and
Pedersen (2013), who report a positive effect of
window size that quickly tapers off for window
sizes > 10. Concerning the composition functions,
summation and averaging as ﬁrst and second or-
der composition function worked best, while using
element-wise multiplication did not work well in
any case. Where possible, we display the self-
reported scores from the relevant papers on the
same dataset.

A ﬁrst thing to note is the large difference in
accuracy when changing the set of word repre-
sentations, especially the difference between the
Medline vectors and the vectors derived from the
Mimic-III corpus.
It is currently unclear what
causes these performance differences, although it
is likely that the small vocabulary, caused by the
noisiness of the clinical data in the MIMIC-III cor-

pus, reduces performance. Compared to previous
approaches, our approach outperforms the MRD,
2-MRD, and UMLS::SenseRelate approaches, but
does not manage to improve on the scores of the
step models. Recall, however, that the step models
largely rely on relationships in the UMLS ontology
to estimate concept relatedness.

To compare how our models improved when in-
cluding relation information, we also experimented
with adding deﬁnitions of related concepts, i.e. con-
cepts which had a sibling, parent or child rela-
tionship to each concept. In contrast to patterns
observed in earlier work, this did not have a sig-
niﬁcant, and often a detrimental, effect on perfor-
mance. Note that this makes our model entirely
independent of the actual UMLS hierarchy, and
more ﬂexible as a result, as we only use the map-
pings from deﬁnition to CUI for disambiguation,
and no other information, such as relations or se-
mantic type. In addition, our system is also fast:
on a consumer-grade laptop, our approach takes 10
seconds to vectorize and disambiguate all abstracts
in the MSH dataset, not taking into account the
time it takes to load the embeddings into memory.
Our approach obtains an accuracy of > 90% on
103 terms, showing that it is able to disambiguate
a large variety of terms. For some terms, how-
ever, the performance was below random guessing.
These are shown in Table 3. The pattern of er-
rors is quite clear: Our approach has trouble with
disambiguation if the deﬁnitions of the concepts
themselves are lexically very similar. As an exam-
ple, on the term Hemlock our approach performs
below chance level because one of the concepts
denotes a family of poisonous plants, while the
other reports a tree, also called hemlock, the de-
scription of which mentions that it is explicitly not
poisonous. We expect these kinds of problems to
be alleviated with the addition of more data.

6 Conclusion and future work

In this paper we presented a novel approach to
WSD in the biomedical domain which achieves
comparable performance to existing methods with-
out incorporating relational information from an

ontology. This makes the approach easily transfer-
able to other languages, for which such ontologies
might not exist, and to other domains. The large
variation in accuracy when changing sets of word
embeddings also raises interesting prospects for
improvement; better word representations will lead
to an improvement in our approach without modify-
ing the approach itself. Additionally, we would like
to experiment with different composition functions
for composing the deﬁnition and concept vectors.

Acknowledgments

Part of this research was carried out in the frame-
work of the Accumulate IWT SBO project, funded
by the government agency for Innovation by Sci-
ence and Technology (IWT). We would also like to
thank Elyne Scheurwegs for making the small set
of Medline abstract available to us.

References

[Alexopoulou et al.2009] Dimitra Alexopoulou, Bill
Andreopoulos, Heiko Dietze, Andreas Doms, Fa-
J¨org Hakenberg, Khaled Khelif,
bien Gandon,
Michael Schroeder, and Thomas W¨achter.
2009.
Biomedical word sense disambiguation with on-
tologies and metadata: automation meets accuracy.
BMC bioinformatics, 10(1):1.

[Bodenreider2004] Olivier Bodenreider.

2004. The
uniﬁed medical language system (UMLS): integrat-
ing biomedical terminology. Nucleic acids research,
32(suppl 1):D267–D270.

[Chen et al.2014] Xinxiong Chen, Zhiyuan Liu, and
A uniﬁed model for
In

Maosong Sun.
word sense representation and disambiguation.
EMNLP, pages 1025–1035. Citeseer.

2014.

[De Smedt and Daelemans2012] Tom De Smedt and
Walter Daelemans. 2012. Pattern for Python. The
Journal of Machine Learning Research, 13(1):2063–
2067.

[Fellbaum1998] Christiane Fellbaum. 1998. WordNet.

Wiley Online Library.

[Humphrey et al.2006] Susanne M Humphrey, Willie J
Rogers, Halil Kilicoglu, Dina Demner-Fushman,
and Thomas C Rindﬂesch. 2006. Word sense disam-
biguation by selecting the best semantic type based
on Journal Descriptor Indexing: Preliminary experi-
ment. Journal of the American Society for Informa-
tion Science and Technology, 57(1):96–113.

[Iyyer et al.2015] Mohit Iyyer, Varun Manjunatha, Jor-
dan Boyd-Graber, and Hal Daum´e III. 2015. Deep
unordered composition rivals syntactic methods for
text classiﬁcation. In Association for Computational
Linguistics.

[Jimeno-Yepes and Berlanga2015] Antonio

Jimeno-
Yepes and Rafael Berlanga.
2015. Knowledge
based word-concept model estimation and re-
Journal of
ﬁnement for biomedical text mining.
biomedical informatics, 53:300–307.

[Jimeno-Yepes et al.2011] Antonio

Jimeno-Yepes,
Bridget T McInnes, and Alan R Aronson. 2011.
Exploiting MeSH indexing in MEDLINE to gener-
ate a data set for word sense disambiguation. BMC
bioinformatics, 12(1):1.

J

[Johnson et al.2016] AEW Johnson, TJ Pollard, L Shen,
L Lehman, M Feng, M Ghassemi, B Moody,
P Szolovits, LA Celi, and RG Mark. 2016. MIMIC-
III, a freely accessible critical care database. Scien-
tiﬁc Data.

[Lesk1986] Michael Lesk.

1986. Automatic sense
disambiguation using machine readable dictionaries:
how to tell a pine cone from an ice cream cone. In
Proceedings of the 5th annual international confer-
ence on Systems documentation, pages 24–26. Asso-
ciation for Computing Machinery.

[McInnes and Pedersen2013] Bridget T McInnes and
Ted Pedersen. 2013. Evaluating measures of seman-
tic similarity and relatedness to disambiguate terms
in biomedical text. Journal of biomedical informat-
ics, 46(6):1116–1124.

[McInnes et al.2009] Bridget T McInnes, Ted Pedersen,
and Serguei VS Pakhomov. 2009. UMLS-Interface
and UMLS-Similarity: open source software for
measuring paths and semantic similarity. In AMIA
Annual Symposium Proceedings, volume 2009, page
431. American Medical Informatics Association.

[McInnes2008] Bridget T McInnes. 2008. An unsu-
pervised vector approach to biomedical term disam-
biguation: integrating UMLS and Medline. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technologies: Student Research Workshop,
pages 49–54. Association for Computational Lin-
guistics.

[Mikolov et al.2013] Tomas Mikolov,

Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in neural
and their compositionality.
information processing systems, pages 3111–3119.

[Ide and V´eronis1998] Nancy Ide and Jean V´eronis.
1998. Introduction to the special issue on word sense
disambiguation: the state of the art. Computational
linguistics, 24(1):2–40.

[Mitchell and Lapata2008] Jeff Mitchell and Mirella
Lapata. 2008. Vector-based models of semantic
composition. In Proceedings of the Association for
Computational Linguistics, pages 236–244.

[Pedersen et al.2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. WordNet:: Sim-
ilarity: measuring the relatedness of concepts.
In
Demonstration papers at HLT-NAACL 2004, pages
38–41. Association for Computational Linguistics.

[Pedregosa et al.2011] Fabian Pedregosa, Ga¨el Varo-
quaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel,
Peter Prettenhofer, Ron Weiss, Vincent Dubourg,
Scikit-learn: Machine learning
et al.
The Journal of Machine Learning
in Python.
Research, 12:2825–2830.

2011.

2010.

[ ˇReh˚uˇrek and Sojka2010] Radim ˇReh˚uˇrek and Petr So-
Software Framework for Topic
jka.
In Proceed-
Modelling with Large Corpora.
ings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Val-
letta, Malta, May. ELRA. http://is.muni.
cz/publication/884893/en.

[Socher et al.2013] Richard Socher, Alex Perelygin,
Jean Y Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Recur-
sive deep models for semantic compositionality over
a sentiment treebank. In Proceedings of the confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), volume 1631, page 1642.

Using Distributed Representations to Disambiguate Biomedical and
Clinical Concepts

St´ephan Tulkens(cid:70) and Simon ˇSuster(cid:70)♠ and Walter Daelemans(cid:70)

(cid:70)CLiPS
University of Antwerp
Prinsstraat 13
2000 Antwerpen
Belgium
{ﬁrstname.lastname}@uantwerpen.be

♠CLCG
University of Groningen
Oude Kijk in ’t Jatstr. 26
9700AS Groningen
The Netherlands

Abstract

In this paper, we report a knowledge-based
method for Word Sense Disambiguation
in the domains of biomedical and clini-
cal text. We combine word representa-
tions created on large corpora with a small
number of deﬁnitions from the UMLS to
create concept representations, which we
then compare to representations of the con-
text of ambiguous terms. Using no re-
lational information, we obtain compara-
ble performance to previous approaches
on the MSH-WSD dataset, which is a
well-known dataset in the biomedical do-
main. Additionally, our method is fast
and easy to set up and extend to other do-
mains. Supplementary materials, includ-
ing source code, can be found at https:
//github.com/clips/yarn

1

Introduction

Word Sense Disambiguation (WSD) is a procedure
in which an ambiguous term or concept is assigned
a single sense appropriate for that context, and is
an important step in the creation of a semantic rep-
resentation of a document (Ide and V´eronis, 1998).
While performing WSD will beneﬁt most natural
language processing applications, disambiguation
of concepts is a critical component of applications
operating on clinical and biomedical text, in which
the same word can denote differing concepts, and
may thus elicit radically different responses.

Compounding this problem of ambiguity is the
fact that clinical text, in general, is noisier than
other domains, and contains a large variety of ab-
breviations, some of which may be speciﬁc to a
single hospital or physician. Additionally, there is a
marked absence of large volumes of annotated clini-
cal text, even for English, which presents a problem

for supervised approaches to Word Sense Disam-
biguation. For other languages, such as Dutch,
there exist no freely available annotated corpora of
clinical text.

A ﬁrst step towards solving this problem could
be the use of distributed representations. Where a
more traditional word representation, such as a TF-
IDF bag-of-words (BoW) representation, carries
frequency information, distributed representations
encode semantic information. A big advantage to
using these representations is that they can be gen-
erated from large corpora of unlabeled text, and
can be trained on very large corpora in a reason-
able amount of time. These representations, es-
pecially when trained using neural architectures
such as word2vec (Mikolov et al., 2013), have
been shown to improve performance on a variety
of tasks when compared to more traditional BoW
representations.

We hypothesize that these kinds of distributional
representations are well-suited for WSD in the clin-
ical and biomedical domain because of the lack
of training data, and the large terminological va-
riety. We present a knowledge-based approach to
Word Sense Disambiguation which creates concept
representations by combining deﬁnitions from the
Uniﬁed Medical Language System (UMLS) with
distributed representations. We test our hypothesis
on the MSH-WSD, which is a well-known dataset
for WSD in the biomedical domain.

2 Related Research

All knowledge-based methods we review use
the Uniﬁed Medical Language System® (UMLS)
Metathesaurus® (Bodenreider, 2004) as a knowl-
edge base, possibly augmented with external
sources, such as MeSH®-indexed abstracts. Gen-
erally speaking, the UMLS contains two separate
information sources that are suitable for use in dis-

6
1
0
2
 
g
u
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
0
6
5
0
.
8
0
6
1
:
v
i
X
r
a

ambiguation: the concept unique identiﬁer (CUI),
which is a unique label for each concept, and the
semantic type (ST), which is a set of 135 broad
labels such as “Animal” or “Chemical”. In gen-
eral, a word is only considered disambiguated if
the correct CUI can be selected; hence, as McInnes
and Pedersen (2013) note, approaches based on se-
mantic types are not able to disambiguate between
approximately 12% of concepts, as some concepts
with the same surface form have an identical ST,
but a different CUI.

In terms of approaches using ST, Humphrey et
al. (2006) create one vector for each semantic type
by creating a BoW representation of all words that
denote that semantic type. For each ambiguous
term, a target word vector is created by taking a
window of words from the right and left of the term.
The concept which is associated with the ST with
the lowest cosine distance is then taken to be the
correct sense of the term. Similarly, Alexopoulou
et al. (2009) create a method which ﬁnds the closest
concept based on a combination of co-occurrence
with other semantic types and ontological similarity
through is-a relationships.

Closest to our approach is the machine read-
able dictionary (MRD) approach (McInnes, 2008;
Jimeno-Yepes et al., 2011), which uses deﬁnitions
from the UMLS to create concept vectors by creat-
ing BoW representations of concepts using all deﬁ-
nitions of the concept and those of related concepts.
This BoW representation contains TF-IDF values
where D is the number of concepts in which a word
appears, thereby reducing the inﬂuence of general
words which occur in many concepts. These rep-
resentations are then compared to the vectorized
contexts of the ambiguous terms using cosine dis-
tance. A reﬁnement of MRD, called second-order
co-occurrence MRD (2-MRD) (McInnes, 2008), re-
places each word in a deﬁnition by a vector which
contains TF-IDF values of co-occurrence counts,
thereby associating each word with a context.

McInnes

introduce
and Pedersen (2013)
UMLS::SenseRelate, an approach which is based
on Pedersen et al. (2004)’s WordNet::SenseRelate.
In this system, each possible sense for an ambigu-
ous term is assigned a distance-weighted score
based on the concepts of the terms surrounding it,
where the concepts of the surrounding terms are
determined using UMLS::Similarity (McInnes et
al., 2009).

Corpus size
Vocabulary
Dimension

Medline Mimic-III Bioasq
920,081 13,097,844 -
196,960 71,663
320

1,701,632
200

320

Table 1: The number of words in the corpus, the
resulting vocabulary size, and the dimension of the
resulting vectors.

called step models, which calculate the probabil-
ity of a word occurring with a certain concept by
considering the number of times a word occurs
in the deﬁnitions of that concept and its related
concepts. It then steps through the UMLS-deﬁned
ontology of concepts, and reﬁnes the probabilities
for each word and each concept based on the rela-
tions within the ontology.

Finally, Chen et al. (2014) present an approach
for general WSD which uses word embeddings
coupled with WordNet (Fellbaum, 1998) as a re-
source to perform sense disambiguation, and which
creates sense-speciﬁc word embeddings from these
sense-disambiguated word representations.

3 Materials

3.1 Test Corpus

We use the MSH-WSD corpus (Jimeno-Yepes et
al., 2011), which consists of a set of 203 ambigu-
ous terms, each associated with multiple concepts,
to evaluate our approach. Of the 203 terms in the
corpus, 106 are regular terms, 88 are acronyms, and
9 can be acronyms and regular terms. For each of
these concepts, up to 100 MeSH abstracts were re-
trieved, resulting in a set of 37,888 abstracts. In our
approach, all abstracts were pre-processed using
the tokenizer from the Pattern package (De Smedt
and Daelemans, 2012), and all stop words were
removed using the English stop word list from
scikit-learn (Pedregosa et al., 2011).

3.2 Word vectors

We evaluate our approach using three sets of vec-
tors: The ﬁrst set was trained on a small set of
Medline abstracts1, and a second set of vectors
created on the entirety of the MIMIC-III corpus
of clinical notes (Johnson et al., 2016). For both
sets, we used the word2vec implementation from
gensim ( ˇReh˚uˇrek and Sojka, 2010), using skip-
gram with negative sampling, a frequency cutoff

1The speciﬁc IDs of these abstracts are available in the

Jimeno-Yepes and Berlanga (2015) present so-

online appendix.

of 5 and a negative sampling of 15. Additionally,
we used a third set of vectors, available from the
BioASQ organisers2, which was trained on a much
larger set of Medline abstracts.3 The model statis-
tics are visualized in Table 1.

4 Approach

Similar to the 2-MRD approach detailed above, our
approach creates concept vectors by replacing each
word in every deﬁnition by the vector representa-
tion of that word. This creates an M × n matrix for
each deﬁnition, where M is the dimensionality of
the word vectors, and n the number of words con-
tained in that deﬁnition. Following this, for each
deﬁnition, we then obtain a single vector of dimen-
sionality M by applying a compositional function
to the matrix, thereby obtaining so-called deﬁnition
vectors, which represent the entire meaning of the
deﬁnition in one vector. Each concept can then be
represented by a M × d matrix, where d is the num-
ber of deﬁnitions that a concept has in the UMLS.
Finally, we apply a second composition function
to this matrix, thereby obtaining a single vector of
dimensionality M which represents the combined
meaning of all deﬁnitions for that concept, i.e. a
concept vector.

For each abstract in the test corpus, we ﬁrst lo-
cate each ambiguous term through a simple lookup.
For each located term in the abstract we create a
vector representation by retrieving all words in a
window of size w surrounding the ambiguous term,
and replacing the words by their vectors. Note
that this window does not include the ambiguous
term itself. These collections of vectors are then
combined into M -dimensional vectors using the
same composition function as above. This is done
separately for each term occurrence within a single
document, creating a M × x matrix, where x is
the number of times the ambiguous term occurs
in a single document. These are then combined
in an M -dimensional term vector using the same
composition we used for the concepts, above. A
schematic representation of our model is given in
Figure 1.

Because all concept and term vectors are created
using the same distributed vectors and composi-
tional functions, the vector space in which they are

2Available on the BioASQ website.
3While we concede that the BioASQ corpora might contain
abstracts from the MSH dataset, it does not contain any explicit
labeled information that might be used in disambiguation.

Figure 1: Our model represents a concept by re-
placing all words W in a deﬁnition D by their
vectors, and then composing these into a deﬁnition
vector with a function f (x). For each concept, all
deﬁnition vectors D are then composed into a con-
cept vector C using a second composition function
g(x).

placed is also comparable. Hence, for each am-
biguous word we encounter, we can use the cosine
distance between the abstract vector of the am-
biguous utterance and each possible sense of that
word to determine the correct sense. This makes
our approach very similar to the Lesk family of
approaches (Lesk, 1986).

In terms of composition function we experi-
mented with elementwise multiplication, averaging
and summation, all of which are unordered com-
positional functions (Mitchell and Lapata, 2008).
In addition, it is worth noting that there’s still a
lively debate whether ordered composition actually
leads to better results for estimating document-,
or sentence-level meaning, when compared to un-
ordered composition (Iyyer et al., 2015; Socher et
al., 2013).

5 Results

The accuracy scores obtained by our models using
the different word vectors are displayed in Table
2. med, mim and bio denote the vectors created
on the small Medline corpus, the Mimic-III corpus
and the BioASQ vectors, respectively. We consider
both a constrained and an unconstrained version
of the task. For each word, the constrained ver-
sion of the task only considers the senses present

med mim bio MRD 2-MRD 0-step

Accuracy C 0.80 0.69 0.84 0.81 0.78
Accuracy U 0.72 0.63 0.75 -

-

0.82
-

2-step
0.86
-

r-step UMLS::SenseRelate
0.89
-

0.75
-

Table 2: Results using constrained (C) and unconstrained (U) terms.

Term Accuracy

DE 0.31
Hemlock 0.4
0.46
0.46
0.47

Brucella Abortus
WT1
Murine Sarcoma Virus

Table 3: The 5 lowest-performing terms.

in the MSH-WSD dataset as possible targets. The
unconstrained version considers all concepts which
are denoted by the ambiguous term in the 2015AB
version of the UMLS as possible targets. The term
cortex, for example, only has 2 concepts asso-
ciated with it in the MSH-WSD dataset, while in
the 2015AB UMLS release it can denote 5 separate
concepts. Because the unconstrained version of the
task considers all words, it therefore gives a better
indication of real-life performance.

Accuracy C and U denote that the scores were ob-
tained in the constrained settings and unconstrained
setting, respectively. All reported scores use a win-
dow size of 6, which was optimized on a randomly
selected set of 20 terms from the MSH-WSD set.
Varying the window size had negligible results: all
window sizes over 6 had comparable results, and
increasing the window size over 30 causes a (small)
decline in results. This is in line with McInnes and
Pedersen (2013), who report a positive effect of
window size that quickly tapers off for window
sizes > 10. Concerning the composition functions,
summation and averaging as ﬁrst and second or-
der composition function worked best, while using
element-wise multiplication did not work well in
any case. Where possible, we display the self-
reported scores from the relevant papers on the
same dataset.

A ﬁrst thing to note is the large difference in
accuracy when changing the set of word repre-
sentations, especially the difference between the
Medline vectors and the vectors derived from the
Mimic-III corpus.
It is currently unclear what
causes these performance differences, although it
is likely that the small vocabulary, caused by the
noisiness of the clinical data in the MIMIC-III cor-

pus, reduces performance. Compared to previous
approaches, our approach outperforms the MRD,
2-MRD, and UMLS::SenseRelate approaches, but
does not manage to improve on the scores of the
step models. Recall, however, that the step models
largely rely on relationships in the UMLS ontology
to estimate concept relatedness.

To compare how our models improved when in-
cluding relation information, we also experimented
with adding deﬁnitions of related concepts, i.e. con-
cepts which had a sibling, parent or child rela-
tionship to each concept. In contrast to patterns
observed in earlier work, this did not have a sig-
niﬁcant, and often a detrimental, effect on perfor-
mance. Note that this makes our model entirely
independent of the actual UMLS hierarchy, and
more ﬂexible as a result, as we only use the map-
pings from deﬁnition to CUI for disambiguation,
and no other information, such as relations or se-
mantic type. In addition, our system is also fast:
on a consumer-grade laptop, our approach takes 10
seconds to vectorize and disambiguate all abstracts
in the MSH dataset, not taking into account the
time it takes to load the embeddings into memory.
Our approach obtains an accuracy of > 90% on
103 terms, showing that it is able to disambiguate
a large variety of terms. For some terms, how-
ever, the performance was below random guessing.
These are shown in Table 3. The pattern of er-
rors is quite clear: Our approach has trouble with
disambiguation if the deﬁnitions of the concepts
themselves are lexically very similar. As an exam-
ple, on the term Hemlock our approach performs
below chance level because one of the concepts
denotes a family of poisonous plants, while the
other reports a tree, also called hemlock, the de-
scription of which mentions that it is explicitly not
poisonous. We expect these kinds of problems to
be alleviated with the addition of more data.

6 Conclusion and future work

In this paper we presented a novel approach to
WSD in the biomedical domain which achieves
comparable performance to existing methods with-
out incorporating relational information from an

ontology. This makes the approach easily transfer-
able to other languages, for which such ontologies
might not exist, and to other domains. The large
variation in accuracy when changing sets of word
embeddings also raises interesting prospects for
improvement; better word representations will lead
to an improvement in our approach without modify-
ing the approach itself. Additionally, we would like
to experiment with different composition functions
for composing the deﬁnition and concept vectors.

Acknowledgments

Part of this research was carried out in the frame-
work of the Accumulate IWT SBO project, funded
by the government agency for Innovation by Sci-
ence and Technology (IWT). We would also like to
thank Elyne Scheurwegs for making the small set
of Medline abstract available to us.

References

[Alexopoulou et al.2009] Dimitra Alexopoulou, Bill
Andreopoulos, Heiko Dietze, Andreas Doms, Fa-
J¨org Hakenberg, Khaled Khelif,
bien Gandon,
Michael Schroeder, and Thomas W¨achter.
2009.
Biomedical word sense disambiguation with on-
tologies and metadata: automation meets accuracy.
BMC bioinformatics, 10(1):1.

[Bodenreider2004] Olivier Bodenreider.

2004. The
uniﬁed medical language system (UMLS): integrat-
ing biomedical terminology. Nucleic acids research,
32(suppl 1):D267–D270.

[Chen et al.2014] Xinxiong Chen, Zhiyuan Liu, and
A uniﬁed model for
In

Maosong Sun.
word sense representation and disambiguation.
EMNLP, pages 1025–1035. Citeseer.

2014.

[De Smedt and Daelemans2012] Tom De Smedt and
Walter Daelemans. 2012. Pattern for Python. The
Journal of Machine Learning Research, 13(1):2063–
2067.

[Fellbaum1998] Christiane Fellbaum. 1998. WordNet.

Wiley Online Library.

[Humphrey et al.2006] Susanne M Humphrey, Willie J
Rogers, Halil Kilicoglu, Dina Demner-Fushman,
and Thomas C Rindﬂesch. 2006. Word sense disam-
biguation by selecting the best semantic type based
on Journal Descriptor Indexing: Preliminary experi-
ment. Journal of the American Society for Informa-
tion Science and Technology, 57(1):96–113.

[Iyyer et al.2015] Mohit Iyyer, Varun Manjunatha, Jor-
dan Boyd-Graber, and Hal Daum´e III. 2015. Deep
unordered composition rivals syntactic methods for
text classiﬁcation. In Association for Computational
Linguistics.

[Jimeno-Yepes and Berlanga2015] Antonio

Jimeno-
Yepes and Rafael Berlanga.
2015. Knowledge
based word-concept model estimation and re-
Journal of
ﬁnement for biomedical text mining.
biomedical informatics, 53:300–307.

[Jimeno-Yepes et al.2011] Antonio

Jimeno-Yepes,
Bridget T McInnes, and Alan R Aronson. 2011.
Exploiting MeSH indexing in MEDLINE to gener-
ate a data set for word sense disambiguation. BMC
bioinformatics, 12(1):1.

J

[Johnson et al.2016] AEW Johnson, TJ Pollard, L Shen,
L Lehman, M Feng, M Ghassemi, B Moody,
P Szolovits, LA Celi, and RG Mark. 2016. MIMIC-
III, a freely accessible critical care database. Scien-
tiﬁc Data.

[Lesk1986] Michael Lesk.

1986. Automatic sense
disambiguation using machine readable dictionaries:
how to tell a pine cone from an ice cream cone. In
Proceedings of the 5th annual international confer-
ence on Systems documentation, pages 24–26. Asso-
ciation for Computing Machinery.

[McInnes and Pedersen2013] Bridget T McInnes and
Ted Pedersen. 2013. Evaluating measures of seman-
tic similarity and relatedness to disambiguate terms
in biomedical text. Journal of biomedical informat-
ics, 46(6):1116–1124.

[McInnes et al.2009] Bridget T McInnes, Ted Pedersen,
and Serguei VS Pakhomov. 2009. UMLS-Interface
and UMLS-Similarity: open source software for
measuring paths and semantic similarity. In AMIA
Annual Symposium Proceedings, volume 2009, page
431. American Medical Informatics Association.

[McInnes2008] Bridget T McInnes. 2008. An unsu-
pervised vector approach to biomedical term disam-
biguation: integrating UMLS and Medline. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technologies: Student Research Workshop,
pages 49–54. Association for Computational Lin-
guistics.

[Mikolov et al.2013] Tomas Mikolov,

Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in neural
and their compositionality.
information processing systems, pages 3111–3119.

[Ide and V´eronis1998] Nancy Ide and Jean V´eronis.
1998. Introduction to the special issue on word sense
disambiguation: the state of the art. Computational
linguistics, 24(1):2–40.

[Mitchell and Lapata2008] Jeff Mitchell and Mirella
Lapata. 2008. Vector-based models of semantic
composition. In Proceedings of the Association for
Computational Linguistics, pages 236–244.

[Pedersen et al.2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. WordNet:: Sim-
ilarity: measuring the relatedness of concepts.
In
Demonstration papers at HLT-NAACL 2004, pages
38–41. Association for Computational Linguistics.

[Pedregosa et al.2011] Fabian Pedregosa, Ga¨el Varo-
quaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel,
Peter Prettenhofer, Ron Weiss, Vincent Dubourg,
Scikit-learn: Machine learning
et al.
The Journal of Machine Learning
in Python.
Research, 12:2825–2830.

2011.

2010.

[ ˇReh˚uˇrek and Sojka2010] Radim ˇReh˚uˇrek and Petr So-
Software Framework for Topic
jka.
In Proceed-
Modelling with Large Corpora.
ings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Val-
letta, Malta, May. ELRA. http://is.muni.
cz/publication/884893/en.

[Socher et al.2013] Richard Socher, Alex Perelygin,
Jean Y Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Recur-
sive deep models for semantic compositionality over
a sentiment treebank. In Proceedings of the confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), volume 1631, page 1642.

Using Distributed Representations to Disambiguate Biomedical and
Clinical Concepts

St´ephan Tulkens(cid:70) and Simon ˇSuster(cid:70)♠ and Walter Daelemans(cid:70)

(cid:70)CLiPS
University of Antwerp
Prinsstraat 13
2000 Antwerpen
Belgium
{ﬁrstname.lastname}@uantwerpen.be

♠CLCG
University of Groningen
Oude Kijk in ’t Jatstr. 26
9700AS Groningen
The Netherlands

Abstract

In this paper, we report a knowledge-based
method for Word Sense Disambiguation
in the domains of biomedical and clini-
cal text. We combine word representa-
tions created on large corpora with a small
number of deﬁnitions from the UMLS to
create concept representations, which we
then compare to representations of the con-
text of ambiguous terms. Using no re-
lational information, we obtain compara-
ble performance to previous approaches
on the MSH-WSD dataset, which is a
well-known dataset in the biomedical do-
main. Additionally, our method is fast
and easy to set up and extend to other do-
mains. Supplementary materials, includ-
ing source code, can be found at https:
//github.com/clips/yarn

1

Introduction

Word Sense Disambiguation (WSD) is a procedure
in which an ambiguous term or concept is assigned
a single sense appropriate for that context, and is
an important step in the creation of a semantic rep-
resentation of a document (Ide and V´eronis, 1998).
While performing WSD will beneﬁt most natural
language processing applications, disambiguation
of concepts is a critical component of applications
operating on clinical and biomedical text, in which
the same word can denote differing concepts, and
may thus elicit radically different responses.

Compounding this problem of ambiguity is the
fact that clinical text, in general, is noisier than
other domains, and contains a large variety of ab-
breviations, some of which may be speciﬁc to a
single hospital or physician. Additionally, there is a
marked absence of large volumes of annotated clini-
cal text, even for English, which presents a problem

for supervised approaches to Word Sense Disam-
biguation. For other languages, such as Dutch,
there exist no freely available annotated corpora of
clinical text.

A ﬁrst step towards solving this problem could
be the use of distributed representations. Where a
more traditional word representation, such as a TF-
IDF bag-of-words (BoW) representation, carries
frequency information, distributed representations
encode semantic information. A big advantage to
using these representations is that they can be gen-
erated from large corpora of unlabeled text, and
can be trained on very large corpora in a reason-
able amount of time. These representations, es-
pecially when trained using neural architectures
such as word2vec (Mikolov et al., 2013), have
been shown to improve performance on a variety
of tasks when compared to more traditional BoW
representations.

We hypothesize that these kinds of distributional
representations are well-suited for WSD in the clin-
ical and biomedical domain because of the lack
of training data, and the large terminological va-
riety. We present a knowledge-based approach to
Word Sense Disambiguation which creates concept
representations by combining deﬁnitions from the
Uniﬁed Medical Language System (UMLS) with
distributed representations. We test our hypothesis
on the MSH-WSD, which is a well-known dataset
for WSD in the biomedical domain.

2 Related Research

All knowledge-based methods we review use
the Uniﬁed Medical Language System® (UMLS)
Metathesaurus® (Bodenreider, 2004) as a knowl-
edge base, possibly augmented with external
sources, such as MeSH®-indexed abstracts. Gen-
erally speaking, the UMLS contains two separate
information sources that are suitable for use in dis-

6
1
0
2
 
g
u
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
0
6
5
0
.
8
0
6
1
:
v
i
X
r
a

ambiguation: the concept unique identiﬁer (CUI),
which is a unique label for each concept, and the
semantic type (ST), which is a set of 135 broad
labels such as “Animal” or “Chemical”. In gen-
eral, a word is only considered disambiguated if
the correct CUI can be selected; hence, as McInnes
and Pedersen (2013) note, approaches based on se-
mantic types are not able to disambiguate between
approximately 12% of concepts, as some concepts
with the same surface form have an identical ST,
but a different CUI.

In terms of approaches using ST, Humphrey et
al. (2006) create one vector for each semantic type
by creating a BoW representation of all words that
denote that semantic type. For each ambiguous
term, a target word vector is created by taking a
window of words from the right and left of the term.
The concept which is associated with the ST with
the lowest cosine distance is then taken to be the
correct sense of the term. Similarly, Alexopoulou
et al. (2009) create a method which ﬁnds the closest
concept based on a combination of co-occurrence
with other semantic types and ontological similarity
through is-a relationships.

Closest to our approach is the machine read-
able dictionary (MRD) approach (McInnes, 2008;
Jimeno-Yepes et al., 2011), which uses deﬁnitions
from the UMLS to create concept vectors by creat-
ing BoW representations of concepts using all deﬁ-
nitions of the concept and those of related concepts.
This BoW representation contains TF-IDF values
where D is the number of concepts in which a word
appears, thereby reducing the inﬂuence of general
words which occur in many concepts. These rep-
resentations are then compared to the vectorized
contexts of the ambiguous terms using cosine dis-
tance. A reﬁnement of MRD, called second-order
co-occurrence MRD (2-MRD) (McInnes, 2008), re-
places each word in a deﬁnition by a vector which
contains TF-IDF values of co-occurrence counts,
thereby associating each word with a context.

McInnes

introduce
and Pedersen (2013)
UMLS::SenseRelate, an approach which is based
on Pedersen et al. (2004)’s WordNet::SenseRelate.
In this system, each possible sense for an ambigu-
ous term is assigned a distance-weighted score
based on the concepts of the terms surrounding it,
where the concepts of the surrounding terms are
determined using UMLS::Similarity (McInnes et
al., 2009).

Corpus size
Vocabulary
Dimension

Medline Mimic-III Bioasq
920,081 13,097,844 -
196,960 71,663
320

1,701,632
200

320

Table 1: The number of words in the corpus, the
resulting vocabulary size, and the dimension of the
resulting vectors.

called step models, which calculate the probabil-
ity of a word occurring with a certain concept by
considering the number of times a word occurs
in the deﬁnitions of that concept and its related
concepts. It then steps through the UMLS-deﬁned
ontology of concepts, and reﬁnes the probabilities
for each word and each concept based on the rela-
tions within the ontology.

Finally, Chen et al. (2014) present an approach
for general WSD which uses word embeddings
coupled with WordNet (Fellbaum, 1998) as a re-
source to perform sense disambiguation, and which
creates sense-speciﬁc word embeddings from these
sense-disambiguated word representations.

3 Materials

3.1 Test Corpus

We use the MSH-WSD corpus (Jimeno-Yepes et
al., 2011), which consists of a set of 203 ambigu-
ous terms, each associated with multiple concepts,
to evaluate our approach. Of the 203 terms in the
corpus, 106 are regular terms, 88 are acronyms, and
9 can be acronyms and regular terms. For each of
these concepts, up to 100 MeSH abstracts were re-
trieved, resulting in a set of 37,888 abstracts. In our
approach, all abstracts were pre-processed using
the tokenizer from the Pattern package (De Smedt
and Daelemans, 2012), and all stop words were
removed using the English stop word list from
scikit-learn (Pedregosa et al., 2011).

3.2 Word vectors

We evaluate our approach using three sets of vec-
tors: The ﬁrst set was trained on a small set of
Medline abstracts1, and a second set of vectors
created on the entirety of the MIMIC-III corpus
of clinical notes (Johnson et al., 2016). For both
sets, we used the word2vec implementation from
gensim ( ˇReh˚uˇrek and Sojka, 2010), using skip-
gram with negative sampling, a frequency cutoff

1The speciﬁc IDs of these abstracts are available in the

Jimeno-Yepes and Berlanga (2015) present so-

online appendix.

of 5 and a negative sampling of 15. Additionally,
we used a third set of vectors, available from the
BioASQ organisers2, which was trained on a much
larger set of Medline abstracts.3 The model statis-
tics are visualized in Table 1.

4 Approach

Similar to the 2-MRD approach detailed above, our
approach creates concept vectors by replacing each
word in every deﬁnition by the vector representa-
tion of that word. This creates an M × n matrix for
each deﬁnition, where M is the dimensionality of
the word vectors, and n the number of words con-
tained in that deﬁnition. Following this, for each
deﬁnition, we then obtain a single vector of dimen-
sionality M by applying a compositional function
to the matrix, thereby obtaining so-called deﬁnition
vectors, which represent the entire meaning of the
deﬁnition in one vector. Each concept can then be
represented by a M × d matrix, where d is the num-
ber of deﬁnitions that a concept has in the UMLS.
Finally, we apply a second composition function
to this matrix, thereby obtaining a single vector of
dimensionality M which represents the combined
meaning of all deﬁnitions for that concept, i.e. a
concept vector.

For each abstract in the test corpus, we ﬁrst lo-
cate each ambiguous term through a simple lookup.
For each located term in the abstract we create a
vector representation by retrieving all words in a
window of size w surrounding the ambiguous term,
and replacing the words by their vectors. Note
that this window does not include the ambiguous
term itself. These collections of vectors are then
combined into M -dimensional vectors using the
same composition function as above. This is done
separately for each term occurrence within a single
document, creating a M × x matrix, where x is
the number of times the ambiguous term occurs
in a single document. These are then combined
in an M -dimensional term vector using the same
composition we used for the concepts, above. A
schematic representation of our model is given in
Figure 1.

Because all concept and term vectors are created
using the same distributed vectors and composi-
tional functions, the vector space in which they are

2Available on the BioASQ website.
3While we concede that the BioASQ corpora might contain
abstracts from the MSH dataset, it does not contain any explicit
labeled information that might be used in disambiguation.

Figure 1: Our model represents a concept by re-
placing all words W in a deﬁnition D by their
vectors, and then composing these into a deﬁnition
vector with a function f (x). For each concept, all
deﬁnition vectors D are then composed into a con-
cept vector C using a second composition function
g(x).

placed is also comparable. Hence, for each am-
biguous word we encounter, we can use the cosine
distance between the abstract vector of the am-
biguous utterance and each possible sense of that
word to determine the correct sense. This makes
our approach very similar to the Lesk family of
approaches (Lesk, 1986).

In terms of composition function we experi-
mented with elementwise multiplication, averaging
and summation, all of which are unordered com-
positional functions (Mitchell and Lapata, 2008).
In addition, it is worth noting that there’s still a
lively debate whether ordered composition actually
leads to better results for estimating document-,
or sentence-level meaning, when compared to un-
ordered composition (Iyyer et al., 2015; Socher et
al., 2013).

5 Results

The accuracy scores obtained by our models using
the different word vectors are displayed in Table
2. med, mim and bio denote the vectors created
on the small Medline corpus, the Mimic-III corpus
and the BioASQ vectors, respectively. We consider
both a constrained and an unconstrained version
of the task. For each word, the constrained ver-
sion of the task only considers the senses present

med mim bio MRD 2-MRD 0-step

Accuracy C 0.80 0.69 0.84 0.81 0.78
Accuracy U 0.72 0.63 0.75 -

-

0.82
-

2-step
0.86
-

r-step UMLS::SenseRelate
0.89
-

0.75
-

Table 2: Results using constrained (C) and unconstrained (U) terms.

Term Accuracy

DE 0.31
Hemlock 0.4
0.46
0.46
0.47

Brucella Abortus
WT1
Murine Sarcoma Virus

Table 3: The 5 lowest-performing terms.

in the MSH-WSD dataset as possible targets. The
unconstrained version considers all concepts which
are denoted by the ambiguous term in the 2015AB
version of the UMLS as possible targets. The term
cortex, for example, only has 2 concepts asso-
ciated with it in the MSH-WSD dataset, while in
the 2015AB UMLS release it can denote 5 separate
concepts. Because the unconstrained version of the
task considers all words, it therefore gives a better
indication of real-life performance.

Accuracy C and U denote that the scores were ob-
tained in the constrained settings and unconstrained
setting, respectively. All reported scores use a win-
dow size of 6, which was optimized on a randomly
selected set of 20 terms from the MSH-WSD set.
Varying the window size had negligible results: all
window sizes over 6 had comparable results, and
increasing the window size over 30 causes a (small)
decline in results. This is in line with McInnes and
Pedersen (2013), who report a positive effect of
window size that quickly tapers off for window
sizes > 10. Concerning the composition functions,
summation and averaging as ﬁrst and second or-
der composition function worked best, while using
element-wise multiplication did not work well in
any case. Where possible, we display the self-
reported scores from the relevant papers on the
same dataset.

A ﬁrst thing to note is the large difference in
accuracy when changing the set of word repre-
sentations, especially the difference between the
Medline vectors and the vectors derived from the
Mimic-III corpus.
It is currently unclear what
causes these performance differences, although it
is likely that the small vocabulary, caused by the
noisiness of the clinical data in the MIMIC-III cor-

pus, reduces performance. Compared to previous
approaches, our approach outperforms the MRD,
2-MRD, and UMLS::SenseRelate approaches, but
does not manage to improve on the scores of the
step models. Recall, however, that the step models
largely rely on relationships in the UMLS ontology
to estimate concept relatedness.

To compare how our models improved when in-
cluding relation information, we also experimented
with adding deﬁnitions of related concepts, i.e. con-
cepts which had a sibling, parent or child rela-
tionship to each concept. In contrast to patterns
observed in earlier work, this did not have a sig-
niﬁcant, and often a detrimental, effect on perfor-
mance. Note that this makes our model entirely
independent of the actual UMLS hierarchy, and
more ﬂexible as a result, as we only use the map-
pings from deﬁnition to CUI for disambiguation,
and no other information, such as relations or se-
mantic type. In addition, our system is also fast:
on a consumer-grade laptop, our approach takes 10
seconds to vectorize and disambiguate all abstracts
in the MSH dataset, not taking into account the
time it takes to load the embeddings into memory.
Our approach obtains an accuracy of > 90% on
103 terms, showing that it is able to disambiguate
a large variety of terms. For some terms, how-
ever, the performance was below random guessing.
These are shown in Table 3. The pattern of er-
rors is quite clear: Our approach has trouble with
disambiguation if the deﬁnitions of the concepts
themselves are lexically very similar. As an exam-
ple, on the term Hemlock our approach performs
below chance level because one of the concepts
denotes a family of poisonous plants, while the
other reports a tree, also called hemlock, the de-
scription of which mentions that it is explicitly not
poisonous. We expect these kinds of problems to
be alleviated with the addition of more data.

6 Conclusion and future work

In this paper we presented a novel approach to
WSD in the biomedical domain which achieves
comparable performance to existing methods with-
out incorporating relational information from an

ontology. This makes the approach easily transfer-
able to other languages, for which such ontologies
might not exist, and to other domains. The large
variation in accuracy when changing sets of word
embeddings also raises interesting prospects for
improvement; better word representations will lead
to an improvement in our approach without modify-
ing the approach itself. Additionally, we would like
to experiment with different composition functions
for composing the deﬁnition and concept vectors.

Acknowledgments

Part of this research was carried out in the frame-
work of the Accumulate IWT SBO project, funded
by the government agency for Innovation by Sci-
ence and Technology (IWT). We would also like to
thank Elyne Scheurwegs for making the small set
of Medline abstract available to us.

References

[Alexopoulou et al.2009] Dimitra Alexopoulou, Bill
Andreopoulos, Heiko Dietze, Andreas Doms, Fa-
J¨org Hakenberg, Khaled Khelif,
bien Gandon,
Michael Schroeder, and Thomas W¨achter.
2009.
Biomedical word sense disambiguation with on-
tologies and metadata: automation meets accuracy.
BMC bioinformatics, 10(1):1.

[Bodenreider2004] Olivier Bodenreider.

2004. The
uniﬁed medical language system (UMLS): integrat-
ing biomedical terminology. Nucleic acids research,
32(suppl 1):D267–D270.

[Chen et al.2014] Xinxiong Chen, Zhiyuan Liu, and
A uniﬁed model for
In

Maosong Sun.
word sense representation and disambiguation.
EMNLP, pages 1025–1035. Citeseer.

2014.

[De Smedt and Daelemans2012] Tom De Smedt and
Walter Daelemans. 2012. Pattern for Python. The
Journal of Machine Learning Research, 13(1):2063–
2067.

[Fellbaum1998] Christiane Fellbaum. 1998. WordNet.

Wiley Online Library.

[Humphrey et al.2006] Susanne M Humphrey, Willie J
Rogers, Halil Kilicoglu, Dina Demner-Fushman,
and Thomas C Rindﬂesch. 2006. Word sense disam-
biguation by selecting the best semantic type based
on Journal Descriptor Indexing: Preliminary experi-
ment. Journal of the American Society for Informa-
tion Science and Technology, 57(1):96–113.

[Iyyer et al.2015] Mohit Iyyer, Varun Manjunatha, Jor-
dan Boyd-Graber, and Hal Daum´e III. 2015. Deep
unordered composition rivals syntactic methods for
text classiﬁcation. In Association for Computational
Linguistics.

[Jimeno-Yepes and Berlanga2015] Antonio

Jimeno-
Yepes and Rafael Berlanga.
2015. Knowledge
based word-concept model estimation and re-
Journal of
ﬁnement for biomedical text mining.
biomedical informatics, 53:300–307.

[Jimeno-Yepes et al.2011] Antonio

Jimeno-Yepes,
Bridget T McInnes, and Alan R Aronson. 2011.
Exploiting MeSH indexing in MEDLINE to gener-
ate a data set for word sense disambiguation. BMC
bioinformatics, 12(1):1.

J

[Johnson et al.2016] AEW Johnson, TJ Pollard, L Shen,
L Lehman, M Feng, M Ghassemi, B Moody,
P Szolovits, LA Celi, and RG Mark. 2016. MIMIC-
III, a freely accessible critical care database. Scien-
tiﬁc Data.

[Lesk1986] Michael Lesk.

1986. Automatic sense
disambiguation using machine readable dictionaries:
how to tell a pine cone from an ice cream cone. In
Proceedings of the 5th annual international confer-
ence on Systems documentation, pages 24–26. Asso-
ciation for Computing Machinery.

[McInnes and Pedersen2013] Bridget T McInnes and
Ted Pedersen. 2013. Evaluating measures of seman-
tic similarity and relatedness to disambiguate terms
in biomedical text. Journal of biomedical informat-
ics, 46(6):1116–1124.

[McInnes et al.2009] Bridget T McInnes, Ted Pedersen,
and Serguei VS Pakhomov. 2009. UMLS-Interface
and UMLS-Similarity: open source software for
measuring paths and semantic similarity. In AMIA
Annual Symposium Proceedings, volume 2009, page
431. American Medical Informatics Association.

[McInnes2008] Bridget T McInnes. 2008. An unsu-
pervised vector approach to biomedical term disam-
biguation: integrating UMLS and Medline. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technologies: Student Research Workshop,
pages 49–54. Association for Computational Lin-
guistics.

[Mikolov et al.2013] Tomas Mikolov,

Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in neural
and their compositionality.
information processing systems, pages 3111–3119.

[Ide and V´eronis1998] Nancy Ide and Jean V´eronis.
1998. Introduction to the special issue on word sense
disambiguation: the state of the art. Computational
linguistics, 24(1):2–40.

[Mitchell and Lapata2008] Jeff Mitchell and Mirella
Lapata. 2008. Vector-based models of semantic
composition. In Proceedings of the Association for
Computational Linguistics, pages 236–244.

[Pedersen et al.2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. WordNet:: Sim-
ilarity: measuring the relatedness of concepts.
In
Demonstration papers at HLT-NAACL 2004, pages
38–41. Association for Computational Linguistics.

[Pedregosa et al.2011] Fabian Pedregosa, Ga¨el Varo-
quaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel,
Peter Prettenhofer, Ron Weiss, Vincent Dubourg,
Scikit-learn: Machine learning
et al.
The Journal of Machine Learning
in Python.
Research, 12:2825–2830.

2011.

2010.

[ ˇReh˚uˇrek and Sojka2010] Radim ˇReh˚uˇrek and Petr So-
Software Framework for Topic
jka.
In Proceed-
Modelling with Large Corpora.
ings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Val-
letta, Malta, May. ELRA. http://is.muni.
cz/publication/884893/en.

[Socher et al.2013] Richard Socher, Alex Perelygin,
Jean Y Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Recur-
sive deep models for semantic compositionality over
a sentiment treebank. In Proceedings of the confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), volume 1631, page 1642.

Using Distributed Representations to Disambiguate Biomedical and
Clinical Concepts

St´ephan Tulkens(cid:70) and Simon ˇSuster(cid:70)♠ and Walter Daelemans(cid:70)

(cid:70)CLiPS
University of Antwerp
Prinsstraat 13
2000 Antwerpen
Belgium
{ﬁrstname.lastname}@uantwerpen.be

♠CLCG
University of Groningen
Oude Kijk in ’t Jatstr. 26
9700AS Groningen
The Netherlands

Abstract

In this paper, we report a knowledge-based
method for Word Sense Disambiguation
in the domains of biomedical and clini-
cal text. We combine word representa-
tions created on large corpora with a small
number of deﬁnitions from the UMLS to
create concept representations, which we
then compare to representations of the con-
text of ambiguous terms. Using no re-
lational information, we obtain compara-
ble performance to previous approaches
on the MSH-WSD dataset, which is a
well-known dataset in the biomedical do-
main. Additionally, our method is fast
and easy to set up and extend to other do-
mains. Supplementary materials, includ-
ing source code, can be found at https:
//github.com/clips/yarn

1

Introduction

Word Sense Disambiguation (WSD) is a procedure
in which an ambiguous term or concept is assigned
a single sense appropriate for that context, and is
an important step in the creation of a semantic rep-
resentation of a document (Ide and V´eronis, 1998).
While performing WSD will beneﬁt most natural
language processing applications, disambiguation
of concepts is a critical component of applications
operating on clinical and biomedical text, in which
the same word can denote differing concepts, and
may thus elicit radically different responses.

Compounding this problem of ambiguity is the
fact that clinical text, in general, is noisier than
other domains, and contains a large variety of ab-
breviations, some of which may be speciﬁc to a
single hospital or physician. Additionally, there is a
marked absence of large volumes of annotated clini-
cal text, even for English, which presents a problem

for supervised approaches to Word Sense Disam-
biguation. For other languages, such as Dutch,
there exist no freely available annotated corpora of
clinical text.

A ﬁrst step towards solving this problem could
be the use of distributed representations. Where a
more traditional word representation, such as a TF-
IDF bag-of-words (BoW) representation, carries
frequency information, distributed representations
encode semantic information. A big advantage to
using these representations is that they can be gen-
erated from large corpora of unlabeled text, and
can be trained on very large corpora in a reason-
able amount of time. These representations, es-
pecially when trained using neural architectures
such as word2vec (Mikolov et al., 2013), have
been shown to improve performance on a variety
of tasks when compared to more traditional BoW
representations.

We hypothesize that these kinds of distributional
representations are well-suited for WSD in the clin-
ical and biomedical domain because of the lack
of training data, and the large terminological va-
riety. We present a knowledge-based approach to
Word Sense Disambiguation which creates concept
representations by combining deﬁnitions from the
Uniﬁed Medical Language System (UMLS) with
distributed representations. We test our hypothesis
on the MSH-WSD, which is a well-known dataset
for WSD in the biomedical domain.

2 Related Research

All knowledge-based methods we review use
the Uniﬁed Medical Language System® (UMLS)
Metathesaurus® (Bodenreider, 2004) as a knowl-
edge base, possibly augmented with external
sources, such as MeSH®-indexed abstracts. Gen-
erally speaking, the UMLS contains two separate
information sources that are suitable for use in dis-

6
1
0
2
 
g
u
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
0
6
5
0
.
8
0
6
1
:
v
i
X
r
a

ambiguation: the concept unique identiﬁer (CUI),
which is a unique label for each concept, and the
semantic type (ST), which is a set of 135 broad
labels such as “Animal” or “Chemical”. In gen-
eral, a word is only considered disambiguated if
the correct CUI can be selected; hence, as McInnes
and Pedersen (2013) note, approaches based on se-
mantic types are not able to disambiguate between
approximately 12% of concepts, as some concepts
with the same surface form have an identical ST,
but a different CUI.

In terms of approaches using ST, Humphrey et
al. (2006) create one vector for each semantic type
by creating a BoW representation of all words that
denote that semantic type. For each ambiguous
term, a target word vector is created by taking a
window of words from the right and left of the term.
The concept which is associated with the ST with
the lowest cosine distance is then taken to be the
correct sense of the term. Similarly, Alexopoulou
et al. (2009) create a method which ﬁnds the closest
concept based on a combination of co-occurrence
with other semantic types and ontological similarity
through is-a relationships.

Closest to our approach is the machine read-
able dictionary (MRD) approach (McInnes, 2008;
Jimeno-Yepes et al., 2011), which uses deﬁnitions
from the UMLS to create concept vectors by creat-
ing BoW representations of concepts using all deﬁ-
nitions of the concept and those of related concepts.
This BoW representation contains TF-IDF values
where D is the number of concepts in which a word
appears, thereby reducing the inﬂuence of general
words which occur in many concepts. These rep-
resentations are then compared to the vectorized
contexts of the ambiguous terms using cosine dis-
tance. A reﬁnement of MRD, called second-order
co-occurrence MRD (2-MRD) (McInnes, 2008), re-
places each word in a deﬁnition by a vector which
contains TF-IDF values of co-occurrence counts,
thereby associating each word with a context.

McInnes

introduce
and Pedersen (2013)
UMLS::SenseRelate, an approach which is based
on Pedersen et al. (2004)’s WordNet::SenseRelate.
In this system, each possible sense for an ambigu-
ous term is assigned a distance-weighted score
based on the concepts of the terms surrounding it,
where the concepts of the surrounding terms are
determined using UMLS::Similarity (McInnes et
al., 2009).

Corpus size
Vocabulary
Dimension

Medline Mimic-III Bioasq
920,081 13,097,844 -
196,960 71,663
320

1,701,632
200

320

Table 1: The number of words in the corpus, the
resulting vocabulary size, and the dimension of the
resulting vectors.

called step models, which calculate the probabil-
ity of a word occurring with a certain concept by
considering the number of times a word occurs
in the deﬁnitions of that concept and its related
concepts. It then steps through the UMLS-deﬁned
ontology of concepts, and reﬁnes the probabilities
for each word and each concept based on the rela-
tions within the ontology.

Finally, Chen et al. (2014) present an approach
for general WSD which uses word embeddings
coupled with WordNet (Fellbaum, 1998) as a re-
source to perform sense disambiguation, and which
creates sense-speciﬁc word embeddings from these
sense-disambiguated word representations.

3 Materials

3.1 Test Corpus

We use the MSH-WSD corpus (Jimeno-Yepes et
al., 2011), which consists of a set of 203 ambigu-
ous terms, each associated with multiple concepts,
to evaluate our approach. Of the 203 terms in the
corpus, 106 are regular terms, 88 are acronyms, and
9 can be acronyms and regular terms. For each of
these concepts, up to 100 MeSH abstracts were re-
trieved, resulting in a set of 37,888 abstracts. In our
approach, all abstracts were pre-processed using
the tokenizer from the Pattern package (De Smedt
and Daelemans, 2012), and all stop words were
removed using the English stop word list from
scikit-learn (Pedregosa et al., 2011).

3.2 Word vectors

We evaluate our approach using three sets of vec-
tors: The ﬁrst set was trained on a small set of
Medline abstracts1, and a second set of vectors
created on the entirety of the MIMIC-III corpus
of clinical notes (Johnson et al., 2016). For both
sets, we used the word2vec implementation from
gensim ( ˇReh˚uˇrek and Sojka, 2010), using skip-
gram with negative sampling, a frequency cutoff

1The speciﬁc IDs of these abstracts are available in the

Jimeno-Yepes and Berlanga (2015) present so-

online appendix.

of 5 and a negative sampling of 15. Additionally,
we used a third set of vectors, available from the
BioASQ organisers2, which was trained on a much
larger set of Medline abstracts.3 The model statis-
tics are visualized in Table 1.

4 Approach

Similar to the 2-MRD approach detailed above, our
approach creates concept vectors by replacing each
word in every deﬁnition by the vector representa-
tion of that word. This creates an M × n matrix for
each deﬁnition, where M is the dimensionality of
the word vectors, and n the number of words con-
tained in that deﬁnition. Following this, for each
deﬁnition, we then obtain a single vector of dimen-
sionality M by applying a compositional function
to the matrix, thereby obtaining so-called deﬁnition
vectors, which represent the entire meaning of the
deﬁnition in one vector. Each concept can then be
represented by a M × d matrix, where d is the num-
ber of deﬁnitions that a concept has in the UMLS.
Finally, we apply a second composition function
to this matrix, thereby obtaining a single vector of
dimensionality M which represents the combined
meaning of all deﬁnitions for that concept, i.e. a
concept vector.

For each abstract in the test corpus, we ﬁrst lo-
cate each ambiguous term through a simple lookup.
For each located term in the abstract we create a
vector representation by retrieving all words in a
window of size w surrounding the ambiguous term,
and replacing the words by their vectors. Note
that this window does not include the ambiguous
term itself. These collections of vectors are then
combined into M -dimensional vectors using the
same composition function as above. This is done
separately for each term occurrence within a single
document, creating a M × x matrix, where x is
the number of times the ambiguous term occurs
in a single document. These are then combined
in an M -dimensional term vector using the same
composition we used for the concepts, above. A
schematic representation of our model is given in
Figure 1.

Because all concept and term vectors are created
using the same distributed vectors and composi-
tional functions, the vector space in which they are

2Available on the BioASQ website.
3While we concede that the BioASQ corpora might contain
abstracts from the MSH dataset, it does not contain any explicit
labeled information that might be used in disambiguation.

Figure 1: Our model represents a concept by re-
placing all words W in a deﬁnition D by their
vectors, and then composing these into a deﬁnition
vector with a function f (x). For each concept, all
deﬁnition vectors D are then composed into a con-
cept vector C using a second composition function
g(x).

placed is also comparable. Hence, for each am-
biguous word we encounter, we can use the cosine
distance between the abstract vector of the am-
biguous utterance and each possible sense of that
word to determine the correct sense. This makes
our approach very similar to the Lesk family of
approaches (Lesk, 1986).

In terms of composition function we experi-
mented with elementwise multiplication, averaging
and summation, all of which are unordered com-
positional functions (Mitchell and Lapata, 2008).
In addition, it is worth noting that there’s still a
lively debate whether ordered composition actually
leads to better results for estimating document-,
or sentence-level meaning, when compared to un-
ordered composition (Iyyer et al., 2015; Socher et
al., 2013).

5 Results

The accuracy scores obtained by our models using
the different word vectors are displayed in Table
2. med, mim and bio denote the vectors created
on the small Medline corpus, the Mimic-III corpus
and the BioASQ vectors, respectively. We consider
both a constrained and an unconstrained version
of the task. For each word, the constrained ver-
sion of the task only considers the senses present

med mim bio MRD 2-MRD 0-step

Accuracy C 0.80 0.69 0.84 0.81 0.78
Accuracy U 0.72 0.63 0.75 -

-

0.82
-

2-step
0.86
-

r-step UMLS::SenseRelate
0.89
-

0.75
-

Table 2: Results using constrained (C) and unconstrained (U) terms.

Term Accuracy

DE 0.31
Hemlock 0.4
0.46
0.46
0.47

Brucella Abortus
WT1
Murine Sarcoma Virus

Table 3: The 5 lowest-performing terms.

in the MSH-WSD dataset as possible targets. The
unconstrained version considers all concepts which
are denoted by the ambiguous term in the 2015AB
version of the UMLS as possible targets. The term
cortex, for example, only has 2 concepts asso-
ciated with it in the MSH-WSD dataset, while in
the 2015AB UMLS release it can denote 5 separate
concepts. Because the unconstrained version of the
task considers all words, it therefore gives a better
indication of real-life performance.

Accuracy C and U denote that the scores were ob-
tained in the constrained settings and unconstrained
setting, respectively. All reported scores use a win-
dow size of 6, which was optimized on a randomly
selected set of 20 terms from the MSH-WSD set.
Varying the window size had negligible results: all
window sizes over 6 had comparable results, and
increasing the window size over 30 causes a (small)
decline in results. This is in line with McInnes and
Pedersen (2013), who report a positive effect of
window size that quickly tapers off for window
sizes > 10. Concerning the composition functions,
summation and averaging as ﬁrst and second or-
der composition function worked best, while using
element-wise multiplication did not work well in
any case. Where possible, we display the self-
reported scores from the relevant papers on the
same dataset.

A ﬁrst thing to note is the large difference in
accuracy when changing the set of word repre-
sentations, especially the difference between the
Medline vectors and the vectors derived from the
Mimic-III corpus.
It is currently unclear what
causes these performance differences, although it
is likely that the small vocabulary, caused by the
noisiness of the clinical data in the MIMIC-III cor-

pus, reduces performance. Compared to previous
approaches, our approach outperforms the MRD,
2-MRD, and UMLS::SenseRelate approaches, but
does not manage to improve on the scores of the
step models. Recall, however, that the step models
largely rely on relationships in the UMLS ontology
to estimate concept relatedness.

To compare how our models improved when in-
cluding relation information, we also experimented
with adding deﬁnitions of related concepts, i.e. con-
cepts which had a sibling, parent or child rela-
tionship to each concept. In contrast to patterns
observed in earlier work, this did not have a sig-
niﬁcant, and often a detrimental, effect on perfor-
mance. Note that this makes our model entirely
independent of the actual UMLS hierarchy, and
more ﬂexible as a result, as we only use the map-
pings from deﬁnition to CUI for disambiguation,
and no other information, such as relations or se-
mantic type. In addition, our system is also fast:
on a consumer-grade laptop, our approach takes 10
seconds to vectorize and disambiguate all abstracts
in the MSH dataset, not taking into account the
time it takes to load the embeddings into memory.
Our approach obtains an accuracy of > 90% on
103 terms, showing that it is able to disambiguate
a large variety of terms. For some terms, how-
ever, the performance was below random guessing.
These are shown in Table 3. The pattern of er-
rors is quite clear: Our approach has trouble with
disambiguation if the deﬁnitions of the concepts
themselves are lexically very similar. As an exam-
ple, on the term Hemlock our approach performs
below chance level because one of the concepts
denotes a family of poisonous plants, while the
other reports a tree, also called hemlock, the de-
scription of which mentions that it is explicitly not
poisonous. We expect these kinds of problems to
be alleviated with the addition of more data.

6 Conclusion and future work

In this paper we presented a novel approach to
WSD in the biomedical domain which achieves
comparable performance to existing methods with-
out incorporating relational information from an

ontology. This makes the approach easily transfer-
able to other languages, for which such ontologies
might not exist, and to other domains. The large
variation in accuracy when changing sets of word
embeddings also raises interesting prospects for
improvement; better word representations will lead
to an improvement in our approach without modify-
ing the approach itself. Additionally, we would like
to experiment with different composition functions
for composing the deﬁnition and concept vectors.

Acknowledgments

Part of this research was carried out in the frame-
work of the Accumulate IWT SBO project, funded
by the government agency for Innovation by Sci-
ence and Technology (IWT). We would also like to
thank Elyne Scheurwegs for making the small set
of Medline abstract available to us.

References

[Alexopoulou et al.2009] Dimitra Alexopoulou, Bill
Andreopoulos, Heiko Dietze, Andreas Doms, Fa-
J¨org Hakenberg, Khaled Khelif,
bien Gandon,
Michael Schroeder, and Thomas W¨achter.
2009.
Biomedical word sense disambiguation with on-
tologies and metadata: automation meets accuracy.
BMC bioinformatics, 10(1):1.

[Bodenreider2004] Olivier Bodenreider.

2004. The
uniﬁed medical language system (UMLS): integrat-
ing biomedical terminology. Nucleic acids research,
32(suppl 1):D267–D270.

[Chen et al.2014] Xinxiong Chen, Zhiyuan Liu, and
A uniﬁed model for
In

Maosong Sun.
word sense representation and disambiguation.
EMNLP, pages 1025–1035. Citeseer.

2014.

[De Smedt and Daelemans2012] Tom De Smedt and
Walter Daelemans. 2012. Pattern for Python. The
Journal of Machine Learning Research, 13(1):2063–
2067.

[Fellbaum1998] Christiane Fellbaum. 1998. WordNet.

Wiley Online Library.

[Humphrey et al.2006] Susanne M Humphrey, Willie J
Rogers, Halil Kilicoglu, Dina Demner-Fushman,
and Thomas C Rindﬂesch. 2006. Word sense disam-
biguation by selecting the best semantic type based
on Journal Descriptor Indexing: Preliminary experi-
ment. Journal of the American Society for Informa-
tion Science and Technology, 57(1):96–113.

[Iyyer et al.2015] Mohit Iyyer, Varun Manjunatha, Jor-
dan Boyd-Graber, and Hal Daum´e III. 2015. Deep
unordered composition rivals syntactic methods for
text classiﬁcation. In Association for Computational
Linguistics.

[Jimeno-Yepes and Berlanga2015] Antonio

Jimeno-
Yepes and Rafael Berlanga.
2015. Knowledge
based word-concept model estimation and re-
Journal of
ﬁnement for biomedical text mining.
biomedical informatics, 53:300–307.

[Jimeno-Yepes et al.2011] Antonio

Jimeno-Yepes,
Bridget T McInnes, and Alan R Aronson. 2011.
Exploiting MeSH indexing in MEDLINE to gener-
ate a data set for word sense disambiguation. BMC
bioinformatics, 12(1):1.

J

[Johnson et al.2016] AEW Johnson, TJ Pollard, L Shen,
L Lehman, M Feng, M Ghassemi, B Moody,
P Szolovits, LA Celi, and RG Mark. 2016. MIMIC-
III, a freely accessible critical care database. Scien-
tiﬁc Data.

[Lesk1986] Michael Lesk.

1986. Automatic sense
disambiguation using machine readable dictionaries:
how to tell a pine cone from an ice cream cone. In
Proceedings of the 5th annual international confer-
ence on Systems documentation, pages 24–26. Asso-
ciation for Computing Machinery.

[McInnes and Pedersen2013] Bridget T McInnes and
Ted Pedersen. 2013. Evaluating measures of seman-
tic similarity and relatedness to disambiguate terms
in biomedical text. Journal of biomedical informat-
ics, 46(6):1116–1124.

[McInnes et al.2009] Bridget T McInnes, Ted Pedersen,
and Serguei VS Pakhomov. 2009. UMLS-Interface
and UMLS-Similarity: open source software for
measuring paths and semantic similarity. In AMIA
Annual Symposium Proceedings, volume 2009, page
431. American Medical Informatics Association.

[McInnes2008] Bridget T McInnes. 2008. An unsu-
pervised vector approach to biomedical term disam-
biguation: integrating UMLS and Medline. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technologies: Student Research Workshop,
pages 49–54. Association for Computational Lin-
guistics.

[Mikolov et al.2013] Tomas Mikolov,

Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in neural
and their compositionality.
information processing systems, pages 3111–3119.

[Ide and V´eronis1998] Nancy Ide and Jean V´eronis.
1998. Introduction to the special issue on word sense
disambiguation: the state of the art. Computational
linguistics, 24(1):2–40.

[Mitchell and Lapata2008] Jeff Mitchell and Mirella
Lapata. 2008. Vector-based models of semantic
composition. In Proceedings of the Association for
Computational Linguistics, pages 236–244.

[Pedersen et al.2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. WordNet:: Sim-
ilarity: measuring the relatedness of concepts.
In
Demonstration papers at HLT-NAACL 2004, pages
38–41. Association for Computational Linguistics.

[Pedregosa et al.2011] Fabian Pedregosa, Ga¨el Varo-
quaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel,
Peter Prettenhofer, Ron Weiss, Vincent Dubourg,
Scikit-learn: Machine learning
et al.
The Journal of Machine Learning
in Python.
Research, 12:2825–2830.

2011.

2010.

[ ˇReh˚uˇrek and Sojka2010] Radim ˇReh˚uˇrek and Petr So-
Software Framework for Topic
jka.
In Proceed-
Modelling with Large Corpora.
ings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Val-
letta, Malta, May. ELRA. http://is.muni.
cz/publication/884893/en.

[Socher et al.2013] Richard Socher, Alex Perelygin,
Jean Y Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Recur-
sive deep models for semantic compositionality over
a sentiment treebank. In Proceedings of the confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), volume 1631, page 1642.

Using Distributed Representations to Disambiguate Biomedical and
Clinical Concepts

St´ephan Tulkens(cid:70) and Simon ˇSuster(cid:70)♠ and Walter Daelemans(cid:70)

(cid:70)CLiPS
University of Antwerp
Prinsstraat 13
2000 Antwerpen
Belgium
{ﬁrstname.lastname}@uantwerpen.be

♠CLCG
University of Groningen
Oude Kijk in ’t Jatstr. 26
9700AS Groningen
The Netherlands

Abstract

In this paper, we report a knowledge-based
method for Word Sense Disambiguation
in the domains of biomedical and clini-
cal text. We combine word representa-
tions created on large corpora with a small
number of deﬁnitions from the UMLS to
create concept representations, which we
then compare to representations of the con-
text of ambiguous terms. Using no re-
lational information, we obtain compara-
ble performance to previous approaches
on the MSH-WSD dataset, which is a
well-known dataset in the biomedical do-
main. Additionally, our method is fast
and easy to set up and extend to other do-
mains. Supplementary materials, includ-
ing source code, can be found at https:
//github.com/clips/yarn

1

Introduction

Word Sense Disambiguation (WSD) is a procedure
in which an ambiguous term or concept is assigned
a single sense appropriate for that context, and is
an important step in the creation of a semantic rep-
resentation of a document (Ide and V´eronis, 1998).
While performing WSD will beneﬁt most natural
language processing applications, disambiguation
of concepts is a critical component of applications
operating on clinical and biomedical text, in which
the same word can denote differing concepts, and
may thus elicit radically different responses.

Compounding this problem of ambiguity is the
fact that clinical text, in general, is noisier than
other domains, and contains a large variety of ab-
breviations, some of which may be speciﬁc to a
single hospital or physician. Additionally, there is a
marked absence of large volumes of annotated clini-
cal text, even for English, which presents a problem

for supervised approaches to Word Sense Disam-
biguation. For other languages, such as Dutch,
there exist no freely available annotated corpora of
clinical text.

A ﬁrst step towards solving this problem could
be the use of distributed representations. Where a
more traditional word representation, such as a TF-
IDF bag-of-words (BoW) representation, carries
frequency information, distributed representations
encode semantic information. A big advantage to
using these representations is that they can be gen-
erated from large corpora of unlabeled text, and
can be trained on very large corpora in a reason-
able amount of time. These representations, es-
pecially when trained using neural architectures
such as word2vec (Mikolov et al., 2013), have
been shown to improve performance on a variety
of tasks when compared to more traditional BoW
representations.

We hypothesize that these kinds of distributional
representations are well-suited for WSD in the clin-
ical and biomedical domain because of the lack
of training data, and the large terminological va-
riety. We present a knowledge-based approach to
Word Sense Disambiguation which creates concept
representations by combining deﬁnitions from the
Uniﬁed Medical Language System (UMLS) with
distributed representations. We test our hypothesis
on the MSH-WSD, which is a well-known dataset
for WSD in the biomedical domain.

2 Related Research

All knowledge-based methods we review use
the Uniﬁed Medical Language System® (UMLS)
Metathesaurus® (Bodenreider, 2004) as a knowl-
edge base, possibly augmented with external
sources, such as MeSH®-indexed abstracts. Gen-
erally speaking, the UMLS contains two separate
information sources that are suitable for use in dis-

6
1
0
2
 
g
u
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
0
6
5
0
.
8
0
6
1
:
v
i
X
r
a

ambiguation: the concept unique identiﬁer (CUI),
which is a unique label for each concept, and the
semantic type (ST), which is a set of 135 broad
labels such as “Animal” or “Chemical”. In gen-
eral, a word is only considered disambiguated if
the correct CUI can be selected; hence, as McInnes
and Pedersen (2013) note, approaches based on se-
mantic types are not able to disambiguate between
approximately 12% of concepts, as some concepts
with the same surface form have an identical ST,
but a different CUI.

In terms of approaches using ST, Humphrey et
al. (2006) create one vector for each semantic type
by creating a BoW representation of all words that
denote that semantic type. For each ambiguous
term, a target word vector is created by taking a
window of words from the right and left of the term.
The concept which is associated with the ST with
the lowest cosine distance is then taken to be the
correct sense of the term. Similarly, Alexopoulou
et al. (2009) create a method which ﬁnds the closest
concept based on a combination of co-occurrence
with other semantic types and ontological similarity
through is-a relationships.

Closest to our approach is the machine read-
able dictionary (MRD) approach (McInnes, 2008;
Jimeno-Yepes et al., 2011), which uses deﬁnitions
from the UMLS to create concept vectors by creat-
ing BoW representations of concepts using all deﬁ-
nitions of the concept and those of related concepts.
This BoW representation contains TF-IDF values
where D is the number of concepts in which a word
appears, thereby reducing the inﬂuence of general
words which occur in many concepts. These rep-
resentations are then compared to the vectorized
contexts of the ambiguous terms using cosine dis-
tance. A reﬁnement of MRD, called second-order
co-occurrence MRD (2-MRD) (McInnes, 2008), re-
places each word in a deﬁnition by a vector which
contains TF-IDF values of co-occurrence counts,
thereby associating each word with a context.

McInnes

introduce
and Pedersen (2013)
UMLS::SenseRelate, an approach which is based
on Pedersen et al. (2004)’s WordNet::SenseRelate.
In this system, each possible sense for an ambigu-
ous term is assigned a distance-weighted score
based on the concepts of the terms surrounding it,
where the concepts of the surrounding terms are
determined using UMLS::Similarity (McInnes et
al., 2009).

Corpus size
Vocabulary
Dimension

Medline Mimic-III Bioasq
920,081 13,097,844 -
196,960 71,663
320

1,701,632
200

320

Table 1: The number of words in the corpus, the
resulting vocabulary size, and the dimension of the
resulting vectors.

called step models, which calculate the probabil-
ity of a word occurring with a certain concept by
considering the number of times a word occurs
in the deﬁnitions of that concept and its related
concepts. It then steps through the UMLS-deﬁned
ontology of concepts, and reﬁnes the probabilities
for each word and each concept based on the rela-
tions within the ontology.

Finally, Chen et al. (2014) present an approach
for general WSD which uses word embeddings
coupled with WordNet (Fellbaum, 1998) as a re-
source to perform sense disambiguation, and which
creates sense-speciﬁc word embeddings from these
sense-disambiguated word representations.

3 Materials

3.1 Test Corpus

We use the MSH-WSD corpus (Jimeno-Yepes et
al., 2011), which consists of a set of 203 ambigu-
ous terms, each associated with multiple concepts,
to evaluate our approach. Of the 203 terms in the
corpus, 106 are regular terms, 88 are acronyms, and
9 can be acronyms and regular terms. For each of
these concepts, up to 100 MeSH abstracts were re-
trieved, resulting in a set of 37,888 abstracts. In our
approach, all abstracts were pre-processed using
the tokenizer from the Pattern package (De Smedt
and Daelemans, 2012), and all stop words were
removed using the English stop word list from
scikit-learn (Pedregosa et al., 2011).

3.2 Word vectors

We evaluate our approach using three sets of vec-
tors: The ﬁrst set was trained on a small set of
Medline abstracts1, and a second set of vectors
created on the entirety of the MIMIC-III corpus
of clinical notes (Johnson et al., 2016). For both
sets, we used the word2vec implementation from
gensim ( ˇReh˚uˇrek and Sojka, 2010), using skip-
gram with negative sampling, a frequency cutoff

1The speciﬁc IDs of these abstracts are available in the

Jimeno-Yepes and Berlanga (2015) present so-

online appendix.

of 5 and a negative sampling of 15. Additionally,
we used a third set of vectors, available from the
BioASQ organisers2, which was trained on a much
larger set of Medline abstracts.3 The model statis-
tics are visualized in Table 1.

4 Approach

Similar to the 2-MRD approach detailed above, our
approach creates concept vectors by replacing each
word in every deﬁnition by the vector representa-
tion of that word. This creates an M × n matrix for
each deﬁnition, where M is the dimensionality of
the word vectors, and n the number of words con-
tained in that deﬁnition. Following this, for each
deﬁnition, we then obtain a single vector of dimen-
sionality M by applying a compositional function
to the matrix, thereby obtaining so-called deﬁnition
vectors, which represent the entire meaning of the
deﬁnition in one vector. Each concept can then be
represented by a M × d matrix, where d is the num-
ber of deﬁnitions that a concept has in the UMLS.
Finally, we apply a second composition function
to this matrix, thereby obtaining a single vector of
dimensionality M which represents the combined
meaning of all deﬁnitions for that concept, i.e. a
concept vector.

For each abstract in the test corpus, we ﬁrst lo-
cate each ambiguous term through a simple lookup.
For each located term in the abstract we create a
vector representation by retrieving all words in a
window of size w surrounding the ambiguous term,
and replacing the words by their vectors. Note
that this window does not include the ambiguous
term itself. These collections of vectors are then
combined into M -dimensional vectors using the
same composition function as above. This is done
separately for each term occurrence within a single
document, creating a M × x matrix, where x is
the number of times the ambiguous term occurs
in a single document. These are then combined
in an M -dimensional term vector using the same
composition we used for the concepts, above. A
schematic representation of our model is given in
Figure 1.

Because all concept and term vectors are created
using the same distributed vectors and composi-
tional functions, the vector space in which they are

2Available on the BioASQ website.
3While we concede that the BioASQ corpora might contain
abstracts from the MSH dataset, it does not contain any explicit
labeled information that might be used in disambiguation.

Figure 1: Our model represents a concept by re-
placing all words W in a deﬁnition D by their
vectors, and then composing these into a deﬁnition
vector with a function f (x). For each concept, all
deﬁnition vectors D are then composed into a con-
cept vector C using a second composition function
g(x).

placed is also comparable. Hence, for each am-
biguous word we encounter, we can use the cosine
distance between the abstract vector of the am-
biguous utterance and each possible sense of that
word to determine the correct sense. This makes
our approach very similar to the Lesk family of
approaches (Lesk, 1986).

In terms of composition function we experi-
mented with elementwise multiplication, averaging
and summation, all of which are unordered com-
positional functions (Mitchell and Lapata, 2008).
In addition, it is worth noting that there’s still a
lively debate whether ordered composition actually
leads to better results for estimating document-,
or sentence-level meaning, when compared to un-
ordered composition (Iyyer et al., 2015; Socher et
al., 2013).

5 Results

The accuracy scores obtained by our models using
the different word vectors are displayed in Table
2. med, mim and bio denote the vectors created
on the small Medline corpus, the Mimic-III corpus
and the BioASQ vectors, respectively. We consider
both a constrained and an unconstrained version
of the task. For each word, the constrained ver-
sion of the task only considers the senses present

med mim bio MRD 2-MRD 0-step

Accuracy C 0.80 0.69 0.84 0.81 0.78
Accuracy U 0.72 0.63 0.75 -

-

0.82
-

2-step
0.86
-

r-step UMLS::SenseRelate
0.89
-

0.75
-

Table 2: Results using constrained (C) and unconstrained (U) terms.

Term Accuracy

DE 0.31
Hemlock 0.4
0.46
0.46
0.47

Brucella Abortus
WT1
Murine Sarcoma Virus

Table 3: The 5 lowest-performing terms.

in the MSH-WSD dataset as possible targets. The
unconstrained version considers all concepts which
are denoted by the ambiguous term in the 2015AB
version of the UMLS as possible targets. The term
cortex, for example, only has 2 concepts asso-
ciated with it in the MSH-WSD dataset, while in
the 2015AB UMLS release it can denote 5 separate
concepts. Because the unconstrained version of the
task considers all words, it therefore gives a better
indication of real-life performance.

Accuracy C and U denote that the scores were ob-
tained in the constrained settings and unconstrained
setting, respectively. All reported scores use a win-
dow size of 6, which was optimized on a randomly
selected set of 20 terms from the MSH-WSD set.
Varying the window size had negligible results: all
window sizes over 6 had comparable results, and
increasing the window size over 30 causes a (small)
decline in results. This is in line with McInnes and
Pedersen (2013), who report a positive effect of
window size that quickly tapers off for window
sizes > 10. Concerning the composition functions,
summation and averaging as ﬁrst and second or-
der composition function worked best, while using
element-wise multiplication did not work well in
any case. Where possible, we display the self-
reported scores from the relevant papers on the
same dataset.

A ﬁrst thing to note is the large difference in
accuracy when changing the set of word repre-
sentations, especially the difference between the
Medline vectors and the vectors derived from the
Mimic-III corpus.
It is currently unclear what
causes these performance differences, although it
is likely that the small vocabulary, caused by the
noisiness of the clinical data in the MIMIC-III cor-

pus, reduces performance. Compared to previous
approaches, our approach outperforms the MRD,
2-MRD, and UMLS::SenseRelate approaches, but
does not manage to improve on the scores of the
step models. Recall, however, that the step models
largely rely on relationships in the UMLS ontology
to estimate concept relatedness.

To compare how our models improved when in-
cluding relation information, we also experimented
with adding deﬁnitions of related concepts, i.e. con-
cepts which had a sibling, parent or child rela-
tionship to each concept. In contrast to patterns
observed in earlier work, this did not have a sig-
niﬁcant, and often a detrimental, effect on perfor-
mance. Note that this makes our model entirely
independent of the actual UMLS hierarchy, and
more ﬂexible as a result, as we only use the map-
pings from deﬁnition to CUI for disambiguation,
and no other information, such as relations or se-
mantic type. In addition, our system is also fast:
on a consumer-grade laptop, our approach takes 10
seconds to vectorize and disambiguate all abstracts
in the MSH dataset, not taking into account the
time it takes to load the embeddings into memory.
Our approach obtains an accuracy of > 90% on
103 terms, showing that it is able to disambiguate
a large variety of terms. For some terms, how-
ever, the performance was below random guessing.
These are shown in Table 3. The pattern of er-
rors is quite clear: Our approach has trouble with
disambiguation if the deﬁnitions of the concepts
themselves are lexically very similar. As an exam-
ple, on the term Hemlock our approach performs
below chance level because one of the concepts
denotes a family of poisonous plants, while the
other reports a tree, also called hemlock, the de-
scription of which mentions that it is explicitly not
poisonous. We expect these kinds of problems to
be alleviated with the addition of more data.

6 Conclusion and future work

In this paper we presented a novel approach to
WSD in the biomedical domain which achieves
comparable performance to existing methods with-
out incorporating relational information from an

ontology. This makes the approach easily transfer-
able to other languages, for which such ontologies
might not exist, and to other domains. The large
variation in accuracy when changing sets of word
embeddings also raises interesting prospects for
improvement; better word representations will lead
to an improvement in our approach without modify-
ing the approach itself. Additionally, we would like
to experiment with different composition functions
for composing the deﬁnition and concept vectors.

Acknowledgments

Part of this research was carried out in the frame-
work of the Accumulate IWT SBO project, funded
by the government agency for Innovation by Sci-
ence and Technology (IWT). We would also like to
thank Elyne Scheurwegs for making the small set
of Medline abstract available to us.

References

[Alexopoulou et al.2009] Dimitra Alexopoulou, Bill
Andreopoulos, Heiko Dietze, Andreas Doms, Fa-
J¨org Hakenberg, Khaled Khelif,
bien Gandon,
Michael Schroeder, and Thomas W¨achter.
2009.
Biomedical word sense disambiguation with on-
tologies and metadata: automation meets accuracy.
BMC bioinformatics, 10(1):1.

[Bodenreider2004] Olivier Bodenreider.

2004. The
uniﬁed medical language system (UMLS): integrat-
ing biomedical terminology. Nucleic acids research,
32(suppl 1):D267–D270.

[Chen et al.2014] Xinxiong Chen, Zhiyuan Liu, and
A uniﬁed model for
In

Maosong Sun.
word sense representation and disambiguation.
EMNLP, pages 1025–1035. Citeseer.

2014.

[De Smedt and Daelemans2012] Tom De Smedt and
Walter Daelemans. 2012. Pattern for Python. The
Journal of Machine Learning Research, 13(1):2063–
2067.

[Fellbaum1998] Christiane Fellbaum. 1998. WordNet.

Wiley Online Library.

[Humphrey et al.2006] Susanne M Humphrey, Willie J
Rogers, Halil Kilicoglu, Dina Demner-Fushman,
and Thomas C Rindﬂesch. 2006. Word sense disam-
biguation by selecting the best semantic type based
on Journal Descriptor Indexing: Preliminary experi-
ment. Journal of the American Society for Informa-
tion Science and Technology, 57(1):96–113.

[Iyyer et al.2015] Mohit Iyyer, Varun Manjunatha, Jor-
dan Boyd-Graber, and Hal Daum´e III. 2015. Deep
unordered composition rivals syntactic methods for
text classiﬁcation. In Association for Computational
Linguistics.

[Jimeno-Yepes and Berlanga2015] Antonio

Jimeno-
Yepes and Rafael Berlanga.
2015. Knowledge
based word-concept model estimation and re-
Journal of
ﬁnement for biomedical text mining.
biomedical informatics, 53:300–307.

[Jimeno-Yepes et al.2011] Antonio

Jimeno-Yepes,
Bridget T McInnes, and Alan R Aronson. 2011.
Exploiting MeSH indexing in MEDLINE to gener-
ate a data set for word sense disambiguation. BMC
bioinformatics, 12(1):1.

J

[Johnson et al.2016] AEW Johnson, TJ Pollard, L Shen,
L Lehman, M Feng, M Ghassemi, B Moody,
P Szolovits, LA Celi, and RG Mark. 2016. MIMIC-
III, a freely accessible critical care database. Scien-
tiﬁc Data.

[Lesk1986] Michael Lesk.

1986. Automatic sense
disambiguation using machine readable dictionaries:
how to tell a pine cone from an ice cream cone. In
Proceedings of the 5th annual international confer-
ence on Systems documentation, pages 24–26. Asso-
ciation for Computing Machinery.

[McInnes and Pedersen2013] Bridget T McInnes and
Ted Pedersen. 2013. Evaluating measures of seman-
tic similarity and relatedness to disambiguate terms
in biomedical text. Journal of biomedical informat-
ics, 46(6):1116–1124.

[McInnes et al.2009] Bridget T McInnes, Ted Pedersen,
and Serguei VS Pakhomov. 2009. UMLS-Interface
and UMLS-Similarity: open source software for
measuring paths and semantic similarity. In AMIA
Annual Symposium Proceedings, volume 2009, page
431. American Medical Informatics Association.

[McInnes2008] Bridget T McInnes. 2008. An unsu-
pervised vector approach to biomedical term disam-
biguation: integrating UMLS and Medline. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technologies: Student Research Workshop,
pages 49–54. Association for Computational Lin-
guistics.

[Mikolov et al.2013] Tomas Mikolov,

Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in neural
and their compositionality.
information processing systems, pages 3111–3119.

[Ide and V´eronis1998] Nancy Ide and Jean V´eronis.
1998. Introduction to the special issue on word sense
disambiguation: the state of the art. Computational
linguistics, 24(1):2–40.

[Mitchell and Lapata2008] Jeff Mitchell and Mirella
Lapata. 2008. Vector-based models of semantic
composition. In Proceedings of the Association for
Computational Linguistics, pages 236–244.

[Pedersen et al.2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. WordNet:: Sim-
ilarity: measuring the relatedness of concepts.
In
Demonstration papers at HLT-NAACL 2004, pages
38–41. Association for Computational Linguistics.

[Pedregosa et al.2011] Fabian Pedregosa, Ga¨el Varo-
quaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel,
Peter Prettenhofer, Ron Weiss, Vincent Dubourg,
Scikit-learn: Machine learning
et al.
The Journal of Machine Learning
in Python.
Research, 12:2825–2830.

2011.

2010.

[ ˇReh˚uˇrek and Sojka2010] Radim ˇReh˚uˇrek and Petr So-
Software Framework for Topic
jka.
In Proceed-
Modelling with Large Corpora.
ings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Val-
letta, Malta, May. ELRA. http://is.muni.
cz/publication/884893/en.

[Socher et al.2013] Richard Socher, Alex Perelygin,
Jean Y Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Recur-
sive deep models for semantic compositionality over
a sentiment treebank. In Proceedings of the confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), volume 1631, page 1642.

Using Distributed Representations to Disambiguate Biomedical and
Clinical Concepts

St´ephan Tulkens(cid:70) and Simon ˇSuster(cid:70)♠ and Walter Daelemans(cid:70)

(cid:70)CLiPS
University of Antwerp
Prinsstraat 13
2000 Antwerpen
Belgium
{ﬁrstname.lastname}@uantwerpen.be

♠CLCG
University of Groningen
Oude Kijk in ’t Jatstr. 26
9700AS Groningen
The Netherlands

Abstract

In this paper, we report a knowledge-based
method for Word Sense Disambiguation
in the domains of biomedical and clini-
cal text. We combine word representa-
tions created on large corpora with a small
number of deﬁnitions from the UMLS to
create concept representations, which we
then compare to representations of the con-
text of ambiguous terms. Using no re-
lational information, we obtain compara-
ble performance to previous approaches
on the MSH-WSD dataset, which is a
well-known dataset in the biomedical do-
main. Additionally, our method is fast
and easy to set up and extend to other do-
mains. Supplementary materials, includ-
ing source code, can be found at https:
//github.com/clips/yarn

1

Introduction

Word Sense Disambiguation (WSD) is a procedure
in which an ambiguous term or concept is assigned
a single sense appropriate for that context, and is
an important step in the creation of a semantic rep-
resentation of a document (Ide and V´eronis, 1998).
While performing WSD will beneﬁt most natural
language processing applications, disambiguation
of concepts is a critical component of applications
operating on clinical and biomedical text, in which
the same word can denote differing concepts, and
may thus elicit radically different responses.

Compounding this problem of ambiguity is the
fact that clinical text, in general, is noisier than
other domains, and contains a large variety of ab-
breviations, some of which may be speciﬁc to a
single hospital or physician. Additionally, there is a
marked absence of large volumes of annotated clini-
cal text, even for English, which presents a problem

for supervised approaches to Word Sense Disam-
biguation. For other languages, such as Dutch,
there exist no freely available annotated corpora of
clinical text.

A ﬁrst step towards solving this problem could
be the use of distributed representations. Where a
more traditional word representation, such as a TF-
IDF bag-of-words (BoW) representation, carries
frequency information, distributed representations
encode semantic information. A big advantage to
using these representations is that they can be gen-
erated from large corpora of unlabeled text, and
can be trained on very large corpora in a reason-
able amount of time. These representations, es-
pecially when trained using neural architectures
such as word2vec (Mikolov et al., 2013), have
been shown to improve performance on a variety
of tasks when compared to more traditional BoW
representations.

We hypothesize that these kinds of distributional
representations are well-suited for WSD in the clin-
ical and biomedical domain because of the lack
of training data, and the large terminological va-
riety. We present a knowledge-based approach to
Word Sense Disambiguation which creates concept
representations by combining deﬁnitions from the
Uniﬁed Medical Language System (UMLS) with
distributed representations. We test our hypothesis
on the MSH-WSD, which is a well-known dataset
for WSD in the biomedical domain.

2 Related Research

All knowledge-based methods we review use
the Uniﬁed Medical Language System® (UMLS)
Metathesaurus® (Bodenreider, 2004) as a knowl-
edge base, possibly augmented with external
sources, such as MeSH®-indexed abstracts. Gen-
erally speaking, the UMLS contains two separate
information sources that are suitable for use in dis-

6
1
0
2
 
g
u
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
0
6
5
0
.
8
0
6
1
:
v
i
X
r
a

ambiguation: the concept unique identiﬁer (CUI),
which is a unique label for each concept, and the
semantic type (ST), which is a set of 135 broad
labels such as “Animal” or “Chemical”. In gen-
eral, a word is only considered disambiguated if
the correct CUI can be selected; hence, as McInnes
and Pedersen (2013) note, approaches based on se-
mantic types are not able to disambiguate between
approximately 12% of concepts, as some concepts
with the same surface form have an identical ST,
but a different CUI.

In terms of approaches using ST, Humphrey et
al. (2006) create one vector for each semantic type
by creating a BoW representation of all words that
denote that semantic type. For each ambiguous
term, a target word vector is created by taking a
window of words from the right and left of the term.
The concept which is associated with the ST with
the lowest cosine distance is then taken to be the
correct sense of the term. Similarly, Alexopoulou
et al. (2009) create a method which ﬁnds the closest
concept based on a combination of co-occurrence
with other semantic types and ontological similarity
through is-a relationships.

Closest to our approach is the machine read-
able dictionary (MRD) approach (McInnes, 2008;
Jimeno-Yepes et al., 2011), which uses deﬁnitions
from the UMLS to create concept vectors by creat-
ing BoW representations of concepts using all deﬁ-
nitions of the concept and those of related concepts.
This BoW representation contains TF-IDF values
where D is the number of concepts in which a word
appears, thereby reducing the inﬂuence of general
words which occur in many concepts. These rep-
resentations are then compared to the vectorized
contexts of the ambiguous terms using cosine dis-
tance. A reﬁnement of MRD, called second-order
co-occurrence MRD (2-MRD) (McInnes, 2008), re-
places each word in a deﬁnition by a vector which
contains TF-IDF values of co-occurrence counts,
thereby associating each word with a context.

McInnes

introduce
and Pedersen (2013)
UMLS::SenseRelate, an approach which is based
on Pedersen et al. (2004)’s WordNet::SenseRelate.
In this system, each possible sense for an ambigu-
ous term is assigned a distance-weighted score
based on the concepts of the terms surrounding it,
where the concepts of the surrounding terms are
determined using UMLS::Similarity (McInnes et
al., 2009).

Corpus size
Vocabulary
Dimension

Medline Mimic-III Bioasq
920,081 13,097,844 -
196,960 71,663
320

1,701,632
200

320

Table 1: The number of words in the corpus, the
resulting vocabulary size, and the dimension of the
resulting vectors.

called step models, which calculate the probabil-
ity of a word occurring with a certain concept by
considering the number of times a word occurs
in the deﬁnitions of that concept and its related
concepts. It then steps through the UMLS-deﬁned
ontology of concepts, and reﬁnes the probabilities
for each word and each concept based on the rela-
tions within the ontology.

Finally, Chen et al. (2014) present an approach
for general WSD which uses word embeddings
coupled with WordNet (Fellbaum, 1998) as a re-
source to perform sense disambiguation, and which
creates sense-speciﬁc word embeddings from these
sense-disambiguated word representations.

3 Materials

3.1 Test Corpus

We use the MSH-WSD corpus (Jimeno-Yepes et
al., 2011), which consists of a set of 203 ambigu-
ous terms, each associated with multiple concepts,
to evaluate our approach. Of the 203 terms in the
corpus, 106 are regular terms, 88 are acronyms, and
9 can be acronyms and regular terms. For each of
these concepts, up to 100 MeSH abstracts were re-
trieved, resulting in a set of 37,888 abstracts. In our
approach, all abstracts were pre-processed using
the tokenizer from the Pattern package (De Smedt
and Daelemans, 2012), and all stop words were
removed using the English stop word list from
scikit-learn (Pedregosa et al., 2011).

3.2 Word vectors

We evaluate our approach using three sets of vec-
tors: The ﬁrst set was trained on a small set of
Medline abstracts1, and a second set of vectors
created on the entirety of the MIMIC-III corpus
of clinical notes (Johnson et al., 2016). For both
sets, we used the word2vec implementation from
gensim ( ˇReh˚uˇrek and Sojka, 2010), using skip-
gram with negative sampling, a frequency cutoff

1The speciﬁc IDs of these abstracts are available in the

Jimeno-Yepes and Berlanga (2015) present so-

online appendix.

of 5 and a negative sampling of 15. Additionally,
we used a third set of vectors, available from the
BioASQ organisers2, which was trained on a much
larger set of Medline abstracts.3 The model statis-
tics are visualized in Table 1.

4 Approach

Similar to the 2-MRD approach detailed above, our
approach creates concept vectors by replacing each
word in every deﬁnition by the vector representa-
tion of that word. This creates an M × n matrix for
each deﬁnition, where M is the dimensionality of
the word vectors, and n the number of words con-
tained in that deﬁnition. Following this, for each
deﬁnition, we then obtain a single vector of dimen-
sionality M by applying a compositional function
to the matrix, thereby obtaining so-called deﬁnition
vectors, which represent the entire meaning of the
deﬁnition in one vector. Each concept can then be
represented by a M × d matrix, where d is the num-
ber of deﬁnitions that a concept has in the UMLS.
Finally, we apply a second composition function
to this matrix, thereby obtaining a single vector of
dimensionality M which represents the combined
meaning of all deﬁnitions for that concept, i.e. a
concept vector.

For each abstract in the test corpus, we ﬁrst lo-
cate each ambiguous term through a simple lookup.
For each located term in the abstract we create a
vector representation by retrieving all words in a
window of size w surrounding the ambiguous term,
and replacing the words by their vectors. Note
that this window does not include the ambiguous
term itself. These collections of vectors are then
combined into M -dimensional vectors using the
same composition function as above. This is done
separately for each term occurrence within a single
document, creating a M × x matrix, where x is
the number of times the ambiguous term occurs
in a single document. These are then combined
in an M -dimensional term vector using the same
composition we used for the concepts, above. A
schematic representation of our model is given in
Figure 1.

Because all concept and term vectors are created
using the same distributed vectors and composi-
tional functions, the vector space in which they are

2Available on the BioASQ website.
3While we concede that the BioASQ corpora might contain
abstracts from the MSH dataset, it does not contain any explicit
labeled information that might be used in disambiguation.

Figure 1: Our model represents a concept by re-
placing all words W in a deﬁnition D by their
vectors, and then composing these into a deﬁnition
vector with a function f (x). For each concept, all
deﬁnition vectors D are then composed into a con-
cept vector C using a second composition function
g(x).

placed is also comparable. Hence, for each am-
biguous word we encounter, we can use the cosine
distance between the abstract vector of the am-
biguous utterance and each possible sense of that
word to determine the correct sense. This makes
our approach very similar to the Lesk family of
approaches (Lesk, 1986).

In terms of composition function we experi-
mented with elementwise multiplication, averaging
and summation, all of which are unordered com-
positional functions (Mitchell and Lapata, 2008).
In addition, it is worth noting that there’s still a
lively debate whether ordered composition actually
leads to better results for estimating document-,
or sentence-level meaning, when compared to un-
ordered composition (Iyyer et al., 2015; Socher et
al., 2013).

5 Results

The accuracy scores obtained by our models using
the different word vectors are displayed in Table
2. med, mim and bio denote the vectors created
on the small Medline corpus, the Mimic-III corpus
and the BioASQ vectors, respectively. We consider
both a constrained and an unconstrained version
of the task. For each word, the constrained ver-
sion of the task only considers the senses present

med mim bio MRD 2-MRD 0-step

Accuracy C 0.80 0.69 0.84 0.81 0.78
Accuracy U 0.72 0.63 0.75 -

-

0.82
-

2-step
0.86
-

r-step UMLS::SenseRelate
0.89
-

0.75
-

Table 2: Results using constrained (C) and unconstrained (U) terms.

Term Accuracy

DE 0.31
Hemlock 0.4
0.46
0.46
0.47

Brucella Abortus
WT1
Murine Sarcoma Virus

Table 3: The 5 lowest-performing terms.

in the MSH-WSD dataset as possible targets. The
unconstrained version considers all concepts which
are denoted by the ambiguous term in the 2015AB
version of the UMLS as possible targets. The term
cortex, for example, only has 2 concepts asso-
ciated with it in the MSH-WSD dataset, while in
the 2015AB UMLS release it can denote 5 separate
concepts. Because the unconstrained version of the
task considers all words, it therefore gives a better
indication of real-life performance.

Accuracy C and U denote that the scores were ob-
tained in the constrained settings and unconstrained
setting, respectively. All reported scores use a win-
dow size of 6, which was optimized on a randomly
selected set of 20 terms from the MSH-WSD set.
Varying the window size had negligible results: all
window sizes over 6 had comparable results, and
increasing the window size over 30 causes a (small)
decline in results. This is in line with McInnes and
Pedersen (2013), who report a positive effect of
window size that quickly tapers off for window
sizes > 10. Concerning the composition functions,
summation and averaging as ﬁrst and second or-
der composition function worked best, while using
element-wise multiplication did not work well in
any case. Where possible, we display the self-
reported scores from the relevant papers on the
same dataset.

A ﬁrst thing to note is the large difference in
accuracy when changing the set of word repre-
sentations, especially the difference between the
Medline vectors and the vectors derived from the
Mimic-III corpus.
It is currently unclear what
causes these performance differences, although it
is likely that the small vocabulary, caused by the
noisiness of the clinical data in the MIMIC-III cor-

pus, reduces performance. Compared to previous
approaches, our approach outperforms the MRD,
2-MRD, and UMLS::SenseRelate approaches, but
does not manage to improve on the scores of the
step models. Recall, however, that the step models
largely rely on relationships in the UMLS ontology
to estimate concept relatedness.

To compare how our models improved when in-
cluding relation information, we also experimented
with adding deﬁnitions of related concepts, i.e. con-
cepts which had a sibling, parent or child rela-
tionship to each concept. In contrast to patterns
observed in earlier work, this did not have a sig-
niﬁcant, and often a detrimental, effect on perfor-
mance. Note that this makes our model entirely
independent of the actual UMLS hierarchy, and
more ﬂexible as a result, as we only use the map-
pings from deﬁnition to CUI for disambiguation,
and no other information, such as relations or se-
mantic type. In addition, our system is also fast:
on a consumer-grade laptop, our approach takes 10
seconds to vectorize and disambiguate all abstracts
in the MSH dataset, not taking into account the
time it takes to load the embeddings into memory.
Our approach obtains an accuracy of > 90% on
103 terms, showing that it is able to disambiguate
a large variety of terms. For some terms, how-
ever, the performance was below random guessing.
These are shown in Table 3. The pattern of er-
rors is quite clear: Our approach has trouble with
disambiguation if the deﬁnitions of the concepts
themselves are lexically very similar. As an exam-
ple, on the term Hemlock our approach performs
below chance level because one of the concepts
denotes a family of poisonous plants, while the
other reports a tree, also called hemlock, the de-
scription of which mentions that it is explicitly not
poisonous. We expect these kinds of problems to
be alleviated with the addition of more data.

6 Conclusion and future work

In this paper we presented a novel approach to
WSD in the biomedical domain which achieves
comparable performance to existing methods with-
out incorporating relational information from an

ontology. This makes the approach easily transfer-
able to other languages, for which such ontologies
might not exist, and to other domains. The large
variation in accuracy when changing sets of word
embeddings also raises interesting prospects for
improvement; better word representations will lead
to an improvement in our approach without modify-
ing the approach itself. Additionally, we would like
to experiment with different composition functions
for composing the deﬁnition and concept vectors.

Acknowledgments

Part of this research was carried out in the frame-
work of the Accumulate IWT SBO project, funded
by the government agency for Innovation by Sci-
ence and Technology (IWT). We would also like to
thank Elyne Scheurwegs for making the small set
of Medline abstract available to us.

References

[Alexopoulou et al.2009] Dimitra Alexopoulou, Bill
Andreopoulos, Heiko Dietze, Andreas Doms, Fa-
J¨org Hakenberg, Khaled Khelif,
bien Gandon,
Michael Schroeder, and Thomas W¨achter.
2009.
Biomedical word sense disambiguation with on-
tologies and metadata: automation meets accuracy.
BMC bioinformatics, 10(1):1.

[Bodenreider2004] Olivier Bodenreider.

2004. The
uniﬁed medical language system (UMLS): integrat-
ing biomedical terminology. Nucleic acids research,
32(suppl 1):D267–D270.

[Chen et al.2014] Xinxiong Chen, Zhiyuan Liu, and
A uniﬁed model for
In

Maosong Sun.
word sense representation and disambiguation.
EMNLP, pages 1025–1035. Citeseer.

2014.

[De Smedt and Daelemans2012] Tom De Smedt and
Walter Daelemans. 2012. Pattern for Python. The
Journal of Machine Learning Research, 13(1):2063–
2067.

[Fellbaum1998] Christiane Fellbaum. 1998. WordNet.

Wiley Online Library.

[Humphrey et al.2006] Susanne M Humphrey, Willie J
Rogers, Halil Kilicoglu, Dina Demner-Fushman,
and Thomas C Rindﬂesch. 2006. Word sense disam-
biguation by selecting the best semantic type based
on Journal Descriptor Indexing: Preliminary experi-
ment. Journal of the American Society for Informa-
tion Science and Technology, 57(1):96–113.

[Iyyer et al.2015] Mohit Iyyer, Varun Manjunatha, Jor-
dan Boyd-Graber, and Hal Daum´e III. 2015. Deep
unordered composition rivals syntactic methods for
text classiﬁcation. In Association for Computational
Linguistics.

[Jimeno-Yepes and Berlanga2015] Antonio

Jimeno-
Yepes and Rafael Berlanga.
2015. Knowledge
based word-concept model estimation and re-
Journal of
ﬁnement for biomedical text mining.
biomedical informatics, 53:300–307.

[Jimeno-Yepes et al.2011] Antonio

Jimeno-Yepes,
Bridget T McInnes, and Alan R Aronson. 2011.
Exploiting MeSH indexing in MEDLINE to gener-
ate a data set for word sense disambiguation. BMC
bioinformatics, 12(1):1.

J

[Johnson et al.2016] AEW Johnson, TJ Pollard, L Shen,
L Lehman, M Feng, M Ghassemi, B Moody,
P Szolovits, LA Celi, and RG Mark. 2016. MIMIC-
III, a freely accessible critical care database. Scien-
tiﬁc Data.

[Lesk1986] Michael Lesk.

1986. Automatic sense
disambiguation using machine readable dictionaries:
how to tell a pine cone from an ice cream cone. In
Proceedings of the 5th annual international confer-
ence on Systems documentation, pages 24–26. Asso-
ciation for Computing Machinery.

[McInnes and Pedersen2013] Bridget T McInnes and
Ted Pedersen. 2013. Evaluating measures of seman-
tic similarity and relatedness to disambiguate terms
in biomedical text. Journal of biomedical informat-
ics, 46(6):1116–1124.

[McInnes et al.2009] Bridget T McInnes, Ted Pedersen,
and Serguei VS Pakhomov. 2009. UMLS-Interface
and UMLS-Similarity: open source software for
measuring paths and semantic similarity. In AMIA
Annual Symposium Proceedings, volume 2009, page
431. American Medical Informatics Association.

[McInnes2008] Bridget T McInnes. 2008. An unsu-
pervised vector approach to biomedical term disam-
biguation: integrating UMLS and Medline. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technologies: Student Research Workshop,
pages 49–54. Association for Computational Lin-
guistics.

[Mikolov et al.2013] Tomas Mikolov,

Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in neural
and their compositionality.
information processing systems, pages 3111–3119.

[Ide and V´eronis1998] Nancy Ide and Jean V´eronis.
1998. Introduction to the special issue on word sense
disambiguation: the state of the art. Computational
linguistics, 24(1):2–40.

[Mitchell and Lapata2008] Jeff Mitchell and Mirella
Lapata. 2008. Vector-based models of semantic
composition. In Proceedings of the Association for
Computational Linguistics, pages 236–244.

[Pedersen et al.2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. WordNet:: Sim-
ilarity: measuring the relatedness of concepts.
In
Demonstration papers at HLT-NAACL 2004, pages
38–41. Association for Computational Linguistics.

[Pedregosa et al.2011] Fabian Pedregosa, Ga¨el Varo-
quaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel,
Peter Prettenhofer, Ron Weiss, Vincent Dubourg,
Scikit-learn: Machine learning
et al.
The Journal of Machine Learning
in Python.
Research, 12:2825–2830.

2011.

2010.

[ ˇReh˚uˇrek and Sojka2010] Radim ˇReh˚uˇrek and Petr So-
Software Framework for Topic
jka.
In Proceed-
Modelling with Large Corpora.
ings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Val-
letta, Malta, May. ELRA. http://is.muni.
cz/publication/884893/en.

[Socher et al.2013] Richard Socher, Alex Perelygin,
Jean Y Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Recur-
sive deep models for semantic compositionality over
a sentiment treebank. In Proceedings of the confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), volume 1631, page 1642.

Using Distributed Representations to Disambiguate Biomedical and
Clinical Concepts

St´ephan Tulkens(cid:70) and Simon ˇSuster(cid:70)♠ and Walter Daelemans(cid:70)

(cid:70)CLiPS
University of Antwerp
Prinsstraat 13
2000 Antwerpen
Belgium
{ﬁrstname.lastname}@uantwerpen.be

♠CLCG
University of Groningen
Oude Kijk in ’t Jatstr. 26
9700AS Groningen
The Netherlands

Abstract

In this paper, we report a knowledge-based
method for Word Sense Disambiguation
in the domains of biomedical and clini-
cal text. We combine word representa-
tions created on large corpora with a small
number of deﬁnitions from the UMLS to
create concept representations, which we
then compare to representations of the con-
text of ambiguous terms. Using no re-
lational information, we obtain compara-
ble performance to previous approaches
on the MSH-WSD dataset, which is a
well-known dataset in the biomedical do-
main. Additionally, our method is fast
and easy to set up and extend to other do-
mains. Supplementary materials, includ-
ing source code, can be found at https:
//github.com/clips/yarn

1

Introduction

Word Sense Disambiguation (WSD) is a procedure
in which an ambiguous term or concept is assigned
a single sense appropriate for that context, and is
an important step in the creation of a semantic rep-
resentation of a document (Ide and V´eronis, 1998).
While performing WSD will beneﬁt most natural
language processing applications, disambiguation
of concepts is a critical component of applications
operating on clinical and biomedical text, in which
the same word can denote differing concepts, and
may thus elicit radically different responses.

Compounding this problem of ambiguity is the
fact that clinical text, in general, is noisier than
other domains, and contains a large variety of ab-
breviations, some of which may be speciﬁc to a
single hospital or physician. Additionally, there is a
marked absence of large volumes of annotated clini-
cal text, even for English, which presents a problem

for supervised approaches to Word Sense Disam-
biguation. For other languages, such as Dutch,
there exist no freely available annotated corpora of
clinical text.

A ﬁrst step towards solving this problem could
be the use of distributed representations. Where a
more traditional word representation, such as a TF-
IDF bag-of-words (BoW) representation, carries
frequency information, distributed representations
encode semantic information. A big advantage to
using these representations is that they can be gen-
erated from large corpora of unlabeled text, and
can be trained on very large corpora in a reason-
able amount of time. These representations, es-
pecially when trained using neural architectures
such as word2vec (Mikolov et al., 2013), have
been shown to improve performance on a variety
of tasks when compared to more traditional BoW
representations.

We hypothesize that these kinds of distributional
representations are well-suited for WSD in the clin-
ical and biomedical domain because of the lack
of training data, and the large terminological va-
riety. We present a knowledge-based approach to
Word Sense Disambiguation which creates concept
representations by combining deﬁnitions from the
Uniﬁed Medical Language System (UMLS) with
distributed representations. We test our hypothesis
on the MSH-WSD, which is a well-known dataset
for WSD in the biomedical domain.

2 Related Research

All knowledge-based methods we review use
the Uniﬁed Medical Language System® (UMLS)
Metathesaurus® (Bodenreider, 2004) as a knowl-
edge base, possibly augmented with external
sources, such as MeSH®-indexed abstracts. Gen-
erally speaking, the UMLS contains two separate
information sources that are suitable for use in dis-

6
1
0
2
 
g
u
A
 
9
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
0
6
5
0
.
8
0
6
1
:
v
i
X
r
a

ambiguation: the concept unique identiﬁer (CUI),
which is a unique label for each concept, and the
semantic type (ST), which is a set of 135 broad
labels such as “Animal” or “Chemical”. In gen-
eral, a word is only considered disambiguated if
the correct CUI can be selected; hence, as McInnes
and Pedersen (2013) note, approaches based on se-
mantic types are not able to disambiguate between
approximately 12% of concepts, as some concepts
with the same surface form have an identical ST,
but a different CUI.

In terms of approaches using ST, Humphrey et
al. (2006) create one vector for each semantic type
by creating a BoW representation of all words that
denote that semantic type. For each ambiguous
term, a target word vector is created by taking a
window of words from the right and left of the term.
The concept which is associated with the ST with
the lowest cosine distance is then taken to be the
correct sense of the term. Similarly, Alexopoulou
et al. (2009) create a method which ﬁnds the closest
concept based on a combination of co-occurrence
with other semantic types and ontological similarity
through is-a relationships.

Closest to our approach is the machine read-
able dictionary (MRD) approach (McInnes, 2008;
Jimeno-Yepes et al., 2011), which uses deﬁnitions
from the UMLS to create concept vectors by creat-
ing BoW representations of concepts using all deﬁ-
nitions of the concept and those of related concepts.
This BoW representation contains TF-IDF values
where D is the number of concepts in which a word
appears, thereby reducing the inﬂuence of general
words which occur in many concepts. These rep-
resentations are then compared to the vectorized
contexts of the ambiguous terms using cosine dis-
tance. A reﬁnement of MRD, called second-order
co-occurrence MRD (2-MRD) (McInnes, 2008), re-
places each word in a deﬁnition by a vector which
contains TF-IDF values of co-occurrence counts,
thereby associating each word with a context.

McInnes

introduce
and Pedersen (2013)
UMLS::SenseRelate, an approach which is based
on Pedersen et al. (2004)’s WordNet::SenseRelate.
In this system, each possible sense for an ambigu-
ous term is assigned a distance-weighted score
based on the concepts of the terms surrounding it,
where the concepts of the surrounding terms are
determined using UMLS::Similarity (McInnes et
al., 2009).

Corpus size
Vocabulary
Dimension

Medline Mimic-III Bioasq
920,081 13,097,844 -
196,960 71,663
320

1,701,632
200

320

Table 1: The number of words in the corpus, the
resulting vocabulary size, and the dimension of the
resulting vectors.

called step models, which calculate the probabil-
ity of a word occurring with a certain concept by
considering the number of times a word occurs
in the deﬁnitions of that concept and its related
concepts. It then steps through the UMLS-deﬁned
ontology of concepts, and reﬁnes the probabilities
for each word and each concept based on the rela-
tions within the ontology.

Finally, Chen et al. (2014) present an approach
for general WSD which uses word embeddings
coupled with WordNet (Fellbaum, 1998) as a re-
source to perform sense disambiguation, and which
creates sense-speciﬁc word embeddings from these
sense-disambiguated word representations.

3 Materials

3.1 Test Corpus

We use the MSH-WSD corpus (Jimeno-Yepes et
al., 2011), which consists of a set of 203 ambigu-
ous terms, each associated with multiple concepts,
to evaluate our approach. Of the 203 terms in the
corpus, 106 are regular terms, 88 are acronyms, and
9 can be acronyms and regular terms. For each of
these concepts, up to 100 MeSH abstracts were re-
trieved, resulting in a set of 37,888 abstracts. In our
approach, all abstracts were pre-processed using
the tokenizer from the Pattern package (De Smedt
and Daelemans, 2012), and all stop words were
removed using the English stop word list from
scikit-learn (Pedregosa et al., 2011).

3.2 Word vectors

We evaluate our approach using three sets of vec-
tors: The ﬁrst set was trained on a small set of
Medline abstracts1, and a second set of vectors
created on the entirety of the MIMIC-III corpus
of clinical notes (Johnson et al., 2016). For both
sets, we used the word2vec implementation from
gensim ( ˇReh˚uˇrek and Sojka, 2010), using skip-
gram with negative sampling, a frequency cutoff

1The speciﬁc IDs of these abstracts are available in the

Jimeno-Yepes and Berlanga (2015) present so-

online appendix.

of 5 and a negative sampling of 15. Additionally,
we used a third set of vectors, available from the
BioASQ organisers2, which was trained on a much
larger set of Medline abstracts.3 The model statis-
tics are visualized in Table 1.

4 Approach

Similar to the 2-MRD approach detailed above, our
approach creates concept vectors by replacing each
word in every deﬁnition by the vector representa-
tion of that word. This creates an M × n matrix for
each deﬁnition, where M is the dimensionality of
the word vectors, and n the number of words con-
tained in that deﬁnition. Following this, for each
deﬁnition, we then obtain a single vector of dimen-
sionality M by applying a compositional function
to the matrix, thereby obtaining so-called deﬁnition
vectors, which represent the entire meaning of the
deﬁnition in one vector. Each concept can then be
represented by a M × d matrix, where d is the num-
ber of deﬁnitions that a concept has in the UMLS.
Finally, we apply a second composition function
to this matrix, thereby obtaining a single vector of
dimensionality M which represents the combined
meaning of all deﬁnitions for that concept, i.e. a
concept vector.

For each abstract in the test corpus, we ﬁrst lo-
cate each ambiguous term through a simple lookup.
For each located term in the abstract we create a
vector representation by retrieving all words in a
window of size w surrounding the ambiguous term,
and replacing the words by their vectors. Note
that this window does not include the ambiguous
term itself. These collections of vectors are then
combined into M -dimensional vectors using the
same composition function as above. This is done
separately for each term occurrence within a single
document, creating a M × x matrix, where x is
the number of times the ambiguous term occurs
in a single document. These are then combined
in an M -dimensional term vector using the same
composition we used for the concepts, above. A
schematic representation of our model is given in
Figure 1.

Because all concept and term vectors are created
using the same distributed vectors and composi-
tional functions, the vector space in which they are

2Available on the BioASQ website.
3While we concede that the BioASQ corpora might contain
abstracts from the MSH dataset, it does not contain any explicit
labeled information that might be used in disambiguation.

Figure 1: Our model represents a concept by re-
placing all words W in a deﬁnition D by their
vectors, and then composing these into a deﬁnition
vector with a function f (x). For each concept, all
deﬁnition vectors D are then composed into a con-
cept vector C using a second composition function
g(x).

placed is also comparable. Hence, for each am-
biguous word we encounter, we can use the cosine
distance between the abstract vector of the am-
biguous utterance and each possible sense of that
word to determine the correct sense. This makes
our approach very similar to the Lesk family of
approaches (Lesk, 1986).

In terms of composition function we experi-
mented with elementwise multiplication, averaging
and summation, all of which are unordered com-
positional functions (Mitchell and Lapata, 2008).
In addition, it is worth noting that there’s still a
lively debate whether ordered composition actually
leads to better results for estimating document-,
or sentence-level meaning, when compared to un-
ordered composition (Iyyer et al., 2015; Socher et
al., 2013).

5 Results

The accuracy scores obtained by our models using
the different word vectors are displayed in Table
2. med, mim and bio denote the vectors created
on the small Medline corpus, the Mimic-III corpus
and the BioASQ vectors, respectively. We consider
both a constrained and an unconstrained version
of the task. For each word, the constrained ver-
sion of the task only considers the senses present

med mim bio MRD 2-MRD 0-step

Accuracy C 0.80 0.69 0.84 0.81 0.78
Accuracy U 0.72 0.63 0.75 -

-

0.82
-

2-step
0.86
-

r-step UMLS::SenseRelate
0.89
-

0.75
-

Table 2: Results using constrained (C) and unconstrained (U) terms.

Term Accuracy

DE 0.31
Hemlock 0.4
0.46
0.46
0.47

Brucella Abortus
WT1
Murine Sarcoma Virus

Table 3: The 5 lowest-performing terms.

in the MSH-WSD dataset as possible targets. The
unconstrained version considers all concepts which
are denoted by the ambiguous term in the 2015AB
version of the UMLS as possible targets. The term
cortex, for example, only has 2 concepts asso-
ciated with it in the MSH-WSD dataset, while in
the 2015AB UMLS release it can denote 5 separate
concepts. Because the unconstrained version of the
task considers all words, it therefore gives a better
indication of real-life performance.

Accuracy C and U denote that the scores were ob-
tained in the constrained settings and unconstrained
setting, respectively. All reported scores use a win-
dow size of 6, which was optimized on a randomly
selected set of 20 terms from the MSH-WSD set.
Varying the window size had negligible results: all
window sizes over 6 had comparable results, and
increasing the window size over 30 causes a (small)
decline in results. This is in line with McInnes and
Pedersen (2013), who report a positive effect of
window size that quickly tapers off for window
sizes > 10. Concerning the composition functions,
summation and averaging as ﬁrst and second or-
der composition function worked best, while using
element-wise multiplication did not work well in
any case. Where possible, we display the self-
reported scores from the relevant papers on the
same dataset.

A ﬁrst thing to note is the large difference in
accuracy when changing the set of word repre-
sentations, especially the difference between the
Medline vectors and the vectors derived from the
Mimic-III corpus.
It is currently unclear what
causes these performance differences, although it
is likely that the small vocabulary, caused by the
noisiness of the clinical data in the MIMIC-III cor-

pus, reduces performance. Compared to previous
approaches, our approach outperforms the MRD,
2-MRD, and UMLS::SenseRelate approaches, but
does not manage to improve on the scores of the
step models. Recall, however, that the step models
largely rely on relationships in the UMLS ontology
to estimate concept relatedness.

To compare how our models improved when in-
cluding relation information, we also experimented
with adding deﬁnitions of related concepts, i.e. con-
cepts which had a sibling, parent or child rela-
tionship to each concept. In contrast to patterns
observed in earlier work, this did not have a sig-
niﬁcant, and often a detrimental, effect on perfor-
mance. Note that this makes our model entirely
independent of the actual UMLS hierarchy, and
more ﬂexible as a result, as we only use the map-
pings from deﬁnition to CUI for disambiguation,
and no other information, such as relations or se-
mantic type. In addition, our system is also fast:
on a consumer-grade laptop, our approach takes 10
seconds to vectorize and disambiguate all abstracts
in the MSH dataset, not taking into account the
time it takes to load the embeddings into memory.
Our approach obtains an accuracy of > 90% on
103 terms, showing that it is able to disambiguate
a large variety of terms. For some terms, how-
ever, the performance was below random guessing.
These are shown in Table 3. The pattern of er-
rors is quite clear: Our approach has trouble with
disambiguation if the deﬁnitions of the concepts
themselves are lexically very similar. As an exam-
ple, on the term Hemlock our approach performs
below chance level because one of the concepts
denotes a family of poisonous plants, while the
other reports a tree, also called hemlock, the de-
scription of which mentions that it is explicitly not
poisonous. We expect these kinds of problems to
be alleviated with the addition of more data.

6 Conclusion and future work

In this paper we presented a novel approach to
WSD in the biomedical domain which achieves
comparable performance to existing methods with-
out incorporating relational information from an

ontology. This makes the approach easily transfer-
able to other languages, for which such ontologies
might not exist, and to other domains. The large
variation in accuracy when changing sets of word
embeddings also raises interesting prospects for
improvement; better word representations will lead
to an improvement in our approach without modify-
ing the approach itself. Additionally, we would like
to experiment with different composition functions
for composing the deﬁnition and concept vectors.

Acknowledgments

Part of this research was carried out in the frame-
work of the Accumulate IWT SBO project, funded
by the government agency for Innovation by Sci-
ence and Technology (IWT). We would also like to
thank Elyne Scheurwegs for making the small set
of Medline abstract available to us.

References

[Alexopoulou et al.2009] Dimitra Alexopoulou, Bill
Andreopoulos, Heiko Dietze, Andreas Doms, Fa-
J¨org Hakenberg, Khaled Khelif,
bien Gandon,
Michael Schroeder, and Thomas W¨achter.
2009.
Biomedical word sense disambiguation with on-
tologies and metadata: automation meets accuracy.
BMC bioinformatics, 10(1):1.

[Bodenreider2004] Olivier Bodenreider.

2004. The
uniﬁed medical language system (UMLS): integrat-
ing biomedical terminology. Nucleic acids research,
32(suppl 1):D267–D270.

[Chen et al.2014] Xinxiong Chen, Zhiyuan Liu, and
A uniﬁed model for
In

Maosong Sun.
word sense representation and disambiguation.
EMNLP, pages 1025–1035. Citeseer.

2014.

[De Smedt and Daelemans2012] Tom De Smedt and
Walter Daelemans. 2012. Pattern for Python. The
Journal of Machine Learning Research, 13(1):2063–
2067.

[Fellbaum1998] Christiane Fellbaum. 1998. WordNet.

Wiley Online Library.

[Humphrey et al.2006] Susanne M Humphrey, Willie J
Rogers, Halil Kilicoglu, Dina Demner-Fushman,
and Thomas C Rindﬂesch. 2006. Word sense disam-
biguation by selecting the best semantic type based
on Journal Descriptor Indexing: Preliminary experi-
ment. Journal of the American Society for Informa-
tion Science and Technology, 57(1):96–113.

[Iyyer et al.2015] Mohit Iyyer, Varun Manjunatha, Jor-
dan Boyd-Graber, and Hal Daum´e III. 2015. Deep
unordered composition rivals syntactic methods for
text classiﬁcation. In Association for Computational
Linguistics.

[Jimeno-Yepes and Berlanga2015] Antonio

Jimeno-
Yepes and Rafael Berlanga.
2015. Knowledge
based word-concept model estimation and re-
Journal of
ﬁnement for biomedical text mining.
biomedical informatics, 53:300–307.

[Jimeno-Yepes et al.2011] Antonio

Jimeno-Yepes,
Bridget T McInnes, and Alan R Aronson. 2011.
Exploiting MeSH indexing in MEDLINE to gener-
ate a data set for word sense disambiguation. BMC
bioinformatics, 12(1):1.

J

[Johnson et al.2016] AEW Johnson, TJ Pollard, L Shen,
L Lehman, M Feng, M Ghassemi, B Moody,
P Szolovits, LA Celi, and RG Mark. 2016. MIMIC-
III, a freely accessible critical care database. Scien-
tiﬁc Data.

[Lesk1986] Michael Lesk.

1986. Automatic sense
disambiguation using machine readable dictionaries:
how to tell a pine cone from an ice cream cone. In
Proceedings of the 5th annual international confer-
ence on Systems documentation, pages 24–26. Asso-
ciation for Computing Machinery.

[McInnes and Pedersen2013] Bridget T McInnes and
Ted Pedersen. 2013. Evaluating measures of seman-
tic similarity and relatedness to disambiguate terms
in biomedical text. Journal of biomedical informat-
ics, 46(6):1116–1124.

[McInnes et al.2009] Bridget T McInnes, Ted Pedersen,
and Serguei VS Pakhomov. 2009. UMLS-Interface
and UMLS-Similarity: open source software for
measuring paths and semantic similarity. In AMIA
Annual Symposium Proceedings, volume 2009, page
431. American Medical Informatics Association.

[McInnes2008] Bridget T McInnes. 2008. An unsu-
pervised vector approach to biomedical term disam-
biguation: integrating UMLS and Medline. In Pro-
ceedings of the 46th Annual Meeting of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technologies: Student Research Workshop,
pages 49–54. Association for Computational Lin-
guistics.

[Mikolov et al.2013] Tomas Mikolov,

Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in neural
and their compositionality.
information processing systems, pages 3111–3119.

[Ide and V´eronis1998] Nancy Ide and Jean V´eronis.
1998. Introduction to the special issue on word sense
disambiguation: the state of the art. Computational
linguistics, 24(1):2–40.

[Mitchell and Lapata2008] Jeff Mitchell and Mirella
Lapata. 2008. Vector-based models of semantic
composition. In Proceedings of the Association for
Computational Linguistics, pages 236–244.

[Pedersen et al.2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. WordNet:: Sim-
ilarity: measuring the relatedness of concepts.
In
Demonstration papers at HLT-NAACL 2004, pages
38–41. Association for Computational Linguistics.

[Pedregosa et al.2011] Fabian Pedregosa, Ga¨el Varo-
quaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel,
Peter Prettenhofer, Ron Weiss, Vincent Dubourg,
Scikit-learn: Machine learning
et al.
The Journal of Machine Learning
in Python.
Research, 12:2825–2830.

2011.

2010.

[ ˇReh˚uˇrek and Sojka2010] Radim ˇReh˚uˇrek and Petr So-
Software Framework for Topic
jka.
In Proceed-
Modelling with Large Corpora.
ings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Val-
letta, Malta, May. ELRA. http://is.muni.
cz/publication/884893/en.

[Socher et al.2013] Richard Socher, Alex Perelygin,
Jean Y Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Recur-
sive deep models for semantic compositionality over
a sentiment treebank. In Proceedings of the confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), volume 1631, page 1642.

