Kernalised Multi-resolution Convnet for Visual Tracking

Di Wu, Wenbin Zou∗, Xia Li
Shenzhen University†
dwu, wzou, lixia@szu.edu.cn

Yong Zhao
Peking University Shenzhen Graduate School
yongzhao@pkusz.edu.cn

7
1
0
2
 
g
u
A
 
2
 
 
]

V
C
.
s
c
[
 
 
1
v
7
7
5
0
0
.
8
0
7
1
:
v
i
X
r
a

Abstract

Visual tracking is intrinsically a temporal problem. Dis-
criminative Correlation Filters (DCF) have demonstrated
excellent performance for high-speed generic visual object
tracking. Built upon their seminal work, there has been a
plethora of recent improvements relying on convolutional
neural network (CNN) pretrained on ImageNet as a feature
extractor for visual tracking. However, most of their works
relying on ad hoc analysis to design the weights for different
layers either using boosting or hedging techniques as an en-
semble tracker. In this paper, we go beyond the conventional
DCF framework and propose a Kernalised Multi-resolution
Convnet (KMC) formulation that utilises hierarchical re-
sponse maps to directly output the target movement. When
directly deployed the learnt network to predict the unseen
challenging UAV tracking dataset without any weight ad-
justment, the proposed model consistently achieves excel-
lent tracking performance. Moreover, the transfered multi-
reslution CNN renders it possible to be integrated into the
RNN temporal learning framework, therefore opening the
door on the end-to-end temporal deep learning (TDL) for
visual tracking.

1. Introduction

Visual tracking is the task of predicting the trajectory of a
target in a video. The task of tracking, a crucial component
of many computer vision systems, can be naturally speciﬁed
as an online learning problem. This paper focuses on the
challenging problem of monocular, generic, realistic object
tracking.

Visual object tracking has recently witnessed substantial
progress due to powerful features extracted using deep con-
volutional neural networks. For online applications, one
simple approach to transfer ofﬂine pre-trained CNN fea-
tures is to add one or more randomly initialised CNN layers,
named as adaptation layers, on top of the pre-trained CNN

∗Corresponding author.
†Shenzhen Key Lab of Advanced Telecommunication and Information

Processing, College of Information Engineering, Shenzhen University.

Figure 1: Response maps generated from hierarchical deep fea-
tures. Left: two frames from the “Bolt” sequence, red boxes are
image patches with padding added. Right: hierarchical response
maps from correlation ﬁlters (response maps have been resized
and rescaled to the same shape and scale). It can be seen that be-
cause different layers of deep features encode different semantics,
the maximum response points from different layers also vary.

model. However, as [26] empirically observe that this trans-
fer learning method suffers from severe over-ﬁtting because
of the limited training data. It is also observed in [29, 30]
that the differences in mean activations in intermediate lay-
ers have noticeable effect for multi-model fusion. The on-
line learnt parameters mainly focus on recent training sam-
ples and are less likely to be well generalised to both histor-
ical and future samples. This phenomenon can be fatal to
online visual tracking where the target often undergoes sig-
niﬁcant appearance changes or heavy occlusion. CNNs also
have shown impressive performance as an ofﬂine feature ex-
tractor for tracking [27, 18] in lieu of traditional handcrafted
features (e.g., HOG, Color Moment). Features from these
deep convolutional layers are discriminative while preserv-
ing spatial and structural information.

The marriage between DCF [9], which has the advan-
tage of being efﬁcient in training translational images in
the fourier space, and deep features, which excel at image
representation, further advances the visual tracking com-
munity [21, 7]. However, there are some conﬂicting con-

1

clusions concerning the representation power from differ-
ent layers of CNN: shallow convolutional layers have been
highlighted in the DCF-based methods [5] whereas [21]
found that the performance generally increases as layer
depth is increased hence only convolutional layers deeper
than 10th is used. Fig. 1 shows the response maps gener-
ated from hierarchical deep features. Since different layers
from convnet capture different semantic meanings, maxi-
mum translational locations obtained via different convnet
layers could vary: low level layers excel at discriminating
intra-class variations whereas high level layers excel at dis-
criminating inter-class variations. Hence, for generic object
tracking, it is imperative to combine features from different
layers to have a generalised representation from hierarchies
of response maps.

Visual tracking is intrinsically also a temporal problem,
but many previous approaches [1, 26] mainly focus on the
design of a robust appearance model. An end-to-end ob-
ject tracking approach which directly maps from raw sensor
input to object tracks in sensor space is proposed in [20].
Their proposed method works well in a simulated environ-
ment, but the applicability on the realistic RGB imagery
dataset is worth further investigation. On reason that tem-
poral deep learning based techniques are challenging to be
employed in realistic settings is due to the difﬁculty for re-
current net to have robust, meaningful temporal input.

In this paper, we propose a kernelised multi-resolution
convnet tracking algorithm that utilises the intermediate re-
sponse maps from the correlation ﬁlter output and learns the
implicit translational output from the multi-resolution con-
vnet. The key contributions can be summarised as follows:

• We incorporate kernalised form into CNN feature rep-
resentations for the non-linear regression tasks that
consistently outperforms its linear counterpart, con-
cluding the kernalised version should be the preferred
choice in the process of designing correlation ﬁlters.

• We learn a novel representation for multi-resolution re-
sponse maps generated from different layers of a pre-
trained CNN, negating the need to design the weights
for various convnets layers. Moreover, the learnt rep-
resentation renders it possible to be included into the
end-to-end temporal deep learning (TDL) pipeline.

• We use an adaptive hedge method to update the model
learning rate, taking the model stability into consider-
ation, making the update of target model smoother and
more robust.

2. Related Work

We review recent tracking methods closely related to this

work as in the following three sections.

Correlation ﬁlter based trackers: Most recent works [1,
26] strives for a better appearance models for the tracker.
DCF [1, 9] have demonstrated excellent performance for
high-speed visual object tracking. The key to their success
is by observing that the resulting data matrix of translated
patches is circulant, the cyclic shifts could be diagonalised
with the Discrete Fourier Transform, reducing both storage
and computation to obtain the next frame response map.
However, most of these trackers depend on the spatiotem-
poral consistency of visual cues. Therefore, they can handle
mostly short-term tracking or object with static appearance.

Deep feature based trackers: Recent works exploit the
structure of CNN to learn the target online:
a three-
layer CNN is trained on-the-ﬂight in [15]; a deep autoen-
coder [28] is ﬁrst pre-trained ofﬂine and then ﬁnetuned
for binary classiﬁcation in online tracking. Since the pre-
training is performed in an unsupervised way by recon-
structing gray images with very low resolution, the learned
deep features has limited discriminative power for track-
ing. Moreover, without pre-traning and with limited train-
ing samples obtained online, CNN fails to capture object se-
mantics and is not robust to deformation. Both [15] and [28]
train deep networks online with limited training samples,
and inevitably suffer from overﬁtting. Transferring the hi-
erarchical features learned for image classiﬁcation tasks
have been shown to be effective for numerous vision tasks,
e.g., image segmentation [17], salient object detection [34].
More recent methods [26, 25, 11, 18] adopt deep convolu-
tion networks trained on a large scale image classiﬁcation
task [22] to improve tracking performance. The rich rep-
resentation of transferred features from deep nets enables
trackers to construct more robust, power appearance model
over the traditional hand crafted feature based trackers.

Spatio-temporal model based trackers: Variations in the
appearance of the object in tracking, such as variations in
geometry/photometry, camera viewpoint, partial occlusion
or out-of-view, pose a major challenge to object tracking.
TLD [14] employs two experts to identify the false nega-
tives and false positives to train a detector. The experts are
independent, which ensures mutual compensation of their
errors to alleviate the problem of drifting. A short and long
term cognitive psychology principle is adopted in [12] to
design a ﬂexible representation that can adopt to changes
in object appearance during tracking. A parameter-free
Hedging algorithm is proposed in [2] for the problem of
decision-theoretic online learning, especially for the appli-
cations when the number of actions is very large and opti-
mally setting the parameter is not well understood. An im-
proved Hedge algorithm considering historical performance
is proposed in [21] to weight the decision from different
CNN layers.

2

Figure 2: The proposed algorithm overview: we ﬁrst extract hierarchical CNN features from the image patch of interest; then we project
deep features into kernel space for correlation ﬁlters; the target translational movement is predicted via multi-resolution convnets and later
we update the scale and appearance models using an adaptive learning rate scheme.

3. Proposed Algorithm

As shown in Fig. 2, the proposed approach consists of
three steps: extracting hierarchical CNN features, project-
ing features into kernel space for correlation ﬁlters, predict-
ing target translational movement via multi-resolution con-
vnet. We ﬁrst review the correlation ﬁlter as our building
block. Then we present the technical details of the projec-
tion of deep feature to kernel space, the model of learning
translational output using a multi-resolution convnets and
an adaptive learning rate scheme for model update.

Correlation Filter: Correlation ﬁlters based trackers [1, 9,
3, 10] exploit the circulant structure of training and testing
samples to greatly accelerate the training and testing pro-
cess. Let Xk ∈ (cid:60)D×M ×N denotes the feature sets where
D denotes the number of feature maps; M, N denote the
shape of feature maps and k denotes the k-th input feature
map extracted from the k-th convolutional layers from a
pretrained CNN; Y ∈ (cid:60)M ×N denotes a gaussian shaped
label matrix which is subject to a 2D Gaussian distribution
with zero mean and standard deviation proportional to the
target size. The goal of training is to ﬁnd a set of ﬁlters
Wk that minimises the squared error over sets of circulant
translated samples Xk and their regression targets Y:
(cid:107)Y − Xk • Wk(cid:107)2 + λ(cid:107)Wk(cid:107)2

Wk = argmin

(1)

Wk

where

Xk • Wk =

Xk

d (cid:12) Wk
d

D
(cid:88)

d=1
with the symbol (cid:12) denotes element-wise product. The min-
imizer has a closed-form:

W =

XT Y
XT X + λI

(2)

(3)

3

where I is an identity matrix and λ is a regularisation pa-
rameter that controls overﬁtting. We drop the superscript k
for notational simplicity. In general, a large system of lin-
ear equations must be solved to compute the solution, which
can become prohibitive in a real-time setting. With training
data being cyclic shifts patches, all operations can be done
element-wise on their diagonal elements [10].

The ﬁlter can be modelled in the Fourier domain by:

W = argmin

(cid:107)Y − X • W k(cid:107)2

F + λ(cid:107)W(cid:107)2
F

(4)

W

where

X • W =

X d (cid:12) W d

(5)

D
(cid:88)

d=1

The corresponding minimizer in the Fourier domain has

the closed-form:

W =

X ∗ (cid:12) Y
(X ∗ (cid:12) X + λI)

(6)

where ∗ denotes the Hermitian transpose and since diago-
nal matrices are symmetric, taking the Hermitian transpose
only left behind a complex-conjugate.

3.1. Kernelisation of Deep transfered features

In Kernelized Correlation Filter (KCF) [10], the feature
X is mapped to a Hilbert space φ(X). By employing a ker-
nel κ(X, X(cid:48)) = (cid:104)φ(X), φ(X(cid:48))(cid:105), Eq. 4 becomes:

W = argmin

(cid:107)Y − (cid:104)φ(X ), W(cid:105)(cid:107)2

F + λ(cid:107)W(cid:107)2
F

(7)

W

The power of the kernel trick comes from the implicit use
of a high-dimensional feature space φ(X) without ever in-
stantiating a vector in the space. Even though the regression

two consecutive frames; middle:

Figure 3: Response maps for one frame of the “Biker” sequence.
Left:
ideal response map (2D
gaussian); right: the actual response map. Arrows represent the
translational vector. Due to various factors, e.g., motion blur, tar-
get appearance change, scale change, boundary effect, the actual
response map is not a univariate gaussian and the maximum re-
sponse does not strictly correspond to the translational movement.

function’s complexity grows with the number of samples
which is the major drawback of the kernel trick, assuming
circulant data and the adopted kernel being shift invariant,
the kernel correlation can be computed efﬁciently. In our
experiment, an radial basis function (RBF) kernel, which
satisﬁes the shift invariant property, is adopted:

1
σ

kXX (cid:48)

= exp(−

((cid:107)X(cid:107)2 + (cid:107)X (cid:48)(cid:107)2 − 2F −1(X ∗ (cid:12) X (cid:48)))
(8)
where F −1 denotes the Inverse DFT and the full kernel cor-
relation can be computed in only O(n log n) time.

Expressing the solution W as a linear combination of the
samples: W = (cid:80)
i αiφ(X i) renders an alternative repre-
sentation α to be in the dual space, as opposed to the primal
space W (Representer Theorem [23]). The variables under
optimisation are thus α:

α = (kXX (cid:48)

+ λI)−1Y

(9)

For detection stage for translation estimation, given an
image patch feature Zk ∈ (cid:60)D×M ×N , the response map is
obtained by:

Figure 4: A multi-resolution convnet is deployed to decode the
hierarchical response maps.

consecutive frames, the precondition is that of static target
appearance, i.e., the maximum response map corresponds
to the cyclic shifts of appearance unchanged target. Due
to subtle changes of target appearance, e.g., rotation, scale,
cyclic boundary effect or motion blur of two consecutive
frames, the accuracy of ﬁnding the translation using maxi-
mum response could be compromised (c.f. Fig. 3).

Moreover, when using deep features from the pre-trained
convnets for the input representation X, the hierarchies
of feature maps capture different semantic information(c.f.
Fig. 1): features from deep layers capture rich category level
semantic information, which is useful for object classiﬁca-
tion, but they are not the optimal representation for visual
tracking because spatial details captured by earlier layers
are also important for accurately localising the targets. On
the other hand, as the features in the earlier layers capture
low level visual characteristics and are more class-generic
rather than discriminative as ones in the later layers, meth-
ods based on features from earlier layers are likely to fail in
challenging scenarios when the target size is small and high
level object is the target of tracking.

We interpret the response maps from the hierarchies of
convolutional layers as a nonlinear counter part of an image
pyramid representation and employ a learning framework
for mapping the hierarchical outputs to target translation.
A multi-resolution neural network (c.f. Fig. 4) is deployed
to exploit the implicit translational information from hierar-
chies response maps:

R(Zk) = F −1(F(kXZk(cid:48)

) (cid:12) F(αk))

(10)

∆x, ∆y ← convnets(R(Zk))

(12)

The model for frame t is updated with learning rate η as:

αk

t = (1 − η)αk

t−1 + ηαk

(11)

3.2. Decoding Response map using Multi-Res CNN

The loss of the convnets is set to be the root mean square
(RMS) of the normalised translational movement (∆x, ∆y)
and the predicted movement (∆x(cid:48), ∆y(cid:48)):

Lpos =

((cid:107) ∆x − ∆x(cid:48) (cid:107)2

2 + (cid:107) ∆y − ∆y(cid:48) (cid:107)2
2)

(13)

(cid:114) 1
2

Previous works estimate the next frame location via the
maximum response point on the response map. We ar-
gue that albeit the maximum response point carries phys-
ical meaning of the translational information between two

The proposed formulation enables efﬁcient integration of
multi-resolution response maps to decode translational in-
formation implicitly and demonstrates its competitiveness
over the corresponding counterpart baselines that using the

4

mean of maximum responses as the indicator for target
movement.

3.3. Adaptive learning rate

Traditional correlation based ﬁlter updates the model by
a ﬁxed parameter η. For generic object tracking, however,
there are two crucial factors for model update using deep
features: (1) the appearance of object of interest usually
changes at irregular pace (sometimes gently and sometimes
vehemently). This means that the scale of the learning rate
should reﬂect the appearance change of the target; (2) de-
pending on whether the object of interest being a low level
visual cues or high level object entity, the hierarchies of re-
sponse maps render correspondingly different maximum re-
sponse values.

Let the ultimate target position is predicted (xp, yp) at
time t, each layer k will incur a loss from its response map
Rk
t :

t = max(Rk
lk

t ) − Rk

t (xp, yp)

(14)

We then measure the stability of layer k at time t:

sk
t =

t − µk
|lk
t |
σk
t
t are the mean and variance for the loss lk

(15)

t dur-

t , σk
where µk
ing time period ∆t.
A larger sk

t indicates the layer is less correlated with the
object at frame t hence the model update η should be de-
creased as well. Therefore, we propose an adaptive learning
rate for different layers of model update that is linear to the
model stability as:

ηk = sk

t × η

(16)

Algorithm 1 summarises the main steps of the proposed

approach for visual tracking.

4. Experiments

We validate our proposed KMC framework by evaluat-
ing two genres of experiments: one compares with corre-
lation ﬁlter based baseline trackers and one compares with
several state-of-the-art trackers. We perform comprehen-
sive experiments on three datasets: OTB-2013 [31], OTB-
2015 [32] and UAV123 [19].

Implementation details: For feature extraction, we crop
the image patch with 2.2 padding size and resize the im-
age patch to 240 ∗ 160 because the average target size is of
ratio 3
2 . Feature bandwidth σ in Eq. 8 is 0.2. The learn-
ing rate η for Eq. 16 is 0.0025 which is only one ﬁfth of
ones chosen in [21] because of the model is in kernel space
and tends to be more stable. After the forward propaga-
tion, we use the VGG-Net with 19 layers and ﬁve out-
puts before the max pooling layers (i.e., ‘block1 conv2(cid:48),

Algorithm 1: Kernalised Multi-resolution Convnet

Input: target position (x1, y1) and size (w1, h1), ﬁrst

tracking frame;

Output: predicted target positions (xt, yt) and sizes
(wt, ht) in the following frames.

1 for t = 1, 2, . . . do
2

3

4

5

6

7

8

9

10

Crop target images with padding;
Extract deep features from layers before max
pooling layers in VGG-Net19 to obtain 5
hierarchical representations X;
Project deep features into kernel space 3.1;
if t = 1 then

Obtain deep kernel model α (Eq.9);

else

Update target position prediction by the
trained multi-resolution convnet 3.2;
Update model scale using DSST [4];
Update kernel model using adaptive learning
rate 3.3;

‘block3 conv4(cid:48),

‘block2 conv2(cid:48),
‘block5 conv4(cid:48)). Note
that instead of heuristically choosing layers as in [21, 7],
we have included all layer outputs before the max pooling
layers. This chosen methodology makes the approach more
generic and less dataset dependant.

The convnet in Sec. 3.2 is trained on OTB-2015 dataset.
Note that albeit trained on the OTB-2015 dataset, the con-
vnet during detection will not be able to see the exactly
same response maps twice unless exact location update is
achieved. Hence it requires the convnet to be able to gener-
alise to unseen hierarchical response maps and predict rea-
sonable translational output. Furthermore, we verify the
generalisation of the KMC over HDT [21] tracker on the
unseen UAV123 dataset.

For scale estimation, we use a scale pyramid representa-
tion as in [4] with 11 scales and a relative scale factor 1.02.
The updates consistently helps more accurate update of tar-
get models while maintaining the computational cost low.

Evaluation Methodology: Following the evaluation strat-
egy of
[31], all trackers are compared using two mea-
sures: precision and success. Precision is measured as the
distance between the centres of the ground truth bounding
box and the corresponding tracker generated bounding box.
The precision plot shows the percentage of tracker bound-
ing boxed within a given threshold distance in pixels of
the ground truth. To rank the trackers, the conventional
threshold of 20 pixels (P20) is adopted. Success is mea-
sured as the intersection over union of pixels. The success
plot shows the percentage of tracker bounding boxes whose
overlap score is larger than a given threshold and the track-

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Method

Dataset

DeepDCF
DeepKCF
MaxRes
Multi-Resolution CNN
Fixed Learning Rate
Adaptive Learning Rate

OTB-2013

OTB-2015

P20
58.0
66.2 (8.2 ↑)
57.5
70.6(13.1 ↑)
74.0(3.4 ↑)
74.2 (0.2 ↑)

AUC
38.4
46.1 (7.7 ↑)
38.8
46.7 (7.9 ↑)
51.8(5.1 ↑)
52.1(0.3 ↑)

P20
64.3
71.4 (7.1 ↑)
65.8
77.8 (12.0 ↑)
79.8(2.0 ↑)
79.9(0.1 ↑)

AUC
44.0
49.1 (5.1 ↑)
45.9
53.6 (7.7 ↑)
57.0(3.4 ↑)
57.4(0.4 ↑)

Table 1: Baseline comparisons with arrows indicating performance changes comparing with the upper row parallel experiment. First two
rows investigate the impact of projecting deep features into a kernel space vs. using linear deep features. Next two rows compare the
traditional using max location vs. the proposed multi-resolution convnets paradigm for decoding target translational movement. Last two
rows illustrate the beneﬁt of adaptive learning rate through time (with scale update).

ers are ranked according to the Area Under Curve (AUC)
criteria. All sequences are evaluated using One-Pass Evalu-
ation (OPE) as in [31].

4.1. Baseline comparison

Beneﬁt of Kernalising deep features: We ﬁrst evalu-
ate the impact of projecting deep features into a kernel
space (DeepKCF) vs. using the traditional linear projection
(DeepDCF) in ﬁrst two rows of Tab. 1. The deep features
used are the layer from ‘block3 conv4(cid:48). It can be seen that
projecting deep feature into kernel space consistently yields
a noticeable increase in both performance measures. The
computation cost for kernel projection is O(n log n) and for
linear kernel is O(n). Therefore, given marginal increase in
computational cost, we conclude that for constructing ap-
pearance model, kernel projection is the preferred method-
ology versus the traditional linear projection.

Beneﬁt of multi-resolution CNN: We then evaluate the
impact of decoding response map using a multi-resolution
CNN (Multi-Resolution CNN) vs. directly using the max
location information (MaxRes) for decoding target location
from hierarchical response maps in Tab. 1. MaxRes use the
maximum position from the mean of hierarchical response
maps with later response maps being resized to the largest
ﬁrst layer response map of size 240 ∗ 160. The performance
of MaxRes is even worse than using single layer, signify-
ing the need to intelligently combine multiply layer out-
put. The learnt convnets consistently performs better than
that of MaxRes for a large margin. Moreover, the learn-
ing paradigm opens the door for the integration of recurrent
network into the pipeline.

Beneﬁt of adaptive learning rate through time: We also
investigate the scheme of adjusting the learning rate adap-
tively vs. ﬁxing the learning rate through time in last two
rows of Tab. 1. Consistent improvements across two metrics

are observed. The margin of improvement, however, is less
prominent. We reason that in part, it is due to almost sat-
urate performance; on the other hand, an end-to-end learn-
ing scheme for model update could be more preferable and
should be further investigated.

4.2. Comparisons with the-state-of-the-art trackers

We validate our KMC tracker in a comprehensive
comparison with 10 state-of-the-art trackers: HDT [21],
SRDCF [6], MEEM [33], MUSTER [12], SAMF [16],
DSST [4], KCF [10], Struck [8], TLD [13], DCF [9].

OTB-2015 Dataset: OTB-2015 dataset contains 100 video
sequences and is the superset of OTB-2013 [31] dataset
which contains the original 50 video sequences. The Re-
sults on the OBT dataset is shown in the top row of Fig. 5.
It can be seen that our tracker performs on par with a range
of state-of-the-art trackers across two evaluation metrics.
The most similar tracker is HDT where both correlation
ﬁlter and deep features are adopted. However, HDT has
heuristically chosen a set of higher level deep feature lay-
ers whereas in our approach, all layers before max pooling
are universally chosen. The SRDCF tracker is better than
our tracker in the success metric due to heavy regularisation
for negative training examples in SRDCF tracker. Hence
it enables larger search area during tracking. Furthermore,
because SRDCF tracker is able to search for larger area dur-
ing detection (42 vs. our 3.22 the target size), it indirectly
manages the training set to handle the scenarios of occlu-
sion and drift, of which the proposed KMC has been mostly
penalised.

UAV123 Dataset: Visual tracking on unmanned aerial ve-
hicles (UAVs) is a very promising application, since the
camera can follow the target based on visual feedback and
actively change its orientation and position to optimise the
tracking performance. This UAV123 dataset contains a total

6

(a) Precision plot for OTB-2015 Dataset

(b) Success plot for OTB-2015 Dataset

(c) Precision plot for UAV123 Dataset

(d) Success plot for UAV123 Dataset

Figure 5: Precision plot and success plot of the state-of-the-art trackers for OTB-2015 and UAV123 dataset.

of 123 video sequences and more than 110K frames. The
major difference between this UAV123 dataset and other
popular tracking datasets is the effect of camera viewpoint
change arising from UAV motion, the variation in bounding
box size and aspect ratio with respect to the initial frame,
and longer tracking sequences on average due to the avail-
ability of mounted camera moving with the target.

Results are shown in the bottom row of Fig. 5. It can
be seen that due to heuristically chosen layers tuned to
the OBT dataset in HDT, its performance suffers on this
UAV123 dataset.
It worths noting that our trained multi-
resolution convnet has never seen any image in this dataset.
Moreover, no pesky threshold, hyperparameters nor weights
are altered when applying the trained KMC network on the
unseen UAV123 dataset, proving its ability to generalise
across various tracking scenarios.

5. Conclusions

In this paper, we propose a novel kernelised multi-
resolution convnet tracking algorithm that utilises the in-
termediate response maps from the kernelised correlation
ﬁlter outputs. The multi-resolution convnet learns the im-
plicit translational output accurately and later an adaptive

learning scheme is adopted for model update. The learning
paradigm is able to generalise across various datasets with-
out change of hyperparameters. Moreover, it opens the door
on the end-to-end temporal deep learning. Future works
include better regularisation method as in [6] for negative
instance mining, multi-layer fusion [24] and incorporating
recurrent nets on top to model temporal dynamics.

Details of the Code

The python based code can be found at:
https://github.com/stevenwudi/KMC_cvprw_2017

Acknowledgment

This project has received funding from National Natural Science Founda-
tion of China (NSFC) (61401287); Natural Science Foundation of Shen-
zhen (JCYJ20160307154003475, JCYJ2016050617265125).

References

[1] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui.
Visual object tracking using adaptive correlation ﬁlters. In
Conference on Computer Vision and Pattern Recognition,
2010.

7

[2] K. Chaudhuri, Y. Freund, and D. J. Hsu. A parameter-free
hedging algorithm. In Advances in neural information pro-
cessing systems, 2009.

[19] M. Mueller, N. Smith, and B. Ghanem. A benchmark and
simulator for uav tracking. In European Conference on Com-
puter Vision, 2016.

[3] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Accu-
In British

rate scale estimation for robust visual tracking.
Machine Vision Conference, 2014.

[4] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Dis-
criminative scale space tracking. In Transactions on Pattern
Analysis and Machine Intelligence, 2017.

[5] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg. Con-
volutional features for correlation ﬁlter based visual track-
International Conference on Computer Vision, work-
ing.
shop, 2015.

[6] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg. Learn-
ing spatially regularized correlation ﬁlters for visual track-
ing. International Conference on Computer Vision, 2015.
[7] M. Danelljan, K. F. S. Robinson, Andreas, and M. Felsberg.
Beyond correlation ﬁlters: Learning continuous convolution
operators for visual tracking. European Conference on Com-
puter Vision, 2016.

[8] S. Hare, A. Saffari, and P. Torr. Struck: Structured output
tracking with kernels. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, 2011.

[9] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. Ex-
ploiting the circulant structure of tracking-by-detection with
kernels. In European conference on computer vision, 2012.
[10] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-
IEEE
speed tracking with kernelized correlation ﬁlters.
Transactions on Pattern Analysis and Machine Intelligence,
2015.

[11] S. Hong, T. You, S. Kwak, and B. Han. Online tracking by
learning discriminative saliency map with convolutional neu-
ral network. International Conference on Machine Learning,
2015.

[12] Z. Hong, Z. Chen, C. Wang, X. Mei, D. Prokhorov, and
D. Tao. Multi-store tracker (muster): A cognitive psychology
inspired approach to object tracking. In IEEE Conference on
Computer Vision and Pattern Recognition, 2015.

[13] Z. Kalal, J. Matas, and K. Mikolajczyk. P-n learning: Boot-
strapping binary classiﬁers by structural constraints. In Con-
ference on Computer Vision and Pattern Recognition, 2010.
[14] Z. Kalal, K. Mikolajczyk, and J. Matas. Tracking-learning-
IEEE transactions on pattern analysis and ma-

detection.
chine intelligence, 2012.

[15] H. Li, Y. Li, and F. Porikli. Robust online visual tracking
with a single convolutional neural network. In Asian Confer-
ence on Computer Vision, 2014.

[16] Y. Li and J. Zhu. A scale adaptive kernel correlation ﬁlter
tracker with feature integration. European Conference on
Computer Vision, workshop, 2014.

[17] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. Conference on Com-
puter Vision and Pattern Recognition, 2015.

[18] C. Ma, J.-B. Huang, X. Yang, and M.-H. Yang. Hierarchical
convolutional features for visual tracking. In IEEE Interna-
tional Conference on Computer Vision, 2015.

8

[20] P. Ondr´uˇska and I. Posner. Deep tracking: Seeing beyond
seeing using recurrent neural networks. In Proceedings of the
Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.

[21] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, and J. L. M.-
In IEEE Conference on

H. Yang. Hedged deep tracking.
Computer Vision and Pattern Recognition, 2016.

[22] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recog-
nition challenge. International Journal of Computer Vision,
2015.

[23] B. Scholkopf and A. J. Smola. Learning with kernels: sup-
port vector machines, regularization, optimization, and be-
yond. MIT press, 2001.

[24] L. Shao, D. Wu, and X. Li. Learning deep and wide: A spec-
tral method for learning deep networks. IEEE Transactions
on Neural Networks and Learning Systems, 2014.

[25] L. Wang, W. Ouyang, X. Wang, and H. Lu. Visual track-
ing with fully convolutional networks. In Proceedings of the
IEEE International Conference on Computer Vision, 2015.

[26] L. Wang, W. Ouyang, X. Wang, and H. Lu. Stct: Sequen-
tially training convolutional networks for visual tracking. In
IEEE International Conference on Computer Vision, 2016.

[27] N. Wang and D.-Y. Yeung. Learning a deep compact image
In Advances in Neural

representation for visual tracking.
Information Processing Systems 26. 2013.

[28] N. Wang and D.-Y. Yeung. Learning a deep compact im-
age representation for visual tracking. In Advances in neural
information processing systems, 2013.

[29] D. Wu, L. Pigou, P.-J. Kindermans, N. D.-H. Le, L. Shao,
J. Dambre, and J.-M. Odobez. Deep dynamic neural net-
works for multimodal gesture segmentation and recognition.
IEEE transactions on pattern analysis and machine intelli-
gence, 2016.

[30] D. Wu and L. Shao. Leveraging hierarchical parametric
networks for skeletal joints based action segmentation and
recognition. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2014.

[31] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A
benchmark. In IEEE conference on computer vision and pat-
tern recognition, 2013.

[32] Y. Wu, J. Lim, and M.-H. Yang. Object tracking benchmark.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2015.

[33] M.-S.-S. S. Zhang, J. Meem: robust tracking via multiple ex-
perts using entropy minimization. In European Conference
on Computer Vision, 2014.

[34] W. Zou and N. Komodakis. Harf: Hierarchy-associated rich
features for salient object detection. In Proceedings of the
IEEE International Conference on Computer Vision, 2015.

Kernalised Multi-resolution Convnet for Visual Tracking

Di Wu, Wenbin Zou∗, Xia Li
Shenzhen University†
dwu, wzou, lixia@szu.edu.cn

Yong Zhao
Peking University Shenzhen Graduate School
yongzhao@pkusz.edu.cn

7
1
0
2
 
g
u
A
 
2
 
 
]

V
C
.
s
c
[
 
 
1
v
7
7
5
0
0
.
8
0
7
1
:
v
i
X
r
a

Abstract

Visual tracking is intrinsically a temporal problem. Dis-
criminative Correlation Filters (DCF) have demonstrated
excellent performance for high-speed generic visual object
tracking. Built upon their seminal work, there has been a
plethora of recent improvements relying on convolutional
neural network (CNN) pretrained on ImageNet as a feature
extractor for visual tracking. However, most of their works
relying on ad hoc analysis to design the weights for different
layers either using boosting or hedging techniques as an en-
semble tracker. In this paper, we go beyond the conventional
DCF framework and propose a Kernalised Multi-resolution
Convnet (KMC) formulation that utilises hierarchical re-
sponse maps to directly output the target movement. When
directly deployed the learnt network to predict the unseen
challenging UAV tracking dataset without any weight ad-
justment, the proposed model consistently achieves excel-
lent tracking performance. Moreover, the transfered multi-
reslution CNN renders it possible to be integrated into the
RNN temporal learning framework, therefore opening the
door on the end-to-end temporal deep learning (TDL) for
visual tracking.

1. Introduction

Visual tracking is the task of predicting the trajectory of a
target in a video. The task of tracking, a crucial component
of many computer vision systems, can be naturally speciﬁed
as an online learning problem. This paper focuses on the
challenging problem of monocular, generic, realistic object
tracking.

Visual object tracking has recently witnessed substantial
progress due to powerful features extracted using deep con-
volutional neural networks. For online applications, one
simple approach to transfer ofﬂine pre-trained CNN fea-
tures is to add one or more randomly initialised CNN layers,
named as adaptation layers, on top of the pre-trained CNN

∗Corresponding author.
†Shenzhen Key Lab of Advanced Telecommunication and Information

Processing, College of Information Engineering, Shenzhen University.

Figure 1: Response maps generated from hierarchical deep fea-
tures. Left: two frames from the “Bolt” sequence, red boxes are
image patches with padding added. Right: hierarchical response
maps from correlation ﬁlters (response maps have been resized
and rescaled to the same shape and scale). It can be seen that be-
cause different layers of deep features encode different semantics,
the maximum response points from different layers also vary.

model. However, as [26] empirically observe that this trans-
fer learning method suffers from severe over-ﬁtting because
of the limited training data. It is also observed in [29, 30]
that the differences in mean activations in intermediate lay-
ers have noticeable effect for multi-model fusion. The on-
line learnt parameters mainly focus on recent training sam-
ples and are less likely to be well generalised to both histor-
ical and future samples. This phenomenon can be fatal to
online visual tracking where the target often undergoes sig-
niﬁcant appearance changes or heavy occlusion. CNNs also
have shown impressive performance as an ofﬂine feature ex-
tractor for tracking [27, 18] in lieu of traditional handcrafted
features (e.g., HOG, Color Moment). Features from these
deep convolutional layers are discriminative while preserv-
ing spatial and structural information.

The marriage between DCF [9], which has the advan-
tage of being efﬁcient in training translational images in
the fourier space, and deep features, which excel at image
representation, further advances the visual tracking com-
munity [21, 7]. However, there are some conﬂicting con-

1

clusions concerning the representation power from differ-
ent layers of CNN: shallow convolutional layers have been
highlighted in the DCF-based methods [5] whereas [21]
found that the performance generally increases as layer
depth is increased hence only convolutional layers deeper
than 10th is used. Fig. 1 shows the response maps gener-
ated from hierarchical deep features. Since different layers
from convnet capture different semantic meanings, maxi-
mum translational locations obtained via different convnet
layers could vary: low level layers excel at discriminating
intra-class variations whereas high level layers excel at dis-
criminating inter-class variations. Hence, for generic object
tracking, it is imperative to combine features from different
layers to have a generalised representation from hierarchies
of response maps.

Visual tracking is intrinsically also a temporal problem,
but many previous approaches [1, 26] mainly focus on the
design of a robust appearance model. An end-to-end ob-
ject tracking approach which directly maps from raw sensor
input to object tracks in sensor space is proposed in [20].
Their proposed method works well in a simulated environ-
ment, but the applicability on the realistic RGB imagery
dataset is worth further investigation. On reason that tem-
poral deep learning based techniques are challenging to be
employed in realistic settings is due to the difﬁculty for re-
current net to have robust, meaningful temporal input.

In this paper, we propose a kernelised multi-resolution
convnet tracking algorithm that utilises the intermediate re-
sponse maps from the correlation ﬁlter output and learns the
implicit translational output from the multi-resolution con-
vnet. The key contributions can be summarised as follows:

• We incorporate kernalised form into CNN feature rep-
resentations for the non-linear regression tasks that
consistently outperforms its linear counterpart, con-
cluding the kernalised version should be the preferred
choice in the process of designing correlation ﬁlters.

• We learn a novel representation for multi-resolution re-
sponse maps generated from different layers of a pre-
trained CNN, negating the need to design the weights
for various convnets layers. Moreover, the learnt rep-
resentation renders it possible to be included into the
end-to-end temporal deep learning (TDL) pipeline.

• We use an adaptive hedge method to update the model
learning rate, taking the model stability into consider-
ation, making the update of target model smoother and
more robust.

2. Related Work

We review recent tracking methods closely related to this

work as in the following three sections.

Correlation ﬁlter based trackers: Most recent works [1,
26] strives for a better appearance models for the tracker.
DCF [1, 9] have demonstrated excellent performance for
high-speed visual object tracking. The key to their success
is by observing that the resulting data matrix of translated
patches is circulant, the cyclic shifts could be diagonalised
with the Discrete Fourier Transform, reducing both storage
and computation to obtain the next frame response map.
However, most of these trackers depend on the spatiotem-
poral consistency of visual cues. Therefore, they can handle
mostly short-term tracking or object with static appearance.

Deep feature based trackers: Recent works exploit the
structure of CNN to learn the target online:
a three-
layer CNN is trained on-the-ﬂight in [15]; a deep autoen-
coder [28] is ﬁrst pre-trained ofﬂine and then ﬁnetuned
for binary classiﬁcation in online tracking. Since the pre-
training is performed in an unsupervised way by recon-
structing gray images with very low resolution, the learned
deep features has limited discriminative power for track-
ing. Moreover, without pre-traning and with limited train-
ing samples obtained online, CNN fails to capture object se-
mantics and is not robust to deformation. Both [15] and [28]
train deep networks online with limited training samples,
and inevitably suffer from overﬁtting. Transferring the hi-
erarchical features learned for image classiﬁcation tasks
have been shown to be effective for numerous vision tasks,
e.g., image segmentation [17], salient object detection [34].
More recent methods [26, 25, 11, 18] adopt deep convolu-
tion networks trained on a large scale image classiﬁcation
task [22] to improve tracking performance. The rich rep-
resentation of transferred features from deep nets enables
trackers to construct more robust, power appearance model
over the traditional hand crafted feature based trackers.

Spatio-temporal model based trackers: Variations in the
appearance of the object in tracking, such as variations in
geometry/photometry, camera viewpoint, partial occlusion
or out-of-view, pose a major challenge to object tracking.
TLD [14] employs two experts to identify the false nega-
tives and false positives to train a detector. The experts are
independent, which ensures mutual compensation of their
errors to alleviate the problem of drifting. A short and long
term cognitive psychology principle is adopted in [12] to
design a ﬂexible representation that can adopt to changes
in object appearance during tracking. A parameter-free
Hedging algorithm is proposed in [2] for the problem of
decision-theoretic online learning, especially for the appli-
cations when the number of actions is very large and opti-
mally setting the parameter is not well understood. An im-
proved Hedge algorithm considering historical performance
is proposed in [21] to weight the decision from different
CNN layers.

2

Figure 2: The proposed algorithm overview: we ﬁrst extract hierarchical CNN features from the image patch of interest; then we project
deep features into kernel space for correlation ﬁlters; the target translational movement is predicted via multi-resolution convnets and later
we update the scale and appearance models using an adaptive learning rate scheme.

3. Proposed Algorithm

As shown in Fig. 2, the proposed approach consists of
three steps: extracting hierarchical CNN features, project-
ing features into kernel space for correlation ﬁlters, predict-
ing target translational movement via multi-resolution con-
vnet. We ﬁrst review the correlation ﬁlter as our building
block. Then we present the technical details of the projec-
tion of deep feature to kernel space, the model of learning
translational output using a multi-resolution convnets and
an adaptive learning rate scheme for model update.

Correlation Filter: Correlation ﬁlters based trackers [1, 9,
3, 10] exploit the circulant structure of training and testing
samples to greatly accelerate the training and testing pro-
cess. Let Xk ∈ (cid:60)D×M ×N denotes the feature sets where
D denotes the number of feature maps; M, N denote the
shape of feature maps and k denotes the k-th input feature
map extracted from the k-th convolutional layers from a
pretrained CNN; Y ∈ (cid:60)M ×N denotes a gaussian shaped
label matrix which is subject to a 2D Gaussian distribution
with zero mean and standard deviation proportional to the
target size. The goal of training is to ﬁnd a set of ﬁlters
Wk that minimises the squared error over sets of circulant
translated samples Xk and their regression targets Y:
(cid:107)Y − Xk • Wk(cid:107)2 + λ(cid:107)Wk(cid:107)2

Wk = argmin

(1)

Wk

where

Xk • Wk =

Xk

d (cid:12) Wk
d

D
(cid:88)

d=1
with the symbol (cid:12) denotes element-wise product. The min-
imizer has a closed-form:

W =

XT Y
XT X + λI

(2)

(3)

3

where I is an identity matrix and λ is a regularisation pa-
rameter that controls overﬁtting. We drop the superscript k
for notational simplicity. In general, a large system of lin-
ear equations must be solved to compute the solution, which
can become prohibitive in a real-time setting. With training
data being cyclic shifts patches, all operations can be done
element-wise on their diagonal elements [10].

The ﬁlter can be modelled in the Fourier domain by:

W = argmin

(cid:107)Y − X • W k(cid:107)2

F + λ(cid:107)W(cid:107)2
F

(4)

W

where

X • W =

X d (cid:12) W d

(5)

D
(cid:88)

d=1

The corresponding minimizer in the Fourier domain has

the closed-form:

W =

X ∗ (cid:12) Y
(X ∗ (cid:12) X + λI)

(6)

where ∗ denotes the Hermitian transpose and since diago-
nal matrices are symmetric, taking the Hermitian transpose
only left behind a complex-conjugate.

3.1. Kernelisation of Deep transfered features

In Kernelized Correlation Filter (KCF) [10], the feature
X is mapped to a Hilbert space φ(X). By employing a ker-
nel κ(X, X(cid:48)) = (cid:104)φ(X), φ(X(cid:48))(cid:105), Eq. 4 becomes:

W = argmin

(cid:107)Y − (cid:104)φ(X ), W(cid:105)(cid:107)2

F + λ(cid:107)W(cid:107)2
F

(7)

W

The power of the kernel trick comes from the implicit use
of a high-dimensional feature space φ(X) without ever in-
stantiating a vector in the space. Even though the regression

two consecutive frames; middle:

Figure 3: Response maps for one frame of the “Biker” sequence.
Left:
ideal response map (2D
gaussian); right: the actual response map. Arrows represent the
translational vector. Due to various factors, e.g., motion blur, tar-
get appearance change, scale change, boundary effect, the actual
response map is not a univariate gaussian and the maximum re-
sponse does not strictly correspond to the translational movement.

function’s complexity grows with the number of samples
which is the major drawback of the kernel trick, assuming
circulant data and the adopted kernel being shift invariant,
the kernel correlation can be computed efﬁciently. In our
experiment, an radial basis function (RBF) kernel, which
satisﬁes the shift invariant property, is adopted:

1
σ

kXX (cid:48)

= exp(−

((cid:107)X(cid:107)2 + (cid:107)X (cid:48)(cid:107)2 − 2F −1(X ∗ (cid:12) X (cid:48)))
(8)
where F −1 denotes the Inverse DFT and the full kernel cor-
relation can be computed in only O(n log n) time.

Expressing the solution W as a linear combination of the
samples: W = (cid:80)
i αiφ(X i) renders an alternative repre-
sentation α to be in the dual space, as opposed to the primal
space W (Representer Theorem [23]). The variables under
optimisation are thus α:

α = (kXX (cid:48)

+ λI)−1Y

(9)

For detection stage for translation estimation, given an
image patch feature Zk ∈ (cid:60)D×M ×N , the response map is
obtained by:

Figure 4: A multi-resolution convnet is deployed to decode the
hierarchical response maps.

consecutive frames, the precondition is that of static target
appearance, i.e., the maximum response map corresponds
to the cyclic shifts of appearance unchanged target. Due
to subtle changes of target appearance, e.g., rotation, scale,
cyclic boundary effect or motion blur of two consecutive
frames, the accuracy of ﬁnding the translation using maxi-
mum response could be compromised (c.f. Fig. 3).

Moreover, when using deep features from the pre-trained
convnets for the input representation X, the hierarchies
of feature maps capture different semantic information(c.f.
Fig. 1): features from deep layers capture rich category level
semantic information, which is useful for object classiﬁca-
tion, but they are not the optimal representation for visual
tracking because spatial details captured by earlier layers
are also important for accurately localising the targets. On
the other hand, as the features in the earlier layers capture
low level visual characteristics and are more class-generic
rather than discriminative as ones in the later layers, meth-
ods based on features from earlier layers are likely to fail in
challenging scenarios when the target size is small and high
level object is the target of tracking.

We interpret the response maps from the hierarchies of
convolutional layers as a nonlinear counter part of an image
pyramid representation and employ a learning framework
for mapping the hierarchical outputs to target translation.
A multi-resolution neural network (c.f. Fig. 4) is deployed
to exploit the implicit translational information from hierar-
chies response maps:

R(Zk) = F −1(F(kXZk(cid:48)

) (cid:12) F(αk))

(10)

∆x, ∆y ← convnets(R(Zk))

(12)

The model for frame t is updated with learning rate η as:

αk

t = (1 − η)αk

t−1 + ηαk

(11)

3.2. Decoding Response map using Multi-Res CNN

The loss of the convnets is set to be the root mean square
(RMS) of the normalised translational movement (∆x, ∆y)
and the predicted movement (∆x(cid:48), ∆y(cid:48)):

Lpos =

((cid:107) ∆x − ∆x(cid:48) (cid:107)2

2 + (cid:107) ∆y − ∆y(cid:48) (cid:107)2
2)

(13)

(cid:114) 1
2

Previous works estimate the next frame location via the
maximum response point on the response map. We ar-
gue that albeit the maximum response point carries phys-
ical meaning of the translational information between two

The proposed formulation enables efﬁcient integration of
multi-resolution response maps to decode translational in-
formation implicitly and demonstrates its competitiveness
over the corresponding counterpart baselines that using the

4

mean of maximum responses as the indicator for target
movement.

3.3. Adaptive learning rate

Traditional correlation based ﬁlter updates the model by
a ﬁxed parameter η. For generic object tracking, however,
there are two crucial factors for model update using deep
features: (1) the appearance of object of interest usually
changes at irregular pace (sometimes gently and sometimes
vehemently). This means that the scale of the learning rate
should reﬂect the appearance change of the target; (2) de-
pending on whether the object of interest being a low level
visual cues or high level object entity, the hierarchies of re-
sponse maps render correspondingly different maximum re-
sponse values.

Let the ultimate target position is predicted (xp, yp) at
time t, each layer k will incur a loss from its response map
Rk
t :

t = max(Rk
lk

t ) − Rk

t (xp, yp)

(14)

We then measure the stability of layer k at time t:

sk
t =

t − µk
|lk
t |
σk
t
t are the mean and variance for the loss lk

(15)

t dur-

t , σk
where µk
ing time period ∆t.
A larger sk

t indicates the layer is less correlated with the
object at frame t hence the model update η should be de-
creased as well. Therefore, we propose an adaptive learning
rate for different layers of model update that is linear to the
model stability as:

ηk = sk

t × η

(16)

Algorithm 1 summarises the main steps of the proposed

approach for visual tracking.

4. Experiments

We validate our proposed KMC framework by evaluat-
ing two genres of experiments: one compares with corre-
lation ﬁlter based baseline trackers and one compares with
several state-of-the-art trackers. We perform comprehen-
sive experiments on three datasets: OTB-2013 [31], OTB-
2015 [32] and UAV123 [19].

Implementation details: For feature extraction, we crop
the image patch with 2.2 padding size and resize the im-
age patch to 240 ∗ 160 because the average target size is of
ratio 3
2 . Feature bandwidth σ in Eq. 8 is 0.2. The learn-
ing rate η for Eq. 16 is 0.0025 which is only one ﬁfth of
ones chosen in [21] because of the model is in kernel space
and tends to be more stable. After the forward propaga-
tion, we use the VGG-Net with 19 layers and ﬁve out-
puts before the max pooling layers (i.e., ‘block1 conv2(cid:48),

Algorithm 1: Kernalised Multi-resolution Convnet

Input: target position (x1, y1) and size (w1, h1), ﬁrst

tracking frame;

Output: predicted target positions (xt, yt) and sizes
(wt, ht) in the following frames.

1 for t = 1, 2, . . . do
2

3

4

5

6

7

8

9

10

Crop target images with padding;
Extract deep features from layers before max
pooling layers in VGG-Net19 to obtain 5
hierarchical representations X;
Project deep features into kernel space 3.1;
if t = 1 then

Obtain deep kernel model α (Eq.9);

else

Update target position prediction by the
trained multi-resolution convnet 3.2;
Update model scale using DSST [4];
Update kernel model using adaptive learning
rate 3.3;

‘block3 conv4(cid:48),

‘block2 conv2(cid:48),
‘block5 conv4(cid:48)). Note
that instead of heuristically choosing layers as in [21, 7],
we have included all layer outputs before the max pooling
layers. This chosen methodology makes the approach more
generic and less dataset dependant.

The convnet in Sec. 3.2 is trained on OTB-2015 dataset.
Note that albeit trained on the OTB-2015 dataset, the con-
vnet during detection will not be able to see the exactly
same response maps twice unless exact location update is
achieved. Hence it requires the convnet to be able to gener-
alise to unseen hierarchical response maps and predict rea-
sonable translational output. Furthermore, we verify the
generalisation of the KMC over HDT [21] tracker on the
unseen UAV123 dataset.

For scale estimation, we use a scale pyramid representa-
tion as in [4] with 11 scales and a relative scale factor 1.02.
The updates consistently helps more accurate update of tar-
get models while maintaining the computational cost low.

Evaluation Methodology: Following the evaluation strat-
egy of
[31], all trackers are compared using two mea-
sures: precision and success. Precision is measured as the
distance between the centres of the ground truth bounding
box and the corresponding tracker generated bounding box.
The precision plot shows the percentage of tracker bound-
ing boxed within a given threshold distance in pixels of
the ground truth. To rank the trackers, the conventional
threshold of 20 pixels (P20) is adopted. Success is mea-
sured as the intersection over union of pixels. The success
plot shows the percentage of tracker bounding boxes whose
overlap score is larger than a given threshold and the track-

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Method

Dataset

DeepDCF
DeepKCF
MaxRes
Multi-Resolution CNN
Fixed Learning Rate
Adaptive Learning Rate

OTB-2013

OTB-2015

P20
58.0
66.2 (8.2 ↑)
57.5
70.6(13.1 ↑)
74.0(3.4 ↑)
74.2 (0.2 ↑)

AUC
38.4
46.1 (7.7 ↑)
38.8
46.7 (7.9 ↑)
51.8(5.1 ↑)
52.1(0.3 ↑)

P20
64.3
71.4 (7.1 ↑)
65.8
77.8 (12.0 ↑)
79.8(2.0 ↑)
79.9(0.1 ↑)

AUC
44.0
49.1 (5.1 ↑)
45.9
53.6 (7.7 ↑)
57.0(3.4 ↑)
57.4(0.4 ↑)

Table 1: Baseline comparisons with arrows indicating performance changes comparing with the upper row parallel experiment. First two
rows investigate the impact of projecting deep features into a kernel space vs. using linear deep features. Next two rows compare the
traditional using max location vs. the proposed multi-resolution convnets paradigm for decoding target translational movement. Last two
rows illustrate the beneﬁt of adaptive learning rate through time (with scale update).

ers are ranked according to the Area Under Curve (AUC)
criteria. All sequences are evaluated using One-Pass Evalu-
ation (OPE) as in [31].

4.1. Baseline comparison

Beneﬁt of Kernalising deep features: We ﬁrst evalu-
ate the impact of projecting deep features into a kernel
space (DeepKCF) vs. using the traditional linear projection
(DeepDCF) in ﬁrst two rows of Tab. 1. The deep features
used are the layer from ‘block3 conv4(cid:48). It can be seen that
projecting deep feature into kernel space consistently yields
a noticeable increase in both performance measures. The
computation cost for kernel projection is O(n log n) and for
linear kernel is O(n). Therefore, given marginal increase in
computational cost, we conclude that for constructing ap-
pearance model, kernel projection is the preferred method-
ology versus the traditional linear projection.

Beneﬁt of multi-resolution CNN: We then evaluate the
impact of decoding response map using a multi-resolution
CNN (Multi-Resolution CNN) vs. directly using the max
location information (MaxRes) for decoding target location
from hierarchical response maps in Tab. 1. MaxRes use the
maximum position from the mean of hierarchical response
maps with later response maps being resized to the largest
ﬁrst layer response map of size 240 ∗ 160. The performance
of MaxRes is even worse than using single layer, signify-
ing the need to intelligently combine multiply layer out-
put. The learnt convnets consistently performs better than
that of MaxRes for a large margin. Moreover, the learn-
ing paradigm opens the door for the integration of recurrent
network into the pipeline.

Beneﬁt of adaptive learning rate through time: We also
investigate the scheme of adjusting the learning rate adap-
tively vs. ﬁxing the learning rate through time in last two
rows of Tab. 1. Consistent improvements across two metrics

are observed. The margin of improvement, however, is less
prominent. We reason that in part, it is due to almost sat-
urate performance; on the other hand, an end-to-end learn-
ing scheme for model update could be more preferable and
should be further investigated.

4.2. Comparisons with the-state-of-the-art trackers

We validate our KMC tracker in a comprehensive
comparison with 10 state-of-the-art trackers: HDT [21],
SRDCF [6], MEEM [33], MUSTER [12], SAMF [16],
DSST [4], KCF [10], Struck [8], TLD [13], DCF [9].

OTB-2015 Dataset: OTB-2015 dataset contains 100 video
sequences and is the superset of OTB-2013 [31] dataset
which contains the original 50 video sequences. The Re-
sults on the OBT dataset is shown in the top row of Fig. 5.
It can be seen that our tracker performs on par with a range
of state-of-the-art trackers across two evaluation metrics.
The most similar tracker is HDT where both correlation
ﬁlter and deep features are adopted. However, HDT has
heuristically chosen a set of higher level deep feature lay-
ers whereas in our approach, all layers before max pooling
are universally chosen. The SRDCF tracker is better than
our tracker in the success metric due to heavy regularisation
for negative training examples in SRDCF tracker. Hence
it enables larger search area during tracking. Furthermore,
because SRDCF tracker is able to search for larger area dur-
ing detection (42 vs. our 3.22 the target size), it indirectly
manages the training set to handle the scenarios of occlu-
sion and drift, of which the proposed KMC has been mostly
penalised.

UAV123 Dataset: Visual tracking on unmanned aerial ve-
hicles (UAVs) is a very promising application, since the
camera can follow the target based on visual feedback and
actively change its orientation and position to optimise the
tracking performance. This UAV123 dataset contains a total

6

(a) Precision plot for OTB-2015 Dataset

(b) Success plot for OTB-2015 Dataset

(c) Precision plot for UAV123 Dataset

(d) Success plot for UAV123 Dataset

Figure 5: Precision plot and success plot of the state-of-the-art trackers for OTB-2015 and UAV123 dataset.

of 123 video sequences and more than 110K frames. The
major difference between this UAV123 dataset and other
popular tracking datasets is the effect of camera viewpoint
change arising from UAV motion, the variation in bounding
box size and aspect ratio with respect to the initial frame,
and longer tracking sequences on average due to the avail-
ability of mounted camera moving with the target.

Results are shown in the bottom row of Fig. 5. It can
be seen that due to heuristically chosen layers tuned to
the OBT dataset in HDT, its performance suffers on this
UAV123 dataset.
It worths noting that our trained multi-
resolution convnet has never seen any image in this dataset.
Moreover, no pesky threshold, hyperparameters nor weights
are altered when applying the trained KMC network on the
unseen UAV123 dataset, proving its ability to generalise
across various tracking scenarios.

5. Conclusions

In this paper, we propose a novel kernelised multi-
resolution convnet tracking algorithm that utilises the in-
termediate response maps from the kernelised correlation
ﬁlter outputs. The multi-resolution convnet learns the im-
plicit translational output accurately and later an adaptive

learning scheme is adopted for model update. The learning
paradigm is able to generalise across various datasets with-
out change of hyperparameters. Moreover, it opens the door
on the end-to-end temporal deep learning. Future works
include better regularisation method as in [6] for negative
instance mining, multi-layer fusion [24] and incorporating
recurrent nets on top to model temporal dynamics.

Details of the Code

The python based code can be found at:
https://github.com/stevenwudi/KMC_cvprw_2017

Acknowledgment

This project has received funding from National Natural Science Founda-
tion of China (NSFC) (61401287); Natural Science Foundation of Shen-
zhen (JCYJ20160307154003475, JCYJ2016050617265125).

References

[1] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui.
Visual object tracking using adaptive correlation ﬁlters. In
Conference on Computer Vision and Pattern Recognition,
2010.

7

[2] K. Chaudhuri, Y. Freund, and D. J. Hsu. A parameter-free
hedging algorithm. In Advances in neural information pro-
cessing systems, 2009.

[19] M. Mueller, N. Smith, and B. Ghanem. A benchmark and
simulator for uav tracking. In European Conference on Com-
puter Vision, 2016.

[3] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Accu-
In British

rate scale estimation for robust visual tracking.
Machine Vision Conference, 2014.

[4] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Dis-
criminative scale space tracking. In Transactions on Pattern
Analysis and Machine Intelligence, 2017.

[5] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg. Con-
volutional features for correlation ﬁlter based visual track-
International Conference on Computer Vision, work-
ing.
shop, 2015.

[6] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg. Learn-
ing spatially regularized correlation ﬁlters for visual track-
ing. International Conference on Computer Vision, 2015.
[7] M. Danelljan, K. F. S. Robinson, Andreas, and M. Felsberg.
Beyond correlation ﬁlters: Learning continuous convolution
operators for visual tracking. European Conference on Com-
puter Vision, 2016.

[8] S. Hare, A. Saffari, and P. Torr. Struck: Structured output
tracking with kernels. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, 2011.

[9] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. Ex-
ploiting the circulant structure of tracking-by-detection with
kernels. In European conference on computer vision, 2012.
[10] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-
IEEE
speed tracking with kernelized correlation ﬁlters.
Transactions on Pattern Analysis and Machine Intelligence,
2015.

[11] S. Hong, T. You, S. Kwak, and B. Han. Online tracking by
learning discriminative saliency map with convolutional neu-
ral network. International Conference on Machine Learning,
2015.

[12] Z. Hong, Z. Chen, C. Wang, X. Mei, D. Prokhorov, and
D. Tao. Multi-store tracker (muster): A cognitive psychology
inspired approach to object tracking. In IEEE Conference on
Computer Vision and Pattern Recognition, 2015.

[13] Z. Kalal, J. Matas, and K. Mikolajczyk. P-n learning: Boot-
strapping binary classiﬁers by structural constraints. In Con-
ference on Computer Vision and Pattern Recognition, 2010.
[14] Z. Kalal, K. Mikolajczyk, and J. Matas. Tracking-learning-
IEEE transactions on pattern analysis and ma-

detection.
chine intelligence, 2012.

[15] H. Li, Y. Li, and F. Porikli. Robust online visual tracking
with a single convolutional neural network. In Asian Confer-
ence on Computer Vision, 2014.

[16] Y. Li and J. Zhu. A scale adaptive kernel correlation ﬁlter
tracker with feature integration. European Conference on
Computer Vision, workshop, 2014.

[17] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. Conference on Com-
puter Vision and Pattern Recognition, 2015.

[18] C. Ma, J.-B. Huang, X. Yang, and M.-H. Yang. Hierarchical
convolutional features for visual tracking. In IEEE Interna-
tional Conference on Computer Vision, 2015.

8

[20] P. Ondr´uˇska and I. Posner. Deep tracking: Seeing beyond
seeing using recurrent neural networks. In Proceedings of the
Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.

[21] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, and J. L. M.-
In IEEE Conference on

H. Yang. Hedged deep tracking.
Computer Vision and Pattern Recognition, 2016.

[22] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recog-
nition challenge. International Journal of Computer Vision,
2015.

[23] B. Scholkopf and A. J. Smola. Learning with kernels: sup-
port vector machines, regularization, optimization, and be-
yond. MIT press, 2001.

[24] L. Shao, D. Wu, and X. Li. Learning deep and wide: A spec-
tral method for learning deep networks. IEEE Transactions
on Neural Networks and Learning Systems, 2014.

[25] L. Wang, W. Ouyang, X. Wang, and H. Lu. Visual track-
ing with fully convolutional networks. In Proceedings of the
IEEE International Conference on Computer Vision, 2015.

[26] L. Wang, W. Ouyang, X. Wang, and H. Lu. Stct: Sequen-
tially training convolutional networks for visual tracking. In
IEEE International Conference on Computer Vision, 2016.

[27] N. Wang and D.-Y. Yeung. Learning a deep compact image
In Advances in Neural

representation for visual tracking.
Information Processing Systems 26. 2013.

[28] N. Wang and D.-Y. Yeung. Learning a deep compact im-
age representation for visual tracking. In Advances in neural
information processing systems, 2013.

[29] D. Wu, L. Pigou, P.-J. Kindermans, N. D.-H. Le, L. Shao,
J. Dambre, and J.-M. Odobez. Deep dynamic neural net-
works for multimodal gesture segmentation and recognition.
IEEE transactions on pattern analysis and machine intelli-
gence, 2016.

[30] D. Wu and L. Shao. Leveraging hierarchical parametric
networks for skeletal joints based action segmentation and
recognition. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2014.

[31] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A
benchmark. In IEEE conference on computer vision and pat-
tern recognition, 2013.

[32] Y. Wu, J. Lim, and M.-H. Yang. Object tracking benchmark.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2015.

[33] M.-S.-S. S. Zhang, J. Meem: robust tracking via multiple ex-
perts using entropy minimization. In European Conference
on Computer Vision, 2014.

[34] W. Zou and N. Komodakis. Harf: Hierarchy-associated rich
features for salient object detection. In Proceedings of the
IEEE International Conference on Computer Vision, 2015.

Kernalised Multi-resolution Convnet for Visual Tracking

Di Wu, Wenbin Zou∗, Xia Li
Shenzhen University†
dwu, wzou, lixia@szu.edu.cn

Yong Zhao
Peking University Shenzhen Graduate School
yongzhao@pkusz.edu.cn

7
1
0
2
 
g
u
A
 
2
 
 
]

V
C
.
s
c
[
 
 
1
v
7
7
5
0
0
.
8
0
7
1
:
v
i
X
r
a

Abstract

Visual tracking is intrinsically a temporal problem. Dis-
criminative Correlation Filters (DCF) have demonstrated
excellent performance for high-speed generic visual object
tracking. Built upon their seminal work, there has been a
plethora of recent improvements relying on convolutional
neural network (CNN) pretrained on ImageNet as a feature
extractor for visual tracking. However, most of their works
relying on ad hoc analysis to design the weights for different
layers either using boosting or hedging techniques as an en-
semble tracker. In this paper, we go beyond the conventional
DCF framework and propose a Kernalised Multi-resolution
Convnet (KMC) formulation that utilises hierarchical re-
sponse maps to directly output the target movement. When
directly deployed the learnt network to predict the unseen
challenging UAV tracking dataset without any weight ad-
justment, the proposed model consistently achieves excel-
lent tracking performance. Moreover, the transfered multi-
reslution CNN renders it possible to be integrated into the
RNN temporal learning framework, therefore opening the
door on the end-to-end temporal deep learning (TDL) for
visual tracking.

1. Introduction

Visual tracking is the task of predicting the trajectory of a
target in a video. The task of tracking, a crucial component
of many computer vision systems, can be naturally speciﬁed
as an online learning problem. This paper focuses on the
challenging problem of monocular, generic, realistic object
tracking.

Visual object tracking has recently witnessed substantial
progress due to powerful features extracted using deep con-
volutional neural networks. For online applications, one
simple approach to transfer ofﬂine pre-trained CNN fea-
tures is to add one or more randomly initialised CNN layers,
named as adaptation layers, on top of the pre-trained CNN

∗Corresponding author.
†Shenzhen Key Lab of Advanced Telecommunication and Information

Processing, College of Information Engineering, Shenzhen University.

Figure 1: Response maps generated from hierarchical deep fea-
tures. Left: two frames from the “Bolt” sequence, red boxes are
image patches with padding added. Right: hierarchical response
maps from correlation ﬁlters (response maps have been resized
and rescaled to the same shape and scale). It can be seen that be-
cause different layers of deep features encode different semantics,
the maximum response points from different layers also vary.

model. However, as [26] empirically observe that this trans-
fer learning method suffers from severe over-ﬁtting because
of the limited training data. It is also observed in [29, 30]
that the differences in mean activations in intermediate lay-
ers have noticeable effect for multi-model fusion. The on-
line learnt parameters mainly focus on recent training sam-
ples and are less likely to be well generalised to both histor-
ical and future samples. This phenomenon can be fatal to
online visual tracking where the target often undergoes sig-
niﬁcant appearance changes or heavy occlusion. CNNs also
have shown impressive performance as an ofﬂine feature ex-
tractor for tracking [27, 18] in lieu of traditional handcrafted
features (e.g., HOG, Color Moment). Features from these
deep convolutional layers are discriminative while preserv-
ing spatial and structural information.

The marriage between DCF [9], which has the advan-
tage of being efﬁcient in training translational images in
the fourier space, and deep features, which excel at image
representation, further advances the visual tracking com-
munity [21, 7]. However, there are some conﬂicting con-

1

clusions concerning the representation power from differ-
ent layers of CNN: shallow convolutional layers have been
highlighted in the DCF-based methods [5] whereas [21]
found that the performance generally increases as layer
depth is increased hence only convolutional layers deeper
than 10th is used. Fig. 1 shows the response maps gener-
ated from hierarchical deep features. Since different layers
from convnet capture different semantic meanings, maxi-
mum translational locations obtained via different convnet
layers could vary: low level layers excel at discriminating
intra-class variations whereas high level layers excel at dis-
criminating inter-class variations. Hence, for generic object
tracking, it is imperative to combine features from different
layers to have a generalised representation from hierarchies
of response maps.

Visual tracking is intrinsically also a temporal problem,
but many previous approaches [1, 26] mainly focus on the
design of a robust appearance model. An end-to-end ob-
ject tracking approach which directly maps from raw sensor
input to object tracks in sensor space is proposed in [20].
Their proposed method works well in a simulated environ-
ment, but the applicability on the realistic RGB imagery
dataset is worth further investigation. On reason that tem-
poral deep learning based techniques are challenging to be
employed in realistic settings is due to the difﬁculty for re-
current net to have robust, meaningful temporal input.

In this paper, we propose a kernelised multi-resolution
convnet tracking algorithm that utilises the intermediate re-
sponse maps from the correlation ﬁlter output and learns the
implicit translational output from the multi-resolution con-
vnet. The key contributions can be summarised as follows:

• We incorporate kernalised form into CNN feature rep-
resentations for the non-linear regression tasks that
consistently outperforms its linear counterpart, con-
cluding the kernalised version should be the preferred
choice in the process of designing correlation ﬁlters.

• We learn a novel representation for multi-resolution re-
sponse maps generated from different layers of a pre-
trained CNN, negating the need to design the weights
for various convnets layers. Moreover, the learnt rep-
resentation renders it possible to be included into the
end-to-end temporal deep learning (TDL) pipeline.

• We use an adaptive hedge method to update the model
learning rate, taking the model stability into consider-
ation, making the update of target model smoother and
more robust.

2. Related Work

We review recent tracking methods closely related to this

work as in the following three sections.

Correlation ﬁlter based trackers: Most recent works [1,
26] strives for a better appearance models for the tracker.
DCF [1, 9] have demonstrated excellent performance for
high-speed visual object tracking. The key to their success
is by observing that the resulting data matrix of translated
patches is circulant, the cyclic shifts could be diagonalised
with the Discrete Fourier Transform, reducing both storage
and computation to obtain the next frame response map.
However, most of these trackers depend on the spatiotem-
poral consistency of visual cues. Therefore, they can handle
mostly short-term tracking or object with static appearance.

Deep feature based trackers: Recent works exploit the
structure of CNN to learn the target online:
a three-
layer CNN is trained on-the-ﬂight in [15]; a deep autoen-
coder [28] is ﬁrst pre-trained ofﬂine and then ﬁnetuned
for binary classiﬁcation in online tracking. Since the pre-
training is performed in an unsupervised way by recon-
structing gray images with very low resolution, the learned
deep features has limited discriminative power for track-
ing. Moreover, without pre-traning and with limited train-
ing samples obtained online, CNN fails to capture object se-
mantics and is not robust to deformation. Both [15] and [28]
train deep networks online with limited training samples,
and inevitably suffer from overﬁtting. Transferring the hi-
erarchical features learned for image classiﬁcation tasks
have been shown to be effective for numerous vision tasks,
e.g., image segmentation [17], salient object detection [34].
More recent methods [26, 25, 11, 18] adopt deep convolu-
tion networks trained on a large scale image classiﬁcation
task [22] to improve tracking performance. The rich rep-
resentation of transferred features from deep nets enables
trackers to construct more robust, power appearance model
over the traditional hand crafted feature based trackers.

Spatio-temporal model based trackers: Variations in the
appearance of the object in tracking, such as variations in
geometry/photometry, camera viewpoint, partial occlusion
or out-of-view, pose a major challenge to object tracking.
TLD [14] employs two experts to identify the false nega-
tives and false positives to train a detector. The experts are
independent, which ensures mutual compensation of their
errors to alleviate the problem of drifting. A short and long
term cognitive psychology principle is adopted in [12] to
design a ﬂexible representation that can adopt to changes
in object appearance during tracking. A parameter-free
Hedging algorithm is proposed in [2] for the problem of
decision-theoretic online learning, especially for the appli-
cations when the number of actions is very large and opti-
mally setting the parameter is not well understood. An im-
proved Hedge algorithm considering historical performance
is proposed in [21] to weight the decision from different
CNN layers.

2

Figure 2: The proposed algorithm overview: we ﬁrst extract hierarchical CNN features from the image patch of interest; then we project
deep features into kernel space for correlation ﬁlters; the target translational movement is predicted via multi-resolution convnets and later
we update the scale and appearance models using an adaptive learning rate scheme.

3. Proposed Algorithm

As shown in Fig. 2, the proposed approach consists of
three steps: extracting hierarchical CNN features, project-
ing features into kernel space for correlation ﬁlters, predict-
ing target translational movement via multi-resolution con-
vnet. We ﬁrst review the correlation ﬁlter as our building
block. Then we present the technical details of the projec-
tion of deep feature to kernel space, the model of learning
translational output using a multi-resolution convnets and
an adaptive learning rate scheme for model update.

Correlation Filter: Correlation ﬁlters based trackers [1, 9,
3, 10] exploit the circulant structure of training and testing
samples to greatly accelerate the training and testing pro-
cess. Let Xk ∈ (cid:60)D×M ×N denotes the feature sets where
D denotes the number of feature maps; M, N denote the
shape of feature maps and k denotes the k-th input feature
map extracted from the k-th convolutional layers from a
pretrained CNN; Y ∈ (cid:60)M ×N denotes a gaussian shaped
label matrix which is subject to a 2D Gaussian distribution
with zero mean and standard deviation proportional to the
target size. The goal of training is to ﬁnd a set of ﬁlters
Wk that minimises the squared error over sets of circulant
translated samples Xk and their regression targets Y:
(cid:107)Y − Xk • Wk(cid:107)2 + λ(cid:107)Wk(cid:107)2

Wk = argmin

(1)

Wk

where

Xk • Wk =

Xk

d (cid:12) Wk
d

D
(cid:88)

d=1
with the symbol (cid:12) denotes element-wise product. The min-
imizer has a closed-form:

W =

XT Y
XT X + λI

(2)

(3)

3

where I is an identity matrix and λ is a regularisation pa-
rameter that controls overﬁtting. We drop the superscript k
for notational simplicity. In general, a large system of lin-
ear equations must be solved to compute the solution, which
can become prohibitive in a real-time setting. With training
data being cyclic shifts patches, all operations can be done
element-wise on their diagonal elements [10].

The ﬁlter can be modelled in the Fourier domain by:

W = argmin

(cid:107)Y − X • W k(cid:107)2

F + λ(cid:107)W(cid:107)2
F

(4)

W

where

X • W =

X d (cid:12) W d

(5)

D
(cid:88)

d=1

The corresponding minimizer in the Fourier domain has

the closed-form:

W =

X ∗ (cid:12) Y
(X ∗ (cid:12) X + λI)

(6)

where ∗ denotes the Hermitian transpose and since diago-
nal matrices are symmetric, taking the Hermitian transpose
only left behind a complex-conjugate.

3.1. Kernelisation of Deep transfered features

In Kernelized Correlation Filter (KCF) [10], the feature
X is mapped to a Hilbert space φ(X). By employing a ker-
nel κ(X, X(cid:48)) = (cid:104)φ(X), φ(X(cid:48))(cid:105), Eq. 4 becomes:

W = argmin

(cid:107)Y − (cid:104)φ(X ), W(cid:105)(cid:107)2

F + λ(cid:107)W(cid:107)2
F

(7)

W

The power of the kernel trick comes from the implicit use
of a high-dimensional feature space φ(X) without ever in-
stantiating a vector in the space. Even though the regression

two consecutive frames; middle:

Figure 3: Response maps for one frame of the “Biker” sequence.
Left:
ideal response map (2D
gaussian); right: the actual response map. Arrows represent the
translational vector. Due to various factors, e.g., motion blur, tar-
get appearance change, scale change, boundary effect, the actual
response map is not a univariate gaussian and the maximum re-
sponse does not strictly correspond to the translational movement.

function’s complexity grows with the number of samples
which is the major drawback of the kernel trick, assuming
circulant data and the adopted kernel being shift invariant,
the kernel correlation can be computed efﬁciently. In our
experiment, an radial basis function (RBF) kernel, which
satisﬁes the shift invariant property, is adopted:

1
σ

kXX (cid:48)

= exp(−

((cid:107)X(cid:107)2 + (cid:107)X (cid:48)(cid:107)2 − 2F −1(X ∗ (cid:12) X (cid:48)))
(8)
where F −1 denotes the Inverse DFT and the full kernel cor-
relation can be computed in only O(n log n) time.

Expressing the solution W as a linear combination of the
samples: W = (cid:80)
i αiφ(X i) renders an alternative repre-
sentation α to be in the dual space, as opposed to the primal
space W (Representer Theorem [23]). The variables under
optimisation are thus α:

α = (kXX (cid:48)

+ λI)−1Y

(9)

For detection stage for translation estimation, given an
image patch feature Zk ∈ (cid:60)D×M ×N , the response map is
obtained by:

Figure 4: A multi-resolution convnet is deployed to decode the
hierarchical response maps.

consecutive frames, the precondition is that of static target
appearance, i.e., the maximum response map corresponds
to the cyclic shifts of appearance unchanged target. Due
to subtle changes of target appearance, e.g., rotation, scale,
cyclic boundary effect or motion blur of two consecutive
frames, the accuracy of ﬁnding the translation using maxi-
mum response could be compromised (c.f. Fig. 3).

Moreover, when using deep features from the pre-trained
convnets for the input representation X, the hierarchies
of feature maps capture different semantic information(c.f.
Fig. 1): features from deep layers capture rich category level
semantic information, which is useful for object classiﬁca-
tion, but they are not the optimal representation for visual
tracking because spatial details captured by earlier layers
are also important for accurately localising the targets. On
the other hand, as the features in the earlier layers capture
low level visual characteristics and are more class-generic
rather than discriminative as ones in the later layers, meth-
ods based on features from earlier layers are likely to fail in
challenging scenarios when the target size is small and high
level object is the target of tracking.

We interpret the response maps from the hierarchies of
convolutional layers as a nonlinear counter part of an image
pyramid representation and employ a learning framework
for mapping the hierarchical outputs to target translation.
A multi-resolution neural network (c.f. Fig. 4) is deployed
to exploit the implicit translational information from hierar-
chies response maps:

R(Zk) = F −1(F(kXZk(cid:48)

) (cid:12) F(αk))

(10)

∆x, ∆y ← convnets(R(Zk))

(12)

The model for frame t is updated with learning rate η as:

αk

t = (1 − η)αk

t−1 + ηαk

(11)

3.2. Decoding Response map using Multi-Res CNN

The loss of the convnets is set to be the root mean square
(RMS) of the normalised translational movement (∆x, ∆y)
and the predicted movement (∆x(cid:48), ∆y(cid:48)):

Lpos =

((cid:107) ∆x − ∆x(cid:48) (cid:107)2

2 + (cid:107) ∆y − ∆y(cid:48) (cid:107)2
2)

(13)

(cid:114) 1
2

Previous works estimate the next frame location via the
maximum response point on the response map. We ar-
gue that albeit the maximum response point carries phys-
ical meaning of the translational information between two

The proposed formulation enables efﬁcient integration of
multi-resolution response maps to decode translational in-
formation implicitly and demonstrates its competitiveness
over the corresponding counterpart baselines that using the

4

mean of maximum responses as the indicator for target
movement.

3.3. Adaptive learning rate

Traditional correlation based ﬁlter updates the model by
a ﬁxed parameter η. For generic object tracking, however,
there are two crucial factors for model update using deep
features: (1) the appearance of object of interest usually
changes at irregular pace (sometimes gently and sometimes
vehemently). This means that the scale of the learning rate
should reﬂect the appearance change of the target; (2) de-
pending on whether the object of interest being a low level
visual cues or high level object entity, the hierarchies of re-
sponse maps render correspondingly different maximum re-
sponse values.

Let the ultimate target position is predicted (xp, yp) at
time t, each layer k will incur a loss from its response map
Rk
t :

t = max(Rk
lk

t ) − Rk

t (xp, yp)

(14)

We then measure the stability of layer k at time t:

sk
t =

t − µk
|lk
t |
σk
t
t are the mean and variance for the loss lk

(15)

t dur-

t , σk
where µk
ing time period ∆t.
A larger sk

t indicates the layer is less correlated with the
object at frame t hence the model update η should be de-
creased as well. Therefore, we propose an adaptive learning
rate for different layers of model update that is linear to the
model stability as:

ηk = sk

t × η

(16)

Algorithm 1 summarises the main steps of the proposed

approach for visual tracking.

4. Experiments

We validate our proposed KMC framework by evaluat-
ing two genres of experiments: one compares with corre-
lation ﬁlter based baseline trackers and one compares with
several state-of-the-art trackers. We perform comprehen-
sive experiments on three datasets: OTB-2013 [31], OTB-
2015 [32] and UAV123 [19].

Implementation details: For feature extraction, we crop
the image patch with 2.2 padding size and resize the im-
age patch to 240 ∗ 160 because the average target size is of
ratio 3
2 . Feature bandwidth σ in Eq. 8 is 0.2. The learn-
ing rate η for Eq. 16 is 0.0025 which is only one ﬁfth of
ones chosen in [21] because of the model is in kernel space
and tends to be more stable. After the forward propaga-
tion, we use the VGG-Net with 19 layers and ﬁve out-
puts before the max pooling layers (i.e., ‘block1 conv2(cid:48),

Algorithm 1: Kernalised Multi-resolution Convnet

Input: target position (x1, y1) and size (w1, h1), ﬁrst

tracking frame;

Output: predicted target positions (xt, yt) and sizes
(wt, ht) in the following frames.

1 for t = 1, 2, . . . do
2

3

4

5

6

7

8

9

10

Crop target images with padding;
Extract deep features from layers before max
pooling layers in VGG-Net19 to obtain 5
hierarchical representations X;
Project deep features into kernel space 3.1;
if t = 1 then

Obtain deep kernel model α (Eq.9);

else

Update target position prediction by the
trained multi-resolution convnet 3.2;
Update model scale using DSST [4];
Update kernel model using adaptive learning
rate 3.3;

‘block3 conv4(cid:48),

‘block2 conv2(cid:48),
‘block5 conv4(cid:48)). Note
that instead of heuristically choosing layers as in [21, 7],
we have included all layer outputs before the max pooling
layers. This chosen methodology makes the approach more
generic and less dataset dependant.

The convnet in Sec. 3.2 is trained on OTB-2015 dataset.
Note that albeit trained on the OTB-2015 dataset, the con-
vnet during detection will not be able to see the exactly
same response maps twice unless exact location update is
achieved. Hence it requires the convnet to be able to gener-
alise to unseen hierarchical response maps and predict rea-
sonable translational output. Furthermore, we verify the
generalisation of the KMC over HDT [21] tracker on the
unseen UAV123 dataset.

For scale estimation, we use a scale pyramid representa-
tion as in [4] with 11 scales and a relative scale factor 1.02.
The updates consistently helps more accurate update of tar-
get models while maintaining the computational cost low.

Evaluation Methodology: Following the evaluation strat-
egy of
[31], all trackers are compared using two mea-
sures: precision and success. Precision is measured as the
distance between the centres of the ground truth bounding
box and the corresponding tracker generated bounding box.
The precision plot shows the percentage of tracker bound-
ing boxed within a given threshold distance in pixels of
the ground truth. To rank the trackers, the conventional
threshold of 20 pixels (P20) is adopted. Success is mea-
sured as the intersection over union of pixels. The success
plot shows the percentage of tracker bounding boxes whose
overlap score is larger than a given threshold and the track-

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Method

Dataset

DeepDCF
DeepKCF
MaxRes
Multi-Resolution CNN
Fixed Learning Rate
Adaptive Learning Rate

OTB-2013

OTB-2015

P20
58.0
66.2 (8.2 ↑)
57.5
70.6(13.1 ↑)
74.0(3.4 ↑)
74.2 (0.2 ↑)

AUC
38.4
46.1 (7.7 ↑)
38.8
46.7 (7.9 ↑)
51.8(5.1 ↑)
52.1(0.3 ↑)

P20
64.3
71.4 (7.1 ↑)
65.8
77.8 (12.0 ↑)
79.8(2.0 ↑)
79.9(0.1 ↑)

AUC
44.0
49.1 (5.1 ↑)
45.9
53.6 (7.7 ↑)
57.0(3.4 ↑)
57.4(0.4 ↑)

Table 1: Baseline comparisons with arrows indicating performance changes comparing with the upper row parallel experiment. First two
rows investigate the impact of projecting deep features into a kernel space vs. using linear deep features. Next two rows compare the
traditional using max location vs. the proposed multi-resolution convnets paradigm for decoding target translational movement. Last two
rows illustrate the beneﬁt of adaptive learning rate through time (with scale update).

ers are ranked according to the Area Under Curve (AUC)
criteria. All sequences are evaluated using One-Pass Evalu-
ation (OPE) as in [31].

4.1. Baseline comparison

Beneﬁt of Kernalising deep features: We ﬁrst evalu-
ate the impact of projecting deep features into a kernel
space (DeepKCF) vs. using the traditional linear projection
(DeepDCF) in ﬁrst two rows of Tab. 1. The deep features
used are the layer from ‘block3 conv4(cid:48). It can be seen that
projecting deep feature into kernel space consistently yields
a noticeable increase in both performance measures. The
computation cost for kernel projection is O(n log n) and for
linear kernel is O(n). Therefore, given marginal increase in
computational cost, we conclude that for constructing ap-
pearance model, kernel projection is the preferred method-
ology versus the traditional linear projection.

Beneﬁt of multi-resolution CNN: We then evaluate the
impact of decoding response map using a multi-resolution
CNN (Multi-Resolution CNN) vs. directly using the max
location information (MaxRes) for decoding target location
from hierarchical response maps in Tab. 1. MaxRes use the
maximum position from the mean of hierarchical response
maps with later response maps being resized to the largest
ﬁrst layer response map of size 240 ∗ 160. The performance
of MaxRes is even worse than using single layer, signify-
ing the need to intelligently combine multiply layer out-
put. The learnt convnets consistently performs better than
that of MaxRes for a large margin. Moreover, the learn-
ing paradigm opens the door for the integration of recurrent
network into the pipeline.

Beneﬁt of adaptive learning rate through time: We also
investigate the scheme of adjusting the learning rate adap-
tively vs. ﬁxing the learning rate through time in last two
rows of Tab. 1. Consistent improvements across two metrics

are observed. The margin of improvement, however, is less
prominent. We reason that in part, it is due to almost sat-
urate performance; on the other hand, an end-to-end learn-
ing scheme for model update could be more preferable and
should be further investigated.

4.2. Comparisons with the-state-of-the-art trackers

We validate our KMC tracker in a comprehensive
comparison with 10 state-of-the-art trackers: HDT [21],
SRDCF [6], MEEM [33], MUSTER [12], SAMF [16],
DSST [4], KCF [10], Struck [8], TLD [13], DCF [9].

OTB-2015 Dataset: OTB-2015 dataset contains 100 video
sequences and is the superset of OTB-2013 [31] dataset
which contains the original 50 video sequences. The Re-
sults on the OBT dataset is shown in the top row of Fig. 5.
It can be seen that our tracker performs on par with a range
of state-of-the-art trackers across two evaluation metrics.
The most similar tracker is HDT where both correlation
ﬁlter and deep features are adopted. However, HDT has
heuristically chosen a set of higher level deep feature lay-
ers whereas in our approach, all layers before max pooling
are universally chosen. The SRDCF tracker is better than
our tracker in the success metric due to heavy regularisation
for negative training examples in SRDCF tracker. Hence
it enables larger search area during tracking. Furthermore,
because SRDCF tracker is able to search for larger area dur-
ing detection (42 vs. our 3.22 the target size), it indirectly
manages the training set to handle the scenarios of occlu-
sion and drift, of which the proposed KMC has been mostly
penalised.

UAV123 Dataset: Visual tracking on unmanned aerial ve-
hicles (UAVs) is a very promising application, since the
camera can follow the target based on visual feedback and
actively change its orientation and position to optimise the
tracking performance. This UAV123 dataset contains a total

6

(a) Precision plot for OTB-2015 Dataset

(b) Success plot for OTB-2015 Dataset

(c) Precision plot for UAV123 Dataset

(d) Success plot for UAV123 Dataset

Figure 5: Precision plot and success plot of the state-of-the-art trackers for OTB-2015 and UAV123 dataset.

of 123 video sequences and more than 110K frames. The
major difference between this UAV123 dataset and other
popular tracking datasets is the effect of camera viewpoint
change arising from UAV motion, the variation in bounding
box size and aspect ratio with respect to the initial frame,
and longer tracking sequences on average due to the avail-
ability of mounted camera moving with the target.

Results are shown in the bottom row of Fig. 5. It can
be seen that due to heuristically chosen layers tuned to
the OBT dataset in HDT, its performance suffers on this
UAV123 dataset.
It worths noting that our trained multi-
resolution convnet has never seen any image in this dataset.
Moreover, no pesky threshold, hyperparameters nor weights
are altered when applying the trained KMC network on the
unseen UAV123 dataset, proving its ability to generalise
across various tracking scenarios.

5. Conclusions

In this paper, we propose a novel kernelised multi-
resolution convnet tracking algorithm that utilises the in-
termediate response maps from the kernelised correlation
ﬁlter outputs. The multi-resolution convnet learns the im-
plicit translational output accurately and later an adaptive

learning scheme is adopted for model update. The learning
paradigm is able to generalise across various datasets with-
out change of hyperparameters. Moreover, it opens the door
on the end-to-end temporal deep learning. Future works
include better regularisation method as in [6] for negative
instance mining, multi-layer fusion [24] and incorporating
recurrent nets on top to model temporal dynamics.

Details of the Code

The python based code can be found at:
https://github.com/stevenwudi/KMC_cvprw_2017

Acknowledgment

This project has received funding from National Natural Science Founda-
tion of China (NSFC) (61401287); Natural Science Foundation of Shen-
zhen (JCYJ20160307154003475, JCYJ2016050617265125).

References

[1] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui.
Visual object tracking using adaptive correlation ﬁlters. In
Conference on Computer Vision and Pattern Recognition,
2010.

7

[2] K. Chaudhuri, Y. Freund, and D. J. Hsu. A parameter-free
hedging algorithm. In Advances in neural information pro-
cessing systems, 2009.

[19] M. Mueller, N. Smith, and B. Ghanem. A benchmark and
simulator for uav tracking. In European Conference on Com-
puter Vision, 2016.

[3] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Accu-
In British

rate scale estimation for robust visual tracking.
Machine Vision Conference, 2014.

[4] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Dis-
criminative scale space tracking. In Transactions on Pattern
Analysis and Machine Intelligence, 2017.

[5] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg. Con-
volutional features for correlation ﬁlter based visual track-
International Conference on Computer Vision, work-
ing.
shop, 2015.

[6] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg. Learn-
ing spatially regularized correlation ﬁlters for visual track-
ing. International Conference on Computer Vision, 2015.
[7] M. Danelljan, K. F. S. Robinson, Andreas, and M. Felsberg.
Beyond correlation ﬁlters: Learning continuous convolution
operators for visual tracking. European Conference on Com-
puter Vision, 2016.

[8] S. Hare, A. Saffari, and P. Torr. Struck: Structured output
tracking with kernels. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, 2011.

[9] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. Ex-
ploiting the circulant structure of tracking-by-detection with
kernels. In European conference on computer vision, 2012.
[10] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-
IEEE
speed tracking with kernelized correlation ﬁlters.
Transactions on Pattern Analysis and Machine Intelligence,
2015.

[11] S. Hong, T. You, S. Kwak, and B. Han. Online tracking by
learning discriminative saliency map with convolutional neu-
ral network. International Conference on Machine Learning,
2015.

[12] Z. Hong, Z. Chen, C. Wang, X. Mei, D. Prokhorov, and
D. Tao. Multi-store tracker (muster): A cognitive psychology
inspired approach to object tracking. In IEEE Conference on
Computer Vision and Pattern Recognition, 2015.

[13] Z. Kalal, J. Matas, and K. Mikolajczyk. P-n learning: Boot-
strapping binary classiﬁers by structural constraints. In Con-
ference on Computer Vision and Pattern Recognition, 2010.
[14] Z. Kalal, K. Mikolajczyk, and J. Matas. Tracking-learning-
IEEE transactions on pattern analysis and ma-

detection.
chine intelligence, 2012.

[15] H. Li, Y. Li, and F. Porikli. Robust online visual tracking
with a single convolutional neural network. In Asian Confer-
ence on Computer Vision, 2014.

[16] Y. Li and J. Zhu. A scale adaptive kernel correlation ﬁlter
tracker with feature integration. European Conference on
Computer Vision, workshop, 2014.

[17] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. Conference on Com-
puter Vision and Pattern Recognition, 2015.

[18] C. Ma, J.-B. Huang, X. Yang, and M.-H. Yang. Hierarchical
convolutional features for visual tracking. In IEEE Interna-
tional Conference on Computer Vision, 2015.

8

[20] P. Ondr´uˇska and I. Posner. Deep tracking: Seeing beyond
seeing using recurrent neural networks. In Proceedings of the
Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.

[21] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, and J. L. M.-
In IEEE Conference on

H. Yang. Hedged deep tracking.
Computer Vision and Pattern Recognition, 2016.

[22] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recog-
nition challenge. International Journal of Computer Vision,
2015.

[23] B. Scholkopf and A. J. Smola. Learning with kernels: sup-
port vector machines, regularization, optimization, and be-
yond. MIT press, 2001.

[24] L. Shao, D. Wu, and X. Li. Learning deep and wide: A spec-
tral method for learning deep networks. IEEE Transactions
on Neural Networks and Learning Systems, 2014.

[25] L. Wang, W. Ouyang, X. Wang, and H. Lu. Visual track-
ing with fully convolutional networks. In Proceedings of the
IEEE International Conference on Computer Vision, 2015.

[26] L. Wang, W. Ouyang, X. Wang, and H. Lu. Stct: Sequen-
tially training convolutional networks for visual tracking. In
IEEE International Conference on Computer Vision, 2016.

[27] N. Wang and D.-Y. Yeung. Learning a deep compact image
In Advances in Neural

representation for visual tracking.
Information Processing Systems 26. 2013.

[28] N. Wang and D.-Y. Yeung. Learning a deep compact im-
age representation for visual tracking. In Advances in neural
information processing systems, 2013.

[29] D. Wu, L. Pigou, P.-J. Kindermans, N. D.-H. Le, L. Shao,
J. Dambre, and J.-M. Odobez. Deep dynamic neural net-
works for multimodal gesture segmentation and recognition.
IEEE transactions on pattern analysis and machine intelli-
gence, 2016.

[30] D. Wu and L. Shao. Leveraging hierarchical parametric
networks for skeletal joints based action segmentation and
recognition. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2014.

[31] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A
benchmark. In IEEE conference on computer vision and pat-
tern recognition, 2013.

[32] Y. Wu, J. Lim, and M.-H. Yang. Object tracking benchmark.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2015.

[33] M.-S.-S. S. Zhang, J. Meem: robust tracking via multiple ex-
perts using entropy minimization. In European Conference
on Computer Vision, 2014.

[34] W. Zou and N. Komodakis. Harf: Hierarchy-associated rich
features for salient object detection. In Proceedings of the
IEEE International Conference on Computer Vision, 2015.

Kernalised Multi-resolution Convnet for Visual Tracking

Di Wu, Wenbin Zou∗, Xia Li
Shenzhen University†
dwu, wzou, lixia@szu.edu.cn

Yong Zhao
Peking University Shenzhen Graduate School
yongzhao@pkusz.edu.cn

7
1
0
2
 
g
u
A
 
2
 
 
]

V
C
.
s
c
[
 
 
1
v
7
7
5
0
0
.
8
0
7
1
:
v
i
X
r
a

Abstract

Visual tracking is intrinsically a temporal problem. Dis-
criminative Correlation Filters (DCF) have demonstrated
excellent performance for high-speed generic visual object
tracking. Built upon their seminal work, there has been a
plethora of recent improvements relying on convolutional
neural network (CNN) pretrained on ImageNet as a feature
extractor for visual tracking. However, most of their works
relying on ad hoc analysis to design the weights for different
layers either using boosting or hedging techniques as an en-
semble tracker. In this paper, we go beyond the conventional
DCF framework and propose a Kernalised Multi-resolution
Convnet (KMC) formulation that utilises hierarchical re-
sponse maps to directly output the target movement. When
directly deployed the learnt network to predict the unseen
challenging UAV tracking dataset without any weight ad-
justment, the proposed model consistently achieves excel-
lent tracking performance. Moreover, the transfered multi-
reslution CNN renders it possible to be integrated into the
RNN temporal learning framework, therefore opening the
door on the end-to-end temporal deep learning (TDL) for
visual tracking.

1. Introduction

Visual tracking is the task of predicting the trajectory of a
target in a video. The task of tracking, a crucial component
of many computer vision systems, can be naturally speciﬁed
as an online learning problem. This paper focuses on the
challenging problem of monocular, generic, realistic object
tracking.

Visual object tracking has recently witnessed substantial
progress due to powerful features extracted using deep con-
volutional neural networks. For online applications, one
simple approach to transfer ofﬂine pre-trained CNN fea-
tures is to add one or more randomly initialised CNN layers,
named as adaptation layers, on top of the pre-trained CNN

∗Corresponding author.
†Shenzhen Key Lab of Advanced Telecommunication and Information

Processing, College of Information Engineering, Shenzhen University.

Figure 1: Response maps generated from hierarchical deep fea-
tures. Left: two frames from the “Bolt” sequence, red boxes are
image patches with padding added. Right: hierarchical response
maps from correlation ﬁlters (response maps have been resized
and rescaled to the same shape and scale). It can be seen that be-
cause different layers of deep features encode different semantics,
the maximum response points from different layers also vary.

model. However, as [26] empirically observe that this trans-
fer learning method suffers from severe over-ﬁtting because
of the limited training data. It is also observed in [29, 30]
that the differences in mean activations in intermediate lay-
ers have noticeable effect for multi-model fusion. The on-
line learnt parameters mainly focus on recent training sam-
ples and are less likely to be well generalised to both histor-
ical and future samples. This phenomenon can be fatal to
online visual tracking where the target often undergoes sig-
niﬁcant appearance changes or heavy occlusion. CNNs also
have shown impressive performance as an ofﬂine feature ex-
tractor for tracking [27, 18] in lieu of traditional handcrafted
features (e.g., HOG, Color Moment). Features from these
deep convolutional layers are discriminative while preserv-
ing spatial and structural information.

The marriage between DCF [9], which has the advan-
tage of being efﬁcient in training translational images in
the fourier space, and deep features, which excel at image
representation, further advances the visual tracking com-
munity [21, 7]. However, there are some conﬂicting con-

1

clusions concerning the representation power from differ-
ent layers of CNN: shallow convolutional layers have been
highlighted in the DCF-based methods [5] whereas [21]
found that the performance generally increases as layer
depth is increased hence only convolutional layers deeper
than 10th is used. Fig. 1 shows the response maps gener-
ated from hierarchical deep features. Since different layers
from convnet capture different semantic meanings, maxi-
mum translational locations obtained via different convnet
layers could vary: low level layers excel at discriminating
intra-class variations whereas high level layers excel at dis-
criminating inter-class variations. Hence, for generic object
tracking, it is imperative to combine features from different
layers to have a generalised representation from hierarchies
of response maps.

Visual tracking is intrinsically also a temporal problem,
but many previous approaches [1, 26] mainly focus on the
design of a robust appearance model. An end-to-end ob-
ject tracking approach which directly maps from raw sensor
input to object tracks in sensor space is proposed in [20].
Their proposed method works well in a simulated environ-
ment, but the applicability on the realistic RGB imagery
dataset is worth further investigation. On reason that tem-
poral deep learning based techniques are challenging to be
employed in realistic settings is due to the difﬁculty for re-
current net to have robust, meaningful temporal input.

In this paper, we propose a kernelised multi-resolution
convnet tracking algorithm that utilises the intermediate re-
sponse maps from the correlation ﬁlter output and learns the
implicit translational output from the multi-resolution con-
vnet. The key contributions can be summarised as follows:

• We incorporate kernalised form into CNN feature rep-
resentations for the non-linear regression tasks that
consistently outperforms its linear counterpart, con-
cluding the kernalised version should be the preferred
choice in the process of designing correlation ﬁlters.

• We learn a novel representation for multi-resolution re-
sponse maps generated from different layers of a pre-
trained CNN, negating the need to design the weights
for various convnets layers. Moreover, the learnt rep-
resentation renders it possible to be included into the
end-to-end temporal deep learning (TDL) pipeline.

• We use an adaptive hedge method to update the model
learning rate, taking the model stability into consider-
ation, making the update of target model smoother and
more robust.

2. Related Work

We review recent tracking methods closely related to this

work as in the following three sections.

Correlation ﬁlter based trackers: Most recent works [1,
26] strives for a better appearance models for the tracker.
DCF [1, 9] have demonstrated excellent performance for
high-speed visual object tracking. The key to their success
is by observing that the resulting data matrix of translated
patches is circulant, the cyclic shifts could be diagonalised
with the Discrete Fourier Transform, reducing both storage
and computation to obtain the next frame response map.
However, most of these trackers depend on the spatiotem-
poral consistency of visual cues. Therefore, they can handle
mostly short-term tracking or object with static appearance.

Deep feature based trackers: Recent works exploit the
structure of CNN to learn the target online:
a three-
layer CNN is trained on-the-ﬂight in [15]; a deep autoen-
coder [28] is ﬁrst pre-trained ofﬂine and then ﬁnetuned
for binary classiﬁcation in online tracking. Since the pre-
training is performed in an unsupervised way by recon-
structing gray images with very low resolution, the learned
deep features has limited discriminative power for track-
ing. Moreover, without pre-traning and with limited train-
ing samples obtained online, CNN fails to capture object se-
mantics and is not robust to deformation. Both [15] and [28]
train deep networks online with limited training samples,
and inevitably suffer from overﬁtting. Transferring the hi-
erarchical features learned for image classiﬁcation tasks
have been shown to be effective for numerous vision tasks,
e.g., image segmentation [17], salient object detection [34].
More recent methods [26, 25, 11, 18] adopt deep convolu-
tion networks trained on a large scale image classiﬁcation
task [22] to improve tracking performance. The rich rep-
resentation of transferred features from deep nets enables
trackers to construct more robust, power appearance model
over the traditional hand crafted feature based trackers.

Spatio-temporal model based trackers: Variations in the
appearance of the object in tracking, such as variations in
geometry/photometry, camera viewpoint, partial occlusion
or out-of-view, pose a major challenge to object tracking.
TLD [14] employs two experts to identify the false nega-
tives and false positives to train a detector. The experts are
independent, which ensures mutual compensation of their
errors to alleviate the problem of drifting. A short and long
term cognitive psychology principle is adopted in [12] to
design a ﬂexible representation that can adopt to changes
in object appearance during tracking. A parameter-free
Hedging algorithm is proposed in [2] for the problem of
decision-theoretic online learning, especially for the appli-
cations when the number of actions is very large and opti-
mally setting the parameter is not well understood. An im-
proved Hedge algorithm considering historical performance
is proposed in [21] to weight the decision from different
CNN layers.

2

Figure 2: The proposed algorithm overview: we ﬁrst extract hierarchical CNN features from the image patch of interest; then we project
deep features into kernel space for correlation ﬁlters; the target translational movement is predicted via multi-resolution convnets and later
we update the scale and appearance models using an adaptive learning rate scheme.

3. Proposed Algorithm

As shown in Fig. 2, the proposed approach consists of
three steps: extracting hierarchical CNN features, project-
ing features into kernel space for correlation ﬁlters, predict-
ing target translational movement via multi-resolution con-
vnet. We ﬁrst review the correlation ﬁlter as our building
block. Then we present the technical details of the projec-
tion of deep feature to kernel space, the model of learning
translational output using a multi-resolution convnets and
an adaptive learning rate scheme for model update.

Correlation Filter: Correlation ﬁlters based trackers [1, 9,
3, 10] exploit the circulant structure of training and testing
samples to greatly accelerate the training and testing pro-
cess. Let Xk ∈ (cid:60)D×M ×N denotes the feature sets where
D denotes the number of feature maps; M, N denote the
shape of feature maps and k denotes the k-th input feature
map extracted from the k-th convolutional layers from a
pretrained CNN; Y ∈ (cid:60)M ×N denotes a gaussian shaped
label matrix which is subject to a 2D Gaussian distribution
with zero mean and standard deviation proportional to the
target size. The goal of training is to ﬁnd a set of ﬁlters
Wk that minimises the squared error over sets of circulant
translated samples Xk and their regression targets Y:
(cid:107)Y − Xk • Wk(cid:107)2 + λ(cid:107)Wk(cid:107)2

Wk = argmin

(1)

Wk

where

Xk • Wk =

Xk

d (cid:12) Wk
d

D
(cid:88)

d=1
with the symbol (cid:12) denotes element-wise product. The min-
imizer has a closed-form:

W =

XT Y
XT X + λI

(2)

(3)

3

where I is an identity matrix and λ is a regularisation pa-
rameter that controls overﬁtting. We drop the superscript k
for notational simplicity. In general, a large system of lin-
ear equations must be solved to compute the solution, which
can become prohibitive in a real-time setting. With training
data being cyclic shifts patches, all operations can be done
element-wise on their diagonal elements [10].

The ﬁlter can be modelled in the Fourier domain by:

W = argmin

(cid:107)Y − X • W k(cid:107)2

F + λ(cid:107)W(cid:107)2
F

(4)

W

where

X • W =

X d (cid:12) W d

(5)

D
(cid:88)

d=1

The corresponding minimizer in the Fourier domain has

the closed-form:

W =

X ∗ (cid:12) Y
(X ∗ (cid:12) X + λI)

(6)

where ∗ denotes the Hermitian transpose and since diago-
nal matrices are symmetric, taking the Hermitian transpose
only left behind a complex-conjugate.

3.1. Kernelisation of Deep transfered features

In Kernelized Correlation Filter (KCF) [10], the feature
X is mapped to a Hilbert space φ(X). By employing a ker-
nel κ(X, X(cid:48)) = (cid:104)φ(X), φ(X(cid:48))(cid:105), Eq. 4 becomes:

W = argmin

(cid:107)Y − (cid:104)φ(X ), W(cid:105)(cid:107)2

F + λ(cid:107)W(cid:107)2
F

(7)

W

The power of the kernel trick comes from the implicit use
of a high-dimensional feature space φ(X) without ever in-
stantiating a vector in the space. Even though the regression

two consecutive frames; middle:

Figure 3: Response maps for one frame of the “Biker” sequence.
Left:
ideal response map (2D
gaussian); right: the actual response map. Arrows represent the
translational vector. Due to various factors, e.g., motion blur, tar-
get appearance change, scale change, boundary effect, the actual
response map is not a univariate gaussian and the maximum re-
sponse does not strictly correspond to the translational movement.

function’s complexity grows with the number of samples
which is the major drawback of the kernel trick, assuming
circulant data and the adopted kernel being shift invariant,
the kernel correlation can be computed efﬁciently. In our
experiment, an radial basis function (RBF) kernel, which
satisﬁes the shift invariant property, is adopted:

1
σ

kXX (cid:48)

= exp(−

((cid:107)X(cid:107)2 + (cid:107)X (cid:48)(cid:107)2 − 2F −1(X ∗ (cid:12) X (cid:48)))
(8)
where F −1 denotes the Inverse DFT and the full kernel cor-
relation can be computed in only O(n log n) time.

Expressing the solution W as a linear combination of the
samples: W = (cid:80)
i αiφ(X i) renders an alternative repre-
sentation α to be in the dual space, as opposed to the primal
space W (Representer Theorem [23]). The variables under
optimisation are thus α:

α = (kXX (cid:48)

+ λI)−1Y

(9)

For detection stage for translation estimation, given an
image patch feature Zk ∈ (cid:60)D×M ×N , the response map is
obtained by:

Figure 4: A multi-resolution convnet is deployed to decode the
hierarchical response maps.

consecutive frames, the precondition is that of static target
appearance, i.e., the maximum response map corresponds
to the cyclic shifts of appearance unchanged target. Due
to subtle changes of target appearance, e.g., rotation, scale,
cyclic boundary effect or motion blur of two consecutive
frames, the accuracy of ﬁnding the translation using maxi-
mum response could be compromised (c.f. Fig. 3).

Moreover, when using deep features from the pre-trained
convnets for the input representation X, the hierarchies
of feature maps capture different semantic information(c.f.
Fig. 1): features from deep layers capture rich category level
semantic information, which is useful for object classiﬁca-
tion, but they are not the optimal representation for visual
tracking because spatial details captured by earlier layers
are also important for accurately localising the targets. On
the other hand, as the features in the earlier layers capture
low level visual characteristics and are more class-generic
rather than discriminative as ones in the later layers, meth-
ods based on features from earlier layers are likely to fail in
challenging scenarios when the target size is small and high
level object is the target of tracking.

We interpret the response maps from the hierarchies of
convolutional layers as a nonlinear counter part of an image
pyramid representation and employ a learning framework
for mapping the hierarchical outputs to target translation.
A multi-resolution neural network (c.f. Fig. 4) is deployed
to exploit the implicit translational information from hierar-
chies response maps:

R(Zk) = F −1(F(kXZk(cid:48)

) (cid:12) F(αk))

(10)

∆x, ∆y ← convnets(R(Zk))

(12)

The model for frame t is updated with learning rate η as:

αk

t = (1 − η)αk

t−1 + ηαk

(11)

3.2. Decoding Response map using Multi-Res CNN

The loss of the convnets is set to be the root mean square
(RMS) of the normalised translational movement (∆x, ∆y)
and the predicted movement (∆x(cid:48), ∆y(cid:48)):

Lpos =

((cid:107) ∆x − ∆x(cid:48) (cid:107)2

2 + (cid:107) ∆y − ∆y(cid:48) (cid:107)2
2)

(13)

(cid:114) 1
2

Previous works estimate the next frame location via the
maximum response point on the response map. We ar-
gue that albeit the maximum response point carries phys-
ical meaning of the translational information between two

The proposed formulation enables efﬁcient integration of
multi-resolution response maps to decode translational in-
formation implicitly and demonstrates its competitiveness
over the corresponding counterpart baselines that using the

4

mean of maximum responses as the indicator for target
movement.

3.3. Adaptive learning rate

Traditional correlation based ﬁlter updates the model by
a ﬁxed parameter η. For generic object tracking, however,
there are two crucial factors for model update using deep
features: (1) the appearance of object of interest usually
changes at irregular pace (sometimes gently and sometimes
vehemently). This means that the scale of the learning rate
should reﬂect the appearance change of the target; (2) de-
pending on whether the object of interest being a low level
visual cues or high level object entity, the hierarchies of re-
sponse maps render correspondingly different maximum re-
sponse values.

Let the ultimate target position is predicted (xp, yp) at
time t, each layer k will incur a loss from its response map
Rk
t :

t = max(Rk
lk

t ) − Rk

t (xp, yp)

(14)

We then measure the stability of layer k at time t:

sk
t =

t − µk
|lk
t |
σk
t
t are the mean and variance for the loss lk

(15)

t dur-

t , σk
where µk
ing time period ∆t.
A larger sk

t indicates the layer is less correlated with the
object at frame t hence the model update η should be de-
creased as well. Therefore, we propose an adaptive learning
rate for different layers of model update that is linear to the
model stability as:

ηk = sk

t × η

(16)

Algorithm 1 summarises the main steps of the proposed

approach for visual tracking.

4. Experiments

We validate our proposed KMC framework by evaluat-
ing two genres of experiments: one compares with corre-
lation ﬁlter based baseline trackers and one compares with
several state-of-the-art trackers. We perform comprehen-
sive experiments on three datasets: OTB-2013 [31], OTB-
2015 [32] and UAV123 [19].

Implementation details: For feature extraction, we crop
the image patch with 2.2 padding size and resize the im-
age patch to 240 ∗ 160 because the average target size is of
ratio 3
2 . Feature bandwidth σ in Eq. 8 is 0.2. The learn-
ing rate η for Eq. 16 is 0.0025 which is only one ﬁfth of
ones chosen in [21] because of the model is in kernel space
and tends to be more stable. After the forward propaga-
tion, we use the VGG-Net with 19 layers and ﬁve out-
puts before the max pooling layers (i.e., ‘block1 conv2(cid:48),

Algorithm 1: Kernalised Multi-resolution Convnet

Input: target position (x1, y1) and size (w1, h1), ﬁrst

tracking frame;

Output: predicted target positions (xt, yt) and sizes
(wt, ht) in the following frames.

1 for t = 1, 2, . . . do
2

3

4

5

6

7

8

9

10

Crop target images with padding;
Extract deep features from layers before max
pooling layers in VGG-Net19 to obtain 5
hierarchical representations X;
Project deep features into kernel space 3.1;
if t = 1 then

Obtain deep kernel model α (Eq.9);

else

Update target position prediction by the
trained multi-resolution convnet 3.2;
Update model scale using DSST [4];
Update kernel model using adaptive learning
rate 3.3;

‘block3 conv4(cid:48),

‘block2 conv2(cid:48),
‘block5 conv4(cid:48)). Note
that instead of heuristically choosing layers as in [21, 7],
we have included all layer outputs before the max pooling
layers. This chosen methodology makes the approach more
generic and less dataset dependant.

The convnet in Sec. 3.2 is trained on OTB-2015 dataset.
Note that albeit trained on the OTB-2015 dataset, the con-
vnet during detection will not be able to see the exactly
same response maps twice unless exact location update is
achieved. Hence it requires the convnet to be able to gener-
alise to unseen hierarchical response maps and predict rea-
sonable translational output. Furthermore, we verify the
generalisation of the KMC over HDT [21] tracker on the
unseen UAV123 dataset.

For scale estimation, we use a scale pyramid representa-
tion as in [4] with 11 scales and a relative scale factor 1.02.
The updates consistently helps more accurate update of tar-
get models while maintaining the computational cost low.

Evaluation Methodology: Following the evaluation strat-
egy of
[31], all trackers are compared using two mea-
sures: precision and success. Precision is measured as the
distance between the centres of the ground truth bounding
box and the corresponding tracker generated bounding box.
The precision plot shows the percentage of tracker bound-
ing boxed within a given threshold distance in pixels of
the ground truth. To rank the trackers, the conventional
threshold of 20 pixels (P20) is adopted. Success is mea-
sured as the intersection over union of pixels. The success
plot shows the percentage of tracker bounding boxes whose
overlap score is larger than a given threshold and the track-

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Method

Dataset

DeepDCF
DeepKCF
MaxRes
Multi-Resolution CNN
Fixed Learning Rate
Adaptive Learning Rate

OTB-2013

OTB-2015

P20
58.0
66.2 (8.2 ↑)
57.5
70.6(13.1 ↑)
74.0(3.4 ↑)
74.2 (0.2 ↑)

AUC
38.4
46.1 (7.7 ↑)
38.8
46.7 (7.9 ↑)
51.8(5.1 ↑)
52.1(0.3 ↑)

P20
64.3
71.4 (7.1 ↑)
65.8
77.8 (12.0 ↑)
79.8(2.0 ↑)
79.9(0.1 ↑)

AUC
44.0
49.1 (5.1 ↑)
45.9
53.6 (7.7 ↑)
57.0(3.4 ↑)
57.4(0.4 ↑)

Table 1: Baseline comparisons with arrows indicating performance changes comparing with the upper row parallel experiment. First two
rows investigate the impact of projecting deep features into a kernel space vs. using linear deep features. Next two rows compare the
traditional using max location vs. the proposed multi-resolution convnets paradigm for decoding target translational movement. Last two
rows illustrate the beneﬁt of adaptive learning rate through time (with scale update).

ers are ranked according to the Area Under Curve (AUC)
criteria. All sequences are evaluated using One-Pass Evalu-
ation (OPE) as in [31].

4.1. Baseline comparison

Beneﬁt of Kernalising deep features: We ﬁrst evalu-
ate the impact of projecting deep features into a kernel
space (DeepKCF) vs. using the traditional linear projection
(DeepDCF) in ﬁrst two rows of Tab. 1. The deep features
used are the layer from ‘block3 conv4(cid:48). It can be seen that
projecting deep feature into kernel space consistently yields
a noticeable increase in both performance measures. The
computation cost for kernel projection is O(n log n) and for
linear kernel is O(n). Therefore, given marginal increase in
computational cost, we conclude that for constructing ap-
pearance model, kernel projection is the preferred method-
ology versus the traditional linear projection.

Beneﬁt of multi-resolution CNN: We then evaluate the
impact of decoding response map using a multi-resolution
CNN (Multi-Resolution CNN) vs. directly using the max
location information (MaxRes) for decoding target location
from hierarchical response maps in Tab. 1. MaxRes use the
maximum position from the mean of hierarchical response
maps with later response maps being resized to the largest
ﬁrst layer response map of size 240 ∗ 160. The performance
of MaxRes is even worse than using single layer, signify-
ing the need to intelligently combine multiply layer out-
put. The learnt convnets consistently performs better than
that of MaxRes for a large margin. Moreover, the learn-
ing paradigm opens the door for the integration of recurrent
network into the pipeline.

Beneﬁt of adaptive learning rate through time: We also
investigate the scheme of adjusting the learning rate adap-
tively vs. ﬁxing the learning rate through time in last two
rows of Tab. 1. Consistent improvements across two metrics

are observed. The margin of improvement, however, is less
prominent. We reason that in part, it is due to almost sat-
urate performance; on the other hand, an end-to-end learn-
ing scheme for model update could be more preferable and
should be further investigated.

4.2. Comparisons with the-state-of-the-art trackers

We validate our KMC tracker in a comprehensive
comparison with 10 state-of-the-art trackers: HDT [21],
SRDCF [6], MEEM [33], MUSTER [12], SAMF [16],
DSST [4], KCF [10], Struck [8], TLD [13], DCF [9].

OTB-2015 Dataset: OTB-2015 dataset contains 100 video
sequences and is the superset of OTB-2013 [31] dataset
which contains the original 50 video sequences. The Re-
sults on the OBT dataset is shown in the top row of Fig. 5.
It can be seen that our tracker performs on par with a range
of state-of-the-art trackers across two evaluation metrics.
The most similar tracker is HDT where both correlation
ﬁlter and deep features are adopted. However, HDT has
heuristically chosen a set of higher level deep feature lay-
ers whereas in our approach, all layers before max pooling
are universally chosen. The SRDCF tracker is better than
our tracker in the success metric due to heavy regularisation
for negative training examples in SRDCF tracker. Hence
it enables larger search area during tracking. Furthermore,
because SRDCF tracker is able to search for larger area dur-
ing detection (42 vs. our 3.22 the target size), it indirectly
manages the training set to handle the scenarios of occlu-
sion and drift, of which the proposed KMC has been mostly
penalised.

UAV123 Dataset: Visual tracking on unmanned aerial ve-
hicles (UAVs) is a very promising application, since the
camera can follow the target based on visual feedback and
actively change its orientation and position to optimise the
tracking performance. This UAV123 dataset contains a total

6

(a) Precision plot for OTB-2015 Dataset

(b) Success plot for OTB-2015 Dataset

(c) Precision plot for UAV123 Dataset

(d) Success plot for UAV123 Dataset

Figure 5: Precision plot and success plot of the state-of-the-art trackers for OTB-2015 and UAV123 dataset.

of 123 video sequences and more than 110K frames. The
major difference between this UAV123 dataset and other
popular tracking datasets is the effect of camera viewpoint
change arising from UAV motion, the variation in bounding
box size and aspect ratio with respect to the initial frame,
and longer tracking sequences on average due to the avail-
ability of mounted camera moving with the target.

Results are shown in the bottom row of Fig. 5. It can
be seen that due to heuristically chosen layers tuned to
the OBT dataset in HDT, its performance suffers on this
UAV123 dataset.
It worths noting that our trained multi-
resolution convnet has never seen any image in this dataset.
Moreover, no pesky threshold, hyperparameters nor weights
are altered when applying the trained KMC network on the
unseen UAV123 dataset, proving its ability to generalise
across various tracking scenarios.

5. Conclusions

In this paper, we propose a novel kernelised multi-
resolution convnet tracking algorithm that utilises the in-
termediate response maps from the kernelised correlation
ﬁlter outputs. The multi-resolution convnet learns the im-
plicit translational output accurately and later an adaptive

learning scheme is adopted for model update. The learning
paradigm is able to generalise across various datasets with-
out change of hyperparameters. Moreover, it opens the door
on the end-to-end temporal deep learning. Future works
include better regularisation method as in [6] for negative
instance mining, multi-layer fusion [24] and incorporating
recurrent nets on top to model temporal dynamics.

Details of the Code

The python based code can be found at:
https://github.com/stevenwudi/KMC_cvprw_2017

Acknowledgment

This project has received funding from National Natural Science Founda-
tion of China (NSFC) (61401287); Natural Science Foundation of Shen-
zhen (JCYJ20160307154003475, JCYJ2016050617265125).

References

[1] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui.
Visual object tracking using adaptive correlation ﬁlters. In
Conference on Computer Vision and Pattern Recognition,
2010.

7

[2] K. Chaudhuri, Y. Freund, and D. J. Hsu. A parameter-free
hedging algorithm. In Advances in neural information pro-
cessing systems, 2009.

[19] M. Mueller, N. Smith, and B. Ghanem. A benchmark and
simulator for uav tracking. In European Conference on Com-
puter Vision, 2016.

[3] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Accu-
In British

rate scale estimation for robust visual tracking.
Machine Vision Conference, 2014.

[4] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Dis-
criminative scale space tracking. In Transactions on Pattern
Analysis and Machine Intelligence, 2017.

[5] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg. Con-
volutional features for correlation ﬁlter based visual track-
International Conference on Computer Vision, work-
ing.
shop, 2015.

[6] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg. Learn-
ing spatially regularized correlation ﬁlters for visual track-
ing. International Conference on Computer Vision, 2015.
[7] M. Danelljan, K. F. S. Robinson, Andreas, and M. Felsberg.
Beyond correlation ﬁlters: Learning continuous convolution
operators for visual tracking. European Conference on Com-
puter Vision, 2016.

[8] S. Hare, A. Saffari, and P. Torr. Struck: Structured output
tracking with kernels. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, 2011.

[9] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. Ex-
ploiting the circulant structure of tracking-by-detection with
kernels. In European conference on computer vision, 2012.
[10] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-
IEEE
speed tracking with kernelized correlation ﬁlters.
Transactions on Pattern Analysis and Machine Intelligence,
2015.

[11] S. Hong, T. You, S. Kwak, and B. Han. Online tracking by
learning discriminative saliency map with convolutional neu-
ral network. International Conference on Machine Learning,
2015.

[12] Z. Hong, Z. Chen, C. Wang, X. Mei, D. Prokhorov, and
D. Tao. Multi-store tracker (muster): A cognitive psychology
inspired approach to object tracking. In IEEE Conference on
Computer Vision and Pattern Recognition, 2015.

[13] Z. Kalal, J. Matas, and K. Mikolajczyk. P-n learning: Boot-
strapping binary classiﬁers by structural constraints. In Con-
ference on Computer Vision and Pattern Recognition, 2010.
[14] Z. Kalal, K. Mikolajczyk, and J. Matas. Tracking-learning-
IEEE transactions on pattern analysis and ma-

detection.
chine intelligence, 2012.

[15] H. Li, Y. Li, and F. Porikli. Robust online visual tracking
with a single convolutional neural network. In Asian Confer-
ence on Computer Vision, 2014.

[16] Y. Li and J. Zhu. A scale adaptive kernel correlation ﬁlter
tracker with feature integration. European Conference on
Computer Vision, workshop, 2014.

[17] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. Conference on Com-
puter Vision and Pattern Recognition, 2015.

[18] C. Ma, J.-B. Huang, X. Yang, and M.-H. Yang. Hierarchical
convolutional features for visual tracking. In IEEE Interna-
tional Conference on Computer Vision, 2015.

8

[20] P. Ondr´uˇska and I. Posner. Deep tracking: Seeing beyond
seeing using recurrent neural networks. In Proceedings of the
Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.

[21] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, and J. L. M.-
In IEEE Conference on

H. Yang. Hedged deep tracking.
Computer Vision and Pattern Recognition, 2016.

[22] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recog-
nition challenge. International Journal of Computer Vision,
2015.

[23] B. Scholkopf and A. J. Smola. Learning with kernels: sup-
port vector machines, regularization, optimization, and be-
yond. MIT press, 2001.

[24] L. Shao, D. Wu, and X. Li. Learning deep and wide: A spec-
tral method for learning deep networks. IEEE Transactions
on Neural Networks and Learning Systems, 2014.

[25] L. Wang, W. Ouyang, X. Wang, and H. Lu. Visual track-
ing with fully convolutional networks. In Proceedings of the
IEEE International Conference on Computer Vision, 2015.

[26] L. Wang, W. Ouyang, X. Wang, and H. Lu. Stct: Sequen-
tially training convolutional networks for visual tracking. In
IEEE International Conference on Computer Vision, 2016.

[27] N. Wang and D.-Y. Yeung. Learning a deep compact image
In Advances in Neural

representation for visual tracking.
Information Processing Systems 26. 2013.

[28] N. Wang and D.-Y. Yeung. Learning a deep compact im-
age representation for visual tracking. In Advances in neural
information processing systems, 2013.

[29] D. Wu, L. Pigou, P.-J. Kindermans, N. D.-H. Le, L. Shao,
J. Dambre, and J.-M. Odobez. Deep dynamic neural net-
works for multimodal gesture segmentation and recognition.
IEEE transactions on pattern analysis and machine intelli-
gence, 2016.

[30] D. Wu and L. Shao. Leveraging hierarchical parametric
networks for skeletal joints based action segmentation and
recognition. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2014.

[31] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A
benchmark. In IEEE conference on computer vision and pat-
tern recognition, 2013.

[32] Y. Wu, J. Lim, and M.-H. Yang. Object tracking benchmark.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2015.

[33] M.-S.-S. S. Zhang, J. Meem: robust tracking via multiple ex-
perts using entropy minimization. In European Conference
on Computer Vision, 2014.

[34] W. Zou and N. Komodakis. Harf: Hierarchy-associated rich
features for salient object detection. In Proceedings of the
IEEE International Conference on Computer Vision, 2015.

Kernalised Multi-resolution Convnet for Visual Tracking

Di Wu, Wenbin Zou∗, Xia Li
Shenzhen University†
dwu, wzou, lixia@szu.edu.cn

Yong Zhao
Peking University Shenzhen Graduate School
yongzhao@pkusz.edu.cn

7
1
0
2
 
g
u
A
 
2
 
 
]

V
C
.
s
c
[
 
 
1
v
7
7
5
0
0
.
8
0
7
1
:
v
i
X
r
a

Abstract

Visual tracking is intrinsically a temporal problem. Dis-
criminative Correlation Filters (DCF) have demonstrated
excellent performance for high-speed generic visual object
tracking. Built upon their seminal work, there has been a
plethora of recent improvements relying on convolutional
neural network (CNN) pretrained on ImageNet as a feature
extractor for visual tracking. However, most of their works
relying on ad hoc analysis to design the weights for different
layers either using boosting or hedging techniques as an en-
semble tracker. In this paper, we go beyond the conventional
DCF framework and propose a Kernalised Multi-resolution
Convnet (KMC) formulation that utilises hierarchical re-
sponse maps to directly output the target movement. When
directly deployed the learnt network to predict the unseen
challenging UAV tracking dataset without any weight ad-
justment, the proposed model consistently achieves excel-
lent tracking performance. Moreover, the transfered multi-
reslution CNN renders it possible to be integrated into the
RNN temporal learning framework, therefore opening the
door on the end-to-end temporal deep learning (TDL) for
visual tracking.

1. Introduction

Visual tracking is the task of predicting the trajectory of a
target in a video. The task of tracking, a crucial component
of many computer vision systems, can be naturally speciﬁed
as an online learning problem. This paper focuses on the
challenging problem of monocular, generic, realistic object
tracking.

Visual object tracking has recently witnessed substantial
progress due to powerful features extracted using deep con-
volutional neural networks. For online applications, one
simple approach to transfer ofﬂine pre-trained CNN fea-
tures is to add one or more randomly initialised CNN layers,
named as adaptation layers, on top of the pre-trained CNN

∗Corresponding author.
†Shenzhen Key Lab of Advanced Telecommunication and Information

Processing, College of Information Engineering, Shenzhen University.

Figure 1: Response maps generated from hierarchical deep fea-
tures. Left: two frames from the “Bolt” sequence, red boxes are
image patches with padding added. Right: hierarchical response
maps from correlation ﬁlters (response maps have been resized
and rescaled to the same shape and scale). It can be seen that be-
cause different layers of deep features encode different semantics,
the maximum response points from different layers also vary.

model. However, as [26] empirically observe that this trans-
fer learning method suffers from severe over-ﬁtting because
of the limited training data. It is also observed in [29, 30]
that the differences in mean activations in intermediate lay-
ers have noticeable effect for multi-model fusion. The on-
line learnt parameters mainly focus on recent training sam-
ples and are less likely to be well generalised to both histor-
ical and future samples. This phenomenon can be fatal to
online visual tracking where the target often undergoes sig-
niﬁcant appearance changes or heavy occlusion. CNNs also
have shown impressive performance as an ofﬂine feature ex-
tractor for tracking [27, 18] in lieu of traditional handcrafted
features (e.g., HOG, Color Moment). Features from these
deep convolutional layers are discriminative while preserv-
ing spatial and structural information.

The marriage between DCF [9], which has the advan-
tage of being efﬁcient in training translational images in
the fourier space, and deep features, which excel at image
representation, further advances the visual tracking com-
munity [21, 7]. However, there are some conﬂicting con-

1

clusions concerning the representation power from differ-
ent layers of CNN: shallow convolutional layers have been
highlighted in the DCF-based methods [5] whereas [21]
found that the performance generally increases as layer
depth is increased hence only convolutional layers deeper
than 10th is used. Fig. 1 shows the response maps gener-
ated from hierarchical deep features. Since different layers
from convnet capture different semantic meanings, maxi-
mum translational locations obtained via different convnet
layers could vary: low level layers excel at discriminating
intra-class variations whereas high level layers excel at dis-
criminating inter-class variations. Hence, for generic object
tracking, it is imperative to combine features from different
layers to have a generalised representation from hierarchies
of response maps.

Visual tracking is intrinsically also a temporal problem,
but many previous approaches [1, 26] mainly focus on the
design of a robust appearance model. An end-to-end ob-
ject tracking approach which directly maps from raw sensor
input to object tracks in sensor space is proposed in [20].
Their proposed method works well in a simulated environ-
ment, but the applicability on the realistic RGB imagery
dataset is worth further investigation. On reason that tem-
poral deep learning based techniques are challenging to be
employed in realistic settings is due to the difﬁculty for re-
current net to have robust, meaningful temporal input.

In this paper, we propose a kernelised multi-resolution
convnet tracking algorithm that utilises the intermediate re-
sponse maps from the correlation ﬁlter output and learns the
implicit translational output from the multi-resolution con-
vnet. The key contributions can be summarised as follows:

• We incorporate kernalised form into CNN feature rep-
resentations for the non-linear regression tasks that
consistently outperforms its linear counterpart, con-
cluding the kernalised version should be the preferred
choice in the process of designing correlation ﬁlters.

• We learn a novel representation for multi-resolution re-
sponse maps generated from different layers of a pre-
trained CNN, negating the need to design the weights
for various convnets layers. Moreover, the learnt rep-
resentation renders it possible to be included into the
end-to-end temporal deep learning (TDL) pipeline.

• We use an adaptive hedge method to update the model
learning rate, taking the model stability into consider-
ation, making the update of target model smoother and
more robust.

2. Related Work

We review recent tracking methods closely related to this

work as in the following three sections.

Correlation ﬁlter based trackers: Most recent works [1,
26] strives for a better appearance models for the tracker.
DCF [1, 9] have demonstrated excellent performance for
high-speed visual object tracking. The key to their success
is by observing that the resulting data matrix of translated
patches is circulant, the cyclic shifts could be diagonalised
with the Discrete Fourier Transform, reducing both storage
and computation to obtain the next frame response map.
However, most of these trackers depend on the spatiotem-
poral consistency of visual cues. Therefore, they can handle
mostly short-term tracking or object with static appearance.

Deep feature based trackers: Recent works exploit the
structure of CNN to learn the target online:
a three-
layer CNN is trained on-the-ﬂight in [15]; a deep autoen-
coder [28] is ﬁrst pre-trained ofﬂine and then ﬁnetuned
for binary classiﬁcation in online tracking. Since the pre-
training is performed in an unsupervised way by recon-
structing gray images with very low resolution, the learned
deep features has limited discriminative power for track-
ing. Moreover, without pre-traning and with limited train-
ing samples obtained online, CNN fails to capture object se-
mantics and is not robust to deformation. Both [15] and [28]
train deep networks online with limited training samples,
and inevitably suffer from overﬁtting. Transferring the hi-
erarchical features learned for image classiﬁcation tasks
have been shown to be effective for numerous vision tasks,
e.g., image segmentation [17], salient object detection [34].
More recent methods [26, 25, 11, 18] adopt deep convolu-
tion networks trained on a large scale image classiﬁcation
task [22] to improve tracking performance. The rich rep-
resentation of transferred features from deep nets enables
trackers to construct more robust, power appearance model
over the traditional hand crafted feature based trackers.

Spatio-temporal model based trackers: Variations in the
appearance of the object in tracking, such as variations in
geometry/photometry, camera viewpoint, partial occlusion
or out-of-view, pose a major challenge to object tracking.
TLD [14] employs two experts to identify the false nega-
tives and false positives to train a detector. The experts are
independent, which ensures mutual compensation of their
errors to alleviate the problem of drifting. A short and long
term cognitive psychology principle is adopted in [12] to
design a ﬂexible representation that can adopt to changes
in object appearance during tracking. A parameter-free
Hedging algorithm is proposed in [2] for the problem of
decision-theoretic online learning, especially for the appli-
cations when the number of actions is very large and opti-
mally setting the parameter is not well understood. An im-
proved Hedge algorithm considering historical performance
is proposed in [21] to weight the decision from different
CNN layers.

2

Figure 2: The proposed algorithm overview: we ﬁrst extract hierarchical CNN features from the image patch of interest; then we project
deep features into kernel space for correlation ﬁlters; the target translational movement is predicted via multi-resolution convnets and later
we update the scale and appearance models using an adaptive learning rate scheme.

3. Proposed Algorithm

As shown in Fig. 2, the proposed approach consists of
three steps: extracting hierarchical CNN features, project-
ing features into kernel space for correlation ﬁlters, predict-
ing target translational movement via multi-resolution con-
vnet. We ﬁrst review the correlation ﬁlter as our building
block. Then we present the technical details of the projec-
tion of deep feature to kernel space, the model of learning
translational output using a multi-resolution convnets and
an adaptive learning rate scheme for model update.

Correlation Filter: Correlation ﬁlters based trackers [1, 9,
3, 10] exploit the circulant structure of training and testing
samples to greatly accelerate the training and testing pro-
cess. Let Xk ∈ (cid:60)D×M ×N denotes the feature sets where
D denotes the number of feature maps; M, N denote the
shape of feature maps and k denotes the k-th input feature
map extracted from the k-th convolutional layers from a
pretrained CNN; Y ∈ (cid:60)M ×N denotes a gaussian shaped
label matrix which is subject to a 2D Gaussian distribution
with zero mean and standard deviation proportional to the
target size. The goal of training is to ﬁnd a set of ﬁlters
Wk that minimises the squared error over sets of circulant
translated samples Xk and their regression targets Y:
(cid:107)Y − Xk • Wk(cid:107)2 + λ(cid:107)Wk(cid:107)2

Wk = argmin

(1)

Wk

where

Xk • Wk =

Xk

d (cid:12) Wk
d

D
(cid:88)

d=1
with the symbol (cid:12) denotes element-wise product. The min-
imizer has a closed-form:

W =

XT Y
XT X + λI

(2)

(3)

3

where I is an identity matrix and λ is a regularisation pa-
rameter that controls overﬁtting. We drop the superscript k
for notational simplicity. In general, a large system of lin-
ear equations must be solved to compute the solution, which
can become prohibitive in a real-time setting. With training
data being cyclic shifts patches, all operations can be done
element-wise on their diagonal elements [10].

The ﬁlter can be modelled in the Fourier domain by:

W = argmin

(cid:107)Y − X • W k(cid:107)2

F + λ(cid:107)W(cid:107)2
F

(4)

W

where

X • W =

X d (cid:12) W d

(5)

D
(cid:88)

d=1

The corresponding minimizer in the Fourier domain has

the closed-form:

W =

X ∗ (cid:12) Y
(X ∗ (cid:12) X + λI)

(6)

where ∗ denotes the Hermitian transpose and since diago-
nal matrices are symmetric, taking the Hermitian transpose
only left behind a complex-conjugate.

3.1. Kernelisation of Deep transfered features

In Kernelized Correlation Filter (KCF) [10], the feature
X is mapped to a Hilbert space φ(X). By employing a ker-
nel κ(X, X(cid:48)) = (cid:104)φ(X), φ(X(cid:48))(cid:105), Eq. 4 becomes:

W = argmin

(cid:107)Y − (cid:104)φ(X ), W(cid:105)(cid:107)2

F + λ(cid:107)W(cid:107)2
F

(7)

W

The power of the kernel trick comes from the implicit use
of a high-dimensional feature space φ(X) without ever in-
stantiating a vector in the space. Even though the regression

two consecutive frames; middle:

Figure 3: Response maps for one frame of the “Biker” sequence.
Left:
ideal response map (2D
gaussian); right: the actual response map. Arrows represent the
translational vector. Due to various factors, e.g., motion blur, tar-
get appearance change, scale change, boundary effect, the actual
response map is not a univariate gaussian and the maximum re-
sponse does not strictly correspond to the translational movement.

function’s complexity grows with the number of samples
which is the major drawback of the kernel trick, assuming
circulant data and the adopted kernel being shift invariant,
the kernel correlation can be computed efﬁciently. In our
experiment, an radial basis function (RBF) kernel, which
satisﬁes the shift invariant property, is adopted:

1
σ

kXX (cid:48)

= exp(−

((cid:107)X(cid:107)2 + (cid:107)X (cid:48)(cid:107)2 − 2F −1(X ∗ (cid:12) X (cid:48)))
(8)
where F −1 denotes the Inverse DFT and the full kernel cor-
relation can be computed in only O(n log n) time.

Expressing the solution W as a linear combination of the
samples: W = (cid:80)
i αiφ(X i) renders an alternative repre-
sentation α to be in the dual space, as opposed to the primal
space W (Representer Theorem [23]). The variables under
optimisation are thus α:

α = (kXX (cid:48)

+ λI)−1Y

(9)

For detection stage for translation estimation, given an
image patch feature Zk ∈ (cid:60)D×M ×N , the response map is
obtained by:

Figure 4: A multi-resolution convnet is deployed to decode the
hierarchical response maps.

consecutive frames, the precondition is that of static target
appearance, i.e., the maximum response map corresponds
to the cyclic shifts of appearance unchanged target. Due
to subtle changes of target appearance, e.g., rotation, scale,
cyclic boundary effect or motion blur of two consecutive
frames, the accuracy of ﬁnding the translation using maxi-
mum response could be compromised (c.f. Fig. 3).

Moreover, when using deep features from the pre-trained
convnets for the input representation X, the hierarchies
of feature maps capture different semantic information(c.f.
Fig. 1): features from deep layers capture rich category level
semantic information, which is useful for object classiﬁca-
tion, but they are not the optimal representation for visual
tracking because spatial details captured by earlier layers
are also important for accurately localising the targets. On
the other hand, as the features in the earlier layers capture
low level visual characteristics and are more class-generic
rather than discriminative as ones in the later layers, meth-
ods based on features from earlier layers are likely to fail in
challenging scenarios when the target size is small and high
level object is the target of tracking.

We interpret the response maps from the hierarchies of
convolutional layers as a nonlinear counter part of an image
pyramid representation and employ a learning framework
for mapping the hierarchical outputs to target translation.
A multi-resolution neural network (c.f. Fig. 4) is deployed
to exploit the implicit translational information from hierar-
chies response maps:

R(Zk) = F −1(F(kXZk(cid:48)

) (cid:12) F(αk))

(10)

∆x, ∆y ← convnets(R(Zk))

(12)

The model for frame t is updated with learning rate η as:

αk

t = (1 − η)αk

t−1 + ηαk

(11)

3.2. Decoding Response map using Multi-Res CNN

The loss of the convnets is set to be the root mean square
(RMS) of the normalised translational movement (∆x, ∆y)
and the predicted movement (∆x(cid:48), ∆y(cid:48)):

Lpos =

((cid:107) ∆x − ∆x(cid:48) (cid:107)2

2 + (cid:107) ∆y − ∆y(cid:48) (cid:107)2
2)

(13)

(cid:114) 1
2

Previous works estimate the next frame location via the
maximum response point on the response map. We ar-
gue that albeit the maximum response point carries phys-
ical meaning of the translational information between two

The proposed formulation enables efﬁcient integration of
multi-resolution response maps to decode translational in-
formation implicitly and demonstrates its competitiveness
over the corresponding counterpart baselines that using the

4

mean of maximum responses as the indicator for target
movement.

3.3. Adaptive learning rate

Traditional correlation based ﬁlter updates the model by
a ﬁxed parameter η. For generic object tracking, however,
there are two crucial factors for model update using deep
features: (1) the appearance of object of interest usually
changes at irregular pace (sometimes gently and sometimes
vehemently). This means that the scale of the learning rate
should reﬂect the appearance change of the target; (2) de-
pending on whether the object of interest being a low level
visual cues or high level object entity, the hierarchies of re-
sponse maps render correspondingly different maximum re-
sponse values.

Let the ultimate target position is predicted (xp, yp) at
time t, each layer k will incur a loss from its response map
Rk
t :

t = max(Rk
lk

t ) − Rk

t (xp, yp)

(14)

We then measure the stability of layer k at time t:

sk
t =

t − µk
|lk
t |
σk
t
t are the mean and variance for the loss lk

(15)

t dur-

t , σk
where µk
ing time period ∆t.
A larger sk

t indicates the layer is less correlated with the
object at frame t hence the model update η should be de-
creased as well. Therefore, we propose an adaptive learning
rate for different layers of model update that is linear to the
model stability as:

ηk = sk

t × η

(16)

Algorithm 1 summarises the main steps of the proposed

approach for visual tracking.

4. Experiments

We validate our proposed KMC framework by evaluat-
ing two genres of experiments: one compares with corre-
lation ﬁlter based baseline trackers and one compares with
several state-of-the-art trackers. We perform comprehen-
sive experiments on three datasets: OTB-2013 [31], OTB-
2015 [32] and UAV123 [19].

Implementation details: For feature extraction, we crop
the image patch with 2.2 padding size and resize the im-
age patch to 240 ∗ 160 because the average target size is of
ratio 3
2 . Feature bandwidth σ in Eq. 8 is 0.2. The learn-
ing rate η for Eq. 16 is 0.0025 which is only one ﬁfth of
ones chosen in [21] because of the model is in kernel space
and tends to be more stable. After the forward propaga-
tion, we use the VGG-Net with 19 layers and ﬁve out-
puts before the max pooling layers (i.e., ‘block1 conv2(cid:48),

Algorithm 1: Kernalised Multi-resolution Convnet

Input: target position (x1, y1) and size (w1, h1), ﬁrst

tracking frame;

Output: predicted target positions (xt, yt) and sizes
(wt, ht) in the following frames.

1 for t = 1, 2, . . . do
2

3

4

5

6

7

8

9

10

Crop target images with padding;
Extract deep features from layers before max
pooling layers in VGG-Net19 to obtain 5
hierarchical representations X;
Project deep features into kernel space 3.1;
if t = 1 then

Obtain deep kernel model α (Eq.9);

else

Update target position prediction by the
trained multi-resolution convnet 3.2;
Update model scale using DSST [4];
Update kernel model using adaptive learning
rate 3.3;

‘block3 conv4(cid:48),

‘block2 conv2(cid:48),
‘block5 conv4(cid:48)). Note
that instead of heuristically choosing layers as in [21, 7],
we have included all layer outputs before the max pooling
layers. This chosen methodology makes the approach more
generic and less dataset dependant.

The convnet in Sec. 3.2 is trained on OTB-2015 dataset.
Note that albeit trained on the OTB-2015 dataset, the con-
vnet during detection will not be able to see the exactly
same response maps twice unless exact location update is
achieved. Hence it requires the convnet to be able to gener-
alise to unseen hierarchical response maps and predict rea-
sonable translational output. Furthermore, we verify the
generalisation of the KMC over HDT [21] tracker on the
unseen UAV123 dataset.

For scale estimation, we use a scale pyramid representa-
tion as in [4] with 11 scales and a relative scale factor 1.02.
The updates consistently helps more accurate update of tar-
get models while maintaining the computational cost low.

Evaluation Methodology: Following the evaluation strat-
egy of
[31], all trackers are compared using two mea-
sures: precision and success. Precision is measured as the
distance between the centres of the ground truth bounding
box and the corresponding tracker generated bounding box.
The precision plot shows the percentage of tracker bound-
ing boxed within a given threshold distance in pixels of
the ground truth. To rank the trackers, the conventional
threshold of 20 pixels (P20) is adopted. Success is mea-
sured as the intersection over union of pixels. The success
plot shows the percentage of tracker bounding boxes whose
overlap score is larger than a given threshold and the track-

5

(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)

Method

Dataset

DeepDCF
DeepKCF
MaxRes
Multi-Resolution CNN
Fixed Learning Rate
Adaptive Learning Rate

OTB-2013

OTB-2015

P20
58.0
66.2 (8.2 ↑)
57.5
70.6(13.1 ↑)
74.0(3.4 ↑)
74.2 (0.2 ↑)

AUC
38.4
46.1 (7.7 ↑)
38.8
46.7 (7.9 ↑)
51.8(5.1 ↑)
52.1(0.3 ↑)

P20
64.3
71.4 (7.1 ↑)
65.8
77.8 (12.0 ↑)
79.8(2.0 ↑)
79.9(0.1 ↑)

AUC
44.0
49.1 (5.1 ↑)
45.9
53.6 (7.7 ↑)
57.0(3.4 ↑)
57.4(0.4 ↑)

Table 1: Baseline comparisons with arrows indicating performance changes comparing with the upper row parallel experiment. First two
rows investigate the impact of projecting deep features into a kernel space vs. using linear deep features. Next two rows compare the
traditional using max location vs. the proposed multi-resolution convnets paradigm for decoding target translational movement. Last two
rows illustrate the beneﬁt of adaptive learning rate through time (with scale update).

ers are ranked according to the Area Under Curve (AUC)
criteria. All sequences are evaluated using One-Pass Evalu-
ation (OPE) as in [31].

4.1. Baseline comparison

Beneﬁt of Kernalising deep features: We ﬁrst evalu-
ate the impact of projecting deep features into a kernel
space (DeepKCF) vs. using the traditional linear projection
(DeepDCF) in ﬁrst two rows of Tab. 1. The deep features
used are the layer from ‘block3 conv4(cid:48). It can be seen that
projecting deep feature into kernel space consistently yields
a noticeable increase in both performance measures. The
computation cost for kernel projection is O(n log n) and for
linear kernel is O(n). Therefore, given marginal increase in
computational cost, we conclude that for constructing ap-
pearance model, kernel projection is the preferred method-
ology versus the traditional linear projection.

Beneﬁt of multi-resolution CNN: We then evaluate the
impact of decoding response map using a multi-resolution
CNN (Multi-Resolution CNN) vs. directly using the max
location information (MaxRes) for decoding target location
from hierarchical response maps in Tab. 1. MaxRes use the
maximum position from the mean of hierarchical response
maps with later response maps being resized to the largest
ﬁrst layer response map of size 240 ∗ 160. The performance
of MaxRes is even worse than using single layer, signify-
ing the need to intelligently combine multiply layer out-
put. The learnt convnets consistently performs better than
that of MaxRes for a large margin. Moreover, the learn-
ing paradigm opens the door for the integration of recurrent
network into the pipeline.

Beneﬁt of adaptive learning rate through time: We also
investigate the scheme of adjusting the learning rate adap-
tively vs. ﬁxing the learning rate through time in last two
rows of Tab. 1. Consistent improvements across two metrics

are observed. The margin of improvement, however, is less
prominent. We reason that in part, it is due to almost sat-
urate performance; on the other hand, an end-to-end learn-
ing scheme for model update could be more preferable and
should be further investigated.

4.2. Comparisons with the-state-of-the-art trackers

We validate our KMC tracker in a comprehensive
comparison with 10 state-of-the-art trackers: HDT [21],
SRDCF [6], MEEM [33], MUSTER [12], SAMF [16],
DSST [4], KCF [10], Struck [8], TLD [13], DCF [9].

OTB-2015 Dataset: OTB-2015 dataset contains 100 video
sequences and is the superset of OTB-2013 [31] dataset
which contains the original 50 video sequences. The Re-
sults on the OBT dataset is shown in the top row of Fig. 5.
It can be seen that our tracker performs on par with a range
of state-of-the-art trackers across two evaluation metrics.
The most similar tracker is HDT where both correlation
ﬁlter and deep features are adopted. However, HDT has
heuristically chosen a set of higher level deep feature lay-
ers whereas in our approach, all layers before max pooling
are universally chosen. The SRDCF tracker is better than
our tracker in the success metric due to heavy regularisation
for negative training examples in SRDCF tracker. Hence
it enables larger search area during tracking. Furthermore,
because SRDCF tracker is able to search for larger area dur-
ing detection (42 vs. our 3.22 the target size), it indirectly
manages the training set to handle the scenarios of occlu-
sion and drift, of which the proposed KMC has been mostly
penalised.

UAV123 Dataset: Visual tracking on unmanned aerial ve-
hicles (UAVs) is a very promising application, since the
camera can follow the target based on visual feedback and
actively change its orientation and position to optimise the
tracking performance. This UAV123 dataset contains a total

6

(a) Precision plot for OTB-2015 Dataset

(b) Success plot for OTB-2015 Dataset

(c) Precision plot for UAV123 Dataset

(d) Success plot for UAV123 Dataset

Figure 5: Precision plot and success plot of the state-of-the-art trackers for OTB-2015 and UAV123 dataset.

of 123 video sequences and more than 110K frames. The
major difference between this UAV123 dataset and other
popular tracking datasets is the effect of camera viewpoint
change arising from UAV motion, the variation in bounding
box size and aspect ratio with respect to the initial frame,
and longer tracking sequences on average due to the avail-
ability of mounted camera moving with the target.

Results are shown in the bottom row of Fig. 5. It can
be seen that due to heuristically chosen layers tuned to
the OBT dataset in HDT, its performance suffers on this
UAV123 dataset.
It worths noting that our trained multi-
resolution convnet has never seen any image in this dataset.
Moreover, no pesky threshold, hyperparameters nor weights
are altered when applying the trained KMC network on the
unseen UAV123 dataset, proving its ability to generalise
across various tracking scenarios.

5. Conclusions

In this paper, we propose a novel kernelised multi-
resolution convnet tracking algorithm that utilises the in-
termediate response maps from the kernelised correlation
ﬁlter outputs. The multi-resolution convnet learns the im-
plicit translational output accurately and later an adaptive

learning scheme is adopted for model update. The learning
paradigm is able to generalise across various datasets with-
out change of hyperparameters. Moreover, it opens the door
on the end-to-end temporal deep learning. Future works
include better regularisation method as in [6] for negative
instance mining, multi-layer fusion [24] and incorporating
recurrent nets on top to model temporal dynamics.

Details of the Code

The python based code can be found at:
https://github.com/stevenwudi/KMC_cvprw_2017

Acknowledgment

This project has received funding from National Natural Science Founda-
tion of China (NSFC) (61401287); Natural Science Foundation of Shen-
zhen (JCYJ20160307154003475, JCYJ2016050617265125).

References

[1] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui.
Visual object tracking using adaptive correlation ﬁlters. In
Conference on Computer Vision and Pattern Recognition,
2010.

7

[2] K. Chaudhuri, Y. Freund, and D. J. Hsu. A parameter-free
hedging algorithm. In Advances in neural information pro-
cessing systems, 2009.

[19] M. Mueller, N. Smith, and B. Ghanem. A benchmark and
simulator for uav tracking. In European Conference on Com-
puter Vision, 2016.

[3] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Accu-
In British

rate scale estimation for robust visual tracking.
Machine Vision Conference, 2014.

[4] M. Danelljan, G. H¨ager, F. Khan, and M. Felsberg. Dis-
criminative scale space tracking. In Transactions on Pattern
Analysis and Machine Intelligence, 2017.

[5] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg. Con-
volutional features for correlation ﬁlter based visual track-
International Conference on Computer Vision, work-
ing.
shop, 2015.

[6] M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg. Learn-
ing spatially regularized correlation ﬁlters for visual track-
ing. International Conference on Computer Vision, 2015.
[7] M. Danelljan, K. F. S. Robinson, Andreas, and M. Felsberg.
Beyond correlation ﬁlters: Learning continuous convolution
operators for visual tracking. European Conference on Com-
puter Vision, 2016.

[8] S. Hare, A. Saffari, and P. Torr. Struck: Structured output
tracking with kernels. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, 2011.

[9] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. Ex-
ploiting the circulant structure of tracking-by-detection with
kernels. In European conference on computer vision, 2012.
[10] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-
IEEE
speed tracking with kernelized correlation ﬁlters.
Transactions on Pattern Analysis and Machine Intelligence,
2015.

[11] S. Hong, T. You, S. Kwak, and B. Han. Online tracking by
learning discriminative saliency map with convolutional neu-
ral network. International Conference on Machine Learning,
2015.

[12] Z. Hong, Z. Chen, C. Wang, X. Mei, D. Prokhorov, and
D. Tao. Multi-store tracker (muster): A cognitive psychology
inspired approach to object tracking. In IEEE Conference on
Computer Vision and Pattern Recognition, 2015.

[13] Z. Kalal, J. Matas, and K. Mikolajczyk. P-n learning: Boot-
strapping binary classiﬁers by structural constraints. In Con-
ference on Computer Vision and Pattern Recognition, 2010.
[14] Z. Kalal, K. Mikolajczyk, and J. Matas. Tracking-learning-
IEEE transactions on pattern analysis and ma-

detection.
chine intelligence, 2012.

[15] H. Li, Y. Li, and F. Porikli. Robust online visual tracking
with a single convolutional neural network. In Asian Confer-
ence on Computer Vision, 2014.

[16] Y. Li and J. Zhu. A scale adaptive kernel correlation ﬁlter
tracker with feature integration. European Conference on
Computer Vision, workshop, 2014.

[17] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. Conference on Com-
puter Vision and Pattern Recognition, 2015.

[18] C. Ma, J.-B. Huang, X. Yang, and M.-H. Yang. Hierarchical
convolutional features for visual tracking. In IEEE Interna-
tional Conference on Computer Vision, 2015.

8

[20] P. Ondr´uˇska and I. Posner. Deep tracking: Seeing beyond
seeing using recurrent neural networks. In Proceedings of the
Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.

[21] Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, and J. L. M.-
In IEEE Conference on

H. Yang. Hedged deep tracking.
Computer Vision and Pattern Recognition, 2016.

[22] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recog-
nition challenge. International Journal of Computer Vision,
2015.

[23] B. Scholkopf and A. J. Smola. Learning with kernels: sup-
port vector machines, regularization, optimization, and be-
yond. MIT press, 2001.

[24] L. Shao, D. Wu, and X. Li. Learning deep and wide: A spec-
tral method for learning deep networks. IEEE Transactions
on Neural Networks and Learning Systems, 2014.

[25] L. Wang, W. Ouyang, X. Wang, and H. Lu. Visual track-
ing with fully convolutional networks. In Proceedings of the
IEEE International Conference on Computer Vision, 2015.

[26] L. Wang, W. Ouyang, X. Wang, and H. Lu. Stct: Sequen-
tially training convolutional networks for visual tracking. In
IEEE International Conference on Computer Vision, 2016.

[27] N. Wang and D.-Y. Yeung. Learning a deep compact image
In Advances in Neural

representation for visual tracking.
Information Processing Systems 26. 2013.

[28] N. Wang and D.-Y. Yeung. Learning a deep compact im-
age representation for visual tracking. In Advances in neural
information processing systems, 2013.

[29] D. Wu, L. Pigou, P.-J. Kindermans, N. D.-H. Le, L. Shao,
J. Dambre, and J.-M. Odobez. Deep dynamic neural net-
works for multimodal gesture segmentation and recognition.
IEEE transactions on pattern analysis and machine intelli-
gence, 2016.

[30] D. Wu and L. Shao. Leveraging hierarchical parametric
networks for skeletal joints based action segmentation and
recognition. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2014.

[31] Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A
benchmark. In IEEE conference on computer vision and pat-
tern recognition, 2013.

[32] Y. Wu, J. Lim, and M.-H. Yang. Object tracking benchmark.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2015.

[33] M.-S.-S. S. Zhang, J. Meem: robust tracking via multiple ex-
perts using entropy minimization. In European Conference
on Computer Vision, 2014.

[34] W. Zou and N. Komodakis. Harf: Hierarchy-associated rich
features for salient object detection. In Proceedings of the
IEEE International Conference on Computer Vision, 2015.

