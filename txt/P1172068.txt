3D Face Morphable Models “In-the-Wild”

James Booth

Epameinondas Antonakos
Yannis Panagakis

Stylianos Ploumpis
Stefanos Zafeiriou

George Trigeorgis

Imperial College London, UK
{james.booth,e.antonakos,s.ploumpis,g.trigeorgis,i.panagakis,s.zafeiriou}@imperial.ac.uk

7
1
0
2
 
n
a
J
 
9
1
 
 
]

V
C
.
s
c
[
 
 
1
v
0
6
3
5
0
.
1
0
7
1
:
v
i
X
r
a

Abstract

3D Morphable Models (3DMMs) are powerful statistical
models of 3D facial shape and texture, and among the state-
of-the-art methods for reconstructing facial shape from sin-
gle images. With the advent of new 3D sensors, many 3D fa-
cial datasets have been collected containing both neutral as
well as expressive faces. However, all datasets are captured
under controlled conditions. Thus, even though powerful
3D facial shape models can be learnt from such data, it is
difﬁcult to build statistical texture models that are sufﬁcient
to reconstruct faces captured in unconstrained conditions
(“in-the-wild”). In this paper, we propose the ﬁrst, to the
best of our knowledge, “in-the-wild” 3DMM by combining
a powerful statistical model of facial shape, which describes
both identity and expression, with an “in-the-wild” texture
model. We show that the employment of such an “in-the-
wild” texture model greatly simpliﬁes the ﬁtting procedure,
because there is no need to optimize with regards to the illu-
mination parameters. Furthermore, we propose a new fast
algorithm for ﬁtting the 3DMM in arbitrary images. Fi-
nally, we have captured the ﬁrst 3D facial database with
relatively unconstrained conditions and report quantitative
evaluations with state-of-the-art performance. Complemen-
tary qualitative reconstruction results are demonstrated on
standard “in-the-wild” facial databases. An open source
implementation of our technique is released as part of the
Menpo Project [1].

1. Introduction

During the past few years, we have witnessed signiﬁcant
improvements in various face analysis tasks such as face
detection [20, 43] and 2D facial landmark localization on
static images [41, 22, 7, 39, 44, 5, 6, 37]. This is primarily
attributed to the fact that the community has made a con-
siderable effort to collect and annotate facial images cap-
tured under unconstrained conditions [25, 46, 10, 33, 32]
(commonly referred to as “in-the-wild”) and to the discrim-
inative methodologies that can capitalise on the availability

Figure 1. Our “in-the-wild” Morphable Model is capable of recov-
ering accurate 3D facial shape for a wide variety of images.

of such large amount of data. Nevertheless, discriminative
techniques cannot be applied for 3D facial shape estimation
“in-the-wild”, due to lack of ground-truth data.

3D facial shape estimation from single images has at-
tracted the attention of many researchers the past twenty
years. The two main lines of research are (i) ﬁtting a 3D
Morphable Model (3DMM) [12, 13] and (ii) applying Shape
from Shading (SfS) techniques [35, 36, 23]. The 3DMM ﬁt-
ting proposed in the work of Blanz and Vetter [12, 13] was
among the ﬁrst model-based 3D facial recovery approaches.
The method requires the construction of a 3DMM which is a
statistical model of facial texture and shape in a space where
there are explicit correspondences. The ﬁrst 3DMM was
built using 200 faces captured in well-controlled conditions
displaying only the neutral expression. That is the reason
why the method was only shown to work on real-world, but
not “in-the-wild”, images. State-of-the-art SfS techniques
capitalise on special multi-linear decompositions that ﬁnd
an approximate spherical harmonic decomposition of the il-
lumination. Furthermore, in order to beneﬁt from the large
availability of “in-the-wild” images, these methods jointly
reconstruct large collections of images. Nevertheless, even

1

thought the results of [35, 23] are quite interesting, given
that there is no prior of the facial surface, the methods
only recover 2.5D representations of the faces and partic-
ular smooth approximations of the facial normals.

3D facial shape recovery from a single image under “in-
the-wild” conditions is still an open and challenging prob-
lem in computer vision mainly due to the fact that:

• The general problem of extracting the 3D facial shape
from a single image is an ill-posed problem which is
notoriously difﬁcult to be solved without the use of
any statistical priors for the shape and texture of faces.
That is, without prior knowledge regarding the shape
of the object at-hand there are inherent ambiguities
present in the problem. The pixel intensity at a location
in an image is the result of a complex combination of
the underlying shape of the object, the surface albedo
and normal characteristics, camera parameters and the
arrangement of scene lighting and other objects in the
scene. Hence, there are potentially inﬁnite solutions to
the problem.

• Learning statistical priors of the 3D facial shape and
texture for “in-the-wild” images is currently very difﬁ-
cult by using modern acquisition devices. That is, even
though there is a considerable improvement in 3D ac-
quisition devices, they still cannot operate in arbitrary
conditions. Hence, all the current 3D facial databases
have been captured in controlled conditions.

With the available 3D facial data, it is feasible to learn
a powerful statistical model of the facial shape that gen-
eralises well for both identity and expression [15, 31, 14].
However, it is not possible to construct a statistical model
of the facial texture that generalises well for “in-the-wild”
images and is, at the same time, in correspondence with the
statistical shape model. That is the reason why current state-
of-the-art 3D face reconstruction methodologies rely solely
on ﬁtting a statistical 3D facial shape prior on a sparse set
of landmarks [3, 17].

In this paper, we make a number of contributions that
enable the use of 3DMMs for “in-the-wild” face reconstruc-
tion (Fig. 1). In particular, our contributions are:

• We propose a methodology for learning a statisti-
cal texture model from “in-the-wild” facial images,
which is in full correspondence with a statistical shape
prior that exhibits both identity and expression varia-
tions. Motivated by the success of feature-based (e.g.,
HOG [16], SIFT [26]) Active Appearance Models
(AAMs) [4, 5] we further show how to learn feature-
based texture models for 3DMMs. We show that the
advantage of using the “in-the-wild” feature-based tex-
ture model is that the ﬁtting strategy gets simpliﬁed

since there is not need to optimize with respect to the
illumination parameters.

• By capitalising on the recent advancements in ﬁtting
statistical deformable models [30, 38, 5, 2], we pro-
pose a novel and fast algorithm for ﬁtting “in-the-wild”
3DMMs. Furthermore, we make the implementation
of our algorithm publicly available, which we believe
can be of great beneﬁt to the community, given the
lack of robust open-source implementations for ﬁtting
3DMMs.

• Due to lack of ground-truth data, the majority of the
3D face reconstruction papers report only qualitative
results.
In this paper, in order to provide quantita-
tive evaluations, we collected a new dataset of 3D fa-
cial surfaces, using Kinect Fusion [19, 29], which has
many “in-the-wild” characteristics, even though it is
captured indoors.

• We release an open source implementation of our tech-

nique as part of the Menpo Project. [1]

The remainder of the paper is structured as follows.
In Section 2 we elaborate on the construction of our “in-
the-wild” 3DMM, whilst in Section 3 we outline the pro-
posed optimization for ﬁtting “in-the-wild” images with our
model. Section 4 describes our new dataset, the ﬁrst of its
kind, to provide images with a ground-truth 3D facial shape
that exhibit many “in-the-wild” characteristics. We outline
a series of quantitative and qualitative experiments in Sec-
tion 5, and end with conclusions in Section 6.

2. Model Training

A 3DMM consists of three parametric models:

the

shape, camera and texture models.

2.1. Shape Model

Let us denote the 3D mesh (shape) of an object with N

vertexes as a 3N × 1 vector

s = (cid:2)xT

1 , . . . , xT
N

(cid:3)T

= [x1, y1, z1, . . . , xN , yN , zN ]T (1)

where xi = [xi, yi, zi]T are the object-centered Cartesian
coordinates of the i-th vertex. A 3D shape model can be
constructed by ﬁrst bringing a set of 3D training meshes
into dense correspondence so that each is described with
the same number of vertexes and all samples have a shared
semantic ordering. The corresponded meshes, {si}, are
then brought into a shape space by applying Generalized
Procrustes Analysis and then Principal Component Anal-
ysis (PCA) is performed which results in {¯s, Us}, where
¯s ∈ R3N is the mean shape vector and Us ∈ R3N ×ns is the

Quaternions. We parametrize the 3D rotation with
quaternions [24, 40]. The quaternion uses four parameters
q = [q0, q1, q2, q3]T in order to express a 3D rotation as

Rv = 2





1

2 − q2
2 − q2
3
q1q2 + q0q3
q1q3 − q0q2

q1q2 − q0q3
1
2 − q2
1 − q2
3
q2q3 + q0q1

q1q3 + q0q2
q2q3 − q0q1
1
1 − q2
2 − q2
2





1 − q2

2 − q2

i.e. qTq = 1,

(5)
Note that by enforcing a unit norm constraint on the quater-
the rotation matrix con-
nion vector,
straints of orthogonality with unit determinant are with-
held. Given the unit norm property, the quaternion can be
seen as a three-parameter vector [q1, q2, q3]T and a scalar
q0 = (cid:112)1 − q2
3. Most existing works on 3DMM
parametrize the rotation matrix Rv using the three Eu-
ler angles that deﬁne the rotations around the horizontal,
vertical and camera axes. Even thought Euler angles are
more naturally interpretable, they have strong disadvantages
when employed within an optimization procedure, most no-
tably the solution ambiguity and the gimbal lock effect.
Parametrization based on quaternions overcomes these dis-
advantages and further ensures computational efﬁciency, ro-
bustness and simpler differentiation.

Camera function. The projection operation performed
by the camera model of the 3DMM can be expressed with
the function P(s, c) : R3N → R2N , which applies the
transformations of Eqs. 3 and 4 on the points of provided
3D mesh s with

c = [f, q1, q2, q3, tx, ty, tz]T

(6)

being the vector of camera parameters with length nc = 7.
For abbreviation purposes, we represent the camera model
of the 3DMM with the function W : Rns,nc → R2N as

W(p, c) ≡ P (S(p), c)

(7)

where S(p) is a 3D mesh instance using Eq. 2.

2.3. “In-the-Wild” Feature-Based Texture Model

The generation of an “in-the-wild” texture model is a
key component of the proposed 3DMM. To this end, we
take advantage of the existing large facial “in-the-wild”
databases that are annotated in terms of sparse landmarks.
Assume that for a set of M “in-the-wild” images {Ii}M
1 ,
we have access to the associated camera and shape param-
eters {pi, ci}. Let us also deﬁne a dense feature extraction
function

F : RH×W → RH×W ×C

(8)

where C is the number of channels of the feature-based im-
age. For each image, we ﬁrst compute its feature-based rep-
resentation as Fi = F(Ii) and then use Eq. 7 to sample
it at each vertex location to build back a vectorized texture
sample ti = Fi (W(pi, ci)) ∈ RCN . This texture sample

Figure 2. Left: The mean and ﬁrst four shape and SIFT texture
principal components of our “in-the-wild” SIFT texture model.
Right: To aid in interpretation we also show the equivalent RGB
basis.

orthonormal basis after keeping the ﬁrst ns principal com-
ponents. This model can be used to generate novel 3D shape
instances using the function S : Rns → R3N as

S(p) = ¯s + Usp

(2)

where p = [p1, . . . , pns ]T are the ns shape parameters.

2.2. Camera Model

The purpose of the camera model is to map (project)
the object-centered Cartesian coordinates of a 3D mesh in-
stance s into 2D Cartesian coordinates on an image plane.
In this work, we employ a pinhole camera model, which
utilizes a perspective transformation. However, an ortho-
graphic projection model can also be used in the same way.
Perspective projection. The projection of a 3D point
x = [x, y, z]T into its 2D location in the image plane x(cid:48) =
[x(cid:48), y(cid:48)]T involves two steps. First, the 3D point is rotated
and translated using a linear view transformation, under the
assumption that the camera is still

[vx, vy, vz]T = Rvx + tv

where Rv ∈ R3×3 and tv = [tx, ty, tz]T are the 3D rota-
tion and translation components, respectively. Then, a non-
linear perspective transformation is applied as

x(cid:48) =

(cid:20) vx
vy

f
vz

(cid:21)

+

(cid:20) cx
cy

(cid:21)

(3)

(4)

where f is the focal length in pixel units (we assume that
the x and y components of the focal length are equal) and
[cx, cy]T is the principal point that is set to the image center.

Finally, an iterative procedure is used in order to reﬁne
the texture. That is, we started with the 3D ﬁts provided by
using only the 2D landmarks [21]. Then, a texture model
is learned using the above procedure. The texture model
was used with the proposed 3DMM ﬁtting algorithm on the
same data and texture model was reﬁned.

3. Model Fitting

We propose to ﬁt the 3DMM on an input image using
Gauss-Newton iterative optimization. To this end, herein,
we ﬁrst formulate the cost function and then present two
optimization procedures.

3.1. Cost Function

The overall cost function of the proposed 3DMM for-
mulation consists of a texture-based term, an optional error
term based on sparse 2D landmarks and optional regulariza-
tion terms on the parameters.

Texture reconstruction cost. The main term of the opti-
mization problem is the one that aims to estimate the shape,
2 norm
texture and camera parameters that minimize the (cid:96)2
of the difference between the image feature-based texture
that corresponds to the projected 2D locations of the 3D
shape instance and the texture instance of the 3DMM. Let
us denote by F = F(I) the feature-based representation
with C channels of an input image I using Eq. 8. Then, the
texture reconstruction cost is expressed as

arg min
p,c,λ

(cid:107)F (W(p, c)) − T (λ)(cid:107)2

(11)

Note that F (W(p, c)) ∈ RCN denotes the operation of
sampling the feature-based input image on the projected 2D
locations of the 3D shape instance acquired by the camera
model (Eq. 7).

Regularization.

In order to avoid over-ﬁtting effects,
we augment the cost function with two optional regular-
ization terms over the shape and texture parameters. Let
us denote as Σs ∈ Rns×ns and Σt ∈ Rnt×nt the diago-
nal matrices with the eigenvalues in their main diagonal for
the shape and texture models, respectively. Based on the
PCA nature of the shape and texture models, it is assumed
that their parameters follow normal prior distributions, i.e.
p ∼ N (0, Σs) and λ ∼ N (0, Σt). We formulate the
2 of the parameters’ vectors
regularization terms as the (cid:96)2
weighted with the corresponding inverse eigenvalues, i.e.

arg min
p,λ

cs (cid:107)p(cid:107)2

Σ−1
s

+ ct (cid:107)λ(cid:107)2

Σ−1
t

(12)

where cs and ct are constants that weight the contribution
of the regularization terms in the cost function.

2D landmarks cost. In order to rapidly adapt the cam-
era parameters in the cost of Eq. 11, we further expand the

Figure 3. Building an ITW texture model

will be nonsensical for some regions mainly due to self-
occlusions present in the mesh projected in the image space
W(pi, ci). To alleviate these issues, we cast a ray from the
camera to each vertex and test for self-intersections with the
triangulation of the mesh in order to learn a per-vertex oc-
clusion mask mi ∈ RN for the projected sample.

Let us create the matrix X = [t1, . . . , tM ] ∈ RCN ×M
by concatenating the M grossly corrupted feature-based
texture vectors with missing entries that are represented by
the masks mi. To robustly build a texture model based
on this heavily contaminated incomplete data, we need to
recover a low-rank matrix L ∈ RCN ×M representing the
clean facial texture and a sparse matrix E ∈ RCN ×M ac-
counting for gross but sparse non-Gaussian noise such that
X = L + E. To simultaneously recover both L and E from
incomplete and grossly corrupted observations, the Princi-
pal Component Pursuit with missing values [34] is solved

arg min
L,E

(cid:107)L(cid:107)∗ + λ(cid:107)E(cid:107)1

s.t. PΩ(X) = PΩ(L + E),

(9)

where (cid:107)·(cid:107)∗ denotes the nuclear norm, (cid:107)·(cid:107)1 is the matrix (cid:96)1-
norm and λ > 0 is a regularizer. Ω represents the set of
locations corresponding to the observed entries of X (i.e.,
(i, j) ∈ Ω if mi = mj = 1). Then, PΩ(X) is deﬁned as
the projection of the matrix X on the observed entries Ω,
namely PΩ(X)ij = xij if (i, j) ∈ Ω and PΩ(X)ij = 0
otherwise. The unique solution of the convex optimization
problem in Eq. 9 is found by employing an Alternating Di-
rection Method of Multipliers-based algorithm [11].

The ﬁnal texture model is created by applying PCA
on the set of reconstructed feature-based textures acquired
from the previous procedure. This results in {¯t, Ut}, where
¯t ∈ RCN is the mean texture vector and Ut ∈ RCN ×nt
is the orthonormal basis after keeping the ﬁrst nt prin-
cipal components. This model can be used to generate
novel 3D feature-based texture instances with the function
T : Rnt → RCN as

T (λ) = ¯t + Utλ

(10)

where λ = [λ1, . . . , λnt]T are the nt texture parameters.

optimization problem with the term

expressed as

arg min
p,c

cl (cid:107)Wl(p, c) − sl(cid:107)2

(13)

arg min
∆p,∆c,∆λ

(cid:107)F (W(p + ∆p, c + ∆c)) − T (λ + ∆λ)(cid:107)2 +

where sl = [x1, y1, . . . , xL, yL]T denotes a set of L sparse
2D landmark points (L (cid:28) N ) deﬁned on the image coordi-
nate system and Wl(p, c) returns the 2L × 1 vector of 2D
projected locations of these L sparse landmarks. Intuitively,
this term aims to drive the optimization procedure using the
selected sparse landmarks as anchors for which we have the
optimal locations sl. This optional landmarks-based cost is
weighted with the constant cl.

Overall cost function. The overall 3DMM cost function
is formulated as the sum of the terms in Eqs. 11, 12, 13, i.e.

arg min
p,c,λ

(cid:107)F (W(p, c)) − T (λ)(cid:107)2 + cl (cid:107)Wl(p, c) − sl(cid:107)2 +

+ cs (cid:107)p(cid:107)2

Σ−1
s

+ ct (cid:107)λ(cid:107)2

Σ−1
t

(14)
The landmarks term as well as the regularization terms are
optional and aim to facilitate the optimization procedure in
order to converge faster and to a better minimum. Note that
thanks to the proposed “in-the-wild” feature-based texture
model, the cost function does not include any parametric
illumination model similar to the ones in the relative litera-
ture [12, 13], which greatly simpliﬁes the optimization.

3.2. Gauss-Newton Optimization

Inspired by the extensive literature in Lucas-Kanade 2D
image alignment [8, 28, 30, 38, 5, 2], we formulate a Gauss-
Newton optimization framework. Speciﬁcally, given that
the camera projection model is applied on the image part of
Eq. 14, the proposed optimization has a “forward” nature.

Parameters update. The shape, texture and camera pa-

rameters are updated in an additive manner, i.e.

p ← p + ∆p, λ ← λ + ∆λ, c ← c + ∆c

(15)

where ∆p, ∆λ and ∆c are their increments estimated at
each ﬁtting iteration. Note that in the case of the quaternion
used to parametrize the 3D rotation matrix, the update is
performed as the multiplication

q ←(∆q)q =

(cid:20)

=

(cid:20) ∆q0
(cid:21) (cid:20) q0
∆q1:3
q1:3
∆q0q0 − ∆qT

(cid:21)

=

1:3q1:3

∆q0q1:3 + q0∆q1:3 + ∆q1:3 × q1:3

(cid:21) (16)

However, we will still denote it as an addition for simplicity.
Finally, we found that it is beneﬁcial to keep the focal length
constant in most cases, due to its ambiguity with tz.

Linearization. By introducing the additive incremental
updates on the parameters of Eq. 14, the cost function is

+ cl (cid:107)Wl(p + ∆p, c + ∆c) − sl(cid:107)2 +
+ cs (cid:107)p + ∆p(cid:107)2
+ ct (cid:107)λ + ∆λ(cid:107)2

Σ−1
s

Σ−1
t

(17)
Note that the texture reconstruction and landmarks con-
straint terms of this cost function are non-linear due to
the camera model operation. We need to linearize them
around (p, c) using ﬁrst order Taylor series expansion at
(p + ∆p, c + ∆c) = (p, c) ⇒ (∆p, ∆c) = 0. The lin-
earization for the image term gives

F (W(p + ∆p, c + ∆c)) ≈F (W(p, c)) +

+ JF,p∆p + JF,c∆c

(18)

(cid:12)
(cid:12)
(cid:12)p=p

and JF,c = ∇F ∂W
∂c

(cid:12)
where JF,p = ∇F ∂W
(cid:12)c=c are
∂p
the image Jacobians with respect to the shape and cam-
era parameters, respectively. Note that most dense feature-
extraction functions F(·) are non-differentiable, thus we
simply compute the gradient of the multi-channel feature
image ∇F. Similarly, the linearization on the sparse land-
marks projection term gives

Wl(p + ∆p, c + ∆c) ≈ Wl(p, c) + JWl,p∆p + JWl,c∆c
(19)
(cid:12)
where JWl,p = ∂Wl
(cid:12)c=c are the
∂p
camera Jacobians. Please refer to the supplementary mate-
rial for more details on the computation of these derivatives.

and JWl,c = ∂Wl
∂c

(cid:12)
(cid:12)
(cid:12)p=p

3.2.1 Simultaneous

Herein, we aim to simultaneously solve for all parameters’
increments. By substituting Eqs. 18 and 19 in Eq. 17 we get

arg min
∆p,∆c,∆λ
(cid:107)F (W(p, c)) + JF,p∆p + JF,c∆c − T (λ + ∆λ)(cid:107)2 +
+ cl (cid:107)Wl(p, c) + JWl,p∆p + JWl,c∆c − sl(cid:107)2 +
+ cs (cid:107)p + ∆p(cid:107)2

+ ct (cid:107)λ + ∆λ(cid:107)2

Σ−1
s

Σ−1
t

(20)
Let us concatenate the parameters and their increments as
T
b = [pT, cT, λT]
. By tak-
ing the derivative of the ﬁnal linearized cost function with
respect to ∆b and equalizing with zero, we get the solution

T
and ∆b = [∆pT, ∆cT, ∆λT]

b = −H−1 (cid:0)JT

FeF + clJT
Wl

el + csΣ−1

s p + ctΣ−1

t λ(cid:1)

where H = JF
Hessian with

TJF + clJWl

TJWl + csΣ−1

s + ctΣ−1

t

JF = (cid:2)JT
JWl = (cid:2)JT

F,p, JT
Wl,p, JT

F,c, −UT
t

(cid:3)T

Wl,c, 0nt×2L

(cid:3)T

(21)
is the

(22)

and

eF = F (W(p, c)) − T (λ)
el = Wl(p, c) − sl

(23)

are the residual terms. The computational complexity of
the Simultaneous algorithm per iteration is dominated by
the texture reconstruction term as O((ns + nc + nt)3 +
CN (ns + nc + nt)2), which in practice is too slow.

3.2.2 Project-Out

We propose to use a Project-Out optimization approach that
is much faster than the Simultaneous. The main idea is to
optimize on the orthogonal complement of the texture sub-
space which will eliminate the need to solve for the tex-
ture parameters increment at each iteration. By substituting
Eqs. 18 and 19 into Eq. 17 and removing the incremental
update on the texture parameters as well as the texture pa-
rameters regularization term, we end up with the problem

arg min
∆p,∆c,λ

(cid:107)F (W(p, c)) + JF,p∆p + JF,c∆c − T (λ)(cid:107)2 +

+ cl (cid:107)Wl(p, c) + JWl,p∆p + JWl,c∆c − sl(cid:107)2 +
+ cs (cid:107)p + ∆p(cid:107)2

Σ−1
s

(24)
The solution of Eq. 24 with respect to λ is readily given by

λ = Ut

T (F(W(p, c)) + JF,p∆p + JF,c∆c − ¯t)

(25)

By plugging Eq. 25 into Eq. 24, we get

arg min
∆p,∆c

(cid:107)F (W(p, c)) + JF,p∆p + JF,c∆c − ¯t(cid:107)2

P +

+ cl (cid:107)Wl(p, c) + JWl,p∆p + JWl,c∆c − sl(cid:107)2 +
+ cs (cid:107)p + ∆p(cid:107)2

Σ−1
s

(26)
T is the orthogonal complement of the
where P = E−UtUt
texture subspace that functions as the “project-out” operator
with E denoting the CN × CN unitary matrix. Note that
in order to derive Eq. 26, we use the properties PT = P
and PTP = P. By differentiating Eq. 26 and equalizing to
zero, we get the solution

∆p = Hp

∆c = Hc

−1 (cid:0)JT
−1 (cid:0)JT

F,pPeF + clJT
F,cPeF + clJT

Wl,pel + csΣ−1
(cid:1)
Wl,cel

s p(cid:1)

(27)

where

Hp = JT
Hc = JT

F,pPJF,p + clJT
F,cPJF,c + clJT

Wl,pJWl,p + csΣ−1
Wl,cJWl,c

are the Hessian matrices and

eF = F (W(p, c)) − ¯t
el = Wl(p, c) − sl

are the residual terms. The texture parameters can be esti-
mated at the end of the iterative procedure using Eq. 25.

Note that the most expensive operation is JT

F,pP. How-
ever, if we ﬁrst do JT
F,pUt and then multiply this result
with UT
t , the total cost becomes O(CN ntns). The same
stands for JT
F,cP. Consequently, the cost per iteration is
O((ns + nc)3 + CN nt(ns + nc) + CN (ns + nc)2) which
is much faster than the Simultaneous algorithm.

Residual masking. In practice, we apply a mask on the
texture reconstruction residual of the Gauss-Newton opti-
mization, in order to speed-up the 3DMM ﬁtting. This mask
is constructed by ﬁrst acquiring the set of visible vertexes
using z-buffering and then randomly selecting K of them.
By keeping the number of vertexes small (K ≈ 5000 (cid:28)
N ), we manage to greatly speed-up the ﬁtting process with-
out any accuracy penalty.

4. KF-ITW Dataset

For the evaluation of the 3DMM, we have constructed
KF-ITW, the ﬁrst dataset of 3D faces captured under rela-
tively unconstrained conditions. The dataset consists of 17
different subjects recorded under various illumination con-
ditions performing a range of expressions (neutral, happy,
surprise). We employed the KinectFusion [19, 29] frame-
work to acquire a 3D representation of the subjects with a
Kinect v1 sensor.

The fused mesh for each subject serves as a 3D face
ground-truth in which we can evaluate our algorithm and
compare it to other methods. A voxel grid of size 6083 was
utilized to get the detailed 3D scans of the faces. In order
to accurately reconstruct the entire surface of the faces, a
circular motion scanning pattern was carried out. Each sub-
ject was instructed to stay still in a ﬁxed pose during the
entire scanning process. The frame rate for every subject
was constant to 8 frames per second. After getting the 3D
scans from the KinectFusion framework we ﬁt our shape
model in a non-rigid manner to get a clear mesh with a dis-
tinct number of vertexes for the evaluation process. Finally,
each mesh was manually annotated with the iBUG 49 sparse
landmark set.

5. Experiments

To train our model, which we label as ITW, we use a vari-
ant of the Basel Face Model (BFM) [31] that we trained to
contain both identities drawn from the original BFM model
along with expressions provided by [15]. We trained the
“in-the-wild” texture model on the images of iBUG, LFPW
& AFW datasets [32] as described in Sec. 2.3 using the 3D
shape ﬁts provided by [45]. Additionally, we elect to use the
project-out formulation for the throughout our experiments
due its superior run-time performance and equivalent ﬁtting
performance to the simultaneous one.

(28)

(29)

Method
ITW
Linear
Classic

AUC
0.678
0.615
0.531

Failure Rate (%)
1.79
4.02
13.9

Table 1. Accuracy results for facial shape estimation on KF-ITW
database. The table reports the Area Under the Curve (AUC) and
Failure Rate of the Cumulative Error Distributions of Fig. 4.

Figure 4. Accuracy results for facial shape estimation on KF-ITW
database. The results are presented as Cumulative Error Distribu-
tions of the normalized dense vertex error. Table 1 reports addi-
tional measures.

5.1. 3D Shape Recovery

Herein, we evaluate our “in-the-wild” 3DMM (ITW) in
terms of 3D shape estimation accuracy against two pop-
ular state-of-the-art alternative 3DMM formulations. The
ﬁrst one is a classic 3DMM with the original Basel labo-
ratory texture model and full lighting equation which we
term Classic. The second is the texture-less linear model
proposed in [17, 18] which we refer to as Linear. For Lin-
ear code we use the Surrey Model with related blendshapes
along with the implementation given in [18].

We use the ground-truth annotations provided in the KF-
ITW dataset to initialize and ﬁt all three techniques to the
“in-the-wild” style images in the dataset. The mean mesh
of each model under test is landmarked with the same 49-
point markup used in the dataset, and is registered against
the ground truth mesh by performing a Procrustes alignment
using the sparse annotations followed by Non-Rigid Iter-
ative Closest Point (N-ICP) to iteratively deform the two
surfaces until they are brought into correspondence. This
provides a per-model ‘ground-truth’ for the 3D shape re-
covery problem for each image under test. Our error metric
is the per-vertex dense error between the recovered shape
and the model-speciﬁc corresponded ground-truth ﬁt, nor-
malized by the inter-ocular distance for the test mesh. Fig. 4
shows the cumulative error distribution for this experiment
for the three models under test. Table 1 reports the corre-
sponding Area Under the Curve (AUC) and failure rates.
The Classic model struggles to ﬁt to the “in-the-wild” con-
ditions present in the test set, and performs the worst. The
texture-free Linear model does better, but the ITW model is
most able to recover the facial shapes due to its ideal feature
basis for the “in-the-wild” conditions.

Figure 6 demonstrates qualitative results on a wide range
of ﬁts of “in-the-wild” images drawn from the Helen and

Figure 5. Results on facial surface normal estimation in the form
of Cumulative Error Distribution of mean angular error.

300W datasets [32, 33] that qualitatively highlight the ef-
fectiveness of the proposed technique. We note that in a
wide variety of expression, identity, lighting and occlusion
conditions our model is able to robustly reconstruct a real-
istic 3D facial shape that stands up to scrutiny.

5.2. Quantitative Normal Recovery

As a second evaluation, we use our technique to ﬁnd
per-pixel normals and compare against two well estab-
lished Shape-from-Shading (SfS) techniques: PS-NL [9]
and IMM [23]. For experimental evaluation we employ im-
ages of 100 subjects from the Photoface database [42]. As
a set of four illumination conditions are provided for each
subject then we can generate ground-truth facial surface
normals using calibrated 4-source Photometric Stereo [27].
In Fig. 5 we show the cumulative error distribution in terms
of the mean angular error. ITW slightly outperforms IMM
even though both IMM and PS-NL use all four available im-
ages of each subject.

6. Conclusion

We have presented a novel formulation of 3DMMs re-
imagined for use in “in-the-wild” conditions. We capitalise
on the annotated “in-the-wild” facial databases to propose
a methodology for learning an “in-the-wild” feature-based
texture model suitable for 3DMM ﬁtting without having to
optimise for illumination parameters. Furthermore, we pro-
pose a novel optimisation procedure for 3DMM ﬁtting. We

Figure 6. Examples of in the wild ﬁts of our ITW 3DMM taken from 300W [32].

show that we are able to recover shapes with more detail
than is possible using purely landmark-driven approaches.
Our newly introduced “in-the-wild” KinectFusion dataset
allows for the ﬁrst time a quantitative evaluation of 3D fa-

cial reconstruction techniques in the wild, and on these eval-
uations we demonstrate that our in the wild formulation is
state of the art, outperforming classical 3DMM approaches
by a considerable margin.

References

[1] J. Alabort-i Medina, E. Antonakos, J. Booth, P. Snape, and
S. Zafeiriou. Menpo: A comprehensive platform for para-
metric image alignment and visual deformable models.
In
Proceedings of the ACM International Conference on Multi-
media, MM ’14, pages 679–682, New York, NY, USA, 2014.
ACM. 1, 2

[2] J. Alabort-i Medina and S. Zafeiriou. A uniﬁed framework
for compositional ﬁtting of active appearance models. Inter-
national Journal of Computer Vision, pages 1–39, 2016. 2,
5

[3] O. Aldrian and W. A. Smith. Inverse rendering of faces with
a 3d morphable model. IEEE transactions on pattern analy-
sis and machine intelligence, 35(5):1080–1093, 2013. 2
[4] E. Antonakos, J. Alabort-i-Medina, G. Tzimiropoulos, and
In Proceed-
S. Zafeiriou. Hog active appearance models.
ings of IEEE International Conference on Image Processing,
pages 224–228. IEEE, 2014. 2

[5] E. Antonakos, J. Alabort-i-Medina, G. Tzimiropoulos, and
S. Zafeiriou. Feature-based lucas-kanade and active ap-
pearance models. IEEE Transactions on Image Processing,
24(9):2617–2632, September 2015. 1, 2, 5

[6] E. Antonakos, J. Alabort-i-Medina, and S. Zafeiriou. Ac-
In Proceedings of IEEE Interna-
tive pictorial structures.
tional Conference on Computer Vision & Pattern Recogni-
tion, pages 5435–5444, Boston, MA, USA, June 2015. IEEE.
1

[7] A. Asthana, S. Zafeiriou, S. Cheng, and M. Pantic. Incre-
In Proceedings of the
mental face alignment in the wild.
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1859–1866, 2014. 1

[8] S. Baker and I. Matthews. Lucas-kanade 20 years on: A uni-
fying framework. International journal of computer vision,
56(3):221–255, 2004. 5

[9] R. Basri, D. Jacobs, and I. Kemelmacher. Photometric stereo
with general, unknown lighting. IJCV, 72(3):239–257, 2007.
7

[10] P. N. Belhumeur, D. W. Jacobs, D. J. Kriegman, and N. Ku-
mar. Localizing parts of faces using a consensus of exem-
IEEE transactions on pattern analysis and machine
plars.
intelligence, 35(12):2930–2940, 2013. 1

[11] D. P. Bertsekas. Constrained optimization and Lagrange

multiplier methods. Academic press, 2014. 4

[12] V. Blanz and T. Vetter. A morphable model for the synthesis
of 3d faces. In Proceedings of the 26th annual conference on
Computer graphics and interactive techniques, pages 187–
194. ACM Press/Addison-Wesley Publishing Co., 1999. 1,
5

[13] V. Blanz and T. Vetter. Face recognition based on ﬁtting a 3d
IEEE Transactions on pattern analysis

morphable model.
and machine intelligence, 25(9):1063–1074, 2003. 1, 5
[14] J. Booth, A. Roussos, S. Zafeiriou, A. Ponniah, and D. Dun-
away. A 3d morphable model learnt from 10,000 faces. In
CVPR, 2016. 2

[15] C. Cao, Y. Weng, S. Zhou, Y. Tong, and K. Zhou. Faceware-
house: A 3d facial expression database for visual computing.

IEEE Transactions on Visualization and Computer Graphics,
20(3):413–425, 2014. 2, 6

[16] N. Dalal and B. Triggs. Histograms of oriented gradi-
In 2005 IEEE Computer Soci-
ents for human detection.
ety Conference on Computer Vision and Pattern Recognition
(CVPR’05), volume 1, pages 886–893. IEEE, 2005. 2

[17] P. Huber, Z.-H. Feng, W. Christmas,

J. Kittler, and
M. R¨atsch. Fitting 3d morphable face models using local
features. In Image Processing (ICIP), 2015 IEEE Interna-
tional Conference on, pages 1195–1199. IEEE, 2015. 2, 7

[18] P. Huber, G. Hu, R. Tena, P. Mortazavian, W. P. Koppen,
W. Christmas, M. R¨atsch, and J. Kittler. A multiresolution
3d morphable face model and ﬁtting framework. In Proceed-
ings of the 11th International Joint Conference on Computer
Vision, Imaging and Computer Graphics Theory and Appli-
cations, 2016. 7

[19] S. Izadi, D. Kim, O. Hilliges, D. Molyneaux, R. Newcombe,
P. Kohli, J. Shotton, S. Hodges, D. Freeman, A. Davison,
et al. Kinectfusion: real-time 3d reconstruction and inter-
action using a moving depth camera. In Proceedings of the
24th annual ACM symposium on User interface software and
technology, pages 559–568. ACM, 2011. 2, 6

[20] V. Jain and E. Learned-Miller. Fddb: A benchmark for face
detection in unconstrained settings. Technical Report UM-
CS-2010-009, University of Massachusetts, Amherst, 2010.
1

[21] A. Jourabloo and X. Liu. Large-pose face alignment via cnn-

based dense 3d model ﬁtting. In CVPR, 2016. 4

[22] V. Kazemi and J. Sullivan. One millisecond face alignment
with an ensemble of regression trees. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1867–1874, 2014. 1

[23] I. Kemelmacher-Shlizerman.

Internet based morphable
model. In Proceedings of the IEEE International Conference
on Computer Vision, pages 3256–3263, 2013. 1, 2, 7
[24] J. B. Kuipers et al. Quaternions and rotation sequences, vol-
ume 66. Princeton university press Princeton, 1999. 3
[25] V. Le, J. Brandt, Z. Lin, L. Bourdev, and T. S. Huang. In-
teractive facial feature localization. In European Conference
on Computer Vision, pages 679–692. Springer, 2012. 1
[26] D. G. Lowe. Object recognition from local scale-invariant
features. In Computer vision, 1999. The proceedings of the
seventh IEEE international conference on, volume 2, pages
1150–1157. Ieee, 1999. 2

[27] D. Marr and H. K. Nishihara. Representation and recog-
three-dimensional
nition of
shapes. Royal Society of London B: Biological Sciences,
200(1140):269–294, 1978. 7

the spatial organization of

[28] I. Matthews and S. Baker. Active appearance models revis-
ited. International Journal of Computer Vision, 60(2):135–
164, 2004. 5

[29] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and
A. Fitzgibbon. Kinectfusion: Real-time dense surface map-
ping and tracking. In Mixed and augmented reality (ISMAR),
2011 10th IEEE international symposium on, pages 127–
136. IEEE, 2011. 2, 6

[43] S. Zafeiriou, C. Zhang, and Z. Zhang. A survey on face
detection in the wild: past, present and future. Computer
Vision and Image Understanding, 138:1–24, 2015. 1
[44] S. Zhu, C. Li, C. Change Loy, and X. Tang. Face alignment
In Proceedings of the
by coarse-to-ﬁne shape searching.
IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 4998–5006, 2015. 1

[45] X. Zhu, Z. Lei, X. Liu, H. Shi, and S. Z. Li. Face alignment
across large poses: A 3d solution. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
2016. 6

[46] X. Zhu and D. Ramanan. Face detection, pose estimation,
and landmark localization in the wild. In Computer Vision
and Pattern Recognition (CVPR), 2012 IEEE Conference on,
pages 2879–2886. IEEE, 2012. 1

[30] G. Papandreou and P. Maragos. Adaptive and constrained al-
gorithms for inverse compositional active appearance model
ﬁtting. In Computer Vision and Pattern Recognition, 2008.
CVPR 2008. IEEE Conference on, pages 1–8. IEEE, 2008.
2, 5

[31] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vet-
ter. A 3d face model for pose and illumination invariant face
In Advanced video and signal based surveil-
recognition.
lance, 2009. AVSS’09. Sixth IEEE International Conference
on, pages 296–301. IEEE, 2009. 2, 6

[32] C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou,
and M. Pantic. 300 faces in-the-wild challenge: Database
Image and Vision Computing, Special Issue
and results.
on Facial Landmark Localisation ”In-The-Wild”, 47:3–18,
2016. 1, 6, 7, 8

[33] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic.
300 faces in-the-wild challenge: The ﬁrst facial landmark
In Proceedings of IEEE Intl Conf.
localization challenge.
on Computer Vision (ICCV-W 2013), 300 Faces in-the-Wild
Challenge (300-W), Sydney, Australia, December 2013. 1, 7
[34] F. Shang, Y. Liu, J. Cheng, and H. Cheng. Robust principal
In Proceedings of
component analysis with missing data.
the 23rd ACM International Conference on Conference on
Information and Knowledge Management, CIKM ’14, pages
1149–1158, New York, NY, USA, 2014. ACM. 4

[35] P. Snape, Y. Panagakis, and S. Zafeiriou. Automatic con-
In Pro-
struction of robust spherical harmonic subspaces.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 91–100, 2015. 1, 2

[36] P. Snape and S. Zafeiriou. Kernel-pca analysis of surface nor-
In Proceedings of the IEEE
mals for shape-from-shading.
Conference on Computer Vision and Pattern Recognition,
pages 1059–1066, 2014. 1

[37] G. Trigeorgis, P. Snape, M. Nicolaou, E. Antonakos, and
S. Zafeiriou. Mnemonic descent method: A recurrent pro-
cess applied for end-to-end face alignment. In Proceedings
of IEEE International Conference on Computer Vision &
Pattern Recognition, Las Vegas, NV, USA, June 2016. IEEE.
1

[38] G. Tzimiropoulos and M. Pantic. Optimization problems for
fast aam ﬁtting in-the-wild. In Proceedings of the IEEE in-
ternational conference on computer vision, pages 593–600,
2013. 2, 5

[39] G. Tzimiropoulos and M. Pantic. Gauss-newton deformable
part models for face alignment in-the-wild. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1851–1858, 2014. 1

[40] M. Wheeler and K. Ikeuchi. Iterative estimation of rotation
and translation using the quaternion: School of computer sci-
ence, 1995. 3

[41] X. Xiong and F. De la Torre. Supervised descent method
and its applications to face alignment. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 532–539, 2013. 1

[42] S. Zafeiriou, M. Hansen, G. Atkinson, V. Argyriou,
M. Petrou, M. Smith, and L. Smith. The photoface database.
In CVPR, pages 132–139, June 2011. 7

