A Comparison of Adaptation Techniques and
Recurrent Neural Network Architectures

Jan Vanˇek[0000−0002−2639−6731], Josef Mich´alek[0000−0001−7757−3163],
Jan Zelinka[0000−0001−6834−9178], and Josef Psutka[0000−0002−0764−3207]

University of West Bohemia
Univerzitn´ı 8, 301 00 Pilsen, Czech Republic
{vanekyj,orcus,zelinka,psutka}@kky.zcu.cz

Abstract. Recently, recurrent neural networks have become state-of-
the-art in acoustic modeling for automatic speech recognition. The long
short-term memory (LSTM) units are the most popular ones. However,
alternative units like gated recurrent unit (GRU) and its modiﬁcations
outperformed LSTM in some publications. In this paper, we compared
ﬁve neural network (NN) architectures with various adaptation and fea-
ture normalization techniques. We have evaluated feature-space maxi-
mum likelihood linear regression, ﬁve variants of i-vector adaptation and
two variants of cepstral mean normalization. The most adaptation and
normalization techniques were developed for feed-forward NNs and, ac-
cording to results in this paper, not all of them worked also with RNNs.
For experiments, we have chosen a well known and available TIMIT
phone recognition task. The phone recognition is much more sensitive to
the quality of AM than large vocabulary task with a complex language
model. Also, we published the open-source scripts to easily replicate the
results and to help continue the development.

Keywords: neural networks · acoustic model · TIMIT · LSTM · GRU
· phone recognition · adaptation · i-vectors

1

Introduction

Neural Networks (NNs) and deep NNs (DNNs) became dominant in the ﬁeld of
the acoustic modeling several years ago. Simple feed-forward (FF) DNNs were
faded away in recent years. The current progress is based on the modeling of a
longer temporal context of individual feature frames. Main two ways are actu-
ally popular: First, a larger context is modeled by a time-delayed NN (TDNN)
[16], [9]. TDNNs model long term temporal dependencies with training times
comparable to standard feed-forward DNNs. In the TDNN architecture, the ini-
tial transforms learn narrow contexts and the deeper layers process the hidden
activations from a wider temporal context. Hence the higher layers have the
ability to learn wider temporal relationships. The second way to learn the longer
temporal context is to use recurrent NNs (RNNs). The most popular RNN archi-
tecture is a long short-term memory (LSTM) that has been designed to address

8
1
0
2
 
l
u
J
 
2
1
 
 
]
S
A
.
s
s
e
e
[
 
 
1
v
1
4
4
6
0
.
7
0
8
1
:
v
i
X
r
a

2

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

the vanishing and exploding gradient problems of conventional RNNs. Unlike
feed-forward neural networks, RNNs have cyclic connections making them pow-
erful for modeling sequences [12]. The main drawback is much slower training
due to the sequential nature of the learning algorithm. An unfolding of the re-
current network during training was proposed in [13] to speed-up the training,
however it is still signiﬁcantly slower than FF NNs or TDNNs. More recently,
another type of recurrent unit, a gated recurrent unit (GRU), was proposed in
[2], [3]. Similarly to the LSTM unit, the GRU has gating units that modulate the
ﬂow of information inside the unit, however, without having a separate mem-
ory cells. Further revising GRUs leaded to a simpliﬁed architecture potentially
more suitable for speech recognition in [11]. First, removing the reset gate in the
GRU design resulted in a simpler single-gate architecture called modiﬁed GRU
(M-GRU). Second, replacing tanh with ReLU activations in the state update
equations was proposed and called M-reluGRU. A more detailed overview of the
RNN architectures follows in Section 3.

Even if large datasets are used for the DNN training, an adaptation of an
acoustic model (AM) to a test speaker and environment is beneﬁcial. A lot of
techniques have been reported on the adaptation, such as the classical maxi-
mum a posterior (MAP) and maximum likelihood linear regression (MLLR) for
traditional GMM-HMM acoustic models. Although this technique can be mod-
iﬁed for an NN-based acoustic model, a much simpler application has so-called
[4] because fMLLR changes only features and
feature space MLLR (fMLLR)
it does not adapt NN parameters. This speaker adaptation technique can be
easily applied in an NN-based acoustic model [15], [8], [10]. Therefore, fMLLR
can be used to any DNN architecture. I-vectors originally developed for speaker
recognition can be used to the speaker and environment adaptation also [14],
[7]. Alternative approach is using of discriminative speaker codes [17], [6]. More
detailed description of the adaptation techniques used in this paper follows.

2 Adaptation of DNNs

The simplest way of the adaptation is a feature level adaptation. When adapting
input features, NN can have any structure. The most popular and well known
technique is fMLLR based on an underlying HMM-GMM that is used during
initial stage of the NN training.

2.1 fMLLR

The fMLLR transforms feature frames with a speaker-speciﬁc square matrix A
and a bias b. For HMM-GMMs, A and b are estimated to maximize the like-
lihood of the adaptation data given the model [4], [15]. In the training phase,
the speaker-speciﬁc transform may be updated several-times alternating HMM-
GMM update. These approach is usually called a speaker adaptive training
(SAT). The result of the training phase is a canonical model that requires us-
ing adaptation during testing phase. However, two-pass processing is required

A Comparison of Adaptation Techniques and RNN Architectures

3

during test phase. The ﬁrst pass produces unsupervised alignment that is used
to estimate the transform parameters via maximum likelihood. The model used
for alignment does not need to be the identical model to the ﬁnal canonical one.
Because all the steps are using the underline HMM-GMM any NN architecture
may be used to train the ﬁnal NN acoustic model.

2.2 i-vectors

The i-vector extraction is a well known technique, so we focused here to more
practical points. An detailed description of i-vectors can be found in [14], [7] and
further papers referenced in there.

The i-vector extraction is comprised from following steps:

1. An universal background model (UBM) needs to be trained. Usually a GMM
with 512 to 2048 diagonal components is used. The quality of GMM is not
critical, so some speed-up methods can be utilized. Features for UBM do
not need to match witch features for NN nor i-vector accumulators. Usu-
ally, features with cepstral mean normalization (CMN) or cepstral mean
and variance normalization (CMVN) are used for UBM. The normalization
techniques reduce speaker and environment variability. Features without any
normalization are used for the i-vector accumulators to carry more speaker-
and environment-related information.

2. Zero-order and centered ﬁrst-order statistics are accumulated for every speaker

3. The i-vector extraction transforms are estimated iteratively by expecta-

according to the UBM posteriors.

tion/maximization (EM) algorithm.

4. The i-vector for individual speakers is evaluated. For training speakers, zero-
order and centered ﬁrst-order statistics have been already accumulated. For
other speakers, statistics must be accumulated. Then, the i-vector is evalu-
ated by the i-vector extraction transforms computed in the third step.

The four step process seems simple but there are some details that need to be
mentioned:

– CMN or CMVN may be computed online or oﬄine. The oﬄine variant may
be per-utterance or per-speaker. The online variant starts from global cep-
stral mean and it is subsequently updated. An exponential forgetting is us-
able for very long utterances. The training setup should match with the
testing one.

– The accumulated statistic should be saturated or scaled-down for long ut-

terances due to an i-vector overﬁtting.

– The oﬄine scenario is not proper for training. The number of speakers and
thus variants of i-vectors is very limited and leads to NN overﬁtting. The
online scenario is recommended for training, in the Kaldi Switchboard exam-
ple recipe the number of speakers is also boosted by pseudo-speakers. Two
training utterances represent one pseudo-speaker. The oﬄine scenario may
be used in the test phase.

4

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

3 Recurrent Neural Network Architectures

3.1 Long Short-Term Memory

Long short-term memory (LSTM) is a widely used type of recurrent neural net-
work (RNN). Standard RNNs suﬀer from both exploding and vanishing gradient
problems. Both of these problems are caused by the fact, that information ﬂowing
through the RNN passes through many stages of multiplication. The gradient is
essentially equal to the weight matrix raised to a high power. This results in the
gradient growing or shrinking at an exponential rate to the number of timesteps.
The exploding gradient problem can be solved simply by truncating the gra-
dient. On the other hand, the vanishing gradient problem is harder to overcome.
It does not simply cause the gradient to be small; the gradient components
corresponding to long-term dependencies are small while the components corre-
sponding to short-term dependencies are large. Resulting RNN can then learn
short-term dependencies but not long-term dependencies.

The LSTM was proposed in 1997 by Hochreiter and Scmidhuber [5] as a
solution to the vanishing gradient problem. Let ct denote a hidden state of a
LSTM. The main idea is that instead of computing ct directly from ct−1 with
matrix-vector product followed by an activation function, the LSTM computes
∆ct and adds it to ct−1 to get ct. The addition operation is what eliminates the
vanishing gradient problem.

Each LSTM cell is composed of smaller units called gates, which control the
ﬂow of information through the cell. The forget gate ft controls what information
will be discarded from the cell state, input gate it controls what new information
will be stored in the cell state and output gate ot controls what information from
the cell state will be used in the output.

The LSTM has two hidden states, ct and ht. The state ct ﬁghts the gradient
vanishing problem while ht allows the network to make complex decisions over
short periods of time. There are several slightly diﬀerent LSTM variants. The
architecture used in this paper is speciﬁed by the following equations:

it = σ(Wxixt + Whiht−1 + bi)
ft = σ(Wxf xt + Whf ht−1 + bf )
ot = σ(Wxoxt + Whoht−1 + bo)
ct = ft ∗ ct−1 + it ∗ tanh(Wxcxt + Whcht−1 + bc)
ht = ot ∗ tanh(ct)

The ﬁgure 1 shows the internal structure of LSTM.

3.2 Gated Recurrent Unit

A gated recurrent unit (GRU) was proposed in 2014 by Cho et al.[3] Similarly to
the LSTM unit, the GRU has gating units that modulate the ﬂow of information
inside the unit, however, without having a separate memory cells.

A Comparison of Adaptation Techniques and RNN Architectures

5

ht

ct

ht

ct−1

×

+

tanh

it

×

ot

×

σ

σ

tanh

σ

ht−1

ft

xt

Fig. 1. Structure of a LSTM unit

The update gate zt decides how much the unit updates its activation and
reset gate rt determines which information will be kept from the old state. GRU
does not have any mechanism to control what information to output, therefore
it exposes the whole state.

The main diﬀerences between LSTM unit and GRU are:

– GRU has 2 gates, LSTM has 3 gates
– GRUs do not have an internal memory diﬀerent from the unit output, LSTMs
have an internal memory ct and the output is controlled by an output gate

– Second nonlinearity is not applied when computing the output of GRUs

The GRU unit used in this work is described by the following equations:

rt = σ(Wrxt + Urht−1 + br)
zt = σ(Wzxt + Uzht−1 + bz)
˜ht = tanh(W xt + U (rt ∗ ht−1) + bh)
ht = (1 − zt) ∗ ht−1 + zt ∗ ˜ht

3.3 Modiﬁed Gated Recurrent Unit with ReLU
Ravanelli introduced a simpliﬁed GRU architecture, called M-reluGRU, in [11].
This simpliﬁed architecture does not have the reset gate and uses ReLU as an
activation function instead of tanh.

The M-reluGRU unit is described by the following equations:

zt = σ(Wz xt + Uzht−1 + bz)
˜ht = ReLU(W xt + U ht−1 + bh)
ht = (1 − zt) ∗ ht−1 + zt ∗ ˜ht

6

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

We have also used this unit with the reset gate to evaluate the impact of
the missing reset gate on the network performance. This unit is eﬀectivelly a
normal GRU with ReLU as an activation function and we called it reluGRU in
this paper.

The ﬁgure 2 shows the internal structure and diﬀerence of GRU and M-

reluGRU units.

ht

ht

ht−1

×

+

ht

ht−1

+

ht

1−

×

rt

zt

σ

σ

×

˜ht
tanh

×

1−

zt

σ

×

˜ht
ReLU

xt

xt

(a) GRU

(b) M-reluGRU

Fig. 2. Structure of GRU and M-reluGRU units

4 Experiments

We have chosen TIMIT, a small phone recognition task, as a benchmark of the
NN architectures and adaptation techniques. The TIMIT corpus is well known
and available. The small size allows a rapid testing and simulates a low-resource
scenario that is still an issue for many minor languages. The phone recognition is
much more sensitive to quality of AM than large vocabulary task with a complex
language model.

The TIMIT corpus contains recordings of phonetically-balanced prompted
English speech. It was recorded using a Sennheiser close-talking microphone
at 16 kHz rate with 16 bit sample resolution. TIMIT contains a total of 6300
sentences (5.4 hours), consisting of 10 sentences spoken by each of 630 speakers
from 8 major dialect regions of the United States. All sentences were manually
segmented at the phone level.

The prompts for the 6300 utterances consist of 2 dialect sentences (SA), 450
phonetically compact sentences (SX) and 1890 phonetically-diverse sentences
(SI).

The training set contains 3696 utterances from 462 speakers. The core test
set consists of 192 utterances, 8 from each of 24 speakers (2 males and 1 female
from each dialect region). The training and test sets do not overlap.

A Comparison of Adaptation Techniques and RNN Architectures

7

4.1 Speech Data, Processing, and Test Description

As mentioned above, we used TIMIT data available from LDC as a corpus
LDC93S1. Then, we ran the Kaldi TIMIT example script s5, which trained
various NN-based phone recognition systems with a common HMM-GMM tied-
triphone model and alignments. The common baseline system consisted of the
following methods: It started from MFCC features which were augmented by ∆
and ∆∆ coeﬃcients and then processed by LDA. Final feature vector dimension
was 40. We obtained ﬁnal alignments by HMM-GMM tied-triphone model with
1909 tied-states (may vary slightly if rerun the script). We trained the model
with MLLT and SAT methods, and we used fMLLR for the SAT training and
a test phase adaptation. We dumped all training, development and test fMLLR
processed data, and alignments to disk. Therefore, it was easy to do compatible
experiments from the same common starting point. We also dumped MFCC
processed by LDA with no normalization and CMN calculated both per speaker
and per utterance.

We employed a bigram language/phone model for the ﬁnal phone recognition.
A bigram model is a very weak model for phone recognition; however, it forced
focus to the acoustic part of the system, and it boosted benchmark sensitivity.
The training, as well as the recognition, was done for 48 phones. We mapped
the ﬁnal results on TIMIT core test set to 39 phones (as is usual for TIMIT
corpus processing), and phone error rate (PER) was evaluated by the provided
NIST script to be compatible with previously published works. In contrast to the
Kaldi recipe, we used a diﬀerent phone decoder. It is a standard Viterbi-based
triphone decoder. It gives better results than the Kaldi standard WFST decoder
on the TIMIT phone recognition task.

We have used an open-source Chainer 3.2 DNNs Python tranining tool that

supports NVidia GPUs [1]. It is multiplatform and easy to use.

4.2 DNN Training

First, as a reference to RNNs, we trained feed-forward (FF) DNN with ReLU
activation function without any pre-training. We used dropout p = 0.2. We
stacked 11 input feature frames to 440 NN input dimension, like in Kaldi example
s5. We have used a network with 8 hidden layers and 2048 ReLU neurons, because
it gave the best performance according to our preliminary experiments. The
ﬁnal softmax layer had 1909 neurons. We used SGD with momentum 0.9. The
networks were trained in 3 stages with learning rate 1e–2, 4e–3 and 1e–4. The
batch size was gradually increased from initial 256 to 1024, and ﬁnally to 2048.
The training in each stage was stopped when the development data criterion
increased in comparison to the last epoch.

Then we have trained LSTM, GRU, reluGRU and M-reluGRU networks. For
all of these recurrent networks, we have used identical training setup. We used
4 layers with 1024 units in each. The dropout used was p = 0.2. We have used
output time delay equal to 5 time steps. RNNs were trained in 4 stages. The ﬁrst
stage used Adam optimization algorithm with batch size 512. The other stages

8

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

used SGD with momentum 0.9, batch size 128 and learning rate equal to 1e–3,
1e–4, and 1e–5 respectivelly. The training in each stage was stopped when the
development data criterion increased in comparison to the last epoch, as in FF
network case.

We have trained each network on several input data and i-vector combina-
tions. We used fMLLR data described in the previous section, MFCC and MFCC
with CMN. The normalization was calculated either per speaker or per utter-
ance. For training and testing, we used no i-vectors, online i-vectors and oﬄine
i-vectors calculated also either per speaker or per utterance. We also evaluated
online i-vectors for training and oﬄine i-vectors for testing. The i-vectors were
computed according to Kaldi Switchboard example script. However, because of
small TIMIT size, we did not use any reduction of data. Entire training dataset
was used to estimate i-vector extractor in all steps. The i-vector extractor has
been trained only once and online, per-speaker, and per-utterance i-vectors sets
were extracted by the same extractor transforms.

Because of stochastic nature of results due to random initialization and
stochastic gradient descent, we have performed each experiment 10 times in
total. Then, we have calculated the average phone error rate (PER) and its
standard deviation.

4.3 Results

We have evaluated average PER, its standard deviation for all combinations
of three features variants, six i-vector variants, and ﬁve NNs architectures. We
had to split the results into two tables because of the page size. Table 1 shows
the average PER for each experiment for FF, LSTM, and GRU NNs archi-
tectures. Table 2 compares three variants of GRU-based NNs: GRU, reluGRU,
M-reluGRU. A subset of the most valuable results is also depicted in Figure 3. It
is clear that fMLLR adaptation technique worked quite well. All the NN archi-
tectures gave the best result with fMLLR. The i-vector adaptation had a stable
gain only for FF NN. Two variants of the i-vector adaptation were the best:
online i-vectors for training and online or oﬄine per-speaker for testing. Results
of RNNs with the i-vector adaptation were interesting, beacause there was no
signiﬁcant gain. The results with adaptation were rather worse. Between RNN
architectures, LSTM was the winner (PER 15.43 % with fMLLR). The GRU and
reluGRU gave comparable PERs, 15.7 % with fMLLR, that was slightly worse
than LSTM. M-reluGRU did not performed well and the results were often worse
than FF.

5 Conclusion

In this paper, we have compared feed-forward and several recurrent network
architectures on input data with fMLLR or i-vector adaptation techniques. The
used recurrent networks were based on LSTM and GRU units. We have also
evaluated two GRU modiﬁcations: reluGRU, with ReLU activation function,

A Comparison of Adaptation Techniques and RNN Architectures

9

Table 1: Phone Error Rate [%] for FF, LSTM and GRU Networks

Data

Training

Testing

FF

i-vectors

Phone Error Rate [%]
LSTM

GRU

fMLLR

MFCC

MFCC
with
CMN
per
speaker

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
MFCC
Oﬀ. spk. Oﬀ. spk.
with
CMN
Oﬀ. utt.
Oﬀ. utt.
per
Oﬀ. spk.
Online
Oﬀ. utt.
utterance Online
Online
Online

17.00 ± 0.13
17.17 ± 0.16
17.32 ± 0.15
17.17 ± 0.16
17.10 ± 0.21
17.18 ± 0.14

19.42 ± 0.18
19.02 ± 0.15
19.29 ± 0.19
18.22 ± 0.19
18.48 ± 0.16
18.19 ± 0.19

18.49 ± 0.19
18.47 ± 0.20
18.59 ± 0.10
18.11 ± 0.24
18.17 ± 0.22
18.21 ± 0.19

19.44 ± 0.27
19.10 ± 0.17
19.32 ± 0.14
18.70 ± 0.18
18.63 ± 0.16
18.73 ± 0.18

15.43 ± 0.28
16.08 ± 0.19
16.34 ± 0.32
16.14 ± 0.22
16.27 ± 0.34
16.23 ± 0.26

16.98 ± 0.27
17.50 ± 0.19
18.12 ± 0.27
17.19 ± 0.26
17.27 ± 0.26
17.21 ± 0.15

16.53 ± 0.20
17.20 ± 0.23
17.45 ± 0.19
16.90 ± 0.24
17.34 ± 0.31
17.25 ± 0.26

16.98 ± 0.20
17.60 ± 0.31
18.28 ± 0.35
17.53 ± 0.23
17.60 ± 0.23
17.66 ± 0.23

15.69 ± 0.19
16.04 ± 0.29
16.43 ± 0.25
16.15 ± 0.28
16.14 ± 0.24
16.23 ± 0.19

17.48 ± 0.19
17.63 ± 0.22
18.09 ± 0.29
17.00 ± 0.28
17.21 ± 0.20
17.33 ± 0.37

17.00 ± 0.25
17.33 ± 0.21
17.36 ± 0.21
17.04 ± 0.16
17.06 ± 0.20
17.24 ± 0.31

17.54 ± 0.20
17.64 ± 0.33
18.15 ± 0.35
17.33 ± 0.18
17.46 ± 0.19
17.43 ± 0.19

and M-reluGRU, with ReLU activation function and without the reset gate. As
features, we have used MFCC processed by LDA without normalization or with
CMN calculated either per speaker or per utterance, and also fMLLR adaptation.
We have also augmented the features with several variants of i-vectors: online
or oﬄine calculated either per speaker or per utterance. Due to the stochastic
nature of the used optimizers, we have performed all experiments 10 times in
total and calculated the average phone error rate and its standard deviation.

For all networks, we have obtained the best results with fMLLR adaptation.
The i-vector adaptation consistently improved the results only for FF networks.
In the case of RNN, i-vectors did not lead to any signiﬁcant improvement; it even
gave worse results in all LSTM experiments and in some experiments with GRU
variants. We have achieved the best results with LSTM network (PER 15.43 %
with fMLLR). GRU and reluGRU were slightly worse (both having PER 15.7 %
with fMLLR). M-reluGRU was in some cases even worse than FF network.

For all our experiments, we have used Chainer 3.2 DNN training framework
with Python programming language and we have published our open-source

10

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

Table 2: Phone Error Rate [%] for GRU and Its Modiﬁcations

Data

Training

Testing

GRU

i-vectors

Phone Error Rate [%]
reluGRU

fMLLR

MFCC

MFCC
with
CMN
per
speaker

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
MFCC
Oﬀ. spk. Oﬀ. spk.
with
CMN
Oﬀ. utt.
Oﬀ. utt.
per
Oﬀ. spk.
Online
Oﬀ. utt.
utterance Online
Online
Online

15.69 ± 0.19
16.04 ± 0.29
16.43 ± 0.25
16.15 ± 0.28
16.14 ± 0.24
16.23 ± 0.19

17.48 ± 0.19
17.63 ± 0.22
18.09 ± 0.29
17.00 ± 0.28
17.21 ± 0.20
17.33 ± 0.37

17.00 ± 0.25
17.33 ± 0.21
17.36 ± 0.21
17.04 ± 0.16
17.06 ± 0.20
17.24 ± 0.31

17.54 ± 0.20
17.64 ± 0.33
18.15 ± 0.35
17.33 ± 0.18
17.46 ± 0.19
17.43 ± 0.19

15.70 ± 0.56
16.28 ± 0.38
16.33 ± 0.13
16.19 ± 0.22
16.23 ± 0.18
16.39 ± 0.33

17.30 ± 0.50
18.32 ± 0.39
18.35 ± 0.37
17.30 ± 0.38
17.52 ± 0.47
17.41 ± 0.44

16.91 ± 0.22
17.70 ± 0.39
17.91 ± 0.35
17.39 ± 0.27
17.48 ± 0.29
17.45 ± 0.27

17.50 ± 0.29
18.05 ± 0.27
18.52 ± 0.33
17.79 ± 0.31
18.05 ± 0.24
17.85 ± 0.18

M-reluGRU

17.06 ± 0.77
17.50 ± 0.72
18.25 ± 0.85
17.76 ± 0.94
17.85 ± 0.76
17.60 ± 0.67

19.64 ± 1.05
20.13 ± 0.93
20.70 ± 0.65
19.38 ± 0.96
19.44 ± 0.89
19.29 ± 0.89

18.23 ± 0.53
19.44 ± 0.66
19.43 ± 1.17
19.03 ± 1.07
18.93 ± 0.74
18.89 ± 0.78

19.26 ± 0.85
19.08 ± 0.77
21.04 ± 0.97
20.10 ± 0.95
19.63 ± 0.99
20.01 ± 0.69

scripts at https://github.com/OrcusCZ/NNAcousticModeling to easily repli-
cate the results and to help continue the development.

Acknowledgement

This work was supported by the project no. P103/12/G084 of the Grant Agency
of the Czech Republic and by the grant of the University of West Bohemia,
project No. SGS-2016-039. Access to computing and storage facilities owned by
parties and projects contributing to the National Grid Infrastructure MetaCen-
trum provided under the programme ”Projects of Large Research, Development,
and Innovations Infrastructures” (CESNET LM2015042), is greatly appreciated.

A Comparison of Adaptation Techniques and RNN Architectures

11

]

%

[

R
E
P

20

19

18

17

16

15

fMLLR

MFCC

MFCC
+
i-vectors

MFCC
CMN
Spk.

MFCC
CMN
Spk. +
i-vectors

MFCC
CMN
Utt.

MFCC
CMN
Utt. +
i-vectors

FF

LSTM

GRU

reluGRU

M-reluGRU

Fig. 3. Phone Error Rate [%] on Features with Best Performing i-vector Variants

References

1. A ﬂexible framework of neural networks for deep learning. https://chainer.org
2. Cho, K., Van Merri¨enboer, B., Bahdanau, D., Bengio, Y.: On the proper-
ties of neural machine translation: Encoder-decoder approaches. arXiv preprint
arXiv:1409.1259 (2014)

3. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recur-
rent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014)
4. Gales, M.: Maximum likelihood linear transformations for HMM-based speech

recognition. Computer Speech & Language 12(2), 75 – 98 (1998)

5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

9(8), 1735–1780 (1997)

6. Huang, Z., Tang, J., Xue, S., Dai, L.: Speaker Adaptation of RNN-BLSTM
for Speec Recognition based on Speaker Code. Icassp 1, 5305–5309 (2016).
https://doi.org/10.1109/ICASSP.2016.7472690

7. Karaﬁ´at, M., Burget, L., Matˇejka, P., Glembek, O., ˇCernock´y, J.: iVector-based
discriminative adaptation for automatic speech recognition. 2011 IEEE Workshop
on Automatic Speech Recognition and Understanding, ASRU 2011, Proceedings
pp. 152–157 (2011). https://doi.org/10.1109/ASRU.2011.6163922

8. Parthasarathi, S.H.K., Hoﬀmeister, B., Matsoukas, S., Mandal, A., Strom, N.,
Garimella, S.: fmllr based feature-space speaker adaptation of dnn acoustic models.
In: INTERSPEECH. pp. 3630–3634. ISCA (2015)

9. Peddinti, V., Povey, D., Khudanpur, S.: A time delay neural network architecture
for eﬃcient modeling of long temporal contexts. Proceedings of the Annual Con-
ference of the International Speech Communication Association, INTERSPEECH
2015-Janua, 3214–3218 (2015)

12

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

10. Rath, S.P., Povey, D., Vesel´y, K., ˇCernock´y, J.: Improved feature processing for

deep neural networks. In: INTERSPEECH. pp. 109–113. ISCA (2013)

11. Ravanelli, M., Brakel, P., Omologo, M., Bengio, Y., Kessler, F.B.: Improving speech
recognition by revising gated recurrent units. In: Interspeech2017. pp. 1308–1312
(2017). https://doi.org/10.21437/Interspeech.2017-775

12. Sak, H., Senior, A., Beaufays, F.: Long Short-Term Memory Based
for Large Vocabulary Speech
Interspeech 1, 338–342 (2014). https://doi.org/arXiv:1402.1128,

Recurrent Neural Network Architectures
Recognition.
http://arxiv.org/abs/1402.1128

13. Saon,
for
http://mazsola.iit.uni-miskolc.hu/{~}czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS141054.PDF

Unfolded
INTERSPEECH

H.:
Recognition.

Recurrent
1,

Networks
(2014),

Neural
343–347

G.,
Speech

Soltau,

14. Saon, G., Soltau, H., Nahamoo, D., Picheny, M.: Speaker adaptation of neural
network acoustic models using i-vectors. In: 2013 IEEE Workshop on Automatic
Speech Recognition and Understanding. pp. 55–59 (2013)

15. Seide, F., Chen, X., Yu, D.: Feature engineering in context-dependent deep neural

networks for conversational speech transcription. In: in ASRU (2011)

16. Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., Lang, K.J.: Phoneme
on
time-delay
neural
and Signal Processing 37(3),
1989).

IEEE Transactions

recognition
using
Acoustics, Speech,
https://doi.org/10.1109/29.21701

networks.

328–339

(Mar

17. Xue, S., Abdel-Hamid, O., Jiang, H., Dai, L., Liu, Q.: Fast adaptation
of deep neural network based on discriminant codes for speech recognition.
IEEE/ACM Transactions on Speech and Language Processing 22(12) (2014).
https://doi.org/10.1109/TASLP.2014.2346313

A Comparison of Adaptation Techniques and
Recurrent Neural Network Architectures

Jan Vanˇek[0000−0002−2639−6731], Josef Mich´alek[0000−0001−7757−3163],
Jan Zelinka[0000−0001−6834−9178], and Josef Psutka[0000−0002−0764−3207]

University of West Bohemia
Univerzitn´ı 8, 301 00 Pilsen, Czech Republic
{vanekyj,orcus,zelinka,psutka}@kky.zcu.cz

Abstract. Recently, recurrent neural networks have become state-of-
the-art in acoustic modeling for automatic speech recognition. The long
short-term memory (LSTM) units are the most popular ones. However,
alternative units like gated recurrent unit (GRU) and its modiﬁcations
outperformed LSTM in some publications. In this paper, we compared
ﬁve neural network (NN) architectures with various adaptation and fea-
ture normalization techniques. We have evaluated feature-space maxi-
mum likelihood linear regression, ﬁve variants of i-vector adaptation and
two variants of cepstral mean normalization. The most adaptation and
normalization techniques were developed for feed-forward NNs and, ac-
cording to results in this paper, not all of them worked also with RNNs.
For experiments, we have chosen a well known and available TIMIT
phone recognition task. The phone recognition is much more sensitive to
the quality of AM than large vocabulary task with a complex language
model. Also, we published the open-source scripts to easily replicate the
results and to help continue the development.

Keywords: neural networks · acoustic model · TIMIT · LSTM · GRU
· phone recognition · adaptation · i-vectors

1

Introduction

Neural Networks (NNs) and deep NNs (DNNs) became dominant in the ﬁeld of
the acoustic modeling several years ago. Simple feed-forward (FF) DNNs were
faded away in recent years. The current progress is based on the modeling of a
longer temporal context of individual feature frames. Main two ways are actu-
ally popular: First, a larger context is modeled by a time-delayed NN (TDNN)
[16], [9]. TDNNs model long term temporal dependencies with training times
comparable to standard feed-forward DNNs. In the TDNN architecture, the ini-
tial transforms learn narrow contexts and the deeper layers process the hidden
activations from a wider temporal context. Hence the higher layers have the
ability to learn wider temporal relationships. The second way to learn the longer
temporal context is to use recurrent NNs (RNNs). The most popular RNN archi-
tecture is a long short-term memory (LSTM) that has been designed to address

8
1
0
2
 
l
u
J
 
2
1
 
 
]
S
A
.
s
s
e
e
[
 
 
1
v
1
4
4
6
0
.
7
0
8
1
:
v
i
X
r
a

2

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

the vanishing and exploding gradient problems of conventional RNNs. Unlike
feed-forward neural networks, RNNs have cyclic connections making them pow-
erful for modeling sequences [12]. The main drawback is much slower training
due to the sequential nature of the learning algorithm. An unfolding of the re-
current network during training was proposed in [13] to speed-up the training,
however it is still signiﬁcantly slower than FF NNs or TDNNs. More recently,
another type of recurrent unit, a gated recurrent unit (GRU), was proposed in
[2], [3]. Similarly to the LSTM unit, the GRU has gating units that modulate the
ﬂow of information inside the unit, however, without having a separate mem-
ory cells. Further revising GRUs leaded to a simpliﬁed architecture potentially
more suitable for speech recognition in [11]. First, removing the reset gate in the
GRU design resulted in a simpler single-gate architecture called modiﬁed GRU
(M-GRU). Second, replacing tanh with ReLU activations in the state update
equations was proposed and called M-reluGRU. A more detailed overview of the
RNN architectures follows in Section 3.

Even if large datasets are used for the DNN training, an adaptation of an
acoustic model (AM) to a test speaker and environment is beneﬁcial. A lot of
techniques have been reported on the adaptation, such as the classical maxi-
mum a posterior (MAP) and maximum likelihood linear regression (MLLR) for
traditional GMM-HMM acoustic models. Although this technique can be mod-
iﬁed for an NN-based acoustic model, a much simpler application has so-called
[4] because fMLLR changes only features and
feature space MLLR (fMLLR)
it does not adapt NN parameters. This speaker adaptation technique can be
easily applied in an NN-based acoustic model [15], [8], [10]. Therefore, fMLLR
can be used to any DNN architecture. I-vectors originally developed for speaker
recognition can be used to the speaker and environment adaptation also [14],
[7]. Alternative approach is using of discriminative speaker codes [17], [6]. More
detailed description of the adaptation techniques used in this paper follows.

2 Adaptation of DNNs

The simplest way of the adaptation is a feature level adaptation. When adapting
input features, NN can have any structure. The most popular and well known
technique is fMLLR based on an underlying HMM-GMM that is used during
initial stage of the NN training.

2.1 fMLLR

The fMLLR transforms feature frames with a speaker-speciﬁc square matrix A
and a bias b. For HMM-GMMs, A and b are estimated to maximize the like-
lihood of the adaptation data given the model [4], [15]. In the training phase,
the speaker-speciﬁc transform may be updated several-times alternating HMM-
GMM update. These approach is usually called a speaker adaptive training
(SAT). The result of the training phase is a canonical model that requires us-
ing adaptation during testing phase. However, two-pass processing is required

A Comparison of Adaptation Techniques and RNN Architectures

3

during test phase. The ﬁrst pass produces unsupervised alignment that is used
to estimate the transform parameters via maximum likelihood. The model used
for alignment does not need to be the identical model to the ﬁnal canonical one.
Because all the steps are using the underline HMM-GMM any NN architecture
may be used to train the ﬁnal NN acoustic model.

2.2 i-vectors

The i-vector extraction is a well known technique, so we focused here to more
practical points. An detailed description of i-vectors can be found in [14], [7] and
further papers referenced in there.

The i-vector extraction is comprised from following steps:

1. An universal background model (UBM) needs to be trained. Usually a GMM
with 512 to 2048 diagonal components is used. The quality of GMM is not
critical, so some speed-up methods can be utilized. Features for UBM do
not need to match witch features for NN nor i-vector accumulators. Usu-
ally, features with cepstral mean normalization (CMN) or cepstral mean
and variance normalization (CMVN) are used for UBM. The normalization
techniques reduce speaker and environment variability. Features without any
normalization are used for the i-vector accumulators to carry more speaker-
and environment-related information.

2. Zero-order and centered ﬁrst-order statistics are accumulated for every speaker

3. The i-vector extraction transforms are estimated iteratively by expecta-

according to the UBM posteriors.

tion/maximization (EM) algorithm.

4. The i-vector for individual speakers is evaluated. For training speakers, zero-
order and centered ﬁrst-order statistics have been already accumulated. For
other speakers, statistics must be accumulated. Then, the i-vector is evalu-
ated by the i-vector extraction transforms computed in the third step.

The four step process seems simple but there are some details that need to be
mentioned:

– CMN or CMVN may be computed online or oﬄine. The oﬄine variant may
be per-utterance or per-speaker. The online variant starts from global cep-
stral mean and it is subsequently updated. An exponential forgetting is us-
able for very long utterances. The training setup should match with the
testing one.

– The accumulated statistic should be saturated or scaled-down for long ut-

terances due to an i-vector overﬁtting.

– The oﬄine scenario is not proper for training. The number of speakers and
thus variants of i-vectors is very limited and leads to NN overﬁtting. The
online scenario is recommended for training, in the Kaldi Switchboard exam-
ple recipe the number of speakers is also boosted by pseudo-speakers. Two
training utterances represent one pseudo-speaker. The oﬄine scenario may
be used in the test phase.

4

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

3 Recurrent Neural Network Architectures

3.1 Long Short-Term Memory

Long short-term memory (LSTM) is a widely used type of recurrent neural net-
work (RNN). Standard RNNs suﬀer from both exploding and vanishing gradient
problems. Both of these problems are caused by the fact, that information ﬂowing
through the RNN passes through many stages of multiplication. The gradient is
essentially equal to the weight matrix raised to a high power. This results in the
gradient growing or shrinking at an exponential rate to the number of timesteps.
The exploding gradient problem can be solved simply by truncating the gra-
dient. On the other hand, the vanishing gradient problem is harder to overcome.
It does not simply cause the gradient to be small; the gradient components
corresponding to long-term dependencies are small while the components corre-
sponding to short-term dependencies are large. Resulting RNN can then learn
short-term dependencies but not long-term dependencies.

The LSTM was proposed in 1997 by Hochreiter and Scmidhuber [5] as a
solution to the vanishing gradient problem. Let ct denote a hidden state of a
LSTM. The main idea is that instead of computing ct directly from ct−1 with
matrix-vector product followed by an activation function, the LSTM computes
∆ct and adds it to ct−1 to get ct. The addition operation is what eliminates the
vanishing gradient problem.

Each LSTM cell is composed of smaller units called gates, which control the
ﬂow of information through the cell. The forget gate ft controls what information
will be discarded from the cell state, input gate it controls what new information
will be stored in the cell state and output gate ot controls what information from
the cell state will be used in the output.

The LSTM has two hidden states, ct and ht. The state ct ﬁghts the gradient
vanishing problem while ht allows the network to make complex decisions over
short periods of time. There are several slightly diﬀerent LSTM variants. The
architecture used in this paper is speciﬁed by the following equations:

it = σ(Wxixt + Whiht−1 + bi)
ft = σ(Wxf xt + Whf ht−1 + bf )
ot = σ(Wxoxt + Whoht−1 + bo)
ct = ft ∗ ct−1 + it ∗ tanh(Wxcxt + Whcht−1 + bc)
ht = ot ∗ tanh(ct)

The ﬁgure 1 shows the internal structure of LSTM.

3.2 Gated Recurrent Unit

A gated recurrent unit (GRU) was proposed in 2014 by Cho et al.[3] Similarly to
the LSTM unit, the GRU has gating units that modulate the ﬂow of information
inside the unit, however, without having a separate memory cells.

A Comparison of Adaptation Techniques and RNN Architectures

5

ht

ct

ht

ct−1

×

+

tanh

it

×

ot

×

σ

σ

tanh

σ

ht−1

ft

xt

Fig. 1. Structure of a LSTM unit

The update gate zt decides how much the unit updates its activation and
reset gate rt determines which information will be kept from the old state. GRU
does not have any mechanism to control what information to output, therefore
it exposes the whole state.

The main diﬀerences between LSTM unit and GRU are:

– GRU has 2 gates, LSTM has 3 gates
– GRUs do not have an internal memory diﬀerent from the unit output, LSTMs
have an internal memory ct and the output is controlled by an output gate

– Second nonlinearity is not applied when computing the output of GRUs

The GRU unit used in this work is described by the following equations:

rt = σ(Wrxt + Urht−1 + br)
zt = σ(Wzxt + Uzht−1 + bz)
˜ht = tanh(W xt + U (rt ∗ ht−1) + bh)
ht = (1 − zt) ∗ ht−1 + zt ∗ ˜ht

3.3 Modiﬁed Gated Recurrent Unit with ReLU
Ravanelli introduced a simpliﬁed GRU architecture, called M-reluGRU, in [11].
This simpliﬁed architecture does not have the reset gate and uses ReLU as an
activation function instead of tanh.

The M-reluGRU unit is described by the following equations:

zt = σ(Wz xt + Uzht−1 + bz)
˜ht = ReLU(W xt + U ht−1 + bh)
ht = (1 − zt) ∗ ht−1 + zt ∗ ˜ht

6

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

We have also used this unit with the reset gate to evaluate the impact of
the missing reset gate on the network performance. This unit is eﬀectivelly a
normal GRU with ReLU as an activation function and we called it reluGRU in
this paper.

The ﬁgure 2 shows the internal structure and diﬀerence of GRU and M-

reluGRU units.

ht

ht

ht−1

×

+

ht

ht−1

+

ht

1−

×

rt

zt

σ

σ

×

˜ht
tanh

×

1−

zt

σ

×

˜ht
ReLU

xt

xt

(a) GRU

(b) M-reluGRU

Fig. 2. Structure of GRU and M-reluGRU units

4 Experiments

We have chosen TIMIT, a small phone recognition task, as a benchmark of the
NN architectures and adaptation techniques. The TIMIT corpus is well known
and available. The small size allows a rapid testing and simulates a low-resource
scenario that is still an issue for many minor languages. The phone recognition is
much more sensitive to quality of AM than large vocabulary task with a complex
language model.

The TIMIT corpus contains recordings of phonetically-balanced prompted
English speech. It was recorded using a Sennheiser close-talking microphone
at 16 kHz rate with 16 bit sample resolution. TIMIT contains a total of 6300
sentences (5.4 hours), consisting of 10 sentences spoken by each of 630 speakers
from 8 major dialect regions of the United States. All sentences were manually
segmented at the phone level.

The prompts for the 6300 utterances consist of 2 dialect sentences (SA), 450
phonetically compact sentences (SX) and 1890 phonetically-diverse sentences
(SI).

The training set contains 3696 utterances from 462 speakers. The core test
set consists of 192 utterances, 8 from each of 24 speakers (2 males and 1 female
from each dialect region). The training and test sets do not overlap.

A Comparison of Adaptation Techniques and RNN Architectures

7

4.1 Speech Data, Processing, and Test Description

As mentioned above, we used TIMIT data available from LDC as a corpus
LDC93S1. Then, we ran the Kaldi TIMIT example script s5, which trained
various NN-based phone recognition systems with a common HMM-GMM tied-
triphone model and alignments. The common baseline system consisted of the
following methods: It started from MFCC features which were augmented by ∆
and ∆∆ coeﬃcients and then processed by LDA. Final feature vector dimension
was 40. We obtained ﬁnal alignments by HMM-GMM tied-triphone model with
1909 tied-states (may vary slightly if rerun the script). We trained the model
with MLLT and SAT methods, and we used fMLLR for the SAT training and
a test phase adaptation. We dumped all training, development and test fMLLR
processed data, and alignments to disk. Therefore, it was easy to do compatible
experiments from the same common starting point. We also dumped MFCC
processed by LDA with no normalization and CMN calculated both per speaker
and per utterance.

We employed a bigram language/phone model for the ﬁnal phone recognition.
A bigram model is a very weak model for phone recognition; however, it forced
focus to the acoustic part of the system, and it boosted benchmark sensitivity.
The training, as well as the recognition, was done for 48 phones. We mapped
the ﬁnal results on TIMIT core test set to 39 phones (as is usual for TIMIT
corpus processing), and phone error rate (PER) was evaluated by the provided
NIST script to be compatible with previously published works. In contrast to the
Kaldi recipe, we used a diﬀerent phone decoder. It is a standard Viterbi-based
triphone decoder. It gives better results than the Kaldi standard WFST decoder
on the TIMIT phone recognition task.

We have used an open-source Chainer 3.2 DNNs Python tranining tool that

supports NVidia GPUs [1]. It is multiplatform and easy to use.

4.2 DNN Training

First, as a reference to RNNs, we trained feed-forward (FF) DNN with ReLU
activation function without any pre-training. We used dropout p = 0.2. We
stacked 11 input feature frames to 440 NN input dimension, like in Kaldi example
s5. We have used a network with 8 hidden layers and 2048 ReLU neurons, because
it gave the best performance according to our preliminary experiments. The
ﬁnal softmax layer had 1909 neurons. We used SGD with momentum 0.9. The
networks were trained in 3 stages with learning rate 1e–2, 4e–3 and 1e–4. The
batch size was gradually increased from initial 256 to 1024, and ﬁnally to 2048.
The training in each stage was stopped when the development data criterion
increased in comparison to the last epoch.

Then we have trained LSTM, GRU, reluGRU and M-reluGRU networks. For
all of these recurrent networks, we have used identical training setup. We used
4 layers with 1024 units in each. The dropout used was p = 0.2. We have used
output time delay equal to 5 time steps. RNNs were trained in 4 stages. The ﬁrst
stage used Adam optimization algorithm with batch size 512. The other stages

8

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

used SGD with momentum 0.9, batch size 128 and learning rate equal to 1e–3,
1e–4, and 1e–5 respectivelly. The training in each stage was stopped when the
development data criterion increased in comparison to the last epoch, as in FF
network case.

We have trained each network on several input data and i-vector combina-
tions. We used fMLLR data described in the previous section, MFCC and MFCC
with CMN. The normalization was calculated either per speaker or per utter-
ance. For training and testing, we used no i-vectors, online i-vectors and oﬄine
i-vectors calculated also either per speaker or per utterance. We also evaluated
online i-vectors for training and oﬄine i-vectors for testing. The i-vectors were
computed according to Kaldi Switchboard example script. However, because of
small TIMIT size, we did not use any reduction of data. Entire training dataset
was used to estimate i-vector extractor in all steps. The i-vector extractor has
been trained only once and online, per-speaker, and per-utterance i-vectors sets
were extracted by the same extractor transforms.

Because of stochastic nature of results due to random initialization and
stochastic gradient descent, we have performed each experiment 10 times in
total. Then, we have calculated the average phone error rate (PER) and its
standard deviation.

4.3 Results

We have evaluated average PER, its standard deviation for all combinations
of three features variants, six i-vector variants, and ﬁve NNs architectures. We
had to split the results into two tables because of the page size. Table 1 shows
the average PER for each experiment for FF, LSTM, and GRU NNs archi-
tectures. Table 2 compares three variants of GRU-based NNs: GRU, reluGRU,
M-reluGRU. A subset of the most valuable results is also depicted in Figure 3. It
is clear that fMLLR adaptation technique worked quite well. All the NN archi-
tectures gave the best result with fMLLR. The i-vector adaptation had a stable
gain only for FF NN. Two variants of the i-vector adaptation were the best:
online i-vectors for training and online or oﬄine per-speaker for testing. Results
of RNNs with the i-vector adaptation were interesting, beacause there was no
signiﬁcant gain. The results with adaptation were rather worse. Between RNN
architectures, LSTM was the winner (PER 15.43 % with fMLLR). The GRU and
reluGRU gave comparable PERs, 15.7 % with fMLLR, that was slightly worse
than LSTM. M-reluGRU did not performed well and the results were often worse
than FF.

5 Conclusion

In this paper, we have compared feed-forward and several recurrent network
architectures on input data with fMLLR or i-vector adaptation techniques. The
used recurrent networks were based on LSTM and GRU units. We have also
evaluated two GRU modiﬁcations: reluGRU, with ReLU activation function,

A Comparison of Adaptation Techniques and RNN Architectures

9

Table 1: Phone Error Rate [%] for FF, LSTM and GRU Networks

Data

Training

Testing

FF

i-vectors

Phone Error Rate [%]
LSTM

GRU

fMLLR

MFCC

MFCC
with
CMN
per
speaker

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
MFCC
Oﬀ. spk. Oﬀ. spk.
with
CMN
Oﬀ. utt.
Oﬀ. utt.
per
Oﬀ. spk.
Online
Oﬀ. utt.
utterance Online
Online
Online

17.00 ± 0.13
17.17 ± 0.16
17.32 ± 0.15
17.17 ± 0.16
17.10 ± 0.21
17.18 ± 0.14

19.42 ± 0.18
19.02 ± 0.15
19.29 ± 0.19
18.22 ± 0.19
18.48 ± 0.16
18.19 ± 0.19

18.49 ± 0.19
18.47 ± 0.20
18.59 ± 0.10
18.11 ± 0.24
18.17 ± 0.22
18.21 ± 0.19

19.44 ± 0.27
19.10 ± 0.17
19.32 ± 0.14
18.70 ± 0.18
18.63 ± 0.16
18.73 ± 0.18

15.43 ± 0.28
16.08 ± 0.19
16.34 ± 0.32
16.14 ± 0.22
16.27 ± 0.34
16.23 ± 0.26

16.98 ± 0.27
17.50 ± 0.19
18.12 ± 0.27
17.19 ± 0.26
17.27 ± 0.26
17.21 ± 0.15

16.53 ± 0.20
17.20 ± 0.23
17.45 ± 0.19
16.90 ± 0.24
17.34 ± 0.31
17.25 ± 0.26

16.98 ± 0.20
17.60 ± 0.31
18.28 ± 0.35
17.53 ± 0.23
17.60 ± 0.23
17.66 ± 0.23

15.69 ± 0.19
16.04 ± 0.29
16.43 ± 0.25
16.15 ± 0.28
16.14 ± 0.24
16.23 ± 0.19

17.48 ± 0.19
17.63 ± 0.22
18.09 ± 0.29
17.00 ± 0.28
17.21 ± 0.20
17.33 ± 0.37

17.00 ± 0.25
17.33 ± 0.21
17.36 ± 0.21
17.04 ± 0.16
17.06 ± 0.20
17.24 ± 0.31

17.54 ± 0.20
17.64 ± 0.33
18.15 ± 0.35
17.33 ± 0.18
17.46 ± 0.19
17.43 ± 0.19

and M-reluGRU, with ReLU activation function and without the reset gate. As
features, we have used MFCC processed by LDA without normalization or with
CMN calculated either per speaker or per utterance, and also fMLLR adaptation.
We have also augmented the features with several variants of i-vectors: online
or oﬄine calculated either per speaker or per utterance. Due to the stochastic
nature of the used optimizers, we have performed all experiments 10 times in
total and calculated the average phone error rate and its standard deviation.

For all networks, we have obtained the best results with fMLLR adaptation.
The i-vector adaptation consistently improved the results only for FF networks.
In the case of RNN, i-vectors did not lead to any signiﬁcant improvement; it even
gave worse results in all LSTM experiments and in some experiments with GRU
variants. We have achieved the best results with LSTM network (PER 15.43 %
with fMLLR). GRU and reluGRU were slightly worse (both having PER 15.7 %
with fMLLR). M-reluGRU was in some cases even worse than FF network.

For all our experiments, we have used Chainer 3.2 DNN training framework
with Python programming language and we have published our open-source

10

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

Table 2: Phone Error Rate [%] for GRU and Its Modiﬁcations

Data

Training

Testing

GRU

i-vectors

Phone Error Rate [%]
reluGRU

fMLLR

MFCC

MFCC
with
CMN
per
speaker

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
MFCC
Oﬀ. spk. Oﬀ. spk.
with
CMN
Oﬀ. utt.
Oﬀ. utt.
per
Oﬀ. spk.
Online
Oﬀ. utt.
utterance Online
Online
Online

15.69 ± 0.19
16.04 ± 0.29
16.43 ± 0.25
16.15 ± 0.28
16.14 ± 0.24
16.23 ± 0.19

17.48 ± 0.19
17.63 ± 0.22
18.09 ± 0.29
17.00 ± 0.28
17.21 ± 0.20
17.33 ± 0.37

17.00 ± 0.25
17.33 ± 0.21
17.36 ± 0.21
17.04 ± 0.16
17.06 ± 0.20
17.24 ± 0.31

17.54 ± 0.20
17.64 ± 0.33
18.15 ± 0.35
17.33 ± 0.18
17.46 ± 0.19
17.43 ± 0.19

15.70 ± 0.56
16.28 ± 0.38
16.33 ± 0.13
16.19 ± 0.22
16.23 ± 0.18
16.39 ± 0.33

17.30 ± 0.50
18.32 ± 0.39
18.35 ± 0.37
17.30 ± 0.38
17.52 ± 0.47
17.41 ± 0.44

16.91 ± 0.22
17.70 ± 0.39
17.91 ± 0.35
17.39 ± 0.27
17.48 ± 0.29
17.45 ± 0.27

17.50 ± 0.29
18.05 ± 0.27
18.52 ± 0.33
17.79 ± 0.31
18.05 ± 0.24
17.85 ± 0.18

M-reluGRU

17.06 ± 0.77
17.50 ± 0.72
18.25 ± 0.85
17.76 ± 0.94
17.85 ± 0.76
17.60 ± 0.67

19.64 ± 1.05
20.13 ± 0.93
20.70 ± 0.65
19.38 ± 0.96
19.44 ± 0.89
19.29 ± 0.89

18.23 ± 0.53
19.44 ± 0.66
19.43 ± 1.17
19.03 ± 1.07
18.93 ± 0.74
18.89 ± 0.78

19.26 ± 0.85
19.08 ± 0.77
21.04 ± 0.97
20.10 ± 0.95
19.63 ± 0.99
20.01 ± 0.69

scripts at https://github.com/OrcusCZ/NNAcousticModeling to easily repli-
cate the results and to help continue the development.

Acknowledgement

This work was supported by the project no. P103/12/G084 of the Grant Agency
of the Czech Republic and by the grant of the University of West Bohemia,
project No. SGS-2016-039. Access to computing and storage facilities owned by
parties and projects contributing to the National Grid Infrastructure MetaCen-
trum provided under the programme ”Projects of Large Research, Development,
and Innovations Infrastructures” (CESNET LM2015042), is greatly appreciated.

A Comparison of Adaptation Techniques and RNN Architectures

11

]

%

[

R
E
P

20

19

18

17

16

15

fMLLR

MFCC

MFCC
+
i-vectors

MFCC
CMN
Spk.

MFCC
CMN
Spk. +
i-vectors

MFCC
CMN
Utt.

MFCC
CMN
Utt. +
i-vectors

FF

LSTM

GRU

reluGRU

M-reluGRU

Fig. 3. Phone Error Rate [%] on Features with Best Performing i-vector Variants

References

1. A ﬂexible framework of neural networks for deep learning. https://chainer.org
2. Cho, K., Van Merri¨enboer, B., Bahdanau, D., Bengio, Y.: On the proper-
ties of neural machine translation: Encoder-decoder approaches. arXiv preprint
arXiv:1409.1259 (2014)

3. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recur-
rent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014)
4. Gales, M.: Maximum likelihood linear transformations for HMM-based speech

recognition. Computer Speech & Language 12(2), 75 – 98 (1998)

5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

9(8), 1735–1780 (1997)

6. Huang, Z., Tang, J., Xue, S., Dai, L.: Speaker Adaptation of RNN-BLSTM
for Speec Recognition based on Speaker Code. Icassp 1, 5305–5309 (2016).
https://doi.org/10.1109/ICASSP.2016.7472690

7. Karaﬁ´at, M., Burget, L., Matˇejka, P., Glembek, O., ˇCernock´y, J.: iVector-based
discriminative adaptation for automatic speech recognition. 2011 IEEE Workshop
on Automatic Speech Recognition and Understanding, ASRU 2011, Proceedings
pp. 152–157 (2011). https://doi.org/10.1109/ASRU.2011.6163922

8. Parthasarathi, S.H.K., Hoﬀmeister, B., Matsoukas, S., Mandal, A., Strom, N.,
Garimella, S.: fmllr based feature-space speaker adaptation of dnn acoustic models.
In: INTERSPEECH. pp. 3630–3634. ISCA (2015)

9. Peddinti, V., Povey, D., Khudanpur, S.: A time delay neural network architecture
for eﬃcient modeling of long temporal contexts. Proceedings of the Annual Con-
ference of the International Speech Communication Association, INTERSPEECH
2015-Janua, 3214–3218 (2015)

12

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

10. Rath, S.P., Povey, D., Vesel´y, K., ˇCernock´y, J.: Improved feature processing for

deep neural networks. In: INTERSPEECH. pp. 109–113. ISCA (2013)

11. Ravanelli, M., Brakel, P., Omologo, M., Bengio, Y., Kessler, F.B.: Improving speech
recognition by revising gated recurrent units. In: Interspeech2017. pp. 1308–1312
(2017). https://doi.org/10.21437/Interspeech.2017-775

12. Sak, H., Senior, A., Beaufays, F.: Long Short-Term Memory Based
for Large Vocabulary Speech
Interspeech 1, 338–342 (2014). https://doi.org/arXiv:1402.1128,

Recurrent Neural Network Architectures
Recognition.
http://arxiv.org/abs/1402.1128

13. Saon,
for
http://mazsola.iit.uni-miskolc.hu/{~}czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS141054.PDF

Unfolded
INTERSPEECH

H.:
Recognition.

Recurrent
1,

Networks
(2014),

Neural
343–347

G.,
Speech

Soltau,

14. Saon, G., Soltau, H., Nahamoo, D., Picheny, M.: Speaker adaptation of neural
network acoustic models using i-vectors. In: 2013 IEEE Workshop on Automatic
Speech Recognition and Understanding. pp. 55–59 (2013)

15. Seide, F., Chen, X., Yu, D.: Feature engineering in context-dependent deep neural

networks for conversational speech transcription. In: in ASRU (2011)

16. Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., Lang, K.J.: Phoneme
on
time-delay
neural
and Signal Processing 37(3),
1989).

IEEE Transactions

recognition
using
Acoustics, Speech,
https://doi.org/10.1109/29.21701

networks.

328–339

(Mar

17. Xue, S., Abdel-Hamid, O., Jiang, H., Dai, L., Liu, Q.: Fast adaptation
of deep neural network based on discriminant codes for speech recognition.
IEEE/ACM Transactions on Speech and Language Processing 22(12) (2014).
https://doi.org/10.1109/TASLP.2014.2346313

A Comparison of Adaptation Techniques and
Recurrent Neural Network Architectures

Jan Vanˇek[0000−0002−2639−6731], Josef Mich´alek[0000−0001−7757−3163],
Jan Zelinka[0000−0001−6834−9178], and Josef Psutka[0000−0002−0764−3207]

University of West Bohemia
Univerzitn´ı 8, 301 00 Pilsen, Czech Republic
{vanekyj,orcus,zelinka,psutka}@kky.zcu.cz

Abstract. Recently, recurrent neural networks have become state-of-
the-art in acoustic modeling for automatic speech recognition. The long
short-term memory (LSTM) units are the most popular ones. However,
alternative units like gated recurrent unit (GRU) and its modiﬁcations
outperformed LSTM in some publications. In this paper, we compared
ﬁve neural network (NN) architectures with various adaptation and fea-
ture normalization techniques. We have evaluated feature-space maxi-
mum likelihood linear regression, ﬁve variants of i-vector adaptation and
two variants of cepstral mean normalization. The most adaptation and
normalization techniques were developed for feed-forward NNs and, ac-
cording to results in this paper, not all of them worked also with RNNs.
For experiments, we have chosen a well known and available TIMIT
phone recognition task. The phone recognition is much more sensitive to
the quality of AM than large vocabulary task with a complex language
model. Also, we published the open-source scripts to easily replicate the
results and to help continue the development.

Keywords: neural networks · acoustic model · TIMIT · LSTM · GRU
· phone recognition · adaptation · i-vectors

1

Introduction

Neural Networks (NNs) and deep NNs (DNNs) became dominant in the ﬁeld of
the acoustic modeling several years ago. Simple feed-forward (FF) DNNs were
faded away in recent years. The current progress is based on the modeling of a
longer temporal context of individual feature frames. Main two ways are actu-
ally popular: First, a larger context is modeled by a time-delayed NN (TDNN)
[16], [9]. TDNNs model long term temporal dependencies with training times
comparable to standard feed-forward DNNs. In the TDNN architecture, the ini-
tial transforms learn narrow contexts and the deeper layers process the hidden
activations from a wider temporal context. Hence the higher layers have the
ability to learn wider temporal relationships. The second way to learn the longer
temporal context is to use recurrent NNs (RNNs). The most popular RNN archi-
tecture is a long short-term memory (LSTM) that has been designed to address

8
1
0
2
 
l
u
J
 
2
1
 
 
]
S
A
.
s
s
e
e
[
 
 
1
v
1
4
4
6
0
.
7
0
8
1
:
v
i
X
r
a

2

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

the vanishing and exploding gradient problems of conventional RNNs. Unlike
feed-forward neural networks, RNNs have cyclic connections making them pow-
erful for modeling sequences [12]. The main drawback is much slower training
due to the sequential nature of the learning algorithm. An unfolding of the re-
current network during training was proposed in [13] to speed-up the training,
however it is still signiﬁcantly slower than FF NNs or TDNNs. More recently,
another type of recurrent unit, a gated recurrent unit (GRU), was proposed in
[2], [3]. Similarly to the LSTM unit, the GRU has gating units that modulate the
ﬂow of information inside the unit, however, without having a separate mem-
ory cells. Further revising GRUs leaded to a simpliﬁed architecture potentially
more suitable for speech recognition in [11]. First, removing the reset gate in the
GRU design resulted in a simpler single-gate architecture called modiﬁed GRU
(M-GRU). Second, replacing tanh with ReLU activations in the state update
equations was proposed and called M-reluGRU. A more detailed overview of the
RNN architectures follows in Section 3.

Even if large datasets are used for the DNN training, an adaptation of an
acoustic model (AM) to a test speaker and environment is beneﬁcial. A lot of
techniques have been reported on the adaptation, such as the classical maxi-
mum a posterior (MAP) and maximum likelihood linear regression (MLLR) for
traditional GMM-HMM acoustic models. Although this technique can be mod-
iﬁed for an NN-based acoustic model, a much simpler application has so-called
[4] because fMLLR changes only features and
feature space MLLR (fMLLR)
it does not adapt NN parameters. This speaker adaptation technique can be
easily applied in an NN-based acoustic model [15], [8], [10]. Therefore, fMLLR
can be used to any DNN architecture. I-vectors originally developed for speaker
recognition can be used to the speaker and environment adaptation also [14],
[7]. Alternative approach is using of discriminative speaker codes [17], [6]. More
detailed description of the adaptation techniques used in this paper follows.

2 Adaptation of DNNs

The simplest way of the adaptation is a feature level adaptation. When adapting
input features, NN can have any structure. The most popular and well known
technique is fMLLR based on an underlying HMM-GMM that is used during
initial stage of the NN training.

2.1 fMLLR

The fMLLR transforms feature frames with a speaker-speciﬁc square matrix A
and a bias b. For HMM-GMMs, A and b are estimated to maximize the like-
lihood of the adaptation data given the model [4], [15]. In the training phase,
the speaker-speciﬁc transform may be updated several-times alternating HMM-
GMM update. These approach is usually called a speaker adaptive training
(SAT). The result of the training phase is a canonical model that requires us-
ing adaptation during testing phase. However, two-pass processing is required

A Comparison of Adaptation Techniques and RNN Architectures

3

during test phase. The ﬁrst pass produces unsupervised alignment that is used
to estimate the transform parameters via maximum likelihood. The model used
for alignment does not need to be the identical model to the ﬁnal canonical one.
Because all the steps are using the underline HMM-GMM any NN architecture
may be used to train the ﬁnal NN acoustic model.

2.2 i-vectors

The i-vector extraction is a well known technique, so we focused here to more
practical points. An detailed description of i-vectors can be found in [14], [7] and
further papers referenced in there.

The i-vector extraction is comprised from following steps:

1. An universal background model (UBM) needs to be trained. Usually a GMM
with 512 to 2048 diagonal components is used. The quality of GMM is not
critical, so some speed-up methods can be utilized. Features for UBM do
not need to match witch features for NN nor i-vector accumulators. Usu-
ally, features with cepstral mean normalization (CMN) or cepstral mean
and variance normalization (CMVN) are used for UBM. The normalization
techniques reduce speaker and environment variability. Features without any
normalization are used for the i-vector accumulators to carry more speaker-
and environment-related information.

2. Zero-order and centered ﬁrst-order statistics are accumulated for every speaker

3. The i-vector extraction transforms are estimated iteratively by expecta-

according to the UBM posteriors.

tion/maximization (EM) algorithm.

4. The i-vector for individual speakers is evaluated. For training speakers, zero-
order and centered ﬁrst-order statistics have been already accumulated. For
other speakers, statistics must be accumulated. Then, the i-vector is evalu-
ated by the i-vector extraction transforms computed in the third step.

The four step process seems simple but there are some details that need to be
mentioned:

– CMN or CMVN may be computed online or oﬄine. The oﬄine variant may
be per-utterance or per-speaker. The online variant starts from global cep-
stral mean and it is subsequently updated. An exponential forgetting is us-
able for very long utterances. The training setup should match with the
testing one.

– The accumulated statistic should be saturated or scaled-down for long ut-

terances due to an i-vector overﬁtting.

– The oﬄine scenario is not proper for training. The number of speakers and
thus variants of i-vectors is very limited and leads to NN overﬁtting. The
online scenario is recommended for training, in the Kaldi Switchboard exam-
ple recipe the number of speakers is also boosted by pseudo-speakers. Two
training utterances represent one pseudo-speaker. The oﬄine scenario may
be used in the test phase.

4

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

3 Recurrent Neural Network Architectures

3.1 Long Short-Term Memory

Long short-term memory (LSTM) is a widely used type of recurrent neural net-
work (RNN). Standard RNNs suﬀer from both exploding and vanishing gradient
problems. Both of these problems are caused by the fact, that information ﬂowing
through the RNN passes through many stages of multiplication. The gradient is
essentially equal to the weight matrix raised to a high power. This results in the
gradient growing or shrinking at an exponential rate to the number of timesteps.
The exploding gradient problem can be solved simply by truncating the gra-
dient. On the other hand, the vanishing gradient problem is harder to overcome.
It does not simply cause the gradient to be small; the gradient components
corresponding to long-term dependencies are small while the components corre-
sponding to short-term dependencies are large. Resulting RNN can then learn
short-term dependencies but not long-term dependencies.

The LSTM was proposed in 1997 by Hochreiter and Scmidhuber [5] as a
solution to the vanishing gradient problem. Let ct denote a hidden state of a
LSTM. The main idea is that instead of computing ct directly from ct−1 with
matrix-vector product followed by an activation function, the LSTM computes
∆ct and adds it to ct−1 to get ct. The addition operation is what eliminates the
vanishing gradient problem.

Each LSTM cell is composed of smaller units called gates, which control the
ﬂow of information through the cell. The forget gate ft controls what information
will be discarded from the cell state, input gate it controls what new information
will be stored in the cell state and output gate ot controls what information from
the cell state will be used in the output.

The LSTM has two hidden states, ct and ht. The state ct ﬁghts the gradient
vanishing problem while ht allows the network to make complex decisions over
short periods of time. There are several slightly diﬀerent LSTM variants. The
architecture used in this paper is speciﬁed by the following equations:

it = σ(Wxixt + Whiht−1 + bi)
ft = σ(Wxf xt + Whf ht−1 + bf )
ot = σ(Wxoxt + Whoht−1 + bo)
ct = ft ∗ ct−1 + it ∗ tanh(Wxcxt + Whcht−1 + bc)
ht = ot ∗ tanh(ct)

The ﬁgure 1 shows the internal structure of LSTM.

3.2 Gated Recurrent Unit

A gated recurrent unit (GRU) was proposed in 2014 by Cho et al.[3] Similarly to
the LSTM unit, the GRU has gating units that modulate the ﬂow of information
inside the unit, however, without having a separate memory cells.

A Comparison of Adaptation Techniques and RNN Architectures

5

ht

ct

ht

ct−1

×

+

tanh

it

×

ot

×

σ

σ

tanh

σ

ht−1

ft

xt

Fig. 1. Structure of a LSTM unit

The update gate zt decides how much the unit updates its activation and
reset gate rt determines which information will be kept from the old state. GRU
does not have any mechanism to control what information to output, therefore
it exposes the whole state.

The main diﬀerences between LSTM unit and GRU are:

– GRU has 2 gates, LSTM has 3 gates
– GRUs do not have an internal memory diﬀerent from the unit output, LSTMs
have an internal memory ct and the output is controlled by an output gate

– Second nonlinearity is not applied when computing the output of GRUs

The GRU unit used in this work is described by the following equations:

rt = σ(Wrxt + Urht−1 + br)
zt = σ(Wzxt + Uzht−1 + bz)
˜ht = tanh(W xt + U (rt ∗ ht−1) + bh)
ht = (1 − zt) ∗ ht−1 + zt ∗ ˜ht

3.3 Modiﬁed Gated Recurrent Unit with ReLU
Ravanelli introduced a simpliﬁed GRU architecture, called M-reluGRU, in [11].
This simpliﬁed architecture does not have the reset gate and uses ReLU as an
activation function instead of tanh.

The M-reluGRU unit is described by the following equations:

zt = σ(Wz xt + Uzht−1 + bz)
˜ht = ReLU(W xt + U ht−1 + bh)
ht = (1 − zt) ∗ ht−1 + zt ∗ ˜ht

6

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

We have also used this unit with the reset gate to evaluate the impact of
the missing reset gate on the network performance. This unit is eﬀectivelly a
normal GRU with ReLU as an activation function and we called it reluGRU in
this paper.

The ﬁgure 2 shows the internal structure and diﬀerence of GRU and M-

reluGRU units.

ht

ht

ht−1

×

+

ht

ht−1

+

ht

1−

×

rt

zt

σ

σ

×

˜ht
tanh

×

1−

zt

σ

×

˜ht
ReLU

xt

xt

(a) GRU

(b) M-reluGRU

Fig. 2. Structure of GRU and M-reluGRU units

4 Experiments

We have chosen TIMIT, a small phone recognition task, as a benchmark of the
NN architectures and adaptation techniques. The TIMIT corpus is well known
and available. The small size allows a rapid testing and simulates a low-resource
scenario that is still an issue for many minor languages. The phone recognition is
much more sensitive to quality of AM than large vocabulary task with a complex
language model.

The TIMIT corpus contains recordings of phonetically-balanced prompted
English speech. It was recorded using a Sennheiser close-talking microphone
at 16 kHz rate with 16 bit sample resolution. TIMIT contains a total of 6300
sentences (5.4 hours), consisting of 10 sentences spoken by each of 630 speakers
from 8 major dialect regions of the United States. All sentences were manually
segmented at the phone level.

The prompts for the 6300 utterances consist of 2 dialect sentences (SA), 450
phonetically compact sentences (SX) and 1890 phonetically-diverse sentences
(SI).

The training set contains 3696 utterances from 462 speakers. The core test
set consists of 192 utterances, 8 from each of 24 speakers (2 males and 1 female
from each dialect region). The training and test sets do not overlap.

A Comparison of Adaptation Techniques and RNN Architectures

7

4.1 Speech Data, Processing, and Test Description

As mentioned above, we used TIMIT data available from LDC as a corpus
LDC93S1. Then, we ran the Kaldi TIMIT example script s5, which trained
various NN-based phone recognition systems with a common HMM-GMM tied-
triphone model and alignments. The common baseline system consisted of the
following methods: It started from MFCC features which were augmented by ∆
and ∆∆ coeﬃcients and then processed by LDA. Final feature vector dimension
was 40. We obtained ﬁnal alignments by HMM-GMM tied-triphone model with
1909 tied-states (may vary slightly if rerun the script). We trained the model
with MLLT and SAT methods, and we used fMLLR for the SAT training and
a test phase adaptation. We dumped all training, development and test fMLLR
processed data, and alignments to disk. Therefore, it was easy to do compatible
experiments from the same common starting point. We also dumped MFCC
processed by LDA with no normalization and CMN calculated both per speaker
and per utterance.

We employed a bigram language/phone model for the ﬁnal phone recognition.
A bigram model is a very weak model for phone recognition; however, it forced
focus to the acoustic part of the system, and it boosted benchmark sensitivity.
The training, as well as the recognition, was done for 48 phones. We mapped
the ﬁnal results on TIMIT core test set to 39 phones (as is usual for TIMIT
corpus processing), and phone error rate (PER) was evaluated by the provided
NIST script to be compatible with previously published works. In contrast to the
Kaldi recipe, we used a diﬀerent phone decoder. It is a standard Viterbi-based
triphone decoder. It gives better results than the Kaldi standard WFST decoder
on the TIMIT phone recognition task.

We have used an open-source Chainer 3.2 DNNs Python tranining tool that

supports NVidia GPUs [1]. It is multiplatform and easy to use.

4.2 DNN Training

First, as a reference to RNNs, we trained feed-forward (FF) DNN with ReLU
activation function without any pre-training. We used dropout p = 0.2. We
stacked 11 input feature frames to 440 NN input dimension, like in Kaldi example
s5. We have used a network with 8 hidden layers and 2048 ReLU neurons, because
it gave the best performance according to our preliminary experiments. The
ﬁnal softmax layer had 1909 neurons. We used SGD with momentum 0.9. The
networks were trained in 3 stages with learning rate 1e–2, 4e–3 and 1e–4. The
batch size was gradually increased from initial 256 to 1024, and ﬁnally to 2048.
The training in each stage was stopped when the development data criterion
increased in comparison to the last epoch.

Then we have trained LSTM, GRU, reluGRU and M-reluGRU networks. For
all of these recurrent networks, we have used identical training setup. We used
4 layers with 1024 units in each. The dropout used was p = 0.2. We have used
output time delay equal to 5 time steps. RNNs were trained in 4 stages. The ﬁrst
stage used Adam optimization algorithm with batch size 512. The other stages

8

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

used SGD with momentum 0.9, batch size 128 and learning rate equal to 1e–3,
1e–4, and 1e–5 respectivelly. The training in each stage was stopped when the
development data criterion increased in comparison to the last epoch, as in FF
network case.

We have trained each network on several input data and i-vector combina-
tions. We used fMLLR data described in the previous section, MFCC and MFCC
with CMN. The normalization was calculated either per speaker or per utter-
ance. For training and testing, we used no i-vectors, online i-vectors and oﬄine
i-vectors calculated also either per speaker or per utterance. We also evaluated
online i-vectors for training and oﬄine i-vectors for testing. The i-vectors were
computed according to Kaldi Switchboard example script. However, because of
small TIMIT size, we did not use any reduction of data. Entire training dataset
was used to estimate i-vector extractor in all steps. The i-vector extractor has
been trained only once and online, per-speaker, and per-utterance i-vectors sets
were extracted by the same extractor transforms.

Because of stochastic nature of results due to random initialization and
stochastic gradient descent, we have performed each experiment 10 times in
total. Then, we have calculated the average phone error rate (PER) and its
standard deviation.

4.3 Results

We have evaluated average PER, its standard deviation for all combinations
of three features variants, six i-vector variants, and ﬁve NNs architectures. We
had to split the results into two tables because of the page size. Table 1 shows
the average PER for each experiment for FF, LSTM, and GRU NNs archi-
tectures. Table 2 compares three variants of GRU-based NNs: GRU, reluGRU,
M-reluGRU. A subset of the most valuable results is also depicted in Figure 3. It
is clear that fMLLR adaptation technique worked quite well. All the NN archi-
tectures gave the best result with fMLLR. The i-vector adaptation had a stable
gain only for FF NN. Two variants of the i-vector adaptation were the best:
online i-vectors for training and online or oﬄine per-speaker for testing. Results
of RNNs with the i-vector adaptation were interesting, beacause there was no
signiﬁcant gain. The results with adaptation were rather worse. Between RNN
architectures, LSTM was the winner (PER 15.43 % with fMLLR). The GRU and
reluGRU gave comparable PERs, 15.7 % with fMLLR, that was slightly worse
than LSTM. M-reluGRU did not performed well and the results were often worse
than FF.

5 Conclusion

In this paper, we have compared feed-forward and several recurrent network
architectures on input data with fMLLR or i-vector adaptation techniques. The
used recurrent networks were based on LSTM and GRU units. We have also
evaluated two GRU modiﬁcations: reluGRU, with ReLU activation function,

A Comparison of Adaptation Techniques and RNN Architectures

9

Table 1: Phone Error Rate [%] for FF, LSTM and GRU Networks

Data

Training

Testing

FF

i-vectors

Phone Error Rate [%]
LSTM

GRU

fMLLR

MFCC

MFCC
with
CMN
per
speaker

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
MFCC
Oﬀ. spk. Oﬀ. spk.
with
CMN
Oﬀ. utt.
Oﬀ. utt.
per
Oﬀ. spk.
Online
Oﬀ. utt.
utterance Online
Online
Online

17.00 ± 0.13
17.17 ± 0.16
17.32 ± 0.15
17.17 ± 0.16
17.10 ± 0.21
17.18 ± 0.14

19.42 ± 0.18
19.02 ± 0.15
19.29 ± 0.19
18.22 ± 0.19
18.48 ± 0.16
18.19 ± 0.19

18.49 ± 0.19
18.47 ± 0.20
18.59 ± 0.10
18.11 ± 0.24
18.17 ± 0.22
18.21 ± 0.19

19.44 ± 0.27
19.10 ± 0.17
19.32 ± 0.14
18.70 ± 0.18
18.63 ± 0.16
18.73 ± 0.18

15.43 ± 0.28
16.08 ± 0.19
16.34 ± 0.32
16.14 ± 0.22
16.27 ± 0.34
16.23 ± 0.26

16.98 ± 0.27
17.50 ± 0.19
18.12 ± 0.27
17.19 ± 0.26
17.27 ± 0.26
17.21 ± 0.15

16.53 ± 0.20
17.20 ± 0.23
17.45 ± 0.19
16.90 ± 0.24
17.34 ± 0.31
17.25 ± 0.26

16.98 ± 0.20
17.60 ± 0.31
18.28 ± 0.35
17.53 ± 0.23
17.60 ± 0.23
17.66 ± 0.23

15.69 ± 0.19
16.04 ± 0.29
16.43 ± 0.25
16.15 ± 0.28
16.14 ± 0.24
16.23 ± 0.19

17.48 ± 0.19
17.63 ± 0.22
18.09 ± 0.29
17.00 ± 0.28
17.21 ± 0.20
17.33 ± 0.37

17.00 ± 0.25
17.33 ± 0.21
17.36 ± 0.21
17.04 ± 0.16
17.06 ± 0.20
17.24 ± 0.31

17.54 ± 0.20
17.64 ± 0.33
18.15 ± 0.35
17.33 ± 0.18
17.46 ± 0.19
17.43 ± 0.19

and M-reluGRU, with ReLU activation function and without the reset gate. As
features, we have used MFCC processed by LDA without normalization or with
CMN calculated either per speaker or per utterance, and also fMLLR adaptation.
We have also augmented the features with several variants of i-vectors: online
or oﬄine calculated either per speaker or per utterance. Due to the stochastic
nature of the used optimizers, we have performed all experiments 10 times in
total and calculated the average phone error rate and its standard deviation.

For all networks, we have obtained the best results with fMLLR adaptation.
The i-vector adaptation consistently improved the results only for FF networks.
In the case of RNN, i-vectors did not lead to any signiﬁcant improvement; it even
gave worse results in all LSTM experiments and in some experiments with GRU
variants. We have achieved the best results with LSTM network (PER 15.43 %
with fMLLR). GRU and reluGRU were slightly worse (both having PER 15.7 %
with fMLLR). M-reluGRU was in some cases even worse than FF network.

For all our experiments, we have used Chainer 3.2 DNN training framework
with Python programming language and we have published our open-source

10

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

Table 2: Phone Error Rate [%] for GRU and Its Modiﬁcations

Data

Training

Testing

GRU

i-vectors

Phone Error Rate [%]
reluGRU

fMLLR

MFCC

MFCC
with
CMN
per
speaker

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
MFCC
Oﬀ. spk. Oﬀ. spk.
with
CMN
Oﬀ. utt.
Oﬀ. utt.
per
Oﬀ. spk.
Online
Oﬀ. utt.
utterance Online
Online
Online

15.69 ± 0.19
16.04 ± 0.29
16.43 ± 0.25
16.15 ± 0.28
16.14 ± 0.24
16.23 ± 0.19

17.48 ± 0.19
17.63 ± 0.22
18.09 ± 0.29
17.00 ± 0.28
17.21 ± 0.20
17.33 ± 0.37

17.00 ± 0.25
17.33 ± 0.21
17.36 ± 0.21
17.04 ± 0.16
17.06 ± 0.20
17.24 ± 0.31

17.54 ± 0.20
17.64 ± 0.33
18.15 ± 0.35
17.33 ± 0.18
17.46 ± 0.19
17.43 ± 0.19

15.70 ± 0.56
16.28 ± 0.38
16.33 ± 0.13
16.19 ± 0.22
16.23 ± 0.18
16.39 ± 0.33

17.30 ± 0.50
18.32 ± 0.39
18.35 ± 0.37
17.30 ± 0.38
17.52 ± 0.47
17.41 ± 0.44

16.91 ± 0.22
17.70 ± 0.39
17.91 ± 0.35
17.39 ± 0.27
17.48 ± 0.29
17.45 ± 0.27

17.50 ± 0.29
18.05 ± 0.27
18.52 ± 0.33
17.79 ± 0.31
18.05 ± 0.24
17.85 ± 0.18

M-reluGRU

17.06 ± 0.77
17.50 ± 0.72
18.25 ± 0.85
17.76 ± 0.94
17.85 ± 0.76
17.60 ± 0.67

19.64 ± 1.05
20.13 ± 0.93
20.70 ± 0.65
19.38 ± 0.96
19.44 ± 0.89
19.29 ± 0.89

18.23 ± 0.53
19.44 ± 0.66
19.43 ± 1.17
19.03 ± 1.07
18.93 ± 0.74
18.89 ± 0.78

19.26 ± 0.85
19.08 ± 0.77
21.04 ± 0.97
20.10 ± 0.95
19.63 ± 0.99
20.01 ± 0.69

scripts at https://github.com/OrcusCZ/NNAcousticModeling to easily repli-
cate the results and to help continue the development.

Acknowledgement

This work was supported by the project no. P103/12/G084 of the Grant Agency
of the Czech Republic and by the grant of the University of West Bohemia,
project No. SGS-2016-039. Access to computing and storage facilities owned by
parties and projects contributing to the National Grid Infrastructure MetaCen-
trum provided under the programme ”Projects of Large Research, Development,
and Innovations Infrastructures” (CESNET LM2015042), is greatly appreciated.

A Comparison of Adaptation Techniques and RNN Architectures

11

]

%

[

R
E
P

20

19

18

17

16

15

fMLLR

MFCC

MFCC
+
i-vectors

MFCC
CMN
Spk.

MFCC
CMN
Spk. +
i-vectors

MFCC
CMN
Utt.

MFCC
CMN
Utt. +
i-vectors

FF

LSTM

GRU

reluGRU

M-reluGRU

Fig. 3. Phone Error Rate [%] on Features with Best Performing i-vector Variants

References

1. A ﬂexible framework of neural networks for deep learning. https://chainer.org
2. Cho, K., Van Merri¨enboer, B., Bahdanau, D., Bengio, Y.: On the proper-
ties of neural machine translation: Encoder-decoder approaches. arXiv preprint
arXiv:1409.1259 (2014)

3. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recur-
rent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014)
4. Gales, M.: Maximum likelihood linear transformations for HMM-based speech

recognition. Computer Speech & Language 12(2), 75 – 98 (1998)

5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

9(8), 1735–1780 (1997)

6. Huang, Z., Tang, J., Xue, S., Dai, L.: Speaker Adaptation of RNN-BLSTM
for Speec Recognition based on Speaker Code. Icassp 1, 5305–5309 (2016).
https://doi.org/10.1109/ICASSP.2016.7472690

7. Karaﬁ´at, M., Burget, L., Matˇejka, P., Glembek, O., ˇCernock´y, J.: iVector-based
discriminative adaptation for automatic speech recognition. 2011 IEEE Workshop
on Automatic Speech Recognition and Understanding, ASRU 2011, Proceedings
pp. 152–157 (2011). https://doi.org/10.1109/ASRU.2011.6163922

8. Parthasarathi, S.H.K., Hoﬀmeister, B., Matsoukas, S., Mandal, A., Strom, N.,
Garimella, S.: fmllr based feature-space speaker adaptation of dnn acoustic models.
In: INTERSPEECH. pp. 3630–3634. ISCA (2015)

9. Peddinti, V., Povey, D., Khudanpur, S.: A time delay neural network architecture
for eﬃcient modeling of long temporal contexts. Proceedings of the Annual Con-
ference of the International Speech Communication Association, INTERSPEECH
2015-Janua, 3214–3218 (2015)

12

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

10. Rath, S.P., Povey, D., Vesel´y, K., ˇCernock´y, J.: Improved feature processing for

deep neural networks. In: INTERSPEECH. pp. 109–113. ISCA (2013)

11. Ravanelli, M., Brakel, P., Omologo, M., Bengio, Y., Kessler, F.B.: Improving speech
recognition by revising gated recurrent units. In: Interspeech2017. pp. 1308–1312
(2017). https://doi.org/10.21437/Interspeech.2017-775

12. Sak, H., Senior, A., Beaufays, F.: Long Short-Term Memory Based
for Large Vocabulary Speech
Interspeech 1, 338–342 (2014). https://doi.org/arXiv:1402.1128,

Recurrent Neural Network Architectures
Recognition.
http://arxiv.org/abs/1402.1128

13. Saon,
for
http://mazsola.iit.uni-miskolc.hu/{~}czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS141054.PDF

Unfolded
INTERSPEECH

H.:
Recognition.

Recurrent
1,

Networks
(2014),

Neural
343–347

G.,
Speech

Soltau,

14. Saon, G., Soltau, H., Nahamoo, D., Picheny, M.: Speaker adaptation of neural
network acoustic models using i-vectors. In: 2013 IEEE Workshop on Automatic
Speech Recognition and Understanding. pp. 55–59 (2013)

15. Seide, F., Chen, X., Yu, D.: Feature engineering in context-dependent deep neural

networks for conversational speech transcription. In: in ASRU (2011)

16. Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., Lang, K.J.: Phoneme
on
time-delay
neural
and Signal Processing 37(3),
1989).

IEEE Transactions

recognition
using
Acoustics, Speech,
https://doi.org/10.1109/29.21701

networks.

328–339

(Mar

17. Xue, S., Abdel-Hamid, O., Jiang, H., Dai, L., Liu, Q.: Fast adaptation
of deep neural network based on discriminant codes for speech recognition.
IEEE/ACM Transactions on Speech and Language Processing 22(12) (2014).
https://doi.org/10.1109/TASLP.2014.2346313

A Comparison of Adaptation Techniques and
Recurrent Neural Network Architectures

Jan Vanˇek[0000−0002−2639−6731], Josef Mich´alek[0000−0001−7757−3163],
Jan Zelinka[0000−0001−6834−9178], and Josef Psutka[0000−0002−0764−3207]

University of West Bohemia
Univerzitn´ı 8, 301 00 Pilsen, Czech Republic
{vanekyj,orcus,zelinka,psutka}@kky.zcu.cz

Abstract. Recently, recurrent neural networks have become state-of-
the-art in acoustic modeling for automatic speech recognition. The long
short-term memory (LSTM) units are the most popular ones. However,
alternative units like gated recurrent unit (GRU) and its modiﬁcations
outperformed LSTM in some publications. In this paper, we compared
ﬁve neural network (NN) architectures with various adaptation and fea-
ture normalization techniques. We have evaluated feature-space maxi-
mum likelihood linear regression, ﬁve variants of i-vector adaptation and
two variants of cepstral mean normalization. The most adaptation and
normalization techniques were developed for feed-forward NNs and, ac-
cording to results in this paper, not all of them worked also with RNNs.
For experiments, we have chosen a well known and available TIMIT
phone recognition task. The phone recognition is much more sensitive to
the quality of AM than large vocabulary task with a complex language
model. Also, we published the open-source scripts to easily replicate the
results and to help continue the development.

Keywords: neural networks · acoustic model · TIMIT · LSTM · GRU
· phone recognition · adaptation · i-vectors

1

Introduction

Neural Networks (NNs) and deep NNs (DNNs) became dominant in the ﬁeld of
the acoustic modeling several years ago. Simple feed-forward (FF) DNNs were
faded away in recent years. The current progress is based on the modeling of a
longer temporal context of individual feature frames. Main two ways are actu-
ally popular: First, a larger context is modeled by a time-delayed NN (TDNN)
[16], [9]. TDNNs model long term temporal dependencies with training times
comparable to standard feed-forward DNNs. In the TDNN architecture, the ini-
tial transforms learn narrow contexts and the deeper layers process the hidden
activations from a wider temporal context. Hence the higher layers have the
ability to learn wider temporal relationships. The second way to learn the longer
temporal context is to use recurrent NNs (RNNs). The most popular RNN archi-
tecture is a long short-term memory (LSTM) that has been designed to address

8
1
0
2
 
l
u
J
 
2
1
 
 
]
S
A
.
s
s
e
e
[
 
 
1
v
1
4
4
6
0
.
7
0
8
1
:
v
i
X
r
a

2

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

the vanishing and exploding gradient problems of conventional RNNs. Unlike
feed-forward neural networks, RNNs have cyclic connections making them pow-
erful for modeling sequences [12]. The main drawback is much slower training
due to the sequential nature of the learning algorithm. An unfolding of the re-
current network during training was proposed in [13] to speed-up the training,
however it is still signiﬁcantly slower than FF NNs or TDNNs. More recently,
another type of recurrent unit, a gated recurrent unit (GRU), was proposed in
[2], [3]. Similarly to the LSTM unit, the GRU has gating units that modulate the
ﬂow of information inside the unit, however, without having a separate mem-
ory cells. Further revising GRUs leaded to a simpliﬁed architecture potentially
more suitable for speech recognition in [11]. First, removing the reset gate in the
GRU design resulted in a simpler single-gate architecture called modiﬁed GRU
(M-GRU). Second, replacing tanh with ReLU activations in the state update
equations was proposed and called M-reluGRU. A more detailed overview of the
RNN architectures follows in Section 3.

Even if large datasets are used for the DNN training, an adaptation of an
acoustic model (AM) to a test speaker and environment is beneﬁcial. A lot of
techniques have been reported on the adaptation, such as the classical maxi-
mum a posterior (MAP) and maximum likelihood linear regression (MLLR) for
traditional GMM-HMM acoustic models. Although this technique can be mod-
iﬁed for an NN-based acoustic model, a much simpler application has so-called
[4] because fMLLR changes only features and
feature space MLLR (fMLLR)
it does not adapt NN parameters. This speaker adaptation technique can be
easily applied in an NN-based acoustic model [15], [8], [10]. Therefore, fMLLR
can be used to any DNN architecture. I-vectors originally developed for speaker
recognition can be used to the speaker and environment adaptation also [14],
[7]. Alternative approach is using of discriminative speaker codes [17], [6]. More
detailed description of the adaptation techniques used in this paper follows.

2 Adaptation of DNNs

The simplest way of the adaptation is a feature level adaptation. When adapting
input features, NN can have any structure. The most popular and well known
technique is fMLLR based on an underlying HMM-GMM that is used during
initial stage of the NN training.

2.1 fMLLR

The fMLLR transforms feature frames with a speaker-speciﬁc square matrix A
and a bias b. For HMM-GMMs, A and b are estimated to maximize the like-
lihood of the adaptation data given the model [4], [15]. In the training phase,
the speaker-speciﬁc transform may be updated several-times alternating HMM-
GMM update. These approach is usually called a speaker adaptive training
(SAT). The result of the training phase is a canonical model that requires us-
ing adaptation during testing phase. However, two-pass processing is required

A Comparison of Adaptation Techniques and RNN Architectures

3

during test phase. The ﬁrst pass produces unsupervised alignment that is used
to estimate the transform parameters via maximum likelihood. The model used
for alignment does not need to be the identical model to the ﬁnal canonical one.
Because all the steps are using the underline HMM-GMM any NN architecture
may be used to train the ﬁnal NN acoustic model.

2.2 i-vectors

The i-vector extraction is a well known technique, so we focused here to more
practical points. An detailed description of i-vectors can be found in [14], [7] and
further papers referenced in there.

The i-vector extraction is comprised from following steps:

1. An universal background model (UBM) needs to be trained. Usually a GMM
with 512 to 2048 diagonal components is used. The quality of GMM is not
critical, so some speed-up methods can be utilized. Features for UBM do
not need to match witch features for NN nor i-vector accumulators. Usu-
ally, features with cepstral mean normalization (CMN) or cepstral mean
and variance normalization (CMVN) are used for UBM. The normalization
techniques reduce speaker and environment variability. Features without any
normalization are used for the i-vector accumulators to carry more speaker-
and environment-related information.

2. Zero-order and centered ﬁrst-order statistics are accumulated for every speaker

3. The i-vector extraction transforms are estimated iteratively by expecta-

according to the UBM posteriors.

tion/maximization (EM) algorithm.

4. The i-vector for individual speakers is evaluated. For training speakers, zero-
order and centered ﬁrst-order statistics have been already accumulated. For
other speakers, statistics must be accumulated. Then, the i-vector is evalu-
ated by the i-vector extraction transforms computed in the third step.

The four step process seems simple but there are some details that need to be
mentioned:

– CMN or CMVN may be computed online or oﬄine. The oﬄine variant may
be per-utterance or per-speaker. The online variant starts from global cep-
stral mean and it is subsequently updated. An exponential forgetting is us-
able for very long utterances. The training setup should match with the
testing one.

– The accumulated statistic should be saturated or scaled-down for long ut-

terances due to an i-vector overﬁtting.

– The oﬄine scenario is not proper for training. The number of speakers and
thus variants of i-vectors is very limited and leads to NN overﬁtting. The
online scenario is recommended for training, in the Kaldi Switchboard exam-
ple recipe the number of speakers is also boosted by pseudo-speakers. Two
training utterances represent one pseudo-speaker. The oﬄine scenario may
be used in the test phase.

4

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

3 Recurrent Neural Network Architectures

3.1 Long Short-Term Memory

Long short-term memory (LSTM) is a widely used type of recurrent neural net-
work (RNN). Standard RNNs suﬀer from both exploding and vanishing gradient
problems. Both of these problems are caused by the fact, that information ﬂowing
through the RNN passes through many stages of multiplication. The gradient is
essentially equal to the weight matrix raised to a high power. This results in the
gradient growing or shrinking at an exponential rate to the number of timesteps.
The exploding gradient problem can be solved simply by truncating the gra-
dient. On the other hand, the vanishing gradient problem is harder to overcome.
It does not simply cause the gradient to be small; the gradient components
corresponding to long-term dependencies are small while the components corre-
sponding to short-term dependencies are large. Resulting RNN can then learn
short-term dependencies but not long-term dependencies.

The LSTM was proposed in 1997 by Hochreiter and Scmidhuber [5] as a
solution to the vanishing gradient problem. Let ct denote a hidden state of a
LSTM. The main idea is that instead of computing ct directly from ct−1 with
matrix-vector product followed by an activation function, the LSTM computes
∆ct and adds it to ct−1 to get ct. The addition operation is what eliminates the
vanishing gradient problem.

Each LSTM cell is composed of smaller units called gates, which control the
ﬂow of information through the cell. The forget gate ft controls what information
will be discarded from the cell state, input gate it controls what new information
will be stored in the cell state and output gate ot controls what information from
the cell state will be used in the output.

The LSTM has two hidden states, ct and ht. The state ct ﬁghts the gradient
vanishing problem while ht allows the network to make complex decisions over
short periods of time. There are several slightly diﬀerent LSTM variants. The
architecture used in this paper is speciﬁed by the following equations:

it = σ(Wxixt + Whiht−1 + bi)
ft = σ(Wxf xt + Whf ht−1 + bf )
ot = σ(Wxoxt + Whoht−1 + bo)
ct = ft ∗ ct−1 + it ∗ tanh(Wxcxt + Whcht−1 + bc)
ht = ot ∗ tanh(ct)

The ﬁgure 1 shows the internal structure of LSTM.

3.2 Gated Recurrent Unit

A gated recurrent unit (GRU) was proposed in 2014 by Cho et al.[3] Similarly to
the LSTM unit, the GRU has gating units that modulate the ﬂow of information
inside the unit, however, without having a separate memory cells.

A Comparison of Adaptation Techniques and RNN Architectures

5

ht

ct

ht

ct−1

×

+

tanh

it

×

ot

×

σ

σ

tanh

σ

ht−1

ft

xt

Fig. 1. Structure of a LSTM unit

The update gate zt decides how much the unit updates its activation and
reset gate rt determines which information will be kept from the old state. GRU
does not have any mechanism to control what information to output, therefore
it exposes the whole state.

The main diﬀerences between LSTM unit and GRU are:

– GRU has 2 gates, LSTM has 3 gates
– GRUs do not have an internal memory diﬀerent from the unit output, LSTMs
have an internal memory ct and the output is controlled by an output gate

– Second nonlinearity is not applied when computing the output of GRUs

The GRU unit used in this work is described by the following equations:

rt = σ(Wrxt + Urht−1 + br)
zt = σ(Wzxt + Uzht−1 + bz)
˜ht = tanh(W xt + U (rt ∗ ht−1) + bh)
ht = (1 − zt) ∗ ht−1 + zt ∗ ˜ht

3.3 Modiﬁed Gated Recurrent Unit with ReLU
Ravanelli introduced a simpliﬁed GRU architecture, called M-reluGRU, in [11].
This simpliﬁed architecture does not have the reset gate and uses ReLU as an
activation function instead of tanh.

The M-reluGRU unit is described by the following equations:

zt = σ(Wz xt + Uzht−1 + bz)
˜ht = ReLU(W xt + U ht−1 + bh)
ht = (1 − zt) ∗ ht−1 + zt ∗ ˜ht

6

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

We have also used this unit with the reset gate to evaluate the impact of
the missing reset gate on the network performance. This unit is eﬀectivelly a
normal GRU with ReLU as an activation function and we called it reluGRU in
this paper.

The ﬁgure 2 shows the internal structure and diﬀerence of GRU and M-

reluGRU units.

ht

ht

ht−1

×

+

ht

ht−1

+

ht

1−

×

rt

zt

σ

σ

×

˜ht
tanh

×

1−

zt

σ

×

˜ht
ReLU

xt

xt

(a) GRU

(b) M-reluGRU

Fig. 2. Structure of GRU and M-reluGRU units

4 Experiments

We have chosen TIMIT, a small phone recognition task, as a benchmark of the
NN architectures and adaptation techniques. The TIMIT corpus is well known
and available. The small size allows a rapid testing and simulates a low-resource
scenario that is still an issue for many minor languages. The phone recognition is
much more sensitive to quality of AM than large vocabulary task with a complex
language model.

The TIMIT corpus contains recordings of phonetically-balanced prompted
English speech. It was recorded using a Sennheiser close-talking microphone
at 16 kHz rate with 16 bit sample resolution. TIMIT contains a total of 6300
sentences (5.4 hours), consisting of 10 sentences spoken by each of 630 speakers
from 8 major dialect regions of the United States. All sentences were manually
segmented at the phone level.

The prompts for the 6300 utterances consist of 2 dialect sentences (SA), 450
phonetically compact sentences (SX) and 1890 phonetically-diverse sentences
(SI).

The training set contains 3696 utterances from 462 speakers. The core test
set consists of 192 utterances, 8 from each of 24 speakers (2 males and 1 female
from each dialect region). The training and test sets do not overlap.

A Comparison of Adaptation Techniques and RNN Architectures

7

4.1 Speech Data, Processing, and Test Description

As mentioned above, we used TIMIT data available from LDC as a corpus
LDC93S1. Then, we ran the Kaldi TIMIT example script s5, which trained
various NN-based phone recognition systems with a common HMM-GMM tied-
triphone model and alignments. The common baseline system consisted of the
following methods: It started from MFCC features which were augmented by ∆
and ∆∆ coeﬃcients and then processed by LDA. Final feature vector dimension
was 40. We obtained ﬁnal alignments by HMM-GMM tied-triphone model with
1909 tied-states (may vary slightly if rerun the script). We trained the model
with MLLT and SAT methods, and we used fMLLR for the SAT training and
a test phase adaptation. We dumped all training, development and test fMLLR
processed data, and alignments to disk. Therefore, it was easy to do compatible
experiments from the same common starting point. We also dumped MFCC
processed by LDA with no normalization and CMN calculated both per speaker
and per utterance.

We employed a bigram language/phone model for the ﬁnal phone recognition.
A bigram model is a very weak model for phone recognition; however, it forced
focus to the acoustic part of the system, and it boosted benchmark sensitivity.
The training, as well as the recognition, was done for 48 phones. We mapped
the ﬁnal results on TIMIT core test set to 39 phones (as is usual for TIMIT
corpus processing), and phone error rate (PER) was evaluated by the provided
NIST script to be compatible with previously published works. In contrast to the
Kaldi recipe, we used a diﬀerent phone decoder. It is a standard Viterbi-based
triphone decoder. It gives better results than the Kaldi standard WFST decoder
on the TIMIT phone recognition task.

We have used an open-source Chainer 3.2 DNNs Python tranining tool that

supports NVidia GPUs [1]. It is multiplatform and easy to use.

4.2 DNN Training

First, as a reference to RNNs, we trained feed-forward (FF) DNN with ReLU
activation function without any pre-training. We used dropout p = 0.2. We
stacked 11 input feature frames to 440 NN input dimension, like in Kaldi example
s5. We have used a network with 8 hidden layers and 2048 ReLU neurons, because
it gave the best performance according to our preliminary experiments. The
ﬁnal softmax layer had 1909 neurons. We used SGD with momentum 0.9. The
networks were trained in 3 stages with learning rate 1e–2, 4e–3 and 1e–4. The
batch size was gradually increased from initial 256 to 1024, and ﬁnally to 2048.
The training in each stage was stopped when the development data criterion
increased in comparison to the last epoch.

Then we have trained LSTM, GRU, reluGRU and M-reluGRU networks. For
all of these recurrent networks, we have used identical training setup. We used
4 layers with 1024 units in each. The dropout used was p = 0.2. We have used
output time delay equal to 5 time steps. RNNs were trained in 4 stages. The ﬁrst
stage used Adam optimization algorithm with batch size 512. The other stages

8

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

used SGD with momentum 0.9, batch size 128 and learning rate equal to 1e–3,
1e–4, and 1e–5 respectivelly. The training in each stage was stopped when the
development data criterion increased in comparison to the last epoch, as in FF
network case.

We have trained each network on several input data and i-vector combina-
tions. We used fMLLR data described in the previous section, MFCC and MFCC
with CMN. The normalization was calculated either per speaker or per utter-
ance. For training and testing, we used no i-vectors, online i-vectors and oﬄine
i-vectors calculated also either per speaker or per utterance. We also evaluated
online i-vectors for training and oﬄine i-vectors for testing. The i-vectors were
computed according to Kaldi Switchboard example script. However, because of
small TIMIT size, we did not use any reduction of data. Entire training dataset
was used to estimate i-vector extractor in all steps. The i-vector extractor has
been trained only once and online, per-speaker, and per-utterance i-vectors sets
were extracted by the same extractor transforms.

Because of stochastic nature of results due to random initialization and
stochastic gradient descent, we have performed each experiment 10 times in
total. Then, we have calculated the average phone error rate (PER) and its
standard deviation.

4.3 Results

We have evaluated average PER, its standard deviation for all combinations
of three features variants, six i-vector variants, and ﬁve NNs architectures. We
had to split the results into two tables because of the page size. Table 1 shows
the average PER for each experiment for FF, LSTM, and GRU NNs archi-
tectures. Table 2 compares three variants of GRU-based NNs: GRU, reluGRU,
M-reluGRU. A subset of the most valuable results is also depicted in Figure 3. It
is clear that fMLLR adaptation technique worked quite well. All the NN archi-
tectures gave the best result with fMLLR. The i-vector adaptation had a stable
gain only for FF NN. Two variants of the i-vector adaptation were the best:
online i-vectors for training and online or oﬄine per-speaker for testing. Results
of RNNs with the i-vector adaptation were interesting, beacause there was no
signiﬁcant gain. The results with adaptation were rather worse. Between RNN
architectures, LSTM was the winner (PER 15.43 % with fMLLR). The GRU and
reluGRU gave comparable PERs, 15.7 % with fMLLR, that was slightly worse
than LSTM. M-reluGRU did not performed well and the results were often worse
than FF.

5 Conclusion

In this paper, we have compared feed-forward and several recurrent network
architectures on input data with fMLLR or i-vector adaptation techniques. The
used recurrent networks were based on LSTM and GRU units. We have also
evaluated two GRU modiﬁcations: reluGRU, with ReLU activation function,

A Comparison of Adaptation Techniques and RNN Architectures

9

Table 1: Phone Error Rate [%] for FF, LSTM and GRU Networks

Data

Training

Testing

FF

i-vectors

Phone Error Rate [%]
LSTM

GRU

fMLLR

MFCC

MFCC
with
CMN
per
speaker

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
MFCC
Oﬀ. spk. Oﬀ. spk.
with
CMN
Oﬀ. utt.
Oﬀ. utt.
per
Oﬀ. spk.
Online
Oﬀ. utt.
utterance Online
Online
Online

17.00 ± 0.13
17.17 ± 0.16
17.32 ± 0.15
17.17 ± 0.16
17.10 ± 0.21
17.18 ± 0.14

19.42 ± 0.18
19.02 ± 0.15
19.29 ± 0.19
18.22 ± 0.19
18.48 ± 0.16
18.19 ± 0.19

18.49 ± 0.19
18.47 ± 0.20
18.59 ± 0.10
18.11 ± 0.24
18.17 ± 0.22
18.21 ± 0.19

19.44 ± 0.27
19.10 ± 0.17
19.32 ± 0.14
18.70 ± 0.18
18.63 ± 0.16
18.73 ± 0.18

15.43 ± 0.28
16.08 ± 0.19
16.34 ± 0.32
16.14 ± 0.22
16.27 ± 0.34
16.23 ± 0.26

16.98 ± 0.27
17.50 ± 0.19
18.12 ± 0.27
17.19 ± 0.26
17.27 ± 0.26
17.21 ± 0.15

16.53 ± 0.20
17.20 ± 0.23
17.45 ± 0.19
16.90 ± 0.24
17.34 ± 0.31
17.25 ± 0.26

16.98 ± 0.20
17.60 ± 0.31
18.28 ± 0.35
17.53 ± 0.23
17.60 ± 0.23
17.66 ± 0.23

15.69 ± 0.19
16.04 ± 0.29
16.43 ± 0.25
16.15 ± 0.28
16.14 ± 0.24
16.23 ± 0.19

17.48 ± 0.19
17.63 ± 0.22
18.09 ± 0.29
17.00 ± 0.28
17.21 ± 0.20
17.33 ± 0.37

17.00 ± 0.25
17.33 ± 0.21
17.36 ± 0.21
17.04 ± 0.16
17.06 ± 0.20
17.24 ± 0.31

17.54 ± 0.20
17.64 ± 0.33
18.15 ± 0.35
17.33 ± 0.18
17.46 ± 0.19
17.43 ± 0.19

and M-reluGRU, with ReLU activation function and without the reset gate. As
features, we have used MFCC processed by LDA without normalization or with
CMN calculated either per speaker or per utterance, and also fMLLR adaptation.
We have also augmented the features with several variants of i-vectors: online
or oﬄine calculated either per speaker or per utterance. Due to the stochastic
nature of the used optimizers, we have performed all experiments 10 times in
total and calculated the average phone error rate and its standard deviation.

For all networks, we have obtained the best results with fMLLR adaptation.
The i-vector adaptation consistently improved the results only for FF networks.
In the case of RNN, i-vectors did not lead to any signiﬁcant improvement; it even
gave worse results in all LSTM experiments and in some experiments with GRU
variants. We have achieved the best results with LSTM network (PER 15.43 %
with fMLLR). GRU and reluGRU were slightly worse (both having PER 15.7 %
with fMLLR). M-reluGRU was in some cases even worse than FF network.

For all our experiments, we have used Chainer 3.2 DNN training framework
with Python programming language and we have published our open-source

10

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

Table 2: Phone Error Rate [%] for GRU and Its Modiﬁcations

Data

Training

Testing

GRU

i-vectors

Phone Error Rate [%]
reluGRU

fMLLR

MFCC

MFCC
with
CMN
per
speaker

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
Oﬀ. spk. Oﬀ. spk.
Oﬀ. utt.
Oﬀ. utt.
Oﬀ. spk.
Online
Oﬀ. utt.
Online
Online
Online

-

-
MFCC
Oﬀ. spk. Oﬀ. spk.
with
CMN
Oﬀ. utt.
Oﬀ. utt.
per
Oﬀ. spk.
Online
Oﬀ. utt.
utterance Online
Online
Online

15.69 ± 0.19
16.04 ± 0.29
16.43 ± 0.25
16.15 ± 0.28
16.14 ± 0.24
16.23 ± 0.19

17.48 ± 0.19
17.63 ± 0.22
18.09 ± 0.29
17.00 ± 0.28
17.21 ± 0.20
17.33 ± 0.37

17.00 ± 0.25
17.33 ± 0.21
17.36 ± 0.21
17.04 ± 0.16
17.06 ± 0.20
17.24 ± 0.31

17.54 ± 0.20
17.64 ± 0.33
18.15 ± 0.35
17.33 ± 0.18
17.46 ± 0.19
17.43 ± 0.19

15.70 ± 0.56
16.28 ± 0.38
16.33 ± 0.13
16.19 ± 0.22
16.23 ± 0.18
16.39 ± 0.33

17.30 ± 0.50
18.32 ± 0.39
18.35 ± 0.37
17.30 ± 0.38
17.52 ± 0.47
17.41 ± 0.44

16.91 ± 0.22
17.70 ± 0.39
17.91 ± 0.35
17.39 ± 0.27
17.48 ± 0.29
17.45 ± 0.27

17.50 ± 0.29
18.05 ± 0.27
18.52 ± 0.33
17.79 ± 0.31
18.05 ± 0.24
17.85 ± 0.18

M-reluGRU

17.06 ± 0.77
17.50 ± 0.72
18.25 ± 0.85
17.76 ± 0.94
17.85 ± 0.76
17.60 ± 0.67

19.64 ± 1.05
20.13 ± 0.93
20.70 ± 0.65
19.38 ± 0.96
19.44 ± 0.89
19.29 ± 0.89

18.23 ± 0.53
19.44 ± 0.66
19.43 ± 1.17
19.03 ± 1.07
18.93 ± 0.74
18.89 ± 0.78

19.26 ± 0.85
19.08 ± 0.77
21.04 ± 0.97
20.10 ± 0.95
19.63 ± 0.99
20.01 ± 0.69

scripts at https://github.com/OrcusCZ/NNAcousticModeling to easily repli-
cate the results and to help continue the development.

Acknowledgement

This work was supported by the project no. P103/12/G084 of the Grant Agency
of the Czech Republic and by the grant of the University of West Bohemia,
project No. SGS-2016-039. Access to computing and storage facilities owned by
parties and projects contributing to the National Grid Infrastructure MetaCen-
trum provided under the programme ”Projects of Large Research, Development,
and Innovations Infrastructures” (CESNET LM2015042), is greatly appreciated.

A Comparison of Adaptation Techniques and RNN Architectures

11

]

%

[

R
E
P

20

19

18

17

16

15

fMLLR

MFCC

MFCC
+
i-vectors

MFCC
CMN
Spk.

MFCC
CMN
Spk. +
i-vectors

MFCC
CMN
Utt.

MFCC
CMN
Utt. +
i-vectors

FF

LSTM

GRU

reluGRU

M-reluGRU

Fig. 3. Phone Error Rate [%] on Features with Best Performing i-vector Variants

References

1. A ﬂexible framework of neural networks for deep learning. https://chainer.org
2. Cho, K., Van Merri¨enboer, B., Bahdanau, D., Bengio, Y.: On the proper-
ties of neural machine translation: Encoder-decoder approaches. arXiv preprint
arXiv:1409.1259 (2014)

3. Chung, J., Gulcehre, C., Cho, K., Bengio, Y.: Empirical evaluation of gated recur-
rent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 (2014)
4. Gales, M.: Maximum likelihood linear transformations for HMM-based speech

recognition. Computer Speech & Language 12(2), 75 – 98 (1998)

5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation

9(8), 1735–1780 (1997)

6. Huang, Z., Tang, J., Xue, S., Dai, L.: Speaker Adaptation of RNN-BLSTM
for Speec Recognition based on Speaker Code. Icassp 1, 5305–5309 (2016).
https://doi.org/10.1109/ICASSP.2016.7472690

7. Karaﬁ´at, M., Burget, L., Matˇejka, P., Glembek, O., ˇCernock´y, J.: iVector-based
discriminative adaptation for automatic speech recognition. 2011 IEEE Workshop
on Automatic Speech Recognition and Understanding, ASRU 2011, Proceedings
pp. 152–157 (2011). https://doi.org/10.1109/ASRU.2011.6163922

8. Parthasarathi, S.H.K., Hoﬀmeister, B., Matsoukas, S., Mandal, A., Strom, N.,
Garimella, S.: fmllr based feature-space speaker adaptation of dnn acoustic models.
In: INTERSPEECH. pp. 3630–3634. ISCA (2015)

9. Peddinti, V., Povey, D., Khudanpur, S.: A time delay neural network architecture
for eﬃcient modeling of long temporal contexts. Proceedings of the Annual Con-
ference of the International Speech Communication Association, INTERSPEECH
2015-Janua, 3214–3218 (2015)

12

J. Vanˇek, J. Mich´alek, J. Zelinka, J. Psutka

10. Rath, S.P., Povey, D., Vesel´y, K., ˇCernock´y, J.: Improved feature processing for

deep neural networks. In: INTERSPEECH. pp. 109–113. ISCA (2013)

11. Ravanelli, M., Brakel, P., Omologo, M., Bengio, Y., Kessler, F.B.: Improving speech
recognition by revising gated recurrent units. In: Interspeech2017. pp. 1308–1312
(2017). https://doi.org/10.21437/Interspeech.2017-775

12. Sak, H., Senior, A., Beaufays, F.: Long Short-Term Memory Based
for Large Vocabulary Speech
Interspeech 1, 338–342 (2014). https://doi.org/arXiv:1402.1128,

Recurrent Neural Network Architectures
Recognition.
http://arxiv.org/abs/1402.1128

13. Saon,
for
http://mazsola.iit.uni-miskolc.hu/{~}czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS141054.PDF

Unfolded
INTERSPEECH

H.:
Recognition.

Recurrent
1,

Networks
(2014),

Neural
343–347

G.,
Speech

Soltau,

14. Saon, G., Soltau, H., Nahamoo, D., Picheny, M.: Speaker adaptation of neural
network acoustic models using i-vectors. In: 2013 IEEE Workshop on Automatic
Speech Recognition and Understanding. pp. 55–59 (2013)

15. Seide, F., Chen, X., Yu, D.: Feature engineering in context-dependent deep neural

networks for conversational speech transcription. In: in ASRU (2011)

16. Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., Lang, K.J.: Phoneme
on
time-delay
neural
and Signal Processing 37(3),
1989).

IEEE Transactions

recognition
using
Acoustics, Speech,
https://doi.org/10.1109/29.21701

networks.

328–339

(Mar

17. Xue, S., Abdel-Hamid, O., Jiang, H., Dai, L., Liu, Q.: Fast adaptation
of deep neural network based on discriminant codes for speech recognition.
IEEE/ACM Transactions on Speech and Language Processing 22(12) (2014).
https://doi.org/10.1109/TASLP.2014.2346313

