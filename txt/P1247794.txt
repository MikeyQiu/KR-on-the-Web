0
2
0
2
 
b
e
F
 
4
1
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
2
0
0
.
1
1
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2020

PROGRESSIVE MEMORY BANKS FOR INCREMENTAL
DOMAIN ADAPTATION

Nabiha Asghar∗†◦ Lili Mou∗‡ Kira A. Selby†◦ Kevin D. Pantasdo◦ Pascal Poupart†◦ Xin Jiang♦
†Vector Institute for AI, Toronto, Canada
◦Cheriton School of Computer Science, University of Waterloo, Canada
{nasghar,kaselby,kevin.pantasdo,ppoupart}@uwaterloo.ca
‡Dept. Computing Science, University of Alberta; Alberta Machine Intelligence Institute (AMII)
doublepower.mou@gmail.com
♦Noah’s Ark Lab, Huawei Technologies, Hong Kong
jiang.xin@huawei.com

ABSTRACT

This paper addresses the problem of incremental domain adaptation (IDA) in natu-
ral language processing (NLP). We assume each domain comes one after another,
and that we could only access data in the current domain. The goal of IDA is to
build a uniﬁed model performing well on all the domains that we have encoun-
tered. We adopt the recurrent neural network (RNN) widely used in NLP, but
augment it with a directly parameterized memory bank, which is retrieved by an
attention mechanism at each step of RNN transition. The memory bank provides a
natural way of IDA: when adapting our model to a new domain, we progressively
add new slots to the memory bank, which increases the number of parameters,
and thus the model capacity. We learn the new memory slots and ﬁne-tune exist-
ing parameters by back-propagation. Experimental results show that our approach
achieves signiﬁcantly better performance than ﬁne-tuning alone. Compared with
expanding hidden states, our approach is more robust for old domains, shown by
both empirical and theoretical results. Our model also outperforms previous work
of IDA including elastic weight consolidation and progressive neural networks in
the experiments.1

1

INTRODUCTION

Domain adaptation aims to transfer knowledge from one domain (called the source domain) to
another (called the target domain) in a machine learning system.2 If the data of the target domain are
not large enough, using data from the source domain typically helps to improve model performance
in the target domain. This is important for neural networks, which are data-hungry and prone to
overﬁtting. In this paper, we especially focus on incremental domain adaptation (IDA)3, where we
assume different domains come sequentially one after another. We only have access to the data in
the current domain, but hope to build a uniﬁed model that performs well on all the domains that we
have encountered (Xu et al., 2014; Rusu et al., 2016; Kirkpatrick et al., 2017).

Incremental domain adaptation is useful in scenarios where data are proprietary, or available only for
a short period of time (Li & Hoiem, 2018). It is desired to preserve as much knowledge as possible
in the learned model and not to rely on the availability of the data. Another application of IDA
is a quick adaptation to new domains. If the environment of a deployed machine learning system
changes frequently, traditional methods like jointly training all domains require the learning machine
to be re-trained from scratch every time a new domain comes. Fine-tuning a neural network by a

∗ Equal contribution.
1Our IDA code is available at https://github.com/nabihach/IDA.
2In our work, the domain is deﬁned by datasets. Usually, the data from different genres or times typically

have different underlying distributions.

3In the literature, IDA is sometimes referred to as incremental, continual, or lifelong learning. We use

“incremental domain adaptation” in this paper to emphasize that our domain is not changed continuously.

1

Published as a conference paper at ICLR 2020

few steps of gradient updates does transfer quickly, but it suffers from the catastrophic forgetting
problem (Kirkpatrick et al., 2017). Suppose during prediction a data point is not labeled with its
domain, the (single) ﬁne-tuned model cannot predict well for samples in previous domains, as it
tends to “forget” quickly during ﬁne-tuning.

A recent trend of domain adaptation in the deep learning regime is the progressive neural net-
work (Rusu et al., 2016), which progressively grows the network capacity if a new domain comes.
Typically, this is done by enlarging the model with new hidden states and a new predictor (Fig-
ure 1a). To avoid interfering with existing knowledge, the newly added hidden states are not fed
back to the previously trained states. During training, all existing parameters are frozen, and only
the newly added ones are trained. For inference, the new predictor is used for all domains, which is
sometimes undesired as the new predictor is trained with only the last domain.

In this paper, we propose a progressive memory bank for incremental domain adaptation in natural
language processing (NLP). Our model augments a recurrent neural network (RNN) with a memory
bank, which is a set of distributed, real-valued vectors capturing domain knowledge. The memory
is retrieved by an attention mechanism during RNN information processing. When our model is
adapted to new domains, we progressively increase the slots in the memory bank. But different
from Rusu et al. (2016), we ﬁne-tune all the parameters, including RNN and the previous memory
bank. Empirically, when the model capacity increases, the RNN does not forget much even if
the entire network is ﬁne-tuned. Compared with expanding RNN hidden states, the newly added
memory slots cause less contamination of the existing knowledge in RNN states, as will be shown
by a theorem.

In our paper, we evaluate the proposed approach on a classiﬁcation task known as multi-genre nat-
ural language inference (MultiNLI). Appendix C provides additional evidence when our approach
is applied to text generation. Experiments consistently support our hypothesis that the proposed
approach adapts well to target domains without catastrophic forgetting of the source. Our model
outperforms the na¨ıve ﬁne-tuning method, the original progressive neural network, as well as other
IDA techniques including elastic weight consolidation (EWC, Kirkpatrick et al., 2017).

2 RELATED WORK

Domain Adaptation. Domain adaptation has been widely studied in machine learning, including
the NLP domain. For neural NLP applications, Mou et al. (2016) analyze two straightforward set-
tings, namely, multi-task learning (jointly training all domains) and ﬁne-tuning (training one domain
and ﬁne-tuning on the other). A recent advance of domain adaptation is adversarial learning, where
the neural features are trained not to classify the domain (Ganin et al., 2016; Liu et al., 2017). How-
ever, all these approaches (except ﬁne-tuning) require all domains to be available simultaneously,
and thus are not IDA approaches.

Kirkpatrick et al. (2017) address the catastrophic forgetting problem of ﬁne-tuning neural networks,
and propose a regularization term based on the Fisher information matrix; they call the method
elastic weight consolidation (EWC). While some follow-up studies report EWC achieves high per-
formance in their scenarios (Zenke et al., 2017; Lee et al., 2017; Thompson et al., 2019), others
show that EWC is less effective (Wen & Itti, 2018; Yoon et al., 2018; Wu et al., 2018). Lee et al.
(2017) propose incremental moment matching between the posteriors of the old model and the new
model, achieving similar performance to EWC. Schwarz et al. (2018) augment EWC with knowl-
edge distillation, making it more memory-efﬁcient.

Rusu et al. (2016) propose a progressive neural network that progressively increases the number
of hidden states (Figure 1a). To avoid overriding existing information, they freeze the weights of
the learned network, and do not feed new states to old ones. This results in multiple predictors,
requiring that a data sample is labeled with its domain during the test time. If we otherwise use
the last predictor to predict samples from all domains, its performance may be low for previous
domains, as the predictor is only trained with the last domain.

Yoon et al. (2018) propose an extension of the progressive network. They identify which existing
hidden units are relevant for the new task (with their sparse penalty), and ﬁne-tune only the corre-
sponding subnetwork. However, sparsity is not common for RNNs in NLP applications, as sparse
recurrent connections are harmful. A similar phenomenon is that dropout of recurrent connections

2

Published as a conference paper at ICLR 2020

(a) Progressive
Figure 1:
neural network Rusu et al.
(2016). (b) One step of RNN
transition in our progressive
memory network. Colors in-
dicate different domains.

yields poor performance (Bayer et al., 2013). Xu & Zhu (2018) deal with new domains by adap-
tively adding nodes to the network via reinforcement learning. This approach may require a very
large number of trials to identify the right number of nodes to be added to each layer (Yoon et al.,
2019).

Li & Hoiem (2018) address IDA with a knowledge distillation approach, where they preserve a set
of outputs of the old network on pseudo-training data. Then they jointly optimize for the accuracy
on the new training domain as well as the pseudo-training data. Kim et al. (2019)’s variant of
this approach uses maximum-entropy regularization to control the transfer of distilled knowledge.
However, in NLP applications, it is non-trivial to obtain pseudo-training data for distillation.

Memory-Based Neural Networks. Our work is related to memory-based neural networks.
Sukhbaatar et al. (2015) propose an end-to-end memory network that assigns each memory slot
to an entity, and aggregates information by multiple attention-based layers. They design their archi-
tecture for the bAbI question answering task, and assign a slot to each sentence. Such idea can be
extended to various scenarios, for example, assigning slots to external knowledge for question an-
swering (Das et al., 2017) and assigning slots to dialogue history for a conversation system (Madotto
et al., 2018).

A related idea is to use episodic memory, which stores data samples from all previously seen do-
mains (thus it is not an IDA approach). This is used for experience replay while training on subse-
quent domains (Lopez-Paz & Ranzato, 2017; Rebufﬁ et al., 2017; Chaudhry et al., 2018; d’Autume
et al., 2019).

Another type of memory in the neural network regime is the neural Turing machine (NTM, Graves
et al., 2016). This memory is not directly parameterized, but is read or written by a neural controller.
Therefore, it serves as temporary scratch paper and does not store knowledge itself. Zhang et al.
(2018b) combine the above two styles of memory for task-oriented dialogue systems, where they
have both slot-value memory and read-and-write memory.

Different from the work above, our memory bank stores knowledge in a distributed fashion, where
each slot does not correspond to a concrete entity or data sample. Our memory is directly parame-
terized, interacting in a different way from RNN weights and providing a natural way of incremental
domain adaptation.

3 PROPOSED APPROACH

Our model is based on a recurrent neural network (RNN). At each step, the RNN takes the embed-
ding of the current input (e.g., a word), and changes its states accordingly. This is represented by
hi = RNN(hi−1, xi), where hi and hi−1 are the hidden states at time steps i and i−1, respectively.
xi is the input at the ith step. Typically, long short term memory (LSTM, Hochreiter & Schmidhu-
ber, 1997) or Gated Recurrent Units (GRU, Cho et al., 2014) are used as RNN transitions. In the
rest of this section, we will describe a memory-augmented RNN, and how it is used for incremental
domain adaptation (IDA).

3

Published as a conference paper at ICLR 2020

3.1 AUGMENTING RNN WITH MEMORY BANKS

We enhance the RNN with an external memory bank, as shown in Figure 1b. The memory bank
augments the overall model capacity by storing additional parameters in memory slots. At each
time step, our model computes an attention probability to retrieve memory content, which is then
fed to the computation of RNN transition.

Particularly, we adopt a key-value memory bank, inspired by Miller et al. (2016). Each memory slot
contains a key vector and a value vector. The former is used to compute the attention weight for
memory retrieval, whereas the latter is the value of memory content.

For the ith step, the memory mechanism computes an attention probability αi by

(cid:101)αi,j = exp{h(cid:62)

i−1m(key)

j

}, αi,j =

(cid:101)αi,j
j(cid:48)=1 (cid:101)αi,j(cid:48)

(cid:80)N

where m(key)
is the key vector of the jth slot of the memory (among N slots in total). Then the
model retrieves memory content by a weighted sum of all memory values, where the weight is the
attention probability, given by

j

(1)

(2)

(cid:88)N

ci =

j=1

αi,jm(val)

j

Here, m(val)
concatenated with the current word xi, and fed to the RNN as input for state transition.

is the value vector of the jth memory slot. We call ci the memory content. Then, ci is

j

Using the key-value memory bank allows separate (thus more ﬂexible) computation of memory
retrieval weights and memory content, compared with traditional attention where a candidate vector
is used to compute both attention probability and attention content.

It should be emphasized that the memory bank in our model captures distributed knowledge, which
is different from other work where the memory slots correspond to speciﬁc entities (Eric et al.,
2017). The attention mechanism accomplishes memory retrieval in a “soft” manner, which means
the retrieval strength is a real-valued probability. This enables us to train both memory content and
its retrieval end-to-end, along with other neural parameters.

We would also like to point out that the memory bank alone does not help RNN much. However, it
is natural to use a memory-augmented RNN for incremental domain adaptation, as described below.

3.2 PROGRESSIVELY INCREASING MEMORY FOR INCREMENTAL DOMAIN ADAPTATION

The memory bank in Subsection 3.1 can be progressively expanded to adapt a model in a source
domain to new domains. This is done by adding new memory slots to the bank which are learned
exclusively from the target data.

Suppose the memory bank is expanded with another M slots in a new domain, in addition to previous
N slots. We then have N + M slots in total. The model computes attention probability over the
expanded memory and obtains the attention vector in the same way as Equations (1)–(2), except that
the summation is computed from 1 to N + M . This is given by

α(expand)
i,j

=

(cid:101)αi,j
(cid:80)N +M
j(cid:48)=1 (cid:101)αi,j(cid:48)

,

c(expand)
i

=

(cid:88)N +M
j=1

α(expand)
i,j m(val)

j

(3)

To initialize the expanded model, we load all previous parameters, including RNN weights and the
learned N slots, but randomly initialize the progressively expanded M slots. During training, we
update all parameters by gradient descent. That is to say, new parameters are learned from their
initializations, whereas old parameters are ﬁne-tuned during IDA. The process is applied whenever
a new domain comes, as shown in Algorithm 1.

We would like to discuss the following issues.

4

Published as a conference paper at ICLR 2020

Freezing vs. Fine-tuning learned parame-
ters. Inspired by the progressive neural net-
work (Rusu et al., 2016), we ﬁnd it tempt-
ing to freeze RNN parameters and the learned
memory but only tune new memory for IDA.
However, our preliminary results show that
if we freeze all existing parameters, the in-
creased memory does not add much to the
model capacity, and that its performance is
worse than ﬁne-tuning all parameters.

Algorithm 1: Progressive Memory for IDA
Input: A sequence of domains D0, D1, · · · , Dn
Output: A single model for all domains
Initialize a memory-augmented RNN
Train the model on D0
for D1, · · · , Dn do

Expand the memory with new slots
Load RNN weights and existing memory banks
Train the model by updating all parameters

end
Return: The resulting model

Fine-tuning vs. Fine-tuning while increas-
ing memory slots.
It is reported that ﬁne-
tuning a model (without increasing model ca-
pacity) suffers from the problem of catastrophic forgetting (Kirkpatrick et al., 2017). We wish to
investigate whether our approach suffers from the same problem, since we ﬁne-tune learned param-
eters when progressively increasing memory slots. Our intuition is that the increased model capacity
helps to learn the new domain with less overriding of the previously learned model. Experiments
conﬁrm our conjecture, as the memory-augmented RNN tends to forget more if the memory size is
not increased.

Expanding hidden states vs. Expanding memory. Another way of progressively increasing model
capacity is to expand the size of RNN layers. This setting is similar to the progressive neural net-
work, except that all weights are ﬁne-tuned and new states are connected to existing states.

However, we hereby show a theorem, indicating that the expanded memory results in less contami-
nation/overriding of the learned knowledge in the RNN, compared with the expanded hidden states.
The main idea is to measure the effect of model expansion quantitatively by the expected square dif-
ference on hi before and after expansion, where the expectation reﬂects the average effect of model
expansion in different scenarios.
Theorem 1. Let RNN have vanilla transition with the linear activation function, and let the RNN
state at the last step hi−1 be ﬁxed. For a particular data point, if the memory attention satisﬁes
(cid:80)N +M
j=1 (cid:101)αi,j, then memory expansion yields a lower expected mean squared differ-

j=N +1 (cid:101)αi,j ≤ (cid:80)N

ence in hi than RNN state expansion. That is,

(cid:104)

i − hi(cid:107)2(cid:105)
i − hi(cid:107)2(cid:105)
refers to the hidden states if the memory is expanded. h(s)
i

(cid:107)h(m)

(cid:107)h(s)

≤ E

E

(cid:104)

i

where h(m)
refers to the original dimen-
sions of the RNN states, if we expand the size of RNN states themselves. Here, we compute the
expectation by assuming weights and hidden states are iid from a zero-mean Gaussian distribution
(with variance σ2).

(4)

Proof Sketch: We focus on one step of RNN transition and assume that hi−1 is the same when the
model capacity is increased. Further, we assume that hi is D-dimensional, that each memory slot
mj is d-dimensional, and that the additional RNN units (when we expand the hidden state) are also
d-dimensional.

We compute how the original dimensions in the hidden state are changed if we expand RNN. We
denote the expanded hidden states by (cid:101)hi−1 and (cid:102)hi for the two time steps. We denote the weights
connecting from (cid:101)hi−1 to hi by (cid:102)W ∈ RD×d. We focus on the original D-dimensional space, denoted
as h(s)
i

. The connection is shown in Figure 2a, Appendix A. We have

E(cid:2)(cid:107)h(s)

i − hi(cid:107)2(cid:3) = E(cid:2)(cid:107)(cid:102)W · (cid:101)hi−1(cid:107)2(cid:3) =

D
(cid:88)

(cid:20)(cid:16)

E

j=1

(cid:17)2(cid:21)

(cid:101)w(cid:62)

j (cid:101)hi−1

D
(cid:88)

E

(cid:34)(cid:18) d
(cid:88)

=

j=1

k=1

(cid:101)wjk(cid:101)hi−1[k])

(cid:19)2(cid:35)

D
(cid:88)

d
(cid:88)

(cid:104)(cid:0)

E

=

j=1

i=1

= D · d · Var(cid:0)w(cid:1) · Var(h) = Ddσ2σ2

(cid:1)2(cid:105)

E

(cid:101)hi−1[k](cid:1)2(cid:105)
(cid:104)(cid:0)

(cid:101)wjk

(5)

(6)

(7)

5

Published as a conference paper at ICLR 2020

Table 1: Corpus statistics and the baseline
performance (% accuracy) of our BiLSTM
model (without domain adaptation) and re-
sults reported in previous work.

Fic Gov Slate Tel Travel
77k 77k 77k 83k
# training samples
Our Implementation 65.0 66.5 56.2 64.5
64.7 69.2 57.9 64.4

Yu et al. (2018)

77k
62.7
65.8

Table 2: Results on two-domain adapta-
tion. F: Fine-tuning. V: Expanding vocab-
ulary. H: Expanding RNN hidden states. M:
Our proposed method of expanding memory.
We also compare with previous work elas-
tic weight consolidation (EWC, Kirkpatrick
et al., 2017) and the progressive neural net-
work (Rusu et al., 2016). For the statistical
test (compared with Line 8), ↑, ↓: p < 0.05
and ⇑, ⇓: p < 0.01. The absence of an ar-
row indicates that the performance difference
compared with Line 8 is statistically insignif-
icant with p lower than 0.05.

#Line Model Trained on/by

1
2
3
4
5
6
7
8
9
10
11
12

RNN

+
N
N
R

e

S
T
m S
T
M
S+T
S→T (F)
S→T (F+M)
S→T (F+M+V)
S→T (F+H)
S→T (F+H+V)
S→T (EWC)
S→T (Progressive)

m
e
M
+
N
N
R

% Accuracy on
T
61.23⇓
66.49⇓
60.87⇓
67.01⇓
70.00
69.90↓
70.21
70.82
68.35⇓
68.02⇓
64.10⇓
68.25⇓

S
65.01⇓
56.46⇓
65.41⇓
56.77⇓
66.02↓
65.62↓
66.23
67.55
64.09⇓
63.68⇓
66.02⇓
64.47⇓

Similarly,

E(cid:2)(cid:107)h(m)

= Ddσ2Var(cid:0)∆ck

i − hi(cid:107)2(cid:3) = E

(cid:13)W(c)∆c(cid:107)2(cid:105)
(cid:104)(cid:13)
where ∆c def= c(cid:48) − c. The vectors c and c(cid:48) are the current step’s attention content before and after
memory expansion, respectively, shown in Figure 2b, Appendix A. (We omit the time step in the
notation for simplicity.) W(c) is the weight matrix connecting attention content to RNN states.
To prove the theorem, it remains to show that Var(∆ck) ≤ σ2. We do this by analyzing how
attention is computed at each time step, and bounding each attention weight. For details, see the
complete proof in Appendix A.

(8)

(cid:1)

In the theorem, we have an assumption (cid:80)N +M
j=N +1 (cid:101)αi,j ≤ (cid:80)N
j=1 (cid:101)αi,j, requiring that the total attention
to existing memory slots is larger than to the progressively added slots. This is fairly reasonable
because: (1) During training, attention is trained in an ad hoc fashion to newly-added information,
and thus some αi,j for 1 ≤ j ≤ N might be learned so that it is larger than a random memory slot;
and (2) For a new domain, we do not add a huge number of slots, and thus (cid:80)N +M
j=N +1 (cid:101)αi,j will not
dominate.

It is noted that our theorem is not to provide an explicit optimization/generalization bound for IDA,
but shows that expanding memory is more stable than expanding hidden states. This is particu-
larly important at the beginning steps of IDA, as the progressively growing parameters are randomly
initialized and are basically noise. Although our theoretical analysis uses a restricted setting (i.e.,
vanilla RNN transition and linear activation), it provides the key insight that our approach is appro-
priate for IDA.

4 EXPERIMENTS

In this section, we evaluate our approach on an NLP classiﬁcation task. In particular, we choose the
multi-genre natural language inference (MultiNLI), due to its large number of samples in various
domains. The task is to determine the relationship between two sentences among target labels:
entailment, contradiction, and neutral. In Appendix C, we conduct supplementary experiments on
text generation with our memory-augmented RNN for IDA.

Dataset and Setup. The MultiNLI corpus (Williams et al., 2018) is particularly suitable for IDA,
as it contains training samples for 5 genres: Slate, Fiction (Fic), Telephone (Tel),
Government (Gov), and Travel. In total, we have 390k training samples. The corpus also
contains held-out (non-training) labeled data in these domains. We split it into two parts for valida-
tion and test.4

4MultiNLI also contains 5 genres without training samples, namely, 9/11, Face-to-face, Letters,
OUP, and Verbatim. We ignore these genres, because we focus on incremental domain adaptation instead of

6

Published as a conference paper at ICLR 2020

Training domains
Fic
Fic → Gov
Fic → Gov → Slate
Fic → Gov → Slate → Tel
Fic → Gov → Slate → Tel → Travel

Fic
65.41
67.55
67.04
68.46
69.36

Performance on
Slate
55.83
61.04
63.29
63.39
63.96

Tel
61.39
65.07
64.66
71.60
69.74

Gov
58.87
70.82
71.55
71.10
72.47

Travel
57.35
61.90
63.53
61.50
68.39

Table 3: Dynamics of the progressive memory network for IDA with 5 domains. Upper-triangular
values in gray are out-of-domain (zero-shot) performance.

Group

Non-
IDA

IDA

Setting
In-domain training
Fic + Gov + Slate + Tel + Travel (multi-task)
Fic → Gov → Slate → Tel → Travel (F+V)
Fic → Gov → Slate → Tel → Travel (F+V+M)
Fic → Gov → Slate → Tel → Travel (EWC)
Fic → Gov → Slate → Tel → Travel (Progressive)

Fic
65.41⇓
70.60↑
67.24↓
69.36
67.12⇓
65.22⇓

Gov
67.01⇓
73.30
70.82⇓
72.47
68.71⇓
67.87⇓

Slate
59.30⇓
63.80
62.41↓
63.96
59.90⇓
61.13⇓

Tel
67.20⇓
69.15
67.62↓
69.74
66.09⇓
66.96⇓

Travel
64.70⇓
67.07↓
68.39
68.39
65.70⇓
67.90

Table 4: Comparing our approach with variants and previous work in the multi-domain setting. In
this experiment, we use the memory-augmented RNN as the neural architecture. Italics represent
best results in the IDA group. ↑, ↓: p < 0.05 and ⇑, ⇓: p < 0.01 (compared with F+V+M).

The ﬁrst row in Table 1 shows the size of the training set in each domain. As seen, the corpus is
mostly balanced across domains, although Tel has slightly more examples.

For the base model, we train a bi-directional LSTM (BiLSTM). The details of network architecture,
training, and hyper-parameter tuning are given in Appendix B. We see in Table 1 that we achieve
similar performance to Yu et al. (2018). Furthermore, our BiLSTM achieves an accuracy of 68.37
on the ofﬁcial MultiNLI test set,5 which is better than 67.51 reported in the original MultiNLI paper
(Williams et al., 2018) using BiLSTM. This shows that our implementation and tuning are fair for
the basic BiLSTM, and that our model is ready for the study of IDA.

Transfer between Two Domains. We would like to compare our approach with a large number
of baselines and variants. Thus, we randomly choose two domains as a testbed: Fic as the source
domain and Gov as the target domain. We show results in Table 2.

First, we analyze the performance of RNN and the memory-augmented RNN in the non-transfer
setting (Lines 1–2 vs. Lines 3–4). As seen, the memory-augmented RNN achieves slightly better
but generally similar performance, compared with RNN (both with LSTM units). This shows that,
in the non-transfer setting, the memory bank does not help the RNN much. However, this later
conﬁrms that the performance improvement is indeed due to our IDA technique, instead of simply a
better neural architecture.

We then apply two straightforward methods of domain adaptation: multi-task learning (Line 5) and
ﬁne-tuning (Line 6). Multi-task learning jointly optimizes source and target objectives, denoted by
“S+T.” On the other hand, the ﬁne-tuning approach trains the model on the source ﬁrst, and then ﬁne-
tunes on the target. In our experiments, these two methods perform similarly on the target domain,
which is consistent with Mou et al. (2016). On the source domain, ﬁne-tuning performs signiﬁcantly
worse than multi-task learning, as it suffers from the catastrophic forgetting problem. We notice that,
in terms of source performance, the ﬁne-tuning approach (Line 6) is slightly better than trained on
the source domain only (Line 3). This is probably because our domains are somewhat correlated as
opposed to Kirkpatrick et al. (2017), and thus training with more data on target slightly improves
the performance on source. However, ﬁne-tuning does achieve the worst performance on source
compared with other domain adaptation approaches (among Lines 5–8). Thus, we nevertheless use
the terminology “catastrophic forgetting,” and our research goal is still to improve IDA performance.

The main results of our approach are Lines 7 and 8. We apply the proposed progressive memory
network to IDA and we ﬁne-tune all weights. We see that on both source and target domains,

zero-shot learning. Also, the labels for the ofﬁcial test set of MultiNLI are not publicly available, and therefore
we cannot use it to evaluate performance on individual domains. Our split of the held-out set for validation and
test applies to all competing methods, and thus is a fair setting.

5Evaluation on the ofﬁcial MultiNLI test set requires submission to Kaggle.

7

Published as a conference paper at ICLR 2020

our approach outperforms the ﬁne-tuning method alone where the memory size is not increased
(comparing Lines 7 and 6). This veriﬁes our conjecture that, if the model capacity is increased, the
new domain results in less overriding of the learned knowledge in the neural network. Our proposed
approach is also “orthogonal” to the expansion of the vocabulary size, where target-speciﬁc words
are randomly initialized and learned on the target domain. As seen, this combines well with our
memory expansion and yields the best performance on both source and target (Line 8).

We now compare an alternative way of increasing model capacity, i.e., expanding hidden states
(Lines 9 and 10). For fair comparison, we ensure that the total number of model parameters after
memory expansion is equal to the number of model parameters after hidden state expansion. We
see that the performance of hidden state expansion is poor especially on the source domain, even
if we ﬁne-tune all parameters. This experiment provides empirical evidence to our theorem that
expanding memory is more robust than expanding hidden states.

We also compare the results with previous work on IDA. We re-implement6 EWC (Kirkpatrick
et al., 2017). It does not achieve satisfactory results in our application. We investigate other pub-
lished papers using the same method and ﬁnd inconsistent results: EWC works well in some ap-
plications (Zenke et al., 2017; Lee et al., 2017) but performs poorly on others (Yoon et al., 2018;
Wu et al., 2018). Wen & Itti (2018) even report near random performance with EWC. We also
re-implement the progressive neural network (Rusu et al., 2016). We use the target predictor to do
inference for both source and target domains. Progressive neural network yields low performance,
particularly on source, probably because the predictor is trained with only the target domain.

We measure the statistical signiﬁcance of the results against Line 8 with the one-tailed Wilcoxon’s
signed-rank test (Wilcoxon, 1945), by bootstrapping a subset of 200 samples for 10 times with
replacement. The test shows our approach is signiﬁcantly better than others, on both source and
target domains.

IDA with All Domains. Having analyzed our approach, baselines, and variants on two domains in
detail, we are now ready to test the performance of IDA with multiple domains, namely, Fic, Gov,
Slate, Tel, and Travel. In this experiment, we assume these domains come one after another,
and our goal is to achieve high performance on all domains.

Table 3 shows the dynamics of IDA with our progressive memory network. Comparing the upper-
triangular values (in gray, showing out-of-domain performance) with diagonal values, we see that
our approach can be quickly adapted to the new domain in an incremental fashion. Comparing lower-
triangular values with the diagonal, we see that our approach does not suffer from the catastrophic
forgetting problem as the performance of previous domains is gradually increasing if trained with
more domains. After all data are observed, our model achieves the best performance in most domains
(last row in Table 3), despite the incremental nature of our approach.

We now compare our approach with other baselines and variants in the multi-domain setting, shown
in Table 4. Due to the large number of settings, we only choose a selected subset of variants from
Table 2 for the comparison.

As seen, our approach of progressively growing memory achieves the same performance as ﬁne-
tuning on the last domain (both with vocabulary expansion), but for all previous 4 domains, we
achieve signiﬁcantly better performance than ﬁne-tuning. Our model is comparable to multi-task
learning on all domains. It should also be mentioned that multi-task learning requires training the
model when data from all domains are available simultaneously. It is not an incremental approach
for domain adaptation, and thus cannot be applied to the scenarios introduced in Section 1. We
include this setting mainly because we are curious about the performance of non-incremental domain
adaptation.

We also compare with previous methods for IDA in Table 4. Our method outperforms EWC and the
progressive neural network in all domains; the results are consistent with Table 2.

6Implementation based on https://github.com/ariseff/overcoming-catastrophic

8

Published as a conference paper at ICLR 2020

5 CONCLUSION

In this paper, we propose a progressive memory network for incremental domain adaptation (IDA).
We augment an RNN with an attention-based memory bank. During IDA, we add new slots to the
memory bank and tune all parameters by back-propagation. Empirically, the progressive memory
network does not suffer from the catastrophic forgetting problem as in na¨ıve ﬁne-tuning. Our intu-
ition is that the new memory slots increase the neural network’s model capacity, and thus, the new
knowledge causes signiﬁcantly less overriding of the existing network. Compared with expanding
hidden states, our progressive memory bank provides a more robust way of increasing model ca-
pacity, shown by both a theorem and experiments. We also outperform previous work for IDA,
including elastic weight consolidation (EWC) and the original progressive neural network.

This work was funded by Huawei Technologies, Hong Kong. Lili Mou is supported by the Amii
Fellow Program, and the CIFAR AI Chair Program.

ACKNOWLEDGMENTS

REFERENCES

Justin Bayer, Christian Osendorfer, Daniela Korhammer, Nutan Chen, Sebastian Urban, and Patrick
van der Smagt. On fast dropout and its applicability to recurrent networks. arXiv preprint
arXiv:1311.0701, 2013.

Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient

lifelong learning with A-GEM. arXiv preprint arXiv:1812.00420, 2018.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder–decoder
for statistical machine translation. In EMNLP, pp. 1724–1734, 2014.

Cristian Danescu-Niculescu-Mizil and Lillian Lee. Chameleons in imagined conversations: A new
approach to understanding coordination of linguistic style in dialogs. In Proc. Workshop on Cog-
nitive Modeling and Computational Linguistics, pp. 76–87, 2011.

Rajarshi Das, Manzil Zaheer, Siva Reddy, and Andrew McCallum. Question answering on knowl-
edge bases and text using universal schema and memory networks. In ACL, pp. 358–365, 2017.

Cyprien de Masson d’Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic

memory in lifelong language learning. arXiv preprint arXiv:1906.01076, 2019.

Mihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. Key-value retrieval

networks for task-oriented dialogue. In SIGDIAL, pp. 37–49, 2017.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. JMLR, 17(1):2096–2030, 2016.

Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwi´nska, Sergio G´omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538
(7626):471–476, 2016.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):

1735–1780, 1997.

Dahyun Kim, Jihwan Bae, Yeonsik Jo, and Jonghyun Choi. Incremental learning with maximum en-
tropy regularization: Rethinking forgetting and intransigence. arXiv preprint arXiv:1902.00829,
2019.

9

Published as a conference paper at ICLR 2020

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences,
114(13):3521–3526, 2017.

Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming
catastrophic forgetting by incremental moment matching. In NeurIPS, pp. 4652–4662, 2017.

Zhizhong Li and Derek Hoiem. Learning without forgetting.

IEEE TPAMI, 40(12):2935–2947,

2018.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.
How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics
for dialogue response generation. In EMNLP, pp. 2122–2132, 2016.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task learning for text classiﬁca-

tion. In ACL, pp. 1–10, 2017.

NeurIPS, pp. 6467–6476, 2017.

David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. The Ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dialogue systems. In SIGDIAL, pp. 285–294, 2015.

Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. Mem2seq: Effectively incorporating knowl-

edge bases into end-to-end task-oriented dialog systems. In ACL, pp. 1468–1478, 2018.

Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason
In EMNLP, pp. 1400–

Weston. Key-value memory networks for directly reading documents.
1409, 2016.

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. How transferable are neural

networks in NLP applications? In EMNLP, pp. 479–489, 2016.

Jeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global vectors for word

representation. In EMNLP, pp. 1532–1543, 2014.

Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.

icarl:

Incremental classiﬁer and representation learning. In CVPR, pp. 2001–2010, 2017.

Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016.

Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye
Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for contin-
ual learning. In ICML, pp. 4535–4544, 2018.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C
Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for gen-
erating dialogues. In AAAI, pp. 3295–3301, 2017.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In NIPS, pp.

2440–2448, 2015.

In NIPS, pp. 3104–3112, 2014.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.

Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. Overcoming
catastrophic forgetting during domain adaptation of neural machine translation. In NAACL, pp.
2062–2068, 2019.

Shixian Wen and Laurent Itti. Overcoming catastrophic forgetting problem by weight consolidation

and long-term memory. arXiv preprint arXiv:1805.07441, 2018.

10

Published as a conference paper at ICLR 2020

Frank Wilcoxon.

Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80–83,

1945.

Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-

tence understanding through inference. In NAACL-HLT, pp. 1112–1122, 2018.

Chenshen Wu, Luis Herranz, Xialei Liu, Yaxing Wang, Joost van de Weijer, and Bogdan Raducanu.
Memory replay GANs: learning to generate images from new categories without forgetting. arXiv
preprint arXiv:1809.02058, 2018.

Jiaolong Xu, Sebastian Ramos, David V´azquez, Antonio M L´opez, and D Ponsa.

Incremental

domain adaptation of deformable part-based models. In BMVC, 2014.

Ju Xu and Zhanxing Zhu. Reinforced continual learning. In NeurIPS, pp. 899–908. 2018.

Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically

expandable networks. ICLR, 2018.

Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Oracle: Order robust adaptive

continual learning. arXiv preprint arXiv:1902.09432, 2019.

Jianfei Yu, Minghui Qiu, Jing Jiang, Jun Huang, Shuangyong Song, Wei Chu, and Haiqing Chen.
Modelling domain relationships for transfer learning on retrieval-based question answering sys-
tems in e-commerce. In WSDM, pp. 682–690, 2018.

Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.

In ICML, pp. 3987–3995, 2017.

Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, Xiujun Li, Chris Brockett, and Bill Dolan.
Generating informative and diverse conversational responses via adversarial information maxi-
mization. arXiv preprint arXiv:1809.05972, 2018a.

Zheng Zhang, Minlie Huang, Zhongzhou Zhao, Feng Ji, Haiqing Chen, and Xiaoyan Zhu.
Memory-augmented dialogue management for task-oriented dialogue systems. arXiv preprint
arXiv:1805.00150, 2018b.

11

Published as a conference paper at ICLR 2020

Figure 2: Hidden state expansion vs. memory expansion at step t.

A PROOF OF THEOREM 1

Theorem 1. Let RNN have vanilla transition with the linear activation function, and let the RNN
state at the last step hi−1 be ﬁxed. For a particular data point, if the memory attention satisﬁes
(cid:80)N +M
j=1 (cid:101)αi,j, then memory expansion yields a lower expected mean squared differ-

j=N +1 (cid:101)αi,j ≤ (cid:80)N

ence in hi than RNN state expansion. That is,

(cid:104)

i − hi(cid:107)2(cid:105)
i − hi(cid:107)2(cid:105)
refers to the hidden states if the memory is expanded. h(s)
i

(cid:107)h(m)

(cid:107)h(s)

≤ E

E

(cid:104)

i

where h(m)
refers to the original dimen-
sions of the RNN states, if we expand the size of RNN states themselves. Here, we compute the
expectation by assuming weights and hidden states are iid from a zero-mean Gaussian distribution
(with variance σ2).

(9)

Proof: Let hi−1 be the hidden state of the last step. We focus on one step of transition and assume
that hi−1 is the same when the model capacity is increased. We consider a simpliﬁed case where
the RNN has vanilla transition with the linear activation function. We measure the effect of model
expansion quantitatively by the expected square difference on hi before and after model expansion.

Suppose the original hidden state hi is D-dimensional. We assume each memory slot is d-
dimensional, and that the additional RNN units when expanding the hidden state are also d-
dimensional. We further assume each variable in the expanded memory and expanded weights ((cid:102)W
in Figure 2) are iid with zero mean and variance σ2. This assumption is reasonable as it enables a fair
comparison of expanding memory and expanding hidden states. Finally, we assume every variable
in the learned memory slots, i.e., mjk, follows the same distribution (zero mean, variance σ2).

We compute how the original dimensions in the hidden state are changed if we expand RNN. We
denote the expanded hidden states by (cid:101)hi−1 and (cid:101)hi for the two time steps. We denote the weights
connecting from (cid:101)hi−1 to hi by (cid:102)W ∈ RD×d. We focus on the original D-dimensional space, denoted
as h(s)
i

. The connection is shown in Figure 2a. We have

E(cid:2)(cid:107)h(s)

i − hi(cid:107)2(cid:3)

= E(cid:2)(cid:107)(cid:102)W · (cid:101)hi−1(cid:107)2(cid:3)
(cid:16)

(cid:20) D
(cid:88)

= E

(cid:101)w(cid:62)

j (cid:101)hi−1

j=1
(cid:20)(cid:16)

E

=

D
(cid:88)

j=1

(cid:101)w(cid:62)

j (cid:101)hi−1

(cid:17)2(cid:21)

(cid:17)2(cid:21)

12

(10)

(11)

(12)

Published as a conference paper at ICLR 2020

D
(cid:88)

E

(cid:34)(cid:18) d
(cid:88)

j=1

k=1

(cid:19)2(cid:35)

(cid:101)wjk(cid:101)hi−1[k])

(cid:104)(cid:0)

E

(cid:101)wjk(cid:101)hi−1[k](cid:1)2(cid:105)

=

=

=

D
(cid:88)

d
(cid:88)

j=1

k=1

D
(cid:88)

d
(cid:88)

j=1

k=1

(cid:104)(cid:0)

E

(cid:101)wjk

(cid:1)2(cid:105)

E

(cid:104)(cid:0)
(cid:101)hi−1[k](cid:1)2(cid:105)

= D · d · Var(cid:0)w(cid:1) · Var(h)
= Ddσ2σ2

where (14) is due to the independence and zero-mean assumptions of every element in (cid:102)W and hi−1.
(15) is due to the independence assumption between (cid:102)W and hi−1.
Next, we compute the effect of expanding memory slots. Notice that (cid:107)h(m)
i − hi(cid:107) = W(c)∆c. Here,
is the RNN hidden state after memory expansion. ∆c def= c(cid:48) −c, where c and c(cid:48) are the attention
h(m)
i
content vectors before and after memory expansion, respectively, at the current time step.7 W(c) is
the weight matrix connecting attention content to RNN states. The connection is shown in Figure 2b.
Reusing the result of (16), we immediately obtain

E(cid:2)(cid:107)h(m)

i − hi(cid:107)2(cid:3)

(cid:13)W(c)∆c(cid:107)2(cid:105)
(cid:104)(cid:13)
= E
(cid:1)
= Ddσ2Var(cid:0)∆ck

where ∆ck is an element of the vector ∆c.
To prove Equation (2), it remains to show that Var(∆ck) ≤ σ2. We now analyze how attention is
computed.
Let (cid:101)α1, · · · , (cid:101)αN +M be the unnormalized attention weights over the N +M memory slots. We notice
that (cid:101)α1, · · · , (cid:101)αN remain the same after memory expansion. Then, the original attention probability
is given by αj = (cid:101)αj/((cid:101)α1 + · · · + (cid:101)αN ) for j = 1, · · · , N . After memory expansion, the attention
probability becomes α(cid:48)

j = (cid:101)αj/((cid:101)α1 + · · · + (cid:101)αN +M ), illustrated in Figure 3. We have

(cid:19)

mj +

N +M
(cid:88)

(cid:16)

j=N +1

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

(cid:17)

mj

(cid:16)

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

(cid:17)

mj

∆c = c(cid:48) − c

N
(cid:88)

j=1

(α(cid:48)

j − αj)mj +

α(cid:48)

jmj

N +M
(cid:88)

j=N +1

N
(cid:88)

(cid:18)

j=1

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

−

N
(cid:88)

(cid:18) −(cid:101)αj

(cid:101)αN +1+···+ (cid:101)αN +M
(cid:101)α1+···+ (cid:101)αN

(cid:101)α1 + · · · + (cid:101)αN +M

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN
(cid:19)

N +M
(cid:88)

mj +

j=N +1

=

=

=

def=

j=1

N +M
(cid:88)

j=1

βjmj

where

βj

def=





(cid:101)αN +1+···+ (cid:101)αN +M
(cid:101)α1+···+ (cid:101)αN

−(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M
(cid:101)αj
,
(cid:101)α1 + · · · + (cid:101)αN +M

,

if 1 ≤ j ≤ N

if N +1 ≤ j ≤ N + M

7We omit the time step in the notation for simplicity.

13

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

Published as a conference paper at ICLR 2020

Figure 3: Attention probabilities before and after memory expansion.

By our assumption of total attention (cid:80)N +M
|βj| ≤ |α(cid:48)

j|,

j=N +1 (cid:101)αj ≤ (cid:80)N

j=1 (cid:101)αj, we have

∀1 ≤ j ≤ N + M

Then, we have

Var(∆ck) = E[(c(cid:48)

k − ck)2(cid:3) ∀1 ≤ k ≤ d

E(cid:2)(cid:107)c(cid:48) − c(cid:107)2(cid:3)
(cid:34) d

(cid:88)

(cid:18) N +M
(cid:88)

E

k=1

j=1
(cid:34)(cid:18) N +M
(cid:88)

d
(cid:88)

E

k=1

j=1

(cid:19)2(cid:35)

(cid:19)2(cid:35)

βjmjk

βjmjk

(cid:104)(cid:0)βjmjk
E

(cid:1)2(cid:105)

E(cid:2)β2

(cid:3)E(cid:2)m2

j

jk

(cid:3)

=

=

=

=

=

=

1
d

1
d

1
d

1
d

1
d

1
d

d
(cid:88)

N +M
(cid:88)

k=1

j=1

d
(cid:88)

N +M
(cid:88)

k=1

j=1

d
(cid:88)

N +M
(cid:88)

k=1


j=1

N +M
(cid:88)







j=1

N +M
(cid:88)

j=1

= σ2E

≤ σ2E

E[β2

j ]σ2



β2
j





(α(cid:48)

j)2



(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)
Here, (31) is due to the assumption that mjk is independent and zero-mean, and (32) is due to the
independence assumption between βj and mjk. To obtain (36), we notice that (cid:80)N +M
j = 1 with
0 ≤ α(cid:48)

j ≤ 1 (∀1 ≤ j ≤ N + M ). Thus, (cid:80)N +M

j)2 ≤ 1, concluding our proof.

j=1 α(cid:48)

(α(cid:48)

j=1

≤ σ2

B HYPERPARAMETERS

We choose the base model and most of its settings by following the original MultiNLI paper
(Williams et al., 2018): 300D RNN hidden states, 300D pretrained GloVe embeddings (Penning-
ton et al., 2014) for initialization, batch size of 32, and the Adam optimizer for training. The initial

14

Published as a conference paper at ICLR 2020

S
n
o

.
c
c
A
n
o
i
t
a
d
i
l
a
V

68

67

66

65

1

71

70.5

70

T
n
o

.
c
c
A
n
o
i
t
a
d
i
l
a
V

69.5

1

100

200

300

400

500

100

200

300

400

500

# of Memory Slots

# of Memory Slots

(a)

(b)

Figure 4: Tuning the number of memory slots to be added per domain in the MultiNLI experiment.
The two graphs show validation performance of our IDA model S→T (F+M+V).

learning rate for Adam is tuned over the set {0.3, 0.03, 0.003, 0.0003, 0.00003}. It is set to 0.0003
based on validation performance.

For the memory, we set each slot to be 300-dimensioonal, which is the same as the RNN and em-
bedding size.

We tune the number of progressive memory slots in Figure 4, which shows the validation perfor-
mance on the source (Fic) and target (Gov) domains. We see that the performance is close to
ﬁne-tuning alone if only one memory slot is added. It improves quickly between 1 and 200 slots,
and tapers off around 500. We thus choose to add 500 slots for each domain.

C ADDITIONAL EXPERIMENT ON DIALOGUE GENERATION

We further evaluate our approach on the task of dialogue response generation. Given an input
text sequence, the task is to generate an appropriate output text sequence as a response in human-
computer dialogue. This supplementary experiment provides additional evidence of our approach in
generation tasks.

Datasets, Setup, and Metrics. We use the Cornell Movie Dialogs Corpus (Danescu-Niculescu-
Mizil & Lee, 2011) as the source. It contains ∼220k message-response pairs from movie transcripts.
We use a 200k-10k-10k training-validation-test split.

For the target domain, we manually construct a very small dataset from the Ubuntu Dialogue Cor-
pus (Lowe et al., 2015) to mimic the scenario where quick adaptation has to be done to a new domain
with little training data. In particular, we choose a random subset of 15k message-response pairs,
and use a 9k-3k-3k split.

The base model is a sequence-to-sequence (Seq2Seq) neural network (Sutskever et al., 2014) with
attention from the decoder to the encoder. We use a single-layer RNN encoder and a single-layer
RNN decoder, each containing 1024 cells following Sutskever et al. (2014). We use GRUs instead
of LSTM units due to efﬁciency concerns. We have separate memory banks for the encoder and
decoder, since they are essentially different RNNs. The source and target vocabularies are 27k and
10k, respectively. Each memory slot is 1024D, because the RNN states are 1024D in this experiment.
For each domain, we progressively add 1024 slots; tuning the number of slots is done in a manner
similar to the MultiNLI experiment. As before, we use Adam with an initial learning rate of 0.0003
and other default parameters.

Following previous work, we use BLEU-2 (Eric et al., 2017; Madotto et al., 2018) and average
Word2Vec embedding similarity (W2V-Sim, Serban et al., 2017; Zhang et al., 2018a) as the eval-
uation metrics. BLEU-2 is the geometric mean of unigram and bigram word precision penalized
by length, and correlates with human satisfaction to some extent (Liu et al., 2016). W2V-Sim is
deﬁned as the cosine similarity between the averaged Word2Vec embeddings of the model outputs

15

Published as a conference paper at ICLR 2020

#

1
2
3
4
5
6
7
8
9
10
11
12

Line Model Trained on/by

RNN

+
N
N
R

e

S
T
m S
T
M
S+T
S→T (F)
S→T (F+M)
S→T (F+M+V)
S→T (F+H)
S→T (F+H+V)
S→T (EWC)
S→T (Progressive)

m
e
M
+
N
N
R

BLEU-2 on
T
S
0.738⇓
2.842⇑
1.265⇓
0.795⇓
0.712⇓
3.074⇑
1.287⇓
0.920⇓
0.889⇓
2.650⇑
1.101⇓
1.210⇓
1.207⇓
1.435⇓
1.637
1.652
1.606↓
1.036⇓
1.419⇓
1.257⇓
1.382↓
1.397⇓
1.408↓
1.299⇓

W2V-Sim on
T
S
0.456⇓
0.480⇓
0.480⇓
0.454⇓
0.471⇓
0.498⇓
0.487⇓
0.462⇓
0.462⇓
0.471⇓
0.514⇓
0.509⇓
0.526
0.522
0.525
0.522
0.495⇓
0.503⇓
0.492⇓
0.504⇓
0.514⇓
0.513⇓
0.503⇓
0.502⇓

Table 5: Results on two-domain adaptation for dialogue response generation. F: Fine-tuning. V:
Expanding vocabulary. H: Expanding RNN hidden states. M: Our proposed method of expand-
ing memory. We also compare with EWC (Kirkpatrick et al., 2017) and progressive neural net-
work (Rusu et al., 2016). ↑, ↓: p < 0.05 and ⇑, ⇓: p < 0.01 (compared with Line 8).

Prompt
i’m steve. what’s your name?
i love you man
so how’s it going with your new roomie?
i didn’t do anything wrong did i
i just got a messed up stomach
what are you trying to handicap me?
im scared
i kinda forgot the ﬁx i tried a year ago
how would i remaster the ubuntu install
my terminal is stuck
i am rebooting now
i get an installation error
what does the kernel-package do?
what directory should i be in?

Response
my name is opera
i love you too
it should be ﬁne
we don’t have much experience
i know, you had to do some things
this is not really who i am
what do we do with it?
can you try the output ?
could you have a link both access to the network connection
what version is it?
so your issue is a great work cd, sometimes i get some
i am getting an expert for it
you will need to get it to see if there is a setting
you should be able to install the grub cd at the drive

Table 6: Sample outputs of our IDA model S→T (F+M+V) from Table 5.

and the ground truths. Intuitively, BLEU measures hard word-level overlap between two sequences,
whereas W2V-Sim measures soft similarity in a distributed semantic space.

Results. The results for dialogue response generation are shown in Table 5. We see that BLEU-2 and
W2V similarity are not necessarily consistent. For example, the memory-augmented RNN trained
solely on source achieves the best source BLEU-2, whereas the proposed progressive memory has
the highest W2V cosine similarity on S. However, our model variants achieve the best performance
on most metrics (Lines 7 and 8). Moreover, it consistently outperforms all other IDA approaches.
Following the previous experiment, we conduct statistical comparison with Line 8. The test shows
that our method is signiﬁcantly better than the other IDA methods.

In general, the evaluation of dialogue systems is noisy due to the lack of appropriate metrics (Liu
et al., 2016). Nevertheless, our experiment provides additional evidence of the effectiveness of our
approach. It also highlights our model’s viability for both classiﬁcation and generation tasks.

Case Study. Table 6 shows sample outputs of our IDA model on test prompts from the Cornell
Movie Corpus (source) and the Ubuntu Dialogue Corpus (target). We see that casual prompts from
the movie domain result in casual responses, whereas Ubuntu queries result in Ubuntu-related re-
sponses. With the expansion of vocabulary, our model is able to learn new words like “grub”; with
progressive memory, it learns Ubuntu jargon like “network connection.” This shows evidence of the
success of incremental domain adaptation.

16

0
2
0
2
 
b
e
F
 
4
1
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
2
0
0
.
1
1
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2020

PROGRESSIVE MEMORY BANKS FOR INCREMENTAL
DOMAIN ADAPTATION

Nabiha Asghar∗†◦ Lili Mou∗‡ Kira A. Selby†◦ Kevin D. Pantasdo◦ Pascal Poupart†◦ Xin Jiang♦
†Vector Institute for AI, Toronto, Canada
◦Cheriton School of Computer Science, University of Waterloo, Canada
{nasghar,kaselby,kevin.pantasdo,ppoupart}@uwaterloo.ca
‡Dept. Computing Science, University of Alberta; Alberta Machine Intelligence Institute (AMII)
doublepower.mou@gmail.com
♦Noah’s Ark Lab, Huawei Technologies, Hong Kong
jiang.xin@huawei.com

ABSTRACT

This paper addresses the problem of incremental domain adaptation (IDA) in natu-
ral language processing (NLP). We assume each domain comes one after another,
and that we could only access data in the current domain. The goal of IDA is to
build a uniﬁed model performing well on all the domains that we have encoun-
tered. We adopt the recurrent neural network (RNN) widely used in NLP, but
augment it with a directly parameterized memory bank, which is retrieved by an
attention mechanism at each step of RNN transition. The memory bank provides a
natural way of IDA: when adapting our model to a new domain, we progressively
add new slots to the memory bank, which increases the number of parameters,
and thus the model capacity. We learn the new memory slots and ﬁne-tune exist-
ing parameters by back-propagation. Experimental results show that our approach
achieves signiﬁcantly better performance than ﬁne-tuning alone. Compared with
expanding hidden states, our approach is more robust for old domains, shown by
both empirical and theoretical results. Our model also outperforms previous work
of IDA including elastic weight consolidation and progressive neural networks in
the experiments.1

1

INTRODUCTION

Domain adaptation aims to transfer knowledge from one domain (called the source domain) to
another (called the target domain) in a machine learning system.2 If the data of the target domain are
not large enough, using data from the source domain typically helps to improve model performance
in the target domain. This is important for neural networks, which are data-hungry and prone to
overﬁtting. In this paper, we especially focus on incremental domain adaptation (IDA)3, where we
assume different domains come sequentially one after another. We only have access to the data in
the current domain, but hope to build a uniﬁed model that performs well on all the domains that we
have encountered (Xu et al., 2014; Rusu et al., 2016; Kirkpatrick et al., 2017).

Incremental domain adaptation is useful in scenarios where data are proprietary, or available only for
a short period of time (Li & Hoiem, 2018). It is desired to preserve as much knowledge as possible
in the learned model and not to rely on the availability of the data. Another application of IDA
is a quick adaptation to new domains. If the environment of a deployed machine learning system
changes frequently, traditional methods like jointly training all domains require the learning machine
to be re-trained from scratch every time a new domain comes. Fine-tuning a neural network by a

∗ Equal contribution.
1Our IDA code is available at https://github.com/nabihach/IDA.
2In our work, the domain is deﬁned by datasets. Usually, the data from different genres or times typically

have different underlying distributions.

3In the literature, IDA is sometimes referred to as incremental, continual, or lifelong learning. We use

“incremental domain adaptation” in this paper to emphasize that our domain is not changed continuously.

1

Published as a conference paper at ICLR 2020

few steps of gradient updates does transfer quickly, but it suffers from the catastrophic forgetting
problem (Kirkpatrick et al., 2017). Suppose during prediction a data point is not labeled with its
domain, the (single) ﬁne-tuned model cannot predict well for samples in previous domains, as it
tends to “forget” quickly during ﬁne-tuning.

A recent trend of domain adaptation in the deep learning regime is the progressive neural net-
work (Rusu et al., 2016), which progressively grows the network capacity if a new domain comes.
Typically, this is done by enlarging the model with new hidden states and a new predictor (Fig-
ure 1a). To avoid interfering with existing knowledge, the newly added hidden states are not fed
back to the previously trained states. During training, all existing parameters are frozen, and only
the newly added ones are trained. For inference, the new predictor is used for all domains, which is
sometimes undesired as the new predictor is trained with only the last domain.

In this paper, we propose a progressive memory bank for incremental domain adaptation in natural
language processing (NLP). Our model augments a recurrent neural network (RNN) with a memory
bank, which is a set of distributed, real-valued vectors capturing domain knowledge. The memory
is retrieved by an attention mechanism during RNN information processing. When our model is
adapted to new domains, we progressively increase the slots in the memory bank. But different
from Rusu et al. (2016), we ﬁne-tune all the parameters, including RNN and the previous memory
bank. Empirically, when the model capacity increases, the RNN does not forget much even if
the entire network is ﬁne-tuned. Compared with expanding RNN hidden states, the newly added
memory slots cause less contamination of the existing knowledge in RNN states, as will be shown
by a theorem.

In our paper, we evaluate the proposed approach on a classiﬁcation task known as multi-genre nat-
ural language inference (MultiNLI). Appendix C provides additional evidence when our approach
is applied to text generation. Experiments consistently support our hypothesis that the proposed
approach adapts well to target domains without catastrophic forgetting of the source. Our model
outperforms the na¨ıve ﬁne-tuning method, the original progressive neural network, as well as other
IDA techniques including elastic weight consolidation (EWC, Kirkpatrick et al., 2017).

2 RELATED WORK

Domain Adaptation. Domain adaptation has been widely studied in machine learning, including
the NLP domain. For neural NLP applications, Mou et al. (2016) analyze two straightforward set-
tings, namely, multi-task learning (jointly training all domains) and ﬁne-tuning (training one domain
and ﬁne-tuning on the other). A recent advance of domain adaptation is adversarial learning, where
the neural features are trained not to classify the domain (Ganin et al., 2016; Liu et al., 2017). How-
ever, all these approaches (except ﬁne-tuning) require all domains to be available simultaneously,
and thus are not IDA approaches.

Kirkpatrick et al. (2017) address the catastrophic forgetting problem of ﬁne-tuning neural networks,
and propose a regularization term based on the Fisher information matrix; they call the method
elastic weight consolidation (EWC). While some follow-up studies report EWC achieves high per-
formance in their scenarios (Zenke et al., 2017; Lee et al., 2017; Thompson et al., 2019), others
show that EWC is less effective (Wen & Itti, 2018; Yoon et al., 2018; Wu et al., 2018). Lee et al.
(2017) propose incremental moment matching between the posteriors of the old model and the new
model, achieving similar performance to EWC. Schwarz et al. (2018) augment EWC with knowl-
edge distillation, making it more memory-efﬁcient.

Rusu et al. (2016) propose a progressive neural network that progressively increases the number
of hidden states (Figure 1a). To avoid overriding existing information, they freeze the weights of
the learned network, and do not feed new states to old ones. This results in multiple predictors,
requiring that a data sample is labeled with its domain during the test time. If we otherwise use
the last predictor to predict samples from all domains, its performance may be low for previous
domains, as the predictor is only trained with the last domain.

Yoon et al. (2018) propose an extension of the progressive network. They identify which existing
hidden units are relevant for the new task (with their sparse penalty), and ﬁne-tune only the corre-
sponding subnetwork. However, sparsity is not common for RNNs in NLP applications, as sparse
recurrent connections are harmful. A similar phenomenon is that dropout of recurrent connections

2

Published as a conference paper at ICLR 2020

(a) Progressive
Figure 1:
neural network Rusu et al.
(2016). (b) One step of RNN
transition in our progressive
memory network. Colors in-
dicate different domains.

yields poor performance (Bayer et al., 2013). Xu & Zhu (2018) deal with new domains by adap-
tively adding nodes to the network via reinforcement learning. This approach may require a very
large number of trials to identify the right number of nodes to be added to each layer (Yoon et al.,
2019).

Li & Hoiem (2018) address IDA with a knowledge distillation approach, where they preserve a set
of outputs of the old network on pseudo-training data. Then they jointly optimize for the accuracy
on the new training domain as well as the pseudo-training data. Kim et al. (2019)’s variant of
this approach uses maximum-entropy regularization to control the transfer of distilled knowledge.
However, in NLP applications, it is non-trivial to obtain pseudo-training data for distillation.

Memory-Based Neural Networks. Our work is related to memory-based neural networks.
Sukhbaatar et al. (2015) propose an end-to-end memory network that assigns each memory slot
to an entity, and aggregates information by multiple attention-based layers. They design their archi-
tecture for the bAbI question answering task, and assign a slot to each sentence. Such idea can be
extended to various scenarios, for example, assigning slots to external knowledge for question an-
swering (Das et al., 2017) and assigning slots to dialogue history for a conversation system (Madotto
et al., 2018).

A related idea is to use episodic memory, which stores data samples from all previously seen do-
mains (thus it is not an IDA approach). This is used for experience replay while training on subse-
quent domains (Lopez-Paz & Ranzato, 2017; Rebufﬁ et al., 2017; Chaudhry et al., 2018; d’Autume
et al., 2019).

Another type of memory in the neural network regime is the neural Turing machine (NTM, Graves
et al., 2016). This memory is not directly parameterized, but is read or written by a neural controller.
Therefore, it serves as temporary scratch paper and does not store knowledge itself. Zhang et al.
(2018b) combine the above two styles of memory for task-oriented dialogue systems, where they
have both slot-value memory and read-and-write memory.

Different from the work above, our memory bank stores knowledge in a distributed fashion, where
each slot does not correspond to a concrete entity or data sample. Our memory is directly parame-
terized, interacting in a different way from RNN weights and providing a natural way of incremental
domain adaptation.

3 PROPOSED APPROACH

Our model is based on a recurrent neural network (RNN). At each step, the RNN takes the embed-
ding of the current input (e.g., a word), and changes its states accordingly. This is represented by
hi = RNN(hi−1, xi), where hi and hi−1 are the hidden states at time steps i and i−1, respectively.
xi is the input at the ith step. Typically, long short term memory (LSTM, Hochreiter & Schmidhu-
ber, 1997) or Gated Recurrent Units (GRU, Cho et al., 2014) are used as RNN transitions. In the
rest of this section, we will describe a memory-augmented RNN, and how it is used for incremental
domain adaptation (IDA).

3

Published as a conference paper at ICLR 2020

3.1 AUGMENTING RNN WITH MEMORY BANKS

We enhance the RNN with an external memory bank, as shown in Figure 1b. The memory bank
augments the overall model capacity by storing additional parameters in memory slots. At each
time step, our model computes an attention probability to retrieve memory content, which is then
fed to the computation of RNN transition.

Particularly, we adopt a key-value memory bank, inspired by Miller et al. (2016). Each memory slot
contains a key vector and a value vector. The former is used to compute the attention weight for
memory retrieval, whereas the latter is the value of memory content.

For the ith step, the memory mechanism computes an attention probability αi by

(cid:101)αi,j = exp{h(cid:62)

i−1m(key)

j

}, αi,j =

(cid:101)αi,j
j(cid:48)=1 (cid:101)αi,j(cid:48)

(cid:80)N

where m(key)
is the key vector of the jth slot of the memory (among N slots in total). Then the
model retrieves memory content by a weighted sum of all memory values, where the weight is the
attention probability, given by

j

(1)

(2)

(cid:88)N

ci =

j=1

αi,jm(val)

j

Here, m(val)
concatenated with the current word xi, and fed to the RNN as input for state transition.

is the value vector of the jth memory slot. We call ci the memory content. Then, ci is

j

Using the key-value memory bank allows separate (thus more ﬂexible) computation of memory
retrieval weights and memory content, compared with traditional attention where a candidate vector
is used to compute both attention probability and attention content.

It should be emphasized that the memory bank in our model captures distributed knowledge, which
is different from other work where the memory slots correspond to speciﬁc entities (Eric et al.,
2017). The attention mechanism accomplishes memory retrieval in a “soft” manner, which means
the retrieval strength is a real-valued probability. This enables us to train both memory content and
its retrieval end-to-end, along with other neural parameters.

We would also like to point out that the memory bank alone does not help RNN much. However, it
is natural to use a memory-augmented RNN for incremental domain adaptation, as described below.

3.2 PROGRESSIVELY INCREASING MEMORY FOR INCREMENTAL DOMAIN ADAPTATION

The memory bank in Subsection 3.1 can be progressively expanded to adapt a model in a source
domain to new domains. This is done by adding new memory slots to the bank which are learned
exclusively from the target data.

Suppose the memory bank is expanded with another M slots in a new domain, in addition to previous
N slots. We then have N + M slots in total. The model computes attention probability over the
expanded memory and obtains the attention vector in the same way as Equations (1)–(2), except that
the summation is computed from 1 to N + M . This is given by

α(expand)
i,j

=

(cid:101)αi,j
(cid:80)N +M
j(cid:48)=1 (cid:101)αi,j(cid:48)

,

c(expand)
i

=

(cid:88)N +M
j=1

α(expand)
i,j m(val)

j

(3)

To initialize the expanded model, we load all previous parameters, including RNN weights and the
learned N slots, but randomly initialize the progressively expanded M slots. During training, we
update all parameters by gradient descent. That is to say, new parameters are learned from their
initializations, whereas old parameters are ﬁne-tuned during IDA. The process is applied whenever
a new domain comes, as shown in Algorithm 1.

We would like to discuss the following issues.

4

Published as a conference paper at ICLR 2020

Freezing vs. Fine-tuning learned parame-
ters. Inspired by the progressive neural net-
work (Rusu et al., 2016), we ﬁnd it tempt-
ing to freeze RNN parameters and the learned
memory but only tune new memory for IDA.
However, our preliminary results show that
if we freeze all existing parameters, the in-
creased memory does not add much to the
model capacity, and that its performance is
worse than ﬁne-tuning all parameters.

Algorithm 1: Progressive Memory for IDA
Input: A sequence of domains D0, D1, · · · , Dn
Output: A single model for all domains
Initialize a memory-augmented RNN
Train the model on D0
for D1, · · · , Dn do

Expand the memory with new slots
Load RNN weights and existing memory banks
Train the model by updating all parameters

end
Return: The resulting model

Fine-tuning vs. Fine-tuning while increas-
ing memory slots.
It is reported that ﬁne-
tuning a model (without increasing model ca-
pacity) suffers from the problem of catastrophic forgetting (Kirkpatrick et al., 2017). We wish to
investigate whether our approach suffers from the same problem, since we ﬁne-tune learned param-
eters when progressively increasing memory slots. Our intuition is that the increased model capacity
helps to learn the new domain with less overriding of the previously learned model. Experiments
conﬁrm our conjecture, as the memory-augmented RNN tends to forget more if the memory size is
not increased.

Expanding hidden states vs. Expanding memory. Another way of progressively increasing model
capacity is to expand the size of RNN layers. This setting is similar to the progressive neural net-
work, except that all weights are ﬁne-tuned and new states are connected to existing states.

However, we hereby show a theorem, indicating that the expanded memory results in less contami-
nation/overriding of the learned knowledge in the RNN, compared with the expanded hidden states.
The main idea is to measure the effect of model expansion quantitatively by the expected square dif-
ference on hi before and after expansion, where the expectation reﬂects the average effect of model
expansion in different scenarios.
Theorem 1. Let RNN have vanilla transition with the linear activation function, and let the RNN
state at the last step hi−1 be ﬁxed. For a particular data point, if the memory attention satisﬁes
(cid:80)N +M
j=1 (cid:101)αi,j, then memory expansion yields a lower expected mean squared differ-

j=N +1 (cid:101)αi,j ≤ (cid:80)N

ence in hi than RNN state expansion. That is,

(cid:104)

i − hi(cid:107)2(cid:105)
i − hi(cid:107)2(cid:105)
refers to the hidden states if the memory is expanded. h(s)
i

(cid:107)h(m)

(cid:107)h(s)

≤ E

E

(cid:104)

i

where h(m)
refers to the original dimen-
sions of the RNN states, if we expand the size of RNN states themselves. Here, we compute the
expectation by assuming weights and hidden states are iid from a zero-mean Gaussian distribution
(with variance σ2).

(4)

Proof Sketch: We focus on one step of RNN transition and assume that hi−1 is the same when the
model capacity is increased. Further, we assume that hi is D-dimensional, that each memory slot
mj is d-dimensional, and that the additional RNN units (when we expand the hidden state) are also
d-dimensional.

We compute how the original dimensions in the hidden state are changed if we expand RNN. We
denote the expanded hidden states by (cid:101)hi−1 and (cid:102)hi for the two time steps. We denote the weights
connecting from (cid:101)hi−1 to hi by (cid:102)W ∈ RD×d. We focus on the original D-dimensional space, denoted
as h(s)
i

. The connection is shown in Figure 2a, Appendix A. We have

E(cid:2)(cid:107)h(s)

i − hi(cid:107)2(cid:3) = E(cid:2)(cid:107)(cid:102)W · (cid:101)hi−1(cid:107)2(cid:3) =

D
(cid:88)

(cid:20)(cid:16)

E

j=1

(cid:17)2(cid:21)

(cid:101)w(cid:62)

j (cid:101)hi−1

D
(cid:88)

E

(cid:34)(cid:18) d
(cid:88)

=

j=1

k=1

(cid:101)wjk(cid:101)hi−1[k])

(cid:19)2(cid:35)

D
(cid:88)

d
(cid:88)

(cid:104)(cid:0)

E

=

j=1

i=1

= D · d · Var(cid:0)w(cid:1) · Var(h) = Ddσ2σ2

(cid:1)2(cid:105)

E

(cid:101)hi−1[k](cid:1)2(cid:105)
(cid:104)(cid:0)

(cid:101)wjk

(5)

(6)

(7)

5

Published as a conference paper at ICLR 2020

Table 1: Corpus statistics and the baseline
performance (% accuracy) of our BiLSTM
model (without domain adaptation) and re-
sults reported in previous work.

Fic Gov Slate Tel Travel
77k 77k 77k 83k
# training samples
Our Implementation 65.0 66.5 56.2 64.5
64.7 69.2 57.9 64.4

Yu et al. (2018)

77k
62.7
65.8

Table 2: Results on two-domain adapta-
tion. F: Fine-tuning. V: Expanding vocab-
ulary. H: Expanding RNN hidden states. M:
Our proposed method of expanding memory.
We also compare with previous work elas-
tic weight consolidation (EWC, Kirkpatrick
et al., 2017) and the progressive neural net-
work (Rusu et al., 2016). For the statistical
test (compared with Line 8), ↑, ↓: p < 0.05
and ⇑, ⇓: p < 0.01. The absence of an ar-
row indicates that the performance difference
compared with Line 8 is statistically insignif-
icant with p lower than 0.05.

#Line Model Trained on/by

1
2
3
4
5
6
7
8
9
10
11
12

RNN

+
N
N
R

e

S
T
m S
T
M
S+T
S→T (F)
S→T (F+M)
S→T (F+M+V)
S→T (F+H)
S→T (F+H+V)
S→T (EWC)
S→T (Progressive)

m
e
M
+
N
N
R

% Accuracy on
T
61.23⇓
66.49⇓
60.87⇓
67.01⇓
70.00
69.90↓
70.21
70.82
68.35⇓
68.02⇓
64.10⇓
68.25⇓

S
65.01⇓
56.46⇓
65.41⇓
56.77⇓
66.02↓
65.62↓
66.23
67.55
64.09⇓
63.68⇓
66.02⇓
64.47⇓

Similarly,

E(cid:2)(cid:107)h(m)

= Ddσ2Var(cid:0)∆ck

i − hi(cid:107)2(cid:3) = E

(cid:13)W(c)∆c(cid:107)2(cid:105)
(cid:104)(cid:13)
where ∆c def= c(cid:48) − c. The vectors c and c(cid:48) are the current step’s attention content before and after
memory expansion, respectively, shown in Figure 2b, Appendix A. (We omit the time step in the
notation for simplicity.) W(c) is the weight matrix connecting attention content to RNN states.
To prove the theorem, it remains to show that Var(∆ck) ≤ σ2. We do this by analyzing how
attention is computed at each time step, and bounding each attention weight. For details, see the
complete proof in Appendix A.

(8)

(cid:1)

In the theorem, we have an assumption (cid:80)N +M
j=N +1 (cid:101)αi,j ≤ (cid:80)N
j=1 (cid:101)αi,j, requiring that the total attention
to existing memory slots is larger than to the progressively added slots. This is fairly reasonable
because: (1) During training, attention is trained in an ad hoc fashion to newly-added information,
and thus some αi,j for 1 ≤ j ≤ N might be learned so that it is larger than a random memory slot;
and (2) For a new domain, we do not add a huge number of slots, and thus (cid:80)N +M
j=N +1 (cid:101)αi,j will not
dominate.

It is noted that our theorem is not to provide an explicit optimization/generalization bound for IDA,
but shows that expanding memory is more stable than expanding hidden states. This is particu-
larly important at the beginning steps of IDA, as the progressively growing parameters are randomly
initialized and are basically noise. Although our theoretical analysis uses a restricted setting (i.e.,
vanilla RNN transition and linear activation), it provides the key insight that our approach is appro-
priate for IDA.

4 EXPERIMENTS

In this section, we evaluate our approach on an NLP classiﬁcation task. In particular, we choose the
multi-genre natural language inference (MultiNLI), due to its large number of samples in various
domains. The task is to determine the relationship between two sentences among target labels:
entailment, contradiction, and neutral. In Appendix C, we conduct supplementary experiments on
text generation with our memory-augmented RNN for IDA.

Dataset and Setup. The MultiNLI corpus (Williams et al., 2018) is particularly suitable for IDA,
as it contains training samples for 5 genres: Slate, Fiction (Fic), Telephone (Tel),
Government (Gov), and Travel. In total, we have 390k training samples. The corpus also
contains held-out (non-training) labeled data in these domains. We split it into two parts for valida-
tion and test.4

4MultiNLI also contains 5 genres without training samples, namely, 9/11, Face-to-face, Letters,
OUP, and Verbatim. We ignore these genres, because we focus on incremental domain adaptation instead of

6

Published as a conference paper at ICLR 2020

Training domains
Fic
Fic → Gov
Fic → Gov → Slate
Fic → Gov → Slate → Tel
Fic → Gov → Slate → Tel → Travel

Fic
65.41
67.55
67.04
68.46
69.36

Performance on
Slate
55.83
61.04
63.29
63.39
63.96

Tel
61.39
65.07
64.66
71.60
69.74

Gov
58.87
70.82
71.55
71.10
72.47

Travel
57.35
61.90
63.53
61.50
68.39

Table 3: Dynamics of the progressive memory network for IDA with 5 domains. Upper-triangular
values in gray are out-of-domain (zero-shot) performance.

Group

Non-
IDA

IDA

Setting
In-domain training
Fic + Gov + Slate + Tel + Travel (multi-task)
Fic → Gov → Slate → Tel → Travel (F+V)
Fic → Gov → Slate → Tel → Travel (F+V+M)
Fic → Gov → Slate → Tel → Travel (EWC)
Fic → Gov → Slate → Tel → Travel (Progressive)

Fic
65.41⇓
70.60↑
67.24↓
69.36
67.12⇓
65.22⇓

Gov
67.01⇓
73.30
70.82⇓
72.47
68.71⇓
67.87⇓

Slate
59.30⇓
63.80
62.41↓
63.96
59.90⇓
61.13⇓

Tel
67.20⇓
69.15
67.62↓
69.74
66.09⇓
66.96⇓

Travel
64.70⇓
67.07↓
68.39
68.39
65.70⇓
67.90

Table 4: Comparing our approach with variants and previous work in the multi-domain setting. In
this experiment, we use the memory-augmented RNN as the neural architecture. Italics represent
best results in the IDA group. ↑, ↓: p < 0.05 and ⇑, ⇓: p < 0.01 (compared with F+V+M).

The ﬁrst row in Table 1 shows the size of the training set in each domain. As seen, the corpus is
mostly balanced across domains, although Tel has slightly more examples.

For the base model, we train a bi-directional LSTM (BiLSTM). The details of network architecture,
training, and hyper-parameter tuning are given in Appendix B. We see in Table 1 that we achieve
similar performance to Yu et al. (2018). Furthermore, our BiLSTM achieves an accuracy of 68.37
on the ofﬁcial MultiNLI test set,5 which is better than 67.51 reported in the original MultiNLI paper
(Williams et al., 2018) using BiLSTM. This shows that our implementation and tuning are fair for
the basic BiLSTM, and that our model is ready for the study of IDA.

Transfer between Two Domains. We would like to compare our approach with a large number
of baselines and variants. Thus, we randomly choose two domains as a testbed: Fic as the source
domain and Gov as the target domain. We show results in Table 2.

First, we analyze the performance of RNN and the memory-augmented RNN in the non-transfer
setting (Lines 1–2 vs. Lines 3–4). As seen, the memory-augmented RNN achieves slightly better
but generally similar performance, compared with RNN (both with LSTM units). This shows that,
in the non-transfer setting, the memory bank does not help the RNN much. However, this later
conﬁrms that the performance improvement is indeed due to our IDA technique, instead of simply a
better neural architecture.

We then apply two straightforward methods of domain adaptation: multi-task learning (Line 5) and
ﬁne-tuning (Line 6). Multi-task learning jointly optimizes source and target objectives, denoted by
“S+T.” On the other hand, the ﬁne-tuning approach trains the model on the source ﬁrst, and then ﬁne-
tunes on the target. In our experiments, these two methods perform similarly on the target domain,
which is consistent with Mou et al. (2016). On the source domain, ﬁne-tuning performs signiﬁcantly
worse than multi-task learning, as it suffers from the catastrophic forgetting problem. We notice that,
in terms of source performance, the ﬁne-tuning approach (Line 6) is slightly better than trained on
the source domain only (Line 3). This is probably because our domains are somewhat correlated as
opposed to Kirkpatrick et al. (2017), and thus training with more data on target slightly improves
the performance on source. However, ﬁne-tuning does achieve the worst performance on source
compared with other domain adaptation approaches (among Lines 5–8). Thus, we nevertheless use
the terminology “catastrophic forgetting,” and our research goal is still to improve IDA performance.

The main results of our approach are Lines 7 and 8. We apply the proposed progressive memory
network to IDA and we ﬁne-tune all weights. We see that on both source and target domains,

zero-shot learning. Also, the labels for the ofﬁcial test set of MultiNLI are not publicly available, and therefore
we cannot use it to evaluate performance on individual domains. Our split of the held-out set for validation and
test applies to all competing methods, and thus is a fair setting.

5Evaluation on the ofﬁcial MultiNLI test set requires submission to Kaggle.

7

Published as a conference paper at ICLR 2020

our approach outperforms the ﬁne-tuning method alone where the memory size is not increased
(comparing Lines 7 and 6). This veriﬁes our conjecture that, if the model capacity is increased, the
new domain results in less overriding of the learned knowledge in the neural network. Our proposed
approach is also “orthogonal” to the expansion of the vocabulary size, where target-speciﬁc words
are randomly initialized and learned on the target domain. As seen, this combines well with our
memory expansion and yields the best performance on both source and target (Line 8).

We now compare an alternative way of increasing model capacity, i.e., expanding hidden states
(Lines 9 and 10). For fair comparison, we ensure that the total number of model parameters after
memory expansion is equal to the number of model parameters after hidden state expansion. We
see that the performance of hidden state expansion is poor especially on the source domain, even
if we ﬁne-tune all parameters. This experiment provides empirical evidence to our theorem that
expanding memory is more robust than expanding hidden states.

We also compare the results with previous work on IDA. We re-implement6 EWC (Kirkpatrick
et al., 2017). It does not achieve satisfactory results in our application. We investigate other pub-
lished papers using the same method and ﬁnd inconsistent results: EWC works well in some ap-
plications (Zenke et al., 2017; Lee et al., 2017) but performs poorly on others (Yoon et al., 2018;
Wu et al., 2018). Wen & Itti (2018) even report near random performance with EWC. We also
re-implement the progressive neural network (Rusu et al., 2016). We use the target predictor to do
inference for both source and target domains. Progressive neural network yields low performance,
particularly on source, probably because the predictor is trained with only the target domain.

We measure the statistical signiﬁcance of the results against Line 8 with the one-tailed Wilcoxon’s
signed-rank test (Wilcoxon, 1945), by bootstrapping a subset of 200 samples for 10 times with
replacement. The test shows our approach is signiﬁcantly better than others, on both source and
target domains.

IDA with All Domains. Having analyzed our approach, baselines, and variants on two domains in
detail, we are now ready to test the performance of IDA with multiple domains, namely, Fic, Gov,
Slate, Tel, and Travel. In this experiment, we assume these domains come one after another,
and our goal is to achieve high performance on all domains.

Table 3 shows the dynamics of IDA with our progressive memory network. Comparing the upper-
triangular values (in gray, showing out-of-domain performance) with diagonal values, we see that
our approach can be quickly adapted to the new domain in an incremental fashion. Comparing lower-
triangular values with the diagonal, we see that our approach does not suffer from the catastrophic
forgetting problem as the performance of previous domains is gradually increasing if trained with
more domains. After all data are observed, our model achieves the best performance in most domains
(last row in Table 3), despite the incremental nature of our approach.

We now compare our approach with other baselines and variants in the multi-domain setting, shown
in Table 4. Due to the large number of settings, we only choose a selected subset of variants from
Table 2 for the comparison.

As seen, our approach of progressively growing memory achieves the same performance as ﬁne-
tuning on the last domain (both with vocabulary expansion), but for all previous 4 domains, we
achieve signiﬁcantly better performance than ﬁne-tuning. Our model is comparable to multi-task
learning on all domains. It should also be mentioned that multi-task learning requires training the
model when data from all domains are available simultaneously. It is not an incremental approach
for domain adaptation, and thus cannot be applied to the scenarios introduced in Section 1. We
include this setting mainly because we are curious about the performance of non-incremental domain
adaptation.

We also compare with previous methods for IDA in Table 4. Our method outperforms EWC and the
progressive neural network in all domains; the results are consistent with Table 2.

6Implementation based on https://github.com/ariseff/overcoming-catastrophic

8

Published as a conference paper at ICLR 2020

5 CONCLUSION

In this paper, we propose a progressive memory network for incremental domain adaptation (IDA).
We augment an RNN with an attention-based memory bank. During IDA, we add new slots to the
memory bank and tune all parameters by back-propagation. Empirically, the progressive memory
network does not suffer from the catastrophic forgetting problem as in na¨ıve ﬁne-tuning. Our intu-
ition is that the new memory slots increase the neural network’s model capacity, and thus, the new
knowledge causes signiﬁcantly less overriding of the existing network. Compared with expanding
hidden states, our progressive memory bank provides a more robust way of increasing model ca-
pacity, shown by both a theorem and experiments. We also outperform previous work for IDA,
including elastic weight consolidation (EWC) and the original progressive neural network.

This work was funded by Huawei Technologies, Hong Kong. Lili Mou is supported by the Amii
Fellow Program, and the CIFAR AI Chair Program.

ACKNOWLEDGMENTS

REFERENCES

Justin Bayer, Christian Osendorfer, Daniela Korhammer, Nutan Chen, Sebastian Urban, and Patrick
van der Smagt. On fast dropout and its applicability to recurrent networks. arXiv preprint
arXiv:1311.0701, 2013.

Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient

lifelong learning with A-GEM. arXiv preprint arXiv:1812.00420, 2018.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder–decoder
for statistical machine translation. In EMNLP, pp. 1724–1734, 2014.

Cristian Danescu-Niculescu-Mizil and Lillian Lee. Chameleons in imagined conversations: A new
approach to understanding coordination of linguistic style in dialogs. In Proc. Workshop on Cog-
nitive Modeling and Computational Linguistics, pp. 76–87, 2011.

Rajarshi Das, Manzil Zaheer, Siva Reddy, and Andrew McCallum. Question answering on knowl-
edge bases and text using universal schema and memory networks. In ACL, pp. 358–365, 2017.

Cyprien de Masson d’Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic

memory in lifelong language learning. arXiv preprint arXiv:1906.01076, 2019.

Mihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. Key-value retrieval

networks for task-oriented dialogue. In SIGDIAL, pp. 37–49, 2017.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. JMLR, 17(1):2096–2030, 2016.

Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwi´nska, Sergio G´omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538
(7626):471–476, 2016.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):

1735–1780, 1997.

Dahyun Kim, Jihwan Bae, Yeonsik Jo, and Jonghyun Choi. Incremental learning with maximum en-
tropy regularization: Rethinking forgetting and intransigence. arXiv preprint arXiv:1902.00829,
2019.

9

Published as a conference paper at ICLR 2020

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences,
114(13):3521–3526, 2017.

Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming
catastrophic forgetting by incremental moment matching. In NeurIPS, pp. 4652–4662, 2017.

Zhizhong Li and Derek Hoiem. Learning without forgetting.

IEEE TPAMI, 40(12):2935–2947,

2018.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.
How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics
for dialogue response generation. In EMNLP, pp. 2122–2132, 2016.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task learning for text classiﬁca-

tion. In ACL, pp. 1–10, 2017.

NeurIPS, pp. 6467–6476, 2017.

David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. The Ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dialogue systems. In SIGDIAL, pp. 285–294, 2015.

Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. Mem2seq: Effectively incorporating knowl-

edge bases into end-to-end task-oriented dialog systems. In ACL, pp. 1468–1478, 2018.

Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason
In EMNLP, pp. 1400–

Weston. Key-value memory networks for directly reading documents.
1409, 2016.

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. How transferable are neural

networks in NLP applications? In EMNLP, pp. 479–489, 2016.

Jeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global vectors for word

representation. In EMNLP, pp. 1532–1543, 2014.

Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.

icarl:

Incremental classiﬁer and representation learning. In CVPR, pp. 2001–2010, 2017.

Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016.

Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye
Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for contin-
ual learning. In ICML, pp. 4535–4544, 2018.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C
Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for gen-
erating dialogues. In AAAI, pp. 3295–3301, 2017.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In NIPS, pp.

2440–2448, 2015.

In NIPS, pp. 3104–3112, 2014.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.

Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. Overcoming
catastrophic forgetting during domain adaptation of neural machine translation. In NAACL, pp.
2062–2068, 2019.

Shixian Wen and Laurent Itti. Overcoming catastrophic forgetting problem by weight consolidation

and long-term memory. arXiv preprint arXiv:1805.07441, 2018.

10

Published as a conference paper at ICLR 2020

Frank Wilcoxon.

Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80–83,

1945.

Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-

tence understanding through inference. In NAACL-HLT, pp. 1112–1122, 2018.

Chenshen Wu, Luis Herranz, Xialei Liu, Yaxing Wang, Joost van de Weijer, and Bogdan Raducanu.
Memory replay GANs: learning to generate images from new categories without forgetting. arXiv
preprint arXiv:1809.02058, 2018.

Jiaolong Xu, Sebastian Ramos, David V´azquez, Antonio M L´opez, and D Ponsa.

Incremental

domain adaptation of deformable part-based models. In BMVC, 2014.

Ju Xu and Zhanxing Zhu. Reinforced continual learning. In NeurIPS, pp. 899–908. 2018.

Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically

expandable networks. ICLR, 2018.

Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Oracle: Order robust adaptive

continual learning. arXiv preprint arXiv:1902.09432, 2019.

Jianfei Yu, Minghui Qiu, Jing Jiang, Jun Huang, Shuangyong Song, Wei Chu, and Haiqing Chen.
Modelling domain relationships for transfer learning on retrieval-based question answering sys-
tems in e-commerce. In WSDM, pp. 682–690, 2018.

Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.

In ICML, pp. 3987–3995, 2017.

Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, Xiujun Li, Chris Brockett, and Bill Dolan.
Generating informative and diverse conversational responses via adversarial information maxi-
mization. arXiv preprint arXiv:1809.05972, 2018a.

Zheng Zhang, Minlie Huang, Zhongzhou Zhao, Feng Ji, Haiqing Chen, and Xiaoyan Zhu.
Memory-augmented dialogue management for task-oriented dialogue systems. arXiv preprint
arXiv:1805.00150, 2018b.

11

Published as a conference paper at ICLR 2020

Figure 2: Hidden state expansion vs. memory expansion at step t.

A PROOF OF THEOREM 1

Theorem 1. Let RNN have vanilla transition with the linear activation function, and let the RNN
state at the last step hi−1 be ﬁxed. For a particular data point, if the memory attention satisﬁes
(cid:80)N +M
j=1 (cid:101)αi,j, then memory expansion yields a lower expected mean squared differ-

j=N +1 (cid:101)αi,j ≤ (cid:80)N

ence in hi than RNN state expansion. That is,

(cid:104)

i − hi(cid:107)2(cid:105)
i − hi(cid:107)2(cid:105)
refers to the hidden states if the memory is expanded. h(s)
i

(cid:107)h(m)

(cid:107)h(s)

≤ E

E

(cid:104)

i

where h(m)
refers to the original dimen-
sions of the RNN states, if we expand the size of RNN states themselves. Here, we compute the
expectation by assuming weights and hidden states are iid from a zero-mean Gaussian distribution
(with variance σ2).

(9)

Proof: Let hi−1 be the hidden state of the last step. We focus on one step of transition and assume
that hi−1 is the same when the model capacity is increased. We consider a simpliﬁed case where
the RNN has vanilla transition with the linear activation function. We measure the effect of model
expansion quantitatively by the expected square difference on hi before and after model expansion.

Suppose the original hidden state hi is D-dimensional. We assume each memory slot is d-
dimensional, and that the additional RNN units when expanding the hidden state are also d-
dimensional. We further assume each variable in the expanded memory and expanded weights ((cid:102)W
in Figure 2) are iid with zero mean and variance σ2. This assumption is reasonable as it enables a fair
comparison of expanding memory and expanding hidden states. Finally, we assume every variable
in the learned memory slots, i.e., mjk, follows the same distribution (zero mean, variance σ2).

We compute how the original dimensions in the hidden state are changed if we expand RNN. We
denote the expanded hidden states by (cid:101)hi−1 and (cid:101)hi for the two time steps. We denote the weights
connecting from (cid:101)hi−1 to hi by (cid:102)W ∈ RD×d. We focus on the original D-dimensional space, denoted
as h(s)
i

. The connection is shown in Figure 2a. We have

E(cid:2)(cid:107)h(s)

i − hi(cid:107)2(cid:3)

= E(cid:2)(cid:107)(cid:102)W · (cid:101)hi−1(cid:107)2(cid:3)
(cid:16)

(cid:20) D
(cid:88)

= E

(cid:101)w(cid:62)

j (cid:101)hi−1

j=1
(cid:20)(cid:16)

E

=

D
(cid:88)

j=1

(cid:101)w(cid:62)

j (cid:101)hi−1

(cid:17)2(cid:21)

(cid:17)2(cid:21)

12

(10)

(11)

(12)

Published as a conference paper at ICLR 2020

D
(cid:88)

E

(cid:34)(cid:18) d
(cid:88)

j=1

k=1

(cid:19)2(cid:35)

(cid:101)wjk(cid:101)hi−1[k])

(cid:104)(cid:0)

E

(cid:101)wjk(cid:101)hi−1[k](cid:1)2(cid:105)

=

=

=

D
(cid:88)

d
(cid:88)

j=1

k=1

D
(cid:88)

d
(cid:88)

j=1

k=1

(cid:104)(cid:0)

E

(cid:101)wjk

(cid:1)2(cid:105)

E

(cid:104)(cid:0)
(cid:101)hi−1[k](cid:1)2(cid:105)

= D · d · Var(cid:0)w(cid:1) · Var(h)
= Ddσ2σ2

where (14) is due to the independence and zero-mean assumptions of every element in (cid:102)W and hi−1.
(15) is due to the independence assumption between (cid:102)W and hi−1.
Next, we compute the effect of expanding memory slots. Notice that (cid:107)h(m)
i − hi(cid:107) = W(c)∆c. Here,
is the RNN hidden state after memory expansion. ∆c def= c(cid:48) −c, where c and c(cid:48) are the attention
h(m)
i
content vectors before and after memory expansion, respectively, at the current time step.7 W(c) is
the weight matrix connecting attention content to RNN states. The connection is shown in Figure 2b.
Reusing the result of (16), we immediately obtain

E(cid:2)(cid:107)h(m)

i − hi(cid:107)2(cid:3)

(cid:13)W(c)∆c(cid:107)2(cid:105)
(cid:104)(cid:13)
= E
(cid:1)
= Ddσ2Var(cid:0)∆ck

where ∆ck is an element of the vector ∆c.
To prove Equation (2), it remains to show that Var(∆ck) ≤ σ2. We now analyze how attention is
computed.
Let (cid:101)α1, · · · , (cid:101)αN +M be the unnormalized attention weights over the N +M memory slots. We notice
that (cid:101)α1, · · · , (cid:101)αN remain the same after memory expansion. Then, the original attention probability
is given by αj = (cid:101)αj/((cid:101)α1 + · · · + (cid:101)αN ) for j = 1, · · · , N . After memory expansion, the attention
probability becomes α(cid:48)

j = (cid:101)αj/((cid:101)α1 + · · · + (cid:101)αN +M ), illustrated in Figure 3. We have

(cid:19)

mj +

N +M
(cid:88)

(cid:16)

j=N +1

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

(cid:17)

mj

(cid:16)

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

(cid:17)

mj

∆c = c(cid:48) − c

N
(cid:88)

j=1

(α(cid:48)

j − αj)mj +

α(cid:48)

jmj

N +M
(cid:88)

j=N +1

N
(cid:88)

(cid:18)

j=1

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

−

N
(cid:88)

(cid:18) −(cid:101)αj

(cid:101)αN +1+···+ (cid:101)αN +M
(cid:101)α1+···+ (cid:101)αN

(cid:101)α1 + · · · + (cid:101)αN +M

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN
(cid:19)

N +M
(cid:88)

mj +

j=N +1

=

=

=

def=

j=1

N +M
(cid:88)

j=1

βjmj

where

βj

def=





(cid:101)αN +1+···+ (cid:101)αN +M
(cid:101)α1+···+ (cid:101)αN

−(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M
(cid:101)αj
,
(cid:101)α1 + · · · + (cid:101)αN +M

,

if 1 ≤ j ≤ N

if N +1 ≤ j ≤ N + M

7We omit the time step in the notation for simplicity.

13

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

Published as a conference paper at ICLR 2020

Figure 3: Attention probabilities before and after memory expansion.

By our assumption of total attention (cid:80)N +M
|βj| ≤ |α(cid:48)

j|,

j=N +1 (cid:101)αj ≤ (cid:80)N

j=1 (cid:101)αj, we have

∀1 ≤ j ≤ N + M

Then, we have

Var(∆ck) = E[(c(cid:48)

k − ck)2(cid:3) ∀1 ≤ k ≤ d

E(cid:2)(cid:107)c(cid:48) − c(cid:107)2(cid:3)
(cid:34) d

(cid:88)

(cid:18) N +M
(cid:88)

E

k=1

j=1
(cid:34)(cid:18) N +M
(cid:88)

d
(cid:88)

E

k=1

j=1

(cid:19)2(cid:35)

(cid:19)2(cid:35)

βjmjk

βjmjk

(cid:104)(cid:0)βjmjk
E

(cid:1)2(cid:105)

E(cid:2)β2

(cid:3)E(cid:2)m2

j

jk

(cid:3)

=

=

=

=

=

=

1
d

1
d

1
d

1
d

1
d

1
d

d
(cid:88)

N +M
(cid:88)

k=1

j=1

d
(cid:88)

N +M
(cid:88)

k=1

j=1

d
(cid:88)

N +M
(cid:88)

k=1


j=1

N +M
(cid:88)







j=1

N +M
(cid:88)

j=1

= σ2E

≤ σ2E

E[β2

j ]σ2



β2
j





(α(cid:48)

j)2



(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)
Here, (31) is due to the assumption that mjk is independent and zero-mean, and (32) is due to the
independence assumption between βj and mjk. To obtain (36), we notice that (cid:80)N +M
j = 1 with
0 ≤ α(cid:48)

j ≤ 1 (∀1 ≤ j ≤ N + M ). Thus, (cid:80)N +M

j)2 ≤ 1, concluding our proof.

j=1 α(cid:48)

(α(cid:48)

j=1

≤ σ2

B HYPERPARAMETERS

We choose the base model and most of its settings by following the original MultiNLI paper
(Williams et al., 2018): 300D RNN hidden states, 300D pretrained GloVe embeddings (Penning-
ton et al., 2014) for initialization, batch size of 32, and the Adam optimizer for training. The initial

14

Published as a conference paper at ICLR 2020

S
n
o

.
c
c
A
n
o
i
t
a
d
i
l
a
V

68

67

66

65

1

71

70.5

70

T
n
o

.
c
c
A
n
o
i
t
a
d
i
l
a
V

69.5

1

100

200

300

400

500

100

200

300

400

500

# of Memory Slots

# of Memory Slots

(a)

(b)

Figure 4: Tuning the number of memory slots to be added per domain in the MultiNLI experiment.
The two graphs show validation performance of our IDA model S→T (F+M+V).

learning rate for Adam is tuned over the set {0.3, 0.03, 0.003, 0.0003, 0.00003}. It is set to 0.0003
based on validation performance.

For the memory, we set each slot to be 300-dimensioonal, which is the same as the RNN and em-
bedding size.

We tune the number of progressive memory slots in Figure 4, which shows the validation perfor-
mance on the source (Fic) and target (Gov) domains. We see that the performance is close to
ﬁne-tuning alone if only one memory slot is added. It improves quickly between 1 and 200 slots,
and tapers off around 500. We thus choose to add 500 slots for each domain.

C ADDITIONAL EXPERIMENT ON DIALOGUE GENERATION

We further evaluate our approach on the task of dialogue response generation. Given an input
text sequence, the task is to generate an appropriate output text sequence as a response in human-
computer dialogue. This supplementary experiment provides additional evidence of our approach in
generation tasks.

Datasets, Setup, and Metrics. We use the Cornell Movie Dialogs Corpus (Danescu-Niculescu-
Mizil & Lee, 2011) as the source. It contains ∼220k message-response pairs from movie transcripts.
We use a 200k-10k-10k training-validation-test split.

For the target domain, we manually construct a very small dataset from the Ubuntu Dialogue Cor-
pus (Lowe et al., 2015) to mimic the scenario where quick adaptation has to be done to a new domain
with little training data. In particular, we choose a random subset of 15k message-response pairs,
and use a 9k-3k-3k split.

The base model is a sequence-to-sequence (Seq2Seq) neural network (Sutskever et al., 2014) with
attention from the decoder to the encoder. We use a single-layer RNN encoder and a single-layer
RNN decoder, each containing 1024 cells following Sutskever et al. (2014). We use GRUs instead
of LSTM units due to efﬁciency concerns. We have separate memory banks for the encoder and
decoder, since they are essentially different RNNs. The source and target vocabularies are 27k and
10k, respectively. Each memory slot is 1024D, because the RNN states are 1024D in this experiment.
For each domain, we progressively add 1024 slots; tuning the number of slots is done in a manner
similar to the MultiNLI experiment. As before, we use Adam with an initial learning rate of 0.0003
and other default parameters.

Following previous work, we use BLEU-2 (Eric et al., 2017; Madotto et al., 2018) and average
Word2Vec embedding similarity (W2V-Sim, Serban et al., 2017; Zhang et al., 2018a) as the eval-
uation metrics. BLEU-2 is the geometric mean of unigram and bigram word precision penalized
by length, and correlates with human satisfaction to some extent (Liu et al., 2016). W2V-Sim is
deﬁned as the cosine similarity between the averaged Word2Vec embeddings of the model outputs

15

Published as a conference paper at ICLR 2020

#

1
2
3
4
5
6
7
8
9
10
11
12

Line Model Trained on/by

RNN

+
N
N
R

e

S
T
m S
T
M
S+T
S→T (F)
S→T (F+M)
S→T (F+M+V)
S→T (F+H)
S→T (F+H+V)
S→T (EWC)
S→T (Progressive)

m
e
M
+
N
N
R

BLEU-2 on
T
S
0.738⇓
2.842⇑
1.265⇓
0.795⇓
0.712⇓
3.074⇑
1.287⇓
0.920⇓
0.889⇓
2.650⇑
1.101⇓
1.210⇓
1.207⇓
1.435⇓
1.637
1.652
1.606↓
1.036⇓
1.419⇓
1.257⇓
1.382↓
1.397⇓
1.408↓
1.299⇓

W2V-Sim on
T
S
0.456⇓
0.480⇓
0.480⇓
0.454⇓
0.471⇓
0.498⇓
0.487⇓
0.462⇓
0.462⇓
0.471⇓
0.514⇓
0.509⇓
0.526
0.522
0.525
0.522
0.495⇓
0.503⇓
0.492⇓
0.504⇓
0.514⇓
0.513⇓
0.503⇓
0.502⇓

Table 5: Results on two-domain adaptation for dialogue response generation. F: Fine-tuning. V:
Expanding vocabulary. H: Expanding RNN hidden states. M: Our proposed method of expand-
ing memory. We also compare with EWC (Kirkpatrick et al., 2017) and progressive neural net-
work (Rusu et al., 2016). ↑, ↓: p < 0.05 and ⇑, ⇓: p < 0.01 (compared with Line 8).

Prompt
i’m steve. what’s your name?
i love you man
so how’s it going with your new roomie?
i didn’t do anything wrong did i
i just got a messed up stomach
what are you trying to handicap me?
im scared
i kinda forgot the ﬁx i tried a year ago
how would i remaster the ubuntu install
my terminal is stuck
i am rebooting now
i get an installation error
what does the kernel-package do?
what directory should i be in?

Response
my name is opera
i love you too
it should be ﬁne
we don’t have much experience
i know, you had to do some things
this is not really who i am
what do we do with it?
can you try the output ?
could you have a link both access to the network connection
what version is it?
so your issue is a great work cd, sometimes i get some
i am getting an expert for it
you will need to get it to see if there is a setting
you should be able to install the grub cd at the drive

Table 6: Sample outputs of our IDA model S→T (F+M+V) from Table 5.

and the ground truths. Intuitively, BLEU measures hard word-level overlap between two sequences,
whereas W2V-Sim measures soft similarity in a distributed semantic space.

Results. The results for dialogue response generation are shown in Table 5. We see that BLEU-2 and
W2V similarity are not necessarily consistent. For example, the memory-augmented RNN trained
solely on source achieves the best source BLEU-2, whereas the proposed progressive memory has
the highest W2V cosine similarity on S. However, our model variants achieve the best performance
on most metrics (Lines 7 and 8). Moreover, it consistently outperforms all other IDA approaches.
Following the previous experiment, we conduct statistical comparison with Line 8. The test shows
that our method is signiﬁcantly better than the other IDA methods.

In general, the evaluation of dialogue systems is noisy due to the lack of appropriate metrics (Liu
et al., 2016). Nevertheless, our experiment provides additional evidence of the effectiveness of our
approach. It also highlights our model’s viability for both classiﬁcation and generation tasks.

Case Study. Table 6 shows sample outputs of our IDA model on test prompts from the Cornell
Movie Corpus (source) and the Ubuntu Dialogue Corpus (target). We see that casual prompts from
the movie domain result in casual responses, whereas Ubuntu queries result in Ubuntu-related re-
sponses. With the expansion of vocabulary, our model is able to learn new words like “grub”; with
progressive memory, it learns Ubuntu jargon like “network connection.” This shows evidence of the
success of incremental domain adaptation.

16

0
2
0
2
 
b
e
F
 
4
1
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
2
0
0
.
1
1
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2020

PROGRESSIVE MEMORY BANKS FOR INCREMENTAL
DOMAIN ADAPTATION

Nabiha Asghar∗†◦ Lili Mou∗‡ Kira A. Selby†◦ Kevin D. Pantasdo◦ Pascal Poupart†◦ Xin Jiang♦
†Vector Institute for AI, Toronto, Canada
◦Cheriton School of Computer Science, University of Waterloo, Canada
{nasghar,kaselby,kevin.pantasdo,ppoupart}@uwaterloo.ca
‡Dept. Computing Science, University of Alberta; Alberta Machine Intelligence Institute (AMII)
doublepower.mou@gmail.com
♦Noah’s Ark Lab, Huawei Technologies, Hong Kong
jiang.xin@huawei.com

ABSTRACT

This paper addresses the problem of incremental domain adaptation (IDA) in natu-
ral language processing (NLP). We assume each domain comes one after another,
and that we could only access data in the current domain. The goal of IDA is to
build a uniﬁed model performing well on all the domains that we have encoun-
tered. We adopt the recurrent neural network (RNN) widely used in NLP, but
augment it with a directly parameterized memory bank, which is retrieved by an
attention mechanism at each step of RNN transition. The memory bank provides a
natural way of IDA: when adapting our model to a new domain, we progressively
add new slots to the memory bank, which increases the number of parameters,
and thus the model capacity. We learn the new memory slots and ﬁne-tune exist-
ing parameters by back-propagation. Experimental results show that our approach
achieves signiﬁcantly better performance than ﬁne-tuning alone. Compared with
expanding hidden states, our approach is more robust for old domains, shown by
both empirical and theoretical results. Our model also outperforms previous work
of IDA including elastic weight consolidation and progressive neural networks in
the experiments.1

1

INTRODUCTION

Domain adaptation aims to transfer knowledge from one domain (called the source domain) to
another (called the target domain) in a machine learning system.2 If the data of the target domain are
not large enough, using data from the source domain typically helps to improve model performance
in the target domain. This is important for neural networks, which are data-hungry and prone to
overﬁtting. In this paper, we especially focus on incremental domain adaptation (IDA)3, where we
assume different domains come sequentially one after another. We only have access to the data in
the current domain, but hope to build a uniﬁed model that performs well on all the domains that we
have encountered (Xu et al., 2014; Rusu et al., 2016; Kirkpatrick et al., 2017).

Incremental domain adaptation is useful in scenarios where data are proprietary, or available only for
a short period of time (Li & Hoiem, 2018). It is desired to preserve as much knowledge as possible
in the learned model and not to rely on the availability of the data. Another application of IDA
is a quick adaptation to new domains. If the environment of a deployed machine learning system
changes frequently, traditional methods like jointly training all domains require the learning machine
to be re-trained from scratch every time a new domain comes. Fine-tuning a neural network by a

∗ Equal contribution.
1Our IDA code is available at https://github.com/nabihach/IDA.
2In our work, the domain is deﬁned by datasets. Usually, the data from different genres or times typically

have different underlying distributions.

3In the literature, IDA is sometimes referred to as incremental, continual, or lifelong learning. We use

“incremental domain adaptation” in this paper to emphasize that our domain is not changed continuously.

1

Published as a conference paper at ICLR 2020

few steps of gradient updates does transfer quickly, but it suffers from the catastrophic forgetting
problem (Kirkpatrick et al., 2017). Suppose during prediction a data point is not labeled with its
domain, the (single) ﬁne-tuned model cannot predict well for samples in previous domains, as it
tends to “forget” quickly during ﬁne-tuning.

A recent trend of domain adaptation in the deep learning regime is the progressive neural net-
work (Rusu et al., 2016), which progressively grows the network capacity if a new domain comes.
Typically, this is done by enlarging the model with new hidden states and a new predictor (Fig-
ure 1a). To avoid interfering with existing knowledge, the newly added hidden states are not fed
back to the previously trained states. During training, all existing parameters are frozen, and only
the newly added ones are trained. For inference, the new predictor is used for all domains, which is
sometimes undesired as the new predictor is trained with only the last domain.

In this paper, we propose a progressive memory bank for incremental domain adaptation in natural
language processing (NLP). Our model augments a recurrent neural network (RNN) with a memory
bank, which is a set of distributed, real-valued vectors capturing domain knowledge. The memory
is retrieved by an attention mechanism during RNN information processing. When our model is
adapted to new domains, we progressively increase the slots in the memory bank. But different
from Rusu et al. (2016), we ﬁne-tune all the parameters, including RNN and the previous memory
bank. Empirically, when the model capacity increases, the RNN does not forget much even if
the entire network is ﬁne-tuned. Compared with expanding RNN hidden states, the newly added
memory slots cause less contamination of the existing knowledge in RNN states, as will be shown
by a theorem.

In our paper, we evaluate the proposed approach on a classiﬁcation task known as multi-genre nat-
ural language inference (MultiNLI). Appendix C provides additional evidence when our approach
is applied to text generation. Experiments consistently support our hypothesis that the proposed
approach adapts well to target domains without catastrophic forgetting of the source. Our model
outperforms the na¨ıve ﬁne-tuning method, the original progressive neural network, as well as other
IDA techniques including elastic weight consolidation (EWC, Kirkpatrick et al., 2017).

2 RELATED WORK

Domain Adaptation. Domain adaptation has been widely studied in machine learning, including
the NLP domain. For neural NLP applications, Mou et al. (2016) analyze two straightforward set-
tings, namely, multi-task learning (jointly training all domains) and ﬁne-tuning (training one domain
and ﬁne-tuning on the other). A recent advance of domain adaptation is adversarial learning, where
the neural features are trained not to classify the domain (Ganin et al., 2016; Liu et al., 2017). How-
ever, all these approaches (except ﬁne-tuning) require all domains to be available simultaneously,
and thus are not IDA approaches.

Kirkpatrick et al. (2017) address the catastrophic forgetting problem of ﬁne-tuning neural networks,
and propose a regularization term based on the Fisher information matrix; they call the method
elastic weight consolidation (EWC). While some follow-up studies report EWC achieves high per-
formance in their scenarios (Zenke et al., 2017; Lee et al., 2017; Thompson et al., 2019), others
show that EWC is less effective (Wen & Itti, 2018; Yoon et al., 2018; Wu et al., 2018). Lee et al.
(2017) propose incremental moment matching between the posteriors of the old model and the new
model, achieving similar performance to EWC. Schwarz et al. (2018) augment EWC with knowl-
edge distillation, making it more memory-efﬁcient.

Rusu et al. (2016) propose a progressive neural network that progressively increases the number
of hidden states (Figure 1a). To avoid overriding existing information, they freeze the weights of
the learned network, and do not feed new states to old ones. This results in multiple predictors,
requiring that a data sample is labeled with its domain during the test time. If we otherwise use
the last predictor to predict samples from all domains, its performance may be low for previous
domains, as the predictor is only trained with the last domain.

Yoon et al. (2018) propose an extension of the progressive network. They identify which existing
hidden units are relevant for the new task (with their sparse penalty), and ﬁne-tune only the corre-
sponding subnetwork. However, sparsity is not common for RNNs in NLP applications, as sparse
recurrent connections are harmful. A similar phenomenon is that dropout of recurrent connections

2

Published as a conference paper at ICLR 2020

(a) Progressive
Figure 1:
neural network Rusu et al.
(2016). (b) One step of RNN
transition in our progressive
memory network. Colors in-
dicate different domains.

yields poor performance (Bayer et al., 2013). Xu & Zhu (2018) deal with new domains by adap-
tively adding nodes to the network via reinforcement learning. This approach may require a very
large number of trials to identify the right number of nodes to be added to each layer (Yoon et al.,
2019).

Li & Hoiem (2018) address IDA with a knowledge distillation approach, where they preserve a set
of outputs of the old network on pseudo-training data. Then they jointly optimize for the accuracy
on the new training domain as well as the pseudo-training data. Kim et al. (2019)’s variant of
this approach uses maximum-entropy regularization to control the transfer of distilled knowledge.
However, in NLP applications, it is non-trivial to obtain pseudo-training data for distillation.

Memory-Based Neural Networks. Our work is related to memory-based neural networks.
Sukhbaatar et al. (2015) propose an end-to-end memory network that assigns each memory slot
to an entity, and aggregates information by multiple attention-based layers. They design their archi-
tecture for the bAbI question answering task, and assign a slot to each sentence. Such idea can be
extended to various scenarios, for example, assigning slots to external knowledge for question an-
swering (Das et al., 2017) and assigning slots to dialogue history for a conversation system (Madotto
et al., 2018).

A related idea is to use episodic memory, which stores data samples from all previously seen do-
mains (thus it is not an IDA approach). This is used for experience replay while training on subse-
quent domains (Lopez-Paz & Ranzato, 2017; Rebufﬁ et al., 2017; Chaudhry et al., 2018; d’Autume
et al., 2019).

Another type of memory in the neural network regime is the neural Turing machine (NTM, Graves
et al., 2016). This memory is not directly parameterized, but is read or written by a neural controller.
Therefore, it serves as temporary scratch paper and does not store knowledge itself. Zhang et al.
(2018b) combine the above two styles of memory for task-oriented dialogue systems, where they
have both slot-value memory and read-and-write memory.

Different from the work above, our memory bank stores knowledge in a distributed fashion, where
each slot does not correspond to a concrete entity or data sample. Our memory is directly parame-
terized, interacting in a different way from RNN weights and providing a natural way of incremental
domain adaptation.

3 PROPOSED APPROACH

Our model is based on a recurrent neural network (RNN). At each step, the RNN takes the embed-
ding of the current input (e.g., a word), and changes its states accordingly. This is represented by
hi = RNN(hi−1, xi), where hi and hi−1 are the hidden states at time steps i and i−1, respectively.
xi is the input at the ith step. Typically, long short term memory (LSTM, Hochreiter & Schmidhu-
ber, 1997) or Gated Recurrent Units (GRU, Cho et al., 2014) are used as RNN transitions. In the
rest of this section, we will describe a memory-augmented RNN, and how it is used for incremental
domain adaptation (IDA).

3

Published as a conference paper at ICLR 2020

3.1 AUGMENTING RNN WITH MEMORY BANKS

We enhance the RNN with an external memory bank, as shown in Figure 1b. The memory bank
augments the overall model capacity by storing additional parameters in memory slots. At each
time step, our model computes an attention probability to retrieve memory content, which is then
fed to the computation of RNN transition.

Particularly, we adopt a key-value memory bank, inspired by Miller et al. (2016). Each memory slot
contains a key vector and a value vector. The former is used to compute the attention weight for
memory retrieval, whereas the latter is the value of memory content.

For the ith step, the memory mechanism computes an attention probability αi by

(cid:101)αi,j = exp{h(cid:62)

i−1m(key)

j

}, αi,j =

(cid:101)αi,j
j(cid:48)=1 (cid:101)αi,j(cid:48)

(cid:80)N

where m(key)
is the key vector of the jth slot of the memory (among N slots in total). Then the
model retrieves memory content by a weighted sum of all memory values, where the weight is the
attention probability, given by

j

(1)

(2)

(cid:88)N

ci =

j=1

αi,jm(val)

j

Here, m(val)
concatenated with the current word xi, and fed to the RNN as input for state transition.

is the value vector of the jth memory slot. We call ci the memory content. Then, ci is

j

Using the key-value memory bank allows separate (thus more ﬂexible) computation of memory
retrieval weights and memory content, compared with traditional attention where a candidate vector
is used to compute both attention probability and attention content.

It should be emphasized that the memory bank in our model captures distributed knowledge, which
is different from other work where the memory slots correspond to speciﬁc entities (Eric et al.,
2017). The attention mechanism accomplishes memory retrieval in a “soft” manner, which means
the retrieval strength is a real-valued probability. This enables us to train both memory content and
its retrieval end-to-end, along with other neural parameters.

We would also like to point out that the memory bank alone does not help RNN much. However, it
is natural to use a memory-augmented RNN for incremental domain adaptation, as described below.

3.2 PROGRESSIVELY INCREASING MEMORY FOR INCREMENTAL DOMAIN ADAPTATION

The memory bank in Subsection 3.1 can be progressively expanded to adapt a model in a source
domain to new domains. This is done by adding new memory slots to the bank which are learned
exclusively from the target data.

Suppose the memory bank is expanded with another M slots in a new domain, in addition to previous
N slots. We then have N + M slots in total. The model computes attention probability over the
expanded memory and obtains the attention vector in the same way as Equations (1)–(2), except that
the summation is computed from 1 to N + M . This is given by

α(expand)
i,j

=

(cid:101)αi,j
(cid:80)N +M
j(cid:48)=1 (cid:101)αi,j(cid:48)

,

c(expand)
i

=

(cid:88)N +M
j=1

α(expand)
i,j m(val)

j

(3)

To initialize the expanded model, we load all previous parameters, including RNN weights and the
learned N slots, but randomly initialize the progressively expanded M slots. During training, we
update all parameters by gradient descent. That is to say, new parameters are learned from their
initializations, whereas old parameters are ﬁne-tuned during IDA. The process is applied whenever
a new domain comes, as shown in Algorithm 1.

We would like to discuss the following issues.

4

Published as a conference paper at ICLR 2020

Freezing vs. Fine-tuning learned parame-
ters. Inspired by the progressive neural net-
work (Rusu et al., 2016), we ﬁnd it tempt-
ing to freeze RNN parameters and the learned
memory but only tune new memory for IDA.
However, our preliminary results show that
if we freeze all existing parameters, the in-
creased memory does not add much to the
model capacity, and that its performance is
worse than ﬁne-tuning all parameters.

Algorithm 1: Progressive Memory for IDA
Input: A sequence of domains D0, D1, · · · , Dn
Output: A single model for all domains
Initialize a memory-augmented RNN
Train the model on D0
for D1, · · · , Dn do

Expand the memory with new slots
Load RNN weights and existing memory banks
Train the model by updating all parameters

end
Return: The resulting model

Fine-tuning vs. Fine-tuning while increas-
ing memory slots.
It is reported that ﬁne-
tuning a model (without increasing model ca-
pacity) suffers from the problem of catastrophic forgetting (Kirkpatrick et al., 2017). We wish to
investigate whether our approach suffers from the same problem, since we ﬁne-tune learned param-
eters when progressively increasing memory slots. Our intuition is that the increased model capacity
helps to learn the new domain with less overriding of the previously learned model. Experiments
conﬁrm our conjecture, as the memory-augmented RNN tends to forget more if the memory size is
not increased.

Expanding hidden states vs. Expanding memory. Another way of progressively increasing model
capacity is to expand the size of RNN layers. This setting is similar to the progressive neural net-
work, except that all weights are ﬁne-tuned and new states are connected to existing states.

However, we hereby show a theorem, indicating that the expanded memory results in less contami-
nation/overriding of the learned knowledge in the RNN, compared with the expanded hidden states.
The main idea is to measure the effect of model expansion quantitatively by the expected square dif-
ference on hi before and after expansion, where the expectation reﬂects the average effect of model
expansion in different scenarios.
Theorem 1. Let RNN have vanilla transition with the linear activation function, and let the RNN
state at the last step hi−1 be ﬁxed. For a particular data point, if the memory attention satisﬁes
(cid:80)N +M
j=1 (cid:101)αi,j, then memory expansion yields a lower expected mean squared differ-

j=N +1 (cid:101)αi,j ≤ (cid:80)N

ence in hi than RNN state expansion. That is,

(cid:104)

i − hi(cid:107)2(cid:105)
i − hi(cid:107)2(cid:105)
refers to the hidden states if the memory is expanded. h(s)
i

(cid:107)h(m)

(cid:107)h(s)

≤ E

E

(cid:104)

i

where h(m)
refers to the original dimen-
sions of the RNN states, if we expand the size of RNN states themselves. Here, we compute the
expectation by assuming weights and hidden states are iid from a zero-mean Gaussian distribution
(with variance σ2).

(4)

Proof Sketch: We focus on one step of RNN transition and assume that hi−1 is the same when the
model capacity is increased. Further, we assume that hi is D-dimensional, that each memory slot
mj is d-dimensional, and that the additional RNN units (when we expand the hidden state) are also
d-dimensional.

We compute how the original dimensions in the hidden state are changed if we expand RNN. We
denote the expanded hidden states by (cid:101)hi−1 and (cid:102)hi for the two time steps. We denote the weights
connecting from (cid:101)hi−1 to hi by (cid:102)W ∈ RD×d. We focus on the original D-dimensional space, denoted
as h(s)
i

. The connection is shown in Figure 2a, Appendix A. We have

E(cid:2)(cid:107)h(s)

i − hi(cid:107)2(cid:3) = E(cid:2)(cid:107)(cid:102)W · (cid:101)hi−1(cid:107)2(cid:3) =

D
(cid:88)

(cid:20)(cid:16)

E

j=1

(cid:17)2(cid:21)

(cid:101)w(cid:62)

j (cid:101)hi−1

D
(cid:88)

E

(cid:34)(cid:18) d
(cid:88)

=

j=1

k=1

(cid:101)wjk(cid:101)hi−1[k])

(cid:19)2(cid:35)

D
(cid:88)

d
(cid:88)

(cid:104)(cid:0)

E

=

j=1

i=1

= D · d · Var(cid:0)w(cid:1) · Var(h) = Ddσ2σ2

(cid:1)2(cid:105)

E

(cid:101)hi−1[k](cid:1)2(cid:105)
(cid:104)(cid:0)

(cid:101)wjk

(5)

(6)

(7)

5

Published as a conference paper at ICLR 2020

Table 1: Corpus statistics and the baseline
performance (% accuracy) of our BiLSTM
model (without domain adaptation) and re-
sults reported in previous work.

Fic Gov Slate Tel Travel
77k 77k 77k 83k
# training samples
Our Implementation 65.0 66.5 56.2 64.5
64.7 69.2 57.9 64.4

Yu et al. (2018)

77k
62.7
65.8

Table 2: Results on two-domain adapta-
tion. F: Fine-tuning. V: Expanding vocab-
ulary. H: Expanding RNN hidden states. M:
Our proposed method of expanding memory.
We also compare with previous work elas-
tic weight consolidation (EWC, Kirkpatrick
et al., 2017) and the progressive neural net-
work (Rusu et al., 2016). For the statistical
test (compared with Line 8), ↑, ↓: p < 0.05
and ⇑, ⇓: p < 0.01. The absence of an ar-
row indicates that the performance difference
compared with Line 8 is statistically insignif-
icant with p lower than 0.05.

#Line Model Trained on/by

1
2
3
4
5
6
7
8
9
10
11
12

RNN

+
N
N
R

e

S
T
m S
T
M
S+T
S→T (F)
S→T (F+M)
S→T (F+M+V)
S→T (F+H)
S→T (F+H+V)
S→T (EWC)
S→T (Progressive)

m
e
M
+
N
N
R

% Accuracy on
T
61.23⇓
66.49⇓
60.87⇓
67.01⇓
70.00
69.90↓
70.21
70.82
68.35⇓
68.02⇓
64.10⇓
68.25⇓

S
65.01⇓
56.46⇓
65.41⇓
56.77⇓
66.02↓
65.62↓
66.23
67.55
64.09⇓
63.68⇓
66.02⇓
64.47⇓

Similarly,

E(cid:2)(cid:107)h(m)

= Ddσ2Var(cid:0)∆ck

i − hi(cid:107)2(cid:3) = E

(cid:13)W(c)∆c(cid:107)2(cid:105)
(cid:104)(cid:13)
where ∆c def= c(cid:48) − c. The vectors c and c(cid:48) are the current step’s attention content before and after
memory expansion, respectively, shown in Figure 2b, Appendix A. (We omit the time step in the
notation for simplicity.) W(c) is the weight matrix connecting attention content to RNN states.
To prove the theorem, it remains to show that Var(∆ck) ≤ σ2. We do this by analyzing how
attention is computed at each time step, and bounding each attention weight. For details, see the
complete proof in Appendix A.

(8)

(cid:1)

In the theorem, we have an assumption (cid:80)N +M
j=N +1 (cid:101)αi,j ≤ (cid:80)N
j=1 (cid:101)αi,j, requiring that the total attention
to existing memory slots is larger than to the progressively added slots. This is fairly reasonable
because: (1) During training, attention is trained in an ad hoc fashion to newly-added information,
and thus some αi,j for 1 ≤ j ≤ N might be learned so that it is larger than a random memory slot;
and (2) For a new domain, we do not add a huge number of slots, and thus (cid:80)N +M
j=N +1 (cid:101)αi,j will not
dominate.

It is noted that our theorem is not to provide an explicit optimization/generalization bound for IDA,
but shows that expanding memory is more stable than expanding hidden states. This is particu-
larly important at the beginning steps of IDA, as the progressively growing parameters are randomly
initialized and are basically noise. Although our theoretical analysis uses a restricted setting (i.e.,
vanilla RNN transition and linear activation), it provides the key insight that our approach is appro-
priate for IDA.

4 EXPERIMENTS

In this section, we evaluate our approach on an NLP classiﬁcation task. In particular, we choose the
multi-genre natural language inference (MultiNLI), due to its large number of samples in various
domains. The task is to determine the relationship between two sentences among target labels:
entailment, contradiction, and neutral. In Appendix C, we conduct supplementary experiments on
text generation with our memory-augmented RNN for IDA.

Dataset and Setup. The MultiNLI corpus (Williams et al., 2018) is particularly suitable for IDA,
as it contains training samples for 5 genres: Slate, Fiction (Fic), Telephone (Tel),
Government (Gov), and Travel. In total, we have 390k training samples. The corpus also
contains held-out (non-training) labeled data in these domains. We split it into two parts for valida-
tion and test.4

4MultiNLI also contains 5 genres without training samples, namely, 9/11, Face-to-face, Letters,
OUP, and Verbatim. We ignore these genres, because we focus on incremental domain adaptation instead of

6

Published as a conference paper at ICLR 2020

Training domains
Fic
Fic → Gov
Fic → Gov → Slate
Fic → Gov → Slate → Tel
Fic → Gov → Slate → Tel → Travel

Fic
65.41
67.55
67.04
68.46
69.36

Performance on
Slate
55.83
61.04
63.29
63.39
63.96

Tel
61.39
65.07
64.66
71.60
69.74

Gov
58.87
70.82
71.55
71.10
72.47

Travel
57.35
61.90
63.53
61.50
68.39

Table 3: Dynamics of the progressive memory network for IDA with 5 domains. Upper-triangular
values in gray are out-of-domain (zero-shot) performance.

Group

Non-
IDA

IDA

Setting
In-domain training
Fic + Gov + Slate + Tel + Travel (multi-task)
Fic → Gov → Slate → Tel → Travel (F+V)
Fic → Gov → Slate → Tel → Travel (F+V+M)
Fic → Gov → Slate → Tel → Travel (EWC)
Fic → Gov → Slate → Tel → Travel (Progressive)

Fic
65.41⇓
70.60↑
67.24↓
69.36
67.12⇓
65.22⇓

Gov
67.01⇓
73.30
70.82⇓
72.47
68.71⇓
67.87⇓

Slate
59.30⇓
63.80
62.41↓
63.96
59.90⇓
61.13⇓

Tel
67.20⇓
69.15
67.62↓
69.74
66.09⇓
66.96⇓

Travel
64.70⇓
67.07↓
68.39
68.39
65.70⇓
67.90

Table 4: Comparing our approach with variants and previous work in the multi-domain setting. In
this experiment, we use the memory-augmented RNN as the neural architecture. Italics represent
best results in the IDA group. ↑, ↓: p < 0.05 and ⇑, ⇓: p < 0.01 (compared with F+V+M).

The ﬁrst row in Table 1 shows the size of the training set in each domain. As seen, the corpus is
mostly balanced across domains, although Tel has slightly more examples.

For the base model, we train a bi-directional LSTM (BiLSTM). The details of network architecture,
training, and hyper-parameter tuning are given in Appendix B. We see in Table 1 that we achieve
similar performance to Yu et al. (2018). Furthermore, our BiLSTM achieves an accuracy of 68.37
on the ofﬁcial MultiNLI test set,5 which is better than 67.51 reported in the original MultiNLI paper
(Williams et al., 2018) using BiLSTM. This shows that our implementation and tuning are fair for
the basic BiLSTM, and that our model is ready for the study of IDA.

Transfer between Two Domains. We would like to compare our approach with a large number
of baselines and variants. Thus, we randomly choose two domains as a testbed: Fic as the source
domain and Gov as the target domain. We show results in Table 2.

First, we analyze the performance of RNN and the memory-augmented RNN in the non-transfer
setting (Lines 1–2 vs. Lines 3–4). As seen, the memory-augmented RNN achieves slightly better
but generally similar performance, compared with RNN (both with LSTM units). This shows that,
in the non-transfer setting, the memory bank does not help the RNN much. However, this later
conﬁrms that the performance improvement is indeed due to our IDA technique, instead of simply a
better neural architecture.

We then apply two straightforward methods of domain adaptation: multi-task learning (Line 5) and
ﬁne-tuning (Line 6). Multi-task learning jointly optimizes source and target objectives, denoted by
“S+T.” On the other hand, the ﬁne-tuning approach trains the model on the source ﬁrst, and then ﬁne-
tunes on the target. In our experiments, these two methods perform similarly on the target domain,
which is consistent with Mou et al. (2016). On the source domain, ﬁne-tuning performs signiﬁcantly
worse than multi-task learning, as it suffers from the catastrophic forgetting problem. We notice that,
in terms of source performance, the ﬁne-tuning approach (Line 6) is slightly better than trained on
the source domain only (Line 3). This is probably because our domains are somewhat correlated as
opposed to Kirkpatrick et al. (2017), and thus training with more data on target slightly improves
the performance on source. However, ﬁne-tuning does achieve the worst performance on source
compared with other domain adaptation approaches (among Lines 5–8). Thus, we nevertheless use
the terminology “catastrophic forgetting,” and our research goal is still to improve IDA performance.

The main results of our approach are Lines 7 and 8. We apply the proposed progressive memory
network to IDA and we ﬁne-tune all weights. We see that on both source and target domains,

zero-shot learning. Also, the labels for the ofﬁcial test set of MultiNLI are not publicly available, and therefore
we cannot use it to evaluate performance on individual domains. Our split of the held-out set for validation and
test applies to all competing methods, and thus is a fair setting.

5Evaluation on the ofﬁcial MultiNLI test set requires submission to Kaggle.

7

Published as a conference paper at ICLR 2020

our approach outperforms the ﬁne-tuning method alone where the memory size is not increased
(comparing Lines 7 and 6). This veriﬁes our conjecture that, if the model capacity is increased, the
new domain results in less overriding of the learned knowledge in the neural network. Our proposed
approach is also “orthogonal” to the expansion of the vocabulary size, where target-speciﬁc words
are randomly initialized and learned on the target domain. As seen, this combines well with our
memory expansion and yields the best performance on both source and target (Line 8).

We now compare an alternative way of increasing model capacity, i.e., expanding hidden states
(Lines 9 and 10). For fair comparison, we ensure that the total number of model parameters after
memory expansion is equal to the number of model parameters after hidden state expansion. We
see that the performance of hidden state expansion is poor especially on the source domain, even
if we ﬁne-tune all parameters. This experiment provides empirical evidence to our theorem that
expanding memory is more robust than expanding hidden states.

We also compare the results with previous work on IDA. We re-implement6 EWC (Kirkpatrick
et al., 2017). It does not achieve satisfactory results in our application. We investigate other pub-
lished papers using the same method and ﬁnd inconsistent results: EWC works well in some ap-
plications (Zenke et al., 2017; Lee et al., 2017) but performs poorly on others (Yoon et al., 2018;
Wu et al., 2018). Wen & Itti (2018) even report near random performance with EWC. We also
re-implement the progressive neural network (Rusu et al., 2016). We use the target predictor to do
inference for both source and target domains. Progressive neural network yields low performance,
particularly on source, probably because the predictor is trained with only the target domain.

We measure the statistical signiﬁcance of the results against Line 8 with the one-tailed Wilcoxon’s
signed-rank test (Wilcoxon, 1945), by bootstrapping a subset of 200 samples for 10 times with
replacement. The test shows our approach is signiﬁcantly better than others, on both source and
target domains.

IDA with All Domains. Having analyzed our approach, baselines, and variants on two domains in
detail, we are now ready to test the performance of IDA with multiple domains, namely, Fic, Gov,
Slate, Tel, and Travel. In this experiment, we assume these domains come one after another,
and our goal is to achieve high performance on all domains.

Table 3 shows the dynamics of IDA with our progressive memory network. Comparing the upper-
triangular values (in gray, showing out-of-domain performance) with diagonal values, we see that
our approach can be quickly adapted to the new domain in an incremental fashion. Comparing lower-
triangular values with the diagonal, we see that our approach does not suffer from the catastrophic
forgetting problem as the performance of previous domains is gradually increasing if trained with
more domains. After all data are observed, our model achieves the best performance in most domains
(last row in Table 3), despite the incremental nature of our approach.

We now compare our approach with other baselines and variants in the multi-domain setting, shown
in Table 4. Due to the large number of settings, we only choose a selected subset of variants from
Table 2 for the comparison.

As seen, our approach of progressively growing memory achieves the same performance as ﬁne-
tuning on the last domain (both with vocabulary expansion), but for all previous 4 domains, we
achieve signiﬁcantly better performance than ﬁne-tuning. Our model is comparable to multi-task
learning on all domains. It should also be mentioned that multi-task learning requires training the
model when data from all domains are available simultaneously. It is not an incremental approach
for domain adaptation, and thus cannot be applied to the scenarios introduced in Section 1. We
include this setting mainly because we are curious about the performance of non-incremental domain
adaptation.

We also compare with previous methods for IDA in Table 4. Our method outperforms EWC and the
progressive neural network in all domains; the results are consistent with Table 2.

6Implementation based on https://github.com/ariseff/overcoming-catastrophic

8

Published as a conference paper at ICLR 2020

5 CONCLUSION

In this paper, we propose a progressive memory network for incremental domain adaptation (IDA).
We augment an RNN with an attention-based memory bank. During IDA, we add new slots to the
memory bank and tune all parameters by back-propagation. Empirically, the progressive memory
network does not suffer from the catastrophic forgetting problem as in na¨ıve ﬁne-tuning. Our intu-
ition is that the new memory slots increase the neural network’s model capacity, and thus, the new
knowledge causes signiﬁcantly less overriding of the existing network. Compared with expanding
hidden states, our progressive memory bank provides a more robust way of increasing model ca-
pacity, shown by both a theorem and experiments. We also outperform previous work for IDA,
including elastic weight consolidation (EWC) and the original progressive neural network.

This work was funded by Huawei Technologies, Hong Kong. Lili Mou is supported by the Amii
Fellow Program, and the CIFAR AI Chair Program.

ACKNOWLEDGMENTS

REFERENCES

Justin Bayer, Christian Osendorfer, Daniela Korhammer, Nutan Chen, Sebastian Urban, and Patrick
van der Smagt. On fast dropout and its applicability to recurrent networks. arXiv preprint
arXiv:1311.0701, 2013.

Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient

lifelong learning with A-GEM. arXiv preprint arXiv:1812.00420, 2018.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder–decoder
for statistical machine translation. In EMNLP, pp. 1724–1734, 2014.

Cristian Danescu-Niculescu-Mizil and Lillian Lee. Chameleons in imagined conversations: A new
approach to understanding coordination of linguistic style in dialogs. In Proc. Workshop on Cog-
nitive Modeling and Computational Linguistics, pp. 76–87, 2011.

Rajarshi Das, Manzil Zaheer, Siva Reddy, and Andrew McCallum. Question answering on knowl-
edge bases and text using universal schema and memory networks. In ACL, pp. 358–365, 2017.

Cyprien de Masson d’Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic

memory in lifelong language learning. arXiv preprint arXiv:1906.01076, 2019.

Mihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. Key-value retrieval

networks for task-oriented dialogue. In SIGDIAL, pp. 37–49, 2017.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. JMLR, 17(1):2096–2030, 2016.

Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwi´nska, Sergio G´omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538
(7626):471–476, 2016.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):

1735–1780, 1997.

Dahyun Kim, Jihwan Bae, Yeonsik Jo, and Jonghyun Choi. Incremental learning with maximum en-
tropy regularization: Rethinking forgetting and intransigence. arXiv preprint arXiv:1902.00829,
2019.

9

Published as a conference paper at ICLR 2020

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences,
114(13):3521–3526, 2017.

Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming
catastrophic forgetting by incremental moment matching. In NeurIPS, pp. 4652–4662, 2017.

Zhizhong Li and Derek Hoiem. Learning without forgetting.

IEEE TPAMI, 40(12):2935–2947,

2018.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.
How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics
for dialogue response generation. In EMNLP, pp. 2122–2132, 2016.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task learning for text classiﬁca-

tion. In ACL, pp. 1–10, 2017.

NeurIPS, pp. 6467–6476, 2017.

David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. The Ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dialogue systems. In SIGDIAL, pp. 285–294, 2015.

Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. Mem2seq: Effectively incorporating knowl-

edge bases into end-to-end task-oriented dialog systems. In ACL, pp. 1468–1478, 2018.

Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason
In EMNLP, pp. 1400–

Weston. Key-value memory networks for directly reading documents.
1409, 2016.

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. How transferable are neural

networks in NLP applications? In EMNLP, pp. 479–489, 2016.

Jeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global vectors for word

representation. In EMNLP, pp. 1532–1543, 2014.

Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.

icarl:

Incremental classiﬁer and representation learning. In CVPR, pp. 2001–2010, 2017.

Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016.

Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye
Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for contin-
ual learning. In ICML, pp. 4535–4544, 2018.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C
Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for gen-
erating dialogues. In AAAI, pp. 3295–3301, 2017.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In NIPS, pp.

2440–2448, 2015.

In NIPS, pp. 3104–3112, 2014.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.

Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. Overcoming
catastrophic forgetting during domain adaptation of neural machine translation. In NAACL, pp.
2062–2068, 2019.

Shixian Wen and Laurent Itti. Overcoming catastrophic forgetting problem by weight consolidation

and long-term memory. arXiv preprint arXiv:1805.07441, 2018.

10

Published as a conference paper at ICLR 2020

Frank Wilcoxon.

Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80–83,

1945.

Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-

tence understanding through inference. In NAACL-HLT, pp. 1112–1122, 2018.

Chenshen Wu, Luis Herranz, Xialei Liu, Yaxing Wang, Joost van de Weijer, and Bogdan Raducanu.
Memory replay GANs: learning to generate images from new categories without forgetting. arXiv
preprint arXiv:1809.02058, 2018.

Jiaolong Xu, Sebastian Ramos, David V´azquez, Antonio M L´opez, and D Ponsa.

Incremental

domain adaptation of deformable part-based models. In BMVC, 2014.

Ju Xu and Zhanxing Zhu. Reinforced continual learning. In NeurIPS, pp. 899–908. 2018.

Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically

expandable networks. ICLR, 2018.

Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Oracle: Order robust adaptive

continual learning. arXiv preprint arXiv:1902.09432, 2019.

Jianfei Yu, Minghui Qiu, Jing Jiang, Jun Huang, Shuangyong Song, Wei Chu, and Haiqing Chen.
Modelling domain relationships for transfer learning on retrieval-based question answering sys-
tems in e-commerce. In WSDM, pp. 682–690, 2018.

Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.

In ICML, pp. 3987–3995, 2017.

Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, Xiujun Li, Chris Brockett, and Bill Dolan.
Generating informative and diverse conversational responses via adversarial information maxi-
mization. arXiv preprint arXiv:1809.05972, 2018a.

Zheng Zhang, Minlie Huang, Zhongzhou Zhao, Feng Ji, Haiqing Chen, and Xiaoyan Zhu.
Memory-augmented dialogue management for task-oriented dialogue systems. arXiv preprint
arXiv:1805.00150, 2018b.

11

Published as a conference paper at ICLR 2020

Figure 2: Hidden state expansion vs. memory expansion at step t.

A PROOF OF THEOREM 1

Theorem 1. Let RNN have vanilla transition with the linear activation function, and let the RNN
state at the last step hi−1 be ﬁxed. For a particular data point, if the memory attention satisﬁes
(cid:80)N +M
j=1 (cid:101)αi,j, then memory expansion yields a lower expected mean squared differ-

j=N +1 (cid:101)αi,j ≤ (cid:80)N

ence in hi than RNN state expansion. That is,

(cid:104)

i − hi(cid:107)2(cid:105)
i − hi(cid:107)2(cid:105)
refers to the hidden states if the memory is expanded. h(s)
i

(cid:107)h(m)

(cid:107)h(s)

≤ E

E

(cid:104)

i

where h(m)
refers to the original dimen-
sions of the RNN states, if we expand the size of RNN states themselves. Here, we compute the
expectation by assuming weights and hidden states are iid from a zero-mean Gaussian distribution
(with variance σ2).

(9)

Proof: Let hi−1 be the hidden state of the last step. We focus on one step of transition and assume
that hi−1 is the same when the model capacity is increased. We consider a simpliﬁed case where
the RNN has vanilla transition with the linear activation function. We measure the effect of model
expansion quantitatively by the expected square difference on hi before and after model expansion.

Suppose the original hidden state hi is D-dimensional. We assume each memory slot is d-
dimensional, and that the additional RNN units when expanding the hidden state are also d-
dimensional. We further assume each variable in the expanded memory and expanded weights ((cid:102)W
in Figure 2) are iid with zero mean and variance σ2. This assumption is reasonable as it enables a fair
comparison of expanding memory and expanding hidden states. Finally, we assume every variable
in the learned memory slots, i.e., mjk, follows the same distribution (zero mean, variance σ2).

We compute how the original dimensions in the hidden state are changed if we expand RNN. We
denote the expanded hidden states by (cid:101)hi−1 and (cid:101)hi for the two time steps. We denote the weights
connecting from (cid:101)hi−1 to hi by (cid:102)W ∈ RD×d. We focus on the original D-dimensional space, denoted
as h(s)
i

. The connection is shown in Figure 2a. We have

E(cid:2)(cid:107)h(s)

i − hi(cid:107)2(cid:3)

= E(cid:2)(cid:107)(cid:102)W · (cid:101)hi−1(cid:107)2(cid:3)
(cid:16)

(cid:20) D
(cid:88)

= E

(cid:101)w(cid:62)

j (cid:101)hi−1

j=1
(cid:20)(cid:16)

E

=

D
(cid:88)

j=1

(cid:101)w(cid:62)

j (cid:101)hi−1

(cid:17)2(cid:21)

(cid:17)2(cid:21)

12

(10)

(11)

(12)

Published as a conference paper at ICLR 2020

D
(cid:88)

E

(cid:34)(cid:18) d
(cid:88)

j=1

k=1

(cid:19)2(cid:35)

(cid:101)wjk(cid:101)hi−1[k])

(cid:104)(cid:0)

E

(cid:101)wjk(cid:101)hi−1[k](cid:1)2(cid:105)

=

=

=

D
(cid:88)

d
(cid:88)

j=1

k=1

D
(cid:88)

d
(cid:88)

j=1

k=1

(cid:104)(cid:0)

E

(cid:101)wjk

(cid:1)2(cid:105)

E

(cid:104)(cid:0)
(cid:101)hi−1[k](cid:1)2(cid:105)

= D · d · Var(cid:0)w(cid:1) · Var(h)
= Ddσ2σ2

where (14) is due to the independence and zero-mean assumptions of every element in (cid:102)W and hi−1.
(15) is due to the independence assumption between (cid:102)W and hi−1.
Next, we compute the effect of expanding memory slots. Notice that (cid:107)h(m)
i − hi(cid:107) = W(c)∆c. Here,
is the RNN hidden state after memory expansion. ∆c def= c(cid:48) −c, where c and c(cid:48) are the attention
h(m)
i
content vectors before and after memory expansion, respectively, at the current time step.7 W(c) is
the weight matrix connecting attention content to RNN states. The connection is shown in Figure 2b.
Reusing the result of (16), we immediately obtain

E(cid:2)(cid:107)h(m)

i − hi(cid:107)2(cid:3)

(cid:13)W(c)∆c(cid:107)2(cid:105)
(cid:104)(cid:13)
= E
(cid:1)
= Ddσ2Var(cid:0)∆ck

where ∆ck is an element of the vector ∆c.
To prove Equation (2), it remains to show that Var(∆ck) ≤ σ2. We now analyze how attention is
computed.
Let (cid:101)α1, · · · , (cid:101)αN +M be the unnormalized attention weights over the N +M memory slots. We notice
that (cid:101)α1, · · · , (cid:101)αN remain the same after memory expansion. Then, the original attention probability
is given by αj = (cid:101)αj/((cid:101)α1 + · · · + (cid:101)αN ) for j = 1, · · · , N . After memory expansion, the attention
probability becomes α(cid:48)

j = (cid:101)αj/((cid:101)α1 + · · · + (cid:101)αN +M ), illustrated in Figure 3. We have

(cid:19)

mj +

N +M
(cid:88)

(cid:16)

j=N +1

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

(cid:17)

mj

(cid:16)

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

(cid:17)

mj

∆c = c(cid:48) − c

N
(cid:88)

j=1

(α(cid:48)

j − αj)mj +

α(cid:48)

jmj

N +M
(cid:88)

j=N +1

N
(cid:88)

(cid:18)

j=1

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

−

N
(cid:88)

(cid:18) −(cid:101)αj

(cid:101)αN +1+···+ (cid:101)αN +M
(cid:101)α1+···+ (cid:101)αN

(cid:101)α1 + · · · + (cid:101)αN +M

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN
(cid:19)

N +M
(cid:88)

mj +

j=N +1

=

=

=

def=

j=1

N +M
(cid:88)

j=1

βjmj

where

βj

def=





(cid:101)αN +1+···+ (cid:101)αN +M
(cid:101)α1+···+ (cid:101)αN

−(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M
(cid:101)αj
,
(cid:101)α1 + · · · + (cid:101)αN +M

,

if 1 ≤ j ≤ N

if N +1 ≤ j ≤ N + M

7We omit the time step in the notation for simplicity.

13

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

Published as a conference paper at ICLR 2020

Figure 3: Attention probabilities before and after memory expansion.

By our assumption of total attention (cid:80)N +M
|βj| ≤ |α(cid:48)

j|,

j=N +1 (cid:101)αj ≤ (cid:80)N

j=1 (cid:101)αj, we have

∀1 ≤ j ≤ N + M

Then, we have

Var(∆ck) = E[(c(cid:48)

k − ck)2(cid:3) ∀1 ≤ k ≤ d

E(cid:2)(cid:107)c(cid:48) − c(cid:107)2(cid:3)
(cid:34) d

(cid:88)

(cid:18) N +M
(cid:88)

E

k=1

j=1
(cid:34)(cid:18) N +M
(cid:88)

d
(cid:88)

E

k=1

j=1

(cid:19)2(cid:35)

(cid:19)2(cid:35)

βjmjk

βjmjk

(cid:104)(cid:0)βjmjk
E

(cid:1)2(cid:105)

E(cid:2)β2

(cid:3)E(cid:2)m2

j

jk

(cid:3)

=

=

=

=

=

=

1
d

1
d

1
d

1
d

1
d

1
d

d
(cid:88)

N +M
(cid:88)

k=1

j=1

d
(cid:88)

N +M
(cid:88)

k=1

j=1

d
(cid:88)

N +M
(cid:88)

k=1


j=1

N +M
(cid:88)







j=1

N +M
(cid:88)

j=1

= σ2E

≤ σ2E

E[β2

j ]σ2



β2
j





(α(cid:48)

j)2



(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)
Here, (31) is due to the assumption that mjk is independent and zero-mean, and (32) is due to the
independence assumption between βj and mjk. To obtain (36), we notice that (cid:80)N +M
j = 1 with
0 ≤ α(cid:48)

j ≤ 1 (∀1 ≤ j ≤ N + M ). Thus, (cid:80)N +M

j)2 ≤ 1, concluding our proof.

j=1 α(cid:48)

(α(cid:48)

j=1

≤ σ2

B HYPERPARAMETERS

We choose the base model and most of its settings by following the original MultiNLI paper
(Williams et al., 2018): 300D RNN hidden states, 300D pretrained GloVe embeddings (Penning-
ton et al., 2014) for initialization, batch size of 32, and the Adam optimizer for training. The initial

14

Published as a conference paper at ICLR 2020

S
n
o

.
c
c
A
n
o
i
t
a
d
i
l
a
V

68

67

66

65

1

71

70.5

70

T
n
o

.
c
c
A
n
o
i
t
a
d
i
l
a
V

69.5

1

100

200

300

400

500

100

200

300

400

500

# of Memory Slots

# of Memory Slots

(a)

(b)

Figure 4: Tuning the number of memory slots to be added per domain in the MultiNLI experiment.
The two graphs show validation performance of our IDA model S→T (F+M+V).

learning rate for Adam is tuned over the set {0.3, 0.03, 0.003, 0.0003, 0.00003}. It is set to 0.0003
based on validation performance.

For the memory, we set each slot to be 300-dimensioonal, which is the same as the RNN and em-
bedding size.

We tune the number of progressive memory slots in Figure 4, which shows the validation perfor-
mance on the source (Fic) and target (Gov) domains. We see that the performance is close to
ﬁne-tuning alone if only one memory slot is added. It improves quickly between 1 and 200 slots,
and tapers off around 500. We thus choose to add 500 slots for each domain.

C ADDITIONAL EXPERIMENT ON DIALOGUE GENERATION

We further evaluate our approach on the task of dialogue response generation. Given an input
text sequence, the task is to generate an appropriate output text sequence as a response in human-
computer dialogue. This supplementary experiment provides additional evidence of our approach in
generation tasks.

Datasets, Setup, and Metrics. We use the Cornell Movie Dialogs Corpus (Danescu-Niculescu-
Mizil & Lee, 2011) as the source. It contains ∼220k message-response pairs from movie transcripts.
We use a 200k-10k-10k training-validation-test split.

For the target domain, we manually construct a very small dataset from the Ubuntu Dialogue Cor-
pus (Lowe et al., 2015) to mimic the scenario where quick adaptation has to be done to a new domain
with little training data. In particular, we choose a random subset of 15k message-response pairs,
and use a 9k-3k-3k split.

The base model is a sequence-to-sequence (Seq2Seq) neural network (Sutskever et al., 2014) with
attention from the decoder to the encoder. We use a single-layer RNN encoder and a single-layer
RNN decoder, each containing 1024 cells following Sutskever et al. (2014). We use GRUs instead
of LSTM units due to efﬁciency concerns. We have separate memory banks for the encoder and
decoder, since they are essentially different RNNs. The source and target vocabularies are 27k and
10k, respectively. Each memory slot is 1024D, because the RNN states are 1024D in this experiment.
For each domain, we progressively add 1024 slots; tuning the number of slots is done in a manner
similar to the MultiNLI experiment. As before, we use Adam with an initial learning rate of 0.0003
and other default parameters.

Following previous work, we use BLEU-2 (Eric et al., 2017; Madotto et al., 2018) and average
Word2Vec embedding similarity (W2V-Sim, Serban et al., 2017; Zhang et al., 2018a) as the eval-
uation metrics. BLEU-2 is the geometric mean of unigram and bigram word precision penalized
by length, and correlates with human satisfaction to some extent (Liu et al., 2016). W2V-Sim is
deﬁned as the cosine similarity between the averaged Word2Vec embeddings of the model outputs

15

Published as a conference paper at ICLR 2020

#

1
2
3
4
5
6
7
8
9
10
11
12

Line Model Trained on/by

RNN

+
N
N
R

e

S
T
m S
T
M
S+T
S→T (F)
S→T (F+M)
S→T (F+M+V)
S→T (F+H)
S→T (F+H+V)
S→T (EWC)
S→T (Progressive)

m
e
M
+
N
N
R

BLEU-2 on
T
S
0.738⇓
2.842⇑
1.265⇓
0.795⇓
0.712⇓
3.074⇑
1.287⇓
0.920⇓
0.889⇓
2.650⇑
1.101⇓
1.210⇓
1.207⇓
1.435⇓
1.637
1.652
1.606↓
1.036⇓
1.419⇓
1.257⇓
1.382↓
1.397⇓
1.408↓
1.299⇓

W2V-Sim on
T
S
0.456⇓
0.480⇓
0.480⇓
0.454⇓
0.471⇓
0.498⇓
0.487⇓
0.462⇓
0.462⇓
0.471⇓
0.514⇓
0.509⇓
0.526
0.522
0.525
0.522
0.495⇓
0.503⇓
0.492⇓
0.504⇓
0.514⇓
0.513⇓
0.503⇓
0.502⇓

Table 5: Results on two-domain adaptation for dialogue response generation. F: Fine-tuning. V:
Expanding vocabulary. H: Expanding RNN hidden states. M: Our proposed method of expand-
ing memory. We also compare with EWC (Kirkpatrick et al., 2017) and progressive neural net-
work (Rusu et al., 2016). ↑, ↓: p < 0.05 and ⇑, ⇓: p < 0.01 (compared with Line 8).

Prompt
i’m steve. what’s your name?
i love you man
so how’s it going with your new roomie?
i didn’t do anything wrong did i
i just got a messed up stomach
what are you trying to handicap me?
im scared
i kinda forgot the ﬁx i tried a year ago
how would i remaster the ubuntu install
my terminal is stuck
i am rebooting now
i get an installation error
what does the kernel-package do?
what directory should i be in?

Response
my name is opera
i love you too
it should be ﬁne
we don’t have much experience
i know, you had to do some things
this is not really who i am
what do we do with it?
can you try the output ?
could you have a link both access to the network connection
what version is it?
so your issue is a great work cd, sometimes i get some
i am getting an expert for it
you will need to get it to see if there is a setting
you should be able to install the grub cd at the drive

Table 6: Sample outputs of our IDA model S→T (F+M+V) from Table 5.

and the ground truths. Intuitively, BLEU measures hard word-level overlap between two sequences,
whereas W2V-Sim measures soft similarity in a distributed semantic space.

Results. The results for dialogue response generation are shown in Table 5. We see that BLEU-2 and
W2V similarity are not necessarily consistent. For example, the memory-augmented RNN trained
solely on source achieves the best source BLEU-2, whereas the proposed progressive memory has
the highest W2V cosine similarity on S. However, our model variants achieve the best performance
on most metrics (Lines 7 and 8). Moreover, it consistently outperforms all other IDA approaches.
Following the previous experiment, we conduct statistical comparison with Line 8. The test shows
that our method is signiﬁcantly better than the other IDA methods.

In general, the evaluation of dialogue systems is noisy due to the lack of appropriate metrics (Liu
et al., 2016). Nevertheless, our experiment provides additional evidence of the effectiveness of our
approach. It also highlights our model’s viability for both classiﬁcation and generation tasks.

Case Study. Table 6 shows sample outputs of our IDA model on test prompts from the Cornell
Movie Corpus (source) and the Ubuntu Dialogue Corpus (target). We see that casual prompts from
the movie domain result in casual responses, whereas Ubuntu queries result in Ubuntu-related re-
sponses. With the expansion of vocabulary, our model is able to learn new words like “grub”; with
progressive memory, it learns Ubuntu jargon like “network connection.” This shows evidence of the
success of incremental domain adaptation.

16

0
2
0
2
 
b
e
F
 
4
1
 
 
]
L
C
.
s
c
[
 
 
2
v
9
3
2
0
0
.
1
1
8
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2020

PROGRESSIVE MEMORY BANKS FOR INCREMENTAL
DOMAIN ADAPTATION

Nabiha Asghar∗†◦ Lili Mou∗‡ Kira A. Selby†◦ Kevin D. Pantasdo◦ Pascal Poupart†◦ Xin Jiang♦
†Vector Institute for AI, Toronto, Canada
◦Cheriton School of Computer Science, University of Waterloo, Canada
{nasghar,kaselby,kevin.pantasdo,ppoupart}@uwaterloo.ca
‡Dept. Computing Science, University of Alberta; Alberta Machine Intelligence Institute (AMII)
doublepower.mou@gmail.com
♦Noah’s Ark Lab, Huawei Technologies, Hong Kong
jiang.xin@huawei.com

ABSTRACT

This paper addresses the problem of incremental domain adaptation (IDA) in natu-
ral language processing (NLP). We assume each domain comes one after another,
and that we could only access data in the current domain. The goal of IDA is to
build a uniﬁed model performing well on all the domains that we have encoun-
tered. We adopt the recurrent neural network (RNN) widely used in NLP, but
augment it with a directly parameterized memory bank, which is retrieved by an
attention mechanism at each step of RNN transition. The memory bank provides a
natural way of IDA: when adapting our model to a new domain, we progressively
add new slots to the memory bank, which increases the number of parameters,
and thus the model capacity. We learn the new memory slots and ﬁne-tune exist-
ing parameters by back-propagation. Experimental results show that our approach
achieves signiﬁcantly better performance than ﬁne-tuning alone. Compared with
expanding hidden states, our approach is more robust for old domains, shown by
both empirical and theoretical results. Our model also outperforms previous work
of IDA including elastic weight consolidation and progressive neural networks in
the experiments.1

1

INTRODUCTION

Domain adaptation aims to transfer knowledge from one domain (called the source domain) to
another (called the target domain) in a machine learning system.2 If the data of the target domain are
not large enough, using data from the source domain typically helps to improve model performance
in the target domain. This is important for neural networks, which are data-hungry and prone to
overﬁtting. In this paper, we especially focus on incremental domain adaptation (IDA)3, where we
assume different domains come sequentially one after another. We only have access to the data in
the current domain, but hope to build a uniﬁed model that performs well on all the domains that we
have encountered (Xu et al., 2014; Rusu et al., 2016; Kirkpatrick et al., 2017).

Incremental domain adaptation is useful in scenarios where data are proprietary, or available only for
a short period of time (Li & Hoiem, 2018). It is desired to preserve as much knowledge as possible
in the learned model and not to rely on the availability of the data. Another application of IDA
is a quick adaptation to new domains. If the environment of a deployed machine learning system
changes frequently, traditional methods like jointly training all domains require the learning machine
to be re-trained from scratch every time a new domain comes. Fine-tuning a neural network by a

∗ Equal contribution.
1Our IDA code is available at https://github.com/nabihach/IDA.
2In our work, the domain is deﬁned by datasets. Usually, the data from different genres or times typically

have different underlying distributions.

3In the literature, IDA is sometimes referred to as incremental, continual, or lifelong learning. We use

“incremental domain adaptation” in this paper to emphasize that our domain is not changed continuously.

1

Published as a conference paper at ICLR 2020

few steps of gradient updates does transfer quickly, but it suffers from the catastrophic forgetting
problem (Kirkpatrick et al., 2017). Suppose during prediction a data point is not labeled with its
domain, the (single) ﬁne-tuned model cannot predict well for samples in previous domains, as it
tends to “forget” quickly during ﬁne-tuning.

A recent trend of domain adaptation in the deep learning regime is the progressive neural net-
work (Rusu et al., 2016), which progressively grows the network capacity if a new domain comes.
Typically, this is done by enlarging the model with new hidden states and a new predictor (Fig-
ure 1a). To avoid interfering with existing knowledge, the newly added hidden states are not fed
back to the previously trained states. During training, all existing parameters are frozen, and only
the newly added ones are trained. For inference, the new predictor is used for all domains, which is
sometimes undesired as the new predictor is trained with only the last domain.

In this paper, we propose a progressive memory bank for incremental domain adaptation in natural
language processing (NLP). Our model augments a recurrent neural network (RNN) with a memory
bank, which is a set of distributed, real-valued vectors capturing domain knowledge. The memory
is retrieved by an attention mechanism during RNN information processing. When our model is
adapted to new domains, we progressively increase the slots in the memory bank. But different
from Rusu et al. (2016), we ﬁne-tune all the parameters, including RNN and the previous memory
bank. Empirically, when the model capacity increases, the RNN does not forget much even if
the entire network is ﬁne-tuned. Compared with expanding RNN hidden states, the newly added
memory slots cause less contamination of the existing knowledge in RNN states, as will be shown
by a theorem.

In our paper, we evaluate the proposed approach on a classiﬁcation task known as multi-genre nat-
ural language inference (MultiNLI). Appendix C provides additional evidence when our approach
is applied to text generation. Experiments consistently support our hypothesis that the proposed
approach adapts well to target domains without catastrophic forgetting of the source. Our model
outperforms the na¨ıve ﬁne-tuning method, the original progressive neural network, as well as other
IDA techniques including elastic weight consolidation (EWC, Kirkpatrick et al., 2017).

2 RELATED WORK

Domain Adaptation. Domain adaptation has been widely studied in machine learning, including
the NLP domain. For neural NLP applications, Mou et al. (2016) analyze two straightforward set-
tings, namely, multi-task learning (jointly training all domains) and ﬁne-tuning (training one domain
and ﬁne-tuning on the other). A recent advance of domain adaptation is adversarial learning, where
the neural features are trained not to classify the domain (Ganin et al., 2016; Liu et al., 2017). How-
ever, all these approaches (except ﬁne-tuning) require all domains to be available simultaneously,
and thus are not IDA approaches.

Kirkpatrick et al. (2017) address the catastrophic forgetting problem of ﬁne-tuning neural networks,
and propose a regularization term based on the Fisher information matrix; they call the method
elastic weight consolidation (EWC). While some follow-up studies report EWC achieves high per-
formance in their scenarios (Zenke et al., 2017; Lee et al., 2017; Thompson et al., 2019), others
show that EWC is less effective (Wen & Itti, 2018; Yoon et al., 2018; Wu et al., 2018). Lee et al.
(2017) propose incremental moment matching between the posteriors of the old model and the new
model, achieving similar performance to EWC. Schwarz et al. (2018) augment EWC with knowl-
edge distillation, making it more memory-efﬁcient.

Rusu et al. (2016) propose a progressive neural network that progressively increases the number
of hidden states (Figure 1a). To avoid overriding existing information, they freeze the weights of
the learned network, and do not feed new states to old ones. This results in multiple predictors,
requiring that a data sample is labeled with its domain during the test time. If we otherwise use
the last predictor to predict samples from all domains, its performance may be low for previous
domains, as the predictor is only trained with the last domain.

Yoon et al. (2018) propose an extension of the progressive network. They identify which existing
hidden units are relevant for the new task (with their sparse penalty), and ﬁne-tune only the corre-
sponding subnetwork. However, sparsity is not common for RNNs in NLP applications, as sparse
recurrent connections are harmful. A similar phenomenon is that dropout of recurrent connections

2

Published as a conference paper at ICLR 2020

(a) Progressive
Figure 1:
neural network Rusu et al.
(2016). (b) One step of RNN
transition in our progressive
memory network. Colors in-
dicate different domains.

yields poor performance (Bayer et al., 2013). Xu & Zhu (2018) deal with new domains by adap-
tively adding nodes to the network via reinforcement learning. This approach may require a very
large number of trials to identify the right number of nodes to be added to each layer (Yoon et al.,
2019).

Li & Hoiem (2018) address IDA with a knowledge distillation approach, where they preserve a set
of outputs of the old network on pseudo-training data. Then they jointly optimize for the accuracy
on the new training domain as well as the pseudo-training data. Kim et al. (2019)’s variant of
this approach uses maximum-entropy regularization to control the transfer of distilled knowledge.
However, in NLP applications, it is non-trivial to obtain pseudo-training data for distillation.

Memory-Based Neural Networks. Our work is related to memory-based neural networks.
Sukhbaatar et al. (2015) propose an end-to-end memory network that assigns each memory slot
to an entity, and aggregates information by multiple attention-based layers. They design their archi-
tecture for the bAbI question answering task, and assign a slot to each sentence. Such idea can be
extended to various scenarios, for example, assigning slots to external knowledge for question an-
swering (Das et al., 2017) and assigning slots to dialogue history for a conversation system (Madotto
et al., 2018).

A related idea is to use episodic memory, which stores data samples from all previously seen do-
mains (thus it is not an IDA approach). This is used for experience replay while training on subse-
quent domains (Lopez-Paz & Ranzato, 2017; Rebufﬁ et al., 2017; Chaudhry et al., 2018; d’Autume
et al., 2019).

Another type of memory in the neural network regime is the neural Turing machine (NTM, Graves
et al., 2016). This memory is not directly parameterized, but is read or written by a neural controller.
Therefore, it serves as temporary scratch paper and does not store knowledge itself. Zhang et al.
(2018b) combine the above two styles of memory for task-oriented dialogue systems, where they
have both slot-value memory and read-and-write memory.

Different from the work above, our memory bank stores knowledge in a distributed fashion, where
each slot does not correspond to a concrete entity or data sample. Our memory is directly parame-
terized, interacting in a different way from RNN weights and providing a natural way of incremental
domain adaptation.

3 PROPOSED APPROACH

Our model is based on a recurrent neural network (RNN). At each step, the RNN takes the embed-
ding of the current input (e.g., a word), and changes its states accordingly. This is represented by
hi = RNN(hi−1, xi), where hi and hi−1 are the hidden states at time steps i and i−1, respectively.
xi is the input at the ith step. Typically, long short term memory (LSTM, Hochreiter & Schmidhu-
ber, 1997) or Gated Recurrent Units (GRU, Cho et al., 2014) are used as RNN transitions. In the
rest of this section, we will describe a memory-augmented RNN, and how it is used for incremental
domain adaptation (IDA).

3

Published as a conference paper at ICLR 2020

3.1 AUGMENTING RNN WITH MEMORY BANKS

We enhance the RNN with an external memory bank, as shown in Figure 1b. The memory bank
augments the overall model capacity by storing additional parameters in memory slots. At each
time step, our model computes an attention probability to retrieve memory content, which is then
fed to the computation of RNN transition.

Particularly, we adopt a key-value memory bank, inspired by Miller et al. (2016). Each memory slot
contains a key vector and a value vector. The former is used to compute the attention weight for
memory retrieval, whereas the latter is the value of memory content.

For the ith step, the memory mechanism computes an attention probability αi by

(cid:101)αi,j = exp{h(cid:62)

i−1m(key)

j

}, αi,j =

(cid:101)αi,j
j(cid:48)=1 (cid:101)αi,j(cid:48)

(cid:80)N

where m(key)
is the key vector of the jth slot of the memory (among N slots in total). Then the
model retrieves memory content by a weighted sum of all memory values, where the weight is the
attention probability, given by

j

(1)

(2)

(cid:88)N

ci =

j=1

αi,jm(val)

j

Here, m(val)
concatenated with the current word xi, and fed to the RNN as input for state transition.

is the value vector of the jth memory slot. We call ci the memory content. Then, ci is

j

Using the key-value memory bank allows separate (thus more ﬂexible) computation of memory
retrieval weights and memory content, compared with traditional attention where a candidate vector
is used to compute both attention probability and attention content.

It should be emphasized that the memory bank in our model captures distributed knowledge, which
is different from other work where the memory slots correspond to speciﬁc entities (Eric et al.,
2017). The attention mechanism accomplishes memory retrieval in a “soft” manner, which means
the retrieval strength is a real-valued probability. This enables us to train both memory content and
its retrieval end-to-end, along with other neural parameters.

We would also like to point out that the memory bank alone does not help RNN much. However, it
is natural to use a memory-augmented RNN for incremental domain adaptation, as described below.

3.2 PROGRESSIVELY INCREASING MEMORY FOR INCREMENTAL DOMAIN ADAPTATION

The memory bank in Subsection 3.1 can be progressively expanded to adapt a model in a source
domain to new domains. This is done by adding new memory slots to the bank which are learned
exclusively from the target data.

Suppose the memory bank is expanded with another M slots in a new domain, in addition to previous
N slots. We then have N + M slots in total. The model computes attention probability over the
expanded memory and obtains the attention vector in the same way as Equations (1)–(2), except that
the summation is computed from 1 to N + M . This is given by

α(expand)
i,j

=

(cid:101)αi,j
(cid:80)N +M
j(cid:48)=1 (cid:101)αi,j(cid:48)

,

c(expand)
i

=

(cid:88)N +M
j=1

α(expand)
i,j m(val)

j

(3)

To initialize the expanded model, we load all previous parameters, including RNN weights and the
learned N slots, but randomly initialize the progressively expanded M slots. During training, we
update all parameters by gradient descent. That is to say, new parameters are learned from their
initializations, whereas old parameters are ﬁne-tuned during IDA. The process is applied whenever
a new domain comes, as shown in Algorithm 1.

We would like to discuss the following issues.

4

Published as a conference paper at ICLR 2020

Freezing vs. Fine-tuning learned parame-
ters. Inspired by the progressive neural net-
work (Rusu et al., 2016), we ﬁnd it tempt-
ing to freeze RNN parameters and the learned
memory but only tune new memory for IDA.
However, our preliminary results show that
if we freeze all existing parameters, the in-
creased memory does not add much to the
model capacity, and that its performance is
worse than ﬁne-tuning all parameters.

Algorithm 1: Progressive Memory for IDA
Input: A sequence of domains D0, D1, · · · , Dn
Output: A single model for all domains
Initialize a memory-augmented RNN
Train the model on D0
for D1, · · · , Dn do

Expand the memory with new slots
Load RNN weights and existing memory banks
Train the model by updating all parameters

end
Return: The resulting model

Fine-tuning vs. Fine-tuning while increas-
ing memory slots.
It is reported that ﬁne-
tuning a model (without increasing model ca-
pacity) suffers from the problem of catastrophic forgetting (Kirkpatrick et al., 2017). We wish to
investigate whether our approach suffers from the same problem, since we ﬁne-tune learned param-
eters when progressively increasing memory slots. Our intuition is that the increased model capacity
helps to learn the new domain with less overriding of the previously learned model. Experiments
conﬁrm our conjecture, as the memory-augmented RNN tends to forget more if the memory size is
not increased.

Expanding hidden states vs. Expanding memory. Another way of progressively increasing model
capacity is to expand the size of RNN layers. This setting is similar to the progressive neural net-
work, except that all weights are ﬁne-tuned and new states are connected to existing states.

However, we hereby show a theorem, indicating that the expanded memory results in less contami-
nation/overriding of the learned knowledge in the RNN, compared with the expanded hidden states.
The main idea is to measure the effect of model expansion quantitatively by the expected square dif-
ference on hi before and after expansion, where the expectation reﬂects the average effect of model
expansion in different scenarios.
Theorem 1. Let RNN have vanilla transition with the linear activation function, and let the RNN
state at the last step hi−1 be ﬁxed. For a particular data point, if the memory attention satisﬁes
(cid:80)N +M
j=1 (cid:101)αi,j, then memory expansion yields a lower expected mean squared differ-

j=N +1 (cid:101)αi,j ≤ (cid:80)N

ence in hi than RNN state expansion. That is,

(cid:104)

i − hi(cid:107)2(cid:105)
i − hi(cid:107)2(cid:105)
refers to the hidden states if the memory is expanded. h(s)
i

(cid:107)h(m)

(cid:107)h(s)

≤ E

E

(cid:104)

i

where h(m)
refers to the original dimen-
sions of the RNN states, if we expand the size of RNN states themselves. Here, we compute the
expectation by assuming weights and hidden states are iid from a zero-mean Gaussian distribution
(with variance σ2).

(4)

Proof Sketch: We focus on one step of RNN transition and assume that hi−1 is the same when the
model capacity is increased. Further, we assume that hi is D-dimensional, that each memory slot
mj is d-dimensional, and that the additional RNN units (when we expand the hidden state) are also
d-dimensional.

We compute how the original dimensions in the hidden state are changed if we expand RNN. We
denote the expanded hidden states by (cid:101)hi−1 and (cid:102)hi for the two time steps. We denote the weights
connecting from (cid:101)hi−1 to hi by (cid:102)W ∈ RD×d. We focus on the original D-dimensional space, denoted
as h(s)
i

. The connection is shown in Figure 2a, Appendix A. We have

E(cid:2)(cid:107)h(s)

i − hi(cid:107)2(cid:3) = E(cid:2)(cid:107)(cid:102)W · (cid:101)hi−1(cid:107)2(cid:3) =

D
(cid:88)

(cid:20)(cid:16)

E

j=1

(cid:17)2(cid:21)

(cid:101)w(cid:62)

j (cid:101)hi−1

D
(cid:88)

E

(cid:34)(cid:18) d
(cid:88)

=

j=1

k=1

(cid:101)wjk(cid:101)hi−1[k])

(cid:19)2(cid:35)

D
(cid:88)

d
(cid:88)

(cid:104)(cid:0)

E

=

j=1

i=1

= D · d · Var(cid:0)w(cid:1) · Var(h) = Ddσ2σ2

(cid:1)2(cid:105)

E

(cid:101)hi−1[k](cid:1)2(cid:105)
(cid:104)(cid:0)

(cid:101)wjk

(5)

(6)

(7)

5

Published as a conference paper at ICLR 2020

Table 1: Corpus statistics and the baseline
performance (% accuracy) of our BiLSTM
model (without domain adaptation) and re-
sults reported in previous work.

Fic Gov Slate Tel Travel
77k 77k 77k 83k
# training samples
Our Implementation 65.0 66.5 56.2 64.5
64.7 69.2 57.9 64.4

Yu et al. (2018)

77k
62.7
65.8

Table 2: Results on two-domain adapta-
tion. F: Fine-tuning. V: Expanding vocab-
ulary. H: Expanding RNN hidden states. M:
Our proposed method of expanding memory.
We also compare with previous work elas-
tic weight consolidation (EWC, Kirkpatrick
et al., 2017) and the progressive neural net-
work (Rusu et al., 2016). For the statistical
test (compared with Line 8), ↑, ↓: p < 0.05
and ⇑, ⇓: p < 0.01. The absence of an ar-
row indicates that the performance difference
compared with Line 8 is statistically insignif-
icant with p lower than 0.05.

#Line Model Trained on/by

1
2
3
4
5
6
7
8
9
10
11
12

RNN

+
N
N
R

e

S
T
m S
T
M
S+T
S→T (F)
S→T (F+M)
S→T (F+M+V)
S→T (F+H)
S→T (F+H+V)
S→T (EWC)
S→T (Progressive)

m
e
M
+
N
N
R

% Accuracy on
T
61.23⇓
66.49⇓
60.87⇓
67.01⇓
70.00
69.90↓
70.21
70.82
68.35⇓
68.02⇓
64.10⇓
68.25⇓

S
65.01⇓
56.46⇓
65.41⇓
56.77⇓
66.02↓
65.62↓
66.23
67.55
64.09⇓
63.68⇓
66.02⇓
64.47⇓

Similarly,

E(cid:2)(cid:107)h(m)

= Ddσ2Var(cid:0)∆ck

i − hi(cid:107)2(cid:3) = E

(cid:13)W(c)∆c(cid:107)2(cid:105)
(cid:104)(cid:13)
where ∆c def= c(cid:48) − c. The vectors c and c(cid:48) are the current step’s attention content before and after
memory expansion, respectively, shown in Figure 2b, Appendix A. (We omit the time step in the
notation for simplicity.) W(c) is the weight matrix connecting attention content to RNN states.
To prove the theorem, it remains to show that Var(∆ck) ≤ σ2. We do this by analyzing how
attention is computed at each time step, and bounding each attention weight. For details, see the
complete proof in Appendix A.

(8)

(cid:1)

In the theorem, we have an assumption (cid:80)N +M
j=N +1 (cid:101)αi,j ≤ (cid:80)N
j=1 (cid:101)αi,j, requiring that the total attention
to existing memory slots is larger than to the progressively added slots. This is fairly reasonable
because: (1) During training, attention is trained in an ad hoc fashion to newly-added information,
and thus some αi,j for 1 ≤ j ≤ N might be learned so that it is larger than a random memory slot;
and (2) For a new domain, we do not add a huge number of slots, and thus (cid:80)N +M
j=N +1 (cid:101)αi,j will not
dominate.

It is noted that our theorem is not to provide an explicit optimization/generalization bound for IDA,
but shows that expanding memory is more stable than expanding hidden states. This is particu-
larly important at the beginning steps of IDA, as the progressively growing parameters are randomly
initialized and are basically noise. Although our theoretical analysis uses a restricted setting (i.e.,
vanilla RNN transition and linear activation), it provides the key insight that our approach is appro-
priate for IDA.

4 EXPERIMENTS

In this section, we evaluate our approach on an NLP classiﬁcation task. In particular, we choose the
multi-genre natural language inference (MultiNLI), due to its large number of samples in various
domains. The task is to determine the relationship between two sentences among target labels:
entailment, contradiction, and neutral. In Appendix C, we conduct supplementary experiments on
text generation with our memory-augmented RNN for IDA.

Dataset and Setup. The MultiNLI corpus (Williams et al., 2018) is particularly suitable for IDA,
as it contains training samples for 5 genres: Slate, Fiction (Fic), Telephone (Tel),
Government (Gov), and Travel. In total, we have 390k training samples. The corpus also
contains held-out (non-training) labeled data in these domains. We split it into two parts for valida-
tion and test.4

4MultiNLI also contains 5 genres without training samples, namely, 9/11, Face-to-face, Letters,
OUP, and Verbatim. We ignore these genres, because we focus on incremental domain adaptation instead of

6

Published as a conference paper at ICLR 2020

Training domains
Fic
Fic → Gov
Fic → Gov → Slate
Fic → Gov → Slate → Tel
Fic → Gov → Slate → Tel → Travel

Fic
65.41
67.55
67.04
68.46
69.36

Performance on
Slate
55.83
61.04
63.29
63.39
63.96

Tel
61.39
65.07
64.66
71.60
69.74

Gov
58.87
70.82
71.55
71.10
72.47

Travel
57.35
61.90
63.53
61.50
68.39

Table 3: Dynamics of the progressive memory network for IDA with 5 domains. Upper-triangular
values in gray are out-of-domain (zero-shot) performance.

Group

Non-
IDA

IDA

Setting
In-domain training
Fic + Gov + Slate + Tel + Travel (multi-task)
Fic → Gov → Slate → Tel → Travel (F+V)
Fic → Gov → Slate → Tel → Travel (F+V+M)
Fic → Gov → Slate → Tel → Travel (EWC)
Fic → Gov → Slate → Tel → Travel (Progressive)

Fic
65.41⇓
70.60↑
67.24↓
69.36
67.12⇓
65.22⇓

Gov
67.01⇓
73.30
70.82⇓
72.47
68.71⇓
67.87⇓

Slate
59.30⇓
63.80
62.41↓
63.96
59.90⇓
61.13⇓

Tel
67.20⇓
69.15
67.62↓
69.74
66.09⇓
66.96⇓

Travel
64.70⇓
67.07↓
68.39
68.39
65.70⇓
67.90

Table 4: Comparing our approach with variants and previous work in the multi-domain setting. In
this experiment, we use the memory-augmented RNN as the neural architecture. Italics represent
best results in the IDA group. ↑, ↓: p < 0.05 and ⇑, ⇓: p < 0.01 (compared with F+V+M).

The ﬁrst row in Table 1 shows the size of the training set in each domain. As seen, the corpus is
mostly balanced across domains, although Tel has slightly more examples.

For the base model, we train a bi-directional LSTM (BiLSTM). The details of network architecture,
training, and hyper-parameter tuning are given in Appendix B. We see in Table 1 that we achieve
similar performance to Yu et al. (2018). Furthermore, our BiLSTM achieves an accuracy of 68.37
on the ofﬁcial MultiNLI test set,5 which is better than 67.51 reported in the original MultiNLI paper
(Williams et al., 2018) using BiLSTM. This shows that our implementation and tuning are fair for
the basic BiLSTM, and that our model is ready for the study of IDA.

Transfer between Two Domains. We would like to compare our approach with a large number
of baselines and variants. Thus, we randomly choose two domains as a testbed: Fic as the source
domain and Gov as the target domain. We show results in Table 2.

First, we analyze the performance of RNN and the memory-augmented RNN in the non-transfer
setting (Lines 1–2 vs. Lines 3–4). As seen, the memory-augmented RNN achieves slightly better
but generally similar performance, compared with RNN (both with LSTM units). This shows that,
in the non-transfer setting, the memory bank does not help the RNN much. However, this later
conﬁrms that the performance improvement is indeed due to our IDA technique, instead of simply a
better neural architecture.

We then apply two straightforward methods of domain adaptation: multi-task learning (Line 5) and
ﬁne-tuning (Line 6). Multi-task learning jointly optimizes source and target objectives, denoted by
“S+T.” On the other hand, the ﬁne-tuning approach trains the model on the source ﬁrst, and then ﬁne-
tunes on the target. In our experiments, these two methods perform similarly on the target domain,
which is consistent with Mou et al. (2016). On the source domain, ﬁne-tuning performs signiﬁcantly
worse than multi-task learning, as it suffers from the catastrophic forgetting problem. We notice that,
in terms of source performance, the ﬁne-tuning approach (Line 6) is slightly better than trained on
the source domain only (Line 3). This is probably because our domains are somewhat correlated as
opposed to Kirkpatrick et al. (2017), and thus training with more data on target slightly improves
the performance on source. However, ﬁne-tuning does achieve the worst performance on source
compared with other domain adaptation approaches (among Lines 5–8). Thus, we nevertheless use
the terminology “catastrophic forgetting,” and our research goal is still to improve IDA performance.

The main results of our approach are Lines 7 and 8. We apply the proposed progressive memory
network to IDA and we ﬁne-tune all weights. We see that on both source and target domains,

zero-shot learning. Also, the labels for the ofﬁcial test set of MultiNLI are not publicly available, and therefore
we cannot use it to evaluate performance on individual domains. Our split of the held-out set for validation and
test applies to all competing methods, and thus is a fair setting.

5Evaluation on the ofﬁcial MultiNLI test set requires submission to Kaggle.

7

Published as a conference paper at ICLR 2020

our approach outperforms the ﬁne-tuning method alone where the memory size is not increased
(comparing Lines 7 and 6). This veriﬁes our conjecture that, if the model capacity is increased, the
new domain results in less overriding of the learned knowledge in the neural network. Our proposed
approach is also “orthogonal” to the expansion of the vocabulary size, where target-speciﬁc words
are randomly initialized and learned on the target domain. As seen, this combines well with our
memory expansion and yields the best performance on both source and target (Line 8).

We now compare an alternative way of increasing model capacity, i.e., expanding hidden states
(Lines 9 and 10). For fair comparison, we ensure that the total number of model parameters after
memory expansion is equal to the number of model parameters after hidden state expansion. We
see that the performance of hidden state expansion is poor especially on the source domain, even
if we ﬁne-tune all parameters. This experiment provides empirical evidence to our theorem that
expanding memory is more robust than expanding hidden states.

We also compare the results with previous work on IDA. We re-implement6 EWC (Kirkpatrick
et al., 2017). It does not achieve satisfactory results in our application. We investigate other pub-
lished papers using the same method and ﬁnd inconsistent results: EWC works well in some ap-
plications (Zenke et al., 2017; Lee et al., 2017) but performs poorly on others (Yoon et al., 2018;
Wu et al., 2018). Wen & Itti (2018) even report near random performance with EWC. We also
re-implement the progressive neural network (Rusu et al., 2016). We use the target predictor to do
inference for both source and target domains. Progressive neural network yields low performance,
particularly on source, probably because the predictor is trained with only the target domain.

We measure the statistical signiﬁcance of the results against Line 8 with the one-tailed Wilcoxon’s
signed-rank test (Wilcoxon, 1945), by bootstrapping a subset of 200 samples for 10 times with
replacement. The test shows our approach is signiﬁcantly better than others, on both source and
target domains.

IDA with All Domains. Having analyzed our approach, baselines, and variants on two domains in
detail, we are now ready to test the performance of IDA with multiple domains, namely, Fic, Gov,
Slate, Tel, and Travel. In this experiment, we assume these domains come one after another,
and our goal is to achieve high performance on all domains.

Table 3 shows the dynamics of IDA with our progressive memory network. Comparing the upper-
triangular values (in gray, showing out-of-domain performance) with diagonal values, we see that
our approach can be quickly adapted to the new domain in an incremental fashion. Comparing lower-
triangular values with the diagonal, we see that our approach does not suffer from the catastrophic
forgetting problem as the performance of previous domains is gradually increasing if trained with
more domains. After all data are observed, our model achieves the best performance in most domains
(last row in Table 3), despite the incremental nature of our approach.

We now compare our approach with other baselines and variants in the multi-domain setting, shown
in Table 4. Due to the large number of settings, we only choose a selected subset of variants from
Table 2 for the comparison.

As seen, our approach of progressively growing memory achieves the same performance as ﬁne-
tuning on the last domain (both with vocabulary expansion), but for all previous 4 domains, we
achieve signiﬁcantly better performance than ﬁne-tuning. Our model is comparable to multi-task
learning on all domains. It should also be mentioned that multi-task learning requires training the
model when data from all domains are available simultaneously. It is not an incremental approach
for domain adaptation, and thus cannot be applied to the scenarios introduced in Section 1. We
include this setting mainly because we are curious about the performance of non-incremental domain
adaptation.

We also compare with previous methods for IDA in Table 4. Our method outperforms EWC and the
progressive neural network in all domains; the results are consistent with Table 2.

6Implementation based on https://github.com/ariseff/overcoming-catastrophic

8

Published as a conference paper at ICLR 2020

5 CONCLUSION

In this paper, we propose a progressive memory network for incremental domain adaptation (IDA).
We augment an RNN with an attention-based memory bank. During IDA, we add new slots to the
memory bank and tune all parameters by back-propagation. Empirically, the progressive memory
network does not suffer from the catastrophic forgetting problem as in na¨ıve ﬁne-tuning. Our intu-
ition is that the new memory slots increase the neural network’s model capacity, and thus, the new
knowledge causes signiﬁcantly less overriding of the existing network. Compared with expanding
hidden states, our progressive memory bank provides a more robust way of increasing model ca-
pacity, shown by both a theorem and experiments. We also outperform previous work for IDA,
including elastic weight consolidation (EWC) and the original progressive neural network.

This work was funded by Huawei Technologies, Hong Kong. Lili Mou is supported by the Amii
Fellow Program, and the CIFAR AI Chair Program.

ACKNOWLEDGMENTS

REFERENCES

Justin Bayer, Christian Osendorfer, Daniela Korhammer, Nutan Chen, Sebastian Urban, and Patrick
van der Smagt. On fast dropout and its applicability to recurrent networks. arXiv preprint
arXiv:1311.0701, 2013.

Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient

lifelong learning with A-GEM. arXiv preprint arXiv:1812.00420, 2018.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder–decoder
for statistical machine translation. In EMNLP, pp. 1724–1734, 2014.

Cristian Danescu-Niculescu-Mizil and Lillian Lee. Chameleons in imagined conversations: A new
approach to understanding coordination of linguistic style in dialogs. In Proc. Workshop on Cog-
nitive Modeling and Computational Linguistics, pp. 76–87, 2011.

Rajarshi Das, Manzil Zaheer, Siva Reddy, and Andrew McCallum. Question answering on knowl-
edge bases and text using universal schema and memory networks. In ACL, pp. 358–365, 2017.

Cyprien de Masson d’Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic

memory in lifelong language learning. arXiv preprint arXiv:1906.01076, 2019.

Mihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. Key-value retrieval

networks for task-oriented dialogue. In SIGDIAL, pp. 37–49, 2017.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc¸ois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. JMLR, 17(1):2096–2030, 2016.

Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-
Barwi´nska, Sergio G´omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538
(7626):471–476, 2016.

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):

1735–1780, 1997.

Dahyun Kim, Jihwan Bae, Yeonsik Jo, and Jonghyun Choi. Incremental learning with maximum en-
tropy regularization: Rethinking forgetting and intransigence. arXiv preprint arXiv:1902.00829,
2019.

9

Published as a conference paper at ICLR 2020

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences,
114(13):3521–3526, 2017.

Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming
catastrophic forgetting by incremental moment matching. In NeurIPS, pp. 4652–4662, 2017.

Zhizhong Li and Derek Hoiem. Learning without forgetting.

IEEE TPAMI, 40(12):2935–2947,

2018.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau.
How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics
for dialogue response generation. In EMNLP, pp. 2122–2132, 2016.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversarial multi-task learning for text classiﬁca-

tion. In ACL, pp. 1–10, 2017.

NeurIPS, pp. 6467–6476, 2017.

David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. The Ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dialogue systems. In SIGDIAL, pp. 285–294, 2015.

Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. Mem2seq: Effectively incorporating knowl-

edge bases into end-to-end task-oriented dialog systems. In ACL, pp. 1468–1478, 2018.

Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason
In EMNLP, pp. 1400–

Weston. Key-value memory networks for directly reading documents.
1409, 2016.

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. How transferable are neural

networks in NLP applications? In EMNLP, pp. 479–489, 2016.

Jeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global vectors for word

representation. In EMNLP, pp. 1532–1543, 2014.

Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.

icarl:

Incremental classiﬁer and representation learning. In CVPR, pp. 2001–2010, 2017.

Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016.

Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye
Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for contin-
ual learning. In ICML, pp. 4535–4544, 2018.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C
Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for gen-
erating dialogues. In AAAI, pp. 3295–3301, 2017.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In NIPS, pp.

2440–2448, 2015.

In NIPS, pp. 3104–3112, 2014.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.

Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh, and Philipp Koehn. Overcoming
catastrophic forgetting during domain adaptation of neural machine translation. In NAACL, pp.
2062–2068, 2019.

Shixian Wen and Laurent Itti. Overcoming catastrophic forgetting problem by weight consolidation

and long-term memory. arXiv preprint arXiv:1805.07441, 2018.

10

Published as a conference paper at ICLR 2020

Frank Wilcoxon.

Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80–83,

1945.

Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-

tence understanding through inference. In NAACL-HLT, pp. 1112–1122, 2018.

Chenshen Wu, Luis Herranz, Xialei Liu, Yaxing Wang, Joost van de Weijer, and Bogdan Raducanu.
Memory replay GANs: learning to generate images from new categories without forgetting. arXiv
preprint arXiv:1809.02058, 2018.

Jiaolong Xu, Sebastian Ramos, David V´azquez, Antonio M L´opez, and D Ponsa.

Incremental

domain adaptation of deformable part-based models. In BMVC, 2014.

Ju Xu and Zhanxing Zhu. Reinforced continual learning. In NeurIPS, pp. 899–908. 2018.

Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically

expandable networks. ICLR, 2018.

Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Oracle: Order robust adaptive

continual learning. arXiv preprint arXiv:1902.09432, 2019.

Jianfei Yu, Minghui Qiu, Jing Jiang, Jun Huang, Shuangyong Song, Wei Chu, and Haiqing Chen.
Modelling domain relationships for transfer learning on retrieval-based question answering sys-
tems in e-commerce. In WSDM, pp. 682–690, 2018.

Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.

In ICML, pp. 3987–3995, 2017.

Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, Xiujun Li, Chris Brockett, and Bill Dolan.
Generating informative and diverse conversational responses via adversarial information maxi-
mization. arXiv preprint arXiv:1809.05972, 2018a.

Zheng Zhang, Minlie Huang, Zhongzhou Zhao, Feng Ji, Haiqing Chen, and Xiaoyan Zhu.
Memory-augmented dialogue management for task-oriented dialogue systems. arXiv preprint
arXiv:1805.00150, 2018b.

11

Published as a conference paper at ICLR 2020

Figure 2: Hidden state expansion vs. memory expansion at step t.

A PROOF OF THEOREM 1

Theorem 1. Let RNN have vanilla transition with the linear activation function, and let the RNN
state at the last step hi−1 be ﬁxed. For a particular data point, if the memory attention satisﬁes
(cid:80)N +M
j=1 (cid:101)αi,j, then memory expansion yields a lower expected mean squared differ-

j=N +1 (cid:101)αi,j ≤ (cid:80)N

ence in hi than RNN state expansion. That is,

(cid:104)

i − hi(cid:107)2(cid:105)
i − hi(cid:107)2(cid:105)
refers to the hidden states if the memory is expanded. h(s)
i

(cid:107)h(m)

(cid:107)h(s)

≤ E

E

(cid:104)

i

where h(m)
refers to the original dimen-
sions of the RNN states, if we expand the size of RNN states themselves. Here, we compute the
expectation by assuming weights and hidden states are iid from a zero-mean Gaussian distribution
(with variance σ2).

(9)

Proof: Let hi−1 be the hidden state of the last step. We focus on one step of transition and assume
that hi−1 is the same when the model capacity is increased. We consider a simpliﬁed case where
the RNN has vanilla transition with the linear activation function. We measure the effect of model
expansion quantitatively by the expected square difference on hi before and after model expansion.

Suppose the original hidden state hi is D-dimensional. We assume each memory slot is d-
dimensional, and that the additional RNN units when expanding the hidden state are also d-
dimensional. We further assume each variable in the expanded memory and expanded weights ((cid:102)W
in Figure 2) are iid with zero mean and variance σ2. This assumption is reasonable as it enables a fair
comparison of expanding memory and expanding hidden states. Finally, we assume every variable
in the learned memory slots, i.e., mjk, follows the same distribution (zero mean, variance σ2).

We compute how the original dimensions in the hidden state are changed if we expand RNN. We
denote the expanded hidden states by (cid:101)hi−1 and (cid:101)hi for the two time steps. We denote the weights
connecting from (cid:101)hi−1 to hi by (cid:102)W ∈ RD×d. We focus on the original D-dimensional space, denoted
as h(s)
i

. The connection is shown in Figure 2a. We have

E(cid:2)(cid:107)h(s)

i − hi(cid:107)2(cid:3)

= E(cid:2)(cid:107)(cid:102)W · (cid:101)hi−1(cid:107)2(cid:3)
(cid:16)

(cid:20) D
(cid:88)

= E

(cid:101)w(cid:62)

j (cid:101)hi−1

j=1
(cid:20)(cid:16)

E

=

D
(cid:88)

j=1

(cid:101)w(cid:62)

j (cid:101)hi−1

(cid:17)2(cid:21)

(cid:17)2(cid:21)

12

(10)

(11)

(12)

Published as a conference paper at ICLR 2020

D
(cid:88)

E

(cid:34)(cid:18) d
(cid:88)

j=1

k=1

(cid:19)2(cid:35)

(cid:101)wjk(cid:101)hi−1[k])

(cid:104)(cid:0)

E

(cid:101)wjk(cid:101)hi−1[k](cid:1)2(cid:105)

=

=

=

D
(cid:88)

d
(cid:88)

j=1

k=1

D
(cid:88)

d
(cid:88)

j=1

k=1

(cid:104)(cid:0)

E

(cid:101)wjk

(cid:1)2(cid:105)

E

(cid:104)(cid:0)
(cid:101)hi−1[k](cid:1)2(cid:105)

= D · d · Var(cid:0)w(cid:1) · Var(h)
= Ddσ2σ2

where (14) is due to the independence and zero-mean assumptions of every element in (cid:102)W and hi−1.
(15) is due to the independence assumption between (cid:102)W and hi−1.
Next, we compute the effect of expanding memory slots. Notice that (cid:107)h(m)
i − hi(cid:107) = W(c)∆c. Here,
is the RNN hidden state after memory expansion. ∆c def= c(cid:48) −c, where c and c(cid:48) are the attention
h(m)
i
content vectors before and after memory expansion, respectively, at the current time step.7 W(c) is
the weight matrix connecting attention content to RNN states. The connection is shown in Figure 2b.
Reusing the result of (16), we immediately obtain

E(cid:2)(cid:107)h(m)

i − hi(cid:107)2(cid:3)

(cid:13)W(c)∆c(cid:107)2(cid:105)
(cid:104)(cid:13)
= E
(cid:1)
= Ddσ2Var(cid:0)∆ck

where ∆ck is an element of the vector ∆c.
To prove Equation (2), it remains to show that Var(∆ck) ≤ σ2. We now analyze how attention is
computed.
Let (cid:101)α1, · · · , (cid:101)αN +M be the unnormalized attention weights over the N +M memory slots. We notice
that (cid:101)α1, · · · , (cid:101)αN remain the same after memory expansion. Then, the original attention probability
is given by αj = (cid:101)αj/((cid:101)α1 + · · · + (cid:101)αN ) for j = 1, · · · , N . After memory expansion, the attention
probability becomes α(cid:48)

j = (cid:101)αj/((cid:101)α1 + · · · + (cid:101)αN +M ), illustrated in Figure 3. We have

(cid:19)

mj +

N +M
(cid:88)

(cid:16)

j=N +1

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

(cid:17)

mj

(cid:16)

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

(cid:17)

mj

∆c = c(cid:48) − c

N
(cid:88)

j=1

(α(cid:48)

j − αj)mj +

α(cid:48)

jmj

N +M
(cid:88)

j=N +1

N
(cid:88)

(cid:18)

j=1

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M

−

N
(cid:88)

(cid:18) −(cid:101)αj

(cid:101)αN +1+···+ (cid:101)αN +M
(cid:101)α1+···+ (cid:101)αN

(cid:101)α1 + · · · + (cid:101)αN +M

(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN
(cid:19)

N +M
(cid:88)

mj +

j=N +1

=

=

=

def=

j=1

N +M
(cid:88)

j=1

βjmj

where

βj

def=





(cid:101)αN +1+···+ (cid:101)αN +M
(cid:101)α1+···+ (cid:101)αN

−(cid:101)αj
(cid:101)α1 + · · · + (cid:101)αN +M
(cid:101)αj
,
(cid:101)α1 + · · · + (cid:101)αN +M

,

if 1 ≤ j ≤ N

if N +1 ≤ j ≤ N + M

7We omit the time step in the notation for simplicity.

13

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

Published as a conference paper at ICLR 2020

Figure 3: Attention probabilities before and after memory expansion.

By our assumption of total attention (cid:80)N +M
|βj| ≤ |α(cid:48)

j|,

j=N +1 (cid:101)αj ≤ (cid:80)N

j=1 (cid:101)αj, we have

∀1 ≤ j ≤ N + M

Then, we have

Var(∆ck) = E[(c(cid:48)

k − ck)2(cid:3) ∀1 ≤ k ≤ d

E(cid:2)(cid:107)c(cid:48) − c(cid:107)2(cid:3)
(cid:34) d

(cid:88)

(cid:18) N +M
(cid:88)

E

k=1

j=1
(cid:34)(cid:18) N +M
(cid:88)

d
(cid:88)

E

k=1

j=1

(cid:19)2(cid:35)

(cid:19)2(cid:35)

βjmjk

βjmjk

(cid:104)(cid:0)βjmjk
E

(cid:1)2(cid:105)

E(cid:2)β2

(cid:3)E(cid:2)m2

j

jk

(cid:3)

=

=

=

=

=

=

1
d

1
d

1
d

1
d

1
d

1
d

d
(cid:88)

N +M
(cid:88)

k=1

j=1

d
(cid:88)

N +M
(cid:88)

k=1

j=1

d
(cid:88)

N +M
(cid:88)

k=1


j=1

N +M
(cid:88)







j=1

N +M
(cid:88)

j=1

= σ2E

≤ σ2E

E[β2

j ]σ2



β2
j





(α(cid:48)

j)2



(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)
Here, (31) is due to the assumption that mjk is independent and zero-mean, and (32) is due to the
independence assumption between βj and mjk. To obtain (36), we notice that (cid:80)N +M
j = 1 with
0 ≤ α(cid:48)

j ≤ 1 (∀1 ≤ j ≤ N + M ). Thus, (cid:80)N +M

j)2 ≤ 1, concluding our proof.

j=1 α(cid:48)

(α(cid:48)

j=1

≤ σ2

B HYPERPARAMETERS

We choose the base model and most of its settings by following the original MultiNLI paper
(Williams et al., 2018): 300D RNN hidden states, 300D pretrained GloVe embeddings (Penning-
ton et al., 2014) for initialization, batch size of 32, and the Adam optimizer for training. The initial

14

Published as a conference paper at ICLR 2020

S
n
o

.
c
c
A
n
o
i
t
a
d
i
l
a
V

68

67

66

65

1

71

70.5

70

T
n
o

.
c
c
A
n
o
i
t
a
d
i
l
a
V

69.5

1

100

200

300

400

500

100

200

300

400

500

# of Memory Slots

# of Memory Slots

(a)

(b)

Figure 4: Tuning the number of memory slots to be added per domain in the MultiNLI experiment.
The two graphs show validation performance of our IDA model S→T (F+M+V).

learning rate for Adam is tuned over the set {0.3, 0.03, 0.003, 0.0003, 0.00003}. It is set to 0.0003
based on validation performance.

For the memory, we set each slot to be 300-dimensioonal, which is the same as the RNN and em-
bedding size.

We tune the number of progressive memory slots in Figure 4, which shows the validation perfor-
mance on the source (Fic) and target (Gov) domains. We see that the performance is close to
ﬁne-tuning alone if only one memory slot is added. It improves quickly between 1 and 200 slots,
and tapers off around 500. We thus choose to add 500 slots for each domain.

C ADDITIONAL EXPERIMENT ON DIALOGUE GENERATION

We further evaluate our approach on the task of dialogue response generation. Given an input
text sequence, the task is to generate an appropriate output text sequence as a response in human-
computer dialogue. This supplementary experiment provides additional evidence of our approach in
generation tasks.

Datasets, Setup, and Metrics. We use the Cornell Movie Dialogs Corpus (Danescu-Niculescu-
Mizil & Lee, 2011) as the source. It contains ∼220k message-response pairs from movie transcripts.
We use a 200k-10k-10k training-validation-test split.

For the target domain, we manually construct a very small dataset from the Ubuntu Dialogue Cor-
pus (Lowe et al., 2015) to mimic the scenario where quick adaptation has to be done to a new domain
with little training data. In particular, we choose a random subset of 15k message-response pairs,
and use a 9k-3k-3k split.

The base model is a sequence-to-sequence (Seq2Seq) neural network (Sutskever et al., 2014) with
attention from the decoder to the encoder. We use a single-layer RNN encoder and a single-layer
RNN decoder, each containing 1024 cells following Sutskever et al. (2014). We use GRUs instead
of LSTM units due to efﬁciency concerns. We have separate memory banks for the encoder and
decoder, since they are essentially different RNNs. The source and target vocabularies are 27k and
10k, respectively. Each memory slot is 1024D, because the RNN states are 1024D in this experiment.
For each domain, we progressively add 1024 slots; tuning the number of slots is done in a manner
similar to the MultiNLI experiment. As before, we use Adam with an initial learning rate of 0.0003
and other default parameters.

Following previous work, we use BLEU-2 (Eric et al., 2017; Madotto et al., 2018) and average
Word2Vec embedding similarity (W2V-Sim, Serban et al., 2017; Zhang et al., 2018a) as the eval-
uation metrics. BLEU-2 is the geometric mean of unigram and bigram word precision penalized
by length, and correlates with human satisfaction to some extent (Liu et al., 2016). W2V-Sim is
deﬁned as the cosine similarity between the averaged Word2Vec embeddings of the model outputs

15

Published as a conference paper at ICLR 2020

#

1
2
3
4
5
6
7
8
9
10
11
12

Line Model Trained on/by

RNN

+
N
N
R

e

S
T
m S
T
M
S+T
S→T (F)
S→T (F+M)
S→T (F+M+V)
S→T (F+H)
S→T (F+H+V)
S→T (EWC)
S→T (Progressive)

m
e
M
+
N
N
R

BLEU-2 on
T
S
0.738⇓
2.842⇑
1.265⇓
0.795⇓
0.712⇓
3.074⇑
1.287⇓
0.920⇓
0.889⇓
2.650⇑
1.101⇓
1.210⇓
1.207⇓
1.435⇓
1.637
1.652
1.606↓
1.036⇓
1.419⇓
1.257⇓
1.382↓
1.397⇓
1.408↓
1.299⇓

W2V-Sim on
T
S
0.456⇓
0.480⇓
0.480⇓
0.454⇓
0.471⇓
0.498⇓
0.487⇓
0.462⇓
0.462⇓
0.471⇓
0.514⇓
0.509⇓
0.526
0.522
0.525
0.522
0.495⇓
0.503⇓
0.492⇓
0.504⇓
0.514⇓
0.513⇓
0.503⇓
0.502⇓

Table 5: Results on two-domain adaptation for dialogue response generation. F: Fine-tuning. V:
Expanding vocabulary. H: Expanding RNN hidden states. M: Our proposed method of expand-
ing memory. We also compare with EWC (Kirkpatrick et al., 2017) and progressive neural net-
work (Rusu et al., 2016). ↑, ↓: p < 0.05 and ⇑, ⇓: p < 0.01 (compared with Line 8).

Prompt
i’m steve. what’s your name?
i love you man
so how’s it going with your new roomie?
i didn’t do anything wrong did i
i just got a messed up stomach
what are you trying to handicap me?
im scared
i kinda forgot the ﬁx i tried a year ago
how would i remaster the ubuntu install
my terminal is stuck
i am rebooting now
i get an installation error
what does the kernel-package do?
what directory should i be in?

Response
my name is opera
i love you too
it should be ﬁne
we don’t have much experience
i know, you had to do some things
this is not really who i am
what do we do with it?
can you try the output ?
could you have a link both access to the network connection
what version is it?
so your issue is a great work cd, sometimes i get some
i am getting an expert for it
you will need to get it to see if there is a setting
you should be able to install the grub cd at the drive

Table 6: Sample outputs of our IDA model S→T (F+M+V) from Table 5.

and the ground truths. Intuitively, BLEU measures hard word-level overlap between two sequences,
whereas W2V-Sim measures soft similarity in a distributed semantic space.

Results. The results for dialogue response generation are shown in Table 5. We see that BLEU-2 and
W2V similarity are not necessarily consistent. For example, the memory-augmented RNN trained
solely on source achieves the best source BLEU-2, whereas the proposed progressive memory has
the highest W2V cosine similarity on S. However, our model variants achieve the best performance
on most metrics (Lines 7 and 8). Moreover, it consistently outperforms all other IDA approaches.
Following the previous experiment, we conduct statistical comparison with Line 8. The test shows
that our method is signiﬁcantly better than the other IDA methods.

In general, the evaluation of dialogue systems is noisy due to the lack of appropriate metrics (Liu
et al., 2016). Nevertheless, our experiment provides additional evidence of the effectiveness of our
approach. It also highlights our model’s viability for both classiﬁcation and generation tasks.

Case Study. Table 6 shows sample outputs of our IDA model on test prompts from the Cornell
Movie Corpus (source) and the Ubuntu Dialogue Corpus (target). We see that casual prompts from
the movie domain result in casual responses, whereas Ubuntu queries result in Ubuntu-related re-
sponses. With the expansion of vocabulary, our model is able to learn new words like “grub”; with
progressive memory, it learns Ubuntu jargon like “network connection.” This shows evidence of the
success of incremental domain adaptation.

16

