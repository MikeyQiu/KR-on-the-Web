Dynamic Task Weighting Methods for Multi-task Networks
in Autonomous Driving Systems

Isabelle Leang1, Ganesh Sistu2, Fabian B¨urger1, Andrei Bursuc3 and Senthil Yogamani2

1Valeo DAR Bobigny, France

2Valeo Vision Systems, Ireland

3Valeo.ai, France

0
2
0
2
 
n
a
J
 
7
 
 
]

V
C
.
s
c
[
 
 
1
v
3
2
2
2
0
.
1
0
0
2
:
v
i
X
r
a

Abstract— Deep multi-task networks are of particular in-
terest for autonomous driving systems. They can potentially
strike an excellent trade-off between predictive performance,
hardware constraints and efﬁcient use of information from
multiple types of annotations and modalities. However, training
such models is non-trivial and requires balancing the learning
of all tasks as their respective losses display different scales,
ranges and dynamics across training. Multiple task weighting
methods that adjust the losses in an adaptive way have been
proposed recently on different datasets and combinations of
tasks, making it difﬁcult to compare them. In this work,
we review and systematically evaluate nine task weighting
strategies on common grounds on three automotive datasets
(KITTI, Cityscapes and WoodScape). We then propose a novel
method combining evolutionary meta-learning and task-based
selective backpropagation, for ﬁnding the task weights and
training the network reliably. Our method outperforms state-
of-the-art methods by 3% on a two-task application.

I. INTRODUCTION

Visual perception is at the heart of autonomous systems
and vehicles [17], [16]. This ﬁeld has seen tremendous
progress during the recent wave of Deep Neural Network
(DNN) architectures and methods [25], [45], [49], [15],
[14]. The large majority of computer vision benchmarks are
currently dominated by diverse and increasingly effective
models encouraging further use in practical applications,
e.g. automatic diagnosis for healthcare, trafﬁc surveillance,
autonomous vehicles, etc.

Such methods reach top performances on individual tasks
by leveraging multi-million parameter models requiring pow-
erful hardware usually for training, but also for predictions.
Perception systems in autonomous vehicles must analyse
and understand surrounding at all time in order to support
the multiple micro-decisions needed in trafﬁc, e.g. steering,
accelerating, braking, signaling, etc. Consequently, a plethora
of speciﬁc tasks must be addressed simultaneously, e.g. ob-
ject detection, semantic segmentation [43], depth estimation
[26], motion estimation [44], localization [34], soiling de-
tection [53]. Meanwhile hardware constraints in vehicles
are limiting signiﬁcantly the capacity and the number of
tasks that can be solved. Using a neural network for each
individual task is an unfeasible direction. Thus Multi-Task
Learning (MTL) is a highly appealing solution striking a
good compromise between the two sides, reliable and high
performing methods under limited hardware.

Multi-task networks consist of a shared network back-bone
followed by a collection of ”heads”, usually one for each
task. The ﬂexibility DNNs, make it easy for practitioners
to envision diverse architectures according to the available

Fig. 1: Overview of Task Weighting Methods

data and annotations. The main advantage of uniﬁed model
is improving computational efﬁciency [47], [46]. Moreover,
such models reduce development effort and training time as
shared layers minimize the need of learning multiple set of
parameters in different models. Uniﬁed models learn features
jointly for all tasks which makes them robust to over-ﬁtting
by acting as a regularizer, as demonstrated in various multi-
task networks [22], [36], [50].

However, multitask networks are typically difﬁcult

to
train as different
tasks need to be adequately balanced
such that learned network parameters are useful across all
tasks. Furthermore, tasks might have different difﬁculties
and learning paces [13] and negatively impact each other
once a task starts overﬁtting before others. Multiple MTL
approaches have recently attempted to mitigate this problem
through optimization of multi-task architectures [35], [41],
[33], [32], learning relationships between tasks [31], [48]
or, most commonly, by weighting the task losses [4], [20],
[30] (Fig. 1). Given the versatility of MTL, in most works a
new problem and task conﬁguration is proposed and only a
few baselines are considered. It remains difﬁcult to conclude
which technique is better, given a new problem and dataset.
In this work we benchmark multiple task-weighting methods
for a better view on the progress so far.

Meta-learning derived techniques are increasingly popular
for solving the tedious and difﬁcult task of tuning hyper-
parameters for training a network. Recent methods show
encouraging results in ﬁnding the network architecture for
a given task [57], [29]. We propose an evolutionary meta-

learning strategy for ﬁnding the optimal task weights.

In summary, the contributions of our work are: (1) We
conduct a thorough evaluation of several popular and high-
performing task-weighting approaches on a two-task setup
across three automotive datasets. We observe that among
state-of-the-art methods there is no clear winner across
datasets as methods are relatively close in performance
(including simple baselines) and the ranking is varying. (2)
We propose a simple weight learning technique for the two-
task setting, where the network learns the task weights by
itself. (3) We propose learning the optimal
task-weights
by combining evolutionary meta-learning with task-based
selective backpropagation (deciding which tasks to be turned
off for a number of iterations). This method outperforms
baseline methods across tasks and datasets.

II. RELATED WORK

Multi-task learning. MTL is not a novel problem and has
been studied before the deep learning revival [3]. MTL has
been applied to various applications outside computer vision,
e.g. natural language processing [7], speech processing [18],
reinforcement learning [27]. For additional background on
MTL we refer the reader to this recent review [40].

Multi-task networks. In general, MTL is compatible with
several computer vision problems where the tasks rather
complementary and help out optimization. MultiNet [50]
introduces an architecture for semantic segmentation and ob-
ject detection and classiﬁcation. With UberNet [22], Kokki-
nos tackles 7 computer vision problems over the same
backbone architecture. CrossStich networks [35] learn to
combine multi-task neural activations at multiple interme-
diate layers. Progressive Networks [41] consist of multiple
neural networks which are added sequentially with new tasks
and transfer knowledge from previously trained networks to
the newly added one. In PackNet [33], the authors train a
network over a sequence of tasks and for each new task they
train only the least-active neurons from the previous task. Re-
bufﬁ et al [37] train a network over 10 datasets and tasks, and
for each task require a reduced set of parameters attached to
several intermediate layers. In some cases, a single computer
vision problem can be transformed into a MTL problem, et
al Mask R-CNN for instance segmentation, which is object
detection + classiﬁcation + semantic segmentation [14], or
YOLO for object detection [39].

Task loss weighting. Initial Deep MTL networks made
use of a weighted sum of individual task losses [52], [23].
Recently, more complex heuristics have started to emerge
for balancing the task weights using: per-task uncertainty
estimation [20], difﬁculty of the tasks in terms of precision
and accuracy [13], statistics from task losses over time [30]
or from their corresponding gradients [4].

Meta-learning is a learning mechanism that uses ex-
perience from other tasks. The most common use-case of
meta-learning is the automatic adaptation of an algorithm
for a task at hand. More speciﬁcally, meta-learning can be
used for hyper-parameter optimization [28], for exploring
network architectures [57], [29], [19] or various non-trivial

combinations of variables, e.g. data augmentation [9]. In
this line of research, we adapt an evolutionary meta-learning
strategy for ﬁnding the optimal task weights along with the
strategy for alternatively training one of the two tasks.

III. BACKGROUND

In the following, we provide a formal deﬁnition of the
MTL setting which will allow us to provide a common
background and easier understanding of the multiple task
weighting approaches compared and proposed in this work.
Consider an input data space X and a collection of T tasks
T = {τ1:T } with corresponding labels {Y1:T }. In MTL
problems, we have access to a dataset of N i.i.d. samples
D = {xi, yi
τ is the label of the data point
xi for the task τ . In computer vision xi usually corresponds
to an image, while yi
τ can correspond to a variety of data
types, e.g. scalar, class label, 2D heatmap, 2D class map, list
of 2D/3D coordinates, etc.

T }, where yi

1, . . . , yi

The main component in any MTL is a model f (x; θ) :
X → {Y1:T }, which in our case is a CNN with learnable
parameters θ. The most commonly encountered approach
for MTL in neural networks is hard parameter sharing [3],
where there is a set of hidden layers shared between all
tasks, i.e., backbone, to which multiple task-speciﬁc layers
are connected. Formally, the model f becomes:

(cid:16)

(cid:17)
x; θshared, {θ1:T }

f

: X → {Y1:T }

(1)

For clarity, we denote as {θT } the set of parameters
coming for all task-speciﬁc layers θτ . Each task has its own
speciﬁc loss function Lτ
attached
to both its speciﬁc layers θτ and the common backbone
θshared. The optimization objective for f boils down to the
joint minimization of all the T task losses as following:

(cid:16)
f (cid:0)xi; θshared, θτ

(cid:1) , yi

(cid:17)

τ

min
θshared,{θT }

Ltotal(θshared, {θT }) =

min
θshared,{θT }

T
(cid:88)

τ =1

(cid:16)

(cid:17)

wτ Lτ

f (D; θshared, θτ )

(2)

where wτ are per-task weights that can be static, computed
dynamically or learned by f , in which case wτ ⊂ θτ .

Weighted losses for MTL are intuitive and easy for
formulate, however they are more difﬁcult to deploy. The
main challenge is related to computing {wτ }. This is non-
trivial as the optimal weights for a given task can evolve
in time depending on the difﬁculty of the task and of the
content of the train set [4], [13], e.g. diversity of samples,
class imbalance, etc. Moreover, the task weights can depend
on the afﬁnity between the considered tasks [56] and the
way the help, complement [48] or counter each other [42],
relationships that potentially evolve across training iterations.
Recent moment-based optimization algorithms with adaptive
updates, SGD [1], and adaptive step-size, e.g. ADAM [21],
can also inﬂuence the dynamics of the MTL, by attenuating
the impact of a wrongly tuned weight or on the contrary by
keeping the bias of a previously wrong direction for more
iterations. In practice, this challenging problem is solved via

and remains constant during the training.1

Ltotal = wsegLseg + wdetLdet
wseg = L(t=0)
det /L(t=0)

seg

,

wdet = 1.0

(3)

(4)

where wseg and wdet are the losses for the semantic segmen-
tation branch and object detection respectively, while L(t=0)
is the loss for task τ at the ﬁrst training iterations.

τ

3) Dynamic task loss scaling: For this method, we take
into account the evolution of per-task losses during training.
We compute task weights dynamically, at the end of every
training epoch as follows:

L(t)
total = w(t)
seg = ˜L(t−1)
w(t)

segL(t)
det / ˜L(t−1)

seg + w(t)

seg

,

detL(t)

det

w(t)

det = 1.0

(5)

(6)

where ˜L(t−1)

τ

is the average τ loss over the previous epoch.
4) Uncertainty-based weighting: Kendall et al [20] pro-
pose looking into aleatoric or data uncertainty for computing
the task weights adaptively during training. They argue that
each task has with its own homoscedastic uncertainty στ
which can be learned by the network for each task during
training (στ ⊂ θtau). Since they are based on homoscedastic
uncertainty, the task weights are not input-dependent and
have been shown to converge to a constant value after some
iterations [20]. The loss functions for this method are derived
from the Gaussian likelihood.

5) GradNorm: This method from [4] sees multi

task
network training as a problem of unbalanced gradient magni-
tudes back propagated through the shared layers (encoder).
And proposes a solution to normalize the unbalanced task
gradients by optimizing a new gradient loss that controls the
task loss weights. These task loss weights are updated using
gradient descent of this new loss.

6) Geometric loss: In [5] authors proposed a parameter
free loss function called Geometric Loss Strategy to over
come the manual ﬁne tuning of task weights. A geometric
mean of losses is used instead of weighted arithmetic mean.
For example a T task loss function can be expressed as,

Ltotal = (

1
T

Lτ )

T
(cid:89)

τ =1

(7)

The loss strategy was tested with a three task network
network on KITTI [11] and Cityscapes [8] datasets. The
loss function acts as a dynamically adapted weighted
arithmetic sum in log space,
these weights acts as
regularizers and controls the rate of convergence between
the losses.

In the following we describe our proposed approaches for

task weighting.

1A more technically sound way of selecting the losses would be to look
at the gradients of the losses instead of the values of the losses. However,
we include this baseline as it is frequently performed by practitioners when
tuning hyper-parameters after short trials and inspections.

Fig. 2: Multi-task visual perception network architecture

lengthy and expensive grid search or alternatively via a diver-
sity of heuristics with varying degrees of complexity. In this
work we rather explore the former type of approaches and
propose two heuristics for computing the weights towards
improving performances: two simple dynamic task weighting
loss approaches and a meta-learning based approach with
asynchronous backpropagation.

IV. TASK-WEIGHTING METHODS

In this section, we ﬁrst review the most frequent task
weighting methods encountered in literature and in practice
(§IV-A) and then describe our contributed approaches for this
problem (§IV-B, §IV-C, §IV-D). In this work we consider a
two-task setup, where we train a CNN for joint object de-
tection and semantic segmentation (Fig. 2). In the following
we will adapt the deﬁnitions of the task weighting methods
to this setup with T = {det, seg}.

A. Baselines

1) No task weighting: An often encountered approach in
MTL is to not assign any weights to the task losses
[51],
[36], [23]. The optimized loss is then just the standard sum of
task losses with all task weights set to 1.0. This can happen
also when the practitioner adds an extra-loss on at the output
of the network, not necessarily realising that the problem
has become MTL. While very simple, there are number of
issues with this approach. First the network is now extremely
sensitive to imbalances in task data, task loss ranges and
scales (cross entropy, L2, etc). Due to these variations and
desynchronization, some of the task losses advance faster
than the others. These task will be reaching overﬁtting, by the
time the other task losses converge, highlighting the necessity
of balancing the losses during training.

2) Handcrafted task weighting: Here, the loss weights are
found and set manually. We can achieve this by inspecting
the value of the loss for several samples. Then the losses are
weighted such that they are brought to the same scale: the is
computed using from the values of the loss at ﬁrst iterations

B. Weight learning

In [10] cross connections between a shared encoder and
task speciﬁc decoder are adjusted as learnable parameters.
In [20] task weighting parameters are learned during the
training. Inspired by these two works we propose a single
parameter learning strategy for a two task network as follows,

Ltotal = αLseg + (1 − α)Ldet
α = Sigmoid(w(θshared,θT ))

(8)

(9)

where α is the weight balancing term and it is computed
from the learnable parameter w(θshared,θT ) ⊂ {θshared, θT },
which is updated by backpropagation at each training itera-
tion. Note that here the task weights are updated after each
mini-batch.

This simple weight learning method enables the network
to adjust by itself the pace of learning of the two tasks.
The sigmoid outputting the α term serves as a gating
mechanism [6] to balance the two tasks while taking into
consideration the interactions between the two. Bounding the
weights in [0, 1] implicitly regularizes learning by removing
the risk of having extremely unbalanced task weights.

C. Task weighting using Evolutionary Meta-learning

The task weighting problem can be understood as a hy-
perparameter optimization problem with T numeric variables
equal to the number of tasks. An efﬁcient and extended ver-
sion of Evolution Strategies [38] is used as base optimization
method. The extensions allow the optimization of numerical
and categorical variables simultaneously [2]. Furthermore,
the gradient information with respect to the target metric
similar to Natural Evolution Strategies [54] is exploited.
Finally, in order to prevent to evaluate parts of the search
space multiple times, a Tabu search method [12] is applied.
The search space is deﬁned as numerical variable for each
task τ ∈ T as w(cid:48)
τ ∈ [minτ , maxτ ]. The
weight is optimized on an exponential scale as the optimal
weight ratio can be non-linear. Furthermore, the ﬁnal task
weight coefﬁcients are normalized such that their sum is
one with the goal to leave the overall magnitude in the loss
unchanged, i.e. ¯wτ = w(cid:48)
.
In order to guide the optimization to an equilibrium
between the tasks, the geometric mean between the detection
mAP and the segmentation mIoU is used as target metric.

τ = 10wτ with w(cid:48)

τ
j=1 w(cid:48)
j

(cid:80)T

We accelerate optimization by adopting dynamic weight
transfer that reuses the weights of the current best models
during the training. For each new conﬁguration of hyper-
parameters, we don’t start from scratch, but instead train
from the previously best model. In this way the number of
epochs for each run can be effectively reduced (e.g.
to 8
epochs for Woodscape dataset) by doing continuous ﬁnetun-
ing while simultaneously tuning the hyperparameters.

One drawback of the meta-learning approach is increased
computational cost as several trainings need to be performed
to ﬁnd the optimal solutions. However, this algorithm can
well exploit multiple GPUs for speed up.

(a) Geometric mean of mIoU and mAP over time.

(b) Detection and segmentation performance over time.

Fig. 3: Performance of meta-learning task weighting with
asynchronous backpropagation method on Woodscape.

(a) Tested task weights over time.

(b) Async. backpropagation parameter over time.

Fig. 4: Task weights & asynchronous frequency of detection
task with Meta Asynchronous method on Woodscape dataset.

D. Asynchronous backpropagation with task weighting using
Evolutionary Meta-learning

In order to balance the convergence speed of the tasks,
one method can be to control the backpropagation frequency
of the tasks [51]. In this way, a task that converges faster is
updated less often than a task that takes more time to learn.
An implementation trick is to set the task loss weight to 0.0
for the epochs for which we want to slow down training for
the fast task.
L(t)
total = w(t)
w(t)
seg = 1.0 ∀t
w(t)
det = 1.0 if t mod νdet == 0 else 0.0

seg + w(t)

detL(t)

segL(t)

(10)

(11)

(12)

det

with νdet the update frequency of the detection task. This fre-
quency is optimized by the meta-learning method described
in the previous section using a numeric variable in the range
of 1 to 10, followed by a rounding operation to an integer.

V. RESULTS

We conduct experiments on three automotive datasets. The
proposed meta-learning method outperforms the state of the
art techniques [4] and [20] on all the three datasets with a
3-4% margin. The method’s only drawback is higher compu-
tational resources needed as multiple (shorter) trainings are
performed. However, this can be justiﬁed with an increased
performance and safety of the ﬁnal ADAS application.

A. Implementation details

Network architecture. We have tested all the task weight-
ing methods discussed in the previous section with a two
task network. We have designed a model which is suit-
able for low-power hardware. It consists of ResNet10 as
a shared encoder and YOLO style bounding box decoder
and FCN8 style semantic segmentation decoder. Fig.2 shows
our network architecture. The Encoder head is pre-trained on
ImageNet for all the experiments.

Meta-learning conﬁguration. We optimized four param-
eters simultaneously, namely segmentation task weight (ws),
detection task weightb (wd), asynchronous frequency for
segmentation (fs) and detection (fd). The variable ranges
for the two task weights are minseg = mindet = 0.1
and maxseg = 1000 and maxdet = 100 as segmentation
tasks usually proﬁt from a higher weight due to longer
convergence time. Table. I shows the optimal values found
out via optimization. The values represented are normalized
between 0-1. The following optimization parameters for
these experiments are determined empirically: size of initial
population: 4, number of newly generated conﬁguration: 4,
number parents per generated conﬁguration: 2.

B. Results

KITTI [11]. This dataset for object detection consists of
7481 training images which we divided into training and
validation set. The dataset has bounding box annotations for
cars, pedestrians and cyclists. For semantic segmentation task
we have used [24] that provided 445 images. Instead of 11

semantic classes we used only road, sidewalk and merged the
other classes into void. This not only helps to synchronize
the classes with other two datasets but also to simplify the
analysis as semantic data is already highly imbalanced and
its important to balance the class distributions at overall pixel
count.

Cityscapes dataset [8] consists of 5000 images with pixel
level annotations. We extracted bounding boxes and semantic
annotations from the provided polygon annotations. As the
test data is not deﬁned for bounding box regression, we
have used at 60/20/20 split of the provided 5000 images for
training, validation and testing. Similar to KITTI the pro-
posed method has removed skewness towards segmentation
performance.

WoodScape [55] is an automotive ﬁsheye dataset with an-
notations for multiple tasks like detection, segmentation and
motion estimation. The dataset consists of 6K training, 2K
validation and 2K test images. Similar to other datasets the
existing task weighting methods favoured the segmentation
task over the detection task.

C. Insights into the meta-learning method

In order to understand the optimization of the proposed
meta-learning approach, some insights into the results on
the WoodScape dataset are discussed in the following. Fig-
ure 3a shows the target metric over tested conﬁgurations
for the optimization of task weights and the asynchronous
backpropagation parameter. From initially low values a slow,
but steady increase is observed. The best conﬁguration is
obtained after 44 iterations.

Figure 3b shows the progression of the metrics of the
two tasks during optimzation. The segmentation performance
is initally low and noisy and then steadly increases. The
detection metric reaches it maximum early then degrades
slightly to allow a compromise in favor of the segmentation
towards the end of the optimization. Figure 4a shows the
progression of the task loss weights, and Figure 4b the
progress of the asynchronous backpropagation parameter
over time during optimization. Figure 5 contains qualitative
examples on WoodScape and Cityscapes validation dataset
demonstrating improvements by the proposed method.

VI. CONCLUSION

Multi-task learning provides promising performances in
autonomous driving applications and is key in enabling
efﬁcient implementations at a system level. In this work, we
take a closer look at this paradigm, which albeit popular has
been rarely benchmarked across the same range of tasks and
datasets. We thus evaluate nine different weighting strategies
for ﬁnding the optimal method of training an efﬁcient two
task model. We further propose two novel methods for
learning the optimal weights during training: an adaptive
one and one based on metalearning. Our proposed method
outperform state-of-the-art approaches by 3% in compromise
value. In future work, we intend to extend our benchmarking
to additional tasks, e.g. on the wide range of tasks from the
Woodscape dataset [55].

TABLE I: Task weights and asynchronous backpropagation frequencies computed by several task-weighting methods.

No task weighting Handcrafted task weighting Meta-learning task weighting

Meta-learning task weighting
async backprop

KITTI

Cityscapes

Woodscape

ws
wd
async fs
async fd

ws
wd
async fs
async fd

ws
wd
async fs
async fd

1
1
-
-

1
1
-
-

1
1
-
-

70
1
-
-

40
1
-
-

100
1
-
-

0.8490
0.1510
-
-

0.9478
0.0522
-
-

0.9743
0.0257
-
-

0.9776
0.0224
7
1

0.8692
0.1308
1
5

0.8550
0.1450
1
2

TABLE II: Comparison of various task-weighting methods for two-task network training.

No task
weighting

Handcrafted
task weighting

Dynamic task
loss scaling

Uncertainty
weighting

GradNorm

Geometric
loss

Weight
learning

Meta-learning
task weighting

KITTI

Cityscapes

Woodscape

mAP (detection)
mIoU (segment)
Geometric mean

mAP (detection)
mIoU (segment)
Geometric mean

mAP (detection)
mIoU (segment)
Geometric mean

0.6535
0.8114
0.7282

0.2572
0.6356
0.4043

0.4643
0.7180
0.5774

Overall

Geometric mean

0.5700

0.6289
0.8408
0.7272

0.2970
0.5780
0.4143

0.4438
0.8107
0.5998

0.5804

0.1736
0.8079
0.3745

0.2824
0.5796
0.4045

0.4557
0.8118
0.6082

0.4624

0.6589
0.7974
0.7248

0.2968
0.5646
0.4094

0.4525
0.7806
0.5943

0.5762

0.6653
0.8080
0.7332

0.2870
0.5492
0.3970

0.4511
0.8155
0.6065

0.5789

0.5677
0.8176
0.6813

0.2900
0.5819
0.4108

0.4193
0.8227
0.5874

0.5598

0.6727
0.8040
0.7354

0.2972
0.5573
0.4070

0.4419
0.8227
0.6030

0.5818

0.6974
0.8301
0.7609

0.3091
0.5812
0.4239

0.4677
0.8006
0.6119

0.5989

Meta-learning
task weighting
async backprop

0.7260
0.8408
0.7813

0.3177
0.5815
0.4298

0.4862
0.7838
0.6173

0.6095

(a) Groundtruth

(b) No task weighting

(c) Metalearning asynchronous backprop

Fig. 5: Quantitative results on WoodScape (top) and Cityscapes (bottom) validation dataset.

REFERENCES

[1] L. Bottou. Large-scale machine learning with stochastic gradient de-
scent. In Proceedings of COMPSTAT’2010, pages 177–186. Springer,
2010.

[2] F. B¨urger and J. Pauli. Understanding the interplay of simultaneous
model selection and representation optimization for classiﬁcation
tasks. In ICPRAM, pages 283–290, 2016.

[3] R. Caruana. Multitask learning: A knowledge-based source of induc-
In Proceedings of the Tenth International Conference on

tive bias.
Machine Learning, pages 41–48. Morgan Kaufmann, 1993.

[4] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich. Gradnorm:
Gradient normalization for adaptive loss balancing in deep multitask
networks. In ICML, 2018.

[5] S. Chennupati, G. Sistu, S. Yogamani, and S. A Rawashdeh. Multi-
net++: Multi-stream feature aggregation and geometric loss strategy
the IEEE Conference
for multi-task learning.
on Computer Vision and Pattern Recognition Workshops, pages 0–0,
2019.

In Proceedings of

[6] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,
H. Schwenk, and Y. Bengio. Learning phrase representations using
rnn encoder-decoder for statistical machine translation. arXiv preprint
arXiv:1406.1078, 2014.

[7] R. Collobert and J. Weston. A uniﬁed architecture for natural
language processing: Deep neural networks with multitask learning. In
Proceedings of the 25th international conference on Machine learning,
pages 160–167. ACM, 2008.

[8] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-
nenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for
semantic urban scene understanding. In Proc. of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.

[9] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le.
arXiv

Autoaugment: Learning augmentation policies from data.
preprint arXiv:1805.09501, 2018.

[10] C. Doersch and A. Zisserman. Multi-task self-supervised visual
In Proceedings of the IEEE International Conference on

learning.
Computer Vision, pages 2051–2060, 2017.

[11] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous
driving? the kitti vision benchmark suite. In Conference on Computer
Vision and Pattern Recognition (CVPR), 2012.

[12] F. Glover. Future paths for integer programming and links to artiﬁcial
intelligence. Computers & operations research, 13(5):533–549, 1986.
[13] M. Guo, A. Haque, D.-A. Huang, S. Yeung, and L. Fei-Fei. Dynamic
task prioritization for multitask learning. In European Conference on
Computer Vision, pages 282–299. Springer, 2018.

[14] K. He, G. Gkioxari, P. Doll´ar, and R. B. Girshick. Mask r-cnn. corr
abs/1703.06870 (2017). arXiv preprint arXiv:1703.06870, 2017.
[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 770–778, 2016.

[16] M. Heimberger, J. Horgan, C. Hughes, J. McDonald, and S. Yogamani.
Computer vision in automated parking systems: Design, implementa-
tion and challenges. Image and Vision Computing, 68:88–101, 2017.
[17] J. Horgan, C. Hughes, J. McDonald, and S. Yogamani. Vision-based
driver assistance systems: Survey, taxonomy and advances. In 2015
IEEE 18th International Conference on Intelligent Transportation
Systems, pages 2032–2039. IEEE, 2015.

[18] Z. Huang, J. Li, S. M. Siniscalchi, I.-F. Chen, J. Wu, and C.-H.
Lee. Rapid adaptation for deep neural networks through multi-task
learning. In Sixteenth Annual Conference of the International Speech
Communication Association, 2015.

[19] M. Jaderberg, V. Dalibard, S. Osindero, W. M. Czarnecki, J. Don-
ahue, A. Razavi, O. Vinyals, T. Green, I. Dunning, K. Simonyan,
et al. Population based training of neural networks. arXiv preprint
arXiv:1711.09846, 2017.

[20] A. Kendall, Y. Gal, and R. Cipolla. Multi-task learning using
uncertainty to weigh losses for scene geometry and semantics.
In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018.

[21] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.

arXiv preprint arXiv:1412.6980, 2014.

[22] I. Kokkinos. Ubernet: Training a universal convolutional neural
network for low-, mid-, and high-level vision using diverse datasets
In Proceedings of the IEEE Conference on
and limited memory.
Computer Vision and Pattern Recognition, pages 6129–6138, 2017.

[23] I. Kokkinos. Ubernet: Training a universal convolutional neural
network for low-, mid-, and high-level vision using diverse datasets

and limited memory. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 5454–5463, July 2017.
[24] I. Kreˇso, D. ˇCauˇsevi´c, J. Krapac, and S. ˇSegvi´c. Convolutional scale
In German Conference on

invariance for semantic segmentation.
Pattern Recognition, pages 64–75. Springer, 2016.

[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation
In Advances in neural

with deep convolutional neural networks.
information processing systems, pages 1097–1105, 2012.

[26] V. R. Kumar, S. Milz, C. Witt, M. Simon, K. Amende, J. Petzold,
S. Yogamani, and T. Pech. Monocular ﬁsheye camera depth estimation
using sparse lidar supervision. In 2018 21st International Conference
on Intelligent Transportation Systems (ITSC). IEEE, 2018.

[27] A. Lazaric and M. Ghavamzadeh. Bayesian multi-task reinforcement

learning. 2010.

[28] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar.
Hyperband: A novel bandit-based approach to hyperparameter opti-
mization. arXiv preprint arXiv:1603.06560, 2016.

[29] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille, and
L. Fei-Fei. Auto-deeplab: Hierarchical neural architecture search for
semantic image segmentation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 82–92, 2019.
[30] S. Liu, E. Johns, and A. J. Davison. End-to-end multi-task learning

with attention. arXiv preprint arXiv:1803.10704, 2018.

[31] M. Long, Z. Cao, J. Wang, and S. Y. Philip. Learning multiple
In Advances in neural

tasks with multilinear relationship networks.
information processing systems, pages 1594–1603, 2017.

[32] A. Mallya, D. Davis, and S. Lazebnik. Piggyback: Adapting a single
network to multiple tasks by learning to mask weights. In Proceedings
of the European Conference on Computer Vision (ECCV), pages 67–
82, 2018.

[33] A. Mallya and S. Lazebnik. Packnet: Adding multiple tasks to a single
network by iterative pruning. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 7765–7773, 2018.
[34] S. Milz, G. Arbeiter, C. Witt, B. Abdallah, and S. Yogamani. Visual
slam for automated driving: Exploring the applications of deep learn-
ing. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops, pages 247–257, 2018.

[35] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Cross-stitch
networks for multi-task learning. 2016 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), Jun 2016.

[36] D. Neven, B. D. Brabandere, S. Georgoulis, M. Proesmans, and L. V.
Gool. Fast scene understanding for autonomous driving, 2017.
[37] S.-A. Rebufﬁ, H. Bilen, and A. Vedaldi. Learning multiple visual
domains with residual adapters. In Advances in Neural Information
Processing Systems, pages 506–516, 2017.

[38] I. Rechenberg. Evolutionsstrategien. In Simulationsmethoden in der

Medizin und Biologie, pages 83–114. Springer, 1978.

[39] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look
once: Uniﬁed, real-time object detection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 779–
788, 2016.

[40] S. Ruder, J. Bingel, I. Augenstein, and A. Søgaard. Learning what to
share between loosely related tasks. arXiv preprint arXiv:1705.08142,
2017.

[41] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural
networks. arXiv preprint arXiv:1606.04671, 2016.

[42] O. Sener and V. Koltun. Multi-task learning as multi-objective
optimization. In Advances in Neural Information Processing Systems,
pages 525–536, 2018.

[43] M. Siam, S. Elkerdawy, M. Jagersand, and S. Yogamani. Deep
semantic segmentation for automated driving: Taxonomy, roadmap and
challenges. In 2017 IEEE 20th International Conference on Intelligent
Transportation Systems (ITSC), pages 1–8. IEEE, 2017.

[44] M. Siam, H. Mahgoub, M. Zahran, S. Yogamani, M. Jagersand, and
A. El-Sallab. Modnet: Motion and appearance based moving object
detection network for autonomous driving. In 2018 21st International
Conference on Intelligent Transportation Systems (ITSC), pages 2859–
2864. IEEE, 2018.

[45] K. Simonyan and A. Zisserman. Very deep convolutional networks for
large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
[46] G. Sistu, I. Leang, S. Chennupati, S. Yogamani, C. Hughes, S. Milz,
and S. Rawashdeh. Neurall: Towards a uniﬁed visual perception
model for automated driving. In 2019 IEEE Intelligent Transportation
Systems Conference (ITSC), pages 796–803. IEEE, 2019.

[47] G. Sistu, I. Leang, and S. Yogamani. Real-time joint object detection
arXiv
and semantic segmentation network for automated driving.
preprint arXiv:1901.03912, 2019.

[48] T. Standley, A. R. Zamir, D. Chen, L. Guibas, J. Malik, and
S. Savarese. Which tasks should be learned together in multi-task
learning? arXiv preprint arXiv:1905.07553, 2019.

[49] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, A. Rabinovich, et al. Going deeper with
arXiv preprint arXiv:1409.4842, 1409,
convolutions. arxiv 2014.
2014.

[50] M. Teichmann, M. Weber, M. Zoellner, R. Cipolla, and R. Urtasun.
Multinet: Real-time joint semantic reasoning for autonomous driving.
In 2018 IEEE Intelligent Vehicles Symposium (IV), pages 1013–1020.
IEEE, 2018.

[51] M. Teichmann, M. Weber, J. M. Z¨ollner, R. Cipolla, and R. Urtasun.
Multinet: Real-time joint semantic reasoning for autonomous driving.
In 2018 IEEE Intelligent Vehicles Symposium, IV 2018, Changshu,
Suzhou, China, June 26-30, 2018, pages 1013–1020, 2018.

[52] M. Teichmann, M. Weber, M. Zllner, R. Cipolla, and R. Urtasun.
Multinet: Real-time joint semantic reasoning for autonomous driving.
In 2018 IEEE Intelligent Vehicles Symposium (IV), pages 1013–1020,
June 2018.

[53] M. Uˇriˇc´aˇr, P. Kˇr´ıˇzek, G. Sistu, and S. Yogamani. Soilingnet: Soiling
In 2019 IEEE
detection on automotive surround-view cameras.
Intelligent Transportation Systems Conference (ITSC), pages 67–72.
IEEE, 2019.

[54] D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters, and
J. Schmidhuber. Natural evolution strategies. The Journal of Machine
Learning Research, 15(1):949–980, 2014.

[55] S. Yogamani, C. Hughes, J. Horgan, G. Sistu, P. Varley, D. O’Dea,
M. Uricar, S. Milz, M. Simon, K. Amende, C. Witt, H. Rashed,
S. Chennupati, S. Nayak, S. Mansoor, X. Perrotton, and P. Perez.
Woodscape: A multi-task, multi-camera ﬁsheye dataset
for au-
tonomous driving. In The IEEE International Conference on Computer
Vision (ICCV), October 2019.

[56] A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, and S. Savarese.
Taskonomy: Disentangling task transfer learning. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition,
pages 3712–3722, 2018.

[57] B. Zoph and Q. V. Le. Neural architecture search with reinforcement

learning. arXiv preprint arXiv:1611.01578, 2016.

