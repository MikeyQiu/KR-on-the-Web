7
1
0
2
 
y
a
M
 
9
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
5
8
2
0
0
.
9
0
6
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2017

Understanding Trainable Sparse Coding
via matrix factorization

Thomas Moreau
CMLA, ENS Cachan, CNRS,
Universit´e Paris-Saclay,
94235 Cachan, France
thomas.moreau@cmla.ens-cachan.fr

Joan Bruna
Courant Institute of Mathematical Sciences,∗
New York University ,
New York, NY 10012, USA
joan.bruna@berkeley.edu

Abstract

Sparse coding is a core building block in many data analysis and machine
learning pipelines. Typically it is solved by relying on generic optimization
techniques, such as the Iterative Soft Thresholding Algorithm and its ac-
celerated version (ISTA, FISTA). These methods are optimal in the class
of ﬁrst-order methods for non-smooth, convex functions. However, they do
not exploit the particular structure of the problem at hand nor the input
data distribution. An acceleration using neural networks, coined LISTA,
was proposed in Gregor & Le Cun (2010), which showed empirically that
one could achieve high quality estimates with few iterations by modifying
the parameters of the proximal splitting appropriately.
In this paper we study the reasons for such acceleration. Our mathematical
analysis reveals that it is related to a speciﬁc matrix factorization of the
Gram kernel of the dictionary, which attempts to nearly diagonalise the
kernel with a basis that produces a small perturbation of the (cid:96)1 ball. When
this factorization succeeds, we prove that the resulting splitting algorithm
enjoys an improved convergence bound with respect to the non-adaptive
version. Moreover, our analysis also shows that conditions for acceleration
occur mostly at the beginning of the iterative process, consistent with nu-
merical experiments. We further validate our analysis by showing that on
dictionaries where this factorization does not exist, adaptive acceleration
fails.

1

Introduction

Feature selection is a crucial point in high dimensional data analysis. Diﬀerent techniques
have been developed to tackle this problem eﬃciently, and amongst them sparsity has
emerged as a leading paradigm.
In statistics, the LASSO estimator (Tibshirani, 1996)
provides a reliable way to select features and has been extensively studied in the last two
decades (Hastie et al. (2015) and references therein). In machine learning and signal process-
ing, sparse coding has made its way into several modern architectures, including large scale
computer vision (Coates & Ng, 2011) and biologically inspired models (Cadieu & Olshausen,
2012). Also, Dictionary learning is a generic unsupervised learning method to perform non-
linear dimensionality reduction with eﬃcient computational complexity (Mairal et al., 2009).
All these techniques heavily rely on the resolution of (cid:96)1-regularized least squares.
The (cid:96)1-sparse coding problem is deﬁned as solving, for a given input x ∈ Rn and dictionary
D ∈ Rn×m, the following problem:

z∗(x) = arg min

Fx(z) ∆=

(cid:107)x − Dz(cid:107)2 + λ(cid:107)z(cid:107)1 .

z

1
2

(1)

This problem is convex and can therefore be solved using convex optimization machinery.
Proximal splitting methods (Beck & Teboulle, 2009) alternate between the minimization of
the smooth and diﬀerentiable part using the gradient information and the minimization of
the non-diﬀerentiable part using a proximal operator (Combettes & Bauschke, 2011). These
methods can also be accelerated by considering a momentum term, as it is done in FISTA

∗Work done while appointed at UC Berkeley, Statistics Department (currently on leave)

1

Published as a conference paper at ICLR 2017

(Beck & Teboulle, 2009; Nesterov, 2005). Coordinate descent (Friedman et al., 2007; Osher
& Li, 2009) leverages the closed formula that can be derived for optimizing the problem (1)
for one coordinate zi given that all the other are ﬁxed. At each step of the algorithm, one
coordinate is updated to its optimal value, which yields an inexpensive scheme to perform
each step. The choice of the coordinate to update at each step is critical for the performance
of the optimization procedure. Least Angle Regression (LARS) (Hesterberg et al., 2008) is
another method that computes the whole LASSO regularization path. These algorithms all
provide an optimization procedure that leverages the local properties of the cost function
iteratively. They can be shown to be optimal among the class of ﬁrst-order methods for
generic convex, non-smooth functions (Bubeck, 2014).

But all these results are given in the worst case and do not use the distribution of the
considered problem. One can thus wonder whether a more eﬃcient algorithm to solve
(1) exists for a ﬁxed dictionary D and generic input x drawn from a certain input data
distribution. In Gregor & Le Cun (2010), the authors introduced LISTA, a trained version
of ISTA that adapts the parameters of the proximal splitting algorithm to approximate the
solution of the LASSO using a ﬁnite number of steps. This method exploits the common
structure of the problem to learn a better transform than the generic ISTA step. As ISTA
is composed of a succession of linear operations and piecewise non linearities, the authors
use the neural network framework and the backpropagation to derive an eﬃcient procedure
In Sprechmann et al. (2012), the authors extended LISTA
solving the LASSO problem.
to more generic sparse coding scenarios and showed that adaptive acceleration is possible
under general input distributions and sparsity conditions.

In this paper, we are interested in the following question: Given a ﬁnite computational
budget, what is the optimum estimator of the sparse coding? This question belongs to
the general topic of computational tradeoﬀs in statistical inference. Randomized sketches
(Alaoui & Mahoney, 2015; Yang et al., 2015) reduce the size of convex problems by projecting
expensive kernel operators into random subspaces, and reveal a tradeoﬀ between computa-
tional eﬃciency and statistical accuracy. Agarwal (2012) provides several theoretical results
on perfoming inference under various computational constraints, and Chandrasekaran &
Jordan (2013) considers a hierarchy of convex relaxations that provide practical tradeoﬀs
between accuracy and computational cost. More recently, Oymak et al. (2015) provides
sharp time-data tradeoﬀs in the context of linear inverse problems, showing the existence
of a phase transition between the number of measurements and the convergence rate of
the resulting recovery optimization algorithm. Giryes et al. (2016) builds on this result to
produce an analysis of LISTA that describes acceleration in conditions where the iterative
procedure has linear convergence rate. Finally, Xin et al. (2016) also studies the capabilities
of Deep Neural networks at approximating sparse inference. The authors show that unrolled
iterations lead to better approximation if one allows the weights to vary at each layer, con-
trary to standard splitting algorithms. Whereas their focus is on relaxing the convergence
hypothesis of iterative thresholding algorithms, we study a complementary question, namely
when is speedup possible, without assuming strongly convex optimization. Their results are
consistent with ours, since our analysis also shows that learning shared layer weights is less
eﬀective.

Inspired by the LISTA architecture, our mathematical analysis reveals that adaptive accel-
eration is related to a speciﬁc matrix factorization of the Gram matrix of the dictionary
B = DTD as B = ATSA − R ,where A is unitary, S is diagonal and the residual is positive
semideﬁnite: R (cid:23) 0. Our factorization balances between near diagonalization by asking
that (cid:107)R(cid:107) is small and small perturbation of the (cid:96)1 norm, i.e. (cid:107)Az(cid:107)1 − (cid:107)z(cid:107)1 is small. When
this factorization succeeds, we prove that the resulting splitting algorithm enjoys a conver-
gence rate with improved constants with respect to the non-adaptive version. Moreover,
our analysis also shows that acceleration is mostly possible at the beginning of the iterative
process, when the current estimate is far from the optimal solution, which is consistent with
numerical experiments. We also show that the existence of this factorization is not only
suﬃcient for acceleration, but also necessary. This is shown by constructing dictionaries
whose Gram matrix diagonalizes in a basis that is incoherent with the canonical basis, and
verifying that LISTA fails in that case to accelerate with respect to ISTA.

In our numerical experiments, we design a specialized version of LISTA called FacNet, with
more constrained parameters, which is then used as a tool to show that our theoretical anal-
ysis captures the acceleration mechanism of LISTA. Our theoretical results can be applied
to FacNet and as LISTA is a generalization of this model, it always performs at least as well,
showing that the existence of the factorization is a suﬃcient certiﬁcate for acceleration by

2

Published as a conference paper at ICLR 2017

LISTA. Reciprocally, we show that for cases where no acceleration is possible with FacNet,
the LISTA model also fail to provide acceleration, linking the two speedup mechanisms.
This numerical evidence suggest that the existence of our proposed factorization is suﬃcient
and somewhat necessary for LISTA to show good results.

The rest of the paper is structured as follows. Section 2 presents our mathematical analysis
and proves the convergence of the adaptive algorithm as a function of the quality of the
matrix factorization. Finally, Section 3 presents the generic architectures that will enable
the usage of such schemes and the numerical experiments, which validate our analysis over
a range of diﬀerent scenarios.

2 Accelerating Sparse Coding with Sparse Matrix

Factorizations

2.1 Unitary Proximal Splitting

In this section we describe our setup for accelerating sparse coding based on the Proximal
Splitting method. Let Ω ⊂ Rn be the set describing our input data, and D ∈ Rn×m be a
dictionary, with m > n. We wish to ﬁnd fast and accurate approximations of the sparse
coding z∗(x) of any x ∈ Ω, deﬁned in (1) For simplicity, we denote B = DTD and y = D†x
to rewrite (1) as

z∗(x) = arg min

Fx(z) =

z

1
2
(cid:124)

(y − z)TB(y − z)
(cid:125)
(cid:123)(cid:122)
E(z)

+ λ(cid:107)z(cid:107)1
(cid:124) (cid:123)(cid:122) (cid:125)
G(z)

.

(2)

For clarity, we will refer to Fx as F and to z∗(x) as z∗. The classic proximal splitting
technique ﬁnds z∗ as the limit of sequence (zk)k, obtained by successively constructing a
surrogate loss Fk(z) of the form

2 + λ(cid:107)z(cid:107)1 ,

Fk(z) = E(zk) + (zk − y)TB(z − zk) + Lk(cid:107)z − zk(cid:107)2

(3)
satisfying Fk(z) ≥ F (z) for all z ∈ Rm . Since Fk is separable in each coordinate of z,
zk+1 = arg minz Fk(z) can be computed eﬃciently. This scheme is based on a majoration
of the quadratic form (y − z)TB(y − z) with an isotropic quadratic form Lk(cid:107)zk − z(cid:107)2
2. The
convergence rate of the splitting algorithm is optimized by choosing Lk as the smallest
constant satisfying Fk(z) ≥ F (z), which corresponds to the largest singular value of B.
The computation of zk+1 remains separable by replacing the quadratic form LkI by any
diagonal form. However, the Gram matrix B = DTD might be poorly approximated via
diagonal forms for general dictionaries. Our objective is to accelerate the convergence of
this algorithm by ﬁnding appropriate factorizations of the matrix B such that

B ≈ ATSA , and (cid:107)Az(cid:107)1 ≈ (cid:107)z(cid:107)1 ,
where A is unitary and S is diagonal positive deﬁnite. Given a point zk at iteration k, we
can rewrite F (z) as

F (z) = E(zk) + (zk − y)TB(z − zk) + QB(z, zk) ,

(4)

:=

with QB(v, w)
nal
(cid:101)F (z, zk) := E(zk) + (zk − y)TB(z − zk) + QS(Az, Azk)
since

deﬁnite matrix S and

.
unitary matrix A,
be

(v − w)TB(v − w) + λ(cid:107)v(cid:107)1

positive

can

1
2

For

diago-
any
surrogate
loss
explicitly minimized,

the

arg min

z

(cid:101)F (z, zk) = AT arg min
u

(zk − y)TBAT(u − Azk) + QS(u, Azk)

(cid:16)

= AT arg min

QS

u

(cid:16)

(cid:17)
u, Azk − S−1AB(zk − y)

(cid:17)

(5)

where we use the variable change u = Az. As S is diagonal positive deﬁnite, (5) is separable
and can be computed easily, using a linear operation followed by a point-wise non linear
soft-thresholding. Thus, any couple (A, S) ensures an computationally cheap scheme. The
question is then how to factorize B using S and A in an optimal manner, that is, such that
the resulting proximal splitting sequence converges as fast as possible to the sparse coding
solution.

3

Published as a conference paper at ICLR 2017

2.2 Non-asymptotic Analysis

We will now establish convergence results based on the previous factorization. These bounds
will inform us on how to best choose the factors Ak and Sk in each iteration.
For that purpose, let us deﬁne
(cid:16)

(cid:17)

δA(z) = λ

(cid:107)Az(cid:107)1 − (cid:107)z(cid:107)1

, and R = ATSA − B .

(6)

The quantity δA(z) thus measures how invariant the (cid:96)1 norm is to the unitary operator A,
whereas R corresponds to the residual of approximating the original Gram matrix B by our
factorization ATSA . Given a current estimate zk, we can rewrite

(cid:101)F (z, zk) = F (z) +

(z − zk)TR(z − zk) + δA(z) .

1
2

By imposing that R is a positive semideﬁnite residual one immediately obtains the following
bound.
Proposition 2.1. Suppose that R = ATSA − B is positive deﬁnite, and deﬁne

Then

F (zk+1) − F (z∗) ≤

(cid:107)R(cid:107)(cid:107)zk − z∗(cid:107)2

2+δA(z∗) − δA(zk+1) .

zk+1 = arg min

(cid:101)F (z, zk) .

z

1
2

Proof. By deﬁnition of zk+1 and using the fact that R (cid:31) 0 we have

(7)

(8)

(9)

F (zk+1) − F (z∗) ≤ F (zk+1) − (cid:101)F (zk+1, zk) + (cid:101)F (z∗, zk) − F (z∗)
(zk+1 − zk)TR(zk+1 − zk) − δA(zk+1) +

= −

1
2
(z∗ − zk)TR(z∗ − zk) +

1
(z∗ − zk)TR(z∗ − zk) + δA(z∗)
2
(cid:17)
δA(z∗) − δA(zk+1)

.

(cid:16)

≤

1
2

where the ﬁrst line results from the deﬁnition of zk+1 and the third line makes use of R
positiveness.

This simple bound reveals that to obtain fast approximations to the sparse coding it is
suﬃcient to ﬁnd S and A such that (cid:107)R(cid:107) is small and that the (cid:96)1 commutation term δA
is small. These two conditions will be often in tension: one can always obtain R ≡ 0 by
using the Singular Value Decomposition of B = AT
0 S0A0 and setting A = A0 and S = S0.
However, the resulting A0 might introduce large commutation error δA0 . Similarly, as the
(cid:12)
(cid:12) ≤ (cid:12)
(cid:12)
absolute value is non-expansive, i.e.

(cid:12)
(cid:12)
(cid:12)|a| − |b|

(cid:12), we have that

(cid:12)a − b(cid:12)

|δA(z)| = λ

(cid:12)
(cid:12)
(cid:12)(cid:107)Az(cid:107)1 − (cid:107)z(cid:107)1

(cid:12)
(cid:12)
(cid:12) ≤ λ(cid:107)(A − I)z(cid:107)1

(10)

≤ λ(cid:112)2 max((cid:107)Az(cid:107)0, (cid:107)z(cid:107)0) · (cid:107)A − I(cid:107) · (cid:107)z(cid:107)2 ,

where we have used the Cauchy-Schwartz inequality (cid:107)x(cid:107)1 ≤ (cid:112)(cid:107)x(cid:107)0(cid:107)x(cid:107)2 in the last equation.
In particular, (10) shows that unitary matrices in the neighborhood of I with (cid:107)A − I(cid:107) small
have small (cid:96)1 commutation error δA but can be inappropriate to approximate general B
matrix.

The commutation error also depends upon the sparsity of z and Az . If both z and Az are
sparse then the commutation error is reduced, which can be achieved if A is itself a sparse
unitary matrix. Moreover, since

|δA(z) − δA(z(cid:48))| ≤ λ|(cid:107)z(cid:107)1 − (cid:107)z(cid:48)(cid:107)1| + λ|(cid:107)Az(cid:107)1 − (cid:107)Az(cid:48)(cid:107)1|
and |(cid:107)z(cid:107)1 − (cid:107)z(cid:48)(cid:107)1| ≤ (cid:107)z − z(cid:48)(cid:107)1 ≤ (cid:112)(cid:107)z − z(cid:48)(cid:107)0(cid:107)z − z(cid:48)(cid:107)2
it results that δA is Lipschitz with respect to the Euclidean norm; let us denote by LA(z)
its local Lipschitz constant in z, which can be computed using the norm of the subgradient

4

Published as a conference paper at ICLR 2017

√

in z1. An uniform upper bound for this constant is (1 + (cid:107)A(cid:107)1)λ
smaller when z and Az are both sparse.
Equation (8) deﬁnes an iterative procedure determined by the pairs {(Ak, Sk)}k. The fol-
lowing theorem uses the previous results to compute an upper bound of the resulting sparse
coding estimator.
Theorem 2.2. Let Ak, Sk be the pair of unitary and diagonal matrices corresponding to
iteration k, chosen such that Rk = AT

m, but it is typically much

k SkAk − B (cid:31) 0. It results that
(z∗ − z0)TR0(z∗ − z0) + 2LA0 (z1)(cid:107)z∗ − z1(cid:107)2
2k

+

α − β
2k

,

(11)

F (zk) − F (z∗) ≤

with

α =

(cid:17)
2LAi (zi+1)(cid:107)z∗ − zi+1(cid:107)2 + (z∗ − zi)T(Ri−1 − Ri)(z∗ − zi)

,

β =

(i + 1)

(zi+1 − zi)TRi(zi+1 − zi) + 2δAi (zi+1) − 2δAi (zi)

(cid:16)

(cid:17)

,

k−1
(cid:88)

(cid:16)

i=1

k−1
(cid:88)

i=0

where LA(z) denote the local lipschitz constant of δA at z.

Remarks: If one sets Ak = I and Sk = (cid:107)B(cid:107)I for all k ≥ 0, (11) corresponds to the bound
of the ISTA algorithm (Beck & Teboulle, 2009).

We can specialize the theorem in the case when A0, S0 are chosen to minimize the bound
(9) and Ak = I, Sk = (cid:107)B(cid:107)I for k ≥ 1.
Corollary 2.3. If Ak = I, Sk = (cid:107)B(cid:107)I for k ≥ 1 then

F (zk)−F (z∗) ≤

(z∗ − z0)TR0(z∗ − z0) + 2LA0 (z1)((cid:107)z∗ − z1(cid:107) + (cid:107)z1 − z0(cid:107)) + (z∗ − z1)TR0(z∗ − z1)T
2k

.

(12)

This corollary shows that by simply replacing the ﬁrst step of ISTA by the modiﬁed proximal
step detailed in (5), one can obtain an improved bound at ﬁxed k as soon as
2(cid:107)R0(cid:107) max((cid:107)z∗ − z0(cid:107)2
which, assuming (cid:107)z∗ − z0(cid:107)2 ≥ (cid:107)z∗ − z1(cid:107)2, translates into

2) + 4LA0 (z1) max((cid:107)z∗ − z0(cid:107)2, (cid:107)z∗ − z1(cid:107)2) ≤ (cid:107)B(cid:107)(cid:107)z∗ − z0(cid:107)2
2 ,

2, (cid:107)z∗ − z1(cid:107)2

LA0 (z1)
(cid:107)z∗ − z0(cid:107)2
More generally, given a current estimate zk, searching for a factorization (Ak, Sk) will im-
prove the upper bound when

(cid:107)R0(cid:107) + 2

(cid:107)B(cid:107)
2

(13)

≤

.

LAk (zk+1)
(cid:107)z∗ − zk(cid:107)2
We emphasize that this is not a guarantee of acceleration, since it is based on improving
an upper bound. However, it provides a simple picture on the mechanism that makes
non-asymptotic acceleration possible.

(cid:107)Rk(cid:107) + 2

(cid:107)B(cid:107)
2

(14)

≤

.

2.3

Interpretation

In this section we analyze the consequences of Theorem 2.2 in the design of fast sparse coding
approximations, and provide a possible explanation for the behavior observed numerically.

2.3.1

‘Phase Transition” and Law of Diminishing Returns

(14) reveals that the optimum matrix factorization in terms of minimizing the upper bound
depends upon the current scale of the problem, that is, of the distance (cid:107)z∗ − zk(cid:107). At the
beginning of the optimization, when (cid:107)z∗ − zk(cid:107) is large, the bound (14) makes it easier to
explore the space of factorizations (A, S) with A further away from the identity. Indeed,
the bound tolerates larger increases in LA(zk+1), which is dominated by

LA(zk+1) ≤ λ((cid:112)(cid:107)zk+1(cid:107)0 + (cid:112)(cid:107)Azk+1(cid:107)0) ,
1 This quantity exists as δA is a diﬀerence of convex. See proof of ?? in appendices for precisions.

5

Published as a conference paper at ICLR 2017

X

We

Z

X

W (0)
e

W (1)
e

W (2)
e

Wg

W (1)
g

W (2)
g

Z

(a) ISTA - Recurrent Neural Network

(b) LISTA - Unfolded network

Figure 1: Network architecture for ISTA/LISTA. The unfolded version (b) is trainable
through backpropagation and permits to approximate the sparse coding solution eﬃciently.

i.e. the sparsity of both z1 and A0(z1). On the other hand, when we reach intermediate
solutions zk such that (cid:107)z∗ − zk(cid:107) is small with respect to LA(zk+1), the upper bound is
minimized by choosing factorizations where A is closer and closer to the identity, leading to
the non-adaptive regime of standard ISTA (A = Id).

This is consistent with the numerical experiments, which show that the gains provided by
learned sparse coding methods are mostly concentrated in the ﬁrst iterations. Once the
estimates reach a certain energy level, section 3 shows that LISTA enters a steady state in
which the convergence rate matches that of standard ISTA.

The natural follow-up question is to determine how many layers of adaptive splitting are
suﬃcient before entering the steady regime of convergence. A conservative estimate of this
quantity would require an upper bound of (cid:107)z∗ − zk(cid:107) from the energy bound F (zk) − F (z∗).
Since in general F is convex but not strongly convex, such bound does not exist unless one
can assume that F is locally strongly convex (for instance for suﬃciently small values of F ).

2.3.2

Improving the factorization to particular input distributions

Given an input dataset D = (xi, z(0)
i
z(0)
and sparse coding solutions z∗
i , the factorization adapted to D is deﬁned as
i

i )i≤N , containing examples xi ∈ Rn, initial estimates

, z∗

min
A,S; ATA=I,ATSA−B(cid:31)0

1
N

(cid:88)

i≤N

1
2

(z(0)

i − z∗

i )T(ATSA − B)(z(0)

i − z∗

i ) + δA(z∗

i ) − δA(z1,i) . (15)

Therefore, adapting the factorization to a particular dataset, as opposed to enforcing it
uniformly over a given ball B(z∗; R) (where the radius R ensures that the initial value
z0 ∈ B(z∗; R)), will always improve the upper bound (9). Studying the gains resulting from
the adaptation to the input distribution will be let for future work.

3 Numerical Experiments

This section provides numerical arguments to analyse adaptive optimization algorithms and
their performances, and relates them to the theoretical properties developed in the previous
section. All the experiments were run using Python and Tensorﬂow. For all the experiments,
the training is performed using Adagrad (Duchi et al., 2011). The code to reproduce the
ﬁgures is available online2.

3.1 Adaptive Optimization Networks Architectures

LISTA/LFISTA In Gregor & Le Cun (2010), the authors introduced LISTA, a neural
network constructed by considering ISTA as a recurrent neural net. At each step, ISTA
performs the following 2-step procedure :

1. uk+1 = zk −

DT(Dzk − x) = (I −

zk +

x ,

1
L

1
L
(cid:123)(cid:122)
Wg

DTD)
(cid:125)

(cid:124)

1
DT
L
(cid:124) (cid:123)(cid:122) (cid:125)
We

2.

zk+1 = h λ

(uk+1) where hθ(u) = sign(u)(|u| − θ)+ ,

L






step k of ISTA (16)

2The code can be found at https://github.com/tomMoral/AdaptiveOptim

6

Published as a conference paper at ICLR 2017

This procedure combines a linear operation to compute uk+1 with an element-wise non
linearity.
It can be summarized as a recurrent neural network, presented in Figure 1a.,
with tied weights. The autors in Gregor & Le Cun (2010) considered the architecture ΦK
Θ
with parameters Θ = (W (k)
, θ(k))k=1,...K obtained by unfolding K times the recurrent
g
network, as presented in Figure 1b. The layers φk

, W (k)
e

Θ are deﬁned as

zk+1 = φk

Θ(zk) := hθ(Wgzk + Wex) .

(17)

e = DT

L , W (k)

g = I − DTD

If W (k)
L are ﬁxed for all the K layers, the out-
put of this neural net is exactly the vector zK resulting from K steps of ISTA. With
LISTA, the parameters Θ are learned using back propagation to minimize the cost function:
f (Θ) = Ex

L and θ(k) = λ

Fx(ΦK

(cid:104)

(cid:105)

.

Θ (x))

A similar algorithm can be derived from FISTA, the accelerated version of ISTA to obtain
LFISTA (see Figure 5 in Appendix A ). The architecture is very similar to LISTA, now
with two memory tapes:

zk+1 = hθ(Wgzk + Wmzk−1 + Wex) .

Factorization network Our analysis in Section 2 suggests a refactorization of LISTA in
more a structured class of parameters. Following the same basic architecture, and using (5),
the network FacNet, ΨK

Θ is formed using layers such that:

zk+1 = ψk

Θ(zk) := AThλS−1 (Azk − S−1A(DTDzk − DTx)) ,

(18)

with S diagonal and A unitary, the parameters of the k-th layer. The parameters obtained
after training such a network with back-propagation can be used with the theory devel-
oped in Section 2. Up to the last linear operation AT of the network, this network is a
re-parametrization of LISTA in a more constrained parameter space. Thus, LISTA is a
generalization of this proposed network and should have performances at least as good as
FacNet, for a ﬁxed number of layers.

The optimization can also be performed using backpropagation. To enforce the unitary
constraints on A(k), the cost function is modiﬁed with a penalty:

f (Θ) = Ex

Fx(ΨK

Θ (x))

+

(cid:104)

(cid:105)

(cid:16)

A(k)(cid:17)T

I −

A(k)

(19)

µ
K

K
(cid:88)

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

,

with Θ = (A(k), S(k))k=1...K the parameters of the K layers and µ a scaling factor for the
regularization. The resulting matrix A(k) is then projected on the Stiefel Manifold using a
SVD to obtain ﬁnal parameters, coherent with the network structure.

Linear model Finally, it is important to distinguish the performance gain resulting from
choosing a suitable starting point and the acceleration from our model. To highlights the
gain obtain by changing the starting point, we considered a linear model with one layer
such that zout = A(0)x. This model is learned using SGD with the convex cost function
2 + λ(cid:107)A(0)x(cid:107)1 . It computes a tradeoﬀ between starting from the
f (A(0)) = (cid:107)(I − DA(0))x(cid:107)2
sparsest point 0 and a point with minimal reconstruction error y . Then, we observe the
performance of the classical iteration of ISTA using zout as a stating point instead of 0 .

3.2 Synthetic problems with known distributions

Gaussian dictionary In order to disentangle the role of dictionary structure from the
role of data distribution structure, the minimization problem is tested using a synthetic
generative model with no structure in the weights distribution. First, m atoms di ∈ Rn are
drawn iid from a multivariate Gaussian with mean 0 and covariance In and the dictionary
. The data points are generated from its sparse codes
D is deﬁned as

di/(cid:107)di(cid:107)2

(cid:16)

(cid:17)

following a Bernoulli-Gaussian model. The coeﬃcients z = (z1, . . . , zm) are constructed
with zi = biai, where bi ∼ B(ρ) and ai ∼ N (0, σIm) , where ρ controls the sparsity of the
data. The values are set to m=100, n=64 for the dictionary dimension, ρ = 5/m for the
sparsity level and σ=10 for the activation coeﬃcient generation parameters. The sparsity

i=1...m

7

Published as a conference paper at ICLR 2017

Figure 2: Evolution of the cost function F (zk) − F (z∗) with the number of layers or the
number of iteration k for diﬀerent sparsity level. (left) ρ = 1/20 and (right)ρ = 1/4 .

Figure 3: Evolution of the cost function F (zk) − F (z∗) with the number of layers or the
number of iteration k for a problem generated with an adversarial dictionary.

regularization is set to λ=0.01. The batches used for the training are generated with the
model at each step and the cost function is evaluated over a ﬁxed test set, not used in the
training.

Figure 2 displays the cost performance for methods ISTA/FISTA/Linear relatively to their
iterations and for methods LISTA/LFISTA/FacNet relatively to the number of layers used
to solve our generated problem. Linear has performances comparable to learned methods
with the ﬁrst iteration but a gap appears as the number of layers increases, until a point
where it achieves the same performances as non adaptive methods. This highlights that the
adaptation is possible in the subsequent layers of the networks, going farther than choosing
a suitable starting point for iterative methods. The ﬁrst layers permit to achieve a large
gain over the classical optimization strategy, by leveraging the structure of the problem.
This appears even with no structure in the sparsity patterns of input data, in accordance
with the results in the previous section. We also observe diminishing returns as the number
of layers increases. This results from the phase transition described in Subsubsection 2.3.1,
as the last layers behave as ISTA steps and do not speed up the convergence. The 3 learned
algorithms are always performing at least as well as their classical counterpart, as it was
stated in Theorem 2.2. We also explored the eﬀect of the sparsity level in the training and
learning of adaptive networks. In the denser setting, the arbitrage between the (cid:96)1-norm and
the squared error is easier as the solution has a lot of non zero coeﬃcients. Thus in this
setting, the approximate method is more precise than in the very sparse setting where the
approximation must perform a ﬁne selection of the coeﬃcients. But it also yield lower gain
at the beggining as the sparser solution can move faster.

There is a small gap between LISTA and FacNet in this setup. This can be explained
from the extra constraints on the weights that we impose in the FacNet, which eﬀectively
reduce the parameter space by half. Also, we implement the unitary constraints on the
matrix A by a soft regularization (see (19)), involving an extra hyper-parameter µ that also
contributes to the small performance gap. In any case, these experiments show that our
analysis accounts for most of the acceleration provided by LISTA, as the performance of
both methods are similar, up to optimization errors.

Adversarial dictionary The results from Section 2 show that problems with a gram matrix
composed of large eigenvalues associated to non sparse eigenvectors are harder to accelerate.
Indeed, it is not possible in this case to ﬁnd a quasi diagonalization of the matrix B that

8

Published as a conference paper at ICLR 2017

(a) Pascal VOC 2008

(b) MNIST

Figure 4: Evolution of the cost function F (zk) − F (z∗) with the number of layers or the
number of iteration k for two image datasets.

does not distort the (cid:96)1 norm. It is possible to generate such a dictionary using Harmonic
Analysis. The Discrete Fourier Transform (DFT) distorts a lot the (cid:96)1 ball, since a very
sparse vector in the temporal space is transformed in widely spread spectrum in the Fourier
domain. We can thus design a dictionary for which LISTA and FacNet performances should
be degraded. D =

is constructed such that dj,k = e−2πijζk , with (cid:0)ζk

di/(cid:107)di(cid:107)2

(cid:16)

(cid:17)

(cid:1)

k≤n

randomly selected from

without replacement.

(cid:110)

i=1...m
1/m, . . . , m/2/m

(cid:111)

The resulting performances are reported in Figure 3. The ﬁrst layer provides a big gain by
changing the starting point of the iterative methods. It realizes an arbitrage of the tradeoﬀ
between starting from 0 and starting from y . But the next layers do not yield any extra
gain compared to the original ISTA algorithm. After 4 layers, the cost performance of both
adaptive methods and ISTA are equivalent. It is clear that in this case, FacNet does not
accelerate eﬃciently the sparse coding, in accordance with our result from Section 2. LISTA
also displays poor performances in this setting. This provides further evidence that FacNet
and LISTA share the same acceleration mechanism as adversarial dictionaries for FacNet
are also adversarial for LISTA.

3.3 Sparse coding with over complete dictionary on images

Wavelet encoding for natural images A highly structured dictionary composed of trans-
lation invariant Haar wavelets is used to encode 8x8 patches of images from the PASCAL
VOC 2008 dataset. The network is used to learn an eﬃcient sparse coder for natural im-
ages over this family. 500 images are sampled from dataset to train the encoder. Training
batches are obtained by uniformly sampling patches from the training image set to feed
the stochastic optimization of the network. The encoder is then tested with 10000 patches
sampled from 100 new images from the same dataset.

Learned dictionary for MNIST To evaluate the performance of LISTA for dictionary
learning, LISTA was used to encode MNIST images over an unconstrained dictionary,
learned a priori using classical dictionary learning techniques. The dictionary of 100 atoms
was learned from 10000 MNIST images in grayscale rescaled to 17x17 using the implemen-
tation of Mairal et al. (2009) proposed in scikit-learn, with λ = 0.05. Then, the networks
were trained through backpropagation using all the 60000 images from the training set of
MNIST. Finally, the perfornance of these encoders were evaluated with the 10000 images of
the training set of MNIST.

The Figure 4 displays the cost performance of the adaptive procedures compared to non-
adaptive algorithms. In both scenario, FacNet has performances comparable to the one of
LISTA and their behavior are in accordance with the theory developed in Section 2. The
gains become smaller for each added layer and the initial gain is achieved for dictionary
either structured or unstructured. The MNIST case presents a much larger gain compare
to the experiment with natural images. This results from the diﬀerence of structure of
the input distribution, as the MNIST digits are much more constrained than patches from
natural images and the network is able to leverage it to ﬁnd a better encoder. In the MNIST
case, a network composed of 12 layers is suﬃcient to achieve performance comparable to
ISTA with more than 1000 iterations.

9

Published as a conference paper at ICLR 2017

4 Conclusions

In this paper we studied the problem of ﬁnite computational budget approximation of sparse
coding. Inspired by the ability of neural networks to accelerate over splitting methods on the
ﬁrst few iterations, we have studied which properties of the dictionary matrix and the data
distribution lead to such acceleration. Our analysis reveals that one can obtain acceleration
by ﬁnding approximate matrix factorizations of the dictionary which nearly diagonalize its
Gram matrix, but whose orthogonal transformations leave approximately invariant the (cid:96)1
ball. By appropriately balancing these two conditions, we show that the resulting rotated
proximal splitting scheme has an upper bound which improves over the ISTA upper bound
under appropriate sparsity.

In order to relate this speciﬁc factorization property to the actual LISTA algorithm, we have
introduced a reparametrization of the neural network that speciﬁcally computes the factor-
ization, and incidentally provides reduced learning complexity (less parameters) from the
original LISTA. Numerical experiments of Section 3 show that such reparametrization re-
covers the same gains as the original neural network, providing evidence that our theoretical
analysis is partially explaining the behavior of the LISTA neural network. Our acceleration
scheme is inherently transient, in the sense that once the iterates are suﬃciently close to
the optimum, the factorization is not eﬀective anymore. This transient eﬀect is also consis-
tent with the performance observed numerically, although the possibility remains open to
ﬁnd alternative models that further exploit the particular structure of the sparse coding.
Finally, we provide evidence that successful matrix factorization is not only suﬃcient but
also necessary for acceleration, by showing that Fourier dictionaries are not accelerated.

Despite these initial results, a lot remains to be understood on the general question of
optimal tradeoﬀs between computational budget and statistical accuracy. Our analysis so
far did not take into account any probabilistic consideration (e.g. obtain approximations that
hold with high probability or in expectation). Another area of further study is the extension
of our analysis to the FISTA case, and more generally to other inference tasks that are
currently solved via iterative procedures compatible with neural network parametrizations,
such as inference in Graphical Models using Belief Propagation or other ill-posed inverse
problems.

References

Alekh Agarwal. Computational Trade-oﬀs in Statistical Learning. PhD thesis, University of

California, Berkeley, 2012.

Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with
statistical guarantees. In Advances in Neural Information Processing Systems (NIPS),
pp. 775–783, 2015.

Amir Beck and Marc Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for

Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

S´ebastien Bubeck. Theory of convex optimization for machine learning. preprint, arXiv:1405

(4980), 2014.

Charles F Cadieu and Bruno A Olshausen. Learning intermediate-level representations of

form and motion from natural movies. Neural computation, 24(4):827–866, 2012.

Venkat Chandrasekaran and Michael I Jordan. Computational and statistical tradeoﬀs
via convex relaxation. Proceedings of the National Academy of Sciences, 110(13):E1181–
E1190, 2013.

Adam Coates and Andrew Y Ng. The importance of encoding versus training with sparse
coding and vector quantization. In Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pp. 921–928, 2011.

Patrick L Combettes and Heinz H. Bauschke. Convex Analysis and Monotone Operator

Theory in Hilbert Spaces, volume 1. 2011.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online
learning and stochastic optimization. The Journal of Machine Learning Research, 12:
2121–2159, 2011.

10

Published as a conference paper at ICLR 2017

Jerome Friedman, Trevor Hastie, Holger H¨oﬂing, and Robert Tibshirani. Pathwise coordi-

nate optimization. The Annals of Applied Statistics, 1(2):302–332, 2007.

Raja Giryes, Yonina C Eldar, Alex M Bronstein, and Guillermo Sapiro. Tradeoﬀs between
convergence speed and reconstruction accuracy in inverse problems. preprint, arXiv:1605
(09232), 2016.

Karol Gregor and Yann Le Cun. Learning Fast Approximations of Sparse Coding.

In

International Conference on Machine Learning (ICML), pp. 399–406, 2010.

Trevor Hastie, Robert Tibshirani, and Martin J. Wainwright. Statistical Learning with

Sparsity. CRC Press, 2015.

Tim Hesterberg, Nam Hee Choi, Lukas Meier, and Chris Fraley. Least angle and 1 penalized

regression: A review. Statistics Surveys, 2:61–93, 2008.

J. B. Hiriart-Urruty. How to regularize a diﬀerence of convex functions. Journal of Mathe-

matical Analysis and Applications, 162(1):196–209, 1991.

Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online Learning for Matrix
Factorization and Sparse Coding. Journal of Machine Learning Research, 11(1):19–60,
2009.

Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,

103(1):127–152, 2005.

Stanley Osher and Yingying Li. Coordinate descent optimization for l1 minimization with
application to compressed sensing; a greedy algorithm. Inverse Problems and Imaging, 3
(3):487–503, 2009.

Samet Oymak, Benjamin Recht, and Mahdi Soltanolkotabi. Sharp time–data tradeoﬀs for

linear inverse problems. preprint, arXiv:1507(04793), 2015.

Pablo Sprechmann, Alex Bronstein, and Guillermo Sapiro. Learning Eﬃcient Structured
Sparse Models. In International Conference on Machine Learning (ICML), pp. 615–622,
2012.

Robert Tibshirani. Regression Shrinkage and Selection via the Lasso. Journal of the royal

statistical society. Series B (methodological), 58(1):267–288, 1996.

Bo Xin, Yizhou Wang, Wen Gao, and David Wipf. Maximal sparsity with deep networks?

preprint, arXiv:1605(01636), 2016.

Yun Yang, Mert Pilanci, and Martin J Wainwright. Randomized sketches for kernels: Fast

and optimal non-parametric regression. preprint, arXiv:1501(06195), 2015.

A Learned Fista

A similar algorithm can be derived from FISTA, the accelerated version of ISTA to obtain
LFISTA (see Figure 5 ). The architecture is very similar to LISTA, now with two memory
taps: It introduces a momentum term to improve the convergence rate of ISTA as follows:

1. yk = zk +

(zk − zk−1) ,

2. zk+1 = h λ

∇E(yk)

= h λ
L

(I −

B)yk +

DTx

,

(cid:33)

(cid:32)

1
L

(cid:33)

1
L

tk−1 − 1
tk

(cid:32)

L

yk −

1
L
1 + (cid:112)1 + 4t2
2

k

.

3. tk+1 =

By substituting the expression for yk into the ﬁrst equation, we obtain a generic recurrent
architecture very similar to LISTA, now with two memory taps, that we denote by LFISTA:

zk+1 = hθ(W (k)

g zk + W (k)

m zk−1 + W (k)

e x) .

11

Published as a conference paper at ICLR 2017

X

W (0)
e

W (1)
e

+

W (1)
g

W (1)
m

W (2)
m

W (2)
e

W (3)
e

W (2)
g

+

W (3)
g

+

Z

Figure 5: Network architecture for LFISTA. This network is trainable through backpropa-
gation and permits to approximate the sparse coding solution eﬃciently.

This model is equivalent to running K-steps of FISTA when its parameters are initialized
with

W (k)
g

=

1 +

tk−1 − 1
tk

(cid:33) (cid:32)

(cid:33) (cid:32)

(cid:33)

I −

B

,

1
L
(cid:33)

W (k)

m =

1 − tk−1
tk

I −

B

,

1
L

W (k)
e

=

DT .

(cid:32)

(cid:32)

1
L

The parameters of this new architecture, presented in Figure 5 , are trained analogously as
in the LISTA case.

B Proofs

Lemma B.1. Suppose that R = ATSA − B is positive deﬁnite, and deﬁne

zk+1 = arg min

(cid:101)F (z, zk) , and

z

δA(z) = (cid:107)Az(cid:107)1 − (cid:107)z(cid:107)1. Then we have
1
2

F (zk+1)−F (z∗) ≤

(cid:16)

(cid:17)
(z∗ − zk)TR(z∗ − zk) − (z∗ − zk+1)TR(z∗ − zk+1)

+(cid:104)∂δA(zk+1), zk+1−z∗(cid:105) .

(20)

(21)

Proof. We deﬁne

tzk+1 + (1 − t)z∗(cid:17)
(cid:16)
Since F is convex, f is also convex in [0, 1]. Since f (0) = F (z∗) is the global minimum, it
results that f (cid:48)(t) is increasing in (0, 1], and hence

, t ∈ [0, 1] .

f (t) = F

F (zk+1) − F (z∗) = f (1) − f (0) =

f (cid:48)(t)dt ≤ f (cid:48)(1) ,

(cid:90)

where f (cid:48)(1) is any element of ∂f (1). Since δA(z) is a diﬀerence of convex functions, its
subgradient can be deﬁned as a limit of inﬁmal convolutions Hiriart-Urruty (1991). We
have

∂f (1) = (cid:104)∂F (zk+1), zk+1 − z∗(cid:105) ,

and since

it results that

and thus

∂F (z) = ∂ (cid:101)F (z, zk) − R(z − zk) − ∂δA(z) and 0 ∈ ∂ (cid:101)F (zk+1, zk)

∂F (zk+1) = −R(zk+1 − zk) − ∂δA(zk+1) ,

F (zk+1) − F (z∗) ≤ (z∗ − zk+1)TR(zk+1 − zk) + (cid:104)∂δA(zk+1), (z∗ − zk+1)(cid:105) .

(22)

(21) is obtained by observing that

(z∗ − zk+1)TR(zk+1 − zk) ≤

1
2
thanks to the fact that R (cid:31) 0.

(cid:16)

(cid:17)
(z∗ − zk)TR(z∗ − zk) − (z∗ − zk+1)TR(z∗ − zk+1)

, (23)

12

Published as a conference paper at ICLR 2017

Theorem B.2. Let Ak, Sk be the pair of unitary and diagonal matrices corresponding to
iteration k, chosen such that Rk = AT

k SkAk − B (cid:31) 0. It results that
(z∗ − z0)TR0(z∗ − z0) + 2(cid:104)∇δA0 (z1), (z∗ − z1)(cid:105)
2k

+

α − β
2k

F (zk) − F (z∗) ≤

, with

(24)

α =

(cid:16)

(cid:17)
2(cid:104)∇δAn (zn+1), (z∗ − zn+1)(cid:105) + (z∗ − zn)T(Rn−1 − Rn)(z∗ − zn)

,

β =

(n + 1)

(cid:16)

(cid:17)
(zn+1 − zn)TRn(zn+1 − zn) + 2δAn (zn+1) − 2δAn (zn)

.

k−1
(cid:88)

n=1

k−1
(cid:88)

n=0

Proof: The proof is adapted from (Beck & Teboulle, 2009), Theorem 3.1. From Lemma B.1,
we start by using (21) to bound terms of the form F (zn) − F (z∗):
F (zn)−F (z∗) ≤ (cid:104)∇δAn (zn+1), (z∗−zn+1)(cid:105)+
Adding these inequalities for n = 0 . . . k − 1 we obtain


(z∗ − zn)TRn(z∗ − zn) − (z∗ − zn+1)TRn(z∗ − zn+1)

1
2

(cid:16)

(cid:17)

.


 − kF (z∗) ≤

F (zn)



k−1
(cid:88)

n=0

k−1
(cid:88)

n=0
1
2

+

+

1
2

k−1
(cid:88)

n=1

(cid:104)∇δAn (zn+1), (z∗ − zn+1)(cid:105) +

(25)

(cid:16)

(cid:17)
(z∗ − z0)TR0(z∗ − z0) − (z∗ − zk)TRk−1(z∗ − zk)

+

(z∗ − zn)T(Rn−1 − Rn)(z∗ − zn) .

On the other hand, we also have

F (zn) − F (zn+1) ≥ F (zn) − ˜F (zn, zn) + ˜F (zn+1, zn) − F (zn+1)

= −δAn (zn) + δAn (zn+1) +

(zn+1 − zn)TRn(zn+1 − zn) ,

1
2

which results in
k−1
(cid:88)

n=0

(n + 1)(F (zn) − F (zn+1)) ≥

(n + 1)(zn+1 − zn)TRn(zn+1 − zn) +

+

(n + 1)

δAn(zn+1) − δAn (zn)

(cid:17)

1
2

k−1
(cid:88)

n=0

k−1
(cid:88)

n=0

k−1
(cid:88)

n=0

(cid:16)

1
2

(cid:32)

(26)

(cid:33)







k−1
(cid:88)

n=0

Combining (25) and (26) we obtain

F (zn)

 − kF (zk) ≥

(n + 1)

(zn+1 − zn)TRn(zn+1 − zn) + δAn (zn+1) − δAn (zn)

.

F (zk) − F (z∗) ≤

(z∗ − z0)TR0(z∗ − z0) + 2(cid:104)∇δA0(z1), (z∗ − z1)(cid:105)
2k

+

α − β
2k

(27)

with

k−1
(cid:88)

n=1

k−1
(cid:88)

n=0

α =

(cid:16)
2(cid:104)∇δAn (zn+1), (z∗ − zn+1)(cid:105) + (z∗ − zn)T(Rn−1 − Rn)(z∗ − zn)

(cid:17)

,

β =

(n + 1)

(zn+1 − zn)TRn(zn+1 − zn) + 2δAn (zn+1) − 2δAn (zn)

(cid:16)

(cid:17)

.

(cid:3)
Corollary B.3. If Ak = I, Sk = (cid:107)B(cid:107)I for k > 0 then

F (zk)−F (z∗) ≤

(z∗ − z0)TR0(z∗ − z0) + 2LA0(z1)((cid:107)z∗ − z1(cid:107) + (cid:107)z1 − z0(cid:107)) + (z∗ − z1)TR0(z∗ − z1)T
2k

.

(28)
Proof: We verify that in that case, Rn−1 − Rn ≡ 0 and for n > 1 and δAn ≡ 0 for n > 0 (cid:3).

13

