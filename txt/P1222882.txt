9
1
0
2
 
y
a
M
 
7
2
 
 
]

G
L
.
s
c
[
 
 
5
v
6
5
6
0
0
.
0
1
8
1
:
v
i
X
r
a

Perfect Match:
A Simple Method for Learning Representations For
Counterfactual Inference With Neural Networks

Patrick Schwab1, Lorenz Linhardt2, Walter Karlen1
1Institute of Robotics and Intelligent Systems, ETH Zurich, Switzerland
2 Department of Computer Science, ETH Zurich, Switzerland
patrick.schwab@hest.ethz.ch

Abstract

Learning representations for counterfactual inference from observational data is
of high practical relevance for many domains, such as healthcare, public policy
and economics. Counterfactual inference enables one to answer "What if...?"
questions, such as "What would be the outcome if we gave this patient treatment
t1?". However, current methods for training neural networks for counterfactual
inference on observational data are either overly complex, limited to settings
with only two available treatments, or both. Here, we present Perfect Match
(PM), a method for training neural networks for counterfactual inference that is
easy to implement, compatible with any architecture, does not add computational
complexity or hyperparameters, and extends to any number of treatments. PM is
based on the idea of augmenting samples within a minibatch with their propensity-
matched nearest neighbours. Our experiments demonstrate that PM outperforms
a number of more complex state-of-the-art methods in inferring counterfactual
outcomes across several benchmarks, particularly in settings with many treatments.

1

Introduction

Estimating individual treatment effects1 (ITE) from observational data is an important problem in
many domains. In medicine, for example, we would be interested in using data of people that have
been treated in the past to predict what medications would lead to better outcomes for new patients [1].
Similarly, in economics, a potential application would, for example, be to determine how effective
certain job programs would be based on results of past job training programs [2].

ITE estimation from observational data is difﬁcult for two reasons: Firstly, we never observe all
potential outcomes. If a patient is given a treatment to treat her symptoms, we never observe what
would have happened if the patient was prescribed a potential alternative treatment in the same
situation. Secondly, the assignment of cases to treatments is typically biased such that cases for which
a given treatment is more effective are more likely to have received that treatment. The distribution
of samples may therefore differ signiﬁcantly between the treated group and the overall population. A
supervised model naïvely trained to minimise the factual error would overﬁt to the properties of the
treated group, and thus not generalise well to the entire population.

To address these problems, we introduce Perfect Match (PM), a simple method for training neural
networks for counterfactual inference that extends to any number of treatments. PM effectively
controls for biased assignment of treatments in observational data by augmenting every sample
within a minibatch with its closest matches by propensity score from the other treatments. PM is
easy to use with existing neural network architectures, simple to implement, and does not add any
hyperparameters or computational complexity. We perform experiments that demonstrate that PM

1The ITE is sometimes also referred to as the conditional average treatment effect (CATE).

arXiv Preprint.

is robust to a high level of treatment assignment bias and outperforms a number of more complex
state-of-the-art methods in inferring counterfactual outcomes across several benchmark datasets. The
source code for this work is available at https://github.com/d909b/perfect_match.

Contributions. This work contains the following contributions:

•

•

•

We introduce Perfect Match (PM), a simple methodology based on minibatch matching for learning
neural representations for counterfactual inference in settings with any number of treatments.
We develop performance metrics, model selection criteria, model architectures, and open bench-
marks for estimating individual treatment effects in the setting with multiple available treatments.
We perform extensive experiments on semi-synthetic, real-world data in settings with two and more
treatments. The experiments show that PM outperforms a number of more complex state-of-the-art
methods in inferring counterfactual outcomes from observational data.

2 Related Work

Inferring the causal effects of interventions is a central pursuit in many important
Background.
domains, such as healthcare, economics, and public policy. In medicine, for example, treatment
effects are typically estimated via rigorous prospective studies, such as randomised controlled trials
(RCTs), and their results are used to regulate the approval of treatments. However, in many settings
of interest, randomised experiments are too expensive or time-consuming to execute, or not possible
for ethical reasons [3, 4]. Observational data, i.e. data that has not been collected in a randomised
experiment, on the other hand, is often readily available in large quantities. In these situations,
methods for estimating causal effects from observational data are of paramount importance.

Estimating Individual Treatment Effects. Due to their practical importance, there exists a wide
variety of methods for estimating individual treatment effects from observational data. However,
they are predominantly focused on the most basic setting with exactly two available treatments.
Matching methods are among the conceptually simplest approaches to estimating ITEs. Matching
methods estimate the counterfactual outcome of a sample X with respect to treatment t using the
factual outcomes of its nearest neighbours that received t, with respect to a metric space. These
k-Nearest-Neighbour (kNN) methods [5] operate in the potentially high-dimensional covariate space,
and therefore may suffer from the curse of dimensionality [6]. Propensity Score Matching (PSM) [7]
addresses this issue by matching on the scalar probability p(t
X) of t given the covariates X. Another
|
category of methods for estimating individual treatment effects are adjusted regression models that
apply regression models with both treatment and covariates as inputs. Linear regression models can
either be used for building one model, with the treatment as an input feature, or multiple separate
models, one for each treatment [8]. More complex regression models, such as Treatment-Agnostic
Representation Networks (TARNET) [1] may be used to capture non-linear relationships. Methods
that combine a model of the outcomes and a model of the treatment propensity in a manner that is
robust to misspeciﬁcation of either are referred to as doubly robust [9]. Tree-based methods train
many weak learners to build expressive ensemble models. Examples of tree-based methods are
Bayesian Additive Regression Trees (BART) [10, 11] and Causal Forests (CF) [12]. Representation-
balancing methods seek to learn a high-level representation for which the covariate distributions are
balanced across treatment groups. Examples of representation-balancing methods are Balancing
Neural Networks [13] that attempt to ﬁnd such representations by minimising the discrepancy
distance [14] between treatment groups, and Counterfactual Regression Networks (CFRNET) [1]
that use different metrics such as the Wasserstein distance. Propensity Dropout (PD) [15] adjusts
the regularisation for each sample during training depending on its treatment propensity. Generative
Adversarial Nets for inference of Individualised Treatment Effects (GANITE) [16] address ITE
estimation using counterfactual and ITE generators. GANITE uses a complex architecture with many
hyperparameters and sub-models that may be difﬁcult to implement and optimise. Causal Multi-task
Gaussian Processes (CMGP) [17] apply a multi-task Gaussian Process to ITE estimation. The
optimisation of CMGPs involves a matrix inversion of O(n3) complexity that limits their scalability.

In contrast to existing methods, PM is a simple method that can be used to train expressive non-linear
neural network models for ITE estimation from observational data in settings with any number of
treatments. PM is easy to implement, compatible with any architecture, does not add computational
complexity or hyperparameters, and extends to any number of treatments. While the underlying idea
behind PM is simple and effective, it has, to the best of our knowledge, not yet been explored.

2

3 Methodology

[0 . . p

[0 . . k

Problem Setting. We consider a setting in which we are given N i.i.d. observed samples X,
where each sample consists of p covariates xi with i
1]. For each sample, the potential
outcomes are represented as a vector Y with k entries yj where each entry corresponds to the outcome
when applying one treatment tj out of the set of k available treatments T =
with
j
1]. As training data, we receive samples X and their observed factual outcomes yj when
applying one treatment tj, the other outcomes can not be observed. The set of available treatments
can contain two or more treatments. We refer to the special case of two available treatments as the
binary treatment setting. Given the training data with factual outcomes, we wish to train a predictive
model ˆf that is able to estimate the entire potential outcomes vector ˆY with k entries ˆyj. In literature,
this setting is known as the Rubin-Neyman potential outcomes framework [18].

t0, ..., tk
{

1}

−

−

∈

∈

−

Assumptions. Counterfactual inference from observational data always requires further assump-
tions about the data-generating process [19, 20]. Following [21, 22], we assume unconfoundedness,
which consists of three key parts: (1) Conditional Independence Assumption: The assignment to
treatment t is independent of the outcome yt given the pre-treatment covariates X, (2) Common
Support Assumption: For all values of X, it must be possible to observe all treatments with a
probability greater than 0, and (3) Stable Unit Treatment Value Assumption: The observed outcome
of any one unit must be unaffected by the assignments of treatments to other units. In addition, we
assume smoothness, i.e. that units with similar covariates xi have similar potential outcomes y.

Precision in Estimation of Heterogenous Effect (PEHE). The primary metric that we optimise
for when training models to estimate ITE is the PEHE [23]. In the binary setting, the PEHE measures
the ability of a predictive model to estimate the difference in effect between two treatments t0 and
t1 for samples X. To compute the PEHE, we measure the mean squared error between the true
difference in effect y1(n)
y0(n), drawn from the noiseless underlying outcome distributions µ1
and µ0, and the predicted difference in effect ˆy1(n)

ˆy0(n) indexed by n over N samples:

−

−

N

1
N

n=0(cid:16)
(cid:88)

(cid:15)PEHE =

Eyj (n)

µj (n)[y1(n)

y0(n)]

[ˆy1(n)

ˆy0(n)]

−

−

−

∼

(1)

When the underlying noiseless distributions µj are not known, the true difference in effect y1(n)
−
y0(n) can be estimated using the noisy ground truth outcomes yi (Appendix A). As a secondary
metric, we consider the error (cid:15)ATE in estimating the average treatment effect (ATE) [23]. The ATE
measures the average difference in effect across the whole population (Appendix B). The ATE is not
as important as PEHE for models optimised for ITE estimation, but can be a useful indicator of how
well an ITE estimator performs at comparing two treatments across the entire population. We can
neither calculate (cid:15)PEHE nor (cid:15)ATE without knowing the outcome generating process.

2

(cid:17)

Multiple Treatments. Both (cid:15)PEHE and (cid:15)ATE can be trivially extended to multiple treatments by
considering the average PEHE and ATE between every possible pair of treatments. Note that we
lose the information about the precision in estimating ITE between speciﬁc pairs of treatments by
pairs. However, one can inspect the pair-wise PEHE to obtain the whole picture.
averaging over all

k
2

(cid:0)
ˆ(cid:15)mPEHE =

(cid:1)
1
(cid:1)
(cid:0)k
2

k
1
(cid:88)
−

i
1
(cid:88)
−

i=0

j=0

ˆ(cid:15)PEHE,i,j

(2)

ˆ(cid:15)mATE =

ˆ(cid:15)ATE,i,jt

(3)

1
(cid:1)
(cid:0)k
2

k
1
(cid:88)
−

i
1
(cid:88)
−

i=0

j=0

Model Architecture. The chosen architecture plays a key role in the performance of neural net-
works when attempting to learn representations for counterfactual inference [1, 24]. Shalit et al. [1]
claimed that the naïve approach of appending the treatment index tj may perform poorly if X is
high-dimensional, because the inﬂuence of tj on the hidden layers may be lost during training. Shalit
et al. [1] subsequently introduced the TARNET architecture to rectify this issue. Since the original
TARNET was limited to the binary treatment setting, we extended the TARNET architecture to the
multiple treatment setting (Figure 1). We did so by using k head networks, one for each treatment
over a set of shared base layers, each with L layers. In TARNET, the jth head network is only trained
on samples from treatment tj. The shared layers are trained on all samples. By using a head network
for each treatment, we ensure tj maintains an appropriate degree of inﬂuence on the network output.

3

Figure 1: The TARNET architecture with k heads for
the multiple treatment setting. Each head predicts a
potential outcome ˆyj, and is only trained on samples
that received the respective treatment.

Perfect Match (PM). We consider fully
differentiable neural network models ˆf op-
timised via minibatch stochastic gradient
descent (SGD) to predict potential out-
comes ˆY for a given sample x. To address
the treatment assignment bias inherent in
observational data, we propose to perform
SGD in a space that approximates that of a
randomised experiment using the concept
of balancing scores. Under unconfounded-
ness assumptions, balancing scores have
the property that the assignment to treatment is unconfounded given the balancing score [5, 7, 25].
The conditional probability p(t
X = x) of a given sample x receiving a speciﬁc treatment t, also
|
known as the propensity score [7], and the covariates X themselves are prominent examples of
balancing scores [5, 7]. Using balancing scores, we can construct virtually randomised minibatches
that approximate the corresponding randomised experiment for the given counterfactual inference task
by imputing, for each observed pair of covariates x and factual outcome yt, the remaining unobserved
counterfactual outcomes by the outcomes of nearest neighbours in the training data by some balancing
score, such as the propensity score. Formally, this approach is, when converged, equivalent to a
nearest neighbour estimator for which we are guaranteed to have access to a perfect match, i.e. an
exact match in the balancing score, for observed factual outcomes. We outline the Perfect Match (PM)
algorithm in Algorithm 1 (complexity analysis and implementation details in Appendix D). Upon
convergence at the training data, neural networks trained using virtually randomised minibatches in
the limit N
, a neural network ˆf trained
Theorem 1 Upon convergence, under assumption (1) and for N
according to the PM algorithm is a consistent estimator of the true potential outcomes Y for each t.

remove any treatment assignment bias present in the data.

→ ∞

→ ∞

lim
N
→∞

E( ˆf (x)

X, T = t) = yt
|

(Proof in Appendix C)

The optimal choice of balancing score for use in the PM algorithm depends on the properties of the
dataset. For high-dimensional datasets, the scalar propensity score is preferable because it avoids the
curse of dimensionality that would be associated with matching on the potentially high-dimensional
X directly. For low-dimensional datasets, the covariates X are a good default choice as their use
does not require a model of treatment propensity. The advantage of matching on the minibatch level,
rather than the dataset level [26], is that it reduces the variance during training which in turn leads
to better expected performance for counterfactual inference (Appendix E). In this sense, PM can be
seen as a minibatch sampling strategy [27] designed to improve learning for counterfactual inference.
Propensity Dropout (PD) [15] is another method using balancing scores that has been proposed to
dynamically adjust the dropout regularisation strength for each observed sample depending on its
treatment propensity. PD, in essence, discounts samples that are far from equal propensity for each
treatment during training. This regularises the treatment assignment bias but also introduces data
sparsity as not all available samples are leveraged equally for training. PM, in contrast, fully leverages
all training samples by matching them with other samples with similar treatment propensities.

Model Selection. Besides accounting for the treatment assignment bias, the other major issue in
learning for counterfactual inference from observational data is that, given multiple models, it is
not trivial to decide which one to select. The root problem is that we do not have direct access
to the true error in estimating counterfactual outcomes, only the error in estimating the observed
factual outcomes. This makes it difﬁcult to perform parameter and hyperparameter optimisation, as
we are not able to evaluate which models are better than others for counterfactual inference on a
given dataset. To rectify this problem, we use a nearest neighbour approximation ˆ(cid:15)NN-PEHE of the
ˆ(cid:15)PEHE metric for the binary [1, 28] and multiple treatment settings for model selection. The ˆ(cid:15)NN-PEHE
estimates the treatment effect of a given sample by substituting the true counterfactual outcome with
the outcome yj from a respective nearest neighbour NN matched on X using the Euclidean distance.

ˆ(cid:15)NN-PEHE =

[y1(NN(n))

y0(NN(n))]

[ˆy1(n)

ˆy0(n)]

−
Analogously to Equations (2) and (3), the ˆ(cid:15)NN-PEHE metric can be extended to the multiple treatment
setting by considering the mean ˆ(cid:15)NN-PEHE between all
possible pairs of treatments (Appendix F).

n=0(cid:16)
(cid:88)

−

−

(cid:17)

(4)

2

N

1
N

k
2

(cid:0)

(cid:1)

4

Algorithm 1 Perfect Match (PM). After augmentation, each batch contains an equal number of
samples from each treatment and the covariates xi across treatments are approximately balanced.
Input: Batch of B random samples Xbatch with assigned treatments t, training set Xtrain of N samples,
number of treatment options k, propensity score estimator EPS to calculate the probability p(t
X)
|
of treatment assigned given a sample X

k matched samples

×

Xout ←
for sample X with treatment t in Xbatch do
p(t
for i = 0 to k

Output: Batch Xout consisting of B
1: procedure PERFECT_MATCH:
Empty
2:
3:
4:
5:
6:
7:
8:
9:
10:

←
−
= t then
p(t
psi ←
|
Xmatched ←
Add sample Xmatched to Xout

X)
|
if i

Add X to Xout

EPS(X)

1 do

4 Experiments

Our experiments aimed to answer the following questions:

X)i
get closest match to propensity score psi with treatment i from Xtrain

(1) What is the comparative performance of PM in inferring counterfactual outcomes in the binary

and multiple treatment setting compared to existing state-of-the-art methods?

(2) Does model selection by NN-PEHE outperform selection by factual MSE?
(3) How does the relative number of matched samples within a minibatch affect performance?
(4) How well does PM cope with an increasing treatment assignment bias in the observed data?
(5) How do the learning dynamics of minibatch matching compare to dataset-level matching?

4.1 Datasets

We performed experiments on two real-world and semi-synthetic datasets with binary and multiple
treatments in order to gain a better understanding of the empirical properties of PM.

Infant Health and Development Program (IHDP). The IHDP dataset [23] contains data from a
randomised study on the impact of specialist visits on the cognitive development of children, and
consists of 747 children with 25 covariates describing properties of the children and their mothers.
Children that did not receive specialist visits were part of a control group. The outcomes were
simulated using the NPCI package from [29]2. The IHDP dataset is biased because the treatment
groups had a biased subset of the treated population removed [1].

News. The News dataset was ﬁrst proposed as a benchmark for counterfactual inference by Johans-
son et al. [13] and consists of 5000 randomly sampled news articles from the NY Times corpus3. The
News dataset contains data on the opinion of media consumers on news items. The samples X repre-
sent news items consisting of word counts xi ∈
R is the reader’s opinion of the

N, the outcome yj ∈

2We used the same simulated outcomes as Shalit et al. [1].
3https://archive.ics.uci.edu/ml/datasets/bag+of+words

Figure 2: Correlation analysis of the real PEHE (y-axis) with the mean squared error (MSE; left)
and the nearest neighbour approximation of the precision in estimation of heterogenous effect (NN-
PEHE; right) across over 20000 model evaluations on the validation set of IHDP. Scatterplots show a
subsample of 1400 data points. ρ indicates the Pearson correlation.

5

Figure 3: Comparison of the learning dynamics during training (normalised training epochs; from
start = 0 to end = 100 of training, x-axis) of several matching-based methods on the validation set
of News-8. The coloured lines correspond to the mean value of the factual error (√MSE; red) and
the counterfactual error (√ˆ(cid:15)PEHE; blue) across 10 randomised hyperparameter conﬁgurations (lower
is better). The shaded area indicates the standard deviation. All methods used exactly the same
model and hyperparameters and only differed in how they addressed treatment assignment bias. The
leftmost ﬁgure shows the desired behavior of the counterfactual and factual error jointly decreasing
until convergence. PM best matches the desired behavior of aligning factual and counterfactual error.

news item, and the k available treatments represent various devices that could be used for viewing, e.g.
smartphone, tablet, desktop, television or others [13]. We extended the original dataset speciﬁcation
in [13] to enable the simulation of arbitrary numbers of viewing devices. To model that consumers
prefer to read certain media items on speciﬁc viewing devices, we train a topic model on the whole
NY Times corpus and deﬁne z(X) as the topic distribution of news item X. We then randomly pick
k + 1 centroids in topic space, with k centroids zj per viewing device and one control centroid zc. We
assigned a random Gaussian outcome distribution with mean µj ∼ N
(0.45, 0.15) and standard devi-
(0.1, 0.05) to each centroid. For each sample, we drew ideal potential outcomes from
ation σj ∼ N
that Gaussian outcome distribution ˜yj ∼ N
(0, 0.15). We then deﬁned the
[D(z(X), zj) + D(z(X), zc)] as the ideal potential outcomes
unscaled potential outcomes ¯yj = ˜yj ∗
˜yj weighted by the sum of distances to centroids zj and the control centroid zc using the Euclidean
distance as distance D. We assigned the observed treatment t using t
Bern(softmax(κ ¯yj)) with
|
a treatment assignment bias coefﬁcient κ, and the true potential outcome yj = C ¯yj as the unscaled
potential outcomes ¯yj scaled by a coefﬁcient C = 50. We used four different variants of this dataset
with k = 2, 4, 8, and 16 viewing devices, and κ = 10, 10, 10, and 7, respectively. Higher values of κ
indicate a higher expected assignment bias depending on ¯yj. κ = 0 indicates no assignment bias.

(µj, σj) + (cid:15) with (cid:15)

∼ N

∼

x

All datasets with the exception of IHDP were split into a training (63%), validation (27%) and test
set (10% of samples). For IHDP we used exactly the same splits as previously used by Shalit et al.
[1]. We repeated experiments on IHDP and News 1000 and 50 times, respectively. We reassigned
outcomes and treatments with a new random seed for each repetition.

4.2 Experimental Setup

Models. We evaluated PM, ablations, baselines, and all relevant state-of-the-art methods: kNN
[5], BART [10, 11], Random Forests (RF) [30], CF [12], GANITE [16], Balancing Neural Network
(BNN) [13], TARNET [1], Counterfactual Regression Network using the Wasserstein regulariser
(CFRNETWass) [1], and PD [15]. We trained a Support Vector Machine (SVM) with probability
estimation [31] to estimate p(t
X) for PM on the training set. We also evaluated preprocessing the
|
entire training set with PSM using the same matching routine as PM (PSMPM) and the "MatchIt"
package (PSMMI, [26] before training a TARNET (Appendix G). In addition, we trained an ablation
of PM where we matched on the covariates X (+ on X) directly, if X was low-dimensional (p < 200),
and on a 50-dimensional representation of X obtained via principal components analysis (PCA), if X
was high-dimensional, instead of on the propensity score. We also evaluated PM with a multi-layer
perceptron (+ MLP) that received the treatment index tj as an input instead of using a TARNET.

Architectures. To ensure that differences between methods of learning counterfactual representa-
tions for neural networks are not due to differences in architecture, we based the neural architectures
for TARNET, CFRNETWass, PD and PM on the same, previously described extension of the TARNET
architecture [1] (Appendix H) to the multiple treatment setting.

Hyperparameters. For the IHDP and News datasets we respectively used 30 and 10 optimisation
runs for each method using randomly selected hyperparameters from predeﬁned ranges (Appendix I).
We selected the best model across the runs based on validation set ˆ(cid:15)NN-PEHE or ˆ(cid:15)NN-mPEHE.

6

Metrics. We calculated the (cid:15)PEHE (Eq. 1) and (cid:15)ATE (Appendix B) for the binary IHDP and News-2
datasets, and the ˆ(cid:15)mPEHE (Eq. 2) and ˆ(cid:15)mATE (Eq. 3) for News-4/8/16 datasets.

5 Results and Discussion

Counterfactual Inference. We evaluated the counterfactual inference performance of the listed
models in settings with two or more available treatments (Table 1, ATEs in Appendix Table S3). On
IHDP, the PM variants reached the best performance in terms of √(cid:15)PEHE, and the second best (cid:15)ATE
after CFRNET. On the binary News-2, PM outperformed all other methods in terms of √(cid:15)PEHE and
(cid:15)ATE. On the News-4/8/16 datasets with more than two treatments, PM consistently outperformed
all other methods - in some cases by a large margin - on both metrics with the exception of the
News-4 dataset, where PM came second to PD. The strong performance of PM across a wide range
of datasets with varying amounts of treatments is remarkable considering how simple it is compared
to other, highly specialised methods. Notably, PM consistently outperformed both CFRNET, which
accounted for covariate imbalances between treatments via regularisation rather than matching, and
PSMMI, which accounted for covariate imbalances by preprocessing the entire training set with a
matching algorithm [26]. We also found that matching on the propensity score was, in almost all
cases, not signiﬁcantly different from matching on X directly when X was low-dimensional, or
a low-dimensional representation of X when X was high-dimensional (+ on X). This indicates
that PM is effective with any low-dimensional balancing score. In addition, using PM with the
TARNET architecture outperformed the MLP (+ MLP) in almost all cases, with the exception
of the low-dimensional IHDP. We therefore conclude that matching on the propensity score or
a low-dimensional representation of X and using the TARNET architecture are sensible default
conﬁgurations, particularly when X is high-dimensional. Finally, although TARNETs trained with
PM have similar asymptotic properties as kNN, we found that TARNETs trained with PM signiﬁcantly
outperformed kNN in all cases. This is likely due to the shared base layers that enable them to
efﬁciently share information across the per-treatment representations in the head networks.

Model Selection. To judge whether NN-PEHE is more suitable for model selection for counterfac-
tual inference than MSE, we compared their respective correlations with the PEHE on IHDP. We
found that NN-PEHE correlates signiﬁcantly better with the PEHE than MSE (Figure 2).

Number of Matches per Minibatch. To determine the impact of matching fewer than 100% of
all samples in a batch, we evaluated PM on News-8 trained with varying percentages of matched
samples on the range 0 to 100% in steps of 10% (Figure 4). We found that including more matches
indeed consistently reduces the counterfactual error up to 100% of samples matched. Interestingly,
we found a large improvement over using no matched samples even for relatively small percentages
(<40%) of matched samples per batch. This shows that propensity score matching within a batch is
indeed effective at improving the training of neural networks for counterfactual inference.

Figure 4: Change in error (y-axes) in terms of
precision in estimation of heterogenous effect
(PEHE) and average treatment effect (ATE) when
increasing the percentage of matches in each mini-
batch (x-axis). Symbols correspond to the mean
value of ˆ(cid:15)mATE (red) and √ˆ(cid:15)mPEHE (blue) on the
test set of News-8 across 50 repeated runs with
new outcomes (lower is better).

Figure 5: Comparison of several state-of-the-art
methods for counterfactual inference on the test
set of the News-8 dataset when varying the treat-
ment assignment imbalance κ (x-axis), i.e. how
much the treatment assignment is biased towards
more effective treatments. Symbols correspond
to the mean value of √ˆ(cid:15)mPEHE across 50 repeated
runs with new outcomes (lower is better).

7

Table 1: Comparison of methods for counterfactual inference with two and more available treatments
the standard deviation of √(cid:15)PEHE, √ˆ(cid:15)PEHE,
on IHDP and News-2/4/8/16. We report the mean value
and √ˆ(cid:15)mPEHE on the test sets over 1000 and 50 repeated runs for IHDP and News-2/4/8/16, respectively.
Best results on each benchmark in bold. † = signiﬁcantly different from PM (t-test, α < 0.05).

±

Method

PM
+ on X
+ MLP

kNN
PSMPM
PSMMI

RF
CF

BART
GANITE
PD
TARNET
CFRNETWass

IHDP
√(cid:15)PEHE

0.61
0.57
0.57

6.89
3.39
3.85

7.09
6.55

3.97
8.35
6.55
1.61
1.25

±
±
±

±
±
±

±
±

±
±
±
±
±

0.84
0.81
0.83

† 6.66
† 2.36
† 2.70

† 4.54
† 4.47

† 2.57
† 5.79
† 5.14
† 1.32
0.88

News-2
√ˆ(cid:15)PEHE

News-4
√ˆ(cid:15)mPEHE

News-8
√ˆ(cid:15)mPEHE

News-16
√ˆ(cid:15)mPEHE

16.76
17.06
† 18.38

† 18.14
† 17.49
† 17.40

† 17.39
† 17.59

† 18.53
† 18.28
† 17.52
17.17
16.93

1.26
1.22
1.46

1.64
1.49
1.30

1.24
1.63

2.02
1.66
1.62
1.25
1.12

±
±
±

±
±
±

±
±

±
±
±
±
±

21.58
21.41
† 25.05

† 27.92
† 22.74
† 37.26

† 26.59
† 23.86

† 26.41
† 24.50
20.88
† 23.40
† 22.65

2.58
1.75
2.80

2.44
2.58
2.28

3.02
2.50

3.10
2.27
3.24
2.20
1.97

±
±
±

±
±
±

±
±

±
±
±
±
±

20.76
20.90
† 24.88

† 26.20
† 22.16
† 30.50

† 23.77
† 22.56

† 25.78
† 23.58
21.19
† 22.39
† 21.64

1.86
2.07
1.98

2.18
1.79
1.70

2.14
2.32

2.66
2.48
2.29
2.32
1.82

±
±
±

±
±
±

±
±

±
±
±
±
±

20.24
20.67
† 27.05

† 27.64
† 23.57
† 28.17

† 26.13
† 21.45

† 27.45
† 25.12
† 22.28
† 21.19
† 20.87

1.46
1.42
2.47

2.40
2.48
2.02

2.48
2.23

2.84
3.53
2.25
2.01
1.46

±
±
±

±
±
±

±
±

±
±
±
±
±

Treatment Assignment Bias. To assess how the predictive performance of the different methods
is inﬂuenced by increasing amounts of treatment assignment bias, we evaluated their performances
on News-8 while varying the assignment bias coefﬁcient κ on the range of 5 to 20 (Figure 5). We
found that PM handles high amounts of assignment bias better than existing state-of-the-art methods.

Comparing Minibatch and Dataset Matching. As outlined previously, if we were successful in
balancing the covariates using the balancing score, we would expect that the counterfactual error is
implicitly and consistently improved alongside the factual error. To elucidate to what degree this is
the case when using the matching-based methods we compared, we evaluated the respective training
dynamics of PM, PSMPM and PSMMI (Figure 3). We found that PM better conforms to the desired
behavior than PSMPM and PSMMI. PSMPM, which used the same matching strategy as PM but on the
dataset level, showed a much higher variance than PM. PSMMI was overﬁtting to the treated group.

Limitations. A general limitation of this work, and most related approaches, to counterfactual
inference from observational data is that its underlying theory only holds under the assumption that
there are no unobserved confounders - which guarantees identiﬁability of the causal effects. However,
it has been shown that hidden confounders may not necessarily decrease the performance of ITE
estimators in practice if we observe suitable proxy variables [32, 33].

6 Conclusion

We presented PM, a new and simple method for training neural networks for estimating ITEs from
observational data that extends to any number of available treatments. In addition, we extended
the TARNET architecture and the PEHE metric to settings with more than two treatments, and
introduced a nearest neighbour approximation of PEHE and mPEHE that can be used for model
selection without having access to counterfactual outcomes. We performed experiments on several
real-world and semi-synthetic datasets that showed that PM outperforms a number of more complex
state-of-the-art methods in inferring counterfactual outcomes. We also found that the NN-PEHE
correlates signiﬁcantly better with real PEHE than MSE, that including more matched samples in each
minibatch improves the learning of counterfactual representations, and that PM handles an increasing
treatment assignment bias better than existing state-of-the-art methods. PM may be used for settings
with any amount of treatments, is compatible with any existing neural network architecture, simple
to implement, and does not introduce any additional hyperparameters or computational complexity.
Flexible and expressive models for learning counterfactual representations that generalise to settings
with multiple available treatments could potentially facilitate the derivation of valuable insights from
observational data in several important domains, such as healthcare, economics and public policy.

8

This work was partially funded by the Swiss National Science Foundation (SNSF) project No. 167302
within the National Research Program (NRP) 75 “Big Data”. We gratefully acknowledge the support
of NVIDIA Corporation with the donation of the Titan Xp GPUs used for this research.

Acknowledgments

References

[1] Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect:
Generalization bounds and algorithms. In International Conference on Machine Learning,
2017.

[2] Robert J LaLonde. Evaluating the econometric evaluations of training programs with experi-

mental data. The American economic review, pages 604–620, 1986.

[3] Daniel Carpenter. Reputation and power: organizational image and pharmaceutical regulation

at the FDA. Princeton University Press, 2014.

[4] Laura E. Bothwell, Jeremy A. Greene, Scott H. Podolsky, and David S. Jones. Assessing the
Gold Standard — Lessons from the History of RCTs. New England Journal of Medicine, 374
(22):2175–2181, 2016.

[5] Daniel E Ho, Kosuke Imai, Gary King, and Elizabeth A Stuart. Matching as nonparametric
preprocessing for reducing model dependence in parametric causal inference. Political analysis,
15(3):199–236, 2007.

[6] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the
curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of
computing, pages 604–613. ACM, 1998.

[7] Paul R. Rosenbaum and Donald B. Rubin. The central role of the propensity score in observa-

tional studies for causal effects. Biometrika, 70(1):41–55, 1983.

[8] Nathan Kallus. Recursive partitioning for personalization using observational data. In Interna-

tional Conference on Machine Learning, 2017.

[9] Michele Jonsson Funk, Daniel Westreich, Chris Wiesen, Til Stürmer, M. Alan Brookhart, and
Marie Davidian. Doubly robust estimation of causal effects. American Journal of Epidemiology,
173(7):761–767, 2011.

[10] Hugh A Chipman, Edward I George, Robert E McCulloch, et al. BART: Bayesian additive

regression trees. The Annals of Applied Statistics, 4(1):266–298, 2010.

[11] Hugh Chipman and Robert McCulloch. BayesTree: Bayesian additive regression trees. R

package version 0.3-1.4, 2016.

[12] Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects

using random forests. Journal of the American Statistical Association, 2017.

[13] Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual
inference. In International Conference on Machine Learning, pages 3020–3029, 2016.

[14] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning

bounds and algorithms. arXiv preprint arXiv:0902.3430, 2009.

[15] Ahmed M Alaa, Michael Weisz, and Mihaela van der Schaar. Deep counterfactual networks

with propensity-dropout. arXiv preprint arXiv:1706.05966, 2017.

[16] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GANITE: Estimation of Individu-
alized Treatment Effects using Generative Adversarial Nets. In International Conference on
Learning Representations, 2018.

[17] Ahmed M Alaa and Mihaela van der Schaar. Bayesian inference of individualized treatment
effects using multi-task gaussian processes. In Advances in Neural Information Processing
Systems, pages 3424–3432, 2017.

9

[18] Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions.

Journal of the American Statistical Association, 100(469):322–331, 2005.

[19] Judea Pearl. Causality. Cambridge university press, 2009.

[20] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: founda-

tions and learning algorithms. MIT press, 2017.

[21] Guido W Imbens. The role of the propensity score in estimating dose-response functions.

Biometrika, 87(3):706–710, 2000.

[22] Michael Lechner. Identiﬁcation and estimation of causal effects of multiple treatments under the
conditional independence assumption. In Econometric Evaluation of Labour Market Policies,
pages 43–58. Springer, 2001.

[23] Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computa-

tional and Graphical Statistics, 20(1):217–240, 2011.

[24] Ahmed Alaa and Mihaela Schaar. Limits of estimating heterogeneous treatment effects: Guide-
lines for practical algorithm design. In International Conference on Machine Learning, pages
129–138, 2018.

[25] Keisuke Hirano and Guido W Imbens. The propensity score with continuous treatments. Applied
Bayesian Modeling and Causal Inference from Incomplete-data Perspectives, 226164:73–84,
2004.

[26] Daniel E Ho, Kosuke Imai, Gary King, Elizabeth A Stuart, et al. MatchIt: nonparametric
preprocessing for parametric causal inference. Journal of Statistical Software, 42(8):1–28,
2011.

[27] Dominik Csiba and Peter Richtárik. Importance sampling for minibatches. Journal of Machine

Learning Research, 19(27), 2018.

[28] Alejandro Schuler, Michael Baiocchi, Robert Tibshirani, and Nigam Shah. A comparison
of methods for model selection when estimating individual treatment effects. arXiv preprint
arXiv:1804.05146, 2018.

[29] Vincent Dorie. NPCI: Non-parametrics for causal inference, 2016. URL https://github.

com/vdorie/npci.

[30] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.

[31] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.

[32] Mark R Montgomery, Michele Gragnolati, Kathleen A Burke, and Edmundo Paredes. Measuring

living standards with proxy variables. Demography, 37(2):155–174, 2000.

[33] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling.
Causal effect inference with deep latent-variable models. In Advances in Neural Information
Processing Systems, pages 6446–6456, 2017.

[34] Adam Kapelner and Justin Bleich. bartMachine: Machine learning with Bayesian additive

regression trees. arXiv preprint arXiv:1312.2171, 2013.

[35] Susan Athey, Julie Tibshirani, and Stefan Wager. Generalized random forests. arXiv preprint

arXiv:1610.01271, 2016.

10

