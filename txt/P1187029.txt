9
1
0
2
 
y
a
M
 
9
2
 
 
]

G
L
.
s
c
[
 
 
2
v
0
7
4
0
0
.
2
0
9
1
:
v
i
X
r
a

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN
PARTIAL MONITORING

Tor Lattimore
DeepMind, London

Csaba Szepesv´ari
DeepMind, London

LATTIMORE@GOOGLE.COM

SZEPI@GOOGLE.COM

Abstract
We prove a new minimax theorem connecting the worst-case Bayesian regret and minimax regret
under ﬁnite-action partial monitoring with no assumptions on the space of signals or decisions of
the adversary. We then generalise the information-theoretic tools of Russo and Van Roy (2016) for
proving Bayesian regret bounds and combine them with the minimax theorem to derive minimax
regret bounds for various partial monitoring settings. The highlight is a clean analysis of ‘easy’
and ‘hard’ ﬁnite partial monitoring, with new regret bounds that are independent of arbitrarily large
game-dependent constants and eliminate the logarithmic dependence on the horizon for easy games
that appeared in earlier work. The power of the generalised machinery is further demonstrated by
proving that the minimax regret for k-armed adversarial bandits is at most
2kn, improving on
existing results by a factor of 2. Finally, we provide a simple analysis of the cops and robbers
game, also improving best known constants.
Keywords: Online learning, partial monitoring, minimax theorems, bandits.

√

1. Introduction

Partial monitoring is a generalisation of the multi-armed bandit framework with an interestingly
richer structure. In this paper we are concerned with the ﬁnite-action version. Let k be the number
of actions. A ﬁnite-action partial monitoring game is described by two functions, the loss function
L : [k] × X → [0, 1] and a signal function Φ : [k] × X → Σ, where [k] = {1, 2, . . . , k} and X
and Σ are topological spaces. At the start of the game the adversary secretly chooses a sequence
of outcomes (xt)n
t=1 with xt ∈ X , where n is the horizon. The learner knows L, Φ and n and
sequentially chooses actions (At)n
t=1 from [k]. In round t, after the learner chooses At they suffer
a loss of L(At, xt) and observe only Φ(At, xt) as a way of indirectly learning about the loss. A
policy π is a function mapping action/signal sequences to probability distributions over actions (the
learner is allowed to randomise) and the regret of policy π in environment x = (xt)n

t=1 is

Rn(π, x) = max
a∈[k]

L(At, xt) − L(a, xt)

,

(cid:35)

(cid:34) n
(cid:88)

E

t=1

where the expectation is taken with respect to the randomness in the learner’s choices which follow
π. The minimax regret of a partial monitoring game is

R∗

n = inf
π∈Π

sup
x∈X n

Rn(π, x) ,

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

where Π is the space of all policies. Our objective is to understand how the minimax regret depends
on the horizon n and the structure of L and Φ. Note, this is the oblivious setting because the adver-
sary chooses all the losses at the start of the game. Some classical examples of partial monitoring
games are given in Table 1 and Fig. 5 in the appendix.

Setting

X

Σ

Φ(a, x)

L(a, x)

Full information

Bandit

Cops and robbers

[0, 1]k

[0, 1]

x

xa

[0, 1]k

[0, 1]k

[0, 1]k

[0, 1]k−1 x1, . . . , xa−1, xa+1, . . . , xk

xa

xa

xa

Finite partial monitoring

[d]

arbitrary arbitrary

arbitrary

Table 1: Example environment classes. In the last row, d is a natural number.

Bayesian viewpoint Although our primary objective is to shed light on the minimax adversarial
regret, we establish our results by ﬁrst proving uniform bounds on the Bayesian regret that hold for
any prior. Then a new minimax theorem demonstrates the existence of an algorithm with the same
minimax regret. While these methods are not constructive, we demonstrate that they lead to elegant
analysis of various partial monitoring problems, and better control of the constants in the bounds.

Let Q be a space of probability measures on X n with the Borel σ-algebra. The Bayesian regret

of a policy π with respect to prior ν ∈ Q is

The minimax Bayesian optimal regret is

BRn(π, ν) =

Rn(π, x)dν(x) .

(cid:90)

X n

BR∗

n(Q) = sup
ν∈Q

inf
π∈ΠM

BRn(π, ν) ,

where ΠM is a space of policies so that x (cid:55)→ Rn(π, x) is measurable, which we deﬁne formally in
Section 3. When Q is clear from the context, we write BR∗

n in place of BR∗

n(Q).

Contributions Our ﬁrst contribution is to generalise the machinery developed by Russo and Van
Roy (2016, 2017) and Bubeck et al. (2015). In particular, we prove a minimax theorem for ﬁnite-
action partial monitoring games with no restriction on either the loss or the feedback function. The
theorem establishes that the Bayesian optimal regret and minimax regret are equal: BR∗
n = R∗
n.
Next, the information-theoretic machinery of Russo and Van Roy (2017) is generalised by replacing
the mutual information with an expected Bregman divergence. The power of the generalisation is
demonstrated by showing that R∗
2kn for k-armed adversarial bandits, which improves on the
best known bounds by a factor of 2. The rest of the paper is focussed on applying these ideas to
ﬁnite partial monitoring games. The results enormously simplify existing analysis by sidestepping
the complex localisation arguments. At the same time, our bounds for the class of ‘easy non-
degenerate’ games do not depend on arbitrarily large game-dependent constants, which was true of
all prior analysis. Finally, for a special class of bandits with graph feedback called cops and robbers,
we show that R∗

n ≤ (cid:112)2n log(k), improving on prior work by a factor of 5/

n ≤

√

√

2.

2

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

2. Related work

n = 0 and R∗

n = Θ(n1/2) and ‘hard’ games for which R∗

Since partial monitoring is so generic, the related literature is vast, with most work focussing on
the full information setting (see Cesa-Bianchi and Lugosi (2006)) or the bandit setting (Bubeck and
Cesa-Bianchi (2012); Lattimore and Szepesv´ari (2019)). The information-theoretic machinery that
we build on was introduced by Russo and Van Roy (2016, 2017) in the context of minimizing the
Bayesian regret for stationary stochastic bandits (with varying structural assumptions). Bubeck et al.
(2015) noticed the results also applied to the ‘adversarial’ Bayesian setting and applied minimax the-
ory to prove worst-case bounds for convex bandits. Minimax theory has also been used to transfer
Bayesian regret bounds to adversarial bounds. For example, Abernethy et al. (2009) explores this in
the context of online convex optimisation in the full-information setting and Gravin et al. (2016) for
prediction with expert advice. The ﬁnite version of partial monitoring was introduced by Rustichini
(1999), who developed Hannan consistent algorithms. The main challenge since then has been char-
acterizing the dependence of the regret on the horizon in terms of the structure of the loss and signal
functions. It is now known that all games can be classiﬁed into one of exactly four types. Trivial and
hopeless, for which R∗
n = Ω(n) respectively. Between these extremes there are ‘easy’
games where R∗
n = Θ(n2/3). The classiﬁcation result
is proven by piecing together upper and lower bounds from various papers (Cesa-Bianchi et al.,
2006; Foster and Rakhlin, 2012; Antos et al., 2013; Bart´ok et al., 2014; Lattimore and Szepesv´ari,
2019). A caveat of the classiﬁcation theorem is that the focus is entirely on the dependence of the
minimax regret on the horizon. The leading constant is game-dependent and poorly understood.
Existing bounds for easy games depend on a constant that can be arbitrarily large, even for ﬁxed
d and k. One of the contributions of this paper is to resolve this issue. Another disadvantage of
the current partial monitoring literature, especially in the adversarial setting, is that the algorithms
and analysis tend to be rather complicated. Although our results only prove the existence of an
algorithm witnessing a claimed minimax bound, the Bayesian algorithm and analysis are intuitive
and natural. There is also a literature on stochastic partial monitoring, with early analysis by Bart´ok
et al. (2011). A quite practical algorithm was proposed by Vanchinathan et al. (2014). The asymp-
totics have also been worked out (Komiyama et al., 2015). Although a frequentist regret bound in a
stochastic setting normally implies a Bayesian regret bound, in our Bayesian setup the environments
are not stationary, while all the algorithms for the stochastic case rely heavily that the distribution of
the adversary is stationary. Generalising these algorithms to the non-stationary case does not seem
straightforward. Finally, we should mention there is an alternative deﬁnition of the regret that is
less harsh on the learner. For trivial, easy and hard games it is the same, but for hopeless games the
regret captures the hopelessness of the task and measures the performance of the learner relative to
an achievable objective. We do not consider this deﬁnition here. Readers interested in this varia-
tion can consult the papers by Rustichini (1999); Mannor and Shimkin (2003); Perchet (2011) and
Mannor et al. (2014).

3. Notation and conventions

The maximum/supremum of the empty set is negative inﬁnity. The standard basis vectors in Rd
are e1, . . . , ed. The column vector of all ones is 1 = (1, 1, . . . , 1)(cid:62). The standard inner product is
(cid:104)·, ·(cid:105). The ith coordinate of vector x ∈ Rd is xi. The (d − 1)-dimensional probability simplex is
∆d−1 = {x ∈ [0, 1]d : (cid:107)x(cid:107)1 = 1}. The interior of a topological space Z is int(Z) and its boundary
is ∂Z. The relative entropy between probability measures µ and ν over the same measurable space

3

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

is D (ν || µ) = (cid:82) log( dν
dµ )dν if ν (cid:28) µ and D (ν || µ) = ∞ otherwise, where log is the natural
logarithm. When X is a random variable with X ∈ [a, b] almost surely, then Pinsker’s inequality
combined with straightforward inequalities shows that

(cid:90)

X(dµ − dν) ≤ (b − a) (cid:107)µ − ν(cid:107)TV ≤ (b − a)

D (µ || ν) ,

(1)

(cid:114) 1
2

where (cid:107)µ − ν(cid:107)TV is the total variation distance. When ν (cid:28) µ, the squared Hellinger distance can
be written as h(ν, µ)2 = (cid:82) (1 − (cid:112)dν/dµ)2dµ. Given a measure P and jointly distributed random
elements X and Y we let PX denote the law of X and (unconventionally) we let PX|Y be the
conditional law of X given Y , which satisﬁes PX|Y (·) = P(X ∈ · | Y ). One can think of PX|Y as
a random probability measure over the range of X that depends on Y . In none of our analysis do
we rely on exotic spaces where such regular versions do not exist. When Y ∈ [k] is discrete we let
PX|Y =i denote P(X ∈ · | Y = i) for i ∈ [k]. With this notation the mutual information between
X and Y is I(X; Y ) = E[D(PX|Y || PX )]. The domain of a convex function F : Rd → R ∪ {∞}
is dom(F ) = {x : F (x) < ∞}. The Bregman divergence with respect to convex/differentiable
F is DF : dom(F ) × dom(F ) → [0, ∞]. For x, y ∈ dom(F ) this is deﬁned by DF (x, y) =
F (x) − F (y) − ∇x−yF (y), where ∇vF (y) is the directional derivative of F in direction v at y. The
relative entropy between categorical distributions p, q ∈ ∆k−1 is the Bregman divergence between
p and q where F is the unnormalised negentropy: F (p) = (cid:80)k
i=1(pi log(pi) − pi) with domain
[0, ∞)k. The diameter of a convex set K with respect to F is diamF (K) = supx,y∈K F (x) − F (y).

Probability spaces, policies and environments The Borel σ-algebra on topological space Z is
B(Z). Recall that X and Σ are assumed to carry a topology, which we will use for ensuring
measurability of the regret. More about the choices of these topologies later. We assume the signal
function Φ(a, ·) is B(X )/B(Σ)-measurable and the loss function L(a, ·) is B(X )-measurable. A
policy is a function π : ∪n
t=1([k] × Σ)t−1 → ∆k−1 and the space of all policies is Π. A policy is
measurable if ht−1 (cid:55)→ π(ht−1) is B(([k] × Σ)t)-measurable for all ht−1 = a1, σ1, . . . , at−1, σt−1,
which coincides with the usual deﬁnition of a probability kernel. The space of all measurable
policies is ΠM. In general ΠM is a strict subset of Π. For most of the paper we work in the Bayesian
framework where there is a prior probability measure ν on (X n, B(X n)). Given a prior ν and a
measurable policy π ∈ ΠM, random elements X ∈ X n and A ∈ [k]n are deﬁned on common
probability space (Ω, F, P). We let Φt(a) = Φ(a, Xt) and Lt(a) = L(a, Xt). Expectations E are
with respect to P. For t ∈ [n + 1] we let Ft = σ(A1, Φ(A1, X1), . . . , At−1, Φ(At−1, Xt−1)) ⊆ F,
Et[·] = E[· | Ft] and Pt(·) = P(· | Ft). Note that F1 = {∅, Ω} is the trivial σ algebra. The σ-algebra
F and the measure P are such that

1. The law of the adversaries choices satisﬁes P(X ∈ · ) = ν(·).

2. For any t ∈ [n], the law of the actions almost surely satisﬁes

Pt(At ∈ · ) = Pt(At ∈ · | X) = π(A1, Φ1(A1), . . . , At−1, Φt−1(At−1))(·) .

(2)

The existence of a probability space satisfying these properties is guaranteed by Ionescu-Tulcea
(Kallenberg, 2002, Theorem 6.17). The last condition captures the important assumption that, con-
ditioned on the observed history, At is sampled independently from X. In particular, it implies that

4

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

Xt and At are independent under Pt. The optimal action is A∗ = arg mina∈[k]
not hard to see that the Bayesian regret is well deﬁned and satisﬁes

(cid:80)n

t=1 Lt(a). It is

BRn(π, ν) = E

(cid:34) n
(cid:88)

(cid:35)
Lt(At) − Lt(A∗)

= E

t=1

(cid:35)

∆t

,

(cid:34) n
(cid:88)

t=1

where ∆t = Lt(At) − Lt(A∗). To minimise clutter, when the policy π and prior ν are clear from
the context, we abbreviate BRn(π, ν) to BRn. We let Pta = Pt(At = a), which means that
Pt ∈ ∆k−1 is a probability vector.

4. Minimax theorem

Our ﬁrst main result is a theorem that connects the minimax regret to the worst-case Bayesian regret
over all ﬁnitely supported priors. The regret Rn(π, x) is well deﬁned for any x and any policy
π ∈ Π, but the Bayesian regret depends on measurability of x (cid:55)→ Rn(π, x). If ν is supported on a
ﬁnite set x1, . . . , xm ∈ X n, however, we can write

BRn(π, ν) =

ν({xi})Rn(π, xi) ,

m
(cid:88)

i=1

which does not rely on measurability. By considering ﬁnitely supported priors we free ourselves
from any concern that x (cid:55)→ Rn(π, x) might not be measurable. This also means that if Σ (or X )
came with some topologies, we simply replace them with the discrete topology (which makes all
maps continuous and measurable, implying Π = ΠM).

Theorem 1 Let Q be the space of all ﬁnitely supported probability measures on X n. Then

inf
π∈Π

sup
x∈X n

Rn(π, x) = sup
ν∈Q

min
π∈Π

BRn(π, ν) .

n =
n(Q), which is the form we prove in Appendix A. The strength of this result is that it depends

An equivalent statement of this theorem is that if X and Σ carry the discrete topology then R∗
BR∗
on no assumptions except that the action set is ﬁnite.

Our proof borrows techniques from a related result by Bubeck et al. (2015). The main idea
is to replace the policy space Π with a simpler space of ‘mixtures’ over deterministic policies,
which is related to Kuhn’s celebrated result on the equivalence of behavioral and mixed strategies
(Kuhn, 1953). We then establish that this space is compact and use Sion’s theorem to exchange the
minimum and maximum. While we borrowed the ideas from Bubeck et al. (2015), our proof relies
heavily on the ﬁniteness of the action space, which allowed us to avoid any assumptions on Σ and
X , which also necessitated our choice of Q. Neither of the two results imply each other.

Theorem 1 is a minimax theorem for a special kind of two-player multistage zero-sum determin-
istic partial information game. Minimax theorems for this case are nontrivial because of challenges
related to measurability and the use of Sion’s theorem. Although there is a rich and sophisticated
literature on this topic, we are not aware of any result implying our theorem. Tools include the
approach we took using the weak topology (Bernhard, 1992), or the so-called weak-strong topology
(Leao et al., 2000) and reduction to completely observable games and then using dynamic program-
ming (Ghosh et al., 2004). An interesting challenge is to extend our result to compact action spaces.

5

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

One may hope to generalise the proof by Bubeck et al. (2015), but some important details are miss-
ing (for example, the measurable space on which the priors live is undeﬁned, the measurability of
the regret is unclear as is the compactness of distributions induced by measurable policies). We
believe that the approach of Ghosh et al. (2004) can complete this result.

5. The regret information tradeoff

Unless otherwise mentioned, all expectations E are with respect to the probability measure over
interactions between a ﬁxed policy π ∈ ΠM and an environment sampled from a prior ν on
(X n, B(X n)). Before our generalisation we present a restatement of the core theorem in the analy-
sis by Russo and Van Roy (2016). Let It(X; Y ) be the mutual information between X and Y under
Pt. Although the proof is identical, the setup here is different because the prior ν is arbitrary.

Theorem 2 (Russo and Van Roy (2016)) Suppose there exists a constant β ≥ 0 such that Et[∆t] ≤
(cid:112)βIt(A∗; Φt(At), At) almost surely for all t. Then BRn ≤ (cid:112)nβ log(k).

This elegant result provides a bound on the regret in terms of the information gain about the op-
timal arm. Our generalisation replaces the information gain with an expected Bregman divergence.

t=1 be an Rd-valued martingale adapted to (Ft)n+1
Theorem 3 Let (Mt)n+1
t=1 and Mt ∈ D ⊂ Rd
almost surely for all t. Then let F be a convex function with diamF (D) < ∞. Suppose there exist
constants α, β ≥ 0 such that Et[∆t] ≤ α + (cid:112)βEt[DF (Mt+1, Mt)] almost surely for all t. Then
BRn ≤ αn + (cid:112)nβ diamF (D).

Proof We calculate

Et[DF (Mt+1, Mt)] = Et

F (Mt+1) − F (Mt) −

(cid:20)

(cid:18)

(cid:20)

lim inf
h→0+
(cid:18)

Et

≤ lim inf
h→0+

F (Mt+1) − F (Mt) −

= Et [F (Mt+1)] − F (Mt) + lim inf
h→0+

≤ Et [F (Mt+1)] − F (Mt) + lim inf
h→0+

= Et [F (Mt+1)] − F (Mt) ,

(cid:19)(cid:21)

(cid:21)(cid:19)

F (Mt + h(Mt+1 − Mt)) − F (Mt)
h
F ((1 − h)Mt + hMt+1) − F (Mt)
h
F (Mt) − Et[F ((1 − h)Mt + hMt+1)]
h
F (Mt) − F (Et[(1 − h)Mt + hMt+1])
h

(3)

where the ﬁrst inequality follows from Fatou’s lemma and the second from convexity of F . The last
equality is because Et[Mt+1] = Mt. Hence

BRn = E

∆t

≤ αn + E

(cid:35)

(cid:34) n
(cid:88)

t=1

(cid:34) n
(cid:88)

(cid:35)
(cid:112)βEt[DF (Mt+1, Mt)]

t=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)βnE

(cid:34) n
(cid:88)

(cid:35)
Et[DF (Mt+1, Mt)]

≤ αn +

≤ αn + (cid:112)βn diamF (D) ,

t=1

where the ﬁrst inequality follows from the assumption in the theorem, the second by Cauchy-
Schwarz, while the third follows by Eq. (3), telescoping and the deﬁnition of the diameter.

6

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

A natural choice for Mt is the posterior distribution of the optimal action. Let P ∗

ta = Pt(A∗ =
a), which is the posterior probability that A∗ = a based on the information available at the start
of round t. By the tower rule, we have Et[P ∗
t+1] = P ∗
t=1 is a martingale adapted to
(Ft)n+1
t=1 .

t so that (P ∗

t )n+1

Corollary 4 Let F : Rk → R be a convex function with diamF (∆k−1) < ∞. Suppose there exist
constants α, β ≥ 0 such that Et[∆t] ≤ α +
t )] almost surely for all t. Then
BRn ≤ αn + (cid:112)nβ diamF (∆k−1).

βEt[DF (P ∗

t+1, P ∗

(cid:113)

Remark 5 That Theorem 3 generalises Theorem 2 follows by choosing F as the unnormalised
negentropy for which diamF (∆k−1) ≤ log(k) and Et[DF (P ∗
t )] = It(A∗; Φt(At), At). The
assumption that Mt ∈ Rd can be relaxed. The result continues to hold when Mt takes values in a
bounded subset of a Banach space, where the martingale is deﬁned using the Bochner integral. The
Bregman divergence generalises naturally via the Gateoux derivative.

t+1, P ∗

6. Finite-armed bandits

In the bandit setting the learner observes the loss of the action they play, which is modelled by
choosing Σ = [0, 1], X = [0, 1]k and Φ(a, x) = L(a, x) = xa. The best known bound is by
Bubeck and Cesa-Bianchi (2012), who prove that online mirror descent with an appropriate potential
satisﬁes R∗
n ≤
8kn. Using the same potential in combination with Theorem 3 allows us to
improve this result to R∗

2kn.

√

√

n ≤

Theorem 6 The minimax regret for k-armed adversarial bandits satisﬁes R∗

Proof Let F (p) = −2 (cid:80)k
k. Combine
Corollary 4 and Theorem 1 and Lemma 7 below for Thompson sampling, which is the policy that
samples At from Pt = P ∗
t .

pa, which has domain [0, ∞)k and diamF (∆k−1) ≤ 2

a=1

√

√

n ≤

2kn.
√

Lemma 7 Let F be as above and Pt = P ∗

t . Then Et[∆t] ≤

k1/2Et[DF (P ∗

t+1, P ∗

t )] a.s..

(cid:113)

Remark 8 Potentials other than the negentropy have been used in many applications in bandits
and online convex optimisation. The log barrier, for example, leads to ﬁrst order bounds for k-
armed bandits (Wei and Luo, 2018). Alternative potentials also appear in the context of adversarial
linear bandits (Bubeck et al., 2012, 2018) and follow the perturbed leader (Abernethy et al., 2014).
Investigating the extent to which these applications transfer to the Bayesian setting is an interesting
direction for the future.

7. Finite partial monitoring games

Recall from Table 1 that a ﬁnite partial monitoring game is characterised by functions L : [k] ×
[d] → [0, 1] and Φ : [k] × [d] → Σ where d is a natural number and Σ is arbitrary. Finite partial
monitoring enjoys a rich linear structure, which we now summarise. A picture can help absorbing
these concepts, and is provided with an example at the beginning of Appendix I. For a ∈ [k],
let (cid:96)a ∈ [0, 1]d be the vector with (cid:96)ax = L(a, x). Actions a and b are duplicates if (cid:96)a = (cid:96)b.

7

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

The cell associated with action a is Ca = {u ∈ ∆d−1 : (cid:104)(cid:96)a, u(cid:105) ≤ minb(cid:54)=a(cid:104)(cid:96)b, u(cid:105)}, which is the
subset of distributions u ∈ ∆d−1 where action a minimises Ex∼u[L(a, x)]. Note that Ca ⊂ Rd
is a closed convex polytope and its dimension dim(Ca) is deﬁned as the dimension of the afﬁne
space it generates. An action a is called Pareto optimal if Ca has dimension d − 1 and degenerate
otherwise. Of course ∪aCa = ∆d−1, but cells may have nonempty intersection. When a and b are
not duplicates, the intersection Ca ∩ Cb is a (possibly empty) polytope of dimension at most d − 2.
A pair of Pareto optimal actions a and b are called neighbours if Ca ∩ Cb has dimension d − 2. A
game is called non-degenerate if there are no degenerate actions and no duplicate actions. So far
none of the concepts have depended on the signal function. Local observability is a property of the
signal and loss functions that allows the learner to estimate loss differences between actions a and
b by playing only those actions. For neighbours a and b let Nab = {c : Cc ⊆ Ca ∩ Cb}, which
contains a and its duplicates, b and its duplicates, and degenerate actions c with Cc = Ca ∩ Cb. A
game is globally observable if for all pairs of neighbours there exists a function f : [k] × Σ → R
such that

L(a, x) − L(b, x) =

f (c, Φ(c, x)) .

(4)

k
(cid:88)

c=1

The game is locally observable if for all pairs of neighbours a and b the function f can be chosen
satisfying Eq. (4) and additionally that f (c, Φ(c, x)) = 0 for all c /∈ Nab. In the standard analysis
of partial monitoring the function f is used to derive importance-weighted estimators of the loss
In the following f is used more directly. A quantity that appears naturally in the
differences.
analysis is the supremum norm of the estimation functions f . Given a globally observable game,
we let v ≥ 0 be the smallest value such that for all pairs of neighbours a and b there exists a function
satisfying Eq. (4) with (cid:107)f (cid:107)∞ ≤ v. For locally observable games v is deﬁned in the same way, but
with the additional restriction that f is supported on Nab. The neighbourhood of a is Na = {b :
dim(Ca ∩Cb) ≥ d−2}. The neighbourhood graph over [k] has edges {(a, b) : a, b are neighbours}.
For non-degenerate games, the neighbourhood graph is connected.

The following theorem classiﬁes all partial monitoring games into one of four categories. All
results were known previously except that previous upper bounds for locally observable games were
R∗

n = O((n log(n))1/2).

Theorem 9 The minimax regret for ﬁnite partial monitoring game G satisﬁes the following:

R∗

n =


0

Θ(n1/2)
Θ(n2/3)

Ω(n)

if there are no neighbouring actions
if there are neighbouring actions and G is locally observable
if G is globally observable and not locally observable
otherwise .

Summary of new results The main theorem is the following, which improves on previous bounds
that all depend on arbitrarily large game-dependent constants, even when k and d are ﬁxed.

Theorem 10 For any locally observable non-degenerate game: R∗

n ≤ k3/2(d + 1)(cid:112)8n log(k).

For degenerate locally observable games the bound differs only due to the increased norm of the
estimation functions. In particular, we have the following theorem, which improves on prior work
in terms of constants and logarithmic factors (Lattimore and Szepesv´ari, 2019).

8

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

Theorem 11 For any locally observable game: R∗
the supremum norm of the estimation functions.

n ≤ vk3/2(cid:112)8n log(k), where v is a bound on

The bound for globally observable games has the same order as the prior work, but with slightly

improved constants (Cesa-Bianchi et al., 2006).

Theorem 12 For any globally observable game: R∗
bound on the supremum norm of the estimation functions.

n ≤ 3(nkv)2/3(log(k)/2)1/3, where v is a

Finally, for any locally/globally observable game, Lemma 25 in the appendix shows that the
norm of the estimators is bounded by at most v ≤ d1/2(1 + k)d/2, which provides an explicit upper
bound that is independent of the loss and signal matrix. We believe the exponential dependence on
the dimension is unavoidable in general.

8. Proof of Theorem 10

For this section we assume the game is non-degenerate and locally observable. Before the proof
of Theorem 10 we provide the algorithm, which seems to be novel among previous algorithms for
partial monitoring. Note that Thompson sampling can suffer linear regret in partial monitoring (Ap-
Et[Lt(a)] be the greedy action that minimises the 1-step Bayesian
pendix G). Let Gt = arg mina∈[k]
expected loss. The idea is to deﬁne a directed tree with vertex set [k] and root Gt and where all paths
lead to Gt. A little notation is needed. Deﬁne an undirected graph with vertices Vt and edges Et by
Vt = {a ∈ [k] : Et[Lt(a)] = Et[Lt(Gt)]} and Et = {a, b ∈ Vt : a and b are neighbours}, which is
connected by Lemma 23. Note that Vt = {Gt} when Gt is unique, but this is not always the case.
For a ∈ Vt let ρt(a) be the length of the shortest path from a to Gt in (Vt, Et) with ρt(Gt) = 0 by
deﬁnition. Let Pt : [k] → [k] be the ‘parent’ function:

(cid:40)

Pt(a) =

Et[Lt(b)]
arg minb∈Na
arg minb∈Na∩Vt ρt(b)

if a /∈ Vt
otherwise .

The following lemma is proven in Appendix H.

Lemma 13 The directed graph over vertex set [k] with an edge from a to b if a (cid:54)= Gt and b = P(a)
is a directed tree with root Gt.

Let At(a) be the set of ancestors of action a in the tree deﬁned in Lemma 13. We adopt the
convention that a ∈ At(a). By the previous lemma, Gt ∈ At(a) for all a. Let Dt(a) be the set
of descendants of a, which does not include a (Fig. 7). The depth of an action a in round t is the
distance between a and the root Gt. An action a is called anomalous for P ∈ ∆k−1 in round t if
Pa < maxb∈Dt(a) Pb. Algorithm 2 deﬁnes the ‘water transfer’ operator Wt : ∆k−1 → ∆k−1 that
corrects this deﬁciency by transferring mass towards the root of the tree deﬁned in Lemma 13 while
ensuring that (a) the loss suffered when playing the according to the transformed distribution does
not increase and (b) the distribution is not changed too much. The process is illustrated in Fig. 1 in
Appendix F, where you will also ﬁnd the proof of the next lemma.

Lemma 14 Let P ∈ ∆k−1 and Q = W k

t P = Wt · · · WtP . Then:

9

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

1. (cid:80)k

a=1 QaEt[Lt(a)] ≤ (cid:80)k

a=1 PaEt[Lt(a)].

2. Qa ≤ QPt(a) for all a ∈ [k].

3. Qa ≥ Pa/k for all a ∈ [k].

Our new algorithm samples At from Pt = W k

t P ∗
t . Because of the plumbing and randomisation,
the new algorithm is called Mario sampling (Algorithm 1). The proof of Theorem 10 follows
immediately from Theorems 1 and 2 and the following lemma.

input:

partial monitoring game (Σ, L, Φ) and prior ν

for t = 1, . . . , n

Algorithm 1: Mario sampling

compute P ∗

t and Pt = W k

t P ∗
t .

Then sample At ∼ Pt.

Lemma 15 For Mario sampling: Et[∆t] ≤ (d + 1)k3/2(cid:112)8It(A∗; Φt(At), At) a.s..

Proof We assume an appropriate zero measure set is discarded so that we can omit the qualiﬁcation
‘almost surely’ for the rest of the proof. By the ﬁrst part of Lemma 14,

Et[∆t] ≤

ta (Et[Lt(a)] − Et[Lt(a) | A∗ = a]) .
P ∗

(5)

k
(cid:88)

a=1

For b (cid:54)= Gt let fb, gb : Σ → R be a pair of functions such that max{(cid:107)fb(cid:107)∞ , (cid:107)gb(cid:107)∞} ≤ d + 1
and fb(Φ(b, x)) + gb(Φ(Pt(b), x)) = L(b, x) − L(Pt(b), x) for all x ∈ [d]. The existence of such
functions is guaranteed by Lemma 24 and the fact that Pt(b) ∈ N (b) and because we assumed the
game is non-degenerate, locally observable. The expected loss of a can be decomposed in terms of
the sum of differences to the root,

Et[Lt(a)] = Et

Lt(Gt) +

(Lt(b) − Lt(Pt(b)))





= Et

Lt(Gt) +

fb(Φt(b)) + gb(Φt(Pt(b)))

 .

(6)

In the same way,

Et[Lt(a) | A∗ = a] = Et

Lt(Gt) +

fb(Φt(b)) + gb(Φt(Pt(b)))

A∗ = a

 .

(7)



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)









b∈At(a)\{Gt}

(cid:88)

(cid:88)

b∈At(a)\{Gt}

(cid:88)

b∈At(a)\{Gt}

10

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

Then, because At(a) and Gt are Ft-measurable,

Et[∆t] ≤

ta (Et[Lt(a)] − Et[Lt(a) | A∗ = a])
P ∗

(Eq. (5))

k
(cid:88)

a=1
(cid:34)

=

k
(cid:88)

a=1

P ∗
ta

(cid:88)

b∈At(a)\{Gt}

(cid:88)

+

b∈At(a)\{Gt}

(Et[fb(Φt(b))] − Et[fb(Φt(b)) | A∗ = a])

(Et[gb(Φt(Pt(b)))] − Et[gb(Φt(Pt(b))) | A∗ = a])

(Eqs. (6) and (7))

(cid:35)

≤ (d + 1)

8 D (cid:0)Pt,Φt(b)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(b)
(cid:12)

(cid:1)

(Eq. (1), D ≥ 0)

≤ k(d + 1)

(cid:88)

ta D (cid:0)Pt,Φt(b)|A∗=a
P ∗

(cid:12)
(cid:12)
(cid:12) Pt,Φt(b)
(cid:12)

(cid:1)

(Cauchy-Schwarz)

≤ k3/2(d + 1)

(cid:88)

P ∗
ta

Ptb D (cid:0)Pt,Φt(b)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(b)
(cid:12)

(cid:1)

(Lemma 14, Part 3)

(cid:88)

(cid:113)

b∈At(a)

k
(cid:88)

P ∗
ta

a=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)8

b∈At(a)

k
(cid:88)

a=1

P ∗
ta

(cid:118)
(cid:117)
(cid:117)
(cid:116)8

(cid:118)
(cid:117)
(cid:117)
(cid:116)8

k
(cid:88)

a=1

k
(cid:88)

a=1

b∈At(a)

P ∗
ta

k
(cid:88)

b=1

≤ k3/2(d + 1)

Ptb D (cid:0)Pt,Φt(b)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(b)
(cid:12)

(cid:1)

= k3/2(d + 1)(cid:112)8It(A∗; Φt(At), At) .

(D ≥ 0)

(Lemma 26)

Remark 16 In many games there exists a constant m such that |At(a)| ≤ m almost surely for
all a and t. In this case Part 3 of Lemma 14 improves to Pta ≥ P ∗
ta/m and the application of
Cauchy-Schwarz in Lemma 15 can be strengthened. This means the bound in Theorem 10 becomes
m(d + 1)(cid:112)8kn log(k). For the game illustrated in Fig. 7, m = 5 while k = 7, but more extreme
examples are easily constructed.

9. Discussion and future directions

One of the main beneﬁts of the information-theoretic approach is the simplicity and naturality of the
arguments, which is particularly striking in partial monitoring. Even for the k-armed bandit analysis
there is no tuning of learning rates or careful bounding of dual norms. In exchange, our results
are existential, though we emphasise that the Bayesian setting is interesting in its own right. We
anticipate that Theorem 3 will have many other applications and there is clearly more to understand
about this generalisation. Is it a coincidence that the same potential leads to minimax bounds using
both online stochastic mirror descent and Thompson sampling?

11

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

input: P ∈ ∆k−1 and tree determined by Pt
find action a at the greatest depth such that Pa < maxb∈Dt(a) Pb.
if no such action is found, let WtP = P and return.

for α ∈ [0, 1] let Dt(a; α) = {b ∈ Dt(a) : Pb ≥ α}.
let α∗ be the largest α ∈ {Pb : b ∈ Dt(a)} such that

pα =

1
1 + |Dt(a; α)|

(cid:88)

b∈Dt(a;α)∪{a}

Pb > qα = max{Pb : b ∈ Dt(a) \ Dt(a; α)} .

let (WtP )b = pα∗ if b ∈ Dt(a; α∗) ∪ {a} and (WtP )b = Pb otherwise.

Algorithm 2: The water transfer operator Wt : ∆k−1 → ∆k−1.

Information-directed sampling Thompson sampling depends on the prior, but not the potential
that appears in Theorem 3. Russo and Van Roy (2014) noted that the information-theoretic analysis
is tightest when the algorithm is chosen to minimize Et[∆t]2/Et[DF (P ∗
t )], where F is the
unnormalised negentropy. Our generalisation provides a means of constructing new algorithms by
changing the potential.

t+1, P ∗

Open problems An obvious next step is stress test the applicability of Theorem 3. Bandits with
graph feedback beyond cops and robbers might be a good place to start (Alon et al., 2015). One may
also ask whether in adversarial linear bandits the results by Bubeck et al. (2018) can be replicated
or improved using Theorem 3. There are many open problems in partial monitoring, a few of which
we now describe. We hope some readers will be inspired to work on them!

Adaptivity There exist games where for ‘nice’ adversaries the regret should be O(n1/2) while for
truly adversarial data the regret is as large as Θ(n2/3). Designing algorithms that adapt to a broad
range of adversaries is an interesting challenge. Some work on this topic in the stochastic setting
is by Bart´ok et al. (2012). A related question is understanding how to use the information-theoretic
machinery to provide adaptive bounds.

Constants Our results have eliminated arbitrarily large constants from the analysis of easy non-
degenerate games. Still, we do not yet understand how the regret should depend on the structure of
L or Φ except in special cases. The result in Remark 16 is a small step in this direction, but there is
much to do. The best place to start is probably lower bounds. Currently generic lower bounds for
ﬁnite partial monitoring focus on the dependence on the horizon. One concrete question is whether
or not the minimum supremum norm of the estimation functions that appears in Theorem 12 is a
fundamental quantity.

Stochastic analysis of Mario sampling Theorem 2 and Lemma 15 show that for any prior Mario
sampling satisﬁes BRn ≤ k3/2(d + 1)(cid:112)8n log(k). In the stationary stochastic setting we expect
that for a suitable prior it should be possible to prove a bound on the frequentist regret of this
algorithm. Perhaps the techniques developed by Agrawal and Goyal (2013) or Kaufmann et al.
(2012) generalise to this setting.

12

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

References

J. Abernethy, A. Agarwal, P. L. Bartlett, and A. Rakhlin. A stochastic view of optimal regret through
minimax duality. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.

J. D. Abernethy, C. Lee, A. Sinha, and A. Tewari. Online linear optimization via smoothing. In
M. F. Balcan, V. Feldman, and Cs. Szepesv´ari, editors, Proceedings of The 27th Conference on
Learning Theory, volume 35 of Proceedings of Machine Learning Research, pages 807–823,
Barcelona, Spain, 13–15 Jun 2014. PMLR.

S. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sampling. In C. M. Carvalho
and P. Ravikumar, editors, Proceedings of the 16th International Conference on Artiﬁcial Intel-
ligence and Statistics, volume 31 of Proceedings of Machine Learning Research, pages 99–107,
Scottsdale, Arizona, USA, 29 Apr–01 May 2013. PMLR.

N. Alon, N. Cesa-Bianchi, O. Dekel, and T. Koren. Online learning with feedback graphs: Beyond
In Peter Gr¨unwald, Elad Hazan, and Satyen Kale, editors, Proceedings of The 28th
bandits.
Conference on Learning Theory, volume 40 of Proceedings of Machine Learning Research, pages
23–35, Paris, France, 03–06 Jul 2015. PMLR.

A. Antos, G. Bart´ok, D. P´al, and Cs. Szepesv´ari. Toward a classiﬁcation of ﬁnite partial-monitoring

games. Theoretical Computer Science, 473:77–99, 2013.

G. Bart´ok, D. P´al, and Cs. Szepesv´ari. Minimax regret of ﬁnite partial-monitoring games in stochas-
In Proceedings of the 24th Annual Conference on Learning Theory, pages

tic environments.
133–154, 2011.

G. Bart´ok, N. Zolghadr, and Cs. Szepesv´ari. An adaptive algorithm for ﬁnite stochastic partial
monitoring. In Proceedings of the 29th International Coference on International Conference on
Machine Learning, ICML, pages 1779–1786, USA, 2012. Omnipress.

G. Bart´ok, D. P. Foster, D. P´al, A. Rakhlin, and Cs. Szepesv´ari. Partial monitoring—classiﬁcation,
regret bounds, and algorithms. Mathematics of Operations Research, 39(4):967–997, 2014.

P. Bernhard. Information and strategies in dynamic games. SIAM Journal on Control and Optimiza-

tion, 30(1):212–228, 1992.

V. I. Bogachev. Measure theory, volume 2. Springer Science & Business Media, 2007.

S. Bubeck and N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed
Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorporated,
2012.

S. Bubeck, N. Cesa-Bianchi, and S. Kakade. Towards minimax policies for online linear optimiza-
tion with bandit feedback. In Annual Conference on Learning Theory, volume 23, pages 41–1.
Microtome, 2012.

S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization:

T regret in one
dimension. In P. Gr¨unwald, E. Hazan, and S. Kale, editors, Proceedings of The 28th Conference
on Learning Theory, volume 40 of Proceedings of Machine Learning Research, pages 266–278,
Paris, France, 03–06 Jul 2015. PMLR.

√

13

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

S. Bubeck, M. Cohen, and Y. Li. Sparsity, variance and curvature in multi-armed bandits.

In
F. Janoos, M. Mohri, and K. Sridharan, editors, Proceedings of Algorithmic Learning Theory,
volume 83 of Proceedings of Machine Learning Research, pages 111–127. PMLR, 07–09 Apr
2018.

N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge university press,

2006.

N. Cesa-Bianchi, G. Lugosi, and G. Stoltz. Regret minimization under partial monitoring. Mathe-

matics of Operations Research, 31:562–580, 2006.

D. Foster and A. Rakhlin. No internal regret via neighborhood watch.

In N. D. Lawrence and
M. Girolami, editors, Proceedings of the 15th International Conference on Artiﬁcial Intelligence
and Statistics, volume 22 of Proceedings of Machine Learning Research, pages 382–390, La
Palma, Canary Islands, 21–23 Apr 2012. PMLR.

S. Ghosal and A. van der Vaart. Fundamentals of nonparametric Bayesian inference, volume 44.

Cambridge University Press, 2017.

M. K. Ghosh, D. McDonald, and S. Sinha. Zero-sum stochastic games with partial information.

Journal of optimization theory and applications, 121(1):99–118, 2004.

N. Gravin, Y. Peres, and B. Sivan. Towards optimal algorithms for prediction with expert advice. In
Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms, pages
528–547. SIAM, 2016.

O. Kallenberg. Foundations of modern probability. Springer-Verlag, 2002.

E. Kaufmann, N. Korda, and R. Munos. Thompson sampling: An asymptotically optimal ﬁnite-time
analysis. In NaderH. Bshouty, Gilles Stoltz, Nicolas Vayatis, and Thomas Zeugmann, editors,
Algorithmic Learning Theory, volume 7568 of Lecture Notes in Computer Science, pages 199–
213. Springer Berlin Heidelberg, 2012. ISBN 978-3-642-34105-2.

J. Komiyama, J. Honda, and H. Nakagawa. Regret lower bound and optimal algorithm in ﬁnite
stochastic partial monitoring.
In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and
R. Garnett, editors, Advances in Neural Information Processing Systems 28, NIPS, pages 1792–
1800. Curran Associates, Inc., 2015.

H.W. Kuhn. Extensive games and the problem of information, contributions to the theory of games

II. Annals of Mathematics Studies, 28:193–216, 1953.

T. Lattimore and Cs. Szepesv´ari. Cleaning up the neighbourhood: A full classiﬁcation for adversar-

ial partial monitoring. In International Conference on Algorithmic Learning Theory, 2019.

T. Lattimore and Cs. Szepesv´ari. Bandit Algorithms. Cambridge University Press (preprint), 2019.

D. Leao, J. B. R. do Val, and M. D. Fragoso. Nonstationary zero sum stochastic games with in-
In 39th IEEE Conference on Decision and Control, pages 2278–2283,

complete observation.
2000.

14

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

S. Mannor and N. Shimkin. On-line learning with imperfect monitoring. In Learning Theory and

Kernel Machines, pages 552–566. Springer, 2003.

S. Mannor, V. Perchet, and G. Stoltz. Set-valued approachability and online learning with partial

monitoring. The Journal of Machine Learning Research, 15(1):3247–3295, 2014.

V. Perchet. Approachability of convex sets in games with partial monitoring. Journal of Optimiza-

tion Theory and Applications, 149(3):665–677, 2011.

D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. In Z. Ghahra-
mani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural
Information Processing Systems 27, NIPS, pages 1583–1591. Curran Associates, Inc., 2014.

D. Russo and B. Van Roy. An information-theoretic analysis of Thompson sampling. Journal of

Machine Learning Research, 17(1):2442–2471, 2016. ISSN 1532-4435.

D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. Operations

Research, 66(1):230–252, 2017.

A. Rustichini. Minimizing regret: The general case. Games and Economic Behavior, 29(1):224–

M. Sion. On general minimax theorems. Paciﬁc Journal of mathematics, 8(1):171–176, 1958.

A. B. Tsybakov. Introduction to nonparametric estimation. Springer Science & Business Media,

243, 1999.

2008.

H. P. Vanchinathan, G. Bart´ok, and A. Krause. Efﬁcient partial monitoring with prior information. In
Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, NIPS, pages 1691–1699. Curran Associates, Inc.,
2014.

C-Y. Wei and H. Luo. More adaptive algorithms for adversarial bandits.

In S´ebastien Bubeck,
Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning
Theory, volume 75 of Proceedings of Machine Learning Research, pages 1263–1291. PMLR,
06–09 Jul 2018.

Appendix A. Proof of Theorem 1

The proof depends on a little functional analysis. The important point is that the space of policies
written as probability measures over deterministic policies is compact and the Bayesian regret is
linear and continuous as a function of the measures over policies and priors over environments.
Then minimax theorems can be used to exchange the min and sup. Guaranteeing compactness and
continuity and avoiding any kind of measurability issues requires careful choice of topologies.

For a topological space Z, let Pr(Z) be the space of Radon probability measures when Z is
equipped with the Borel σ-algebra. The weak* topology on Pr(Z) is the coarsest topology such
that µ (cid:55)→ (cid:82) f dµ is continuous for all bounded continuous functions f : Z → R.

15

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

Recall that X is the space of outcomes and Σ is the space of feedbacks and these are arbitrary
sets. A deterministic policy can be represented as a function π : ∪n
t=1Σt−1 → [k]. By the choice
of topology on Σ, these are all continuous, hence, measurable. Let ΠD be the space of all such
policies, ΠDM be the space of the measurable policies amongst these. By Tychonoff’s theorem,
ΠD is compact with the product topology, where [k] has the discrete topology. ΠD is Hausdorff
because the product of Hausdorff spaces is Hausdorff. By Theorem 8.9.3 in the two volume book
by Bogachev (2007), the space Pr(ΠD) is weak*-compact. Clearly Pr(ΠD) is also convex.

Let Q be the space of ﬁnitely supported probability measures on X n, which is a convex subset of
Pr(X n) where X n is taken to have the discrete topology. Equip with Q with the weak* topology.
If f = f (µ, ν) with f : Pr(ΠD) × Q → R is linear and continuous in both µ and ν individually.
Since Pr(ΠD) is compact, by Sion’s minimax theorem (Sion, 1958),1

min
µ∈Pr(ΠD)

sup
ν∈Q

f (µ, ν) = sup
ν∈Q

min
µ∈Pr(ΠD)

f (µ, ν) .

We are going to choose f to be the Bayesian regret and argue that Π can be identiﬁed with Pr(ΠD).
First, we need to check some continuity conditions for the regret. Since X n has the discrete topology
the map x (cid:55)→ Rn(π, x) is continuous for ﬁxed π. Now we check that π (cid:55)→ Rn(π, x), π ∈ ΠD, is
continuous for ﬁxed x. Let Φt(a) = Φ(a, xt) and Lt(a) = L(a, xt), which are both continuous
since [k] has the discrete topology. Then let σt : ΠD → Σ and at : ΠD → [k] be deﬁned inductively
by

at(π) = π(σ1(π), . . . , σt−1(π))

and σt(π) = Φt(at(π)) .

Writing the deﬁnition of the regret,

Rn(π, x) =

Lt(at(π)) − min
a∈[k]

Lt(a) .

n
(cid:88)

t=1

n
(cid:88)

t=1

The second term is constant and, as we mentioned already, a (cid:55)→ Lt(a) is continuous. So it remains
to check that at is continuous for each t. This follows by induction. The deﬁnition of the product
topology means that for any ﬁxed σ1, . . . , σt−1 and b ∈ [k], the set

Ub(σ1, . . . , σt−1) = {π : π(σ1, . . . , σt−1) = b}

is open in ΠD. Let (cid:15) denote the empty tuple. Then a−1
a2 is continuous and leave the rest to the reader. That a2 is continuous follows by writing

1 (b) = Ub((cid:15)) is open in ΠD. We conﬁrm that

a−1
2 (c) =

(Ub((cid:15)) ∩ Uc(Φ1(b))) .

k
(cid:91)

b=1

Hence π (cid:55)→ Rn(π, x) is continuous and also measurable with respect to the Borel σ-algebra on ΠD.
Then let f (µ, ν) be given by

f (µ, ν) =

Rn(π, x)dν(x)dµ(π) =

Rn(π, x)dµ(π)dν(x) ,

(cid:90)

(cid:90)

ΠD

X n

(cid:90)

(cid:90)

X n

ΠD

1. Sion’s theorem is more general, it only assumes that f is quasiconvex/quasiconcave in each argument and upper/lower

semicontinuous respectively.

16

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

ΠD

where the exchange of integrals is justiﬁed by Fubini’s theorem, which is applicable because the
regret is bounded in [−n, n]. Clearly f is linear in both arguments. We now claim that both
µ (cid:55)→ f (µ, ν) and ν (cid:55)→ f (µ, ν) are continuous. To see that ν (cid:55)→ f (µ, ν) is continuous, note
that x (cid:55)→ (cid:82)
dµ(π)Rn(π, x) is a X n → [−n, n] continuous map owning to the choice of the dis-
crete topology on X n. Since Q ⊂ Pr(X n) is equipped with the weak*-topology, this implies the
continuity of ν (cid:55)→ f (µ, ν). The argument for the continuity of µ (cid:55)→ f (µ, ν) is similar: In particular,
ﬁrst note that π (cid:55)→ (cid:82)
X n dν(x)Rn(π, x) is a ΠD → [−n, n] continuous map, since owning to the
choice of Q, the integral with respect to ν is a ﬁnite sum, and we have already established that
for x ∈ X n ﬁxed, π (cid:55)→ Rn(π, x) is a ΠD → [−n, n] continuous map. Again, the choice of the
weak*-topology on Pr(ΠD) implies the desired continuity.

The ﬁnal step is to note that for each policy µ ∈ Pr(ΠD) there exists a policy π ∈ Π such that

for all x ∈ X n,

Rn(π, x) =

Rn(πd, x)dµ(πd) .

(cid:90)

ΠD

In particular, it is not hard to show that π can be deﬁned through π(a1, φ(a1, x), . . . , at, φ(at, x))a =
Pµ,x(At+1 = a|A1 = a1, . . . , At = at), where Pµ,x is the distribution resulting from using µ on the
environment x. Here, the right-hand side is well deﬁned (as a completely regular measure) because
of the choice of A. That π is well deﬁned and is suitable follows from the deﬁnitions. Putting things
together,

R∗

n = inf
π∈Π

sup
x∈X n

Rn(π, x) ≤ min

µ∈Pr(ΠD)

Rn(π, x)dµ(π)

(a)
≤ min

µ∈Pr(ΠD)

sup
ν∈Q

f (µ, ν)

(cid:90)

ΠD

sup
x∈X n
(cid:90)

(b)
= sup
ν∈Q

min
µ∈Pr(ΠD)
(cid:90)

(d)
= sup
ν∈Q

min
π∈Π

X n

f (µ, ν)

(c)
= sup
ν∈Q

min
π∈ΠD

X n

Rn(π, x)dν(x)

Rn(π, x)dν(x)

(e)
= BR∗

n(Q) ,

where in (a) we used the fact that the Dirac measures are in Q, (b) follows from Sion’s theorem. In
(c) we used the fact that the Dirac measures in Pr(ΠD) are minimisers of f (·, ν) for any ν, in (d) we
used that ΠD ⊂ Π and, via a dynamic programming argument, that the deterministic policies from
ΠD minimise the Bayesian regret. For (e), let Pπν be the joint induced by π and ν over [k]n × X n,
Eπν the corresponding expectation and deﬁne rn(a, x) = (cid:80)n
t=1 L(b, xt).
Then, note that Pπν almost surely, Eπν[rn(A, X)|X] = Rn(π, X), and thus, by the tower rule and
because Pπν,X = ν by assumption, (cid:82)
n(Q) ≤ R∗
n
follows from

X n Rn(π, x)dν(x) = BRn(π, ν). That BR∗

t=1 L(at, xt)−minb∈[k]

(cid:80)n

Rn(π, x) = inf
π∈Π

sup
ν∈Q

Rn(π, x)dν(x) ≥ sup
ν∈Q

inf
π∈Π

X n

X n

Rn(π, x)dν(x)

(cid:90)

(cid:90)

R∗

n = inf
π∈Π
= BR∗

sup
x∈X n
n(Q) ,

where the second equality used that for any ﬁxed π ∈ Π, ν (cid:55)→ (cid:82)
X n Rn(π, x)dν(x) is a linear
functional on Pr(X n), which is thus maximised in the extreme points of Pr(X n), which are all
the Dirac measures over X n. Combining Eqs. (8) and (9) gives the desired result.

(8)

(9)

17

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

Appendix B. Proof of Theorem 7

Using the fact that the total variation distance is upper bounded by the Hellinger distance (Tsybakov,
2008, Lemma 2.3) and the ﬁrst inequality in Eq. (1),

Et[∆t] =

ta (Et[Xta] − Et[Xta | A∗ = a])
P ∗

(cid:88)

a:P ∗

ta>0

(cid:88)

≤

≤

ta>0

a:P ∗
(cid:118)
(cid:117)
(cid:117)
(cid:116)k1/2 (cid:88)
(cid:117)

a:P ∗

ta>0

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:90)

P ∗
ta

(cid:32)

(cid:115)

1 −

(cid:33)2

dPt,Xta|A∗=a
dPt,Xta

[0,1]

dPt,Xta

(10)

(cid:90)

(P ∗

ta)3/2

(cid:32)

(cid:115)

1 −

(cid:33)2

dPt,Xta|A∗=a
dPt,Xta

[0,1]

dPt,Xta .

(11)

Eq. (10) is true because the total variation distance is upper bounded by the Hellinger distance.
Eq. (11) uses Cauchy-Schwarz and the fact that (cid:80)k
ta)1/2 ≤ k1/2, which also follows from
Cauchy-Schwarz. The next step is to apply Bayes law to the square root term. There are no measur-
ability problems because both Xta and A∗ live in Polish spaces (Ghosal and van der Vaart, 2017).

a=1(P ∗

(cid:33)2

(cid:90)

dPt,Xta =

(cid:32)

(cid:115)

1 −

Pt(A∗ = a | Xta)(x)
Pt(A∗ = a)

(cid:33)2

dPt,Xta(x)

(cid:32)

(cid:115)

1 −

(cid:90)

[0,1]

dPt,Xta|A∗=a
dPt,Xta

(cid:32)

(cid:115)

= Et



1 −

[0,1]
(cid:33)2


Pt(A∗ = a | Xta)
Pt(A∗ = a)


=

1
(cid:112)Pt(A∗ = a)

Et




(cid:16)(cid:112)Pt(A∗ = a) − (cid:112)Pt(A∗ = a | Xta)
(cid:112)Pt(A∗ = a)

(cid:17)2




 .

Substituting the above into Eq. (11) and using the fact that Pta = P ∗

ta = Pt(A∗ = a) yields

Et[∆t] ≤

k1/2 (cid:88)
a:Pta>0

PtaEt

(cid:16)(cid:112)Pt(A∗ = a) − (cid:112)Pt(A∗ = a | Xta)
(cid:112)Pt(A∗ = a)

(cid:17)2






≤

k1/2 (cid:88)
a:Pta>0

PtaEt

(cid:88)

c:P ∗

tc>0

(cid:16)(cid:112)Pt(A∗ = c) − (cid:112)Pt(A∗ = c | Xta)
(cid:112)Pt(A∗ = c)

(cid:17)2




 ,

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:117)
(cid:116)











where the second inequality follows by introducing the sum over c. Finally, note that

The result follows from a direct computation using the independence of At and Xt under Pt
(Lemma 26).

DF (p, q) =

(cid:0)√

√

(cid:1)2

qc

.

pc −
√

qc

(cid:88)

c:pc(cid:54)=qc

18

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

Appendix C. Cops and robbers

To further demonstrate the ﬂexibility of the approach we consider this special case of bandits with
graph feedback. In cops and robbers the learner observes the losses associated with all actions ex-
cept the played action. Except for constant factors, this problem is no harder than the full informa-
tion setting where all losses are observed. Cops and robbers is formalised in the partial monitoring
framework by choosing Σ = [0, 1]k−1, X = [0, 1]k, L(a, x) = xa and

Φ(a, x) = (x1, . . . , xa−1, xa+1, . . . , xk) .

Theorem 17 The minimax regret of cops and robbers satisﬁes R∗

n ≤ (cid:112)2n log(k).

This improves on the result by Alon et al. (2015) that R∗

n ≤ 5(cid:112)n log(k). We leave for the
future the interesting question of whether or not this method recovers other known results for bandits
with graph feedback. Theorem 17 follows immediately from Theorems 2 and 1, and the following
lemma.

Lemma 18 Thompson sampling for cops and robbers satisﬁes Et[∆t] ≤ (cid:112)2It(A∗; Φt(At), At)
almost surely for all t.

Proof Fix t ∈ [n] and let Gt = arg maxa Pta. Here, we assume that we have already discarded
a suitable set of measure zero, so that we do not need to keep repeating the qualiﬁcation ‘almost
surely’. Then, subtracting and adding Lt(Gt), expanding the deﬁnitions and using that P ∗

t = Pt,

Et[∆t] =

PtaEt[Lt(a) − Lt(Gt)] +

PtaEt[Lt(Gt) − Lt(a) | A∗ = a]

(cid:88)

a(cid:54)=Gt

D (cid:0)Pt,Lt(Gt)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Lt(Gt))(cid:1) +
(cid:12)

D (cid:0)Pt,Lt(a)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Lt(a)
(cid:12)

(cid:1)

(cid:33)

(cid:114) 1
2

(cid:88)

a(cid:54)=Gt

(cid:88)

≤

Pta

(cid:32)(cid:114) 1
2

a(cid:54)=Gt
(cid:112)

≤

(A) +

(B) ,

(cid:112)

where the ﬁrst inequality follows from grouping the terms that involve Lt(Gt) and those that involve
Lt(a) and then using Pinsker’s inequality (1), while the second follows from Cauchy-Schwarz and
the deﬁnitions,

(A) =

(B) =

1 − PtGt
2

1 − PtGt
2

(cid:88)

a(cid:54)=Gt
(cid:88)

a(cid:54)=Gt

Pta D (cid:0)Pt,Lt(Gt)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Lt(Gt)
(cid:12)

(cid:1) ,

Pta D (cid:0)Pt,Lt(a)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Lt(a)
(cid:12)

(cid:1) .

19

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

The result is completed by bounding each term separately. Using that 1 − PtGt = (cid:80)

Ptb,

b(cid:54)=Gt

where the ﬁrst inequality follows from the data processing inequality (for b (cid:54)= Gt, Lt(Gt) is a
deterministic function of Φt(b)) and the last from Lemma 26. The second term is bounded in almost
the same way. Here we use the fact that 1 − PtGt ≤ 1 − Pta for all a ∈ [k]:

(A) =

1 − PtGt
2

Pta D (cid:0)Pt,Lt(Gt)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Lt(Gt)
(cid:12)

(cid:1)

(cid:88)

a(cid:54)=Gt
(cid:88)

b(cid:54)=Gt
(cid:88)

=

≤

(cid:88)

a(cid:54)=Gt
(cid:88)

Pta

Pta

a(cid:54)=Gt

b(cid:54)=Gt

≤

It(A∗; Φt(At), At) ,

1
2

1
2

1
2

Ptb D (cid:0)Pt,Lt(Gt)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Lt(Gt)
(cid:12)

(cid:1)

Ptb D (cid:0)Pt,Φt(b)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(b)
(cid:12)

(cid:1)

(B) =

1 − PtGt
2

(cid:88)

a(cid:54)=Gt

Pta D (cid:0)Pt,Lt(a)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Lt(a)
(cid:12)

(cid:1)

(1 − Pta)Pta D (cid:0)Pt,Lt(a)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Lt(a)
(cid:12)

(cid:1)

(cid:88)

a(cid:54)=Gt
(cid:88)

a(cid:54)=Gt
(cid:88)

a(cid:54)=Gt

≤

=

≤

1
2

1
2

1
2

1
2

Pta

Pta

(cid:88)

b(cid:54)=a
(cid:88)

b(cid:54)=a

Ptb D (cid:0)Pt,Lt(a)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Lt(a)
(cid:12)

(cid:1)

Ptb D (cid:0)Pt,Φt(b)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(b)
(cid:12)

(cid:1)

≤

It(A∗; Φt(At), At) .

Combining the previous displays and rearranging completes the proof.

Appendix D. Proof of Theorem 11

We need the following lemma, which characterises actions c ∈ Nab as having loss vectors (cid:96)c that
are convex combinations of (cid:96)a and (cid:96)b.

Lemma 19 (Bart´ok et al. 2014) For all actions c ∈ Nab there exists an α ∈ [0, 1] such that (cid:96)c =
α(cid:96)a + (1 − α)(cid:96)b.

Proof [Theorem 11] In order to deﬁne the algorithm we ﬁrst choose a subset C ⊆ [k] such that C
contains no duplicate or degenerate actions and ∪c∈CCc = ∆k−1. We assume additionally that P ∗
t
is constant on duplicate actions. Construct the parent function Pt on actions in C in the same way
as Mario sampling. For a (cid:54)= b let Tab = (c1, . . . , cm) be an ordering of

{c ∈ ([k] \ C) ∪ {b} : c = b or exists α ∈ (0, 1] with (cid:96)c = α(cid:96)a + (1 − α)(cid:96)b}

20

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

ordered by decreasing α values and with cm = b. In other words Tab is a sequence of actions starting
with duplicates of a, then actions c for which (cid:96)c is a strict convex combination of (cid:96)a and (cid:96)b, with
actions that are ‘closer’ to a sorted ﬁrst. The last element of Tab is b itself. Duplicates of b are not
included in Tab. Let Taa be the duplicates of a, excluding a, in an arbitrary order. Then deﬁne

P (cid:48)

t(c) =






TcP(c)[1]
Tcc[1]
Tab[i + 1]

if c ∈ C \ {Gt}
if c = Gt and Tcc (cid:54)= ∅
if c = Tab[i] .

t )kP ∗

Let W (cid:48)
t be the water transfer operator using the tree generated by P (cid:48)
t instead of Pt and Pt =
(W (cid:48)
t . Now we follow the proof of Theorem 10. Let t ∈ [n] be ﬁxed. We start by bound-
ing Et[∆t] in terms of the expected information gain. Given b ∈ C \ {Gt} let fb : NaPt(a) → R be
a function with

fb(c, Φt(c)) = Lt(b) − Lt(Pt(b)) ,

which exists by the deﬁnition of local observability. By deﬁnition we may assume that (cid:107)fb(cid:107)∞ ≤ v.
Then

Et[Lt(a)] = Et

Lt(Gt) +

(cid:88)

(cid:88)

b∈At(a)\{Gt}

c∈NbPt(b)



fb(c, Φt(c))

 .

Therefore

Et[∆t] ≤ v

k
(cid:88)

P ∗
ta

(cid:88)

(cid:88)

(cid:113)

2 D (cid:0)Pt,Φt(c)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(c)
(cid:12)

(cid:1)

b∈At(a)\{Gt}

c∈NbPt(b)

(cid:88)

c∈NbPt(b)



a=1
(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)4

≤ vk

k
(cid:88)

a=1

P ∗
ta

(cid:118)
(cid:117)
(cid:117)
(cid:117)
(cid:116)4

(cid:118)
(cid:117)
(cid:117)
(cid:116)8

k
(cid:88)

a=1

k
(cid:88)

(cid:88)

(cid:88)

ta D (cid:0)Pt,Φt(c)|A∗=a
P ∗

(cid:12)
(cid:12)
(cid:12) Pt,Φt(c)
(cid:12)

(cid:1)

b∈At(a)\{Gt}

c∈NbPt(b)

≤ vk3/2

P ∗
ta

(cid:88)

(cid:88)

Ptc D (cid:0)Pt,Φt(c)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(c)
(cid:12)

(cid:1)

b∈At(a)\{Gt}

c∈NbPt(b)

≤ vk3/2

k
(cid:88)

P ∗
ta

a=1

c=1

= vk3/2(cid:112)8It(A∗; Φt(At), At) .

Ptc D (cid:0)Pt,Φt(c)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(c)
(cid:12)

(cid:1)

And the result follows from Theorem 2 and Theorem 1.

Appendix E. Proof of Theorem 12

Again Thompson sampling does not explore sufﬁciently often. The most straightforward correction
is to simply add a small amount of forced exploration, which was also used in combination with

21

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

Exp3 in prior analysis of these games (Cesa-Bianchi et al., 2006). We let

Pt = (1 − γ)P ∗

t + γ1/k ,

(12)

where ties in the arg max that deﬁnes P ∗
means that P ∗
1-step regret in terms of the information gain.

t are broken by prioritising Pareto optimal actions, which
ta = 0 for all degenerate actions. As usual, the crucial step is to bound the expected

Lemma 20 For the policy playing according to Eq. (12) it holds almost surely that

Et[∆t] ≤ γ + kv

(cid:115)

2It(A∗; Φt(At), At)
γ

.

Proof Let a◦ be an arbitrary ﬁxed Pareto optimal action and for each Pareto optimal action a let
fa : [k] × Σ → R be a function with (cid:107)fa(cid:107)∞ ≤ v such that

fa(c, Φ(c, x)) = L(a, x) − L(a◦, x)

for all x ∈ [d] .

The next step is to decompose the expected loss in terms of f :

Et[∆t] =

PtaEt[Lt(a)] −

P ∗
ta

Et[Lt(a) | A∗ = a]

k
(cid:88)

a=1

≤ γ +

ta (Et[Lt(a)] − Et[Lt(a) | A∗ = a])
P ∗

= γ +

ta (Et[Lt(a) − Lt(a◦)] − Et[Lt(a) − Lt(a◦) | A∗ = a])
P ∗

= γ +

fa(c, Φt(c))

− Et

fa(c, Φt(c))

A∗ = a

,

(cid:32)

P ∗
ta

Et

(cid:34) k

(cid:88)

c=1

(cid:35)

(cid:34) k

(cid:88)

c=1

(cid:35)(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where the inequality follows from the deﬁnition of Pt and the fact that losses are bounded in [0, 1].
Then, by Pinsker’s inequality (1),

Et[∆t] ≤ γ + v

2 D (cid:0)Pt,Φt(c)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(c)
(cid:12)

(cid:1)

k
(cid:88)

c=1

k
(cid:88)

a=1

k
(cid:88)

a=1
k
(cid:88)

a=1
k
(cid:88)

a=1

k
(cid:88)

k
(cid:88)

(cid:113)

P ∗
ta

c=1
(cid:118)
(cid:117)
(cid:117)
(cid:116)2k

a=1

k
(cid:88)

k
(cid:88)

P ∗
ta

a=1

c=1

2
γ

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:115)

k
(cid:88)

k
(cid:88)

P ∗
ta

a=1

c=1

≤ γ + v

≤ γ + kv

= γ + kv

2It(A∗; Φt(At), At)
γ

,

22

D (cid:0)Pt,Φt(c)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(c)
(cid:12)

(cid:1)

Ptc D (cid:0)Pt,Φt(c)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(c)
(cid:12)

(cid:1)

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

where the ﬁrst inequality follows from Pinsker’s inequality (1), the second from Cauchy-Schwarz,
the third because 1 ≤ kPtc/γ for all c. The last term follows from Lemma 26.

Proof [Theorem 12] By the previous lemma and Corollary 4,

BRn ≤ nγ + kv

≤ 3(nkv)2/3(log(k)/2)1/3 ,

(cid:115)

2n log(k)
γ

where we choose γ = n−1/3(kv)2/3(log(k)/2)1/3 and note that when γ > 1 the claim in the
theorem is immediate.

Appendix F. The water transfer operator

Here we explain in more detail the water transfer operator deﬁned by Algorithm 2 and provide the
proof of Lemma 14. An example with k = 6 is illustrated below.

P

WtP

W 2

t P

a

b

c

d

e = Gt

f

Figure 1: Water transfer process

The mugs correspond to actions and are connected at the bottom with valves that default to
being closed. The total volume of water sums to 1. Arrows correspond to edges in the tree. The
dark arrows indicate which valves are open in each iteration and show the direction of ﬂow. In the
ﬁrst application of Wt, mug c is anomalous and the water in mugs b and c is averaged. Imagine
opening the valve connecting b and c. The water in a is too low to be included in the average. In the
second application, the water in mugs b, c, d and e is averaged. Further applications of Wt have no
effect because there are no anomalous actions.

Remark 21 Another way to think about the application of Wt to P is as follows. First the anoma-
lous action a is identiﬁed, if it exists. Then water ﬂows continuously into a from the set of descen-
dants of a that contain more water than a until a is no longer anomalous.

23

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

Proof [Lemma 14] To begin, notice that every application of the water transfer operator reduces
the number of anomalous actions by at least one because: (1) If a is selected by Algorithm 2 then
a is not anomalous in WtP and (2) only actions that were anomalous in P can be anomalous in
WtP . Since there are at most k anomalous actions in any P , the water transfer operator ceases
to have any affect after more than k operations. Hence Qa ≥ maxb∈Dt(a) Qb for all a and the
second part follows. For the ﬁrst part we show that the loss of WtP is always smaller than P . Let
¯L(b) = Et[Lt(b)] for b ∈ [k] and a ∈ [k] be the anomalous action in P selected by the algorithm.
Then let C = {b ∈ [k] : (WtP )b (cid:54)= Pb} be the set of actions for which the distribution is changed.
By the deﬁnition of the tree, ¯L(b) ≥ ¯L(a) for any b ∈ C,

k
(cid:88)

b=1

(Pb − (WtP )b) ¯L(b) = (Pa − (WtP )a) ¯L(a) +

(Pb − (WtP )b) ¯L(b)

(cid:88)

b∈C,b(cid:54)=a

(cid:88)

=

b∈C,b(cid:54)=a

(Pb − (WtP )b)( ¯L(b) − ¯L(a)) ≥ 0 ,

which shows that Wt decreases the expected loss. For the last part, notice that during each iteration
of the water transfer operator the update occurs by averaging the contents of a number of mugs so
that all have the same level (Fig. 1). Once a group of mugs have been averaged together, subse-
quently they are always averaged together. It follows that after every iteration the actions [k] can
be partitioned so that the level in each partition is the average of Pa. Suppose that a is in partition
S ⊆ [k]. Then Qa = 1
|S|

b∈S Pb ≥ Pa/k.

(cid:80)

Appendix G. Failure of Thompson sampling for partial monitoring

The following example with k = 3 and d = 2 illustrates the failure of Thompson sampling for
locally observable non-degenerate partial monitoring games. The game is a toy ‘spam ﬁltering’
problem where the learner can either classify an email as spam/not spam or pay a small cost for the
true label. The functions Φ and L are represented by the tables below, with the learner choosing the
rows and adversary the columns.

Losses L

NOT SPAM SPAM

Signals Φ

NOT SPAM SPAM

SPAM

NOT SPAM

UNKNOWN

1

0

c

0

1

c

SPAM

NOT SPAM

⊥

⊥

⊥

⊥

UNKNOWN

NOT SPAM SPAM

Figure 2: The ‘spam’ partial monitoring game. For c < 1/2 the game is locally observable and
non-degenerate. For c = 1/2 the game is locally observable, but degenerate. For c > 1/2
the game is not locally observable, but is globally observable. For c = 0 the game is
trivial.

The learner only elicits meaningful feedback in the spam game by paying a cost of c to observe
the true label. For appropriately chosen c and prior, we will see that Thompson sampling never

24

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

2 δn

2 δn

SPAM + 1

NOT SPAM, where δn
i

chooses the revealing action, cannot learn, and hence suffers linear regret. Let c > 0 and ν be the
mixture of two Dirac’s: ν = 1
is the Dirac measure on (i, i, . . . , i).
With these choices the optimal action is almost surely either SPAM or NOT SPAM. Since choosing
these actions does not reveal any information, the posterior is equal to the prior and Thompson
sampling plays these two actions uniformly at random. Clearly this leads to linear regret relative
to the optimal policy that plays the exploratory action once to identify the adversary and plays
optimally for the remainder. Since this result holds for any strictly positive cost, it also shows that
Thompson sampling does not work for globally observable games.

Appendix H. Structural lemmas for partial monitoring

Lemma 22 Let a, b ∈ [k] be distinct actions in a non-degenerate game and u ∈ Ca. Then there
exists an action c ∈ Nb\{b} such that (cid:104)(cid:96)b−(cid:96)c, u(cid:105) ≥ 0. Furthermore, if u /∈ Cb, then (cid:104)(cid:96)b−(cid:96)c, u(cid:105) > 0.

Proof Let w be a point in the relative interior of Cb, which means that (cid:104)(cid:96)b, w(cid:105) < minc(cid:54)=b(cid:104)(cid:96)c, w(cid:105).
Now let c ∈ Nb \ {b} be an action such that v = u + α(w − u) ∈ Cb ∩ Cc for some α ∈ [0, 1),
which exist because Cb is closed convex set and hence {u + α(w − u) : α ∈ R} ∩ Cb, which is
nonempty, must be a closed segment. Let f (x) = (cid:104)(cid:96)b − (cid:96)c, u + x(w − u)(cid:105). By deﬁnition, f (α) = 0
and f (1) < 0. Since f is linear it follows that f (0) = (cid:104)(cid:96)b − (cid:96)c, u(cid:105) ≥ 0. The second part follows
because if u /∈ Cb, then α > 0, which means that f (0) > f (α) = 0.

w

Cb

Ca

u

Cc

v

w

Cb

u

Ca

Cc

Figure 3: Illustration for the proof of Lemma 22. The bottom left region is Ca and u ∈ Ca so that
a minimises Ex∼u[L(a, x)]. The lemma proves that for the situation in the left ﬁgure:
Ex∼u[L(c, x)] < Ex∼u[L(b, x)]. The strict inequality is replaced by an equality if u ∈
Ca ∩ Cb as in the right ﬁgure, when u = v.

Lemma 23 Consider a non-degenerate game and let u ∈ ∆k−1 and V = {a : u ∈ Ca} and
E = {(a, b) ∈ V : a and b are neighbours}. Then the graph (V, E) is connected.

Proof This must be a known result about the facet graph of convex polytopes. We give a dimension
argument. You may ﬁnd Fig. 4 useful. Let Bε(x) = {y ∈ ∆d−1 : (cid:107)y − x(cid:107)2 ≤ ε}, Hd be the
d-dimensional Hausdorff measure and ri be the relative interior operator. Since the cells are closed,
there exists an ε > 0 such that Bε(u) ∩ Cc = ∅ for all c /∈ V . Then let a∗(v) = {a ∈ [k] : v ∈ Ca}
be the set of actions that are optimal at v ∈ ∆d−1. It is easy to see that if a∗(v) = {a, b} for

25

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

some v ∈ ∆d−1, then a and b are neighbours. Let N = {v ∈ ∆d−1 : |a∗(v)| > 2}, which
by the assumption that there are no duplicate/degenerate actions has dimension at most d − 3 and
hence Hd−2(N ) = 0. Let a, b ∈ V be distinct and v, w ∈ Bε(u) be such that Bδ(v) ⊂ Ca and
Bδ(w) ⊂ Cb for some δ > 0, which by deﬁnition means that the interval [v, w] ∩ Cc = ∅ for all
c /∈ V . Let A be the afﬁne space containing v with normal v − w and P = {arg minx∈A (cid:107)x − y(cid:107)2 :
y ∈ N } be the projection of N onto A. Since projection onto a plane cannot increase the Hausdorff
measure, Hd−2(P ) = 0. On the other hand, the fact that A ∩ Bδ(v) has dimension d − 2 means
that Hd−2(A ∩ Bδ(v)) > 0. Therefore Hd−2(Bδ(v) ∩ (A \ P )) > 0 and hence there exists an
x ∈ Bδ(v) ∩ A and y = x + w − v ∈ Bδ(w) such that [x, y] ∩ N = ∅. Then the set ∪z∈[x,y]a∗(u)
forms a connected path in V between a and b.

A

x
v

Bδ(v)

u

y
w

Bδ(w)

Figure 4: Illustration for the proof of Lemma 23 when d = 3. The whole region shown is a subset
of Bε(u). The set N in this case consists only of u, which has 1-dimensional Hausdorff
measure zero. The cells crossed by the interval [x, y] form the path between a and b in V .

Proof [Lemma 13] By deﬁnition there are no edges starting from Gt. By Lemma 22, for all a /∈ Vt
there is a neighbour b ∈ Na with strictly smaller loss, Et[Lt(b)] < Et[Lt(a)]. Hence the deﬁnition
of Pt(a) ensures there are no cycles and that every path starting from a /∈ Vt eventually leads to Vt.
Then by Lemma 22 the graph (Vt, Et) is connected, which means that for a ∈ Vt the parent Pt(a)
is a vertex b ∈ Vt that is closest to Gt. Hence all paths lead to Gt.

The next two lemmas bound on the supremum norms of the estimation functions. The ﬁrst is
restricted to the non-degenerate case where the result was already known and the second holds for
all globally observable games.

Lemma 24 (Lattimore and Szepesv´ari (2019), Lemma 9) For locally observable non-degenerate
games, the function f in Eq. (4) can be chosen so that (cid:107)f (cid:107)∞ ≤ d + 1.

Lemma 25 If (Φ, L) is globally observable, then for each pair of neighbours a and b there exists a
function f satisfying Eq. (4) such that (cid:107)f (cid:107)∞ ≤ d1/2(1 + k)d/2. If (Φ, L) is also locally observable,
then f can be chosen so that f (c, σ) = 0 for all c /∈ Nab.

Proof We prove only the ﬁrst part. The proof for locally observable games is the same, but the signal
matrices deﬁned below are restricted to c ∈ Nab. Assume without loss of generality that Σ = [d]
and d ≥ 2. For c ∈ [k] deﬁne Sc ∈ {0, 1}d×d to be the matrix with (Sc)σx = 1 if Φ(c, x) = σ.

26

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

Then let S ∈ {0, 1}d×dk be formed by horizontally stacking the matrices {Sc : c ∈ [k]}. By the
deﬁnition of local observability it holds that (cid:96)a −(cid:96)b ∈ im(S). Let S+ be the Moore-Penrose pseudo-
inverse of S and let w = S+((cid:96)a − (cid:96)b), which satisﬁes Sw = (cid:96)a − (cid:96)b. Then f can be chosen so that
(cid:107)f (cid:107)∞ = (cid:107)w(cid:107)∞ ≤ (cid:107)w(cid:107)2. Since losses are bounded in [0, 1] we have (cid:107)w(cid:107)2 ≤ (cid:107)S+(cid:107)2 (cid:107)(cid:96)a − (cid:96)b(cid:107)2 ≤
d1/2σ−1
min, where σmin is the smallest nonzero singular value of S. Hence we need to lower bound
the smallest nonzero eigenvalue of B = SS(cid:62), which is a d × d matrix with entries in {0, 1, . . . , k}.
The characteristic polynomial of B is χ(λ) = det(λI − B) = (cid:80)d
i=0 aiλd, where ad = 1 and, up to
a sign, ai is the sum principle minors of B of size d − i. Since the geometric mean is smaller than
the arithmetic mean, for matrix A ∈ [0, k]i×i it holds that det(A) ≤ (tr(A)/i)i ≤ ki. Hence,

By the binomial theorem,

|ad−i| ≤

(cid:19)

(cid:18)d
i

ki .

d
(cid:88)

i=0

|ai| ≤

d
(cid:88)

i=0

(cid:19)

(cid:18)d
i

ki = (1 + k)d .

Let imin = min{i : ai (cid:54)= 0} and suppose that λ > 0 is the smallest nonzero root of χ, which must
be positive. Then

0 = |χ(λ)| =

aiλi

= λimin

aiλi−imin

≥ λimin

d
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i=imin

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

d
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i=imin

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)1 − (1 + k)dλ
(cid:12)

(cid:12)
(cid:12)
(cid:12) ,

where we used the fact that (ai) are integer-valued. Therefore λ ≥ (1 + k)−d, which means that
(cid:107)S+(cid:107)2 ≤ (1 + k)d/2 and hence (cid:107)f (cid:107)∞ ≤ d1/2(1 + k)d/2.

Appendix I. Figures and examples

Finite partial monitoring example Below is a 4-action ﬁnite-outcome, ﬁnite-action partial mon-
itoring game with feedback set Σ = {⊥,
, }. The left table shows the loss function and the
right shows the signal function. By staying indoors you cannot evaluate the quality of the snow, but
climbing or skiing in poor conditions is no fun.

,

Losses L

SUN

SNOW

RAIN

Signals Φ

SUN

SNOW

RAIN

SKI

CLIMB

MATH

3/4

0

1

1/2

0

3/4

1/2

1

1

1

0

1/4

SKI

CLIMB

MATH

RAINDANCE

RAINDANCE

⊥

⊥

⊥

Figure 5: Example ﬁnite partial monitoring game

The following ﬁgure shows the cell decomposition for the above game, ∆d−1 is parameterised
by (p, q, 1 − p − q). In this game all actions a Pareto optimal. All actions are neighbours of MATH
and otherwise CLIMB and SKI are neighbours and MATH and RAINDANCE. The game is locally

27

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

observable because the loss of all actions can be identiﬁed by playing that action, except for MATH,
the losses of which can be identiﬁed by playing any of its neighbours.

Figure 6: Cell decomposition for the game described above where d = 3. The ﬁgure shows ∆d−1
projected onto the plane by the parameterisation (p, q, 1 − p − q). All actions are Pareto
optimal, so their cells have dimension d − 1 = 2. The intersection of the cells of neigh-
bouring actions are the lines shared by the cells, which have dimension 1.

Tree construction The ﬁgure depicts the cell decomposition for a partial monitoring game with
seven actions and the tree structure deﬁned in Lemma 13. Arrows indicate the parent relationship.
All paths leading towards Gt. Red nodes are descendants of a. Blue nodes are ancestors. Dotted
lines indicate connections in the neighbourhood graph that are not part of the tree.

CLIMB

1

p

MATH

SKI

0

RAINDANCE

q

1

a

Gt

Figure 7: Tree construction

28

AN INFORMATION-THEORETIC APPROACH TO MINIMAX REGRET IN PARTIAL MONITORING

Appendix J. Technical calculation

Lemma 26 Let Pta = Pt(At = a). Then the following hold almost surely:

Et[DF (P ∗

t+1, P ∗

t )] =

PtaEt

(cid:2)DF (Pt,A∗|Φt(a), Pt,A∗)(cid:3) ,

It(A∗; Φt(At), At) =

Ptb Et

(cid:2)D (cid:0)Pt,Φt(b)|A∗=a

(cid:12)
(cid:12)
(cid:12) Pt,Φt(b)
(cid:12)

(cid:1)(cid:3) .

k
(cid:88)

a=1
k
(cid:88)

a=1

P ∗
ta

k
(cid:88)

b=1

Proof Recall that P ∗

t+1 = Pt+1(A∗ ∈ ·) = Pt(A∗ ∈ · | At, Φt(At)). Then

Et[DF (Pt,A∗|At,Φt(At), Pt,A∗)] = Et
= Et
k
(cid:88)

(cid:2)Et[DF (Pt,A∗|At,Φt(At), Pt,A∗) | At](cid:3)
(cid:2)Et[DF (Pt,A∗|Φt(At), Pt,A∗) | At](cid:3)

PtaEt[DF (Pt,A∗|Φt(a), Pt,A∗) | At = a]

=

=

a=1
k
(cid:88)

a=1

PtaEt[DF (Pt,A∗|Φt(a), Pt,A∗)] ,

where in the second and fourth inequalities we used the independence of At and X under Pt. The
second part of the lemma follows from an identical argument.

29

