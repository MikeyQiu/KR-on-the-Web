7
1
0
2
 
l
u
J
 
2
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
6
9
7
0
.
6
0
7
1
:
v
i
X
r
a

Encoding Video and Label Priors for Multi-label Video Classiﬁcation
on YouTube-8M dataset

Seil Na
Seoul National University
seil.na@vision.snu.ac.kr

YoungJae Yu
Seoul National University
yj.yu@vision.snu.ac.kr

Sangho Lee
Seoul National University
sangho.lee@vision.snu.ac.kr

Jisung Kim
SK Telecom Video Tech. Lab
joyful.kim@sk.com

Gunhee Kim
Seoul National University
gunhee@snu.ac.kr

Abstract

YouTube-8M is the largest video dataset for multi-label
video classiﬁcation. In order to tackle the multi-label classi-
ﬁcation on this challenging dataset, it is necessary to solve
several issues such as temporal modeling of videos, label
imbalances, and correlations between labels. We develop
a deep neural network model, which consists of four com-
ponents: Video Pooling Layer, Classiﬁcation Layer, Label
Processing Layer, and Loss Function. We introduce our
newly proposed methods and discusses how existing mod-
els operate in the YouTube-8M Classiﬁcation Task, what in-
sights they have, and why they succeed (or fail) to achieve
good performance. Most of the models we proposed are
very high compared to the baseline models, and the ensem-
ble of the models we used is 8th in the Kaggle Competition.

1. Introduction

Many challenging problems have been studied in com-
puter vision research toward video understanding, such as
video classiﬁcation [9, 13], video captioning [19, 20], video
QA [21], and MovieQA [16], to name a few. YouTube-
8M [1] is the largest video dataset for multi-label video
classiﬁcation. Its main problem is to predict the most rele-
vant labels for a given video out of 4,716 predeﬁned classes.
Therefore, it requires jointly solving two important prob-
lems; video classiﬁcation and multi-label classiﬁcation.

From the view of the video classiﬁcation, YouTube-8M
is challenging in that it covers more general classes like soc-
cer, game, vehicle, and food, while existing video classiﬁ-
cation datasets focus on more speciﬁc class groups, such
as sports in Sports-1M [9], and actions in UCF-101 [13].
Therefore, unlike the importance of modeling motion fea-

tures in UCF-101 [13] or Sports-1M [9], it is important
to capture more generic video information(e.g. temporal
encoding method for video, audio feature modeling) in
YouTube-8M.

From the view of multi-label classiﬁcation, the key is-
sues to solve in YouTube-8M are label imbalances and cor-
relations between labels. YouTube-8M involves 4,716 class
labels, and the number of videos belonging to each class is
signiﬁcantly different, which causes a label imbalance is-
sue that the classiﬁer ﬁts to the biased data. At the same
time, many classes are closely related one another, such
as {Football, Kick, Penalty kick, Indoor soccer} or {Super
Mario Bros, Super Mario World, Super Mario bros 3, Mario
Kart, Mario Kart 8}. It is also challenging to resolve the
correlations between labels to decide ﬁnal prediction.

Based on the challenges of the multi-label video classiﬁ-
cation task on YouTube-8M described the above, we focus
on addressing i) temporal encoding for video, ii) relieving
the label imbalance problem, and iii) utilizing the correlated
label information. Our model consists of four components:
i) video pooling layer, ii) classiﬁcation layer, iii) label pro-
cessing layer, and iv) loss function. The proposed com-
ponents indeed show signiﬁcant performance improvement
over the baseline models of YouTube-8M [1], and ﬁnally
our ensemble model is ranked 8th in the Google Cloud &
YouTube-8M Video Understanding Challenge 1.

2. The Model

Figure 1 shows the overall pipeline of our model. We
ﬁrst present video features we used, and then explain its
four key components in the following sections.

1https://www.kaggle.com/c/youtube8m/

leaderboard. (Team name: SNUVL X SKT)

1

2.1. Video Features

The inputs of the model are frame features and audio
features of a video clip. The frame features are obtained by
sampling a clip at 1-second interval, and extracting 2,048-
dimensional vector from every frame through Inception-V3
Network [15] pretrained to ImageNet [4]. Then the feature
is reduced to an 1,024-dimension via PCA (+ whitening),
then quantized, and ﬁnally L2-normalized. As a result, for
a video of T seconds long, its frame is If ∈ RT ×1,024. The
audio features re extracted using the VGG-inspired acoustic
model [6] followed by L2-normalization, which is denoted
by Ia ∈ RT ×128.

As the input of our model, we concatenate the frame fea-
ture If and the audio feature Ia at every time step, denoted
by I ∈ RT ×1,152. We test the compact bilinear pooling [5]
with various dimensions between If and Ia, but all of them
have signiﬁcantly lower performance than the simple con-
catenation. From now on, we use I to denote the input fea-
tures of a video over all frames, and It ∈ R1,152 as t-th
frame vector.

2.2. Video Pooling Layer

The Video Pooling Layer gθ : RT ×1,152 → Rd is de-
ﬁned as a parametric function that encodes a sequence of
T feature vectors I into a d-dimensional embedding vector.
We test ﬁve different encoding structures as follows.

2.2.1 A Variant of LSTM

The Long Short Term Memory (LSTM) model [7] is one of
the most popular frameworks for modeling sequence data.
We use a variant of the LSTM as follows:

it = σ(ItUi + st−1Wi + bi)
ft = σ(ItU f + st−1Wf + bf )
ot = σ(ItU o + st−1Wo + bo)
gt = tanh(ItUg + st−1Wg + bg)
ct = ct−1 ◦ ft + gt ◦ it
st = tanh(ct) ◦ ot

(1)

(2)

(3)

(4)

(5)

(6)

where t denotes each time step, i, f , o are the input, forget,
output gate, ct, st are long-term and short-term memory re-
spectively.

The baseline model uses only the ﬁnal hidden states of
the LSTM (i.e. cT , sT ), but we additionally exploit the fol-
lowing two states in order to extract as much information as
possible from the LSTM: i) the state Ml = (cid:80)T
t=1 It: the
summation of the input feature It of each time step, and ii)
the state Ol = (cid:80)T
t=1 gt: the summation of the output of
each time step of the LSTM. We concatenate Ml and Ol
with cT and sT . That is, if the cell size of the LSTM is
d, the output gθ of the baseline that uses cT , sT becomes

2

Figure 1. A complete pipeline illustration of our model for
YouTube-8M classiﬁcation. The input of the model concatenates
the frame feature If and audio feature Ia, which is denoted by I.
We divided the model into four components (Video Pooling Layer,
Classiﬁcation Layer, Label Processing Layer, Loss Function) and
experimented with the newly proposed model or variants of previ-
ously proposed model in each component. Each of the four com-
ponents is denoted by (gθ, hθ, Cθ, L), and each output of ﬁrst 3
layer is denoted by (Og, Oh, Oc).

the matrix L ∈ RT ×1152, Position Encoding is simply de-
ﬁned as follows.

I ← I (cid:12) L

Og =

T
(cid:88)

t=1

It

(8)

(9)

where Lij = (1 − i/T ) − (j/1152)(1 − 2 × i/T ), (cid:12) means
element-wise multiplication. After applying Position En-
coding, we used summation of each vector as output of gθ,
that menas Og is d-dimensional vector where d is 1,152.

2.2.4 Self-Attention: Indirect Clustering

The YouTube-8M dataset deals with general topics (e.g.
soccer, game, car, animation) rather than relatively focused
labels like Sports-1M or UCF-101. We here test the follow-
ing hypothesis: since the topic is highly general, it may be
more advantageous to focus on the most dominant parts of
the video rather than temporal/motion information of indi-
vidual frames. Therefore, we suggest an indirect clustering
model using the self-attention mechanism as follows.

We perform a clustering on the video features over all
frames I ∈ RT ×1,152, and ﬁnd the cluster with the largest
size (i.e. the largest number of elements in the cluster).
Then, the vectors in this cluster may represent the main
scene of the video. However, since it takes very long time
to perform clustering on each video, we propose a self-
attention mechanism that acts like clustering as follows.

pt = softmax(

ItIi)

(10)

T
(cid:88)

i=1

where pt is a scalar value that indicates the soft attention to
a frame t while the softmax is applied over p1..T . That is,
the more similar the frame vector is to the other vectors, the
higher its pt value is (i.e. it is more likely to be the main
scene of the video). Finally, the frame encoding is simply
obtained by a weighted sum of frame vectors by the atten-
tion values: gθ = (cid:80)T

t=1 ptIt.

2.2.5 Adaptive Noise

Each of 4,716 classes in the YouTube-8M dataset has a dif-
ferent number of video examples. For example, the Car
class has about 800,000 examples, but the Air Gear class
has only 101 examples. Let n to be the number of labels
associated with a video I; we introduce the adaptive noise
structure to relieve the label imbalance problem as follows.

It ← It + γ · Z

where Z ∼ N (0, I), γ =

(11)

(12)

1
n

·

n
(cid:88)

i=1

1
S(yi)

Figure 2. The ﬁrst component Video Pooling Layer gθ of the en-
It takes T frame vector I as input and outputs d-
tire model.
dimensional vector using different encoding methods respectively.

a (2 × d)-dimensional vector, whereas our model that ad-
ditionally uses Ml and Ol has the output of a (4 × d)-
dimensional vector. Experimentally, we choose the LSTM
cell size d = 1,152. We also apply the layer normaliza-
tion [2] to each layer of the LSTM for fast convergence,
and also use the dropout with dropout rate=0.8 to increase
the generalization capacity.

2.2.2 CNNs

Convolutional neural networks (CNNs) are often used to
jointly capture spatial information from images or video in
many computer vision tasks. That is, the convolution ker-
nels generate output signals considering all the elements in
the window together, and thus they effectively work with
spatially or temporally sequential information (e.g. images,
consecutive characters in NLP, and audio understanding).
As the second candidate of Video Pooling Layer, we use the
CNN to capture temporal information of video as proposed
in [10]:

oc = ReLU(conv(I, wconv, bconv))

(7)

where conv(input, ﬁlter, bias) indicates convolution layer
with stride 1, and ReLU indicates the element-wise ReLU
activation [12]. wconv ∈ Rcv×ch×1×d is a convolution ﬁlter
with the vertical and horizontal ﬁlter size of cv = 5, ch =
1152, and bconv ∈ Rd is a bias. oc ∈ R(T −cv+1)×d indi-
cates the output of the convolution layer. Finally, we apply
max-pooling over time for oc, obtaining the d-dimensional
encoding of gθ.

2.2.3 Position Encoding

We also test the Position Encoding scheme [14] that assigns
different weights to each frame vector. That is, we deﬁne

3

Figure 3. (a)The second component Classiﬁcation Layer hθ of the overall model. It take the d-dimensional vector as input Og and have
score Oh for 4,716 classes as output. However, input of Many-to-Many model is I rather than Og. (b)the 3rd component Label Processing
Layer Cθ of the entire model. It uses label correlation prior to update the score for each class.

where noise Z is sampled from normal distribution, yi de-
notes the i-th label for video, and S(yi) is the number of
video examples that have label yi. It means that we increase
the generalization for small classes by adding more noise to
their frame vectors. We then make summation of the vec-
tors of all frames as the output of gθ, as done in the position
encoding.

2.3.2 Variants of Mixture of Experts

The Mixtures of Experts [8] model is a binary classiﬁer that
adaptively takes into account the scores of several experts
corresponding to a class. For one class, each expert ei has
a probability value between 0 and 1, and gate gi represents
the weight for each expert and is deﬁned as follows.

2.3. Classiﬁcation Layer

Classiﬁcation Layer hθ is deﬁned as follows.

(cid:40)

hθ : RT ×1152 → R4,716,
hθ : Rd → R4,716,

if hθ is Many-to-Many
otherwise

(13)
That is, by default, hθ takes frame encoding Og as an in-
put and outputs score Oh for 4,716 classes. Exceptionally,
as shown in ﬁgure 1, the Many-to-Many model has I as its
input, where I is untouched by video pooling layer gθ. We
have conducted experiments on the following three struc-
tures in this component. (see ﬁgure 3(a))

2.3.1 Many-to-Many

e Og + be)

ei = σ(wT
gi = softmax(wT

g Og + bg)

(14)

(15)

where we, wg are d-dimensional vectors, scalars be, bg
are biases, and softmax is performed for {g1, ..., gE}. (E is
number of experts.) We extend this MoE model to multi-
layer, and construct multiple fully-connected of deﬁning
probability and gate distribution of each expert. For exam-
ple, the gate distribution and the expert distribution of the
2-layer MoE model are deﬁned as follows.

e (WeOg + b(cid:48)

ei = σ(vT
gi = softmax(vT

e) + be)
g (WgOg + b(cid:48)

g) + bg)

(16)

(17)

Unlike other models, Many-to-Many model has a frame
vector I that is not touched by video pooling layer gθ. It uses
LSTM similar to that of 2.2.1, but it calculates the score by
attaching fully-connected layer to each output of each step
in LSTM and average them, which is used as output Oh.
Since this model averages out the score in each frame, it
has the temporal encoding ability of the RNN as well as the
fact that the scores drawn in the more frequently appearing
frames are reﬂected more. As a result, more effective video
encoding could be performed.

where each weight matrix We, Wg is denoted by
We ∈ Rd(cid:48)×d, Wg ∈ Rd(cid:48)×d and each vT
g means d(cid:48)-
e , vT
dimensional vector. be, bg, b(cid:48)
g are biases. Finally, the
score Oh[i] for i-th class is determined by the weighted sum
of each expert distribution; Oh[i] = (cid:80)k

e, b(cid:48)

i=1 eigi.

2.3.3 Multi Layer Perceptron

The Multi Layer Perceptron model
is stack of Fully-
Connected Layers, one of the most basic Neural Network

4

structures. We experimentally set the number of layers to 3
and apply Layer Normalization [2] to each layer.

2.4. Label Processing Layer

Label Processing Layer Cθ is deﬁned as follows.

Where N denotes the number of labels in one video. ei
denotes a embedding vector from penultimate layer, ck de-
notes the yi-th corresponding class center for each ei. The
Ls is the cross-entropy loss, Lc is the center loss. A scalar
λ is a hyperparameter for balancing the two loss functions.

Cθ : R4,716 → R4,716

(18)

2.5.2 Pseudo-Huber Loss

This component is designed to reﬂect the correlation be-
tween the labels into the model. For example, YouTube-8M,
which includes {Soccer, Football, Kick, Indoor soccer} and
{Super Mario Bros, Super Mario World, Super Mario bros
3, Mario Kart, Mario Kart 8}. In order to take advantage of
this property, we set up the label correlation matrix Mc as
follows by counting all the videos in the training set. (See
Figure 3(b))

(19)

Mc ∈ R4,716×4,716
where Mc is correlation matrix and the correlation value
Mc[i, j] between the i-th label and the j-th label is calcu-
lated higher as i-th label and j-th label appear together more
in the same video. Then, a new score is deﬁned as follows to
better reﬂect the correlation between labels through simple
linear combination of matrix-vector multiplication.

Oc = α · Oh + β · McOh + γ · M(cid:48)
Here, Mc is used as a ﬁxed value, and M(cid:48)

cOh
c is a trainable
parameter initialized to the same value as Mc. Scalar values
α, β, γ are hyperparameters for model.

(20)

2.5. Loss Function

2.5.1 Center Loss

Center Loss was ﬁrst proposed for face recognition task [18]
and expanded to other ﬁeld because of its effectiveness for
making dicriminative embedding feature [17]. The purpose
of the center loss is to minimize intra-class variations while
maximizing inter-class variations using the joint supervi-
sion of cross-entropy loss and center loss. The original cen-
ter loss was used for the single label classiﬁcation problem
and it is hard to exploit it in a multi-label classiﬁcation prob-
lem.

If we convert the multi label problem into a single la-
bel problem like [3], increment of centers according to the
combination of labels is the simple expansion of center loss
to a multi-label classiﬁcation problem. However, this sim-
ple expansion is not suitable for YouTube-8M, because the
number of combination for labels is too big to calculate.
Therefore, we modiﬁed the center loss to suit the multi-label
classiﬁcation problem as follow.

Lc =

1
N

N
(cid:88)

i=1

L = Ls + λLc.

(cid:107)ei − ck(cid:107)2

2 , k = yi

(21)

(22)

5

The Huber Loss is a combination of L2 Loss and L1 Loss,
which allows the model to be trained more robustly on noise
instances. In the case of the YouTube-8M, there are classes
with very few instances due to the label imbalance problem,
and Huber Loss is designed to better learn instances belong-
ing to these classes. For a simple, differentiable form, we
use the Pseudo-Huber Loss function, a smooth approxima-
tion of the Huber Loss, as follows.

Lc = δ2((cid:112)1 + (LCE/δ)2 − 1)

(23)

where LCE means that Cross-Entropy Loss between our
prediction Oc and ground-truth label y, δ means hyperpa-
rameter for model.

2.6. Training

For training our model, we choose the Adam [11] opti-
mizer using a batch size of 128, with learning rate = 0.0006,
β1 = 0.9, β2 = 0.999, (cid:15) = 1e − 8. We also apply learning
rate decay with the rate = 0.95 for every 1.5M iterations. We
train the model for 5 epochs with no early stopping. We use
both ofﬁcial training and validation data that YouTube-8M
public dataset provides for training our models.

3. Experiments

We use the test data from the Kaggle competition:
Google Cloud & YouTube-8M Video Understanding Chal-
lenge to measure the performance of the model. The source
for our model is publicly available2.

3.1. Experimental Setting

One of the greatest features of our model is that we
have three issues to solve the YouTube-8M classiﬁca-
tion task;
i)temporal encoding for video, ii)label imbal-
ance problem, iii)correlation between labels, and we have
tried several variations on each component by dividing the
model pipeline into four components; i)Frame Encoding,
ii)Classiﬁcation Layer, iii)Label Processing Layer, iv)Loss
Function. In addition, except in the Many-to-Many model
in our pipeline, each component is completely independent
of one another, so it is a very good structure for experiment-
ing with a number of variant model combinations. How-
ever, it is impossible to do brute-force experiments because
the number of trials is too great to test all combinations of

2https://github.com/seilna/youtube-8m

Method
LSTM
LSTM-M
LSTM-M-O
LSTM-M-O-LN
CNN-64
CNN-256
CNN-1024
Position Encoding
Indirect Clustering
Adaptive Noise

GAP@20
0.811
0.815
0.820
0.815
0.704
0.753
-
0.782
0.801
0.782

Table 1. The results on various transformation structures in Video
Pooling Layer gθ component.

variations that each component can try. Thus, we took the
greedy approach, ﬁxed the remaining 3 components in each
component, and experimented the several structures only
for that component, choosing the best-performing structure
for each component. We used Google Average Precision3
(GAP)@20 as a metric to measure the performance of the
model.

3.2. Quantitative Results

3.2.1 Video Pooling Layer

There are ﬁve transform structures in Video Pooling Layer
gθ; i) variants of LSTM, ii) CNN, iii) Position Encoding, iv)
Indirect Clustering, v) Adaptive Noise. To select the most
suitable structure for gθ, we ﬁxed the rest of the component
hθ to the MoE-2 model, not Cθ used, and Lc used the cross
entropy loss. Based on these settings, the results for each of
the ﬁve structures are shown in table 1.

LSTM simply uses the last hidden state, LSTM-M con-
catenates Ml, LSTM-M-O concatenates both Ml and Ol,
and LSTM-M-O-LN is a model that applies Layer Normal-
ization for each layer of LSTM. CNN-64, CNN-256, and
CNN-1024 refer to models with 64, 256, and 1024 as the
output channels of CNN, respectively.

The results show that the LSTM method has the best per-
formance for encoding frames. Within LSTM, the higher
the utilization of the internal information, the higher the
performance, indicating that higher performance can be ex-
pected if more information can be extracted from other
methods such as skip connection in LSTM. On the other
hand, unlike the expectation, the LSTM with Layer Nor-
malization has a lower performance than that which is not.
Of course, Layer Normalization had the advantage of stable
and fast convergence even when 20-30 times learning rate
was applied, but when compared to the ﬁnal performance
alone, performance was not good.

CNN showed a very poor performance unexpectedly. As

3https://www.kaggle.com/c/youtube8m#evaluation

Method
Many-to-Many
2 Layer MoE-2
2 Layer MoE-16
3 Layer MLP-4096
3 Layer MLP-4096-LN

GAP@20
0.791
0.424
0.421
0.802
0.809

Table 2. The results on various transformation structures in Clas-
siﬁcation Layer hθ component.

CNN’s output channel increased, performance was on the
rise, but channels larger than 256 were not experimented
because of the memory limitations of the GPU. While we
can expect that CNN will perform well for larger chan-
nels, another problem with CNN is the dramatic increase in
computation cost as channels increase. However, if CNNs
with different hyperparameters record similar performance
to LSTM, CNN is likely to be used as good pooling method
because it has the advantage that the convolution operation
is fully parallelizable.

The Position Encoding model showed lower perfor-
mance than LSTM, and one of the possible reasons is that
the sequence modeling power of the model is weaker than
LSTM.

The Indirect Clustering model has lower performance
than LSTM, but it performs better than the Position En-
coding model. This suggests that the assumptions we have
made (the importance of the main scene in video classi-
ﬁcation) are not entirely wrong, suggesting that we need
a model that can cover this issue more delicately. Also,
temporal encoding of LSTM is as important as considering
main scene in video classiﬁcation.

Adaptive Noise do not lead to a signiﬁcant performance
improvement, indicating that a more sophisticated approach
to the label imbalance problem is required.

3.2.2 Classiﬁcation Layer

The Classiﬁcation Layer hθ has three variants:
i)Many-
to-Many, ii)Multi-Layer MoE, iii)MLP. To select the best
method for hθ, we ﬁx the remaining component gθ with In-
direct Clustering, Cθ is not used, and Lc uses cross entropy
loss. (However, in the many-to-many model, gθ is not used
according to the deﬁnition.) Based on these settings, the
results for each of the three structures are shown in Table 3.
2 Layer MoE-2 refers to a 2 layer MoE model with
2 experts, and 2 Layer MoE-16 refers to a 2 layer MoE
model with 16 experts. 3 layer MLP-4096 is MLP
structure with 4096 dimension of each hidden layer and
3 hidden layers, and 3 layer MLP-4096-LN applies
layer normalization to each layer.

In constructing the classiﬁcation layer, the MLP struc-
ture showed the best performance among the three meth-

6

Method
MoE - (1.0, 0.3, 0.0)
MoE - (1.0, 0.1, 0.0)
MoE - (1.0, 0.0, 0.1)
MoE - (1.0, 0.01, 0.0)
MoE - (1.0, 0.0, 0.01)
MoE - (1.0, 0.01, 0.01)

GAP@20
0.784
0.787
0.788
0.790
0.790
0.788

Method
LCE
LCE + Lc(λ = 0.001)
HuberCE(δ = 0.5)
HuberCE(δ = 1.0)
HuberCE(δ = 2.0)
HuberCE(δ = 3.0)

GAP@20
0.798
0.799
0.803
0.801
0.798
0.794

Table 3. The results on various hyperparameters in Label Process-
ing Layer Cθ component.

Table 4. The results on variants in loss function Lc component.

ods. The Many-to-Many model is an LSTM-based model,
but has a lower performance than the basic LSTM structure
in Table 2, indicating that the Many-to-Many framework in
video classiﬁcation is not always a good choice.

On the other hand, disappointingly, the Multi Layer MoE
model showed severe overﬁtting in only two layers. As ex-
pected, this requires a lot of parameters to create an inter-
mediate level of embedding, which is also needed for each
experts, resulting in overﬁtting for many parameters. We
experimented with different number of layers and hidden
layer dimensions for the MLP structure, but 4096 dimen-
sions of 3 layers showed the highest performance. Unusual
is that, when layer normalization was applied, the MLP
model showed improved performance unlike LSTM, and
did not require more delicate hyperparameter tuning.

3.2.3 Label Processing Layer

To select the best method for Cθ, we ﬁx the remaining com-
ponent gθ with Indirect Clustering, hθ using the MoE-16
model and Lc using the cross entropy loss. As deﬁned
in Equation 20, the Label Processing Layer Cθ uses pre-
computed correlation matrix Mc. α, β, and γ values can be
used to control the degree to which the correlation matrix
affects the class score. An experiment is shown in Table 4.
As a result, performance dropped for all β and γ values
greater than zero. In other words, when using the label cor-
relation matrix for the purpose of score update, it showed
rather low classiﬁcation performance.

The possible reason for the result may be that our model
is too naive to reﬂect the correlation label prior and that
label correlation in the YouTube-8M Dataset is not strong
enough to improve the performance of the classiﬁcation
task.

3.2.4 Loss Function

To select the most suitable loss for Lc, we ﬁx the remaining
component gθ with Indirect Clustering, hθ uses the MoE-16
model, and Cθ is not used.

Table 5 shows the performance depending on whether
or not the center loss and the pseudo-huber loss are used.

7

LCE is the cross entropy loss, Lc is the center loss, and
HuberCE is the Pseudo-Huber loss for cross entropy term.
The results show that, ﬁrstly, using the center loss term
Lc gives little performance gain. Considering that we ap-
ply the center loss term to the multi-label, the performance
should be improved if the labels are correlated with each
other. Here we get another insight for the correlation label
prior, which is that the correlation does not exist as strongly
as to achieve performance improvement. Second, Huber
Loss proved its ability to cover noise data somewhat from
the YouTube video annotation system4 by recording a rela-
tively clear performance improvement.

3.3. Ensemble Model

Based on the experiments we conducted in section 3.2,
we recorded test performance of 0.820 as a single model
combining LSTM-M-O, MoE-2, and HuberLoss.(Due to
GPU memory limitation, we could not apply MoE-16 or
Multi Layer MLP model to LSTM based model) In addi-
tion, our ensemble model, which is a simple average of
the scores of several models, showed a test performance of
0.839 and ranked 8th in the kaggle challenge. The interest-
ing thing is that when we assemble several models, we did
not get a signiﬁcant performance increase if we combine
the better single models. Rather, for increasing ensemble
model’s performance, the models incorporated should be as
diverse as possible.

4. Conclusion

We deﬁned three issues that need to be covered in order
to solve the YouTube-8M Video Classiﬁcation task, and we
divided the model pipeline into four components and exper-
imented with the various structures to solve issues in each
component. As a result, almost all of the deformed struc-
tures tried to perform better than the baseline, and their en-
semble model recorded 0.839 in the test performance and
8th in the kaggle challenge. We also provided insights on
the structures we tried on each component, on what roles
each structure plays, and why they work well or poor. Based
on this insight, we will explore ways of better frame encod-
ing or use more elegant label correlation priorities as our

4https://www.youtube.com/watch?v=wf 77z1H-vQ

[20] Y. Yu, H. Ko, J. Choi, and G. Kim. Video captioning and
arXiv preprint

retrieval models with semantic attention.
arXiv:1610.02947, 2016.

[21] L. Zhu, Z. Xu, Y. Yang, and A. G. Hauptmann. Uncovering
temporal context for video question and answering. arXiv
preprint arXiv:1511.04670, 2015.

future work.

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m: A
large-scale video classiﬁcation benchmark. arXiv preprint
arXiv:1609.08675, 2016.

[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization.

arXiv preprint arXiv:1607.06450, 2016.

[3] A. C. de Carvalho and A. A. Freitas. A tutorial on multi-label
classiﬁcation techniques. In Foundations of Computational
Intelligence. Springer, 2009.

[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009.

[5] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell,
and M. Rohrbach. Multimodal compact bilinear pooling
for visual question answering and visual grounding. arXiv
preprint arXiv:1606.01847, 2016.

[6] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke,
A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous,
B. Seybold, et al. Cnn architectures for large-scale audio
classiﬁcation. arXiv preprint arXiv:1609.09430, 2016.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 1997.

[8] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of ex-
perts and the em algorithm. Neural computation, 1994.
[9] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014.

[10] Y. Kim. Convolutional neural networks for sentence classiﬁ-

cation. arXiv preprint arXiv:1408.5882, 2014.

[11] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014.

[12] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010.

[13] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012.

[14] S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end mem-

ory networks. In NIPS, 2015.

[15] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016.

[16] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urta-
sun, and S. Fidler. Movieqa: Understanding stories in movies
through question-answering. In CVPR, 2016.

[17] H. Wang, Z. Li, X. Ji, and Y. Wang. Face r-cnn. arXiv

preprint arXiv:1706.01061, 2017.

[18] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In ECCV,
2016.

[19] J. Xu, T. Mei, T. Yao, and Y. Rui. Msr-vtt: A large video de-
scription dataset for bridging video and language. In CVPR,
2016.

8

7
1
0
2
 
l
u
J
 
2
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
6
9
7
0
.
6
0
7
1
:
v
i
X
r
a

Encoding Video and Label Priors for Multi-label Video Classiﬁcation
on YouTube-8M dataset

Seil Na
Seoul National University
seil.na@vision.snu.ac.kr

YoungJae Yu
Seoul National University
yj.yu@vision.snu.ac.kr

Sangho Lee
Seoul National University
sangho.lee@vision.snu.ac.kr

Jisung Kim
SK Telecom Video Tech. Lab
joyful.kim@sk.com

Gunhee Kim
Seoul National University
gunhee@snu.ac.kr

Abstract

YouTube-8M is the largest video dataset for multi-label
video classiﬁcation. In order to tackle the multi-label classi-
ﬁcation on this challenging dataset, it is necessary to solve
several issues such as temporal modeling of videos, label
imbalances, and correlations between labels. We develop
a deep neural network model, which consists of four com-
ponents: Video Pooling Layer, Classiﬁcation Layer, Label
Processing Layer, and Loss Function. We introduce our
newly proposed methods and discusses how existing mod-
els operate in the YouTube-8M Classiﬁcation Task, what in-
sights they have, and why they succeed (or fail) to achieve
good performance. Most of the models we proposed are
very high compared to the baseline models, and the ensem-
ble of the models we used is 8th in the Kaggle Competition.

1. Introduction

Many challenging problems have been studied in com-
puter vision research toward video understanding, such as
video classiﬁcation [9, 13], video captioning [19, 20], video
QA [21], and MovieQA [16], to name a few. YouTube-
8M [1] is the largest video dataset for multi-label video
classiﬁcation. Its main problem is to predict the most rele-
vant labels for a given video out of 4,716 predeﬁned classes.
Therefore, it requires jointly solving two important prob-
lems; video classiﬁcation and multi-label classiﬁcation.

From the view of the video classiﬁcation, YouTube-8M
is challenging in that it covers more general classes like soc-
cer, game, vehicle, and food, while existing video classiﬁ-
cation datasets focus on more speciﬁc class groups, such
as sports in Sports-1M [9], and actions in UCF-101 [13].
Therefore, unlike the importance of modeling motion fea-

tures in UCF-101 [13] or Sports-1M [9], it is important
to capture more generic video information(e.g. temporal
encoding method for video, audio feature modeling) in
YouTube-8M.

From the view of multi-label classiﬁcation, the key is-
sues to solve in YouTube-8M are label imbalances and cor-
relations between labels. YouTube-8M involves 4,716 class
labels, and the number of videos belonging to each class is
signiﬁcantly different, which causes a label imbalance is-
sue that the classiﬁer ﬁts to the biased data. At the same
time, many classes are closely related one another, such
as {Football, Kick, Penalty kick, Indoor soccer} or {Super
Mario Bros, Super Mario World, Super Mario bros 3, Mario
Kart, Mario Kart 8}. It is also challenging to resolve the
correlations between labels to decide ﬁnal prediction.

Based on the challenges of the multi-label video classiﬁ-
cation task on YouTube-8M described the above, we focus
on addressing i) temporal encoding for video, ii) relieving
the label imbalance problem, and iii) utilizing the correlated
label information. Our model consists of four components:
i) video pooling layer, ii) classiﬁcation layer, iii) label pro-
cessing layer, and iv) loss function. The proposed com-
ponents indeed show signiﬁcant performance improvement
over the baseline models of YouTube-8M [1], and ﬁnally
our ensemble model is ranked 8th in the Google Cloud &
YouTube-8M Video Understanding Challenge 1.

2. The Model

Figure 1 shows the overall pipeline of our model. We
ﬁrst present video features we used, and then explain its
four key components in the following sections.

1https://www.kaggle.com/c/youtube8m/

leaderboard. (Team name: SNUVL X SKT)

1

2.1. Video Features

The inputs of the model are frame features and audio
features of a video clip. The frame features are obtained by
sampling a clip at 1-second interval, and extracting 2,048-
dimensional vector from every frame through Inception-V3
Network [15] pretrained to ImageNet [4]. Then the feature
is reduced to an 1,024-dimension via PCA (+ whitening),
then quantized, and ﬁnally L2-normalized. As a result, for
a video of T seconds long, its frame is If ∈ RT ×1,024. The
audio features re extracted using the VGG-inspired acoustic
model [6] followed by L2-normalization, which is denoted
by Ia ∈ RT ×128.

As the input of our model, we concatenate the frame fea-
ture If and the audio feature Ia at every time step, denoted
by I ∈ RT ×1,152. We test the compact bilinear pooling [5]
with various dimensions between If and Ia, but all of them
have signiﬁcantly lower performance than the simple con-
catenation. From now on, we use I to denote the input fea-
tures of a video over all frames, and It ∈ R1,152 as t-th
frame vector.

2.2. Video Pooling Layer

The Video Pooling Layer gθ : RT ×1,152 → Rd is de-
ﬁned as a parametric function that encodes a sequence of
T feature vectors I into a d-dimensional embedding vector.
We test ﬁve different encoding structures as follows.

2.2.1 A Variant of LSTM

The Long Short Term Memory (LSTM) model [7] is one of
the most popular frameworks for modeling sequence data.
We use a variant of the LSTM as follows:

it = σ(ItUi + st−1Wi + bi)
ft = σ(ItU f + st−1Wf + bf )
ot = σ(ItU o + st−1Wo + bo)
gt = tanh(ItUg + st−1Wg + bg)
ct = ct−1 ◦ ft + gt ◦ it
st = tanh(ct) ◦ ot

(1)

(2)

(3)

(4)

(5)

(6)

where t denotes each time step, i, f , o are the input, forget,
output gate, ct, st are long-term and short-term memory re-
spectively.

The baseline model uses only the ﬁnal hidden states of
the LSTM (i.e. cT , sT ), but we additionally exploit the fol-
lowing two states in order to extract as much information as
possible from the LSTM: i) the state Ml = (cid:80)T
t=1 It: the
summation of the input feature It of each time step, and ii)
the state Ol = (cid:80)T
t=1 gt: the summation of the output of
each time step of the LSTM. We concatenate Ml and Ol
with cT and sT . That is, if the cell size of the LSTM is
d, the output gθ of the baseline that uses cT , sT becomes

2

Figure 1. A complete pipeline illustration of our model for
YouTube-8M classiﬁcation. The input of the model concatenates
the frame feature If and audio feature Ia, which is denoted by I.
We divided the model into four components (Video Pooling Layer,
Classiﬁcation Layer, Label Processing Layer, Loss Function) and
experimented with the newly proposed model or variants of previ-
ously proposed model in each component. Each of the four com-
ponents is denoted by (gθ, hθ, Cθ, L), and each output of ﬁrst 3
layer is denoted by (Og, Oh, Oc).

the matrix L ∈ RT ×1152, Position Encoding is simply de-
ﬁned as follows.

I ← I (cid:12) L

Og =

T
(cid:88)

t=1

It

(8)

(9)

where Lij = (1 − i/T ) − (j/1152)(1 − 2 × i/T ), (cid:12) means
element-wise multiplication. After applying Position En-
coding, we used summation of each vector as output of gθ,
that menas Og is d-dimensional vector where d is 1,152.

2.2.4 Self-Attention: Indirect Clustering

The YouTube-8M dataset deals with general topics (e.g.
soccer, game, car, animation) rather than relatively focused
labels like Sports-1M or UCF-101. We here test the follow-
ing hypothesis: since the topic is highly general, it may be
more advantageous to focus on the most dominant parts of
the video rather than temporal/motion information of indi-
vidual frames. Therefore, we suggest an indirect clustering
model using the self-attention mechanism as follows.

We perform a clustering on the video features over all
frames I ∈ RT ×1,152, and ﬁnd the cluster with the largest
size (i.e. the largest number of elements in the cluster).
Then, the vectors in this cluster may represent the main
scene of the video. However, since it takes very long time
to perform clustering on each video, we propose a self-
attention mechanism that acts like clustering as follows.

pt = softmax(

ItIi)

(10)

T
(cid:88)

i=1

where pt is a scalar value that indicates the soft attention to
a frame t while the softmax is applied over p1..T . That is,
the more similar the frame vector is to the other vectors, the
higher its pt value is (i.e. it is more likely to be the main
scene of the video). Finally, the frame encoding is simply
obtained by a weighted sum of frame vectors by the atten-
tion values: gθ = (cid:80)T

t=1 ptIt.

2.2.5 Adaptive Noise

Each of 4,716 classes in the YouTube-8M dataset has a dif-
ferent number of video examples. For example, the Car
class has about 800,000 examples, but the Air Gear class
has only 101 examples. Let n to be the number of labels
associated with a video I; we introduce the adaptive noise
structure to relieve the label imbalance problem as follows.

It ← It + γ · Z

where Z ∼ N (0, I), γ =

(11)

(12)

1
n

·

n
(cid:88)

i=1

1
S(yi)

Figure 2. The ﬁrst component Video Pooling Layer gθ of the en-
It takes T frame vector I as input and outputs d-
tire model.
dimensional vector using different encoding methods respectively.

a (2 × d)-dimensional vector, whereas our model that ad-
ditionally uses Ml and Ol has the output of a (4 × d)-
dimensional vector. Experimentally, we choose the LSTM
cell size d = 1,152. We also apply the layer normaliza-
tion [2] to each layer of the LSTM for fast convergence,
and also use the dropout with dropout rate=0.8 to increase
the generalization capacity.

2.2.2 CNNs

Convolutional neural networks (CNNs) are often used to
jointly capture spatial information from images or video in
many computer vision tasks. That is, the convolution ker-
nels generate output signals considering all the elements in
the window together, and thus they effectively work with
spatially or temporally sequential information (e.g. images,
consecutive characters in NLP, and audio understanding).
As the second candidate of Video Pooling Layer, we use the
CNN to capture temporal information of video as proposed
in [10]:

oc = ReLU(conv(I, wconv, bconv))

(7)

where conv(input, ﬁlter, bias) indicates convolution layer
with stride 1, and ReLU indicates the element-wise ReLU
activation [12]. wconv ∈ Rcv×ch×1×d is a convolution ﬁlter
with the vertical and horizontal ﬁlter size of cv = 5, ch =
1152, and bconv ∈ Rd is a bias. oc ∈ R(T −cv+1)×d indi-
cates the output of the convolution layer. Finally, we apply
max-pooling over time for oc, obtaining the d-dimensional
encoding of gθ.

2.2.3 Position Encoding

We also test the Position Encoding scheme [14] that assigns
different weights to each frame vector. That is, we deﬁne

3

Figure 3. (a)The second component Classiﬁcation Layer hθ of the overall model. It take the d-dimensional vector as input Og and have
score Oh for 4,716 classes as output. However, input of Many-to-Many model is I rather than Og. (b)the 3rd component Label Processing
Layer Cθ of the entire model. It uses label correlation prior to update the score for each class.

where noise Z is sampled from normal distribution, yi de-
notes the i-th label for video, and S(yi) is the number of
video examples that have label yi. It means that we increase
the generalization for small classes by adding more noise to
their frame vectors. We then make summation of the vec-
tors of all frames as the output of gθ, as done in the position
encoding.

2.3.2 Variants of Mixture of Experts

The Mixtures of Experts [8] model is a binary classiﬁer that
adaptively takes into account the scores of several experts
corresponding to a class. For one class, each expert ei has
a probability value between 0 and 1, and gate gi represents
the weight for each expert and is deﬁned as follows.

2.3. Classiﬁcation Layer

Classiﬁcation Layer hθ is deﬁned as follows.

(cid:40)

hθ : RT ×1152 → R4,716,
hθ : Rd → R4,716,

if hθ is Many-to-Many
otherwise

(13)
That is, by default, hθ takes frame encoding Og as an in-
put and outputs score Oh for 4,716 classes. Exceptionally,
as shown in ﬁgure 1, the Many-to-Many model has I as its
input, where I is untouched by video pooling layer gθ. We
have conducted experiments on the following three struc-
tures in this component. (see ﬁgure 3(a))

2.3.1 Many-to-Many

e Og + be)

ei = σ(wT
gi = softmax(wT

g Og + bg)

(14)

(15)

where we, wg are d-dimensional vectors, scalars be, bg
are biases, and softmax is performed for {g1, ..., gE}. (E is
number of experts.) We extend this MoE model to multi-
layer, and construct multiple fully-connected of deﬁning
probability and gate distribution of each expert. For exam-
ple, the gate distribution and the expert distribution of the
2-layer MoE model are deﬁned as follows.

e (WeOg + b(cid:48)

ei = σ(vT
gi = softmax(vT

e) + be)
g (WgOg + b(cid:48)

g) + bg)

(16)

(17)

Unlike other models, Many-to-Many model has a frame
vector I that is not touched by video pooling layer gθ. It uses
LSTM similar to that of 2.2.1, but it calculates the score by
attaching fully-connected layer to each output of each step
in LSTM and average them, which is used as output Oh.
Since this model averages out the score in each frame, it
has the temporal encoding ability of the RNN as well as the
fact that the scores drawn in the more frequently appearing
frames are reﬂected more. As a result, more effective video
encoding could be performed.

where each weight matrix We, Wg is denoted by
We ∈ Rd(cid:48)×d, Wg ∈ Rd(cid:48)×d and each vT
g means d(cid:48)-
e , vT
dimensional vector. be, bg, b(cid:48)
g are biases. Finally, the
score Oh[i] for i-th class is determined by the weighted sum
of each expert distribution; Oh[i] = (cid:80)k

e, b(cid:48)

i=1 eigi.

2.3.3 Multi Layer Perceptron

The Multi Layer Perceptron model
is stack of Fully-
Connected Layers, one of the most basic Neural Network

4

structures. We experimentally set the number of layers to 3
and apply Layer Normalization [2] to each layer.

2.4. Label Processing Layer

Label Processing Layer Cθ is deﬁned as follows.

Where N denotes the number of labels in one video. ei
denotes a embedding vector from penultimate layer, ck de-
notes the yi-th corresponding class center for each ei. The
Ls is the cross-entropy loss, Lc is the center loss. A scalar
λ is a hyperparameter for balancing the two loss functions.

Cθ : R4,716 → R4,716

(18)

2.5.2 Pseudo-Huber Loss

This component is designed to reﬂect the correlation be-
tween the labels into the model. For example, YouTube-8M,
which includes {Soccer, Football, Kick, Indoor soccer} and
{Super Mario Bros, Super Mario World, Super Mario bros
3, Mario Kart, Mario Kart 8}. In order to take advantage of
this property, we set up the label correlation matrix Mc as
follows by counting all the videos in the training set. (See
Figure 3(b))

(19)

Mc ∈ R4,716×4,716
where Mc is correlation matrix and the correlation value
Mc[i, j] between the i-th label and the j-th label is calcu-
lated higher as i-th label and j-th label appear together more
in the same video. Then, a new score is deﬁned as follows to
better reﬂect the correlation between labels through simple
linear combination of matrix-vector multiplication.

Oc = α · Oh + β · McOh + γ · M(cid:48)
Here, Mc is used as a ﬁxed value, and M(cid:48)

cOh
c is a trainable
parameter initialized to the same value as Mc. Scalar values
α, β, γ are hyperparameters for model.

(20)

2.5. Loss Function

2.5.1 Center Loss

Center Loss was ﬁrst proposed for face recognition task [18]
and expanded to other ﬁeld because of its effectiveness for
making dicriminative embedding feature [17]. The purpose
of the center loss is to minimize intra-class variations while
maximizing inter-class variations using the joint supervi-
sion of cross-entropy loss and center loss. The original cen-
ter loss was used for the single label classiﬁcation problem
and it is hard to exploit it in a multi-label classiﬁcation prob-
lem.

If we convert the multi label problem into a single la-
bel problem like [3], increment of centers according to the
combination of labels is the simple expansion of center loss
to a multi-label classiﬁcation problem. However, this sim-
ple expansion is not suitable for YouTube-8M, because the
number of combination for labels is too big to calculate.
Therefore, we modiﬁed the center loss to suit the multi-label
classiﬁcation problem as follow.

Lc =

1
N

N
(cid:88)

i=1

L = Ls + λLc.

(cid:107)ei − ck(cid:107)2

2 , k = yi

(21)

(22)

5

The Huber Loss is a combination of L2 Loss and L1 Loss,
which allows the model to be trained more robustly on noise
instances. In the case of the YouTube-8M, there are classes
with very few instances due to the label imbalance problem,
and Huber Loss is designed to better learn instances belong-
ing to these classes. For a simple, differentiable form, we
use the Pseudo-Huber Loss function, a smooth approxima-
tion of the Huber Loss, as follows.

Lc = δ2((cid:112)1 + (LCE/δ)2 − 1)

(23)

where LCE means that Cross-Entropy Loss between our
prediction Oc and ground-truth label y, δ means hyperpa-
rameter for model.

2.6. Training

For training our model, we choose the Adam [11] opti-
mizer using a batch size of 128, with learning rate = 0.0006,
β1 = 0.9, β2 = 0.999, (cid:15) = 1e − 8. We also apply learning
rate decay with the rate = 0.95 for every 1.5M iterations. We
train the model for 5 epochs with no early stopping. We use
both ofﬁcial training and validation data that YouTube-8M
public dataset provides for training our models.

3. Experiments

We use the test data from the Kaggle competition:
Google Cloud & YouTube-8M Video Understanding Chal-
lenge to measure the performance of the model. The source
for our model is publicly available2.

3.1. Experimental Setting

One of the greatest features of our model is that we
have three issues to solve the YouTube-8M classiﬁca-
tion task;
i)temporal encoding for video, ii)label imbal-
ance problem, iii)correlation between labels, and we have
tried several variations on each component by dividing the
model pipeline into four components; i)Frame Encoding,
ii)Classiﬁcation Layer, iii)Label Processing Layer, iv)Loss
Function. In addition, except in the Many-to-Many model
in our pipeline, each component is completely independent
of one another, so it is a very good structure for experiment-
ing with a number of variant model combinations. How-
ever, it is impossible to do brute-force experiments because
the number of trials is too great to test all combinations of

2https://github.com/seilna/youtube-8m

Method
LSTM
LSTM-M
LSTM-M-O
LSTM-M-O-LN
CNN-64
CNN-256
CNN-1024
Position Encoding
Indirect Clustering
Adaptive Noise

GAP@20
0.811
0.815
0.820
0.815
0.704
0.753
-
0.782
0.801
0.782

Table 1. The results on various transformation structures in Video
Pooling Layer gθ component.

variations that each component can try. Thus, we took the
greedy approach, ﬁxed the remaining 3 components in each
component, and experimented the several structures only
for that component, choosing the best-performing structure
for each component. We used Google Average Precision3
(GAP)@20 as a metric to measure the performance of the
model.

3.2. Quantitative Results

3.2.1 Video Pooling Layer

There are ﬁve transform structures in Video Pooling Layer
gθ; i) variants of LSTM, ii) CNN, iii) Position Encoding, iv)
Indirect Clustering, v) Adaptive Noise. To select the most
suitable structure for gθ, we ﬁxed the rest of the component
hθ to the MoE-2 model, not Cθ used, and Lc used the cross
entropy loss. Based on these settings, the results for each of
the ﬁve structures are shown in table 1.

LSTM simply uses the last hidden state, LSTM-M con-
catenates Ml, LSTM-M-O concatenates both Ml and Ol,
and LSTM-M-O-LN is a model that applies Layer Normal-
ization for each layer of LSTM. CNN-64, CNN-256, and
CNN-1024 refer to models with 64, 256, and 1024 as the
output channels of CNN, respectively.

The results show that the LSTM method has the best per-
formance for encoding frames. Within LSTM, the higher
the utilization of the internal information, the higher the
performance, indicating that higher performance can be ex-
pected if more information can be extracted from other
methods such as skip connection in LSTM. On the other
hand, unlike the expectation, the LSTM with Layer Nor-
malization has a lower performance than that which is not.
Of course, Layer Normalization had the advantage of stable
and fast convergence even when 20-30 times learning rate
was applied, but when compared to the ﬁnal performance
alone, performance was not good.

CNN showed a very poor performance unexpectedly. As

3https://www.kaggle.com/c/youtube8m#evaluation

Method
Many-to-Many
2 Layer MoE-2
2 Layer MoE-16
3 Layer MLP-4096
3 Layer MLP-4096-LN

GAP@20
0.791
0.424
0.421
0.802
0.809

Table 2. The results on various transformation structures in Clas-
siﬁcation Layer hθ component.

CNN’s output channel increased, performance was on the
rise, but channels larger than 256 were not experimented
because of the memory limitations of the GPU. While we
can expect that CNN will perform well for larger chan-
nels, another problem with CNN is the dramatic increase in
computation cost as channels increase. However, if CNNs
with different hyperparameters record similar performance
to LSTM, CNN is likely to be used as good pooling method
because it has the advantage that the convolution operation
is fully parallelizable.

The Position Encoding model showed lower perfor-
mance than LSTM, and one of the possible reasons is that
the sequence modeling power of the model is weaker than
LSTM.

The Indirect Clustering model has lower performance
than LSTM, but it performs better than the Position En-
coding model. This suggests that the assumptions we have
made (the importance of the main scene in video classi-
ﬁcation) are not entirely wrong, suggesting that we need
a model that can cover this issue more delicately. Also,
temporal encoding of LSTM is as important as considering
main scene in video classiﬁcation.

Adaptive Noise do not lead to a signiﬁcant performance
improvement, indicating that a more sophisticated approach
to the label imbalance problem is required.

3.2.2 Classiﬁcation Layer

The Classiﬁcation Layer hθ has three variants:
i)Many-
to-Many, ii)Multi-Layer MoE, iii)MLP. To select the best
method for hθ, we ﬁx the remaining component gθ with In-
direct Clustering, Cθ is not used, and Lc uses cross entropy
loss. (However, in the many-to-many model, gθ is not used
according to the deﬁnition.) Based on these settings, the
results for each of the three structures are shown in Table 3.
2 Layer MoE-2 refers to a 2 layer MoE model with
2 experts, and 2 Layer MoE-16 refers to a 2 layer MoE
model with 16 experts. 3 layer MLP-4096 is MLP
structure with 4096 dimension of each hidden layer and
3 hidden layers, and 3 layer MLP-4096-LN applies
layer normalization to each layer.

In constructing the classiﬁcation layer, the MLP struc-
ture showed the best performance among the three meth-

6

Method
MoE - (1.0, 0.3, 0.0)
MoE - (1.0, 0.1, 0.0)
MoE - (1.0, 0.0, 0.1)
MoE - (1.0, 0.01, 0.0)
MoE - (1.0, 0.0, 0.01)
MoE - (1.0, 0.01, 0.01)

GAP@20
0.784
0.787
0.788
0.790
0.790
0.788

Method
LCE
LCE + Lc(λ = 0.001)
HuberCE(δ = 0.5)
HuberCE(δ = 1.0)
HuberCE(δ = 2.0)
HuberCE(δ = 3.0)

GAP@20
0.798
0.799
0.803
0.801
0.798
0.794

Table 3. The results on various hyperparameters in Label Process-
ing Layer Cθ component.

Table 4. The results on variants in loss function Lc component.

ods. The Many-to-Many model is an LSTM-based model,
but has a lower performance than the basic LSTM structure
in Table 2, indicating that the Many-to-Many framework in
video classiﬁcation is not always a good choice.

On the other hand, disappointingly, the Multi Layer MoE
model showed severe overﬁtting in only two layers. As ex-
pected, this requires a lot of parameters to create an inter-
mediate level of embedding, which is also needed for each
experts, resulting in overﬁtting for many parameters. We
experimented with different number of layers and hidden
layer dimensions for the MLP structure, but 4096 dimen-
sions of 3 layers showed the highest performance. Unusual
is that, when layer normalization was applied, the MLP
model showed improved performance unlike LSTM, and
did not require more delicate hyperparameter tuning.

3.2.3 Label Processing Layer

To select the best method for Cθ, we ﬁx the remaining com-
ponent gθ with Indirect Clustering, hθ using the MoE-16
model and Lc using the cross entropy loss. As deﬁned
in Equation 20, the Label Processing Layer Cθ uses pre-
computed correlation matrix Mc. α, β, and γ values can be
used to control the degree to which the correlation matrix
affects the class score. An experiment is shown in Table 4.
As a result, performance dropped for all β and γ values
greater than zero. In other words, when using the label cor-
relation matrix for the purpose of score update, it showed
rather low classiﬁcation performance.

The possible reason for the result may be that our model
is too naive to reﬂect the correlation label prior and that
label correlation in the YouTube-8M Dataset is not strong
enough to improve the performance of the classiﬁcation
task.

3.2.4 Loss Function

To select the most suitable loss for Lc, we ﬁx the remaining
component gθ with Indirect Clustering, hθ uses the MoE-16
model, and Cθ is not used.

Table 5 shows the performance depending on whether
or not the center loss and the pseudo-huber loss are used.

7

LCE is the cross entropy loss, Lc is the center loss, and
HuberCE is the Pseudo-Huber loss for cross entropy term.
The results show that, ﬁrstly, using the center loss term
Lc gives little performance gain. Considering that we ap-
ply the center loss term to the multi-label, the performance
should be improved if the labels are correlated with each
other. Here we get another insight for the correlation label
prior, which is that the correlation does not exist as strongly
as to achieve performance improvement. Second, Huber
Loss proved its ability to cover noise data somewhat from
the YouTube video annotation system4 by recording a rela-
tively clear performance improvement.

3.3. Ensemble Model

Based on the experiments we conducted in section 3.2,
we recorded test performance of 0.820 as a single model
combining LSTM-M-O, MoE-2, and HuberLoss.(Due to
GPU memory limitation, we could not apply MoE-16 or
Multi Layer MLP model to LSTM based model) In addi-
tion, our ensemble model, which is a simple average of
the scores of several models, showed a test performance of
0.839 and ranked 8th in the kaggle challenge. The interest-
ing thing is that when we assemble several models, we did
not get a signiﬁcant performance increase if we combine
the better single models. Rather, for increasing ensemble
model’s performance, the models incorporated should be as
diverse as possible.

4. Conclusion

We deﬁned three issues that need to be covered in order
to solve the YouTube-8M Video Classiﬁcation task, and we
divided the model pipeline into four components and exper-
imented with the various structures to solve issues in each
component. As a result, almost all of the deformed struc-
tures tried to perform better than the baseline, and their en-
semble model recorded 0.839 in the test performance and
8th in the kaggle challenge. We also provided insights on
the structures we tried on each component, on what roles
each structure plays, and why they work well or poor. Based
on this insight, we will explore ways of better frame encod-
ing or use more elegant label correlation priorities as our

4https://www.youtube.com/watch?v=wf 77z1H-vQ

[20] Y. Yu, H. Ko, J. Choi, and G. Kim. Video captioning and
arXiv preprint

retrieval models with semantic attention.
arXiv:1610.02947, 2016.

[21] L. Zhu, Z. Xu, Y. Yang, and A. G. Hauptmann. Uncovering
temporal context for video question and answering. arXiv
preprint arXiv:1511.04670, 2015.

future work.

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m: A
large-scale video classiﬁcation benchmark. arXiv preprint
arXiv:1609.08675, 2016.

[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization.

arXiv preprint arXiv:1607.06450, 2016.

[3] A. C. de Carvalho and A. A. Freitas. A tutorial on multi-label
classiﬁcation techniques. In Foundations of Computational
Intelligence. Springer, 2009.

[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009.

[5] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell,
and M. Rohrbach. Multimodal compact bilinear pooling
for visual question answering and visual grounding. arXiv
preprint arXiv:1606.01847, 2016.

[6] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke,
A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous,
B. Seybold, et al. Cnn architectures for large-scale audio
classiﬁcation. arXiv preprint arXiv:1609.09430, 2016.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 1997.

[8] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of ex-
perts and the em algorithm. Neural computation, 1994.
[9] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014.

[10] Y. Kim. Convolutional neural networks for sentence classiﬁ-

cation. arXiv preprint arXiv:1408.5882, 2014.

[11] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014.

[12] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010.

[13] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012.

[14] S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end mem-

ory networks. In NIPS, 2015.

[15] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016.

[16] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urta-
sun, and S. Fidler. Movieqa: Understanding stories in movies
through question-answering. In CVPR, 2016.

[17] H. Wang, Z. Li, X. Ji, and Y. Wang. Face r-cnn. arXiv

preprint arXiv:1706.01061, 2017.

[18] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In ECCV,
2016.

[19] J. Xu, T. Mei, T. Yao, and Y. Rui. Msr-vtt: A large video de-
scription dataset for bridging video and language. In CVPR,
2016.

8

7
1
0
2
 
l
u
J
 
2
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
6
9
7
0
.
6
0
7
1
:
v
i
X
r
a

Encoding Video and Label Priors for Multi-label Video Classiﬁcation
on YouTube-8M dataset

Seil Na
Seoul National University
seil.na@vision.snu.ac.kr

YoungJae Yu
Seoul National University
yj.yu@vision.snu.ac.kr

Sangho Lee
Seoul National University
sangho.lee@vision.snu.ac.kr

Jisung Kim
SK Telecom Video Tech. Lab
joyful.kim@sk.com

Gunhee Kim
Seoul National University
gunhee@snu.ac.kr

Abstract

YouTube-8M is the largest video dataset for multi-label
video classiﬁcation. In order to tackle the multi-label classi-
ﬁcation on this challenging dataset, it is necessary to solve
several issues such as temporal modeling of videos, label
imbalances, and correlations between labels. We develop
a deep neural network model, which consists of four com-
ponents: Video Pooling Layer, Classiﬁcation Layer, Label
Processing Layer, and Loss Function. We introduce our
newly proposed methods and discusses how existing mod-
els operate in the YouTube-8M Classiﬁcation Task, what in-
sights they have, and why they succeed (or fail) to achieve
good performance. Most of the models we proposed are
very high compared to the baseline models, and the ensem-
ble of the models we used is 8th in the Kaggle Competition.

1. Introduction

Many challenging problems have been studied in com-
puter vision research toward video understanding, such as
video classiﬁcation [9, 13], video captioning [19, 20], video
QA [21], and MovieQA [16], to name a few. YouTube-
8M [1] is the largest video dataset for multi-label video
classiﬁcation. Its main problem is to predict the most rele-
vant labels for a given video out of 4,716 predeﬁned classes.
Therefore, it requires jointly solving two important prob-
lems; video classiﬁcation and multi-label classiﬁcation.

From the view of the video classiﬁcation, YouTube-8M
is challenging in that it covers more general classes like soc-
cer, game, vehicle, and food, while existing video classiﬁ-
cation datasets focus on more speciﬁc class groups, such
as sports in Sports-1M [9], and actions in UCF-101 [13].
Therefore, unlike the importance of modeling motion fea-

tures in UCF-101 [13] or Sports-1M [9], it is important
to capture more generic video information(e.g. temporal
encoding method for video, audio feature modeling) in
YouTube-8M.

From the view of multi-label classiﬁcation, the key is-
sues to solve in YouTube-8M are label imbalances and cor-
relations between labels. YouTube-8M involves 4,716 class
labels, and the number of videos belonging to each class is
signiﬁcantly different, which causes a label imbalance is-
sue that the classiﬁer ﬁts to the biased data. At the same
time, many classes are closely related one another, such
as {Football, Kick, Penalty kick, Indoor soccer} or {Super
Mario Bros, Super Mario World, Super Mario bros 3, Mario
Kart, Mario Kart 8}. It is also challenging to resolve the
correlations between labels to decide ﬁnal prediction.

Based on the challenges of the multi-label video classiﬁ-
cation task on YouTube-8M described the above, we focus
on addressing i) temporal encoding for video, ii) relieving
the label imbalance problem, and iii) utilizing the correlated
label information. Our model consists of four components:
i) video pooling layer, ii) classiﬁcation layer, iii) label pro-
cessing layer, and iv) loss function. The proposed com-
ponents indeed show signiﬁcant performance improvement
over the baseline models of YouTube-8M [1], and ﬁnally
our ensemble model is ranked 8th in the Google Cloud &
YouTube-8M Video Understanding Challenge 1.

2. The Model

Figure 1 shows the overall pipeline of our model. We
ﬁrst present video features we used, and then explain its
four key components in the following sections.

1https://www.kaggle.com/c/youtube8m/

leaderboard. (Team name: SNUVL X SKT)

1

2.1. Video Features

The inputs of the model are frame features and audio
features of a video clip. The frame features are obtained by
sampling a clip at 1-second interval, and extracting 2,048-
dimensional vector from every frame through Inception-V3
Network [15] pretrained to ImageNet [4]. Then the feature
is reduced to an 1,024-dimension via PCA (+ whitening),
then quantized, and ﬁnally L2-normalized. As a result, for
a video of T seconds long, its frame is If ∈ RT ×1,024. The
audio features re extracted using the VGG-inspired acoustic
model [6] followed by L2-normalization, which is denoted
by Ia ∈ RT ×128.

As the input of our model, we concatenate the frame fea-
ture If and the audio feature Ia at every time step, denoted
by I ∈ RT ×1,152. We test the compact bilinear pooling [5]
with various dimensions between If and Ia, but all of them
have signiﬁcantly lower performance than the simple con-
catenation. From now on, we use I to denote the input fea-
tures of a video over all frames, and It ∈ R1,152 as t-th
frame vector.

2.2. Video Pooling Layer

The Video Pooling Layer gθ : RT ×1,152 → Rd is de-
ﬁned as a parametric function that encodes a sequence of
T feature vectors I into a d-dimensional embedding vector.
We test ﬁve different encoding structures as follows.

2.2.1 A Variant of LSTM

The Long Short Term Memory (LSTM) model [7] is one of
the most popular frameworks for modeling sequence data.
We use a variant of the LSTM as follows:

it = σ(ItUi + st−1Wi + bi)
ft = σ(ItU f + st−1Wf + bf )
ot = σ(ItU o + st−1Wo + bo)
gt = tanh(ItUg + st−1Wg + bg)
ct = ct−1 ◦ ft + gt ◦ it
st = tanh(ct) ◦ ot

(1)

(2)

(3)

(4)

(5)

(6)

where t denotes each time step, i, f , o are the input, forget,
output gate, ct, st are long-term and short-term memory re-
spectively.

The baseline model uses only the ﬁnal hidden states of
the LSTM (i.e. cT , sT ), but we additionally exploit the fol-
lowing two states in order to extract as much information as
possible from the LSTM: i) the state Ml = (cid:80)T
t=1 It: the
summation of the input feature It of each time step, and ii)
the state Ol = (cid:80)T
t=1 gt: the summation of the output of
each time step of the LSTM. We concatenate Ml and Ol
with cT and sT . That is, if the cell size of the LSTM is
d, the output gθ of the baseline that uses cT , sT becomes

2

Figure 1. A complete pipeline illustration of our model for
YouTube-8M classiﬁcation. The input of the model concatenates
the frame feature If and audio feature Ia, which is denoted by I.
We divided the model into four components (Video Pooling Layer,
Classiﬁcation Layer, Label Processing Layer, Loss Function) and
experimented with the newly proposed model or variants of previ-
ously proposed model in each component. Each of the four com-
ponents is denoted by (gθ, hθ, Cθ, L), and each output of ﬁrst 3
layer is denoted by (Og, Oh, Oc).

the matrix L ∈ RT ×1152, Position Encoding is simply de-
ﬁned as follows.

I ← I (cid:12) L

Og =

T
(cid:88)

t=1

It

(8)

(9)

where Lij = (1 − i/T ) − (j/1152)(1 − 2 × i/T ), (cid:12) means
element-wise multiplication. After applying Position En-
coding, we used summation of each vector as output of gθ,
that menas Og is d-dimensional vector where d is 1,152.

2.2.4 Self-Attention: Indirect Clustering

The YouTube-8M dataset deals with general topics (e.g.
soccer, game, car, animation) rather than relatively focused
labels like Sports-1M or UCF-101. We here test the follow-
ing hypothesis: since the topic is highly general, it may be
more advantageous to focus on the most dominant parts of
the video rather than temporal/motion information of indi-
vidual frames. Therefore, we suggest an indirect clustering
model using the self-attention mechanism as follows.

We perform a clustering on the video features over all
frames I ∈ RT ×1,152, and ﬁnd the cluster with the largest
size (i.e. the largest number of elements in the cluster).
Then, the vectors in this cluster may represent the main
scene of the video. However, since it takes very long time
to perform clustering on each video, we propose a self-
attention mechanism that acts like clustering as follows.

pt = softmax(

ItIi)

(10)

T
(cid:88)

i=1

where pt is a scalar value that indicates the soft attention to
a frame t while the softmax is applied over p1..T . That is,
the more similar the frame vector is to the other vectors, the
higher its pt value is (i.e. it is more likely to be the main
scene of the video). Finally, the frame encoding is simply
obtained by a weighted sum of frame vectors by the atten-
tion values: gθ = (cid:80)T

t=1 ptIt.

2.2.5 Adaptive Noise

Each of 4,716 classes in the YouTube-8M dataset has a dif-
ferent number of video examples. For example, the Car
class has about 800,000 examples, but the Air Gear class
has only 101 examples. Let n to be the number of labels
associated with a video I; we introduce the adaptive noise
structure to relieve the label imbalance problem as follows.

It ← It + γ · Z

where Z ∼ N (0, I), γ =

(11)

(12)

1
n

·

n
(cid:88)

i=1

1
S(yi)

Figure 2. The ﬁrst component Video Pooling Layer gθ of the en-
It takes T frame vector I as input and outputs d-
tire model.
dimensional vector using different encoding methods respectively.

a (2 × d)-dimensional vector, whereas our model that ad-
ditionally uses Ml and Ol has the output of a (4 × d)-
dimensional vector. Experimentally, we choose the LSTM
cell size d = 1,152. We also apply the layer normaliza-
tion [2] to each layer of the LSTM for fast convergence,
and also use the dropout with dropout rate=0.8 to increase
the generalization capacity.

2.2.2 CNNs

Convolutional neural networks (CNNs) are often used to
jointly capture spatial information from images or video in
many computer vision tasks. That is, the convolution ker-
nels generate output signals considering all the elements in
the window together, and thus they effectively work with
spatially or temporally sequential information (e.g. images,
consecutive characters in NLP, and audio understanding).
As the second candidate of Video Pooling Layer, we use the
CNN to capture temporal information of video as proposed
in [10]:

oc = ReLU(conv(I, wconv, bconv))

(7)

where conv(input, ﬁlter, bias) indicates convolution layer
with stride 1, and ReLU indicates the element-wise ReLU
activation [12]. wconv ∈ Rcv×ch×1×d is a convolution ﬁlter
with the vertical and horizontal ﬁlter size of cv = 5, ch =
1152, and bconv ∈ Rd is a bias. oc ∈ R(T −cv+1)×d indi-
cates the output of the convolution layer. Finally, we apply
max-pooling over time for oc, obtaining the d-dimensional
encoding of gθ.

2.2.3 Position Encoding

We also test the Position Encoding scheme [14] that assigns
different weights to each frame vector. That is, we deﬁne

3

Figure 3. (a)The second component Classiﬁcation Layer hθ of the overall model. It take the d-dimensional vector as input Og and have
score Oh for 4,716 classes as output. However, input of Many-to-Many model is I rather than Og. (b)the 3rd component Label Processing
Layer Cθ of the entire model. It uses label correlation prior to update the score for each class.

where noise Z is sampled from normal distribution, yi de-
notes the i-th label for video, and S(yi) is the number of
video examples that have label yi. It means that we increase
the generalization for small classes by adding more noise to
their frame vectors. We then make summation of the vec-
tors of all frames as the output of gθ, as done in the position
encoding.

2.3.2 Variants of Mixture of Experts

The Mixtures of Experts [8] model is a binary classiﬁer that
adaptively takes into account the scores of several experts
corresponding to a class. For one class, each expert ei has
a probability value between 0 and 1, and gate gi represents
the weight for each expert and is deﬁned as follows.

2.3. Classiﬁcation Layer

Classiﬁcation Layer hθ is deﬁned as follows.

(cid:40)

hθ : RT ×1152 → R4,716,
hθ : Rd → R4,716,

if hθ is Many-to-Many
otherwise

(13)
That is, by default, hθ takes frame encoding Og as an in-
put and outputs score Oh for 4,716 classes. Exceptionally,
as shown in ﬁgure 1, the Many-to-Many model has I as its
input, where I is untouched by video pooling layer gθ. We
have conducted experiments on the following three struc-
tures in this component. (see ﬁgure 3(a))

2.3.1 Many-to-Many

e Og + be)

ei = σ(wT
gi = softmax(wT

g Og + bg)

(14)

(15)

where we, wg are d-dimensional vectors, scalars be, bg
are biases, and softmax is performed for {g1, ..., gE}. (E is
number of experts.) We extend this MoE model to multi-
layer, and construct multiple fully-connected of deﬁning
probability and gate distribution of each expert. For exam-
ple, the gate distribution and the expert distribution of the
2-layer MoE model are deﬁned as follows.

e (WeOg + b(cid:48)

ei = σ(vT
gi = softmax(vT

e) + be)
g (WgOg + b(cid:48)

g) + bg)

(16)

(17)

Unlike other models, Many-to-Many model has a frame
vector I that is not touched by video pooling layer gθ. It uses
LSTM similar to that of 2.2.1, but it calculates the score by
attaching fully-connected layer to each output of each step
in LSTM and average them, which is used as output Oh.
Since this model averages out the score in each frame, it
has the temporal encoding ability of the RNN as well as the
fact that the scores drawn in the more frequently appearing
frames are reﬂected more. As a result, more effective video
encoding could be performed.

where each weight matrix We, Wg is denoted by
We ∈ Rd(cid:48)×d, Wg ∈ Rd(cid:48)×d and each vT
g means d(cid:48)-
e , vT
dimensional vector. be, bg, b(cid:48)
g are biases. Finally, the
score Oh[i] for i-th class is determined by the weighted sum
of each expert distribution; Oh[i] = (cid:80)k

e, b(cid:48)

i=1 eigi.

2.3.3 Multi Layer Perceptron

The Multi Layer Perceptron model
is stack of Fully-
Connected Layers, one of the most basic Neural Network

4

structures. We experimentally set the number of layers to 3
and apply Layer Normalization [2] to each layer.

2.4. Label Processing Layer

Label Processing Layer Cθ is deﬁned as follows.

Where N denotes the number of labels in one video. ei
denotes a embedding vector from penultimate layer, ck de-
notes the yi-th corresponding class center for each ei. The
Ls is the cross-entropy loss, Lc is the center loss. A scalar
λ is a hyperparameter for balancing the two loss functions.

Cθ : R4,716 → R4,716

(18)

2.5.2 Pseudo-Huber Loss

This component is designed to reﬂect the correlation be-
tween the labels into the model. For example, YouTube-8M,
which includes {Soccer, Football, Kick, Indoor soccer} and
{Super Mario Bros, Super Mario World, Super Mario bros
3, Mario Kart, Mario Kart 8}. In order to take advantage of
this property, we set up the label correlation matrix Mc as
follows by counting all the videos in the training set. (See
Figure 3(b))

(19)

Mc ∈ R4,716×4,716
where Mc is correlation matrix and the correlation value
Mc[i, j] between the i-th label and the j-th label is calcu-
lated higher as i-th label and j-th label appear together more
in the same video. Then, a new score is deﬁned as follows to
better reﬂect the correlation between labels through simple
linear combination of matrix-vector multiplication.

Oc = α · Oh + β · McOh + γ · M(cid:48)
Here, Mc is used as a ﬁxed value, and M(cid:48)

cOh
c is a trainable
parameter initialized to the same value as Mc. Scalar values
α, β, γ are hyperparameters for model.

(20)

2.5. Loss Function

2.5.1 Center Loss

Center Loss was ﬁrst proposed for face recognition task [18]
and expanded to other ﬁeld because of its effectiveness for
making dicriminative embedding feature [17]. The purpose
of the center loss is to minimize intra-class variations while
maximizing inter-class variations using the joint supervi-
sion of cross-entropy loss and center loss. The original cen-
ter loss was used for the single label classiﬁcation problem
and it is hard to exploit it in a multi-label classiﬁcation prob-
lem.

If we convert the multi label problem into a single la-
bel problem like [3], increment of centers according to the
combination of labels is the simple expansion of center loss
to a multi-label classiﬁcation problem. However, this sim-
ple expansion is not suitable for YouTube-8M, because the
number of combination for labels is too big to calculate.
Therefore, we modiﬁed the center loss to suit the multi-label
classiﬁcation problem as follow.

Lc =

1
N

N
(cid:88)

i=1

L = Ls + λLc.

(cid:107)ei − ck(cid:107)2

2 , k = yi

(21)

(22)

5

The Huber Loss is a combination of L2 Loss and L1 Loss,
which allows the model to be trained more robustly on noise
instances. In the case of the YouTube-8M, there are classes
with very few instances due to the label imbalance problem,
and Huber Loss is designed to better learn instances belong-
ing to these classes. For a simple, differentiable form, we
use the Pseudo-Huber Loss function, a smooth approxima-
tion of the Huber Loss, as follows.

Lc = δ2((cid:112)1 + (LCE/δ)2 − 1)

(23)

where LCE means that Cross-Entropy Loss between our
prediction Oc and ground-truth label y, δ means hyperpa-
rameter for model.

2.6. Training

For training our model, we choose the Adam [11] opti-
mizer using a batch size of 128, with learning rate = 0.0006,
β1 = 0.9, β2 = 0.999, (cid:15) = 1e − 8. We also apply learning
rate decay with the rate = 0.95 for every 1.5M iterations. We
train the model for 5 epochs with no early stopping. We use
both ofﬁcial training and validation data that YouTube-8M
public dataset provides for training our models.

3. Experiments

We use the test data from the Kaggle competition:
Google Cloud & YouTube-8M Video Understanding Chal-
lenge to measure the performance of the model. The source
for our model is publicly available2.

3.1. Experimental Setting

One of the greatest features of our model is that we
have three issues to solve the YouTube-8M classiﬁca-
tion task;
i)temporal encoding for video, ii)label imbal-
ance problem, iii)correlation between labels, and we have
tried several variations on each component by dividing the
model pipeline into four components; i)Frame Encoding,
ii)Classiﬁcation Layer, iii)Label Processing Layer, iv)Loss
Function. In addition, except in the Many-to-Many model
in our pipeline, each component is completely independent
of one another, so it is a very good structure for experiment-
ing with a number of variant model combinations. How-
ever, it is impossible to do brute-force experiments because
the number of trials is too great to test all combinations of

2https://github.com/seilna/youtube-8m

Method
LSTM
LSTM-M
LSTM-M-O
LSTM-M-O-LN
CNN-64
CNN-256
CNN-1024
Position Encoding
Indirect Clustering
Adaptive Noise

GAP@20
0.811
0.815
0.820
0.815
0.704
0.753
-
0.782
0.801
0.782

Table 1. The results on various transformation structures in Video
Pooling Layer gθ component.

variations that each component can try. Thus, we took the
greedy approach, ﬁxed the remaining 3 components in each
component, and experimented the several structures only
for that component, choosing the best-performing structure
for each component. We used Google Average Precision3
(GAP)@20 as a metric to measure the performance of the
model.

3.2. Quantitative Results

3.2.1 Video Pooling Layer

There are ﬁve transform structures in Video Pooling Layer
gθ; i) variants of LSTM, ii) CNN, iii) Position Encoding, iv)
Indirect Clustering, v) Adaptive Noise. To select the most
suitable structure for gθ, we ﬁxed the rest of the component
hθ to the MoE-2 model, not Cθ used, and Lc used the cross
entropy loss. Based on these settings, the results for each of
the ﬁve structures are shown in table 1.

LSTM simply uses the last hidden state, LSTM-M con-
catenates Ml, LSTM-M-O concatenates both Ml and Ol,
and LSTM-M-O-LN is a model that applies Layer Normal-
ization for each layer of LSTM. CNN-64, CNN-256, and
CNN-1024 refer to models with 64, 256, and 1024 as the
output channels of CNN, respectively.

The results show that the LSTM method has the best per-
formance for encoding frames. Within LSTM, the higher
the utilization of the internal information, the higher the
performance, indicating that higher performance can be ex-
pected if more information can be extracted from other
methods such as skip connection in LSTM. On the other
hand, unlike the expectation, the LSTM with Layer Nor-
malization has a lower performance than that which is not.
Of course, Layer Normalization had the advantage of stable
and fast convergence even when 20-30 times learning rate
was applied, but when compared to the ﬁnal performance
alone, performance was not good.

CNN showed a very poor performance unexpectedly. As

3https://www.kaggle.com/c/youtube8m#evaluation

Method
Many-to-Many
2 Layer MoE-2
2 Layer MoE-16
3 Layer MLP-4096
3 Layer MLP-4096-LN

GAP@20
0.791
0.424
0.421
0.802
0.809

Table 2. The results on various transformation structures in Clas-
siﬁcation Layer hθ component.

CNN’s output channel increased, performance was on the
rise, but channels larger than 256 were not experimented
because of the memory limitations of the GPU. While we
can expect that CNN will perform well for larger chan-
nels, another problem with CNN is the dramatic increase in
computation cost as channels increase. However, if CNNs
with different hyperparameters record similar performance
to LSTM, CNN is likely to be used as good pooling method
because it has the advantage that the convolution operation
is fully parallelizable.

The Position Encoding model showed lower perfor-
mance than LSTM, and one of the possible reasons is that
the sequence modeling power of the model is weaker than
LSTM.

The Indirect Clustering model has lower performance
than LSTM, but it performs better than the Position En-
coding model. This suggests that the assumptions we have
made (the importance of the main scene in video classi-
ﬁcation) are not entirely wrong, suggesting that we need
a model that can cover this issue more delicately. Also,
temporal encoding of LSTM is as important as considering
main scene in video classiﬁcation.

Adaptive Noise do not lead to a signiﬁcant performance
improvement, indicating that a more sophisticated approach
to the label imbalance problem is required.

3.2.2 Classiﬁcation Layer

The Classiﬁcation Layer hθ has three variants:
i)Many-
to-Many, ii)Multi-Layer MoE, iii)MLP. To select the best
method for hθ, we ﬁx the remaining component gθ with In-
direct Clustering, Cθ is not used, and Lc uses cross entropy
loss. (However, in the many-to-many model, gθ is not used
according to the deﬁnition.) Based on these settings, the
results for each of the three structures are shown in Table 3.
2 Layer MoE-2 refers to a 2 layer MoE model with
2 experts, and 2 Layer MoE-16 refers to a 2 layer MoE
model with 16 experts. 3 layer MLP-4096 is MLP
structure with 4096 dimension of each hidden layer and
3 hidden layers, and 3 layer MLP-4096-LN applies
layer normalization to each layer.

In constructing the classiﬁcation layer, the MLP struc-
ture showed the best performance among the three meth-

6

Method
MoE - (1.0, 0.3, 0.0)
MoE - (1.0, 0.1, 0.0)
MoE - (1.0, 0.0, 0.1)
MoE - (1.0, 0.01, 0.0)
MoE - (1.0, 0.0, 0.01)
MoE - (1.0, 0.01, 0.01)

GAP@20
0.784
0.787
0.788
0.790
0.790
0.788

Method
LCE
LCE + Lc(λ = 0.001)
HuberCE(δ = 0.5)
HuberCE(δ = 1.0)
HuberCE(δ = 2.0)
HuberCE(δ = 3.0)

GAP@20
0.798
0.799
0.803
0.801
0.798
0.794

Table 3. The results on various hyperparameters in Label Process-
ing Layer Cθ component.

Table 4. The results on variants in loss function Lc component.

ods. The Many-to-Many model is an LSTM-based model,
but has a lower performance than the basic LSTM structure
in Table 2, indicating that the Many-to-Many framework in
video classiﬁcation is not always a good choice.

On the other hand, disappointingly, the Multi Layer MoE
model showed severe overﬁtting in only two layers. As ex-
pected, this requires a lot of parameters to create an inter-
mediate level of embedding, which is also needed for each
experts, resulting in overﬁtting for many parameters. We
experimented with different number of layers and hidden
layer dimensions for the MLP structure, but 4096 dimen-
sions of 3 layers showed the highest performance. Unusual
is that, when layer normalization was applied, the MLP
model showed improved performance unlike LSTM, and
did not require more delicate hyperparameter tuning.

3.2.3 Label Processing Layer

To select the best method for Cθ, we ﬁx the remaining com-
ponent gθ with Indirect Clustering, hθ using the MoE-16
model and Lc using the cross entropy loss. As deﬁned
in Equation 20, the Label Processing Layer Cθ uses pre-
computed correlation matrix Mc. α, β, and γ values can be
used to control the degree to which the correlation matrix
affects the class score. An experiment is shown in Table 4.
As a result, performance dropped for all β and γ values
greater than zero. In other words, when using the label cor-
relation matrix for the purpose of score update, it showed
rather low classiﬁcation performance.

The possible reason for the result may be that our model
is too naive to reﬂect the correlation label prior and that
label correlation in the YouTube-8M Dataset is not strong
enough to improve the performance of the classiﬁcation
task.

3.2.4 Loss Function

To select the most suitable loss for Lc, we ﬁx the remaining
component gθ with Indirect Clustering, hθ uses the MoE-16
model, and Cθ is not used.

Table 5 shows the performance depending on whether
or not the center loss and the pseudo-huber loss are used.

7

LCE is the cross entropy loss, Lc is the center loss, and
HuberCE is the Pseudo-Huber loss for cross entropy term.
The results show that, ﬁrstly, using the center loss term
Lc gives little performance gain. Considering that we ap-
ply the center loss term to the multi-label, the performance
should be improved if the labels are correlated with each
other. Here we get another insight for the correlation label
prior, which is that the correlation does not exist as strongly
as to achieve performance improvement. Second, Huber
Loss proved its ability to cover noise data somewhat from
the YouTube video annotation system4 by recording a rela-
tively clear performance improvement.

3.3. Ensemble Model

Based on the experiments we conducted in section 3.2,
we recorded test performance of 0.820 as a single model
combining LSTM-M-O, MoE-2, and HuberLoss.(Due to
GPU memory limitation, we could not apply MoE-16 or
Multi Layer MLP model to LSTM based model) In addi-
tion, our ensemble model, which is a simple average of
the scores of several models, showed a test performance of
0.839 and ranked 8th in the kaggle challenge. The interest-
ing thing is that when we assemble several models, we did
not get a signiﬁcant performance increase if we combine
the better single models. Rather, for increasing ensemble
model’s performance, the models incorporated should be as
diverse as possible.

4. Conclusion

We deﬁned three issues that need to be covered in order
to solve the YouTube-8M Video Classiﬁcation task, and we
divided the model pipeline into four components and exper-
imented with the various structures to solve issues in each
component. As a result, almost all of the deformed struc-
tures tried to perform better than the baseline, and their en-
semble model recorded 0.839 in the test performance and
8th in the kaggle challenge. We also provided insights on
the structures we tried on each component, on what roles
each structure plays, and why they work well or poor. Based
on this insight, we will explore ways of better frame encod-
ing or use more elegant label correlation priorities as our

4https://www.youtube.com/watch?v=wf 77z1H-vQ

[20] Y. Yu, H. Ko, J. Choi, and G. Kim. Video captioning and
arXiv preprint

retrieval models with semantic attention.
arXiv:1610.02947, 2016.

[21] L. Zhu, Z. Xu, Y. Yang, and A. G. Hauptmann. Uncovering
temporal context for video question and answering. arXiv
preprint arXiv:1511.04670, 2015.

future work.

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m: A
large-scale video classiﬁcation benchmark. arXiv preprint
arXiv:1609.08675, 2016.

[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization.

arXiv preprint arXiv:1607.06450, 2016.

[3] A. C. de Carvalho and A. A. Freitas. A tutorial on multi-label
classiﬁcation techniques. In Foundations of Computational
Intelligence. Springer, 2009.

[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009.

[5] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell,
and M. Rohrbach. Multimodal compact bilinear pooling
for visual question answering and visual grounding. arXiv
preprint arXiv:1606.01847, 2016.

[6] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke,
A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous,
B. Seybold, et al. Cnn architectures for large-scale audio
classiﬁcation. arXiv preprint arXiv:1609.09430, 2016.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 1997.

[8] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of ex-
perts and the em algorithm. Neural computation, 1994.
[9] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014.

[10] Y. Kim. Convolutional neural networks for sentence classiﬁ-

cation. arXiv preprint arXiv:1408.5882, 2014.

[11] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014.

[12] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010.

[13] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012.

[14] S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end mem-

ory networks. In NIPS, 2015.

[15] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016.

[16] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urta-
sun, and S. Fidler. Movieqa: Understanding stories in movies
through question-answering. In CVPR, 2016.

[17] H. Wang, Z. Li, X. Ji, and Y. Wang. Face r-cnn. arXiv

preprint arXiv:1706.01061, 2017.

[18] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In ECCV,
2016.

[19] J. Xu, T. Mei, T. Yao, and Y. Rui. Msr-vtt: A large video de-
scription dataset for bridging video and language. In CVPR,
2016.

8

7
1
0
2
 
l
u
J
 
2
1
 
 
]

V
C
.
s
c
[
 
 
2
v
0
6
9
7
0
.
6
0
7
1
:
v
i
X
r
a

Encoding Video and Label Priors for Multi-label Video Classiﬁcation
on YouTube-8M dataset

Seil Na
Seoul National University
seil.na@vision.snu.ac.kr

YoungJae Yu
Seoul National University
yj.yu@vision.snu.ac.kr

Sangho Lee
Seoul National University
sangho.lee@vision.snu.ac.kr

Jisung Kim
SK Telecom Video Tech. Lab
joyful.kim@sk.com

Gunhee Kim
Seoul National University
gunhee@snu.ac.kr

Abstract

YouTube-8M is the largest video dataset for multi-label
video classiﬁcation. In order to tackle the multi-label classi-
ﬁcation on this challenging dataset, it is necessary to solve
several issues such as temporal modeling of videos, label
imbalances, and correlations between labels. We develop
a deep neural network model, which consists of four com-
ponents: Video Pooling Layer, Classiﬁcation Layer, Label
Processing Layer, and Loss Function. We introduce our
newly proposed methods and discusses how existing mod-
els operate in the YouTube-8M Classiﬁcation Task, what in-
sights they have, and why they succeed (or fail) to achieve
good performance. Most of the models we proposed are
very high compared to the baseline models, and the ensem-
ble of the models we used is 8th in the Kaggle Competition.

1. Introduction

Many challenging problems have been studied in com-
puter vision research toward video understanding, such as
video classiﬁcation [9, 13], video captioning [19, 20], video
QA [21], and MovieQA [16], to name a few. YouTube-
8M [1] is the largest video dataset for multi-label video
classiﬁcation. Its main problem is to predict the most rele-
vant labels for a given video out of 4,716 predeﬁned classes.
Therefore, it requires jointly solving two important prob-
lems; video classiﬁcation and multi-label classiﬁcation.

From the view of the video classiﬁcation, YouTube-8M
is challenging in that it covers more general classes like soc-
cer, game, vehicle, and food, while existing video classiﬁ-
cation datasets focus on more speciﬁc class groups, such
as sports in Sports-1M [9], and actions in UCF-101 [13].
Therefore, unlike the importance of modeling motion fea-

tures in UCF-101 [13] or Sports-1M [9], it is important
to capture more generic video information(e.g. temporal
encoding method for video, audio feature modeling) in
YouTube-8M.

From the view of multi-label classiﬁcation, the key is-
sues to solve in YouTube-8M are label imbalances and cor-
relations between labels. YouTube-8M involves 4,716 class
labels, and the number of videos belonging to each class is
signiﬁcantly different, which causes a label imbalance is-
sue that the classiﬁer ﬁts to the biased data. At the same
time, many classes are closely related one another, such
as {Football, Kick, Penalty kick, Indoor soccer} or {Super
Mario Bros, Super Mario World, Super Mario bros 3, Mario
Kart, Mario Kart 8}. It is also challenging to resolve the
correlations between labels to decide ﬁnal prediction.

Based on the challenges of the multi-label video classiﬁ-
cation task on YouTube-8M described the above, we focus
on addressing i) temporal encoding for video, ii) relieving
the label imbalance problem, and iii) utilizing the correlated
label information. Our model consists of four components:
i) video pooling layer, ii) classiﬁcation layer, iii) label pro-
cessing layer, and iv) loss function. The proposed com-
ponents indeed show signiﬁcant performance improvement
over the baseline models of YouTube-8M [1], and ﬁnally
our ensemble model is ranked 8th in the Google Cloud &
YouTube-8M Video Understanding Challenge 1.

2. The Model

Figure 1 shows the overall pipeline of our model. We
ﬁrst present video features we used, and then explain its
four key components in the following sections.

1https://www.kaggle.com/c/youtube8m/

leaderboard. (Team name: SNUVL X SKT)

1

2.1. Video Features

The inputs of the model are frame features and audio
features of a video clip. The frame features are obtained by
sampling a clip at 1-second interval, and extracting 2,048-
dimensional vector from every frame through Inception-V3
Network [15] pretrained to ImageNet [4]. Then the feature
is reduced to an 1,024-dimension via PCA (+ whitening),
then quantized, and ﬁnally L2-normalized. As a result, for
a video of T seconds long, its frame is If ∈ RT ×1,024. The
audio features re extracted using the VGG-inspired acoustic
model [6] followed by L2-normalization, which is denoted
by Ia ∈ RT ×128.

As the input of our model, we concatenate the frame fea-
ture If and the audio feature Ia at every time step, denoted
by I ∈ RT ×1,152. We test the compact bilinear pooling [5]
with various dimensions between If and Ia, but all of them
have signiﬁcantly lower performance than the simple con-
catenation. From now on, we use I to denote the input fea-
tures of a video over all frames, and It ∈ R1,152 as t-th
frame vector.

2.2. Video Pooling Layer

The Video Pooling Layer gθ : RT ×1,152 → Rd is de-
ﬁned as a parametric function that encodes a sequence of
T feature vectors I into a d-dimensional embedding vector.
We test ﬁve different encoding structures as follows.

2.2.1 A Variant of LSTM

The Long Short Term Memory (LSTM) model [7] is one of
the most popular frameworks for modeling sequence data.
We use a variant of the LSTM as follows:

it = σ(ItUi + st−1Wi + bi)
ft = σ(ItU f + st−1Wf + bf )
ot = σ(ItU o + st−1Wo + bo)
gt = tanh(ItUg + st−1Wg + bg)
ct = ct−1 ◦ ft + gt ◦ it
st = tanh(ct) ◦ ot

(1)

(2)

(3)

(4)

(5)

(6)

where t denotes each time step, i, f , o are the input, forget,
output gate, ct, st are long-term and short-term memory re-
spectively.

The baseline model uses only the ﬁnal hidden states of
the LSTM (i.e. cT , sT ), but we additionally exploit the fol-
lowing two states in order to extract as much information as
possible from the LSTM: i) the state Ml = (cid:80)T
t=1 It: the
summation of the input feature It of each time step, and ii)
the state Ol = (cid:80)T
t=1 gt: the summation of the output of
each time step of the LSTM. We concatenate Ml and Ol
with cT and sT . That is, if the cell size of the LSTM is
d, the output gθ of the baseline that uses cT , sT becomes

2

Figure 1. A complete pipeline illustration of our model for
YouTube-8M classiﬁcation. The input of the model concatenates
the frame feature If and audio feature Ia, which is denoted by I.
We divided the model into four components (Video Pooling Layer,
Classiﬁcation Layer, Label Processing Layer, Loss Function) and
experimented with the newly proposed model or variants of previ-
ously proposed model in each component. Each of the four com-
ponents is denoted by (gθ, hθ, Cθ, L), and each output of ﬁrst 3
layer is denoted by (Og, Oh, Oc).

the matrix L ∈ RT ×1152, Position Encoding is simply de-
ﬁned as follows.

I ← I (cid:12) L

Og =

T
(cid:88)

t=1

It

(8)

(9)

where Lij = (1 − i/T ) − (j/1152)(1 − 2 × i/T ), (cid:12) means
element-wise multiplication. After applying Position En-
coding, we used summation of each vector as output of gθ,
that menas Og is d-dimensional vector where d is 1,152.

2.2.4 Self-Attention: Indirect Clustering

The YouTube-8M dataset deals with general topics (e.g.
soccer, game, car, animation) rather than relatively focused
labels like Sports-1M or UCF-101. We here test the follow-
ing hypothesis: since the topic is highly general, it may be
more advantageous to focus on the most dominant parts of
the video rather than temporal/motion information of indi-
vidual frames. Therefore, we suggest an indirect clustering
model using the self-attention mechanism as follows.

We perform a clustering on the video features over all
frames I ∈ RT ×1,152, and ﬁnd the cluster with the largest
size (i.e. the largest number of elements in the cluster).
Then, the vectors in this cluster may represent the main
scene of the video. However, since it takes very long time
to perform clustering on each video, we propose a self-
attention mechanism that acts like clustering as follows.

pt = softmax(

ItIi)

(10)

T
(cid:88)

i=1

where pt is a scalar value that indicates the soft attention to
a frame t while the softmax is applied over p1..T . That is,
the more similar the frame vector is to the other vectors, the
higher its pt value is (i.e. it is more likely to be the main
scene of the video). Finally, the frame encoding is simply
obtained by a weighted sum of frame vectors by the atten-
tion values: gθ = (cid:80)T

t=1 ptIt.

2.2.5 Adaptive Noise

Each of 4,716 classes in the YouTube-8M dataset has a dif-
ferent number of video examples. For example, the Car
class has about 800,000 examples, but the Air Gear class
has only 101 examples. Let n to be the number of labels
associated with a video I; we introduce the adaptive noise
structure to relieve the label imbalance problem as follows.

It ← It + γ · Z

where Z ∼ N (0, I), γ =

(11)

(12)

1
n

·

n
(cid:88)

i=1

1
S(yi)

Figure 2. The ﬁrst component Video Pooling Layer gθ of the en-
It takes T frame vector I as input and outputs d-
tire model.
dimensional vector using different encoding methods respectively.

a (2 × d)-dimensional vector, whereas our model that ad-
ditionally uses Ml and Ol has the output of a (4 × d)-
dimensional vector. Experimentally, we choose the LSTM
cell size d = 1,152. We also apply the layer normaliza-
tion [2] to each layer of the LSTM for fast convergence,
and also use the dropout with dropout rate=0.8 to increase
the generalization capacity.

2.2.2 CNNs

Convolutional neural networks (CNNs) are often used to
jointly capture spatial information from images or video in
many computer vision tasks. That is, the convolution ker-
nels generate output signals considering all the elements in
the window together, and thus they effectively work with
spatially or temporally sequential information (e.g. images,
consecutive characters in NLP, and audio understanding).
As the second candidate of Video Pooling Layer, we use the
CNN to capture temporal information of video as proposed
in [10]:

oc = ReLU(conv(I, wconv, bconv))

(7)

where conv(input, ﬁlter, bias) indicates convolution layer
with stride 1, and ReLU indicates the element-wise ReLU
activation [12]. wconv ∈ Rcv×ch×1×d is a convolution ﬁlter
with the vertical and horizontal ﬁlter size of cv = 5, ch =
1152, and bconv ∈ Rd is a bias. oc ∈ R(T −cv+1)×d indi-
cates the output of the convolution layer. Finally, we apply
max-pooling over time for oc, obtaining the d-dimensional
encoding of gθ.

2.2.3 Position Encoding

We also test the Position Encoding scheme [14] that assigns
different weights to each frame vector. That is, we deﬁne

3

Figure 3. (a)The second component Classiﬁcation Layer hθ of the overall model. It take the d-dimensional vector as input Og and have
score Oh for 4,716 classes as output. However, input of Many-to-Many model is I rather than Og. (b)the 3rd component Label Processing
Layer Cθ of the entire model. It uses label correlation prior to update the score for each class.

where noise Z is sampled from normal distribution, yi de-
notes the i-th label for video, and S(yi) is the number of
video examples that have label yi. It means that we increase
the generalization for small classes by adding more noise to
their frame vectors. We then make summation of the vec-
tors of all frames as the output of gθ, as done in the position
encoding.

2.3.2 Variants of Mixture of Experts

The Mixtures of Experts [8] model is a binary classiﬁer that
adaptively takes into account the scores of several experts
corresponding to a class. For one class, each expert ei has
a probability value between 0 and 1, and gate gi represents
the weight for each expert and is deﬁned as follows.

2.3. Classiﬁcation Layer

Classiﬁcation Layer hθ is deﬁned as follows.

(cid:40)

hθ : RT ×1152 → R4,716,
hθ : Rd → R4,716,

if hθ is Many-to-Many
otherwise

(13)
That is, by default, hθ takes frame encoding Og as an in-
put and outputs score Oh for 4,716 classes. Exceptionally,
as shown in ﬁgure 1, the Many-to-Many model has I as its
input, where I is untouched by video pooling layer gθ. We
have conducted experiments on the following three struc-
tures in this component. (see ﬁgure 3(a))

2.3.1 Many-to-Many

e Og + be)

ei = σ(wT
gi = softmax(wT

g Og + bg)

(14)

(15)

where we, wg are d-dimensional vectors, scalars be, bg
are biases, and softmax is performed for {g1, ..., gE}. (E is
number of experts.) We extend this MoE model to multi-
layer, and construct multiple fully-connected of deﬁning
probability and gate distribution of each expert. For exam-
ple, the gate distribution and the expert distribution of the
2-layer MoE model are deﬁned as follows.

e (WeOg + b(cid:48)

ei = σ(vT
gi = softmax(vT

e) + be)
g (WgOg + b(cid:48)

g) + bg)

(16)

(17)

Unlike other models, Many-to-Many model has a frame
vector I that is not touched by video pooling layer gθ. It uses
LSTM similar to that of 2.2.1, but it calculates the score by
attaching fully-connected layer to each output of each step
in LSTM and average them, which is used as output Oh.
Since this model averages out the score in each frame, it
has the temporal encoding ability of the RNN as well as the
fact that the scores drawn in the more frequently appearing
frames are reﬂected more. As a result, more effective video
encoding could be performed.

where each weight matrix We, Wg is denoted by
We ∈ Rd(cid:48)×d, Wg ∈ Rd(cid:48)×d and each vT
g means d(cid:48)-
e , vT
dimensional vector. be, bg, b(cid:48)
g are biases. Finally, the
score Oh[i] for i-th class is determined by the weighted sum
of each expert distribution; Oh[i] = (cid:80)k

e, b(cid:48)

i=1 eigi.

2.3.3 Multi Layer Perceptron

The Multi Layer Perceptron model
is stack of Fully-
Connected Layers, one of the most basic Neural Network

4

structures. We experimentally set the number of layers to 3
and apply Layer Normalization [2] to each layer.

2.4. Label Processing Layer

Label Processing Layer Cθ is deﬁned as follows.

Where N denotes the number of labels in one video. ei
denotes a embedding vector from penultimate layer, ck de-
notes the yi-th corresponding class center for each ei. The
Ls is the cross-entropy loss, Lc is the center loss. A scalar
λ is a hyperparameter for balancing the two loss functions.

Cθ : R4,716 → R4,716

(18)

2.5.2 Pseudo-Huber Loss

This component is designed to reﬂect the correlation be-
tween the labels into the model. For example, YouTube-8M,
which includes {Soccer, Football, Kick, Indoor soccer} and
{Super Mario Bros, Super Mario World, Super Mario bros
3, Mario Kart, Mario Kart 8}. In order to take advantage of
this property, we set up the label correlation matrix Mc as
follows by counting all the videos in the training set. (See
Figure 3(b))

(19)

Mc ∈ R4,716×4,716
where Mc is correlation matrix and the correlation value
Mc[i, j] between the i-th label and the j-th label is calcu-
lated higher as i-th label and j-th label appear together more
in the same video. Then, a new score is deﬁned as follows to
better reﬂect the correlation between labels through simple
linear combination of matrix-vector multiplication.

Oc = α · Oh + β · McOh + γ · M(cid:48)
Here, Mc is used as a ﬁxed value, and M(cid:48)

cOh
c is a trainable
parameter initialized to the same value as Mc. Scalar values
α, β, γ are hyperparameters for model.

(20)

2.5. Loss Function

2.5.1 Center Loss

Center Loss was ﬁrst proposed for face recognition task [18]
and expanded to other ﬁeld because of its effectiveness for
making dicriminative embedding feature [17]. The purpose
of the center loss is to minimize intra-class variations while
maximizing inter-class variations using the joint supervi-
sion of cross-entropy loss and center loss. The original cen-
ter loss was used for the single label classiﬁcation problem
and it is hard to exploit it in a multi-label classiﬁcation prob-
lem.

If we convert the multi label problem into a single la-
bel problem like [3], increment of centers according to the
combination of labels is the simple expansion of center loss
to a multi-label classiﬁcation problem. However, this sim-
ple expansion is not suitable for YouTube-8M, because the
number of combination for labels is too big to calculate.
Therefore, we modiﬁed the center loss to suit the multi-label
classiﬁcation problem as follow.

Lc =

1
N

N
(cid:88)

i=1

L = Ls + λLc.

(cid:107)ei − ck(cid:107)2

2 , k = yi

(21)

(22)

5

The Huber Loss is a combination of L2 Loss and L1 Loss,
which allows the model to be trained more robustly on noise
instances. In the case of the YouTube-8M, there are classes
with very few instances due to the label imbalance problem,
and Huber Loss is designed to better learn instances belong-
ing to these classes. For a simple, differentiable form, we
use the Pseudo-Huber Loss function, a smooth approxima-
tion of the Huber Loss, as follows.

Lc = δ2((cid:112)1 + (LCE/δ)2 − 1)

(23)

where LCE means that Cross-Entropy Loss between our
prediction Oc and ground-truth label y, δ means hyperpa-
rameter for model.

2.6. Training

For training our model, we choose the Adam [11] opti-
mizer using a batch size of 128, with learning rate = 0.0006,
β1 = 0.9, β2 = 0.999, (cid:15) = 1e − 8. We also apply learning
rate decay with the rate = 0.95 for every 1.5M iterations. We
train the model for 5 epochs with no early stopping. We use
both ofﬁcial training and validation data that YouTube-8M
public dataset provides for training our models.

3. Experiments

We use the test data from the Kaggle competition:
Google Cloud & YouTube-8M Video Understanding Chal-
lenge to measure the performance of the model. The source
for our model is publicly available2.

3.1. Experimental Setting

One of the greatest features of our model is that we
have three issues to solve the YouTube-8M classiﬁca-
tion task;
i)temporal encoding for video, ii)label imbal-
ance problem, iii)correlation between labels, and we have
tried several variations on each component by dividing the
model pipeline into four components; i)Frame Encoding,
ii)Classiﬁcation Layer, iii)Label Processing Layer, iv)Loss
Function. In addition, except in the Many-to-Many model
in our pipeline, each component is completely independent
of one another, so it is a very good structure for experiment-
ing with a number of variant model combinations. How-
ever, it is impossible to do brute-force experiments because
the number of trials is too great to test all combinations of

2https://github.com/seilna/youtube-8m

Method
LSTM
LSTM-M
LSTM-M-O
LSTM-M-O-LN
CNN-64
CNN-256
CNN-1024
Position Encoding
Indirect Clustering
Adaptive Noise

GAP@20
0.811
0.815
0.820
0.815
0.704
0.753
-
0.782
0.801
0.782

Table 1. The results on various transformation structures in Video
Pooling Layer gθ component.

variations that each component can try. Thus, we took the
greedy approach, ﬁxed the remaining 3 components in each
component, and experimented the several structures only
for that component, choosing the best-performing structure
for each component. We used Google Average Precision3
(GAP)@20 as a metric to measure the performance of the
model.

3.2. Quantitative Results

3.2.1 Video Pooling Layer

There are ﬁve transform structures in Video Pooling Layer
gθ; i) variants of LSTM, ii) CNN, iii) Position Encoding, iv)
Indirect Clustering, v) Adaptive Noise. To select the most
suitable structure for gθ, we ﬁxed the rest of the component
hθ to the MoE-2 model, not Cθ used, and Lc used the cross
entropy loss. Based on these settings, the results for each of
the ﬁve structures are shown in table 1.

LSTM simply uses the last hidden state, LSTM-M con-
catenates Ml, LSTM-M-O concatenates both Ml and Ol,
and LSTM-M-O-LN is a model that applies Layer Normal-
ization for each layer of LSTM. CNN-64, CNN-256, and
CNN-1024 refer to models with 64, 256, and 1024 as the
output channels of CNN, respectively.

The results show that the LSTM method has the best per-
formance for encoding frames. Within LSTM, the higher
the utilization of the internal information, the higher the
performance, indicating that higher performance can be ex-
pected if more information can be extracted from other
methods such as skip connection in LSTM. On the other
hand, unlike the expectation, the LSTM with Layer Nor-
malization has a lower performance than that which is not.
Of course, Layer Normalization had the advantage of stable
and fast convergence even when 20-30 times learning rate
was applied, but when compared to the ﬁnal performance
alone, performance was not good.

CNN showed a very poor performance unexpectedly. As

3https://www.kaggle.com/c/youtube8m#evaluation

Method
Many-to-Many
2 Layer MoE-2
2 Layer MoE-16
3 Layer MLP-4096
3 Layer MLP-4096-LN

GAP@20
0.791
0.424
0.421
0.802
0.809

Table 2. The results on various transformation structures in Clas-
siﬁcation Layer hθ component.

CNN’s output channel increased, performance was on the
rise, but channels larger than 256 were not experimented
because of the memory limitations of the GPU. While we
can expect that CNN will perform well for larger chan-
nels, another problem with CNN is the dramatic increase in
computation cost as channels increase. However, if CNNs
with different hyperparameters record similar performance
to LSTM, CNN is likely to be used as good pooling method
because it has the advantage that the convolution operation
is fully parallelizable.

The Position Encoding model showed lower perfor-
mance than LSTM, and one of the possible reasons is that
the sequence modeling power of the model is weaker than
LSTM.

The Indirect Clustering model has lower performance
than LSTM, but it performs better than the Position En-
coding model. This suggests that the assumptions we have
made (the importance of the main scene in video classi-
ﬁcation) are not entirely wrong, suggesting that we need
a model that can cover this issue more delicately. Also,
temporal encoding of LSTM is as important as considering
main scene in video classiﬁcation.

Adaptive Noise do not lead to a signiﬁcant performance
improvement, indicating that a more sophisticated approach
to the label imbalance problem is required.

3.2.2 Classiﬁcation Layer

The Classiﬁcation Layer hθ has three variants:
i)Many-
to-Many, ii)Multi-Layer MoE, iii)MLP. To select the best
method for hθ, we ﬁx the remaining component gθ with In-
direct Clustering, Cθ is not used, and Lc uses cross entropy
loss. (However, in the many-to-many model, gθ is not used
according to the deﬁnition.) Based on these settings, the
results for each of the three structures are shown in Table 3.
2 Layer MoE-2 refers to a 2 layer MoE model with
2 experts, and 2 Layer MoE-16 refers to a 2 layer MoE
model with 16 experts. 3 layer MLP-4096 is MLP
structure with 4096 dimension of each hidden layer and
3 hidden layers, and 3 layer MLP-4096-LN applies
layer normalization to each layer.

In constructing the classiﬁcation layer, the MLP struc-
ture showed the best performance among the three meth-

6

Method
MoE - (1.0, 0.3, 0.0)
MoE - (1.0, 0.1, 0.0)
MoE - (1.0, 0.0, 0.1)
MoE - (1.0, 0.01, 0.0)
MoE - (1.0, 0.0, 0.01)
MoE - (1.0, 0.01, 0.01)

GAP@20
0.784
0.787
0.788
0.790
0.790
0.788

Method
LCE
LCE + Lc(λ = 0.001)
HuberCE(δ = 0.5)
HuberCE(δ = 1.0)
HuberCE(δ = 2.0)
HuberCE(δ = 3.0)

GAP@20
0.798
0.799
0.803
0.801
0.798
0.794

Table 3. The results on various hyperparameters in Label Process-
ing Layer Cθ component.

Table 4. The results on variants in loss function Lc component.

ods. The Many-to-Many model is an LSTM-based model,
but has a lower performance than the basic LSTM structure
in Table 2, indicating that the Many-to-Many framework in
video classiﬁcation is not always a good choice.

On the other hand, disappointingly, the Multi Layer MoE
model showed severe overﬁtting in only two layers. As ex-
pected, this requires a lot of parameters to create an inter-
mediate level of embedding, which is also needed for each
experts, resulting in overﬁtting for many parameters. We
experimented with different number of layers and hidden
layer dimensions for the MLP structure, but 4096 dimen-
sions of 3 layers showed the highest performance. Unusual
is that, when layer normalization was applied, the MLP
model showed improved performance unlike LSTM, and
did not require more delicate hyperparameter tuning.

3.2.3 Label Processing Layer

To select the best method for Cθ, we ﬁx the remaining com-
ponent gθ with Indirect Clustering, hθ using the MoE-16
model and Lc using the cross entropy loss. As deﬁned
in Equation 20, the Label Processing Layer Cθ uses pre-
computed correlation matrix Mc. α, β, and γ values can be
used to control the degree to which the correlation matrix
affects the class score. An experiment is shown in Table 4.
As a result, performance dropped for all β and γ values
greater than zero. In other words, when using the label cor-
relation matrix for the purpose of score update, it showed
rather low classiﬁcation performance.

The possible reason for the result may be that our model
is too naive to reﬂect the correlation label prior and that
label correlation in the YouTube-8M Dataset is not strong
enough to improve the performance of the classiﬁcation
task.

3.2.4 Loss Function

To select the most suitable loss for Lc, we ﬁx the remaining
component gθ with Indirect Clustering, hθ uses the MoE-16
model, and Cθ is not used.

Table 5 shows the performance depending on whether
or not the center loss and the pseudo-huber loss are used.

7

LCE is the cross entropy loss, Lc is the center loss, and
HuberCE is the Pseudo-Huber loss for cross entropy term.
The results show that, ﬁrstly, using the center loss term
Lc gives little performance gain. Considering that we ap-
ply the center loss term to the multi-label, the performance
should be improved if the labels are correlated with each
other. Here we get another insight for the correlation label
prior, which is that the correlation does not exist as strongly
as to achieve performance improvement. Second, Huber
Loss proved its ability to cover noise data somewhat from
the YouTube video annotation system4 by recording a rela-
tively clear performance improvement.

3.3. Ensemble Model

Based on the experiments we conducted in section 3.2,
we recorded test performance of 0.820 as a single model
combining LSTM-M-O, MoE-2, and HuberLoss.(Due to
GPU memory limitation, we could not apply MoE-16 or
Multi Layer MLP model to LSTM based model) In addi-
tion, our ensemble model, which is a simple average of
the scores of several models, showed a test performance of
0.839 and ranked 8th in the kaggle challenge. The interest-
ing thing is that when we assemble several models, we did
not get a signiﬁcant performance increase if we combine
the better single models. Rather, for increasing ensemble
model’s performance, the models incorporated should be as
diverse as possible.

4. Conclusion

We deﬁned three issues that need to be covered in order
to solve the YouTube-8M Video Classiﬁcation task, and we
divided the model pipeline into four components and exper-
imented with the various structures to solve issues in each
component. As a result, almost all of the deformed struc-
tures tried to perform better than the baseline, and their en-
semble model recorded 0.839 in the test performance and
8th in the kaggle challenge. We also provided insights on
the structures we tried on each component, on what roles
each structure plays, and why they work well or poor. Based
on this insight, we will explore ways of better frame encod-
ing or use more elegant label correlation priorities as our

4https://www.youtube.com/watch?v=wf 77z1H-vQ

[20] Y. Yu, H. Ko, J. Choi, and G. Kim. Video captioning and
arXiv preprint

retrieval models with semantic attention.
arXiv:1610.02947, 2016.

[21] L. Zhu, Z. Xu, Y. Yang, and A. G. Hauptmann. Uncovering
temporal context for video question and answering. arXiv
preprint arXiv:1511.04670, 2015.

future work.

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. Youtube-8m: A
large-scale video classiﬁcation benchmark. arXiv preprint
arXiv:1609.08675, 2016.

[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization.

arXiv preprint arXiv:1607.06450, 2016.

[3] A. C. de Carvalho and A. A. Freitas. A tutorial on multi-label
classiﬁcation techniques. In Foundations of Computational
Intelligence. Springer, 2009.

[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009.

[5] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell,
and M. Rohrbach. Multimodal compact bilinear pooling
for visual question answering and visual grounding. arXiv
preprint arXiv:1606.01847, 2016.

[6] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke,
A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous,
B. Seybold, et al. Cnn architectures for large-scale audio
classiﬁcation. arXiv preprint arXiv:1609.09430, 2016.
[7] S. Hochreiter and J. Schmidhuber. Long short-term memory.

Neural computation, 1997.

[8] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of ex-
perts and the em algorithm. Neural computation, 1994.
[9] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014.

[10] Y. Kim. Convolutional neural networks for sentence classiﬁ-

cation. arXiv preprint arXiv:1408.5882, 2014.

[11] D. Kingma and J. Ba. Adam: A method for stochastic opti-

mization. arXiv preprint arXiv:1412.6980, 2014.

[12] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 2010.

[13] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012.

[14] S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end mem-

ory networks. In NIPS, 2015.

[15] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016.

[16] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urta-
sun, and S. Fidler. Movieqa: Understanding stories in movies
through question-answering. In CVPR, 2016.

[17] H. Wang, Z. Li, X. Ji, and Y. Wang. Face r-cnn. arXiv

preprint arXiv:1706.01061, 2017.

[18] Y. Wen, K. Zhang, Z. Li, and Y. Qiao. A discriminative fea-
ture learning approach for deep face recognition. In ECCV,
2016.

[19] J. Xu, T. Mei, T. Yao, and Y. Rui. Msr-vtt: A large video de-
scription dataset for bridging video and language. In CVPR,
2016.

8

