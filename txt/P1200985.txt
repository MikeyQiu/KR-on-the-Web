8
1
0
2
 
g
u
A
 
3
2
 
 
]
L
C
.
s
c
[
 
 
3
v
0
8
7
1
0
.
7
0
7
1
:
v
i
X
r
a

On the Role of Text Preprocessing in Neural Network Architectures:
An Evaluation Study on Text Categorization and Sentiment Analysis

Jose Camacho-Collados
School of Computer Science
and Informatics
Cardiff University
camachocolladosj@cardiff.ac.uk

Mohammad Taher Pilehvar
School of Computer Engineering
Iran University of
Science and Technology
pilehvar@iust.ac.ir

Abstract

Text preprocessing is often the ﬁrst step in
the pipeline of a Natural Language Process-
ing (NLP) system, with potential impact in
its ﬁnal performance. Despite its importance,
text preprocessing has not received much at-
tention in the deep learning literature. In this
paper we investigate the impact of simple text
preprocessing decisions (particularly tokeniz-
ing, lemmatizing, lowercasing and multiword
grouping) on the performance of a standard
neural text classiﬁer. We perform an extensive
evaluation on standard benchmarks from text
categorization and sentiment analysis. While
our experiments show that a simple tokeniza-
tion of input text is generally adequate, they
also highlight signiﬁcant degrees of variabil-
ity across preprocessing techniques. This re-
veals the importance of paying attention to this
usually-overlooked step in the pipeline, partic-
ularly when comparing different models. Fi-
nally, our evaluation provides insights into the
best preprocessing practices for training word
embeddings.

1 Introduction

Words are often considered as the basic con-
includ-
stituents of texts for many languages,
ing English.1
The ﬁrst module in an NLP
pipeline is a tokenizer which transforms texts
to sequences of words.
in prac-
tise, other preprocessing techniques can be (and
further used together with tokenization.
are)
lowercasing and
These include lemmatization,

However,

1Note that although word-based models are mainstream
in NLP in general and text classiﬁcation in particular, re-
cent work has also considered other linguistic units, such as
characters (Kim et al., 2016; Xiao and Cho, 2016) or word
senses (Li and Jurafsky, 2015; Flekova and Gurevych, 2016;
Pilehvar et al., 2017). These techniques require a different
kind of preprocessing and, while they have been shown ef-
fective in various settings, in this work we only focus on the
mainstream word-based models.

multiword grouping, among others. Although
these preprocessing decisions have been stud-
ied in the context of conventional text classiﬁca-
tion techniques (Leopold and Kindermann, 2002;
Uysal and Gunal, 2014), little attention has been
paid to them in the more recent neural-based
models.
The most similar study to ours is
Zhang and LeCun (2017), which analyzed differ-
ent encoding levels for English and Asian lan-
guages such as Chinese, Japanese and Korean. As
opposed to our work, their analysis was focused on
UTF-8 bytes, characters, words, romanized char-
acters and romanized words as encoding levels,
rather than the preprocessing techniques analyzed
in this paper.

neural

to
the

important

(Goldberg,

word
play

Additionally,

shown
boosting
of

embeddings
an
generalization

have
role
been
capabili-
in
systems
ties
2016;
Camacho-Collados and Pilehvar, 2018). How-
ever, while some studies have focused on
intrinsically analyzing the role of lemmatization
in their underlying training corpus (Ebert et al.,
2016; Kuznetsov and Gurevych, 2018), the impact
on their extrinsic performance when integrated
into a neural network architecture has remained
understudied.2

In this paper we focus on the role of pre-
processing the input text, particularly in how it
is split into individual (meaning-bearing) tokens
and how it affects the performance of standard
neural text classiﬁcation models based on Con-
volutional Neural Networks (LeCun et al., 2010;
Kim, 2014, CNN). CNNs have proven to be ef-
fective in a wide range of NLP applications, in-

2Not only the preprocessing of the corpus may play an im-
portant role but also its nature, domain, etc. Levy et al. (2015)
also showed how small hyperparameter variations may have
an impact on the performance of word embeddings. However,
these considerations remain out of the scope of this paper.

cluding text classiﬁcation tasks such as topic cate-
gorization (Johnson and Zhang, 2015; Tang et al.,
2015; Xiao and Cho, 2016; Conneau et al., 2017)
and polarity detection (Kalchbrenner et al., 2014;
Kim, 2014; Dos Santos and Gatti, 2014; Yin et al.,
2017), which are the tasks considered in this work.
The goal of our evaluation study is to ﬁnd answers
to the following two questions:

1. Are neural network architectures (in particu-
lar CNNs) affected by seemingly small pre-
processing decisions in the input text?

2. Does the preprocessing of the embeddings’
underlying training corpus have an impact
on the ﬁnal performance of a state-of-the-art
neural network text classiﬁer?

According to our experiments in topic catego-
rization and polarity detection, these decisions are
important in certain cases. Moreover, we shed
some light on the motivations of each preprocess-
ing decision and provide some hints on how to nor-
malize the input corpus to better suit each setting.
The accompanying materials of this submission
can be downloaded at the following repository:
github.com/pedrada88/preproc-textclassiﬁcation.

2 Text Preprocessing

Given an input text, words are gathered as input
units of classiﬁcation models through tokeniza-
tion. We refer to the corpus which is only tok-
enized as vanilla. For example, given the sentence
“Apple is asking its manufacturers to move Mac-
Book Air production to the United States.” (run-
ning example), the vanilla tokenized text would be
as follows (white spaces delimiting different word
units):

Apple is asking its manufacturers to move

MacBook Air production to the United States .

We additionally consider three simple prepro-
cessing techniques to be applied to an input text:
lowercasing (Section 2.1), lemmatizing (Section
2.2) and multiword grouping (Section 2.3).

2.1 Lowercasing

This is the simplest preprocessing technique
which consists of lowercasing each single token
of the input text:

apple is asking its manufacturers to move

macbook air production to the united states .

Due to its simplicity,

lowercasing has been
a popular practice in modules of deep learn-
ing libraries and word embedding packages
(Pennington et al., 2014; Faruqui et al., 2015).
Despite its desirable property of reducing sparsity
and vocabulary size, lowercasing may negatively
impact system’s performance by increasing ambi-
guity. For instance, the Apple company in our ex-
ample and the apple fruit would be considered as
identical entities.

2.2 Lemmatizing

The process of lemmatizing consists of replacing
a given token with its corresponding lemma:

Apple be ask its manufacturer to move Mac-

Book Air production to the United States .

linear

Lemmatization has been traditionally a stan-
dard preprocessing technique for
text
classiﬁcation systems (Mullen and Collier, 2004;
Toman et al., 2006; Hassan et al., 2007). How-
ever, it is rarely used as a preprocessing stage
in neural-based systems. The main idea behind
lemmatization is to reduce sparsity, as different in-
ﬂected forms of the same lemma may occur infre-
quently (or not at all) during training. However,
this may come at the cost of neglecting important
syntactic nuances.

2.3 Multiword grouping

This last preprocessing technique consists of
grouping consecutive tokens together into a single
token if found in a given inventory:

Apple is asking its manufacturers to move

MacBook Air production to the United States .

The motivation behind this step lies in the
idiosyncratic nature of multiword expressions
(Sag et al., 2002), e.g. United States in the ex-
ample. The meaning of these multiword expres-
sions are often hardly traceable from their indi-
vidual tokens. As a result, treating multiwords as
single units may lead to better training of a given
model. Because of this, word embedding toolkits
such as Word2vec propose statistical approaches
for extracting these multiwords, or directly include
multiwords along with single words in their pre-
trained embedding spaces (Mikolov et al., 2013b).

3 Evaluation

We considered two tasks for our experiments:
topic categorization, i.e. assigning a topic to a
given document from a pre-deﬁned set of topics,
and polarity detection, i.e. detecting if the senti-
ment of a given piece of text is positive or negative
(Dong et al., 2015). Two different settings were
studied: (1) word embedding’s training corpus and
the evaluation dataset were preprocessed in a simi-
lar manner (Section 3.2); and (2) the two were pre-
processed differently (Section 3.3). In what fol-
lows we describe the common experimental set-
ting as well as the datasets and preprocessing used
for the evaluation.

3.1 Experimental setup

We tried with two classiﬁcation models. The ﬁrst
one is a standard CNN model similar to that of
Kim (2014), using ReLU (Nair and Hinton, 2010)
as non-linear activation function.
In the second
model, we add a recurrent layer (speciﬁcally an
LSTM (Hochreiter and Schmidhuber, 1997)) be-
fore passing the pooled features directly to the
fully connected softmax layer.3 The inclusion
of this LSTM layer has been shown to be able
to effectively replace multiple layers of convolu-
tion and be beneﬁcial particularly for large inputs
(Xiao and Cho, 2016). These models were used
for both topic categorization and polarity detection
tasks, with slight hyperparameter variations given
their different natures (mainly in their text size)
which were ﬁxed across all datasets. The embed-
ding layer was initialized using 300-dimensional
CBOW Word2vec embeddings (Mikolov et al.,
2013a) trained on the 3B-word UMBC WebBase
corpus (Han et al., 2013) with standard hyperpa-
rameters4.

Evaluation datasets. For
the topic catego-
rization task we used the BBC news dataset5
20News
(Greene and Cunningham,
(Lang, 1995), Reuters6 (Lewis et al., 2004) and

2006),

3The

code

the
same as in (Pilehvar et al., 2017), which is available at
https://github.com/pilehvar/sensecnn

this CNN implementation is

for

4Context window of 5 words and hierarchical softmax.
5http://mlg.ucd.ie/datasets/bbc.html
6Due to the large number of labels in the original Reuters
(i.e. 91) and to be consistent with the other datasets, we re-
duce the dataset to its 8 most frequent labels, a reduction al-
ready performed in previous works (Sebastiani, 2002).

Dataset

Type

Labels

# of docs Eval.

C BBC

News
News
News

20News
Reuters
Ohsumed Medical

I
P
O
T

Y RTC
T
I
R
A
L
O
P

IMDB
PL05
PL04
Stanford

Snippets
Reviews
Snippets
Reviews
Phrases

5
6
8
23

2
2
2
2
2

2,225

10-cross
18,846 Train-test
10-cross
23,166 Train-test

9,178

438,000 Train-test
50,000 Train-test
10-cross
10,662
10-cross
2,000
10-cross
119,783

Table 1: Evaluation datasets for topic categoriza-
tion and polarity detection.

Ohsumed7. PL04 (Pang and Lee, 2004), PL058
(Pang and Lee, 2005), RTC9, IMDB (Maas et al.,
2011) and the Stanford sentiment dataset10
(Socher et al., 2013, SF) were considered for
polarity detection. Statistics of the versions of
the datasets used are displayed in Table 1.11 For
both tasks the evaluation was carried out either
by 10-fold cross-validation or using the train-test
splits of the datasets, in case of availability.

Preprocessing. Four different
techniques (see
Section 2) were used to preprocess the datasets as
well as the corpus which was used to train word
embeddings (i.e. UMBC). For tokenization and
lemmatization we relied on Stanford CoreNLP
(Manning et al., 2014). As for multiwords, we
used the phrases from the pre-trained Google
News Word2vec vectors, which were obtained us-
ing a simple statistical approach (Mikolov et al.,
2013b).12

3.2 Experiment 1: Preprocessing effect
Table 2 shows the accuracy13 of the classiﬁcation
models using our four preprocessing techniques.
We observe a certain variability of results depend-
ing on the preprocessing techniques used (aver-

7ftp://medir.ohsu.edu/pub/ohsumed
8Both PL04
and PL05 were
http://www.cs.cornell.edu/people/pabo/movie-review-data/

downloaded

from

9http://www.rottentomatoes.com
10We mapped the numerical value of phrases to either neg-
ative (from 0 to 0.4) or positive (from 0.6 to 1), removing the
neutral phrases according to the scale (from 0.4 to 0.6).

11For the datasets with train-test partitions, the sizes of the
test sets are the following: 7,532 for 20News; 12,733 for
Ohsumed; 25,000 for IMDb; and 1,000 for RTC.

12For future work it would be interesting to explore more
complex methods to learn embeddings for multiword expres-
sions (Yin and Sch¨utze, 2014; Poliak et al., 2017).

13Computed by averaging accuracy of two different runs.
The statistical signiﬁcance was calculated according to an un-
paired t-test at the 5% signiﬁcance level.

Preprocessing

BBC 20News Reuters Ohsumed RTC IMDB

PL05

PL04

Topic categorization

Polarity detection

N
N
C

Vanilla
Lowercased
Lemmatized
Multiword

M Vanilla
T
S
L
+
N
N
C

Lowercased
Lemmatized
Multiword

94.6
94.8
95.4
95.5

97.0
96.4
95.8†
96.2

89.2
89.8
89.4
89.6

90.7
90.9
90.5
89.8†

93.7
94.2
94.0
93.4†

93.1
93.0
93.2
92.7†

35.3
36.0
35.9
34.3†

30.8†
37.5
37.1
29.0†

83.2
83.0
83.1
83.2

84.8
84.0
84.4
84.0

87.5
84.2†
86.8†
87.9

88.9
88.3†
87.7†
88.9

76.3
76.1
75.8†
77.0

79.1
79.5
78.7
79.2

SF

91.2
91.1
91.2
91.2

87.1
87.1
86.8†
87.3

58.7†
59.6†
64.2
59.1†

71.4
73.3
72.6
67.0†

Table 2: Accuracy on the topic categorization and polarity detection tasks using various preprocessing
techniques for the CNN and CNN+LSTM models. † indicates results that are statistically signiﬁcant with
respect to the top result.

age variability14 of ±2.4% for the CNN+LSTM
model, including a statistical signiﬁcance gap in
seven of the nine datasets), which proves the in-
ﬂuence of preprocessing on the ﬁnal results. It is
perhaps not surprising that the lowest variance of
results is seen in the datasets with the larger train-
ing data (i.e. RTC and Stanford). This suggests
that the preprocessing decisions are not so impor-
tant when the training data is large enough, but
they are indeed relevant in benchmarks where the
training data is limited.

As far as the individual preprocessing tech-
niques are concerned, the vanilla setting (tokeniza-
tion only) proves to be consistent across datasets
and tasks, as it performs in the same ballpark as
the best result in 8 of the 9 datasets for both mod-
els (with no noticeable differences between topic
categorization and polarity detection). The only
topic categorization dataset in which tokenization
does not seem enough is Ohsumed, which, un-
like the more general nature of other categoriza-
tion datasets (news), belongs to a specialized do-
main (medical) for which ﬁne-grained distinctions
are required to classify cardiovascular diseases.
In particular for this dataset, word embeddings
trained on a general-domain corpus like UMBC
may not accurately capture the specialized mean-
ing of medical terms and hence, sparsity becomes
In fact, lowercasing and lemmatizing,
an issue.
which are mainly aimed at reducing sparsity, out-
perform the vanilla setting by over six points in

14Average variability was the result of averaging the vari-
ability of each dataset, which was computed as the difference
between the best and the worst preprocessing performances.

the CNN+LSTM setting and clearly outperform
the other preprocessing techniques on the single
CNN model as well.
Nevertheless,

the use of more complex pre-
processing techniques such as lemmatization and
multiword grouping does not help in general.
Even though lemmatization has proved useful in
conventional linear models as an effective way
to deal with sparsity (Mullen and Collier, 2004;
Toman et al., 2006), neural network architectures
seem to be more capable of overcoming sparsity
thanks to the generalization power of word embed-
dings.

3.3 Experiment 2: Cross-preprocessing

This experiment aims at studying the impact of
using different word embeddings (with differ-
ently preprocessed training corpora) on tokenized
datasets (vanilla setting). Table 3 shows the re-
sults for this experiment.
In this experiment
trend, with multiword-
we observe a different
enhanced vectors exhibiting a better performance
both on the single CNN model (best overall per-
formance in seven of the nine datasets) and on
the CNN+LSTM model (best performance in four
datasets and in the same ballpark as the best re-
sults in four of the remaining ﬁve datasets).
In
this case the same set of words is learnt but sin-
gle tokens inside multiword expressions are not
trained.
Instead, these single tokens are consid-
ered in isolation only, without the added noise
when considered inside the multiword expression
as well. For instance, the word Apple has a clearly
different meaning in isolation from the one inside

Embedding
Preprocessing

Vanilla
Lowercased
Lemmatized
Multiword

N
N
C

M Vanilla
T
S
L
+
N
N
C

Lowercased
Lemmatized
Multiword

Topic categorization

Polarity detection

BBC

20News Reuters Ohsumed RTC IMDB

PL05

PL04

94.6
93.9†
94.5
95.6

97.0
96.4
96.6
97.3

89.2
84.6†
88.7†
89.7

90.7†
91.8
91.5
91.3

93.7
93.9
93.8
93.9

93.1
92.5†
92.5†
92.8

35.3
36.2
35.4
35.2

30.8†
30.2†
31.7†
33.6

83.2
83.2
83.0
83.3

84.8
84.5
83.9
84.3

87.5†
85.4†
86.8†
88.1

88.9
88.0†
86.6†
87.3†

76.3
76.3
75.6
75.9

79.1
79.0
78.4†
79.5

58.7†
60.0†
62.5
63.1

71.4
74.2
67.7†
71.8

SF

91.2
91.1
91.2
91.2

87.1†
87.4
87.3
87.5

Table 3: Cross-preprocessing evaluation: accuracy on the topic categorization and polarity detection
tasks using different sets of word embeddings to initialize the embedding layer of the two classiﬁers.
† indicates results that are
All datasets were preprocessed similarly according to the vanilla setting.
statistically signiﬁcant with respect to the top result.

the multiword expression Big Apple, hence it can
be seen as beneﬁcial not to train the word Ap-
ple when part of this multiword expression.
In-
terestingly, using multiword-wise embeddings on
the vanilla setting leads to consistently better re-
sults than using them on the same multiword-
grouped preprocessed dataset in eight of the nine
datasets. This could provide hints on the excellent
results provided by pre-trained Word2vec embed-
dings trained on the Google News corpus, which
learns multiwords similarly to our setting.

Apart from this somewhat surprising ﬁnding,
the use of the embeddings trained on a simple to-
kenized corpus (i.e. vanilla) proved again compet-
itive, as different preprocessing techniques such
as lowercasing and lemmatizing do not seem to
In fact, the relatively weaker performance
help.
of lemmatization and lowercasing in this cross-
processing experiment is somehow expected as the
coverage of word embeddings in vanilla-tokenized
datasets is limited, e.g., many entities which are
capitalized in the datasets are not covered in the
case of lowercasing, and inﬂected forms are miss-
ing in the case of lemmatizing.

4 Conclusions

In this paper we analyzed the impact of simple
text preprocessing decisions on the performance
of a standard word-based neural text classiﬁer.
Our evaluations highlight the importance of be-
ing careful in the choice of how to preprocess our
data and to be consistent when comparing differ-
ent systems.
In general, a simple tokenization
works equally or better than more complex pre-

processing techniques such as lemmatization or
multiword grouping, except for domain-speciﬁc
datasets (such as the medical dataset in our ex-
periments) in which sole tokenization performs
poorly. Additionally, word embeddings trained on
multiword-grouped corpora perform surprisingly
well when applied to simple tokenized datasets.
This property has often been overlooked and,
to the best of our knowledge, we test the hy-
In fact, this ﬁnding
pothesis for the ﬁrst time.
could partially explain the long-lasting success of
pre-trained Word2vec embeddings, which speciﬁ-
cally learn multiword embeddings as part of their
pipeline (Mikolov et al., 2013b).

Moreover, our analysis shows that there is a
high variance in the results depending on the pre-
processing choice (±2.4% on average for the best
performing model), especially when the training
data is not large enough to generalize. Further
analysis and experimentation would be required
to fully understand the signiﬁcance of these re-
sults; but, this work can be viewed as a start-
ing point for studying the impact of text prepro-
cessing in deep learning models. We hope that
our ﬁndings will encourage future researchers to
carefully select and report these preprocessing de-
cisions when evaluating or comparing different
models. Finally, as future work, we plan to extend
our analysis to other tasks (e.g. question answer-
ing), languages (particularly morphologically rich
languages for which these results may vary) and
preprocessing techniques (e.g. stopword removal
or part-of-speech tagging).

Acknowledgments

Jose Camacho-Collados is supported by the ERC
Starting Grant 637277.

References

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Rie Johnson and Tong Zhang. 2015. Effective use
of word order for text categorization with convolu-
tional neural networks. In Proceedings of NAACL,
pages 103–112, Denver, Colorado.

Jose Camacho-Collados and Mohammad Taher Pile-
hvar. 2018. From word to sense embeddings: A sur-
vey on vector representations of meaning. Journal
of Artiﬁcial Intelligence Research (JAIR).

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL, pages
655–665.

Alexis Conneau, Holger Schwenk, Lo¨ıc Barrault, and
Yann Lecun. 2017. Very deep convolutional net-
In Proceedings of
works for text classiﬁcation.
EACL, pages 1107–1116, Valencia, Spain.

Li Dong, Furu Wei, Shujie Liu, Ming Zhou, and Ke Xu.
2015. A statistical parsing framework for sentiment
classiﬁcation. Computational Linguistics.

C´ıcero Nogueira Dos Santos and Maira Gatti. 2014.
Deep convolutional neural networks for sentiment
analysis of short texts. In Proceedings of COLING,
pages 69–78.

Sebastian Ebert, Thomas M¨uller, and Hinrich Sch¨utze.
2016. Lamb: A good shepherd of morphologically
In Proceedings of the 2016 Con-
rich languages.
ference on Empirical Methods in Natural Language
Processing, pages 742–752.

Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris
Dyer, Eduard Hovy, and Noah A. Smith. 2015.
Retroﬁtting word vectors to semantic lexicons.
In
Proceedings of NAACL, pages 1606–1615.

Lucie Flekova and Iryna Gurevych. 2016. Supersense
embeddings: A uniﬁed model for supersense inter-
pretation, prediction, and utilization. In Proceedings
of ACL, Berlin, Germany.

Yoav Goldberg. 2016. A primer on neural network
models for natural language processing. Journal of
Artiﬁcial Intelligence Research, 57:345–420.

Derek Greene and P´adraig Cunningham. 2006. Practi-
cal solutions to the problem of diagonal dominance
in kernel document clustering. In Proceedings of the
23rd International conference on Machine learning,
pages 377–384. ACM.

Lushan Han, Abhay Kashyap, Tim Finin, James May-
ﬁeld, and Jonathan Weese. 2013. UMBC ebiquity-
core: Semantic textual similarity systems. In Pro-
ceedings of the Second Joint Conference on Lexical
and Computational Semantics, volume 1, pages 44–
52.

Samer Hassan, Rada Mihalcea, and Carmen Banea.
2007. Random walk term weighting for improved
text classiﬁcation. International Journal of Seman-
tic Computing, 1(04):421–439.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of EMNLP.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proceedings of AAAI.

Ilia Kuznetsov and Iryna Gurevych. 2018. From text
to lexicon: Bridging the gap between word em-
In Proceedings of
beddings and lexical resources.
the 27th International Conference on Computational
Linguistics, pages 233–244.

Ken Lang. 1995. Newsweeder: Learning to ﬁlter net-
news. In Proceedings of the 12th international con-
ference on machine learning, pages 331–339.

Yann LeCun, Koray Kavukcuoglu, and Cl´ement Fara-
bet. 2010. Convolutional networks and applications
In Circuits and Systems (ISCAS), Pro-
in vision.
ceedings of 2010 IEEE International Symposium on,
pages 253–256. IEEE.

Edda Leopold and J¨org Kindermann. 2002. Text cat-
egorization with support vector machines. how to
represent texts in input space? Machine Learning,
46(1-3):423–444.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.

David D. Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for
Journal of machine
text categorization research.
learning research, 5(Apr):361–397.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
In Proceedings of EMNLP, Lisbon, Portugal.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of ACL-HLT, pages 142–150, Port-
land, Oregon, USA.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efﬁcient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Michal Toman, Roman Tesar, and Karel Jezek. 2006.
Inﬂuence of word normalization on text classiﬁca-
tion. Proceedings of InSciT, 4:354–358.

Alper Kursat Uysal and Serkan Gunal. 2014. The im-
Infor-
pact of preprocessing on text classiﬁcation.
mation Processing & Management, 50(1):104–112.

Yijun Xiao and Kyunghyun Cho. 2016.

Efﬁcient
character-level document classiﬁcation by com-
bining convolution and recurrent layers. CoRR,
abs/1602.00367.

Wenpeng Yin, Katharina Kann, Mo Yu, and Hinrich
Sch¨utze. 2017. Comparative study of cnn and rnn
arXiv preprint
for natural language processing.
arXiv:1702.01923.

Wenpeng Yin and Hinrich Sch¨utze. 2014. An explo-
ration of embeddings for generalized phrases.
In
ACL (Student Research Workshop), pages 41–47.

Xiang Zhang and Yann LeCun. 2017. Which en-
coding is the best for text classiﬁcation in chinese,
arXiv preprint
english,
arXiv:1708.02657.

japanese and korean?

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
In Advances in neural information processing
ity.
systems, pages 3111–3119.

Tony Mullen and Nigel Collier. 2004. Sentiment analy-
sis using support vector machines with diverse infor-
mation sources. In EMNLP, volume 4, pages 412–
418.

Vinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning (ICML-10), pages 807–814.
Omnipress.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the ACL.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
ACL.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
In Proceedings of EMNLP, pages
representation.
1532–1543.

Mohammad Taher Pilehvar, Jose Camacho-Collados,
Roberto Navigli, and Nigel Collier. 2017. Towards
a Seamless Integration of Word Senses into Down-
stream NLP Applications. In Proceedings of ACL,
Vancouver, Canada.

Adam Poliak, Pushpendre Rastogi, M. Patrick Martin,
and Benjamin Van Durme. 2017. Efﬁcient, compo-
sitional, order-sensitive n-gram embeddings. In Pro-
ceedings of EACL.

Ivan A Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics, pages 1–15. Springer.

Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR), 34(1):1–47.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher Manning, Andrew Ng, and
Christopher Potts. 2013. Parsing With Composi-
tional Vector Grammars. In Proceedings of EMNLP.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
In EMNLP, pages 1422–
sentiment classiﬁcation.
1432.

