END-TO-END TRAINING APPROACHES FOR DISCRIMINATIVE SEGMENTAL MODELS

Hao Tang, Weiran Wang, Kevin Gimpel, Karen Livescu

Toyota Technological Institute at Chicago
{haotang, weiranwang, kgimpel, klivescu}@ttic.edu

6
1
0
2
 
t
c
O
 
1
2
 
 
]
L
C
.
s
c
[
 
 
1
v
0
0
7
6
0
.
0
1
6
1
:
v
i
X
r
a

ABSTRACT

Recent work on discriminative segmental models has shown
that they can achieve competitive speech recognition perfor-
mance, using features based on deep neural frame classiﬁers.
However, segmental models can be more challenging to train
than standard frame-based approaches. While some segmen-
tal models have been successfully trained end to end, there is
a lack of understanding of their training under different set-
tings and with different losses.

We investigate a model class based on recent successful
approaches, consisting of a linear model that combines seg-
mental features based on an LSTM frame classiﬁer. Similarly
to hybrid HMM-neural network models, segmental models of
this class can be trained in two stages (frame classiﬁer training
followed by linear segmental model weight training), end to
end (joint training of both frame classiﬁer and linear weights),
or with end-to-end ﬁne-tuning after two-stage training.

We study segmental models trained end to end with hinge
loss, log loss, latent hinge loss, and marginal log loss. We
consider several losses for the case where training alignments
are available as well as where they are not.

We ﬁnd that

log loss provides
in general, marginal
the most consistent strong performance without requiring
ground-truth alignments. We also ﬁnd that training with
dropout is very important in obtaining good performance
with end-to-end training. Finally, the best results are typi-
cally obtained by a combination of two-stage training and
ﬁne-tuning.

Index Terms— Discriminative segmental models, end-

to-end training

1. INTRODUCTION

End-to-end training has proved to be successful, for example,
in connectionist temporal classiﬁcation (CTC) [1], encoder-
decoders [2], hidden Markov model (HMM) based hybrid
systems [3], deep segmental neural networks (DSNN) [4], and
segmental recurrent neural networks (SRNN) [5]. All of these
models have a feature encoder and an output model for gener-
ating label sequences. The feature encoder can be a recurrent
or a feedforward neural network, and the output model can

be a recurrent neural decoder, such as a long short-term mem-
ory network (LSTM), or a probabilistic graphical model, such
as an HMM, a conditional random ﬁeld (CRF), or a semi-
Markov CRF. The actual deﬁnition of end-to-end training is
rarely made explicit in the literature.
In this work, we de-
ﬁne end-to-end training as optimizing the encoder parame-
ters and the output model parameters jointly. The alternative,
which we refer to as two-stage training, optimizes the fea-
ture encoder and output model parameters separately in two
stages.

These two families of training approaches differ in terms
of annotation requirements, computational and learning ef-
ﬁciency, and the loss functions customarily used for each.
Two-stage training typically requires frame-level labels for
the ﬁrst stage, but may therefore require fewer samples to
learn from [6]. End-to-end training avoids the cascading er-
rors of pipelines, but results in hard-to-optimize objectives
that are sensitive to initialization. It is also possible to per-
form end-to-end ﬁne-tuning after two-stage training, which
has been found useful in past work [7].

In this work, we study training approaches for segmental
models. Segmental models have been shown to be successful
when trained end to end from scratch [5]. We focus on a par-
ticular class of segmental models, with LSTMs as encoders,
and linear segmental models as output models. For models
trained in two stages, there is often an extra restriction on
the representation of the encoded features. For example, they
may be log probabilities of triphone states in HMM hybrid
systems [8]. Systems trained end to end (encoder-decoders,
DSNNs, and SRNNs) are not so constrained. To enable fair
comparison, we use model architectures that seamlessly per-
mit both kinds of training without requiring any change to the
model parameterization. The only difference is that two-stage
training leads to interpretable encoded features, but the func-
tional architectures are identical.1

In order to thoroughly compare two-stage and end-to-end
training, we consider a variety of loss functions and training
settings. When end-to-end systems were ﬁrst proposed, such
as CTC-LSTMs, encoder-decoders, and SRNNs, they were
tied to speciﬁc loss functions, such as CTC, per-output cross

1We note that though our model class is suitable for studying end-to-end
systems in various aspects, using better encoders, such as SRNNs, might lead
to better absolute performance.

entropy, and marginal log loss. However, these systems can
be trained with different loss functions; e.g., encoder-decoder
systems can be trained with hinge loss [9]. It is thus impor-
tant to isolate the effect of training loss functions from mod-
els. For our model class, the deﬁnition of encoder and out-
put model is completely independent of the deﬁnition of loss
functions. This allows us to compare training losses while
keeping everything else ﬁxed.

Two-stage training typically uses ﬁne-grained labels for
training the ﬁrst stage, such as segmentations. For some
datasets, such as TIMIT, we have the luxury to use manually
annotated segmentations, but for most of the datasets, we do
not. If needed, segmentations are typically inferred by force
aligning labels to frames. For our model class, the system can
be trained with or without segmentations depending on the
choice of loss function.

In the following sections, we explicitly deﬁne our model
class and loss functions, in particular, hinge loss and log
loss for cases where we have ground truth segmentations,
and latent hinge loss and marginal log loss when we do
not. We perform experiments studying two-stage and end-to-
end training in different settings with different losses. On a
phoneme recognition task, we show that end-to-end training
from scratch with marginal log loss achieves the best re-
sult in the setting without ground truth segmentations, while
two-stage training followed by end-to-end ﬁne-tuning with
log loss achieves the best result in the setting with ground
truth segmentations.We also ﬁnd that dropout is crucial for
combating overﬁtting.

2. DISCRIMINATIVE SEGMENTAL MODELS

Speech recognition, or sequence prediction in general, can
be formulated as a search problem. The search space is a
set of paths, each of which is composed of segments. Each
segment is associated with a weight, and in turn each path
is associated with a weight. Prediction becomes ﬁnding the
highest weighted path in the search space. We formalize this
below.

Let X be the input space, a set of sequences of frames,
e.g., MFCCs or Mel ﬁlter bank outputs. Let L be the label
set, e.g., a phone set for phoneme recognition. A segment
is a tuple (s, t, y), where s is the start time, t is the end
time, and y ∈ L is the label. Two segments e1, e2 are con-
nected if the end time of e1 is the same as the start time
of e2. A path is a sequence of connected segments. A
path p = ((s1, t1, y1), . . . , (sn, tn, yn)) can also be seen
as a label sequence y = (y1, . . . , yn) and a segmentation
z = ((s1, t1), . . . , (sn, tn)), or simply p = (y, z).

Let E be the set of all possible segments. A segmental
model is a tuple (θ, Λ, φΛ), where θ ∈ Rd is a parameter
vector, φΛ : X × E → Rd is a feature function that uses a
feature encoder parameterized by the set of parameters Λ. We
will give deﬁnitions of feature encoders and feature functions

in later sections. With a slight abuse of notation, for a path
p = (y, z), let φΛ(x, p) = φΛ(x, y, z) =
Pe∈p φΛ(x, e).
Prediction can be formulated as

argmax
p∈P

θ⊤φΛ(x, p) = argmax

θ⊤φΛ(x, e),

(1)

p∈P X
e∈p

where P is the set of all paths. Though the output contains
both a label sequence and a segmentation, the segmentation
is often disregarded during evaluation.

Learning a segmental model amounts to ﬁnding pa-
rameters θ and Λ that minimize a speciﬁed loss function.
Learning can be divided into two cases, one with access
to ground truth segmentations, and one without. When
we have ground truth segmentations, we receive a dataset
S = {(x1, y1, z1), . . . , (xm, ym, zm)} and learning aims to
solve

argmin
θ,Λ

1
m

m

X
i=1

ℓ(θ, Λ; xi, yi, zi).

(2)

When we do not have ground truth segmentations, we have a
dataset S = {(x1, y1), . . . , (xm, ym)}, and learning becomes
solving

argmin
θ,Λ

1
m

m

X
i=1

ℓ(θ, Λ; xi, yi).

(3)

3. LOSS FUNCTIONS

Since segmental models fall under structured prediction, any
general loss function for structured prediction is applicable
to segmental models. In particular, we investigate hinge loss
and log loss for the case with ground truth segmentations, and
latent hinge loss and marginal log loss for the case without
ground truth segmentations. All loss deﬁnitions ℓ(θ, Λ) below
are given in terms of a single training sample (x, y, z) where
x ∈ X , (y, z) ∈ P.

Hinge loss is deﬁned as

(y′,z′)∈P hcost((y, z), (y′, z′)) − θ⊤φΛ(x, y, z)
max

+ θ⊤φΛ(x, y′, z′)i,

where cost is a function that measures the distance between
two paths. Log loss is deﬁned as

− log p(y, z|x)

where

p(y, z|x) =

exp(θ⊤φΛ(x, y, z))

1
Z

P(y′,z′)∈P exp(θ⊤φΛ(x, y′, z′)). Both hinge loss
and Z =
and log loss require segmentations. Hinge loss has an explicit

(4)

(5)

(6)

(11)

(12)

cost function , while log loss does not. In fact during predic-
tion, hinge loss is always an upper bound of the cost function.
Hinge loss is non-smooth due to the max operation, while log
loss is smooth. Both hinge loss and log loss are convex in θ,
yet non-convex in Λ if a neural network is used.

Latent hinge loss is deﬁned as

(y′,z′)∈P hcost((y, ˜z), (y′, z′)) − max
max
+ θ⊤φΛ(x, y′, z′)i

z′′ θ⊤φΛ(x, y, z′′)

(7)

where ˜z = argmaxz′′∈Z(y) θ⊤φΛ(x, y, z′′) and Z(y) is the
set of possible segmentations of y. Marginal log loss is de-
ﬁned as

frames over a segment is deﬁned as

φavg(x, (s, t, y)) =

ki ⊗ 1y

(9)

1
t − s

t−1

X
i=s

where ⊗ is the tensor product, and 1y is a |L|-dimensional
one-hot vector for the label y. The frame sample at the r-th
percentile is deﬁned as

φat-r(x, (s, t, y)) = k⌊s+rd⌋ ⊗ 1y

(10)

where d = t − s + 1. The frame at the left boundary is deﬁned
as

φleft-r(x, (s, t, y)) = ks−r ⊗ 1y

− log p(y|x) = − log

p(y, z|x).

(8)

X
z∈Z(y)

and similarly, the frame at the right boundary is

φright-r(x, (s, t, y)) = kt+r ⊗ 1y.

Both latent hinge loss and marginal log loss do not require
ground truth segmentations. During prediction, latent hinge
loss is also an upper bound of the cost function. Latent hinge
loss is non-smooth, while marginal log loss is smooth. Both
latent hinge loss and marginal log loss are non-convex in both
θ and Λ.

Hinge loss training for segmental models ﬁrst appeared in
[10], log loss in [11], and marginal log loss in [12]. For train-
ing ﬁrst-pass segmental models, [13] is the ﬁrst to use hinge
loss, [11] is the ﬁrst to use log loss, and [14] is the ﬁrst to use
marginal log loss. For training ﬁrst-pass segmental models
end to end, [4] is the ﬁrst to use marginal log loss. Other loss
functions, such as empirical Bayes risk and structured ramp
loss, have been used in [15] for training segmental models.

The above loss functions can be optimized with stochastic
gradient descent or its variants. We propagate gradients back
through the feature function φ, allowing all parameters to be
updated jointly.

4. FEATURE FUNCTIONS

Here we deﬁne explicitly feature functions we will use in the
experiments. These feature functions ﬁrst appeared in [16].

We assume there is a feature encoder, for example, an
LSTM, which produces h1, . . . , hT given input x1, . . . , xT .
For any t ∈ {1, . . . , T }, we project ht to a |L|-dimensional
vector and pass the resulting vector through a log-softmax
Pj Wij ht,j −
layer and get kt.
Pj Wℓjht,j), where W is the projection matrix.
log
Pℓ exp(
In this case, the set of parameters Λ includes the projection
matrix W and parameters in the LSTM.

In other words, kt,i =

The following is a list of features, and the ﬁnal feature
function produces a concatenation of feature vectors pro-
duced by the individual feature functions. The average of

Additionally, we have features that do not depend on the

feature encoder. The length score is deﬁned as

φlen(x, (s, t, y)) = 1d ⊗ 1y

(13)

where d = t−s+1. Finally, there is a bias for each individual
label

φbias(x, (s, t, y)) = 1y.

(14)

Gradients are propagated through vectors k1, . . . , kT
to the feature encoder. Parameters of the entire segmental
model, including the feature encoder, can be updated jointly.

5. EXPERIMENTS

We conduct phonetic recognition experiments on TIMIT, a 6-
hour phonetically transcribed dataset. We follow the conven-
tional setting, training models on the 3696-utterance training
set, and evaluate on the 192-utterance core test set. We use
the rest of the 400 utterances in the test set as the development
set. Following the convention, we collapse 61 phones down
to 48 for training, and further collapse them to 39 phones for
evaluation.

The feature encoder we use is a 3-layer bidirectional
LSTM with 256 cells per layer. The outputs of the third layer
are projected from 256 dimensions to 48 and pass through a
log-softmax layer so that the ﬁnal output are log probabilities.
Inputs to the encoder are 39-dimensional MFCCs, normalized
per dimension by subtracting the mean and dividing by the
standard deviation calculated from the training set.

5.1. Two-Stage Training

Since TIMIT is phonetically transcribed, we have access to
phone labels for each individual frame. We ﬁrst train LSTM

Table 1. Frame error rates for different encoder architectures.

feat
CNN
MFCC+fbank
MFCC
LSTM 256x3
LSTM 256x3 +dropout MFCC

dev
22.27
22.60
21.09

test
23.03

21.36

Table 2. Phone error rates for segmental models trained with
hinge loss using log probabilities generated from various en-
coders in Table 1.

feat
MFCC+fbank
CNN
LSTM 256x3
MFCC
LSTM 256x3 +dropout MFCC

dev
21.4
23.1
21.4

test
22.5

22.1

frame classiﬁers with cross entropy loss at each frame. This
LSTM will serve as our feature encoder later on, and train-
ing such LSTM corresponds to the ﬁrst stage of two-stage
learning. LSTM parameters are initialized uniformly in the
range [−0.1, 0.1]. Biases for forget gates are initialized to
one [17], while other biases are initialized to zero. Dropout
for LSTMs [18] is applied at all input layers and the last out-
put layer with a dropout rate of 50%. We compare AdaGrad
with step sizes in {0.01, 0.02, 0.04} and RMSProp with step
size 0.001 and decay 0.9. Mini-batch size is always one utter-
ance. Both optimizers are run for 50 epochs. We choose the
best performing model according to the frame error rate on the
development set, also known as early stopping. No gradient
clipping is used during training. For comparison, following
[13] we train a convolutional neural network (CNN) consist-
ing of 5 layer convolution followed by 3 fully-connected lay-
ers. Frame classiﬁcation results are shown in Table 1. We
observe that the best performing LSTM achieves a compara-
ble frame error rate as the CNN. With dropout, the frame error
rate is further lowered.

After obtaining LSTM frame classiﬁers, we proceed to the
second stage, training segmental models with features based
on LSTM log probabilities. Segmental models are trained
with the four loss functions for 50 epochs with early stop-
ping. Overlap cost [15] is used in hinge loss and latent hinge
loss. A maximum duration of 30 frames is imposed. We use
feature functions described in Section 4. No regularizer is
used except early stopping. We compare AdaGrad with step
sizes in {0.1, 0.2, 0.4} and RMSProp with step size 0.001
and decay 0.9. Phonetic recognition results for hinge loss are
shown in Table 2. We observe that LSTMs perform better in
frame classiﬁcation, but give little improvement over CNNs
in phonetic recognition. Recognition results for the rest of
the losses are in Table 3. Note that even though latent hinge
loss and marginal log loss do not require segmentations dur-
ing training, we do use ground truth segmentations for train-
ing the frame classiﬁer. It is not a common setting, and is
done purely for comparison purposes. We observe that, ex-
cept latent hinge, other losses perform equally well, with log
loss having a slight edge over the others.

5.2. End-to-End Training with Warm Start

After two-stage training, we ﬁne tune the encoder and seg-
mental model jointly to further lower the training loss. The

Table 3. Phone error rates for segmental models trained in
two stages with different losses.

hinge
log loss
latent hinge
marginal log loss

dev
21.4
21.2
23.5
21.6

test
22.1
21.9
24.6
22.5

four losses are compared with and without dropout. When
dropout is used, dropout rate 50% is chosen to match the rate
during frame classiﬁer training. The input layers and the out-
put layer are scaled by 0.5 when no dropout is used. First,
we initialize the models with the one trained with hinge loss
above. We run AdaGrad with step size 0.001 for 10 epochs
with early stopping. Results are shown in Table 4. We ob-
serve healthy reductions in phone error rates by ﬁne tuning
the two-stage system across all loss functions. We also ﬁnd
that ﬁne-tuning without dropout tends to be better than with
dropout. Though ﬁne-tuning with hinge loss leads to the most
error reduction, we note that the two-stage system is trained
with hinge loss. At least we are centain that the two-stage
system trained with hinge loss is a descent initialization for
other losses.

Minimizing other losses from a model trained with hinge
loss is less than ideal. We repeat the above experiments by
warm-starting from a model trained with the loss function that
we are going to minimize. Results are shown in Table 5. We
observe signiﬁcant gains for log loss and marginal log loss
if initialized with the matching loss function. Similarly, the
gains with dropout in these cases are smaller than without
dropout.

5.3. End-to-End Training from Scratch

Next, we train the same architecture end to end from scratch.
We make sure that all the models are initialized identically to
the two-stage systems. The four losses are used for training
with dropout rates in {0, 0.1, 0.2, 0.5}. Ground truth segmen-
tations are used when training with hinge loss and log loss,
and are disregarded when training with latent hinge loss and
marginal log loss. The optimizers we use here are SGD with

Table 4. Phone error rates for segmental models trained
end to end initialized from the two-stage system trained with
hinge loss.

Table 6. Phone error rates for segmental models trained end
to end with dropout.

hinge

log loss

latent hinge

marginal log loss

dropout
0
0.5
0
0.5
0
0.5
0
0.5

dev
19.4
20.8
20.2
21.1
19.3
20.8
20.7
20.9

test
20.7

21.7

21.0

22.2

Table 5. Phone error rates for segmental models trained end
to end initialized from two-stage systems trained with corre-
sponding loss functions.

hinge

log loss

latent hinge
marginal log loss

dropout
0
0.1
0.2
0.5
0
0.1
0.2
0.5

0
0.1
0.2
0.5

dev
23.1
22.4
22.3
28.9
24.8
22.4
20.8
22.3
failed
25.3
22.1
20.0
22.0

test

23.7

22.2

22.0

hinge

log loss

latent hinge

marginal log loss

dropout
0
0.5
0
0.5
0
0.5
0
0.5

dev
19.4
20.8
18.8
20.3
20.0
22.1
19.2
21.0

test
20.7

19.7

21.2

20.8

Table 7. Average cross entropy over frames before and after
end-to-end ﬁne-tuning.

LSTM
256x3 (best train)
256x3 (best dev)
256x3 +dropout
256x3 +dropout +e2e

train CE dev CE dev PER
2.2395
0.0569
0.9442
0.4179
0.7466
0.4595
0.6928
0.3864

21.4
19.4

step sizes in {0.1, 0.5}, momentum 0.9, and gradient clip-
ping at norm 5, AdaGrad with step sizes in {0.01, 0.02, 0.04}
and no clipping, and RMSProp with step size 0.001, decay
0.9, and no clipping. We run earch optimizer for 50 epochs
with early stopping. Results are shown in Table 6. First,
all optimizers above fail to minimize latent hinge loss. All
of them get stuck in local optima, and fail to produce rea-
sonable forced alignments. Even though all loss functions in
end-to-end training are nonconvex, latent hinge loss is more
sensitive to initialization than other losses. The second obser-
vation is that adding dropout improves performance. How-
ever, using the same dropout rate as the two-stage system re-
sults in worse performance. Finally, though behind the best
ﬁne-tuned model, marginal log loss with dropout 0.2 slightly
edges over other losses.

6. DISCUSSION

We have seen that end-to-end training initialized with a two-
stage system leads to the best results. Since in end-to-end
training, the meaning of the intermediate representations is
not enforced anymore, it is unclear how the intermediate rep-

resentations deviate from the learned ones. To answer this, we
measure per-frame cross entropy for the LSTM frame classi-
ﬁer after end-to-end training. Results are shown in Table 7.
First, the per-frame cross entropy for the best performing
LSTM on the training set can be as low as 0.06, which shows
that a 3-layer bidirectional LSTM with 256 cells per layer is
able to essentially memorize the entire TIMIT dataset. How-
ever, it is severely overﬁtting. Early stopping and dropout
help balance cross entropies on the training set and develop-
ment set. In addition, the cross entropies on both sets drop
after end-to-end training. It shows that the meaning of the
intermediate representations is still maintained by the LSTMs
after end-to-end training.

Next, since the system trained with marginal log loss does
not use the ground truth segmentations, and since the evalua-
tion measure (phone error rate) does not consider segmenta-
tions, we do not know if the system is able to discover rea-
sonble phone boundaries without supervision. We approach
this question by aligning the label sequences to the acoustics,
and compare the resulting segmentations against the manually
annotated segmentations. The alignment quality for different
tolerance values is shown in Table 8. Though the results are
behind models trained speciﬁcally to align [19], the segmen-

Table 8. Forced alignment quality on the test set as a percent-
age of correctly positioned phone boundaries within a prede-
ﬁned tolerance, measured with the best-performing segmental
model trained with marginal log loss.

Table 9. Average number of hours per epoch spent on com-
puting gradients excluding LSTM computations.

hinge
0.52

log loss
1.08

latent hinge marginal log loss
2.10
0.73

t ≤ 10ms
64.5

t ≤ 20ms
86.8

t ≤ 30ms
94.7

t ≤ 40ms
96.7

tal model trained with marginal log loss is not supervised with
any ground truth segmentations. Limiting the maximum du-
ration to 30 frames also affects the alignment performance.

Since most speech datasets do not have manually anno-
tated segmentations, it is desirable to train without manual
alignments. As we now know, the alignments produced by
our system trained with marginal log loss are of good qual-
ity. Therefore, we can use the forced alignments to train a
two-stage system followed by end-to-end ﬁne-tuning. We fol-
low the exact same procedure as in the previous two-stage
experiments by training an LSTM frame classiﬁer with the
forced alignments, followed by training a segmental model
with hinge loss. The frame error rate on the development set
of the LSTM classiﬁer is 21.68% against the forced align-
ments and 28.91% against the ground-truth segmentations.
Though the frame error rate is signiﬁcantly worse than when
training with ground-truth segmentations, this two-stage sys-
tem achieves a phone error rate of 21.0% on the development
set. We then ﬁne-tune the entire system with hinge loss. The
ﬁnal system achieves 18.6% phone error rate on the devel-
opment set, and 20.1% on the test set, a signiﬁcant improve-
ment from the model trained end-to-end with marginal log
loss, while not relying on ground truth segmentations.

In terms of efﬁciency in training, all four losses require
forward-backward-like algorithms for computing gradients.
Hinge loss requires one pass on the entire search space, log
loss requires two passes on the entire search space, latent
hinge requires one pass on the entire search space and one
on the segmentation space, and marginal log loss requires
two passes on the entire search space and two passes on the
segmentation space. The average number of hours per epoch
spent on computing gradients, excluding LSTM computa-
tions, is shown in Table 9. To put them into context, feeding
forward and backpropagation for LSTMs takes 1.65 hours per
epoch. The timing is done on a single 3.4GHz four-core CPU.
The number of hours is consistent with the number of passes
required to compute gradients. Note that the time spent on
LSTMs can be halved without incurring a performance loss
by applying frame skipping [20, 21] as shown for segmental
models in [22].

7. CONCLUSION

In this work, we study end-to-end training in the context of
segmental models. The model class of choice includes a 3-
layer bidirectional LSTM as feature encoder and a segmental
model using the features to produce label sequences. This
model class is suitable for studying end-to-end training, due
to its ﬂexibility to be trained either in a two stage manner,
or end to end. The hypothesis is that training such systems
in two stages is easier than end-to-end training from scratch.
On the other hand, end-to-end training can better optimize the
loss function, but it might be sensitive to initialization.

Our model deﬁnition is separated from the deﬁnition of
loss functions, giving us the ﬂexibility to choose loss func-
tions based on the training settings. We consider two com-
mon training settings, one with ground truth segmentations
and one without. Hinge loss and log loss require segmenta-
tions by deﬁnition, while latent hinge loss and marginal log
loss do not.

We show that in the case where we have ground truth seg-
mentations, two-stage training followed by end-to-end train-
ing is signiﬁcantly better than two-stage training alone (im-
proving upon it by 10% relative) and end-to-end training from
scratch.
In addition, we ﬁnd that end-to-end training with
marginal log loss from scratch achieves competitive results.
As a byproduct, the system is able to generate high-quality
forced alignments. To remove the dependency on ground
truth segmentations, we train another model on the forced
alignments in two stages followed by end-to-end ﬁne-tuning,
improving upon end-to-end training from scratch by 8.6% rel-
ative. The ﬁnal product is a strong system trained end to end
without requiring ground truth segmentations.

8. ACKNOWLEDGEMENT

This research was supported by a Google faculty research
award and NSF grant IIS-1433485. The opinions expressed
in this work are those of the authors and do not necessarily
reﬂect the views of the funding agency. The GPUs used for
this research were donated by NVIDIA.

9. REFERENCES

[1] Alex Graves, Abdel-Rahman Mohamed, and Geoffrey
Hinton, “Speech recognition with deep recurrent neural

networks,” in IEEE International Conference on Acous-
tics, Speech and Signal Processing, 2013, pp. 6645–
6649.

[2] Ilya Sutskever, Oriol Vinyals, and Quoc V Le,

“Se-
quence to sequence learning with neural networks,” in
Advances in Neural Information Processing Systems,
2014, pp. 3104–3112.

[3] Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pe-
gah Ghahrmani, Vimal Manohar, Xingyu Na, Yiming
Wang, and Sanjeev Khudanpur,
“Purely sequence-
trained neural networks for ASR based on lattice-free
MMI,” Annual Conference of the International Speech
Communication Association, 2016.

[4] Ossama Abdel-Hamid, Li Deng, Dong Yu, and Hui
“Deep segmental neural networks for speech
Jiang,
recognition.,”
in Annual Conference of International
Speech Communication Association, 2013, pp. 1849–
1853.

[5] Liang Lu, Lingpeng Kong, Chris Dyer, Noah A. Smith,
and Steve Renals, “Segmental recurrent neural networks
for end-to-end speech recognition,” in Annual Confer-
ence of the International Speech Communication Asso-
ciation, 2016.

[6] Shai Shalev-Shwartz and Amnon Shashua,

“On the
sample complexity of end-to-end training vs. semantic
abstraction training,” CoRR, vol. abs/1604.06915, 2016.

[7] Karel Vesel`y, Arnab Ghoshal, Luk´as Burget, and Daniel
Povey, “Sequence-discriminative training of deep neural
networks.,” in Annual Conference of the International
Speech Communication Association, 2013.

[8] Alex Graves, Navdeep Jaitly, and Abdel-Rahman Mo-
hamed, “Hybrid speech recognition with deep bidirec-
tional LSTM,” in IEEE Workshop on Automatic Speech
Recognition and Understanding, 2013, pp. 273–278.

[9] Sam Wiseman and Alexander M. Rush, “Sequence-to-
sequence learning as beam-search optimization,” CoRR,
vol. abs/1606.02960, 2016.

[10] Shi-Xiong Zhang and Mark Gales, “Structured SVMs
for automatic speech recognition,” IEEE Transactions
on Audio, Speech, and Language Processing, vol. 21,
no. 3, pp. 544–555, 2013.

[11] Sunita Sarawagi and William W Cohen, “Semi-Markov
conditional random ﬁelds for information extraction,”
in Advances in Neural Information Processing Systems,
2004, pp. 1185–1192.

[12] Geoffrey Zweig and Patrick Nguyen, “A segmental CRF
approach to large vocabulary continuous speech recog-
nition,” in IEEE Workshop on Automatic Speech Recog-
nition & Understanding, 2009, pp. 152–157.

[13] Hao Tang, Weiran Wang, Kevin Gimpel, and Karen
“Discriminative segmental cascades for
Livescu,
feature-rich phone recognition,”
in IEEE Workshop
on Automatic Speech Recognition and Understanding,
2015.

[14] Geoffrey Zweig, “Classiﬁcation and recognition with
direct segment models,” in IEEE International Confer-
ence on Acoustics, Speech and Signal Processing, 2012,
pp. 4161–4164.

[15] Hao Tang, Kevin Gimpel, and Karen Livescu, “A com-
parison of training approaches for discriminative seg-
mental models,” in Annual Conference of the Interna-
tional Speech Communication Association, 2014.

[16] Yanzhang He and Eric Fosler-Lussier, “Efﬁcient seg-
mental conditional random ﬁelds for phone recogni-
tion,” in Annual Conference of the International Speech
Communication Association, 2012, pp. 1898–1901.

[17] Rafal

Jozefowicz, Wojciceh Zaremba,

and Ilya
“An empirical exploration of recurrent
Sutskever,
network architectures,” in International Conference on
Machine Learning, 2015.

[18] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals,
“Recurrent neural network regularization,” CoRR, vol.
abs/1409:2320, 2014.

[19] Joseph Keshet, Shai Shalev-Shwartz, Yoram Singer, and
Dan Chazan, “A large margin algorithm for speech-to-
phoneme and music-to-score alignment,” IEEE Trans-
actions on Audio, Speech, and Language Processing,
vol. 15, no. 8, pp. 2373–2382, 2007.

[20] Yajie Miao, Jinyu Li, Yongqian Wang, Shixiong Zhang,
and Yifan Gong, “Simplifying long short-term memory
acoustic models for fast training and decoding,” in IEEE
International Conference on Acoustics, Speech and Sig-
nal Processing, 2015.

[21] Vincent Vanhoucke, Matthieu Devin,

and Georg
Heigold, “Multiframe deep neural networks for acoustic
modeling,” in IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP). IEEE,
2013, pp. 7582–7585.

[22] Hao Tang, Weiran Wang, Kevin Gimpel, and Karen
“Efﬁcient segmental cascades for speech
Livescu,
recognition,” in Annual Conference of the International
Speech Communication Association, 2016.

