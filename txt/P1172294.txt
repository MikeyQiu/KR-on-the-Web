Question Answering via Web Extracted Tables and Pipelined
Models

BHAVYA KARKI, FAN HU, NITHIN HARIDAS, SUHAIL BAROT, ZIHUA LIU, LU-
CILE CALLEBERT, MATTHIAS GRABMAIR, and ANTHONY TOMASIC, Carnegie

Mellon University

In this paper, we describe a dataset and baseline result for a question answering that utilizes web tables. It contains

commonly asked questions on the web and their corresponding answers found in tables on websites. Our dataset is novel

in that every question is paired with a table of a different signature. In particular, the dataset contains two classes of tables:

entity-instance tables and the key-value tables. Each QA instance comprises a table of either kind, a natural language

question, and a corresponding structured SQL query. We build our model by dividing question answering into several

tasks, including table retrieval and question element classification, and conduct experiments to measure the performance

of each task. We extract various features specific to each task and compose a full pipeline which constructs the SQL query

from its parts. Our work provides qualitative results and error analysis for each task, and identifies in detail the reasoning

required to generate SQL expressions from natural language questions. This analysis of reasoning informs future models

based on neural machine learning.

1 INTRODUCTION

State of the art question answering (QA) systems, such as Amazon Alexa or even Google’s advanced search

functionalities, produce answers by either retrieving text segments from a large repository of documents, or

by retrieving entities from a large knowledge graph (e.g. Freebase) which need to be populated by hand or

constructed from text using sophisticated NLP technology. At the same time, the the internet already contains

much structured knowledge in the form of tables on websites. In this paper we work towards leveraging

pre-existing tables on website by using them to produce answers to questions, which we translate into SQL

queries that can be executed against the table. Current solutions to question answering of structured queries

generally use neural models, which require very large datasets. We collected a dataset of questions and

answers from tables. Since the collection is expensive it is not large enough to apply sequence-to-sequence

(seq2seq)[7] models that encode a natural language question and produce a structured query as the output. In

this paper, we decompose the reasoning involved in constructing structured queries into a series of steps that

translate a natural language questions the components of SQL queries, such as the SELECT clause, WHERE

clause, and so on.

This approach offers a deeper understanding of the semantic and syntactic information required for genera-
tion of a structured query. This information is critical to guiding future efforts in training set construction,

merging with existing knowledge sources, and integration with structured sources. Tabular information

Authors’ address: Bhavya Karki; Fan Hu; Nithin Haridas; Suhail Barot; Zihua Liu; Lucile Callebert; Matthias Grabmair; Anthony

Tomasic, Carnegie Mellon University, Pittsburgh, PA.

1

9
1
0
2
 
r
p
A
 
6
1
 
 
]
L
C
.
s
c
[
 
 
2
v
3
1
1
7
0
.
3
0
9
1
:
v
i
X
r
a

2

B. Karki, et al.

includes regular tables, structured grids of information, lists of information, and property-value tables. In

summary, we make the our paper makes the following contributions:

• a decomposition of QA-from-tables into individual tasks and providing insights on each task, and
• experimental results that deal with issues pertaining to limited data by extensive feature engineering

and application of machine learning models in an intelligent manner.

2 RELATED WORK

One of the earlier works in using structured data as knowledge sources [8] focuses on exploring tables
as semi-structured knowledge for multiple-choice question (MCQ) answering. The tables contain text in

free form and are mostly general knowledge based facts. They contain answers to more than 9000 crowd-

sourced questions in their base dataset AI2’s Aristo Tablestore. The authors develop a feature-driven QA

model that uses MCQs while performing fact checking and reasoning over tables. Similarly, our models

heavily depend on features from text in tables to solve individual tasks. Hayati et al. [6] use semantic and

n-gram similarity for retrieval. They introduce ReCode, a sub-tree retrieval based framework for neural code

generation using natural language. They use dynamic programming based methods to retrieve semantically

similar sentences, then construct the syntax tree using n-gram of action sequences. The importance of semantic

similarity features was noted and we experiment with incorporating semantic similarity features in our models.

Cheng et al. [3] introduce a neural semantic parser that uses predicate-argument structures to convert natural

language utterances to intermediate representations. These representations are then mapped to target domains.

Similarly, we break down most of our questions into a projection column and conditional row to construct

the structured query. Waltz et al. [15] builds a natural language QA system that produces answers a large

relational database of aircraft flight and maintenance data using a system called PLANES. The system uses a

number of augmented transition networks, each of which matches phrases with a specific meaning, along

with context registers (history keepers) and concept case frames; these are used for judging meaningfulness

of questions, generating dialogue for clarifying partially understood questions, and resolving ellipsis and

pronoun reference problems. While we do not use exact ’phrase matching’ to specific meanings, we employ

machine learning models to infer similar meanings from the questions. Chen et al. [4] proposes to tackle open

domain QA using Wikipedia as the knowledge source. Their approach combines a search component based on

bigram hashing and TF-IDF [13] matching with a multi-layer recurrent neural network model trained to detect

answers in Wikipedia paragraphs. We also employ used TF-IDF[13] for feature extraction used Wikipedia as

a source for some of our data tables. A study by Srihari et al. [14] shows that low-level information extraction

tasks, like Named Entity tagging, are often a necessary components in handling most types of questions. We

also use Named Entity tagging as one of many information extraction features for classification at different
steps of our pipeline.

WikiSQL and Seq2SQL (Zhong et al.) [16] use a deep neural network for translating natural language

questions to corresponding SQL queries. WikiSQL consists of 80654 hand-annotated examples of questions

Question Answering via Web Extracted Tables and Pipelined Models

3

and SQL queries distributed across 24241 tables from Wikipedia, which is an order of magnitude larger than

other comparable datasets. The Seq2SQL model uses rewards from in-the-loop query execution over the

database to learn a policy to generate the query. A pointer network limits the output space of the generated

sequence to the union of the table schema, question utterance, and SQL keywords. Our dataset is similar
WikiSQL but distinguishes between different types of tables. Because of our smaller size dataset we break

down the approach into tasks that can be solved with per-task machine learning models. This approach

essentially eliminates logical form errors that are prevalent in neural models.

3 DATA COLLECTION

3.1 Collecting questions

We created Amazon Mechanical Turk (AMT) tasks that asked workers about "5 questions you recently had
where you searched for answers on the web"1. Workers were asked to generate questions from their past
by going through their browser history, as opposed to other generation methods [8, 11] where workers are
primed to ask questions based on a given table. We did not limit workers to questions that have an answer in
tabular form to facilitate realistic questions. Each worker was asked for 5 triples composed of (i) a natural

language question, (ii) the URL of a page that contained the answer to the question, and (iii) the answer to the

question. Workers were paid $1.75 per question. In a pre-study we observed that workers generated a large

amount of data about sports and weather, leading to bias in data collection. We therefore amended the task

description to ban weather and sports questions.

3.2 Collecting tables and SQL queries

Collecting tables that contain the answer to our questions and corresponding SQL queries requires some

expertise and skill (i.e. identifying tabular data, extracting a table and writing an SQL query). We hence

recruited several students from a Computer Science Department. For each new question (previously collected

through AMT), 1) search the web for a page with tabular data containing the answer. 2) Extract the relevant
table from that page. Workers used two different tools to extract tables: import.io2 and SmartWrap [5]).
3) Write an SQL query to extract the answer from the table. The costs associated with this elaborate, but

necessary, process limited the size of our eventual dataset. If any of the three steps failed for some reason, the

question was discarded.

Upon examining the results, we noted that in many cases the SQL query substituted a word in the question

with a related word in the WHERE clause of the query to insure that the LIKE operator would work. For
example, "Who is the husband of Whoopi Goldberg?" would result in WHERE value LIKE "spouse"
because a row in the table was identified using the word spouse. This SQL query would require the system to

understand some relationship between the concept of husband and the concept of spouse. We capture this issue

1https://cmu-rerc-apt.github.io/QASdatacollection/examples.html
2Commercially available at: https://www.import.io

4

B. Karki, et al.

Fig. 1. An example of entity-instance table

by introducing a word embedding proximity operation [2] ∼. This operator measures the distance of every
value in the column with the given value using the word embedding. Any value with a distance under a fixed
threshold is considered a match. Since husband and spouse are near each other in a typical word embedding3,
rewriting the clause to WHERE value ∼ husband greatly simplifies the learning process, since this logical
form only requires learning which token or tokens to copy from the natural language question to the clause.
This form in a sense moves the concept mapping problem to the implementation of the ∼ operator.

3.3 Dataset Analysis

We collected 341 questions with associated table, SQL query, and expected answer. We divided the dataset

into training set (238 examples), dev set (64 examples) and test set (39 examples). Each question is unique but
some questions might be semantically equivalent (e.g. When is easter this year? and What day is easter on
this year?) or very close (e.g. What is the capital of Louisiana? and What is the capital of Portugal?). Similar
questions question may be answerable using a different table than the one it is paired with. All questions can
be answered via a unique table (i.e. the SQL JOIN operator never appears in our dataset).

A survey led us to distinguish two categories of tables: entity-instance and key-value. An entity instance

table contains information about several entities (e.g. Figure 1 is an entity instance table, it contains the

presidency information of different presidents in the history of the USA), while a key-value table is related to

a single entity (e.g. Figure 2 is a key-value table and it shows some basic information of Donald Trump).

We removed 64 tables from the full set a wide variety of reasons.

3We used word vectors pre-trained on Google news available at https://code.google.com/archive/p/word2vec

Question Answering via Web Extracted Tables and Pipelined Models

5

Fig. 2. An example of key-value table

Fig. 3. System Design Overview

Word2Vec limitations: Occasionally, word embeddings return an incorrect nearest neighbor. For instance,

the Query SELECT "President" FROM "US_Presidents"
WHERE (("President" ∼ "current president")), the nearest neighbor for "president" is "Ulysses
S. Grant" instead of "Donald Trump".

Ambiguous labelling: Some tables had answers that had a lack of clarity regarding how the answer could
be obtained without recourse to external world knowledge. For the question, How do I get a refund for Social
Security Tax erroneously withheld, the table had multiple rows on refunds, but not the context to correctly
formulate an SQL query that satisfied the labelled answer.

Incorrect labelling: Few of the collected tables did not match with the query, or did not make sense.

Occasionally, queries were also formulated incorrectly.

Complex and edge cases: We explain multiple scenarios in detail in Section 8.1, in addition nested queries

and those requiring additional operators like AND, OR did not work well.

Badly transposed tables: Some tables did not allow the answer to be extracted using an SQL query after
being transposed (see Section 5) due to data loss, non-symmetric tables and data corruption during the

operation.

4 SYSTEM DESIGN OVERVIEW

Our approach to the question answering problem follows a granular approach. We divide the problem into

tasks based on the structure of the SQL query desired.

6

B. Karki, et al.

(1) Table Type Recognition

(2) Table Transpose

(3) Source Selection

(4) Complete SELECT Clause

(5) Complete WHERE Clause
(6) Complete ORDER BY Clause

(7) Complete LIMIT Clause

As shown in Figure 3, we build the SQL query step by step, passing along key information from one

step to the next in the pipeline. In the first step, we identify the type of each table, and classify it as either

entity-instance or key-value. We then use this information to transpose all the key-value type tables into an

equivalent entity-instance type table. For each question, we try to determine which table contains the answer.

Once found, we pair the question and table to form the FROM clause for the query. We then extracting which

column or columns from the table need to be present in the query, i.e. the SELECT clause of the query.

Given the correct columns, we predict the correct the row or rows, i.e. the WHERE clause of the query. This

produces a logically correct and complete SQL query for most questions. However, some questions are such

that they require to be ordered by a certain column, or require a fixed number of rows as the answer. For
example, Who is the current President of USA? needs to be ordered by year, and Who are the top 5 goal
scorers in La Liga? needs a specific number of rows. Hence, the next steps are to determine the columns used
for sorting (i.e. the ORDER BY clause of the query) and the number of rows to be displayed (i.e. the LIMIT

clause of the query).

After performing all these steps, we obtain all the clauses needed to construct a SQL query which

corresponds to the given question and can obtain answers by executing it against the appropriate table. This

approach lays bare the reasoning required in each step. However, this approach also propagates error from

one step to the next, in an additive fashion.

5 EXPERIMENTAL DESIGN

For each task, we analyzed our dataset, extracted features and built an individual model. We used different

metrics to evaluate the performance at every step

5.1 Machine Learning Models/Algorithms

5.1.1 Table Type Recognition. The type of the table strongly informs the content of the SQL query, so
we train different classification models with the following features to classify tables as entity-instance or
key-value tables:

Number of columns: In most cases, key-value tables only contain a "key" column and a "value" column.
If the number of table columns is equal to 2, then it most likely is a key-value table. We ignored columns

Question Answering via Web Extracted Tables and Pipelined Models

7

spouse

born

height net

education

14,
June
1946 (age
71 years),
Jamaica
hospital
medical
center,
New York
city, NY

Melania
Trump
(m. 2005),
Marla
Maples
(m. 1993-
1999),
Ivanka
Trump
(m. 1977-
1992)

6’ 3”

worth
3.1
billion
USD
(2018)

Wharton
School
of
the
University
Penn-
of
sylvania
(1966-
1968)
more

,

Table 1. Key-value table after transposing to entity-instance table

containing only URL in tables for this computation. Columns of this type are a side effect of the process that

extracts the table from the web page.

"key" or "property" in columns headers: We observe that most of the key-value tables contain keyword

"key" or "property" in the header of "key" column.

Normalized variance of content length by number of words: We first calculate the number of whitespace-
separated words within each cell, normalize within the column by the maximum cell length, and determine

the variance. For key-value table, the cells within the same column are different attributes of a single entity,

leading to a lack of uniform patterns for cell content. Therefore, the cell lengths of the same column tend to

be diverse for key-value tables. For entity-instance tables, the content length of each cell in the same column

tends to be similar because each column of the entity-instance table corresponds to a specific attribute and it

usually has a similar format in the context of the table. If each column in a table has similar length the table is

likely to be classified as an entity-instance table.

The normalized variance of presence of digits: Similarly, since each column of entity-instance tables
corresponds to a specific attribute, the pattern of the presence of digits would be similar for each cell within

the table. As can be seen from Figure 1, the first column is the presidency period, and each cell of that column

contains a digit-based description of two years description and two days, so that every cell in that column

would contain digits. However, for key-value tables, since cells in the same column correspond to different

attributes, the uniform pattern of the presence of digits generally does not hold. If none of the cells or all

the cells contain digits for each column, the variance would be zero. The higher the value of this feature is,

the more random the presence of digits in a column is. Since the columns of entity-instance tables are more

likely to have a uniform pattern of the presence of the digits, a higher variance in the presence of digits is an

indicator that the table has a higher probability be a key-value table.

5.1.2 Tables Transpose. We transpose all key-value tables into entity-instance tables for uniformity.
Table 1 shows an original key-value table that contains information about Donald Trump that has been

8

B. Karki, et al.

transposed into entity-instance. For key-value tables, the goal of the SQL query is to find the target row, while

for the transposed entity-instance table, the goal is to find the target column. The ground truth SQL query
for the pre-transposed table was "SELECT Value FROM Donald-Trump WHERE Key ∼ birthday",
and the ground truth SQL query for transposed table 1 is "SELECT born FROM Donald-Trump". All
post-transpose algorithms hence only need to consider entity instance tables.

5.1.3 Source Selection. To answer a specific question, we first need to correctly identify the source
table in the dataset. For question preprocessing, we lowercase all characters and tokenize on whitespace and

non-alphanumerics. We then used NLTK [1] to remove stop words and stem the remaining words. We apply

the same method to each tables cell. We calculate the TF-IDF of word stems in questions and tables. The

inverse document frequency (IDF) of each word stem was calculated on all tables, and the term frequency

(TF) of each word stem for each question or table was calculated in the scope of that specific question or

table.

We explored three different approaches to measure the similarity of the question and the table in the TF-IDF

vector space: (1) Cosine similarity of the two vectors, (2) Dot product of the two vector, and (3) Inverse

Euclidean distance of the two vectors (after projected to points in multi-dimensional space).

5.1.4 Column Projection in SELECT Clause. The first choice in composing the SQL query is to
determine which columns belong to the SELECT clause. This task can be treated as a binary classification

task: given the question and a column in its source table, determine whether this column should be included

in the SELECT clause. The input of our classifier are the following features, which are extracted from a pair

of question and column to be processed:

Number of columns in table: The number of columns one table contains influences the probability that
any column within the table being included in the SELECT clause significantly. If the table only has one

column, then the only column must be included in the SELECT clause to construct the SQL query. As the

number of columns in the table increases, the probability of each column being included in the SELECT

clause decreases.

Similarity of column contents and question using word vectors: We calculate 4 types of proximity: the
average proximity of the column contents and the question, the average proximity of the column contents

and the question without stop words, the maximum proximity of the column contents and the question, and
the maximum proximity of the column contents and the question without stop words. For example, if the
question is Who is the president of the USA?, the word "president" would be close to "Donald Trump" in
word vector space and the column containing "Donald Trump" is exactly the column we want to select.

Column Types: To be able to learn answers for different question categories, we built a model for
recognizing the column data type. The feature is a 7-dimensional vector with the probability distribution

among the 7 column types.

Question Types: We classified questions based on Li et al.’s work[9] to produce 6 major types : "AB-
BREVIATION", "ENTITY", "DESCRIPTION", "HUMAN", "LOCATION" and "NUMERIC" and also

Question Answering via Web Extracted Tables and Pipelined Models

9

included yes/no questions as a seventh major type. In addition, the minor types for "NUMERIC", namely,

"date", "count", "period" and "money" were used as the dataset contains many factoid questions. We used

the classification API from Madabushi. et.al[10]. Their API extends Li et al.’s work [9] and provides usable

question type results. We augmented these results by heuristic methods and achieve 89.2% accuracy on
the training set and 96.1% accuracy on the dev set. The mapping for question types and column types are

generally intuitive. For example, for questions about the price of some product the column in the SELECT

clause should have the "Currency" type. Our final representation has 11 different question types.

Semantic similarity of column header and question: We include a feature measuring the similarity of
the column header and the question. We applied same pre-processing technique to the column headers. For

each word stem in the column header and each word stem in the question, we calculated the character level

edit-distance of the two word stems. By surveying our dataset we found that there may be several column
headers in the source table that all contain the same question words. For example, a question asks about "What
is NAIRU?", and the column headers of the first three columns of the source table are "What is NAIRU?
CONCEPTS", "History of NAIRU" and "Different concepts of NAIRU". If we only include the minimum

edit distance, then the distance between any of these column headers and the question is equal to zero and

the feature is useless. To increase the robustness of our feature, we include both the minimum distance and

second lowest distance as features for all word stem pairs.

A concatenation of all features forms a 25-dimensional vector as model input. Our classifier is a multi-layer

perceptron (MLP) with 4 hidden layers (ReLu activation, separated by Batch Normalization layers, hidden

sizes 32, 16, 8, 2 from bottom to the top). We optimize our model using stochastic gradient descent and

cross-entropy loss. Since the number of positive cases is much smaller than the number of negative cases

in our train dataset (among the total of 2046 columns, 273 are included in SELECT clause, 1773 are not

included in SELECT clause), we duplicated the positive cases 6 times to balance our training data.

5.1.5 WHERE Clause Projection. The next component of the SQL query is the WHERE clause, formed
by a pair of column and key word. Similar to the task of column projection in SELECT clause, we build a

binary classifier which takes the question, one column within the table, and one question word as input. The

classifier outputs 1 if and only if the input column and the word both appear in the WHERE clause of the

SQL query. We build the following feature vector:

Minimum normalized edit-distance between input question word and words within input column:
For the WHERE clause, the input column exploits the input word to constrain the query result to some specific

rows within the table. If the input question word appears in the input column, then the probability of this

column and question word pair forming a valid WHERE clause increases.

Average character-level cell length of input column: Since it is hard for humans to find key information
in extended content, a column with lower average cell length is more likely to appear in the WHERE clause.
Number of rows of input column: If the input column only contains a single row, then we may not need
to have a WHERE clause in the SQL query. The classifier in this case would have a higher chance to output 0.

10

B. Karki, et al.

Whether the input column has already been included in SELECT clause: If the input column has
already been included in SELECT clause, then it is less likely that it is also used in WHERE clause. We used

the ground truth instead of the predicted results for the SELECT clause to build this feature, this decision

isolates the prediction errors from the previous model from the current model.

Column Types and Question Types: We used the same column types feature and question types features
as we used in SELECT clause task. In SELECT clause task, for one type of question, the probability that one

corresponding type of column to be included in SELECT clause is higher than the remaining types. However,

for the WHERE clause task, the probability of the column with that specific type appearing in the WHERE

clause is relatively low. For example, for "When" type question, it is unlikely that we still use a column with

"DateTime" type to filter the query result.

POS, NER, and dependency parsing of input word: Common natural language processing features
may give us some clues about the probability of the input word being included in WHERE clause. For

parts-of-speech (POS) tagging, some types of words are more likely to be used as keyword in WHERE

clause than other types of words. For example, nouns have a higher probability to be selected as the search

word in the WHERE clause than adjectives and verbs. Similarly, for named-entity-recogition (NER) tagging,

entities such as "PERSON" are more likely to be selected as the search word compared to other entities, like

"DATETIME". Moreover, dependency parsing helps us to identify the root word in a sentence, which has

a higher chance to be included in the WHERE clause. Therefore, we included POS and NER tagging and

dependency parsing of the input word as features in one-hot encodings. There are a total of 12 POS features,

6 NER features and 37 dependency parsing features.

Using these features, we applied the same MLP model to the WHERE clause task as used for the SELECt

clause task, except that the input size was increased from 25 to 77. We also performed up-sampling to balance

our training data.

belongs to.

5.1.6 Column Type Recognition. We pre-defined 7 column types, "DateTime", "Currency", "Percentage",
"Numerical", "Boolean", "Text" and "URL". We manually labeled all the columns in the train dataset (total of

2046 columns). Our task is that given the content of a column in the table, determine which type this column

For every column, we extract the following features to cater to specific question types: Proportion of

cells that can be directly converted into a valid numerical value; Proportion of cells that contain only digits;

Proportion of cells that contain currency symbols, percentage symbols, boolean values (yes, no, true, false),

years (1500-2020), month strings and abbreviations, weekday strings, and the ‘http’ string to detect URLs.

We build an MLP for column types recognition using 3 hidden layers (ReLu activation, sizes of 32, 32 and 7

from bottom to top), followed by a Softmax layer. The output of the model is a probability distribution among

the 7 types given the input column. We assigned the column with the type that has the highest probability. We

achieve an accuracy of over 93% on our manually labeled training data.

Question Answering via Web Extracted Tables and Pipelined Models

11

5.2 End-to-end Pipeline

Given the separate models we have designed for generating the SQL query for a question, we built an

end-to-end question answering pipeline using the models we described in the previous section. The input of
the pipeline is a question and a collection of tables. The output of the pipeline is a collection of cells which

are used for answering the input question extracted from one table within the input collection of tables. The

system first retrieves a table using the TF-IDF similarity approach explained above. We then predict the

column(s) to be included in the SELECT clause as the answer-containing columns. For row selection, we

make use of the classifier we designed for WHERE clause. Since the prediction result of the WHERE clause

classifier is whether a pair of column of the table and a key word in the question should be included in the

WHERE clause, we need another processing step to map the WHERE clause prediction to rows selection. We
designed two different algorithms for this step in our experiment, Word-Match Based and Word2Vec Based.
Word-Match Based: For each column and key word pair which the classifier predicts as belonging to the
WHERE clause, we go over each cell within the column and, if there is a match between the word in the

cell and the word in the predicted pair, the row where the cell is located is assigned a score of 1. After all

column/word pairs are processed, we select the rows with the highest score. If the classifier predicts that the

WHERE clause is empty for the given table, then all rows are selected.

Word2Vec Based: We use pretrained GloVe embeddings [12] to map the key word and the words in the
columns to 300-dimensional vectors. For each cell in the column, we calculate the minimum Euclidean

distance between the word vectors in the cell and the vector of the key word. The row where the cell is

located is assigned with that minimum distance. After all the pairs are processed, the rows with the global

minimum distance are selected. If the WHERE clause is empty, or the key word doesn’t exist in the pretrained

embeddings, all rows of the table are selected.

5.2.1 Answer Cell Selection. After the columns and rows have been selected, their intersecting cells are
predicted as the answer.

5.3 Evaluation Metrics

entity-instance table.

Table Type Recognition: We calculated the accuracy of different machine learning models for table type
recognition to evaluate their performance on the classification of the input table as a key-value table or an

Source Selection: For the source selection task, we use precision at rank k (P@k) as the evaluation metric,
in which we calculated the precision of the ground truth source table being selected in the top N most similar
tables for a given question, with N = 1, 3, 5, 10.

SELECT Clause and WHERE Clause: For the SELECT clause and WHERE clause tasks, since we treat
each task as a machine learning classification problem, we use precision, recall, and confusion matrices to

measure the performance of our classifiers for train and dev set.

12

B. Karki, et al.

End-to-end Pipeline: For our end-to-end pipeline, we calculated the precision, recall and F1-Score of the

selected cells generated by our pipeline with the golden cells for each question.

6 EXPERIMENTAL RESULTS

6.1 Source Selection

Train Dataset

Dev Dataset

CosS DotP

InvEuc CosS DotP

InvEuc

P@1

60.2% 69.1% 70.2% 72.3% 72.3% 72.3%

P@3

78.5% 80.1% 80.6% 85.1% 87.2% 87.2%

P@5

84.8% 84.8% 85.9% 89.3% 91.5% 91.5%

P@10 89.0% 90.6% 91.6% 89.3% 93.6% 93.6%

We see that Inverse Euclidean distance performs best with an accuracy of 70.2% on train data and 72.3%
on dev data. After error analysis, we found that some error cases are not strictly wrong due to the fact that

multiple sources could be used to answer the same question. Considering these cases, the adjusted P@1

results using Inverse Euclidean distance method are 76.5% on train and 76.6% on dev data, respectively.

6.2 Table Type Recognition

The table type recognition task is straightforward and we achieve around 98.3% on train and 100.0% on dev

data across common models (logistic regression, decision trees, KNN).

6.3 SELECT Clause

Train Dataset

Dev Dataset

Predicted:1 Predicted:0 Predicted:1 Predicted:0

Actual:1

Actual:0

182

209

30

1001

12

297

Test Dataset

Predicted:1 Predicted:0

Actual:1

Actual:0

28

53

38

54

14

196

Train Dataset Dev Dataset

Test Dataset

Accuracy

Recall

Precision

83.2%

85.8%

46.5%

83.5%

76.0%

41.3%

77.0%

66.7%

34.6%

The tables above present confusion matrices along with accuracy, recall and precision results of predicting

the correct columns in the SELECT clause. Our model is able to achieve a good accuracy (83.2% on training

set, 83.5% on dev set and 77.0% on test set) and recall (85.8% on training set, 76.0% on dev set and 66.7% on

Question Answering via Web Extracted Tables and Pipelined Models

13

test set) performance, but suffers from low precision. Improving precision is a goal for future work. Arguably,

for this sub-problem, recall is more important than precision, because we want to make sure we include all

the targeted columns in our predicted SQL query so that users will not miss any information in the returned

result.

6.4 WHERE Clause

Train Dataset

Dev Dataset

Predicted:1 Predicted:0 Predicted:1 Predicted:0

Actual:1

Actual:0

95

106

0

4107

20

35

7

1372

Test Dataset

Predicted:1 Predicted:0

Actual:1

Actual:0

16

39

4

2270

Train Dataset Dev Dataset

Test Dataset

Accuracy

97.5%

Recall

100.0%

Precision

47.3%

97.1%

74.1%

36.3%

98.2%

80.0%

29.1%

From the three tables above, we can see that prediction accuracy for the WHERE clause is very high on

both training set (97.5%) dev set (97.1%) and test set (98.2%). However, for recall and precision, the good

performance on the train dataset does not generalize to the dev set and test set. This is mainly due to the sparsity

of the dataset: not all questions would require a WHERE clause in the converted SQL query. As we can see

from the confusion matrix, we only have 120 cases in the train dataset that contain WHERE clause. Hence
the model is not able to learn all the general rules to identify the correct "column ∼ question_word"
pair in the WHERE clause.

14

6.5 End-to-end Pipeline

B. Karki, et al.

Table Selection Scope: Golden Table

Train Dataset

Dev Dataset

Word2Vec Word

Word2Vec

Table Selection Scope: Individual Set

Train Dataset

Dev Dataset

Word2Vec Word

Word2Vec

Precision

Recall

F1

37.1%

79.8%

45.4%

31.3%

74.1%

38.4%

Precision

Recall

F1

26.5%

56.8%

32.6%

23.3%

55.6%

29.0%

Word

Match

Word

Match

Word

Match

Match

34.7%

72.3%

43.0%

Match

27.1%

51.1%

32.1%

Match

21.8%

44.7%

26.5%

25.2%

61.7%

30.8%

19.9%

44.7%

23.3%

17.8%

40.4%

21.2%

Table Selection Scope: All Sets

Train Dataset

Dev Dataset

Word2Vec Word

Word2Vec

Precision

Recall

F1

26.5%

56.3%

32.5%

23.4%

55.6%

28.9%

Table Selection Scope: Golden Table

Test Dataset

Word Match

Word2Vec

Precision

Recall

F1

29.5%

54.8%

32.0%

21.6%

50.0%

23.5%

Question Answering via Web Extracted Tables and Pipelined Models

15

Table Selection Scope: Individual Set

Test Dataset

Word Match

Word2Vec

Precision

Recall

F1

Precision

Recall

F1

24.7%

50.0%

27.2%

18.4%

38.1%

19.7%

Table Selection Scope: All Sets

Test Dataset

Word Match

Word2Vec

21.5%

45.2%

23.3%

15.1%

33.3%

15.8%

The above table shows the average precision, recall and F1-score of our end-to-end pipeline. We test our
pipeline under different experimental environments: (1) ‘Golden table’: we assume the correct table is selected
for generating the SQL query; (2) Individual Set: all tables in the training or dev or test set, respectively,
are available for retrieval; (3) All sets: Both training, dev and test set tables are available for retrieval. As

expected, with the increase of the table selection scope, the performance of the pipeline decrease. If the

incorrect table is selected at the first step, then there is no chance for the pipeline to find the correct cells

for the input question. Another thing to note is that the performance of using word match as rows selection

method is better than using word2vec as rows selection method.

7 ERROR ANALYSIS

7.1 Source Selection

When performing error analysis for Source Selection, we noticed that some predictions are not strictly
incorrect, although the predicted table is different from the actual table as defined in the ground truth. Some

pairs of tables contain similar information and could be used to answer the same question. The reasons why

we have similar sources in the dataset are due to three main causes: (1) There are some questions that are
semantically equivalent, e.g. When is Easter this year? and What day is Easter on this year?; (2) There are
some questions that are in the same type and are very close. e.g. What is the capital of Louisiana? and What
is the capital of New Jersey?; (3) There are some questions that are asking different information about the
same thing. e.g. What time does the Super Bowl start? and Where is the Super Bowl being played this year?.
Our experimental results do not account for this redundancy.

7.2 SELECT Clause

For a given question, we would like to know whether our model has successfully predicted all the targeted
columns in the SELECT clause, and whether our model has only included the targeted columns in the

16

B. Karki, et al.

predicted SELECT clause. Among all the 64 cases in the dev data, we found 6 (9.4%) exact matches between

the predicted SELECT clause and the actual SELECT clause. We have another 48 (75.0%) cases that we

include all the required columns but also provide additional columns. We have divided the error cases into six

main categories as explained below, all these cases are related to these two types of features to some extent.
Case 1: Question type detected wrongly (7 cases): Our question classifier achieves 89.2% accuracy on the
train set and 96.1% on the dev set. An example misclassification is the question What is Washington Wizards
record?, which should be identified as "NUMERIC". However, the question classifier detects this question as
"ABBREVIATION", therefore, the model would look for "Text" columns instead of "Numeric" columns.

Case 2: Column type detected wrongly (3 cases): The MLP column type predictor provides a good
accuracy performance of around 93% but, in some cases, due to some special format in the table content, fails
to correctly identify the correct column. For example, for the question How long do cats live?, the column
"Lifespan" is the targeted column which is supposed to be identified as "Numeric" type. However, as this
table only has one row, and the content in "Lifespan" column is "4-5 years (In the wild)" which has more
alphabets than digits, it is identified as a "Text" type.

Case 3: Multiple columns in targeted type (24 cases): In some cases, it is difficult for the model
to distinguish two columns in the same data type and with similar information in the absence of more
sophisticated semantic features. For example, for Question "How many centimeters in an inch?", the model
selects both "Centimeter" and "Inch" columns in the table. Both these two columns contain numeric data and

both column headers appear in the question. Moreover, for some questions even humans may need external
information to help us select the correct column. For example, for the question What is the population of
Boston MA?, the table has two population columns: "2018 Population" and "2016 Population", we would
need to know that 2018 is closer to the current year (or make a functionally equivalent temporal assumption).
Case 4: Location type detection needed for WHERE-type questions (5 cases): One of the limitations
in the current column type recognition model is that it is not able to further segregate "Text" type into

sub-classes which includes "Location", "Person", "Description", "Instruction" etc. As a result, we are not able

to get a high-accuracy performance for WHERE type questions. We intend to remedy this in future work.

Case 5: Human Entity type detection needed for WHO-type questions (2 cases): Similarly, we would

need the ability to identify the "Person" type to correctly answer WHO type questions.

Case 6: Binary questions (4 cases): The assumption we made regarding the relationship between question
types and column types fails for binary questions where the answers are either "yes" or "no". We intend to

remedy this in future work.

7.3 WHERE Clause

In addition to the column-level performance as shown in Section 8.4, we also checked question-level

performance on dev dataset: Among the 64 cases in dev dataset, we have 34 (53.1%) exact matches between

the predicted WHERE clause and the actual WHERE clause. We also have another 8 (12.5%) cases (i.e.

Case 1 and Case 2 below) are not considered as wrong because using the predicted WHERE clause also

Question Answering via Web Extracted Tables and Pipelined Models

17

generates the same result as the actual SQL query. After performing error analysis for WHERE clause, we

have identified 4 main categories as explained below.

Case 1: Single row tables (7 cases): For tables that have only a single row, the result would be same
whether we include a WHERE clause or not in the SQL query. Most of the single row tables are generated
after transposing the key-value tables.

Case 2: Slight difference on search keyword (1 case): Sometimes there may be a slight difference
between the predicted search keyword in WHERE clause and the actual one. However, our ∼ operator may
mitigate this problem. For example, for the question What is Washington wizards record?, the WHERE clause
in the actual SQL query is Team ∼ Washington wizards. Our predicted WHERE clause is Team ∼ Washington.
Since in the actual table, the "Team" column only contains "Washington" instead of "Washington wizards",

our predicted WHERE clause would return the same result as the one in the actual SQL query.

Case 3: External information required (4 cases): Some cases would need external information to form
the correct SQL query. For example, for the question Who is the actress that plays Sheldon’s mother? (aside
from the implicit focus on the Big Bang Theory TV show) we would need to have the knowledge that
Sheldon’s mother is Mary Cooper before we could form the correct WHERE clause which is "Character
LIKE Mary Cooper".

Case 4: Incomplete search keyword due to stop-word removal (3 cases): A few cases were predicted
wrongly due to stop-word removal. For example, for the question How many feet are in a mile?, the expected
WHERE clause is "Mile ∼ a mile", but the predicted WHERE clause is "Mile ∼ mile" as stop-word "a"
is removed.

7.4 Row Selection: Word Match vs. Embeddings

In the end-to-end pipeline, we use two methods for answer row selection from the predicted WHERE clause.

For word-match based algorithm, errors mainly stem from a failed match between key word in the where

clause and the word in the cells. The word in the cells may exist as a synonym of the key word instead of

the key word and therefore leads to the failure of the algorithm. For word2vec similarity, errors are mainly

caused by irregular forms of the key word in the WHERE clause. Through experiment, we found that the key

word can be a foreign language word, a number plus a unit, or some other irregular form, all of which do not

exist in the pretained word embeddings we use. Overall, this type of semantic mismatch is severe and leads to
the word match method performing best.

8 DISCUSSION

Data Collection: Table name and column headers were not always available for the tables we collected from
the web. In this case, the students collecting the tables were instructed to come up with a table name and/or

column headers that was relevant regarding the context (i.e. table, web page). However, the students also knew

the question that the table was supposed to answer. The column headers, therefore, could have information

“leaking” about the answer that has to be returned for a question. When these questions are in the dev set,

18

B. Karki, et al.

the gain in accuracy with this leaked information is difficult to gauge. The application scenario assumes that

tables are generally extracted independently of the question itself. In future work we plan to tackle this by

means of automated column header generators.

Table Transpose: Transposing key-value tables into entity-instance tables provides us with a more homo-
geneous structure and makes it easier to run Machine Learning models. Most of the transposed key-value

tables have only a single row. However, we also have a small number of transposed key-value tables with

multiple rows (e.g. comparisons of consumer electronics’ specifications with products as columns and features

as rows).

Source Selection: Table selection using inverse Euclidean distance achieved 70.2% P@1 on the train and
72.3% on the dev dataset. Since we work with similarities based on bag-of-words TF-IDF vector, the model

is prone to error from language nuances in the question. Closely related questions where only one word has

been changed are often misclassified.

SELECT Clause: Our MLP model for the SELECT clause problem produced a precision of 46.5% and
41.3% respectively for train and dev set. The recall of 85.8% and 76.0%, respectively, is a good result

and provides opportunity for future work in reranking the results using additional features. Errors for the

SELECT clause problem arose from multiple sources: (1) misclassification of the question and/or column

type, which may be improved by using pre-trained models or clustering; (2) lack of granularity for column

type recognition, such as those identifying a location or a human entity; (3) ambiguities that resulted from a

table having multiple columns of the same type; (4) yes/no questions which required boolean inference from

the predicted cells.

WHERE Clause: For the WHERE clause prediction, precision is most important and our model achieved
47.3% and 36.3% for train and dev sets, respectively. Due to the data sparsity issue in our current dataset, it is

not able to generalize from training to dev data. Some errors in misclassification of (column, question word)

pairs for the WHERE clause problem were on tables that had a single row, which would could be mitigated
using a default-row implementation. We use a ∼ operator to identify the column that most closely resembles
the word in the question we need to consider. Therefore small errors in the selection of the question word

will also strictly produce incorrect final answers. We pre-process the question to remove stop words from

consideration. However, in some cases, the final result required their use.

8.1 Complex and Edge cases

refine our model in future work.

There are some complex cases or edge cases need to be further studied after/while we collect more data and

AND/OR operator: For some questions, we may need to put AND or OR operators in the converted SQL
query. For example, for Question "What year was Brock Lesnar’s last UFC fight?", the table contains two
(unordered) columns containing contestant names, thus the ground truth SQL query is SELECT "Date"
FROM "Brock Lesnar_Tabe" WHERE
(("Fighter_1" ∼ "Brock Lesnar") OR

Question Answering via Web Extracted Tables and Pipelined Models

19

("Fighter_2" ∼ "Brock Lesnar")) AND
"Event_Name_1" ∼ "UFC" ORDER BY "Date"
DESCENDING LIMIT 1

Sub query: For some questions, we would need to use sub-query to generate a complete SQL query to
locate the correct answer. For example, for Question "Who became president after John Kennedy?", the
ground truth SQL query is SELECT "President" FROM "List of Presidents
of the United States" WHERE "Number" >

(SELECT "Number" FROM "List of Presidents
of the United States" WHERE "President" ∼ "John Kennedy") ORDER BY "Number"
ASCENDING LIMIT 1

Aggregate function: For some questions, we may not be able to find a direct answer in the table. However,
we could get the answer using aggregation functions in SQL. For example, for question "How many science
jobs in Rochester NY?", the ground truth SQL query, for a table containing science jobs, is SELECT COUNT
(Title) FROM "Table_1" WHERE "Location" ∼ "Rochester NY"

Questions with answers change over time: We also have questions that have ambiguities due to under
specification such that the answers change with time. For example, for Question who won the super bowl?,
the answer would be changed by year. Assuming that the table would be updated so that it contains the

most recent record, we have two ways to solve this problem to make sure we always get the most updated
answer: (1) We could sort the table by year and get the most recent one. i.e. ORDER BY year DESCENDING
LIMIT 1; (2) We could call an external function which returns the current year from the environment.
i.e. WHERE year = EXTERNAL ("current year"). The latter approach also applies to questions that
require users’ location. For example, "What is the best restaurant near me?".

9 FUTURE WORK

We need to add more questions and tables to the data set. We have observed that in addition to enabling

effective use of machine learning techniques, we infer valuable insights from individual tables themselves.

Moreover, we also need to have enough training examples to identify the correct features and models to be

used for ORDER BY and LIMIT clause problems. A much larger dataset would enable a single large deep

neural network that encorporates all the types of reasoning in our current pipeline.

Some question types such as WHERE and WHO have a lower accuracy performance compared to other

question types. This poor performance results from low granularity on column type recognition. We need

to study how to break down "Text" columns into several different types such as "Location", "Person",

"Description", "Instruction", etc.

Although semantic inference problems are not easy to solve, from our error analysis, we know that we

are able to further improve the system performance by incorporating more semantic features. This semantic
analysis could help us to better answer the binary questions, and also questions like How many centimeters in
an inch?

An alternative or addition to expanding the data set is using pre-trained models such as those used in [16].

When we have multiple sources that might answer a question, we should evaluate the top k sources and see

if we get the best answer. We could also explore if a composition of re-ranked results could generate a better

B. Karki, et al.

20

answer.

10 CONCLUSION

We proposed to understand the problem of question-answering from structured sources by forming a dataset

for modelling and providing well-defined sub problems. In this work, we were able to provide the division for

tasks and implement baseline models for each tasks. We also performed error analysis on each task model.

The source selection problems was solved with IR techniques. Both solutions were reasonably accurate

and further performance improvements may be gained with fine-tuning.

The SELECT clause problem can be seen as a standard question answering problem that involves mapping
question types and column types effectively. We observed limitations with type classification. We also

observed problems with disambiguation with multiple matches and semantic inference problems.

The WHERE clause problems proved the hardest as question type knowledge did not have a direct effect

on the results. Further, this problem needed both column and the question word to be guessed correctly and

therefore the error for the WHERE clause model is higher.

Our approach by dividing the problem into several well-defined tasks makes it easier to identify the error

causes and interpret required areas of focus. After collecting more data and building models for ORDER BY

and LIMIT clauses, we would like to complete the framework integration.

REFERENCES

O’Reilly Media, Inc.", 2009.

[1] S. Bird, E. Klein, and E. Loper. Natural language processing with Python: analyzing text with the natural language toolkit. "

[2] R. Bordawekar and O. Shmueli. Enabling cognitive intelligence queries in relational databases using low-dimensional word

embeddings. arXiv preprint arXiv:1603.07185, 2016.

[3] J. Cheng, S. Reddy, V. Saraswat, and M. Lapata. Learning structured natural language representations for semantic parsing. arXiv

[4] J. W. . A. B. Danqi Chen, Adam Fisch. Reading wikipedia to answer open-domain questions. arXiv preprint arXiv:1704.00051,

preprint arXiv:1704.08387, 2017.

2017.

All Conference, page 3. ACM, 2015.

arXiv:1808.10025, 2018.

[5] S. Gardiner, A. Tomasic, and J. Zimmerman. Smartwrap: seeing datasets with the crowd’s eyes. In Proceedings of the 12th Web for

[6] S. A. Hayati, R. Olivier, P. Avvaru, P. Yin, A. Tomasic, and G. Neubig. Retrieval-based neural code generation. arXiv preprint

[7] Q. V. L. Ilya Sutskever, Oriol Vinyals. Sequence to sequence learning with neural networks. In arXiv:1409.3215, 1999.

[8] S. K. Jauhar, P. Turney, and E. Hovy. Tables as semi-structured knowledge for question answering. In Proceedings of the 54th

Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 474–483, 2016.

[9] X. Li and D. Roth. Learning question classifiers. In Proceedings of the 19th International Conference on Computational Linguistics

- Volume 1, COLING ’02, pages 1–7, Stroudsburg, PA, USA, 2002. Association for Computational Linguistics.

Question Answering via Web Extracted Tables and Pipelined Models

21

[10] H. T. Madabushi and M. Lee. High accuracy rule-based question classification using question syntax and semantics. In COLING,

2016.

[11] P. Pasupat and P. Liang. Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305, 2015.

[12] J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference

on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014.

[13] J. E. Ramos. Using tf-idf to determine word relevance in document queries. In Proceedings of the first instructional conference on

[14] W. N. I. W. N. Srihari, Rohini ; Li. Information extraction supported question answering. In Defense Technical Information Center,

[15] D. L. Waltz. An english language query answering system for a large relational data base. In Communications of the ACM

machine learning., 2003.

Accession Number: ADA460042, 1999.

21(7):526-539, 1978.

arXiv preprint arXiv:1709.00103, 2017.

[16] V. Zhong, C. Xiong, and R. Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning.

