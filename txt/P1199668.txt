Linear Bandits with Stochastic Delayed Feedback

Claire Vernade 1 Alexandra Carpentier 2 Tor Lattimore 1 Giovanni Zappella 3 Beyza Ermis 3
Michael Brueckner 3

Abstract

of the space where the action vectors lie.

0
2
0
2
 
r
a

M
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
9
8
0
2
0
.
7
0
8
1
:
v
i
X
r
a

for

structured

Stochastic linear bandits are a natural and
well-studied model
explo-
ration/exploitation problems and are widely used
in applications such as online marketing and
recommendation. One of the main challenges
faced by practitioners hoping to apply existing
algorithms is that usually the feedback is
randomly delayed and delays are only partially
observable. For example, while a purchase is
usually observable some time after the display,
the decision of not buying is never explicitly sent
to the system. In other words, the learner only
observes delayed positive events. We formalize
this problem as a novel stochastic delayed linear
bandit and propose OTFLinUCB and OTFLinTS,
two computationally efﬁcient algorithms able to
integrate new information as it becomes available
and to deal with the permanently censored
feedback. We prove optimal ˜O(d
T ) bounds
on the regret of the ﬁrst algorithm and study the
dependency on delay-dependent parameters. Our
model, assumptions and results are validated by
experiments on simulated and real data.

√

1. Introduction

Content optimization for websites and online advertising are
among the main industrial applications of bandit algorithms
(Chapelle & Li, 2011; Chapelle, 2014). The dedicated ser-
vices sequentially choose an option among several possibil-
ities and display it on a web page to a particular customer.
In most real world architectures, for each recommendation
request, the features of the products are joined and hashed
with those of the current user and provide a (ﬁnite) action
set included in Rd. For that purpose, linear bandits (Chu
et al., 2011; Abbasi-Yadkori et al., 2011) are among the
most adopted as they allow to take into account the structure

1DeepMind, London, UK 2Otto-Von-Guericke Universitt,
Magdeburg, Germany 3Amazon, Berlin, Germany. Correspon-
dence to: Claire Vernade <vernade@google.com>.

A key aspect of these interactions through displays on web-
pages is the time needed by a customer to make a decision
and provide feedback to the learning algorithm, also known
as the conversion indicator (Chapelle, 2014; Diemert et al.,
2017). For example, a mid-size e-commerce website can
serve hundreds of recommendations per second, but cus-
tomers need minutes, or even hours, to make a purchase. In
(Chapelle, 2014), the authors ran multiple tests on propri-
etary industrial datasets, providing a good example of how
delays affect the performance of click-through rate estima-
tion. They extract 30 days of display advertising data and
ﬁnd that delays are on average of the order of several hours
and up to several days.

On the other extreme of the time scale, some companies
optimize long-term metrics for customer engagement (e.g.,
accounting for returned products in the sales results) which
by deﬁnition can be computed only several weeks after the
bandit has played the action. Moreover, after a piece of
content is displayed on a page, the user may or may not
decide to react (e.g., click, buy a product). In the negative
case, no signal is sent to the system and the learner cannot
distinguish between actions for which the user did not click
and those where they did, but the conversion is delayed.
Note that in the use cases we consider, when a feedback is
received, the learner is able to attribute it to the past action
that triggered it.

Two major requirements for bandit algorithms in order to be
ready to run in a real online service are the ability to leverage
contextual information and handle delayed feedback. Many
approaches are available to deal with contextual information
(Abbasi-Yadkori et al., 2011; Agarwal et al., 2014; Auer
et al., 2002; Neu, 2015; Beygelzimer et al., 2011; Chu et al.,
2011; Zhou, 2015). Delays have been identiﬁed as a major
problem in online applications (Chapelle, 2014). We give an
overview of the existing literature in Section 7. However, to
the best of our knowledge, no algorithm was able to address
this problem given the requirements deﬁned above.

Contributions Our main contribution is a novel bandit
algorithm called On-The-Fly-LinUCB (OTFLinUCB).
The algorithm is based on LinUCB (Abbasi-Yadkori et al.,

Linear Bandits with Stochastic Delayed Feedback

2011), but with conﬁdence intervals and least-squares esti-
mators that are adapted to account for the delayed and cen-
sored rewards (Section 3). The algorithm is complemented
by a regret analysis, including lower bounds (Section 5). We
then provide a variant inspired by Thompson sampling and
Follow the Perturbed Leader (Section 6) and evaluate the
empirical performance of all algorithms in Section 8.

2. Learning Setting under Delayed Feedback

Notation All vectors are in Rd where 2 ≤ d < ∞ is
ﬁxed. For any symmetric positive deﬁnite matrix M and
vector x ∈ Rd, (cid:107)x(cid:107)M =
xT M x and (cid:107)x(cid:107)2 = (cid:107)x(cid:107)I is the
usual L2-norm of x, where I denotes the identity matrix in
dimension d.

√

Learning setting Our setup involves a learner interacting
with an environment over T rounds. The environment de-
pends on an unknown parameter θ ∈ Rd with (cid:107)θ(cid:107)2 ≤ 1 and
a delay distribution D supported on the natural numbers.
Note, in contrast to Vernade et al. (2017), we do not assume
the learner knows D. Then, in each round t,

1. The learner receives from the environment a ﬁnite set
of actions At ⊂ Rd with |At| = Kt < ∞ and a(cid:62)θ ∈
[0, 1] and (cid:107)a(cid:107)2 ≤ 1 for all a ∈ At.

2. The learner selects an action At from At based on

information observed so far.

3. The environment samples a reward Xt ∈ {0, 1} and a
delay Dt ∈ N that are partially revealed to the learner
and where:

3.a. Xt ∼ B(A(cid:62)

t θ).

3.b. Dt is sampled independently of At from D.

4. Certain rewards resulting from previous actions are
revealed to the learner. For s ≤ t, let Cs,t = 1{Ds ≤
t − s}, which is called the censoring variable and in-
dicates whether or not the reward resulting from the
decision in round s is revealed by round t. Then let
Ys,t = Cs,tXs. The learner observes the collection
{Ys,t : s ≤ t} at the end of round t. If Xt = 1, we say
that the action At converts.

The delays in combination with the Bernoulli noise model
and censored observations introduces an interesting struc-
ture. When Xs = 0, then Ys,t = 0 for all t ≥ s, but
Ys,t = 0 is also possible when Xs = 1, but the reward from
round s has been delayed sufﬁciently. On the other hand, if
Ys,t = 1, the learner can immediately deduce that Xs = 1.

The goal of the learner is to sequentially minimize the cu-

mulative regret after T rounds, which is

R(T, θ) =

(cid:104)θ, A∗

t (cid:105) − (cid:104)θ, At(cid:105) ,

(1)

T
(cid:88)

t=1

t = arg max a∈At (cid:104)θ, a(cid:105) is the action that max-

where A∗
imises the expected reward in round t.
Remark 1. The assumption that a(cid:62)θ ∈ [0, 1] for all a ∈ At
ensures the reward is well deﬁned. A natural alternative is
to replace the linear model with a generalized linear model.
Our algorithms and analysis generalize to this setting in
the natural way using the techniques of (Filippi et al., 2010;
Jun et al., 2017). For simplicity, however, we restrict our
attention to the linear model. The assumption that (cid:107)θ(cid:107)2 ≤ 1
and (cid:107)a(cid:107)2 ≤ 1 for all actions are quite standard and the
dependence of our results on alternative bounds is relatively
mild.

3. Concentration for Least Squares

Estimators with Delays

This section is dedicated to disentangling the delays and the
reward estimation. The combination of an unknown delay
distribution and censored binary rewards makes it hopeless
to store all past actions and wait for every conversion. For
this reason our algorithm depends on a parameter m. If a
reward has not converted within m rounds, the algorithm
assumes it will never convert and ignores any subsequent
signals related to this decision. There is also a practical
advantage, which is that the learner does not need to store
individual actions that occurred more than m rounds in the
past. Deﬁne
˜Ys,t = Ys,t1{Ds ≤ m} = Xs1{Ds ≤ min(m, t − s)} ,
which is the same as Ys,t except rewards that convert after
more than m rounds are ignored. The learner then uses
˜Ys,t to estimate a parameter that is proportional to θ using
L2−regularized least squares. Let λ > 0 be a regularization
parameter and deﬁne

ˆθW
t

:=

AsA(cid:62)

s + λI

(cid:32)t−1
(cid:88)

s=1

s=1

(cid:33)−1 (cid:32)t−1
(cid:88)

(cid:33)

˜Ys,tAs

:= Vt(λ)−1Bt .

(2)

We now state our main deviation result.
Theorem 1. Let τm = P(D1 ≤ m) and δ ∈ (0, 1). Then
the following holds for all t ≤ T with probability at least
1 − 2δ.

(cid:107)ˆθW

t − τmθ(cid:107)Vt(λ) ≤ 2ft,δ +

(cid:107)As(cid:107)V −1

(λ) ,

t

(3)

t−1
(cid:88)

s=t−m

where

(cid:115)

√

ft,δ =

λ +

2 log

+ d log

(cid:19)

(cid:18) 1
δ

(cid:18) dλ + t
dλ

(cid:19)

.

(4)

Linear Bandits with Stochastic Delayed Feedback

s θ and (cid:15)s = 1{Ds ≤ m} − τm be
Proof. Let ηs = Xs − A(cid:62)
the two types of noise affecting our observations, both being
centered Bernoulli and independent of the past conditionally
on As. By (Lattimore & Szepesv´ari, 2019, Theorem 20.4)
it holds with probability at least 1 − 2δ that for all t ≤ T ,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t
(cid:88)

s=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

V −1
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t
(cid:88)

s=1

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

V −1
t

AsXs(cid:15)s

≤ f 2

t,δ, and,

Asηs

≤ f 2

t,δ .,

(5)

where we used that Xs ∈ [0, 1] for the ﬁrst inequality. We
comment on that step further below.

The next step is to decompose Bt with respect to the value
of the censoring variables of the learner. We ﬁrst rewrite
Bt by expliciting the value of the indicator function in each
term of its sum:

Bt =

AsXs1{Ds ≤ m}

t−m−1
(cid:88)

s=1

t−1
(cid:88)

+

s=t−m

=

t−1
(cid:88)

s=1

t−1
(cid:88)

+

s=t−m

AsXs1{Ds ≤ t − s}

AsXs1{Ds ≤ m}

AsXs(1{Ds ≤ t − s} − 1{Ds ≤ m}),

where we added m terms in the ﬁrst sum and removed them
in the second one. The second sum now contains the terms
that will eventually convert but have not been received yet.

Now assume both events in Equation (5) hold. Using the
decomposition above, we have

≤

(cid:107)ˆθW
t − τmθ(cid:107)Vt(λ)
(cid:13)
t−1
(cid:13)
(cid:88)
Vt(λ)−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

AsXs

t−1
(cid:88)

s=t−m

s=1

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Vt(λ)−1

AsXs(τm + (cid:15)s) − τmθ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Vt(λ)

where both the ﬁrst and second inequalities follow from the
triangle inequality applied to (cid:107)·(cid:107)V −1
and the last from the
assumption that the events in Equation (5) hold.

t

Remark 2. The initial step of the proof in Eq (5) might seem
loose but we explain why this term cannot easily be bounded
more tightly. Note that the variance of (cid:15)s is τm(1 − τm) and
thus, when applying (Lattimore & Szepesv´ari, 2019, Theo-
rem 20.4), we could obtain a tighter bound by taking it into
acount, which would lead to τ 2
tδ. But had we included it
there, it would have appeared in the expression of the upper
bound, i.e. in the algorithm, and the learner would have
needed its knowledge to compute the upper bound. This
was the choice made by (Vernade et al., 2017). By removing
it, we pay the price of slightly larger conﬁdence intervals
(more exploration) for not having to give prior information
to the learner. We discuss other possible approaches in
conclusion.

mf 2

Practical considerations As we mentioned already, a
practical advantage of the windowing idea is that the learner
need not store actions for which the feedback has not been
received indeﬁnitely. The cut-off time is often rather long,
even as much as 30 days (Chapelle & Li, 2011).

Choosing the window The windowing parameter is often
a constraint of the system and the lerner cannot choose it.
Our results show the price of this external censoring on
the regret. If the learner is able to choose m the choice is
somewhat delicate. The learner effectively discards 1 − τm
proportion of the data, so ideally τm should be large, which
corresponds to large m. But there is a price for this. The
learner must store m actions and the conﬁdence interval
also depends (somewhat mildly) on m. When the mean
µ = ED∼D[D] of the delay distribution D is ﬁnite and
known, then a somewhat natural choice of the windowing
parameter is m = 2µ. By Markov’s inequality this ensures
that τm ≥ 1/2. The result continues to hold if the learner
only knows an upper bound on µ.

Precisely how m should be chosen depends on the under-
lying problem. We discuss this issue in more detail in Sec-
tion 5 where the regret analysis is provided.

The last term can be naively bounded and gives the second
term of Equation (3). We can bound the ﬁrst term using
Equation (5):

4. Algorithm

(cid:13)
(cid:13)
Vt(λ)−1
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

s=1

≤

AsXs(cid:15)s

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

s=1

≤ 2ft,δ ,

AsXs(τm + (cid:15)s) − τmθ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Vt(λ)−1

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1
(cid:88)

s=1

Asηs

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Vt(λ)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)Vt(λ)−1

We are now equipped to present OTFLinUCB, an optimistic
linear bandit algorithm that uses concentration analysis from
the previous section. The pseudocode of OTFLinUCB is
given in Algorithm 1. It accepts as input a conﬁdence level
δ > 0, a window parameter m > 0 and a regularization
parameter λ > 0. In each round the algorithm computes the
estimator ˆθW
t using Equation (3) and for each arm a ∈ At
computes an upper conﬁdence bound on the expected reward

deﬁned by

Algorithm 1 OTFLinUCB

Linear Bandits with Stochastic Delayed Feedback

(cid:32)

(cid:33)

Ut(a) = (cid:104)a, ˆθW

t (cid:105)+

2ft,δ +

(cid:107)As(cid:107)Vt(λ)−1

(cid:107)a(cid:107)Vt(λ)−1 .

t−1
(cid:88)

s=t−m

(6)
Then action At is chosen to maximize the upper conﬁdence
bound:

At = arg max

Ut(a) ,

a

where ties are broken arbitrarily.

Implementation details The algorithm needs to keep
track of Vt(λ) and Bt as deﬁned in Equation (2). These
can be updated incrementally as actions are taken and infor-
mation is received. The algorithm also uses Vt(λ)−1, which
can be updated incrementally using the Sherman-Morrison
formula. In order to recompute αt,δ the algorithm needs
to store the m most recent actions, which are also used to
update Bt.

Computation complexity The computation complexity
is dominated by three operations: (1) Updating Vt(λ) and
computing its inverse, which takes O(d2) computation steps
using a rank-one update, and (2) computing the radius of the
conﬁdence ellipsoid, which requires O(md2) computations,
one for each of the last m actions. Finally, (3) iterating over
the actions and computing the upper conﬁdence bounds,
which requires O(Ktd2) computations. Hence the total
computation per round is O((Kt + m)d2).

Space complexity The space complexity is dominated
by: (1) Storing the matrix Vt(λ) and (2) storing the m
most recent actions, which are needed to compute the least
squares estimator and the upper conﬁdence bound. Hence,
the space complexity is O(md + d2).

Improved computation complexity Because Vt(λ)
changes in every round, the radius of the conﬁdence set
needs to be recomputed in each round, which requires
O(md2) computations per round. A minor modiﬁcation
reduces the computation complexity to O(d2). The idea is
to notice that for any a ∈ Rd and s ≤ t,

(cid:107)a(cid:107)Vs(λ)−1 ≥ (cid:107)a(cid:107)Vt(λ)−1 .

Hence, one can store a buffer of scalars {(cid:107)As(cid:107)Vs(λ)−1, t −
m ≤ s ≤ t − 1} at the memory cost of O(m). This slightly
increases the upper conﬁdence bounds, but not so much that
the analysis is affected as we discuss in the next section.

5. Regret Analysis

Our main theorem is the following high probability upper
bound on the regret of OTFLinUCB. The proof combines the

Input: Window parameter m > 0, conﬁdence level δ >
0 and λ > 0.
for t = 2, . . . , T do

Receive action set At
Compute width of conﬁdence interval:

αt,δ = 2ft,δ +

(cid:107)As(cid:107)Vt(λ)−1

t−1
(cid:88)

s=t−m

Compute the least squares estimate ˆθW

t using (2)

Compute the optimistic action:

At = arg max

(cid:104)a, ˆθW

t (cid:105) + αt,δ (cid:107)a(cid:107)Vt(λ)−1

a∈At

Play At and receive observations

end for

ideas from (Abbasi-Yadkori et al., 2011) with a novel argu-
ment to handle the conﬁdence bound in Theorem 1, which
has a more complicated form than what usually appears in
the analysis of stochastic linear bandits.

Theorem 2. With probability at least 1 − 2δ the regret of
OTFLinUCB satisﬁes

R(T, θ) ≤

2dT log

(cid:115)

4fn,δ
τm

(cid:19)

(cid:18) dλ + T
dλ

+

4md
τm

log

(cid:18) dλ + T
dλ

(cid:19)

.

Proof. Let αt,δ = 2ft,δ + (cid:80)t−1
s=t−m (cid:107)As(cid:107)Vt(λ)−1, which is
chosen so that the upper conﬁdence bound for action a in
round t is

Ut(a) = (cid:104)a, ˆθW

t (cid:105) + αt,δ (cid:107)a(cid:107)Vt(λ)−1 .

and At = arg max a∈At Ut(a). By Theorem 1, with proba-
bility at least 1 − 2δ it holds that (cid:107)ˆθW
t − τmθ(cid:107)Vt(λ) ≤ αt,δ
for all t. Assume for the remainder that the above event
holds. Then

(cid:104)A∗

t , θ(cid:105) =

(cid:104)τmθ, A∗
t (cid:105)

(cid:16)

1
τm
1
τm
Ut(At)
τm

≤

≤

(cid:104)ˆθW

t , A∗

t (cid:105) + αt,δ (cid:107)A∗

t (cid:107)Vt(λ)−1

(cid:17)

=

Ut(A∗
t )
τm

=

1
τm

(cid:16)

(cid:104)ˆθW

t , At(cid:105) + αt,δ (cid:107)At(cid:107)Vt(λ)−1

(cid:17)

.

Linear Bandits with Stochastic Delayed Feedback

Therefore the regret in round t is bounded by
αt,δ
τm

t − At, θ(cid:105) ≤

t − τmθ(cid:105) +

(cid:104)At, ˆθW

(cid:104)A∗

(cid:107)At(cid:107)Vt(λ)−1

1
τm
2αt,δ
τm

≤

(cid:107)At(cid:107)Vt(λ)−1 ,

where the second inequality follows from Cauchy-Schwarz.
We now substitute the value of αt,δ and bound the overall
regret by

R(T, θ) ≤

ft,δ (cid:107)At(cid:107)V −1

t

4
τm

T
(cid:88)

t=1

+

2
τm

T
(cid:88)

t=1

(cid:107)At(cid:107)Vt(λ)−1

(cid:107)As(cid:107)Vt(λ)−1 .

(7)

t−1
(cid:88)

s=t−m

The ﬁrst sum is bounded in the same way as the standard
setting (Abbasi-Yadkori et al., 2011):
(cid:118)
(cid:117)
(cid:117)
(cid:116)T

ft,δ (cid:107)At(cid:107)Vt(λ)−1 ≤

(cid:107)At(cid:107)2

Vt(λ)−1

T
(cid:88)

T
(cid:88)

4fn,δ
τm

4
τm

t=1

t=1

(cid:115)

2dT log

≤

4fn,δ
τm

(cid:18) dλ + T
dλ

(cid:19)

,

where the ﬁrst inequality follows from Cauchy-Schwarz and
the second from the elliptical potential lemma (Lattimore
& Szepesv´ari, 2019, Lemma 19.4). For the second sum in
Equation (7) we introduce a new trick. Using the fact that
ab ≤ (a2 + b2)/2,

2
τm

T
(cid:88)

t=1

(cid:107)At(cid:107)Vt(λ)−1

(cid:107)As(cid:107)Vt(λ)−1

t−1
(cid:88)

s=t−m

Lower bound We now provide a non-asymptotic mini-
max lower bound for K-armed stochastic Bernoulli bandits
showing that in the windowed setting there is an unavoidable
dependence on τm. Note, an asymptotic problem-dependent
bound for this setting was already known (Vernade et al.,
2017). Although our results are specialized to the ﬁnite-
armed bandit model, we expect that standard analysis for
other action sets should follow along the same lines as (Lat-
timore & Szepesv´ari, 2019) (§24).

Theorem 3. For any policy π and K > 1 and T ≥ 1 and
τm ∈ (0, 1) there exists a K-armed Bernoulli bandit such
that R(T, θ) ≥ c min{T, (cid:112)T K/τm}, where c > 0 is a
universal constant.

Interestingly, the dependence on τm appears in the square
root, while in our upper bounds it is not. We speculate that
the upper bound is loose. In fact, were τm known it would be
possible to improve our upper bounds by using conﬁdence
intervals based on Bernstein’s inequality that exploit the
reduced variance that is a consequence of τm being small.
When τm is unknown you might imagine estimating the
variance. We anticipate this should be possible, but the
complexity of the algorithm and analysis would greatly
increase.

6. Thompson sampling

The standard implementation of Thompson sampling for lin-
ear bandits without delays and Gaussian noise is to sample
˜θt ∼ N (ˆθt, Vt(λ)−1) where ˆθt is the usual regularized least
squares estimator

1
τm

1
τm

T
(cid:88)

t−1
(cid:88)

(cid:16)

t=1

s=t−m

T
(cid:88)

t−1
(cid:88)

(cid:16)

t=1

s=t−m

≤

≤

≤

(cid:107)At(cid:107)2

Vt(λ)−1 + (cid:107)As(cid:107)2

Vt(λ)−1

(cid:17)

(cid:17)

(cid:107)At(cid:107)2

Vt(λ)−1 + (cid:107)As(cid:107)2

Vs(λ)−1

The algorithm then chooses

ˆθt = Vt(λ)−1

AsXs .

t−1
(cid:88)

s=1

At = arg max

(cid:104)a, ˜θt(cid:105) .

a∈At

2m
τm

T
(cid:88)

t=1

(cid:107)At(cid:107)2

Vt(λ)−1 ≤

4md
τm

log

(cid:18) dλ + T
dλ

(cid:19)

,

where in the second inequality we used the fact that for
s ≤ t, Vs(λ) ≤ Vt(λ)1, so that Vs(λ)−1 ≥ Vt(λ)−1. Sub-
stituting the previous two displays into Equation (7) com-
pletes the proof.

Remark 3. The choice of m is left to the learner. It inﬂu-
ences the bound in two ways: (1) The lower-order term is
linear in m, which prevents the user from choosing m very
large. On the other hand, τm is increasing in m, which
pushes the user in the opposite direction. Designing an
adaptive algorithm that optimizes the choice of m online
remains a challenge for the future.

1here we denote A ≤ B if for any x ∈ Rd

+, (cid:107)x(cid:107)A ≤ (cid:107)x(cid:107)B

This algorithm corresponds to Thompson sampling (or pos-
terior sampling) when the prior is Gaussian with zero mean
and λI covariance. No frequentist analysis exists for this
algorithm, but empirically it performs very well. In the de-
layed setting the (Xs)t−1
s=1 are not available to the learner at
time t. Nevertheless, it is possible to propose a randomized
algorithm in the spirit of Thompson sampling. To motivate
our choices, recall that the standard concentration analysis
for least squares regression by (Abbasi-Yadkori et al., 2011)
shows that with high probability

(cid:107)ˆθt − θ(cid:107)Vt(λ) ≤ ft,δ .

Linear Bandits with Stochastic Delayed Feedback

In the delayed setting, on the other hand, Theorem 1 shows
that

Algorithm 2 OTFLinTS:

1: Input: Window parameter m > 0, conﬁdence level

(cid:107)ˆθW

t − θ(cid:107)Vt(λ) ≤ ft,δ +

(cid:107)As(cid:107)Vs(λ)−1

(8)

t−1
(cid:88)

s=t−m

= ft,δ

1 +

(cid:32)

(cid:80)t−1

s=t−m (cid:107)As(cid:107)Vs(λ)−1
ft,δ

(cid:33)

.

(9)

An ansatz guess for a sampling algorithm that uses the
delayed least squares estimator is to compute ˆθW
t and then
sample

˜θt ∼ N

(cid:16)ˆθW

t , βt,δVt(λ)−1(cid:17)

,

where βt,δ = 1 + ((cid:80)t−1
s=t−m (cid:107)As(cid:107)Vt(λ)−1)/ft,δ. The choice
of βt,δ is rather heuristic. A more conservative choice would
be the right-hand side of Equation (9). The resulting algo-
rithm roughly corresponds to sampling from the conﬁdence
set used by our optimistic algorithm. Although this sacri-
ﬁces certain empirical advantages, we expect the analysis
techniques by (Agrawal & Goyal, 2013; Abeille & Lazaric,
2017) could be applied to prove a frequentist regret bound
for this algorithm.

Remark 4. Algorithms based on adding noise to an empir-
ical estimate are often referred to as ‘follow the perturbed
leader’, which has been effectively applied in a variety of
settings (Abeille & Lazaric, 2017; Kveton et al., 2018). An
advantage of sampling approaches is that the optimization
problem to ﬁnd At is a linear program, which for large
structured action sets may be more efﬁcient than ﬁnding the
arm maximizing an upper conﬁdence bound.

Remark 5. A genuine implementation of Thompson sam-
pling would require a prior on the space of delay distribu-
tions as well as the unknown parameter. We are not hopeful
about the existence of a reasonable prior for which comput-
ing or sampling from the posterior is efﬁcient.

7. Related Work

Delays in the environment response is a frequent phe-
nomenon that may take many different forms and should be
properly modelled to design appropriate decision strategies.
For instance, in an early work on applications of bandit algo-
rithms to clinical trials, (Eick, 1988) uses ‘delays’ to model
the survival time of the patients, in which case delays are
the reward rather than external noise, which is a radically
different problem to ours. In the same vein, in the sequential
stochastic shortest path problem (Talebi et al., 2017), the
learner aims at minimising the routing time in a network.

Another example is parallel experimentation, where delays
force the learner to make decisions under temporary uneven
information. For instance, (Desautels et al., 2014; Grover

δ > 0, λ > 0.

2: for t = 2, . . . , T do
3:
4:

Receive action set At
Compute width of conﬁdence interval:

βt,δ = 1 +

(cid:80)t−1

s=t−m (cid:107)As(cid:107)Vs(λ)−1
ft,δ

5:

6:

7:

Compute the least squares estimate ˆθW

t using (2)

Sample ˜θt ∼ N (ˆθW

t , βt,δVt(λ)−1)

Compute action At = arg max a∈At(cid:104)a, ˜θt(cid:105)

Play At and receive observations

8:
9: end for

et al., 2018) consider the problem of running parallel ex-
periments that do not all end simultaneously. They propose
a Bayesian way of handling uncertain outcomes to make
decisions: they sample hallucinated results according to the
current posterior. The related problem of gradient-based
optimization with delayed stochastic gradient information
is studied by (Agarwal & Duchi, 2011).

In online advertising, delays are due to the natural latency
in users’ responses. However, in many works on bandit al-
gorithm, delays are ignored as a ﬁrst approximation. In the
famous empirical study of Thompson sampling (Chapelle &
Li, 2011), a section is dedicated to analyzing the impact of
delays on either Thompson sampling or LinUCB. While this
is an early interest for this problem, they only consider ﬁxed,
non-random, delays of 10, 30 or 60 minutes. Similarly, in
(Mandel et al., 2015), the authors conclude that random-
ized policies are more robust to this type of latencies. The
general problem of online learning under known delayed
feedback is addressed in (Joulani et al., 2013), including full
information settings and partial monitoring, and we refer
the interested reader to their references on those topics. The
most recent and closest work to ours is (Zhou et al., 2019).
The main difference with our approach is that they make
strong assumptions on the distribution of the delays, while
not having any censoring of the feedback. In that sense
their problem is easier than ours because delays are fully
observed. Nonetheless, the key idea of their algorithm is
reminiscent to ours: they inscrease the exploration bonus by
a quantity that corresponds to the amount of missing data
at each round, which is observable in their case, not in ours.
Recent work (Li et al., 2019) address the case of unknown
delays. The idea of ambiguous feedback, where delays are
partially unknown is introduced in (Vernade et al., 2017).

Linear Bandits with Stochastic Delayed Feedback

Many models of delays for online advertising have been pro-
posed to estimate conversion rates in an ofﬂine fashion: e.g.
(Yoshikawa & Imai, 2018) (non-parametric) or (Chapelle,
2014; Diemert et al., 2017) (generalized linear parametric
model).

An alternative, harder model relies only on anonymous feed-
back (Cesa-Bianchi et al., 2018; Pike-Burke et al., 2017;
Arya & Yang, 2019): the rewards, when observed, cannot be
directly linked to the action that triggered them in the past,
and the learner has to deal with mixing processes. Finally, a
recent full-information setting (Mann et al., 2018) suggests
to incorporate intermediate feedback correlated with the
rewards.

8. Experiments

In this section we illustrate the two realistic settings han-
dled by this work. The ﬁrst case below, with geometrically
distributed delays, corresponds to the empirical study done
by (Chapelle, 2014), already reproduced in simulations by
(Vernade et al., 2017). The second case, with arbitrary
heavy-tailed delays, corresponds to another use-case we
extracted from the data released by (Diemert et al., 2017).2

Remark 6. To the best of our knowledge, there is no com-
petitor for this problem. In particular, despite the similarity
of the algorithm DUCB of (Zhou et al., 2019) with ours,
it cannot be implemented when delays are not observed.
Speciﬁcally, DUCB, maintains a quantity Gt which is equal
to the exact amount of missing data (delayed feedback not
converted yet). In our case, this quantity is not observable.
The same comment applies for the QPM-D algorithm of
(Joulani et al., 2013) and similar queue-based approaches.
On the other end, existing algorithms for unknown delays
are not derived for linear bandits with arbitrary action sets.

In the datasets analyzed3 by
Well-behaved delays
(Chapelle, 2014), delays are empirically shown to have an
exponential decay. As in (Vernade et al., 2017), we run real-
istic simulations based on the orders of magnitude provided
in their study. We arbitrarily choose d = 5, K = 10. We ﬁx
the horizon to T = 3000, and we choose a geometric delay
distribution with mean µ = E[Dt] ∈ {100, 500}. In a real
setting, this would correspond to an experiment that lasts
3h, with average delays of 6 and 30 minutes4 respectively.
The online interaction with the environment is simulated:
d} and at each round we sam-
we ﬁx θ = {1/
ple and normalize K actions from {0, 1}d. All result are
averaged over 50 independent runs. We show three cases on

d, . . . , 1/

√

√

2The code for all data analysis and simulations is available at

https://sites.google.com/view/bandits-delayed-feedback

3The datasets were not released.
4Note that in (Chapelle, 2014), time is rather measured in hours

Figure 1: (m, µ) = (100, 100) (τm = 0.63) when delays
are short and even though the timeout is set a bit too low, the
algorithm manages to learn within a reasonable time range;
(m, µ) = (500, 100) (τm = 0.993) when the window pa-
rameter is large enough so almost all feedback is received;
and (m, µ) = (100, 500) when the window is set too low
and the regret suffers from a high 1/τm = 5.5. Note that the
notion of ‘well-tuned‘ window is dependent on the memory
constraints one has. In general, the window is a system
parameter that the learner cannot easily set to their conve-
nience. In all three cases, OTFLinUCB performs better than
OTFLinTS, which we believe is due to the rough posterior
approximation. Comparing the two leftmost ﬁgures, we see
the impact of increasing the window size: the log regime
starts earlier but the asymptotic regret is similar – roughly
R(T ) = 100 in both cases. The rightmost ﬁgure, compared
with the leftmost one, shows the impact of a high level of
censoring. Indeed, with 1/τm = 5.5, only few feedback
are received within the window and the algorithm will need
much longer to learn. This causes the regret increase, and
also explains why at T = 3000, both algorithms are still in
the linear regime.

Interestingly,

Heavy-tailed delays.
the more recent
dataset released by (Diemert et al., 2017)5 features heavy-
tailed delays, despite being sourced from a similar online
marketing problem in the same company. We run simula-
tions in the same setting as before, but this time we gener-
ate the delays according to the empirical delay distribution
extracted from the dataset. On Figure 2, we show the distri-
bution of the log-delays ﬁtted with a gaussian kernel using
the Scipy library. A high proportion of those delays are be-
tween 105 and 106 so we rescaled them by a factor 0.01 to
maintain the experiment horizon to the reasonable value of
T = 104. Nevertheless, many rewards will not be observed
within the horizon. We compare OTFLinUCB and OTFLinTS
with window parameter m = 2000. The results are shown
on Figure 3 and show that OTFLinUCB is able to cope with
heavy-tailed delays reasonably well. It is interesting to ob-
serve that the behavior of both algorithms is similar to the
central plot of Figure 1, when the window is larger than the
expectation of the delays.

9. Discussion

We introduced the delayed stochastic linear bandit setting
and proposed two algorithms. The ﬁrst uses the optimism
principle in combination with ellipsoidal conﬁdence inter-
vals while the second is inspired by Thompson sampling
and follow the perturbed leader.

There are a number of directions for future research, some

5https://ailab.criteo.com/criteo-attribution-modeling-bidding-

and days.

dataset/

Linear Bandits with Stochastic Delayed Feedback

Figure 1. Results of our simulations. From left to right, the plots report the performance of the two algorithms with (m, µ) =
{(100, 100), (500, 100), (100, 500)}.Results are averaged over 100 independent runs.

Figure 2. Empirical distribution of the log10 delays.

should be possible using the machinery of (Filippi et al.,
2010).

Thompson sampling Another obvious question is
whether our variant of Thompson sampling/follow the per-
turbed leader admits a regret analysis. In principle we expect
the analysis by (Agrawal & Goyal, 2013) in combination
with our new ideas in the proof of Theorem 2 can be com-
bined to yield a guarantee, possibly with a different tuning
of the conﬁdence width βt,δ. Investigating a more pure
Bayesian algorithm that updates beliefs about the delay dis-
tribution as well as unknown parameter is also a fascinating
open question, though possibly rather challenging.

Reﬁned lower bounds Our current lower bound is proven
when At = {e1, . . . , ed} is the standard basis vectors for all
t. It would be valuable to reproduce the results where At is
the unit sphere or hypercube, which should be a straightfor-
ward adaptation of the results in (Lattimore & Szepesv´ari,
2019, §24).

Figure 3. Simulation with realistic delays and m = 2000, sampled
according to the distribution of Figure 2.

Acknowledgements

of which we now describe.

Improved algorithms Our lower bound suggests the de-
pendence on τm in Theorem 2 can be improved. Were this
value known we believe that using a concentration anal-
ysis that makes use of the variance should improved the
dependence to match the lower bound. When τm is not
known, however, the problem becomes more delicate. One
can envisage various estimation schemes, but the resulting
algorithm and analysis are likely to be rather complex.

Generalized linear models When the rewards are
Bernoulli it is natural to replace the linear model with a
generalized linear model. As we remarked already, this

This work was started when CV was at Amazon Berlin and
at OvGU Magdeburg, working closely with AC, GZ, BE and
MB. The work of A. Carpentier is partially supported by the
Deutsche Forschungsgemeinschaft (DFG) Emmy Noether
grant MuSyAD (CA 1488/1-1), by the DFG - 314838170,
GRK 2297 MathCoRe, by the DFG GRK 2433 DAEDALUS
(384950143/GRK2433), by the DFG CRC 1294 ’Data As-
similation’, Project A03, by the UFA-DFH through the
French-German Doktorandenkolleg CDFA 01-18 and by
the UFA-DFH through the French-German Doktorandenkol-
leg CDFA 01-18 and by the SFI Sachsen-Anhalt for the
project RE-BCI. Major changes and improvements were
made thanks to TL at DeepMind later on. CV wants to thank
Csaba Szepesv´ari for his useful comments and discussions,
and Vincent Fortuin for precisely reading and commenting.

Linear Bandits with Stochastic Delayed Feedback

References

Abbasi-Yadkori, Y., P´al, D., and Szepesv´ari, C. Improved
algorithms for linear stochastic bandits. In Advances in
Neural Information Processing Systems, pp. 2312–2320,
2011.

Abeille, M. and Lazaric, A. Linear thompson sampling re-
visited. In AISTATS 2017-20th International Conference
on Artiﬁcial Intelligence and Statistics, 2017.

Agarwal, A. and Duchi, J. C. Distributed delayed stochastic
optimization. In Advances in Neural Information Pro-
cessing Systems, pp. 873–881, 2011.

Agarwal, A., Hsu, D., Kale, S., Langford, J., Li, L., and
Schapire, R. Taming the monster: A fast and simple algo-
rithm for contextual bandits. In International Conference
on Machine Learning, pp. 1638–1646, 2014.

Agrawal, S. and Goyal, N. Thompson sampling for contex-
tual bandits with linear payoffs. In International Confer-
ence on Machine Learning, pp. 127–135, 2013.

Arya, S. and Yang, Y. Randomized allocation with non-
parametric estimation for contextual multi-armed bandits
with delayed rewards. arXiv preprint arXiv:1902.00819,
2019.

Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E.
The nonstochastic multiarmed bandit problem. SIAM
journal on computing, 32(1):48–77, 2002.

Beygelzimer, A., Langford, J., Li, L., Reyzin, L., and
Schapire, R. Contextual bandit algorithms with super-
vised learning guarantees. In Proceedings of the Four-
teenth International Conference on Artiﬁcial Intelligence
and Statistics, pp. 19–26, 2011.

Cesa-Bianchi, N., Gentile, C., and Mansour, Y. Nonstochas-
tic bandits with composite anonymous feedback. In Con-
ference On Learning Theory, pp. 750–773, 2018.

Chapelle, O. Modeling delayed feedback in display ad-
In Proceedings of the 20th ACM SIGKDD
vertising.
international conference on Knowledge discovery and
data mining, pp. 1097–1105. ACM, 2014.

Chapelle, O. and Li, L. An empirical evaluation of thompson
sampling. In Advances in neural information processing
systems, pp. 2249–2257, 2011.

Desautels, T., Krause, A., and Burdick, J. W. Paralleliz-
ing exploration-exploitation tradeoffs in gaussian process
bandit optimization. The Journal of Machine Learning
Research, 15(1):3873–3923, 2014.

Diemert, E., Meynet, J., Galland, P., and Lefortier, D. Attri-
bution modeling increases efﬁciency of bidding in display
advertising. In Proceedings of the AdKDD and TargetAd
Workshop, KDD, Halifax, NS, Canada, August, 14, 2017,
pp. To appear. ACM, 2017.

Eick, S. G. The two-armed bandit with delayed responses.

The Annals of Statistics, pp. 254–264, 1988.

Filippi, S., Cappe, O., Garivier, A., and Szepesv´ari, C. Para-
metric bandits: The generalized linear case. In Advances
in Neural Information Processing Systems, pp. 586–594,
2010.

Grover, A., Markov, T., Attia, P., Jin, N., Perkins, N.,
Cheong, B., Chen, M., Yang, Z., Harris, S., Chueh, W.,
et al. Best arm identiﬁcation in multi-armed bandits with
delayed feedback. In International Conference on Artiﬁ-
cial Intelligence and Statistics, pp. 833–842, 2018.

Joulani, P., Gyorgy, A., and Szepesv´ari, C. Online learning
under delayed feedback. In Proceedings of the 30th In-
ternational Conference on Machine Learning (ICML-13),
pp. 1453–1461, 2013.

Jun, K.-S., Bhargava, A., Nowak, R., and Willett, R. Scal-
able generalized linear bandits: Online computation and
hashing. In Advances in Neural Information Processing
Systems, pp. 99–109, 2017.

Kveton, B., Szepesv´ari, C., Wen, Z., Ghavamzadeh, M.,
and Lattimore, T. Garbage in, reward out: Bootstrapping
exploration in multi-armed bandits. 2018.

Lattimore, T. and Szepesv´ari, C. Bandit Algorithms. Cam-

bridge University Press (preprint), 2019.

Li, B., Chen, T., and Giannakis, G. B. Bandit online learn-
In The 22nd International
ing with unknown delays.
Conference on Artiﬁcial Intelligence and Statistics, pp.
993–1002, 2019.

Mandel, T., Brunskill, E., and Popovi´c, Z. Towards more
practical reinforcement learning. In 24th International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2015.
International Joint Conferences on Artiﬁcial Intelligence,
2015.

Chu, W., Li, L., Reyzin, L., and Schapire, R. Contextual
In Proceedings
bandits with linear payoff functions.
of the Fourteenth International Conference on Artiﬁcial
Intelligence and Statistics, pp. 208–214, 2011.

Mann, T. A., Gowal, S., Jiang, R., Hu, H., Lakshmi-
narayanan, B., and Gyorgy, A. Learning from delayed
outcomes with intermediate observations. arXiv preprint
arXiv:1807.09387, 2018.

Linear Bandits with Stochastic Delayed Feedback

Neu, G. Explore no more: Improved high-probability regret
bounds for non-stochastic bandits. In Advances in Neural
Information Processing Systems, pp. 3168–3176, 2015.

Pike-Burke, C., Agrawal, S., Szepesvari, C., and
Grunewalder, S. Bandits with delayed anonymous feed-
back. arXiv preprint arXiv:1709.06853, 2017.

Talebi, M. S., Zou, Z., Combes, R., Proutiere, A., and Jo-
hansson, M. Stochastic online shortest path routing: The
value of feedback. IEEE Transactions on Automatic Con-
trol, 63(4):915–930, 2017.

Vernade, C., Capp´e, O., and Perchet, V. Stochastic ban-
dit models for delayed conversions. In Conference on
Uncertainty in Artiﬁcial Intelligence, 2017.

Yoshikawa, Y. and Imai, Y. A nonparametric delayed feed-
back model for conversion rate prediction. arXiv preprint
arXiv:1802.00255, 2018.

Zhou, L. A survey on contextual multi-armed bandits. arXiv

preprint arXiv:1508.03326, 2015.

Zhou, Z., Xu, R., and Blanchet, J. Learning in generalized
linear contextual bandits with stochastic delays. In Ad-
vances in Neural Information Processing Systems 32, pp.
5198–5209, 2019.

A. Proof of Theorem 3

Linear Bandits with Stochastic Delayed Feedback

For p, q ∈ (0, 1) let d(p, q) = p log(p/q) + (1 − p) log((1 − p)/(1 − q)) be the relative entropy between Bernoulli
distributions with biases p and q respectively. For θ ∈ [0, 1]K let Eθ denote the expectation when the algorithm interacts
with the Bernoulli bandit determined by θ ∈ [0, 1]K. Let θ = (1/2 + ∆, 1/2, . . . , 1/2) where ∆ ∈ (0, 1/4) is some
parameter to be tuned subsequently. Then let

By the pigeonhole principle it follows that Eθ[Ni(T )] ≤ T /(K − 1). Then deﬁne φ ∈ [0, 1]K so that φj = θj for all j (cid:54)= i
and φi = 1/2 + 2∆. By the deﬁnitions of θ and φ we have

i = arg min

Eθ[Nk(T )] .

k>1

Rθ(T ) ≥ ∆(T − Eθ[N1(T )])

and Rφ(T ) ≥ ∆Eφ[N1(T )] ,

which means that

Rθ(T ) ≥

Pθ(N1(T ) ≤ T /2)

and Rφ(T ) ≥

Pφ(N1(T ) > T /2) .

T ∆
2

T ∆
2

Summing the two regrets and applying the Bretagnolle-Huber inequality shows that

Rθ(T ) + Rφ(T ) ≥

(Pθ(N1(T ) ≤ T /2) + Pφ(N1(T ) > T /2))

T ∆
2
T ∆
4

≥

exp (−KL(Pθ, Pφ)) .

The next step is to calculate the relative entropy between Pθ and Pφ. Both bandits behave identically on all arms except
action i. When action i is played the learner effectively observes a reward with bias either τm/2 or τm(1/2 + 2∆). Therefore

KL(Pθ, Pφ) = Eθ [Ni(T )] d(τm/2, τm(1/2 + 2∆)) .

Upper bounding the relative entropy by the χ-squared distance shows that

d(τm/2, τm(1/2 + 2∆)) ≤

2 (τm/2 − τm(1/2 + 2∆))2
τm(1/2 − 2∆)

≤ 32τm∆2 ,

where we used the assumption that 2∆ ≤ 1/4. Therefore

KL(Pθ, Pφ) ≤ 32τm∆2Eθ[Ni(T )] ≤

32τm∆2T
K − 1

.

Rθ(T ) + Rφ(T ) ≥

T ∆
4

(cid:18)

exp

−

32τm∆2T
K − 1

(cid:19)

.

Finally we conclude that

The result follows by tuning ∆.

