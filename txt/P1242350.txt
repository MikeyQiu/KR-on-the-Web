NoReC: The Norwegian Review Corpus

Erik Velldal, Lilja Øvrelid, Eivind Alexander Bergem, Cathrine Stadsnes,
Samia Touileb, Fredrik Jørgensen†
Language Technology Group, Department of Informatics, University of Oslo
{erikve, liljao, eivinabe, cathsta, samiat}@iﬁ.uio.no
†Schibsted Media Group
fredrik.jorgensen@schibsted.com

7
1
0
2
 
t
c
O
 
5
1
 
 
]
L
C
.
s
c
[
 
 
1
v
0
7
3
5
0
.
0
1
7
1
:
v
i
X
r
a

Abstract
This paper presents the Norwegian Review Corpus (NoReC), created for training and evaluating models for document-level sentiment
analysis. The full-text reviews have been collected from major Norwegian news sources and cover a range of different domains, including
literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review
is labeled with a manually assigned score of 1–6, as provided by the rating of the original author. This ﬁrst release of the corpus
comprises more than 35,000 reviews. It is distributed using the CoNLL-U format, pre-processed using UDPipe, along with a rich set of
metadata. The work reported in this paper forms part of the SANT initiative (Sentiment Analysis for Norwegian Text), a project seeking
to provide resources and tools for sentiment analysis and opinion mining for Norwegian. As resources for sentiment analysis have
so far been unavailable for Norwegian, NoReC represents a highly valuable and sought-after addition to Norwegian language technology.

Keywords: Sentiment Analysis, Opinion Mining, Corpus, Norwegian, Reviews

1.

Introduction

Norwegian is in many ways an under-resourced language,
with training and evaluation data still lacking for many core
NLP tasks. The current work aims to alleviate the situation
for the particular task of sentiment analysis. The SANT
project – Sentiment Analysis for Norwegian Text – seeks
to create, and make publicly available, resources and tools
for sentiment analysis for Norwegian. The SANT effort
described in the current paper marks the release of the Nor-
wegian Review Corpus1 (NoReC). The dataset comprises
more than 35,000 full-text reviews from a range of differ-
ent domains, collected from several of the major Norwe-
gian news sources. Each review is rated with a numerical
score on a scale of 1–6, and can be used for training and
evaluating models for document-level sentiment analysis.

1.1. Rating by dice
A particularity of review journalism in Norway,
is the
wholesale adoption of dice rolls (‘terningkast’) as a stan-
dard rating scale: The item under review is rated on a scale
of 1–6, commonly visualized by the face of a die with a
corresponding number of ‘dots’ or pips. The practice is
thought to have been introduced already in 1952 when re-
viewing movies in the newspaper Verdens Gang (VG). By
now it is has found widespread use in all sorts of arts and
consumer journalism and is used when reviewing every-
thing from books, theater and music, to home electronics,
restaurants, and children’s clothing.
The rating practice described above has several beneﬁts for
the goal of document-level SA: (i) It eliminates the need for
costly manual annotation since the numerical rating (i.e.,
the die roll) directly provides us with the labels needed for
training models for detecting the overall document polar-
ity. (ii) There is no need for manually deﬁned mappings to
align different rating schemes as the reviews all use a uni-

1https://github.com/ltgoslo/norec

form scale. (iii) The wide range of available news sources
using the same rating practice, including all the major na-
tional newspapers, facilitates the creation of a large-scale
dataset. (iv) Models trained on the dataset can be expected
to generalize well across domains given the balance of dif-
ferent topics covered in the corpus. For English, a sub-
stantial amount of SA research has been directed towards
predicting the sentiment of movie reviews, collected from
aggregator sites like IMDb.com and RottenTomatoes.com
(Pang and Lee, 2005; Socher et al., 2013) or other types of
consumer reviews (Blitzer et al., 2007; Wang et al., 2010;
Maas et al., 2011). The NoRec data set contains a variety
of different types of reviews from a range of domains.

1.2. Sources and partners

The SANT project represents a newly initiated collabora-
tion between the Language Technology Group (LTG) at the
Department of Informatics at the University of Oslo, and
three of Norway’s largest media groups; the Norwegian
Broadcasting Corporation (NRK – the state-owned public
broadcaster) and the privately held Schibsted Media Group
and Aller Media. This ﬁrst release of NoReC comprises
35,194 reviews extracted from eight different news sources
as contributed by the three media partners. In terms of pub-
lishing date the reviews mainly cover the time span 2003–
2017, although it also includes a handful of reviews dating
back as far as 1998. We brieﬂy present the sources provided
by the different partners below.

Schibsted Media Group The Schibsted group has con-
tributed content from their full portfolio of Norwegian news
sources: VG, Aftenposten, Fædrelandsvennen, Bergens
Tidende, and Stavanger Aftenblad. While the latter three
rank among Norway’s largest regional newspapers, Aften-
posten is the largest national newspaper in terms of circu-
lation and VG is the largest online news source with more
than 2.4 million readers across all platforms.

Source

# Reviews

VG
Dagbladet
Stavanger Aftenblad
P3.no
DinSide.no
Fædrelandsvennen
Bergens Tidene
Aftenposten

Total

11,888
5,305
5,146
5,017
2,944
2,296
1,675
923

35,194

Table 1: Number of reviews across sources.

Aller Media The Aller publishing company has con-
tributed content from two sources. The ﬁrst is the online
version of the newspaper Dagbladet – the second most vis-
ited online news source in Norway – publishing reviews
for music recordings and live performances, theater and
related stage performances, movies, literature, restaurants
and more. The second source, DinSide.no, is a website
specializing in product reviews, covering a wide range of
product types, from home electronics to cars and clothing.

NRK The Norwegian Broadcasting Corporation is a
state-owned media house, with a special mandate to be
a non-commercial, politically independent public broad-
caster. For the review corpus, NRK has contributed content
from the website P3.no which has an extensive back cata-
log of ‘die-rated’ reviews of movies, TV series, computer
games, and music (both recordings and live performances).
Figure 1 shows the number of reviews included in the ﬁnal
corpus, broken down across the various sources.

2. Corpus creation
The original document collections were provided from the
media sources in various JSON, HTML and XML formats,
and a substantial effort has gone into identifying relevant
documents and extracting text and associated metadata.
The extraction process can be summarized by the following
three steps: (i) Identify reviews, (ii) convert review con-
tent to an intermediate and canonical HTML format, (iii)
extract text and pass it through linguistic pre-processing,
producing representations in CoNLL-U format, and ﬁnally
(iv) extract relevant metadata to a JSON representation with
normalized attribute–value names. We brieﬂy comment on
each of these steps in turn below.

Identifying reviews

2.1.
Some of the initial document dumps also included other ar-
ticles beyond reviews, and in these cases reviews had to be
identiﬁed. While in some cases this could be done simply
by checking for an appropriate metadata ﬁeld indicating the
rating score, other cases required checking for links point-
ing to an image of a die (indicating the rating), or similar
heuristics.
For some of the sources, one and the same document may
contain multiple reviews, for example for product com-
parisons.
In these cases we had to identify and separate
out the different sub-reviews. Different publishing conven-
tions required targeting different types of cues in the docu-
ment structure, like headers, bold-faced content or die-face

images. This also involved extraction of titles and rating
scores for the different sub-reviews. The identiﬁed sub-
reviews become separate documents in the NoReC data set.
In total, 35,194 distinct reviews were extracted from the
data provided by the media partners.

2.2. Converting content to canonical HTML
The raw data dumps from the sources are mostly in HTML
format, but may also be e.g. JSON objects, and have differ-
ent conventions for document structuring and use of mark-
up.
In order to streamline the downstream text extrac-
tion, all documents were converted to a ‘canonical’ HTML
format where all textual content is located either inside a
In addition to containing the
header or a paragraph tag.
review text, the raw representations also contains images,
ads and other content not considered part of the running
text.
In order to identify and mark the non-relevant text
we used a combination of heuristics based on simple string
matching and properties like paragraph length and ratio of
content to markup. For example, care was taken to iden-
tify ‘you-might-also-be-interested-in’ type links that are in-
jected throughout the texts in an attempt to keep the reader
on the website and generate more clicks. Importantly, how-
ever, we chose not to remove content when converting to
our intermediate HTML format, instead introducing a new
tag – remove – in which we enclose content considered
non-relevant. This non-destructive approach preserves the
original content, as to not close the door on changes to the
subsequent task of text extraction later.

2.3. Linguistic enrichments and CoNLL-U
Given the canonical HTML format described above, it is
straightforward to extract the relevant text. In order to en-
able various types of downstream uses of the dataset, we
further preprocess the raw text using the UDPipe toolkit
(Straka et al., 2016), representing each review as a CoNLL-
U ﬁle, following the format deﬁned in Universal Dependen-
cies version 2.2 In this step we perform sentence segmenta-
tion, tokenization, lemmatization, morphological analysis,
part-of-speech tagging and dependency parsing, following
the Universal Dependencies scheme (Nivre et al., 2016).
However, the preprocessing set-up is slightly complicated
by the fact that the Norwegian language has two ofﬁcial
written standards – Bokmål (the main variety) and Nynorsk
– both of which are represented in the review corpus. Below
we ﬁrst describe how language identiﬁcation is performed,
and then go on to give more details about UDPipe and the
resulting CoNLL-U data.

Identifying language varieties The two ofﬁcial varieties
are closely related and they are mostly distinguished by
minor lexical differences. Still, the differences are strong
enough that different preprocessing pipelines must be used
for the different standards, hence it is important to identify
the standard within a particular document. Therefore, we
have used langid.py (Lui and Baldwin, 2012) to iden-
tify the standard for each review.3 We performed an evalu-

2http://universaldependencies.org/format.

html

3langid.py can actually identify three different variants:
no, nn and nb, for Norwegian (mixed), Nynorsk and Bokmål,

Documents
Sentences
Tokens
Types, full-forms
Types, lemmas

#

35,194
837,914
14,819,248
521,563
446,532

Table 2: Basic corpus counts.

ation of langid.py on 1599 reviews of which 1487 were
written in Bokmål and 112 in Nynorsk (based on selecting
reviews from authors known to write in a given variety). On
this sample langid.py achieved 100% accuracy. While
the main variety, i.e. Bokmål, dominates the distribution
in the corpus with 34,661 documents, we also identiﬁed
533 documents in Nynorsk (mainly from the sources Fæ-
drelandsvennen, Bergens Tidende and P3.no).

UDpipe conﬁguration We applied UDPipe (Straka et al.,
2016) v.1.2 with its pre-trained models for Norwegian Bok-
mål and Nynorsk. This version of the UDPipe software and
the pre-trained models were developed for the CoNLL 2017
shared task (Zeman et al., 2017), which was devoted to
parsing from raw text to Universal Dependencies for more
than 40 different languages. We use of the models trained
for participation in the shared task (Straka and Straková,
2017), not the models provided as baseline models for the
participants. The Norwegian models were trained on the
UD 2.0 versions of the Norwegian UD treebanks (Øvrelid
and Hohle, 2016; Velldal et al., 2017) in conjunction with
the aforementioned shared task, and the subsequent choice
of model for use (Bokmål vs Nynorsk) was determined by
the language identiﬁed for each particular review.
UDPipe obtained competitive results for Norwegian in the
shared task, with rankings ranging between ﬁrst place
(lemmatization; both variants) and ninth place (Bokmål de-
pendency parsing LAS) out of 33 participating teams. In
terms of performance for the different sub-tasks, UDPipe
reported F1 scores – for Bokmål / Nynorsk respectively –
on sentence segmentation of 96.38 / 92.08, tokenization of
99.79 / 99.93, lemmatization of 96.66 / 96.48, morpho-
logical analysis of 95.56 / 95.25, part-of-speech tagging
of 96.83 / 96.54, and Labeled Accuracy Scores for depen-
dency parsing of 83.89 / 82.74.

CoNLL-U ﬁles When extracting the text from the canon-
ical HTML to pass it to UDPipe, we strip away all mark-up
and discard all content marked for removal as described in
Section 2.2. Double newlines were inserted between para-
graphs and excess whitespace trimmed away. Importantly,
however, the text structure is retained in CoNLL-U by tak-
ing advantage of the support for comments to mark para-
graphs and sentences. In addition to the global document
ID number, each paragraph and sentence is also assigned a
running ID within the document, using the following form:

respectively. While the precise details of how the classiﬁer was
trained are not clear, it appears to us after some experimentation
that the classﬁcation of Bokmål is more accurate when specyﬁng
no rather than nb and hence is what we use here (together with
nn). We still use the language codes nb and nn when adding in-
formation about the detected standards to the metadata in NoReC.

Category

# Reviews

Screen
Music
Literature
Products
Games
Restaurants
Stage
Sports
Misc

Total

13,085
12,410
3,530
3,120
1,765
534
530
118
102

35,194

Table 3: Number of reviews across categories.

• Paragraphs: <review-id>-<paragraph-id>,
e.g. 000001-03 for paragraph 3 in document 1.
• Sentences: <review-id>-<paragraph-id>-
<sentence-id>, e.g. 000001-03-02 for sen-
tence 2 in paragraph 3 in document 1.

After completing the UDPipe pre-processing, the corpus
comprises a total of 837,914 sentences and 14,819,248 to-
kens; see Table 2 for an overview of some core corpus
counts. A script for executing the entire pipeline from text
extraction through UDPipe parsing will be made available
from the NoReC git repository.

2.4. Metadata and thematic categories
For all the identiﬁed reviews, we also extract various kinds
of relevant metadata, made available in a JSON representa-
tion with normalized attribute–value names across reviews.
Metadata in NoReC include information like the URL of
the originally published document, numerical rating, pub-
lishing date, author list, domain or thematic category, orig-
inal ID in the source, and more. Beyond this we also
add information about the identiﬁed language variety (Bok-
mål/Nynorsk), the assigned data split (test/dev/train, as fur-
ther described in Section 2.5.), the assigned document ID,
and ﬁnally a normalized thematic category.

Thematic categories The ‘category’ attribute warrants
some elaboration. The use of thematic categories and/or
tags varies a lot between the different sources, ranging from
highly granular categories to umbrella categories encom-
passing many different domains. Based on the original in-
ventory of categories, each review in NoReC is mapped to
one out of nine normalized thematic categories with En-
glish names. The distribution over categories is shown in
Table 3, sorted by frequency.
For some sources, this normalization is a matter of simple
one-to-one mapping, while for others it is more complex,
involving heuristics based on the presence of certain tags
and keywords in the title. The granularity in the ﬁnal set of
categories is limited by the granularity in the sources. How-
ever, the original (Norwegian) source categories are also
preserved in the metadata (‘source-category’).
As seen from Table 3, the two categories that are by far the
largest are ‘screen’ and ‘music’. While the former covers
reviews about movies and TV-series, the latter covers both
musical recordings and performances. The related category
‘stage’ covers theater, opera, ballet, musical and other stage

s
w
e
i
v
e
R

12,000

10,000

8,000

6,000

4,000

2,000

0

1

2

5

6

3

4

Rating

Figure 1: Number of reviews across ratings.

performances besides music. The perhaps most diverse
category is ‘products’, which comprises product reviews
across a number of sub-categories, ranging from cars and
boats to mobile phones and home electronics, in addition to
travel and more. The remaining categories of ‘literature’,
‘games’, ‘restaurants’, and ‘sports’ are self-explanatory,
while the ‘misc’ category was included to cover topics that
were infrequent or that could not easily be mapped to any
of the other categories by simple heuristics.

Ratings From the perspective of SA, the most immedi-
ately relevant piece of metadata is obviously the rating. As
discussed previously, all the reviews were originally pub-
lished with an integer-valued rating between 1 and 6, visu-
ally indicated using the face of a die. Figure 1 shows the
distribution of reviews relative to rating scores. We see that
rating values of 4 and 5 are the most common, while rather
few reviews were given the lowest possible rating of 1. For
the ﬁnal version of the paper we plan to report more de-
tails on the rating distributions, for example investigating
whether there are differences in the use of the rating scale
between different sources, domains, or authors. For future
work it would also be interesting to try to uncover whether
any rating biases exists, for example related to gender.

2.5. Formats and availability
Distributed under a CC BY-NC 4.0 license,4 NoReC is
available for download from the following git repository:
https://github.com/ltgoslo/norec

Formats NoReC distributes two formats. The ﬁrst is the
CoNLL-U format as described in Section 2.3., containing
sentence segmented and tokenized text annotated with PoS
tags and dependency graphs. This is considered the primary
format. Secondly, we also distribute the canonical HTML
representation of the ‘raw’ review documents as described
in Section 2.2.. For each format, each review is stored as
a separate ﬁle, with the ﬁlename given by the review ID.
To facilitate a low barrier of use for different types of end-
users, we also include scripts for converting from CoNLL-
U to running tokenized text (using either full-forms or lem-
mas) and from HTML to raw text without pre-processing.

4https://creativecommons.org/licenses/

by-nc/4.0/

The metadata for each review is provided as a JSON ob-
ject, all listed in a single ﬁle and indexed on the document
IDs. The NoReC git repository will include a Python mod-
ule with basic functionality for reading the CoNLL-U and
JSON representations, as to make experimentation with the
corpus as accessible and convenient as possible.

Train/dev/test splits To facilitate replicability of experi-
ments, NoReC comes with pre-deﬁned splits for training,
development and testing. These were deﬁned by ﬁrst sort-
ing all reviews for each category by publishing date and
then reserving the ﬁrst 80% for training, the subsequent
10% for development, and the ﬁnal 10% for held-out test-
ing. This splitting strategy makes the test setting as realistic
as possible and avoids having multiple reviews for the same
product (from different sources) across splits.

3. Summary and outlook
The current paper describes the creation of the Norwegian
Review Corpus; NoReC. The ﬁnal dataset comprises more
than 35,000 full-text reviews (≈ 15 million tokens) from
a wide range of different domains, collected from several
major Norwegian news sources. Each review is rated with a
numerical score on a scale of 1–6, and can be used for train-
ing and evaluating models for document-level sentiment
analysis. While the primary distribution format of the cor-
pus is CoNLL-U – based on only the extracted text and ap-
plying UDPipe for a full pre-processing pipeline from sen-
tence segmentation to dependency parsing – the release also
includes HTML representations of the full reviews with all
content preserved. Each review is in addition associated
with a rich set of metadata, including thematic category,
numerical rating, and more. We also provide pre-deﬁned
splits for training, development and testing.
For future work, the SANT project will seek to build on
NoReC to (i) experiment with both polarity classiﬁcation
and rating inference on the document-level using neural ar-
chitectures, (ii) extract SA lexicons encoding the polarity
of individual words, and ﬁnally (iii) also move beyond the
document-level and manually add more ﬁne-grained and
aspect-based SA annotations for a sub-set of the corpus.
Across all these activities, the various thematic categories
will be useful for assessing cross-domain effects (e.g., how
well does an SA classiﬁer trained on movie reviews per-
form for home electronics?) and potentially even for train-
ing domain-speciﬁc models. It will of course also be im-
portant to asses how well SA resources developed on the
basis of the reviews generalize to non-review texts.
Resources for sentiment analysis have so far been unavail-
able for Norwegian. As such, NoReC represents a highly
sought-after addition to Norwegian language technology,
valuable to both industry and the research community alike.

4. Acknowledgments
This work was carried out as part of the SANT research
project (Sentiment Analysis for Norwegian Text), funded
by an IKTPLUSS grant from the Research Council of Nor-
way (project no. 270908). SANT represents a newly initi-
ated collaboration between the Department of Informatics
at the University of Oslo, and three of Norway’s largest

media groups; the Norwegian Broadcasting Corporation,
Schibsted Media Group and Aller Media.

Conference on Knowledge Discovery and Data Mining
(KDD’2010), page 783 – 792.

Zeman, D., Popel, M., Straka, M., Hajic, J., Nivre, J.,
Ginter, F., Luotolahti, J., Pyysalo, S., Petrov, S., Pot-
thast, M., Tyers, F., Badmaeva, E., Gokirmak, M.,
Nedoluzhko, A., Cinkova, S., Hajic jr., J., Hlavacova,
J., Kettnerová, V., Uresova, Z., Kanerva, J., Ojala, S.,
Missilä, A., Manning, C. D., Schuster, S., Reddy, S.,
Taji, D., Habash, N., Leung, H., de Marneffe, M.-C.,
Sanguinetti, M., Simi, M., Kanayama, H., dePaiva, V.,
Droganova, K., Martínez Alonso, H., Çöltekin, c., Su-
lubacak, U., Uszkoreit, H., Macketanz, V., Burchardt,
A., Harris, K., Marheinecke, K., Rehm, G., Kayadelen,
T., Attia, M., Elkahky, A., Yu, Z., Pitler, E., Lertpradit,
S., Mandl, M., Kirchner, J., Alcalde, H. F., Strnadová,
J., Banerjee, E., Manurung, R., Stella, A., Shimada, A.,
Kwak, S., Mendonca, G., Lando, T., Nitisaroj, R., and Li,
J. (2017). Conll 2017 shared task: Multilingual parsing
from raw text to universal dependencies. In Proceedings
of the CoNLL 2017 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies, page 1 – 19,
Vancouver, Canada.

5. Bibliographical references

Blitzer, J., Dredze, M., and Pereira, F. (2007). Biographies,
bollywood, boom-boxes and blenders: Domain adapta-
tion for sentiment classiﬁcation. In Proceedings of the
45th Meeting of the Association for Computational Lin-
guistics, page 187 – 205, Prague, Czech Republic.

Lui, M. and Baldwin, T. (2012).

langid.py: An off-the-
shelf language identiﬁcation tool. In Proceedings of the
50th Meeting of the Association for Computational Lin-
guistics System Demonstrations, page 25 – 30, Jeju, Re-
public of Korea.

Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y.,
and Potts, C. (2011). Learning word vectors for senti-
ment analysis. In Proceedings of the 49th Meeting of the
Association for Computational Linguistics, page 142 –
150, Portland, OR, USA.

Nivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y.,
Hajiˇc, J., Manning, C. D., McDonald, R., Petrov, S.,
Pyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D.
(2016). Universal dependencies v1: A multilingual tree-
In Proceedings of the 10th Interna-
bank collection.
tional Conference on Language Resources and Evalua-
tion, Portorož, Slovenia.
Øvrelid, L. and Hohle, P.

(2016). Norwegian Univer-
sal Dependencies. In Proceedings of the 10th Interna-
tional Conference on Language Resources and Evalua-
tion, page 1579 – 1585, Portorož, Slovenia.

Pang, B. and Lee, L. (2005). Seeing stars: Exploiting class
relationships for sentiment categorization with respect to
rating scales. In Proceedings of the 43rd Meeting of the
Association for Computational Linguistics, page 115 –
124, Ann Arbor, MI, USA.

Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.,
Ng, A., and Potts, C. (2013). Recursive deep models for
semantic compositionality over a sentiment treebank. In
Proceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, page 1631 – 1642,
Seattle, WA, USA.

Straka, M. and Straková, J. (2017). Tokenizing, POS tag-
ging, lemmatizing and parsing UD 2.0 with UDPipe. In
Proceedings of the CoNLL 2017 Shared Task: Multilin-
gual Parsing from Raw Text to Universal Dependencies,
pages 88–99, Vancouver, Canada.

Straka, M., Hajiˇc, J., and Straková, J. (2016). UDPipe:
trainable pipeline for processing CoNLL-U ﬁles per-
forming tokenization, morphological analysis, pos tag-
ging and parsing. In Proceedings of the Tenth Interna-
tional Conference on Language Resources and Evalua-
tion, Portorož, Slovenia.

Velldal, E., Øvrelid, L., and Hohle, P. (2017). Joint UD
parsing of Norwegian Bokmål and Nynorsk. In Proceed-
ings of the 11th Nordic Conference of Computational
Linguistics, page 1 – 10, Gothenburg, Sweden.

Wang, H., Lu, Y., and Zhai, C. (2010). Latent aspect rat-
ing analysis on review text data: A rating regression
In Proceedings of the 16th ACM SIGKDD
approach.

NoReC: The Norwegian Review Corpus

Erik Velldal, Lilja Øvrelid, Eivind Alexander Bergem, Cathrine Stadsnes,
Samia Touileb, Fredrik Jørgensen†
Language Technology Group, Department of Informatics, University of Oslo
{erikve, liljao, eivinabe, cathsta, samiat}@iﬁ.uio.no
†Schibsted Media Group
fredrik.jorgensen@schibsted.com

7
1
0
2
 
t
c
O
 
5
1
 
 
]
L
C
.
s
c
[
 
 
1
v
0
7
3
5
0
.
0
1
7
1
:
v
i
X
r
a

Abstract
This paper presents the Norwegian Review Corpus (NoReC), created for training and evaluating models for document-level sentiment
analysis. The full-text reviews have been collected from major Norwegian news sources and cover a range of different domains, including
literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review
is labeled with a manually assigned score of 1–6, as provided by the rating of the original author. This ﬁrst release of the corpus
comprises more than 35,000 reviews. It is distributed using the CoNLL-U format, pre-processed using UDPipe, along with a rich set of
metadata. The work reported in this paper forms part of the SANT initiative (Sentiment Analysis for Norwegian Text), a project seeking
to provide resources and tools for sentiment analysis and opinion mining for Norwegian. As resources for sentiment analysis have
so far been unavailable for Norwegian, NoReC represents a highly valuable and sought-after addition to Norwegian language technology.

Keywords: Sentiment Analysis, Opinion Mining, Corpus, Norwegian, Reviews

1.

Introduction

Norwegian is in many ways an under-resourced language,
with training and evaluation data still lacking for many core
NLP tasks. The current work aims to alleviate the situation
for the particular task of sentiment analysis. The SANT
project – Sentiment Analysis for Norwegian Text – seeks
to create, and make publicly available, resources and tools
for sentiment analysis for Norwegian. The SANT effort
described in the current paper marks the release of the Nor-
wegian Review Corpus1 (NoReC). The dataset comprises
more than 35,000 full-text reviews from a range of differ-
ent domains, collected from several of the major Norwe-
gian news sources. Each review is rated with a numerical
score on a scale of 1–6, and can be used for training and
evaluating models for document-level sentiment analysis.

1.1. Rating by dice
A particularity of review journalism in Norway,
is the
wholesale adoption of dice rolls (‘terningkast’) as a stan-
dard rating scale: The item under review is rated on a scale
of 1–6, commonly visualized by the face of a die with a
corresponding number of ‘dots’ or pips. The practice is
thought to have been introduced already in 1952 when re-
viewing movies in the newspaper Verdens Gang (VG). By
now it is has found widespread use in all sorts of arts and
consumer journalism and is used when reviewing every-
thing from books, theater and music, to home electronics,
restaurants, and children’s clothing.
The rating practice described above has several beneﬁts for
the goal of document-level SA: (i) It eliminates the need for
costly manual annotation since the numerical rating (i.e.,
the die roll) directly provides us with the labels needed for
training models for detecting the overall document polar-
ity. (ii) There is no need for manually deﬁned mappings to
align different rating schemes as the reviews all use a uni-

1https://github.com/ltgoslo/norec

form scale. (iii) The wide range of available news sources
using the same rating practice, including all the major na-
tional newspapers, facilitates the creation of a large-scale
dataset. (iv) Models trained on the dataset can be expected
to generalize well across domains given the balance of dif-
ferent topics covered in the corpus. For English, a sub-
stantial amount of SA research has been directed towards
predicting the sentiment of movie reviews, collected from
aggregator sites like IMDb.com and RottenTomatoes.com
(Pang and Lee, 2005; Socher et al., 2013) or other types of
consumer reviews (Blitzer et al., 2007; Wang et al., 2010;
Maas et al., 2011). The NoRec data set contains a variety
of different types of reviews from a range of domains.

1.2. Sources and partners

The SANT project represents a newly initiated collabora-
tion between the Language Technology Group (LTG) at the
Department of Informatics at the University of Oslo, and
three of Norway’s largest media groups; the Norwegian
Broadcasting Corporation (NRK – the state-owned public
broadcaster) and the privately held Schibsted Media Group
and Aller Media. This ﬁrst release of NoReC comprises
35,194 reviews extracted from eight different news sources
as contributed by the three media partners. In terms of pub-
lishing date the reviews mainly cover the time span 2003–
2017, although it also includes a handful of reviews dating
back as far as 1998. We brieﬂy present the sources provided
by the different partners below.

Schibsted Media Group The Schibsted group has con-
tributed content from their full portfolio of Norwegian news
sources: VG, Aftenposten, Fædrelandsvennen, Bergens
Tidende, and Stavanger Aftenblad. While the latter three
rank among Norway’s largest regional newspapers, Aften-
posten is the largest national newspaper in terms of circu-
lation and VG is the largest online news source with more
than 2.4 million readers across all platforms.

Source

# Reviews

VG
Dagbladet
Stavanger Aftenblad
P3.no
DinSide.no
Fædrelandsvennen
Bergens Tidene
Aftenposten

Total

11,888
5,305
5,146
5,017
2,944
2,296
1,675
923

35,194

Table 1: Number of reviews across sources.

Aller Media The Aller publishing company has con-
tributed content from two sources. The ﬁrst is the online
version of the newspaper Dagbladet – the second most vis-
ited online news source in Norway – publishing reviews
for music recordings and live performances, theater and
related stage performances, movies, literature, restaurants
and more. The second source, DinSide.no, is a website
specializing in product reviews, covering a wide range of
product types, from home electronics to cars and clothing.

NRK The Norwegian Broadcasting Corporation is a
state-owned media house, with a special mandate to be
a non-commercial, politically independent public broad-
caster. For the review corpus, NRK has contributed content
from the website P3.no which has an extensive back cata-
log of ‘die-rated’ reviews of movies, TV series, computer
games, and music (both recordings and live performances).
Figure 1 shows the number of reviews included in the ﬁnal
corpus, broken down across the various sources.

2. Corpus creation
The original document collections were provided from the
media sources in various JSON, HTML and XML formats,
and a substantial effort has gone into identifying relevant
documents and extracting text and associated metadata.
The extraction process can be summarized by the following
three steps: (i) Identify reviews, (ii) convert review con-
tent to an intermediate and canonical HTML format, (iii)
extract text and pass it through linguistic pre-processing,
producing representations in CoNLL-U format, and ﬁnally
(iv) extract relevant metadata to a JSON representation with
normalized attribute–value names. We brieﬂy comment on
each of these steps in turn below.

Identifying reviews

2.1.
Some of the initial document dumps also included other ar-
ticles beyond reviews, and in these cases reviews had to be
identiﬁed. While in some cases this could be done simply
by checking for an appropriate metadata ﬁeld indicating the
rating score, other cases required checking for links point-
ing to an image of a die (indicating the rating), or similar
heuristics.
For some of the sources, one and the same document may
contain multiple reviews, for example for product com-
parisons.
In these cases we had to identify and separate
out the different sub-reviews. Different publishing conven-
tions required targeting different types of cues in the docu-
ment structure, like headers, bold-faced content or die-face

images. This also involved extraction of titles and rating
scores for the different sub-reviews. The identiﬁed sub-
reviews become separate documents in the NoReC data set.
In total, 35,194 distinct reviews were extracted from the
data provided by the media partners.

2.2. Converting content to canonical HTML
The raw data dumps from the sources are mostly in HTML
format, but may also be e.g. JSON objects, and have differ-
ent conventions for document structuring and use of mark-
up.
In order to streamline the downstream text extrac-
tion, all documents were converted to a ‘canonical’ HTML
format where all textual content is located either inside a
In addition to containing the
header or a paragraph tag.
review text, the raw representations also contains images,
ads and other content not considered part of the running
text.
In order to identify and mark the non-relevant text
we used a combination of heuristics based on simple string
matching and properties like paragraph length and ratio of
content to markup. For example, care was taken to iden-
tify ‘you-might-also-be-interested-in’ type links that are in-
jected throughout the texts in an attempt to keep the reader
on the website and generate more clicks. Importantly, how-
ever, we chose not to remove content when converting to
our intermediate HTML format, instead introducing a new
tag – remove – in which we enclose content considered
non-relevant. This non-destructive approach preserves the
original content, as to not close the door on changes to the
subsequent task of text extraction later.

2.3. Linguistic enrichments and CoNLL-U
Given the canonical HTML format described above, it is
straightforward to extract the relevant text. In order to en-
able various types of downstream uses of the dataset, we
further preprocess the raw text using the UDPipe toolkit
(Straka et al., 2016), representing each review as a CoNLL-
U ﬁle, following the format deﬁned in Universal Dependen-
cies version 2.2 In this step we perform sentence segmenta-
tion, tokenization, lemmatization, morphological analysis,
part-of-speech tagging and dependency parsing, following
the Universal Dependencies scheme (Nivre et al., 2016).
However, the preprocessing set-up is slightly complicated
by the fact that the Norwegian language has two ofﬁcial
written standards – Bokmål (the main variety) and Nynorsk
– both of which are represented in the review corpus. Below
we ﬁrst describe how language identiﬁcation is performed,
and then go on to give more details about UDPipe and the
resulting CoNLL-U data.

Identifying language varieties The two ofﬁcial varieties
are closely related and they are mostly distinguished by
minor lexical differences. Still, the differences are strong
enough that different preprocessing pipelines must be used
for the different standards, hence it is important to identify
the standard within a particular document. Therefore, we
have used langid.py (Lui and Baldwin, 2012) to iden-
tify the standard for each review.3 We performed an evalu-

2http://universaldependencies.org/format.

html

3langid.py can actually identify three different variants:
no, nn and nb, for Norwegian (mixed), Nynorsk and Bokmål,

Documents
Sentences
Tokens
Types, full-forms
Types, lemmas

#

35,194
837,914
14,819,248
521,563
446,532

Table 2: Basic corpus counts.

ation of langid.py on 1599 reviews of which 1487 were
written in Bokmål and 112 in Nynorsk (based on selecting
reviews from authors known to write in a given variety). On
this sample langid.py achieved 100% accuracy. While
the main variety, i.e. Bokmål, dominates the distribution
in the corpus with 34,661 documents, we also identiﬁed
533 documents in Nynorsk (mainly from the sources Fæ-
drelandsvennen, Bergens Tidende and P3.no).

UDpipe conﬁguration We applied UDPipe (Straka et al.,
2016) v.1.2 with its pre-trained models for Norwegian Bok-
mål and Nynorsk. This version of the UDPipe software and
the pre-trained models were developed for the CoNLL 2017
shared task (Zeman et al., 2017), which was devoted to
parsing from raw text to Universal Dependencies for more
than 40 different languages. We use of the models trained
for participation in the shared task (Straka and Straková,
2017), not the models provided as baseline models for the
participants. The Norwegian models were trained on the
UD 2.0 versions of the Norwegian UD treebanks (Øvrelid
and Hohle, 2016; Velldal et al., 2017) in conjunction with
the aforementioned shared task, and the subsequent choice
of model for use (Bokmål vs Nynorsk) was determined by
the language identiﬁed for each particular review.
UDPipe obtained competitive results for Norwegian in the
shared task, with rankings ranging between ﬁrst place
(lemmatization; both variants) and ninth place (Bokmål de-
pendency parsing LAS) out of 33 participating teams. In
terms of performance for the different sub-tasks, UDPipe
reported F1 scores – for Bokmål / Nynorsk respectively –
on sentence segmentation of 96.38 / 92.08, tokenization of
99.79 / 99.93, lemmatization of 96.66 / 96.48, morpho-
logical analysis of 95.56 / 95.25, part-of-speech tagging
of 96.83 / 96.54, and Labeled Accuracy Scores for depen-
dency parsing of 83.89 / 82.74.

CoNLL-U ﬁles When extracting the text from the canon-
ical HTML to pass it to UDPipe, we strip away all mark-up
and discard all content marked for removal as described in
Section 2.2. Double newlines were inserted between para-
graphs and excess whitespace trimmed away. Importantly,
however, the text structure is retained in CoNLL-U by tak-
ing advantage of the support for comments to mark para-
graphs and sentences. In addition to the global document
ID number, each paragraph and sentence is also assigned a
running ID within the document, using the following form:

respectively. While the precise details of how the classiﬁer was
trained are not clear, it appears to us after some experimentation
that the classﬁcation of Bokmål is more accurate when specyﬁng
no rather than nb and hence is what we use here (together with
nn). We still use the language codes nb and nn when adding in-
formation about the detected standards to the metadata in NoReC.

Category

# Reviews

Screen
Music
Literature
Products
Games
Restaurants
Stage
Sports
Misc

Total

13,085
12,410
3,530
3,120
1,765
534
530
118
102

35,194

Table 3: Number of reviews across categories.

• Paragraphs: <review-id>-<paragraph-id>,
e.g. 000001-03 for paragraph 3 in document 1.
• Sentences: <review-id>-<paragraph-id>-
<sentence-id>, e.g. 000001-03-02 for sen-
tence 2 in paragraph 3 in document 1.

After completing the UDPipe pre-processing, the corpus
comprises a total of 837,914 sentences and 14,819,248 to-
kens; see Table 2 for an overview of some core corpus
counts. A script for executing the entire pipeline from text
extraction through UDPipe parsing will be made available
from the NoReC git repository.

2.4. Metadata and thematic categories
For all the identiﬁed reviews, we also extract various kinds
of relevant metadata, made available in a JSON representa-
tion with normalized attribute–value names across reviews.
Metadata in NoReC include information like the URL of
the originally published document, numerical rating, pub-
lishing date, author list, domain or thematic category, orig-
inal ID in the source, and more. Beyond this we also
add information about the identiﬁed language variety (Bok-
mål/Nynorsk), the assigned data split (test/dev/train, as fur-
ther described in Section 2.5.), the assigned document ID,
and ﬁnally a normalized thematic category.

Thematic categories The ‘category’ attribute warrants
some elaboration. The use of thematic categories and/or
tags varies a lot between the different sources, ranging from
highly granular categories to umbrella categories encom-
passing many different domains. Based on the original in-
ventory of categories, each review in NoReC is mapped to
one out of nine normalized thematic categories with En-
glish names. The distribution over categories is shown in
Table 3, sorted by frequency.
For some sources, this normalization is a matter of simple
one-to-one mapping, while for others it is more complex,
involving heuristics based on the presence of certain tags
and keywords in the title. The granularity in the ﬁnal set of
categories is limited by the granularity in the sources. How-
ever, the original (Norwegian) source categories are also
preserved in the metadata (‘source-category’).
As seen from Table 3, the two categories that are by far the
largest are ‘screen’ and ‘music’. While the former covers
reviews about movies and TV-series, the latter covers both
musical recordings and performances. The related category
‘stage’ covers theater, opera, ballet, musical and other stage

s
w
e
i
v
e
R

12,000

10,000

8,000

6,000

4,000

2,000

0

1

2

5

6

3

4

Rating

Figure 1: Number of reviews across ratings.

performances besides music. The perhaps most diverse
category is ‘products’, which comprises product reviews
across a number of sub-categories, ranging from cars and
boats to mobile phones and home electronics, in addition to
travel and more. The remaining categories of ‘literature’,
‘games’, ‘restaurants’, and ‘sports’ are self-explanatory,
while the ‘misc’ category was included to cover topics that
were infrequent or that could not easily be mapped to any
of the other categories by simple heuristics.

Ratings From the perspective of SA, the most immedi-
ately relevant piece of metadata is obviously the rating. As
discussed previously, all the reviews were originally pub-
lished with an integer-valued rating between 1 and 6, visu-
ally indicated using the face of a die. Figure 1 shows the
distribution of reviews relative to rating scores. We see that
rating values of 4 and 5 are the most common, while rather
few reviews were given the lowest possible rating of 1. For
the ﬁnal version of the paper we plan to report more de-
tails on the rating distributions, for example investigating
whether there are differences in the use of the rating scale
between different sources, domains, or authors. For future
work it would also be interesting to try to uncover whether
any rating biases exists, for example related to gender.

2.5. Formats and availability
Distributed under a CC BY-NC 4.0 license,4 NoReC is
available for download from the following git repository:
https://github.com/ltgoslo/norec

Formats NoReC distributes two formats. The ﬁrst is the
CoNLL-U format as described in Section 2.3., containing
sentence segmented and tokenized text annotated with PoS
tags and dependency graphs. This is considered the primary
format. Secondly, we also distribute the canonical HTML
representation of the ‘raw’ review documents as described
in Section 2.2.. For each format, each review is stored as
a separate ﬁle, with the ﬁlename given by the review ID.
To facilitate a low barrier of use for different types of end-
users, we also include scripts for converting from CoNLL-
U to running tokenized text (using either full-forms or lem-
mas) and from HTML to raw text without pre-processing.

4https://creativecommons.org/licenses/

by-nc/4.0/

The metadata for each review is provided as a JSON ob-
ject, all listed in a single ﬁle and indexed on the document
IDs. The NoReC git repository will include a Python mod-
ule with basic functionality for reading the CoNLL-U and
JSON representations, as to make experimentation with the
corpus as accessible and convenient as possible.

Train/dev/test splits To facilitate replicability of experi-
ments, NoReC comes with pre-deﬁned splits for training,
development and testing. These were deﬁned by ﬁrst sort-
ing all reviews for each category by publishing date and
then reserving the ﬁrst 80% for training, the subsequent
10% for development, and the ﬁnal 10% for held-out test-
ing. This splitting strategy makes the test setting as realistic
as possible and avoids having multiple reviews for the same
product (from different sources) across splits.

3. Summary and outlook
The current paper describes the creation of the Norwegian
Review Corpus; NoReC. The ﬁnal dataset comprises more
than 35,000 full-text reviews (≈ 15 million tokens) from
a wide range of different domains, collected from several
major Norwegian news sources. Each review is rated with a
numerical score on a scale of 1–6, and can be used for train-
ing and evaluating models for document-level sentiment
analysis. While the primary distribution format of the cor-
pus is CoNLL-U – based on only the extracted text and ap-
plying UDPipe for a full pre-processing pipeline from sen-
tence segmentation to dependency parsing – the release also
includes HTML representations of the full reviews with all
content preserved. Each review is in addition associated
with a rich set of metadata, including thematic category,
numerical rating, and more. We also provide pre-deﬁned
splits for training, development and testing.
For future work, the SANT project will seek to build on
NoReC to (i) experiment with both polarity classiﬁcation
and rating inference on the document-level using neural ar-
chitectures, (ii) extract SA lexicons encoding the polarity
of individual words, and ﬁnally (iii) also move beyond the
document-level and manually add more ﬁne-grained and
aspect-based SA annotations for a sub-set of the corpus.
Across all these activities, the various thematic categories
will be useful for assessing cross-domain effects (e.g., how
well does an SA classiﬁer trained on movie reviews per-
form for home electronics?) and potentially even for train-
ing domain-speciﬁc models. It will of course also be im-
portant to asses how well SA resources developed on the
basis of the reviews generalize to non-review texts.
Resources for sentiment analysis have so far been unavail-
able for Norwegian. As such, NoReC represents a highly
sought-after addition to Norwegian language technology,
valuable to both industry and the research community alike.

4. Acknowledgments
This work was carried out as part of the SANT research
project (Sentiment Analysis for Norwegian Text), funded
by an IKTPLUSS grant from the Research Council of Nor-
way (project no. 270908). SANT represents a newly initi-
ated collaboration between the Department of Informatics
at the University of Oslo, and three of Norway’s largest

media groups; the Norwegian Broadcasting Corporation,
Schibsted Media Group and Aller Media.

Conference on Knowledge Discovery and Data Mining
(KDD’2010), page 783 – 792.

Zeman, D., Popel, M., Straka, M., Hajic, J., Nivre, J.,
Ginter, F., Luotolahti, J., Pyysalo, S., Petrov, S., Pot-
thast, M., Tyers, F., Badmaeva, E., Gokirmak, M.,
Nedoluzhko, A., Cinkova, S., Hajic jr., J., Hlavacova,
J., Kettnerová, V., Uresova, Z., Kanerva, J., Ojala, S.,
Missilä, A., Manning, C. D., Schuster, S., Reddy, S.,
Taji, D., Habash, N., Leung, H., de Marneffe, M.-C.,
Sanguinetti, M., Simi, M., Kanayama, H., dePaiva, V.,
Droganova, K., Martínez Alonso, H., Çöltekin, c., Su-
lubacak, U., Uszkoreit, H., Macketanz, V., Burchardt,
A., Harris, K., Marheinecke, K., Rehm, G., Kayadelen,
T., Attia, M., Elkahky, A., Yu, Z., Pitler, E., Lertpradit,
S., Mandl, M., Kirchner, J., Alcalde, H. F., Strnadová,
J., Banerjee, E., Manurung, R., Stella, A., Shimada, A.,
Kwak, S., Mendonca, G., Lando, T., Nitisaroj, R., and Li,
J. (2017). Conll 2017 shared task: Multilingual parsing
from raw text to universal dependencies. In Proceedings
of the CoNLL 2017 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies, page 1 – 19,
Vancouver, Canada.

5. Bibliographical references

Blitzer, J., Dredze, M., and Pereira, F. (2007). Biographies,
bollywood, boom-boxes and blenders: Domain adapta-
tion for sentiment classiﬁcation. In Proceedings of the
45th Meeting of the Association for Computational Lin-
guistics, page 187 – 205, Prague, Czech Republic.

Lui, M. and Baldwin, T. (2012).

langid.py: An off-the-
shelf language identiﬁcation tool. In Proceedings of the
50th Meeting of the Association for Computational Lin-
guistics System Demonstrations, page 25 – 30, Jeju, Re-
public of Korea.

Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y.,
and Potts, C. (2011). Learning word vectors for senti-
ment analysis. In Proceedings of the 49th Meeting of the
Association for Computational Linguistics, page 142 –
150, Portland, OR, USA.

Nivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y.,
Hajiˇc, J., Manning, C. D., McDonald, R., Petrov, S.,
Pyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D.
(2016). Universal dependencies v1: A multilingual tree-
In Proceedings of the 10th Interna-
bank collection.
tional Conference on Language Resources and Evalua-
tion, Portorož, Slovenia.
Øvrelid, L. and Hohle, P.

(2016). Norwegian Univer-
sal Dependencies. In Proceedings of the 10th Interna-
tional Conference on Language Resources and Evalua-
tion, page 1579 – 1585, Portorož, Slovenia.

Pang, B. and Lee, L. (2005). Seeing stars: Exploiting class
relationships for sentiment categorization with respect to
rating scales. In Proceedings of the 43rd Meeting of the
Association for Computational Linguistics, page 115 –
124, Ann Arbor, MI, USA.

Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.,
Ng, A., and Potts, C. (2013). Recursive deep models for
semantic compositionality over a sentiment treebank. In
Proceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, page 1631 – 1642,
Seattle, WA, USA.

Straka, M. and Straková, J. (2017). Tokenizing, POS tag-
ging, lemmatizing and parsing UD 2.0 with UDPipe. In
Proceedings of the CoNLL 2017 Shared Task: Multilin-
gual Parsing from Raw Text to Universal Dependencies,
pages 88–99, Vancouver, Canada.

Straka, M., Hajiˇc, J., and Straková, J. (2016). UDPipe:
trainable pipeline for processing CoNLL-U ﬁles per-
forming tokenization, morphological analysis, pos tag-
ging and parsing. In Proceedings of the Tenth Interna-
tional Conference on Language Resources and Evalua-
tion, Portorož, Slovenia.

Velldal, E., Øvrelid, L., and Hohle, P. (2017). Joint UD
parsing of Norwegian Bokmål and Nynorsk. In Proceed-
ings of the 11th Nordic Conference of Computational
Linguistics, page 1 – 10, Gothenburg, Sweden.

Wang, H., Lu, Y., and Zhai, C. (2010). Latent aspect rat-
ing analysis on review text data: A rating regression
In Proceedings of the 16th ACM SIGKDD
approach.

NoReC: The Norwegian Review Corpus

Erik Velldal, Lilja Øvrelid, Eivind Alexander Bergem, Cathrine Stadsnes,
Samia Touileb, Fredrik Jørgensen†
Language Technology Group, Department of Informatics, University of Oslo
{erikve, liljao, eivinabe, cathsta, samiat}@iﬁ.uio.no
†Schibsted Media Group
fredrik.jorgensen@schibsted.com

7
1
0
2
 
t
c
O
 
5
1
 
 
]
L
C
.
s
c
[
 
 
1
v
0
7
3
5
0
.
0
1
7
1
:
v
i
X
r
a

Abstract
This paper presents the Norwegian Review Corpus (NoReC), created for training and evaluating models for document-level sentiment
analysis. The full-text reviews have been collected from major Norwegian news sources and cover a range of different domains, including
literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review
is labeled with a manually assigned score of 1–6, as provided by the rating of the original author. This ﬁrst release of the corpus
comprises more than 35,000 reviews. It is distributed using the CoNLL-U format, pre-processed using UDPipe, along with a rich set of
metadata. The work reported in this paper forms part of the SANT initiative (Sentiment Analysis for Norwegian Text), a project seeking
to provide resources and tools for sentiment analysis and opinion mining for Norwegian. As resources for sentiment analysis have
so far been unavailable for Norwegian, NoReC represents a highly valuable and sought-after addition to Norwegian language technology.

Keywords: Sentiment Analysis, Opinion Mining, Corpus, Norwegian, Reviews

1.

Introduction

Norwegian is in many ways an under-resourced language,
with training and evaluation data still lacking for many core
NLP tasks. The current work aims to alleviate the situation
for the particular task of sentiment analysis. The SANT
project – Sentiment Analysis for Norwegian Text – seeks
to create, and make publicly available, resources and tools
for sentiment analysis for Norwegian. The SANT effort
described in the current paper marks the release of the Nor-
wegian Review Corpus1 (NoReC). The dataset comprises
more than 35,000 full-text reviews from a range of differ-
ent domains, collected from several of the major Norwe-
gian news sources. Each review is rated with a numerical
score on a scale of 1–6, and can be used for training and
evaluating models for document-level sentiment analysis.

1.1. Rating by dice
A particularity of review journalism in Norway,
is the
wholesale adoption of dice rolls (‘terningkast’) as a stan-
dard rating scale: The item under review is rated on a scale
of 1–6, commonly visualized by the face of a die with a
corresponding number of ‘dots’ or pips. The practice is
thought to have been introduced already in 1952 when re-
viewing movies in the newspaper Verdens Gang (VG). By
now it is has found widespread use in all sorts of arts and
consumer journalism and is used when reviewing every-
thing from books, theater and music, to home electronics,
restaurants, and children’s clothing.
The rating practice described above has several beneﬁts for
the goal of document-level SA: (i) It eliminates the need for
costly manual annotation since the numerical rating (i.e.,
the die roll) directly provides us with the labels needed for
training models for detecting the overall document polar-
ity. (ii) There is no need for manually deﬁned mappings to
align different rating schemes as the reviews all use a uni-

1https://github.com/ltgoslo/norec

form scale. (iii) The wide range of available news sources
using the same rating practice, including all the major na-
tional newspapers, facilitates the creation of a large-scale
dataset. (iv) Models trained on the dataset can be expected
to generalize well across domains given the balance of dif-
ferent topics covered in the corpus. For English, a sub-
stantial amount of SA research has been directed towards
predicting the sentiment of movie reviews, collected from
aggregator sites like IMDb.com and RottenTomatoes.com
(Pang and Lee, 2005; Socher et al., 2013) or other types of
consumer reviews (Blitzer et al., 2007; Wang et al., 2010;
Maas et al., 2011). The NoRec data set contains a variety
of different types of reviews from a range of domains.

1.2. Sources and partners

The SANT project represents a newly initiated collabora-
tion between the Language Technology Group (LTG) at the
Department of Informatics at the University of Oslo, and
three of Norway’s largest media groups; the Norwegian
Broadcasting Corporation (NRK – the state-owned public
broadcaster) and the privately held Schibsted Media Group
and Aller Media. This ﬁrst release of NoReC comprises
35,194 reviews extracted from eight different news sources
as contributed by the three media partners. In terms of pub-
lishing date the reviews mainly cover the time span 2003–
2017, although it also includes a handful of reviews dating
back as far as 1998. We brieﬂy present the sources provided
by the different partners below.

Schibsted Media Group The Schibsted group has con-
tributed content from their full portfolio of Norwegian news
sources: VG, Aftenposten, Fædrelandsvennen, Bergens
Tidende, and Stavanger Aftenblad. While the latter three
rank among Norway’s largest regional newspapers, Aften-
posten is the largest national newspaper in terms of circu-
lation and VG is the largest online news source with more
than 2.4 million readers across all platforms.

Source

# Reviews

VG
Dagbladet
Stavanger Aftenblad
P3.no
DinSide.no
Fædrelandsvennen
Bergens Tidene
Aftenposten

Total

11,888
5,305
5,146
5,017
2,944
2,296
1,675
923

35,194

Table 1: Number of reviews across sources.

Aller Media The Aller publishing company has con-
tributed content from two sources. The ﬁrst is the online
version of the newspaper Dagbladet – the second most vis-
ited online news source in Norway – publishing reviews
for music recordings and live performances, theater and
related stage performances, movies, literature, restaurants
and more. The second source, DinSide.no, is a website
specializing in product reviews, covering a wide range of
product types, from home electronics to cars and clothing.

NRK The Norwegian Broadcasting Corporation is a
state-owned media house, with a special mandate to be
a non-commercial, politically independent public broad-
caster. For the review corpus, NRK has contributed content
from the website P3.no which has an extensive back cata-
log of ‘die-rated’ reviews of movies, TV series, computer
games, and music (both recordings and live performances).
Figure 1 shows the number of reviews included in the ﬁnal
corpus, broken down across the various sources.

2. Corpus creation
The original document collections were provided from the
media sources in various JSON, HTML and XML formats,
and a substantial effort has gone into identifying relevant
documents and extracting text and associated metadata.
The extraction process can be summarized by the following
three steps: (i) Identify reviews, (ii) convert review con-
tent to an intermediate and canonical HTML format, (iii)
extract text and pass it through linguistic pre-processing,
producing representations in CoNLL-U format, and ﬁnally
(iv) extract relevant metadata to a JSON representation with
normalized attribute–value names. We brieﬂy comment on
each of these steps in turn below.

Identifying reviews

2.1.
Some of the initial document dumps also included other ar-
ticles beyond reviews, and in these cases reviews had to be
identiﬁed. While in some cases this could be done simply
by checking for an appropriate metadata ﬁeld indicating the
rating score, other cases required checking for links point-
ing to an image of a die (indicating the rating), or similar
heuristics.
For some of the sources, one and the same document may
contain multiple reviews, for example for product com-
parisons.
In these cases we had to identify and separate
out the different sub-reviews. Different publishing conven-
tions required targeting different types of cues in the docu-
ment structure, like headers, bold-faced content or die-face

images. This also involved extraction of titles and rating
scores for the different sub-reviews. The identiﬁed sub-
reviews become separate documents in the NoReC data set.
In total, 35,194 distinct reviews were extracted from the
data provided by the media partners.

2.2. Converting content to canonical HTML
The raw data dumps from the sources are mostly in HTML
format, but may also be e.g. JSON objects, and have differ-
ent conventions for document structuring and use of mark-
up.
In order to streamline the downstream text extrac-
tion, all documents were converted to a ‘canonical’ HTML
format where all textual content is located either inside a
In addition to containing the
header or a paragraph tag.
review text, the raw representations also contains images,
ads and other content not considered part of the running
text.
In order to identify and mark the non-relevant text
we used a combination of heuristics based on simple string
matching and properties like paragraph length and ratio of
content to markup. For example, care was taken to iden-
tify ‘you-might-also-be-interested-in’ type links that are in-
jected throughout the texts in an attempt to keep the reader
on the website and generate more clicks. Importantly, how-
ever, we chose not to remove content when converting to
our intermediate HTML format, instead introducing a new
tag – remove – in which we enclose content considered
non-relevant. This non-destructive approach preserves the
original content, as to not close the door on changes to the
subsequent task of text extraction later.

2.3. Linguistic enrichments and CoNLL-U
Given the canonical HTML format described above, it is
straightforward to extract the relevant text. In order to en-
able various types of downstream uses of the dataset, we
further preprocess the raw text using the UDPipe toolkit
(Straka et al., 2016), representing each review as a CoNLL-
U ﬁle, following the format deﬁned in Universal Dependen-
cies version 2.2 In this step we perform sentence segmenta-
tion, tokenization, lemmatization, morphological analysis,
part-of-speech tagging and dependency parsing, following
the Universal Dependencies scheme (Nivre et al., 2016).
However, the preprocessing set-up is slightly complicated
by the fact that the Norwegian language has two ofﬁcial
written standards – Bokmål (the main variety) and Nynorsk
– both of which are represented in the review corpus. Below
we ﬁrst describe how language identiﬁcation is performed,
and then go on to give more details about UDPipe and the
resulting CoNLL-U data.

Identifying language varieties The two ofﬁcial varieties
are closely related and they are mostly distinguished by
minor lexical differences. Still, the differences are strong
enough that different preprocessing pipelines must be used
for the different standards, hence it is important to identify
the standard within a particular document. Therefore, we
have used langid.py (Lui and Baldwin, 2012) to iden-
tify the standard for each review.3 We performed an evalu-

2http://universaldependencies.org/format.

html

3langid.py can actually identify three different variants:
no, nn and nb, for Norwegian (mixed), Nynorsk and Bokmål,

Documents
Sentences
Tokens
Types, full-forms
Types, lemmas

#

35,194
837,914
14,819,248
521,563
446,532

Table 2: Basic corpus counts.

ation of langid.py on 1599 reviews of which 1487 were
written in Bokmål and 112 in Nynorsk (based on selecting
reviews from authors known to write in a given variety). On
this sample langid.py achieved 100% accuracy. While
the main variety, i.e. Bokmål, dominates the distribution
in the corpus with 34,661 documents, we also identiﬁed
533 documents in Nynorsk (mainly from the sources Fæ-
drelandsvennen, Bergens Tidende and P3.no).

UDpipe conﬁguration We applied UDPipe (Straka et al.,
2016) v.1.2 with its pre-trained models for Norwegian Bok-
mål and Nynorsk. This version of the UDPipe software and
the pre-trained models were developed for the CoNLL 2017
shared task (Zeman et al., 2017), which was devoted to
parsing from raw text to Universal Dependencies for more
than 40 different languages. We use of the models trained
for participation in the shared task (Straka and Straková,
2017), not the models provided as baseline models for the
participants. The Norwegian models were trained on the
UD 2.0 versions of the Norwegian UD treebanks (Øvrelid
and Hohle, 2016; Velldal et al., 2017) in conjunction with
the aforementioned shared task, and the subsequent choice
of model for use (Bokmål vs Nynorsk) was determined by
the language identiﬁed for each particular review.
UDPipe obtained competitive results for Norwegian in the
shared task, with rankings ranging between ﬁrst place
(lemmatization; both variants) and ninth place (Bokmål de-
pendency parsing LAS) out of 33 participating teams. In
terms of performance for the different sub-tasks, UDPipe
reported F1 scores – for Bokmål / Nynorsk respectively –
on sentence segmentation of 96.38 / 92.08, tokenization of
99.79 / 99.93, lemmatization of 96.66 / 96.48, morpho-
logical analysis of 95.56 / 95.25, part-of-speech tagging
of 96.83 / 96.54, and Labeled Accuracy Scores for depen-
dency parsing of 83.89 / 82.74.

CoNLL-U ﬁles When extracting the text from the canon-
ical HTML to pass it to UDPipe, we strip away all mark-up
and discard all content marked for removal as described in
Section 2.2. Double newlines were inserted between para-
graphs and excess whitespace trimmed away. Importantly,
however, the text structure is retained in CoNLL-U by tak-
ing advantage of the support for comments to mark para-
graphs and sentences. In addition to the global document
ID number, each paragraph and sentence is also assigned a
running ID within the document, using the following form:

respectively. While the precise details of how the classiﬁer was
trained are not clear, it appears to us after some experimentation
that the classﬁcation of Bokmål is more accurate when specyﬁng
no rather than nb and hence is what we use here (together with
nn). We still use the language codes nb and nn when adding in-
formation about the detected standards to the metadata in NoReC.

Category

# Reviews

Screen
Music
Literature
Products
Games
Restaurants
Stage
Sports
Misc

Total

13,085
12,410
3,530
3,120
1,765
534
530
118
102

35,194

Table 3: Number of reviews across categories.

• Paragraphs: <review-id>-<paragraph-id>,
e.g. 000001-03 for paragraph 3 in document 1.
• Sentences: <review-id>-<paragraph-id>-
<sentence-id>, e.g. 000001-03-02 for sen-
tence 2 in paragraph 3 in document 1.

After completing the UDPipe pre-processing, the corpus
comprises a total of 837,914 sentences and 14,819,248 to-
kens; see Table 2 for an overview of some core corpus
counts. A script for executing the entire pipeline from text
extraction through UDPipe parsing will be made available
from the NoReC git repository.

2.4. Metadata and thematic categories
For all the identiﬁed reviews, we also extract various kinds
of relevant metadata, made available in a JSON representa-
tion with normalized attribute–value names across reviews.
Metadata in NoReC include information like the URL of
the originally published document, numerical rating, pub-
lishing date, author list, domain or thematic category, orig-
inal ID in the source, and more. Beyond this we also
add information about the identiﬁed language variety (Bok-
mål/Nynorsk), the assigned data split (test/dev/train, as fur-
ther described in Section 2.5.), the assigned document ID,
and ﬁnally a normalized thematic category.

Thematic categories The ‘category’ attribute warrants
some elaboration. The use of thematic categories and/or
tags varies a lot between the different sources, ranging from
highly granular categories to umbrella categories encom-
passing many different domains. Based on the original in-
ventory of categories, each review in NoReC is mapped to
one out of nine normalized thematic categories with En-
glish names. The distribution over categories is shown in
Table 3, sorted by frequency.
For some sources, this normalization is a matter of simple
one-to-one mapping, while for others it is more complex,
involving heuristics based on the presence of certain tags
and keywords in the title. The granularity in the ﬁnal set of
categories is limited by the granularity in the sources. How-
ever, the original (Norwegian) source categories are also
preserved in the metadata (‘source-category’).
As seen from Table 3, the two categories that are by far the
largest are ‘screen’ and ‘music’. While the former covers
reviews about movies and TV-series, the latter covers both
musical recordings and performances. The related category
‘stage’ covers theater, opera, ballet, musical and other stage

s
w
e
i
v
e
R

12,000

10,000

8,000

6,000

4,000

2,000

0

1

2

5

6

3

4

Rating

Figure 1: Number of reviews across ratings.

performances besides music. The perhaps most diverse
category is ‘products’, which comprises product reviews
across a number of sub-categories, ranging from cars and
boats to mobile phones and home electronics, in addition to
travel and more. The remaining categories of ‘literature’,
‘games’, ‘restaurants’, and ‘sports’ are self-explanatory,
while the ‘misc’ category was included to cover topics that
were infrequent or that could not easily be mapped to any
of the other categories by simple heuristics.

Ratings From the perspective of SA, the most immedi-
ately relevant piece of metadata is obviously the rating. As
discussed previously, all the reviews were originally pub-
lished with an integer-valued rating between 1 and 6, visu-
ally indicated using the face of a die. Figure 1 shows the
distribution of reviews relative to rating scores. We see that
rating values of 4 and 5 are the most common, while rather
few reviews were given the lowest possible rating of 1. For
the ﬁnal version of the paper we plan to report more de-
tails on the rating distributions, for example investigating
whether there are differences in the use of the rating scale
between different sources, domains, or authors. For future
work it would also be interesting to try to uncover whether
any rating biases exists, for example related to gender.

2.5. Formats and availability
Distributed under a CC BY-NC 4.0 license,4 NoReC is
available for download from the following git repository:
https://github.com/ltgoslo/norec

Formats NoReC distributes two formats. The ﬁrst is the
CoNLL-U format as described in Section 2.3., containing
sentence segmented and tokenized text annotated with PoS
tags and dependency graphs. This is considered the primary
format. Secondly, we also distribute the canonical HTML
representation of the ‘raw’ review documents as described
in Section 2.2.. For each format, each review is stored as
a separate ﬁle, with the ﬁlename given by the review ID.
To facilitate a low barrier of use for different types of end-
users, we also include scripts for converting from CoNLL-
U to running tokenized text (using either full-forms or lem-
mas) and from HTML to raw text without pre-processing.

4https://creativecommons.org/licenses/

by-nc/4.0/

The metadata for each review is provided as a JSON ob-
ject, all listed in a single ﬁle and indexed on the document
IDs. The NoReC git repository will include a Python mod-
ule with basic functionality for reading the CoNLL-U and
JSON representations, as to make experimentation with the
corpus as accessible and convenient as possible.

Train/dev/test splits To facilitate replicability of experi-
ments, NoReC comes with pre-deﬁned splits for training,
development and testing. These were deﬁned by ﬁrst sort-
ing all reviews for each category by publishing date and
then reserving the ﬁrst 80% for training, the subsequent
10% for development, and the ﬁnal 10% for held-out test-
ing. This splitting strategy makes the test setting as realistic
as possible and avoids having multiple reviews for the same
product (from different sources) across splits.

3. Summary and outlook
The current paper describes the creation of the Norwegian
Review Corpus; NoReC. The ﬁnal dataset comprises more
than 35,000 full-text reviews (≈ 15 million tokens) from
a wide range of different domains, collected from several
major Norwegian news sources. Each review is rated with a
numerical score on a scale of 1–6, and can be used for train-
ing and evaluating models for document-level sentiment
analysis. While the primary distribution format of the cor-
pus is CoNLL-U – based on only the extracted text and ap-
plying UDPipe for a full pre-processing pipeline from sen-
tence segmentation to dependency parsing – the release also
includes HTML representations of the full reviews with all
content preserved. Each review is in addition associated
with a rich set of metadata, including thematic category,
numerical rating, and more. We also provide pre-deﬁned
splits for training, development and testing.
For future work, the SANT project will seek to build on
NoReC to (i) experiment with both polarity classiﬁcation
and rating inference on the document-level using neural ar-
chitectures, (ii) extract SA lexicons encoding the polarity
of individual words, and ﬁnally (iii) also move beyond the
document-level and manually add more ﬁne-grained and
aspect-based SA annotations for a sub-set of the corpus.
Across all these activities, the various thematic categories
will be useful for assessing cross-domain effects (e.g., how
well does an SA classiﬁer trained on movie reviews per-
form for home electronics?) and potentially even for train-
ing domain-speciﬁc models. It will of course also be im-
portant to asses how well SA resources developed on the
basis of the reviews generalize to non-review texts.
Resources for sentiment analysis have so far been unavail-
able for Norwegian. As such, NoReC represents a highly
sought-after addition to Norwegian language technology,
valuable to both industry and the research community alike.

4. Acknowledgments
This work was carried out as part of the SANT research
project (Sentiment Analysis for Norwegian Text), funded
by an IKTPLUSS grant from the Research Council of Nor-
way (project no. 270908). SANT represents a newly initi-
ated collaboration between the Department of Informatics
at the University of Oslo, and three of Norway’s largest

media groups; the Norwegian Broadcasting Corporation,
Schibsted Media Group and Aller Media.

Conference on Knowledge Discovery and Data Mining
(KDD’2010), page 783 – 792.

Zeman, D., Popel, M., Straka, M., Hajic, J., Nivre, J.,
Ginter, F., Luotolahti, J., Pyysalo, S., Petrov, S., Pot-
thast, M., Tyers, F., Badmaeva, E., Gokirmak, M.,
Nedoluzhko, A., Cinkova, S., Hajic jr., J., Hlavacova,
J., Kettnerová, V., Uresova, Z., Kanerva, J., Ojala, S.,
Missilä, A., Manning, C. D., Schuster, S., Reddy, S.,
Taji, D., Habash, N., Leung, H., de Marneffe, M.-C.,
Sanguinetti, M., Simi, M., Kanayama, H., dePaiva, V.,
Droganova, K., Martínez Alonso, H., Çöltekin, c., Su-
lubacak, U., Uszkoreit, H., Macketanz, V., Burchardt,
A., Harris, K., Marheinecke, K., Rehm, G., Kayadelen,
T., Attia, M., Elkahky, A., Yu, Z., Pitler, E., Lertpradit,
S., Mandl, M., Kirchner, J., Alcalde, H. F., Strnadová,
J., Banerjee, E., Manurung, R., Stella, A., Shimada, A.,
Kwak, S., Mendonca, G., Lando, T., Nitisaroj, R., and Li,
J. (2017). Conll 2017 shared task: Multilingual parsing
from raw text to universal dependencies. In Proceedings
of the CoNLL 2017 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies, page 1 – 19,
Vancouver, Canada.

5. Bibliographical references

Blitzer, J., Dredze, M., and Pereira, F. (2007). Biographies,
bollywood, boom-boxes and blenders: Domain adapta-
tion for sentiment classiﬁcation. In Proceedings of the
45th Meeting of the Association for Computational Lin-
guistics, page 187 – 205, Prague, Czech Republic.

Lui, M. and Baldwin, T. (2012).

langid.py: An off-the-
shelf language identiﬁcation tool. In Proceedings of the
50th Meeting of the Association for Computational Lin-
guistics System Demonstrations, page 25 – 30, Jeju, Re-
public of Korea.

Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y.,
and Potts, C. (2011). Learning word vectors for senti-
ment analysis. In Proceedings of the 49th Meeting of the
Association for Computational Linguistics, page 142 –
150, Portland, OR, USA.

Nivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y.,
Hajiˇc, J., Manning, C. D., McDonald, R., Petrov, S.,
Pyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D.
(2016). Universal dependencies v1: A multilingual tree-
In Proceedings of the 10th Interna-
bank collection.
tional Conference on Language Resources and Evalua-
tion, Portorož, Slovenia.
Øvrelid, L. and Hohle, P.

(2016). Norwegian Univer-
sal Dependencies. In Proceedings of the 10th Interna-
tional Conference on Language Resources and Evalua-
tion, page 1579 – 1585, Portorož, Slovenia.

Pang, B. and Lee, L. (2005). Seeing stars: Exploiting class
relationships for sentiment categorization with respect to
rating scales. In Proceedings of the 43rd Meeting of the
Association for Computational Linguistics, page 115 –
124, Ann Arbor, MI, USA.

Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.,
Ng, A., and Potts, C. (2013). Recursive deep models for
semantic compositionality over a sentiment treebank. In
Proceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, page 1631 – 1642,
Seattle, WA, USA.

Straka, M. and Straková, J. (2017). Tokenizing, POS tag-
ging, lemmatizing and parsing UD 2.0 with UDPipe. In
Proceedings of the CoNLL 2017 Shared Task: Multilin-
gual Parsing from Raw Text to Universal Dependencies,
pages 88–99, Vancouver, Canada.

Straka, M., Hajiˇc, J., and Straková, J. (2016). UDPipe:
trainable pipeline for processing CoNLL-U ﬁles per-
forming tokenization, morphological analysis, pos tag-
ging and parsing. In Proceedings of the Tenth Interna-
tional Conference on Language Resources and Evalua-
tion, Portorož, Slovenia.

Velldal, E., Øvrelid, L., and Hohle, P. (2017). Joint UD
parsing of Norwegian Bokmål and Nynorsk. In Proceed-
ings of the 11th Nordic Conference of Computational
Linguistics, page 1 – 10, Gothenburg, Sweden.

Wang, H., Lu, Y., and Zhai, C. (2010). Latent aspect rat-
ing analysis on review text data: A rating regression
In Proceedings of the 16th ACM SIGKDD
approach.

NoReC: The Norwegian Review Corpus

Erik Velldal, Lilja Øvrelid, Eivind Alexander Bergem, Cathrine Stadsnes,
Samia Touileb, Fredrik Jørgensen†
Language Technology Group, Department of Informatics, University of Oslo
{erikve, liljao, eivinabe, cathsta, samiat}@iﬁ.uio.no
†Schibsted Media Group
fredrik.jorgensen@schibsted.com

7
1
0
2
 
t
c
O
 
5
1
 
 
]
L
C
.
s
c
[
 
 
1
v
0
7
3
5
0
.
0
1
7
1
:
v
i
X
r
a

Abstract
This paper presents the Norwegian Review Corpus (NoReC), created for training and evaluating models for document-level sentiment
analysis. The full-text reviews have been collected from major Norwegian news sources and cover a range of different domains, including
literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review
is labeled with a manually assigned score of 1–6, as provided by the rating of the original author. This ﬁrst release of the corpus
comprises more than 35,000 reviews. It is distributed using the CoNLL-U format, pre-processed using UDPipe, along with a rich set of
metadata. The work reported in this paper forms part of the SANT initiative (Sentiment Analysis for Norwegian Text), a project seeking
to provide resources and tools for sentiment analysis and opinion mining for Norwegian. As resources for sentiment analysis have
so far been unavailable for Norwegian, NoReC represents a highly valuable and sought-after addition to Norwegian language technology.

Keywords: Sentiment Analysis, Opinion Mining, Corpus, Norwegian, Reviews

1.

Introduction

Norwegian is in many ways an under-resourced language,
with training and evaluation data still lacking for many core
NLP tasks. The current work aims to alleviate the situation
for the particular task of sentiment analysis. The SANT
project – Sentiment Analysis for Norwegian Text – seeks
to create, and make publicly available, resources and tools
for sentiment analysis for Norwegian. The SANT effort
described in the current paper marks the release of the Nor-
wegian Review Corpus1 (NoReC). The dataset comprises
more than 35,000 full-text reviews from a range of differ-
ent domains, collected from several of the major Norwe-
gian news sources. Each review is rated with a numerical
score on a scale of 1–6, and can be used for training and
evaluating models for document-level sentiment analysis.

1.1. Rating by dice
A particularity of review journalism in Norway,
is the
wholesale adoption of dice rolls (‘terningkast’) as a stan-
dard rating scale: The item under review is rated on a scale
of 1–6, commonly visualized by the face of a die with a
corresponding number of ‘dots’ or pips. The practice is
thought to have been introduced already in 1952 when re-
viewing movies in the newspaper Verdens Gang (VG). By
now it is has found widespread use in all sorts of arts and
consumer journalism and is used when reviewing every-
thing from books, theater and music, to home electronics,
restaurants, and children’s clothing.
The rating practice described above has several beneﬁts for
the goal of document-level SA: (i) It eliminates the need for
costly manual annotation since the numerical rating (i.e.,
the die roll) directly provides us with the labels needed for
training models for detecting the overall document polar-
ity. (ii) There is no need for manually deﬁned mappings to
align different rating schemes as the reviews all use a uni-

1https://github.com/ltgoslo/norec

form scale. (iii) The wide range of available news sources
using the same rating practice, including all the major na-
tional newspapers, facilitates the creation of a large-scale
dataset. (iv) Models trained on the dataset can be expected
to generalize well across domains given the balance of dif-
ferent topics covered in the corpus. For English, a sub-
stantial amount of SA research has been directed towards
predicting the sentiment of movie reviews, collected from
aggregator sites like IMDb.com and RottenTomatoes.com
(Pang and Lee, 2005; Socher et al., 2013) or other types of
consumer reviews (Blitzer et al., 2007; Wang et al., 2010;
Maas et al., 2011). The NoRec data set contains a variety
of different types of reviews from a range of domains.

1.2. Sources and partners

The SANT project represents a newly initiated collabora-
tion between the Language Technology Group (LTG) at the
Department of Informatics at the University of Oslo, and
three of Norway’s largest media groups; the Norwegian
Broadcasting Corporation (NRK – the state-owned public
broadcaster) and the privately held Schibsted Media Group
and Aller Media. This ﬁrst release of NoReC comprises
35,194 reviews extracted from eight different news sources
as contributed by the three media partners. In terms of pub-
lishing date the reviews mainly cover the time span 2003–
2017, although it also includes a handful of reviews dating
back as far as 1998. We brieﬂy present the sources provided
by the different partners below.

Schibsted Media Group The Schibsted group has con-
tributed content from their full portfolio of Norwegian news
sources: VG, Aftenposten, Fædrelandsvennen, Bergens
Tidende, and Stavanger Aftenblad. While the latter three
rank among Norway’s largest regional newspapers, Aften-
posten is the largest national newspaper in terms of circu-
lation and VG is the largest online news source with more
than 2.4 million readers across all platforms.

Source

# Reviews

VG
Dagbladet
Stavanger Aftenblad
P3.no
DinSide.no
Fædrelandsvennen
Bergens Tidene
Aftenposten

Total

11,888
5,305
5,146
5,017
2,944
2,296
1,675
923

35,194

Table 1: Number of reviews across sources.

Aller Media The Aller publishing company has con-
tributed content from two sources. The ﬁrst is the online
version of the newspaper Dagbladet – the second most vis-
ited online news source in Norway – publishing reviews
for music recordings and live performances, theater and
related stage performances, movies, literature, restaurants
and more. The second source, DinSide.no, is a website
specializing in product reviews, covering a wide range of
product types, from home electronics to cars and clothing.

NRK The Norwegian Broadcasting Corporation is a
state-owned media house, with a special mandate to be
a non-commercial, politically independent public broad-
caster. For the review corpus, NRK has contributed content
from the website P3.no which has an extensive back cata-
log of ‘die-rated’ reviews of movies, TV series, computer
games, and music (both recordings and live performances).
Figure 1 shows the number of reviews included in the ﬁnal
corpus, broken down across the various sources.

2. Corpus creation
The original document collections were provided from the
media sources in various JSON, HTML and XML formats,
and a substantial effort has gone into identifying relevant
documents and extracting text and associated metadata.
The extraction process can be summarized by the following
three steps: (i) Identify reviews, (ii) convert review con-
tent to an intermediate and canonical HTML format, (iii)
extract text and pass it through linguistic pre-processing,
producing representations in CoNLL-U format, and ﬁnally
(iv) extract relevant metadata to a JSON representation with
normalized attribute–value names. We brieﬂy comment on
each of these steps in turn below.

Identifying reviews

2.1.
Some of the initial document dumps also included other ar-
ticles beyond reviews, and in these cases reviews had to be
identiﬁed. While in some cases this could be done simply
by checking for an appropriate metadata ﬁeld indicating the
rating score, other cases required checking for links point-
ing to an image of a die (indicating the rating), or similar
heuristics.
For some of the sources, one and the same document may
contain multiple reviews, for example for product com-
parisons.
In these cases we had to identify and separate
out the different sub-reviews. Different publishing conven-
tions required targeting different types of cues in the docu-
ment structure, like headers, bold-faced content or die-face

images. This also involved extraction of titles and rating
scores for the different sub-reviews. The identiﬁed sub-
reviews become separate documents in the NoReC data set.
In total, 35,194 distinct reviews were extracted from the
data provided by the media partners.

2.2. Converting content to canonical HTML
The raw data dumps from the sources are mostly in HTML
format, but may also be e.g. JSON objects, and have differ-
ent conventions for document structuring and use of mark-
up.
In order to streamline the downstream text extrac-
tion, all documents were converted to a ‘canonical’ HTML
format where all textual content is located either inside a
In addition to containing the
header or a paragraph tag.
review text, the raw representations also contains images,
ads and other content not considered part of the running
text.
In order to identify and mark the non-relevant text
we used a combination of heuristics based on simple string
matching and properties like paragraph length and ratio of
content to markup. For example, care was taken to iden-
tify ‘you-might-also-be-interested-in’ type links that are in-
jected throughout the texts in an attempt to keep the reader
on the website and generate more clicks. Importantly, how-
ever, we chose not to remove content when converting to
our intermediate HTML format, instead introducing a new
tag – remove – in which we enclose content considered
non-relevant. This non-destructive approach preserves the
original content, as to not close the door on changes to the
subsequent task of text extraction later.

2.3. Linguistic enrichments and CoNLL-U
Given the canonical HTML format described above, it is
straightforward to extract the relevant text. In order to en-
able various types of downstream uses of the dataset, we
further preprocess the raw text using the UDPipe toolkit
(Straka et al., 2016), representing each review as a CoNLL-
U ﬁle, following the format deﬁned in Universal Dependen-
cies version 2.2 In this step we perform sentence segmenta-
tion, tokenization, lemmatization, morphological analysis,
part-of-speech tagging and dependency parsing, following
the Universal Dependencies scheme (Nivre et al., 2016).
However, the preprocessing set-up is slightly complicated
by the fact that the Norwegian language has two ofﬁcial
written standards – Bokmål (the main variety) and Nynorsk
– both of which are represented in the review corpus. Below
we ﬁrst describe how language identiﬁcation is performed,
and then go on to give more details about UDPipe and the
resulting CoNLL-U data.

Identifying language varieties The two ofﬁcial varieties
are closely related and they are mostly distinguished by
minor lexical differences. Still, the differences are strong
enough that different preprocessing pipelines must be used
for the different standards, hence it is important to identify
the standard within a particular document. Therefore, we
have used langid.py (Lui and Baldwin, 2012) to iden-
tify the standard for each review.3 We performed an evalu-

2http://universaldependencies.org/format.

html

3langid.py can actually identify three different variants:
no, nn and nb, for Norwegian (mixed), Nynorsk and Bokmål,

Documents
Sentences
Tokens
Types, full-forms
Types, lemmas

#

35,194
837,914
14,819,248
521,563
446,532

Table 2: Basic corpus counts.

ation of langid.py on 1599 reviews of which 1487 were
written in Bokmål and 112 in Nynorsk (based on selecting
reviews from authors known to write in a given variety). On
this sample langid.py achieved 100% accuracy. While
the main variety, i.e. Bokmål, dominates the distribution
in the corpus with 34,661 documents, we also identiﬁed
533 documents in Nynorsk (mainly from the sources Fæ-
drelandsvennen, Bergens Tidende and P3.no).

UDpipe conﬁguration We applied UDPipe (Straka et al.,
2016) v.1.2 with its pre-trained models for Norwegian Bok-
mål and Nynorsk. This version of the UDPipe software and
the pre-trained models were developed for the CoNLL 2017
shared task (Zeman et al., 2017), which was devoted to
parsing from raw text to Universal Dependencies for more
than 40 different languages. We use of the models trained
for participation in the shared task (Straka and Straková,
2017), not the models provided as baseline models for the
participants. The Norwegian models were trained on the
UD 2.0 versions of the Norwegian UD treebanks (Øvrelid
and Hohle, 2016; Velldal et al., 2017) in conjunction with
the aforementioned shared task, and the subsequent choice
of model for use (Bokmål vs Nynorsk) was determined by
the language identiﬁed for each particular review.
UDPipe obtained competitive results for Norwegian in the
shared task, with rankings ranging between ﬁrst place
(lemmatization; both variants) and ninth place (Bokmål de-
pendency parsing LAS) out of 33 participating teams. In
terms of performance for the different sub-tasks, UDPipe
reported F1 scores – for Bokmål / Nynorsk respectively –
on sentence segmentation of 96.38 / 92.08, tokenization of
99.79 / 99.93, lemmatization of 96.66 / 96.48, morpho-
logical analysis of 95.56 / 95.25, part-of-speech tagging
of 96.83 / 96.54, and Labeled Accuracy Scores for depen-
dency parsing of 83.89 / 82.74.

CoNLL-U ﬁles When extracting the text from the canon-
ical HTML to pass it to UDPipe, we strip away all mark-up
and discard all content marked for removal as described in
Section 2.2. Double newlines were inserted between para-
graphs and excess whitespace trimmed away. Importantly,
however, the text structure is retained in CoNLL-U by tak-
ing advantage of the support for comments to mark para-
graphs and sentences. In addition to the global document
ID number, each paragraph and sentence is also assigned a
running ID within the document, using the following form:

respectively. While the precise details of how the classiﬁer was
trained are not clear, it appears to us after some experimentation
that the classﬁcation of Bokmål is more accurate when specyﬁng
no rather than nb and hence is what we use here (together with
nn). We still use the language codes nb and nn when adding in-
formation about the detected standards to the metadata in NoReC.

Category

# Reviews

Screen
Music
Literature
Products
Games
Restaurants
Stage
Sports
Misc

Total

13,085
12,410
3,530
3,120
1,765
534
530
118
102

35,194

Table 3: Number of reviews across categories.

• Paragraphs: <review-id>-<paragraph-id>,
e.g. 000001-03 for paragraph 3 in document 1.
• Sentences: <review-id>-<paragraph-id>-
<sentence-id>, e.g. 000001-03-02 for sen-
tence 2 in paragraph 3 in document 1.

After completing the UDPipe pre-processing, the corpus
comprises a total of 837,914 sentences and 14,819,248 to-
kens; see Table 2 for an overview of some core corpus
counts. A script for executing the entire pipeline from text
extraction through UDPipe parsing will be made available
from the NoReC git repository.

2.4. Metadata and thematic categories
For all the identiﬁed reviews, we also extract various kinds
of relevant metadata, made available in a JSON representa-
tion with normalized attribute–value names across reviews.
Metadata in NoReC include information like the URL of
the originally published document, numerical rating, pub-
lishing date, author list, domain or thematic category, orig-
inal ID in the source, and more. Beyond this we also
add information about the identiﬁed language variety (Bok-
mål/Nynorsk), the assigned data split (test/dev/train, as fur-
ther described in Section 2.5.), the assigned document ID,
and ﬁnally a normalized thematic category.

Thematic categories The ‘category’ attribute warrants
some elaboration. The use of thematic categories and/or
tags varies a lot between the different sources, ranging from
highly granular categories to umbrella categories encom-
passing many different domains. Based on the original in-
ventory of categories, each review in NoReC is mapped to
one out of nine normalized thematic categories with En-
glish names. The distribution over categories is shown in
Table 3, sorted by frequency.
For some sources, this normalization is a matter of simple
one-to-one mapping, while for others it is more complex,
involving heuristics based on the presence of certain tags
and keywords in the title. The granularity in the ﬁnal set of
categories is limited by the granularity in the sources. How-
ever, the original (Norwegian) source categories are also
preserved in the metadata (‘source-category’).
As seen from Table 3, the two categories that are by far the
largest are ‘screen’ and ‘music’. While the former covers
reviews about movies and TV-series, the latter covers both
musical recordings and performances. The related category
‘stage’ covers theater, opera, ballet, musical and other stage

s
w
e
i
v
e
R

12,000

10,000

8,000

6,000

4,000

2,000

0

1

2

5

6

3

4

Rating

Figure 1: Number of reviews across ratings.

performances besides music. The perhaps most diverse
category is ‘products’, which comprises product reviews
across a number of sub-categories, ranging from cars and
boats to mobile phones and home electronics, in addition to
travel and more. The remaining categories of ‘literature’,
‘games’, ‘restaurants’, and ‘sports’ are self-explanatory,
while the ‘misc’ category was included to cover topics that
were infrequent or that could not easily be mapped to any
of the other categories by simple heuristics.

Ratings From the perspective of SA, the most immedi-
ately relevant piece of metadata is obviously the rating. As
discussed previously, all the reviews were originally pub-
lished with an integer-valued rating between 1 and 6, visu-
ally indicated using the face of a die. Figure 1 shows the
distribution of reviews relative to rating scores. We see that
rating values of 4 and 5 are the most common, while rather
few reviews were given the lowest possible rating of 1. For
the ﬁnal version of the paper we plan to report more de-
tails on the rating distributions, for example investigating
whether there are differences in the use of the rating scale
between different sources, domains, or authors. For future
work it would also be interesting to try to uncover whether
any rating biases exists, for example related to gender.

2.5. Formats and availability
Distributed under a CC BY-NC 4.0 license,4 NoReC is
available for download from the following git repository:
https://github.com/ltgoslo/norec

Formats NoReC distributes two formats. The ﬁrst is the
CoNLL-U format as described in Section 2.3., containing
sentence segmented and tokenized text annotated with PoS
tags and dependency graphs. This is considered the primary
format. Secondly, we also distribute the canonical HTML
representation of the ‘raw’ review documents as described
in Section 2.2.. For each format, each review is stored as
a separate ﬁle, with the ﬁlename given by the review ID.
To facilitate a low barrier of use for different types of end-
users, we also include scripts for converting from CoNLL-
U to running tokenized text (using either full-forms or lem-
mas) and from HTML to raw text without pre-processing.

4https://creativecommons.org/licenses/

by-nc/4.0/

The metadata for each review is provided as a JSON ob-
ject, all listed in a single ﬁle and indexed on the document
IDs. The NoReC git repository will include a Python mod-
ule with basic functionality for reading the CoNLL-U and
JSON representations, as to make experimentation with the
corpus as accessible and convenient as possible.

Train/dev/test splits To facilitate replicability of experi-
ments, NoReC comes with pre-deﬁned splits for training,
development and testing. These were deﬁned by ﬁrst sort-
ing all reviews for each category by publishing date and
then reserving the ﬁrst 80% for training, the subsequent
10% for development, and the ﬁnal 10% for held-out test-
ing. This splitting strategy makes the test setting as realistic
as possible and avoids having multiple reviews for the same
product (from different sources) across splits.

3. Summary and outlook
The current paper describes the creation of the Norwegian
Review Corpus; NoReC. The ﬁnal dataset comprises more
than 35,000 full-text reviews (≈ 15 million tokens) from
a wide range of different domains, collected from several
major Norwegian news sources. Each review is rated with a
numerical score on a scale of 1–6, and can be used for train-
ing and evaluating models for document-level sentiment
analysis. While the primary distribution format of the cor-
pus is CoNLL-U – based on only the extracted text and ap-
plying UDPipe for a full pre-processing pipeline from sen-
tence segmentation to dependency parsing – the release also
includes HTML representations of the full reviews with all
content preserved. Each review is in addition associated
with a rich set of metadata, including thematic category,
numerical rating, and more. We also provide pre-deﬁned
splits for training, development and testing.
For future work, the SANT project will seek to build on
NoReC to (i) experiment with both polarity classiﬁcation
and rating inference on the document-level using neural ar-
chitectures, (ii) extract SA lexicons encoding the polarity
of individual words, and ﬁnally (iii) also move beyond the
document-level and manually add more ﬁne-grained and
aspect-based SA annotations for a sub-set of the corpus.
Across all these activities, the various thematic categories
will be useful for assessing cross-domain effects (e.g., how
well does an SA classiﬁer trained on movie reviews per-
form for home electronics?) and potentially even for train-
ing domain-speciﬁc models. It will of course also be im-
portant to asses how well SA resources developed on the
basis of the reviews generalize to non-review texts.
Resources for sentiment analysis have so far been unavail-
able for Norwegian. As such, NoReC represents a highly
sought-after addition to Norwegian language technology,
valuable to both industry and the research community alike.

4. Acknowledgments
This work was carried out as part of the SANT research
project (Sentiment Analysis for Norwegian Text), funded
by an IKTPLUSS grant from the Research Council of Nor-
way (project no. 270908). SANT represents a newly initi-
ated collaboration between the Department of Informatics
at the University of Oslo, and three of Norway’s largest

media groups; the Norwegian Broadcasting Corporation,
Schibsted Media Group and Aller Media.

Conference on Knowledge Discovery and Data Mining
(KDD’2010), page 783 – 792.

Zeman, D., Popel, M., Straka, M., Hajic, J., Nivre, J.,
Ginter, F., Luotolahti, J., Pyysalo, S., Petrov, S., Pot-
thast, M., Tyers, F., Badmaeva, E., Gokirmak, M.,
Nedoluzhko, A., Cinkova, S., Hajic jr., J., Hlavacova,
J., Kettnerová, V., Uresova, Z., Kanerva, J., Ojala, S.,
Missilä, A., Manning, C. D., Schuster, S., Reddy, S.,
Taji, D., Habash, N., Leung, H., de Marneffe, M.-C.,
Sanguinetti, M., Simi, M., Kanayama, H., dePaiva, V.,
Droganova, K., Martínez Alonso, H., Çöltekin, c., Su-
lubacak, U., Uszkoreit, H., Macketanz, V., Burchardt,
A., Harris, K., Marheinecke, K., Rehm, G., Kayadelen,
T., Attia, M., Elkahky, A., Yu, Z., Pitler, E., Lertpradit,
S., Mandl, M., Kirchner, J., Alcalde, H. F., Strnadová,
J., Banerjee, E., Manurung, R., Stella, A., Shimada, A.,
Kwak, S., Mendonca, G., Lando, T., Nitisaroj, R., and Li,
J. (2017). Conll 2017 shared task: Multilingual parsing
from raw text to universal dependencies. In Proceedings
of the CoNLL 2017 Shared Task: Multilingual Parsing
from Raw Text to Universal Dependencies, page 1 – 19,
Vancouver, Canada.

5. Bibliographical references

Blitzer, J., Dredze, M., and Pereira, F. (2007). Biographies,
bollywood, boom-boxes and blenders: Domain adapta-
tion for sentiment classiﬁcation. In Proceedings of the
45th Meeting of the Association for Computational Lin-
guistics, page 187 – 205, Prague, Czech Republic.

Lui, M. and Baldwin, T. (2012).

langid.py: An off-the-
shelf language identiﬁcation tool. In Proceedings of the
50th Meeting of the Association for Computational Lin-
guistics System Demonstrations, page 25 – 30, Jeju, Re-
public of Korea.

Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y.,
and Potts, C. (2011). Learning word vectors for senti-
ment analysis. In Proceedings of the 49th Meeting of the
Association for Computational Linguistics, page 142 –
150, Portland, OR, USA.

Nivre, J., de Marneffe, M.-C., Ginter, F., Goldberg, Y.,
Hajiˇc, J., Manning, C. D., McDonald, R., Petrov, S.,
Pyysalo, S., Silveira, N., Tsarfaty, R., and Zeman, D.
(2016). Universal dependencies v1: A multilingual tree-
In Proceedings of the 10th Interna-
bank collection.
tional Conference on Language Resources and Evalua-
tion, Portorož, Slovenia.
Øvrelid, L. and Hohle, P.

(2016). Norwegian Univer-
sal Dependencies. In Proceedings of the 10th Interna-
tional Conference on Language Resources and Evalua-
tion, page 1579 – 1585, Portorož, Slovenia.

Pang, B. and Lee, L. (2005). Seeing stars: Exploiting class
relationships for sentiment categorization with respect to
rating scales. In Proceedings of the 43rd Meeting of the
Association for Computational Linguistics, page 115 –
124, Ann Arbor, MI, USA.

Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.,
Ng, A., and Potts, C. (2013). Recursive deep models for
semantic compositionality over a sentiment treebank. In
Proceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, page 1631 – 1642,
Seattle, WA, USA.

Straka, M. and Straková, J. (2017). Tokenizing, POS tag-
ging, lemmatizing and parsing UD 2.0 with UDPipe. In
Proceedings of the CoNLL 2017 Shared Task: Multilin-
gual Parsing from Raw Text to Universal Dependencies,
pages 88–99, Vancouver, Canada.

Straka, M., Hajiˇc, J., and Straková, J. (2016). UDPipe:
trainable pipeline for processing CoNLL-U ﬁles per-
forming tokenization, morphological analysis, pos tag-
ging and parsing. In Proceedings of the Tenth Interna-
tional Conference on Language Resources and Evalua-
tion, Portorož, Slovenia.

Velldal, E., Øvrelid, L., and Hohle, P. (2017). Joint UD
parsing of Norwegian Bokmål and Nynorsk. In Proceed-
ings of the 11th Nordic Conference of Computational
Linguistics, page 1 – 10, Gothenburg, Sweden.

Wang, H., Lu, Y., and Zhai, C. (2010). Latent aspect rat-
ing analysis on review text data: A rating regression
In Proceedings of the 16th ACM SIGKDD
approach.

