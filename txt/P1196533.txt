8
1
0
2
 
v
o
N
 
0
1
 
 
]
L
M

.
t
a
t
s
[
 
 
2
v
2
3
4
6
0
.
1
0
8
1
:
v
i
X
r
a

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Robust Kronecker Component Analysis

Mehdi Bahri, Student Member, IEEE, Yannis Panagakis, and Stefanos Zafeiriou, Member, IEEE

Abstract—Dictionary learning and component analysis models are fundamental for learning compact representations that are relevant
to a given task (feature extraction, dimensionality reduction, denoising, etc.). The model complexity is encoded by means of speciﬁc
structure, such as sparsity, low-rankness, or nonnegativity. Unfortunately, approaches like K-SVD - that learn dictionaries for sparse
coding via Singular Value Decomposition (SVD) - are hard to scale to high-volume and high-dimensional visual data, and fragile in the
presence of outliers. Conversely, robust component analysis methods such as the Robust Principal Component Analysis (RPCA) are
able to recover low-complexity (e.g., low-rank) representations from data corrupted with noise of unknown magnitude and support, but
do not provide a dictionary that respects the structure of the data (e.g., images), and also involve expensive computations. In this paper,
we propose a novel Kronecker-decomposable component analysis model, coined as Robust Kronecker Component Analysis (RKCA),
that combines ideas from sparse dictionary learning and robust component analysis. RKCA has several appealing properties, including
robustness to gross corruption; it can be used for low-rank modeling, and leverages separability to solve signiﬁcantly smaller problems.
We design an efﬁcient learning algorithm by drawing links with a restricted form of tensor factorization, and analyze its optimality and
low-rankness properties. The effectiveness of the proposed approach is demonstrated on real-world applications, namely background
subtraction and image denoising and completion, by performing a thorough comparison with the current state of the art.

Index Terms—Component Analysis, Dictionary Learning, Separable Dictionaries, Low-rank, Sparsity, Global Optimality.

(cid:70)

1 INTRODUCTION

C OMPONENT analysis models and representation learn-

ing methods are ﬂexible data-driven alternatives to
analytical dictionaries (e.g., Fourier analysis, or wavelets)
for signal and data representation. The underlying principle
is to solve an optimization problem that encodes the desired
properties of the representations and of the bases, and de-
scribes what task(s) the representation should help solving.
Starting from Principal Component Analysis [1], [2], a rich
set of algorithms has been developed for feature extrac-
tion, dimensionality reduction, clustering, classiﬁcation, or
denoising - to name but a few. The importance of learned
components and representations cannot be overstated, and
neither can their effectiveness in dramatically improving
machine perception. Prominent examples are Convolutional
Neural Networks [3], [4], which through hierarchical feature
extraction build ad-hoc representations enabling state of the
art performance on a wide range of problems [5].

In this work, we propose the Robust Kronecker Com-
ponent Analysis (RKCA) family of algorithms for the un-
supervised learning of compact representations of tensor
data. Our method offers to bridge (multilinear) Robust
PCA [6], [7] and Sparse Dictionary Learning [8], [9] from
the perspective of a robust low-rank tensor factorization.
Although our method is generic enough to be presented for
arbitrary tensors, we focus on 3-dimensional tensors, and
especially those obtained by concatenation of 2-dimensional
tensor observations (i.e., matrices). We present a framework
for jointly learning a (Kronecker) separable dictionary and
sparse representations in the presence of outliers, such that

• The authors are with the Department of Computing, Imperial College

London, London, UK, SW7 2RH.
E-mail: mehdi.bahri15@imperial.ac.uk

• Y. Panagakis is also with Middlesex University London, UK.
•
S. Zafeiriou is also with the University of Oulu, Finland.

Manuscript received April 19, 2005; revised August 26, 2015.

the learned dictionary is low-rank, and the outliers are sepa-
rated from the data. The double perspective adopted allows
us to draw on recent work from the tensor factorization
literature to provide some theoretical optimality guarantees,
discussed in Section 4.

1.1 Robust PCA and sparse dictionary learning
Assuming a set of N data samples x1, . . . , xn ∈ Rm rep-
resented as the columns of a matrix X, structured matrix
factorizations seek to decompose X into meaningful com-
ponents of a given structure, by solving a regularization
problem of the form:

min
Z

l(X, Z) + λ g(Z),

(1)

where Z is an approximation of the data with respect to a
loss l(·), g(·) is a possibly non-smooth regularizer that en-
courages the desired structure, and λ ≥ 0 is a regularization
parameter balancing the two terms. Popular instances of (1)
include Principal Component Analysis (PCA) [1], [2] and its
variants, e.g., Sparse PCA [10], Robust PCA (RPCA) [11], as
well as sparse dictionary learning [8], [9], [12].

Concretely, when Z is taken to be factorized as Z = DR
we obtain a range of different models depending on the
choice of the regularization and of the properties of D. For
instance, assuming Z = DR is a low-rank approximation of
X and λ = 0, (1) yields PCA, while by imposing sparsity on
D, Sparse PCA [10] is obtained. To handle data corrupted
by sparse noise of large magnitude, RPCA [11] assumes that
the observation matrix, X, is the sum of a low-rank matrix
A and of a sparse matrix E that collects the gross errors, or
outliers. This model is actually a special instance of (1) when
Z = A+E, g(Z) = ||A||∗ +λ||E||1, and l(·) is the Frobenius
norm. Here, || · ||∗ denotes the low-rank promoting nuclear
norm and || · ||1 denotes the (cid:96)1 norm that enforces sparsity.
Matrix RPCA has been extended to tensors in multiple

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

ways, relying on varying deﬁnitions of the tensor rank. We
refer to [6], [7] for an overview of CP-based and Tucker-
based tensor RPCA models, and to Section 7 for speciﬁcs on
the models compared in this paper.

Assuming D is over-complete and requiring R =
[r1, . . . , rn] to be sparse, (1) leads to sparse dictionary learn-
ing by solving the non-convex optimization problem:

min
R,D

||X − DR||2

F + λ

||ri||0,

(2)

n
(cid:88)

i=1

where ||·||F is the Frobenius norm and ||·||0 is the (cid:96)0 pseudo-
norm, counting the number of non-zero elements. In K-SVD
and its variants, problem (2) is solved in an iterative manner
that alternates between sparse coding of the data samples
on the current dictionary, and a process of updating the
dictionary atoms to better ﬁt the data using the Singular
Value Decomposition (SVD) [13], [14].

1.2 Limits of the classical approach

While the Robust PCA and the sparse dictionary learning
paradigms have been immensely successful in practice, they
suffer from limitations that can limit their applicability.

The recovery guarantees of Robust PCA and of similar
compressed-sensing approaches have been derived under
strong incoherence assumptions, that may not be satisﬁed
even if the data strictly follows the low-rank assumption.
In fact, whenever the data lies on a low-dimensional linear
subspace but isn’t uniformly distributed, the extra structure
can induce signiﬁcant coherence. [15] studies this problem
in details, and in [15], [16], the authors show the adverse
effects of coherence can be mitigated in existing low-rank
modeling methods, such as [17], by choosing the represen-
tation dictionary to be low-rank.

Modern representation learning methods have to deal
with increasingly large amounts of
increasingly high-
dimensional data. Classical low-rank modeling and dictio-
nary learning models tend to be too expensive for that set-
ting: typically, algorithms of the K-SVD family suffer from
a high computational burden, preventing their applicability
to high-dimensional and large scale data.

To overcome the issue of scalability in dictionary learn-
ing, a separable structure on the dictionary can be en-
forced. For instance, Separable Dictionary Learning (SeDiL)
[18] considers a set of samples in matrix form, namely,
X = (Xi)i, admitting sparse representations on a pair of
bases A, B, of which the Kronecker product constructs the
dictionary. The corresponding objective is:

min
A,B,R

1
2

(cid:88)

i

||Xi−ARiBT||2

F+λg(R)+κr(A)+κr(B), (3)

where the regularizers g(·) and r(·) promote sparsity in
the representations, and low mutual-coherence of the dic-
tionary D = B ⊗ A, respectively. Here, D is constrained
to have orthogonal columns, i.e., the pair A, B shall lie
on the product manifold of two product of sphere man-
ifolds. A different approach is taken in [19]: a separable
2D dictionary is learned in a two-step strategy similar to
that of K-SVD. Each matrix observation Xi is represented
as ARiBT. In the ﬁrst step, the sparse representations
Ri are found by 2D Orthogonal Matching Pursuit (OMP)

2

[20]. In the second step, a CANDECOMP/PARAFAC (CP)
[21], [22], [23] decomposition is performed on a tensor of
residuals via Regularized Alternating Least Squares to solve
1. However, those models
minA,B,R ||X − R ×1 A ×2 B||F
learn over-complete dictionaries on a multitude of small
patches extracted from the images. For large images or large
number of images, the number of patches can easily become
prohibitively large, undermining the scalability beneﬁts of
learning separable dictionaries. Moreover, none of these
models is robust to gross corruption in the dataset.

Beyond scalability, separable dictionaries have been
shown to be theoretically appealing when dealing with
tensor-valued observations. According to [24], the neces-
sary number of samples for accurate (up to a given error)
reconstruction of a Kronecker-structured dictionary within
a local neighborhood scales with the sum of the product
of the dimensions of the constituting dictionaries when
learning on tensor data (see [25] for 2-dimensional data,
and [26], [27] for N -order tensor data), compared to the
product for vectorized observations. This suggests better
performance is achievable compared to classical methods
on tensor observations.

1.3 Outline and contributions

Here, we propose novel methods for separable dictionary
learning based on robust tensor factorisations that learn si-
multaneously the dictionary and the sparse representations.
We do not seek overcompleteness, but rather promote low-
rankness in a pair of dictionaries, and sparsity in the codes
to learn a low-rank representation of the input tensor. In
this regard, our methods combine ideas from both Sparse
Dictionary Learning and Robust PCA, as well as tensor
factorizations. Our solvers are based on the Alternating
Direction of Multipliers Method (ADMM) [28].

A preliminary version of this work has been presented

in [29]. This paper offers the following novelties:

• We generalize the results of [29] and propose new
regularizers that yield stronger optimality properties,
we call the resulting framework Robust Kronecker
Component Analysis (RKCA).

• Unlike previous models that rely solely on regular-
ization to impose low-rankness, our work presents
surrogates of speciﬁc deﬁnitions of the tensor rank
that impose low-rankness through both regularisa-
tion and structure. Speciﬁcally, our method can be
seen as modeling a low-rank tensor by tensor sparse
coding with additional regularization on the factors.
• We show that RKCA with these well-chosen regu-
larizers can be reformulated in the framework of
[30], [31] allowing us to provide global optimality
guarantees. It is worth mentioning that our proof
applies to any Tucker factorization problem.

• We demonstrate that RKCA can perform tensor com-

pletion in the presence of gross corruption.

• We derive a Linearized ADMM (LADMM) algorithm

for RKCA to improve scalability.

• The experimental evaluation has been enriched to
include the Low-Rank Representation method [17],

1. cf. Deﬁnition 2.4 for the product ×n

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

Fig. 1: Mode-1 ﬁbers (tubes) and mode-3 slices of a 3-way
tensor. Adapted from [33].

as well as comparisons with the recent Deep Image
Prior [32].
Finally, we offer new perspectives on the low-rank
promoting properties of RKCA.

•

To the best of our knowledge, our method is the ﬁrst to
leverage dictionaries that are both low-rank and separable
for robust representation learning.

The rest of the manuscript is organized as follows. Sec-
tion 2 reviews fundamental deﬁnitions and results of tensor
algebra and of the Kronecker product. Section 3 is dedicated
to deriving the RKCA model, relating RKCA to separa-
ble dictionary learning, and deriving RKCA with missing
values. In Section 4, we discuss optimality guarantees by
formulating RKCA as an equivalent CP factorization with
duplicated factors. Perspectives on the low-rank promoting
properties can be found in Section 5, and a discussion
on the computational cost and implementation details of
the methods in Section 6. Finally, we present in Section
7 experimental evidence of the effectiveness of RKCA on
synthetic and real-world data.

2 PRELIMINARIES

In this section we review fundamental properties of tensor
algebra and of the Kronecker product, and present the
notations and conventions followed in the paper.

2.1 Tensor algebra

We refer to multidimensional arrays of real numbers of
dimension I1 × I2 × . . . × IN as N -dimensional, or N -
way real tensors. The order of the tensor is the number
of indices required to address its elements. Consequently,
each element of an N th-order tensor X is addressed by N
.
indices, i.e., (X )i1,i2,...,iN
= xi1,i2,...,iN . We denote tensors
by bold calligraphic letters, e.g., X .

The sets of real and integer numbers are denoted by R
and Z, respectively. An N th-order real-valued tensor X is
deﬁned over the tensor space RI1×I2×···×IN , where In ∈ Z
for n = 1, 2, . . . , N .

Matrices (vectors) are second order (ﬁrst order) tensors
and are denoted by uppercase (lowercase) bold letters, e.g.,
X (x). The ith column of a matrix X will be written xi for
convenience.

2.1.1 Elementary deﬁnitions

obtained by ﬁxing all but the nth index to given values.
Similarly, tensor slices are obtained by ﬁxing all but two
indices.

Figure 1 illustrates the concepts of ﬁbers and slices for
a third order tensor. For a third order tensor, the slices are
effectively matrices obtained by ”slicing” the tensor along a
mode.

A tensor’s elements can be re-arranged to form a new
tensor of different dimension. We deﬁne the tensor vec-
torisation and tensor matricisations (or unfoldings) as re-
arrangements of particular interest.

be

2.2

(Tensor

vectorisation). Let X

∈
Deﬁnition
RI1×I2×...×IN
an N th-order
tensor
vectorisation, or ﬂattening, operator X (cid:55)→ vec(X ) maps
each element xi1,i2,...,iN of X to a unique element xj of a
real-valued vector vec(X ) of dimension (cid:81)N
m=1 Im with the
following bijection:

tensor. The

j = 1 +

(ik − 1) Jk

and Jk =

Im.

(4)

Deﬁnition 2.3 (Mode-n unfolding and tensorisation). Let
X ∈ RI1×I2×...×IN be an N th-order tensor. The mode-n
tensor matricisation - or unfolding, with n ∈ {1, 2, . . . , N },
is the matrix X[n] of dimensions (In, (cid:81)
k(cid:54)=n Ik) such that
tensor entry xi1,i2,...,iN is mapped to a unique element xin,j,
with the following bijection:

j = 1 +

(ik − 1) Jk

and Jk =

Im.

(5)

k−1
(cid:89)

m=1

k−1
(cid:89)

m=1
m(cid:54)=n

N
(cid:88)

k=1

N
(cid:88)

k=1
k(cid:54)=n

The inverse operator of the mode-n unfolding is the

mode-n tensorisation and is denoted foldn(X[n]) = X .

In other words, the mode-n unfolding is the matrix
whose columns are the mode-n ﬁbers obtained by ﬁrst
varying I1, then I2, up until IN with the exception of In.

Having deﬁned both tensor ﬁbers, slices, and unfoldings;
we can now deﬁne a higher-order analogue for matrix-
vector multiplication as the tensor mode-n product.

Deﬁnition 2.4 (Mode-n product). The mode-n product of
X ∈ RI1×I2×···×IN with U ∈ RJ×In is the tensor X ×n U ∈
RI1×···×In−1×J×In+1×···×IN such that
In(cid:88)

(X ×n U)i1···in−1jin+1iN =

xi1i2···iN ujin .

(6)

in=1

Effectively, each mode-n ﬁber is multiplied by U.

The mode-n product can be expressed as a matrix prod-

uct as per Proposition 2.1.

Proposition 2.1. With the notations of Deﬁnition 2.4:
i=N,i(cid:54)=nUi)T,
i=1 Ui)[n] = UnX[n](⊗1

(X ×N

(7)

where ⊗ is the Kronecker product of Deﬁnition 2.5.

Tensor ﬁbers and tensor slices are special subsets of the
elements of a tensor.

2.1.2 Tensor rank

Deﬁnition 2.1 (Tensor slices and ﬁbers). We deﬁne the
mode-n tensor ﬁbers as all subsets of the tensor elements

Contrary to matrices, the tensor rank is not uniquely de-
ﬁned. In this paper, we are interested in the tensor Tucker
rank, which we deﬁne as the vector of the ranks of a tensor’s

4

(13)

(14)

(15)

(16)

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

mode-n unfoldings (i.e., its mode-n ranks), and the tensor
multi-rank [34], [35] deﬁned as the vector of the ranks of its
frontal slices.

More details about tensors, such as the deﬁnitions of the
tensor CP-rank and information about common decomposi-
tions can be found in [33], for example.

2.2 Properties of the Kronecker product

We now remind the reader of the deﬁnition and of important
properties of the Kronecker product.
Deﬁnition 2.5 (Kronecker product). Let A ∈ Rm×n and
B ∈ Rp×q then

A ⊗ B =






...

a11B a12B . . . a1nB

...

...




 .

...

an1B an2B . . . annB

Proposition 2.2 is a fundamental result relating the Kro-

necker product and tensor vectorisation.

Proposition 2.2. Let Y = X ×1 U1 ×2 U2 . . . ×N UN then

vec(Y) = (cid:0)⊗1

i=N Ui

(cid:1) vec(X ).

In particular, if Y = AXBT then

vec(Y) = (B ⊗ A)vec(X).

Tucker factorization, and deﬁning f (·) as a combination of
penalties on the factors. More speciﬁcally, we assume:

L = R ×1 A ×2 B.

Figure 2 illustrates the decomposition.

Fig. 2: Illustration of the decomposition.

(8)

(9)

(10)

We construct f to promote low-rankness in the ﬁrst two
modes of L and therefore in each of its frontal slices. In this
work, we will discuss three different choices of f depending
on the positive-homogeneity degree of the full regularization.
Deﬁnition 3.1 (Def. 3 of [31, page 6]). A function θ : RD1
×
. . . × RDN
→ RD is positively homogeneous with degree
p if θ(αx1, . . . , αxN ) = αpθ(x1, . . . , xN ), ∀α ≥ 0. Notably
θ(0) = 0 holds.

The following choice of f is the degree 2 regularizer and

recovers the model presented in [29]:

f (L) = α||R||1 + ||B ⊗ A||F,

The Kronecker product is compatible with most common
matrix decomposition. For the Singular Value Decomposi-
tion, we have Proposition 2.3.
Proposition 2.3. Let Y = USVT and X = HTGT then

Y ⊗ X = (U ⊗ H)(S ⊗ T)(V ⊗ G)T.

and

(11)

while choice

f (L) = α||R||1||B ⊗ A||F,

f (L) = α||R||1||B ⊗ A||∗,

Where we used the identity (A ⊗ B)T = AT ⊗ BT.

We refer the reader to one of the many linear algebra ref-
erences for further information about the Kronecker product
and its properties.

3 MODEL DERIVATION
In this section we start by describing RKCA as a structured
tensor factorization. We then show its equivalence to a dic-
tionary learning problem, discuss how to extend the model
to handle missing values, and ﬁnally derive a ﬁrst algorithm
that will serve as the basis for the rest of the discussion.

3.1 The tensor factorization perspective
Consider a set of N two dimensional observations (i.e.,
matrices) Xi ∈ Rm×n, i = 1, . . . , N stacked as the frontal
slices of a tensor X . We study Tensor Robust PCA problems:

minL,E
s.t

f (L) + g(E),
X = L + E

(12)

where L and E are respectively the low-rank and sparse
components of X . We will deﬁne f (·) and g(·) to be regu-
larization functions, possibly non-smooth and non-convex,
meant to promote structural properties of L and E - in our
case, low-rankness of L, and sparsity in E.

The Robust Kronecker Component Analysis (RKCA) is
obtained by assuming L factorizes in a restricted form of

are degree 3 regularizers.

In all three cases, the regularizer comprises a norm on the
Kronecker product of the Tucker factors, hence the proposed
method is coined as RKCA. The interpretation of this is
made clear in Section 3.2.

3.2 Robust separable dictionary learning

We now establish how to interpret RKCA as a robust sepa-
rable dictionary learning scheme for tensor-valued observa-
tions. We remind the reader that we do not seek overcom-
pleteness, but rather choose to learn low-rank dictionaries
as a mean of modeling low-rankness in the observations.
We impose separability to preserve the spatial correlation
within the tensor inputs, and for scalability (c.f., Section 3.4).
These choices are further motivated by the work of [15] (for
low-rankness), and [24] (for separability).

Consider the following Sparse Dictionary Learning prob-
lem with Frobenius-norm regularization on the dictionary
D, where we decompose N observations xi ∈ Rmn on
D ∈ Rmn×r1r2 with representations ri ∈ Rr1r2 :

min
D,R

(cid:88)

i

||xi − Dri||2

2 + λ

||ri||1 + ||D||F.

(17)

(cid:88)

i

We assume a Kronecker-decomposable dictionary D = B ⊗
A with A ∈ Rm×r1 , B ∈ Rn×r2 . To model the presence of
outliers, we introduce a set of vectors ei ∈ Rmn and, with
d = r1r2 + mn, deﬁne the block vectors and matrices:

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

yi =

(cid:21)

(cid:20)ri
ei

∈ Rd C = (cid:2)B ⊗ A I(cid:3) ∈ Rmn×d.

(18)

We obtain a two-level structured dictionary C and the
associated sparse encodings yi. Breaking-down the vari-
ables to reduce dimensionality and discarding ||I||F:

min
A,B,R,E

(cid:88)

i

||xi − (B ⊗ A)ri − ei||2
2

(cid:88)

+λ

i

(cid:88)

i

||ri||1 + λ

||ei||1 + ||B ⊗ A||F.

Suppose now that the observations xi were obtained by
vectorizing two-dimensional data such as images, i.e., xi =
vec(Xi), Xi ∈ Rm×n. Without loss of generality, we choose
r1 = r2 = r and ri = vec(Ri), Ri ∈ Rr×r, and recast the
problem as:

(cid:88)

min
A,B,R,E

i
||Ri||1 + λ

(cid:88)

i

(cid:88)

+λ

i

||Xi − ARiBT − Ei||2
F

||Ei||1 + ||B ⊗ A||F.

(20)

Equivalently, enforcing the equality constraints and concate-
nating the matrices Xi, Ri, and Ei as the frontal slices of
3-way tensors, we obtain problem (21).

minA,B,R,E α||R||1 + λ||E||1 + ||B ⊗ A||F,
s.t

X = R ×1 A ×2 B + E

(21)

where R is a tensor whose ith frontal slice encodes the
representation of Xi on the column basis A and on the row
basis B. Problem (21) corresponds to choosing f to be (14)
and g to be the element-wise (cid:96)1 norm in (12).

We verify r ∈ N, r ≤ min(m, n) is a natural upper bound
on the rank of each frontal slice of L and on its mode-1 and
mode-2 ranks:

Li = ARiBT.

(22)

We have thus shown how the two perspectives are

equivalent.

3.3 Tensor completion with RKCA

Handling outliers is important for real-world applications,
but data corruption is polymorphic, and missing values
are common. We now present an extension to the original
problem where we assume we are only given incomplete
and possibly grossly-corrupted observations, and wish to
perform simultaneous removal of the outliers and imputa-
tion of the missing values.

Given a set Ω ⊂ NI1×I2×...IN we deﬁne the sampling
operator πΩ : RI1×I2×...IN → RI1×I2×...IN as the projection
on the space of N -way tensors whose only non-zero entries
are indexed by N -tuples in Ω, i.e., ∀X ∈ RI1×I2×...IN :
(cid:40)

πΩ(xi1,i2,...,iN ) =

1 (i1, i2, . . . , iN ) ∈ Ω
0 (i1, i2, . . . , iN ) /∈ Ω

.

non-zero entries are indexed by N -tuples of Ω. Denoting by
¯Ω the complement of Ω, we have ∀X , (cid:104)πΩ(X ), π ¯Ω(X )(cid:105) = 0.

With partial observations, we solve:

minL,E
s.t

f (L) + ||E||1,
πΩ(X ) = πΩ(L) + πΩ(E).

5

(23)

By the orthogonality of πΩ,

||E||1 = ||πΩ(E)||1 +
||π ¯Ω(E)||1 . Without loss of generality we follow [7], [36] and
assume π ¯Ω(X ) = 0 such that π ¯Ω(X ) = π ¯Ω(L)+π ¯Ω(E). This
implies that we do not seek to recover possible corruption
on the missing values but directly the missing element.
Problem (23) is therefore equivalent to:

(19)

minL,E
s.t

f (L) + ||πΩ(E)||1,
X = L + E.

(24)

To solve (24) we need to compute the proximal operator
of ||πΩ(E)||1, we show (proof in Appendix A.1) that the
corresponding operator is the selective shrinkage S Ω
λ :
λ (X ) = Sλ(πΩ(X )) + π ¯Ω(X ).

∀X ∈ RI1×I2×...IN , S Ω

(25)

We present tensor completion results in Section 7.4.

3.4 An efﬁcient algorithm for the degree 2 regularizer

Finally, we derive an efﬁcient algorithm for the degree 2-
regularized problem. This discussion will serve as a basis
for the other regularizers which only require minor modiﬁ-
cations of this algorithm.

Problem (21) is not jointly convex, but is convex in
each component individually. We resort to an alternating-
direction method and propose a non-convex ADMM proce-
dure that operates on the frontal slices.

Minimizing ||B ⊗ A||F presents a challenge: the product
is high-dimensional, the two bases are coupled, and the
loss is non-smooth. Let ||.||p denote the Schatten-p norm2.
3 and remarking
Using the identity ||A⊗B||p = ||A||p||B||p
that ||B||F||A||F ≤ ||A||2
F+||B||2
, we minimize a simpler
upper bound4. The resulting sub-problems are smaller, and
therefore easier to solve computationally. In order to obtain
exact proximal steps for the encodings Ri, we introduce a
split variable Ki such that ∀i, Ki = Ri. Thus, we solve:

2

F

min
A,B,R,K,E
s.t
s.t

α||R||1 + λ||E||1 + 1
X = K ×1 A ×2 B + E,
R = K.

2 (||A||2

F + ||B||2

F),

(26)

By introducing the Lagrange multipliers Λ and Y, such
that the ith frontal slice corresponds to the ith constraint,
and the dual step sizes µ and µK, we formulate the Aug-
mented Lagrangian of problem (26):

L(A, B, R, E, K, Λ, Y, µ, µK) = λ

||Ri||1 + λ

||Ei||1+

(cid:88)

(cid:88)

(||A||2

F + ||B||2

F) +

(cid:88)

i
i
(cid:104)Λi, Xi − AKiBT − Ei(cid:105)+

(cid:104)Yi, Ri − Ki(cid:105) +

||Xi − AKiBT − Ei||2

F+

1
2
(cid:88)

i
µK
2

i
µ
2

(cid:88)

i

||Ri − Ki||2
F.

(cid:88)

i

(27)

It is simple to show πΩ is an orthogonal projection.
Clearly, πΩ is linear and idempotent (πΩ ◦ πΩ = πΩ). A
tensor X is in the null space of πΩ if and only if none of its

2. The Schatten-p norm of A is the (cid:96)p norm of its singular values.
3. From the compatibility of the Kronecker product with the SVD.
4. ||ABT||∗ [37].

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

We can now derive the ADMM updates. Each Ei is given
by shrinkage after rescaling [11]:

Ei = Sλ/µ(Xi − AKiBT +

Λi).

(28)

1
µ

From the developments of Section 3.3, extending our al-
gorithms to handle missing value only involves using the
selective shrinkage operator (25) in (28).

A similar rule is immediate to derive for Ri, and solving
for A and B is straightforward with some matrix algebra.
We therefore focus on the computation of the split variable
Ki. Differentiating, we ﬁnd Ki satisﬁes:

µKKi + µATAKiBTB =

(29)

AT(Λi + µ(Xi − Ei))B + µKRi + Yi.

The key is here to recognize Equation (29) is a Stein equation,
and can be solved in cubical time and quadratic space in r
by solvers for discrete-time Sylvester equations - such as the
Hessenberg-Schur method [38] - instead of the naive O(r6)
time, O(r4) space solution of vectorizing the equation in a
size-r2 linear system. We obtain Algorithm 1.

Algorithm 1 RKCA with degree 2 regularization.
1: procedure RKCA(X , r, λ, α)
2:
3:
4:

A0, B0, E 0, R0, K0, Λ0, Y 0, µ0, µ0
while not converged do

E t+1 ← Sλ/µt (X − Kt ×1 At ×2 Bt + 1
˜X
At+1 ← ((cid:80)

← X − E t+1

µt Λt)

t+1

5:
6:

K ← INITIALIZE(X )

i)T) /

i(µt ˜Xt+1
i Kt
i(µt ˜Xt+1
i(Kt

i)Bt(Kt
i + Λt
i)T)
i(Bt)TBt(Kt
i + Λt
i)TAt+1Kt
i) /
i)T(At+1)TAt+1Kt
i)

(I + µt (cid:80)

Bt+1 ← ((cid:80)

for all i do
Kt+1

(I + µt (cid:80)
i ← STEIN(− µt
µt
K
(cid:104)
(At+1)T(Λt
(Kt+1

1
µt
K

(At+1)TAt+1,
i + µt ˜Xt+1
i
Yt
i )

i − 1
µt
K

(Bt+1)TBt+1,
(cid:105)

)Bt+1 + Yt
i

+ Rt
i)

7:

8:
9:

t+1

− Kt+1 ×1 At+1 ×2 Bt+1)

K(Rt+1 − Kt+1)

K

i ← Sα/µt

Rt+1
end for
Λt+1 ← Λt + µt( ˜X
Y t+1 ← Y t + µt
µt+1 ← min(µ∗, ρµt)
µt+1
K, ρµt
K ← min(µ∗
end while
return A, B, R, E

10:
11:
12:
13:
14:
15:
16:
17:
18: end procedure

K)

4 RKCA AND GLOBAL OPTIMALITY
The work of [30], [31] suggests global optimality can be
achieved from any initialization in tensor factorization mod-
els given that the factorization and regularization mappings
match in certain ways. We summarize the main results of
this work and show our model with regularizer (15) or (16)
respects the conditions for global optimality.

4.1 Review of the main results

We ﬁrst review some of the concepts manipulated in [30],
[31] and give an overview of the main results.

Deﬁnition 4.1 (Def. 1 of [31, page 6]). A size-r set of K
factors (X 1, . . . , X K)r is deﬁned to be a set of K tensors
where the ﬁnal dimension of each tensor is equal to r:
(X 1, . . . , X K)r ∈ R(D1×r) × . . . × R(DK ×r).

6

× . . . × RDK

Deﬁnition 4.2 (Def. 6 of [31, page 6]). An elemental map-
ping, φ : RD1
→ RD is any mapping which is
positively homogeneous with degree p (cid:54)= 0. The r-element
factorization mapping Φr : R(D1×r) × . . . × R(DK ×r) → RD
is deﬁned as:

Φr(X 1, . . . , X K) =

φ(X 1

i , . . . , X K

i ).

(30)

r
(cid:88)

i=1

Deﬁnition 4.3 (Def. 7 of [31, page 8]). An elemental reg-
ularization function g : RD1
→ R+ ∪ ∞, is
deﬁned to be any function which is positive semideﬁnite
and positively homogeneous.

× . . . × RDK

Deﬁnition 4.4 (Def. 8 of [31, page 9]). Given an elemental
mapping φ and an elemental regularization function g, the
authors deﬁne (φ, g) to be a nondegenerate pair if 1) g and
φ are both positively homogeneous with degree p, for some
p (cid:54)= 0 and 2) ∀X ∈ Im(φ)\0, ∃µ ∈ (0, ∞] and (˜z1, . . . , ˜zK)
such that φ(˜z1, . . . , ˜zK) = X , g(˜z1, . . . , ˜zK) = µ,
and g(z1, . . . , zK) ≥ µ for all (z1, . . . , zK) such that
φ(z1, . . . , zK) = X .

The main result, Theorem 15 of [31], provides a charac-
terization of the global optima of CP-based tensor factoriza-
tion problems.

Theorem 4.5 (Theorem 15 of [31, page 15]). Given a function
fr(X 1, . . . , X K, Q), any local minimizer of the optimization
problem:

min
(X 1,...,X K )r,Q

fr(X 1, . . . , X K, Q) ≡

(31)

(cid:96)(Φr(X 1, . . . , X K), Q) + λ

g(X 1

i , . . . , X K

i ) + H(Q)

r
(cid:88)

i=1

such that (X 1
i0
is a global minimizer.

, . . . , X K
i0

) = (0, . . . , 0) for some i0 ∈ {1, . . . , r}

Where Q is an optional set of non-factorized variables
(in our case E), H is convex and possibly non-smooth, (cid:96) is
jointly convex and once-differentiable in (X , Q), and (φ, g)
is a nondegenerate pair.

4.2 Outline of the proof

In order to apply the aforementioned results, we reformu-
late RKCA in an equivalent formulation that satisﬁes the
hypotheses of [30], [31]. The arguments we develop actually
hold for the general Tucker factorization. This section out-
lines the proof of equivalence, the detailed developments
can be found in Appendix C.

The main challenge is to ﬁnd a factorization function that
can be expressed as a sum of elemental mappings. Problem
(21) as-is does not lend itself to such a formulation: even
though the factors A and B form a size-r set of 2 factors,
the core tensor R is of size N on its last dimension. We note
however that the set of frontal slices of R is a size-r set of
N factors, but this formulation doesn’t satisfy Proposition
10 of [31] and it is not immediately obvious how to deﬁne
concatenation of the factors of X and Y and how to verify
the convexity of the factorization-regularization function Ω
(Equation (18) and Proposition 11 of [31] omitted here for
brevity). Additionally, the results of [30], [31] have been

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

proved in the case of a single sum over the last dimension
of the factors, but our factorization being a special case of
Tucker decomposition is most naturally described as:

L = R ×1 A ×2 B

= R ×1 A ×2 B ×3 IN
(cid:88)

(cid:88)

(cid:88)

=

ri,j,k · ai ⊗ bj ⊗ ck.

i

j

k

(32)

(33)

(34)

First, we note the order of the sums can be permuted
such that it is clear our model expresses L as the sum of N
tensors of size m × n × N , where all frontal slices are the
null matrix except for one:
(cid:88)

(cid:88)

(cid:88)

ri,j,k · ai ⊗ bj ⊗ ck.

(35)

L =

With (ck)i = δi,k =

k

i

j

(cid:40)

1 i = j
0 i (cid:54)= j

.

Next, we seek a transformation such that (35) does not
involve cross-terms. Our idea is to unfold the Tucker factor-
ization by duplicating elements of the factors to express it in
the form (30) where the sum is over a size-s set of 3 factors
(or M factors for the general Tucker).

• We deﬁne σ = vec(R).
• We construct ˜A and ˜B from A and B by duplicating

•

columns.
IN doesn’t depend on the data and can be injected
directly in the elemental mapping.

We show the factorization function is deﬁned over the size-
N r2 set of 3 factors (σ, ˜A, ˜B) by:

ΦN r2 (σ, ˜A, ˜B) =

σl · ˜al ⊗ ˜bl ⊗ δl,((cid:100) l

r2 (cid:101)−1) mod r2+1. (36)

N r2
(cid:88)

l=1

4.3 RKCA with degree 3 regularization

We now check that the degree 3 regularizers introduced in
Section 3 are compatible with our reformulated factoriza-
tion. Recall both regularizers are of the form:

g(L) = α||R||1||A||q||B||q.

(37)

With ||.||p the Schatten-p norm, p = 1 for the Nuclear norm
and p = 2 for the Frobenius norm. As we shall see in Section
5.2, g is low-rank promoting.

In the case of the Frobenius penalty, the resulting opti-

mization problem is:

minA,B,R,E λ||R||1||B||F||A||F + λ||E||1,
s.t

X = R ×1 A ×2 B + E.

(38)

The regularizer g adapted to our transformation is:

g(σ, ˜A, ˜B) = ||σ||1

|| ˜A||F
N r

|| ˜B||F
N r

.

(39)

This stems from the fact that the Frobenius norm is
equivalent to the element-wise (cid:96)2 norm, and each element
of A and B appears N r times in ˜A and ˜B.

7

preserves the ranks of the target matrices, and the regular-
ization function is simply:

f (σ, ˜A, ˜B) = ||σ||1|| ˜A||∗|| ˜B||∗.

(40)

It should be clear that the newly deﬁned φ and g are both
positively homogeneous of degree 3, and form a nondegen-
erate pair. Property 10 of [31] also holds.

Hence, we argue that RKCA with a product of norms

regularization enjoys the optimality guarantees of [31].

5 RKCA LEARNS LOW-RANK DICTIONARIES

In this Section we explain the low-rank promoting behav-
ior of RKCA from complementary perspectives. First, we
show the regularizers deﬁned in equations (14), (15), and
(16) directly provide upper bounds on the mode-1 and
mode-2 ranks of the low-rank component, and thus on the
rank of each of its frontal slices. This perspective was ﬁrst
presented in [29]. The second approach to explaining the
models’ properties studies the optimization sub-problems
associated with the two bases A and B. Based on recent
work [39], we show these sub-problems are equivalent to
rank-minimization problems and admit closed-form solu-
tions that involve forms of singular value thresholding.
Finally, we discuss connections with recent work on low-
rank inducing norms.

5.1 Direct penalization of the rank of L

Seeing the models from the perspective of Robust PCA,
which seeks a low-rank representation A of the dataset
X, we minimize the rank of the low-rank tensor L. More
precisely, we show in Theorem 5.1 that we simultaneously
penalize the Tucker rank and the multi-rank of L.

Theorem 5.1. RKCA encourages low mode-1 and mode-2 rank,
and thus, low-rankness in each frontal slice of L, for suitable
choices of the parameters λ and α.

Proof. We minimize either λ||E||1 + α||R||1 + ||A ⊗ B||F,
λ||E||1 + α||R||1||A ⊗ B||F, or λ||E||1 + α||R||1||A ⊗ B||∗.
From the equivalence of norms in ﬁnite-dimensions, ∃k ∈
R∗
+, ||A ⊗ B||∗ ≤ k||A ⊗ B||F. In the case of the Frobe-
nius norm, we can choose α = α(cid:48)
k to reduce
the problem to that of the nuclear norm. In all cases,
we penalize rank(A ⊗ B) = rank(A)rank(B). Given that
the rank is a non-negative integer, rank(A) or rank(B)
decreases necessarily. Therefore, we minimize the mode-1
and mode-2 ranks of L = R ×1 A ×2 B. Additionally,
∀i, rank(ARiBT) ≤ min(rank(A), rank(B), rank(Ri)).

k , λ = λ(cid:48)

Exhibiting a valid k may help in the choice of parame-
||A||∗ ≤ (cid:112)min(m, n)||A||F:
ters. We show ∀A ∈ Rm×n,
√
We know ∀x ∈ Rn, ||x||1 ≤
n||x||2. Recalling the nu-
clear norm and the Frobenius norm are the Schatten-1 and
Schatten-2 norms, and A has min(m, n) singular values, the
result follows.

In the case of the Nuclear norm, we simply observe that
the numbers of linearly independent columns in ˜A and ˜B
are clearly the same as in A and B, so our transformation

In practice, we ﬁnd that on synthetic data designed to
test the models, we effectively recover the ranks of A and B
regardless of the choice of r, as seen in Figure 3.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

5.2.2 Optimal solutions and interpretation
According to [17] the optimal solution of min ||C||∗, s.t X =
DC assuming feasible solutions exist and D (cid:54)= 0 is C∗ =
VrVT
r is the skinny SVD of D. [40] showed
it is also the optimal solution of min ||C||F, s.t X = DC.

r where UrΣrVT

It is therefore easy to show that the optimal solution of
min ||CT||∗ s.t X = DCT under similar conditions is UrUT
r .

Proof. Letting X = UΣVT we have XT = VΣUT.

p VAT

p

Letting UA
q VBT
q ΣB

the skinny SVD of DA and
q the skinny SVD of DB the optimal solutions

p ΣA

UB
for A and B are therefore UA

q UBT
q .

p and UB

p is the PCA basis of DA and that UA

p UAT
For the sake of brevity, we shall describe the interpreta-
tion for A as the same holds for B by symmetry. It should be
noted that UA
p is
the matrix of the orthogonal projection onto that basis. Thus,
the optimal A is the projection of DA onto its principal
components. Remembering that DA = (IN ⊗ B)RT
[1], DA is
the product of B with each code Ri matricized such that the
ith line of DA is the concatenation of all the ith columns of
all the partial reconstructions RkBT, and all the columns
of DA are all the lines of RkBT, which can be seen as
the partial reconstruction of the low-rank images in their
common row space Span(B).

p UAT

Hence, we can see the process of updating A and B as al-
ternating PCAs in the row and column spaces, respectively.

5.3 A note on low-rank inducing norms

The recently published [41] suggests one more interpre-
tation of our regularizers. In [41], the authors show a
family of
low-rank inducing norms, based on solving
rank-constrained Frobenius norm minimization, or rank-
constrained Spectral norm minimization problems can out-
perform the standard nuclear norm relaxation in rank-
minimization problems. Given that the role of the parameter
r in our model is to impose an upper bound on the ranks
of the dictionaries and of the reconstructions, we suggest
the choice of Frobenius regularization instead of nuclear
norm regularization is further justiﬁed. Deeper analysis of
this connection is required and is left for future work.

In this section, we discuss the computational complexity of
our algorithms, and the scalability issues depending on the
regularization. We discuss parameter tuning in Appendix
F.1. The initialization procedure has already been discussed
in [29], and is included in Appendix F.2.

Fig. 3: Sample spectrums of A and B. Ground truth attained
(A: 42, B: 12). r = 100. Degree 2 regularizer.

5.2 Analysis of the sub-problems in A and B

We present an alternative interpretation of the low-rank
properties of our methods. In [39] the authors show that
Frobenius norm and Nuclear norm regularizations either
lead to the same optimal solution - even in the presence of
noise - when the dictionary provides enough representative
power, or are equivalent in the sense that they describe two
different bases on the column space of the dictionary. We
explain the behavior of our algorithms by showing the sub-
problems in A and B are rank-minimization problems.

5.2.1 Sub-problems in A and B

For generality purposes, let us begin with the problem:

min
A,B,R,E
s.t

β(R)||A||F||B||F + λ||E||1 + W (R),

X = R ×1 A ×2 B ×3 IN + E.

(41)

In the degree 3 case we have β(R) = α||R||1 and
W (R) = 0, in the degree 2 case, β(R) = 1 and W (R) =
α||R||1. We now omit the dependency in R for β and W as
they are constant in the sub-problems in A and B.

The sub-problem in A is:

[β||B||F] ||A||F,

min
A
s.t X = R ×1 A ×2 B ×3 IN + E.

Equivalently, in a matricized way using Property (2.1):

[β||B||F] ||A||F,

min
A
s.t X[1] = AR[1](IN ⊗ B)T + E[1].

(42)

(43)

γ||AT||F,

s.t

˜XT

[1] = DAAT.

(44)

min
A

With ˜XT

[1] = XT

[1] − ET

[1], γ = [β||B||F], DA = (IN ⊗ B)RT
[1].

The sub-problem in B is very similar:

Noting that ||A||F = ||AT||F we reformulate the prob-

6 IMPLEMENTATION DETAILS AND COMPLEXITY

lem in the form of Equation (4) of [39]:

[β||A||F] ||B||F,

min
B
s.t X = R ×1 A ×2 B ×3 IN + E.

Matricizing on the second mode:

κ||BT||F,

s.t

˜XT

[2] = DBBT.

min
B

With ˜XT

[2] = XT

[2] − ET

[2], κ = [β||A||F], DB = (IN ⊗ A)RT
[2].

6.1 Computational complexity and LADMM

(45)

(46)

The substitution method used in Section 3.4 is effective
but comes with an additional cost that limits its scalability.
Notably, the cost of solving N Stein equations cannot be ne-
glected in practice. Additionally, the added parameters can
make tuning difﬁcult. Linearization provides an alternative
approach that can scale better to large dimensions and large
datasets. In Appendix D, we give detailed derivations of
how RKCA can be implemented with LADMM.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

The time and space complexity per iteration of Al-
gorithm 1 (i.e., with the degree two regularizer) are
O(N (mnr + (m + n)r + mn + min(m, n)r2 + r3 + r2)) and
O(N (mn + r2) + (m + n)r + r2). Since r ≤ min(m, n), the
terms in r are asymptotically negligible, but in practice it is
useful to know how the computational requirements scale
with the size of the dictionary. Similarly, the initialization
procedure has cost O(N (mn min(m, n) + (min(m, n))3 +
mn) + mn) in time and needs quadratic space per slice,
assuming a standard algorithm is used for the SVD [42].

Switching to an LADMM update for R eliminates the
need of solving costly Stein equations. The soft-shrinkage
operator is applied to the tensor (R ×1 A ×2 B − ∆) ×1
AT ×2 BT, which has N r2 elements, and can be computed
in O(N (min(m, n)r2 + mn + r2 + r3) + mr2 + nr2) by
remembering that (X ×1 A) ×1 B = X ×1 BA. The space
complexity of the update is O(N (min(m, n)r + r2) + r2).

Updating A and B with a Frobenius norm or with a
nuclear norm penalty requires computing a proximal oper-
ator, and solving a linear system in the case of ADMM with
substitution. We focus on the computation of the proximal
operator in Section 6.2. The substitution adds an additional
time and space complexity of O(mr + nr).

Several key steps of Algorithm 1 and its variants, such as
summations of independent terms, are trivially distributed
in a MapReduce [43] way. Proximal operators are separable in
nature and are therefore parallelizable. Consequently, highly
parallel and distributed implementations are possible, and
computational complexity can be further reduced by adap-
tively adopting sparse linear algebra structures.

6.2 Frobenius and nuclear norm penalties
In Equation (26) we used an upper bound on ||A||F||B||F
to obtain smooth sub-problems. In the degree 3-regularized
case, we did not apply the same bound. Given the non-
smoothness of the Frobenius norm, we must resort to other
approaches such as substitution or linearization (c.f. Ap-
pendix B and Appendix D), as with the Nuclear norm.

In Section 5.2, we then showed how Frobenius and Nu-
clear norm penalties can yield the same optimal solutions,
and how in our case they would be equivalent in solving
the sub-problems associated with A and B. It is therefore
natural to wonder why we should choose one over the other,
and what the practical implications of this choice are.

It can be shown that the proximal operator of the
Schatten-p norm has a closed form expression that re-
quires the computation of the SVD of the matrix (Propo-
sition A.1, Appendix A.2). Computing the SVD is costly,
with a typical algorithm [42] requiring O(mn min(m, n) +
(min(m, n))3) ﬂoating-point operations. Storing the result-
ing triple (U, S, V) requires m × min(m, n) + min(m, n)2 +
n × min(m, n) space. Although faster algorithms have been
developed, scalability remains a concern.

However, the Frobenius norm is equal to the matrix
element-wise (cid:96)2 norm, whose proximal operator is only the
projection on the unit ball and doesn’t require any costly
matrix decomposition. The choice of the Frobenius norm
is therefore justiﬁed in the high dimensional setting. A
comparable use of the Frobenius norm in lieu of the Nuclear
norm for representation learning can be seen in [44], also
motivated by scalability.

9

Fig. 4: Convergence on synthetic data with 30% and 60%
corruption.

7 EXPERIMENTAL EVALUATION

We ﬁrst provide experimental veriﬁcation of the correctness
of Algorithm 1 (with degree 2 regularization) on synthetic
data. We then compared the performance of RKCA with
degree 2 regularization against a range of state-of-the-art
tensor decomposition algorithms on four low-rank model-
ing computer vision benchmarks: two for image denoising,
and two for background subtraction. As a baseline, we
report the performance of matrix Robust PCA implemented
via inexact ALM (RPCA) [11], [45], of Non-Negative Robust
Dictionary Learning (RNNDL) [46], and of the Low-Rank
Representations algorithm [17] implemented with both ex-
act ALM (LRRe) and inexact ALM (LRRi). We chose the fol-
lowing methods to include recent representatives of various
existing approaches to low-rank modeling on tensors: The
singleton version of Higher-Order Robust PCA (HORPCA-
S) [7] optimizes the Tucker rank of the tensor through the
sum of the nuclear norms of its unfoldings. In [47], the au-
thors consider a similar model but with robust M-estimators
as loss functions, either a Cauchy loss or a Welsh loss, and
support both hard and soft thresholding; we tested the soft-
thresholding models (Cauchy ST and Welsh ST). Non-convex
Tensor Robust PCA (NC TRPCA) [48] adapts to tensors the
matrix non-convex RPCA [49]. Finally, the two Tensor RPCA
algorithms [34], [35] (TRPCA ’14 and TRPCA ’16) work with
slightly different deﬁnitions of the tensor nuclear norm as a
convex surrogate of the tensor multi-rank. In addition to the
aforementioned low-rank matrix and tensor factorizations,
we include in the comparison the recently proposed Deep
Image Prior [32] with two different architectures (DIP 1 and
DIP 2) as an example of deep learning based approach for
denoising. Our choice of comparing to DIP is motivated by
the fact that DIP is fully unsupervised, designed for denois-
ing (among other tasks), does not require a large dataset for
training, and is close in spirit to tensor factorizations. Given
the small sizes of the datasets at hand, training deep neural
networks would have been impractical.

For all models but RNNDL, we used the implementation made
available by their respective authors, either publicly or on request.
Our implementation of RNNDL was tested for conformity with
the original paper.

For each model, with the exception of DIP, we identiﬁed
a maximum of two parameters to tune via grid-search in
order to keep parameter tuning tractable. When criteria or
heuristics for choosing the parameters were provided by
the authors, we chose the search space around the value
obtained from them. In all cases, the tuning process explored
a wide range of parameters to maximize performance. In the

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

7.2.1 Experimental procedure

We compared the algorithms on two benchmarks. The ﬁrst
is an excerpt of the Highway dataset [50], and consists in a
video sequence of cars travelling on a highway; the back-
ground is completely static. We kept 400 gray-scale images
re-sized to 48 × 64 pixels. The second is the Airport Hall
dataset ( [51]) and has been chosen as a more challenging
benchmark since the background is not fully static and the
scene is richer. We used the same excerpt of 300 frames
(frames 3301 to 3600) as in [52], and kept the frames in their
original size of 144 × 176 pixels.

We treat background subtraction as a binary classiﬁca-
tion problem. Since ground truth frames are available for
our excerpts, we report the AUC [53] on both videos. The
value of α was set to 1 × 10−2 for both experiments.

7.2.2 Results

The original, ground truth, and recovered frames are in
Figure 6 for the Hall experiment (Highway in Appendix H).
Table 1 presents the AUC scores of the algorithms,
ranked in order of their mean performance on the two
benchmarks. The two matrix methods rank high on both
benchmarks and only half of the tensor algorithms match
or outperform this baseline. Our proposed model matches
the best performance on the Highway dataset and provides
signiﬁcantly higher performance than the other on the more
challenging Hall benchmark. Visual inspection of the results
show RKCA is the only method that doesn’t fully cap-
ture the immobile people in the background, and therefore
achieves the best trade-off between foreground detection
and background-foreground contamination.

Algorithm
RKCA (proposed)
TRPCA ’16
NC TRPCA
RPCA (baseline)
RNNDL (baseline)
LRR Exact (baseline)
LRR Inexact (baseline)
HORPCA-S
Cauchy ST
Welsh ST
TRPCA ’14

Highway
0.94
0.94
0.93
0.94
0.94
0.94
0.93
0.93
0.83
0.82
0.76

Hall
0.88
0.86
0.86
0.85
0.85
0.84
0.84
0.86
0.76
0.71
0.61

TABLE 1: AUC on Highway and Hall ordered by mean AUC.

7.3 Image denoising

Many natural and artiﬁcial images exhibit an inherent low-
rank structure and are suitably denoised by low-rank mod-
eling algorithms. In this section, we assess the performance
of the cohort on two datasets chosen for their popularity,
and for the typical use cases they represent.

We consider collections of grayscale images, and color
images represented as 3-way tensors. Laplacian (salt &
pepper) noise was introduced separately in all frontal slices
of the observation tensor at three different levels: 10%, 30%,
and 60%, to simulate medium, high, and gross corruption.
In these experiments we set the value of α to 1 × 10−3 for
noise levels up to 30%, and to 1 × 10−2 at the 60% level.

We report two complementary image quality metrics.
The Peak Signal To Noise Ratio (PSNR) will be used as an
indicator of the element-wise reconstruction quality of the

Fig. 5: Recovery performance with 60% corruption. Relative
(cid:96)2 error and density.

special case of DIP, we re-used the two architectures imple-
mented in the denoising example in the code made available
online by the authors (c.f. Appendix G). We then trained for
2500 iterations while keeping track of the best reconstruction
so-far measured by the PSNR. The best reconstruction is
then saved for further comparison. Following [32], the input
is chosen to be random. As in [17], the data matrix itself was
used as the dictionary for LRRe and LRRi.

When the performance of one method was signiﬁcantly
worse than that of the other, the result is not reported so as
not to clutter the text (see Appendix H). This is the case of
Separable Dictionary Learning [18] whose drastically differ-
ent nature renders unsuitable for robust low-rank modeling,
but was compared for completeness. For the same reason,
we did not compare our method against K-SVD [13], or [19].
Finally, we provide tensor completion experiments with

and without gross corruption in Section 7.4.

7.1 Validation on synthetic data

We generated synthetic data following the RKCA model’s
assumptions by ﬁrst sampling two random bases A and
B of known ranks rA and rB, N Gaussian slices for the
core R, and forming the ground truth L = R ×1 A ×2 B.
We modeled additive random sparse Laplacian noise with
a tensor E whose entries are 0 with probability p, and 1
or −1 with equal probability otherwise. We generated data
for p = 70% and p = 40%, leading to a noise density of,
respectively, 30% and 60%. We measured the reconstruction
error on L and E, and the density of E for varying values
of λ, and α = 1 × 10−2. Our model achieved near-exact
recovery of both L and E, and exact recovery of the density
of E, for suitable values of λ. Evidence is presented in Figure
5 for the 60% noise case.

Algorithm 1 appears robust to small changes in λ, which
suggests more than one value can lead to optimal results,
and that a simple criterion that provides consistently good
reconstruction might be derived, as in Robust PCA [11]. In
the 30% noise case, we did not observe an increase in the
density of E as λ increases, and the (cid:96)2 error on both E and
L was of the order of 1 × 10−7.

7.2 Background subtraction

Background subtraction is a common task in computer
vision and can be tackled by robust low-rank modeling: the
static or mostly static background of a video sequence can
effectively be represented as a low-rank tensor while the
foreground forms a sparse component of outliers.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

7.3.1 Monochromatic face images

(f) HORPCA-S (g) NCTRPCA (h) RNNDL

(i) RPCA

(j) LRRe

(a) Original

(b) GT

(c) Cauchy ST

(d) Welsh ST

(e) TRPCA ’16

(f) HORPCA-S (g) NCTRPCA (h) RNNDL

(i) RPCA

(j) LRRe

(k) LRRi

(l) RKCA

Fig. 6: Background subtraction results on Airport Hall. TR-
PCA ’14 removed.

signals, while the Feature Similarity Index (FSIM, FSIMc for
color images) [54] evaluates the recovery of structural infor-
mation. Quantitative metrics are not perfect replacements
for subjective assessment of image quality; therefore, we
present reconstructed images for veriﬁcation. Our measure
of choice for determining which images to compare visually
is the FSIM(c) for its higher correlation with human evalua-
tion than the PSNR [55].

Our face denoising experiment uses the Extented Yale-B
dataset [56] of 10 different subject, each under 64 different
lighting conditions. According to [57], [58], face images of
one subject under various illuminations lie approximately
on a 9-dimensional subspace, and are therefore suitable for
low-rank modeling. We used the pre-cropped 64 images
of the ﬁrst subject and kept them at full resolution. The
resulting collection of images constitutes a 3-way tensor
of 64 images of size 192 × 168. Each mode corresponds
respectively to the columns and rows of the images, and
to the illumination component. All three are expected to be
low-rank due to the spatial correlation within frontal slices
and to the correlation between images of the same subject
under different illuminations. We present the comparative
quantitative performance of the methods tested in Figure 7,
and provide visualizations of the reconstructed ﬁrst image
at the 30% noise level in Figure 8. We report the metrics
averaged on the 64 images. For DIP, we chose to use a single
model for the 64 images, rather than one model per image,
to provide a better comparison with the other approaches.
In this case, the input dimension was chosen to be 128.

At the 10% noise level, nearly every method provided
good to excellent recovery of the original images. We there-
fore omit this noise level (cf. Appendix H, Table 7 and
ﬁgures). On the other hand, most methods, with the notable
exception of RKCA, NC TRPCA, and TRPCA ’16, failed to
provide acceptable reconstruction in the gross corruption
case. Thus, we present the denoised images at the 30% level,
and compare the performance of the three best performing
methods in Table 2 for the 60% noise level.

Clear differences appeared at the 30% noise level, as
demonstrated both by the quantitative metrics, and by

Fig. 7: Mean PSNR and FSIM on the 64 images of the ﬁrst
subject of Yale at noise levels 10%, 30%, and 60%.

(a) Original

(b) Noisy

(c) Cauchy ST

(d) Welsh ST

(e) TRPCA ’16

(k) LRRi

(l) DIP1

(m) DIP2

(n) RKCA

Fig. 8: Results on the Yale benchmark with 30% noise.
TRPCA ’14 removed.

visual inspection of Figure 8. Overall, performance was
markedly lower than at the 10% level, and most methods
started to lose much of the details. Visual inspection of the
results conﬁrms a higher reconstruction quality for RKCA.
We invite the reader to look at the texture of the skin, the
white of the eye, and at the reﬂection of the light on the
subject’s skin and pupil. The latter, in particular, is very
close in nature to the white pixel corruption of the salt &
pepper noise. Out of all methods, RKCA provided the best
reconstruction quality: it is the only algorithm that removed
all the noise and for which all the aforementioned details
are distinguishable in the reconstruction. Both deep learn-
ing approaches provided reconstructions with visually-
identiﬁable features, but under-performed compared to the
best robust factorization models.

At the 60% noise level, our method scored markedly
higher than its competitors on image quality metrics, as
seen both in Figure 7 and in Table 2. Visualizing the re-
constructions conﬁrms the difference: the image recovered
by RKCA at the 60% noise level is comparable to the output
of competing algorithms at the 30% noise level.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

12

Noisy

PSNR
FSIM

RKCA

27.1490
0.8913

NC TRPCA

TRPCA ’16

22.8502
0.8509

22.566
0.8427

TABLE 2: Three best results on Yale at 60% noise.

Fig. 9: PSNR and FSIMc of all methods on the Facade
benchmark at noise levels 10%, 30%, and 60%.

7.3.2 Color image denoising
Our benchmark is the Facade image [59]: the geometric
nature of the building’s front wall, and the strong correlation
between the RGB bands indicate the data can be modeled by
a low-rank 3-way tensor where each frontal slice is a color
channel. The rich details and lighting make it interesting to
assess ﬁne reconstruction. The input dimension for DIP was
set to 32, following the color image denoising examples.

At the 10% noise level, RKCA attained the highest PSNR,
and the highest FSIMc value. Most methods provided excel-
lent reconstruction, in agreement with the high values of the
metrics shown in Figure 9. As in the previous benchmark,
full results are in Appendix H (Table 8 and ﬁgures). At the
30% noise level, Cauchy ST exhibited the highest PSNR and
RKCA the second highest, while TRPCA ’16 and RKCA
were tied for ﬁrst place on the FSIMc metric. Details are
provided in Figure 10. Clear differences in reconstruction
quality are visible, and are best seen on the ﬁne details of the
picture, such as the black iron ornaments, or the light com-
ing through the window. Our method best preserved the
dynamics of the lighting, and the sharpness of the details,
and in the end provided the reconstruction visually closest
to the original. Competing models tend to oversmooth the
image, and to make the light dimmer; indicating substantial
losses of high-frequency and dynamic information. RKCA
appears to also provide the best color ﬁdelity. Similar to
the Yale-B experiment, the deep-learning models are able to
handle the gross corruption to a certain extent, but suffer
from more distortion than the robust factorizations.

In the gross-corruption case, RKCA was the only method
with TRPCA ’16 and HORPCA-S to provide a reconstruction
with distinguishable details, and did it best (Figure 3) while
achieving the highest score on both quantitative metrics.

(a) Original

(b) Noisy

(c) Cauchy ST

(d) Welsh ST

(e) TRPCA ’14

(f) TRPCA ’16 (g) HORPCA-S (h) NCTRPCA

(i) RPCA

(j) LRRe

(k) LRRi

(l) DIP1

(m) DIP2

(n) RKCA

Fig. 10: Results on the Facade benchmark with 30% noise.

Noisy

PSNR
FSIMc

RKCA

24.4491
0.9179

TRPCA ’16

HORPCA-S

23.6552
0.9109

22.8811
0.9060

TABLE 3: Three best results on Facade at 60% noise.

7.4 Tensor completion

To showcase the tensor completion capabilities of our al-
gorithm, we implemented an LADMM method to solve
problem (24) with f (L) = α||R||1 + 1
F).
We provide comparison with one robust tensor completion
model (HORPCA-S with missing values [7]) and the matrix
Robust PCA with missing values [11].

F + ||B||2

2 (||A||2

7.4.1 Yale-B with noise and missing values

Our ﬁrst experiment extends Section 7.3.1: we investigate
the case where apart from corruption, some values are
missing. We generated the data by ﬁrst introducing 30% salt
& pepper noise, then removing 30% of the pixels at random.

Noisy

PSNR
FSIM

RKCA

22.7069
0.9332

HORPCA-S

22.3827
0.9187

RPCA

19.6597
0.8717

TABLE 4: Reconstruction of the ﬁrst face of Yale-B with 30%
salt & pepper noise and 30% missing values.

As seen in Table 4, RKCA markedly outperformed both
other models in terms of FSIM, which translates into a more
natural reconstruction. HORPCA-S achieved a similar PSNR
but many details are lost, while RPCA removed much of the
image’s details and left some corruption.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7.4.2 300 Faces in the Wild

Our second experiment is on the completion of unwarped
3D faces with partial self-occlusions taken from the 300 faces
in the wild challenge [60], [61], [62].

Original

RKCA

HORPCA

RPCA

TABLE 5: Completion experiment on the 300W dataset.

We present in Table 5 the occluded frames 1, 44, and 76
of a video of the dataset, and the completed frames obtained
with RKCA, HORPCA-S, and RPCA. Since no corruption is
present in the dataset, the λ parameters were ﬁxed to a high
value (1 × 104) such that the algorithms behaved as tensor
or matrix completion models. For RKCA, we bounded the
rank of the reconstruction to 50 and performed grid-search
on 5 values of the α parameter.

It is clear from Table 5 that RKCA provides the most
completion. HORPCA-S is able to complete sparse missing
values at the center of the image but is lacking on the self-
occlusions at the right. Matrix RPCA failed to complete the
frames on this benchmark.

8 CONCLUSION

In this work we presented RKCA, a framework for robust
low-rank representation learning on separable dictionaries
that expresses the problem as a tensor factorization. We
proposed three different regularizers and derived the steps
for both ADMM and LADMM solvers. We showed the fac-
torization can be expressed in an equivalent way that gives
optimality guarantees when coupled with a regularizer that
is positively-homogeneous of degree 3. We then further
discussed the low-rank properties of the models, and their
practical implementation. We reached the conclusion that
our model with a degree 2 regularizer, achieved a good
trade-off between experimental effectiveness and parameter
tuning difﬁculty.

Future work can seek to develop a supervised variant
of RKCA that could be applied to the same tasks. Another
extension that we leave for future work is to consider non-
convex regularizers: it is well-known that letting p → 0
in the element-wise (cid:96)p and in the Scatten-p norms recover
respectively the (cid:96)0 pseudo-norm and the rank functions. We
refer to [63] for an example in the matrix RPCA case. Finally,
we believe RKCA could be extended with deep learning
by replacing the factorization of the low-rank component

13

by a deep neural network, such as an auto-encoder. Effec-
tively, the bilinear factorization we use can be seen as an
elementary linear auto-encoder, and replacing it with a more
involved non-linear model would be straightforward.

ACKNOWLEDGMENTS
Mehdi Bahri was partially funded by the Department of
Computing, Imperial College London. The work of Y.
Panagakis has been partially supported by the European
Community Horizon 2020 [H2020/2014-2020] under Grant
Agreement No. 645094 (SEWA). S. Zafeiriou was partially
funded by EPSRC Project EP/N007743/1 (FACER2VM) and
also partially funded by a Google Faculty Research Award.

REFERENCES

[1] K. Pearson, “On lines and planes of closest ﬁt to systems of points

in space,” Philosophical Magazine, vol. 2, no. 6, pp. 559–572, 1901.

[2] H. Hotelling, B. A. Olshausen, and D. J. Field, “Analysis of a
complex of statistical variables into principal components.” Journal
of Educational Psychology, vol. 24, no. 6, pp. 417–441, 1933.

[3] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation Applied to
Handwritten Zip Code Recognition,” Neural Computation, vol. 1,
no. 4, pp. 541–551, Dec. 1989.

[4] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, Nov. 1998.

[5] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol.

521, p. 436, May 2015.

[6] N. Xue, G. Papamakarios, M. Bahri, Y. Panagakis, and S. Zafeiriou,
“Robust low-rank tensor modelling using Tucker and CP de-
composition,” in 2017 25th European Signal Processing Conference
(EUSIPCO), Sep. 2017, pp. 1185–1189.

[7] D. Goldfarb and Z. Qin, “Robust Low-Rank Tensor Recovery:
Models and Algorithms,” SIAM Journal on Matrix Analysis and
Applications, vol. 35, no. 1, pp. 225–253, Jan. 2014.

[8] R. Rubinstein, A. M. Bruckstein, and M. Elad, “Dictionaries for
Sparse Representation Modeling,” Proceedings of the IEEE, vol. 98,
no. 6, pp. 1045–1057, Jun. 2010.

[9] B. A. Olshausen and D. J. Field, “Sparse coding with an over-
complete basis set: A strategy employed by V1?” Vision Research,
vol. 37, no. 23, pp. 3311–3325, Dec. 1997.

[10] H. Zou, T. Hastie, and R. Tibshirani, “Sparse Principal Component
Analysis,” Journal of Computational and Graphical Statistics, vol. 15,
no. 2, pp. 265–286, 2006.

[11] E. J. Cands, X. Li, Y. Ma, and J. Wright, “Robust Principal Compo-

nent Analysis?” J. ACM, vol. 58, no. 3, pp. 11:1–11:37, Jun. 2011.

[12] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. S. Huang, and S. Yan,
“Sparse Representation for Computer Vision and Pattern Recog-
nition,” Proceedings of the IEEE, vol. 98, no. 6, pp. 1031–1044, Jun.
2010.

[13] M. Aharon, M. Elad, and A. Bruckstein, “K-SVD: An Algorithm for
Designing Overcomplete Dictionaries for Sparse Representation,”
IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311–
4322, Nov. 2006.

[14] J. Mairal, M. Elad, and G. Sapiro, “Sparse Representation for Color
Image Restoration,” IEEE Transactions on Image Processing, vol. 17,
no. 1, pp. 53–69, Jan. 2008.

[15] G. Liu and P. Li, “Low-Rank Matrix Completion in the Presence
of High Coherence,” IEEE Transactions on Signal Processing, vol. 64,
no. 21, pp. 5623–5633, Nov. 2016.

[16] G. Liu, Q. Liu, and P. Li, “Blessing of Dimensionality: Recovering
Mixture Data via Dictionary Pursuit,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 39, no. 1, pp. 47–60, Jan. 2017.
[17] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust Recovery
of Subspace Structures by Low-Rank Representation,” IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1,
pp. 171–184, Jan. 2013.

[18] S. Hawe, M. Seibert, and M. Kleinsteuber, “Separable Dictionary
Learning,” in Proceedings of the 2013 IEEE Conference on Computer
Vision and Pattern Recognition, ser. CVPR ’13. Washington, DC,
USA: IEEE Computer Society, 2013, pp. 438–445.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

14

[19] S. Hsieh, C. Lu, and S. Pei, “2d sparse dictionary learning via
tensor decomposition,” in 2014 IEEE Global Conference on Signal
and Information Processing (GlobalSIP), Dec. 2014, pp. 492–496.
[20] Y. Fang, J. Wu, and B. Huang, “2d sparse signal recovery via 2d
orthogonal matching pursuit,” Science China Information Sciences,
vol. 55, pp. 889–897, 2012.

[21] F. L. Hitchcock, “The Expression of a Tensor or a Polyadic as a
Sum of Products,” Journal of Mathematics and Physics, vol. 6, no.
1-4, pp. 164–189, Apr. 1927.

[22] R. Harshman, “Foundations of the PARAFAC procedure: Models
and conditions for an” explanatory” multi-modal factor analysis,”
UCLA Working Papers in Phonetics, vol. 16, no. 1, p. 84, 1970.
[23] J. D. Carroll and J.-J. Chang, “Analysis of individual differences in
multidimensional scaling via an n-way generalization of Eckart-
Young decomposition,” Psychometrika, vol. 35, no. 3, pp. 283–319,
Sep. 1970.

[24] Z. Shakeri, A. D. Sarwate, and W. U. Bajwa, “Identiﬁcation of
kronecker-structured dictionaries: An asymptotic analysis,” in
2017 IEEE 7th International Workshop on Computational Advances in
Multi-Sensor Adaptive Processing (CAMSAP), Dec. 2017, pp. 1–5.
[25] Z. Shakeri, W. U. Bajwa, and A. D. Sarwate, “Minimax lower
bounds for Kronecker-structured dictionary learning,” in 2016
IEEE International Symposium on Information Theory (ISIT), Jul. 2016,
pp. 1148–1152.

[26] ——, “Minimax Lower Bounds on Dictionary Learning for Tensor
Data,” IEEE Transactions on Information Theory, vol. 64, no. 4, pp.
2706–2726, Apr. 2018.

[27] ——, “Sample complexity bounds for dictionary learning of tensor
data,” in 2017 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), Mar. 2017, pp. 4501–4505.

[28] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed
Optimization and Statistical Learning via the Alternating Direc-
tion Method of Multipliers,” Found. Trends Mach. Learn., vol. 3,
no. 1, pp. 1–122, Jan. 2011.

[29] M. Bahri, Y. Panagakis, and S. Zafeiriou, “Robust Kronecker-
Decomposable Component Analysis for Low-Rank Modeling,” in
2017 IEEE International Conference on Computer Vision (ICCV), Oct.
2017, pp. 3372–3381.

[30] B. D. Haeffele, E. D. Young, and R. Vidal, “Structured Low-rank
Matrix Factorization: Optimality, Algorithm and Applications to
Image Processing,” in Proceedings of the 31st International Conference
on International Conference on Machine Learning - Volume 32, ser.
ICML’14. Beijing, China: JMLR.org, 2014, pp. II–2007–II–2015.

[31] B. D. Haeffele and R. Vidal, “Global Optimality in Tensor Factor-
ization, Deep Learning, and Beyond,” CoRR, vol. abs/1506.07540,
2015.

[32] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Deep Image Prior,” in
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2018.

[33] T. Kolda and B. Bader, “Tensor Decompositions and Applications,”

SIAM Review, vol. 51, no. 3, pp. 455–500, Aug. 2009.

[34] Z. Zhang, G. Ely, S. Aeron, N. Hao, and M. Kilmer, “Novel
Methods for Multilinear Data Completion and De-noising Based
on Tensor-SVD,” in 2014 IEEE Conference on Computer Vision and
Pattern Recognition, Jun. 2014, pp. 3842–3849.

[35] C. Lu, J. Feng, Y. Chen, W. Liu, Z. Lin, and S. Yan, “Tensor Robust
Principal Component Analysis: Exact Recovery of Corrupted Low-
Rank Tensors via Convex Optimization,” in 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), Jun. 2016, pp.
5249–5257.

[36] F. Shang, Y. Liu, J. Cheng, and H. Cheng, “Robust Principal
Component Analysis with Missing Data,” in Proceedings of the
23rd ACM International Conference on Conference on Information and
Knowledge Management, ser. CIKM ’14. New York, NY, USA: ACM,
2014, pp. 1149–1158.

[37] B. Recht, M. Fazel, and P. Parrilo, “Guaranteed Minimum-Rank
Solutions of Linear Matrix Equations via Nuclear Norm Minimiza-
tion,” SIAM Review, vol. 52, no. 3, pp. 471–501, Jan. 2010.

[38] G. Golub, S. Nash, and C. Van Loan, “A Hessenberg-Schur method
for the problem AX + XB= C,” IEEE Transactions on Automatic
Control, vol. 24, no. 6, pp. 909–913, Dec. 1979.

[40] H. Zhang, Z. Yi, and X. Peng, “FlRR: fast low-rank representation
using Frobenius-norm,” Electronics Letters, vol. 50, no. 13, pp. 936–
938, Jun. 2014.

[41] C. Grussler, A. Rantzer, and P. Giselsson, “Low-Rank Optimization
with Convex Constraints,” IEEE Transactions on Automatic Control,
pp. 1–1, 2018.

[42] T. F. Chan, “An Improved Algorithm for Computing the Singular
Value Decomposition,” ACM Trans. Math. Softw., vol. 8, no. 1, pp.
72–83, Mar. 1982.

[43] J. Dean and S. Ghemawat, “MapReduce: Simpliﬁed Data Process-
ing on Large Clusters,” in OSDI’04: Sixth Symposium on Operating
System Design and Implementation, San Francisco, CA, 2004, pp.
137–150.

[44] X. Peng, J. Lu, Z. Yi, and R. Yan, “Automatic Subspace Learning
via Principal Coefﬁcients Embedding,” IEEE Transactions on Cyber-
netics, vol. 47, no. 11, pp. 3583–3596, Nov. 2017.

[45] Z. Lin, M. Chen, and Y. Ma, “The Augmented Lagrange Multiplier
Method for Exact Recovery of Corrupted Low-Rank Matrices,”
ArXiv e-prints, Sep. 2010.

[46] Q. Pan, D. Kong, C. Ding, and B. Luo, “Robust Non-negative
Dictionary Learning,” in Proceedings of the Twenty-Eighth AAAI
Conference on Artiﬁcial Intelligence, ser. AAAI’14. Qu´ebec City,
Qu´ebec, Canada: AAAI Press, 2014, pp. 2027–2033.

[47] Y. Yang, Y. Feng, and J. A. K. Suykens, “Robust Low-Rank Tensor
Recovery With Regularized Redescending M-Estimator,” IEEE
Transactions on Neural Networks and Learning Systems, vol. 27, no. 9,
pp. 1933–1946, Sep. 2016.

[48] A. Anandkumar, P. Jain, Y. Shi, and U. N. Niranjan, “Tensor
vs. Matrix Methods: Robust Tensor Decomposition under Block
Sparse Perturbations,” in Proceedings of the 19th International Con-
ference on Artiﬁcial Intelligence and Statistics, AISTATS 2016, Cadiz,
Spain, May 9-11, 2016, 2016, pp. 268–276.

[49] P. Netrapalli, N. U N, S. Sanghavi, A. Anandkumar, and P. Jain,
“Non-convex Robust PCA,” in Advances in Neural Information
Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, Eds. Curran Associates, Inc.,
2014, pp. 1107–1115.

Jodoin, F. Porikli,

J. Konrad, and P.

[50] N. Goyette, P.

Ish-
war, “Changedetection.net: A new change detection benchmark
dataset,” in 2012 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition Workshops, Jun. 2012, pp. 1–8.
[51] Liyuan Li, Weimin Huang, Irene Yu-Hua Gu, and Qi Tian, “Sta-
tistical modeling of complex backgrounds for foreground object
detection,” IEEE Transactions on Image Processing, vol. 13, no. 11,
pp. 1459–1472, Nov. 2004.

[52] Q. Zhao, G. Zhou, L. Zhang, A. Cichocki, and S. Amari, “Bayesian
Robust Tensor Factorization for Incomplete Multiway Data,” IEEE
Transactions on Neural Networks and Learning Systems, vol. 27, no. 4,
pp. 736–748, Apr. 2016.

[53] T. Fawcett, “An introduction to ROC analysis,” Pattern Recognition

Letters, vol. 27, no. 8, pp. 861 – 874, 2006.

[54] L. Zhang, L. Zhang, X. Mou, and D. Zhang, “FSIM: A Feature
Similarity Index for Image Quality Assessment,” IEEE Transactions
on Image Processing, vol. 20, no. 8, pp. 2378–2386, Aug. 2011.
[55] P. Mohammadi, A. Ebrahimi-Moghadam, and S. Shirani, “Sub-
jective and Objective Quality Assessment of Image: A Survey,”
Majlesi Journal of Electrical Engineering, vol. 9, no. 1, Dec. 2014.
[56] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman, “From
few to many: illumination cone models for face recognition under
variable lighting and pose,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 23, no. 6, pp. 643–660, Jun. 2001.
[57] R. Ramamoorthi and P. Hanrahan, “An Efﬁcient Representation
for Irradiance Environment Maps,” in Proceedings of the 28th An-
nual Conference on Computer Graphics and Interactive Techniques, ser.
SIGGRAPH ’01. New York, NY, USA: ACM, 2001, pp. 497–500.

[58] R. Basri and D. W. Jacobs, “Lambertian reﬂectance and linear
subspaces,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 25, no. 2, pp. 218–233, Feb. 2003.

[59] X. Chen, Z. Han, Y. Wang, Q. Zhao, D. Meng, and Y. Tang,
“Robust Tensor Factorization with Unknown Noise,” in 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), Jun.
2016, pp. 5213–5221.

[39] X. Peng, C. Lu, Z. Yi, and H. Tang, “Connections Between Nuclear-
Norm and Frobenius-Norm-Based Representations,” IEEE Trans-
actions on Neural Networks and Learning Systems, vol. 29, no. 1, pp.
218–224, Jan. 2018.

[60] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic, “300
Faces in-the-Wild Challenge: The First Facial Landmark Localiza-
tion Challenge,” in 2013 IEEE International Conference on Computer
Vision Workshops, Dec. 2013, pp. 397–403.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

15

[61] ——, “A Semi-automatic Methodology for Facial Landmark An-
notation,” in 2013 IEEE Conference on Computer Vision and Pattern
Recognition Workshops, Jun. 2013, pp. 896–903.

[62] C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, and
M. Pantic, “300 Faces In-The-Wild Challenge: database and re-
sults,” Image and Vision Computing, vol. 47, pp. 3 – 18, 2016.
[63] G. Papamakarios, Y. Panagakis, and S. Zafeiriou, “Generalised
Scalable Robust Principal Component Analysis,” in Proceedings of
the British Machine Vision Conference, 2014.

[64] A. J. Laub, Matrix Analysis For Scientists And Engineers. Philadel-
phia, PA, USA: Society for Industrial and Applied Mathematics,
2004.

[65] Z. Lin, R. Liu, and Z. Su, “Linearized Alternating Direction
Method with Adaptive Penalty for Low-Rank Representation,”
in Advances in Neural Information Processing Systems 24, J. Shawe-
Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger,
Eds. Curran Associates, Inc., 2011, pp. 612–620.

[66] Z. Xu, M. Figueiredo, and T. Goldstein, “Adaptive ADMM with
Spectral Penalty Parameter Selection,” in Proceedings of the 20th
International Conference on Artiﬁcial Intelligence and Statistics, ser.
Proceedings of Machine Learning Research, A. Singh and J. Zhu,
Eds., vol. 54.
Fort Lauderdale, FL, USA: PMLR, Apr. 2017, pp.
718–727.

[67] Y. Xu, M. Liu, Q. Lin, and T. Yang, “ADMM without a Fixed
Penalty Parameter: Faster Convergence with New Adaptive Pe-
nalization,” in Advances in Neural Information Processing Systems
30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc.,
2017, pp. 1267–1277.

[68] M. Hong, Z. Luo, and M. Razaviyayn, “Convergence Analysis
of Alternating Direction Method of Multipliers for a Family of
Nonconvex Problems,” SIAM Journal on Optimization, vol. 26, no. 1,
pp. 337–364, Jan. 2016.

[69] Y. Wang, W. Yin, and J. Zeng, “Global Convergence of ADMM in
Nonconvex Nonsmooth Optimization,” Journal of Scientiﬁc Com-
puting, Jun. 2018.

Stefanos Zafeiriou (M’09) is currently a Reader
in Machine Learning and Computer Vision with
the Department of Computing, Imperial College
London, London, U.K, and a Distinguishing Re-
search Fellow with the University of Oulu un-
der the Finnish Distinguishing Professor Pro-
gramme. He was a recipient of the Prestigious
Junior Research Fellowships from Imperial Col-
lege London in 2011 to start his own indepen-
dent research group. He was the recipient of
the Presidents Medal for Excellence in Research
Supervision for 2016. He is recipient of many best paper awards includ-
ing the best student paper award in FG’2018. In 2018, he received an
EPSRC Fellowship. He currently serves as an Associate Editor of the
IEEE Transactions on Affective Computing and Computer Vision and
Image Understanding journal. In the past he held editorship positions
in IEEE Transactions on Cybernetics the Image and Vision Computing
Journal. He has been a Guest Editor of over six journal special issues
and co-organised over 13 workshops/special sessions on specialised
computer vision topics in top venues, such as CVPR/FG/ICCV/ECCV
(including three very successfully challenges run in ICCV’13, ICCV’15
and CVPR’17 on facial
landmark localisation/tracking). He has co-
authored over 60 journal papers mainly on novel statistical machine
learning methodologies applied to computer vision problems, such as
2-D/3-D face analysis, deformable object ﬁtting and tracking, shape
from shading, and human behaviour analysis, published in the most
prestigious journals in his ﬁeld of research, such as the IEEE T-PAMI,
the International Journal of Computer Vision, the IEEE T-IP, the IEEE
T-NNLS, the IEEE T-VCG, and the IEEE T-IFS, and many papers in
top conferences, such as CVPR, ICCV, ECCV, ICML. His students are
frequent recipients of very prestigious and highly competitive fellow-
ships, such as the Google Fellowship x2, the Intel Fellowship, and the
Qualcomm Fellowship x3. He has more than 6700 citations to his work,
h-index 42. He was the General Chair of BMVC 2017.

Mehdi Bahri is a PhD student with Stefanos
Zafeiriou at the Department of Computing, Im-
perial College London. He received the Dipl ˆome
d’Ing ´enieur in Applied Mathematics with Hon-
ours from Grenoble Institute of Technology - En-
simag and the MSc in Advanced Computing with
Distinction from Imperial College London, both
in 2016. He published his research in ICCV and
spent one year in the industry. His research inter-
ests include representation learning, geometric
deep learning, and Bayesian learning.

Yannis Panagakis is a Senior Lecturer (Asso-
ciate Professor equivalent) in Computer Science
at Middlesex University London and a Research
Fellow at the Department of Computing, Imperial
College London. His research interests lie in ma-
chine learning and its interface with signal pro-
cessing, high-dimensional statistics, and compu-
tational optimization. Speciﬁcally, Yannis is work-
ing on models and algorithms for robust and ef-
ﬁcient learning from high-dimensional data and
signals representing audio, visual, affective, and
social information. He has been awarded the prestigious Marie-Curie
Fellowship, among various scholarships and awards for his studies and
research. Dr Panagakis currently serves as the Managing Editor of
the Image and Vision Computing Journal. He co-organized the BMVC
2017 conference and several workshops and special sessions in top
venues such as ICCV. He received his PhD and MSc degrees from the
Department of Informatics, Aristotle University of Thessaloniki and his
BSc degree in Informatics and Telecommunication from the University
of Athens, Greece.

P||πΩ(.)||1(X ) = argmin

||πΩ(Y)||1 +

||X − Y||2
F

(47)

we ﬁnd the Sylvester’s equation for U∗

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

APPENDIX A
PROXIMAL OPERATORS
A.1 Selective shrinkage operator
We prove the result for a general D-dimensional tensor X .

Proof. The proximal operator of ||πΩ(.)||1 satisﬁes:

1
2

1
2

Y

Y

By direct case analysis, if i = (i1, i2, . . . , iN ) ∈ Ω:

(P||πΩ(.)||1)i = argmin

||Y i||1 +

||X i − Y i||2
F

(48)

Which is the expression of the standard shrinkage operator
S(X i). If i ∈ ¯Ω, then:

(P||πΩ(.)||1)i = argmin

||X i − Y i||2

F = X i

(49)

1
2

Y

A.2 Schatten-p norms
Proposition A.1. Let A ∈ Rm×n and p > 0, denote by ||.||p
the Schatten-p norm and the (cid:96)p norm, and by diag(.) the operator
deﬁne by:

diag(X) is the main diagonal of the matrix X

diag(X) is the square matrix with the vector x as main
diagonal

Then if A = UΣVT the SVD of A

proxλ||.||p (A) = U diag(proxλ||.||p (diag(Σ)))VT

(50)

•

•

APPENDIX B
UPDATING THE BASES BY SUBSTITUTION
In the case of the degree three regularizers, we can use the
substitution method to tackle the non-smoothness of the
Frobenius norm and of the Nuclear norm, regardless of the
method used for the core tensor R. We present below the
derivation of the corresponding updates.

Let us introduce the auxiliary variables U and V for A

and B respectively. The constrained problem becomes

minA,B,U,V,K,R,E λ||R||1||B||F||A||F + λ||E||1
s.t

X = K ×1 U ×2 V + E
R = K
A = U
B = V

(51)

And the Augmented Lagrangian can easily be formu-
lated by introducing the Lagrange multipliers and the
penalty parameters YU, YV, µU, and µV.

We present the derivations for A only as the ones for B

are easily found by substituting the correct variables.

We have
A∗ = argmin

A

A

(cid:20) α
µU

[α||B||F||R||1] ||A||F + (cid:104)YU, A − U(cid:105) +

||A − U||2
F

= argmin

||B||F||R||1

||A||F +

||U −

(cid:21)

1
2

1
µU

YU − A||2
F

= prox α
µU

||B||F||R||1||.||F (U −

YU)

1
µU

µU
2
(52)

(53)

(54)

The update

obtained by derivation

U∗ = argmin

of U is
µ
2
µU
2

U

(cid:104)Λi, Xi − UKiVT − Ei(cid:105) +

(cid:88)

i

(cid:88)

||Xi − UKiVT − Ei||2

F+

i
||A − U||2

F + (cid:104)YU, A − U(cid:105)

Taking the derivative with respect to U and setting to 0

U∗(−

µ
µU

(cid:88)

i

KiVTVKT

i ) − U∗+

(cid:34)

1
µU

µUA + YU + µ

(Xi − Ei +

Λi)VKT
i

= 0

(cid:88)

i

1
µ

16

(56)

(55)

(cid:35)

APPENDIX C
PROOF
C.1 Deﬁnitions on integer sequences

Let us ﬁrst deﬁne in a clear manner the concepts of periodic
and group-periodic integer sequences.

Deﬁnition C.1. An integer sequence a1, a2, . . . , an is said to
be periodic with period p (i.e., p-periodic) if:

∀i ≥ 1, ai = ai+p

(57)

We will need to manipulate sequences of periodic pat-
terns of repeating number, for this purpose we deﬁne a
group periodic sequence as follows.

Deﬁnition C.2. We will
sequence
a1, a2, . . . , an is group periodic with group length l and
period p, or (l, p)-group periodic, if:

say an integer

∀i ≥ 1, ai = ai+pl

(58)

And:

∀k ≥ 0, ∀lk + 1 ≤ i, j ≤ l(k + 1), ai = aj

(59)

For instance, 1 1 0 0 2 2 1 1 0 0 2 2 is (2, 3)-group periodic.
Since the sum involves all N r2 elements of R it is clear
such an unfolding must too. Since there are respectively r,
r, and N columns in A, B, and IN and max(r, N, N r2) =
N r2, we have s = N r2.

We deﬁne a bijection from N3 to N such that each triple
index (i, j, k) is mapped to a unique positive integer l by
setting, in the general M -dimensional Tucker case:

l = 1 +

(ik − 1) Jk

and Lk =

Dm,

(60)

M
(cid:88)

k=1

k−1
(cid:89)

m=1

where Dm denotes the dimension along mode m. In our
case we have D1 = r, D2 = r, D3 = N . This re-indexing
corresponds to the standard deﬁnition of the vec operator
where the columns of each frontal slice of the 3-way tensor
are stacked in order.

We now describe the new factors of our decomposition:
The construction of ˜A and ˜B must be consistent with the
ordering of the elements in the re-indexing. For the reader’s
convenience, we visualize in Table 6 the correspondence in
the simple case where r = 2 and N = 3. To map back l to i,
j, k we observe the 3 sequences are (group) periodic. In fact,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

l
i
j
k

1
1

2
2

3
1

4
2

5
1

6
2

7
1

8
2

9
1

10
2

11
1

12
2

1

2

1

2

1

2

1

2

3

TABLE 6:
dimensional case.

Illustration of our re-indexing in a small-

the sequence corresponding to the innermost sum of (32) is
(r2, N )-group periodic. We ﬁnd that:

k = ((cid:100)

l
r2 (cid:101) − 1) mod N + 1

The sequence for j is (r, r)-group periodic and the sequence
for i is r-periodic, so we have:

j = ((cid:100)

(cid:101) − 1) mod r + 1

(62)

l
r

And:

i = (l − 1) mod r + 1

(63)
The augmented factors all have dimension N r2 over
their last mode. For ˜A we duplicate each column N r2 − r
times by concatenating N r copies of A on the column
dimension. For ˜B, each column is copied r − 1 times be-
fore stacking the next column, and the resulting matrix is
concatenated N times. The ˜cl are deﬁned by:

(˜cl)i = δi,((cid:100) l

r2 (cid:101)−1) mod N +1

The elemental mapping of our factorization is therefore:

φ(σ, ˜a, ˜b) = σ · ˜a ⊗ ˜b ⊗ δ.,((cid:100) l

r2 (cid:101)−1) mod N +1

The factorization function is directly obtained from the

elemental mapping.

(64)

(65)

APPENDIX D
LINEARIZED ADMM FOR SCALABILITY
The substitution method used in Section 3.4 is effective in
practice but comes with an additional cost that limits its
scalability. Notably, the cost of solving a Stein equation for
each frontal slice of R cannot be neglected. Additionally, the
added parameters can make tuning difﬁcult. Linearization
provides an alternative approach that can scale better to
large dimensions and large datasets, in this appendix, we
show how it can be applied to our models.

We give detailed derivations of the linearized updates
for A, B, and R. The development for R can directly
be applied to solving problem (21) and its variants with a
simple change in the shrinkage factor.

D.1 Overview

We brieﬂy remind the reader of the linearization method.
Provided an unconstrained optimization problem:

min
x

f (x) + g(x)

(66)

Where f is smooth and g is non-smooth and x ∈ Rd. We
replace f by the following quadratic approximation around
y:

ql(x, y) = f (y) + (cid:104)∇f (y), y − x(cid:105) +

||y − x||2
2

(67)

l
2

When ∇f is Lipschitz continuous with Lipschitz con-
stant L and l ≥ L the quadratic approximation is an upper
bound of f . If g has proximal operator proxg(.) then:

g(x) + ql(x, y)

= g(y) + f (y) + (cid:104)∇f (y), y − x(cid:105) +
(cid:18)

(cid:19)

= g(y) +

||x −

y −

∇f (y)

l
2

1
l

l
2

||y − x||2
2

||2
2 + f (y)

17

(68)

(69)

(70)

And argminx g(x) + ql(x, y) = proxg(y − 1
iterative algorithm, we choose x = xk+1 and y = xk.

l ∇f (y)). In an

We start from the augmented Lagrangian of problem (38)

(61)

with no auxiliary variables:

L(A, B, R, E, Λ, µ) = α||A||F||B||F||R||1+
λ||E||1 + (cid:104)Λ, X − R ×1 A ×2 B − E(cid:105)+
||X − R ×1 A ×2 B − E||2
F

µ
2

(71)

D.2 Updating the core R

From the augmented Lagrangian of problem (38), updating
R requires solving the minimization problem:

min
R

α||A||F||B||F||R||1+

µ
2

||X − R ×1 A ×2 B − E +

1
µ

Λ||2
F

(72)

Let ∆ = X − E + 1

µ Λ and δ = vec(∆), r = vec(R).
From the properties of the Kronecker product, vec(R ×1
A ×2 B) = (IN ⊗ B ⊗ A)vec(r). For the sake of brevity
we let Γ = IN ⊗ B ⊗ A. Thanks to the separability of the
(cid:96)1-norm penalty, using these notations (72) is equivalent to
the following vector minimization problem:

min
r

α
µ

||A||F||B||F||r||1 +

||Γr − δ||2
2

(73)

1
2

Vector calculus gives the following gradient for the

quadratic part:

∇r(r) =

||Γr − δ||2

2 = ΓT(Γr − δ)

(74)

∂
∂r

1
2

The computation of

the Lipschitz constant

is then

straightforward:

||∇r(r1) − ∇r(r2)||2 = ||ΓTΓ(r1 − r2)||2

≤ |||ΓTΓ||| · ||r1 − r2||2

Where |||.||| denotes the induced norm, i.e., the largest
singular value. We again make use of the properties of the
Kronecker product to ﬁnd:

|||ΓTΓ||| = |||(IN ⊗ B ⊗ A)T(IN ⊗ B ⊗ A)|||

N IN ⊗ BTB ⊗ ATA|||

= |||IT
= |||BTB||| · |||ATA|||
= σ2
max(A)

max(B) · σ2

(75)

(76)

(77)

(78)

(79)

(80)

In this case, we recommend direct calculation of the
constant instead of using a backtracking procedure because
the matrices A and B will often be of small size (if working
on small input data or if providing a small upper bound

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

r on the rank) and the computation can be performed by
partial SVD in all cases.

Finally, going back to tensor form the updated Rt+1 is:
(cid:19)

(cid:18)

Sα(cid:48)

Rt −

[(Rt ×1 A ×2 B) − ∆)] ×1 AT ×2 BT

1
LR

Where LR ≥ σ2
max(B) · σ2
(21) and α(cid:48) = α||A||F||B||F for problem (38).

(81)
max(A) and α(cid:48) = α for problem

D.3 For A and B
The sub-problem for A is:

min
A

α||A||F||B||F||R||1+

µ
2

||X − R ×1 A ×2 B − E +

1
µ

Λ||2
F

(82)

Our problem is separable in the frontal slices of X and
therefore in the frontal slices of the components of the fac-
torization and of the Lagrange multipliers. Using the same
∆ notation from the update of R and denoting Ci = RiBT,
we solve:

A∗ = argmin

||A||F +

||∆i − ACi||2
F

(83)

(cid:21)

(cid:20) α
µ

A

(cid:80)

We compute ∇A(A) = ∂
∂A
i(ACi − ∆i)CT
i .
The Lipschitz constant is:

(cid:80)

i ||∆i − ACi||2

F =

(cid:88)

1
2

i

1
2

(A2Ci − ∆i)CT

i ||F

(cid:88)

i

||∇A(A1) − ∇A(A2)||F
(A1Ci − ∆i)CT

= ||

(cid:88)

i −

i
(cid:88)

i

= ||

(A1 − A2)CiCT

i ||F

= ||(A1 − A2)

(cid:88)

CiCT

i ||F

i
≤ ||A1 − A2||F||

(cid:88)

CiCT

i ||F

i

(84)

(85)

(86)

(87)

(88)

From the sub-multiplicativity of Schatten norms.
Let a = 1
µLA

(α||Bt||F||Rt||1), we have:

At+1 = proxa||.||F (At −

(cid:88)

(AtCt

i − ∆t

i)CtT
i )

(89)

LA ≥ || (cid:80)

i CiCT

i ||F = (cid:80)

i ||F = (cid:80)

i ||RiBTBRT

i ||F.

1
LA

i
i ||CiCT

The sub-problem for B is:

min
B

α||A||F||B||F||R||1+

µ
2

||X − R ×1 A ×2 B − E +

1
µ

Λ||2
F

(90)

And similar to A the Lipschitz constant:

||∇B(B1) − ∇B(B2)||F
(B1GT

i − ∆T

= ||

(cid:88)

i )Gi −

(B2GT

i − ∆T

(92)
i )Gi||F (93)

(cid:88)

i

i
(cid:88)

i

= ||

(B1 − B2)GT

i Gi||F

= ||(B1 − B2)

i
≤ ||A1 − A2||F||

(cid:88)

GT

i Gi||F

GT

i Gi||F

(cid:88)

i

And:

18

(94)

(95)

(96)

1
LB

Bt+1 = proxb||.||F(Bt −

(cid:88)

(Gt

iBtT

− ∆t

i)TGt
i)

(97)

i GT

LB ≥ || (cid:80)

i Gi||F = (cid:80)

i
i ATARi||F.
i ||RT
Remarking that C = R ×2 B then LA = ||C[2]CT
[2]||F
where C[2] is the mode-2 matricization of C. The ith line of
C[2] is the concatenation of the ith columns of all the frontal
slices of C so C[2]CT
[2], the matrix of the dot products of
the lines of C[2], captures interactions across the ith rows
of all the input images. Conversely, G = permute(R ×1
A, [2, 1, 3]), so LB = ||G[2]GT
[2]||F where G[2] is the mode-2
matricization of G. The ith line of G[2] is the concatenation
of the ith lines of all the frontal slices of G so G[2]GT
[2],
the matrix of the dot products of the lines of G[2], captures
interactions across the ith columns of all the input images.

APPENDIX E
SCHATTEN NORMS AND THE KRONECKER PRODUCT

In this Section we prove the identity used in Section 2.2 on
the Schatten norm of a Kronecker product.

Let us ﬁrst remind the reader of the deﬁnition of the

Schatten-p norm:
Deﬁnition E.1. Let A a real-valued matrix, A ∈ Rm×n. The
Schatten-p norm of A is deﬁned as:
sp
i )1/p

||A||p = (

(cid:88)

Where si is the ith singular value of A.

i

We argued the result stems from the compatibility of the
Kronecker product with the singular value decomposition,
that is:
Proposition E.1. Let A = UAΣAVT
real-valued matrices given by their SVD, then:

A, B = UBΣBVT

B two

A ⊗ B = (UA ⊗ UB)(ΣA ⊗ ΣB)(VT

A ⊗ VT
B)

Similarly, using the same ∆ notation from the update
i = At+1Ri ∀i (Gi = ARi) and b =

of R and letting Gt

Proof. See [64].

1
µLB

(α||At+1||F||Rt||1) we ﬁnd:

B∗ = argmin

||B||F +

||∆i − GiBT||2
F

(91)

(cid:21)

(cid:20) α
µ

B

We compute ∇B(B) = ∂
∂B
i(BGT
i )Gi.

i − ∆T

(cid:80)

(cid:80)

i ||∆i − GiBT||2

F =

(cid:88)

1
2

i

1
2

The identity we used is formally expressed in Theorem

E.1, of which we give the proof for completeness.
Theorem E.1. Let A ∈ Rm×n and B ∈ Rp×q, then:

∀p > 0, ||A ⊗ B||p = ||A||p||B||p

Where ||.||p denotes the Schatten-p norm.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

Proof. From Proposition 1, the singular values of A ⊗ B are
the σA,iσB,j so:

||A ⊗ B||p = (

(σA,iσB,j)p)1/p

(cid:88)

i,j
(cid:88)

i,j
(cid:88)

i
(cid:88)

i

= (

= (

= (

A,iσp
σp

B,j)1/p

(cid:88)

σp
A,i

σp
B,j)1/p

j
σp
A,i)1/p(

σp
B,j)1/p

(cid:88)

j

= ||A||p||B||p

APPENDIX F
FURTHER IMPLEMENTATION DETAILS

F.1 Parameter tuning and adaptive ADMM

A key concern with the practical implementation of ADMM
and LADMM algorithms is parameter tuning and initializa-
tion, and is exacerbated by the introduction of additional
variables and constraints. A constraint of the form X = Y
translates to additional terms (cid:104)Λ, X − Y, +(cid:105) µ
2 ||X − Y||2
F
in the augmented Lagrangian of the problem, where Λ is
a tensor of Lagrange multipliers of the dimension of X ,
and µ is a penalty parameter. In standard (L)ADMM, µ is
initialized and updated such that the sequence (µt)t≥1 is
bounded and non-decreasing, generally through a simple
update µt+1 = ρµt. The initial value µ0 and the value of ρ
directly inﬂuence the speed of convergence and the quality
of the reconstructed images. As more constraints are added,
more penalty parameters are introduced and must be tuned
and updated properly, possibly independently from one
another. Although the literature on ADMM with adaptive
penalty updates [65], [66], [67] suggests adaptive updates
for the ρ variables, these solutions also come with their
added complexity and possibly with more parameters to
tune. In the case of LADMM, the practitioner must also
ensure an adequate choice of value is made to upper bound
the Lipschitz constants of the gradients in each update.

Due to the sensitivity of the ADMM and LADMM algo-
rithms to parameter tuning, we found that the algorithm
that provided the most beneﬁts in practice and the best
reconstructions was the one that was the simplest to tune.
This explains our choice of Algorithm 1 and its LADMM
variant for the experiments.

F.2 Convergence and initialization

Problems (21) and (38) are non-convex, therefore global
convergence is a priori not guaranteed. Recent work [68],
[69] studies the convergence of ADMM for non-convex
and possibly non-smooth objective functions with linear
constraints. Here, the constraints are not linear. We proposed
problem (21) based on [30], [31] so global convergence could
theoretically be attained with a local descent algorithm.
However, problem (21) doesn’t offer any guarantees and
in both cases the (L)ADMM scheme employed does not
necessarily converge to a local minimum. In this section, we

19

provide experimental results for Algorithm 1 and discuss
the initialization strategy implemented for all the variants.

(cid:80)

(cid:80)

ηN
i ||Xi||F

We propose a simple initialization scheme for Algo-
rithm 1 and its variants adapted from [45]. We initialize
the bases A and B and the core R by performing SVD
on each observation Xi = UiSiVT
i . We set Ri = Si,
(cid:80)
i Ui and B = 1
A = 1
i Vi. To initialize the dual-
N
N
variables for the constraint Xi − ARiBT − Ei = 0, we take
µ0 =
where η is a scaling coefﬁcient, chosen in
ηN
practice to be η = 1.25 as in [45]. We chose µ0
i ||Ri||F
and similarly for A and B when applicable. These corre-
spond to averaging the initial values for each individual
slice and its corresponding constraint. Our convergence
criterion corresponds to primal-feasibility of problem (26)
or (38), and is given by max(errrec, errR, errA, errB) ≤
(cid:15) where errrec = maxi
and errR =

K =

(cid:80)

F

||B−V||2
F
||B||2
F

||Ri−Ki||2
F
||Ri||2
F

, errA = ||A−U||2
maxi
. For the
||A||2
F
LADMM versions we only have max(errrec). Empirically,
we obtained systematic convergence of Algorithm 1 to a
good solution, and a linear convergence rate, as shown in
Figure 4. Similar results were found for the ADMM with
substitution algorithm for problem (38) and for the LADMM
variants of both problems.

||Xi−ARiBT−Ei||2
F
||Xi||2
F
, and

APPENDIX G
DEEP IMAGE PRIOR
We tried two different network architectures presented in
the paper and in the denoising example code provided by
the authors5, abbreviated as DIP 1 and DIP 2. In both DIP
implementations, we used an auto-encoder structure with
skip connections.

We used as input 128 random noise input channels for
Yale database with an output of the 64 images of the person
in the Yale database. That way, all the information from all
the facial images was used.

For Facade we used 64 input channels and 3 outputs. In
DIP 1 the number channels down was 5 × 128, while in DIP
2 [8, 16, 32, 64, 128].

In DIP 1 the numbers of channels up were 5 × 128, while
in DIP [128, 64, 32, 16, 8]. In DIP 1 the number of channels
per skip connection was 4, while in DIP 2 was [0, 0, 4, 4].

Bilinear upsampling was used in both cases with re-
ﬂection padding. Adam optimiser was used with learning
rate of 0.01. The structure of the input noise was the one
described in the paper [32].

APPENDIX H
ADDITIONAL EXPERIMENTAL RESULTS
In this Section we provide additional information on the
results reported in the paper, as well as supporting material.

H.1 Additional denoising results on Yale

This section presents supplementary denoising results. In
addition to the ﬁrst illumination, we show the reconstruc-
tion obtained on the third illumination, and include the
output of all algorithms.

5. https://github.com/DmitryUlyanov/deep-image-

prior/blob/661c4f45f2416604b3afb43afc5003213e9865e7/denoising.ipynb

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

20

RKCA
Cauchy ST
TRPCA 2016
HORPCA-S
Welsh ST
NCTRPCA
TRPCA 2014
RPCA (baseline)
RNNDL (baseline)
LRR Exact (baseline)
LRR Inexact (baseline)
DIP Model 1
DIP Model 2

10.00%

30.00%

60.00%

PSNR
36.1887
36.8013
35.1450
34.3085
31.6702
23.6362
19.1429
30.1698
20.0746
19.9762
20.1770
27.2166
25.1907

FSIM
0.9878
0.9826
0.9779
0.9698
0.9539
0.8839
0.7902
0.9440
0.8503
0.8058
0.7565
0.9016
0.8796

PSNR
32.7931
30.7079
28.6409
28.6301
25.8976
22.5348
17.1359
25.7667
17.1977
19.5246
17.4091
22.4471
21.4762

FSIM
0.9509
0.9424
0.9299
0.9179
0.9008
0.8816
0.7627
0.8954
0.8087
0.8044
0.6855
0.8555
0.8588

PSNR
27.1490
20.8269
22.5660
22.1410
16.5715
22.8502
15.0052
17.9296
12.9945
18.3642
15.8379
17.8652
16.3906

FSIM
0.8913
0.8131
0.8427
0.8115
0.7335
0.8509
0.7201
0.7234
0.6391
0.7787
0.6576
0.7661
0.8180

TABLE 7: Numerical values of the image quality metrics on the Yale-B benchmark for the three noise levels. Best
performance in bold.

(a) Noisy

(b) Original

(c) Cauchy ST

(d) Welsh ST

(a) Noisy

(b) Original

(c) Cauchy ST

(d) Welsh ST

(e) TRPCA ’14 (f) TRPCA ’16 (g) HORPCA-S (h) NCTRPCA

(e) TRPCA ’14 (f) TRPCA ’16 (g) HORPCA-S (h) NCTRPCA

(i) RNNDL

(j) RPCA

(k) LRRe

(l) LRRi

(i) RNNDL

(j) RPCA

(k) LRRe

(l) LRRi

(m) DIP1

(n) DIP2

(o) RKCA

(m) DIP1

(n) DIP2

(o) RKCA

Fig. 11: Comparative performance on the Yale benchmark
with 10% salt & pepper noise - ﬁrst illumination.

Fig. 12: Comparative performance on the Yale benchmark
with 10% salt & pepper noise - third illumination.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

21

(a) Noisy

(b) Original

(c) Cauchy ST

(d) Welsh ST

(a) Noisy

(b) Original

(c) Cauchy ST

(d) Welsh ST

(e) TRPCA ’14 (f) TRPCA ’16 (g) HORPCA-S (h) NCTRPCA

(e) TRPCA ’14 (f) TRPCA ’16 (g) HORPCA-S (h) NCTRPCA

(i) RNNDL

(j) RPCA

(k) LRRe

(l) LRRi

(i) RNNDL

(j) RPCA

(k) LRRe

(l) LRRi

(m) DIP1

(n) DIP2

(o) RKCA

(m) DIP1

(n) DIP2

(o) RKCA

Fig. 13: Comparative performance on the Yale benchmark
with 30% salt & pepper noise - ﬁrst illumination.

Fig. 14: Comparative performance on the Yale benchmark
with 30% salt & pepper noise - third illumination.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

22

(a) Noisy

(b) Original

(c) Cauchy ST

(d) Welsh ST

(a) Noisy

(b) Original

(c) Cauchy ST

(d) Welsh ST

(e) TRPCA ’14 (f) TRPCA ’16 (g) HORPCA-S (h) NCTRPCA

(e) TRPCA ’14 (f) TRPCA ’16 (g) HORPCA-S (h) NCTRPCA

(i) RNNDL

(j) RPCA

(k) LRRe

(l) LRRi

(i) RNNDL

(j) RPCA

(k) LRRe

(l) LRRi

(m) DIP1

(n) DIP2

(o) RKCA

(m) DIP1

(n) DIP2

(o) RKCA

Fig. 15: Comparative performance on the Yale benchmark
with 60% salt & pepper noise - ﬁrst illumination.

Fig. 16: Comparative performance on the Yale benchmark
with 60% salt & pepper noise - third illumination.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

H.2 Additional denoising results on Facade

We present the full denoising results at the 10% and 60%
noise level, as well as the 30% level for completeness.

23

(a) Original

(b) Noisy

(c) Cauchy ST

(d) Welsh ST

(a) Original

(b) Noisy

(c) Cauchy ST

(d) Welsh ST

(e) TRPCA ’14 (f) TRPCA ’16 (g) HORPCA-S (h) NCTRPCA

(e) TRPCA ’14 (f) TRPCA ’16 (g) HORPCA-S (h) NCTRPCA

(i) RNNDL

(j) RPCA

(k) LRRe

(l) LRRi

(i) RNNDL

(j) RPCA

(k) LRRe

(l) LRRi

(m) DIP1

(n) DIP2

(o) RKCA

Fig. 17: Comparative performance on the Facade benchmark
with 10% noise.

(m) DIP1

(n) DIP2

(o) RKCA

Fig. 18: Comparative performance on the Facade benchmark
with 30% noise.

(a) Original

(b) Noisy

(c) Cauchy ST

(d) Welsh ST

(e) TRPCA ’14 (f) TRPCA ’16 (g) HORPCA-S (h) NCTRPCA

(i) RNNDL

(j) RPCA

(k) LRRe

(l) LRRi

(m) DIP1

(n) DIP2

(o) RKCA

Fig. 19: Comparative performance on the Facade benchmark
with 60% noise.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

24

RKCA
Cauchy ST
TRPCA 2016
HORPCA-S
Welsh ST
NCTRPCA
TRPCA 2014
RPCA (baseline)
RNNDL (baseline)
LRR Exact (baseline)
LRR Inexact (baseline)
DIP Model 1
DIP Model 2

10.00%

30.00%

60.00%

PSNR
33.7184
31.7107
32.7462
33.1370
28.0183
27.8527
31.8137
21.3266
21.1150
19.0759
16.4344
24.5215
24.1887

FSIMc
0.9960
0.9924
0.9955
0.9938
0.9725
0.9744
0.9886
0.9404
0.9263
0.8929
0.8830
0.9211
0.9181

PSNR
28.6235
29.0492
27.0979
27.5200
25.2946
21.2992
25.9989
13.6751
14.2439
19.2146
15.8606
20.4301
21.4694

FSIMc
0.9764
0.9724
0.9764
0.9714
0.9309
0.8881
0.9484
0.8145
0.8041
0.9001
0.8776
0.8385
0.8612

PSNR
24.4491
22.3150
23.6552
22.8811
19.8508
19.4207
21.4069
9.2293
10.0129
19.0804
15.7061
17.2867
18.5480

FSIMc
0.9179
0.8820
0.9109
0.9060
0.8421
0.8121
0.8529
0.6633
0.6690
0.8993
0.8762
0.7258
0.7410

TABLE 8: Numerical values of the image quality metrics on the Facade benchmark for the three noise levels. Best
performance in bold.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

H.3 Background subtraction

In this Section we present the DET curves obtained on
the Highway and Hall experiments, and the backgrounds
obtained with each algorithm on the Highway benchmark.

25

Fig. 20: DET curves on the Highway dataset.

Fig. 21: DET curves on the Airport Hall dataset.

(a) Original

(b) GT

(c) Cauchy ST

(d) Welsh ST (e) TRPCA ’14 (f) TRPCA ’16 (g) HORPCA-S

(h) NCTRPCA (i) RNNDL

(j) RPCA

(k) LRRe

(l) LRRi

(m) RKCA

Fig. 22: Background subtraction results on Highway.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

26

(a) First
10%

face,

(b) First
30%

face,

(c) First
60%

face,

(d)
face 1

Original

(a) First
10%

face,

(b) First
30%

face,

(c) First
60%

face,

(d)
face 1

Original

(e) Third face,
10%

(f) Third face,
30%

(g) Third face,
60%

(h)
face 3

Original

(e) Third face,
10%

(f) Third face,
30%

(g) Third face,
60%

(h)
face 3

Original

Fig. 23: Best reconstructions obtained with SeDiL on the ﬁrst
and third illumination according to the best overall PSNR
score.

Fig. 24: Best reconstructions obtained with SeDiL on the ﬁrst
and third illumination according to the best overall FSIM
score.

H.4 Comparison with SeDiL

We present an experiment to assess the performance of
SeDiL on the Yale-B benchmark. We chose this experiment
because - as presented in the original paper [18] - SeDiL is
designed for denoising grayscale images.

H.4.1 Design

The data and the noise are the same as in the Yale-B
experimental results presented in the paper. We describe the
tuning procedure employed.

We tuned in SeDiL: κ and λ for training the dictionary,
and the parameter used in FISTA for denoising that we shall
denote as λF . In the original paper, the authors choose κ =
λ = 0.1
4w2 with w the dimension of the square image patches,
and λF = σnoise
100 . We tuned the parameters via grid-search,
choosing:

• κ = κ0
• λ = λ0
• λF ∈ 5 ∗ logspace(−4, 0, 15)

4w2 , κ0 ∈ linspace(0.05, 0.5, 5)
4w2 , λ0 ∈ linspace(0.05, 0.5, 5)

We kept ρ = 100 and patch sizes of 8 × 8 and extracted
40000 random patches for training from the 64 images of
the ﬁrst subject. We then followed the procedure described
in the paper for denoising and extracted 8 × 8 patches in a
sliding window with step size 1, denoised the patches with
FISTA, and reconstructed the denoised image by averaging
the denoised patches.

H.4.2 Results
We present the results at the three noise levels: 10%, 30%,
60%. We report the mean PSNR and FSIM on the 64 images,
the PSNR and FSIM on the ﬁrst image, and show the
reconstructions obtained.

H.4.3 Comments

As expected, the method is not robust to gross corruption.
In all cases, the best results were obtained for κ0 = λ0 = 0.5
and λF = 5, indicative of the lack of robustness.

APPENDIX I
TIME INFLUENCE OF THE MODEL PARAMETERS

To investigate the impact of the dimensions of the data
and of the model parameters we devise an experiment
on synthetic data (of the same kind used to validate our
approach in Section 7.1 of the paper).

Assuming a dataset of N random low-rank matrices Xi
of dimension m × n, and the RKCA model with degree 2
regularisation, we vary independently:

1) The ﬁrst dimension m of the data (number of rows
per observation, or height of the images) in [50, 100,
150, 200, 250, 500, 750, 1000] for N = 100 (Figure 25)
2) The second dimension n (number of columns, or
width of the images) in [50, 100, 150, 200, 250, 500,
750, 1000] for N = 100 (Figure 26)

3) The number of observations N in [10, 25, 50, 100,
150, 200, 250, 500, 750, 1000] for m = n = 100
(Figure 28)

4) The upper bound we place on the rank of each
reconstruction r in [5, 10, 15, 20, 25, 30, 50, 75,
100, 150, 200, 500, 750, 1000] for N = 100 and
m = n = 1000 (Figure 27)

We set r = 50 for the experiments on the data dimensions,
and we let λ =
.

1√

N max(n,m)

We set the convergence threshold to 1e − 5. All exper-
iments were conducted in isolation on a Standard E8s v3
instance in Microsoft Azure Cloud.

We verify experimentally the derivations made for the
time complexity. We see that the time is linearly dependent
on each dimension of the observations as well as on the
number of observations. The dependency on the upper
bound on the rank is quadratic, as claimed in Section 6 of the
paper. All runs reached convergence at the 1e−5 threshold.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
Best PSNR face 1
Noise level
23.937344
10%
20.988780
30%
17.480783
60%

Best mean PSNR
23.863287
19.249498
16.095463

Best FSIM face 1
0.840798
0.785824
0.703943

Best mean FSIM
0.862061
0.807403
0.778116

27

TABLE 9: Best PSNR and FSIM obtained with SeDiL on the ﬁrst face and averaged on the 64 faces at the 3 noise levels.

Fig. 25: Experimental veriﬁcation of the linear impact of
the ﬁrst data dimension m on the run-time per iteration of
RKCA with degree 2 regularisation. Left: time per iteration
in seconds, right: number of iterations to convergence at the
1e − 5 threshold.

Fig. 28: Experimental veriﬁcation of the linear impact of
the number of tensor observations N on the run-time per
iteration of RKCA with degree 2 regularisation. Left: time
per iteration in seconds, right: number of iterations to con-
vergence at the 1e − 5 threshold.

Fig. 26: Experimental veriﬁcation of the linear impact of the
second data dimension n on the run-time per iteration of
RKCA with degree 2 regularisation. Left: time per iteration
in seconds, right: number of iterations to convergence at the
1e − 5 threshold.

use of optimised compiled C, C++, or Fortran extensions, or
fast SVD implementations, to speed up speciﬁc bottlenecks.
This makes the comparison unreliable as most other codes
are not written with performance in mind.

For indicative purposes, we compare the total run-time
of the different algorithms, including Low-rank Representa-
tion with Inexact ALM [17] added for this revision.

Assuming a dataset of N random low-rank matrices Xi
of dimension n × n, and the RKCA model with degree 2
regularisation, we vary independently:

1) The number of tensor observations N in [10, 25, 50,
100, 150, 200, 250, 500, 750, 1000] for n = 100 (Figure
29)

2) The number of elements per observation n2 with n
in [15, 25, 50, 100, 150, 200, 250, 500, 750, 1000] for
N = 100 (Figure 30)

We choose square observations of size m2. For RKCA, we
test both r = n (RKCA) and r = 15 ﬁxed (RKCA Fixed rank).

Fig. 27: Experimental veriﬁcation of the quadratic impact
of the rank upper bound r on the run-time per iteration of
RKCA with degree 2 regularisation. Left: time per iteration
in seconds, right: number of iterations to convergence at the
1e − 5 threshold.

APPENDIX J
RUNTIME COMPARISON

We do not provide actual run-time values in the paper as we
do not intend to benchmark the different implementations
of the methods tested. Some of the implementations pro-
vided by the authors of the algorithms we compared make

Fig. 29: Runtime as a function of the number of observations.

We can see the importance of a-priory knowledge on
the rank of the latent low-rank tensor: the informed model
(RKCA Fixed rank) has the third fastest run-time for the

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

28

Fig. 30: Runtime as a function of the size of the observations.

three largest number of observations, and closely matches
competing implementations in the large-observations test.
It is to be noted that the fastest methods, Cauchy ST and
Welsh ST use fast linearized iterative schemes and rely on
MATLAB’s optimisation toolbox.

