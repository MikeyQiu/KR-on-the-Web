7
1
0
2
 
v
o
N
 
2
2
 
 
]

G
L
.
s
c
[
 
 
1
v
7
6
2
8
0
.
1
1
7
1
:
v
i
X
r
a

GraphGAN: Graph Representation Learning with Generative Adversarial Nets

Hongwei Wang1,2, Jia Wang3, Jialin Wang4,3, Miao Zhao3,
Weinan Zhang1, Fuzheng Zhang2, Xing Xie2 and Minyi Guo1∗
1Shanghai Jiao Tong University, wanghongwei55@gmail.com, {wnzhang, myguo}@sjtu.edu.cn
2Microsoft Research Asia, {fuzzhang, xing.xie}@microsoft.com
3The Hong Kong Polytechnic University, {csjiawang, csmiaozhao}@comp.polyu.edu.hk
4Huazhong University of Science and Technology, wangjialin@hust.edu.cn

Abstract

The goal of graph representation learning is to embed each
vertex in a graph into a low-dimensional vector space. Exist-
ing graph representation learning methods can be classiﬁed
into two categories: generative models that learn the under-
lying connectivity distribution in the graph, and discrimina-
tive models that predict the probability of edge existence be-
tween a pair of vertices. In this paper, we propose Graph-
GAN, an innovative graph representation learning framework
unifying above two classes of methods, in which the genera-
tive model and discriminative model play a game-theoretical
minimax game. Speciﬁcally, for a given vertex, the genera-
tive model tries to ﬁt its underlying true connectivity distri-
bution over all other vertices and produces “fake” samples to
fool the discriminative model, while the discriminative model
tries to detect whether the sampled vertex is from ground truth
or generated by the generative model. With the competition
between these two models, both of them can alternately and
iteratively boost their performance. Moreover, when consid-
ering the implementation of generative model, we propose
a novel graph softmax to overcome the limitations of tradi-
tional softmax function, which can be proven satisfying de-
sirable properties of normalization, graph structure aware-
ness, and computational efﬁciency. Through extensive exper-
iments on real-world datasets, we demonstrate that Graph-
GAN achieves substantial gains in a variety of applications,
including link prediction, node classiﬁcation, and recommen-
dation, over state-of-the-art baselines.

Introduction
Graph representation learning, also known as network em-
bedding, aims to represent each vertex in a graph (network)
as a low-dimensional vector, which could facilitate tasks
of network analysis and prediction over vertices and edges.
Learned embeddings are capable to beneﬁt a wide range of
real-world applications such as link prediction (Gao, De-
noyer, and Gallinari 2011), node classiﬁcation (Tang, Ag-
garwal, and Liu 2016), recommendation (Yu et al. 2014),
visualization (Maaten and Hinton 2008), knowledge graph
representation (Lin et al. 2015), clustering (Tian et al. 2014),
text embedding (Tang, Qu, and Mei 2015), and social net-
work analysis (Liu et al. 2016). Recently, researchers have

∗M. Guo is the corresponding author.

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

examined applying representation learning methods to var-
ious types of graphs, such as weighted graphs (Grover and
Leskovec 2016), directed graphs (Zhou et al. 2017), signed
graphs (Wang et al. 2017b), heterogeneous graphs (Wang et
al. 2018), and attributed graphs (Huang, Li, and Hu 2017).
In addition, several prior works also try to preserve spe-
ciﬁc properties during the learning process, such as global
structures (Wang, Cui, and Zhu 2016), community structures
(Wang et al. 2017c), group information (Chen, Zhang, and
Huang 2016), and asymmetric transitivity (Ou et al. 2016).
Arguably, most existing methods of graph representation
learning can be classiﬁed into two categories. The ﬁrst is
generative graph representation learning model (Perozzi,
Al-Rfou, and Skiena 2014; Grover and Leskovec 2016;
Zhou et al. 2017; Dong, Chawla, and Swami 2017; Li et al.
2017a). Similar to classic generative models such as Gaus-
sian Mixture Model (Lindsay 1995) or Latent Dirichlet Al-
location (Blei, Ng, and Jordan 2003), generative graph rep-
resentation learning models assume that, for each vertex
vc, there exists an underlying true connectivity distribution
ptrue(v|vc), which implies vc’s connectivity preference (or
relevance distribution) over all other vertices in the graph.
The edges in the graph can thus be viewed as observed sam-
ples generated by these conditional distributions, and these
generative models learn vertex embeddings by maximizing
the likelihood of edges in the graph. For example, DeepWalk
(Perozzi, Al-Rfou, and Skiena 2014) uses random walk to
sample “context” vertices for each vertex, and tries to maxi-
mize the log-likelihood of observing context vertices for the
given vertex. Node2vec (Grover and Leskovec 2016) further
extends the idea by proposing a biased random walk proce-
dure, which provides more ﬂexibility when generating the
context for a given vertex.

The second kind of graph representation learning method
is the discriminative model (Wang et al. 2018; Cao, Lu, and
Xu 2016; Wang, Cui, and Zhu 2016; Li et al. 2017b). Differ-
ent from generative models, discriminative graph represen-
tation learning models do not treat edges as generated from
an underlying conditional distribution, but aim to learn a
classiﬁer for predicting the existence of edges directly. Typ-
ically, discriminative models consider two vertices vi and vj
jointly as features, and predict the probability of an edge ex-
isting between the two vertices, i.e., p(cid:0)edge|(vi, vj)(cid:1), based
on the training data in the graph. For instance, SDNE (Wang,

Cui, and Zhu 2016) uses the sparse adjacency vector of ver-
tices as raw features for each vertex, and applies an autoen-
coder to extract short and condense features for vertices un-
der the supervision of edge existence. PPNE (Li et al. 2017b)
directly learns vertex embeddings with supervised learning
on positive samples (connected vertex pairs) and negative
samples (disconnected vertex pairs), also preserving the in-
herent properties of vertices during the learning process.

Although generative and discriminative models are gen-
erally two disjoint classes of graph representation learn-
ing methods, they can be considered two sides of the same
coin (Wang et al. 2017a). In fact, LINE (Tang et al. 2015)
has done a preliminary trial on implicitly combining these
two objectives (the ﬁrst-order and second-order proximity,
as called in LINE). Recently, Generative Adversarial Nets
(GAN) (Goodfellow et al. 2014) have received a great deal
of attention. By designing a game-theoretical minimax game
to combine generative and discriminative models, GAN and
its variants achieve success in various applications, such as
image generation (Denton et al. 2015), sequence generation
(Yu et al. 2017), dialogue generation (Li et al. 2017c), infor-
mation retrieval (Wang et al. 2017a), and domain adaption
(Zhang, Barzilay, and Jaakkola 2017).

Inspired by GAN, in this paper we propose GraphGAN,
a novel framework that uniﬁes generative and discrimina-
tive thinking for graph representation learning. Speciﬁcally,
we aim to train two models during the learning process of
GraphGAN: 1) Generator G(v|vc), which tries to ﬁt the un-
derlying true connectivity distribution ptrue(v|vc) as much
as possible, and generates the most likely vertices to be con-
nected with vc; 2) Discriminator D(v, vc), which tries to
distinguish well-connected vertex pairs from ill-connected
ones, and calculates the probability of whether an edge ex-
ists between v and vc. In the proposed GraphGAN, the gen-
erator G and the discriminator D act as two players in a
minimax game: the generator tries to produce the most indis-
tinguishable “fake” vertices under guidance provided by the
discriminator, while the discriminator tries to draw a clear
line between the ground truth and “counterfeits” to avoid be-
ing fooled by the generator. Competition in this game drives
both of them to improve their capability, until the generator
is indistinguishable from the true connectivity distribution.
Under the GraphGAN framework, we study the choices
of generator and discriminator. Unfortunately, we ﬁnd that
the traditional softmax function (and its variants) is not suit-
able for the generator for two reasons: 1) softmax treats all
other vertices in the graph equally for a given vertex, lacking
the consideration on graph structure and proximity informa-
tion; 2) the calculation of softmax involves all vertices in the
graph, which is time-consuming and computationally inefﬁ-
cient. To overcome these limitations, in GraphGAN we pro-
pose a new implementation of generator called Graph Soft-
max. Graph softmax provides a new deﬁnition of connec-
tivity distribution in a graph. We prove graph softmax satis-
fying desirable properties of normalization, graph structure
awareness, and computational efﬁciency. Accordingly, we
propose a random-walk-based online generating strategy for
generator, which is consistent with the deﬁnition of graph
softmax and can greatly reduce computation complexity.

Empirically, we apply GraphGAN to three real-world sce-
narios, i.e., link prediction, node classiﬁcation, and recom-
mendation, using ﬁve real-world graph-structured datasets.
The experiment results show that GraphGAN achieves sub-
stantial gains compared with state-of-the-art baselines in the
ﬁeld of graph representation learning. Speciﬁcally, Graph-
GAN outperforms baselines by 0.59% to 11.13% in link
prediction and by 0.95% to 21.71% in node classiﬁcation
both on Accuracy. Additionally, GraphGAN improves Pre-
cision@20 by at least 38.56% and Recall@20 by at least
52.33% in recommendation. We attribute the superiority of
GraphGAN to its uniﬁed adversarial learning framework as
well as the design of the proximity-aware graph softmax that
naturally captures structural information from graphs.

Graph Generative Adversarial Nets
In this section, we introduce the framework of GraphGAN
and discuss the details of implementation and optimization
of the generator and the discriminator. We then present the
graph softmax implemented as the generator, and prove its
superior properties over the traditional softmax function.

GraphGAN Framework
We formulate the generative adversarial nets for graph rep-
resentation learning as follows. Let G = (V, E) be a given
graph, where V = {v1, ..., vV } represents the set of vertices
and E = {eij}V
i,j=1 represents the set of edges. For a given
vertex vc, we deﬁne N (vc) as the set of vertices directly
connected to vc, the size of which is typically much smaller
than the total number of vertices V . We denote the under-
lying true connectivity distribution for vertex vc as condi-
tional probability ptrue(v|vc), which reﬂects vc’s connectiv-
ity preference distribution over all other vertices in V. From
this point of view, N (vc) can be seen as a set of observed
samples drawn from ptrue(v|vc). Given the graph G, we aim
to learn the following two models:

Generator G(v|vc; θG), which tries to approximate the
underlying true connectivity distribution ptrue(v|vc), and
generates (or selects, if more precise) the most likely ver-
tices to be connected with vc from vertex set V.

Discriminator D(v, vc; θD), which aims to discriminate
the connectivity for the vertex pair (v, vc). D(v, vc; θD) out-
puts a single scalar representing the probability of an edge
existing between v and vc.

Generator G and discriminator D act as two opponents:
generator G would try to ﬁt ptrue(v|vc) perfectly and gen-
erate relevant vertices similar to vc’s real immediate neigh-
bors to deceive the discriminator, while discriminator D, on
the contrary, would try to detect whether these vertices are
ground-truth neighbors of vc or the ones generated by its
counterpart G. Formally, G and D are playing the following
two-player minimax game with value function V (G, D):

min
θG

max
θD

V (G, D) =

V
(cid:88)

(cid:16)

c=1

Ev∼ptrue(·|vc)

(cid:2) log D(v, vc; θD)(cid:3)

+ Ev∼G(·|vc;θG)

(cid:2) log (cid:0)1 − D(v, vc; θD)(cid:1)(cid:3)(cid:17)

.

(1)

c=1

V
(cid:88)

N
(cid:88)

c=1

i=1

V
(cid:88)

N
(cid:88)

c=1

i=1

V
(cid:88)

c=1

=

=

=

other words, the generator shifts its approximated connec-
tivity distribution (through its parameters θG) to increase the
scores of its generated samples, as judged by D. Because the
sampling of v is discrete, following (Schulman et al. 2015;
Yu et al. 2017), we propose computing the gradient of
V (G, D) with respect to θG by policy gradient:

∇θGV (G, D)
V
(cid:88)

=∇θG

Ev∼G(·|vc)

(cid:2) log (cid:0)1 − D(v, vc)(cid:1)(cid:3)

∇θG G(vi|vc) log (cid:0)1 − D(vi, vc)(cid:1)

G(vi|vc)∇θG log G(vi|vc) log (cid:0)1 − D(vi, vc)(cid:1)

Ev∼G(·|vc)

(cid:2)∇θG log G(v|vc) log (cid:0)1 − D(v, vc)(cid:1)(cid:3).

(4)

To understand the above formula, it is worth noting that
gradient ∇θGV (G, D) is an expected summation over the
gradients ∇θG log G(v|vc; θG) weighted by log-probability
log (cid:0)1−D(v, vc; θD)(cid:1), which, intuitively speaking, indicates
that vertices with a higher probability of being negative sam-
ples will “tug” generator G stronger away from themselves,
since we apply gradient descent on θG.

We now discuss the implementation of G. A straightfor-
ward way is to deﬁne the generator as a softmax function
over all other vertices (Wang et al. 2017a), i.e.,

G(v|vc) =

(cid:80)

exp(g(cid:62)

v gvc )
exp(g(cid:62)

v gvc)

,

(5)

v(cid:54)=vc
where gv, gvc ∈ Rk are the k-dimensional representation
vectors of vertex v and vc respectively for generator G, and
θG is the union of all gv’s. Under this setting, to update θG
in each iteration, we calculate the approximated connectiv-
ity distribution G(v|vc; θG) based on Eq. (5), draw a set of
samples (v, vc) randomly according to G, and update θG by
stochastic gradient descent. Softmax provides a concise and
intuitive deﬁnition for the connectivity distribution in G, but
it has two limitations in graph representation learning: 1)
The calculation of softmax in Eq. (5) involves all vertices
in the graph, which implies that for each generated sample
v, we need to calculate gradients ∇θG log G(v|vc; θG) and
update all vertices. This is computationally inefﬁcient, es-
pecially for real-world large-scale graphs with millions of
vertices. 2) The graph structure encodes rich information of
proximity among vertices, but softmax completely ignores
the utilization of structural information from graphs as it
treats vertices without any discrimination. Recently, hierar-
chical softmax (Morin and Bengio 2005) and negative sam-
pling (Mikolov et al. 2013) are popular alternatives to soft-
max. Although these methods can alleviate the computation
to some extent, neither of them considers structural informa-
tion of a graph, thereby being unable to achieve satisfactory
performance when applied to graph representation learning.

Figure 1: Illustration of GraphGAN framework.

Based on Eq. (1), the optimal parameters of the genera-
tor and the discriminator can be learned by alternately max-
imizing and minimizing the value function V (G, D). The
GraphGAN framework is illustrated as shown in Figure 1. In
each iteration, discriminator D is trained with positive sam-
ples from ptrue(·|vc) (vertices in green) and negative sam-
ples from generator G(·|vc; θG) (vertices with blue stripes),
and generator G is updated with policy gradient under the
guidance of D (detailed later in this section). Competition
between G and D drives both of them to improve their meth-
ods until G is indistinguishable from the true connectivity
distribution. We discuss the implementation and optimiza-
tion of D and G as follows.

Discriminator Optimization
Given positive samples from true connectivity distribution
and negative samples from the generator, the objective for
the discriminator is to maximize the log-probability of as-
signing the correct labels to both positive and negative sam-
ples, which could be solved by stochastic gradient ascent if
D is differentiable with respect to θD. In GraphGAN, we
deﬁne D as the sigmoid function of the inner product of two
input vertices:

D(v, vc) = σ(d(cid:62)

v dvc ) =

1

1 + exp(−d(cid:62)

v dvc)

,

(2)

where dv, dvc ∈ Rk are the k-dimensional representation
vectors of vertices v and vc respectively for discriminator D,
and θD is the union of all dv’s. Any discriminative model
can serve as D here such as SDNE (Wang, Cui, and Zhu
2016), and we leave the further study of choice of discrim-
inator for future work. Note that Eq. (2) simply involves v
and vc, which indicates that given a sample pair (v, vc), we
need to update only dv and dvc by ascending the gradient
with respect to them:

∇θD V (G, D) =

(cid:26)∇θD log D(v, vc), if v ∼ ptrue;
∇θD

(cid:0)1 − log D(v, vc)(cid:1), if v ∼ G.

(3)

Generator Optimization
In contrast to discriminator, the generator aims to mini-
mize the log-probability that the discriminator correctly as-
signs negative labels to the samples generated by G. In

Graph Softmax for Generator
To address the aforementioned problems, in GraphGAN we
propose a new alternative to softmax for the generator called
graph softmax. The key idea of graph softmax is to deﬁne a
new method of computing connectivity distribution in gen-
erator G(·|vc; θG) that satisﬁes the following three desirable
properties:
• Normalized. The generator should produce a valid proba-

bility distribution, i.e., (cid:80)

G(v|vc; θG) = 1.

v(cid:54)=vc
• Graph-structure-aware. The generator should take advan-
tage of the structural information of a graph to approxi-
mate the true connectivity distribution. Intuitively, for two
vertices in a graph, their connectivity probability should
decline with the increase of their shortest distance.

• Computationally efﬁcient. Distinguishable from full soft-
max, the computation of G(v|vc; θG) should only involve
a small number of vertices in the graph.
We discuss graph softmax in detail as follows. To calcu-
late the connectivity distribution G(·|vc; θG), we ﬁrst per-
form Breadth First Search (BFS) on the original graph G
starting from vertex vc, which provides us with a BFS-tree
Tc rooted at vc. Given Tc, we denote Nc(v) as the set of
neighbors of v (i.e., vertices that are directly connected to v)
in Tc, including its parent vertex and all child vertices if ex-
ist. For a given vertex v and one of its neighbors vi ∈ Nc(v),
we deﬁne the relevance probability of vi given v as

pc(vi|v) =

exp(g(cid:62)
vi
vj ∈Nc(v) exp(g(cid:62)
vj

gv)

(cid:80)

,

gv)

(6)

which is actually a softmax function over Nc(v). To calcu-
late G(v|vc; θG), note that each vertex v can be reached by
a unique path from the root vc in Tc. Denote the path as
Pvc→v = (vr0 , vr1, ..., vrm ) where vr0 = vc and vrm = v.
Then the graph softmax deﬁnes G(v|vc; θG) as follows:
pc(vrj |vrj−1)(cid:1) · pc(vrm−1|vrm ),

G(v|vc) (cid:44) (cid:0) (cid:89)m

(7)

j=1

where pc(·|·) is the relevance probability deﬁned in Eq. (6).
We prove that our proposed graph softmax satisﬁes the
above three properties, i.e., graph softmax is normalized,
graph-structure-aware, and computationally efﬁcient.
Theorem 1. (cid:80)

G(v|vc; θG) = 1 in graph softmax.

v(cid:54)=vc

Proof. Before proving the theorem, we ﬁrst give a proposi-
tion as follows. Denote STv as the sub-tree rooted at v in Tc
(v (cid:54)= vc). Then we have

(cid:88)

vi∈STv

G(vi|vc) =

pc(vrj |vrj−1),

(8)

(cid:89)m

j=1

where (vr0, vr1 , ..., vrm) is on the path Pvc→v, vr0 = vc
and vrm = v. The proposition can be proved by bottom-up
induction on the BFS-tree Tc:
• For each leaf vertex v, we have (cid:80)

G(vi|vc; θG) =
j=1 pc(vrj |vrj−1)(cid:1) · pc(vrm−1|vrm) =
G(v|vc; θG) = (cid:0) (cid:81)m
(cid:81)m
j=1 pc(vrj |vrj−1). The last step is due to the fact that
leaf vertex v has only one neighbor (parent vertex vrm−1 ),
therefore, pc(vrm−1|vrm) = pc(vrm−1 |v) = 1.

vi∈STv

• For each non-leaf vertex v, we denote Cc(v) as the set of
children vertices of v in Tc. By induction hypothesis, each
children vertex vk ∈ Cc(v) satisﬁes the proposition in Eq.
(8). Thus we have

G(vi|vc)
(cid:88)

(cid:88)

vk∈Cc(v)

vi∈STvk

pc(vrj |vrj−1 )(cid:1)pc(vrm−1|vrm)

G(vi|vc)

(cid:16)(cid:0) (cid:89)m

pc(vrj |vrj−1)(cid:1)pc(vk|vrm)

(cid:17)

vk∈Cc(v)

j=1

pc(vrj |vrj−1 )(cid:1)(cid:0)pc(vrm−1|v) +

(cid:88)

pc(vk|v)(cid:1)

vk∈Cc(v)

(cid:88)

vi∈STv

=G(v|vc) +
=(cid:0) (cid:89)m
(cid:88)

j=1

+

m
(cid:89)

=(cid:0)

j=1
(cid:89)m

=

pc(vrj |vrj−1).

j=1

So far we have proven Eq. (8). Applying Eq. (8) to all
children vertices of vc, we have (cid:80)
G(v|vc; θG) =
v(cid:54)=vc
G(v|vc; θG) = (cid:80)
(cid:80)
vk∈Cc(vc) pc(vk|vc)

vk∈Cc(vc)

(cid:80)

v∈STvk

= 1.

Theorem 2. In graph softmax, G(v|vc; θG) decreases expo-
nentially with the increase of the shortest distance between
v and vc in original graph G.

Proof. According to the deﬁnition of graph softmax,
G(v|vc; θG) is the product of m + 1 terms of relevance prob-
ability, where m is the length of path Pvc→v. Note that m is
also the shortest distance between vc and v in graph G, since
BFS-tree Tc preserves the shortest distances between vc and
all other vertices in the original graph. Therefore, we con-
clude that G(v|vc; θG) is exponentially proportional to the
inverse of the shortest distance between v and vc in G.

Following Theorem 2, we further justify that graph soft-
max characterizes the real pattern of connectivity distribu-
tion precisely by conducting an empirical study in the ex-
periment part.
Theorem 3. In graph softmax, calculation of G(v|vc; θG)
depends on O(d log V ) vertices, where d is average degree
of vertices and V is the number of vertices in graph G.

Proof. According to Eq. (6) and Eq. (7), the calculation of
G(v|vc; θG) involves two types of vertices: vertices on the
path Pvc→v and vertices directly connected to the path (i.e.,
vertices whose distance from the path is 1). In general, the
maximal length of the path is log V , which is the depth of
the BFS-tree, and each vertex in the path is connected to d
vertices on average. Therefore, the total number of involved
vertices in G(v|vc; θG) is O(d log V ).

Next, we discuss the generating (or sampling) strategy for
generator G. A feasible way of generating vertices is to cal-
culate G(v|vc; θG) for all vertices v (cid:54)= vc, and perform ran-
dom sampling proportionally to their approximated connec-
tivity probabilities. Here we propose an online generating
method, which is more computationally efﬁcient and con-
sistent with the deﬁnition of graph softmax. To generate a

Figure 2: Online generating strategy for generator G. The blue digits are the relevance probability pc(vi|v), and the blue solid
arrow indicates the direction that G chooses to move in. Upon completion of sampling, the vertex with blue stripes is the
sampled one, and all colored vertices in the rightmost tree require updating accordingly.

Algorithm 1 Online generating strategy for the generator

Algorithm 2 GraphGAN framework

Require: BFS-tree Tc, representation vectors {gi}i∈V
Ensure: generated sample vgen
1: vpre ← vc, vcur ← vc;
2: while true do
3:
4:
5:
6:
7:
8:
9:
10: end while

Randomly select vi proportionally to pc(vi|vcur) in Eq. (6);
if vi = vpre then
vgen ← vcur;
return vgen

vpre ← vcur, vcur ← vi;

end if

else

vertex, we perform a random walk starting at the root vc in
Tc with respect to the transition probability deﬁned in Eq.
(6). During the process of random walk, if the currently vis-
ited vertex is v and generator G decides to visit v’s parent
(i.e., turning around on the path) for the ﬁrst time, then v is
chosen as the generated vertex.

The online generating strategy for the generator is for-
mally described in Algorithm 1. We denote the currently
visited vertex as vcur and the previously visited one as
vpre. Note that Algorithm 1 terminates in O(log V ) steps,
since the random-walk path will turn around at the latest
when reaching leaf vertices. Similar to the computation of
graph softmax, the complexity of the above online generat-
ing method is O(d log V ), which is signiﬁcantly lower than
the ofﬂine method with a complexity of O(V · d log V ). Fig-
ure 2 gives an illustrative example of the generating strategy
as well as the computation process of graph softmax. In each
step of a random walk, a blue vertex vi is chosen out of all
neighbors of vcur by random selection proportionally to the
relevance probability pc(vi|vcur) deﬁned in Eq. (6). Once
vi equals vpre, i.e., the random walk revisits vcur’s parent
vpre, vcur would be sampled out (indicated as the vertex
with blue stripes in the ﬁgure), and all vertices along the
path Pvc→vcur as well as the vertices that are directly con-
nected to this path need to be updated according to Eq. (4),
(6) and (7).

Finally, the overall logic of GraphGAN is summarized in
Algorithm 2. We provide time complexity analysis of Graph-
GAN as follows. The complexity of BFS-tree construction
for all vertices in line 2 is O(cid:0)V (V + E)(cid:1) = O(dV 2) since
the time complexity of BFS is O(V + E) (Cormen 2009).

Require: dimension of embedding k, size of generating samples

s, size of discriminating samples t

Ensure: generator G(v|vc; θG), discriminator D(v, vc; θD)
1: Initialize and pre-train G(v|vc; θG) and D(v, vc; θD);
2: Construct BFS-tree Tc for all vc ∈ V;
3: while GraphGAN not converge do
4:
5:

for G-steps do

G(v|vc; θG) generates s vertices for each vertex vc ac-
cording to Algorithm 1;
Update θG according to Eq. (4), (6) and (7);

6:
7:
8:
9:

end for
for D-steps do

Sample t positive vertices from ground truth and t nega-
tive vertices from G(v|vc; θG) for each vertex vc;
Update θD according to Eq. (2) and (3);

10:
end for
11:
12: end while
13: return G(v|vc; θG) and D(v, vc; θD)

In each iteration, the complexity of both line 5 and line 6 is
O(sV · d log V · k), and the complexity of line 9 and line 10
is O(tV · d log V · k) and O(tV · k), respectively. In general,
if we treat k, s, t, and d as constants, the complexity of each
iteration in GraphGAN is O(V log V ).

Experiments
In this section, we evaluate the performance of GraphGAN1
on a series of real-world datasets. Speciﬁcally, we choose
three application scenarios for experiments, i.e., link predic-
tion, node classiﬁcation, and recommendation.

Experiments Setup
We utilize the following ﬁve datasets in our experiments:
• arXiv-AstroPh2 is from the e-print arXiv and covers sci-
entiﬁc collaborations between authors with papers sub-
mitted to the Astro Physics category. The vertices repre-
sent authors and the edge indicates co-author relationship.
This graph has 18,772 vertices and 198,110 edges.

• arXiv-GrQc3 is also from arXiv and covers scientiﬁc col-
laborations between authors with papers submitted to the

1https://github.com/hwwang55/GraphGAN
2https://snap.stanford.edu/data/ca-AstroPh.html
3https://snap.stanford.edu/data/ca-GrQc.html

(a) arXiv-AstroPh

(b) arXiv-GrQc

Figure 3: The correlation between the probability of edge
existence and the shortest distance for a given vertex pair.

General Relativity and Quantum Cosmology categories.
This graph has 5,242 vertices and 14,496 edges.

• BlogCatalog4 is a network of social relationships of the
bloggers listed on the BlogCatalog website. The labels of
vertices represent blogger interests inferred through the
metadata provided by the bloggers. This graph has 10,312
vertices, 333,982 edges, and 39 different labels.

• Wikipedia5 is a co-occurrence network of words appear-
ing in the ﬁrst 109 bytes of the Eglish Wikipedia dump.
The labels represent the inferred Part-of-Speech (POS)
tags of words. This graph has 4,777 vertices, 184,812
edges, and 40 different labels.

• MovieLens-1M6 is a bipartite graph consisting of approx-
imately 1 million ratings (edges) with 6,040 users and
3,706 movies in MovieLens website.

We compare our proposed GraphGAN with the following

four baselines for graph representation learning:

• DeepWalk (Perozzi, Al-Rfou, and Skiena 2014) adopts
random walk and Skip-Gram to learn vertex embeddings.
• LINE (Tang et al. 2015) preserves the ﬁrst-order and
second-order proximity among vertices in the graph.
• Node2vec (Grover and Leskovec 2016) is a variant of
DeepWalk and designs a biased random walk to learn ver-
tex embeddings.

• Struc2vec (Ribeiro, Saverese, and Figueiredo 2017) cap-

tures the structural identity of vertices in a graph.

For all three experiment scenarios, we perform stochastic
gradient descent to update parameters in GraphGAN with
learning rate 0.001. In each iteration, we set s as 20 and t as
the number of positive samples in the test set for each ver-
tex, then run G-steps and D-steps for 30 times, respectively.
The dimension of representation vectors k for all methods is
set as 20. The above hyper-parameters are chosen by cross
validation. The ﬁnal learned vertex representations are gi’s.
Parameter settings for all baselines are as default.

Empirical Study
We conduct an empirical study to investigate the real pat-
tern of connectivity distribution in graphs. Speciﬁcally, for
a given vertex pair, we aim to reveal how the probability of
edge existence changes with their shortest distance in the

4http://socialcomputing.asu.edu/datasets/BlogCatalog
5http://www.mattmahoney.net/dc/textdata
6https://grouplens.org/datasets/movielens/1m/

Table 1: Accuracy and Macro-F1 on arXiv-AstroPh and
arXiv-GrQc in link prediction.

Model

arXiv-AstroPh
Acc Macro-F1
DeepWalk
0.841
LINE
0.820
Node2vec
0.845
0.821
Struc2vec
GraphGAN 0.855

0.839
0.814
0.854
0.810
0.859

arXiv-GrQc
Acc Macro-F1
0.803
0.764
0.844
0.780
0.849

0.812
0.761
0.842
0.776
0.853

graph. To achieve this, we ﬁrst randomly sample 1 million
vertex pairs from arXiv-AstroPh and arXiv-GrQc datasets,
respectively. For each selected vertex pair, we remove the
edge between them if it exists (because it is treated as hid-
den ground truth), and calculate their shortest distance. We
count the probability of edge existence for all possible short-
est distances, and plot the results in Figure 3 (the discon-
nected case is omitted). It is evident that the probability of
edge existence between vertex pair drops dramatically with
the increase of their shortest distance. We also plot the log
probability curves in Figure 3, which generally trends to-
wards linear decline with R2 = 0.831 and 0.710. The above
ﬁnding empirically demonstrates that the probability of edge
existence between a pair of vertices is approximately ex-
ponentially proportional to the inverse of their shortest dis-
tance, which strongly proves that graph softmax captures the
essence of real-world graphs according to Theorem 2.

Link Prediction
In link prediction, our goal is to predict whether there ex-
ists an edge between two given vertices. Therefore, this task
shows the performance of edge predictability of different
graph representation learning methods. We randomly hide
10% of edges in the original graph as ground truth, and use
the left graph to train all graph representation learning mod-
els. After training, we obtain the representation vectors for
all vertices and use logistic regression method to predict the
probability of edge existence for a given vertex pair. Our
test set consists of the hidden 10% vertex pairs (edges) in
the original graph as the positive samples and randomly se-
lected disconnected vertex pairs as negative samples with
equal number. We use arXiv-AstroPh and arXiv-GrQc as
datasets, and report the results of Accuracy and Macro-F1
in Table 1. We have the following observations: 1) Perfor-
mance of LINE and struc2vec is relatively poor in link pre-
diction, as they cannot quite capture the pattern of edge exis-
tence in graphs. 2) DeepWalk and node2vec perform better
than LINE and struc2vec. This is probably because Deep-
Walk and node2vec both utilize the random-walk-based
Skip-Gram model, which is better at extracting proximity
information among vertices. 3) GraphGAN outperforms all
the baselines in link prediction. Speciﬁcally, GraphGAN
improves Accuracy on arXiv-AstroPh and arXiv-GrQc by
1.18% to 4.27% and 0.59% to 11.13%, respectively. Our ex-
planation is that adversarial training provides GraphGAN a
higher learning ﬂexibility than the single-model training for
baselines.

To intuitively understand the learning stability of Graph-
GAN, we further illustrate the learning curves of the gener-

(a) Generator

(b) Discriminator

(a) Precision@K

(b) Recall@K

Figure 4: Learning curves of the generator and the discrimi-
nator of GraphGAN on arXiv-GrQc in link prediction.

Figure 5: Precision@K and Recall@K on MovieLens-1M in
recommendation.

Model

Table 2: Accuracy and Macro-F1 on BlogCatalog and
Wikipedia in node classiﬁcation.
BlogCatalog
Acc Macro-F1
0.225
DeepWalk
0.205
LINE
0.215
Node2vec
Struc2vec
0.228
GraphGAN 0.232

Wikipedia
Acc Macro-F1
0.194
0.175
0.191
0.211
0.213

0.214
0.192
0.206
0.216
0.221

0.183
0.164
0.179
0.190
0.194

ator and the discriminator on arXiv-GrQc in Figure 4. From
Figure 4 we observe that the minimax game in GraphGAN
arrives at an equilibrium where the generator performs out-
standingly well after convergence, while performance of the
discriminator boosts at ﬁrst but gradually falls below 0.8.
Note that the discriminator does not degrade to a random-
guess level, because the generator still provides lots of true
negative samples in practice. The result suggests that, differ-
ent with IRGAN (Wang et al. 2017a), the design of graph
softmax enables the generator in GraphGAN to draw sam-
ples and learn vertex embeddings more efﬁciently.

Node Classiﬁcation
In node classiﬁcation, each vertex is assigned one or multi-
ple labels. After we observe a fraction of vertices and their
labels, we aim to predict labels for the remaining vertices.
Therefore, the performance of node classiﬁcation can reveal
the distinguishability of vertices under different graph rep-
resentation learning methods. To conduct the experiment,
we train GraphGAN and baselines on the whole graph to
obtain vertex representations, and use logistic regression as
classiﬁer to perform node classiﬁcation with 9:1 train-test
ratio. We use BlogCatalog and Wikipedia as datasets. The
results of Accuracy and Macro-F1 are presented in Table
2. As we can see, GraphGAN outperforms all baselines on
both datasets. For example, GraphGAN achieves gains of
1.75% to 13.17% and 0.95% to 21.71% on Accuracy on two
datasets, respectively. This indicates that though GraphGAN
is directly designed to optimize the approximated connec-
tivity distribution on edges, it can still effectively encode the
information of vertices into the learned representations.

Recommendation
We use Movielens-1M as dataset for recommendation. For
each user, we aim to recommend a set of movies which
have not been watched but may be liked by the user. We

ﬁrst treat all 4-star and 5-star ratings as edges to obtain a
bipartite graph, then randomly hide 10% of edges in the
original graph as the test set and construct a BFS-tree for
each user. Note that different from the above two exper-
iment scenarios where the connectivity distribution is de-
ﬁned on all other vertices for certain vertex, in recommen-
dation the probability of connectivity for one user is only
distributed over a fraction of vertices, i.e., all movies, in
the graph. Therefore, we “shortcut” all user vertices in the
BFS-tree (except the root) by adding direct edges within all
movie pairs that are linked by a user vertex. After train-
ing and obtaining representations of users and movies, for
each user, we select K of his unwatched movies with the
highest inner product as the recommendation result. The
results of Precision@K and Recall@K are shown in Fig-
ure 5, from which we can observe that GraphGAN is con-
sistently above all baselines, and achieves statistically sig-
niﬁcant improvements on both metrics. Take Precision@20
as an example, GraphGAN outperforms DeepWalk, LINE,
node2vec, and struc2vec by 38.56%, 59.60%, 124.95%, and
156.85%, respectively. Therefore, we can draw the conclu-
sion that GraphGAN maintains a more decent performance
in ranking-based tasks compared with other graph represen-
tation learning methods.

Conclusions
In this paper, we propose GraphGAN that uniﬁes two
schools of graph representation learning methodologies, i.e.,
generative methods and discriminative methods, via adver-
sarial training in a minimax game. Under the GraphGAN
framework, both the generator and the discriminator could
beneﬁt from each other: the generator is guided by the sig-
nals from the discriminator and improves its generating per-
formance, while the discriminator is pushed by the generator
to better distinguish ground truth from generated samples.
Moreover, we propose graph softmax as the implementa-
tion of the generator, which solves the inherent limitations
of the traditional softmax. We conduct experiments on ﬁve
real-world datasets in three scenarios, and the results demon-
strate that GraphGAN signiﬁcantly outperforms strong base-
lines in all experiments due to its adversarial framework and
proximity-aware graph softmax.

Acknowledgments
This work was partially sponsored by the National Basic Re-
search 973 Program of China under Grant 2015CB352403.

References
Blei, D. M.; Ng, A. Y.; and Jordan, M. I. 2003. Latent
dirichlet allocation. Journal of machine Learning research
3(Jan):993–1022.
Cao, S.; Lu, W.; and Xu, Q. 2016. Deep neural networks for
learning graph representations. In AAAI, 1145–1152.
Chen, J.; Zhang, Q.; and Huang, X. 2016. Incorporate group
In CIKM,
information to enhance network embedding.
1901–1904. ACM.
Cormen, T. H. 2009. Introduction to algorithms. MIT press.
Denton, E. L.; Chintala, S.; Fergus, R.; et al. 2015. Deep
generative image models using a laplacian pyramid of ad-
versarial networks. In NIPS, 1486–1494.
Dong, Y.; Chawla, N. V.; and Swami, A. 2017. metap-
ath2vec: Scalable representation learning for heterogeneous
networks. In KDD, 135–144. ACM.
Gao, S.; Denoyer, L.; and Gallinari, P. 2011. Temporal link
prediction by integrating content and structure information.
In CIKM, 1169–1174. ACM.
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.
2014. Generative adversarial nets. In NIPS, 2672–2680.
Grover, A., and Leskovec, J. 2016. node2vec: Scalable fea-
ture learning for networks. In KDD, 855–864. ACM.
Huang, X.; Li, J.; and Hu, X. 2017. Label informed at-
tributed network embedding. In WSDM, 731–739. ACM.
Li, C.; Li, Z.; Wang, S.; Yang, Y.; Zhang, X.; and Zhou, J.
In Interna-
2017a. Semi-supervised network embedding.
tional Conference on Database Systems for Advanced Ap-
plications, 131–147. Springer.
Li, C.; Wang, S.; Yang, D.; Li, Z.; Yang, Y.; Zhang, X.; and
Zhou, J. 2017b. Ppne: Property preserving network embed-
ding. In International Conference on Database Systems for
Advanced Applications, 163–179. Springer.
Li, J.; Monroe, W.; Shi, T.; Ritter, A.; and Jurafsky, D.
2017c. Adversarial learning for neural dialogue generation.
arXiv preprint arXiv:1701.06547.
Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X. 2015. Learn-
ing entity and relation embeddings for knowledge graph
completion. In AAAI, 2181–2187.
Lindsay, B. G. 1995. Mixture models: theory, geometry and
In NSF-CBMS regional conference series in
applications.
probability and statistics, i–163. JSTOR.
Liu, L.; Cheung, W. K.; Li, X.; and Liao, L. 2016. Aligning
users across social networks using network embedding. In
IJCAI, 1774–1780.
Maaten, L. v. d., and Hinton, G. 2008. Visualizing data using
t-sne. Journal of Machine Learning Research 9(Nov):2579–
2605.
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and
Dean, J. 2013. Distributed representations of words and
phrases and their compositionality. In NIPS, 3111–3119.
Morin, F., and Bengio, Y. 2005. Hierarchical probabilistic
neural network language model. In Aistats, volume 5, 246–
252.

Ou, M.; Cui, P.; Pei, J.; Zhang, Z.; and Zhu, W. 2016. Asym-
In KDD,
metric transitivity preserving graph embedding.
1105–1114.
Perozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk:
Online learning of social representations. In KDD, 701–710.
ACM.
Ribeiro, L. F.; Saverese, P. H.; and Figueiredo, D. R. 2017.
struc2vec: Learning node representations from structural
identity. In KDD, 385–394. ACM.
Schulman, J.; Heess, N.; Weber, T.; and Abbeel, P. 2015.
Gradient estimation using stochastic computation graphs. In
NIPS, 3528–3536.
Tang, J.; Aggarwal, C.; and Liu, H. 2016. Node classi-
In Proceedings of the
ﬁcation in signed social networks.
2016 SIAM International Conference on Data Mining, 54–
62. SIAM.
Tang, J.; Qu, M.; Wang, M.; Zhang, M.; Yan, J.; and Mei,
Q. 2015. Line: Large-scale information network embed-
ding. In WWW, 1067–1077. International World Wide Web
Conferences Steering Committee.
Tang, J.; Qu, M.; and Mei, Q. 2015. Pte: Predictive text em-
bedding through large-scale heterogeneous text networks. In
KDD, 1165–1174. ACM.
Tian, F.; Gao, B.; Cui, Q.; Chen, E.; and Liu, T.-Y. 2014.
Learning deep representations for graph clustering. In AAAI,
1293–1299.
Wang, J.; Yu, L.; Zhang, W.; Gong, Y.; Xu, Y.; Wang, B.;
Zhang, P.; and Zhang, D. 2017a. Irgan: A minimax game for
unifying generative and discriminative information retrieval
models. In SIGIR. ACM.
Wang, S.; Tang, J.; Aggarwal, C.; Chang, Y.; and Liu, H.
2017b. Signed network embedding in social media.
In
Proceedings of the 2017 SIAM International Conference on
Data Mining, 327–335. SIAM.
Wang, X.; Cui, P.; Wang, J.; Pei, J.; Zhu, W.; and Yang,
S. 2017c. Community preserving network embedding. In
AAAI, 203–209.
Wang, H.; Zhang, F.; Hou, M.; Xie, X.; Guo, M.; and Liu,
Q. 2018. Shine: Signed heterogeneous information network
embedding for sentiment link prediction. In WSDM. ACM.
Wang, D.; Cui, P.; and Zhu, W. 2016. Structural deep net-
work embedding. In KDD, 1225–1234. ACM.
Yu, X.; Ren, X.; Sun, Y.; Gu, Q.; Sturt, B.; Khandelwal, U.;
Norick, B.; and Han, J. 2014. Personalized entity recom-
mendation: A heterogeneous information network approach.
In WSDM, 283–292. ACM.
Yu, L.; Zhang, W.; Wang, J.; and Yu, Y. 2017. Seqgan:
Sequence generative adversarial nets with policy gradient.
In AAAI, 2852–2858.
Zhang, Y.; Barzilay, R.; and Jaakkola, T. 2017. Aspect-
augmented adversarial networks for domain adaptation.
arXiv preprint arXiv:1701.00188.
Zhou, C.; Liu, Y.; Liu, X.; Liu, Z.; and Gao, J. 2017. Scal-
able graph embedding for asymmetric proximity. In AAAI,
2942–2948.

