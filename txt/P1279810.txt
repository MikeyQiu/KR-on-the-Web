Makeup like a superstar: Deep Localized Makeup Transfer Network

Si Liu1, Xinyu Ou1,2,3, Ruihe Qian1,4, Wei Wang1 and Xiaochun Cao1 ∗
1State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences
2School of Computer Science and Technology, Huazhong University of Science and Technology
3YNGBZX, Yunnan Open University
4University of Electronic Science and Technology of China, Yingcai Experimental School
{liusi, caoxiaochun}@iie.ac.cn, ouxinyu@hust.edu.cn, 406618818@qq.com, wang wei.buaa@163.com

6
1
0
2
 
r
p
A
 
5
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
0
1
7
0
.
4
0
6
1
:
v
i
X
r
a

Abstract

In this paper, we propose a novel Deep Localized
Makeup Transfer Network to automatically rec-
ommend the most suitable makeup for a female
and synthesis the makeup on her face. Given a
before-makeup face, her most suitable makeup is
determined automatically. Then, both the before-
makeup and the reference faces are fed into the
proposed Deep Transfer Network to generate the
after-makeup face. Our end-to-end makeup transfer
network have several nice properties including: (1)
with complete functions: including foundation, lip
gloss, and eye shadow transfer; (2) cosmetic spe-
ciﬁc: different cosmetics are transferred in different
manners; (3) localized: different cosmetics are ap-
plied on different facial regions; (4) producing nat-
urally looking results without obvious artifacts; (5)
controllable makeup lightness: various results from
light makeup to heavy makeup can be generated.
Qualitative and quantitative experiments show that
our network performs much better than the meth-
ods of [Guo and Sim, 2009] and two variants of
NerualStyle [Gatys et al., 2015a].

1 Introduction
Makeup makes the people more attractive, and there are more
and more commercial facial makeup systems in the market.
Virtual Hairstyle1 provides manual hairstyle try-on. Virtual
Makeover TAAZ2 offers to try some pre-prepared cosmetic
elements, such as, lipsticks and eye liners. However, all these
softwares rely on the pre-determined cosmetics which cannot
meet up with users’ individual needs.

Different from the existing work, our goal is to design a
real application system to automatically recommend the most
suitable makeup for a female and synthesis the makeup on
her face. As shown in Figure 1, we simulate an applicable
makeup process with two functions. I: The ﬁrst function is
makeup recommendation, where personalization is taken spe-
cial cares. More speciﬁcally, females with similar face, eye

∗corresponding author
1http://www.hairstyles.knowage.info/
2http://www.taaz.com/

Figure 1: Our system has two functions. I: recommend the
most suitable makeup for each before-makeup face II: trans-
fer the foundation, eye shadow and lip gloss from the refer-
ence to the before-makeup face. The lightness of the makeup
can be tuned. For better view of all ﬁgures in this paper,
please refer to the original color pdf ﬁle and pay special at-
tention to eye shadow, lip gloss and foundation transfer.

or mouth shapes are suitable for similar makeups [Liu et al.,
2014]. To this end, given a before-makeup face, we ﬁnd the
visually similar face from a reference dataset. The similarity
is measured by the Euclidean distance between deep features
produced by an off-the-shelf deep face recognition network
[Parkhi et al., 2015]. To sum up, the recommendation is per-
sonalized, data-driven and easy to implement. II: The second
function is makeup transfer from the reference face to the
before-makeup face. The makeup transfer function should
satisfy ﬁve criteria. 1) With complete functions: we consider
three kinds of most commonly used cosmetics, i.e., founda-
tion, eye shadow and lip gloss. Note that our model is quite
generic and easily extended to other types of cosmetics. 2)
Cosmetic speciﬁc: different cosmetics are transferred in their
own ways. For example, maintaining the shape is important
for the eye shadow rather than for the foundation. 3) Local-
ized: all cosmetics are applied locally on their corresponding
facial parts. For example, lip gloss is put on the lip while eye
shadow is worn the eyelid. 4) Naturally looking: the cosmet-
ics need to be seamlessly fused with the before-makeup face.
In other words, the after-makeup face should look natural. 5)
Controllable makeup lightness: we can change the lightness
of each type of cosmetic.

To meet the ﬁve afore-mentioned criteria, we propose a
Deep Localized Makeup Transfer Network, whose ﬂowchart

Figure 2: The proposed Deep Localized Makeup Transfer Network contains two sequential steps. (i) the correspondences
between the facial part (in the before-makeup face) and the cosmetic (in the reference face) are built based on the face parsing
network. (ii) Eye shadow, foundation and lip gloss are locally transferred with a global smoothness regularization.

is shown in Figure 2. The network transfers the makeup
from the recommended reference face to a before-makeup
face. Firstly, both before-makeup and reference faces are fed
into a face parsing network to generate two corresponding
labelmaps. The parsing network is based on the Fully Convo-
lutional Networks [Long et al., 2014] by (i) emphasizing the
makeup relevant facial parts, such as eye shadow and (ii) con-
sidering the symmetry structure of the frontal faces. Based on
the parsing results, the local region of the before-makeup face
(e.g., mouth) corresponds to its counterpart (e.g., lip) in the
reference face. Secondly, three most common cosmetics, i.e,
eye shadow, lip gloss and foundation are transferred in their
own manners. For example, keeping the shape is the most im-
portant for transferring eye shadow while the smoothing the
skin’s texture is the most important for foundation. So the eye
shadow is transferred via directly altering the corresponding
deep features [Mahendran and Vedaldi, 2015] while founda-
tion is transferred via regularizing the inner product of the
feature maps [Gatys et al., 2015a]. The after-makeup face is
initialized as the before-makeup face, then gradually updated
via Stochastic Gradient Descent to produce naturally looking
results. By tuning up the weight of each cosmetic, a series
of after-makeup faces with increasingly heavier makeup can
be generated. In this way, our system can produce various
results with controllable makeup lightness.

Compared with traditional makeup transfer methods [Guo
and Sim, 2009; Tong et al., 2007; Scherbaum et al., 2011;
Liu et al., 2014], which require complex data preprocessing
or annotation and their results are inferior, our contributions
are as follows. I: To the best of our knowledge, it is the ﬁrst
makeup transferring method based on deep learning frame-
work and can produce very natural-looking results. Our sys-
tem can transfer foundation, eye shadow and lip gloss, and
therefore is with complete functions. Furthermore, the light-
ness of the makeup can be controlled to meet the needs of
various users. II: we propose an end-to-end Deep Localized
Makeup Transfer Network to ﬁrst build part vs cosmetic cor-
respondence and then transfer makeup. Compared with Neru-
alStyle [Gatys et al., 2015a] which fuses two images globally,
our method transfers makeup locally from the cosmetic re-

gions to their corresponding facial parts. Therefore, a lot of
unnatural results are avoided.

2 Related Work
We will introduce the related makeup transfer methods and
the most representative deep image synthesis methods.

Facial Makeup Studies: The work of Guo et al. [Guo and
Sim, 2009] is the ﬁrst attempt for the makeup transfer task. It
ﬁrst decomposes the before-makeup and reference faces into
three layers. Then, they transfer information between the cor-
responding layers. One major disadvantage is that it needs
warp the reference face to the before-makeup face, which is
very challenging. Scherbaum et al. [Scherbaum et al., 2011]
propose to use a 3D morphable face model to facilitate facial
makeup. It also requires the before-after makeup face pairs
for the same person, which are difﬁcult to collect in real appli-
[Tong et al., 2007] propose a “cosmetic-
cation. Tong et al.
transfer” procedure to realistically transfer the cosmetic style
captured in the example-pair to another person’s face. The
requirement of before-after makeup pairs limits the practi-
cability of the system. Liu et al [Liu et al., 2014] propose
an automatic makeup recommendation and synthesis system
called beauty e-expert. Their contribution is in the recom-
mendation module. To sum up, our method greatly relaxes
the requirements of traditional makeup methods, and gener-
ates more naturally-looking results.

Deep Image Synthesis Methods: Recently, deep learning
has achieved great success in fashion analysis works [Liu et
al., 2015; Liang et al., 2015]. Dosovitskiy et al. [Dosovit-
skiy et al., 2015; Dosovitskiy and Brox, 2015] use a gener-
ative CNN to generate images of objects given object type,
[Simonyan et al.,
viewpoint, and color. Simonyan et al.
2013] generate an image, which visualizes the notion of the
class captured by a net. Mahendran et al. [Mahendran and
Vedaldi, 2015] contribute a general framework to invert both
hand-crafted and deep representations to the images. Gatys et
al. [Gatys et al., 2015b] present a parametric texture model
based on the CNN which can synthesise high-quality natu-
ral textures. Generative adversarial network [Goodfellow et
al., 2014] consists of two components; a generator and a dis-
criminator. The generated image is very natural without ob-

vious artifacts. Goodfellow et al. introduce [Goodfellow et
al., 2015] a simple and fast method of generating adversar-
ial examples. Their main aim is to enhance the CNN train-
ing instead image synthesis. Kulkarni et al. [Kulkarni et al.,
2015] present the Deep Convolution Inverse Graphics Net-
work, which learns an interpretable representation of images
for 3D rendering. All existing deep methods can only gener-
ate one image. However, we mainly focus on how to generate
a new image having the nature of the two input images. Re-
cently, deep learning techniques have been applied in many
image editing tasks, such as image colorization [Cheng et al.,
2015], photo adjustment [Yan et al., 2014], ﬁlter learning [Xu
et al., 2015], image inpainting [Ren et al., 2015], shadow re-
moval [Shen et al., 2015] and super-resolution [Dong et al.,
2014]. These methods are operated on a single image. Neru-
alStyle [Gatys et al., 2015a] is the most similar with us. They
use CNN to synthesis a new image by combining the structure
layer from one image and the style layers of another. The key
difference between their work and ours is that our network is
applied locally, which can produce more natural results.

3 Approach

In this Section, we sequentially introduce our makeup recom-
mendation and makeup transfer methods in detail.
3.1 Makeup Recommendation

The most important criterion of makeup recommendation is
personalization [Liu et al., 2014].

Females that look like are suitable for similar makeups.
Given a before-makeup face, we ﬁnd several similar faces
in the reference dataset. The similarities are deﬁned as the
Euclidean distances between the deep features extracted by
feeding the face into an off-the-shelf face recognition model
[Parkhi et al., 2015] named VGG-Face. The deep feature
is the concatenation of the two (cid:96)2 normalized FC-4096 lay-
ers. The VGG-Face model is trained based on VGG-Very-
Deep-16 CNN architecture [Simonyan and Zisserman, 2014]
and aims to accurately identify different people regardless of
whether she puts on makeup, which meets our requirement.
Therefore, the extracted features can capture the most dis-
criminative structure of faces. Finally, the retrieved results
serve as the reference faces to transfer their cosmetics to
the before-makeup face. Figure 3 shows the makeup recom-
mendation results. It illustrates that the recommended refer-
ence faces have similar facial shapes with the before-makeup
faces, and therefore the recommendation is personalized.

3.2 Facial Parts vs. Cosmetics Correspondence
In order to transfer the makeup, we need build the correspon-
dence between facial parts of the before-makeup face and the
cosmetic regions of the reference face. As a result, the cos-
metic can be between the matched pairs. Most of the corre-
spondences can be obtained by the face parsing results, e.g.,
“lip” vs “lip gloss”. The only exception is the eye shadow
transfer. Because the before-makeup face does not have eye
shadow region and the shape of the eyes are different, we need
to warp the eye shadow shape of the reference face.

Face Parsing: Our face parsing model is based on the
Fully Convolution Network (FCN) [Long et al., 2014].
It
is trained using both the before-makeup and reference faces.
The 11 parsing labels are shown in Table 1. The network
takes input of arbitrary size and produces correspondingly-
sized output with efﬁcient inference and learning.

When training the face parsing model, we pay more atten-
tion to the makeup relevant labels. For example, compared
with “background”, we bias toward the “left eye shadow”.
Therefore, we propose a weighted cross-entropy loss which is
a weighted sum over the spatial dimensions of the ﬁnal layer:
(1)

(cid:96)(cid:48) (yij, p (xij; θ)) · w (yij),

(cid:96) (x; θ) =

(cid:88)

ij

where (cid:96)(cid:48) is the cross entropy loss deﬁned on each pixel. yij
and p (xij; θ) are the ground truth and predicted label of the
pixel xij, and w (yij) is the label weight. The weight is set
empirically by maximizing the F1 score in the validation set.
Because the faces in the collected datasets are in frontal
view, and preprocessed by face detection and alignment3. In
the testing phase, we enforce the symmetric prior and re-
place the prediction conﬁdence of both point p and its hor-
izontally mirrored counterparts f (p) by their average xp,c =
(cid:1), where c denotes the channels. Like
1
2
[Chen et al., 2015; Papandreou et al., 2015], the symmet-
ric prior is only added in the testing phase currently. In the
further we will explore how to enforce the structure prior in
the training phase. Figure 4 shows the parsing results of the
original FCN, weighted FCN, and symmetric weighted FCN.

(cid:80) (cid:0)xp,c + xf (p),c

Figure 3: Two examples of makeup recommendation. The
ﬁrst columns are the before-makeup faces, other columns are
the recommended reference faces.

3www.faceplusplus.com/

Figure 4: Two face parsing results. The input image, the re-
sults parsed by FCN, weighted FCN and symmetric weighted
FCN are shown sequentially.

Eye Shadow Warping: Based on the parsing results,
most correspondences, e.g., “face” vs “foundation”, are built.
However, there is no eye shadow in the before-makeup face,

Labels L/R eye L/R eyebrow inner mouth L/R eye shadow Up/Low lip (lip gloss) background Face (Foundation)
Subsets

before (ref)

before (ref)

both

both

both

both

ref

Table 1: 11 Labels from both before-makeup (referred to as “before”) and reference face sets (referred to as “ref”). Certain
labels, such as “L eye”, belong to both dataset, while certain labels such as “L eye shadow” belong to reference face set only.
“L”, “R”, “Up” and “Low” stands for “Left”, “Right”, “Upper” and “Lower” respectively.
therefore we need to generate an eye shadow mask in the
before-makeup face. Moreover, because the shapes of eyes
and eye brows are different in the face pair, the shape of eye
shadow need to be slightly warped. More speciﬁcally, we get
8 landmarks on eyes and eye brow regions, including inner,
upper-middle, lower-middle and outer corner of eyes and eye
brows. Then the eye shadows are warped by the thin plate
spline [Bookstein, 1989].

A∗ = arg min

= arg min

ij (R (s(cid:48)

ij (A (s(cid:48)

b)) − Ωl

r))(cid:13)
2
(cid:13)
2

A∈RH×W ×C

A∈RH×W ×C

L
(cid:80)
l=1

Rf (A)

(cid:13)
(cid:13)Ωl

(3)
Here, L is the number of layers used. Technically, we use
5 layers, including conv1-1, conv2-1, conv3-1, conv4-1 and
conv5-1. The Gram matrix Ωl ∈ RNl×Nl is deﬁned in (4),
where Nl is the number of feature maps in the l-th layer and
Ωl
ij is the inner product between the vectorised feature map i
and j in layer l:

(cid:88)

Ωl

ij =

Ωl

ikΩl
jk

(4)

k
The results for foundation transfer are shown in Figure (6).
The girl’s skin is exquisite after the foundation transfer.

3.3 Makeup Transfer
Makeup transfer is conducted based on the correspondences
among image pairs. Next we will elaborate how to trans-
fer eye shadow, lip gloss and foundation. Keeping the facial
structure should also be considered and incorporated into the
ﬁnal objective function.

Eye Shadow Transfer needs to consider both the shape
and color. Let take the left eye shadow as an example. The
right eye shadow is transferred similarly. Suppose sr is the
binary mask of left eye shadow in the reference face while sb
is the warped binary mask in the before-makeup face. Note
that after eye shadow warping, sr and sb are of the same shape
and size. Technically, eye shadow transfer is to replace sb’s
deep feature representation in a certain layer (conv1-1 in this
paper) with sr. The mathematical form for the loss of left eye
shadow transfer is Rl(A):

A∗ = arg min

A∈RH×W ×C

= arg min

A∈RH×W ×C

Rl(A)
(cid:13)
(cid:13)P (cid:0)Ωl (A (s(cid:48)

(2)
where H, W and C are the height, width and channel num-
ber of the input image. Ωl
: RH×W ×C → Rd is the
d-dim feature representation of the conv1-1 layer of the
face parsing model, A and R are the after-makeup face
and reference face, respectively.
r are achieved
by mapping sb to sr from the data layer to the conv1-1
layer via the convolutional feature masking [Dai et al., 2015;
b) and R (s(cid:48)
He et al., 2015]. A (s(cid:48)
r) are the eye shadow re-
gions corresponding to the masks s(cid:48)
b and s(cid:48)
r. Similarly, we
can deﬁne the loss function for right eye shadow Rr(A). The
results for both eye shadow transfer are shown in Figure 5,
where both the color and shape of the eye shadows are trans-
ferred.

b and s(cid:48)

s(cid:48)

Figure 5: Two results of eye shadow transfer. The before-
makeup, reference and after-makeup eye shadow are shown.
Lip Gloss and Foundation Transfer require transferring

color and texture. The lip gloss Rf (A) is deﬁned as in (3).

b))(cid:1) − P (cid:0)Ωl (R (s(cid:48)

r))(cid:1)(cid:13)
2
(cid:13)
2

Figure 6: Two exemplars of foundation transfer. In each row,
before-makeup, reference and after-makeup are shown.

The upper lip gloss loss Rup(A) and lower lip gloss loss
Rlow(A) are deﬁned in similar way as (3). And the lip gloss
transfer results are shown in Figure 7. After lip gloss transfer,
the colors of the lip are changed to the reference lip.

Figure 7: Two exemplars of lip gloss transfer results. In each
row, before-makeup, reference and after-makeup are shown.
Structure Preservation term Rs(A) is deﬁned as in (2).
The only difference is that every elements of sb and sr are 1.
Overall Makeup Transfer considers eye shadow, lip gloss

and foundation, and also preserves the face structure.
λe (Rl(A) + Rr(A)) + λf Rf (A)

A∗ = arg min

A∈RH×W ×C
+λl (Rup(A) + Rlow(A)) + λsRs(A) + RV β (A)
(5)
the total variance

To make the results more natural,
(cid:16)
term RV β = (cid:80)
is
added. Rl(A), Rr(A), Rf (A), Rup(A), Rlow(A) and Rs(A)
are the left, right eye shadow, foundation, upper, lower lip

(Ai,j+1 − Aij)2+(Ai+1,j − Aij)2(cid:17) β

i,j

2

Figure 8: The lightness of the makeup can be controlled.

gloss and face structure loss. And λe, λf , λl and λe are
the weights to balance different cosmetics. By tuning these
weights, the lightness of makeup can be adjusted. For exam-
ple, by increasing the λe, the eye shadow will be darker.

The overall energy function (5) is optimized via Stochastic
Gradient Descent (SGD) by using momentum [Mahendran
and Vedaldi, 2015]:

µt+1 ← mµt − ηt∇E(A) At+1 ← At + µt
(6)
where the µt is a weighed average of the last several gradi-
ents, with decaying factor m = 0.9. A0 is initialized as the
before-makeup face.
4 Experiments
4.1 Experimental Setting
Data Collection and Parameters: We collect a new dataset
with 1000 before-makeup faces and 1000 reference faces.
Some before-makeup faces are nude makeup or very light
makeup. Among the 2000 faces, 100 before-makeup faces
and 500 reference faces are randomly selected as test set. The
remaining 1300 faces and 100 faces are used as training and
validation set. Given one before-makeup test face, the most
similar ones among the 500 reference test faces are chosen
to transfer the makeup. The weights [λs λe λl λf ] are set
as [10 40 500 100]. The weights of different labels in the
weighted FCN are set as [1.4 1.2 1] for {eyebrows, eyes,
eye shadows}, {lip, inner mouth} and {face, background},
respectively. These weights are set empirically by the valida-
tion set.

Baseline Methods: To the best of our knowledge, the only
makeup transfer work is Guo and Sim [Guo and Sim, 2009].
We also compare with two variants of Gatys et al.’ method
[Gatys et al., 2015a]. They use CNN to synthesis a new
image by combining the content layer from one image and
the style layers of another image. The ﬁrst variant is called
NerualStyle-CC treating both before-makeup and reference

Figure 9: Qualitative comparisons between the state-of-the-
arts and ours.
faces as content. Another variant is named as NerualStyle-CS
which uses the before-makeup as content and reference face
as style. We cannot compare with other related makeup meth-
ods, such as, Tong et al. [Tong et al., 2007], Scherbaum et al.
[Scherbaum et al., 2011] or Beauty E-expert system [Liu et
al., 2014] require before-and-after makeup image pairs, 3D
information or extensive labeling of facial attributes. The
proposed model can transfer the makeup in 6 seconds for an
224 × 224 image pair using TITAN X GPU.

4.2 Makeup Lightness
In order to show our method can generate after-makeup face
with various makeup lightness, ranging from light makeup to
dark makeup, we gradually increase certain makeup weights
λe, λf and λl. Four results are shown in Figure 8. The ﬁrst
two rows use the same before-makeup and reference faces.
The girl’s eye shadows become gradually darker in the ﬁrst
row. In the second row, the lip color becomes redder while
other cosmetics keep unchanged. The third and fourth rows
show another example. The eye shadow and lip gloss are
increasingly darker in the third and last row, respectively.

4.3 Comparison with State-of-the-arts
We compare with Guo and Sim4 [Guo and Sim, 2009],
NerualStyle-CC and NerualStyle-CS5 [Gatys et al., 2015a].
Follwing the evaluation metric in [Liu et al., 2014], the com-
parison is conducted both qualitatively and quantitatively.

The qualitative results are shown in Figure 9. Both Guo
and Sim [Guo and Sim, 2009] and ours produce naturally
looking results. However, our result transfers the lip gloss and
eye shadows with the same lightness as the reference faces,
while Guo and Sim [Guo and Sim, 2009] always transfers
much lighter makeup than the reference face. For example,
in the ﬁrst row, the lip gloss of both the reference face and
our results are deep red. But the lip gloss of Guo and Sim
is orange-red. In addition, the eye shadows of both the refer-
ence face and our results are very dark and smoky. However,

4We sincerely thank the authors for sharing their code.
5we use the code https://github.com/jcjohnson/neural-style and

ﬁne-tune the parameter for best visual effects.

Figure 10: Different girls wear the same makeup.

Figure 11: The same girl wears different makeups.

4.4 More Makeup Transfer Results

Guo and Sim can only produce very light eye shadow. Com-
pared with NerualStyle-CC and NerualStyle-CS [Gatys et al.,
2015a], our after-makeup faces contain much less artifacts. It
is because our makeup transfer is conducted between the lo-
cal regions, such as lip vs lip gloss while the NerualStyle-CC
and NerualStyle-CS [Gatys et al., 2015a] transfer the makeup
globally. Global makeup transfer suffers from the mismatch
problem between the image pair. It shows the advantages of
our deep localized makeup transfer network.

The quantitative comparison mainly focuses on the qual-
ity of makeup transfer and the degree of harmony. For each of
the 100 before-makeup test faces, ﬁve most similar reference
faces are found. Thus we have totally 100 × 5 after-makeup
results for each makeup transfer method. We compare our
method with each of the 3 baselines sequentially. Each time,
a 4-tuple, i.e., a before-makeup face, a reference face, the
after-makeup face by our method and the after-makeup face
by one of the baseline, are sent to 20 participants to com-
pare. Note that the two after-makeup faces are shown in ran-
dom order. The participants rate the results into ﬁve degrees:
“much better”, “better”, “same”, “worse”, and “much worse”.
The percentages of each degree are shown in Table 2. Our
method is much better or better than Guo and Sim in 9.7%
and 55.9% cases. We are much better than NerualStyle-CC
and NerualStyle-CS in 82.7% and 82.8% cases.

much better better same worse much worse
9.7% 55.9% 22.4% 11.1% 1.0%
0%
82.7% 14.0% 3.24% 0.15%
0%
82.8% 14.9% 2.06% 0.29%

Guo and Sim
NerualStyle-CC
NerualStyle-CS
Table 2: Quantitative comparisons between our method and
three other makeup transfer methods.

In Figure 10, for each reference face, we select ﬁve most sim-
ilar looking before-makeup girls. Then the same makeup is
applied on different before-makeup girls. It shows that the
eye shadow, lip gloss and foundation are transferred success-
fully to the eye lid, lip and face areas. Note that our method
can handle the makeup transfer between different facial ex-
pressions. For example, in Figure 10, the second girl in the
left panel is grinning. However, the reference face is not smil-
ing. Thanks to the localization property of our method, the lip
gloss does not transfer to the teeth in the after-makeup face.

In Figure 11, for each before-makeup face, we select ﬁve
most similar looking reference girls. This function is quite
useful in real application, because the users can virtually try
different makeup and choose the favorite one.

5 Conclusion

In this paper, we propose a novel Deep Localized Makeup
Transfer Network to automatically transfer the makeup from
a reference face to a before-makeup face. The proposed deep
transfer network has ﬁve nice properties: with complete func-
tion, cosmetic speciﬁc, localized, producing naturally looking
results and controllable makeup lightness. Extensive experi-
ments show that our network performs better than the state-
of-the-arts. In the future, we plan to explore the extensibility
of the network. For example, one before-makeup face can be
combined with two reference faces. The after-makeup face
has the eye shadow of one reference face and lip gloss of an-
other reference face.

Acknowledgments
This work was supported by National Natural Science Foun-
dation of China (No.61572493, Grant U1536203), 100 Tal-
ents Programme of The Chinese Academy of Sciences,
and “Strategic Priority Research Program” of the Chinese
Academy of Sciences (XDA06010701).

References
[Bookstein, 1989] Fred L. Bookstein. Principal warps: Thin-plate
splines and the decomposition of deformations. IEEE Transac-
tions on Pattern Analysis & Machine Intelligence, (6):567–585,
1989.

[Chen et al., 2015] Liang-Chieh Chen, George Papandreou,

Ia-
sonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic
image segmentation with deep convolutional nets and fully con-
nected crfs. In International Conference on Learning Represen-
tations, 2015.

[Cheng et al., 2015] Zezhou Cheng, Qingxiong Yang, and Bin
In IEEE International Conference

Sheng. Deep colorization.
on Computer Vision. 2015.

[Dai et al., 2015] Jifeng Dai, Kaiming He, and Jian Sun. Con-
volutional feature masking for joint object and stuff segmenta-
tion. Computer Vision and Pattern Recognition, pages 3992–
4000, 2015.

[Dong et al., 2014] Chao Dong, Chen Change Loy, Kaiming He,
and Xiaoou Tang. Learning a deep convolutional network for
In European Conference on Computer
image super-resolution.
Vision, pages 184–199. 2014.

[Dosovitskiy and Brox, 2015] Alexey Dosovitskiy and Thomas
Brox. Inverting convolutional networks with convolutional net-
works. CoRR, abs/1506.02753, 2015.

[Dosovitskiy et al., 2015] Alexey Dosovitskiy, Jost Tobias Sprin-
genberg, and Thomas Brox. Learning to generate chairs with con-
volutional neural networks. Computer Vision and Pattern Recog-
nition, pages 1538–1546, 2015.

[Gatys et al., 2015a] Leon A Gatys, Alexander S Ecker, and
Matthias Bethge. A neural algorithm of artistic style. CoRR,
abs/1508.06576, 2015.

[Gatys et al., 2015b] Leon A. Gatys, Alexander S. Ecker, and
Matthias Bethge. Texture synthesis and the controlled genera-
tion of natural stimuli using convolutional neural networks.
In
Advances in Neural Information Processing Systems 28, 2015.

[Goodfellow et al., 2014] Ian Goodfellow,

Jean Pouget-Abadie,
Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial
In Advances in Neural Information Processing Systems,
nets.
pages 2672–2680, 2014.

[Goodfellow et al., 2015] Ian J. Goodfellow, Jonathon Shlens, and
Christian Szegedy. Explaining and harnessing adversarial exam-
ples. In International Conference on Learning Representations,
2015.

[Guo and Sim, 2009] Dong Guo and Terence Sim. Digital face
In Computer Vision and Pattern Recog-

makeup by example.
nition, pages 73–79, 2009.

[He et al., 2015] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun. Spatial pyramid pooling in deep convolutional net-
works for visual recognition. IEEE Transactions on Pattern Anal-
ysis & Machine Intelligence, 37(9):1904–1916, 2015.

[Kulkarni et al., 2015] Tejas D Kulkarni, Will Whitney, Pushmeet
Kohli, and Joshua B Tenenbaum. Deep convolutional inverse
graphics network. arXiv preprint arXiv:1503.03167, 2015.

[Liang et al., 2015] Xiaodan Liang, Si Liu, Xiaohui Shen, Jianchao
Yang, Luoqi Liu, Jian Dong, Liang Lin, and Shuicheng Yan.
Deep human parsing with active template regression. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 2015.
[Liu et al., 2014] Luoqi Liu, Junliang Xing, Si Liu, Hui Xu,
Xi Zhou, and Shuicheng Yan. Wow! you are so beautiful today!
ACM Transactions on Multimedia Computing, Communications,
and Applications, 11(1s):20, 2014.

[Liu et al., 2015] Si Liu, Xiaodan Liang, Luoqi Liu, Xiaohui Shen,
Jianchao Yang, Changsheng Xu, Liang Lin, Xiaochun Cao, and
Shuicheng Yan. Matching-cnn meets knn: quasi-parametric hu-
man parsing. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2015.

[Long et al., 2014] Jonathan Long, Evan Shelhamer, and Trevor
Darrell. Fully convolutional networks for semantic segmentation.
arXiv preprint arXiv:1411.4038, 2014.

[Mahendran and Vedaldi, 2015] Aravindh Mahendran and Andrea
Vedaldi. Understanding deep image representations by inverting
them. Computer Vision and Pattern Recognition, pages 5188–
5196, 2015.

[Papandreou et al., 2015] George Papandreou, Liang-Chieh Chen,
Kevin Murphy, and Alan L Yuille. Weakly- and semi-
supervised learning of a dcnn for semantic image segmentation.
arxiv:1502.02734, 2015.

[Parkhi et al., 2015] Omkar M Parkhi, Andrea Vedaldi, Andrew
Zisserman, A Vedaldi, K Lenc, M Jaderberg, K Simonyan,
A Vedaldi, A Zisserman, K Lenc, et al. Deep face recognition. In
British Machine Vision Conference, 2015.

[Ren et al., 2015] Jimmy SJ Ren, Li Xu, Qiong Yan, and Wenxiu
In Advances in

Sun. Shepard convolutional neural networks.
Neural Information Processing Systems, pages 901–909, 2015.
[Scherbaum et al., 2011] Kristina Scherbaum, Tobias Ritschel,
Matthias Hullin, Thorsten Thorm¨ahlen, Volker Blanz, and Hans-
Peter Seidel. Computer-suggested facial makeup. In Computer
Graphics Forum, volume 30, pages 485–492, 2011.

[Shen et al., 2015] Li Shen, Teck Wee Chua, and Karianto Leman.
Shadow optimization from structured deep edge detection. arXiv
preprint arXiv:1505.01589, 2015.

[Simonyan and Zisserman, 2014] Karen Simonyan and Andrew
Zisserman. Very deep convolutional networks for large-scale im-
age recognition. arXiv preprint arXiv:1409.1556, 2014.

[Simonyan et al., 2013] Karen Simonyan, Andrea Vedaldi, and An-
drew Zisserman. Deep inside convolutional networks: Visualis-
ing image classiﬁcation models and saliency maps. CoRR, 2013.
[Tong et al., 2007] Wai-Shun Tong, Chi-Keung Tang, Michael S
Brown, and Ying-Qing Xu. Example-based cosmetic transfer.
In Computer Graphics and Applications, pages 211–218, 2007.
[Xu et al., 2015] Li Xu, Jimmy Ren, Qiong Yan, Renjie Liao, and
Jiaya Jia. Deep edge-aware ﬁlters. In International Conference
on Machine Learning, pages 1669–1678, 2015.

[Yan et al., 2014] Zhicheng Yan, Hao Zhang, Baoyuan Wang, Syl-
vain Paris, and Yizhou Yu. Automatic photo adjustment using
deep neural networks. CoRR, abs/1412.7725, 2014.

