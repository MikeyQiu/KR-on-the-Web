7
1
0
2
 
v
o
N
 
7
 
 
]

G
L
.
s
c
[
 
 
2
v
7
6
3
9
0
.
5
0
7
1
:
v
i
X
r
a

Stabilizing Training of Generative Adversarial
Networks through Regularization

Kevin Roth
Department of Computer Science
ETH Zürich
kevin.roth@inf.ethz.ch

Aurelien Lucchi
Department of Computer Science
ETH Zürich
aurelien.lucchi@inf.ethz.ch

Sebastian Nowozin
Microsoft Research
Cambridge, UK
sebastian.Nowozin@microsoft.com

Thomas Hofmann
Department of Computer Science
ETH Zürich
thomas.hofmann@inf.ethz.ch

Abstract

Deep generative models based on Generative Adversarial Networks (GANs) have
demonstrated impressive sample quality but in order to work they require a careful
choice of architecture, parameter initialization, and selection of hyper-parameters.
This fragility is in part due to a dimensional mismatch or non-overlapping support
between the model distribution and the data distribution, causing their density ratio
and the associated f -divergence to be undeﬁned. We overcome this fundamental
limitation and propose a new regularization approach with low computational cost
that yields a stable GAN training procedure. We demonstrate the effectiveness of
this regularizer across several architectures trained on common benchmark image
generation tasks. Our regularization turns GAN models into reliable building
blocks for deep learning.1

1

Introduction

A recent trend in the world of generative models is the use of deep neural networks as data generating
mechanisms. Two notable approaches in this area are variational auto-encoders (VAEs) [14, 28] as
well as generative adversarial networks (GAN) [8]. GANs are especially appealing as they move
away from the common likelihood maximization viewpoint and instead use an adversarial game
approach for training generative models. Let us denote by P(x) and Qθ(x) the data and model
distribution, respectively. The basic idea behind GANs is to pair up a θ-parametrized generator
network that produces Qθ with a discriminator which aims to distinguish between P and Qθ, whereas
the generator aims for making Qθ indistinguishable from P. Effectively the discriminator represents
a class of objective functions F that measures dissimilarity of pairs of probability distributions. The
ﬁnal objective is then formed via a supremum over F, leading to the saddle point problem

(cid:20)
(cid:96)(Qθ; F) := sup
F ∈F

min
θ

(cid:21)
F (P, Qθ)

.

The standard way of representing a speciﬁc F is through a family of statistics or discriminants φ ∈ Φ,
typically realized by a neural network [8, 26]. In GANs, we use these discriminators in a logistic
classiﬁcation loss as follows

F (P, Q; φ) = EP [g(φ(x))] + EQ [g(−φ(x))] ,

1Code available at https://github.com/rothk/Stabilizing_GANs

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

(1)

(2)

where g(z) = ln(σ(z)) is the log-logistic function (for reference, σ(φ(x)) = D(x) in [8]).
As shown in [8], for the Bayes-optimal discriminator φ∗ ∈ Φ, the above generator objective reduces
to the Jensen-Shannon (JS) divergence between P and Q. The work of [25] later generalized this to
a more general class of f -divergences, which gives more ﬂexibility in cases where the generative
model may not be expressive enough or where data may be scarce.

We consider three different challenges for learning the model distribution:

(A) empirical estimation: the model family may contain the true distribution or a good approximation
thereof, but one has to identify it based on a ﬁnite training sample drawn from P. This is commonly
addressed by the use of regularization techniques to avoid overﬁtting, e.g. in the context of estimating
f -divergences with M -estimators [24]. In our work, we suggest a novel (Tikhonov) regularizer,
derived and motivated from a training-with-noise scenario, where P and Q are convolved with white
Gaussian noise [30, 3], namely

Fγ(P, Q; φ) := F (P ∗ Λ, Q ∗ Λ; φ), Λ = N (0, γI) .

(3)

(B) density misspeciﬁcation: the model distribution and true distribution both have a density function
with respect to the same base measure but there exists no parameter for which these densities are
sufﬁciently similar. Here, the principle of parameter estimation via divergence minimization is
provably sound in that it achieves a well-deﬁned limit [1, 21]. It therefore provides a solid foundation
for statistical inference that is robust with regard to model misspeciﬁcations.

(C) dimensional misspeciﬁcation: the model distribution and the true distribution do not have a
density function with respect to the same base measure or – even worse – supp(P) ∩ supp(Q) may
be negligible. This may occur, whenever the model and/or data are conﬁned to low-dimensional
manifolds [3, 23]. As pointed out in [3], a geometric mismatch can be detrimental for f -GAN
models as the resulting f -divergence is not ﬁnite (the sup in Eq. (1) is +∞). As a remedy, it has
been suggested to use an alternative family of distance functions known as integral probability
metrics [22, 31]. These include the Wasserstein distance used in Wasserstein GANs (WGAN) [3] as
well as RKHS-induced maximum mean discrepancies [9, 16, 6], which all remain well-deﬁned. We
will provide evidence (analytically and experimentally) that the noise-induced regularization method
proposed in this paper effectively makes f -GAN models robust against dimensional misspeciﬁcations.
While this introduces some dependency on the (Euclidean) metric of the ambient data space, it does
so on a well-controlled length scale (the amplitude of noise or strength of the regularization γ) and
by retaining the beneﬁts of f -divergences. This is a rather gentle modiﬁcation compared to the more
radical departure taken in Wasserstein GANs, which rely solely on the ambient space metric (through
the notion of optimal mass transport).

In what follows, we will take Eq. (3) as the starting point and derive an approximation via a regularizer
that is simple to implement as an integral operator penalizing the squared gradient norm. As opposed
to a naïve norm penalization, each f -divergence has its own characteristic weighting function over
the input space, which depends on the discriminator output. We demonstrate the effectiveness
of our approach on a simple Gaussian mixture as well as on several benchmark image datasets
commonly used for generative models. In both cases, our proposed regularization yields stable GAN
training and produces samples of higher visual quality. We also perform pairwise tests of regularized
vs. unregularized GANs using a novel cross-testing protocol.

In summary, we make the following contributions:

• We systematically derive a novel, efﬁciently computable regularization method for f -GAN.
• We show how this addresses the dimensional misspeciﬁcation challenge.
• We empirically demonstrate stable GAN training across a broad set of models.

2 Background

The fundamental way to learn a generative model in machine learning is to (i) deﬁne a parametric
family of probability densities {Qθ}, θ ∈ Θ ⊆ Rd, and (ii) ﬁnd parameters θ∗ ∈ Θ such that Qθ is
closest (in some sense) to the true distribution P. There are various ways to measure how close model
and real distribution are, or equivalently, various ways to deﬁne a distance or divergence function
between P and Q. In the following we review different notions of divergences used in the literature.

2

f -divergence. GANs [8] are known to minimize the Jensen-Shannon divergence between P and
Q. This was generalized in [25] to f -divergences induced by a convex functions f . An interesting
property of f -divergences is that they permit a variational characterization [24, 27] via

Df (P||Q) := EQ

f ◦

(cid:20)

(cid:21)

dP
dQ

(cid:90)

=

sup
u

X

(cid:18)

u ·

(cid:19)

dP
dQ − f c(u)

dQ,

(4)

where dP/dQ is the Radon-Nikodym derivative and f c(t) ≡ supu∈domf
dual of f . By deﬁning an arbitrary class of statistics Ψ (cid:51) ψ : X → R we arrive at the bound

{ut − f (u)} is the Fenchel

Df (P||Q) ≥ sup
ψ

(cid:90) (cid:18)

ψ ·

(cid:19)

dP
dQ − f c ◦ ψ

dQ = sup
ψ

{EP[ψ] − EQ[f c ◦ ψ]} .

(5)

Eq. (5) thus gives us a variational lower bound on the f -divergence as an expectation over P and
Q, which is easier to evaluate (e.g. via sampling from P and Q, respectively) than the density
based formulation. We can see that by identifying ψ = g ◦ φ and with the choice of f such that
f c = − ln(1 − exp), we get f c ◦ ψ = − ln(1 − σ(φ)) = −g(−φ) thus recovering Eq. (2).

Integral Probability Metrics (IPM). An alternative family of divergences are integral probability
metrics [22, 31], which ﬁnd a witness function to distinguish between P and Q. This class of
methods yields an objective similar to Eq. (2) that requires optimizing a distance function between
two distributions over a function class F. Particular choices for F yield the kernel maximum mean
discrepancy approach of [9, 16] or Wasserstein GANs [3]. The latter distance is deﬁned as

W (P, Q) = sup

{EP[f ] − EQ[f ]},

(cid:107)f (cid:107)L≤1

(6)

where the supremum is taken over functions f which have a bounded Lipschitz constant.

As shown in [3], the Wasserstein metric implies a different notion of convergence compared to the
JS divergence used in the original GAN. Essentially, the Wasserstein metric is said to be weak as it
requires the use of a weaker topology, thus making it easier for a sequence of distribution to converge.
The use of a weaker topology is achieved by restricting the function class to the set of bounded
Lipschitz functions. This yields a hard constraint on the function class that is empirically hard to
satisfy. In [3], this constraint is implemented via weight clipping, which is acknowledged to be a
"terrible way" to enforce the Lipschitz constraint. As will be shown later, our regularization penalty
can be seen as a soft constraint on the Lipschitz constant of the function class which is easy to
implement in practice. Recently, [10] has also proposed a similar regularization; while their proposal
was motivated for Wasserstein GANs and does not extend to f -divergences it is interesting to observe
that both their and our regularization work on the gradient.

Training with Noise. As suggested in [3, 30], one can break the dimensional misspeciﬁcation
discussed in Section 1 by adding continuous noise to the inputs of the discriminator, therefore
smoothing the probability distribution. However, this requires to add high-dimensional noise, which
introduces signiﬁcant variance in the parameter estimation process. Counteracting this requires a
lot of samples and therefore ultimately leads to a costly or impractical solution. Instead we propose
an approach that relies on analytic convolution of the densities P and Q with Gaussian noise. As
we demonstrate below, this yields a simple weighted penalty function on the norm of the gradients.
Conceptually we think of this noise not as being part of the generative process (as in [3]), but rather
as a way to deﬁne a smoother family of discriminants for the variational bound of f -divergences.

Regularization for Mode Dropping. Other regularization techniques address the problem of mode
dropping and are complementary to our approach. This includes the work of [7] which incorporates a
supervised training signal as a regularizer on top of the discriminator target. To implement supervision
the authors use an additional auto-encoder as well as a two-step training procedure which might
be computationally expensive. A similar approach was proposed by [20] that stabilizes GANs by
unrolling the optimization of the discriminator. The main drawback of this approach is that the
computational cost scales with the number of unrolling steps. In general, it is not clear to what extent
these methods not only stabilize GAN training, but also address the conceptual challenges listed in
Section 1.

3

3 Noise-Induced Regularization

From now onwards, we consider the general f -GAN [25] objective deﬁned as

F (P, Q; ψ) ≡ EP[ψ] − EQ[f c ◦ ψ].

(7)

3.1 Noise Convolution

From a practitioners point of view, training with noise can be realized by adding zero-mean random
variables ξ to samples x ∼ P, Q during training. Here we focus on normal white noise ξ ∼ Λ =
N (0, γ I) (the same analysis goes through with a Laplacian noise distribution for instance). From a
theoretical perspective, adding noise is tantamount to convolving the corresponding distribution as

EPEΛ[ψ(x + ξ)] =

ψ(x)

p(x − ξ)λ(ξ)dξ dx =

ψ(x)(p ∗ λ)(x)dx = EP∗Λ[ψ].

(8)

(cid:90)

(cid:90)

(cid:90)

where p and λ are probability densities of P and Λ, respectively, with regard to the Lebesgue measure.
The noise distribution Λ as well as the resulting P∗Λ are guaranteed to have full support in the ambient
space, i.e. λ(x) > 0 and (p ∗ λ)(x) > 0 (∀x). Technically, applying this to both P and Q makes the
resulting generalized f -divergence well-deﬁned, even when the generative model is dimensionally
misspeciﬁed. Note that approximating EΛ through sampling was previously investigated in [30, 3].

3.2 Convolved Discriminants

With symmetric noise, λ(ξ) = λ(−ξ), we can write Eq. (8) equivalently as

EP∗Λ[ψ] = EPEΛ[ψ(x + ξ)] =

p(x)

ψ(x − ξ)λ(−ξ) dξ dx = EP[ψ ∗ λ].

(9)

(cid:90)

(cid:90)

For the Q-expectation in Eq. (7) one gets, by the same argument, EQ∗Λ[f c ◦ ψ] = EQ [(f c ◦ ψ) ∗ λ].
Formally, this generalizes the variational bound for f -divergences in the following manner:
F (P ∗ Λ, Q ∗ Λ; ψ) = F (P, Q; ψ ∗ λ, (f c ◦ ψ) ∗ λ), F (P, Q; ρ, τ ) := EP[ρ] − EQ[τ ]

(10)

Assuming that F is closed under Λ convolutions, the regularization will result in a relative weakening
of the discriminator as we take the sup over a smaller, more regular family. Clearly, the low-pass
effect of Λ-convolutions can be well understood in the Fourier domain. In this equivalent formulation,
we leave P and Q unchanged, yet we change the view the discriminator can take on the ambient data
space: metaphorically speaking, the generator is paired up with a short-sighted adversary.

3.3 Analytic Approximations

In general, it may be difﬁcult to analytically compute ψ ∗ λ or – equivalently – EΛ[ψ(x + ξ)].
However, for small γ we can use a Taylor approximation of ψ around ξ = 0 (cf. [5]):

ψ(x + ξ) = ψ(x) + [∇ ψ(x)]T ξ +

ξT [∇2 ψ(x)] ξ + O(ξ3)

where ∇2 denotes the Hessian, whose trace Tr(∇2) = (cid:52) is known as the Laplace operator. The
properties of white noise result in the approximation

EΛ[ψ(x + ξ)] = ψ(x) +

(cid:52)ψ(x) + O(γ2)

(11)

(12)

and thereby lead directly to an approximation of Fγ (see Eq. (3)) via F = F0 plus a correction, i.e.

Fγ(P, Q; ψ) = F(P, Q; ψ) +

{EP [(cid:52)ψ] − EQ [(cid:52)(f c ◦ ψ)]} + O(γ2) .

(13)

γ
2

We can interpret Eq. (13) as follows: the Laplacian measures how much the scalar ﬁelds ψ and
f c ◦ ψ differ at each point from their local average. It is thereby an inﬁnitesimal proxy for the (exact)
convolution.

The Laplace operator is a sum of d terms, where d is the dimensionality of the ambient data space. As
such it does not suffer from the quadratic blow-up involved in computing the Hessian. If we realize
the discriminator ψ via a deep network, however, then we need to be able to compute the Laplacian
of composed functions. For concreteness, let us assume that ψ = h ◦ G, G = (g1, . . . , gk) and look

1
2

γ
2

4

at a single input x, i.e. gi : R → R, then
g(cid:48)
i · (∂ih ◦ G),

(h ◦ G)(cid:48) =

(cid:88)

(h ◦ G)(cid:48)(cid:48) =

i

(cid:88)

i

g(cid:48)(cid:48)
i · (∂ih ◦ G) +

i · g(cid:48)
g(cid:48)

j · (∂i∂jh ◦ G)

(14)

(cid:88)

i,j

So at the intermediate layer, we would need to effectively operate with a full Hessian, which is
computationally demanding, as has already been observed in [5].

3.4 Efﬁcient Gradient-Based Regularization

We would like to derive a (more) tractable strategy for regularizing ψ, which (i) avoids the detrimental
variance that comes from sampling ξ, (ii) does not rely on explicitly convolving the distributions
P and Q, and (iii) avoids the computation of Laplacians as in Eq. (13). Clearly, this requires to
make further simpliﬁcations. We suggest to exploit properties of the maximizer ψ∗ of F that can be
characterized by [24]

(f c(cid:48) ◦ ψ∗) dQ = dP =⇒ EP[h] = EQ[(f c(cid:48) ◦ ψ∗) · h]

(15)
The relevance of this becomes clear, if we apply the chain rule to (cid:52)(f c ◦ ψ), assuming that f c is
twice differentiable

(∀h, integrable).

(cid:52)(f c ◦ ψ) = (f c(cid:48)(cid:48) ◦ ψ) · ||∇ψ||2 + (cid:0)f c(cid:48) ◦ ψ(cid:1) (cid:52)ψ,

as now we get a convenient cancellation of the Laplacians at ψ = ψ∗ + O(γ)
(f c(cid:48)(cid:48) ◦ ψ∗) · (cid:107)∇ψ∗(cid:107)2(cid:105)

Fγ(P, Q; ψ∗) = F(P, Q; ψ∗) −

EQ

(cid:104)

+ O(γ2) .

γ
2

We can (heuristically) turn this into a regularizer by taking the leading terms,

Fγ(P, Q; ψ) ≈ F(P, Q; ψ) −

Ωf (Q; ψ), Ωf (Q; ψ) := EQ

(cid:104)

(f c(cid:48)(cid:48) ◦ ψ) · (cid:107)∇ψ(cid:107)2(cid:105)

.

(18)

γ
2

Note that we do not assume that the Laplacian terms cancel far away from the optimum, i.e. we do not
assume Eq. (15) to hold for ψ far away from ψ∗. Instead, the underlying assumption we make is that
optimizing the gradient-norm regularized objective Fγ(P, Q; ψ) makes ψ converge to ψ∗ + O(γ),
for which we know that the Laplacian terms cancel [5, 2].
The convexity of f c implies that the weighting function of the squared gradient norm is non-negative,
i.e. f c(cid:48)(cid:48) ≥ 0, which in turn implies that the regularizer − γ
2 Ωf (Q; ψ) is upper bounded (by zero).
Maximization of Fγ(P, Q; ψ) with respect to ψ is therefore well-deﬁned. Further considerations
regarding the well-deﬁnedness of the regularizer can be found in sec. 7.2 in the Appendix.

(16)

(17)

We have shown that training with noise is equivalent to regularizing the discriminator. Inspired by
the above analysis, we propose the following class of f -GAN regularizers:

4 Regularizing GANs

Regularized f -GAN

Fγ(P, Q; ψ) = EP [ψ] − EQ [f c ◦ ψ] −
(f c(cid:48)(cid:48) ◦ ψ) (cid:107)∇ψ(cid:107)2(cid:105)
(cid:104)

Ωf (Q; ψ) := EQ

γ
2

Ωf (Q; ψ)

(19)

The regularizer corresponding to the commonly used parametrization of the Jensen-Shannon GAN
can be derived analogously as shown in the Appendix. We obtain,

Regularized Jensen-Shannon GAN

Fγ(P, Q; ϕ) = EP [ln(ϕ)] + EQ [ln(1 − ϕ)] −

ΩJS(P, Q; ϕ)

γ
2
(cid:2)(1 − ϕ(x))2||∇φ(x)||2(cid:3) + EQ

ΩJS(P, Q; ϕ) := EP

(cid:2)ϕ(x)2||∇φ(x)||2(cid:3)

(20)

where φ = σ−1(ϕ) denotes the logit of the discriminator ϕ. We prefer to compute the gradient of φ
as it is easier to implement and more robust than computing gradients after applying the sigmoid.

5

Algorithm 1 Regularized JS-GAN. Default values: γ0 = 2.0, α = 0.01 (with annealing), γ = 0.1 (without
annealing), nϕ = 1
Require: Initial noise variance γ0, annealing decay rate α, number of discriminator update steps nϕ

per generator iteration, minibatch size m, number of training iterations T
Require: Initial discriminator parameters ω0, initial generator parameters θ0

for t = 1, ..., T do

γ ← γ0 · αt/T # annealing
for 1, ..., nϕ do

Sample minibatch of real data {x(1), ..., x(m)} ∼ P.
Sample minibatch of latent variables from prior {z(1), ..., z(m)} ∼ p(z).

F (ω, θ) =

m
(cid:88)

(cid:104)

(cid:16)

(cid:17)
ϕω(x(i))

ln

(cid:16)

+ ln

1 − ϕω(Gθ(z(i)))

(cid:17)(cid:105)

1
m

1
m

i=1

m
(cid:88)

(cid:20)(cid:16)

i=1
(cid:16)

(cid:16)

1
m

m
(cid:88)

i=1

Ω(ω, θ) =

1 − ϕω(x(i))

(cid:17)2(cid:12)
(cid:12)

(cid:12)∇φω(x(i))(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

2

+ ϕω

(cid:0)Gθ(z(i))(cid:1)2(cid:12)
(cid:12)

(cid:12)∇˜xφω(˜x)(cid:12)
(cid:12)

(cid:12)˜x=Gθ(z(i))

2(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ω ← ω + ∇ω

F (ω, θ) −

Ω(ω, θ)

# gradient ascent

end for
Sample minibatch of latent variables from prior {z(1), ..., z(m)} ∼ p(z).

γ
2

F (ω, θ) =

ln

1 − ϕω(Gθ(z(i)))

or Falt(ω, θ) = −

(cid:17)

(cid:17)

1
m

m
(cid:88)

i=1

(cid:16)
ϕω(Gθ(z(i)))

(cid:17)

ln

θ ← θ − ∇θF(ω, θ) # gradient descent

end for
The gradient-based updates can be performed with any gradient-based learning rule. We used
Adam in our experiments.

4.1 Training Algorithm

Regularizing the discriminator provides an efﬁcient way to convolve the distributions and is thereby
sufﬁcient to address the dimensional misspeciﬁcation challenges outlined in the introduction. This
leaves open the possibility to use the regularizer also in the objective of the generator. On the one
hand, optimizing the generator through the regularized objective may provide useful gradient signal
and therefore accelerate training. On the other hand, it destabilizes training close to convergence
(if not dealt with properly), since the generator is incentiviced to put probability mass where the
discriminator has large gradients. In the case of JS-GANs, we recommend to pair up the regularized
objective of the discriminator with the “alternative” or “non-saturating” objective for the generator,
proposed in [8], which is known to provide strong gradients out of the box (see Algorithm 1).

4.2 Annealing

The regularizer variance γ lends itself nicely to annealing. Our experimental results indicate that a
reasonable annealing scheme consists in regularizing with a large initial γ early in training and then
(exponentially) decaying γ to a small non-zero value. We leave to future work the question of how to
determine an optimal annealing schedule.

5 Experiments

5.1

2D submanifold mixture of Gaussians in 3D space

To demonstrate the stabilizing effect of the regularizer, we train a simple GAN architecture [20] on a
2D submanifold mixture of seven Gaussians arranged in a circle and embedded in 3D space (further
details and an illustration of the mixture distribution are provided in the Appendix). We emphasize
that this mixture is degenerate with respect to the base measure deﬁned in ambient space as it does
not have fully dimensional support, thus precisely representing one of the failure scenarios commonly

6

.

G
E
R
N
U

1
0
0

.

0

.

1

Figure 1: 2D submanifold mixture. The ﬁrst row shows one of several unstable unregularized GANs
trained to learn the dimensionally misspeciﬁed mixture distribution. The remaining rows show
regularized GANs (with regularized objective for the discriminator and unregularized objective for
the generator) for different levels of regularization γ. Even for small but non-zero noise variance, the
regularized GAN can essentially be trained indeﬁnitely without collapse. The color of the samples is
proportional to the density estimated from a Gaussian KDE ﬁt. The target distribution is shown in
Fig. 5. GANs were trained with one discriminator update per generator update step (indicated).

described in the literature [3]. The results are shown in Fig. 1 for both standard unregularized GAN
training as well as our regularized variant.

While the unregularized GAN collapses in literally every run after around 50k iterations, due to the
fact that the discriminator concentrates on ever smaller differences between generated and true data
(the stakes are getting higher as training progresses), the regularized variant can be trained essentially
indeﬁnitely (well beyond 200k iterations) without collapse for various degrees of noise variance, with
and without annealing. The stabilizing effect of the regularizer is even more pronounced when the
GANs are trained with ﬁve discriminator updates per generator update step, as shown in Fig. 6.

5.2 Stability across various architectures

To demonstrate the stability of the regularized training procedure and to showcase the excellent
quality of the samples generated from it, we trained various network architectures on the CelebA [17],
CIFAR-10 [15] and LSUN bedrooms [32] datasets. In addition to the deep convolutional GAN
(DCGAN) of [26], we trained several common architectures that are known to be hard to train
[4, 26, 19], therefore allowing us to establish a comparison to the concurrently proposed gradient-
penalty regularizer for Wasserstein GANs [10]. Among these architectures are a DCGAN without
any normalization in either the discriminator or the generator, a DCGAN with tanh activations and a
deep residual network (ResNet) GAN [11]. We used the open-source implementation of [10] for our
experiments on CelebA and LSUN, with one notable exception: we use batch normalization also for
the discriminator (as our regularizer does not depend on the optimal transport plan or more precisely
the gradient penalty being imposed along it).
All networks were trained using the Adam optimizer [13] with learning rate 2 × 10−4 and hyper-
parameters recommended by [26]. We trained all datasets using batches of size 64, for a total of
200K generator iterations in the case of LSUN and 100k iterations on CelebA. The results of these
experiments are shown in Figs. 3 & 2. Further implementation details can be found in the Appendix.

5.3 Training time

We empirically found regularization to increase the overall training time by a marginal factor
of roughly 1.4 (due to the additional backpropagation through the computational graph of the
discriminator gradients). More importantly, however, (regularized) f -GANs are known to converge
(or at least generate good looking samples) faster than their WGAN relatives [10].

7

RESNET

DCGAN

NO NORMALIZATION

TANH

Figure 2: Stability accross various architectures: ResNet, DCGAN, DCGAN without normalization
and DCGAN with tanh activations (details in the Appendix). All samples were generated from
regularized GANs with exponentially annealed γ0 = 2.0 (and alternative generator loss) as described
in Algorithm 1. Samples were produced after 200k generator iterations on the LSUN dataset (see also
Fig. 8 for a full-resolution image of the ResNet GAN). Samples for the unregularized architectures
can be found in the Appendix.

UNREG.

0.5

1.0

2.0

Figure 3: Annealed Regularization. CelebA samples generated by (un)regularized ResNet GANs.
The initial level of regularization γ0 is shown below each batch of images. γ0 was exponentially
annealed as described in Algorithm 1. The regularized GANs can be trained essentially indeﬁnitely
without collapse, the superior quality is again evident. Samples were produced after 100k generator
iterations.

5.4 Regularization vs. explicitly adding noise

We compare our regularizer against the common practitioner’s approach to explicitly adding noise to
images during training. In order to compare both approaches (analytic regularizer vs. explicit noise),
we ﬁx a common batch size (64 in our case) and subsequently train with different noise-to-signal
ratios (NSR): we take (batch-size/NSR) samples (both from the dataset and generated ones) to each
of which a number of NSR noise vectors is added and feed them to the discriminator (so that overall
both models are trained on the same batch size). We experimented with NSR 1, 2, 4, 8 and show the
best performing ratio (further ratios in the Appendix). Explicitly adding noise in high-dimensional
ambient spaces introduces additional sampling variance which is not present in the regularized variant.
The results, shown in Fig. 4, conﬁrm that the regularizer stabilizes across a broad range of noise
levels and manages to produce images of considerably higher quality than the unregularized variants.

5.5 Cross-testing protocol

We propose the following pairwise cross-testing protocol to assess the relative quality of two GAN
models: unregularized GAN (Model 1) vs. regularized GAN (Model 2). We ﬁrst report the confusion
matrix (classiﬁcation of 10k samples from the test set against 10k generated samples) for each model
separately. We then classify 10k samples generated by Model 1 with the discriminator of Model 2
and vice versa. For both models, we report the fraction of false positives (FP) (Type I error) and false
negatives (FN) (Type II error). The discriminator with the lower FP (and/or lower FN) rate deﬁnes
the better model, in the sense that it is able to more accurately classify out-of-data samples, which
indicates better generalization properties. We obtained the following results on CIFAR-10:

8

UNREGULARIZED

0.01

EXPLICIT NOISE
0.1

1.0

0.001

0.01

0.1

1.0

REGULARIZED

Figure 4: CIFAR-10 samples generated by (un)regularized DCGANs (with alternative generator loss),
as well as by training a DCGAN with explicitly added noise (noise-to-signal ratio 4). The level of
regularization or noise γ is shown above each batch of images. The regularizer stabilizes across a
broad range of noise levels and manages to produce images of higher quality than the unregularized
variants. Samples were produced after 50 training epochs.

Regularized GAN (γ = 0.1)

Unregularized GAN

True condition
Positive Negative
0.0002
0.9688
0.9998
0.0312

Predicted

Positive
Negative

Cross-testing: FP: 0.0

True condition
Positive Negative
0.0013
0.9987

1.0
0.0

Predicted

Positive
Negative

Cross-testing: FP: 1.0

For both models, the discriminator is able to recognize his own generator’s samples (low FP in the
confusion matrix). The regularized GAN also manages to perfectly classify the unregularized GAN’s
samples as fake (cross-testing FP 0.0) whereas the unregularized GAN classiﬁes the samples of the
regularized GAN as real (cross-testing FP 1.0). In other words, the regularized model is able to fool
the unregularized one, whereas the regularized variant cannot be fooled.

6 Conclusion

We introduced a regularization scheme to train deep generative models based on generative adversarial
networks (GANs). While dimensional misspeciﬁcations or non-overlapping support between the
data and model distributions can cause severe failure modes for GANs, we showed that this can be
addressed by adding a penalty on the weighted gradient-norm of the discriminator. Our main result is
a simple yet effective modiﬁcation of the standard training algorithm for GANs, turning them into
reliable building blocks for deep learning that can essentially be trained indeﬁnitely without collapse.
Our experiments demonstrate that our regularizer improves stability, prevents GANs from overﬁtting
and therefore leads to better generalization properties (cf cross-testing protocol). Further research on
the optimization of GANs as well as their convergence and generalization can readily be built upon
our theoretical results.

9

Acknowledgements

We would like to thank Devon Hjelm for pointing out that the regularizer works well with ResNets.
KR is thankful to Yannic Kilcher, Lars Mescheder, Paulina Grnarova and the dalab team for insightful
discussions. Big thanks also to Ishaan Gulrajani and Taehoon Kim for their open-source GAN
implementations. This work was supported by Microsoft Research through its PhD Scholarship
Programme.

References

matical Soc., 2007.

[1] Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry. American Mathe-

[2] Guozhong An. The effects of adding noise during backpropagation training on a generalization

performance. Neural Comput., pages 643–674, 1996.

[3] Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adver-

sarial networks. In ICLR, 2017.

[4] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial

networks. In PMLR, Proceedings of Machine Learning Research, 2017.

[5] Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computa-

tion, 7:108–116, 1995.

[6] Diane Bouchacourt, Pawan K Mudigonda, and Sebastian Nowozin. Disco nets: Dissimilarity
coefﬁcients networks. In Advances in Neural Information Processing Systems, pages 352–360,
2016.

[7] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized

generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.

[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. In Advances in
Neural Information Processing Systems, pages 2672–2680, 2014.

[9] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research, 13:723–773, 2012.

[10] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems,
2017.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page
770–778, 2016.

[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In PMLR, Proceedings of Machine Learning Research,
pages 448–456, 2015.

[13] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. The

International Conference on Learning Representations (ICLR), 2014.

[14] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. The International

Conference on Learning Representations (ICLR), 2013.

[15] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

[16] Yujia Li, Kevin Swersky, and Richard S Zemel. Generative moment matching networks. In

CS UToronto, 2009.

ICML, pages 1718–1727, 2015.

[17] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in
the wild. In Proceedings of the IEEE International Conference on Computer Vision, pages
3730–3738, 2015.

[18] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the

wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.

10

[19] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances

in Neural Information Processing Systems, 2017.

[20] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial

networks. In International Conference on Learning Representations (ICLR), 2016.

[21] Tom Minka. Divergence measures and message passing. Technical report, Microsoft Research,

2005.

[22] Alfred Müller. Integral probability metrics and their generating classes of functions. Advances

in Applied Probability, 29:429–443, 1997.

[23] Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis.

In Advances in Neural Information Processing Systems, pages 1786–1794, 2010.

[24] XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence func-
tionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information
Theory, 56(11):5847–5861, 2010.

[25] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neu-
ral samplers using variational divergence minimization. In Advances in Neural Information
Processing Systems, pages 271–279, 2016.

[26] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[27] Mark D Reid and Robert C Williamson. Information, divergence and risk for binary experiments.

Journal of Machine Learning Research, 12:731–817, 2011.

[28] Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31st International
Conference on Machine Learning, 2014.

[29] David W Scott. Multivariate density estimation: theory, practice, and visualization. John Wiley

& Sons, 2015.

[30] Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszár. Amortised

map inference for image super-resolution. arXiv preprint arXiv:1610.04490, 2016.

[31] Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, and Gert RG
Lanckriet. On integral probability metrics, phi-divergences and binary classiﬁcation. arXiv
preprint arXiv:0901.2698, 2009.

[32] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction
of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365, 2015.

11

7 APPENDIX

7.1 Derivation of the Jensen-Shannon Regularizer.

The Jensen-Shannon GAN is typically encountered in one of two equivalent parametrizations: the
commonly used “original” GAN parametrization [8],

and the Fenchel-dual f -GAN parametrization [25], where f c = − ln(1 − exp),
EP [ψ] − EQ [f c ◦ ψ] = EP [ψ] + EQ [ln(1 − exp(ψ))]

EP [ln(ϕ)] + EQ [ln(1 − ϕ)]

(21)

(22)

Depending on whether we train the JS-GAN through its Fenchel-dual parametrization or with the
original GAN objective we have to either use the regularizer in the general f -GAN form in Eq. (19),
or in the speciﬁc Jensen-Shannon parametrization given in Eq. (20).

We now show how to derive the speciﬁc Jensen-Shannon regularizer, which is basically a repetition
of the derivation of the general f -GAN regularizer presented in the main text. Using the same
terminology and following the same line of thought as in section 3.3, i.e. assuming the noise variance
is small, we can Taylor approximate the statistics ϕ around ξ = 0,

ϕ(x + ξ) = ϕ(x) + [∇ ϕ(x)] ξ +

ξT [∇2 ϕ(x)] ξ + O(ξ3)

(23)

1
2

Expanding the noise convolved version of the objective in Eq. (21) and making use of the zero-mean
and uncorrelatedness properties of the noise distribution, as well as applying the chain rule, we obtain

Fγ(P, Q; ϕ) = EP [ln(ϕ)] + EQ [ln(1 − ϕ)]
(cid:52)ϕ − ||∇ ln(ϕ)||2(cid:105)
(cid:104) 1
ϕ

γ
2

EP

+

(cid:26)

(cid:104)

+ EQ

−

1
1 − ϕ

(cid:52)ϕ − ||∇ ln(1 − ϕ)||2(cid:105)(cid:27)

+ O(γ2) .

(24)
Following the same arguments as in section 3.4, one can again show that the Laplacian terms cancel
at ϕ = ϕ∗ + O(γ) and we arrive at

Fγ(P, Q; ϕ) = EP [ln(ϕ)] + EQ [ln(1 − ϕ)]
(cid:2)||∇ ln(ϕ)||2(cid:3) + EQ

γ
2
allowing us to read off the corresponding JS regularizer,

(cid:8)EP

−

ΩJS(P, Q; ϕ) = EP

(cid:2)||∇ ln(1 − ϕ(x))||2(cid:3)

(cid:2)||∇ ln(1 − ϕ)||2(cid:3)(cid:9) + O(γ2) ,

1

(cid:2)||∇ ln ϕ(x)||2(cid:3) + EQ
(ϕ(x))2 ||∇ϕ(x)||2(cid:105)
(cid:104)
(ϕ(x))2 ||σ(cid:48)(φ(x))∇φ(x)||2(cid:105)

+ EQ

1

(cid:104)

(cid:104)

= EP

= EP

1

(1 − ϕ(x))2 ||∇ϕ(x))||2(cid:105)

+ EQ

(cid:104)

1

(1 − ϕ(x))2 ||σ(cid:48)(φ(x))∇φ(x)||2(cid:105)

In order to obtain the regularizer in the logit-parameterization, with ϕ(x) = σ(φ(x)), we make use
of the following property of the sigmoid

σ(cid:48)(t) = σ(t)(1 − σ(t))

which allows us to write

ΩJS(P, Q; φ) = EP

(cid:2)(1 − ϕ(x))2||∇φ(x)||2(cid:3) + EQ

(cid:2)ϕ(x)2||∇φ(x)||2(cid:3)

Let us ﬁnally also show that these two parametrizations are indeed equivalent at the optimum:

f c(cid:48)(cid:48)(t) =

exp(t)
(1 − exp(t))2

=⇒ f c(cid:48)(cid:48)(ln(ϕ∗)) =

ϕ∗
(1 − ϕ∗)2 =

ϕ∗
1 − ϕ∗ +

ϕ∗2
(1 − ϕ∗)2

Thus,

f c(cid:48)(cid:48)(ln(ϕ∗))(cid:107)∇ ln(ϕ∗)(cid:107)2dQ =

(25)

(26)

(27)

(28)

(29)

(30)

(cid:16) ϕ∗
1 − ϕ∗ +
= (cid:107)∇ ln(ϕ∗)(cid:107)2dP + (cid:107)

ϕ∗2
(1 − ϕ∗)2

(cid:17)

(cid:107)∇ ln(ϕ∗)(cid:107)2dQ

ϕ∗
1 − ϕ∗ ∇ ln(ϕ∗)(cid:107)2dQ

= (cid:107)∇ ln(ϕ∗)(cid:107)2dP + (cid:107)∇ ln(1 − ϕ∗)(cid:107)2dQ

12

7.2 Further Considerations Regarding the Regularizer

Following-up on our discussion of the regularizer in section 3.4: Due to a support or dimensionality
mismatch we may have sup F (P, Q; ψ) = +∞ and ψ∗ may not exist. However, with ψ∗
(cid:15) being the
maximizer of F(cid:15) (which is guaranteed to exist for any (cid:15) > 0), we get

Fγ(P ∗ N (0, (cid:15)I), Q ∗ N (0, (cid:15)I)) = F(cid:15)(P, Q; ψ∗

(cid:15) ) −

Ωf (Q ∗ N (0, (cid:15)I); ψ∗

(cid:15) ) + O(γ2) .

(31)

γ
2

(cid:15) (cid:107). The sequence of regularizers Ωf (Q ∗ N (0, (cid:15)I); ·),
As (cid:15) → 0, F(cid:15) may diverge and so may (cid:107)∇ψ∗
however, converges (at least pointwise) to a well deﬁned limit, which is Ωf (Q; ·). This shows that
the regularizer is well-deﬁned even under dimensional misspeciﬁcations.

We can also justify the approximation in Eq. (18) more rigorously. Starting from the Taylor approxi-
mation of f c(cid:48) at ψ∗, we get pointwise

f c(cid:48) ◦ ψ = f c(cid:48) ◦ ψ∗ + f c(cid:48)(cid:48) ◦ (ψ − ψ∗) + O(|ψ − ψ∗|2)

(32)
(cid:2)(f c(cid:48)(cid:48) ◦ (ψ − ψ∗)) · (cid:52)ψ(cid:3). Using
So in ﬁrst order of (cid:107)ψ −ψ∗(cid:107)∞, the approximation error is ∆ := EQ
Green’s identity, we can derive the following bound, ∆ ≤ O(Lδ), where we assume |f c(cid:48)(cid:48)(cid:48)| ≤ L and
(cid:107)∇ψ∗ − ∇ψ(cid:107)∞ < δ. However, as this result only gives a formal guarantee for (ψ, ∇ψ) sufﬁciently
close to (ψ∗, ∇ψ∗), we refrain from presenting further technical details.

7.3 2D Submanifold Mixture of Gaussians in 3D Space

Experimental Setup. The experimental setup is inspired by the two-dimensional mixture of Gaus-
sians in [20]. The dataset is constructed as follows. We sample from a mixture of seven Gaussians
with standard deviation 0.01 and means equally spaced around the unit circle. This 2D mixture
√
is then embedded in 3D space (x, y) → (x, y, 0), rotated by π/4 around the axis (1, −1, 0)/
2
3. As illustrated in Fig. 5, this yields a mixture distribution that lives
and translated by (1, 1, 1)/
in a tilted 2D submanifold embedded in 3D space. It is important to emphasize that the mixture
distribution is by design degenerate with respect to the base measure in 3D as it does not have full
dimensional support. This precisely represents the dimensional misspeciﬁcation scenario for GANs
that we aim to address with our regularizer.

√

Figure 5: 2D submanifold mixture of Gaussians in 3D, obtained by embedding a two-dimensional
mixture with means equally spaced around the unit circle in 3D ambient space. For visualization
purposes, the standard deviation of the shown mixture components is 10x larger than the one used
in the experiments. Samples are colored proportional to their density which we estimated from a
Gaussian KDE with bandwidth selected using Scott’s rule [29].

Architecture. The architecture corresponds to the one used in [20] with one notable exception. We
use 2 dimensional latent vectors z, sampled from a multivariate normal prior, (whereas [20] uses 256
dimensional z), as we found lower dimensional latent variables greatly improve the performance of
the unregularized GAN against which we compare. We did all experiments also for latent vectors of
dimension 15: the obtained results are in accordance with those presented in the main text and below.

Both networks are optimized using Adam with a learning rate of 1e−3 and standard hyper-parameters.
We trained on batches of size 512. Ten batches were generated to produce one image of the mixture
at given time steps. The generator and discriminator network parameters were updated alternatively.

13

.

G
E
R
N
U

1
0
0

.

1
0

.

0
1

.

Figure 6: 2D submanifold mixture. The ﬁrst row shows one of several unstable unregularized
GANs trying to learn the dimensionally misspeciﬁed mixture distribution. The remaining rows show
regularized GANs (with regularized objective for the discriminator and unregularized objective for
the generator) for different levels of regularization γ. Even for small but non-zero noise variance, the
regularized GAN can essentially be trained indeﬁnitely without collapse. The color of the samples is
proportional to the density estimated from a Gaussian KDE ﬁt. The target distribution is shown in
Fig. 5. GANs were trained with ﬁve discriminator updates per generator update step (indicated).

7.4

Image Datasets and Network Architectures

We trained on CelebA [18], CIFAR-10 [15] and LSUN [32]. All datasets were trained on minibatches
of size 64. The respective GAN architectures are listed below.

DCGAN. For
the CIFAR-10 experiments, we used the DCGAN (Deep Convolutional
GAN) architecture of [26] implemented in Tensorﬂow by https://github.com/carpedm20/
DCGAN-tensorflow.

For the LSUN experiments, we used the DCGAN architecture of [26] implemented in Tensorﬂow by
https://github.com/igul222/improved_wgan_training.

In both cases, the discriminator uses batch normalization [12] in all but the ﬁrst and last layer (except
where explicitly stated otherwise). The generator uses batch normalization in all layers except the last
one. The latent code is sampled from a 100-dimensional uniform distribution over [0, 1] (carpedm20)
resp. 128-dimensional normal-(0,1) distribution (Gulrajani).

Both networks were trained using the Adam optimizer [13] (with hyper-parameters recommended
by the DCGAN authors) for various different learning rates in the range [0.001, 0.0001]. The
recommended learning rate 2 × 10−4 was found to perform best.

ResNet GAN. For the CelebA and LSUN experiments, we used a deep residual network
ResNet [11] implemented in Tensorﬂow by https://github.com/igul222/improved_wgan_
training (implementation details can be found in [10]).

The ResNets use pre-activation residual blocks with two 3 x 3 convolutional layers each and ReLU
nonlinearities. The generator has one linear layer, followed by four residual blocks, one deconvolu-
tional layer and tanh output activations. The discriminator has one convolutional layer, followed by
four residual blocks and a linear output layer (the discriminator logits are then fed into the sigmoid
GAN loss).

14

We use batch normalization [12] in the generator and discriminator (except explicitly stated otherwise).
Both networks were optimized using Adam with learning rate 2 × 10−4 and standard hyperparameters.
For further architectural details, please refer to [10] and the excellent open-source implementation
referenced above.

7.5 Further Experimental Results.

RESNET

DCGAN

NO NORMALIZATION

TANH

Figure 7: Unregularized GANs (with alternative generator loss) accross various architectures: ResNet,
DCGAN, DCGAN without normalization and DCGAN with tanh activations. Samples were produced
after 200k generator iterations on the LSUN dataset. Samples for the regularized architectures can
be found in the main text. Some of these architectures are known to be hard to train.

Figure 8: Regularized ResNet GAN. Full-resolution 64 × 64 images generated by the regularized
ResNet GAN after 200k generator iterations on the LSUN dataset. The initial level of regulariza-
tion γ0 = 2.0 was exponentially annealed to γT = 0.01.

15

1

2

4

8

1
0
0

.

1
0

.

0
.
1

1
0
.
0

1
.
0

0
.
1

Figure 9: DCGAN trained with explicitly added noise. Top 3 rows: discriminator and generator
trained with noise. Bottom 3 rows: discriminator is trained with noise while generator is trained
without noise. White normal noise is added to images from the dataset as well as to samples from the
generator during training. The generator was trained through the alternative loss. The level of noise
γ is shown in the left most column, the different noise-to-signal ratios (NSR) above each column of
images. Samples were produced after 50 training epochs.

16

7
1
0
2
 
v
o
N
 
7
 
 
]

G
L
.
s
c
[
 
 
2
v
7
6
3
9
0
.
5
0
7
1
:
v
i
X
r
a

Stabilizing Training of Generative Adversarial
Networks through Regularization

Kevin Roth
Department of Computer Science
ETH Zürich
kevin.roth@inf.ethz.ch

Aurelien Lucchi
Department of Computer Science
ETH Zürich
aurelien.lucchi@inf.ethz.ch

Sebastian Nowozin
Microsoft Research
Cambridge, UK
sebastian.Nowozin@microsoft.com

Thomas Hofmann
Department of Computer Science
ETH Zürich
thomas.hofmann@inf.ethz.ch

Abstract

Deep generative models based on Generative Adversarial Networks (GANs) have
demonstrated impressive sample quality but in order to work they require a careful
choice of architecture, parameter initialization, and selection of hyper-parameters.
This fragility is in part due to a dimensional mismatch or non-overlapping support
between the model distribution and the data distribution, causing their density ratio
and the associated f -divergence to be undeﬁned. We overcome this fundamental
limitation and propose a new regularization approach with low computational cost
that yields a stable GAN training procedure. We demonstrate the effectiveness of
this regularizer across several architectures trained on common benchmark image
generation tasks. Our regularization turns GAN models into reliable building
blocks for deep learning.1

1

Introduction

A recent trend in the world of generative models is the use of deep neural networks as data generating
mechanisms. Two notable approaches in this area are variational auto-encoders (VAEs) [14, 28] as
well as generative adversarial networks (GAN) [8]. GANs are especially appealing as they move
away from the common likelihood maximization viewpoint and instead use an adversarial game
approach for training generative models. Let us denote by P(x) and Qθ(x) the data and model
distribution, respectively. The basic idea behind GANs is to pair up a θ-parametrized generator
network that produces Qθ with a discriminator which aims to distinguish between P and Qθ, whereas
the generator aims for making Qθ indistinguishable from P. Effectively the discriminator represents
a class of objective functions F that measures dissimilarity of pairs of probability distributions. The
ﬁnal objective is then formed via a supremum over F, leading to the saddle point problem

(cid:20)
(cid:96)(Qθ; F) := sup
F ∈F

min
θ

(cid:21)
F (P, Qθ)

.

The standard way of representing a speciﬁc F is through a family of statistics or discriminants φ ∈ Φ,
typically realized by a neural network [8, 26]. In GANs, we use these discriminators in a logistic
classiﬁcation loss as follows

F (P, Q; φ) = EP [g(φ(x))] + EQ [g(−φ(x))] ,

1Code available at https://github.com/rothk/Stabilizing_GANs

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

(1)

(2)

where g(z) = ln(σ(z)) is the log-logistic function (for reference, σ(φ(x)) = D(x) in [8]).
As shown in [8], for the Bayes-optimal discriminator φ∗ ∈ Φ, the above generator objective reduces
to the Jensen-Shannon (JS) divergence between P and Q. The work of [25] later generalized this to
a more general class of f -divergences, which gives more ﬂexibility in cases where the generative
model may not be expressive enough or where data may be scarce.

We consider three different challenges for learning the model distribution:

(A) empirical estimation: the model family may contain the true distribution or a good approximation
thereof, but one has to identify it based on a ﬁnite training sample drawn from P. This is commonly
addressed by the use of regularization techniques to avoid overﬁtting, e.g. in the context of estimating
f -divergences with M -estimators [24]. In our work, we suggest a novel (Tikhonov) regularizer,
derived and motivated from a training-with-noise scenario, where P and Q are convolved with white
Gaussian noise [30, 3], namely

Fγ(P, Q; φ) := F (P ∗ Λ, Q ∗ Λ; φ), Λ = N (0, γI) .

(3)

(B) density misspeciﬁcation: the model distribution and true distribution both have a density function
with respect to the same base measure but there exists no parameter for which these densities are
sufﬁciently similar. Here, the principle of parameter estimation via divergence minimization is
provably sound in that it achieves a well-deﬁned limit [1, 21]. It therefore provides a solid foundation
for statistical inference that is robust with regard to model misspeciﬁcations.

(C) dimensional misspeciﬁcation: the model distribution and the true distribution do not have a
density function with respect to the same base measure or – even worse – supp(P) ∩ supp(Q) may
be negligible. This may occur, whenever the model and/or data are conﬁned to low-dimensional
manifolds [3, 23]. As pointed out in [3], a geometric mismatch can be detrimental for f -GAN
models as the resulting f -divergence is not ﬁnite (the sup in Eq. (1) is +∞). As a remedy, it has
been suggested to use an alternative family of distance functions known as integral probability
metrics [22, 31]. These include the Wasserstein distance used in Wasserstein GANs (WGAN) [3] as
well as RKHS-induced maximum mean discrepancies [9, 16, 6], which all remain well-deﬁned. We
will provide evidence (analytically and experimentally) that the noise-induced regularization method
proposed in this paper effectively makes f -GAN models robust against dimensional misspeciﬁcations.
While this introduces some dependency on the (Euclidean) metric of the ambient data space, it does
so on a well-controlled length scale (the amplitude of noise or strength of the regularization γ) and
by retaining the beneﬁts of f -divergences. This is a rather gentle modiﬁcation compared to the more
radical departure taken in Wasserstein GANs, which rely solely on the ambient space metric (through
the notion of optimal mass transport).

In what follows, we will take Eq. (3) as the starting point and derive an approximation via a regularizer
that is simple to implement as an integral operator penalizing the squared gradient norm. As opposed
to a naïve norm penalization, each f -divergence has its own characteristic weighting function over
the input space, which depends on the discriminator output. We demonstrate the effectiveness
of our approach on a simple Gaussian mixture as well as on several benchmark image datasets
commonly used for generative models. In both cases, our proposed regularization yields stable GAN
training and produces samples of higher visual quality. We also perform pairwise tests of regularized
vs. unregularized GANs using a novel cross-testing protocol.

In summary, we make the following contributions:

• We systematically derive a novel, efﬁciently computable regularization method for f -GAN.
• We show how this addresses the dimensional misspeciﬁcation challenge.
• We empirically demonstrate stable GAN training across a broad set of models.

2 Background

The fundamental way to learn a generative model in machine learning is to (i) deﬁne a parametric
family of probability densities {Qθ}, θ ∈ Θ ⊆ Rd, and (ii) ﬁnd parameters θ∗ ∈ Θ such that Qθ is
closest (in some sense) to the true distribution P. There are various ways to measure how close model
and real distribution are, or equivalently, various ways to deﬁne a distance or divergence function
between P and Q. In the following we review different notions of divergences used in the literature.

2

f -divergence. GANs [8] are known to minimize the Jensen-Shannon divergence between P and
Q. This was generalized in [25] to f -divergences induced by a convex functions f . An interesting
property of f -divergences is that they permit a variational characterization [24, 27] via

Df (P||Q) := EQ

f ◦

(cid:20)

(cid:21)

dP
dQ

(cid:90)

=

sup
u

X

(cid:18)

u ·

(cid:19)

dP
dQ − f c(u)

dQ,

(4)

where dP/dQ is the Radon-Nikodym derivative and f c(t) ≡ supu∈domf
dual of f . By deﬁning an arbitrary class of statistics Ψ (cid:51) ψ : X → R we arrive at the bound

{ut − f (u)} is the Fenchel

Df (P||Q) ≥ sup
ψ

(cid:90) (cid:18)

ψ ·

(cid:19)

dP
dQ − f c ◦ ψ

dQ = sup
ψ

{EP[ψ] − EQ[f c ◦ ψ]} .

(5)

Eq. (5) thus gives us a variational lower bound on the f -divergence as an expectation over P and
Q, which is easier to evaluate (e.g. via sampling from P and Q, respectively) than the density
based formulation. We can see that by identifying ψ = g ◦ φ and with the choice of f such that
f c = − ln(1 − exp), we get f c ◦ ψ = − ln(1 − σ(φ)) = −g(−φ) thus recovering Eq. (2).

Integral Probability Metrics (IPM). An alternative family of divergences are integral probability
metrics [22, 31], which ﬁnd a witness function to distinguish between P and Q. This class of
methods yields an objective similar to Eq. (2) that requires optimizing a distance function between
two distributions over a function class F. Particular choices for F yield the kernel maximum mean
discrepancy approach of [9, 16] or Wasserstein GANs [3]. The latter distance is deﬁned as

W (P, Q) = sup

{EP[f ] − EQ[f ]},

(cid:107)f (cid:107)L≤1

(6)

where the supremum is taken over functions f which have a bounded Lipschitz constant.

As shown in [3], the Wasserstein metric implies a different notion of convergence compared to the
JS divergence used in the original GAN. Essentially, the Wasserstein metric is said to be weak as it
requires the use of a weaker topology, thus making it easier for a sequence of distribution to converge.
The use of a weaker topology is achieved by restricting the function class to the set of bounded
Lipschitz functions. This yields a hard constraint on the function class that is empirically hard to
satisfy. In [3], this constraint is implemented via weight clipping, which is acknowledged to be a
"terrible way" to enforce the Lipschitz constraint. As will be shown later, our regularization penalty
can be seen as a soft constraint on the Lipschitz constant of the function class which is easy to
implement in practice. Recently, [10] has also proposed a similar regularization; while their proposal
was motivated for Wasserstein GANs and does not extend to f -divergences it is interesting to observe
that both their and our regularization work on the gradient.

Training with Noise. As suggested in [3, 30], one can break the dimensional misspeciﬁcation
discussed in Section 1 by adding continuous noise to the inputs of the discriminator, therefore
smoothing the probability distribution. However, this requires to add high-dimensional noise, which
introduces signiﬁcant variance in the parameter estimation process. Counteracting this requires a
lot of samples and therefore ultimately leads to a costly or impractical solution. Instead we propose
an approach that relies on analytic convolution of the densities P and Q with Gaussian noise. As
we demonstrate below, this yields a simple weighted penalty function on the norm of the gradients.
Conceptually we think of this noise not as being part of the generative process (as in [3]), but rather
as a way to deﬁne a smoother family of discriminants for the variational bound of f -divergences.

Regularization for Mode Dropping. Other regularization techniques address the problem of mode
dropping and are complementary to our approach. This includes the work of [7] which incorporates a
supervised training signal as a regularizer on top of the discriminator target. To implement supervision
the authors use an additional auto-encoder as well as a two-step training procedure which might
be computationally expensive. A similar approach was proposed by [20] that stabilizes GANs by
unrolling the optimization of the discriminator. The main drawback of this approach is that the
computational cost scales with the number of unrolling steps. In general, it is not clear to what extent
these methods not only stabilize GAN training, but also address the conceptual challenges listed in
Section 1.

3

3 Noise-Induced Regularization

From now onwards, we consider the general f -GAN [25] objective deﬁned as

F (P, Q; ψ) ≡ EP[ψ] − EQ[f c ◦ ψ].

(7)

3.1 Noise Convolution

From a practitioners point of view, training with noise can be realized by adding zero-mean random
variables ξ to samples x ∼ P, Q during training. Here we focus on normal white noise ξ ∼ Λ =
N (0, γ I) (the same analysis goes through with a Laplacian noise distribution for instance). From a
theoretical perspective, adding noise is tantamount to convolving the corresponding distribution as

EPEΛ[ψ(x + ξ)] =

ψ(x)

p(x − ξ)λ(ξ)dξ dx =

ψ(x)(p ∗ λ)(x)dx = EP∗Λ[ψ].

(8)

(cid:90)

(cid:90)

(cid:90)

where p and λ are probability densities of P and Λ, respectively, with regard to the Lebesgue measure.
The noise distribution Λ as well as the resulting P∗Λ are guaranteed to have full support in the ambient
space, i.e. λ(x) > 0 and (p ∗ λ)(x) > 0 (∀x). Technically, applying this to both P and Q makes the
resulting generalized f -divergence well-deﬁned, even when the generative model is dimensionally
misspeciﬁed. Note that approximating EΛ through sampling was previously investigated in [30, 3].

3.2 Convolved Discriminants

With symmetric noise, λ(ξ) = λ(−ξ), we can write Eq. (8) equivalently as

EP∗Λ[ψ] = EPEΛ[ψ(x + ξ)] =

p(x)

ψ(x − ξ)λ(−ξ) dξ dx = EP[ψ ∗ λ].

(9)

(cid:90)

(cid:90)

For the Q-expectation in Eq. (7) one gets, by the same argument, EQ∗Λ[f c ◦ ψ] = EQ [(f c ◦ ψ) ∗ λ].
Formally, this generalizes the variational bound for f -divergences in the following manner:
F (P ∗ Λ, Q ∗ Λ; ψ) = F (P, Q; ψ ∗ λ, (f c ◦ ψ) ∗ λ), F (P, Q; ρ, τ ) := EP[ρ] − EQ[τ ]

(10)

Assuming that F is closed under Λ convolutions, the regularization will result in a relative weakening
of the discriminator as we take the sup over a smaller, more regular family. Clearly, the low-pass
effect of Λ-convolutions can be well understood in the Fourier domain. In this equivalent formulation,
we leave P and Q unchanged, yet we change the view the discriminator can take on the ambient data
space: metaphorically speaking, the generator is paired up with a short-sighted adversary.

3.3 Analytic Approximations

In general, it may be difﬁcult to analytically compute ψ ∗ λ or – equivalently – EΛ[ψ(x + ξ)].
However, for small γ we can use a Taylor approximation of ψ around ξ = 0 (cf. [5]):

ψ(x + ξ) = ψ(x) + [∇ ψ(x)]T ξ +

ξT [∇2 ψ(x)] ξ + O(ξ3)

where ∇2 denotes the Hessian, whose trace Tr(∇2) = (cid:52) is known as the Laplace operator. The
properties of white noise result in the approximation

EΛ[ψ(x + ξ)] = ψ(x) +

(cid:52)ψ(x) + O(γ2)

(11)

(12)

and thereby lead directly to an approximation of Fγ (see Eq. (3)) via F = F0 plus a correction, i.e.

Fγ(P, Q; ψ) = F(P, Q; ψ) +

{EP [(cid:52)ψ] − EQ [(cid:52)(f c ◦ ψ)]} + O(γ2) .

(13)

γ
2

We can interpret Eq. (13) as follows: the Laplacian measures how much the scalar ﬁelds ψ and
f c ◦ ψ differ at each point from their local average. It is thereby an inﬁnitesimal proxy for the (exact)
convolution.

The Laplace operator is a sum of d terms, where d is the dimensionality of the ambient data space. As
such it does not suffer from the quadratic blow-up involved in computing the Hessian. If we realize
the discriminator ψ via a deep network, however, then we need to be able to compute the Laplacian
of composed functions. For concreteness, let us assume that ψ = h ◦ G, G = (g1, . . . , gk) and look

1
2

γ
2

4

at a single input x, i.e. gi : R → R, then
g(cid:48)
i · (∂ih ◦ G),

(h ◦ G)(cid:48) =

(cid:88)

(h ◦ G)(cid:48)(cid:48) =

i

(cid:88)

i

g(cid:48)(cid:48)
i · (∂ih ◦ G) +

i · g(cid:48)
g(cid:48)

j · (∂i∂jh ◦ G)

(14)

(cid:88)

i,j

So at the intermediate layer, we would need to effectively operate with a full Hessian, which is
computationally demanding, as has already been observed in [5].

3.4 Efﬁcient Gradient-Based Regularization

We would like to derive a (more) tractable strategy for regularizing ψ, which (i) avoids the detrimental
variance that comes from sampling ξ, (ii) does not rely on explicitly convolving the distributions
P and Q, and (iii) avoids the computation of Laplacians as in Eq. (13). Clearly, this requires to
make further simpliﬁcations. We suggest to exploit properties of the maximizer ψ∗ of F that can be
characterized by [24]

(f c(cid:48) ◦ ψ∗) dQ = dP =⇒ EP[h] = EQ[(f c(cid:48) ◦ ψ∗) · h]

(15)
The relevance of this becomes clear, if we apply the chain rule to (cid:52)(f c ◦ ψ), assuming that f c is
twice differentiable

(∀h, integrable).

(cid:52)(f c ◦ ψ) = (f c(cid:48)(cid:48) ◦ ψ) · ||∇ψ||2 + (cid:0)f c(cid:48) ◦ ψ(cid:1) (cid:52)ψ,

as now we get a convenient cancellation of the Laplacians at ψ = ψ∗ + O(γ)
(f c(cid:48)(cid:48) ◦ ψ∗) · (cid:107)∇ψ∗(cid:107)2(cid:105)

Fγ(P, Q; ψ∗) = F(P, Q; ψ∗) −

EQ

(cid:104)

+ O(γ2) .

γ
2

We can (heuristically) turn this into a regularizer by taking the leading terms,

Fγ(P, Q; ψ) ≈ F(P, Q; ψ) −

Ωf (Q; ψ), Ωf (Q; ψ) := EQ

(cid:104)

(f c(cid:48)(cid:48) ◦ ψ) · (cid:107)∇ψ(cid:107)2(cid:105)

.

(18)

γ
2

Note that we do not assume that the Laplacian terms cancel far away from the optimum, i.e. we do not
assume Eq. (15) to hold for ψ far away from ψ∗. Instead, the underlying assumption we make is that
optimizing the gradient-norm regularized objective Fγ(P, Q; ψ) makes ψ converge to ψ∗ + O(γ),
for which we know that the Laplacian terms cancel [5, 2].
The convexity of f c implies that the weighting function of the squared gradient norm is non-negative,
i.e. f c(cid:48)(cid:48) ≥ 0, which in turn implies that the regularizer − γ
2 Ωf (Q; ψ) is upper bounded (by zero).
Maximization of Fγ(P, Q; ψ) with respect to ψ is therefore well-deﬁned. Further considerations
regarding the well-deﬁnedness of the regularizer can be found in sec. 7.2 in the Appendix.

(16)

(17)

We have shown that training with noise is equivalent to regularizing the discriminator. Inspired by
the above analysis, we propose the following class of f -GAN regularizers:

4 Regularizing GANs

Regularized f -GAN

Fγ(P, Q; ψ) = EP [ψ] − EQ [f c ◦ ψ] −
(f c(cid:48)(cid:48) ◦ ψ) (cid:107)∇ψ(cid:107)2(cid:105)
(cid:104)

Ωf (Q; ψ) := EQ

γ
2

Ωf (Q; ψ)

(19)

The regularizer corresponding to the commonly used parametrization of the Jensen-Shannon GAN
can be derived analogously as shown in the Appendix. We obtain,

Regularized Jensen-Shannon GAN

Fγ(P, Q; ϕ) = EP [ln(ϕ)] + EQ [ln(1 − ϕ)] −

ΩJS(P, Q; ϕ)

γ
2
(cid:2)(1 − ϕ(x))2||∇φ(x)||2(cid:3) + EQ

ΩJS(P, Q; ϕ) := EP

(cid:2)ϕ(x)2||∇φ(x)||2(cid:3)

(20)

where φ = σ−1(ϕ) denotes the logit of the discriminator ϕ. We prefer to compute the gradient of φ
as it is easier to implement and more robust than computing gradients after applying the sigmoid.

5

Algorithm 1 Regularized JS-GAN. Default values: γ0 = 2.0, α = 0.01 (with annealing), γ = 0.1 (without
annealing), nϕ = 1
Require: Initial noise variance γ0, annealing decay rate α, number of discriminator update steps nϕ

per generator iteration, minibatch size m, number of training iterations T
Require: Initial discriminator parameters ω0, initial generator parameters θ0

for t = 1, ..., T do

γ ← γ0 · αt/T # annealing
for 1, ..., nϕ do

Sample minibatch of real data {x(1), ..., x(m)} ∼ P.
Sample minibatch of latent variables from prior {z(1), ..., z(m)} ∼ p(z).

F (ω, θ) =

m
(cid:88)

(cid:104)

(cid:16)

(cid:17)
ϕω(x(i))

ln

(cid:16)

+ ln

1 − ϕω(Gθ(z(i)))

(cid:17)(cid:105)

1
m

1
m

i=1

m
(cid:88)

(cid:20)(cid:16)

i=1
(cid:16)

(cid:16)

1
m

m
(cid:88)

i=1

Ω(ω, θ) =

1 − ϕω(x(i))

(cid:17)2(cid:12)
(cid:12)

(cid:12)∇φω(x(i))(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

2

+ ϕω

(cid:0)Gθ(z(i))(cid:1)2(cid:12)
(cid:12)

(cid:12)∇˜xφω(˜x)(cid:12)
(cid:12)

(cid:12)˜x=Gθ(z(i))

2(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ω ← ω + ∇ω

F (ω, θ) −

Ω(ω, θ)

# gradient ascent

end for
Sample minibatch of latent variables from prior {z(1), ..., z(m)} ∼ p(z).

γ
2

F (ω, θ) =

ln

1 − ϕω(Gθ(z(i)))

or Falt(ω, θ) = −

(cid:17)

(cid:17)

1
m

m
(cid:88)

i=1

(cid:16)
ϕω(Gθ(z(i)))

(cid:17)

ln

θ ← θ − ∇θF(ω, θ) # gradient descent

end for
The gradient-based updates can be performed with any gradient-based learning rule. We used
Adam in our experiments.

4.1 Training Algorithm

Regularizing the discriminator provides an efﬁcient way to convolve the distributions and is thereby
sufﬁcient to address the dimensional misspeciﬁcation challenges outlined in the introduction. This
leaves open the possibility to use the regularizer also in the objective of the generator. On the one
hand, optimizing the generator through the regularized objective may provide useful gradient signal
and therefore accelerate training. On the other hand, it destabilizes training close to convergence
(if not dealt with properly), since the generator is incentiviced to put probability mass where the
discriminator has large gradients. In the case of JS-GANs, we recommend to pair up the regularized
objective of the discriminator with the “alternative” or “non-saturating” objective for the generator,
proposed in [8], which is known to provide strong gradients out of the box (see Algorithm 1).

4.2 Annealing

The regularizer variance γ lends itself nicely to annealing. Our experimental results indicate that a
reasonable annealing scheme consists in regularizing with a large initial γ early in training and then
(exponentially) decaying γ to a small non-zero value. We leave to future work the question of how to
determine an optimal annealing schedule.

5 Experiments

5.1

2D submanifold mixture of Gaussians in 3D space

To demonstrate the stabilizing effect of the regularizer, we train a simple GAN architecture [20] on a
2D submanifold mixture of seven Gaussians arranged in a circle and embedded in 3D space (further
details and an illustration of the mixture distribution are provided in the Appendix). We emphasize
that this mixture is degenerate with respect to the base measure deﬁned in ambient space as it does
not have fully dimensional support, thus precisely representing one of the failure scenarios commonly

6

.

G
E
R
N
U

1
0
0

.

0

.

1

Figure 1: 2D submanifold mixture. The ﬁrst row shows one of several unstable unregularized GANs
trained to learn the dimensionally misspeciﬁed mixture distribution. The remaining rows show
regularized GANs (with regularized objective for the discriminator and unregularized objective for
the generator) for different levels of regularization γ. Even for small but non-zero noise variance, the
regularized GAN can essentially be trained indeﬁnitely without collapse. The color of the samples is
proportional to the density estimated from a Gaussian KDE ﬁt. The target distribution is shown in
Fig. 5. GANs were trained with one discriminator update per generator update step (indicated).

described in the literature [3]. The results are shown in Fig. 1 for both standard unregularized GAN
training as well as our regularized variant.

While the unregularized GAN collapses in literally every run after around 50k iterations, due to the
fact that the discriminator concentrates on ever smaller differences between generated and true data
(the stakes are getting higher as training progresses), the regularized variant can be trained essentially
indeﬁnitely (well beyond 200k iterations) without collapse for various degrees of noise variance, with
and without annealing. The stabilizing effect of the regularizer is even more pronounced when the
GANs are trained with ﬁve discriminator updates per generator update step, as shown in Fig. 6.

5.2 Stability across various architectures

To demonstrate the stability of the regularized training procedure and to showcase the excellent
quality of the samples generated from it, we trained various network architectures on the CelebA [17],
CIFAR-10 [15] and LSUN bedrooms [32] datasets. In addition to the deep convolutional GAN
(DCGAN) of [26], we trained several common architectures that are known to be hard to train
[4, 26, 19], therefore allowing us to establish a comparison to the concurrently proposed gradient-
penalty regularizer for Wasserstein GANs [10]. Among these architectures are a DCGAN without
any normalization in either the discriminator or the generator, a DCGAN with tanh activations and a
deep residual network (ResNet) GAN [11]. We used the open-source implementation of [10] for our
experiments on CelebA and LSUN, with one notable exception: we use batch normalization also for
the discriminator (as our regularizer does not depend on the optimal transport plan or more precisely
the gradient penalty being imposed along it).
All networks were trained using the Adam optimizer [13] with learning rate 2 × 10−4 and hyper-
parameters recommended by [26]. We trained all datasets using batches of size 64, for a total of
200K generator iterations in the case of LSUN and 100k iterations on CelebA. The results of these
experiments are shown in Figs. 3 & 2. Further implementation details can be found in the Appendix.

5.3 Training time

We empirically found regularization to increase the overall training time by a marginal factor
of roughly 1.4 (due to the additional backpropagation through the computational graph of the
discriminator gradients). More importantly, however, (regularized) f -GANs are known to converge
(or at least generate good looking samples) faster than their WGAN relatives [10].

7

RESNET

DCGAN

NO NORMALIZATION

TANH

Figure 2: Stability accross various architectures: ResNet, DCGAN, DCGAN without normalization
and DCGAN with tanh activations (details in the Appendix). All samples were generated from
regularized GANs with exponentially annealed γ0 = 2.0 (and alternative generator loss) as described
in Algorithm 1. Samples were produced after 200k generator iterations on the LSUN dataset (see also
Fig. 8 for a full-resolution image of the ResNet GAN). Samples for the unregularized architectures
can be found in the Appendix.

UNREG.

0.5

1.0

2.0

Figure 3: Annealed Regularization. CelebA samples generated by (un)regularized ResNet GANs.
The initial level of regularization γ0 is shown below each batch of images. γ0 was exponentially
annealed as described in Algorithm 1. The regularized GANs can be trained essentially indeﬁnitely
without collapse, the superior quality is again evident. Samples were produced after 100k generator
iterations.

5.4 Regularization vs. explicitly adding noise

We compare our regularizer against the common practitioner’s approach to explicitly adding noise to
images during training. In order to compare both approaches (analytic regularizer vs. explicit noise),
we ﬁx a common batch size (64 in our case) and subsequently train with different noise-to-signal
ratios (NSR): we take (batch-size/NSR) samples (both from the dataset and generated ones) to each
of which a number of NSR noise vectors is added and feed them to the discriminator (so that overall
both models are trained on the same batch size). We experimented with NSR 1, 2, 4, 8 and show the
best performing ratio (further ratios in the Appendix). Explicitly adding noise in high-dimensional
ambient spaces introduces additional sampling variance which is not present in the regularized variant.
The results, shown in Fig. 4, conﬁrm that the regularizer stabilizes across a broad range of noise
levels and manages to produce images of considerably higher quality than the unregularized variants.

5.5 Cross-testing protocol

We propose the following pairwise cross-testing protocol to assess the relative quality of two GAN
models: unregularized GAN (Model 1) vs. regularized GAN (Model 2). We ﬁrst report the confusion
matrix (classiﬁcation of 10k samples from the test set against 10k generated samples) for each model
separately. We then classify 10k samples generated by Model 1 with the discriminator of Model 2
and vice versa. For both models, we report the fraction of false positives (FP) (Type I error) and false
negatives (FN) (Type II error). The discriminator with the lower FP (and/or lower FN) rate deﬁnes
the better model, in the sense that it is able to more accurately classify out-of-data samples, which
indicates better generalization properties. We obtained the following results on CIFAR-10:

8

UNREGULARIZED

0.01

EXPLICIT NOISE
0.1

1.0

0.001

0.01

0.1

1.0

REGULARIZED

Figure 4: CIFAR-10 samples generated by (un)regularized DCGANs (with alternative generator loss),
as well as by training a DCGAN with explicitly added noise (noise-to-signal ratio 4). The level of
regularization or noise γ is shown above each batch of images. The regularizer stabilizes across a
broad range of noise levels and manages to produce images of higher quality than the unregularized
variants. Samples were produced after 50 training epochs.

Regularized GAN (γ = 0.1)

Unregularized GAN

True condition
Positive Negative
0.0002
0.9688
0.9998
0.0312

Predicted

Positive
Negative

Cross-testing: FP: 0.0

True condition
Positive Negative
0.0013
0.9987

1.0
0.0

Predicted

Positive
Negative

Cross-testing: FP: 1.0

For both models, the discriminator is able to recognize his own generator’s samples (low FP in the
confusion matrix). The regularized GAN also manages to perfectly classify the unregularized GAN’s
samples as fake (cross-testing FP 0.0) whereas the unregularized GAN classiﬁes the samples of the
regularized GAN as real (cross-testing FP 1.0). In other words, the regularized model is able to fool
the unregularized one, whereas the regularized variant cannot be fooled.

6 Conclusion

We introduced a regularization scheme to train deep generative models based on generative adversarial
networks (GANs). While dimensional misspeciﬁcations or non-overlapping support between the
data and model distributions can cause severe failure modes for GANs, we showed that this can be
addressed by adding a penalty on the weighted gradient-norm of the discriminator. Our main result is
a simple yet effective modiﬁcation of the standard training algorithm for GANs, turning them into
reliable building blocks for deep learning that can essentially be trained indeﬁnitely without collapse.
Our experiments demonstrate that our regularizer improves stability, prevents GANs from overﬁtting
and therefore leads to better generalization properties (cf cross-testing protocol). Further research on
the optimization of GANs as well as their convergence and generalization can readily be built upon
our theoretical results.

9

Acknowledgements

We would like to thank Devon Hjelm for pointing out that the regularizer works well with ResNets.
KR is thankful to Yannic Kilcher, Lars Mescheder, Paulina Grnarova and the dalab team for insightful
discussions. Big thanks also to Ishaan Gulrajani and Taehoon Kim for their open-source GAN
implementations. This work was supported by Microsoft Research through its PhD Scholarship
Programme.

References

matical Soc., 2007.

[1] Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry. American Mathe-

[2] Guozhong An. The effects of adding noise during backpropagation training on a generalization

performance. Neural Comput., pages 643–674, 1996.

[3] Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adver-

sarial networks. In ICLR, 2017.

[4] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial

networks. In PMLR, Proceedings of Machine Learning Research, 2017.

[5] Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computa-

tion, 7:108–116, 1995.

[6] Diane Bouchacourt, Pawan K Mudigonda, and Sebastian Nowozin. Disco nets: Dissimilarity
coefﬁcients networks. In Advances in Neural Information Processing Systems, pages 352–360,
2016.

[7] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized

generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.

[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. In Advances in
Neural Information Processing Systems, pages 2672–2680, 2014.

[9] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research, 13:723–773, 2012.

[10] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems,
2017.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page
770–778, 2016.

[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In PMLR, Proceedings of Machine Learning Research,
pages 448–456, 2015.

[13] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. The

International Conference on Learning Representations (ICLR), 2014.

[14] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. The International

Conference on Learning Representations (ICLR), 2013.

[15] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

[16] Yujia Li, Kevin Swersky, and Richard S Zemel. Generative moment matching networks. In

CS UToronto, 2009.

ICML, pages 1718–1727, 2015.

[17] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in
the wild. In Proceedings of the IEEE International Conference on Computer Vision, pages
3730–3738, 2015.

[18] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the

wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.

10

[19] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances

in Neural Information Processing Systems, 2017.

[20] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial

networks. In International Conference on Learning Representations (ICLR), 2016.

[21] Tom Minka. Divergence measures and message passing. Technical report, Microsoft Research,

2005.

[22] Alfred Müller. Integral probability metrics and their generating classes of functions. Advances

in Applied Probability, 29:429–443, 1997.

[23] Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis.

In Advances in Neural Information Processing Systems, pages 1786–1794, 2010.

[24] XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence func-
tionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information
Theory, 56(11):5847–5861, 2010.

[25] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neu-
ral samplers using variational divergence minimization. In Advances in Neural Information
Processing Systems, pages 271–279, 2016.

[26] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[27] Mark D Reid and Robert C Williamson. Information, divergence and risk for binary experiments.

Journal of Machine Learning Research, 12:731–817, 2011.

[28] Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31st International
Conference on Machine Learning, 2014.

[29] David W Scott. Multivariate density estimation: theory, practice, and visualization. John Wiley

& Sons, 2015.

[30] Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszár. Amortised

map inference for image super-resolution. arXiv preprint arXiv:1610.04490, 2016.

[31] Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, and Gert RG
Lanckriet. On integral probability metrics, phi-divergences and binary classiﬁcation. arXiv
preprint arXiv:0901.2698, 2009.

[32] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction
of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365, 2015.

11

7 APPENDIX

7.1 Derivation of the Jensen-Shannon Regularizer.

The Jensen-Shannon GAN is typically encountered in one of two equivalent parametrizations: the
commonly used “original” GAN parametrization [8],

and the Fenchel-dual f -GAN parametrization [25], where f c = − ln(1 − exp),
EP [ψ] − EQ [f c ◦ ψ] = EP [ψ] + EQ [ln(1 − exp(ψ))]

EP [ln(ϕ)] + EQ [ln(1 − ϕ)]

(21)

(22)

Depending on whether we train the JS-GAN through its Fenchel-dual parametrization or with the
original GAN objective we have to either use the regularizer in the general f -GAN form in Eq. (19),
or in the speciﬁc Jensen-Shannon parametrization given in Eq. (20).

We now show how to derive the speciﬁc Jensen-Shannon regularizer, which is basically a repetition
of the derivation of the general f -GAN regularizer presented in the main text. Using the same
terminology and following the same line of thought as in section 3.3, i.e. assuming the noise variance
is small, we can Taylor approximate the statistics ϕ around ξ = 0,

ϕ(x + ξ) = ϕ(x) + [∇ ϕ(x)] ξ +

ξT [∇2 ϕ(x)] ξ + O(ξ3)

(23)

1
2

Expanding the noise convolved version of the objective in Eq. (21) and making use of the zero-mean
and uncorrelatedness properties of the noise distribution, as well as applying the chain rule, we obtain

Fγ(P, Q; ϕ) = EP [ln(ϕ)] + EQ [ln(1 − ϕ)]
(cid:52)ϕ − ||∇ ln(ϕ)||2(cid:105)
(cid:104) 1
ϕ

γ
2

EP

+

(cid:26)

(cid:104)

+ EQ

−

1
1 − ϕ

(cid:52)ϕ − ||∇ ln(1 − ϕ)||2(cid:105)(cid:27)

+ O(γ2) .

(24)
Following the same arguments as in section 3.4, one can again show that the Laplacian terms cancel
at ϕ = ϕ∗ + O(γ) and we arrive at

Fγ(P, Q; ϕ) = EP [ln(ϕ)] + EQ [ln(1 − ϕ)]
(cid:2)||∇ ln(ϕ)||2(cid:3) + EQ

γ
2
allowing us to read off the corresponding JS regularizer,

(cid:8)EP

−

ΩJS(P, Q; ϕ) = EP

(cid:2)||∇ ln(1 − ϕ(x))||2(cid:3)

(cid:2)||∇ ln(1 − ϕ)||2(cid:3)(cid:9) + O(γ2) ,

1

(cid:2)||∇ ln ϕ(x)||2(cid:3) + EQ
(ϕ(x))2 ||∇ϕ(x)||2(cid:105)
(cid:104)
(ϕ(x))2 ||σ(cid:48)(φ(x))∇φ(x)||2(cid:105)

+ EQ

1

(cid:104)

(cid:104)

= EP

= EP

1

(1 − ϕ(x))2 ||∇ϕ(x))||2(cid:105)

+ EQ

(cid:104)

1

(1 − ϕ(x))2 ||σ(cid:48)(φ(x))∇φ(x)||2(cid:105)

In order to obtain the regularizer in the logit-parameterization, with ϕ(x) = σ(φ(x)), we make use
of the following property of the sigmoid

σ(cid:48)(t) = σ(t)(1 − σ(t))

which allows us to write

ΩJS(P, Q; φ) = EP

(cid:2)(1 − ϕ(x))2||∇φ(x)||2(cid:3) + EQ

(cid:2)ϕ(x)2||∇φ(x)||2(cid:3)

Let us ﬁnally also show that these two parametrizations are indeed equivalent at the optimum:

f c(cid:48)(cid:48)(t) =

exp(t)
(1 − exp(t))2

=⇒ f c(cid:48)(cid:48)(ln(ϕ∗)) =

ϕ∗
(1 − ϕ∗)2 =

ϕ∗
1 − ϕ∗ +

ϕ∗2
(1 − ϕ∗)2

Thus,

f c(cid:48)(cid:48)(ln(ϕ∗))(cid:107)∇ ln(ϕ∗)(cid:107)2dQ =

(25)

(26)

(27)

(28)

(29)

(30)

(cid:16) ϕ∗
1 − ϕ∗ +
= (cid:107)∇ ln(ϕ∗)(cid:107)2dP + (cid:107)

ϕ∗2
(1 − ϕ∗)2

(cid:17)

(cid:107)∇ ln(ϕ∗)(cid:107)2dQ

ϕ∗
1 − ϕ∗ ∇ ln(ϕ∗)(cid:107)2dQ

= (cid:107)∇ ln(ϕ∗)(cid:107)2dP + (cid:107)∇ ln(1 − ϕ∗)(cid:107)2dQ

12

7.2 Further Considerations Regarding the Regularizer

Following-up on our discussion of the regularizer in section 3.4: Due to a support or dimensionality
mismatch we may have sup F (P, Q; ψ) = +∞ and ψ∗ may not exist. However, with ψ∗
(cid:15) being the
maximizer of F(cid:15) (which is guaranteed to exist for any (cid:15) > 0), we get

Fγ(P ∗ N (0, (cid:15)I), Q ∗ N (0, (cid:15)I)) = F(cid:15)(P, Q; ψ∗

(cid:15) ) −

Ωf (Q ∗ N (0, (cid:15)I); ψ∗

(cid:15) ) + O(γ2) .

(31)

γ
2

(cid:15) (cid:107). The sequence of regularizers Ωf (Q ∗ N (0, (cid:15)I); ·),
As (cid:15) → 0, F(cid:15) may diverge and so may (cid:107)∇ψ∗
however, converges (at least pointwise) to a well deﬁned limit, which is Ωf (Q; ·). This shows that
the regularizer is well-deﬁned even under dimensional misspeciﬁcations.

We can also justify the approximation in Eq. (18) more rigorously. Starting from the Taylor approxi-
mation of f c(cid:48) at ψ∗, we get pointwise

f c(cid:48) ◦ ψ = f c(cid:48) ◦ ψ∗ + f c(cid:48)(cid:48) ◦ (ψ − ψ∗) + O(|ψ − ψ∗|2)

(32)
(cid:2)(f c(cid:48)(cid:48) ◦ (ψ − ψ∗)) · (cid:52)ψ(cid:3). Using
So in ﬁrst order of (cid:107)ψ −ψ∗(cid:107)∞, the approximation error is ∆ := EQ
Green’s identity, we can derive the following bound, ∆ ≤ O(Lδ), where we assume |f c(cid:48)(cid:48)(cid:48)| ≤ L and
(cid:107)∇ψ∗ − ∇ψ(cid:107)∞ < δ. However, as this result only gives a formal guarantee for (ψ, ∇ψ) sufﬁciently
close to (ψ∗, ∇ψ∗), we refrain from presenting further technical details.

7.3 2D Submanifold Mixture of Gaussians in 3D Space

Experimental Setup. The experimental setup is inspired by the two-dimensional mixture of Gaus-
sians in [20]. The dataset is constructed as follows. We sample from a mixture of seven Gaussians
with standard deviation 0.01 and means equally spaced around the unit circle. This 2D mixture
√
is then embedded in 3D space (x, y) → (x, y, 0), rotated by π/4 around the axis (1, −1, 0)/
2
3. As illustrated in Fig. 5, this yields a mixture distribution that lives
and translated by (1, 1, 1)/
in a tilted 2D submanifold embedded in 3D space. It is important to emphasize that the mixture
distribution is by design degenerate with respect to the base measure in 3D as it does not have full
dimensional support. This precisely represents the dimensional misspeciﬁcation scenario for GANs
that we aim to address with our regularizer.

√

Figure 5: 2D submanifold mixture of Gaussians in 3D, obtained by embedding a two-dimensional
mixture with means equally spaced around the unit circle in 3D ambient space. For visualization
purposes, the standard deviation of the shown mixture components is 10x larger than the one used
in the experiments. Samples are colored proportional to their density which we estimated from a
Gaussian KDE with bandwidth selected using Scott’s rule [29].

Architecture. The architecture corresponds to the one used in [20] with one notable exception. We
use 2 dimensional latent vectors z, sampled from a multivariate normal prior, (whereas [20] uses 256
dimensional z), as we found lower dimensional latent variables greatly improve the performance of
the unregularized GAN against which we compare. We did all experiments also for latent vectors of
dimension 15: the obtained results are in accordance with those presented in the main text and below.

Both networks are optimized using Adam with a learning rate of 1e−3 and standard hyper-parameters.
We trained on batches of size 512. Ten batches were generated to produce one image of the mixture
at given time steps. The generator and discriminator network parameters were updated alternatively.

13

.

G
E
R
N
U

1
0
0

.

1
0

.

0
1

.

Figure 6: 2D submanifold mixture. The ﬁrst row shows one of several unstable unregularized
GANs trying to learn the dimensionally misspeciﬁed mixture distribution. The remaining rows show
regularized GANs (with regularized objective for the discriminator and unregularized objective for
the generator) for different levels of regularization γ. Even for small but non-zero noise variance, the
regularized GAN can essentially be trained indeﬁnitely without collapse. The color of the samples is
proportional to the density estimated from a Gaussian KDE ﬁt. The target distribution is shown in
Fig. 5. GANs were trained with ﬁve discriminator updates per generator update step (indicated).

7.4

Image Datasets and Network Architectures

We trained on CelebA [18], CIFAR-10 [15] and LSUN [32]. All datasets were trained on minibatches
of size 64. The respective GAN architectures are listed below.

DCGAN. For
the CIFAR-10 experiments, we used the DCGAN (Deep Convolutional
GAN) architecture of [26] implemented in Tensorﬂow by https://github.com/carpedm20/
DCGAN-tensorflow.

For the LSUN experiments, we used the DCGAN architecture of [26] implemented in Tensorﬂow by
https://github.com/igul222/improved_wgan_training.

In both cases, the discriminator uses batch normalization [12] in all but the ﬁrst and last layer (except
where explicitly stated otherwise). The generator uses batch normalization in all layers except the last
one. The latent code is sampled from a 100-dimensional uniform distribution over [0, 1] (carpedm20)
resp. 128-dimensional normal-(0,1) distribution (Gulrajani).

Both networks were trained using the Adam optimizer [13] (with hyper-parameters recommended
by the DCGAN authors) for various different learning rates in the range [0.001, 0.0001]. The
recommended learning rate 2 × 10−4 was found to perform best.

ResNet GAN. For the CelebA and LSUN experiments, we used a deep residual network
ResNet [11] implemented in Tensorﬂow by https://github.com/igul222/improved_wgan_
training (implementation details can be found in [10]).

The ResNets use pre-activation residual blocks with two 3 x 3 convolutional layers each and ReLU
nonlinearities. The generator has one linear layer, followed by four residual blocks, one deconvolu-
tional layer and tanh output activations. The discriminator has one convolutional layer, followed by
four residual blocks and a linear output layer (the discriminator logits are then fed into the sigmoid
GAN loss).

14

We use batch normalization [12] in the generator and discriminator (except explicitly stated otherwise).
Both networks were optimized using Adam with learning rate 2 × 10−4 and standard hyperparameters.
For further architectural details, please refer to [10] and the excellent open-source implementation
referenced above.

7.5 Further Experimental Results.

RESNET

DCGAN

NO NORMALIZATION

TANH

Figure 7: Unregularized GANs (with alternative generator loss) accross various architectures: ResNet,
DCGAN, DCGAN without normalization and DCGAN with tanh activations. Samples were produced
after 200k generator iterations on the LSUN dataset. Samples for the regularized architectures can
be found in the main text. Some of these architectures are known to be hard to train.

Figure 8: Regularized ResNet GAN. Full-resolution 64 × 64 images generated by the regularized
ResNet GAN after 200k generator iterations on the LSUN dataset. The initial level of regulariza-
tion γ0 = 2.0 was exponentially annealed to γT = 0.01.

15

1

2

4

8

1
0
0

.

1
0

.

0
.
1

1
0
.
0

1
.
0

0
.
1

Figure 9: DCGAN trained with explicitly added noise. Top 3 rows: discriminator and generator
trained with noise. Bottom 3 rows: discriminator is trained with noise while generator is trained
without noise. White normal noise is added to images from the dataset as well as to samples from the
generator during training. The generator was trained through the alternative loss. The level of noise
γ is shown in the left most column, the different noise-to-signal ratios (NSR) above each column of
images. Samples were produced after 50 training epochs.

16

7
1
0
2
 
v
o
N
 
7
 
 
]

G
L
.
s
c
[
 
 
2
v
7
6
3
9
0
.
5
0
7
1
:
v
i
X
r
a

Stabilizing Training of Generative Adversarial
Networks through Regularization

Kevin Roth
Department of Computer Science
ETH Zürich
kevin.roth@inf.ethz.ch

Aurelien Lucchi
Department of Computer Science
ETH Zürich
aurelien.lucchi@inf.ethz.ch

Sebastian Nowozin
Microsoft Research
Cambridge, UK
sebastian.Nowozin@microsoft.com

Thomas Hofmann
Department of Computer Science
ETH Zürich
thomas.hofmann@inf.ethz.ch

Abstract

Deep generative models based on Generative Adversarial Networks (GANs) have
demonstrated impressive sample quality but in order to work they require a careful
choice of architecture, parameter initialization, and selection of hyper-parameters.
This fragility is in part due to a dimensional mismatch or non-overlapping support
between the model distribution and the data distribution, causing their density ratio
and the associated f -divergence to be undeﬁned. We overcome this fundamental
limitation and propose a new regularization approach with low computational cost
that yields a stable GAN training procedure. We demonstrate the effectiveness of
this regularizer across several architectures trained on common benchmark image
generation tasks. Our regularization turns GAN models into reliable building
blocks for deep learning.1

1

Introduction

A recent trend in the world of generative models is the use of deep neural networks as data generating
mechanisms. Two notable approaches in this area are variational auto-encoders (VAEs) [14, 28] as
well as generative adversarial networks (GAN) [8]. GANs are especially appealing as they move
away from the common likelihood maximization viewpoint and instead use an adversarial game
approach for training generative models. Let us denote by P(x) and Qθ(x) the data and model
distribution, respectively. The basic idea behind GANs is to pair up a θ-parametrized generator
network that produces Qθ with a discriminator which aims to distinguish between P and Qθ, whereas
the generator aims for making Qθ indistinguishable from P. Effectively the discriminator represents
a class of objective functions F that measures dissimilarity of pairs of probability distributions. The
ﬁnal objective is then formed via a supremum over F, leading to the saddle point problem

(cid:20)
(cid:96)(Qθ; F) := sup
F ∈F

min
θ

(cid:21)
F (P, Qθ)

.

The standard way of representing a speciﬁc F is through a family of statistics or discriminants φ ∈ Φ,
typically realized by a neural network [8, 26]. In GANs, we use these discriminators in a logistic
classiﬁcation loss as follows

F (P, Q; φ) = EP [g(φ(x))] + EQ [g(−φ(x))] ,

1Code available at https://github.com/rothk/Stabilizing_GANs

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

(1)

(2)

where g(z) = ln(σ(z)) is the log-logistic function (for reference, σ(φ(x)) = D(x) in [8]).
As shown in [8], for the Bayes-optimal discriminator φ∗ ∈ Φ, the above generator objective reduces
to the Jensen-Shannon (JS) divergence between P and Q. The work of [25] later generalized this to
a more general class of f -divergences, which gives more ﬂexibility in cases where the generative
model may not be expressive enough or where data may be scarce.

We consider three different challenges for learning the model distribution:

(A) empirical estimation: the model family may contain the true distribution or a good approximation
thereof, but one has to identify it based on a ﬁnite training sample drawn from P. This is commonly
addressed by the use of regularization techniques to avoid overﬁtting, e.g. in the context of estimating
f -divergences with M -estimators [24]. In our work, we suggest a novel (Tikhonov) regularizer,
derived and motivated from a training-with-noise scenario, where P and Q are convolved with white
Gaussian noise [30, 3], namely

Fγ(P, Q; φ) := F (P ∗ Λ, Q ∗ Λ; φ), Λ = N (0, γI) .

(3)

(B) density misspeciﬁcation: the model distribution and true distribution both have a density function
with respect to the same base measure but there exists no parameter for which these densities are
sufﬁciently similar. Here, the principle of parameter estimation via divergence minimization is
provably sound in that it achieves a well-deﬁned limit [1, 21]. It therefore provides a solid foundation
for statistical inference that is robust with regard to model misspeciﬁcations.

(C) dimensional misspeciﬁcation: the model distribution and the true distribution do not have a
density function with respect to the same base measure or – even worse – supp(P) ∩ supp(Q) may
be negligible. This may occur, whenever the model and/or data are conﬁned to low-dimensional
manifolds [3, 23]. As pointed out in [3], a geometric mismatch can be detrimental for f -GAN
models as the resulting f -divergence is not ﬁnite (the sup in Eq. (1) is +∞). As a remedy, it has
been suggested to use an alternative family of distance functions known as integral probability
metrics [22, 31]. These include the Wasserstein distance used in Wasserstein GANs (WGAN) [3] as
well as RKHS-induced maximum mean discrepancies [9, 16, 6], which all remain well-deﬁned. We
will provide evidence (analytically and experimentally) that the noise-induced regularization method
proposed in this paper effectively makes f -GAN models robust against dimensional misspeciﬁcations.
While this introduces some dependency on the (Euclidean) metric of the ambient data space, it does
so on a well-controlled length scale (the amplitude of noise or strength of the regularization γ) and
by retaining the beneﬁts of f -divergences. This is a rather gentle modiﬁcation compared to the more
radical departure taken in Wasserstein GANs, which rely solely on the ambient space metric (through
the notion of optimal mass transport).

In what follows, we will take Eq. (3) as the starting point and derive an approximation via a regularizer
that is simple to implement as an integral operator penalizing the squared gradient norm. As opposed
to a naïve norm penalization, each f -divergence has its own characteristic weighting function over
the input space, which depends on the discriminator output. We demonstrate the effectiveness
of our approach on a simple Gaussian mixture as well as on several benchmark image datasets
commonly used for generative models. In both cases, our proposed regularization yields stable GAN
training and produces samples of higher visual quality. We also perform pairwise tests of regularized
vs. unregularized GANs using a novel cross-testing protocol.

In summary, we make the following contributions:

• We systematically derive a novel, efﬁciently computable regularization method for f -GAN.
• We show how this addresses the dimensional misspeciﬁcation challenge.
• We empirically demonstrate stable GAN training across a broad set of models.

2 Background

The fundamental way to learn a generative model in machine learning is to (i) deﬁne a parametric
family of probability densities {Qθ}, θ ∈ Θ ⊆ Rd, and (ii) ﬁnd parameters θ∗ ∈ Θ such that Qθ is
closest (in some sense) to the true distribution P. There are various ways to measure how close model
and real distribution are, or equivalently, various ways to deﬁne a distance or divergence function
between P and Q. In the following we review different notions of divergences used in the literature.

2

f -divergence. GANs [8] are known to minimize the Jensen-Shannon divergence between P and
Q. This was generalized in [25] to f -divergences induced by a convex functions f . An interesting
property of f -divergences is that they permit a variational characterization [24, 27] via

Df (P||Q) := EQ

f ◦

(cid:20)

(cid:21)

dP
dQ

(cid:90)

=

sup
u

X

(cid:18)

u ·

(cid:19)

dP
dQ − f c(u)

dQ,

(4)

where dP/dQ is the Radon-Nikodym derivative and f c(t) ≡ supu∈domf
dual of f . By deﬁning an arbitrary class of statistics Ψ (cid:51) ψ : X → R we arrive at the bound

{ut − f (u)} is the Fenchel

Df (P||Q) ≥ sup
ψ

(cid:90) (cid:18)

ψ ·

(cid:19)

dP
dQ − f c ◦ ψ

dQ = sup
ψ

{EP[ψ] − EQ[f c ◦ ψ]} .

(5)

Eq. (5) thus gives us a variational lower bound on the f -divergence as an expectation over P and
Q, which is easier to evaluate (e.g. via sampling from P and Q, respectively) than the density
based formulation. We can see that by identifying ψ = g ◦ φ and with the choice of f such that
f c = − ln(1 − exp), we get f c ◦ ψ = − ln(1 − σ(φ)) = −g(−φ) thus recovering Eq. (2).

Integral Probability Metrics (IPM). An alternative family of divergences are integral probability
metrics [22, 31], which ﬁnd a witness function to distinguish between P and Q. This class of
methods yields an objective similar to Eq. (2) that requires optimizing a distance function between
two distributions over a function class F. Particular choices for F yield the kernel maximum mean
discrepancy approach of [9, 16] or Wasserstein GANs [3]. The latter distance is deﬁned as

W (P, Q) = sup

{EP[f ] − EQ[f ]},

(cid:107)f (cid:107)L≤1

(6)

where the supremum is taken over functions f which have a bounded Lipschitz constant.

As shown in [3], the Wasserstein metric implies a different notion of convergence compared to the
JS divergence used in the original GAN. Essentially, the Wasserstein metric is said to be weak as it
requires the use of a weaker topology, thus making it easier for a sequence of distribution to converge.
The use of a weaker topology is achieved by restricting the function class to the set of bounded
Lipschitz functions. This yields a hard constraint on the function class that is empirically hard to
satisfy. In [3], this constraint is implemented via weight clipping, which is acknowledged to be a
"terrible way" to enforce the Lipschitz constraint. As will be shown later, our regularization penalty
can be seen as a soft constraint on the Lipschitz constant of the function class which is easy to
implement in practice. Recently, [10] has also proposed a similar regularization; while their proposal
was motivated for Wasserstein GANs and does not extend to f -divergences it is interesting to observe
that both their and our regularization work on the gradient.

Training with Noise. As suggested in [3, 30], one can break the dimensional misspeciﬁcation
discussed in Section 1 by adding continuous noise to the inputs of the discriminator, therefore
smoothing the probability distribution. However, this requires to add high-dimensional noise, which
introduces signiﬁcant variance in the parameter estimation process. Counteracting this requires a
lot of samples and therefore ultimately leads to a costly or impractical solution. Instead we propose
an approach that relies on analytic convolution of the densities P and Q with Gaussian noise. As
we demonstrate below, this yields a simple weighted penalty function on the norm of the gradients.
Conceptually we think of this noise not as being part of the generative process (as in [3]), but rather
as a way to deﬁne a smoother family of discriminants for the variational bound of f -divergences.

Regularization for Mode Dropping. Other regularization techniques address the problem of mode
dropping and are complementary to our approach. This includes the work of [7] which incorporates a
supervised training signal as a regularizer on top of the discriminator target. To implement supervision
the authors use an additional auto-encoder as well as a two-step training procedure which might
be computationally expensive. A similar approach was proposed by [20] that stabilizes GANs by
unrolling the optimization of the discriminator. The main drawback of this approach is that the
computational cost scales with the number of unrolling steps. In general, it is not clear to what extent
these methods not only stabilize GAN training, but also address the conceptual challenges listed in
Section 1.

3

3 Noise-Induced Regularization

From now onwards, we consider the general f -GAN [25] objective deﬁned as

F (P, Q; ψ) ≡ EP[ψ] − EQ[f c ◦ ψ].

(7)

3.1 Noise Convolution

From a practitioners point of view, training with noise can be realized by adding zero-mean random
variables ξ to samples x ∼ P, Q during training. Here we focus on normal white noise ξ ∼ Λ =
N (0, γ I) (the same analysis goes through with a Laplacian noise distribution for instance). From a
theoretical perspective, adding noise is tantamount to convolving the corresponding distribution as

EPEΛ[ψ(x + ξ)] =

ψ(x)

p(x − ξ)λ(ξ)dξ dx =

ψ(x)(p ∗ λ)(x)dx = EP∗Λ[ψ].

(8)

(cid:90)

(cid:90)

(cid:90)

where p and λ are probability densities of P and Λ, respectively, with regard to the Lebesgue measure.
The noise distribution Λ as well as the resulting P∗Λ are guaranteed to have full support in the ambient
space, i.e. λ(x) > 0 and (p ∗ λ)(x) > 0 (∀x). Technically, applying this to both P and Q makes the
resulting generalized f -divergence well-deﬁned, even when the generative model is dimensionally
misspeciﬁed. Note that approximating EΛ through sampling was previously investigated in [30, 3].

3.2 Convolved Discriminants

With symmetric noise, λ(ξ) = λ(−ξ), we can write Eq. (8) equivalently as

EP∗Λ[ψ] = EPEΛ[ψ(x + ξ)] =

p(x)

ψ(x − ξ)λ(−ξ) dξ dx = EP[ψ ∗ λ].

(9)

(cid:90)

(cid:90)

For the Q-expectation in Eq. (7) one gets, by the same argument, EQ∗Λ[f c ◦ ψ] = EQ [(f c ◦ ψ) ∗ λ].
Formally, this generalizes the variational bound for f -divergences in the following manner:
F (P ∗ Λ, Q ∗ Λ; ψ) = F (P, Q; ψ ∗ λ, (f c ◦ ψ) ∗ λ), F (P, Q; ρ, τ ) := EP[ρ] − EQ[τ ]

(10)

Assuming that F is closed under Λ convolutions, the regularization will result in a relative weakening
of the discriminator as we take the sup over a smaller, more regular family. Clearly, the low-pass
effect of Λ-convolutions can be well understood in the Fourier domain. In this equivalent formulation,
we leave P and Q unchanged, yet we change the view the discriminator can take on the ambient data
space: metaphorically speaking, the generator is paired up with a short-sighted adversary.

3.3 Analytic Approximations

In general, it may be difﬁcult to analytically compute ψ ∗ λ or – equivalently – EΛ[ψ(x + ξ)].
However, for small γ we can use a Taylor approximation of ψ around ξ = 0 (cf. [5]):

ψ(x + ξ) = ψ(x) + [∇ ψ(x)]T ξ +

ξT [∇2 ψ(x)] ξ + O(ξ3)

where ∇2 denotes the Hessian, whose trace Tr(∇2) = (cid:52) is known as the Laplace operator. The
properties of white noise result in the approximation

EΛ[ψ(x + ξ)] = ψ(x) +

(cid:52)ψ(x) + O(γ2)

(11)

(12)

and thereby lead directly to an approximation of Fγ (see Eq. (3)) via F = F0 plus a correction, i.e.

Fγ(P, Q; ψ) = F(P, Q; ψ) +

{EP [(cid:52)ψ] − EQ [(cid:52)(f c ◦ ψ)]} + O(γ2) .

(13)

γ
2

We can interpret Eq. (13) as follows: the Laplacian measures how much the scalar ﬁelds ψ and
f c ◦ ψ differ at each point from their local average. It is thereby an inﬁnitesimal proxy for the (exact)
convolution.

The Laplace operator is a sum of d terms, where d is the dimensionality of the ambient data space. As
such it does not suffer from the quadratic blow-up involved in computing the Hessian. If we realize
the discriminator ψ via a deep network, however, then we need to be able to compute the Laplacian
of composed functions. For concreteness, let us assume that ψ = h ◦ G, G = (g1, . . . , gk) and look

1
2

γ
2

4

at a single input x, i.e. gi : R → R, then
g(cid:48)
i · (∂ih ◦ G),

(h ◦ G)(cid:48) =

(cid:88)

(h ◦ G)(cid:48)(cid:48) =

i

(cid:88)

i

g(cid:48)(cid:48)
i · (∂ih ◦ G) +

i · g(cid:48)
g(cid:48)

j · (∂i∂jh ◦ G)

(14)

(cid:88)

i,j

So at the intermediate layer, we would need to effectively operate with a full Hessian, which is
computationally demanding, as has already been observed in [5].

3.4 Efﬁcient Gradient-Based Regularization

We would like to derive a (more) tractable strategy for regularizing ψ, which (i) avoids the detrimental
variance that comes from sampling ξ, (ii) does not rely on explicitly convolving the distributions
P and Q, and (iii) avoids the computation of Laplacians as in Eq. (13). Clearly, this requires to
make further simpliﬁcations. We suggest to exploit properties of the maximizer ψ∗ of F that can be
characterized by [24]

(f c(cid:48) ◦ ψ∗) dQ = dP =⇒ EP[h] = EQ[(f c(cid:48) ◦ ψ∗) · h]

(15)
The relevance of this becomes clear, if we apply the chain rule to (cid:52)(f c ◦ ψ), assuming that f c is
twice differentiable

(∀h, integrable).

(cid:52)(f c ◦ ψ) = (f c(cid:48)(cid:48) ◦ ψ) · ||∇ψ||2 + (cid:0)f c(cid:48) ◦ ψ(cid:1) (cid:52)ψ,

as now we get a convenient cancellation of the Laplacians at ψ = ψ∗ + O(γ)
(f c(cid:48)(cid:48) ◦ ψ∗) · (cid:107)∇ψ∗(cid:107)2(cid:105)

Fγ(P, Q; ψ∗) = F(P, Q; ψ∗) −

EQ

(cid:104)

+ O(γ2) .

γ
2

We can (heuristically) turn this into a regularizer by taking the leading terms,

Fγ(P, Q; ψ) ≈ F(P, Q; ψ) −

Ωf (Q; ψ), Ωf (Q; ψ) := EQ

(cid:104)

(f c(cid:48)(cid:48) ◦ ψ) · (cid:107)∇ψ(cid:107)2(cid:105)

.

(18)

γ
2

Note that we do not assume that the Laplacian terms cancel far away from the optimum, i.e. we do not
assume Eq. (15) to hold for ψ far away from ψ∗. Instead, the underlying assumption we make is that
optimizing the gradient-norm regularized objective Fγ(P, Q; ψ) makes ψ converge to ψ∗ + O(γ),
for which we know that the Laplacian terms cancel [5, 2].
The convexity of f c implies that the weighting function of the squared gradient norm is non-negative,
i.e. f c(cid:48)(cid:48) ≥ 0, which in turn implies that the regularizer − γ
2 Ωf (Q; ψ) is upper bounded (by zero).
Maximization of Fγ(P, Q; ψ) with respect to ψ is therefore well-deﬁned. Further considerations
regarding the well-deﬁnedness of the regularizer can be found in sec. 7.2 in the Appendix.

(16)

(17)

We have shown that training with noise is equivalent to regularizing the discriminator. Inspired by
the above analysis, we propose the following class of f -GAN regularizers:

4 Regularizing GANs

Regularized f -GAN

Fγ(P, Q; ψ) = EP [ψ] − EQ [f c ◦ ψ] −
(f c(cid:48)(cid:48) ◦ ψ) (cid:107)∇ψ(cid:107)2(cid:105)
(cid:104)

Ωf (Q; ψ) := EQ

γ
2

Ωf (Q; ψ)

(19)

The regularizer corresponding to the commonly used parametrization of the Jensen-Shannon GAN
can be derived analogously as shown in the Appendix. We obtain,

Regularized Jensen-Shannon GAN

Fγ(P, Q; ϕ) = EP [ln(ϕ)] + EQ [ln(1 − ϕ)] −

ΩJS(P, Q; ϕ)

γ
2
(cid:2)(1 − ϕ(x))2||∇φ(x)||2(cid:3) + EQ

ΩJS(P, Q; ϕ) := EP

(cid:2)ϕ(x)2||∇φ(x)||2(cid:3)

(20)

where φ = σ−1(ϕ) denotes the logit of the discriminator ϕ. We prefer to compute the gradient of φ
as it is easier to implement and more robust than computing gradients after applying the sigmoid.

5

Algorithm 1 Regularized JS-GAN. Default values: γ0 = 2.0, α = 0.01 (with annealing), γ = 0.1 (without
annealing), nϕ = 1
Require: Initial noise variance γ0, annealing decay rate α, number of discriminator update steps nϕ

per generator iteration, minibatch size m, number of training iterations T
Require: Initial discriminator parameters ω0, initial generator parameters θ0

for t = 1, ..., T do

γ ← γ0 · αt/T # annealing
for 1, ..., nϕ do

Sample minibatch of real data {x(1), ..., x(m)} ∼ P.
Sample minibatch of latent variables from prior {z(1), ..., z(m)} ∼ p(z).

F (ω, θ) =

m
(cid:88)

(cid:104)

(cid:16)

(cid:17)
ϕω(x(i))

ln

(cid:16)

+ ln

1 − ϕω(Gθ(z(i)))

(cid:17)(cid:105)

1
m

1
m

i=1

m
(cid:88)

(cid:20)(cid:16)

i=1
(cid:16)

(cid:16)

1
m

m
(cid:88)

i=1

Ω(ω, θ) =

1 − ϕω(x(i))

(cid:17)2(cid:12)
(cid:12)

(cid:12)∇φω(x(i))(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

2

+ ϕω

(cid:0)Gθ(z(i))(cid:1)2(cid:12)
(cid:12)

(cid:12)∇˜xφω(˜x)(cid:12)
(cid:12)

(cid:12)˜x=Gθ(z(i))

2(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ω ← ω + ∇ω

F (ω, θ) −

Ω(ω, θ)

# gradient ascent

end for
Sample minibatch of latent variables from prior {z(1), ..., z(m)} ∼ p(z).

γ
2

F (ω, θ) =

ln

1 − ϕω(Gθ(z(i)))

or Falt(ω, θ) = −

(cid:17)

(cid:17)

1
m

m
(cid:88)

i=1

(cid:16)
ϕω(Gθ(z(i)))

(cid:17)

ln

θ ← θ − ∇θF(ω, θ) # gradient descent

end for
The gradient-based updates can be performed with any gradient-based learning rule. We used
Adam in our experiments.

4.1 Training Algorithm

Regularizing the discriminator provides an efﬁcient way to convolve the distributions and is thereby
sufﬁcient to address the dimensional misspeciﬁcation challenges outlined in the introduction. This
leaves open the possibility to use the regularizer also in the objective of the generator. On the one
hand, optimizing the generator through the regularized objective may provide useful gradient signal
and therefore accelerate training. On the other hand, it destabilizes training close to convergence
(if not dealt with properly), since the generator is incentiviced to put probability mass where the
discriminator has large gradients. In the case of JS-GANs, we recommend to pair up the regularized
objective of the discriminator with the “alternative” or “non-saturating” objective for the generator,
proposed in [8], which is known to provide strong gradients out of the box (see Algorithm 1).

4.2 Annealing

The regularizer variance γ lends itself nicely to annealing. Our experimental results indicate that a
reasonable annealing scheme consists in regularizing with a large initial γ early in training and then
(exponentially) decaying γ to a small non-zero value. We leave to future work the question of how to
determine an optimal annealing schedule.

5 Experiments

5.1

2D submanifold mixture of Gaussians in 3D space

To demonstrate the stabilizing effect of the regularizer, we train a simple GAN architecture [20] on a
2D submanifold mixture of seven Gaussians arranged in a circle and embedded in 3D space (further
details and an illustration of the mixture distribution are provided in the Appendix). We emphasize
that this mixture is degenerate with respect to the base measure deﬁned in ambient space as it does
not have fully dimensional support, thus precisely representing one of the failure scenarios commonly

6

.

G
E
R
N
U

1
0
0

.

0

.

1

Figure 1: 2D submanifold mixture. The ﬁrst row shows one of several unstable unregularized GANs
trained to learn the dimensionally misspeciﬁed mixture distribution. The remaining rows show
regularized GANs (with regularized objective for the discriminator and unregularized objective for
the generator) for different levels of regularization γ. Even for small but non-zero noise variance, the
regularized GAN can essentially be trained indeﬁnitely without collapse. The color of the samples is
proportional to the density estimated from a Gaussian KDE ﬁt. The target distribution is shown in
Fig. 5. GANs were trained with one discriminator update per generator update step (indicated).

described in the literature [3]. The results are shown in Fig. 1 for both standard unregularized GAN
training as well as our regularized variant.

While the unregularized GAN collapses in literally every run after around 50k iterations, due to the
fact that the discriminator concentrates on ever smaller differences between generated and true data
(the stakes are getting higher as training progresses), the regularized variant can be trained essentially
indeﬁnitely (well beyond 200k iterations) without collapse for various degrees of noise variance, with
and without annealing. The stabilizing effect of the regularizer is even more pronounced when the
GANs are trained with ﬁve discriminator updates per generator update step, as shown in Fig. 6.

5.2 Stability across various architectures

To demonstrate the stability of the regularized training procedure and to showcase the excellent
quality of the samples generated from it, we trained various network architectures on the CelebA [17],
CIFAR-10 [15] and LSUN bedrooms [32] datasets. In addition to the deep convolutional GAN
(DCGAN) of [26], we trained several common architectures that are known to be hard to train
[4, 26, 19], therefore allowing us to establish a comparison to the concurrently proposed gradient-
penalty regularizer for Wasserstein GANs [10]. Among these architectures are a DCGAN without
any normalization in either the discriminator or the generator, a DCGAN with tanh activations and a
deep residual network (ResNet) GAN [11]. We used the open-source implementation of [10] for our
experiments on CelebA and LSUN, with one notable exception: we use batch normalization also for
the discriminator (as our regularizer does not depend on the optimal transport plan or more precisely
the gradient penalty being imposed along it).
All networks were trained using the Adam optimizer [13] with learning rate 2 × 10−4 and hyper-
parameters recommended by [26]. We trained all datasets using batches of size 64, for a total of
200K generator iterations in the case of LSUN and 100k iterations on CelebA. The results of these
experiments are shown in Figs. 3 & 2. Further implementation details can be found in the Appendix.

5.3 Training time

We empirically found regularization to increase the overall training time by a marginal factor
of roughly 1.4 (due to the additional backpropagation through the computational graph of the
discriminator gradients). More importantly, however, (regularized) f -GANs are known to converge
(or at least generate good looking samples) faster than their WGAN relatives [10].

7

RESNET

DCGAN

NO NORMALIZATION

TANH

Figure 2: Stability accross various architectures: ResNet, DCGAN, DCGAN without normalization
and DCGAN with tanh activations (details in the Appendix). All samples were generated from
regularized GANs with exponentially annealed γ0 = 2.0 (and alternative generator loss) as described
in Algorithm 1. Samples were produced after 200k generator iterations on the LSUN dataset (see also
Fig. 8 for a full-resolution image of the ResNet GAN). Samples for the unregularized architectures
can be found in the Appendix.

UNREG.

0.5

1.0

2.0

Figure 3: Annealed Regularization. CelebA samples generated by (un)regularized ResNet GANs.
The initial level of regularization γ0 is shown below each batch of images. γ0 was exponentially
annealed as described in Algorithm 1. The regularized GANs can be trained essentially indeﬁnitely
without collapse, the superior quality is again evident. Samples were produced after 100k generator
iterations.

5.4 Regularization vs. explicitly adding noise

We compare our regularizer against the common practitioner’s approach to explicitly adding noise to
images during training. In order to compare both approaches (analytic regularizer vs. explicit noise),
we ﬁx a common batch size (64 in our case) and subsequently train with different noise-to-signal
ratios (NSR): we take (batch-size/NSR) samples (both from the dataset and generated ones) to each
of which a number of NSR noise vectors is added and feed them to the discriminator (so that overall
both models are trained on the same batch size). We experimented with NSR 1, 2, 4, 8 and show the
best performing ratio (further ratios in the Appendix). Explicitly adding noise in high-dimensional
ambient spaces introduces additional sampling variance which is not present in the regularized variant.
The results, shown in Fig. 4, conﬁrm that the regularizer stabilizes across a broad range of noise
levels and manages to produce images of considerably higher quality than the unregularized variants.

5.5 Cross-testing protocol

We propose the following pairwise cross-testing protocol to assess the relative quality of two GAN
models: unregularized GAN (Model 1) vs. regularized GAN (Model 2). We ﬁrst report the confusion
matrix (classiﬁcation of 10k samples from the test set against 10k generated samples) for each model
separately. We then classify 10k samples generated by Model 1 with the discriminator of Model 2
and vice versa. For both models, we report the fraction of false positives (FP) (Type I error) and false
negatives (FN) (Type II error). The discriminator with the lower FP (and/or lower FN) rate deﬁnes
the better model, in the sense that it is able to more accurately classify out-of-data samples, which
indicates better generalization properties. We obtained the following results on CIFAR-10:

8

UNREGULARIZED

0.01

EXPLICIT NOISE
0.1

1.0

0.001

0.01

0.1

1.0

REGULARIZED

Figure 4: CIFAR-10 samples generated by (un)regularized DCGANs (with alternative generator loss),
as well as by training a DCGAN with explicitly added noise (noise-to-signal ratio 4). The level of
regularization or noise γ is shown above each batch of images. The regularizer stabilizes across a
broad range of noise levels and manages to produce images of higher quality than the unregularized
variants. Samples were produced after 50 training epochs.

Regularized GAN (γ = 0.1)

Unregularized GAN

True condition
Positive Negative
0.0002
0.9688
0.9998
0.0312

Predicted

Positive
Negative

Cross-testing: FP: 0.0

True condition
Positive Negative
0.0013
0.9987

1.0
0.0

Predicted

Positive
Negative

Cross-testing: FP: 1.0

For both models, the discriminator is able to recognize his own generator’s samples (low FP in the
confusion matrix). The regularized GAN also manages to perfectly classify the unregularized GAN’s
samples as fake (cross-testing FP 0.0) whereas the unregularized GAN classiﬁes the samples of the
regularized GAN as real (cross-testing FP 1.0). In other words, the regularized model is able to fool
the unregularized one, whereas the regularized variant cannot be fooled.

6 Conclusion

We introduced a regularization scheme to train deep generative models based on generative adversarial
networks (GANs). While dimensional misspeciﬁcations or non-overlapping support between the
data and model distributions can cause severe failure modes for GANs, we showed that this can be
addressed by adding a penalty on the weighted gradient-norm of the discriminator. Our main result is
a simple yet effective modiﬁcation of the standard training algorithm for GANs, turning them into
reliable building blocks for deep learning that can essentially be trained indeﬁnitely without collapse.
Our experiments demonstrate that our regularizer improves stability, prevents GANs from overﬁtting
and therefore leads to better generalization properties (cf cross-testing protocol). Further research on
the optimization of GANs as well as their convergence and generalization can readily be built upon
our theoretical results.

9

Acknowledgements

We would like to thank Devon Hjelm for pointing out that the regularizer works well with ResNets.
KR is thankful to Yannic Kilcher, Lars Mescheder, Paulina Grnarova and the dalab team for insightful
discussions. Big thanks also to Ishaan Gulrajani and Taehoon Kim for their open-source GAN
implementations. This work was supported by Microsoft Research through its PhD Scholarship
Programme.

References

matical Soc., 2007.

[1] Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry. American Mathe-

[2] Guozhong An. The effects of adding noise during backpropagation training on a generalization

performance. Neural Comput., pages 643–674, 1996.

[3] Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adver-

sarial networks. In ICLR, 2017.

[4] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial

networks. In PMLR, Proceedings of Machine Learning Research, 2017.

[5] Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computa-

tion, 7:108–116, 1995.

[6] Diane Bouchacourt, Pawan K Mudigonda, and Sebastian Nowozin. Disco nets: Dissimilarity
coefﬁcients networks. In Advances in Neural Information Processing Systems, pages 352–360,
2016.

[7] Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized

generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.

[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. In Advances in
Neural Information Processing Systems, pages 2672–2680, 2014.

[9] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research, 13:723–773, 2012.

[10] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems,
2017.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page
770–778, 2016.

[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In PMLR, Proceedings of Machine Learning Research,
pages 448–456, 2015.

[13] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. The

International Conference on Learning Representations (ICLR), 2014.

[14] Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. The International

Conference on Learning Representations (ICLR), 2013.

[15] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

[16] Yujia Li, Kevin Swersky, and Richard S Zemel. Generative moment matching networks. In

CS UToronto, 2009.

ICML, pages 1718–1727, 2015.

[17] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in
the wild. In Proceedings of the IEEE International Conference on Computer Vision, pages
3730–3738, 2015.

[18] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the

wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.

10

[19] Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances

in Neural Information Processing Systems, 2017.

[20] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial

networks. In International Conference on Learning Representations (ICLR), 2016.

[21] Tom Minka. Divergence measures and message passing. Technical report, Microsoft Research,

2005.

[22] Alfred Müller. Integral probability metrics and their generating classes of functions. Advances

in Applied Probability, 29:429–443, 1997.

[23] Hariharan Narayanan and Sanjoy Mitter. Sample complexity of testing the manifold hypothesis.

In Advances in Neural Information Processing Systems, pages 1786–1794, 2010.

[24] XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence func-
tionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information
Theory, 56(11):5847–5861, 2010.

[25] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neu-
ral samplers using variational divergence minimization. In Advances in Neural Information
Processing Systems, pages 271–279, 2016.

[26] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

[27] Mark D Reid and Robert C Williamson. Information, divergence and risk for binary experiments.

Journal of Machine Learning Research, 12:731–817, 2011.

[28] Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31st International
Conference on Machine Learning, 2014.

[29] David W Scott. Multivariate density estimation: theory, practice, and visualization. John Wiley

& Sons, 2015.

[30] Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszár. Amortised

map inference for image super-resolution. arXiv preprint arXiv:1610.04490, 2016.

[31] Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, and Gert RG
Lanckriet. On integral probability metrics, phi-divergences and binary classiﬁcation. arXiv
preprint arXiv:0901.2698, 2009.

[32] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction
of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365, 2015.

11

7 APPENDIX

7.1 Derivation of the Jensen-Shannon Regularizer.

The Jensen-Shannon GAN is typically encountered in one of two equivalent parametrizations: the
commonly used “original” GAN parametrization [8],

and the Fenchel-dual f -GAN parametrization [25], where f c = − ln(1 − exp),
EP [ψ] − EQ [f c ◦ ψ] = EP [ψ] + EQ [ln(1 − exp(ψ))]

EP [ln(ϕ)] + EQ [ln(1 − ϕ)]

(21)

(22)

Depending on whether we train the JS-GAN through its Fenchel-dual parametrization or with the
original GAN objective we have to either use the regularizer in the general f -GAN form in Eq. (19),
or in the speciﬁc Jensen-Shannon parametrization given in Eq. (20).

We now show how to derive the speciﬁc Jensen-Shannon regularizer, which is basically a repetition
of the derivation of the general f -GAN regularizer presented in the main text. Using the same
terminology and following the same line of thought as in section 3.3, i.e. assuming the noise variance
is small, we can Taylor approximate the statistics ϕ around ξ = 0,

ϕ(x + ξ) = ϕ(x) + [∇ ϕ(x)] ξ +

ξT [∇2 ϕ(x)] ξ + O(ξ3)

(23)

1
2

Expanding the noise convolved version of the objective in Eq. (21) and making use of the zero-mean
and uncorrelatedness properties of the noise distribution, as well as applying the chain rule, we obtain

Fγ(P, Q; ϕ) = EP [ln(ϕ)] + EQ [ln(1 − ϕ)]
(cid:52)ϕ − ||∇ ln(ϕ)||2(cid:105)
(cid:104) 1
ϕ

γ
2

EP

+

(cid:26)

(cid:104)

+ EQ

−

1
1 − ϕ

(cid:52)ϕ − ||∇ ln(1 − ϕ)||2(cid:105)(cid:27)

+ O(γ2) .

(24)
Following the same arguments as in section 3.4, one can again show that the Laplacian terms cancel
at ϕ = ϕ∗ + O(γ) and we arrive at

Fγ(P, Q; ϕ) = EP [ln(ϕ)] + EQ [ln(1 − ϕ)]
(cid:2)||∇ ln(ϕ)||2(cid:3) + EQ

γ
2
allowing us to read off the corresponding JS regularizer,

(cid:8)EP

−

ΩJS(P, Q; ϕ) = EP

(cid:2)||∇ ln(1 − ϕ(x))||2(cid:3)

(cid:2)||∇ ln(1 − ϕ)||2(cid:3)(cid:9) + O(γ2) ,

1

(cid:2)||∇ ln ϕ(x)||2(cid:3) + EQ
(ϕ(x))2 ||∇ϕ(x)||2(cid:105)
(cid:104)
(ϕ(x))2 ||σ(cid:48)(φ(x))∇φ(x)||2(cid:105)

+ EQ

1

(cid:104)

(cid:104)

= EP

= EP

1

(1 − ϕ(x))2 ||∇ϕ(x))||2(cid:105)

+ EQ

(cid:104)

1

(1 − ϕ(x))2 ||σ(cid:48)(φ(x))∇φ(x)||2(cid:105)

In order to obtain the regularizer in the logit-parameterization, with ϕ(x) = σ(φ(x)), we make use
of the following property of the sigmoid

σ(cid:48)(t) = σ(t)(1 − σ(t))

which allows us to write

ΩJS(P, Q; φ) = EP

(cid:2)(1 − ϕ(x))2||∇φ(x)||2(cid:3) + EQ

(cid:2)ϕ(x)2||∇φ(x)||2(cid:3)

Let us ﬁnally also show that these two parametrizations are indeed equivalent at the optimum:

f c(cid:48)(cid:48)(t) =

exp(t)
(1 − exp(t))2

=⇒ f c(cid:48)(cid:48)(ln(ϕ∗)) =

ϕ∗
(1 − ϕ∗)2 =

ϕ∗
1 − ϕ∗ +

ϕ∗2
(1 − ϕ∗)2

Thus,

f c(cid:48)(cid:48)(ln(ϕ∗))(cid:107)∇ ln(ϕ∗)(cid:107)2dQ =

(25)

(26)

(27)

(28)

(29)

(30)

(cid:16) ϕ∗
1 − ϕ∗ +
= (cid:107)∇ ln(ϕ∗)(cid:107)2dP + (cid:107)

ϕ∗2
(1 − ϕ∗)2

(cid:17)

(cid:107)∇ ln(ϕ∗)(cid:107)2dQ

ϕ∗
1 − ϕ∗ ∇ ln(ϕ∗)(cid:107)2dQ

= (cid:107)∇ ln(ϕ∗)(cid:107)2dP + (cid:107)∇ ln(1 − ϕ∗)(cid:107)2dQ

12

7.2 Further Considerations Regarding the Regularizer

Following-up on our discussion of the regularizer in section 3.4: Due to a support or dimensionality
mismatch we may have sup F (P, Q; ψ) = +∞ and ψ∗ may not exist. However, with ψ∗
(cid:15) being the
maximizer of F(cid:15) (which is guaranteed to exist for any (cid:15) > 0), we get

Fγ(P ∗ N (0, (cid:15)I), Q ∗ N (0, (cid:15)I)) = F(cid:15)(P, Q; ψ∗

(cid:15) ) −

Ωf (Q ∗ N (0, (cid:15)I); ψ∗

(cid:15) ) + O(γ2) .

(31)

γ
2

(cid:15) (cid:107). The sequence of regularizers Ωf (Q ∗ N (0, (cid:15)I); ·),
As (cid:15) → 0, F(cid:15) may diverge and so may (cid:107)∇ψ∗
however, converges (at least pointwise) to a well deﬁned limit, which is Ωf (Q; ·). This shows that
the regularizer is well-deﬁned even under dimensional misspeciﬁcations.

We can also justify the approximation in Eq. (18) more rigorously. Starting from the Taylor approxi-
mation of f c(cid:48) at ψ∗, we get pointwise

f c(cid:48) ◦ ψ = f c(cid:48) ◦ ψ∗ + f c(cid:48)(cid:48) ◦ (ψ − ψ∗) + O(|ψ − ψ∗|2)

(32)
(cid:2)(f c(cid:48)(cid:48) ◦ (ψ − ψ∗)) · (cid:52)ψ(cid:3). Using
So in ﬁrst order of (cid:107)ψ −ψ∗(cid:107)∞, the approximation error is ∆ := EQ
Green’s identity, we can derive the following bound, ∆ ≤ O(Lδ), where we assume |f c(cid:48)(cid:48)(cid:48)| ≤ L and
(cid:107)∇ψ∗ − ∇ψ(cid:107)∞ < δ. However, as this result only gives a formal guarantee for (ψ, ∇ψ) sufﬁciently
close to (ψ∗, ∇ψ∗), we refrain from presenting further technical details.

7.3 2D Submanifold Mixture of Gaussians in 3D Space

Experimental Setup. The experimental setup is inspired by the two-dimensional mixture of Gaus-
sians in [20]. The dataset is constructed as follows. We sample from a mixture of seven Gaussians
with standard deviation 0.01 and means equally spaced around the unit circle. This 2D mixture
√
is then embedded in 3D space (x, y) → (x, y, 0), rotated by π/4 around the axis (1, −1, 0)/
2
3. As illustrated in Fig. 5, this yields a mixture distribution that lives
and translated by (1, 1, 1)/
in a tilted 2D submanifold embedded in 3D space. It is important to emphasize that the mixture
distribution is by design degenerate with respect to the base measure in 3D as it does not have full
dimensional support. This precisely represents the dimensional misspeciﬁcation scenario for GANs
that we aim to address with our regularizer.

√

Figure 5: 2D submanifold mixture of Gaussians in 3D, obtained by embedding a two-dimensional
mixture with means equally spaced around the unit circle in 3D ambient space. For visualization
purposes, the standard deviation of the shown mixture components is 10x larger than the one used
in the experiments. Samples are colored proportional to their density which we estimated from a
Gaussian KDE with bandwidth selected using Scott’s rule [29].

Architecture. The architecture corresponds to the one used in [20] with one notable exception. We
use 2 dimensional latent vectors z, sampled from a multivariate normal prior, (whereas [20] uses 256
dimensional z), as we found lower dimensional latent variables greatly improve the performance of
the unregularized GAN against which we compare. We did all experiments also for latent vectors of
dimension 15: the obtained results are in accordance with those presented in the main text and below.

Both networks are optimized using Adam with a learning rate of 1e−3 and standard hyper-parameters.
We trained on batches of size 512. Ten batches were generated to produce one image of the mixture
at given time steps. The generator and discriminator network parameters were updated alternatively.

13

.

G
E
R
N
U

1
0
0

.

1
0

.

0
1

.

Figure 6: 2D submanifold mixture. The ﬁrst row shows one of several unstable unregularized
GANs trying to learn the dimensionally misspeciﬁed mixture distribution. The remaining rows show
regularized GANs (with regularized objective for the discriminator and unregularized objective for
the generator) for different levels of regularization γ. Even for small but non-zero noise variance, the
regularized GAN can essentially be trained indeﬁnitely without collapse. The color of the samples is
proportional to the density estimated from a Gaussian KDE ﬁt. The target distribution is shown in
Fig. 5. GANs were trained with ﬁve discriminator updates per generator update step (indicated).

7.4

Image Datasets and Network Architectures

We trained on CelebA [18], CIFAR-10 [15] and LSUN [32]. All datasets were trained on minibatches
of size 64. The respective GAN architectures are listed below.

DCGAN. For
the CIFAR-10 experiments, we used the DCGAN (Deep Convolutional
GAN) architecture of [26] implemented in Tensorﬂow by https://github.com/carpedm20/
DCGAN-tensorflow.

For the LSUN experiments, we used the DCGAN architecture of [26] implemented in Tensorﬂow by
https://github.com/igul222/improved_wgan_training.

In both cases, the discriminator uses batch normalization [12] in all but the ﬁrst and last layer (except
where explicitly stated otherwise). The generator uses batch normalization in all layers except the last
one. The latent code is sampled from a 100-dimensional uniform distribution over [0, 1] (carpedm20)
resp. 128-dimensional normal-(0,1) distribution (Gulrajani).

Both networks were trained using the Adam optimizer [13] (with hyper-parameters recommended
by the DCGAN authors) for various different learning rates in the range [0.001, 0.0001]. The
recommended learning rate 2 × 10−4 was found to perform best.

ResNet GAN. For the CelebA and LSUN experiments, we used a deep residual network
ResNet [11] implemented in Tensorﬂow by https://github.com/igul222/improved_wgan_
training (implementation details can be found in [10]).

The ResNets use pre-activation residual blocks with two 3 x 3 convolutional layers each and ReLU
nonlinearities. The generator has one linear layer, followed by four residual blocks, one deconvolu-
tional layer and tanh output activations. The discriminator has one convolutional layer, followed by
four residual blocks and a linear output layer (the discriminator logits are then fed into the sigmoid
GAN loss).

14

We use batch normalization [12] in the generator and discriminator (except explicitly stated otherwise).
Both networks were optimized using Adam with learning rate 2 × 10−4 and standard hyperparameters.
For further architectural details, please refer to [10] and the excellent open-source implementation
referenced above.

7.5 Further Experimental Results.

RESNET

DCGAN

NO NORMALIZATION

TANH

Figure 7: Unregularized GANs (with alternative generator loss) accross various architectures: ResNet,
DCGAN, DCGAN without normalization and DCGAN with tanh activations. Samples were produced
after 200k generator iterations on the LSUN dataset. Samples for the regularized architectures can
be found in the main text. Some of these architectures are known to be hard to train.

Figure 8: Regularized ResNet GAN. Full-resolution 64 × 64 images generated by the regularized
ResNet GAN after 200k generator iterations on the LSUN dataset. The initial level of regulariza-
tion γ0 = 2.0 was exponentially annealed to γT = 0.01.

15

1

2

4

8

1
0
0

.

1
0

.

0
.
1

1
0
.
0

1
.
0

0
.
1

Figure 9: DCGAN trained with explicitly added noise. Top 3 rows: discriminator and generator
trained with noise. Bottom 3 rows: discriminator is trained with noise while generator is trained
without noise. White normal noise is added to images from the dataset as well as to samples from the
generator during training. The generator was trained through the alternative loss. The level of noise
γ is shown in the left most column, the different noise-to-signal ratios (NSR) above each column of
images. Samples were produced after 50 training epochs.

16

