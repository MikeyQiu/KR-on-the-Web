Parallelized Tensor Train Learning of Polynomial
Classiﬁers
Zhongming Chen∗, Kim Batselier†, Johan A.K. Suykens‡ and Ngai Wong†

1

7
1
0
2
 
v
o
N
 
6
 
 
]

G
L
.
s
c
[
 
 
4
v
5
0
5
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract—In pattern classiﬁcation, polynomial classiﬁers are
well-studied methods as they are capable of generating complex
decision surfaces. Unfortunately, the use of multivariate polyno-
mials is limited to kernels as in support vector machines, because
polynomials quickly become impractical for high-dimensional
problems. In this paper, we effectively overcome the curse of
dimensionality by employing the tensor train format to represent
a polynomial classiﬁer. Based on the structure of tensor trains,
two learning algorithms are proposed which involve solving
different optimization problems of low computational complex-
ity. Furthermore, we show how both regularization to prevent
overﬁtting and parallelization, which enables the use of large
training sets, are incorporated into these methods. The efﬁciency
and efﬁcacy of our tensor-based polynomial classiﬁer are then
demonstrated on the two popular datasets USPS and MNIST.

Index Terms—Supervised learning, tensor train, pattern clas-

siﬁcation, polynomial classiﬁer.

I. INTRODUCTION

Pattern classiﬁcation is the machine learning task of iden-
tifying to which category a new observation belongs, on the
basis of a training set of observations whose category mem-
bership is known. This type of machine learning algorithm
that uses a known training dataset to make predictions is
called supervised learning, which has been extensively studied
and has wide applications in the ﬁelds of bioinformatics
[1], computer-aided diagnosis (CAD) [2], machine vision [3],
speech recognition [4], handwriting recognition [5], spam
detection and many others [6], [7], [8]. Usually, different kinds
of learning methods use different models to generalize from
training examples to novel test examples.

As pointed out in [9], [10], one of the important invariants
in these applications is the local structure: variables that are
spatially or temporally nearby are highly correlated. Local
correlations beneﬁt extracting local features because conﬁgu-
rations of neighboring variables can be classiﬁed into a small
number of categories (e.g. edges, corners...). For instance, in
handwritten character recognition, correlations between image
pixels that are nearby tend to be more reliable than the
ones of distant pixels. Learning methods incorporating this
kind of prior knowledge often demonstrate state-of-the-art
performance in practical applications. One popular method
for handwritten character recognition is using convolutional
neural networks (CNNs) [11], [12] which are variations of

∗Department of Mathematics, School of Science, Hangzhou Dianzi Uni-

versity, Hangzhou 310018, China. Email: czm183015@126.com.

†Department of Electrical and Electronic Engineering, The University of

Hong Kong. Email: {kimb, nwong}@eee.hku.hk.

‡KU Leuven, ESAT, STADIUS. B-3001 Leuven, Belgium. Email:

jo-

han.suykens@esat.kuleuven.be.

multilayer perceptrons designed to use minimal amounts of
preprocessing. In this model, each unit in a layer receives
inputs from a set of units located in a small neighborhood
in the previous layer, and these mappings share the same
weight vector and bias in a given convolutional layer. An
important component of a CNN are the pooling layers, which
implement a nonlinear form of down-sampling. In this way,
the amount of parameters and computational load are reduced
in the network. Another popular method uses support vector
machines (SVMs) [13], [14]. The original ﬁnite-dimensional
feature space is mapped into a much higher-dimensional
space, where the inner product is easily computed through the
‘kernel trick’. By considering the Wolfe dual representation,
one can ﬁnd the maximum-margin hyperplane to separate the
examples of different categories in that space. However, it is
worth mentioning that these models require a large amount of
memory and a long processing time to train the parameters.
For instance, if there are thousands of nodes in the CNN,
the weight matrices of fully-connected layers are of the order
of millions. The major limitation of basic SVMs is the high
computational complexity which is at least quadratic with the
dataset size. One way to deal with large datasets in the SVM-
framework is by using a ﬁxed-size least squares SVM (ﬁxed-
size LS-SVM) [15], which approximates the kernel mapping
in such a way that the problem can be solved in the primal
space.

II. TENSORS IN MACHINE LEARNING

Tensors are a multidimensional generalization of matrices
to higher orders and have recently gained attention in the
ﬁeld of machine learning. The classiﬁcation via tensors, as
opposed to matrices or vectors, was ﬁrst considered in [16], by
extending the concept of spectral regularization for matrices to
tensors. The tensor data is assumed to satisfy a particular low-
rank Tucker decomposition, which unfortunately still suffers
from an exponential storage complexity. Other work has
focused speeding-up the convolution operation in CNNs [17]
by approximating this operation with a low-rank polyadic
decomposition of a tensor. In [18], the weight matrices of
fully-connected layers of neural networks are represented
by tensor trains (TTs), effectively reducing the number of
parameters. TTs have also been used to represent nonlinear
predictors [19] and classiﬁers [20]. The key idea here is always
to approximate a mapping that is determined by an exponential
number of parameters nd by a TT with a storage complexity
of dnr2 parameters. To our knowledge, this idea has not yet
been applied to polynomial classiﬁers that also suffer from the

curse of dimensionality. The usual approach to circumvent the
exponential number of polynomial coefﬁcients would be to
use SVMs with a polynomial kernel and solve the problem
in the dual space. In this article, we exploit
the efﬁcient
representation of a multivariate polynomial as a TT in order to
avoid the curse of dimensionality, allowing us to work directly
in the feature space. The main contributions are:

• We derive a compact description of a polynomial clas-
siﬁer using the TT format, avoiding the curse of dimen-
sionality.

• Two efﬁcient learning algorithms are proposed by exploit-

ing the TT structure.

• Both regularization and a parallel implementation are
incorporated into our methods, thus avoiding overﬁtting
and allowing the use of large training datasets.

This paper is organized as follows. In Section III, we give
a brief introduction to tensor basics, including the TT de-
composition, important tensor operations and properties. The
framework of TT learning for pattern classiﬁcation is presented
in Section IV. Based on different loss functions, two efﬁcient
learning algorithms are proposed in Section V, together with a
discussion on regularization and parallelization. In Section VI,
we test our algorithms on two popular datasets: USPS and
MNIST and compare their performance with polynomial clas-
siﬁers trained with LS-SVMs [15]. Finally, some conclusions
and further work are summarized in Section VII.

Throughout this paper, we use small letters x, y, . . . , for
scalars, small bold letters x, y, . . . , for vectors, capital letters
A, B, . . . , for matrices, and calligraphic letters A, B, . . . , for
tensors. The transpose of a matrix A or vector x is denoted by
A(cid:62) and x(cid:62), respectively. The identity matrix of dimension n is
denoted by In. A list of abbreviations used here is summarized
in Table I.

TABLE I
LIST OF ABBREVIATIONS

Tensor Train
Convolutional Neural Network

TT
CNN
SVM Support Vector Machine
TTLS
TTLR
USPS

Tensor Train learning by Least Squares
Tensor Train learning by Logistic Regression
US Postal Service database

MNIST Modiﬁed NIST database

III. PRELIMINARIES

A. Tensors and pure-power-n polynomials

A real dth-order or d-way tensor is a multidimensional array
A ∈ Rn1×n2×···×nd that generalizes the notions of vectors
and matrices to higher orders. Each of the entries Ai1i2···id
is determined by d indices. The numbers n1, n2, . . . , nd are
called the dimensions of the tensor. An example tensor with
dimensions 4, 3, 2 is shown in Fig. 1. We now give a brief
introduction to some required tensor operations and properties,
more information can be found in [21].

2

Fig. 1. An example tensor A = (Ai1i2i3 ) ∈ R4×3×2, where i1, i2, i3
denote the indices for each mode respectively.

The k-mode product B = A ×k U of a tensor A ∈

Rn1×n2×···×nd and a matrix U ∈ Rn(cid:48)

k×nk is deﬁned by

Bi1···ik−1jik+1···id =

Ai1···ik−1ikik+1···id Ujik ,

(1)

nk(cid:88)

ik=1

and B ∈ Rn1×···×nk−1×n(cid:48)
k×nk+1×···×nd . In particular, given
a d-way tensor A ∈ Rn×n×···×n and a vector x ∈ Rn, the
multidimensional contraction, denoted by Axd, is the scalar

Axd = A ×1 x(cid:62) ×2 x(cid:62) ×3 · · · ×d x(cid:62),

(2)

which is obtained as a homogeneous polynomial of x ∈ Rn
with degree d. The inner product of two same-sized tensors
A, B ∈ Rn1×n2×···×nd is the sum of the products of their
entries, i.e.,

(cid:104)A, B(cid:105) =

· · ·

Ai1i2···id Bi1i2···id .

(3)

n1(cid:88)

n2(cid:88)

nd(cid:88)

i1=1

i2=1

id=1

The Frobenius norm of a tensor A ∈ Rn1×n2×···×nd is given
by

(cid:107)A(cid:107)F = (cid:112)(cid:104)A, A(cid:105).
The vectorization of a tensor A ∈ Rn1×n2×···×nd is de-
noted by vec(A) and maps the tensor element with indices
(i1, i2, . . . , id) to the vector element with index i where

(4)

i = i1 + (i2 − 1)n1 + · · · + (id − 1)

nk.

d−1
(cid:89)

k=1

Given d vectors x(i) ∈ Rni, i = 1, 2, . . . , d, their outer product
is denoted by x(1) ◦ x(2) ◦ · · · ◦ x(d), which is a tensor in
Rn1×n2×···×nd such that its entry with indices (i1, i2, . . . , id)
is equal to the product of the corresponding vector elements,
namely, x(1)
i1

. It follows immediately that

· · · x(d)
id

x(2)
i2

vec(x(1) ◦ x(2) ◦ · · · ◦ x(d)) = x(d) ⊗ x(d−1) ⊗ · · · ⊗ x(1), (5)

where the symbol “⊗” denotes the Kronecker product.

We now illustrate how to represent a polynomial by using
tensors. Denote by R[x] the polynomial ring in d variables
x = (x1, x2, . . . , xd)(cid:62) with coefﬁcients in the ﬁeld R.
Deﬁnition 1. Given a vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd, a
polynomial f ∈ R[x] with d variables is called pure-power-˜n

f (x) = A ×1 v(x1)(cid:62) ×2 v(x2)(cid:62) ×3 · · · ×d v(xd)(cid:62).

(7)

rk ≤ min(

ni,

ni),

k = 1, 2, . . . , d − 1.

k
(cid:89)

d
(cid:89)

i=1

i=k+1

if the degree of f is at most ˜ni with respect to each variable
xi, i = 1, 2, . . . , d.
Example 1. The polynomial f = 4x1 +x3
is a pure-power-˜n polynomial with ˜n = (3, 1, 2).

1 −2x1x2x3 −7x2x2
3

The set of all pure-power-˜n polynomials with the degree
vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd is denoted by R[x]˜n. For
any f (x) ∈ R[x]˜n, there are a total of (cid:81)d
k=1(˜nk + 1) distinct
monomials

d
(cid:89)

k=1

xik−1
k

,

1 ≤ ik ≤ ˜nk + 1,

k = 1, 2, . . . , d.

For x = (x1, x2, . . . , xd)(cid:62) ∈ Rd, denote by {v(xk)}d
Vandermonde vectors

k=1 the

v(xk) := (1, xk, . . . , x˜nk

k )(cid:62) ∈ R˜nk+1.

(6)

follows that

It
there is a one-to-one mapping between
pure-power-˜n polynomials and tensors. To be speciﬁc, for
any f (x) ∈ R[x]˜n,
there exists a unique tensor A ∈
R(˜n1+1)×(˜n2+1)×···×(˜nd+1) such that

1, x3

1), v(x2) = (1, x2), v(x3) = (1, x3, x2

Example 2. We revisit the polynomial f from Example 1
and illustrate its corresponding tensor representation. Since
˜n = (3, 1, 2), we construct the Vandermonde vectors v(x1) =
(1, x1, x2
3). The
nonzero entries of the corresponding 4×2×3 tensor A are then
A211 = 4, A411 = 1, A222 = −2, A123 = −7. The indices of
the tensor A are easily found from grouping together corre-
sponding indices of the Vandermonde vectors. For example,
the tensor index 123 corresponding with the monomial x2x2
3
is found from v(x1)1 = 1, v(x2)2 = x2, v(x3)3 = x2
3.

B. Tensor trains

It is well known that the number of tensor elements grows
exponentially with the order d. Even when the dimensions
are small, the storage cost for all elements is prohibitive for
large d. The TT decomposition [22] gives an efﬁcient way (in
storage and computation) to overcome this so-called curse of
dimensionality.

The main idea of the TT decomposition is to re-express
the entries of a tensor A ∈ Rn1×n2×···×nd as a product of
matrices

Ai1i2···id = G1(i1)G2(i2) · · · Gd(id),

(8)

where Gk(ik) is an rk−1 × rk matrix for each index ik, also
called the TT-core. To turn the matrix-by-matrix product (8)
into a scalar, boundary conditions r0 = rd = 1 have to be
introduced. The quantities {rk}d
k=0 are called the TT-ranks.
Note that each core Gk is a third-order tensor with dimensions
rk−1, nk and rk. The TT-decomposition for a tensor A ∈
Rn1×n2×n3 is illustrated in Fig. 2. The most common way
to convert a given tensor A into a TT would be the TT-SVD
algorithm [22, p. 2301].

Example 3. TT-SVD algorithm [22, p. 2301]. Using the TT-
SVD algorithm, we can convert the tensor A from Example

3

Fig. 2. The TT decomposition for a tensor in Rn1×n2×n3 .

2 into a TT that consists of TT-cores G1 ∈ R1×4×3, G2 ∈
R3×2×3, G3 ∈ R3×3×1.

Note that throughout this article, we will not need to use
the TT-SVD algorithm. Instead, we will initialize the TT-cores
randomly and iteratively update the cores one-by-one in an
alternating fashion. It turns out that if all TT-ranks are bounded
by r, the storage of the TT grows linearly with the order d as
O(dnr2), where n = max{n1, n2, . . . , nd}.

Proposition 1 (Theorem 2.1 of [23]). For any tensor A ∈
Rn1×n2×···×nd , there exists a TT-decomposition with TT-ranks

We also mention that the TT representation of a tensor is
not unique. For instance, let Q be an orthogonal matrix in
Rr1×r1, namely, QQ(cid:62) = Q(cid:62)Q = Ir1. Then the tensor A in
(8) also has the TT-decomposition

Ai1i2···id = G(cid:48)

1(i1)G(cid:48)

2(i2) · · · Gd(id),

(9)

where

1(i1) = G1(i1)Q, G(cid:48)
G(cid:48)

2(i2) = Q(cid:62)G2(i2).

Numerical stability of our learning algorithms is guaranteed
by keeping all the TT-cores left-orthogonal or right-orthogonal
[24], which is achieved through a sequence of QR decompo-
sitions as explained in Section V.

Deﬁnition 2. The rk−1 × nk × rk core Gk is called left-
orthogonal if

and the rk−1 × nk × rk core Gk is called right-orthogonal if

nk(cid:88)

ik=1

nk(cid:88)

ik=1

Gk(ik)(cid:62)Gk(ik) = Irk ,

Gk(ik)Gk(ik)(cid:62) = Irk−1.

As stated before, the structure of a TT also beneﬁts the

computation of the general multidimensional contraction:

f = A ×1 (v(1))(cid:62) ×2 (v(2))(cid:62) ×3 · · · ×d (v(d))(cid:62),

(10)

where A ∈ Rn1×n2×···×nd and v(i) = (v(i)
ni )(cid:62) ∈
Rni, i = 1, 2, . . . , d. If a tensor A is given in the TT-
format (8), then we have

2 , . . . , v(i)

1 , v(i)

f =

d
(cid:89)

nk(cid:88)

k=1

ik=1

v(k)
ik

Gk(ik).

(11)

The described procedure for fast TT contraction is summarized
in Algorithm 1. In order to simplify the analysis on the
computational complexity of Algorithm 1, we assume that
r1 = r2 = · · · = rd−1 = r and n1 = n2 = · · · = nd = n.
There are two required steps to compute the contraction of
a TT with vector. First, we need to construct d matrices
V (k) by contracting the TT-cores Gk with the vectors v(k).
This operation is equivalent with d matrix-vector products
with a total computational cost of approximately O(dr2n)
ﬂops. Fortunately, the contraction of one TT-core is completely
independent from the other contractions and hence can be
done in parallel over d processors, reducing the computational
complexity to O(r2n) per processor. Maximal values for r and
n in our experiments are 10 and 4, respectively, so that the
contraction of one TT-core is approximately equivalent with
the product of a 100 × 4 matrix with a 4 × 1 vector. The ﬁnal
step in Algorithm 1 is the product of all matrices V (k) with
a total computational complexity of O(dr2). If we again set
r = 10, n = 4, then this ﬁnal step in Algorithm 1 is equivalent
with the product of a 100×40 matrix with a 40×1 vector. For
more basic operations implemented in the TT-format, such as
tensor addition and computing the Frobenius norm, the reader
is referred to [22].

Algorithm 1 Fast TT contraction [22]
Input: Vectors v(k) ∈ Rnk , k = 1, 2, . . . , d and a tensor A

in the TT-format with cores Gk

Output: The multidimensional contraction f in (10)

ik=1 v(k)

ik

Gk(ik)

%Computed in parallel

1: for k = 1 : d do
V (k) = (cid:80)nk
2:
3: end for
4: f := V (1)
5: for k = 2 : d do
f := f V (k)
6:
7: end for
8: return f

IV. TT LEARNING

It is easy for us to recognize a face, understand spoken
words, read handwritten characters and identify the gender
of a person. Machines, however, make decisions based on
data measured by a large number of sensors. In this section,
we present the framework of TT learning. Like most pattern
recognition systems [25], our TT learning method consists in
dividing the system into three main modules, shown in Fig. 3.
The ﬁrst module is called feature extraction, which is of
paramount importance in any pattern classiﬁcation problem.
The goal of this module is to build features via transformations
of the raw input, namely, the original data measured by a large
number of sensors. The basic reasoning behind transform-
based features is that an appropriately chosen transforma-
tion can exploit and remove information redundancies, which
usually exist in the set of samples obtained by measuring
devices. The set of features exhibit high information packaging
properties compared with the original input samples. This
means that most of the classiﬁcation-related information is

4

compressed into a relatively small number of features, leading
to a reduction of the necessary feature space dimension.
Feature extraction beneﬁts training the classiﬁer in terms of
memory and computation, and also alleviates the problem of
overﬁtting since we get rid of redundant information. To deal
with the task of feature extraction, some linear or nonlinear
transformation techniques are widely used. For example, the
Karhunen-Lo`eve transform, related to principal component
analysis (PCA), is one popular method for feature generation
and dimensionality reduction. A nonlinear kernel version of
the classical PCA is called kernel PCA, which is an extension
of PCA using kernel methods. The discrete Fourier transform
(DFT) can be another good choice due to the fact that for
many practical applications, most of the energy lies in the
low-frequency components. Compared with PCA, the basis
vectors in the DFT are ﬁxed and problem-dependent, which
leads to a low computational complexity.

The second module, the TT classiﬁer, is the core of TT
learning. The purpose of this module is to mark a new
observation based on its features generated by the previous
module. As will be discussed, the task of pattern classiﬁcation
can be divided into a sequence of binary classiﬁcations. For
each particular binary classiﬁcation, the TT classiﬁer assigns
to each new observation a score that indicates which class it
belongs to. In order to construct a good classiﬁer, we exploit
the fact that we know the labels for each sample of a given
dataset. The TT classiﬁer is trained optimally with respect
to an optimality criterion. In some ways, the TT classiﬁer
can be regarded as a kind of generalized linear classiﬁer,
it does a linear classiﬁcation in a higher dimensional space
generated by the items of a given pure-power polynomial. The
local information is encoded by the products of features. In
contrast to kernel-based SVM classiﬁers that work in the dual
space, the TT classiﬁer is able to work directly in the high
dimensional space by exploiting the TT-format. Similar with
the backpropagation algorithm for multilayer perceptrons, the
structure of a TT allows for updating the cores in an alternating
way. In the next section, we will describe the training of two
TT classiﬁers through the optimization of two different loss
functions.

The last module in Fig. 3 is the decision module that
decides which category a new observation belongs to. For
binary classiﬁcation, decisions are made according to the
sign of the score assigned by the TT classiﬁer, namely, the
decision depends on the value of corresponding discriminant
function. In an m-class problem, there are several strategies to
decompose it into a sequence of binary classiﬁcation problems.
A straightforward extension is the one-against-all, where m
binary classiﬁcation problems are involved. We seek to design
discriminant functions {gi(x)}m
i=1 so that gi(x) > gj(x),
∀j (cid:54)= i if x belongs to the ith class. Classiﬁcation is then
achieved according to the rule:

assign x to the ith class if i = argmaxk gk(x).
An alternative technique is the one-against-one, where we
need to consider m(m − 1)/2 pairs of classes. The decision
is made on the basis of a majority vote. It means that each
classiﬁer casts one vote and the ﬁnal class is the one with

Fig. 3. Framework of TT learning.

the most votes. When the number m is too large, one can
also apply the technique of binary coding. It turns out that
only (cid:100)log2 m(cid:101) classiﬁers are needed, where (cid:100)·(cid:101) is the ceiling
operation. In this case, each class is represented by a unique
binary code word of length (cid:100)log2 m(cid:101). The decision is then
made on the basis of minimal Hamming distance.

V. LEARNING ALGORITHMS
For notational convenience, we deﬁne nk := ˜nk + 1
and continue to use this notation for the remainder of the
article. As stated before, TT classiﬁers are designed for binary
classiﬁcation. Given a set of N training examples of the form
j=1 such that x(j) ∈ Rd is the feature vector of
{(x(j), y(j))}N
the jth example and y(j) ∈ {−1, 1} is the corresponding class
label of x(j). Let ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd be the degree
vector. Each feature is then mapped to a higher dimensional
space generated by all corresponding pure-power-˜n monomials
through the mapping T : Rd → Rn1×n2×···×nd

5

Here we consider that the tensor A is expressed as a tensor
train with cores {Gk}d
k=1. The main idea of the TT learning
algorithms is to update the cores in an alternating way by
optimizing an appropriate loss function. Prior to updating the
TT-cores, the TT-ranks are ﬁxed and a particular initial guess
of {Gk}d
k=1 is made. The TT-ranks can be interpreted as tuning
parameters, higher values will result in a better ﬁt at the risk
of overﬁtting. It is straightforward to extend our algorithms by
means of the Density Matrix Renormalization Group (DMRG)
method [26] such that the TT-ranks are updated adaptively.
Each core is updated in the order

G1 → G2 → · · · → Gd → Gd−1 → · · · → G1 → · · ·

until convergence, which is guaranteed under certain condi-
tions as described in [27], [28]. It turns out that updating
one TT-core is equivalent with minimizing a loss function in
a small number of variables, which can be done in a very
efﬁcient manner. The following lemma shows how the inner
product (cid:104)T (x), A(cid:105) in the generic feature space is a linear
function in any of the TT-cores Gk.
Lemma 1. Given a vector ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd, let
T be the mapping deﬁned by (12), and let A be a TT with
cores Gk ∈ Rrk−1×nk×rk , k = 1, 2, . . . , d. For any x ∈ Rd
and k = 1, . . . , d, we have that

(cid:104)T (x), A(cid:105) = (cid:0)qk(x)(cid:62) ⊗ v(xk)(cid:62) ⊗ pk(x)(cid:1) vec(Gk),

(14)

T (x)i1i2···id =

xik−1
k

.

d
(cid:89)

k=1

(12)

where

For x = (x1, x2, . . . , xd)(cid:62) ∈ Rd, let {v(xk)}d
Vandermonde vectors deﬁned in (6). Clearly, we have

k=1 be the

p1(x) = 1, pk(x)
k≥2

=

k−1
(cid:89)

i=1

(cid:0)Gi ×2 v(xi)(cid:62)(cid:1) ∈ R1×rk−1,

T (x) = v(x1) ◦ v(x2) ◦ · · · ◦ v(xd).

(13)

and

This high-dimensional pure-power polynomial space beneﬁts
the learning task from the following aspects:

• all interactions between features are described by the

qk(x)
k<d

=

d
(cid:89)

i=k+1

monomials of pure-power polynomials;

Proof. By deﬁnition, we have

(cid:0)Gi ×2 v(xi)(cid:62)(cid:1) ∈ Rrk×1, qd(x) = 1.

• the dimension of the tensor space grows exponentially
k=1 nk, which increases the probability
training examples linearly into two-

with d, namely, (cid:81)d
of separating all
classes;

• the one-to-one mapping between pure-power polynomials
and tensors enables the use of tensor trains to lift the curse
of dimensionality.

With these preparations, our goal

is to ﬁnd a decision
hyperplane to separate these two-class examples in the tensor
space, also called the generic feature space. In other words,
like the inductive learning described in [16], we try to ﬁnd a
tensor A ∈ Rn1×n2×···×nd such that

y(j)(cid:104)T (x(j)), A(cid:105) > 0,

j = 1, 2, . . . , N.

Note that the bias is absorbed into the ﬁrst element of A. Note
that the learning problem can also be interpreted as ﬁnding a
pure-power-˜n polynomial g(x) such that

and

g(x(j)) > 0,

∀y(j) = 1,

g(x(j)) < 0,

∀y(j) = −1.

(cid:104)T (x), A(cid:105) = A ×1 v(x1)(cid:62) ×2 · · · ×d v(xd)(cid:62)

= (cid:0)G1 ×2 v(x1)(cid:62)(cid:1) · · · (cid:0)Gd ×2 v(xd)(cid:62)(cid:1)
= Gk ×1 pk(x) ×2 v(xk)(cid:62) ×3 qk(x)(cid:62)
= (cid:0)qk(x)(cid:62) ⊗ v(xk)(cid:62) ⊗ pk(x)(cid:1) vec(Gk)

for any k = 1, 2, . . . , d. This completes the proof.

Example 4. In this example we illustrate the advantageous
representation of a pure-power polynomial f as a TT. Suppose
we have a polynomial f with d = 10 and all degrees ˜ni =
9 (i = 1, . . . , 10). All coefﬁcients of f (x) can then be stored
into a 10-way tensor 10 × 10 × · · · × 10 tensor A such that
the evaluation of f in a particular x is given by (7). The TT-
representation of f consists of 10 TT-cores G1, . . . , G10, with
a storage complexity of O(100r2), where r is the maximal TT-
rank. This demonstrates the potential of the TT-representation
in avoiding the curse of dimensionality when the TT-ranks are
small.

for
Example
T (x), A, v(xk), qk(x), pk(x) for the following quadratic

5. Next, we

expressions

illustrate

the

6







T (x) =

1 +
2. Since d = 2 and ˜n1 = ˜n2 = 2, both T and A

polynomial in two variables f (x) = 1 + 3x1 − x2 − x2
7x1x2 + 9x2
are the following 3 × 3 matrices


x2
1
x2
2
x1 x1x2 x1x2
2
1x2
1 x2
x2
2
The TT-representation of A consists of a 1 × 3 × 3 tensor G1
and a 3×3×1 tensor G2. Suppose now that k = 2 and we want
to compute the evaluation of the polynomial f in a particular
x, which is (cid:104)T (x), A(cid:105). From Lemma 1 we then have that
(cid:104)T (x), A(cid:105) = (cid:0)q2(x)(cid:62) ⊗ v(x2)(cid:62) ⊗ p2(x)(cid:1) vec(G2),

1 −1
7
3
0
−1


9
0
 .
0

1x2 x2

 , A =



We have thus shown that updating the core Gk is equivalent
with solving a least squares optimization problem in rk−1nkrk
variables. Minimizing (17) with respect to Gk for any k =
1, . . . , d results in solving the linear system

(C (cid:62)

k Ck) vec(Gk) = C (cid:62)

k y.

(19)

Supposing r1 = r2 = · · · = rd−1 = r and n1 = n2 = · · · =
nd = n, then the computational complexity of solving (19) is
O((r2n)3). For the maximal values of r = 10 and n = 4 in
our experiments, this implies that we need to solve a linear
system of order 400, which takes about 0.01 seconds using
MATLAB on our desktop computer.

with

q2(x) = 1 ∈ R,
v(x2) = (cid:0)1 x2 x2
p2(x) = G1 ×2 v(x1)(cid:62) ∈ R1×3,
v(x1) = (cid:0)1 x1 x2

∈ R3,

∈ R3.

(cid:1)(cid:62)

(cid:1)(cid:62)

2

1

In what follows, we ﬁrst present two learning algorithms
based on different loss functions. These algorithms will learn
the tensor A directly in the TT-representation from a given
dataset. Two enhancements, namely, regularization for better
accuracy and parallelization for higher speed will be described
in the last two subsections.

A. TT Learning by Least Squares

Least squares estimation is the simplest and thus most
common estimation method. In the generic feature space, we
attempt to design a linear classiﬁer so that its desired output
is exactly 1 or −1. However, we have to live with errors, that
is, the true output will not always be equal to the desired one.
The least squares estimator is then found from minimizing the
following mean square error function

B. TT Learning by Logistic Regression

Since our goal is to ﬁnd a hyperplane to separate two-class
training examples in the generic feature space, we may not
care about the particular value of the output. Indeed, only
the sign of the output makes sense. This gives us the idea to
decrease the number of sign differences as much as possible
when updating the TT-cores, i.e., to minimize the number of
misclassiﬁed examples. However, this model is discrete so
that a difﬁcult combinatorial optimization problem is involved.
Instead, we try to ﬁnd a suboptimal solution in the sense of
minimizing a continuous cost function that penalizes misclas-
siﬁed examples. Here, we consider the logistic regression cost
function. First, consider the standard sigmoid function

σ(z) =

1
1 + e−z ,

z ∈ R,

where the output always takes values between 0 and 1. An
important property is that its derivative can be expressed by
the function itself, i.e.,

J(A) =

(cid:16)

(cid:104)T (x(j)), A(cid:105) − y(j)(cid:17)2

.

(15)

The logistic function for the jth example x(j) is given by

1
N

N
(cid:88)

j=1

σ(cid:48)(z) = σ(z)(1 − σ(z)).

hA(x(j)) := σ

(cid:104)T (x(j)), A(cid:105)

(cid:16)

(cid:17)

.

(20)

(21)

We now show how updating a TT-core Gk is equivalent with
solving a relatively small linear system. First, we deﬁne the
N × rk−1nkrk matrix




Ck =







qk(x(1))(cid:62) ⊗ v(x(1)
qk(x(2))(cid:62) ⊗ v(x(2)

k )(cid:62) ⊗ pk(x(1))
k )(cid:62) ⊗ pk(x(2))
...







qk(x(N ))(cid:62) ⊗ v(x(N )

k

)(cid:62) ⊗ pk(x(N ))

for any k = 1, 2, . . . , d. The matrix Ck is hence obtained
from the concatenation of the row vectors qk(x)(cid:62) ⊗v(xk)(cid:62) ⊗
pk(x) from (14) for N samples x(1), . . . , x(N ). It follows from
Lemma 1 that

J(A) =

(cid:107)Ck vec(Gk) − y(cid:107)2

1
N

We can also interpret the logistic function as the probability
that the example x(j) belongs to the class denoted by the
label 1. The predicted label ˜y(j) for x(j) is then obtained
according to the rule

(16)

(cid:40)

hA(x(j)) ≥ 0.5 ⇔ (cid:104)T (x(j)), A(cid:105) ≥ 0 → ˜y(j) = 1,
hA(x(j)) < 0.5 ⇔ (cid:104)T (x(j)), A(cid:105) < 0 → ˜y(j) = −1.

For a particular example x(j), we deﬁne the cost function as

Cost(x(j), A) =




− ln

(cid:16)

(cid:17)
hA(x(j))



− ln

(cid:16)

(cid:17)
1 − hA(x(j))

if y(j) = 1,

if y(j) = −1.

where

y = (y(1), y(2), . . . , y(N ))(cid:62) ∈ RN .

The goal now is to ﬁnd a tensor A such that hA(x(j)) is near
1 if y(j) = 1 or near 0 if y(j) = −1. As a result, the logistic

(17)

(18)

regression cost function for the whole training dataset is given
by

J(A) =

Cost(x(j), A)

1
N

N
(cid:88)

j=1

=

−1
N

N
(cid:88)

j=1

(cid:20) 1 + y(j)
2

(cid:16)

ln

hA(x(j))

+

(cid:17)

(22)

1 − y(j)
2

(cid:16)

ln

1 − hA(x(j))

(cid:17)(cid:21)

.

It is important to note that (22) is convex though the sigmoid
function is not. This guarantees that we can ﬁnd the globally
optimal solution instead of a local optimum.

From equation (21) and Lemma 1, one can see that the
function J(A) can also be regarded as a function of the core
Gk since

and

(cid:104)T (x(j)), A(cid:105) = Ck(j, :) vec(Gk)

where Ck(j,
:) denote the jth row vector of Ck deﬁned in
(16). It follows that updating the core Gk is equivalent with
solving a convex optimization problem in rk−1nkrk variables.
Let

hA =

(cid:16)
hA(x(1)), hA(x(2)), . . . , hA(x(N ))

(cid:17)(cid:62)

∈ RN (23)

and DA be the diagonal matrix in RN ×N with the jth
diagonal element given by hA(x(j)) (cid:0)1 − hA(x(j))(cid:1). By using
the property (20) one can derive the gradient and Hessian with
respect to Gk as

∇Gk J(A) =

C (cid:62)
k

hA −

(cid:18)

1
N

(cid:19)

y + 1
2

and

∇2
Gk

J(A) =

C (cid:62)

k DACk,

1
N

respectively, where y is deﬁned in (18) and 1 denotes the
all-ones vector in RN . Although we do not have a closed-
form solution to update the core Gk, the gradient and Hessian
allows us to ﬁnd the solution by efﬁcient iterative methods, e.g.
Newton’s method whose convergence is at least quadratic in a
neighbourhood of the solution. A quasi-Newton method, like
the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm, is
another good choice if the inverse of the Hessian is difﬁcult
to compute.

(24)

(25)

C. Regularization

The cost functions (15) and (22) of the two TT learning
algorithms do not have any regularization term, which may
result in overﬁtting and hence bad generalization properties of
the obtained TT classiﬁer. Next, we discuss how the addition
of a regularization term to (15) and (22) results in a small
modiﬁcation of the small optimization problem that needs to
be solved when updating the TT-cores Gk.

Consider the regularized optimization problem

˜J(A) = J(A) + γR(A),

(26)

7

where J(A) is given by (15) or (22), γ is a parameter that
balances the loss function and the regularization term. Here
we use the Tikhonov regularization, namely,

(cid:104)A, A(cid:105).

R(A) =

1
2
Thanks to the TT structure, the gradient of R(A) with respect
to the TT-core Gk can be equivalently rewritten as a linear
transformation of vec(Gk). In other words, there is a matrix
Dk ∈ Rrk−1nkrk×rk−1nkrk determined by the cores {Gj}j(cid:54)=k
such that ∇Gk R(A) = Dkvec(Gk). See Appendix A for more
details. It follows that

(27)

∇Gk

˜J(A) = ∇Gk J(A) + γDkvec(Gk)

∇2
Gk

˜J(A) = ∇2
Gk

J(A) + γDk.

These small modiﬁcations lead to small changes when updat-
ing the core Gk. For instance, the ﬁrst-order condition of (26)
for the least squares model results in solving the modiﬁed
linear system
(cid:18)

(cid:19)

C (cid:62)

k Ck +

γDk

vec(Gk) = C (cid:62)

k y,

(28)

N
2

when compared with the original linear system (19).

D. Orthogonalization and Parallelization

The matrix Ck from (16) needs to be reconstructed for each
TT-core Gk during the execution of the two TT learning algo-
rithms. Fortunately, this can be done efﬁciently by exploiting
the TT structure. In particular, after updating the core Gk in the
left-to-right sweep, the new row vectors {pk+1(x(j))}N
j=1 to
construct the next matrix Ck+1 can be easily computed from
pk+1(x(j)) = Gk ×1 pk(x(j)) ×2 v(x(j)

k )(cid:62).

Similarly, in the right-to-left sweep, the new column vectors
{qk−1(x(j))}N
j=1 to construct the next matrix Ck−1 can be
easily computed from

qk−1(x(j)) = Gk ×2 v(x(j)

k )(cid:62) ×3 qk(x(j))(cid:62).

To make the learning algorithms numerically stable, the
techniques of orthogonalization are also applied. The main
idea is to make sure that before updating the core Gk, the cores
G1, . . . , Gk−1 are left-orthogonal and the cores Gk+1, . . . , Gd
are right-orthogonal by a sequence of QR decompositions. In
this way, the condition number of the constructed matrix Ck
is upper bounded so that the subproblem is well-posed. After
updating the core Gk, we orthogonalize it with an extra QR
decomposition, and absorb the upper triangular matrix into
the next core (depending on the direction of updating). More
details on the orthogonalization step can be found in [27].

Another computational challenge is the potentially large
size N of the training dataset. Luckily, the dimension of the
optimization problem when updating Gk in the TT learning
algorithms is rk−1(nk + 1)rk, which is much smaller and
independent from N . We only need to compute the products
C (cid:62)
k DACk in (19), (24) and (25).
These computations are easily done in parallel. Speciﬁcally,

k hA and C (cid:62)

k Ck, C (cid:62)

k y, C (cid:62)

8

given a proper partition {Nl}L
we divide the large matrix Ck into several blocks, namely,

l=1 satisfying (cid:80)L

l=1 Nl = N ,

TABLE II
DATASET DESCRIPTION

Ck =

∈ RN ×rk−1(nk+1)rk ,

















C (1)
k
C (2)
k
...
C (L)
k

where C (l)
example, the product C (cid:62)

k ∈ RNl×rk−1nkrk , l = 1, 2, . . . , L. Then, for

k DACk can be computed by

C (cid:62)

k DACk =

(C (l)

k )(cid:62)D(l)

A C (l)
k ,

L
(cid:88)

l=1

where D(l)
A denotes the corresponding diagonal block. Each
term in the summation on the right-hand side of the above
equation can be computed over L distributed cores, with
a computational complexity of O(r4n2
kNl) for each core,
supposing rk−1 = rk = r. The other matrix products can
also be computed in a similar way.

We summarize our learning algorithms in Algorithm 2.
The two most computationally expensive steps are lines 5
and 7. As we mentioned before, solving (26) takes approx-
imately O((r2n)3) ﬂops. If the QR decomposition of line 7
is computed through Householder transformations, then the
computational complexity is approximately O(r3n2) ﬂops. For
the maximal values of n = 4 and r = 10 in our experiments,
this amounts to solving computing the inverse and the QR
factorization of a 400 × 400 matrix. Note that based on the
decision strategy, an m-class problem is decomposed into a
sequence of two-class problems whose TT classiﬁers can be
trained in parallel.

Algorithm 2 Tensor Train Learning Algorithm
Input: Training dataset of pairs {(x(j), y(j))}N

j=1, TT-ranks
k=1, degree vector ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd and

{rk}d−1
regularization parameter γ

Output: Tensor A in TT format with cores {Gk}d
1: Initialize right orthogonal cores {Gk}d

k=1
k=1 of prescribed

ranks

2: while termination condition is not satisﬁed do
3: %Left-to-right sweep
4:
5:

for k = 1, 2, . . . , d − 1 do

G∗
k ← ﬁnd the minimal solution of the regularized
optimization problem (26) with respect to Gk
Uk ← reshape(G∗
[Q, R] ← compute QR decomposition of Uk
Gk ← reshape(Q, rk−1, nk, rk)
Vk+1 ← R ∗ reshape(Gk+1, rk, nk+1, rk+1)
Gk+1 ← reshape(Vk+1, rk, nk+1, rk+1)

k, rk−1nk, rk)

6:
7:
8:
9:
10:

end for
Perform the right-to-left sweep

11:
12:
13: end while

Image size

Training size

Test size

USPS
MNIST

16 × 16
28 × 28

7291
60000

2007
10000

• The DMRG method [26] can also be used to update
the cores. This involves updating two cores at a time
so that the TT-ranks are adaptively determined by means
of a singular value decomposition (SVD). This may give
better performance at the cost of a higher computational
complexity. It also removes the need to ﬁx the TT-ranks
a priori.

• The local linear convergence of Algorithm 2 has been
established in [27], [28] under certain conditions. In
particular,
if the TT-ranks are correctly estimated for
convex optimization problems, then the obtained solution
is guaranteed to be the global optimum. When choosing
the TT-ranks, one should keep the upper bounds of the
TT-ranks from Proposition 1 in mind.

VI. EXPERIMENTS

In this section, we test our TT learning algorithms and
compare their performance with LS-SVMs with polynomial
kernels on two popular digit recognition datasets: USPS
and MNIST. All our algorithms were implemented in MAT-
LAB Version R2016a, which can be freely downloaded from
https://github.com/kbatseli/TTClassiﬁer. We compare our TT-
polynomial classiﬁers with a polynomial classiﬁer based on
LS-SVMs with a polynomial kernel. The LS-SVM-polynomial
classiﬁer was trained with the MATLAB LS-SVMlab tool-
box, which can be freely downloaded from http://www.esat.
kuleuven.be/sista/lssvmlab/. The numerical experiments were
done on a desktop PC with an Intel i5 quad-core processor
running at 3.3GHz and 16GB of RAM.

The US Postal Service (USPS) database1contains 9298
handwritten digits, including 7291 for training and 2007 for
testing. Each digit is a 16×16 grayscale image. It is known that
the USPS test set is rather difﬁcult and the human error rate is
2.5%. The Modiﬁed NIST (MNIST) database2 of handwritten
digits has a training set of 60,000 examples, and a test set of
10,000 examples. It is a subset of a larger set available from
NIST. The digits have been size-normalized and centered in
a 28×28 image. The description of these two databases is
summarized in Table II.

Before extracting features of the handwritten digits, we ﬁrst
execute the pre-process of deskewing which is the process
of straightening an image that has been scanned or written
crookedly. By choosing a varying number d, the corresponding
feature vectors are then extracted from a pre-trained CNN
model 1-20-P-100-P-d-10, which represents a net with an input
images of size 28×28, a convolutional layer with 20 maps and

We end this section with the following remarks:
• Other loss functions can also be used in the framework
of TT learning provided that there exists an efﬁcient way
to solve the corresponding subproblems.

1The USPS database is downloaded from

http://statweb.stanford.edu/∼tibs/ElemStatLearn/data.html

2The MNIST database is downloaded from

http://yann.lecun.com/exdb/mnist/

5×5 ﬁlters, a max-pooling layer over non-overlapping regions
of size 2×2, a convolutional layer with 100 maps and 5×5
ﬁlters, a max-pooling layer over non-overlapping regions of
size 2×2, a convolutional layer with d maps and 4×4 ﬁlters,
and a fully connected output layer with 10 neurons. As the
variants of the well-known CNN model LeNet-5 [9], these
CNN models have been trained well on the MNIST database.
For an input 28×28 image, we get a feature vector of length
d from the third hidden layer. Note that for USPS database,
we must resize image data to the size of image input layer.
We mention that these techniques of deskewing and feature
extraction using pre-trained CNN model are widely used in
the literature [9], [10], [29].

For the decision module, we adopt the one-against-all deci-
sion strategy where ten TT classiﬁers are trained to separate
each digit from all
the others. In the implementation of
Algorithm 2, we normalize each initial core such that its
Frobenius norm is equal to one. The degree vector is given
by ˜n1 = · · · = ˜nd = ˜n. The TT-ranks are upper bounded
by rmax. The values of d, ˜n, rmax were chosen to minimize
the test error rate and to ensure that each of the subproblems
to update the TT-cores Gk could be solved in a reasonable
time. The dimension of each subproblem is at most n r2
max.
For example, in the USPS case, we ﬁrst ﬁxed the values of
n and rmax. We then ﬁxed the value of d and incremented
n and rmax to see whether this resulted in a better test error
rate. We use the optimality criterion

| ˜J(A+) − ˜J(A)|
| ˜J(A)|

≤ 10−2,

where A+ is the updated tensor from tensor A after one sweep.
And the maximum number of sweeps is 4, namely, 4(d−1) it-
erations through the entire training data are performed for each
session. To simplify notations, we use “TTLS” and “TTLR” to
denote the TT learning algorithms based on minimizing loss
functions (15) and (22), respectively. For these two models, the
regularization parameter γ is determined by the technique of
10-fold cross-validation. In other words, we randomly assign
the training data to ten sets of equal size. The parameter γ is
chosen so that the mean over all test errors is minimal.

The numerical results for USPS database and MNIST
database are reported in Tables III and IV, respectively. The
monotonic decrease is always seen when training the ten TT
classiﬁers. Fig. 4 shows the convergence of both TT learning
algorithms on the USPS data for the case d = 20, ˜n =
1, rmax = 8 when training the classiﬁer for the character “6”.
In addition, we also trained a polynomial classiﬁer using LS-
SVMs with polynomial kernels on these two databases. Using
the basic LS-SVM scheme, a training error rate of 0 and a test
error rate of 8.37% were obtained for the USPS dataset after
more than three and a half hours of computation. This runtime
includes the time required to tune the tuning parameters via
10-fold cross-validation. When using an RBF kernel with the
LS-SVM, it is possible to attain a test error of 2.14% [30],
but then the classiﬁer is not polynomial anymore. The MNIST
dataset resulted in consistent out-of-memory errors, which is
to be expected as the basic SVM scheme is not intended
for large data sets. We would also like to point out that a

9

Fig. 4. The convergence of TT learning algorithms.

test error of 1.1% is reported on the MNIST website for a
polynomial classiﬁer of degree 4 that uses conventional SVMs
and deskewing.

VII. CONCLUSION

This paper presents the framework of TT learning for pattern
classiﬁcation. For the ﬁrst time, TTs are used to represent
polynomial classiﬁers, enabling the learning algorithms to
work directly in the high-dimensional feature space. Two
efﬁcient learning algorithms are proposed based on different
loss functions. The numerical experiments show that each TT
classiﬁer is trained in up to several minutes with competitive
test errors. When compared with other polynomial classiﬁers,
the proposed learning algorithms can be easily parallelized
and have considerable advantage on storage and computation
time. We also mention that these results can be improved by
adding virtual examples [10]. Future improvements are the
implementation of on-line learning algorithms, together with
the extension of the binary TT classiﬁer to the multi-class case.

APPENDIX A
Given the degree vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd, let A ∈
Rn1×n2×···×nd be the tensor in TT format with cores Gk ∈
Rrk−1×nk×rk , k = 1, 2, . . . , d. To investigate the gradient of
R(A) in (27) with respect to the TT-core Gk, we give a small
variation (cid:15) to the ith element of vec(Gk), resulting in a new
tensor A(cid:15) given by

A(cid:15) = A + (cid:15)I (k)

,

i
where 1 ≤ i ≤ rk−1nkrk and I (k)
is the tensor which has
the same TT-cores with A except that the vectorization of the
core Gk is replaced by the unit vector in Rrk−1nkrk with the
ith element equal to 1 and 0 otherwise. Then we have

i

[∇Gk R(A)]i = lim
(cid:15)→0

R(A(cid:15)) − R(A)
(cid:15)
On the other hand, by the deﬁnition of vectorization, the ith
element of vec(Gk) ∈ Rrk−1nkrk is mapped from the tensor
element of Gk ∈ Rrk−1×nk×rk with indices (αk−1, jk, αk)
satisfying

= (cid:104)A, I (k)

(29)

(cid:105).

i

i = αk−1 + (jk − 1)rk−1 + (αk − 1)rk−1nk,

where 1 ≤ αk−1 ≤ rk−1, 1 ≤ jk ≤ nk and 1 ≤ αk ≤ rk.
Denote by E(αk−1,αk) the matrix in Rrk−1×rk such that the

TABLE III
NUMERICAL RESULTS FOR DATASET USPS

Train error

TTLS
Test error

Train error

TTLR
Test error

10

d

20
25
30
35
40
20
25
30
35
40
20
25
30
20
25
30
35
40

d

20
25
30
35
40
20
25
30
35
40
20
30
40
20
30
40
40
40
40

˜n

1
1
1
1
1
2
2
2
2
2
3
3
3
1
1
1
1
1

˜n

1
1
1
1
1
2
2
2
2
2
3
3
3
1
1
1
2
3
4

rmax
8
8
8
8
8
8
8
8
8
8
8
8
8
10
10
10
10
10

rmax
8
8
8
8
8
8
8
8
8
8
8
8
8
10
10
10
10
10
10

0.58%
0.47%
0.49%
0.38%
0.34%
0.49%
0.38%
0.51%
0.45%
0.34%
0.59%
0.49%
0.63%
0.56%
0.41%
0.51%
0.34%
0.44%

0.16%
0.15%
0.13%
0.19%
0.21%
0.19%
0.13%
0.19%
0.17%
0.17%
0.20%
0.22%
0.21%
0.20%
0.16%
0.17%
0.13%
0.18%
0.24%

Time(s)
4.33% 1.10×10
4.04% 1.57×10
3.84% 1.83×10
4.04% 2.21×10
3.89% 2.59×10
4.29% 2.04×10
4.29% 2.68×10
4.14% 3.28×10
4.38% 3.87×10
4.09% 4.48×10
4.58% 3.26×10
4.53% 4.07×10
4.43% 4.93×10
4.33% 1.91×10
3.94% 2.48×10
3.84% 3.09×10
3.94% 3.71×10
3.84% 4.36×10

Time(s)
7.14×10
0.96%
9.38×10
0.97%
0.86% 11.73×10
0.81% 13.95×10
0.94% 16.22×10
1.03% 12.08×10
1.00% 15.72×10
0.88% 18.94×10
0.86% 23.11×10
0.90% 25.67×10
1.10% 17.45×10
1.02% 27.21×10
0.97% 36.48×10
0.99% 10.14×10
0.88% 17.15×10
0.88% 24.05×10
0.93% 39.61×10
0.94% 59.66×10
1.02% 78.83×10

0.45%
0.27%
0.30%
0.26%
0.14%
0.55%
0.29%
0.47%
0.32%
0.21%
0.64%
0.56%
0.51%
0.44%
0.27%
0.25%
0.22%
0.33%

0.12%
0.09%
0.10%
0.03%
0.07%
0.14%
0.13%
0.10%
0.13%
0.17%
0.19%
0.21%
0.07%
0.19%
0.17%
0.13%
0.14%
0.10%
0.14%

Time(s)
4.87×10
4.33%
6.49×10
3.99%
8.18×10
3.99%
9.77×10
4.24%
3.74% 11.39×10
8.29×10
4.24%
4.14% 10.82×10
3.99% 13.34×10
4.38% 15.88×10
3.74% 18.39×10
4.33% 11.47×10
4.14% 14.98×10
4.09% 18.35×10
4.33% 17.69×10
3.94% 24.72×10
4.04% 31.14×10
4.29% 38.17×10
3.84% 45.24×10

Time(s)
0.99% 17.52×10
0.98% 22.96×10
0.86% 28.96×10
0.82% 34.55×10
0.83% 40.31×10
1.01% 29.45×10
0.98% 38.32×10
0.91% 46.77×10
0.83% 56.54×10
0.88% 64.68×10
1.00% 42.24×10
0.90% 65.89×10
0.89% 90.37×10
0.98% 36.66×10
0.90% 62.24×10
0.89% 83.62×10
0.92% 144.1×10
0.90% 207.7×10
0.91% 264.2×10

TABLE IV
NUMERICAL RESULTS FOR DATASET MNIST

Train error

TTLS
Test error

Train error

TTLR
Test error

element with index (αk−1, αk) equal to 1 and 0 otherwise. By
simple computation, one can obtain that

and

(cid:104)A, I (k)

(cid:105) =

i

(cid:88)

Ai1i2···id (I k

i )i1i2···id

i1,i2,...id
(cid:16)

= ak

(cid:17)
E(αk−1,αk) ⊗ Gk(jk)

bk,

where

ak =

k−1
(cid:89)

nl(cid:88)

l=1

il=1

(cid:2)Gl(il) ⊗ Gl(il)(cid:3) ∈ R1×r2

k−1

(31)

bk =

d
(cid:89)

nl(cid:88)

l=k+1

il=1

(cid:2)Gl(il) ⊗ Gl(il)(cid:3) ∈ Rr2

k×1.

(32)

(30)

Let a(1)
that

k , a(2)

k , . . . , a(rk−1)

k

∈ R1×rk−1 be the row vectors such

ak = (a(1)

k , a(2)

k , . . . , a(rk−1)

k

) ∈ R1×r2

k−1,

and let b(1)

k , b(2)

k , . . . , b(rk)

k

∈ Rrk×1 be the column vectors

such that

bk =

∈ Rr2

k×1,

















b(1)
k
b(2)
k
...
b(rk)
k

Combining (29) and (30) together, we have
[∇Gk R(A)]i = a(αk−1)
k
(cid:16)
(b(αk)
k

Gk(jk)b(αk)
)(cid:62) ⊗ (e(jk))(cid:62) ⊗ a(αk−1)

=

k

k

(cid:17)

vec(Gk),

where e(j) ∈ Rnk denotes the unit vector with the jth element
equal to 1 and 0 otherwise. If we deﬁne the rk−1nkrk ×
rk−1nkrk matrix




Dk =







(b(1)
(b(1)

k )(cid:62) ⊗ (e(1))(cid:62) ⊗ a(1)
k )(cid:62) ⊗ (e(1))(cid:62) ⊗ a(2)
...

k

k

(b(rk)
k

)(cid:62) ⊗ (enk )(cid:62) ⊗ a(rk−1)

k

,







(33)

it follows immediately that ∇Gk R(A) = Dkvec(Gk).

ACKNOWLEDGMENT
Zhongming Chen acknowledges the support of the Na-
tional Natural Science Foundation of China (Grant No.
11701132). Johan Suykens acknowledges support of ERC
AdG A-DATADRIVE-B (290923), KUL: CoE PFV/10/002
(OPTEC); FWO: G.0377.12, G.088114N, G0A4917N; IUAP
P7/19 DYSCO. Ngai Wong acknowledges the support of
the Hong Kong Research Grants Council under the General
Research Fund (GRF) project 17246416.

REFERENCES

[1] H.-B. Shen and K.-C. Chou, “Ensemble classiﬁer for protein fold pattern
recognition,” Bioinformatics, vol. 22, no. 14, pp. 1717–1722, 2006.
[2] H.-D. Cheng, X. Cai, X. Chen, L. Hu, and X. Lou, “Computer-aided
detection and classiﬁcation of microcalciﬁcations in mammograms: a
survey,” Pattern recognition, vol. 36, no. 12, pp. 2967–2991, 2003.
[3] B. Roberto, Template matching techniques in computer vision: theory

and practice. Wiley, Hoboken, 2009.

[4] B.-H. Juang, W. Hou, and C.-H. Lee, “Minimum classiﬁcation error
rate methods for speech recognition,” IEEE Transactions on Speech and
Audio processing, vol. 5, no. 3, pp. 257–265, 1997.

[5] L. Xu, A. Krzyzak, and C. Y. Suen, “Methods of combining multiple
classiﬁers and their applications to handwriting recognition,” IEEE
transactions on systems, man, and cybernetics, vol. 22, no. 3, pp. 418–
435, 1992.

[6] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern classiﬁcation.

John

Wiley & Sons, 2012.

[7] Y. Zhang, S. Wang, G. Ji, and P. Phillips, “Fruit classiﬁcation using
computer vision and feedforward neural network,” Journal of Food
Engineering, vol. 143, pp. 167–177, 2014.

[8] S. Wang, X. Yang, Y. Zhang, P. Phillips, J. Yang, and T.-F. Yuan,
“Identiﬁcation of Green, Oolong and Black Teas in China via Wavelet
Packet Entropy and Fuzzy Support Vector Machine,” Entropy, vol. 17,
no. 10, pp. 6663–6682, 2015.

11

[9] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.

[10] D. Decoste and B. Sch¨olkopf, “Training invariant support vector ma-
chines,” Machine learning, vol. 46, no. 1-3, pp. 161–190, 2002.
[11] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation applied to handwritten
zip code recognition,” Neural computation, vol. 1, no. 4, pp. 541–551,
1989.

[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.

[13] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning,

vol. 20, no. 3, pp. 273–297, 1995.

[14] Y.-W. Chang, C.-J. Hsieh, K.-W. Chang, M. Ringgaard, and C.-J. Lin,
“Training and testing low-degree polynomial data mappings via linear
SVM,” Journal of Machine Learning Research, vol. 11, no. Apr, pp.
1471–1490, 2010.

[15] J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor,
and J. Vandewalle, Least Squares Support Vector Machines. World
Scientiﬁc, Singapore, 2002.

[16] M. Signoretto, Q. T. Dinh, L. De Lathauwer, and J. A. K. Suykens,
“Learning with tensors: a framework based on convex optimization and
spectral regularization,” Machine Learning, vol. 94, no. 3, pp. 303–351,
2014.

[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempit-
sky, “Speeding-up convolutional neural networks using ﬁne-tuned cp-
decomposition,” arXiv preprint arXiv:1412.6553, 2014.

[18] A. Novikov, D. Podoprikhin, A. Osokin, and D. P. Vetrov, “Tensoriz-
ing neural networks,” in Advances in Neural Information Processing
Systems, 2015, pp. 442–450.

[19] A. Novikov, M. Troﬁmov, and I. Oseledets, “Exponential machines,”

arXiv preprint arXiv:1605.03795, 2016.

[20] E. M. Stoudenmire and D. J. Schwab, “Supervised learning with
quantum-inspired tensor networks,” arXiv preprint arXiv:1605.05775,
2016.

[21] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,”

SIAM review, vol. 51, no. 3, pp. 455–500, 2009.

[22] I. Oseledets, “Tensor-train decomposition,” SIAM Journal on Scientiﬁc

Computing, vol. 33, no. 5, pp. 2295–2317, 2011.

[23] I. Oseledets and E. Tyrtyshnikov, “TT-cross approximation for multidi-
mensional arrays,” Linear Algebra and its Applications, vol. 432, no. 1,
pp. 70–88, 2010.

[24] D. Savostyanov and I. Oseledets, “Fast adaptive interpolation of multi-
dimensional arrays in tensor train format,” in 2011 7th International
Workshop on Multidimensional (nD) Systems (nDs).
IEEE, 2011, pp.
1–8.

[25] S. Theodoridis and K. Koutroumbas, Pattern Recognition, Fourth Edi-

tion, 4th ed. Academic Press, 2008.

[26] S. R. White, “Density matrix formulation for quantum renormalization
groups,” Physical Review Letters, vol. 69, no. 19, p. 2863, 1992.
[27] S. Holtz, T. Rohwedder, and R. Schneider, “The alternating linear
scheme for tensor optimization in the tensor train format,” SIAM Journal
on Scientiﬁc Computing, vol. 34, no. 2, pp. A683–A713, 2012.
[28] T. Rohwedder and A. Uschmajew, “On local convergence of alternating
schemes for optimization of convex problems in the tensor train format,”
SIAM Journal on Numerical Analysis, vol. 51, no. 2, pp. 1134–1162,
2013.

[29] F. Lauer, C. Y. Suen, and G. Bloch, “A trainable feature extractor for
handwritten digit recognition,” Pattern Recognition, vol. 40, no. 6, pp.
1816–1824, 2007.

[30] J. A. K. Suykens, “Deep restricted kernel machines using conjugate
feature duality,” Neural Computation, vol. 29, no. 8, pp. 2123–2163,
2017.

Parallelized Tensor Train Learning of Polynomial
Classiﬁers
Zhongming Chen∗, Kim Batselier†, Johan A.K. Suykens‡ and Ngai Wong†

1

7
1
0
2
 
v
o
N
 
6
 
 
]

G
L
.
s
c
[
 
 
4
v
5
0
5
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract—In pattern classiﬁcation, polynomial classiﬁers are
well-studied methods as they are capable of generating complex
decision surfaces. Unfortunately, the use of multivariate polyno-
mials is limited to kernels as in support vector machines, because
polynomials quickly become impractical for high-dimensional
problems. In this paper, we effectively overcome the curse of
dimensionality by employing the tensor train format to represent
a polynomial classiﬁer. Based on the structure of tensor trains,
two learning algorithms are proposed which involve solving
different optimization problems of low computational complex-
ity. Furthermore, we show how both regularization to prevent
overﬁtting and parallelization, which enables the use of large
training sets, are incorporated into these methods. The efﬁciency
and efﬁcacy of our tensor-based polynomial classiﬁer are then
demonstrated on the two popular datasets USPS and MNIST.

Index Terms—Supervised learning, tensor train, pattern clas-

siﬁcation, polynomial classiﬁer.

I. INTRODUCTION

Pattern classiﬁcation is the machine learning task of iden-
tifying to which category a new observation belongs, on the
basis of a training set of observations whose category mem-
bership is known. This type of machine learning algorithm
that uses a known training dataset to make predictions is
called supervised learning, which has been extensively studied
and has wide applications in the ﬁelds of bioinformatics
[1], computer-aided diagnosis (CAD) [2], machine vision [3],
speech recognition [4], handwriting recognition [5], spam
detection and many others [6], [7], [8]. Usually, different kinds
of learning methods use different models to generalize from
training examples to novel test examples.

As pointed out in [9], [10], one of the important invariants
in these applications is the local structure: variables that are
spatially or temporally nearby are highly correlated. Local
correlations beneﬁt extracting local features because conﬁgu-
rations of neighboring variables can be classiﬁed into a small
number of categories (e.g. edges, corners...). For instance, in
handwritten character recognition, correlations between image
pixels that are nearby tend to be more reliable than the
ones of distant pixels. Learning methods incorporating this
kind of prior knowledge often demonstrate state-of-the-art
performance in practical applications. One popular method
for handwritten character recognition is using convolutional
neural networks (CNNs) [11], [12] which are variations of

∗Department of Mathematics, School of Science, Hangzhou Dianzi Uni-

versity, Hangzhou 310018, China. Email: czm183015@126.com.

†Department of Electrical and Electronic Engineering, The University of

Hong Kong. Email: {kimb, nwong}@eee.hku.hk.

‡KU Leuven, ESAT, STADIUS. B-3001 Leuven, Belgium. Email:

jo-

han.suykens@esat.kuleuven.be.

multilayer perceptrons designed to use minimal amounts of
preprocessing. In this model, each unit in a layer receives
inputs from a set of units located in a small neighborhood
in the previous layer, and these mappings share the same
weight vector and bias in a given convolutional layer. An
important component of a CNN are the pooling layers, which
implement a nonlinear form of down-sampling. In this way,
the amount of parameters and computational load are reduced
in the network. Another popular method uses support vector
machines (SVMs) [13], [14]. The original ﬁnite-dimensional
feature space is mapped into a much higher-dimensional
space, where the inner product is easily computed through the
‘kernel trick’. By considering the Wolfe dual representation,
one can ﬁnd the maximum-margin hyperplane to separate the
examples of different categories in that space. However, it is
worth mentioning that these models require a large amount of
memory and a long processing time to train the parameters.
For instance, if there are thousands of nodes in the CNN,
the weight matrices of fully-connected layers are of the order
of millions. The major limitation of basic SVMs is the high
computational complexity which is at least quadratic with the
dataset size. One way to deal with large datasets in the SVM-
framework is by using a ﬁxed-size least squares SVM (ﬁxed-
size LS-SVM) [15], which approximates the kernel mapping
in such a way that the problem can be solved in the primal
space.

II. TENSORS IN MACHINE LEARNING

Tensors are a multidimensional generalization of matrices
to higher orders and have recently gained attention in the
ﬁeld of machine learning. The classiﬁcation via tensors, as
opposed to matrices or vectors, was ﬁrst considered in [16], by
extending the concept of spectral regularization for matrices to
tensors. The tensor data is assumed to satisfy a particular low-
rank Tucker decomposition, which unfortunately still suffers
from an exponential storage complexity. Other work has
focused speeding-up the convolution operation in CNNs [17]
by approximating this operation with a low-rank polyadic
decomposition of a tensor. In [18], the weight matrices of
fully-connected layers of neural networks are represented
by tensor trains (TTs), effectively reducing the number of
parameters. TTs have also been used to represent nonlinear
predictors [19] and classiﬁers [20]. The key idea here is always
to approximate a mapping that is determined by an exponential
number of parameters nd by a TT with a storage complexity
of dnr2 parameters. To our knowledge, this idea has not yet
been applied to polynomial classiﬁers that also suffer from the

curse of dimensionality. The usual approach to circumvent the
exponential number of polynomial coefﬁcients would be to
use SVMs with a polynomial kernel and solve the problem
in the dual space. In this article, we exploit
the efﬁcient
representation of a multivariate polynomial as a TT in order to
avoid the curse of dimensionality, allowing us to work directly
in the feature space. The main contributions are:

• We derive a compact description of a polynomial clas-
siﬁer using the TT format, avoiding the curse of dimen-
sionality.

• Two efﬁcient learning algorithms are proposed by exploit-

ing the TT structure.

• Both regularization and a parallel implementation are
incorporated into our methods, thus avoiding overﬁtting
and allowing the use of large training datasets.

This paper is organized as follows. In Section III, we give
a brief introduction to tensor basics, including the TT de-
composition, important tensor operations and properties. The
framework of TT learning for pattern classiﬁcation is presented
in Section IV. Based on different loss functions, two efﬁcient
learning algorithms are proposed in Section V, together with a
discussion on regularization and parallelization. In Section VI,
we test our algorithms on two popular datasets: USPS and
MNIST and compare their performance with polynomial clas-
siﬁers trained with LS-SVMs [15]. Finally, some conclusions
and further work are summarized in Section VII.

Throughout this paper, we use small letters x, y, . . . , for
scalars, small bold letters x, y, . . . , for vectors, capital letters
A, B, . . . , for matrices, and calligraphic letters A, B, . . . , for
tensors. The transpose of a matrix A or vector x is denoted by
A(cid:62) and x(cid:62), respectively. The identity matrix of dimension n is
denoted by In. A list of abbreviations used here is summarized
in Table I.

TABLE I
LIST OF ABBREVIATIONS

Tensor Train
Convolutional Neural Network

TT
CNN
SVM Support Vector Machine
TTLS
TTLR
USPS

Tensor Train learning by Least Squares
Tensor Train learning by Logistic Regression
US Postal Service database

MNIST Modiﬁed NIST database

III. PRELIMINARIES

A. Tensors and pure-power-n polynomials

A real dth-order or d-way tensor is a multidimensional array
A ∈ Rn1×n2×···×nd that generalizes the notions of vectors
and matrices to higher orders. Each of the entries Ai1i2···id
is determined by d indices. The numbers n1, n2, . . . , nd are
called the dimensions of the tensor. An example tensor with
dimensions 4, 3, 2 is shown in Fig. 1. We now give a brief
introduction to some required tensor operations and properties,
more information can be found in [21].

2

Fig. 1. An example tensor A = (Ai1i2i3 ) ∈ R4×3×2, where i1, i2, i3
denote the indices for each mode respectively.

The k-mode product B = A ×k U of a tensor A ∈

Rn1×n2×···×nd and a matrix U ∈ Rn(cid:48)

k×nk is deﬁned by

Bi1···ik−1jik+1···id =

Ai1···ik−1ikik+1···id Ujik ,

(1)

nk(cid:88)

ik=1

and B ∈ Rn1×···×nk−1×n(cid:48)
k×nk+1×···×nd . In particular, given
a d-way tensor A ∈ Rn×n×···×n and a vector x ∈ Rn, the
multidimensional contraction, denoted by Axd, is the scalar

Axd = A ×1 x(cid:62) ×2 x(cid:62) ×3 · · · ×d x(cid:62),

(2)

which is obtained as a homogeneous polynomial of x ∈ Rn
with degree d. The inner product of two same-sized tensors
A, B ∈ Rn1×n2×···×nd is the sum of the products of their
entries, i.e.,

(cid:104)A, B(cid:105) =

· · ·

Ai1i2···id Bi1i2···id .

(3)

n1(cid:88)

n2(cid:88)

nd(cid:88)

i1=1

i2=1

id=1

The Frobenius norm of a tensor A ∈ Rn1×n2×···×nd is given
by

(cid:107)A(cid:107)F = (cid:112)(cid:104)A, A(cid:105).
The vectorization of a tensor A ∈ Rn1×n2×···×nd is de-
noted by vec(A) and maps the tensor element with indices
(i1, i2, . . . , id) to the vector element with index i where

(4)

i = i1 + (i2 − 1)n1 + · · · + (id − 1)

nk.

d−1
(cid:89)

k=1

Given d vectors x(i) ∈ Rni, i = 1, 2, . . . , d, their outer product
is denoted by x(1) ◦ x(2) ◦ · · · ◦ x(d), which is a tensor in
Rn1×n2×···×nd such that its entry with indices (i1, i2, . . . , id)
is equal to the product of the corresponding vector elements,
namely, x(1)
i1

. It follows immediately that

· · · x(d)
id

x(2)
i2

vec(x(1) ◦ x(2) ◦ · · · ◦ x(d)) = x(d) ⊗ x(d−1) ⊗ · · · ⊗ x(1), (5)

where the symbol “⊗” denotes the Kronecker product.

We now illustrate how to represent a polynomial by using
tensors. Denote by R[x] the polynomial ring in d variables
x = (x1, x2, . . . , xd)(cid:62) with coefﬁcients in the ﬁeld R.
Deﬁnition 1. Given a vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd, a
polynomial f ∈ R[x] with d variables is called pure-power-˜n

f (x) = A ×1 v(x1)(cid:62) ×2 v(x2)(cid:62) ×3 · · · ×d v(xd)(cid:62).

(7)

rk ≤ min(

ni,

ni),

k = 1, 2, . . . , d − 1.

k
(cid:89)

d
(cid:89)

i=1

i=k+1

if the degree of f is at most ˜ni with respect to each variable
xi, i = 1, 2, . . . , d.
Example 1. The polynomial f = 4x1 +x3
is a pure-power-˜n polynomial with ˜n = (3, 1, 2).

1 −2x1x2x3 −7x2x2
3

The set of all pure-power-˜n polynomials with the degree
vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd is denoted by R[x]˜n. For
any f (x) ∈ R[x]˜n, there are a total of (cid:81)d
k=1(˜nk + 1) distinct
monomials

d
(cid:89)

k=1

xik−1
k

,

1 ≤ ik ≤ ˜nk + 1,

k = 1, 2, . . . , d.

For x = (x1, x2, . . . , xd)(cid:62) ∈ Rd, denote by {v(xk)}d
Vandermonde vectors

k=1 the

v(xk) := (1, xk, . . . , x˜nk

k )(cid:62) ∈ R˜nk+1.

(6)

follows that

It
there is a one-to-one mapping between
pure-power-˜n polynomials and tensors. To be speciﬁc, for
any f (x) ∈ R[x]˜n,
there exists a unique tensor A ∈
R(˜n1+1)×(˜n2+1)×···×(˜nd+1) such that

1, x3

1), v(x2) = (1, x2), v(x3) = (1, x3, x2

Example 2. We revisit the polynomial f from Example 1
and illustrate its corresponding tensor representation. Since
˜n = (3, 1, 2), we construct the Vandermonde vectors v(x1) =
(1, x1, x2
3). The
nonzero entries of the corresponding 4×2×3 tensor A are then
A211 = 4, A411 = 1, A222 = −2, A123 = −7. The indices of
the tensor A are easily found from grouping together corre-
sponding indices of the Vandermonde vectors. For example,
the tensor index 123 corresponding with the monomial x2x2
3
is found from v(x1)1 = 1, v(x2)2 = x2, v(x3)3 = x2
3.

B. Tensor trains

It is well known that the number of tensor elements grows
exponentially with the order d. Even when the dimensions
are small, the storage cost for all elements is prohibitive for
large d. The TT decomposition [22] gives an efﬁcient way (in
storage and computation) to overcome this so-called curse of
dimensionality.

The main idea of the TT decomposition is to re-express
the entries of a tensor A ∈ Rn1×n2×···×nd as a product of
matrices

Ai1i2···id = G1(i1)G2(i2) · · · Gd(id),

(8)

where Gk(ik) is an rk−1 × rk matrix for each index ik, also
called the TT-core. To turn the matrix-by-matrix product (8)
into a scalar, boundary conditions r0 = rd = 1 have to be
introduced. The quantities {rk}d
k=0 are called the TT-ranks.
Note that each core Gk is a third-order tensor with dimensions
rk−1, nk and rk. The TT-decomposition for a tensor A ∈
Rn1×n2×n3 is illustrated in Fig. 2. The most common way
to convert a given tensor A into a TT would be the TT-SVD
algorithm [22, p. 2301].

Example 3. TT-SVD algorithm [22, p. 2301]. Using the TT-
SVD algorithm, we can convert the tensor A from Example

3

Fig. 2. The TT decomposition for a tensor in Rn1×n2×n3 .

2 into a TT that consists of TT-cores G1 ∈ R1×4×3, G2 ∈
R3×2×3, G3 ∈ R3×3×1.

Note that throughout this article, we will not need to use
the TT-SVD algorithm. Instead, we will initialize the TT-cores
randomly and iteratively update the cores one-by-one in an
alternating fashion. It turns out that if all TT-ranks are bounded
by r, the storage of the TT grows linearly with the order d as
O(dnr2), where n = max{n1, n2, . . . , nd}.

Proposition 1 (Theorem 2.1 of [23]). For any tensor A ∈
Rn1×n2×···×nd , there exists a TT-decomposition with TT-ranks

We also mention that the TT representation of a tensor is
not unique. For instance, let Q be an orthogonal matrix in
Rr1×r1, namely, QQ(cid:62) = Q(cid:62)Q = Ir1. Then the tensor A in
(8) also has the TT-decomposition

Ai1i2···id = G(cid:48)

1(i1)G(cid:48)

2(i2) · · · Gd(id),

(9)

where

1(i1) = G1(i1)Q, G(cid:48)
G(cid:48)

2(i2) = Q(cid:62)G2(i2).

Numerical stability of our learning algorithms is guaranteed
by keeping all the TT-cores left-orthogonal or right-orthogonal
[24], which is achieved through a sequence of QR decompo-
sitions as explained in Section V.

Deﬁnition 2. The rk−1 × nk × rk core Gk is called left-
orthogonal if

and the rk−1 × nk × rk core Gk is called right-orthogonal if

nk(cid:88)

ik=1

nk(cid:88)

ik=1

Gk(ik)(cid:62)Gk(ik) = Irk ,

Gk(ik)Gk(ik)(cid:62) = Irk−1.

As stated before, the structure of a TT also beneﬁts the

computation of the general multidimensional contraction:

f = A ×1 (v(1))(cid:62) ×2 (v(2))(cid:62) ×3 · · · ×d (v(d))(cid:62),

(10)

where A ∈ Rn1×n2×···×nd and v(i) = (v(i)
ni )(cid:62) ∈
Rni, i = 1, 2, . . . , d. If a tensor A is given in the TT-
format (8), then we have

2 , . . . , v(i)

1 , v(i)

f =

d
(cid:89)

nk(cid:88)

k=1

ik=1

v(k)
ik

Gk(ik).

(11)

The described procedure for fast TT contraction is summarized
in Algorithm 1. In order to simplify the analysis on the
computational complexity of Algorithm 1, we assume that
r1 = r2 = · · · = rd−1 = r and n1 = n2 = · · · = nd = n.
There are two required steps to compute the contraction of
a TT with vector. First, we need to construct d matrices
V (k) by contracting the TT-cores Gk with the vectors v(k).
This operation is equivalent with d matrix-vector products
with a total computational cost of approximately O(dr2n)
ﬂops. Fortunately, the contraction of one TT-core is completely
independent from the other contractions and hence can be
done in parallel over d processors, reducing the computational
complexity to O(r2n) per processor. Maximal values for r and
n in our experiments are 10 and 4, respectively, so that the
contraction of one TT-core is approximately equivalent with
the product of a 100 × 4 matrix with a 4 × 1 vector. The ﬁnal
step in Algorithm 1 is the product of all matrices V (k) with
a total computational complexity of O(dr2). If we again set
r = 10, n = 4, then this ﬁnal step in Algorithm 1 is equivalent
with the product of a 100×40 matrix with a 40×1 vector. For
more basic operations implemented in the TT-format, such as
tensor addition and computing the Frobenius norm, the reader
is referred to [22].

Algorithm 1 Fast TT contraction [22]
Input: Vectors v(k) ∈ Rnk , k = 1, 2, . . . , d and a tensor A

in the TT-format with cores Gk

Output: The multidimensional contraction f in (10)

ik=1 v(k)

ik

Gk(ik)

%Computed in parallel

1: for k = 1 : d do
V (k) = (cid:80)nk
2:
3: end for
4: f := V (1)
5: for k = 2 : d do
f := f V (k)
6:
7: end for
8: return f

IV. TT LEARNING

It is easy for us to recognize a face, understand spoken
words, read handwritten characters and identify the gender
of a person. Machines, however, make decisions based on
data measured by a large number of sensors. In this section,
we present the framework of TT learning. Like most pattern
recognition systems [25], our TT learning method consists in
dividing the system into three main modules, shown in Fig. 3.
The ﬁrst module is called feature extraction, which is of
paramount importance in any pattern classiﬁcation problem.
The goal of this module is to build features via transformations
of the raw input, namely, the original data measured by a large
number of sensors. The basic reasoning behind transform-
based features is that an appropriately chosen transforma-
tion can exploit and remove information redundancies, which
usually exist in the set of samples obtained by measuring
devices. The set of features exhibit high information packaging
properties compared with the original input samples. This
means that most of the classiﬁcation-related information is

4

compressed into a relatively small number of features, leading
to a reduction of the necessary feature space dimension.
Feature extraction beneﬁts training the classiﬁer in terms of
memory and computation, and also alleviates the problem of
overﬁtting since we get rid of redundant information. To deal
with the task of feature extraction, some linear or nonlinear
transformation techniques are widely used. For example, the
Karhunen-Lo`eve transform, related to principal component
analysis (PCA), is one popular method for feature generation
and dimensionality reduction. A nonlinear kernel version of
the classical PCA is called kernel PCA, which is an extension
of PCA using kernel methods. The discrete Fourier transform
(DFT) can be another good choice due to the fact that for
many practical applications, most of the energy lies in the
low-frequency components. Compared with PCA, the basis
vectors in the DFT are ﬁxed and problem-dependent, which
leads to a low computational complexity.

The second module, the TT classiﬁer, is the core of TT
learning. The purpose of this module is to mark a new
observation based on its features generated by the previous
module. As will be discussed, the task of pattern classiﬁcation
can be divided into a sequence of binary classiﬁcations. For
each particular binary classiﬁcation, the TT classiﬁer assigns
to each new observation a score that indicates which class it
belongs to. In order to construct a good classiﬁer, we exploit
the fact that we know the labels for each sample of a given
dataset. The TT classiﬁer is trained optimally with respect
to an optimality criterion. In some ways, the TT classiﬁer
can be regarded as a kind of generalized linear classiﬁer,
it does a linear classiﬁcation in a higher dimensional space
generated by the items of a given pure-power polynomial. The
local information is encoded by the products of features. In
contrast to kernel-based SVM classiﬁers that work in the dual
space, the TT classiﬁer is able to work directly in the high
dimensional space by exploiting the TT-format. Similar with
the backpropagation algorithm for multilayer perceptrons, the
structure of a TT allows for updating the cores in an alternating
way. In the next section, we will describe the training of two
TT classiﬁers through the optimization of two different loss
functions.

The last module in Fig. 3 is the decision module that
decides which category a new observation belongs to. For
binary classiﬁcation, decisions are made according to the
sign of the score assigned by the TT classiﬁer, namely, the
decision depends on the value of corresponding discriminant
function. In an m-class problem, there are several strategies to
decompose it into a sequence of binary classiﬁcation problems.
A straightforward extension is the one-against-all, where m
binary classiﬁcation problems are involved. We seek to design
discriminant functions {gi(x)}m
i=1 so that gi(x) > gj(x),
∀j (cid:54)= i if x belongs to the ith class. Classiﬁcation is then
achieved according to the rule:

assign x to the ith class if i = argmaxk gk(x).
An alternative technique is the one-against-one, where we
need to consider m(m − 1)/2 pairs of classes. The decision
is made on the basis of a majority vote. It means that each
classiﬁer casts one vote and the ﬁnal class is the one with

Fig. 3. Framework of TT learning.

the most votes. When the number m is too large, one can
also apply the technique of binary coding. It turns out that
only (cid:100)log2 m(cid:101) classiﬁers are needed, where (cid:100)·(cid:101) is the ceiling
operation. In this case, each class is represented by a unique
binary code word of length (cid:100)log2 m(cid:101). The decision is then
made on the basis of minimal Hamming distance.

V. LEARNING ALGORITHMS
For notational convenience, we deﬁne nk := ˜nk + 1
and continue to use this notation for the remainder of the
article. As stated before, TT classiﬁers are designed for binary
classiﬁcation. Given a set of N training examples of the form
j=1 such that x(j) ∈ Rd is the feature vector of
{(x(j), y(j))}N
the jth example and y(j) ∈ {−1, 1} is the corresponding class
label of x(j). Let ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd be the degree
vector. Each feature is then mapped to a higher dimensional
space generated by all corresponding pure-power-˜n monomials
through the mapping T : Rd → Rn1×n2×···×nd

5

Here we consider that the tensor A is expressed as a tensor
train with cores {Gk}d
k=1. The main idea of the TT learning
algorithms is to update the cores in an alternating way by
optimizing an appropriate loss function. Prior to updating the
TT-cores, the TT-ranks are ﬁxed and a particular initial guess
of {Gk}d
k=1 is made. The TT-ranks can be interpreted as tuning
parameters, higher values will result in a better ﬁt at the risk
of overﬁtting. It is straightforward to extend our algorithms by
means of the Density Matrix Renormalization Group (DMRG)
method [26] such that the TT-ranks are updated adaptively.
Each core is updated in the order

G1 → G2 → · · · → Gd → Gd−1 → · · · → G1 → · · ·

until convergence, which is guaranteed under certain condi-
tions as described in [27], [28]. It turns out that updating
one TT-core is equivalent with minimizing a loss function in
a small number of variables, which can be done in a very
efﬁcient manner. The following lemma shows how the inner
product (cid:104)T (x), A(cid:105) in the generic feature space is a linear
function in any of the TT-cores Gk.
Lemma 1. Given a vector ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd, let
T be the mapping deﬁned by (12), and let A be a TT with
cores Gk ∈ Rrk−1×nk×rk , k = 1, 2, . . . , d. For any x ∈ Rd
and k = 1, . . . , d, we have that

(cid:104)T (x), A(cid:105) = (cid:0)qk(x)(cid:62) ⊗ v(xk)(cid:62) ⊗ pk(x)(cid:1) vec(Gk),

(14)

T (x)i1i2···id =

xik−1
k

.

d
(cid:89)

k=1

(12)

where

For x = (x1, x2, . . . , xd)(cid:62) ∈ Rd, let {v(xk)}d
Vandermonde vectors deﬁned in (6). Clearly, we have

k=1 be the

p1(x) = 1, pk(x)
k≥2

=

k−1
(cid:89)

i=1

(cid:0)Gi ×2 v(xi)(cid:62)(cid:1) ∈ R1×rk−1,

T (x) = v(x1) ◦ v(x2) ◦ · · · ◦ v(xd).

(13)

and

This high-dimensional pure-power polynomial space beneﬁts
the learning task from the following aspects:

• all interactions between features are described by the

qk(x)
k<d

=

d
(cid:89)

i=k+1

monomials of pure-power polynomials;

Proof. By deﬁnition, we have

(cid:0)Gi ×2 v(xi)(cid:62)(cid:1) ∈ Rrk×1, qd(x) = 1.

• the dimension of the tensor space grows exponentially
k=1 nk, which increases the probability
training examples linearly into two-

with d, namely, (cid:81)d
of separating all
classes;

• the one-to-one mapping between pure-power polynomials
and tensors enables the use of tensor trains to lift the curse
of dimensionality.

With these preparations, our goal

is to ﬁnd a decision
hyperplane to separate these two-class examples in the tensor
space, also called the generic feature space. In other words,
like the inductive learning described in [16], we try to ﬁnd a
tensor A ∈ Rn1×n2×···×nd such that

y(j)(cid:104)T (x(j)), A(cid:105) > 0,

j = 1, 2, . . . , N.

Note that the bias is absorbed into the ﬁrst element of A. Note
that the learning problem can also be interpreted as ﬁnding a
pure-power-˜n polynomial g(x) such that

and

g(x(j)) > 0,

∀y(j) = 1,

g(x(j)) < 0,

∀y(j) = −1.

(cid:104)T (x), A(cid:105) = A ×1 v(x1)(cid:62) ×2 · · · ×d v(xd)(cid:62)

= (cid:0)G1 ×2 v(x1)(cid:62)(cid:1) · · · (cid:0)Gd ×2 v(xd)(cid:62)(cid:1)
= Gk ×1 pk(x) ×2 v(xk)(cid:62) ×3 qk(x)(cid:62)
= (cid:0)qk(x)(cid:62) ⊗ v(xk)(cid:62) ⊗ pk(x)(cid:1) vec(Gk)

for any k = 1, 2, . . . , d. This completes the proof.

Example 4. In this example we illustrate the advantageous
representation of a pure-power polynomial f as a TT. Suppose
we have a polynomial f with d = 10 and all degrees ˜ni =
9 (i = 1, . . . , 10). All coefﬁcients of f (x) can then be stored
into a 10-way tensor 10 × 10 × · · · × 10 tensor A such that
the evaluation of f in a particular x is given by (7). The TT-
representation of f consists of 10 TT-cores G1, . . . , G10, with
a storage complexity of O(100r2), where r is the maximal TT-
rank. This demonstrates the potential of the TT-representation
in avoiding the curse of dimensionality when the TT-ranks are
small.

for
Example
T (x), A, v(xk), qk(x), pk(x) for the following quadratic

5. Next, we

expressions

illustrate

the

6







T (x) =

1 +
2. Since d = 2 and ˜n1 = ˜n2 = 2, both T and A

polynomial in two variables f (x) = 1 + 3x1 − x2 − x2
7x1x2 + 9x2
are the following 3 × 3 matrices


x2
1
x2
2
x1 x1x2 x1x2
2
1x2
1 x2
x2
2
The TT-representation of A consists of a 1 × 3 × 3 tensor G1
and a 3×3×1 tensor G2. Suppose now that k = 2 and we want
to compute the evaluation of the polynomial f in a particular
x, which is (cid:104)T (x), A(cid:105). From Lemma 1 we then have that
(cid:104)T (x), A(cid:105) = (cid:0)q2(x)(cid:62) ⊗ v(x2)(cid:62) ⊗ p2(x)(cid:1) vec(G2),

1 −1
7
3
0
−1


9
0
 .
0

1x2 x2

 , A =



We have thus shown that updating the core Gk is equivalent
with solving a least squares optimization problem in rk−1nkrk
variables. Minimizing (17) with respect to Gk for any k =
1, . . . , d results in solving the linear system

(C (cid:62)

k Ck) vec(Gk) = C (cid:62)

k y.

(19)

Supposing r1 = r2 = · · · = rd−1 = r and n1 = n2 = · · · =
nd = n, then the computational complexity of solving (19) is
O((r2n)3). For the maximal values of r = 10 and n = 4 in
our experiments, this implies that we need to solve a linear
system of order 400, which takes about 0.01 seconds using
MATLAB on our desktop computer.

with

q2(x) = 1 ∈ R,
v(x2) = (cid:0)1 x2 x2
p2(x) = G1 ×2 v(x1)(cid:62) ∈ R1×3,
v(x1) = (cid:0)1 x1 x2

∈ R3.

∈ R3,

(cid:1)(cid:62)

(cid:1)(cid:62)

2

1

In what follows, we ﬁrst present two learning algorithms
based on different loss functions. These algorithms will learn
the tensor A directly in the TT-representation from a given
dataset. Two enhancements, namely, regularization for better
accuracy and parallelization for higher speed will be described
in the last two subsections.

A. TT Learning by Least Squares

Least squares estimation is the simplest and thus most
common estimation method. In the generic feature space, we
attempt to design a linear classiﬁer so that its desired output
is exactly 1 or −1. However, we have to live with errors, that
is, the true output will not always be equal to the desired one.
The least squares estimator is then found from minimizing the
following mean square error function

B. TT Learning by Logistic Regression

Since our goal is to ﬁnd a hyperplane to separate two-class
training examples in the generic feature space, we may not
care about the particular value of the output. Indeed, only
the sign of the output makes sense. This gives us the idea to
decrease the number of sign differences as much as possible
when updating the TT-cores, i.e., to minimize the number of
misclassiﬁed examples. However, this model is discrete so
that a difﬁcult combinatorial optimization problem is involved.
Instead, we try to ﬁnd a suboptimal solution in the sense of
minimizing a continuous cost function that penalizes misclas-
siﬁed examples. Here, we consider the logistic regression cost
function. First, consider the standard sigmoid function

σ(z) =

1
1 + e−z ,

z ∈ R,

where the output always takes values between 0 and 1. An
important property is that its derivative can be expressed by
the function itself, i.e.,

J(A) =

(cid:16)

(cid:104)T (x(j)), A(cid:105) − y(j)(cid:17)2

.

(15)

The logistic function for the jth example x(j) is given by

1
N

N
(cid:88)

j=1

σ(cid:48)(z) = σ(z)(1 − σ(z)).

hA(x(j)) := σ

(cid:104)T (x(j)), A(cid:105)

(cid:16)

(cid:17)

.

(20)

(21)

We now show how updating a TT-core Gk is equivalent with
solving a relatively small linear system. First, we deﬁne the
N × rk−1nkrk matrix




Ck =







qk(x(1))(cid:62) ⊗ v(x(1)
qk(x(2))(cid:62) ⊗ v(x(2)

k )(cid:62) ⊗ pk(x(1))
k )(cid:62) ⊗ pk(x(2))
...







qk(x(N ))(cid:62) ⊗ v(x(N )

k

)(cid:62) ⊗ pk(x(N ))

for any k = 1, 2, . . . , d. The matrix Ck is hence obtained
from the concatenation of the row vectors qk(x)(cid:62) ⊗v(xk)(cid:62) ⊗
pk(x) from (14) for N samples x(1), . . . , x(N ). It follows from
Lemma 1 that

J(A) =

(cid:107)Ck vec(Gk) − y(cid:107)2

1
N

We can also interpret the logistic function as the probability
that the example x(j) belongs to the class denoted by the
label 1. The predicted label ˜y(j) for x(j) is then obtained
according to the rule

(16)

(cid:40)

hA(x(j)) ≥ 0.5 ⇔ (cid:104)T (x(j)), A(cid:105) ≥ 0 → ˜y(j) = 1,
hA(x(j)) < 0.5 ⇔ (cid:104)T (x(j)), A(cid:105) < 0 → ˜y(j) = −1.

For a particular example x(j), we deﬁne the cost function as

Cost(x(j), A) =




− ln

(cid:16)

(cid:17)
hA(x(j))



− ln

(cid:16)

(cid:17)
1 − hA(x(j))

if y(j) = 1,

if y(j) = −1.

where

y = (y(1), y(2), . . . , y(N ))(cid:62) ∈ RN .

The goal now is to ﬁnd a tensor A such that hA(x(j)) is near
1 if y(j) = 1 or near 0 if y(j) = −1. As a result, the logistic

(17)

(18)

regression cost function for the whole training dataset is given
by

J(A) =

Cost(x(j), A)

1
N

N
(cid:88)

j=1

=

−1
N

N
(cid:88)

j=1

(cid:20) 1 + y(j)
2

(cid:16)

ln

hA(x(j))

+

(cid:17)

(22)

1 − y(j)
2

(cid:16)

ln

1 − hA(x(j))

(cid:17)(cid:21)

.

It is important to note that (22) is convex though the sigmoid
function is not. This guarantees that we can ﬁnd the globally
optimal solution instead of a local optimum.

From equation (21) and Lemma 1, one can see that the
function J(A) can also be regarded as a function of the core
Gk since

and

(cid:104)T (x(j)), A(cid:105) = Ck(j, :) vec(Gk)

where Ck(j,
:) denote the jth row vector of Ck deﬁned in
(16). It follows that updating the core Gk is equivalent with
solving a convex optimization problem in rk−1nkrk variables.
Let

hA =

(cid:16)
hA(x(1)), hA(x(2)), . . . , hA(x(N ))

(cid:17)(cid:62)

∈ RN (23)

and DA be the diagonal matrix in RN ×N with the jth
diagonal element given by hA(x(j)) (cid:0)1 − hA(x(j))(cid:1). By using
the property (20) one can derive the gradient and Hessian with
respect to Gk as

∇Gk J(A) =

C (cid:62)
k

hA −

(cid:18)

1
N

(cid:19)

y + 1
2

and

∇2
Gk

J(A) =

C (cid:62)

k DACk,

1
N

respectively, where y is deﬁned in (18) and 1 denotes the
all-ones vector in RN . Although we do not have a closed-
form solution to update the core Gk, the gradient and Hessian
allows us to ﬁnd the solution by efﬁcient iterative methods, e.g.
Newton’s method whose convergence is at least quadratic in a
neighbourhood of the solution. A quasi-Newton method, like
the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm, is
another good choice if the inverse of the Hessian is difﬁcult
to compute.

(24)

(25)

C. Regularization

The cost functions (15) and (22) of the two TT learning
algorithms do not have any regularization term, which may
result in overﬁtting and hence bad generalization properties of
the obtained TT classiﬁer. Next, we discuss how the addition
of a regularization term to (15) and (22) results in a small
modiﬁcation of the small optimization problem that needs to
be solved when updating the TT-cores Gk.

Consider the regularized optimization problem

˜J(A) = J(A) + γR(A),

(26)

7

where J(A) is given by (15) or (22), γ is a parameter that
balances the loss function and the regularization term. Here
we use the Tikhonov regularization, namely,

(cid:104)A, A(cid:105).

R(A) =

1
2
Thanks to the TT structure, the gradient of R(A) with respect
to the TT-core Gk can be equivalently rewritten as a linear
transformation of vec(Gk). In other words, there is a matrix
Dk ∈ Rrk−1nkrk×rk−1nkrk determined by the cores {Gj}j(cid:54)=k
such that ∇Gk R(A) = Dkvec(Gk). See Appendix A for more
details. It follows that

(27)

∇Gk

˜J(A) = ∇Gk J(A) + γDkvec(Gk)

∇2
Gk

˜J(A) = ∇2
Gk

J(A) + γDk.

These small modiﬁcations lead to small changes when updat-
ing the core Gk. For instance, the ﬁrst-order condition of (26)
for the least squares model results in solving the modiﬁed
linear system
(cid:18)

(cid:19)

C (cid:62)

k Ck +

γDk

vec(Gk) = C (cid:62)

k y,

(28)

N
2

when compared with the original linear system (19).

D. Orthogonalization and Parallelization

The matrix Ck from (16) needs to be reconstructed for each
TT-core Gk during the execution of the two TT learning algo-
rithms. Fortunately, this can be done efﬁciently by exploiting
the TT structure. In particular, after updating the core Gk in the
left-to-right sweep, the new row vectors {pk+1(x(j))}N
j=1 to
construct the next matrix Ck+1 can be easily computed from
pk+1(x(j)) = Gk ×1 pk(x(j)) ×2 v(x(j)

k )(cid:62).

Similarly, in the right-to-left sweep, the new column vectors
{qk−1(x(j))}N
j=1 to construct the next matrix Ck−1 can be
easily computed from

qk−1(x(j)) = Gk ×2 v(x(j)

k )(cid:62) ×3 qk(x(j))(cid:62).

To make the learning algorithms numerically stable, the
techniques of orthogonalization are also applied. The main
idea is to make sure that before updating the core Gk, the cores
G1, . . . , Gk−1 are left-orthogonal and the cores Gk+1, . . . , Gd
are right-orthogonal by a sequence of QR decompositions. In
this way, the condition number of the constructed matrix Ck
is upper bounded so that the subproblem is well-posed. After
updating the core Gk, we orthogonalize it with an extra QR
decomposition, and absorb the upper triangular matrix into
the next core (depending on the direction of updating). More
details on the orthogonalization step can be found in [27].

Another computational challenge is the potentially large
size N of the training dataset. Luckily, the dimension of the
optimization problem when updating Gk in the TT learning
algorithms is rk−1(nk + 1)rk, which is much smaller and
independent from N . We only need to compute the products
C (cid:62)
k DACk in (19), (24) and (25).
These computations are easily done in parallel. Speciﬁcally,

k hA and C (cid:62)

k Ck, C (cid:62)

k y, C (cid:62)

8

given a proper partition {Nl}L
we divide the large matrix Ck into several blocks, namely,

l=1 satisfying (cid:80)L

l=1 Nl = N ,

TABLE II
DATASET DESCRIPTION

Ck =

∈ RN ×rk−1(nk+1)rk ,

















C (1)
k
C (2)
k
...
C (L)
k

where C (l)
example, the product C (cid:62)

k ∈ RNl×rk−1nkrk , l = 1, 2, . . . , L. Then, for

k DACk can be computed by

C (cid:62)

k DACk =

(C (l)

k )(cid:62)D(l)

A C (l)
k ,

L
(cid:88)

l=1

where D(l)
A denotes the corresponding diagonal block. Each
term in the summation on the right-hand side of the above
equation can be computed over L distributed cores, with
a computational complexity of O(r4n2
kNl) for each core,
supposing rk−1 = rk = r. The other matrix products can
also be computed in a similar way.

We summarize our learning algorithms in Algorithm 2.
The two most computationally expensive steps are lines 5
and 7. As we mentioned before, solving (26) takes approx-
imately O((r2n)3) ﬂops. If the QR decomposition of line 7
is computed through Householder transformations, then the
computational complexity is approximately O(r3n2) ﬂops. For
the maximal values of n = 4 and r = 10 in our experiments,
this amounts to solving computing the inverse and the QR
factorization of a 400 × 400 matrix. Note that based on the
decision strategy, an m-class problem is decomposed into a
sequence of two-class problems whose TT classiﬁers can be
trained in parallel.

Algorithm 2 Tensor Train Learning Algorithm
Input: Training dataset of pairs {(x(j), y(j))}N

j=1, TT-ranks
k=1, degree vector ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd and

{rk}d−1
regularization parameter γ

Output: Tensor A in TT format with cores {Gk}d
1: Initialize right orthogonal cores {Gk}d

k=1
k=1 of prescribed

ranks

2: while termination condition is not satisﬁed do
3: %Left-to-right sweep
4:
5:

for k = 1, 2, . . . , d − 1 do

G∗
k ← ﬁnd the minimal solution of the regularized
optimization problem (26) with respect to Gk
Uk ← reshape(G∗
[Q, R] ← compute QR decomposition of Uk
Gk ← reshape(Q, rk−1, nk, rk)
Vk+1 ← R ∗ reshape(Gk+1, rk, nk+1, rk+1)
Gk+1 ← reshape(Vk+1, rk, nk+1, rk+1)

k, rk−1nk, rk)

6:
7:
8:
9:
10:

end for
Perform the right-to-left sweep

11:
12:
13: end while

Image size

Training size

Test size

USPS
MNIST

16 × 16
28 × 28

7291
60000

2007
10000

• The DMRG method [26] can also be used to update
the cores. This involves updating two cores at a time
so that the TT-ranks are adaptively determined by means
of a singular value decomposition (SVD). This may give
better performance at the cost of a higher computational
complexity. It also removes the need to ﬁx the TT-ranks
a priori.

• The local linear convergence of Algorithm 2 has been
established in [27], [28] under certain conditions. In
particular,
if the TT-ranks are correctly estimated for
convex optimization problems, then the obtained solution
is guaranteed to be the global optimum. When choosing
the TT-ranks, one should keep the upper bounds of the
TT-ranks from Proposition 1 in mind.

VI. EXPERIMENTS

In this section, we test our TT learning algorithms and
compare their performance with LS-SVMs with polynomial
kernels on two popular digit recognition datasets: USPS
and MNIST. All our algorithms were implemented in MAT-
LAB Version R2016a, which can be freely downloaded from
https://github.com/kbatseli/TTClassiﬁer. We compare our TT-
polynomial classiﬁers with a polynomial classiﬁer based on
LS-SVMs with a polynomial kernel. The LS-SVM-polynomial
classiﬁer was trained with the MATLAB LS-SVMlab tool-
box, which can be freely downloaded from http://www.esat.
kuleuven.be/sista/lssvmlab/. The numerical experiments were
done on a desktop PC with an Intel i5 quad-core processor
running at 3.3GHz and 16GB of RAM.

The US Postal Service (USPS) database1contains 9298
handwritten digits, including 7291 for training and 2007 for
testing. Each digit is a 16×16 grayscale image. It is known that
the USPS test set is rather difﬁcult and the human error rate is
2.5%. The Modiﬁed NIST (MNIST) database2 of handwritten
digits has a training set of 60,000 examples, and a test set of
10,000 examples. It is a subset of a larger set available from
NIST. The digits have been size-normalized and centered in
a 28×28 image. The description of these two databases is
summarized in Table II.

Before extracting features of the handwritten digits, we ﬁrst
execute the pre-process of deskewing which is the process
of straightening an image that has been scanned or written
crookedly. By choosing a varying number d, the corresponding
feature vectors are then extracted from a pre-trained CNN
model 1-20-P-100-P-d-10, which represents a net with an input
images of size 28×28, a convolutional layer with 20 maps and

We end this section with the following remarks:
• Other loss functions can also be used in the framework
of TT learning provided that there exists an efﬁcient way
to solve the corresponding subproblems.

1The USPS database is downloaded from

http://statweb.stanford.edu/∼tibs/ElemStatLearn/data.html

2The MNIST database is downloaded from

http://yann.lecun.com/exdb/mnist/

5×5 ﬁlters, a max-pooling layer over non-overlapping regions
of size 2×2, a convolutional layer with 100 maps and 5×5
ﬁlters, a max-pooling layer over non-overlapping regions of
size 2×2, a convolutional layer with d maps and 4×4 ﬁlters,
and a fully connected output layer with 10 neurons. As the
variants of the well-known CNN model LeNet-5 [9], these
CNN models have been trained well on the MNIST database.
For an input 28×28 image, we get a feature vector of length
d from the third hidden layer. Note that for USPS database,
we must resize image data to the size of image input layer.
We mention that these techniques of deskewing and feature
extraction using pre-trained CNN model are widely used in
the literature [9], [10], [29].

For the decision module, we adopt the one-against-all deci-
sion strategy where ten TT classiﬁers are trained to separate
each digit from all
the others. In the implementation of
Algorithm 2, we normalize each initial core such that its
Frobenius norm is equal to one. The degree vector is given
by ˜n1 = · · · = ˜nd = ˜n. The TT-ranks are upper bounded
by rmax. The values of d, ˜n, rmax were chosen to minimize
the test error rate and to ensure that each of the subproblems
to update the TT-cores Gk could be solved in a reasonable
time. The dimension of each subproblem is at most n r2
max.
For example, in the USPS case, we ﬁrst ﬁxed the values of
n and rmax. We then ﬁxed the value of d and incremented
n and rmax to see whether this resulted in a better test error
rate. We use the optimality criterion

| ˜J(A+) − ˜J(A)|
| ˜J(A)|

≤ 10−2,

where A+ is the updated tensor from tensor A after one sweep.
And the maximum number of sweeps is 4, namely, 4(d−1) it-
erations through the entire training data are performed for each
session. To simplify notations, we use “TTLS” and “TTLR” to
denote the TT learning algorithms based on minimizing loss
functions (15) and (22), respectively. For these two models, the
regularization parameter γ is determined by the technique of
10-fold cross-validation. In other words, we randomly assign
the training data to ten sets of equal size. The parameter γ is
chosen so that the mean over all test errors is minimal.

The numerical results for USPS database and MNIST
database are reported in Tables III and IV, respectively. The
monotonic decrease is always seen when training the ten TT
classiﬁers. Fig. 4 shows the convergence of both TT learning
algorithms on the USPS data for the case d = 20, ˜n =
1, rmax = 8 when training the classiﬁer for the character “6”.
In addition, we also trained a polynomial classiﬁer using LS-
SVMs with polynomial kernels on these two databases. Using
the basic LS-SVM scheme, a training error rate of 0 and a test
error rate of 8.37% were obtained for the USPS dataset after
more than three and a half hours of computation. This runtime
includes the time required to tune the tuning parameters via
10-fold cross-validation. When using an RBF kernel with the
LS-SVM, it is possible to attain a test error of 2.14% [30],
but then the classiﬁer is not polynomial anymore. The MNIST
dataset resulted in consistent out-of-memory errors, which is
to be expected as the basic SVM scheme is not intended
for large data sets. We would also like to point out that a

9

Fig. 4. The convergence of TT learning algorithms.

test error of 1.1% is reported on the MNIST website for a
polynomial classiﬁer of degree 4 that uses conventional SVMs
and deskewing.

VII. CONCLUSION

This paper presents the framework of TT learning for pattern
classiﬁcation. For the ﬁrst time, TTs are used to represent
polynomial classiﬁers, enabling the learning algorithms to
work directly in the high-dimensional feature space. Two
efﬁcient learning algorithms are proposed based on different
loss functions. The numerical experiments show that each TT
classiﬁer is trained in up to several minutes with competitive
test errors. When compared with other polynomial classiﬁers,
the proposed learning algorithms can be easily parallelized
and have considerable advantage on storage and computation
time. We also mention that these results can be improved by
adding virtual examples [10]. Future improvements are the
implementation of on-line learning algorithms, together with
the extension of the binary TT classiﬁer to the multi-class case.

APPENDIX A
Given the degree vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd, let A ∈
Rn1×n2×···×nd be the tensor in TT format with cores Gk ∈
Rrk−1×nk×rk , k = 1, 2, . . . , d. To investigate the gradient of
R(A) in (27) with respect to the TT-core Gk, we give a small
variation (cid:15) to the ith element of vec(Gk), resulting in a new
tensor A(cid:15) given by

A(cid:15) = A + (cid:15)I (k)

,

i
where 1 ≤ i ≤ rk−1nkrk and I (k)
is the tensor which has
the same TT-cores with A except that the vectorization of the
core Gk is replaced by the unit vector in Rrk−1nkrk with the
ith element equal to 1 and 0 otherwise. Then we have

i

[∇Gk R(A)]i = lim
(cid:15)→0

R(A(cid:15)) − R(A)
(cid:15)
On the other hand, by the deﬁnition of vectorization, the ith
element of vec(Gk) ∈ Rrk−1nkrk is mapped from the tensor
element of Gk ∈ Rrk−1×nk×rk with indices (αk−1, jk, αk)
satisfying

= (cid:104)A, I (k)

(29)

(cid:105).

i

i = αk−1 + (jk − 1)rk−1 + (αk − 1)rk−1nk,

where 1 ≤ αk−1 ≤ rk−1, 1 ≤ jk ≤ nk and 1 ≤ αk ≤ rk.
Denote by E(αk−1,αk) the matrix in Rrk−1×rk such that the

TABLE III
NUMERICAL RESULTS FOR DATASET USPS

Train error

TTLS
Test error

Train error

TTLR
Test error

10

d

20
25
30
35
40
20
25
30
35
40
20
25
30
20
25
30
35
40

d

20
25
30
35
40
20
25
30
35
40
20
30
40
20
30
40
40
40
40

˜n

1
1
1
1
1
2
2
2
2
2
3
3
3
1
1
1
1
1

˜n

1
1
1
1
1
2
2
2
2
2
3
3
3
1
1
1
2
3
4

rmax
8
8
8
8
8
8
8
8
8
8
8
8
8
10
10
10
10
10

rmax
8
8
8
8
8
8
8
8
8
8
8
8
8
10
10
10
10
10
10

0.58%
0.47%
0.49%
0.38%
0.34%
0.49%
0.38%
0.51%
0.45%
0.34%
0.59%
0.49%
0.63%
0.56%
0.41%
0.51%
0.34%
0.44%

0.16%
0.15%
0.13%
0.19%
0.21%
0.19%
0.13%
0.19%
0.17%
0.17%
0.20%
0.22%
0.21%
0.20%
0.16%
0.17%
0.13%
0.18%
0.24%

Time(s)
4.33% 1.10×10
4.04% 1.57×10
3.84% 1.83×10
4.04% 2.21×10
3.89% 2.59×10
4.29% 2.04×10
4.29% 2.68×10
4.14% 3.28×10
4.38% 3.87×10
4.09% 4.48×10
4.58% 3.26×10
4.53% 4.07×10
4.43% 4.93×10
4.33% 1.91×10
3.94% 2.48×10
3.84% 3.09×10
3.94% 3.71×10
3.84% 4.36×10

Time(s)
7.14×10
0.96%
9.38×10
0.97%
0.86% 11.73×10
0.81% 13.95×10
0.94% 16.22×10
1.03% 12.08×10
1.00% 15.72×10
0.88% 18.94×10
0.86% 23.11×10
0.90% 25.67×10
1.10% 17.45×10
1.02% 27.21×10
0.97% 36.48×10
0.99% 10.14×10
0.88% 17.15×10
0.88% 24.05×10
0.93% 39.61×10
0.94% 59.66×10
1.02% 78.83×10

0.45%
0.27%
0.30%
0.26%
0.14%
0.55%
0.29%
0.47%
0.32%
0.21%
0.64%
0.56%
0.51%
0.44%
0.27%
0.25%
0.22%
0.33%

0.12%
0.09%
0.10%
0.03%
0.07%
0.14%
0.13%
0.10%
0.13%
0.17%
0.19%
0.21%
0.07%
0.19%
0.17%
0.13%
0.14%
0.10%
0.14%

Time(s)
4.87×10
4.33%
6.49×10
3.99%
8.18×10
3.99%
9.77×10
4.24%
3.74% 11.39×10
8.29×10
4.24%
4.14% 10.82×10
3.99% 13.34×10
4.38% 15.88×10
3.74% 18.39×10
4.33% 11.47×10
4.14% 14.98×10
4.09% 18.35×10
4.33% 17.69×10
3.94% 24.72×10
4.04% 31.14×10
4.29% 38.17×10
3.84% 45.24×10

Time(s)
0.99% 17.52×10
0.98% 22.96×10
0.86% 28.96×10
0.82% 34.55×10
0.83% 40.31×10
1.01% 29.45×10
0.98% 38.32×10
0.91% 46.77×10
0.83% 56.54×10
0.88% 64.68×10
1.00% 42.24×10
0.90% 65.89×10
0.89% 90.37×10
0.98% 36.66×10
0.90% 62.24×10
0.89% 83.62×10
0.92% 144.1×10
0.90% 207.7×10
0.91% 264.2×10

TABLE IV
NUMERICAL RESULTS FOR DATASET MNIST

Train error

TTLS
Test error

Train error

TTLR
Test error

element with index (αk−1, αk) equal to 1 and 0 otherwise. By
simple computation, one can obtain that

and

(cid:104)A, I (k)

(cid:105) =

i

(cid:88)

Ai1i2···id (I k

i )i1i2···id

i1,i2,...id
(cid:16)

= ak

(cid:17)
E(αk−1,αk) ⊗ Gk(jk)

bk,

where

ak =

k−1
(cid:89)

nl(cid:88)

l=1

il=1

(cid:2)Gl(il) ⊗ Gl(il)(cid:3) ∈ R1×r2

k−1

(31)

bk =

d
(cid:89)

nl(cid:88)

l=k+1

il=1

(cid:2)Gl(il) ⊗ Gl(il)(cid:3) ∈ Rr2

k×1.

(32)

(30)

Let a(1)
that

k , a(2)

k , . . . , a(rk−1)

k

∈ R1×rk−1 be the row vectors such

ak = (a(1)

k , a(2)

k , . . . , a(rk−1)

k

) ∈ R1×r2

k−1,

and let b(1)

k , b(2)

k , . . . , b(rk)

k

∈ Rrk×1 be the column vectors

such that

bk =

∈ Rr2

k×1,

















b(1)
k
b(2)
k
...
b(rk)
k

Combining (29) and (30) together, we have
[∇Gk R(A)]i = a(αk−1)
k
(cid:16)
(b(αk)
k

Gk(jk)b(αk)
)(cid:62) ⊗ (e(jk))(cid:62) ⊗ a(αk−1)

=

k

k

(cid:17)

vec(Gk),

where e(j) ∈ Rnk denotes the unit vector with the jth element
equal to 1 and 0 otherwise. If we deﬁne the rk−1nkrk ×
rk−1nkrk matrix




Dk =







(b(1)
(b(1)

k )(cid:62) ⊗ (e(1))(cid:62) ⊗ a(1)
k )(cid:62) ⊗ (e(1))(cid:62) ⊗ a(2)
...

k

k

(b(rk)
k

)(cid:62) ⊗ (enk )(cid:62) ⊗ a(rk−1)

k

,







(33)

it follows immediately that ∇Gk R(A) = Dkvec(Gk).

ACKNOWLEDGMENT
Zhongming Chen acknowledges the support of the Na-
tional Natural Science Foundation of China (Grant No.
11701132). Johan Suykens acknowledges support of ERC
AdG A-DATADRIVE-B (290923), KUL: CoE PFV/10/002
(OPTEC); FWO: G.0377.12, G.088114N, G0A4917N; IUAP
P7/19 DYSCO. Ngai Wong acknowledges the support of
the Hong Kong Research Grants Council under the General
Research Fund (GRF) project 17246416.

REFERENCES

[1] H.-B. Shen and K.-C. Chou, “Ensemble classiﬁer for protein fold pattern
recognition,” Bioinformatics, vol. 22, no. 14, pp. 1717–1722, 2006.
[2] H.-D. Cheng, X. Cai, X. Chen, L. Hu, and X. Lou, “Computer-aided
detection and classiﬁcation of microcalciﬁcations in mammograms: a
survey,” Pattern recognition, vol. 36, no. 12, pp. 2967–2991, 2003.
[3] B. Roberto, Template matching techniques in computer vision: theory

and practice. Wiley, Hoboken, 2009.

[4] B.-H. Juang, W. Hou, and C.-H. Lee, “Minimum classiﬁcation error
rate methods for speech recognition,” IEEE Transactions on Speech and
Audio processing, vol. 5, no. 3, pp. 257–265, 1997.

[5] L. Xu, A. Krzyzak, and C. Y. Suen, “Methods of combining multiple
classiﬁers and their applications to handwriting recognition,” IEEE
transactions on systems, man, and cybernetics, vol. 22, no. 3, pp. 418–
435, 1992.

[6] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern classiﬁcation.

John

Wiley & Sons, 2012.

[7] Y. Zhang, S. Wang, G. Ji, and P. Phillips, “Fruit classiﬁcation using
computer vision and feedforward neural network,” Journal of Food
Engineering, vol. 143, pp. 167–177, 2014.

[8] S. Wang, X. Yang, Y. Zhang, P. Phillips, J. Yang, and T.-F. Yuan,
“Identiﬁcation of Green, Oolong and Black Teas in China via Wavelet
Packet Entropy and Fuzzy Support Vector Machine,” Entropy, vol. 17,
no. 10, pp. 6663–6682, 2015.

11

[9] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.

[10] D. Decoste and B. Sch¨olkopf, “Training invariant support vector ma-
chines,” Machine learning, vol. 46, no. 1-3, pp. 161–190, 2002.
[11] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation applied to handwritten
zip code recognition,” Neural computation, vol. 1, no. 4, pp. 541–551,
1989.

[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.

[13] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning,

vol. 20, no. 3, pp. 273–297, 1995.

[14] Y.-W. Chang, C.-J. Hsieh, K.-W. Chang, M. Ringgaard, and C.-J. Lin,
“Training and testing low-degree polynomial data mappings via linear
SVM,” Journal of Machine Learning Research, vol. 11, no. Apr, pp.
1471–1490, 2010.

[15] J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor,
and J. Vandewalle, Least Squares Support Vector Machines. World
Scientiﬁc, Singapore, 2002.

[16] M. Signoretto, Q. T. Dinh, L. De Lathauwer, and J. A. K. Suykens,
“Learning with tensors: a framework based on convex optimization and
spectral regularization,” Machine Learning, vol. 94, no. 3, pp. 303–351,
2014.

[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempit-
sky, “Speeding-up convolutional neural networks using ﬁne-tuned cp-
decomposition,” arXiv preprint arXiv:1412.6553, 2014.

[18] A. Novikov, D. Podoprikhin, A. Osokin, and D. P. Vetrov, “Tensoriz-
ing neural networks,” in Advances in Neural Information Processing
Systems, 2015, pp. 442–450.

[19] A. Novikov, M. Troﬁmov, and I. Oseledets, “Exponential machines,”

arXiv preprint arXiv:1605.03795, 2016.

[20] E. M. Stoudenmire and D. J. Schwab, “Supervised learning with
quantum-inspired tensor networks,” arXiv preprint arXiv:1605.05775,
2016.

[21] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,”

SIAM review, vol. 51, no. 3, pp. 455–500, 2009.

[22] I. Oseledets, “Tensor-train decomposition,” SIAM Journal on Scientiﬁc

Computing, vol. 33, no. 5, pp. 2295–2317, 2011.

[23] I. Oseledets and E. Tyrtyshnikov, “TT-cross approximation for multidi-
mensional arrays,” Linear Algebra and its Applications, vol. 432, no. 1,
pp. 70–88, 2010.

[24] D. Savostyanov and I. Oseledets, “Fast adaptive interpolation of multi-
dimensional arrays in tensor train format,” in 2011 7th International
Workshop on Multidimensional (nD) Systems (nDs).
IEEE, 2011, pp.
1–8.

[25] S. Theodoridis and K. Koutroumbas, Pattern Recognition, Fourth Edi-

tion, 4th ed. Academic Press, 2008.

[26] S. R. White, “Density matrix formulation for quantum renormalization
groups,” Physical Review Letters, vol. 69, no. 19, p. 2863, 1992.
[27] S. Holtz, T. Rohwedder, and R. Schneider, “The alternating linear
scheme for tensor optimization in the tensor train format,” SIAM Journal
on Scientiﬁc Computing, vol. 34, no. 2, pp. A683–A713, 2012.
[28] T. Rohwedder and A. Uschmajew, “On local convergence of alternating
schemes for optimization of convex problems in the tensor train format,”
SIAM Journal on Numerical Analysis, vol. 51, no. 2, pp. 1134–1162,
2013.

[29] F. Lauer, C. Y. Suen, and G. Bloch, “A trainable feature extractor for
handwritten digit recognition,” Pattern Recognition, vol. 40, no. 6, pp.
1816–1824, 2007.

[30] J. A. K. Suykens, “Deep restricted kernel machines using conjugate
feature duality,” Neural Computation, vol. 29, no. 8, pp. 2123–2163,
2017.

Parallelized Tensor Train Learning of Polynomial
Classiﬁers
Zhongming Chen∗, Kim Batselier†, Johan A.K. Suykens‡ and Ngai Wong†

1

7
1
0
2
 
v
o
N
 
6
 
 
]

G
L
.
s
c
[
 
 
4
v
5
0
5
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract—In pattern classiﬁcation, polynomial classiﬁers are
well-studied methods as they are capable of generating complex
decision surfaces. Unfortunately, the use of multivariate polyno-
mials is limited to kernels as in support vector machines, because
polynomials quickly become impractical for high-dimensional
problems. In this paper, we effectively overcome the curse of
dimensionality by employing the tensor train format to represent
a polynomial classiﬁer. Based on the structure of tensor trains,
two learning algorithms are proposed which involve solving
different optimization problems of low computational complex-
ity. Furthermore, we show how both regularization to prevent
overﬁtting and parallelization, which enables the use of large
training sets, are incorporated into these methods. The efﬁciency
and efﬁcacy of our tensor-based polynomial classiﬁer are then
demonstrated on the two popular datasets USPS and MNIST.

Index Terms—Supervised learning, tensor train, pattern clas-

siﬁcation, polynomial classiﬁer.

I. INTRODUCTION

Pattern classiﬁcation is the machine learning task of iden-
tifying to which category a new observation belongs, on the
basis of a training set of observations whose category mem-
bership is known. This type of machine learning algorithm
that uses a known training dataset to make predictions is
called supervised learning, which has been extensively studied
and has wide applications in the ﬁelds of bioinformatics
[1], computer-aided diagnosis (CAD) [2], machine vision [3],
speech recognition [4], handwriting recognition [5], spam
detection and many others [6], [7], [8]. Usually, different kinds
of learning methods use different models to generalize from
training examples to novel test examples.

As pointed out in [9], [10], one of the important invariants
in these applications is the local structure: variables that are
spatially or temporally nearby are highly correlated. Local
correlations beneﬁt extracting local features because conﬁgu-
rations of neighboring variables can be classiﬁed into a small
number of categories (e.g. edges, corners...). For instance, in
handwritten character recognition, correlations between image
pixels that are nearby tend to be more reliable than the
ones of distant pixels. Learning methods incorporating this
kind of prior knowledge often demonstrate state-of-the-art
performance in practical applications. One popular method
for handwritten character recognition is using convolutional
neural networks (CNNs) [11], [12] which are variations of

∗Department of Mathematics, School of Science, Hangzhou Dianzi Uni-

versity, Hangzhou 310018, China. Email: czm183015@126.com.

†Department of Electrical and Electronic Engineering, The University of

Hong Kong. Email: {kimb, nwong}@eee.hku.hk.

‡KU Leuven, ESAT, STADIUS. B-3001 Leuven, Belgium. Email:

jo-

han.suykens@esat.kuleuven.be.

multilayer perceptrons designed to use minimal amounts of
preprocessing. In this model, each unit in a layer receives
inputs from a set of units located in a small neighborhood
in the previous layer, and these mappings share the same
weight vector and bias in a given convolutional layer. An
important component of a CNN are the pooling layers, which
implement a nonlinear form of down-sampling. In this way,
the amount of parameters and computational load are reduced
in the network. Another popular method uses support vector
machines (SVMs) [13], [14]. The original ﬁnite-dimensional
feature space is mapped into a much higher-dimensional
space, where the inner product is easily computed through the
‘kernel trick’. By considering the Wolfe dual representation,
one can ﬁnd the maximum-margin hyperplane to separate the
examples of different categories in that space. However, it is
worth mentioning that these models require a large amount of
memory and a long processing time to train the parameters.
For instance, if there are thousands of nodes in the CNN,
the weight matrices of fully-connected layers are of the order
of millions. The major limitation of basic SVMs is the high
computational complexity which is at least quadratic with the
dataset size. One way to deal with large datasets in the SVM-
framework is by using a ﬁxed-size least squares SVM (ﬁxed-
size LS-SVM) [15], which approximates the kernel mapping
in such a way that the problem can be solved in the primal
space.

II. TENSORS IN MACHINE LEARNING

Tensors are a multidimensional generalization of matrices
to higher orders and have recently gained attention in the
ﬁeld of machine learning. The classiﬁcation via tensors, as
opposed to matrices or vectors, was ﬁrst considered in [16], by
extending the concept of spectral regularization for matrices to
tensors. The tensor data is assumed to satisfy a particular low-
rank Tucker decomposition, which unfortunately still suffers
from an exponential storage complexity. Other work has
focused speeding-up the convolution operation in CNNs [17]
by approximating this operation with a low-rank polyadic
decomposition of a tensor. In [18], the weight matrices of
fully-connected layers of neural networks are represented
by tensor trains (TTs), effectively reducing the number of
parameters. TTs have also been used to represent nonlinear
predictors [19] and classiﬁers [20]. The key idea here is always
to approximate a mapping that is determined by an exponential
number of parameters nd by a TT with a storage complexity
of dnr2 parameters. To our knowledge, this idea has not yet
been applied to polynomial classiﬁers that also suffer from the

curse of dimensionality. The usual approach to circumvent the
exponential number of polynomial coefﬁcients would be to
use SVMs with a polynomial kernel and solve the problem
in the dual space. In this article, we exploit
the efﬁcient
representation of a multivariate polynomial as a TT in order to
avoid the curse of dimensionality, allowing us to work directly
in the feature space. The main contributions are:

• We derive a compact description of a polynomial clas-
siﬁer using the TT format, avoiding the curse of dimen-
sionality.

• Two efﬁcient learning algorithms are proposed by exploit-

ing the TT structure.

• Both regularization and a parallel implementation are
incorporated into our methods, thus avoiding overﬁtting
and allowing the use of large training datasets.

This paper is organized as follows. In Section III, we give
a brief introduction to tensor basics, including the TT de-
composition, important tensor operations and properties. The
framework of TT learning for pattern classiﬁcation is presented
in Section IV. Based on different loss functions, two efﬁcient
learning algorithms are proposed in Section V, together with a
discussion on regularization and parallelization. In Section VI,
we test our algorithms on two popular datasets: USPS and
MNIST and compare their performance with polynomial clas-
siﬁers trained with LS-SVMs [15]. Finally, some conclusions
and further work are summarized in Section VII.

Throughout this paper, we use small letters x, y, . . . , for
scalars, small bold letters x, y, . . . , for vectors, capital letters
A, B, . . . , for matrices, and calligraphic letters A, B, . . . , for
tensors. The transpose of a matrix A or vector x is denoted by
A(cid:62) and x(cid:62), respectively. The identity matrix of dimension n is
denoted by In. A list of abbreviations used here is summarized
in Table I.

TABLE I
LIST OF ABBREVIATIONS

Tensor Train
Convolutional Neural Network

TT
CNN
SVM Support Vector Machine
TTLS
TTLR
USPS

Tensor Train learning by Least Squares
Tensor Train learning by Logistic Regression
US Postal Service database

MNIST Modiﬁed NIST database

III. PRELIMINARIES

A. Tensors and pure-power-n polynomials

A real dth-order or d-way tensor is a multidimensional array
A ∈ Rn1×n2×···×nd that generalizes the notions of vectors
and matrices to higher orders. Each of the entries Ai1i2···id
is determined by d indices. The numbers n1, n2, . . . , nd are
called the dimensions of the tensor. An example tensor with
dimensions 4, 3, 2 is shown in Fig. 1. We now give a brief
introduction to some required tensor operations and properties,
more information can be found in [21].

2

Fig. 1. An example tensor A = (Ai1i2i3 ) ∈ R4×3×2, where i1, i2, i3
denote the indices for each mode respectively.

The k-mode product B = A ×k U of a tensor A ∈

Rn1×n2×···×nd and a matrix U ∈ Rn(cid:48)

k×nk is deﬁned by

Bi1···ik−1jik+1···id =

Ai1···ik−1ikik+1···id Ujik ,

(1)

nk(cid:88)

ik=1

and B ∈ Rn1×···×nk−1×n(cid:48)
k×nk+1×···×nd . In particular, given
a d-way tensor A ∈ Rn×n×···×n and a vector x ∈ Rn, the
multidimensional contraction, denoted by Axd, is the scalar

Axd = A ×1 x(cid:62) ×2 x(cid:62) ×3 · · · ×d x(cid:62),

(2)

which is obtained as a homogeneous polynomial of x ∈ Rn
with degree d. The inner product of two same-sized tensors
A, B ∈ Rn1×n2×···×nd is the sum of the products of their
entries, i.e.,

(cid:104)A, B(cid:105) =

· · ·

Ai1i2···id Bi1i2···id .

(3)

n1(cid:88)

n2(cid:88)

nd(cid:88)

i1=1

i2=1

id=1

The Frobenius norm of a tensor A ∈ Rn1×n2×···×nd is given
by

(cid:107)A(cid:107)F = (cid:112)(cid:104)A, A(cid:105).
The vectorization of a tensor A ∈ Rn1×n2×···×nd is de-
noted by vec(A) and maps the tensor element with indices
(i1, i2, . . . , id) to the vector element with index i where

(4)

i = i1 + (i2 − 1)n1 + · · · + (id − 1)

nk.

d−1
(cid:89)

k=1

Given d vectors x(i) ∈ Rni, i = 1, 2, . . . , d, their outer product
is denoted by x(1) ◦ x(2) ◦ · · · ◦ x(d), which is a tensor in
Rn1×n2×···×nd such that its entry with indices (i1, i2, . . . , id)
is equal to the product of the corresponding vector elements,
namely, x(1)
i1

. It follows immediately that

· · · x(d)
id

x(2)
i2

vec(x(1) ◦ x(2) ◦ · · · ◦ x(d)) = x(d) ⊗ x(d−1) ⊗ · · · ⊗ x(1), (5)

where the symbol “⊗” denotes the Kronecker product.

We now illustrate how to represent a polynomial by using
tensors. Denote by R[x] the polynomial ring in d variables
x = (x1, x2, . . . , xd)(cid:62) with coefﬁcients in the ﬁeld R.
Deﬁnition 1. Given a vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd, a
polynomial f ∈ R[x] with d variables is called pure-power-˜n

f (x) = A ×1 v(x1)(cid:62) ×2 v(x2)(cid:62) ×3 · · · ×d v(xd)(cid:62).

(7)

rk ≤ min(

ni,

ni),

k = 1, 2, . . . , d − 1.

k
(cid:89)

d
(cid:89)

i=1

i=k+1

if the degree of f is at most ˜ni with respect to each variable
xi, i = 1, 2, . . . , d.
Example 1. The polynomial f = 4x1 +x3
is a pure-power-˜n polynomial with ˜n = (3, 1, 2).

1 −2x1x2x3 −7x2x2
3

The set of all pure-power-˜n polynomials with the degree
vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd is denoted by R[x]˜n. For
any f (x) ∈ R[x]˜n, there are a total of (cid:81)d
k=1(˜nk + 1) distinct
monomials

d
(cid:89)

k=1

xik−1
k

,

1 ≤ ik ≤ ˜nk + 1,

k = 1, 2, . . . , d.

For x = (x1, x2, . . . , xd)(cid:62) ∈ Rd, denote by {v(xk)}d
Vandermonde vectors

k=1 the

v(xk) := (1, xk, . . . , x˜nk

k )(cid:62) ∈ R˜nk+1.

(6)

follows that

It
there is a one-to-one mapping between
pure-power-˜n polynomials and tensors. To be speciﬁc, for
any f (x) ∈ R[x]˜n,
there exists a unique tensor A ∈
R(˜n1+1)×(˜n2+1)×···×(˜nd+1) such that

1, x3

1), v(x2) = (1, x2), v(x3) = (1, x3, x2

Example 2. We revisit the polynomial f from Example 1
and illustrate its corresponding tensor representation. Since
˜n = (3, 1, 2), we construct the Vandermonde vectors v(x1) =
(1, x1, x2
3). The
nonzero entries of the corresponding 4×2×3 tensor A are then
A211 = 4, A411 = 1, A222 = −2, A123 = −7. The indices of
the tensor A are easily found from grouping together corre-
sponding indices of the Vandermonde vectors. For example,
the tensor index 123 corresponding with the monomial x2x2
3
is found from v(x1)1 = 1, v(x2)2 = x2, v(x3)3 = x2
3.

B. Tensor trains

It is well known that the number of tensor elements grows
exponentially with the order d. Even when the dimensions
are small, the storage cost for all elements is prohibitive for
large d. The TT decomposition [22] gives an efﬁcient way (in
storage and computation) to overcome this so-called curse of
dimensionality.

The main idea of the TT decomposition is to re-express
the entries of a tensor A ∈ Rn1×n2×···×nd as a product of
matrices

Ai1i2···id = G1(i1)G2(i2) · · · Gd(id),

(8)

where Gk(ik) is an rk−1 × rk matrix for each index ik, also
called the TT-core. To turn the matrix-by-matrix product (8)
into a scalar, boundary conditions r0 = rd = 1 have to be
introduced. The quantities {rk}d
k=0 are called the TT-ranks.
Note that each core Gk is a third-order tensor with dimensions
rk−1, nk and rk. The TT-decomposition for a tensor A ∈
Rn1×n2×n3 is illustrated in Fig. 2. The most common way
to convert a given tensor A into a TT would be the TT-SVD
algorithm [22, p. 2301].

Example 3. TT-SVD algorithm [22, p. 2301]. Using the TT-
SVD algorithm, we can convert the tensor A from Example

3

Fig. 2. The TT decomposition for a tensor in Rn1×n2×n3 .

2 into a TT that consists of TT-cores G1 ∈ R1×4×3, G2 ∈
R3×2×3, G3 ∈ R3×3×1.

Note that throughout this article, we will not need to use
the TT-SVD algorithm. Instead, we will initialize the TT-cores
randomly and iteratively update the cores one-by-one in an
alternating fashion. It turns out that if all TT-ranks are bounded
by r, the storage of the TT grows linearly with the order d as
O(dnr2), where n = max{n1, n2, . . . , nd}.

Proposition 1 (Theorem 2.1 of [23]). For any tensor A ∈
Rn1×n2×···×nd , there exists a TT-decomposition with TT-ranks

We also mention that the TT representation of a tensor is
not unique. For instance, let Q be an orthogonal matrix in
Rr1×r1, namely, QQ(cid:62) = Q(cid:62)Q = Ir1. Then the tensor A in
(8) also has the TT-decomposition

Ai1i2···id = G(cid:48)

1(i1)G(cid:48)

2(i2) · · · Gd(id),

(9)

where

1(i1) = G1(i1)Q, G(cid:48)
G(cid:48)

2(i2) = Q(cid:62)G2(i2).

Numerical stability of our learning algorithms is guaranteed
by keeping all the TT-cores left-orthogonal or right-orthogonal
[24], which is achieved through a sequence of QR decompo-
sitions as explained in Section V.

Deﬁnition 2. The rk−1 × nk × rk core Gk is called left-
orthogonal if

and the rk−1 × nk × rk core Gk is called right-orthogonal if

nk(cid:88)

ik=1

nk(cid:88)

ik=1

Gk(ik)(cid:62)Gk(ik) = Irk ,

Gk(ik)Gk(ik)(cid:62) = Irk−1.

As stated before, the structure of a TT also beneﬁts the

computation of the general multidimensional contraction:

f = A ×1 (v(1))(cid:62) ×2 (v(2))(cid:62) ×3 · · · ×d (v(d))(cid:62),

(10)

where A ∈ Rn1×n2×···×nd and v(i) = (v(i)
ni )(cid:62) ∈
Rni, i = 1, 2, . . . , d. If a tensor A is given in the TT-
format (8), then we have

2 , . . . , v(i)

1 , v(i)

f =

d
(cid:89)

nk(cid:88)

k=1

ik=1

v(k)
ik

Gk(ik).

(11)

The described procedure for fast TT contraction is summarized
in Algorithm 1. In order to simplify the analysis on the
computational complexity of Algorithm 1, we assume that
r1 = r2 = · · · = rd−1 = r and n1 = n2 = · · · = nd = n.
There are two required steps to compute the contraction of
a TT with vector. First, we need to construct d matrices
V (k) by contracting the TT-cores Gk with the vectors v(k).
This operation is equivalent with d matrix-vector products
with a total computational cost of approximately O(dr2n)
ﬂops. Fortunately, the contraction of one TT-core is completely
independent from the other contractions and hence can be
done in parallel over d processors, reducing the computational
complexity to O(r2n) per processor. Maximal values for r and
n in our experiments are 10 and 4, respectively, so that the
contraction of one TT-core is approximately equivalent with
the product of a 100 × 4 matrix with a 4 × 1 vector. The ﬁnal
step in Algorithm 1 is the product of all matrices V (k) with
a total computational complexity of O(dr2). If we again set
r = 10, n = 4, then this ﬁnal step in Algorithm 1 is equivalent
with the product of a 100×40 matrix with a 40×1 vector. For
more basic operations implemented in the TT-format, such as
tensor addition and computing the Frobenius norm, the reader
is referred to [22].

Algorithm 1 Fast TT contraction [22]
Input: Vectors v(k) ∈ Rnk , k = 1, 2, . . . , d and a tensor A

in the TT-format with cores Gk

Output: The multidimensional contraction f in (10)

ik=1 v(k)

ik

Gk(ik)

%Computed in parallel

1: for k = 1 : d do
V (k) = (cid:80)nk
2:
3: end for
4: f := V (1)
5: for k = 2 : d do
f := f V (k)
6:
7: end for
8: return f

IV. TT LEARNING

It is easy for us to recognize a face, understand spoken
words, read handwritten characters and identify the gender
of a person. Machines, however, make decisions based on
data measured by a large number of sensors. In this section,
we present the framework of TT learning. Like most pattern
recognition systems [25], our TT learning method consists in
dividing the system into three main modules, shown in Fig. 3.
The ﬁrst module is called feature extraction, which is of
paramount importance in any pattern classiﬁcation problem.
The goal of this module is to build features via transformations
of the raw input, namely, the original data measured by a large
number of sensors. The basic reasoning behind transform-
based features is that an appropriately chosen transforma-
tion can exploit and remove information redundancies, which
usually exist in the set of samples obtained by measuring
devices. The set of features exhibit high information packaging
properties compared with the original input samples. This
means that most of the classiﬁcation-related information is

4

compressed into a relatively small number of features, leading
to a reduction of the necessary feature space dimension.
Feature extraction beneﬁts training the classiﬁer in terms of
memory and computation, and also alleviates the problem of
overﬁtting since we get rid of redundant information. To deal
with the task of feature extraction, some linear or nonlinear
transformation techniques are widely used. For example, the
Karhunen-Lo`eve transform, related to principal component
analysis (PCA), is one popular method for feature generation
and dimensionality reduction. A nonlinear kernel version of
the classical PCA is called kernel PCA, which is an extension
of PCA using kernel methods. The discrete Fourier transform
(DFT) can be another good choice due to the fact that for
many practical applications, most of the energy lies in the
low-frequency components. Compared with PCA, the basis
vectors in the DFT are ﬁxed and problem-dependent, which
leads to a low computational complexity.

The second module, the TT classiﬁer, is the core of TT
learning. The purpose of this module is to mark a new
observation based on its features generated by the previous
module. As will be discussed, the task of pattern classiﬁcation
can be divided into a sequence of binary classiﬁcations. For
each particular binary classiﬁcation, the TT classiﬁer assigns
to each new observation a score that indicates which class it
belongs to. In order to construct a good classiﬁer, we exploit
the fact that we know the labels for each sample of a given
dataset. The TT classiﬁer is trained optimally with respect
to an optimality criterion. In some ways, the TT classiﬁer
can be regarded as a kind of generalized linear classiﬁer,
it does a linear classiﬁcation in a higher dimensional space
generated by the items of a given pure-power polynomial. The
local information is encoded by the products of features. In
contrast to kernel-based SVM classiﬁers that work in the dual
space, the TT classiﬁer is able to work directly in the high
dimensional space by exploiting the TT-format. Similar with
the backpropagation algorithm for multilayer perceptrons, the
structure of a TT allows for updating the cores in an alternating
way. In the next section, we will describe the training of two
TT classiﬁers through the optimization of two different loss
functions.

The last module in Fig. 3 is the decision module that
decides which category a new observation belongs to. For
binary classiﬁcation, decisions are made according to the
sign of the score assigned by the TT classiﬁer, namely, the
decision depends on the value of corresponding discriminant
function. In an m-class problem, there are several strategies to
decompose it into a sequence of binary classiﬁcation problems.
A straightforward extension is the one-against-all, where m
binary classiﬁcation problems are involved. We seek to design
discriminant functions {gi(x)}m
i=1 so that gi(x) > gj(x),
∀j (cid:54)= i if x belongs to the ith class. Classiﬁcation is then
achieved according to the rule:

assign x to the ith class if i = argmaxk gk(x).
An alternative technique is the one-against-one, where we
need to consider m(m − 1)/2 pairs of classes. The decision
is made on the basis of a majority vote. It means that each
classiﬁer casts one vote and the ﬁnal class is the one with

Fig. 3. Framework of TT learning.

the most votes. When the number m is too large, one can
also apply the technique of binary coding. It turns out that
only (cid:100)log2 m(cid:101) classiﬁers are needed, where (cid:100)·(cid:101) is the ceiling
operation. In this case, each class is represented by a unique
binary code word of length (cid:100)log2 m(cid:101). The decision is then
made on the basis of minimal Hamming distance.

V. LEARNING ALGORITHMS
For notational convenience, we deﬁne nk := ˜nk + 1
and continue to use this notation for the remainder of the
article. As stated before, TT classiﬁers are designed for binary
classiﬁcation. Given a set of N training examples of the form
j=1 such that x(j) ∈ Rd is the feature vector of
{(x(j), y(j))}N
the jth example and y(j) ∈ {−1, 1} is the corresponding class
label of x(j). Let ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd be the degree
vector. Each feature is then mapped to a higher dimensional
space generated by all corresponding pure-power-˜n monomials
through the mapping T : Rd → Rn1×n2×···×nd

5

Here we consider that the tensor A is expressed as a tensor
train with cores {Gk}d
k=1. The main idea of the TT learning
algorithms is to update the cores in an alternating way by
optimizing an appropriate loss function. Prior to updating the
TT-cores, the TT-ranks are ﬁxed and a particular initial guess
of {Gk}d
k=1 is made. The TT-ranks can be interpreted as tuning
parameters, higher values will result in a better ﬁt at the risk
of overﬁtting. It is straightforward to extend our algorithms by
means of the Density Matrix Renormalization Group (DMRG)
method [26] such that the TT-ranks are updated adaptively.
Each core is updated in the order

G1 → G2 → · · · → Gd → Gd−1 → · · · → G1 → · · ·

until convergence, which is guaranteed under certain condi-
tions as described in [27], [28]. It turns out that updating
one TT-core is equivalent with minimizing a loss function in
a small number of variables, which can be done in a very
efﬁcient manner. The following lemma shows how the inner
product (cid:104)T (x), A(cid:105) in the generic feature space is a linear
function in any of the TT-cores Gk.
Lemma 1. Given a vector ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd, let
T be the mapping deﬁned by (12), and let A be a TT with
cores Gk ∈ Rrk−1×nk×rk , k = 1, 2, . . . , d. For any x ∈ Rd
and k = 1, . . . , d, we have that

(cid:104)T (x), A(cid:105) = (cid:0)qk(x)(cid:62) ⊗ v(xk)(cid:62) ⊗ pk(x)(cid:1) vec(Gk),

(14)

T (x)i1i2···id =

xik−1
k

.

d
(cid:89)

k=1

(12)

where

For x = (x1, x2, . . . , xd)(cid:62) ∈ Rd, let {v(xk)}d
Vandermonde vectors deﬁned in (6). Clearly, we have

k=1 be the

p1(x) = 1, pk(x)
k≥2

=

k−1
(cid:89)

i=1

(cid:0)Gi ×2 v(xi)(cid:62)(cid:1) ∈ R1×rk−1,

T (x) = v(x1) ◦ v(x2) ◦ · · · ◦ v(xd).

(13)

and

This high-dimensional pure-power polynomial space beneﬁts
the learning task from the following aspects:

• all interactions between features are described by the

qk(x)
k<d

=

d
(cid:89)

i=k+1

monomials of pure-power polynomials;

Proof. By deﬁnition, we have

(cid:0)Gi ×2 v(xi)(cid:62)(cid:1) ∈ Rrk×1, qd(x) = 1.

• the dimension of the tensor space grows exponentially
k=1 nk, which increases the probability
training examples linearly into two-

with d, namely, (cid:81)d
of separating all
classes;

• the one-to-one mapping between pure-power polynomials
and tensors enables the use of tensor trains to lift the curse
of dimensionality.

With these preparations, our goal

is to ﬁnd a decision
hyperplane to separate these two-class examples in the tensor
space, also called the generic feature space. In other words,
like the inductive learning described in [16], we try to ﬁnd a
tensor A ∈ Rn1×n2×···×nd such that

y(j)(cid:104)T (x(j)), A(cid:105) > 0,

j = 1, 2, . . . , N.

Note that the bias is absorbed into the ﬁrst element of A. Note
that the learning problem can also be interpreted as ﬁnding a
pure-power-˜n polynomial g(x) such that

and

g(x(j)) > 0,

∀y(j) = 1,

g(x(j)) < 0,

∀y(j) = −1.

(cid:104)T (x), A(cid:105) = A ×1 v(x1)(cid:62) ×2 · · · ×d v(xd)(cid:62)

= (cid:0)G1 ×2 v(x1)(cid:62)(cid:1) · · · (cid:0)Gd ×2 v(xd)(cid:62)(cid:1)
= Gk ×1 pk(x) ×2 v(xk)(cid:62) ×3 qk(x)(cid:62)
= (cid:0)qk(x)(cid:62) ⊗ v(xk)(cid:62) ⊗ pk(x)(cid:1) vec(Gk)

for any k = 1, 2, . . . , d. This completes the proof.

Example 4. In this example we illustrate the advantageous
representation of a pure-power polynomial f as a TT. Suppose
we have a polynomial f with d = 10 and all degrees ˜ni =
9 (i = 1, . . . , 10). All coefﬁcients of f (x) can then be stored
into a 10-way tensor 10 × 10 × · · · × 10 tensor A such that
the evaluation of f in a particular x is given by (7). The TT-
representation of f consists of 10 TT-cores G1, . . . , G10, with
a storage complexity of O(100r2), where r is the maximal TT-
rank. This demonstrates the potential of the TT-representation
in avoiding the curse of dimensionality when the TT-ranks are
small.

for
Example
T (x), A, v(xk), qk(x), pk(x) for the following quadratic

5. Next, we

expressions

illustrate

the

6







T (x) =

1 +
2. Since d = 2 and ˜n1 = ˜n2 = 2, both T and A

polynomial in two variables f (x) = 1 + 3x1 − x2 − x2
7x1x2 + 9x2
are the following 3 × 3 matrices


x2
1
x2
2
x1 x1x2 x1x2
2
1x2
1 x2
x2
2
The TT-representation of A consists of a 1 × 3 × 3 tensor G1
and a 3×3×1 tensor G2. Suppose now that k = 2 and we want
to compute the evaluation of the polynomial f in a particular
x, which is (cid:104)T (x), A(cid:105). From Lemma 1 we then have that
(cid:104)T (x), A(cid:105) = (cid:0)q2(x)(cid:62) ⊗ v(x2)(cid:62) ⊗ p2(x)(cid:1) vec(G2),

1 −1
7
3
0
−1


9
0
 .
0

1x2 x2

 , A =



We have thus shown that updating the core Gk is equivalent
with solving a least squares optimization problem in rk−1nkrk
variables. Minimizing (17) with respect to Gk for any k =
1, . . . , d results in solving the linear system

(C (cid:62)

k Ck) vec(Gk) = C (cid:62)

k y.

(19)

Supposing r1 = r2 = · · · = rd−1 = r and n1 = n2 = · · · =
nd = n, then the computational complexity of solving (19) is
O((r2n)3). For the maximal values of r = 10 and n = 4 in
our experiments, this implies that we need to solve a linear
system of order 400, which takes about 0.01 seconds using
MATLAB on our desktop computer.

with

q2(x) = 1 ∈ R,
v(x2) = (cid:0)1 x2 x2
p2(x) = G1 ×2 v(x1)(cid:62) ∈ R1×3,
v(x1) = (cid:0)1 x1 x2

∈ R3,

∈ R3.

(cid:1)(cid:62)

(cid:1)(cid:62)

2

1

In what follows, we ﬁrst present two learning algorithms
based on different loss functions. These algorithms will learn
the tensor A directly in the TT-representation from a given
dataset. Two enhancements, namely, regularization for better
accuracy and parallelization for higher speed will be described
in the last two subsections.

A. TT Learning by Least Squares

Least squares estimation is the simplest and thus most
common estimation method. In the generic feature space, we
attempt to design a linear classiﬁer so that its desired output
is exactly 1 or −1. However, we have to live with errors, that
is, the true output will not always be equal to the desired one.
The least squares estimator is then found from minimizing the
following mean square error function

B. TT Learning by Logistic Regression

Since our goal is to ﬁnd a hyperplane to separate two-class
training examples in the generic feature space, we may not
care about the particular value of the output. Indeed, only
the sign of the output makes sense. This gives us the idea to
decrease the number of sign differences as much as possible
when updating the TT-cores, i.e., to minimize the number of
misclassiﬁed examples. However, this model is discrete so
that a difﬁcult combinatorial optimization problem is involved.
Instead, we try to ﬁnd a suboptimal solution in the sense of
minimizing a continuous cost function that penalizes misclas-
siﬁed examples. Here, we consider the logistic regression cost
function. First, consider the standard sigmoid function

σ(z) =

1
1 + e−z ,

z ∈ R,

where the output always takes values between 0 and 1. An
important property is that its derivative can be expressed by
the function itself, i.e.,

J(A) =

(cid:16)

(cid:104)T (x(j)), A(cid:105) − y(j)(cid:17)2

.

(15)

The logistic function for the jth example x(j) is given by

1
N

N
(cid:88)

j=1

σ(cid:48)(z) = σ(z)(1 − σ(z)).

hA(x(j)) := σ

(cid:104)T (x(j)), A(cid:105)

(cid:16)

(cid:17)

.

(20)

(21)

We now show how updating a TT-core Gk is equivalent with
solving a relatively small linear system. First, we deﬁne the
N × rk−1nkrk matrix




Ck =







qk(x(1))(cid:62) ⊗ v(x(1)
qk(x(2))(cid:62) ⊗ v(x(2)

k )(cid:62) ⊗ pk(x(1))
k )(cid:62) ⊗ pk(x(2))
...







qk(x(N ))(cid:62) ⊗ v(x(N )

k

)(cid:62) ⊗ pk(x(N ))

for any k = 1, 2, . . . , d. The matrix Ck is hence obtained
from the concatenation of the row vectors qk(x)(cid:62) ⊗v(xk)(cid:62) ⊗
pk(x) from (14) for N samples x(1), . . . , x(N ). It follows from
Lemma 1 that

J(A) =

(cid:107)Ck vec(Gk) − y(cid:107)2

1
N

We can also interpret the logistic function as the probability
that the example x(j) belongs to the class denoted by the
label 1. The predicted label ˜y(j) for x(j) is then obtained
according to the rule

(16)

(cid:40)

hA(x(j)) ≥ 0.5 ⇔ (cid:104)T (x(j)), A(cid:105) ≥ 0 → ˜y(j) = 1,
hA(x(j)) < 0.5 ⇔ (cid:104)T (x(j)), A(cid:105) < 0 → ˜y(j) = −1.

For a particular example x(j), we deﬁne the cost function as

Cost(x(j), A) =




− ln

(cid:16)

(cid:17)
hA(x(j))



− ln

(cid:16)

(cid:17)
1 − hA(x(j))

if y(j) = 1,

if y(j) = −1.

where

y = (y(1), y(2), . . . , y(N ))(cid:62) ∈ RN .

The goal now is to ﬁnd a tensor A such that hA(x(j)) is near
1 if y(j) = 1 or near 0 if y(j) = −1. As a result, the logistic

(17)

(18)

regression cost function for the whole training dataset is given
by

J(A) =

Cost(x(j), A)

1
N

N
(cid:88)

j=1

=

−1
N

N
(cid:88)

j=1

(cid:20) 1 + y(j)
2

(cid:16)

ln

hA(x(j))

+

(cid:17)

(22)

1 − y(j)
2

(cid:16)

ln

1 − hA(x(j))

(cid:17)(cid:21)

.

It is important to note that (22) is convex though the sigmoid
function is not. This guarantees that we can ﬁnd the globally
optimal solution instead of a local optimum.

From equation (21) and Lemma 1, one can see that the
function J(A) can also be regarded as a function of the core
Gk since

and

(cid:104)T (x(j)), A(cid:105) = Ck(j, :) vec(Gk)

where Ck(j,
:) denote the jth row vector of Ck deﬁned in
(16). It follows that updating the core Gk is equivalent with
solving a convex optimization problem in rk−1nkrk variables.
Let

hA =

(cid:16)
hA(x(1)), hA(x(2)), . . . , hA(x(N ))

(cid:17)(cid:62)

∈ RN (23)

and DA be the diagonal matrix in RN ×N with the jth
diagonal element given by hA(x(j)) (cid:0)1 − hA(x(j))(cid:1). By using
the property (20) one can derive the gradient and Hessian with
respect to Gk as

∇Gk J(A) =

C (cid:62)
k

hA −

(cid:18)

1
N

(cid:19)

y + 1
2

and

∇2
Gk

J(A) =

C (cid:62)

k DACk,

1
N

respectively, where y is deﬁned in (18) and 1 denotes the
all-ones vector in RN . Although we do not have a closed-
form solution to update the core Gk, the gradient and Hessian
allows us to ﬁnd the solution by efﬁcient iterative methods, e.g.
Newton’s method whose convergence is at least quadratic in a
neighbourhood of the solution. A quasi-Newton method, like
the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm, is
another good choice if the inverse of the Hessian is difﬁcult
to compute.

(24)

(25)

C. Regularization

The cost functions (15) and (22) of the two TT learning
algorithms do not have any regularization term, which may
result in overﬁtting and hence bad generalization properties of
the obtained TT classiﬁer. Next, we discuss how the addition
of a regularization term to (15) and (22) results in a small
modiﬁcation of the small optimization problem that needs to
be solved when updating the TT-cores Gk.

Consider the regularized optimization problem

˜J(A) = J(A) + γR(A),

(26)

7

where J(A) is given by (15) or (22), γ is a parameter that
balances the loss function and the regularization term. Here
we use the Tikhonov regularization, namely,

(cid:104)A, A(cid:105).

R(A) =

1
2
Thanks to the TT structure, the gradient of R(A) with respect
to the TT-core Gk can be equivalently rewritten as a linear
transformation of vec(Gk). In other words, there is a matrix
Dk ∈ Rrk−1nkrk×rk−1nkrk determined by the cores {Gj}j(cid:54)=k
such that ∇Gk R(A) = Dkvec(Gk). See Appendix A for more
details. It follows that

(27)

∇Gk

˜J(A) = ∇Gk J(A) + γDkvec(Gk)

∇2
Gk

˜J(A) = ∇2
Gk

J(A) + γDk.

These small modiﬁcations lead to small changes when updat-
ing the core Gk. For instance, the ﬁrst-order condition of (26)
for the least squares model results in solving the modiﬁed
linear system
(cid:18)

(cid:19)

C (cid:62)

k Ck +

γDk

vec(Gk) = C (cid:62)

k y,

(28)

N
2

when compared with the original linear system (19).

D. Orthogonalization and Parallelization

The matrix Ck from (16) needs to be reconstructed for each
TT-core Gk during the execution of the two TT learning algo-
rithms. Fortunately, this can be done efﬁciently by exploiting
the TT structure. In particular, after updating the core Gk in the
left-to-right sweep, the new row vectors {pk+1(x(j))}N
j=1 to
construct the next matrix Ck+1 can be easily computed from
pk+1(x(j)) = Gk ×1 pk(x(j)) ×2 v(x(j)

k )(cid:62).

Similarly, in the right-to-left sweep, the new column vectors
{qk−1(x(j))}N
j=1 to construct the next matrix Ck−1 can be
easily computed from

qk−1(x(j)) = Gk ×2 v(x(j)

k )(cid:62) ×3 qk(x(j))(cid:62).

To make the learning algorithms numerically stable, the
techniques of orthogonalization are also applied. The main
idea is to make sure that before updating the core Gk, the cores
G1, . . . , Gk−1 are left-orthogonal and the cores Gk+1, . . . , Gd
are right-orthogonal by a sequence of QR decompositions. In
this way, the condition number of the constructed matrix Ck
is upper bounded so that the subproblem is well-posed. After
updating the core Gk, we orthogonalize it with an extra QR
decomposition, and absorb the upper triangular matrix into
the next core (depending on the direction of updating). More
details on the orthogonalization step can be found in [27].

Another computational challenge is the potentially large
size N of the training dataset. Luckily, the dimension of the
optimization problem when updating Gk in the TT learning
algorithms is rk−1(nk + 1)rk, which is much smaller and
independent from N . We only need to compute the products
C (cid:62)
k DACk in (19), (24) and (25).
These computations are easily done in parallel. Speciﬁcally,

k hA and C (cid:62)

k Ck, C (cid:62)

k y, C (cid:62)

8

given a proper partition {Nl}L
we divide the large matrix Ck into several blocks, namely,

l=1 satisfying (cid:80)L

l=1 Nl = N ,

TABLE II
DATASET DESCRIPTION

Ck =

∈ RN ×rk−1(nk+1)rk ,

















C (1)
k
C (2)
k
...
C (L)
k

where C (l)
example, the product C (cid:62)

k ∈ RNl×rk−1nkrk , l = 1, 2, . . . , L. Then, for

k DACk can be computed by

C (cid:62)

k DACk =

(C (l)

k )(cid:62)D(l)

A C (l)
k ,

L
(cid:88)

l=1

where D(l)
A denotes the corresponding diagonal block. Each
term in the summation on the right-hand side of the above
equation can be computed over L distributed cores, with
a computational complexity of O(r4n2
kNl) for each core,
supposing rk−1 = rk = r. The other matrix products can
also be computed in a similar way.

We summarize our learning algorithms in Algorithm 2.
The two most computationally expensive steps are lines 5
and 7. As we mentioned before, solving (26) takes approx-
imately O((r2n)3) ﬂops. If the QR decomposition of line 7
is computed through Householder transformations, then the
computational complexity is approximately O(r3n2) ﬂops. For
the maximal values of n = 4 and r = 10 in our experiments,
this amounts to solving computing the inverse and the QR
factorization of a 400 × 400 matrix. Note that based on the
decision strategy, an m-class problem is decomposed into a
sequence of two-class problems whose TT classiﬁers can be
trained in parallel.

Algorithm 2 Tensor Train Learning Algorithm
Input: Training dataset of pairs {(x(j), y(j))}N

j=1, TT-ranks
k=1, degree vector ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd and

{rk}d−1
regularization parameter γ

Output: Tensor A in TT format with cores {Gk}d
1: Initialize right orthogonal cores {Gk}d

k=1
k=1 of prescribed

ranks

2: while termination condition is not satisﬁed do
3: %Left-to-right sweep
4:
5:

for k = 1, 2, . . . , d − 1 do

G∗
k ← ﬁnd the minimal solution of the regularized
optimization problem (26) with respect to Gk
Uk ← reshape(G∗
[Q, R] ← compute QR decomposition of Uk
Gk ← reshape(Q, rk−1, nk, rk)
Vk+1 ← R ∗ reshape(Gk+1, rk, nk+1, rk+1)
Gk+1 ← reshape(Vk+1, rk, nk+1, rk+1)

k, rk−1nk, rk)

6:
7:
8:
9:
10:

end for
Perform the right-to-left sweep

11:
12:
13: end while

Image size

Training size

Test size

USPS
MNIST

16 × 16
28 × 28

7291
60000

2007
10000

• The DMRG method [26] can also be used to update
the cores. This involves updating two cores at a time
so that the TT-ranks are adaptively determined by means
of a singular value decomposition (SVD). This may give
better performance at the cost of a higher computational
complexity. It also removes the need to ﬁx the TT-ranks
a priori.

• The local linear convergence of Algorithm 2 has been
established in [27], [28] under certain conditions. In
particular,
if the TT-ranks are correctly estimated for
convex optimization problems, then the obtained solution
is guaranteed to be the global optimum. When choosing
the TT-ranks, one should keep the upper bounds of the
TT-ranks from Proposition 1 in mind.

VI. EXPERIMENTS

In this section, we test our TT learning algorithms and
compare their performance with LS-SVMs with polynomial
kernels on two popular digit recognition datasets: USPS
and MNIST. All our algorithms were implemented in MAT-
LAB Version R2016a, which can be freely downloaded from
https://github.com/kbatseli/TTClassiﬁer. We compare our TT-
polynomial classiﬁers with a polynomial classiﬁer based on
LS-SVMs with a polynomial kernel. The LS-SVM-polynomial
classiﬁer was trained with the MATLAB LS-SVMlab tool-
box, which can be freely downloaded from http://www.esat.
kuleuven.be/sista/lssvmlab/. The numerical experiments were
done on a desktop PC with an Intel i5 quad-core processor
running at 3.3GHz and 16GB of RAM.

The US Postal Service (USPS) database1contains 9298
handwritten digits, including 7291 for training and 2007 for
testing. Each digit is a 16×16 grayscale image. It is known that
the USPS test set is rather difﬁcult and the human error rate is
2.5%. The Modiﬁed NIST (MNIST) database2 of handwritten
digits has a training set of 60,000 examples, and a test set of
10,000 examples. It is a subset of a larger set available from
NIST. The digits have been size-normalized and centered in
a 28×28 image. The description of these two databases is
summarized in Table II.

Before extracting features of the handwritten digits, we ﬁrst
execute the pre-process of deskewing which is the process
of straightening an image that has been scanned or written
crookedly. By choosing a varying number d, the corresponding
feature vectors are then extracted from a pre-trained CNN
model 1-20-P-100-P-d-10, which represents a net with an input
images of size 28×28, a convolutional layer with 20 maps and

We end this section with the following remarks:
• Other loss functions can also be used in the framework
of TT learning provided that there exists an efﬁcient way
to solve the corresponding subproblems.

1The USPS database is downloaded from

http://statweb.stanford.edu/∼tibs/ElemStatLearn/data.html

2The MNIST database is downloaded from

http://yann.lecun.com/exdb/mnist/

5×5 ﬁlters, a max-pooling layer over non-overlapping regions
of size 2×2, a convolutional layer with 100 maps and 5×5
ﬁlters, a max-pooling layer over non-overlapping regions of
size 2×2, a convolutional layer with d maps and 4×4 ﬁlters,
and a fully connected output layer with 10 neurons. As the
variants of the well-known CNN model LeNet-5 [9], these
CNN models have been trained well on the MNIST database.
For an input 28×28 image, we get a feature vector of length
d from the third hidden layer. Note that for USPS database,
we must resize image data to the size of image input layer.
We mention that these techniques of deskewing and feature
extraction using pre-trained CNN model are widely used in
the literature [9], [10], [29].

For the decision module, we adopt the one-against-all deci-
sion strategy where ten TT classiﬁers are trained to separate
each digit from all
the others. In the implementation of
Algorithm 2, we normalize each initial core such that its
Frobenius norm is equal to one. The degree vector is given
by ˜n1 = · · · = ˜nd = ˜n. The TT-ranks are upper bounded
by rmax. The values of d, ˜n, rmax were chosen to minimize
the test error rate and to ensure that each of the subproblems
to update the TT-cores Gk could be solved in a reasonable
time. The dimension of each subproblem is at most n r2
max.
For example, in the USPS case, we ﬁrst ﬁxed the values of
n and rmax. We then ﬁxed the value of d and incremented
n and rmax to see whether this resulted in a better test error
rate. We use the optimality criterion

| ˜J(A+) − ˜J(A)|
| ˜J(A)|

≤ 10−2,

where A+ is the updated tensor from tensor A after one sweep.
And the maximum number of sweeps is 4, namely, 4(d−1) it-
erations through the entire training data are performed for each
session. To simplify notations, we use “TTLS” and “TTLR” to
denote the TT learning algorithms based on minimizing loss
functions (15) and (22), respectively. For these two models, the
regularization parameter γ is determined by the technique of
10-fold cross-validation. In other words, we randomly assign
the training data to ten sets of equal size. The parameter γ is
chosen so that the mean over all test errors is minimal.

The numerical results for USPS database and MNIST
database are reported in Tables III and IV, respectively. The
monotonic decrease is always seen when training the ten TT
classiﬁers. Fig. 4 shows the convergence of both TT learning
algorithms on the USPS data for the case d = 20, ˜n =
1, rmax = 8 when training the classiﬁer for the character “6”.
In addition, we also trained a polynomial classiﬁer using LS-
SVMs with polynomial kernels on these two databases. Using
the basic LS-SVM scheme, a training error rate of 0 and a test
error rate of 8.37% were obtained for the USPS dataset after
more than three and a half hours of computation. This runtime
includes the time required to tune the tuning parameters via
10-fold cross-validation. When using an RBF kernel with the
LS-SVM, it is possible to attain a test error of 2.14% [30],
but then the classiﬁer is not polynomial anymore. The MNIST
dataset resulted in consistent out-of-memory errors, which is
to be expected as the basic SVM scheme is not intended
for large data sets. We would also like to point out that a

9

Fig. 4. The convergence of TT learning algorithms.

test error of 1.1% is reported on the MNIST website for a
polynomial classiﬁer of degree 4 that uses conventional SVMs
and deskewing.

VII. CONCLUSION

This paper presents the framework of TT learning for pattern
classiﬁcation. For the ﬁrst time, TTs are used to represent
polynomial classiﬁers, enabling the learning algorithms to
work directly in the high-dimensional feature space. Two
efﬁcient learning algorithms are proposed based on different
loss functions. The numerical experiments show that each TT
classiﬁer is trained in up to several minutes with competitive
test errors. When compared with other polynomial classiﬁers,
the proposed learning algorithms can be easily parallelized
and have considerable advantage on storage and computation
time. We also mention that these results can be improved by
adding virtual examples [10]. Future improvements are the
implementation of on-line learning algorithms, together with
the extension of the binary TT classiﬁer to the multi-class case.

APPENDIX A
Given the degree vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd, let A ∈
Rn1×n2×···×nd be the tensor in TT format with cores Gk ∈
Rrk−1×nk×rk , k = 1, 2, . . . , d. To investigate the gradient of
R(A) in (27) with respect to the TT-core Gk, we give a small
variation (cid:15) to the ith element of vec(Gk), resulting in a new
tensor A(cid:15) given by

A(cid:15) = A + (cid:15)I (k)

,

i
where 1 ≤ i ≤ rk−1nkrk and I (k)
is the tensor which has
the same TT-cores with A except that the vectorization of the
core Gk is replaced by the unit vector in Rrk−1nkrk with the
ith element equal to 1 and 0 otherwise. Then we have

i

[∇Gk R(A)]i = lim
(cid:15)→0

R(A(cid:15)) − R(A)
(cid:15)
On the other hand, by the deﬁnition of vectorization, the ith
element of vec(Gk) ∈ Rrk−1nkrk is mapped from the tensor
element of Gk ∈ Rrk−1×nk×rk with indices (αk−1, jk, αk)
satisfying

= (cid:104)A, I (k)

(29)

(cid:105).

i

i = αk−1 + (jk − 1)rk−1 + (αk − 1)rk−1nk,

where 1 ≤ αk−1 ≤ rk−1, 1 ≤ jk ≤ nk and 1 ≤ αk ≤ rk.
Denote by E(αk−1,αk) the matrix in Rrk−1×rk such that the

TABLE III
NUMERICAL RESULTS FOR DATASET USPS

Train error

TTLS
Test error

Train error

TTLR
Test error

10

d

20
25
30
35
40
20
25
30
35
40
20
25
30
20
25
30
35
40

d

20
25
30
35
40
20
25
30
35
40
20
30
40
20
30
40
40
40
40

˜n

1
1
1
1
1
2
2
2
2
2
3
3
3
1
1
1
1
1

˜n

1
1
1
1
1
2
2
2
2
2
3
3
3
1
1
1
2
3
4

rmax
8
8
8
8
8
8
8
8
8
8
8
8
8
10
10
10
10
10

rmax
8
8
8
8
8
8
8
8
8
8
8
8
8
10
10
10
10
10
10

0.58%
0.47%
0.49%
0.38%
0.34%
0.49%
0.38%
0.51%
0.45%
0.34%
0.59%
0.49%
0.63%
0.56%
0.41%
0.51%
0.34%
0.44%

0.16%
0.15%
0.13%
0.19%
0.21%
0.19%
0.13%
0.19%
0.17%
0.17%
0.20%
0.22%
0.21%
0.20%
0.16%
0.17%
0.13%
0.18%
0.24%

Time(s)
4.33% 1.10×10
4.04% 1.57×10
3.84% 1.83×10
4.04% 2.21×10
3.89% 2.59×10
4.29% 2.04×10
4.29% 2.68×10
4.14% 3.28×10
4.38% 3.87×10
4.09% 4.48×10
4.58% 3.26×10
4.53% 4.07×10
4.43% 4.93×10
4.33% 1.91×10
3.94% 2.48×10
3.84% 3.09×10
3.94% 3.71×10
3.84% 4.36×10

Time(s)
7.14×10
0.96%
9.38×10
0.97%
0.86% 11.73×10
0.81% 13.95×10
0.94% 16.22×10
1.03% 12.08×10
1.00% 15.72×10
0.88% 18.94×10
0.86% 23.11×10
0.90% 25.67×10
1.10% 17.45×10
1.02% 27.21×10
0.97% 36.48×10
0.99% 10.14×10
0.88% 17.15×10
0.88% 24.05×10
0.93% 39.61×10
0.94% 59.66×10
1.02% 78.83×10

0.45%
0.27%
0.30%
0.26%
0.14%
0.55%
0.29%
0.47%
0.32%
0.21%
0.64%
0.56%
0.51%
0.44%
0.27%
0.25%
0.22%
0.33%

0.12%
0.09%
0.10%
0.03%
0.07%
0.14%
0.13%
0.10%
0.13%
0.17%
0.19%
0.21%
0.07%
0.19%
0.17%
0.13%
0.14%
0.10%
0.14%

Time(s)
4.87×10
4.33%
6.49×10
3.99%
8.18×10
3.99%
9.77×10
4.24%
3.74% 11.39×10
8.29×10
4.24%
4.14% 10.82×10
3.99% 13.34×10
4.38% 15.88×10
3.74% 18.39×10
4.33% 11.47×10
4.14% 14.98×10
4.09% 18.35×10
4.33% 17.69×10
3.94% 24.72×10
4.04% 31.14×10
4.29% 38.17×10
3.84% 45.24×10

Time(s)
0.99% 17.52×10
0.98% 22.96×10
0.86% 28.96×10
0.82% 34.55×10
0.83% 40.31×10
1.01% 29.45×10
0.98% 38.32×10
0.91% 46.77×10
0.83% 56.54×10
0.88% 64.68×10
1.00% 42.24×10
0.90% 65.89×10
0.89% 90.37×10
0.98% 36.66×10
0.90% 62.24×10
0.89% 83.62×10
0.92% 144.1×10
0.90% 207.7×10
0.91% 264.2×10

TABLE IV
NUMERICAL RESULTS FOR DATASET MNIST

Train error

TTLS
Test error

Train error

TTLR
Test error

element with index (αk−1, αk) equal to 1 and 0 otherwise. By
simple computation, one can obtain that

and

(cid:104)A, I (k)

(cid:105) =

i

(cid:88)

Ai1i2···id (I k

i )i1i2···id

i1,i2,...id
(cid:16)

= ak

(cid:17)
E(αk−1,αk) ⊗ Gk(jk)

bk,

where

ak =

k−1
(cid:89)

nl(cid:88)

l=1

il=1

(cid:2)Gl(il) ⊗ Gl(il)(cid:3) ∈ R1×r2

k−1

(31)

bk =

d
(cid:89)

nl(cid:88)

l=k+1

il=1

(cid:2)Gl(il) ⊗ Gl(il)(cid:3) ∈ Rr2

k×1.

(32)

(30)

Let a(1)
that

k , a(2)

k , . . . , a(rk−1)

k

∈ R1×rk−1 be the row vectors such

ak = (a(1)

k , a(2)

k , . . . , a(rk−1)

k

) ∈ R1×r2

k−1,

and let b(1)

k , b(2)

k , . . . , b(rk)

k

∈ Rrk×1 be the column vectors

such that

bk =

∈ Rr2

k×1,

















b(1)
k
b(2)
k
...
b(rk)
k

Combining (29) and (30) together, we have
[∇Gk R(A)]i = a(αk−1)
k
(cid:16)
(b(αk)
k

Gk(jk)b(αk)
)(cid:62) ⊗ (e(jk))(cid:62) ⊗ a(αk−1)

=

k

k

(cid:17)

vec(Gk),

where e(j) ∈ Rnk denotes the unit vector with the jth element
equal to 1 and 0 otherwise. If we deﬁne the rk−1nkrk ×
rk−1nkrk matrix




Dk =







(b(1)
(b(1)

k )(cid:62) ⊗ (e(1))(cid:62) ⊗ a(1)
k )(cid:62) ⊗ (e(1))(cid:62) ⊗ a(2)
...

k

k

(b(rk)
k

)(cid:62) ⊗ (enk )(cid:62) ⊗ a(rk−1)

k

,







(33)

it follows immediately that ∇Gk R(A) = Dkvec(Gk).

ACKNOWLEDGMENT
Zhongming Chen acknowledges the support of the Na-
tional Natural Science Foundation of China (Grant No.
11701132). Johan Suykens acknowledges support of ERC
AdG A-DATADRIVE-B (290923), KUL: CoE PFV/10/002
(OPTEC); FWO: G.0377.12, G.088114N, G0A4917N; IUAP
P7/19 DYSCO. Ngai Wong acknowledges the support of
the Hong Kong Research Grants Council under the General
Research Fund (GRF) project 17246416.

REFERENCES

[1] H.-B. Shen and K.-C. Chou, “Ensemble classiﬁer for protein fold pattern
recognition,” Bioinformatics, vol. 22, no. 14, pp. 1717–1722, 2006.
[2] H.-D. Cheng, X. Cai, X. Chen, L. Hu, and X. Lou, “Computer-aided
detection and classiﬁcation of microcalciﬁcations in mammograms: a
survey,” Pattern recognition, vol. 36, no. 12, pp. 2967–2991, 2003.
[3] B. Roberto, Template matching techniques in computer vision: theory

and practice. Wiley, Hoboken, 2009.

[4] B.-H. Juang, W. Hou, and C.-H. Lee, “Minimum classiﬁcation error
rate methods for speech recognition,” IEEE Transactions on Speech and
Audio processing, vol. 5, no. 3, pp. 257–265, 1997.

[5] L. Xu, A. Krzyzak, and C. Y. Suen, “Methods of combining multiple
classiﬁers and their applications to handwriting recognition,” IEEE
transactions on systems, man, and cybernetics, vol. 22, no. 3, pp. 418–
435, 1992.

[6] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern classiﬁcation.

John

Wiley & Sons, 2012.

[7] Y. Zhang, S. Wang, G. Ji, and P. Phillips, “Fruit classiﬁcation using
computer vision and feedforward neural network,” Journal of Food
Engineering, vol. 143, pp. 167–177, 2014.

[8] S. Wang, X. Yang, Y. Zhang, P. Phillips, J. Yang, and T.-F. Yuan,
“Identiﬁcation of Green, Oolong and Black Teas in China via Wavelet
Packet Entropy and Fuzzy Support Vector Machine,” Entropy, vol. 17,
no. 10, pp. 6663–6682, 2015.

11

[9] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.

[10] D. Decoste and B. Sch¨olkopf, “Training invariant support vector ma-
chines,” Machine learning, vol. 46, no. 1-3, pp. 161–190, 2002.
[11] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation applied to handwritten
zip code recognition,” Neural computation, vol. 1, no. 4, pp. 541–551,
1989.

[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.

[13] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning,

vol. 20, no. 3, pp. 273–297, 1995.

[14] Y.-W. Chang, C.-J. Hsieh, K.-W. Chang, M. Ringgaard, and C.-J. Lin,
“Training and testing low-degree polynomial data mappings via linear
SVM,” Journal of Machine Learning Research, vol. 11, no. Apr, pp.
1471–1490, 2010.

[15] J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor,
and J. Vandewalle, Least Squares Support Vector Machines. World
Scientiﬁc, Singapore, 2002.

[16] M. Signoretto, Q. T. Dinh, L. De Lathauwer, and J. A. K. Suykens,
“Learning with tensors: a framework based on convex optimization and
spectral regularization,” Machine Learning, vol. 94, no. 3, pp. 303–351,
2014.

[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempit-
sky, “Speeding-up convolutional neural networks using ﬁne-tuned cp-
decomposition,” arXiv preprint arXiv:1412.6553, 2014.

[18] A. Novikov, D. Podoprikhin, A. Osokin, and D. P. Vetrov, “Tensoriz-
ing neural networks,” in Advances in Neural Information Processing
Systems, 2015, pp. 442–450.

[19] A. Novikov, M. Troﬁmov, and I. Oseledets, “Exponential machines,”

arXiv preprint arXiv:1605.03795, 2016.

[20] E. M. Stoudenmire and D. J. Schwab, “Supervised learning with
quantum-inspired tensor networks,” arXiv preprint arXiv:1605.05775,
2016.

[21] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,”

SIAM review, vol. 51, no. 3, pp. 455–500, 2009.

[22] I. Oseledets, “Tensor-train decomposition,” SIAM Journal on Scientiﬁc

Computing, vol. 33, no. 5, pp. 2295–2317, 2011.

[23] I. Oseledets and E. Tyrtyshnikov, “TT-cross approximation for multidi-
mensional arrays,” Linear Algebra and its Applications, vol. 432, no. 1,
pp. 70–88, 2010.

[24] D. Savostyanov and I. Oseledets, “Fast adaptive interpolation of multi-
dimensional arrays in tensor train format,” in 2011 7th International
Workshop on Multidimensional (nD) Systems (nDs).
IEEE, 2011, pp.
1–8.

[25] S. Theodoridis and K. Koutroumbas, Pattern Recognition, Fourth Edi-

tion, 4th ed. Academic Press, 2008.

[26] S. R. White, “Density matrix formulation for quantum renormalization
groups,” Physical Review Letters, vol. 69, no. 19, p. 2863, 1992.
[27] S. Holtz, T. Rohwedder, and R. Schneider, “The alternating linear
scheme for tensor optimization in the tensor train format,” SIAM Journal
on Scientiﬁc Computing, vol. 34, no. 2, pp. A683–A713, 2012.
[28] T. Rohwedder and A. Uschmajew, “On local convergence of alternating
schemes for optimization of convex problems in the tensor train format,”
SIAM Journal on Numerical Analysis, vol. 51, no. 2, pp. 1134–1162,
2013.

[29] F. Lauer, C. Y. Suen, and G. Bloch, “A trainable feature extractor for
handwritten digit recognition,” Pattern Recognition, vol. 40, no. 6, pp.
1816–1824, 2007.

[30] J. A. K. Suykens, “Deep restricted kernel machines using conjugate
feature duality,” Neural Computation, vol. 29, no. 8, pp. 2123–2163,
2017.

Parallelized Tensor Train Learning of Polynomial
Classiﬁers
Zhongming Chen∗, Kim Batselier†, Johan A.K. Suykens‡ and Ngai Wong†

1

7
1
0
2
 
v
o
N
 
6
 
 
]

G
L
.
s
c
[
 
 
4
v
5
0
5
6
0
.
2
1
6
1
:
v
i
X
r
a

Abstract—In pattern classiﬁcation, polynomial classiﬁers are
well-studied methods as they are capable of generating complex
decision surfaces. Unfortunately, the use of multivariate polyno-
mials is limited to kernels as in support vector machines, because
polynomials quickly become impractical for high-dimensional
problems. In this paper, we effectively overcome the curse of
dimensionality by employing the tensor train format to represent
a polynomial classiﬁer. Based on the structure of tensor trains,
two learning algorithms are proposed which involve solving
different optimization problems of low computational complex-
ity. Furthermore, we show how both regularization to prevent
overﬁtting and parallelization, which enables the use of large
training sets, are incorporated into these methods. The efﬁciency
and efﬁcacy of our tensor-based polynomial classiﬁer are then
demonstrated on the two popular datasets USPS and MNIST.

Index Terms—Supervised learning, tensor train, pattern clas-

siﬁcation, polynomial classiﬁer.

I. INTRODUCTION

Pattern classiﬁcation is the machine learning task of iden-
tifying to which category a new observation belongs, on the
basis of a training set of observations whose category mem-
bership is known. This type of machine learning algorithm
that uses a known training dataset to make predictions is
called supervised learning, which has been extensively studied
and has wide applications in the ﬁelds of bioinformatics
[1], computer-aided diagnosis (CAD) [2], machine vision [3],
speech recognition [4], handwriting recognition [5], spam
detection and many others [6], [7], [8]. Usually, different kinds
of learning methods use different models to generalize from
training examples to novel test examples.

As pointed out in [9], [10], one of the important invariants
in these applications is the local structure: variables that are
spatially or temporally nearby are highly correlated. Local
correlations beneﬁt extracting local features because conﬁgu-
rations of neighboring variables can be classiﬁed into a small
number of categories (e.g. edges, corners...). For instance, in
handwritten character recognition, correlations between image
pixels that are nearby tend to be more reliable than the
ones of distant pixels. Learning methods incorporating this
kind of prior knowledge often demonstrate state-of-the-art
performance in practical applications. One popular method
for handwritten character recognition is using convolutional
neural networks (CNNs) [11], [12] which are variations of

∗Department of Mathematics, School of Science, Hangzhou Dianzi Uni-

versity, Hangzhou 310018, China. Email: czm183015@126.com.

†Department of Electrical and Electronic Engineering, The University of

Hong Kong. Email: {kimb, nwong}@eee.hku.hk.

‡KU Leuven, ESAT, STADIUS. B-3001 Leuven, Belgium. Email:

jo-

han.suykens@esat.kuleuven.be.

multilayer perceptrons designed to use minimal amounts of
preprocessing. In this model, each unit in a layer receives
inputs from a set of units located in a small neighborhood
in the previous layer, and these mappings share the same
weight vector and bias in a given convolutional layer. An
important component of a CNN are the pooling layers, which
implement a nonlinear form of down-sampling. In this way,
the amount of parameters and computational load are reduced
in the network. Another popular method uses support vector
machines (SVMs) [13], [14]. The original ﬁnite-dimensional
feature space is mapped into a much higher-dimensional
space, where the inner product is easily computed through the
‘kernel trick’. By considering the Wolfe dual representation,
one can ﬁnd the maximum-margin hyperplane to separate the
examples of different categories in that space. However, it is
worth mentioning that these models require a large amount of
memory and a long processing time to train the parameters.
For instance, if there are thousands of nodes in the CNN,
the weight matrices of fully-connected layers are of the order
of millions. The major limitation of basic SVMs is the high
computational complexity which is at least quadratic with the
dataset size. One way to deal with large datasets in the SVM-
framework is by using a ﬁxed-size least squares SVM (ﬁxed-
size LS-SVM) [15], which approximates the kernel mapping
in such a way that the problem can be solved in the primal
space.

II. TENSORS IN MACHINE LEARNING

Tensors are a multidimensional generalization of matrices
to higher orders and have recently gained attention in the
ﬁeld of machine learning. The classiﬁcation via tensors, as
opposed to matrices or vectors, was ﬁrst considered in [16], by
extending the concept of spectral regularization for matrices to
tensors. The tensor data is assumed to satisfy a particular low-
rank Tucker decomposition, which unfortunately still suffers
from an exponential storage complexity. Other work has
focused speeding-up the convolution operation in CNNs [17]
by approximating this operation with a low-rank polyadic
decomposition of a tensor. In [18], the weight matrices of
fully-connected layers of neural networks are represented
by tensor trains (TTs), effectively reducing the number of
parameters. TTs have also been used to represent nonlinear
predictors [19] and classiﬁers [20]. The key idea here is always
to approximate a mapping that is determined by an exponential
number of parameters nd by a TT with a storage complexity
of dnr2 parameters. To our knowledge, this idea has not yet
been applied to polynomial classiﬁers that also suffer from the

curse of dimensionality. The usual approach to circumvent the
exponential number of polynomial coefﬁcients would be to
use SVMs with a polynomial kernel and solve the problem
in the dual space. In this article, we exploit
the efﬁcient
representation of a multivariate polynomial as a TT in order to
avoid the curse of dimensionality, allowing us to work directly
in the feature space. The main contributions are:

• We derive a compact description of a polynomial clas-
siﬁer using the TT format, avoiding the curse of dimen-
sionality.

• Two efﬁcient learning algorithms are proposed by exploit-

ing the TT structure.

• Both regularization and a parallel implementation are
incorporated into our methods, thus avoiding overﬁtting
and allowing the use of large training datasets.

This paper is organized as follows. In Section III, we give
a brief introduction to tensor basics, including the TT de-
composition, important tensor operations and properties. The
framework of TT learning for pattern classiﬁcation is presented
in Section IV. Based on different loss functions, two efﬁcient
learning algorithms are proposed in Section V, together with a
discussion on regularization and parallelization. In Section VI,
we test our algorithms on two popular datasets: USPS and
MNIST and compare their performance with polynomial clas-
siﬁers trained with LS-SVMs [15]. Finally, some conclusions
and further work are summarized in Section VII.

Throughout this paper, we use small letters x, y, . . . , for
scalars, small bold letters x, y, . . . , for vectors, capital letters
A, B, . . . , for matrices, and calligraphic letters A, B, . . . , for
tensors. The transpose of a matrix A or vector x is denoted by
A(cid:62) and x(cid:62), respectively. The identity matrix of dimension n is
denoted by In. A list of abbreviations used here is summarized
in Table I.

TABLE I
LIST OF ABBREVIATIONS

Tensor Train
Convolutional Neural Network

TT
CNN
SVM Support Vector Machine
TTLS
TTLR
USPS

Tensor Train learning by Least Squares
Tensor Train learning by Logistic Regression
US Postal Service database

MNIST Modiﬁed NIST database

III. PRELIMINARIES

A. Tensors and pure-power-n polynomials

A real dth-order or d-way tensor is a multidimensional array
A ∈ Rn1×n2×···×nd that generalizes the notions of vectors
and matrices to higher orders. Each of the entries Ai1i2···id
is determined by d indices. The numbers n1, n2, . . . , nd are
called the dimensions of the tensor. An example tensor with
dimensions 4, 3, 2 is shown in Fig. 1. We now give a brief
introduction to some required tensor operations and properties,
more information can be found in [21].

2

Fig. 1. An example tensor A = (Ai1i2i3 ) ∈ R4×3×2, where i1, i2, i3
denote the indices for each mode respectively.

The k-mode product B = A ×k U of a tensor A ∈

Rn1×n2×···×nd and a matrix U ∈ Rn(cid:48)

k×nk is deﬁned by

Bi1···ik−1jik+1···id =

Ai1···ik−1ikik+1···id Ujik ,

(1)

nk(cid:88)

ik=1

and B ∈ Rn1×···×nk−1×n(cid:48)
k×nk+1×···×nd . In particular, given
a d-way tensor A ∈ Rn×n×···×n and a vector x ∈ Rn, the
multidimensional contraction, denoted by Axd, is the scalar

Axd = A ×1 x(cid:62) ×2 x(cid:62) ×3 · · · ×d x(cid:62),

(2)

which is obtained as a homogeneous polynomial of x ∈ Rn
with degree d. The inner product of two same-sized tensors
A, B ∈ Rn1×n2×···×nd is the sum of the products of their
entries, i.e.,

(cid:104)A, B(cid:105) =

· · ·

Ai1i2···id Bi1i2···id .

(3)

n1(cid:88)

n2(cid:88)

nd(cid:88)

i1=1

i2=1

id=1

The Frobenius norm of a tensor A ∈ Rn1×n2×···×nd is given
by

(cid:107)A(cid:107)F = (cid:112)(cid:104)A, A(cid:105).
The vectorization of a tensor A ∈ Rn1×n2×···×nd is de-
noted by vec(A) and maps the tensor element with indices
(i1, i2, . . . , id) to the vector element with index i where

(4)

i = i1 + (i2 − 1)n1 + · · · + (id − 1)

nk.

d−1
(cid:89)

k=1

Given d vectors x(i) ∈ Rni, i = 1, 2, . . . , d, their outer product
is denoted by x(1) ◦ x(2) ◦ · · · ◦ x(d), which is a tensor in
Rn1×n2×···×nd such that its entry with indices (i1, i2, . . . , id)
is equal to the product of the corresponding vector elements,
namely, x(1)
i1

. It follows immediately that

· · · x(d)
id

x(2)
i2

vec(x(1) ◦ x(2) ◦ · · · ◦ x(d)) = x(d) ⊗ x(d−1) ⊗ · · · ⊗ x(1), (5)

where the symbol “⊗” denotes the Kronecker product.

We now illustrate how to represent a polynomial by using
tensors. Denote by R[x] the polynomial ring in d variables
x = (x1, x2, . . . , xd)(cid:62) with coefﬁcients in the ﬁeld R.
Deﬁnition 1. Given a vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd, a
polynomial f ∈ R[x] with d variables is called pure-power-˜n

f (x) = A ×1 v(x1)(cid:62) ×2 v(x2)(cid:62) ×3 · · · ×d v(xd)(cid:62).

(7)

rk ≤ min(

ni,

ni),

k = 1, 2, . . . , d − 1.

k
(cid:89)

d
(cid:89)

i=1

i=k+1

if the degree of f is at most ˜ni with respect to each variable
xi, i = 1, 2, . . . , d.
Example 1. The polynomial f = 4x1 +x3
is a pure-power-˜n polynomial with ˜n = (3, 1, 2).

1 −2x1x2x3 −7x2x2
3

The set of all pure-power-˜n polynomials with the degree
vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd is denoted by R[x]˜n. For
any f (x) ∈ R[x]˜n, there are a total of (cid:81)d
k=1(˜nk + 1) distinct
monomials

d
(cid:89)

k=1

xik−1
k

,

1 ≤ ik ≤ ˜nk + 1,

k = 1, 2, . . . , d.

For x = (x1, x2, . . . , xd)(cid:62) ∈ Rd, denote by {v(xk)}d
Vandermonde vectors

k=1 the

v(xk) := (1, xk, . . . , x˜nk

k )(cid:62) ∈ R˜nk+1.

(6)

follows that

It
there is a one-to-one mapping between
pure-power-˜n polynomials and tensors. To be speciﬁc, for
any f (x) ∈ R[x]˜n,
there exists a unique tensor A ∈
R(˜n1+1)×(˜n2+1)×···×(˜nd+1) such that

1, x3

1), v(x2) = (1, x2), v(x3) = (1, x3, x2

Example 2. We revisit the polynomial f from Example 1
and illustrate its corresponding tensor representation. Since
˜n = (3, 1, 2), we construct the Vandermonde vectors v(x1) =
(1, x1, x2
3). The
nonzero entries of the corresponding 4×2×3 tensor A are then
A211 = 4, A411 = 1, A222 = −2, A123 = −7. The indices of
the tensor A are easily found from grouping together corre-
sponding indices of the Vandermonde vectors. For example,
the tensor index 123 corresponding with the monomial x2x2
3
is found from v(x1)1 = 1, v(x2)2 = x2, v(x3)3 = x2
3.

B. Tensor trains

It is well known that the number of tensor elements grows
exponentially with the order d. Even when the dimensions
are small, the storage cost for all elements is prohibitive for
large d. The TT decomposition [22] gives an efﬁcient way (in
storage and computation) to overcome this so-called curse of
dimensionality.

The main idea of the TT decomposition is to re-express
the entries of a tensor A ∈ Rn1×n2×···×nd as a product of
matrices

Ai1i2···id = G1(i1)G2(i2) · · · Gd(id),

(8)

where Gk(ik) is an rk−1 × rk matrix for each index ik, also
called the TT-core. To turn the matrix-by-matrix product (8)
into a scalar, boundary conditions r0 = rd = 1 have to be
introduced. The quantities {rk}d
k=0 are called the TT-ranks.
Note that each core Gk is a third-order tensor with dimensions
rk−1, nk and rk. The TT-decomposition for a tensor A ∈
Rn1×n2×n3 is illustrated in Fig. 2. The most common way
to convert a given tensor A into a TT would be the TT-SVD
algorithm [22, p. 2301].

Example 3. TT-SVD algorithm [22, p. 2301]. Using the TT-
SVD algorithm, we can convert the tensor A from Example

3

Fig. 2. The TT decomposition for a tensor in Rn1×n2×n3 .

2 into a TT that consists of TT-cores G1 ∈ R1×4×3, G2 ∈
R3×2×3, G3 ∈ R3×3×1.

Note that throughout this article, we will not need to use
the TT-SVD algorithm. Instead, we will initialize the TT-cores
randomly and iteratively update the cores one-by-one in an
alternating fashion. It turns out that if all TT-ranks are bounded
by r, the storage of the TT grows linearly with the order d as
O(dnr2), where n = max{n1, n2, . . . , nd}.

Proposition 1 (Theorem 2.1 of [23]). For any tensor A ∈
Rn1×n2×···×nd , there exists a TT-decomposition with TT-ranks

We also mention that the TT representation of a tensor is
not unique. For instance, let Q be an orthogonal matrix in
Rr1×r1, namely, QQ(cid:62) = Q(cid:62)Q = Ir1. Then the tensor A in
(8) also has the TT-decomposition

Ai1i2···id = G(cid:48)

1(i1)G(cid:48)

2(i2) · · · Gd(id),

(9)

where

1(i1) = G1(i1)Q, G(cid:48)
G(cid:48)

2(i2) = Q(cid:62)G2(i2).

Numerical stability of our learning algorithms is guaranteed
by keeping all the TT-cores left-orthogonal or right-orthogonal
[24], which is achieved through a sequence of QR decompo-
sitions as explained in Section V.

Deﬁnition 2. The rk−1 × nk × rk core Gk is called left-
orthogonal if

and the rk−1 × nk × rk core Gk is called right-orthogonal if

nk(cid:88)

ik=1

nk(cid:88)

ik=1

Gk(ik)(cid:62)Gk(ik) = Irk ,

Gk(ik)Gk(ik)(cid:62) = Irk−1.

As stated before, the structure of a TT also beneﬁts the

computation of the general multidimensional contraction:

f = A ×1 (v(1))(cid:62) ×2 (v(2))(cid:62) ×3 · · · ×d (v(d))(cid:62),

(10)

where A ∈ Rn1×n2×···×nd and v(i) = (v(i)
ni )(cid:62) ∈
Rni, i = 1, 2, . . . , d. If a tensor A is given in the TT-
format (8), then we have

2 , . . . , v(i)

1 , v(i)

f =

d
(cid:89)

nk(cid:88)

k=1

ik=1

v(k)
ik

Gk(ik).

(11)

The described procedure for fast TT contraction is summarized
in Algorithm 1. In order to simplify the analysis on the
computational complexity of Algorithm 1, we assume that
r1 = r2 = · · · = rd−1 = r and n1 = n2 = · · · = nd = n.
There are two required steps to compute the contraction of
a TT with vector. First, we need to construct d matrices
V (k) by contracting the TT-cores Gk with the vectors v(k).
This operation is equivalent with d matrix-vector products
with a total computational cost of approximately O(dr2n)
ﬂops. Fortunately, the contraction of one TT-core is completely
independent from the other contractions and hence can be
done in parallel over d processors, reducing the computational
complexity to O(r2n) per processor. Maximal values for r and
n in our experiments are 10 and 4, respectively, so that the
contraction of one TT-core is approximately equivalent with
the product of a 100 × 4 matrix with a 4 × 1 vector. The ﬁnal
step in Algorithm 1 is the product of all matrices V (k) with
a total computational complexity of O(dr2). If we again set
r = 10, n = 4, then this ﬁnal step in Algorithm 1 is equivalent
with the product of a 100×40 matrix with a 40×1 vector. For
more basic operations implemented in the TT-format, such as
tensor addition and computing the Frobenius norm, the reader
is referred to [22].

Algorithm 1 Fast TT contraction [22]
Input: Vectors v(k) ∈ Rnk , k = 1, 2, . . . , d and a tensor A

in the TT-format with cores Gk

Output: The multidimensional contraction f in (10)

ik=1 v(k)

ik

Gk(ik)

%Computed in parallel

1: for k = 1 : d do
V (k) = (cid:80)nk
2:
3: end for
4: f := V (1)
5: for k = 2 : d do
f := f V (k)
6:
7: end for
8: return f

IV. TT LEARNING

It is easy for us to recognize a face, understand spoken
words, read handwritten characters and identify the gender
of a person. Machines, however, make decisions based on
data measured by a large number of sensors. In this section,
we present the framework of TT learning. Like most pattern
recognition systems [25], our TT learning method consists in
dividing the system into three main modules, shown in Fig. 3.
The ﬁrst module is called feature extraction, which is of
paramount importance in any pattern classiﬁcation problem.
The goal of this module is to build features via transformations
of the raw input, namely, the original data measured by a large
number of sensors. The basic reasoning behind transform-
based features is that an appropriately chosen transforma-
tion can exploit and remove information redundancies, which
usually exist in the set of samples obtained by measuring
devices. The set of features exhibit high information packaging
properties compared with the original input samples. This
means that most of the classiﬁcation-related information is

4

compressed into a relatively small number of features, leading
to a reduction of the necessary feature space dimension.
Feature extraction beneﬁts training the classiﬁer in terms of
memory and computation, and also alleviates the problem of
overﬁtting since we get rid of redundant information. To deal
with the task of feature extraction, some linear or nonlinear
transformation techniques are widely used. For example, the
Karhunen-Lo`eve transform, related to principal component
analysis (PCA), is one popular method for feature generation
and dimensionality reduction. A nonlinear kernel version of
the classical PCA is called kernel PCA, which is an extension
of PCA using kernel methods. The discrete Fourier transform
(DFT) can be another good choice due to the fact that for
many practical applications, most of the energy lies in the
low-frequency components. Compared with PCA, the basis
vectors in the DFT are ﬁxed and problem-dependent, which
leads to a low computational complexity.

The second module, the TT classiﬁer, is the core of TT
learning. The purpose of this module is to mark a new
observation based on its features generated by the previous
module. As will be discussed, the task of pattern classiﬁcation
can be divided into a sequence of binary classiﬁcations. For
each particular binary classiﬁcation, the TT classiﬁer assigns
to each new observation a score that indicates which class it
belongs to. In order to construct a good classiﬁer, we exploit
the fact that we know the labels for each sample of a given
dataset. The TT classiﬁer is trained optimally with respect
to an optimality criterion. In some ways, the TT classiﬁer
can be regarded as a kind of generalized linear classiﬁer,
it does a linear classiﬁcation in a higher dimensional space
generated by the items of a given pure-power polynomial. The
local information is encoded by the products of features. In
contrast to kernel-based SVM classiﬁers that work in the dual
space, the TT classiﬁer is able to work directly in the high
dimensional space by exploiting the TT-format. Similar with
the backpropagation algorithm for multilayer perceptrons, the
structure of a TT allows for updating the cores in an alternating
way. In the next section, we will describe the training of two
TT classiﬁers through the optimization of two different loss
functions.

The last module in Fig. 3 is the decision module that
decides which category a new observation belongs to. For
binary classiﬁcation, decisions are made according to the
sign of the score assigned by the TT classiﬁer, namely, the
decision depends on the value of corresponding discriminant
function. In an m-class problem, there are several strategies to
decompose it into a sequence of binary classiﬁcation problems.
A straightforward extension is the one-against-all, where m
binary classiﬁcation problems are involved. We seek to design
discriminant functions {gi(x)}m
i=1 so that gi(x) > gj(x),
∀j (cid:54)= i if x belongs to the ith class. Classiﬁcation is then
achieved according to the rule:

assign x to the ith class if i = argmaxk gk(x).
An alternative technique is the one-against-one, where we
need to consider m(m − 1)/2 pairs of classes. The decision
is made on the basis of a majority vote. It means that each
classiﬁer casts one vote and the ﬁnal class is the one with

Fig. 3. Framework of TT learning.

the most votes. When the number m is too large, one can
also apply the technique of binary coding. It turns out that
only (cid:100)log2 m(cid:101) classiﬁers are needed, where (cid:100)·(cid:101) is the ceiling
operation. In this case, each class is represented by a unique
binary code word of length (cid:100)log2 m(cid:101). The decision is then
made on the basis of minimal Hamming distance.

V. LEARNING ALGORITHMS
For notational convenience, we deﬁne nk := ˜nk + 1
and continue to use this notation for the remainder of the
article. As stated before, TT classiﬁers are designed for binary
classiﬁcation. Given a set of N training examples of the form
j=1 such that x(j) ∈ Rd is the feature vector of
{(x(j), y(j))}N
the jth example and y(j) ∈ {−1, 1} is the corresponding class
label of x(j). Let ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd be the degree
vector. Each feature is then mapped to a higher dimensional
space generated by all corresponding pure-power-˜n monomials
through the mapping T : Rd → Rn1×n2×···×nd

5

Here we consider that the tensor A is expressed as a tensor
train with cores {Gk}d
k=1. The main idea of the TT learning
algorithms is to update the cores in an alternating way by
optimizing an appropriate loss function. Prior to updating the
TT-cores, the TT-ranks are ﬁxed and a particular initial guess
of {Gk}d
k=1 is made. The TT-ranks can be interpreted as tuning
parameters, higher values will result in a better ﬁt at the risk
of overﬁtting. It is straightforward to extend our algorithms by
means of the Density Matrix Renormalization Group (DMRG)
method [26] such that the TT-ranks are updated adaptively.
Each core is updated in the order

G1 → G2 → · · · → Gd → Gd−1 → · · · → G1 → · · ·

until convergence, which is guaranteed under certain condi-
tions as described in [27], [28]. It turns out that updating
one TT-core is equivalent with minimizing a loss function in
a small number of variables, which can be done in a very
efﬁcient manner. The following lemma shows how the inner
product (cid:104)T (x), A(cid:105) in the generic feature space is a linear
function in any of the TT-cores Gk.
Lemma 1. Given a vector ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd, let
T be the mapping deﬁned by (12), and let A be a TT with
cores Gk ∈ Rrk−1×nk×rk , k = 1, 2, . . . , d. For any x ∈ Rd
and k = 1, . . . , d, we have that

(cid:104)T (x), A(cid:105) = (cid:0)qk(x)(cid:62) ⊗ v(xk)(cid:62) ⊗ pk(x)(cid:1) vec(Gk),

(14)

T (x)i1i2···id =

xik−1
k

.

d
(cid:89)

k=1

(12)

where

For x = (x1, x2, . . . , xd)(cid:62) ∈ Rd, let {v(xk)}d
Vandermonde vectors deﬁned in (6). Clearly, we have

k=1 be the

p1(x) = 1, pk(x)
k≥2

=

k−1
(cid:89)

i=1

(cid:0)Gi ×2 v(xi)(cid:62)(cid:1) ∈ R1×rk−1,

T (x) = v(x1) ◦ v(x2) ◦ · · · ◦ v(xd).

(13)

and

This high-dimensional pure-power polynomial space beneﬁts
the learning task from the following aspects:

• all interactions between features are described by the

qk(x)
k<d

=

d
(cid:89)

i=k+1

monomials of pure-power polynomials;

Proof. By deﬁnition, we have

(cid:0)Gi ×2 v(xi)(cid:62)(cid:1) ∈ Rrk×1, qd(x) = 1.

• the dimension of the tensor space grows exponentially
k=1 nk, which increases the probability
training examples linearly into two-

with d, namely, (cid:81)d
of separating all
classes;

• the one-to-one mapping between pure-power polynomials
and tensors enables the use of tensor trains to lift the curse
of dimensionality.

With these preparations, our goal

is to ﬁnd a decision
hyperplane to separate these two-class examples in the tensor
space, also called the generic feature space. In other words,
like the inductive learning described in [16], we try to ﬁnd a
tensor A ∈ Rn1×n2×···×nd such that

y(j)(cid:104)T (x(j)), A(cid:105) > 0,

j = 1, 2, . . . , N.

Note that the bias is absorbed into the ﬁrst element of A. Note
that the learning problem can also be interpreted as ﬁnding a
pure-power-˜n polynomial g(x) such that

and

g(x(j)) > 0,

∀y(j) = 1,

g(x(j)) < 0,

∀y(j) = −1.

(cid:104)T (x), A(cid:105) = A ×1 v(x1)(cid:62) ×2 · · · ×d v(xd)(cid:62)

= (cid:0)G1 ×2 v(x1)(cid:62)(cid:1) · · · (cid:0)Gd ×2 v(xd)(cid:62)(cid:1)
= Gk ×1 pk(x) ×2 v(xk)(cid:62) ×3 qk(x)(cid:62)
= (cid:0)qk(x)(cid:62) ⊗ v(xk)(cid:62) ⊗ pk(x)(cid:1) vec(Gk)

for any k = 1, 2, . . . , d. This completes the proof.

Example 4. In this example we illustrate the advantageous
representation of a pure-power polynomial f as a TT. Suppose
we have a polynomial f with d = 10 and all degrees ˜ni =
9 (i = 1, . . . , 10). All coefﬁcients of f (x) can then be stored
into a 10-way tensor 10 × 10 × · · · × 10 tensor A such that
the evaluation of f in a particular x is given by (7). The TT-
representation of f consists of 10 TT-cores G1, . . . , G10, with
a storage complexity of O(100r2), where r is the maximal TT-
rank. This demonstrates the potential of the TT-representation
in avoiding the curse of dimensionality when the TT-ranks are
small.

for
Example
T (x), A, v(xk), qk(x), pk(x) for the following quadratic

5. Next, we

expressions

illustrate

the

6







T (x) =

1 +
2. Since d = 2 and ˜n1 = ˜n2 = 2, both T and A

polynomial in two variables f (x) = 1 + 3x1 − x2 − x2
7x1x2 + 9x2
are the following 3 × 3 matrices


x2
1
x2
2
x1 x1x2 x1x2
2
1x2
1 x2
x2
2
The TT-representation of A consists of a 1 × 3 × 3 tensor G1
and a 3×3×1 tensor G2. Suppose now that k = 2 and we want
to compute the evaluation of the polynomial f in a particular
x, which is (cid:104)T (x), A(cid:105). From Lemma 1 we then have that
(cid:104)T (x), A(cid:105) = (cid:0)q2(x)(cid:62) ⊗ v(x2)(cid:62) ⊗ p2(x)(cid:1) vec(G2),

1 −1
7
3
0
−1


9
0
 .
0

1x2 x2

 , A =



We have thus shown that updating the core Gk is equivalent
with solving a least squares optimization problem in rk−1nkrk
variables. Minimizing (17) with respect to Gk for any k =
1, . . . , d results in solving the linear system

(C (cid:62)

k Ck) vec(Gk) = C (cid:62)

k y.

(19)

Supposing r1 = r2 = · · · = rd−1 = r and n1 = n2 = · · · =
nd = n, then the computational complexity of solving (19) is
O((r2n)3). For the maximal values of r = 10 and n = 4 in
our experiments, this implies that we need to solve a linear
system of order 400, which takes about 0.01 seconds using
MATLAB on our desktop computer.

with

q2(x) = 1 ∈ R,
v(x2) = (cid:0)1 x2 x2
p2(x) = G1 ×2 v(x1)(cid:62) ∈ R1×3,
v(x1) = (cid:0)1 x1 x2

∈ R3.

∈ R3,

(cid:1)(cid:62)

(cid:1)(cid:62)

2

1

In what follows, we ﬁrst present two learning algorithms
based on different loss functions. These algorithms will learn
the tensor A directly in the TT-representation from a given
dataset. Two enhancements, namely, regularization for better
accuracy and parallelization for higher speed will be described
in the last two subsections.

A. TT Learning by Least Squares

Least squares estimation is the simplest and thus most
common estimation method. In the generic feature space, we
attempt to design a linear classiﬁer so that its desired output
is exactly 1 or −1. However, we have to live with errors, that
is, the true output will not always be equal to the desired one.
The least squares estimator is then found from minimizing the
following mean square error function

B. TT Learning by Logistic Regression

Since our goal is to ﬁnd a hyperplane to separate two-class
training examples in the generic feature space, we may not
care about the particular value of the output. Indeed, only
the sign of the output makes sense. This gives us the idea to
decrease the number of sign differences as much as possible
when updating the TT-cores, i.e., to minimize the number of
misclassiﬁed examples. However, this model is discrete so
that a difﬁcult combinatorial optimization problem is involved.
Instead, we try to ﬁnd a suboptimal solution in the sense of
minimizing a continuous cost function that penalizes misclas-
siﬁed examples. Here, we consider the logistic regression cost
function. First, consider the standard sigmoid function

σ(z) =

1
1 + e−z ,

z ∈ R,

where the output always takes values between 0 and 1. An
important property is that its derivative can be expressed by
the function itself, i.e.,

J(A) =

(cid:16)

(cid:104)T (x(j)), A(cid:105) − y(j)(cid:17)2

.

(15)

The logistic function for the jth example x(j) is given by

1
N

N
(cid:88)

j=1

σ(cid:48)(z) = σ(z)(1 − σ(z)).

hA(x(j)) := σ

(cid:104)T (x(j)), A(cid:105)

(cid:16)

(cid:17)

.

(20)

(21)

We now show how updating a TT-core Gk is equivalent with
solving a relatively small linear system. First, we deﬁne the
N × rk−1nkrk matrix




Ck =







qk(x(1))(cid:62) ⊗ v(x(1)
qk(x(2))(cid:62) ⊗ v(x(2)

k )(cid:62) ⊗ pk(x(1))
k )(cid:62) ⊗ pk(x(2))
...







qk(x(N ))(cid:62) ⊗ v(x(N )

k

)(cid:62) ⊗ pk(x(N ))

for any k = 1, 2, . . . , d. The matrix Ck is hence obtained
from the concatenation of the row vectors qk(x)(cid:62) ⊗v(xk)(cid:62) ⊗
pk(x) from (14) for N samples x(1), . . . , x(N ). It follows from
Lemma 1 that

J(A) =

(cid:107)Ck vec(Gk) − y(cid:107)2

1
N

We can also interpret the logistic function as the probability
that the example x(j) belongs to the class denoted by the
label 1. The predicted label ˜y(j) for x(j) is then obtained
according to the rule

(16)

(cid:40)

hA(x(j)) ≥ 0.5 ⇔ (cid:104)T (x(j)), A(cid:105) ≥ 0 → ˜y(j) = 1,
hA(x(j)) < 0.5 ⇔ (cid:104)T (x(j)), A(cid:105) < 0 → ˜y(j) = −1.

For a particular example x(j), we deﬁne the cost function as

Cost(x(j), A) =




− ln

(cid:16)

(cid:17)
hA(x(j))



− ln

(cid:16)

(cid:17)
1 − hA(x(j))

if y(j) = 1,

if y(j) = −1.

where

y = (y(1), y(2), . . . , y(N ))(cid:62) ∈ RN .

The goal now is to ﬁnd a tensor A such that hA(x(j)) is near
1 if y(j) = 1 or near 0 if y(j) = −1. As a result, the logistic

(17)

(18)

regression cost function for the whole training dataset is given
by

J(A) =

Cost(x(j), A)

1
N

N
(cid:88)

j=1

=

−1
N

N
(cid:88)

j=1

(cid:20) 1 + y(j)
2

(cid:16)

ln

hA(x(j))

+

(cid:17)

(22)

1 − y(j)
2

(cid:16)

ln

1 − hA(x(j))

(cid:17)(cid:21)

.

It is important to note that (22) is convex though the sigmoid
function is not. This guarantees that we can ﬁnd the globally
optimal solution instead of a local optimum.

From equation (21) and Lemma 1, one can see that the
function J(A) can also be regarded as a function of the core
Gk since

and

(cid:104)T (x(j)), A(cid:105) = Ck(j, :) vec(Gk)

where Ck(j,
:) denote the jth row vector of Ck deﬁned in
(16). It follows that updating the core Gk is equivalent with
solving a convex optimization problem in rk−1nkrk variables.
Let

hA =

(cid:16)
hA(x(1)), hA(x(2)), . . . , hA(x(N ))

(cid:17)(cid:62)

∈ RN (23)

and DA be the diagonal matrix in RN ×N with the jth
diagonal element given by hA(x(j)) (cid:0)1 − hA(x(j))(cid:1). By using
the property (20) one can derive the gradient and Hessian with
respect to Gk as

∇Gk J(A) =

C (cid:62)
k

hA −

(cid:18)

1
N

(cid:19)

y + 1
2

and

∇2
Gk

J(A) =

C (cid:62)

k DACk,

1
N

respectively, where y is deﬁned in (18) and 1 denotes the
all-ones vector in RN . Although we do not have a closed-
form solution to update the core Gk, the gradient and Hessian
allows us to ﬁnd the solution by efﬁcient iterative methods, e.g.
Newton’s method whose convergence is at least quadratic in a
neighbourhood of the solution. A quasi-Newton method, like
the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm, is
another good choice if the inverse of the Hessian is difﬁcult
to compute.

(24)

(25)

C. Regularization

The cost functions (15) and (22) of the two TT learning
algorithms do not have any regularization term, which may
result in overﬁtting and hence bad generalization properties of
the obtained TT classiﬁer. Next, we discuss how the addition
of a regularization term to (15) and (22) results in a small
modiﬁcation of the small optimization problem that needs to
be solved when updating the TT-cores Gk.

Consider the regularized optimization problem

˜J(A) = J(A) + γR(A),

(26)

7

where J(A) is given by (15) or (22), γ is a parameter that
balances the loss function and the regularization term. Here
we use the Tikhonov regularization, namely,

(cid:104)A, A(cid:105).

R(A) =

1
2
Thanks to the TT structure, the gradient of R(A) with respect
to the TT-core Gk can be equivalently rewritten as a linear
transformation of vec(Gk). In other words, there is a matrix
Dk ∈ Rrk−1nkrk×rk−1nkrk determined by the cores {Gj}j(cid:54)=k
such that ∇Gk R(A) = Dkvec(Gk). See Appendix A for more
details. It follows that

(27)

∇Gk

˜J(A) = ∇Gk J(A) + γDkvec(Gk)

∇2
Gk

˜J(A) = ∇2
Gk

J(A) + γDk.

These small modiﬁcations lead to small changes when updat-
ing the core Gk. For instance, the ﬁrst-order condition of (26)
for the least squares model results in solving the modiﬁed
linear system
(cid:18)

(cid:19)

C (cid:62)

k Ck +

γDk

vec(Gk) = C (cid:62)

k y,

(28)

N
2

when compared with the original linear system (19).

D. Orthogonalization and Parallelization

The matrix Ck from (16) needs to be reconstructed for each
TT-core Gk during the execution of the two TT learning algo-
rithms. Fortunately, this can be done efﬁciently by exploiting
the TT structure. In particular, after updating the core Gk in the
left-to-right sweep, the new row vectors {pk+1(x(j))}N
j=1 to
construct the next matrix Ck+1 can be easily computed from
pk+1(x(j)) = Gk ×1 pk(x(j)) ×2 v(x(j)

k )(cid:62).

Similarly, in the right-to-left sweep, the new column vectors
{qk−1(x(j))}N
j=1 to construct the next matrix Ck−1 can be
easily computed from

qk−1(x(j)) = Gk ×2 v(x(j)

k )(cid:62) ×3 qk(x(j))(cid:62).

To make the learning algorithms numerically stable, the
techniques of orthogonalization are also applied. The main
idea is to make sure that before updating the core Gk, the cores
G1, . . . , Gk−1 are left-orthogonal and the cores Gk+1, . . . , Gd
are right-orthogonal by a sequence of QR decompositions. In
this way, the condition number of the constructed matrix Ck
is upper bounded so that the subproblem is well-posed. After
updating the core Gk, we orthogonalize it with an extra QR
decomposition, and absorb the upper triangular matrix into
the next core (depending on the direction of updating). More
details on the orthogonalization step can be found in [27].

Another computational challenge is the potentially large
size N of the training dataset. Luckily, the dimension of the
optimization problem when updating Gk in the TT learning
algorithms is rk−1(nk + 1)rk, which is much smaller and
independent from N . We only need to compute the products
C (cid:62)
k DACk in (19), (24) and (25).
These computations are easily done in parallel. Speciﬁcally,

k hA and C (cid:62)

k Ck, C (cid:62)

k y, C (cid:62)

8

given a proper partition {Nl}L
we divide the large matrix Ck into several blocks, namely,

l=1 satisfying (cid:80)L

l=1 Nl = N ,

TABLE II
DATASET DESCRIPTION

Ck =

∈ RN ×rk−1(nk+1)rk ,

















C (1)
k
C (2)
k
...
C (L)
k

where C (l)
example, the product C (cid:62)

k ∈ RNl×rk−1nkrk , l = 1, 2, . . . , L. Then, for

k DACk can be computed by

C (cid:62)

k DACk =

(C (l)

k )(cid:62)D(l)

A C (l)
k ,

L
(cid:88)

l=1

where D(l)
A denotes the corresponding diagonal block. Each
term in the summation on the right-hand side of the above
equation can be computed over L distributed cores, with
a computational complexity of O(r4n2
kNl) for each core,
supposing rk−1 = rk = r. The other matrix products can
also be computed in a similar way.

We summarize our learning algorithms in Algorithm 2.
The two most computationally expensive steps are lines 5
and 7. As we mentioned before, solving (26) takes approx-
imately O((r2n)3) ﬂops. If the QR decomposition of line 7
is computed through Householder transformations, then the
computational complexity is approximately O(r3n2) ﬂops. For
the maximal values of n = 4 and r = 10 in our experiments,
this amounts to solving computing the inverse and the QR
factorization of a 400 × 400 matrix. Note that based on the
decision strategy, an m-class problem is decomposed into a
sequence of two-class problems whose TT classiﬁers can be
trained in parallel.

Algorithm 2 Tensor Train Learning Algorithm
Input: Training dataset of pairs {(x(j), y(j))}N

j=1, TT-ranks
k=1, degree vector ˜n = (˜n1, ˜n2, . . . , ˜nd)(cid:62) ∈ Nd and

{rk}d−1
regularization parameter γ

Output: Tensor A in TT format with cores {Gk}d
1: Initialize right orthogonal cores {Gk}d

k=1
k=1 of prescribed

ranks

2: while termination condition is not satisﬁed do
3: %Left-to-right sweep
4:
5:

for k = 1, 2, . . . , d − 1 do

G∗
k ← ﬁnd the minimal solution of the regularized
optimization problem (26) with respect to Gk
Uk ← reshape(G∗
[Q, R] ← compute QR decomposition of Uk
Gk ← reshape(Q, rk−1, nk, rk)
Vk+1 ← R ∗ reshape(Gk+1, rk, nk+1, rk+1)
Gk+1 ← reshape(Vk+1, rk, nk+1, rk+1)

k, rk−1nk, rk)

6:
7:
8:
9:
10:

end for
Perform the right-to-left sweep

11:
12:
13: end while

Image size

Training size

Test size

USPS
MNIST

16 × 16
28 × 28

7291
60000

2007
10000

• The DMRG method [26] can also be used to update
the cores. This involves updating two cores at a time
so that the TT-ranks are adaptively determined by means
of a singular value decomposition (SVD). This may give
better performance at the cost of a higher computational
complexity. It also removes the need to ﬁx the TT-ranks
a priori.

• The local linear convergence of Algorithm 2 has been
established in [27], [28] under certain conditions. In
particular,
if the TT-ranks are correctly estimated for
convex optimization problems, then the obtained solution
is guaranteed to be the global optimum. When choosing
the TT-ranks, one should keep the upper bounds of the
TT-ranks from Proposition 1 in mind.

VI. EXPERIMENTS

In this section, we test our TT learning algorithms and
compare their performance with LS-SVMs with polynomial
kernels on two popular digit recognition datasets: USPS
and MNIST. All our algorithms were implemented in MAT-
LAB Version R2016a, which can be freely downloaded from
https://github.com/kbatseli/TTClassiﬁer. We compare our TT-
polynomial classiﬁers with a polynomial classiﬁer based on
LS-SVMs with a polynomial kernel. The LS-SVM-polynomial
classiﬁer was trained with the MATLAB LS-SVMlab tool-
box, which can be freely downloaded from http://www.esat.
kuleuven.be/sista/lssvmlab/. The numerical experiments were
done on a desktop PC with an Intel i5 quad-core processor
running at 3.3GHz and 16GB of RAM.

The US Postal Service (USPS) database1contains 9298
handwritten digits, including 7291 for training and 2007 for
testing. Each digit is a 16×16 grayscale image. It is known that
the USPS test set is rather difﬁcult and the human error rate is
2.5%. The Modiﬁed NIST (MNIST) database2 of handwritten
digits has a training set of 60,000 examples, and a test set of
10,000 examples. It is a subset of a larger set available from
NIST. The digits have been size-normalized and centered in
a 28×28 image. The description of these two databases is
summarized in Table II.

Before extracting features of the handwritten digits, we ﬁrst
execute the pre-process of deskewing which is the process
of straightening an image that has been scanned or written
crookedly. By choosing a varying number d, the corresponding
feature vectors are then extracted from a pre-trained CNN
model 1-20-P-100-P-d-10, which represents a net with an input
images of size 28×28, a convolutional layer with 20 maps and

We end this section with the following remarks:
• Other loss functions can also be used in the framework
of TT learning provided that there exists an efﬁcient way
to solve the corresponding subproblems.

1The USPS database is downloaded from

http://statweb.stanford.edu/∼tibs/ElemStatLearn/data.html

2The MNIST database is downloaded from

http://yann.lecun.com/exdb/mnist/

5×5 ﬁlters, a max-pooling layer over non-overlapping regions
of size 2×2, a convolutional layer with 100 maps and 5×5
ﬁlters, a max-pooling layer over non-overlapping regions of
size 2×2, a convolutional layer with d maps and 4×4 ﬁlters,
and a fully connected output layer with 10 neurons. As the
variants of the well-known CNN model LeNet-5 [9], these
CNN models have been trained well on the MNIST database.
For an input 28×28 image, we get a feature vector of length
d from the third hidden layer. Note that for USPS database,
we must resize image data to the size of image input layer.
We mention that these techniques of deskewing and feature
extraction using pre-trained CNN model are widely used in
the literature [9], [10], [29].

For the decision module, we adopt the one-against-all deci-
sion strategy where ten TT classiﬁers are trained to separate
each digit from all
the others. In the implementation of
Algorithm 2, we normalize each initial core such that its
Frobenius norm is equal to one. The degree vector is given
by ˜n1 = · · · = ˜nd = ˜n. The TT-ranks are upper bounded
by rmax. The values of d, ˜n, rmax were chosen to minimize
the test error rate and to ensure that each of the subproblems
to update the TT-cores Gk could be solved in a reasonable
time. The dimension of each subproblem is at most n r2
max.
For example, in the USPS case, we ﬁrst ﬁxed the values of
n and rmax. We then ﬁxed the value of d and incremented
n and rmax to see whether this resulted in a better test error
rate. We use the optimality criterion

| ˜J(A+) − ˜J(A)|
| ˜J(A)|

≤ 10−2,

where A+ is the updated tensor from tensor A after one sweep.
And the maximum number of sweeps is 4, namely, 4(d−1) it-
erations through the entire training data are performed for each
session. To simplify notations, we use “TTLS” and “TTLR” to
denote the TT learning algorithms based on minimizing loss
functions (15) and (22), respectively. For these two models, the
regularization parameter γ is determined by the technique of
10-fold cross-validation. In other words, we randomly assign
the training data to ten sets of equal size. The parameter γ is
chosen so that the mean over all test errors is minimal.

The numerical results for USPS database and MNIST
database are reported in Tables III and IV, respectively. The
monotonic decrease is always seen when training the ten TT
classiﬁers. Fig. 4 shows the convergence of both TT learning
algorithms on the USPS data for the case d = 20, ˜n =
1, rmax = 8 when training the classiﬁer for the character “6”.
In addition, we also trained a polynomial classiﬁer using LS-
SVMs with polynomial kernels on these two databases. Using
the basic LS-SVM scheme, a training error rate of 0 and a test
error rate of 8.37% were obtained for the USPS dataset after
more than three and a half hours of computation. This runtime
includes the time required to tune the tuning parameters via
10-fold cross-validation. When using an RBF kernel with the
LS-SVM, it is possible to attain a test error of 2.14% [30],
but then the classiﬁer is not polynomial anymore. The MNIST
dataset resulted in consistent out-of-memory errors, which is
to be expected as the basic SVM scheme is not intended
for large data sets. We would also like to point out that a

9

Fig. 4. The convergence of TT learning algorithms.

test error of 1.1% is reported on the MNIST website for a
polynomial classiﬁer of degree 4 that uses conventional SVMs
and deskewing.

VII. CONCLUSION

This paper presents the framework of TT learning for pattern
classiﬁcation. For the ﬁrst time, TTs are used to represent
polynomial classiﬁers, enabling the learning algorithms to
work directly in the high-dimensional feature space. Two
efﬁcient learning algorithms are proposed based on different
loss functions. The numerical experiments show that each TT
classiﬁer is trained in up to several minutes with competitive
test errors. When compared with other polynomial classiﬁers,
the proposed learning algorithms can be easily parallelized
and have considerable advantage on storage and computation
time. We also mention that these results can be improved by
adding virtual examples [10]. Future improvements are the
implementation of on-line learning algorithms, together with
the extension of the binary TT classiﬁer to the multi-class case.

APPENDIX A
Given the degree vector ˜n = (˜n1, ˜n2, . . . , ˜nd) ∈ Nd, let A ∈
Rn1×n2×···×nd be the tensor in TT format with cores Gk ∈
Rrk−1×nk×rk , k = 1, 2, . . . , d. To investigate the gradient of
R(A) in (27) with respect to the TT-core Gk, we give a small
variation (cid:15) to the ith element of vec(Gk), resulting in a new
tensor A(cid:15) given by

A(cid:15) = A + (cid:15)I (k)

,

i
where 1 ≤ i ≤ rk−1nkrk and I (k)
is the tensor which has
the same TT-cores with A except that the vectorization of the
core Gk is replaced by the unit vector in Rrk−1nkrk with the
ith element equal to 1 and 0 otherwise. Then we have

i

[∇Gk R(A)]i = lim
(cid:15)→0

R(A(cid:15)) − R(A)
(cid:15)
On the other hand, by the deﬁnition of vectorization, the ith
element of vec(Gk) ∈ Rrk−1nkrk is mapped from the tensor
element of Gk ∈ Rrk−1×nk×rk with indices (αk−1, jk, αk)
satisfying

= (cid:104)A, I (k)

(29)

(cid:105).

i

i = αk−1 + (jk − 1)rk−1 + (αk − 1)rk−1nk,

where 1 ≤ αk−1 ≤ rk−1, 1 ≤ jk ≤ nk and 1 ≤ αk ≤ rk.
Denote by E(αk−1,αk) the matrix in Rrk−1×rk such that the

TABLE III
NUMERICAL RESULTS FOR DATASET USPS

Train error

TTLS
Test error

Train error

TTLR
Test error

10

d

20
25
30
35
40
20
25
30
35
40
20
25
30
20
25
30
35
40

d

20
25
30
35
40
20
25
30
35
40
20
30
40
20
30
40
40
40
40

˜n

1
1
1
1
1
2
2
2
2
2
3
3
3
1
1
1
1
1

˜n

1
1
1
1
1
2
2
2
2
2
3
3
3
1
1
1
2
3
4

rmax
8
8
8
8
8
8
8
8
8
8
8
8
8
10
10
10
10
10

rmax
8
8
8
8
8
8
8
8
8
8
8
8
8
10
10
10
10
10
10

0.58%
0.47%
0.49%
0.38%
0.34%
0.49%
0.38%
0.51%
0.45%
0.34%
0.59%
0.49%
0.63%
0.56%
0.41%
0.51%
0.34%
0.44%

0.16%
0.15%
0.13%
0.19%
0.21%
0.19%
0.13%
0.19%
0.17%
0.17%
0.20%
0.22%
0.21%
0.20%
0.16%
0.17%
0.13%
0.18%
0.24%

Time(s)
4.33% 1.10×10
4.04% 1.57×10
3.84% 1.83×10
4.04% 2.21×10
3.89% 2.59×10
4.29% 2.04×10
4.29% 2.68×10
4.14% 3.28×10
4.38% 3.87×10
4.09% 4.48×10
4.58% 3.26×10
4.53% 4.07×10
4.43% 4.93×10
4.33% 1.91×10
3.94% 2.48×10
3.84% 3.09×10
3.94% 3.71×10
3.84% 4.36×10

Time(s)
7.14×10
0.96%
9.38×10
0.97%
0.86% 11.73×10
0.81% 13.95×10
0.94% 16.22×10
1.03% 12.08×10
1.00% 15.72×10
0.88% 18.94×10
0.86% 23.11×10
0.90% 25.67×10
1.10% 17.45×10
1.02% 27.21×10
0.97% 36.48×10
0.99% 10.14×10
0.88% 17.15×10
0.88% 24.05×10
0.93% 39.61×10
0.94% 59.66×10
1.02% 78.83×10

0.45%
0.27%
0.30%
0.26%
0.14%
0.55%
0.29%
0.47%
0.32%
0.21%
0.64%
0.56%
0.51%
0.44%
0.27%
0.25%
0.22%
0.33%

0.12%
0.09%
0.10%
0.03%
0.07%
0.14%
0.13%
0.10%
0.13%
0.17%
0.19%
0.21%
0.07%
0.19%
0.17%
0.13%
0.14%
0.10%
0.14%

Time(s)
4.87×10
4.33%
6.49×10
3.99%
8.18×10
3.99%
9.77×10
4.24%
3.74% 11.39×10
8.29×10
4.24%
4.14% 10.82×10
3.99% 13.34×10
4.38% 15.88×10
3.74% 18.39×10
4.33% 11.47×10
4.14% 14.98×10
4.09% 18.35×10
4.33% 17.69×10
3.94% 24.72×10
4.04% 31.14×10
4.29% 38.17×10
3.84% 45.24×10

Time(s)
0.99% 17.52×10
0.98% 22.96×10
0.86% 28.96×10
0.82% 34.55×10
0.83% 40.31×10
1.01% 29.45×10
0.98% 38.32×10
0.91% 46.77×10
0.83% 56.54×10
0.88% 64.68×10
1.00% 42.24×10
0.90% 65.89×10
0.89% 90.37×10
0.98% 36.66×10
0.90% 62.24×10
0.89% 83.62×10
0.92% 144.1×10
0.90% 207.7×10
0.91% 264.2×10

TABLE IV
NUMERICAL RESULTS FOR DATASET MNIST

Train error

TTLS
Test error

Train error

TTLR
Test error

element with index (αk−1, αk) equal to 1 and 0 otherwise. By
simple computation, one can obtain that

and

(cid:104)A, I (k)

(cid:105) =

i

(cid:88)

Ai1i2···id (I k

i )i1i2···id

i1,i2,...id
(cid:16)

= ak

(cid:17)
E(αk−1,αk) ⊗ Gk(jk)

bk,

where

ak =

k−1
(cid:89)

nl(cid:88)

l=1

il=1

(cid:2)Gl(il) ⊗ Gl(il)(cid:3) ∈ R1×r2

k−1

(31)

bk =

d
(cid:89)

nl(cid:88)

l=k+1

il=1

(cid:2)Gl(il) ⊗ Gl(il)(cid:3) ∈ Rr2

k×1.

(32)

(30)

Let a(1)
that

k , a(2)

k , . . . , a(rk−1)

k

∈ R1×rk−1 be the row vectors such

ak = (a(1)

k , a(2)

k , . . . , a(rk−1)

k

) ∈ R1×r2

k−1,

and let b(1)

k , b(2)

k , . . . , b(rk)

k

∈ Rrk×1 be the column vectors

such that

bk =

∈ Rr2

k×1,

















b(1)
k
b(2)
k
...
b(rk)
k

Combining (29) and (30) together, we have
[∇Gk R(A)]i = a(αk−1)
k
(cid:16)
(b(αk)
k

Gk(jk)b(αk)
)(cid:62) ⊗ (e(jk))(cid:62) ⊗ a(αk−1)

=

k

k

(cid:17)

vec(Gk),

where e(j) ∈ Rnk denotes the unit vector with the jth element
equal to 1 and 0 otherwise. If we deﬁne the rk−1nkrk ×
rk−1nkrk matrix




Dk =







(b(1)
(b(1)

k )(cid:62) ⊗ (e(1))(cid:62) ⊗ a(1)
k )(cid:62) ⊗ (e(1))(cid:62) ⊗ a(2)
...

k

k

(b(rk)
k

)(cid:62) ⊗ (enk )(cid:62) ⊗ a(rk−1)

k

,







(33)

it follows immediately that ∇Gk R(A) = Dkvec(Gk).

ACKNOWLEDGMENT
Zhongming Chen acknowledges the support of the Na-
tional Natural Science Foundation of China (Grant No.
11701132). Johan Suykens acknowledges support of ERC
AdG A-DATADRIVE-B (290923), KUL: CoE PFV/10/002
(OPTEC); FWO: G.0377.12, G.088114N, G0A4917N; IUAP
P7/19 DYSCO. Ngai Wong acknowledges the support of
the Hong Kong Research Grants Council under the General
Research Fund (GRF) project 17246416.

REFERENCES

[1] H.-B. Shen and K.-C. Chou, “Ensemble classiﬁer for protein fold pattern
recognition,” Bioinformatics, vol. 22, no. 14, pp. 1717–1722, 2006.
[2] H.-D. Cheng, X. Cai, X. Chen, L. Hu, and X. Lou, “Computer-aided
detection and classiﬁcation of microcalciﬁcations in mammograms: a
survey,” Pattern recognition, vol. 36, no. 12, pp. 2967–2991, 2003.
[3] B. Roberto, Template matching techniques in computer vision: theory

and practice. Wiley, Hoboken, 2009.

[4] B.-H. Juang, W. Hou, and C.-H. Lee, “Minimum classiﬁcation error
rate methods for speech recognition,” IEEE Transactions on Speech and
Audio processing, vol. 5, no. 3, pp. 257–265, 1997.

[5] L. Xu, A. Krzyzak, and C. Y. Suen, “Methods of combining multiple
classiﬁers and their applications to handwriting recognition,” IEEE
transactions on systems, man, and cybernetics, vol. 22, no. 3, pp. 418–
435, 1992.

[6] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern classiﬁcation.

John

Wiley & Sons, 2012.

[7] Y. Zhang, S. Wang, G. Ji, and P. Phillips, “Fruit classiﬁcation using
computer vision and feedforward neural network,” Journal of Food
Engineering, vol. 143, pp. 167–177, 2014.

[8] S. Wang, X. Yang, Y. Zhang, P. Phillips, J. Yang, and T.-F. Yuan,
“Identiﬁcation of Green, Oolong and Black Teas in China via Wavelet
Packet Entropy and Fuzzy Support Vector Machine,” Entropy, vol. 17,
no. 10, pp. 6663–6682, 2015.

11

[9] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.

[10] D. Decoste and B. Sch¨olkopf, “Training invariant support vector ma-
chines,” Machine learning, vol. 46, no. 1-3, pp. 161–190, 2002.
[11] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
W. Hubbard, and L. D. Jackel, “Backpropagation applied to handwritten
zip code recognition,” Neural computation, vol. 1, no. 4, pp. 541–551,
1989.

[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.

[13] C. Cortes and V. Vapnik, “Support-vector networks,” Machine learning,

vol. 20, no. 3, pp. 273–297, 1995.

[14] Y.-W. Chang, C.-J. Hsieh, K.-W. Chang, M. Ringgaard, and C.-J. Lin,
“Training and testing low-degree polynomial data mappings via linear
SVM,” Journal of Machine Learning Research, vol. 11, no. Apr, pp.
1471–1490, 2010.

[15] J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor,
and J. Vandewalle, Least Squares Support Vector Machines. World
Scientiﬁc, Singapore, 2002.

[16] M. Signoretto, Q. T. Dinh, L. De Lathauwer, and J. A. K. Suykens,
“Learning with tensors: a framework based on convex optimization and
spectral regularization,” Machine Learning, vol. 94, no. 3, pp. 303–351,
2014.

[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempit-
sky, “Speeding-up convolutional neural networks using ﬁne-tuned cp-
decomposition,” arXiv preprint arXiv:1412.6553, 2014.

[18] A. Novikov, D. Podoprikhin, A. Osokin, and D. P. Vetrov, “Tensoriz-
ing neural networks,” in Advances in Neural Information Processing
Systems, 2015, pp. 442–450.

[19] A. Novikov, M. Troﬁmov, and I. Oseledets, “Exponential machines,”

arXiv preprint arXiv:1605.03795, 2016.

[20] E. M. Stoudenmire and D. J. Schwab, “Supervised learning with
quantum-inspired tensor networks,” arXiv preprint arXiv:1605.05775,
2016.

[21] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,”

SIAM review, vol. 51, no. 3, pp. 455–500, 2009.

[22] I. Oseledets, “Tensor-train decomposition,” SIAM Journal on Scientiﬁc

Computing, vol. 33, no. 5, pp. 2295–2317, 2011.

[23] I. Oseledets and E. Tyrtyshnikov, “TT-cross approximation for multidi-
mensional arrays,” Linear Algebra and its Applications, vol. 432, no. 1,
pp. 70–88, 2010.

[24] D. Savostyanov and I. Oseledets, “Fast adaptive interpolation of multi-
dimensional arrays in tensor train format,” in 2011 7th International
Workshop on Multidimensional (nD) Systems (nDs).
IEEE, 2011, pp.
1–8.

[25] S. Theodoridis and K. Koutroumbas, Pattern Recognition, Fourth Edi-

tion, 4th ed. Academic Press, 2008.

[26] S. R. White, “Density matrix formulation for quantum renormalization
groups,” Physical Review Letters, vol. 69, no. 19, p. 2863, 1992.
[27] S. Holtz, T. Rohwedder, and R. Schneider, “The alternating linear
scheme for tensor optimization in the tensor train format,” SIAM Journal
on Scientiﬁc Computing, vol. 34, no. 2, pp. A683–A713, 2012.
[28] T. Rohwedder and A. Uschmajew, “On local convergence of alternating
schemes for optimization of convex problems in the tensor train format,”
SIAM Journal on Numerical Analysis, vol. 51, no. 2, pp. 1134–1162,
2013.

[29] F. Lauer, C. Y. Suen, and G. Bloch, “A trainable feature extractor for
handwritten digit recognition,” Pattern Recognition, vol. 40, no. 6, pp.
1816–1824, 2007.

[30] J. A. K. Suykens, “Deep restricted kernel machines using conjugate
feature duality,” Neural Computation, vol. 29, no. 8, pp. 2123–2163,
2017.

