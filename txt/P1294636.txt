6
1
0
2
 
p
e
S
 
7
 
 
]

V
C
.
s
c
[
 
 
1
v
6
3
0
2
0
.
9
0
6
1
:
v
i
X
r
a

Deep Markov Random Field for Image Modeling

Zhirong Wu

Dahua Lin

Xiaoou Tang

The Chinese University of Hong Kong

Abstract. Markov Random Fields (MRFs), a formulation widely used
in generative image modeling, have long been plagued by the lack of ex-
pressive power. This issue is primarily due to the fact that conventional
MRFs formulations tend to use simplistic factors to capture local pat-
terns. In this paper, we move beyond such limitations, and propose a
novel MRF model that uses fully-connected neurons to express the com-
plex interactions among pixels. Through theoretical analysis, we reveal
an inherent connection between this model and recurrent neural net-
works, and thereon derive an approximated feed-forward network that
couples multiple RNNs along opposite directions. This formulation com-
bines the expressive power of deep neural networks and the cyclic de-
pendency structure of MRF in a uniﬁed model, bringing the modeling
capability to a new level. The feed-forward approximation also allows it
to be eﬃciently learned from data. Experimental results on a variety of
low-level vision tasks show notable improvement over state-of-the-arts.

Keywords: Generative image model, MRF, RNN

1 Introduction

Generative image models play a crucial role in a variety of image processing and
computer vision tasks, such as denoising [1], super-resolution [2], inpainting [3],
and image-based rendering [4]. As repeatedly shown by previous work [5], the
success of image modeling, to a large extent, hinges on whether the model can
successfully capture the spatial relations among pixels.

Existing image models can be roughly categorized as global models and low-
level models. Global models [6,7,8] usually rely on compressed representations
to capture the global structures. Such models are typically used for describing
objects with regular structures, e.g. faces. For generic images, low-level models
are more popular. Thanks to their focus on local patterns instead of global
appearance, low-level models tend to generalize much better, especially when
there can be vast variations in the image content.

Over the past decades, Markov Random Fields (MRFs) have evolved into
one of the most popular models for low-level vision. Speciﬁcally, the clique-based
structure makes them particularly well suited for capturing local relations among
pixels. Whereas MRFs as a generic mathematical framework are very ﬂexible
and provide immense expressive power, the performance of many MRF-based
methods still leaves a lot to be desired when faced with challenging conditions.

2

Zhirong Wu, Dahua Lin, Xiaoou Tang

Fig. 1: We present a new class of markov random ﬁeld models whose potential
functions are expressed by powerful deep neural networks. We show applications
of the model on texture synthesis, image super-resolution and image synthesis.

This occurs due to the widespread use of simplistic potential functions that
largely restrict the expressive power of MRFs.

In recent years, the rise of Deep Neural Networks (DNN) has profoundly
reshaped the landscape of many areas in computer vision. The success of DNNs
is primarily attributed to its unparalleled expressive power, particularly their
strong capability of modeling complex variations. However, DNNs in computer
vision are mostly formulated as end-to-end convolutional networks (CNN) for
classiﬁcation or regression. The modeling of local interactions among pixels,
which is crucial for many low-level vision tasks, has not been suﬃciently explored.
The respective strengths of MRFs and DNNs inspire us to explore a new
approach to low-level image modeling, that is, to bring the expressive power
of DNNs to an MRF formulation. Speciﬁcally, we propose a generative image
model comprised of a grid of hidden states, each corresponding to a pixel. These
latent states are connected to their neighbors – together they form an MRF.
Unlike in classical MRF formulations, we use fully connected layers to express
the relationship among these variables, thus substantially improving the model’s
ability to capture complex patterns.

Through theoretical analysis, we reveal an inherent connection between our
MRF formulation and the RNN [9], which opens an alternative way to MRF
formulation. However, they still diﬀer fundamentally: the dependency structure
of an RNN is acyclic, while that of an MRF is cyclic. Consequently, the hidden
states cannot be inferred in a single feed-forward manner as in a RNN. This posts
a signiﬁcant challenge – how can one derive the back-propagation procedure
without a well-deﬁned forward function?

Our strategy to tackle this diﬃculty is to unroll an iterative inference pro-
cedure into a feed-forward function. This is motivated by the observation that
while the inference is iterative, each cycle of updates is still a feed-forward proce-
dure. Following a carefully devised scheduling policy, which we call the Coupled

Deep Markov Random Field for Image Modeling

3

Acyclic Passes (CAP), the inference can be unrolled into multiple RNNs op-
erating along opposite directions that are coupled together. In this way, local
information can be eﬀectively propagated over the entire network, where each
hidden state can have a complete picture of its context from all directions.

The primary contribution of this work is a new generative model that uniﬁes
MRFs and DNNs in a novel way, as well as a new learning strategy that makes
it possible to learn such a model using mainstream deep learning frameworks.
It is worth noting that the proposed method is generic and can be adapted to a
various problems. In this work, we test it on a variety of low-level vision tasks,
including texture synthesis, image super-resolution, and image synthesis.

2 Related Works

In this paper, we develop a generative image model that incorporates the ex-
pressive power of deep neural networks with an MRF. This work is related to
several streams of research eﬀorts, but moves beyond their respective limitations.

Generative image models. Generative image models generally fall into two
categories: parametric models and non-parametric models. Parametric models
typically use a compressed representation to capture an image’s global appear-
ance. In recent years, deep networks such as autoencoders [10] and adversarial
networks [11,12] have achieved substantial improvement in generating images
with regular structures such as faces or digits. Non-parametric models, including
pixel-based sampling [13,14,15] and patch-based sampling [16,17,18], instead rely
on a large set of exemplars to capture local patterns. Whereas these methods can
produce high quality images with local patterns directly sampled from realistic
images. Exhaustive search over a large exemplar set limits their scalability and
often leads to computational diﬃculties. Our work draws inspiration from both
lines of work. By using DNNs to express local interactions in an MRF, our model
can capture highly complex patterns while maintaining strong scalability.

Markov random ﬁelds. For decades, MRFs have been widely used for low-level
vision tasks, including texture synthesis [19], segmentation [20,21], denoising [1],
and super-resolution [2]. Classical MRF models in earlier work [22] use simple
hand-crafted potentials (e.g., Ising models [23], Gaussian MRFs [24]) to link
neighboring pixels. Later, more ﬂexible models such as FRAME [25] and Fields
of Experts [26] were proposed, which allow the potential functions to be learned
from data. However, in these methods, the potential functions are usually pa-
rameterized as a set of linear ﬁlters, and therefore their expressive power remains
very limited.

Recurrent neural networks. Recurrent neural networks (RNNs), a special
family of deep models, use a chain of nonlinear units to capture sequential rela-
tions. In computer vision, RNNs are primarily used to model sequential changes
in videos [27], visual attention [28,29], and hand-written digit recognition [30].
Previous work explores multi-dimensional RNNs [31] for scene labeling [32] as
well as object detections [33]. The most related work is perhaps the use of 2D

4

Zhirong Wu, Dahua Lin, Xiaoou Tang

Fig. 2: Graphical model of deep MRFs. Left: The hidden states and the pixels
together form an MRF. Right: Each hidden state connects to the neighboring
states, the neighboring pixels, and the pixel at the same location.

RNNs for generating gray-scale textures [34] or color images [35]. A key distinc-
tion of these models from ours is that 2D RNNs rely on an acyclic graphs to
model spatial dependency, e.g. each pixel depends only on its left and upper
neighbors – this severely limits the spatial coherence. Our model, instead, allows
dependencies from all directions via iterative inference unrolling.

MRF and neural networks. Connections between both models have been
discussed long ago [36]. With the rise of deep learning, recent work on image
segmentation [37,38] uses mean ﬁeld method to approximate a conditional ran-
dom ﬁeld (CRF) with CNN layers. A hybrid model of CNN and MRF has also
been proposed for human pose estimation [39]. These works primarily target
prediction problems (e.g. segmentation) and are not as eﬀective at capturing
complex pixel patterns in a purely generative way.

3 Deep Markov Random Field

The primary goal of this work is to develop a generative model for images that
can express complex local relationships among pixels while being tractable for
inference and learning. Formally, we consider an image, denoted by x, as an
undirected graph with a grid structure, as shown in Figure 2 left. Each node u
corresponds to a pixel xu. To capture the interactions among pixels, we intro-
duce, hu, a hidden variable for each pixel denoting the hidden state corresponding
to the pixel xu. In the graph, each node u has a neighborhood, denoted by Nu.
Particularly, we use the 4-connected neighborhood of a 2D grid in this work.

Joint Distribution. We consider three kinds of dependencies: (1) the depen-
dency between a pixel xu and its corresponding hidden state hu, (2) the depen-
dency between a hidden state hu and a neighbor hv with v ∈ Nu, and (3) the
dependency between a hidden state hu and a neighboring pixel xv. They are
respectively captured by factors ζ(xu, hu), φ(hu, hv), and ψ(hu, xv). In addition,
we introduce a regularization factor λ(hu) for each hidden state, which gives us
the leeway to encourage certain distribution over the state values. Bringing these

Deep Markov Random Field for Image Modeling

5

factors together, we formulate an MRF to express the joint distribution:

p(x, h) =

ζ(xu, hu)

(φ(hu, hv)ψ(hu, xv)ψ(hv, xu))

λ(hu). (1)

1
Z

(cid:89)

u∈V

(cid:89)

(u,v)∈E

(cid:89)

u∈V

Here, V and E are respectively the set of vertices and that of the edges in the
image graph, Z is a normalizing constant. Figure 2 shows it structure.

Choices of Factors. Whereas the MRF provides a principled way to express
the dependency structure, the expressive power of the model still largely depends
on the speciﬁc forms of the factors that we choose. For example, the modeling
capacity of classical MRF models are limited by their simplistic factors.

Below, we discuss the factors that we choose for the proposed model. First,
the factor ζ(xu, hu) determines how the pixel values are generated from the hid-
den states. Considering the stochastic nature of natural images, we formalize this
generative process as a Gaussian mixture model (GMM). The rationale behind
is that pixel values are on a low-dimensional space, where a GMM with a small
number of components can usually provide a good approximation to an empir-
ical distribution. Speciﬁcally, we ﬁx the number of components to be K, and
consider the concatenation of component parameters as the linear transform of
the hidden state, hT
u, µc
c=1, where Q is a weight matrix of model
parameters. In this way, the factor ζ(xu, hu) can be written as

u Q = ((πc

u, Σc

u))K

(2)

(3)

(4)

ζ(xu, hu) (cid:44) pGMM(xu|hu) =

uN (xu|µc
πc

u, Σc

u).

K
(cid:88)

c=1

To capture the rich interactions among pixels and their neighbors, we formulate
the relational factors φ(hu, hv) and ψ(hu, xv) with fully connected forms:

φ(hu, hv) = exp (cid:0)hT

u Whv

(cid:1) , ψ(hu, xv) = exp (cid:0)hT

u Rxv

(cid:1) .

Finally, to control the value distribution of the hidden states, we further incor-
porate a regularization term over hu, as

λ(hu) = exp (cid:0)−1T η(hu)(cid:1) = exp

(cid:16)

−η(h(1)

u ) − · · · − η(h(d)
u )

(cid:17)

.

Here, η is an element-wise nonlinear function and d is the dimension of hu. In
summary, the use of GMM in ζ(xu, hu) eﬀectively accounts for the variations
in pixel generation, the fully-connected factors φ(hu, hv) and ψ(hu, xv) enable
the modeling of complex interactions among neighbors, while the regularization
term λ(hu) provides a way to explicitly control the distribution of hidden states.
Together, they substantially increase the capacity of the MRF model.

Inference of Hidden States. With this MRF formulation, the posterior dis-
tribution of the hidden state hu, conditioned on all other variables, is given by

p (hu | xu, xNu , hNu ) ∝ ζ(xu, hu)λ(hu) ·

φ(hu, hv)ψ(hu, xv).

(5)

(cid:89)

v∈Nu

6

Zhirong Wu, Dahua Lin, Xiaoou Tang

h approximation

ReLU

Regularizer

Sigmoid

Regularizer

Fig. 3: Left shows the numerical simulation of approximated inference for the
hidden variables. Right shows the ReLU, sigmoid activation function and their
corresponding regularizations for the hidden variables.

Here, hu depends on its neighboring states, the corresponding pixel values, as
well as that of its neighbors. Since the pixel xu and its neighboring pixels xNu
are highly correlated, to simplify our later computations, we approximate the
posterior distribution as,

p (hu | xu, xNu , hNu ) (cid:39) p (hu | xNu , hNu ) ∝ λ(h)

φ(h, hv)ψ(h, xv).

(6)

(cid:89)

v∈Nu

We performed numerical simulations for this approximation. They are indeed
very close to each other, as illustrated in Figure 3. Consequently, the MAP
estimate of hu can be approximately computed from its neighbors. It turns out
that this optimization problem has an analytic solution given by,

˜hu = σ

Whv + Rxv

.

(cid:33)

(cid:32)

(cid:88)

v∈Nu

(7)

Here, σ is an element-wise function that is related to η as σ−1(z) = η(cid:48)(z), where
η(cid:48) is the ﬁrst-order derivative w.r.t. η, and σ−1 the inverse function of σ.

Connections to RNNs. We observe that Eq.(7) has a form that is similar
to the feed-forward computations in Recurrent Neural Networks (RNN) [9]. In
this sense, we can view the feed-forward RNN as an MAP inference process for
MRF models. Particularly, given the RNN computations in the form of Eq.(7),
one can formulate an MRF as in Eq.(1), where regularization function η can be
derived from σ according to the relation σ−1(z) = η(cid:48)(z), as

η(h) =

σ−1(z)dz + C.

(8)

(cid:90) h

b

Here, b is the minimum of the domain of h, which can be −∞, and C is an
arbitrary constant. This connection provides an alternative way to formulate
an MRF model. More importantly, in this way, RNN models that have been
proven to be successful can be readily transferred to an MRF formulation. Fig-
ure 3 shows the regularization functions η(h) corresponding to popular activation
functions in RNNs, such as sigmoid and ReLU [40].

Deep Markov Random Field for Image Modeling

7

4 Learning via Coupled Recurrent Networks

Except for special cases [41], inference and learning on MRFs is generally in-
tractable. Conventional estimation methods [42,8,43] either take overly long
time to train or tend to yield poor estimates, especially for models with a high-
dimensional parameter space. In this work, we consider an alternative approach
to MRF learning, which allows us to draw on deep learning techniques [44,45]
that have been proven to be highly eﬀective [40].

Variational Learning Principle. Estimation of probabilistic models based on
the maximum likelihood principle is often intractable when the model contains
hidden variables. Expectation-maximization [46] is one of the most widely used
ways to tackle this problem, which iteratively calculates the posterior distribu-
tion of hi (in E-steps) and then optimizes θ (in M-steps) as

ˆθ = argmax

θ

1
n

n
(cid:88)

i=1

Ep(hi|xi,θ) {log p(xi, hi|θ)} .

(9)

Here, θ = {W, Q, R} is the model parameter, xi is the i-th image, and hi is the
corresponding hidden state. As exact computation of this posterior expectation
is intractable, we approximate it based on ˜hi, the MAP estimate of hi, as below:

ˆθ = argmax

θ

1
n

n
(cid:88)

i=1

log p(xi|˜hi, θ), with ˜hi (cid:44) f (xi, θ).

(10)

This is the learning objective of our model. Here, f is the function that approxi-
mately infers the latent state ˜hi given an observed image xi. When the posterior
distribution p(hi|xi, θ) is highly concentrated, which is often the case in vision
tasks, this is a good approximation. For an image x, log p(x|˜h, θ) can be further
expanded as a sum of terms deﬁned on individual pixels:

(cid:88)

u

u((cid:80)

log p(x|˜h, θ) =

log pGMM(xu|˜h) =

πc
uN (xu| ˜µc

u, Σc

u),

(11)

(cid:88)

log

u

K
(cid:88)

c=1

u = µc

u + Σc

where ˜µc
v )R. For our problem, this learning principle can be
interpreted in terms of encoding/decoding – the hidden states ˜h = f (x, θ) can
be understood as an representation that encodes the observed patterns in an
image x, while log p(x|˜h, θ) measures how well ˜h explains the observations.

v hT

Coupled Acyclic Passes. In the proposed model, the dependencies among
neighbors are cyclic. Hence, the MAP estimate ˜h = f (x, θ) cannot be computed
in a single forward pass. Instead, Eq.(7) needs to be applied across the graph
in multiple iterations. Our strategy is to unroll this iterative inference proce-
dure into multiple feed-forward passes along opposite directions, such that these
passes together provide a complete context to each local estimate.

8

Zhirong Wu, Dahua Lin, Xiaoou Tang

Fig. 4: Coupled acyclic passes. We decouple an undirected cyclic graph into two di-
rected acyclic graphs with each one allowing feed-forward computation. Inference is
performed by alternately traversing the two acyclic graphs, while coupling their infor-
mation at each step.

Speciﬁcally, we decompose the underlying dependency graph G = (V, E),
which is undirected, into two acyclic directed graphs Gf = (V, Ef ) and Gb =
(V, Eb), as illustrated in Figure 4, such that each undirected edge {u, v} ∈ E
corresponds uniquely to an edge (u, v) ∈ Ef and an opposite edge (v, u) ∈ Eb.
It can be proved that such a decomposition always exists and that for each node
u ∈ V , the neighborhood Nu can be expressed as Nu = N f (u) ∪ N b(u), where
N f (u) and N b(u) are the set of parents of u respectively along Gf and Gb.

Given such a decomposition, we can derive an iterative computational pro-
cedure, where each cycle couples a forward pass that applies Eq.(7) along Gf
and a backward pass 1 along Gb. After the t-th cycle, the state hu is updated to

h(t)
u = σ

(cid:32)

(cid:88)

(cid:16)

v∈N f (u)

Wh(t−1)
v

+ Rxv

+

Wh(t)

v + Rxv

(cid:17)

(cid:88)

(cid:16)

(cid:17)

(cid:33)
.

(12)

v∈N b(u)

As states above, we have Nu = N f (u) ∪ N b(u). Therefore, over a cycle, the
updated state hu would incorporate information from all its neighbors. Note
that a given graph G can be decomposed in many diﬀerent ways. In this work,
we speciﬁcally choose the one that forms the zigzag path. The advantage over a
simple raster line order is that zigzag path traverses all the nodes continuously,
so that it conserves spatial coherence by making dependence of each node to
all the previous nodes that have been visited before. The forward and backward
passes resulted from such decomposition are shown in Figure 4.

This algorithm has two important properties: First, the acyclic decomposi-
tion allows feed-forward computation as in Eq.(7) to be applied. As a result, the
entire inference procedure can be viewed as a feed-forward network that couples
multiple RNNs operating along diﬀerent directions. Therefore, it can be learned
in a way similar to other deep neural networks, using Stochastic Gradient Descent
(SGD). Second, the feedback mechanism embodied by the backward pass facil-

1 The word forward and backward here means the sequential order in the graph. They
are not feed-forward and back-propagation in the context of deep neural networks.

Deep Markov Random Field for Image Modeling

9

itates the propagation of local information and thus the learning of long-range
dependencies.

Discussions with 2D-RNN. Previous work has explored two-dimensional
extensions of RNN [31], often referred to as 2D-RNN. Such extensions, however,
are formulated upon an acyclic graph, and can be considered as a trimmed down
version of our algorithm. A major drawback of 2D-RNN is that it scans the image
in a raster line order and it is not able to provide a feedback path. Therefore,
the inference of each hidden state can only take into account 1/4 of the context,
and there is no way to recover from a poor inference. As we will show in our
experiments, this may cause undesirable eﬀects. Whereas bidirectional RNNs [47]
may partly mitigate this problem, they decouple the hidden states into multiple
ones that are independent apriori, which would lead to consistency issues. Recent
work [48] also ﬁnds it diﬃcult to use in generative modeling.

Implementation Details For inference and learning, to make the computation
feasible, we just take one forward pass and one backward pass. Thus, each node
is only updated twice while being able to use the information from all possible
contexts. The training patch size varies from 15 to 25 depending on the speciﬁc
experiment. Overall, if we unroll the full inference procedure, our model2 is more
than thousands of layers deep. We use rmsprop [45] for optimization and we don’t
use dropout for regularization, as we ﬁnd it oscillates the training.

5 Experiments

In the following experiments, we test the proposed deep MRF on 3 scenarios for
modeling natural images. We ﬁrst study its basic properties on texture synthesis,
and then we apply it on a prediction problem, image super-resolution. Finally, we
integrate global CNN models with local deep MRF for natural image synthesis.

5.1 Texture Synthesis

The task of texture synthesis is to synthesize new texture images that possess
similar patterns and statistical characteristics as a given texture sample. The
study of this problem originated from graphics [13,14]. The key to successful
texture reproduction, as we learned from previous work, is to eﬀectively capture
the local patterns and variations. Therefore, this task is a perfect testbed to
assess a model’s capability of modeling visual patterns.

Our model works in a purely generative way. Given a sample texture, we train
the model on randomly extracted patches of size 25 × 25, which are larger than
most texels in natural images. We set K = 20, initialize x and h to zeros, and
train the model with back-propagation along the coupled acyclic graph. With
a trained model, we can generate textures by running the RNN to derive the

2 code available at https://github.com/zhirongw/deep-mrf

10

Zhirong Wu, Dahua Lin, Xiaoou Tang

D12

D34

D104

ﬂowers

bark

clouds

t
u
p
n
i

]
4
3
[
N
N
R
D
2

]
3
1
[
s
c
i
h
p
a
r
g

s
r
u
o

Fig. 5: Texture synthesis results.

latent states and at the same time sampling the output pixels. As our model is
stationary, it can generate texture images of arbitrary sizes.

We work on two texture datasets, Brodatz [49] for grayscale images, and
VisTex [50] for color images. From the results shown in Figure 5, our synthesis
visually resembles to high resolution natural images, and the quality is close to
the non-parametric approach [13]. We also compare with the 2D-RNN. [34]. As
we can see, the results obtained using 2D-RNN, which synthesizes based only
on the left and upper regions, exhibit undesirable eﬀects and often evolve into
blacks in the bottom-right parts.

Two fundamental parameters control the behaviors of our texture model.
The training patch size decides the farthest spatial relationships that could be
learned from data. The number of gaussian mixtures control the dynamics of
the texture landscape. We analyze our model by changing the two parameters.
As shown in Figure 6, bigger training patch size and bigger number of mixtures
consistently improves the results. For non-parametric approaches, bigger patch
size would dramatically bring up the computation cost. While for our model,
the inference time holds the same regardless of the patch size that the model
is trained on. Moreover, our parametric model is able to scale to large dataset
without bringing additional computations.

5.2

Image Super-Resolution

Image super-resolution is a task to produce a high resolution image given a single
low resolution one. Whereas previous MRF-based models [2,55] work reasonably,

Deep Markov Random Field for Image Modeling

11

5

10

15

20

25

input

1

2

5

10

20

input

e
z
i
s

h
c
t
a
p

g
n
i
n
i
a
r
t

s
e
r
u
t
x
i
m

f
o

r
e
b
m
u
n

Fig. 6: Texture synthesis by varying the patch size and the number of mixtures.

the quality of their products is inferior to the state-of-the-art models based on
deep learning [52,54]. With deep MRF, we wish to close the gap.

Unlike in texture synthesis, the generation of this task is driven by a low-
resolution image. To incorporate this information, we introduce additional con-
nections between the hidden states and corresponding pixels of the low-resolution
image, as shown in Figure 7. It is noteworthy that we just input a single pixel
(instead of a patch) at each site, and in this way, we can test whether the model
can propagate information across the spatial domain. As the task is determinis-
tic, we use a GMM with a single component and ﬁx its variance. In the testing
stage, we output the mean of the Gaussian component at each location as the
inferred high-resolution pixel. This approach is very generic – the model is not
speciﬁcally tuned for the task and no pre- and post-processing steps are needed.
We train our model on a widely used super-resolution dataset [56] which
contains 91 images, and test it on Set5, Set14, and BSD100 [57]. The training
is on patches of size 16 × 16 and rmsprop with momentum 0.95 is used. We use
PSNR for quantitative evaluation. Following previous work, we only consider the
luminance channel in the YCrCb color space. The two chrominance channels are
upsampled with bicubic interpolation.

As shown in Table 1 and Table 2, our approach outperforms the CNN-based
baseline [52] and compares favorably with the state-of-the-art methods dedicated
to this task [53,54]. One possible explanation for the success is that our model
not only learns the mapping, but also learns the image statistics for high reso-

12

Zhirong Wu, Dahua Lin, Xiaoou Tang

Fig. 7: Adapting deep MRFs to speciﬁc applications. Image super-resolution:
the hidden state receives an additional connection from the low-resolution pixel.
Image synthesis: deep MRF renders the ﬁnal image from a spatial feature map,
which is jointly learned by a variational auto-encoder.

images

2x upscale

baby
bird

Table 1: PSNR (dB) on Set5 dataset with upscale factor 2,3,4.
4x upscale

37.07 38.30 38.48 38.31
36.81 40.40 40.50 40.36

3x upscale
Bicubic CNN SE Ours Bicubic CNN SE Ours Bicubic CNN SE Ours
33.91 35.01 35.22 35.15
31.78 32.98 33.14 32.94
32.58 34.91 35.58 36.14 30.18 31.98 32.54 32.49
butterﬂy 27.43 32.20 31.86 32.74 24.04 27.58 26.86 29.09 22.10 25.07 24.09 25.78
31.59 32.19 32.52 32.41
34.86 35.64 35.69 35.70 32.88 33.55 33.76 33.63
32.14 34.94 35.33 34.84
28.56 30.92 31.36 31.69 26.46 28.21 28.92 28.97
33.66 36.34 36.37 36.38 31.92 32.30 32.56 33.14 28.42 30.09 30.24 30.52

head
women
average

lution images. The training procedure which unrolls the RNN into thousands of
steps that share parameters also reduces the risk of overﬁtting. The results also
demonstrate the particular strength of our model in handling large upscaling
factors and diﬃcult images. Figure 8 shows several examples visually.

5.3 Natural Image Synthesis

Images can be roughly considered as a composition of textures with the guid-
ance of scene and object structures. In this task, we move beyond the synthesis
of homogeneous textures, and try to generate natural images with structural
guidance.

While our model excels in capturing spatial dependencies, learning weak
dependencies across the entire image is both computationally infeasible and an-
alytically ineﬃcient. Instead, we adopt a global model to capture the overall
structure and use it to provide contextual guidance to MRF. Speciﬁcally, we
incorporate the variational auto-encoder (VAE) [10] for this purpose – VAE
generates feature maps at each location and our model uses that feature to ren-
der the ﬁnal image (see Figure 7). Such features may contain information of
scene layouts, objects, and texture categories.

We train the joint model end-to-end from scratch. During each iteration, the
VAE ﬁrst encodes the image into a latent vector, then decodes it to a feature
map with the same size of the input image. We then connect this feature map

Deep Markov Random Field for Image Modeling

13

Table 2: PSNR (dB) on various dataset with upscale factor 3.
Dataset Bicubic A+ [51] CNN [52] SE [53] CSCN [54] Ours
33.14
32.30
29.38
29.00
28.54
28.20

30.39
27.54
BSD100 27.22

33.10
29.41
28.50

32.59
29.13
28.18

32.56
29.16
28.20

Set5
Set14

Original / PSNR

CNN / 31.65 dB

SE / 31.56 dB

Ours / 33.11 dB

Original / PSNR

CNN / 34.91 dB

SE / 35.58 dB

Ours / 36.14 dB

Original / PSNR

CNN / 27.58 dB

SE / 26.86 dB

Ours / 29.09 dB

Fig. 8: Image super resolution results from Set 5 with upscaling factor 3.

to the latent states of the deep MRF. The total loss is deﬁned as the addition of
gaussian mixtures at image space and KL divergence at high-level VAE latent
space. For training, we randomly extracts patches from the feature map. The
gradients from the deep MRF back to the VAE thus only cover the patches being
extracted. During testing, VAE randomly samples from the latent space and
decodes it to generate the global feature maps. The output pixels are sampled
from the GMM with 10 mixtures along the coupled acyclic graph.

We work on the MSRC [58] and SUN database [59] and select some scene cat-
egories with rich natural textures, such as Mountains and Valleys. Each category
contains about a hundred images. As we will see, our approach generalizes much
better than the data-hungry CNN approaches. We train the model on images of
size 64 × 64 with a batch size of 4. For each image, we extract 16 patches of size
15 × 15 for training. Figure 9 shows several images generated from our models,
in comparison with those obtained from the baselines, namely raw VAE [10] and
DCGAN [60]. The CNN architecture is shared for all methods described in the

14

Zhirong Wu, Dahua Lin, Xiaoou Tang

Deep MRF

VAE DCGAN

t
e
e
r
t
s

r
e
v
i
r

n
r
a
b

y
e
l
l
a
v

n
i
a
t
n
u
o
m

y
e
l
l
a

Fig. 9: Image synthesis results.

DCGAN paper [60] to ensure fair comparison. We can see our model successfully
captures a variety of local patterns, such as water, clouds, wall and trees. The
global appearance also looks coherent, real and dynamic. The state-of-the-art
CNN based models, which focuses too much on global structures, often yield
sub-optimal local eﬀects.

6 Conclusions

We present a new class of MRF model whose potential functions are expressed
by powerful fully-connected neurons. Through theoretical analysis, we draw close
connections between probabilistic deep MRFs and end-to-end RNNs. To tackle
the diﬃculty of inference in cyclic graphs, we derive a new framework that decou-
ples a cyclic graph with multiple coupled acyclic passes. Experimental results
show state-of-the-art results on a variety of low-level vision problems, which
demonstrate the strong capability of MRFs with expressive potential functions.

Acknowledgment. This work is supported by the Big Data Collaboration Re-
search grant (CUHK Agreement No. TS1610626) and the Early Career Scheme
(ECS) grant (No: 24204215). We also thank Aditya Khosla for helpful discussions
and comments on a draft of the manuscript.

Deep Markov Random Field for Image Modeling

15

References

1. Portilla, J., Strela, V., Wainwright, M.J., Simoncelli, E.P.: Image denoising us-
ing scale mixtures of gaussians in the wavelet domain. Image Processing, IEEE
Transactions on 12(11) (2003) 1338–1351

2. Freeman, W.T., Pasztor, E.C., Carmichael, O.T.: Learning low-level vision. Inter-

national journal of computer vision 40(1) (2000) 25–47

3. Bertalmio, M., Sapiro, G., Caselles, V., Ballester, C.: Image inpainting. In: Pro-
ceedings of the 27th annual conference on Computer graphics and interactive tech-
niques, ACM Press/Addison-Wesley Publishing Co. (2000) 417–424

4. McMillan, L., Bishop, G.: Plenoptic modeling: An image-based rendering system.
In: Proceedings of the 22nd annual conference on Computer graphics and interac-
tive techniques, ACM (1995) 39–46

5. Huang, J., Mumford, D.: Statistics of natural images and models. In: Computer
Vision and Pattern Recognition, 1999. IEEE Computer Society Conference on.
Volume 1., IEEE (1999)

6. Turk, M.A., Pentland, A.P.: Face recognition using eigenfaces. In: Computer Vision
and Pattern Recognition, 1991. Proceedings CVPR’91., IEEE Computer Society
Conference on, IEEE (1991) 586–591

7. Wright, J., Ma, Y., Mairal, J., Sapiro, G., Huang, T.S., Yan, S.: Sparse representa-
tion for computer vision and pattern recognition. Proceedings of the IEEE 98(6)
(2010) 1031–1044

8. Hinton, G.E.: Training products of experts by minimizing contrastive divergence.

Neural computation 14(8) (2002) 1771–1800

9. Werbos, P.J.: Backpropagation through time: what it does and how to do it.

Proceedings of the IEEE 78(10) (1990) 1550–1560

10. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114 (2013)

11. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
In: Advances in Neural

Courville, A., Bengio, Y.: Generative adversarial nets.
Information Processing Systems. (2014) 2672–2680

12. Denton, E.L., Chintala, S., Fergus, R., et al.: Deep generative image models using
a laplacian pyramid of adversarial networks. In: Advances in Neural Information
Processing Systems. (2015) 1486–1494

13. Efros, A., Leung, T.K., et al.: Texture synthesis by non-parametric sampling.
In: Computer Vision, 1999. The Proceedings of the Seventh IEEE International
Conference on. Volume 2., IEEE (1999) 1033–1038

14. Wei, L.Y., Levoy, M.: Fast texture synthesis using tree-structured vector quanti-
zation. In: Proceedings of the 27th annual conference on Computer graphics and
interactive techniques, ACM Press/Addison-Wesley Publishing Co. (2000) 479–488
15. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-
In: Proceedings of the 28th annual conference on Computer graphics and

gies.
interactive techniques, ACM (2001) 327–340

16. Efros, A.A., Freeman, W.T.: Image quilting for texture synthesis and transfer. In:
Proceedings of the 28th annual conference on Computer graphics and interactive
techniques, ACM (2001) 341–346

17. Hays, J., Efros, A.A.: Scene completion using millions of photographs. ACM

Transactions on Graphics (TOG) 26(3) (2007) 4

18. Lalonde, J.F., Hoiem, D., Efros, A.A., Rother, C., Winn, J., Criminisi, A.: Photo

clip art. ACM transactions on graphics (TOG) 26(3) (2007) 3

16

Zhirong Wu, Dahua Lin, Xiaoou Tang

19. Cross, G.R., Jain, A.K.: Markov random ﬁeld texture models. Pattern Analysis

and Machine Intelligence, IEEE Transactions on (1) (1983) 25–39

20. Boykov, Y.Y., Jolly, M.P.: Interactive graph cuts for optimal boundary & region
segmentation of objects in nd images.
In: Computer Vision, 2001. ICCV 2001.
Proceedings. Eighth IEEE International Conference on. Volume 1., IEEE (2001)
105–112

21. He, X., Zemel, R.S., Carreira-Perpi˜n´an, M. ´A.: Multiscale conditional random ﬁelds
for image labeling. In: Computer vision and pattern recognition, 2004. CVPR 2004.
Proceedings of the 2004 IEEE computer society conference on. Volume 2., IEEE
(2004) II–695

22. Geman, S., Geman, D.: Stochastic relaxation, gibbs distributions, and the bayesian
restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transac-
tions on (6) (1984) 721–741

23. Ising, E.: Beitrag zur theorie des ferromagnetismus. Zeitschrift f¨ur Physik A

Hadrons and Nuclei 31(1) (1925) 253–258

24. Rue, H., Held, L.: Gaussian Markov random ﬁelds: theory and applications. CRC

Press (2005)

25. Zhu, S.C., Wu, Y., Mumford, D.: Filters, random ﬁelds and maximum entropy
(frame): Towards a uniﬁed theory for texture modeling. International Journal of
Computer Vision 27(2) (1998) 107–126

26. Roth, S., Black, M.J.: Fields of experts: A framework for learning image priors.
In: Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
Society Conference on. Volume 2., IEEE (2005) 860–867

27. Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan,
S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual
recognition and description. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. (2015) 2625–2634

28. Mnih, V., Heess, N., Graves, A., et al.: Recurrent models of visual attention. In:

Advances in Neural Information Processing Systems. (2014) 2204–2212

29. Gregor, K., Danihelka, I., Graves, A., Wierstra, D.: Draw: A recurrent neural

network for image generation. arXiv preprint arXiv:1502.04623 (2015)

30. Graves, A., Schmidhuber, J.: Oﬄine handwriting recognition with multidimen-
sional recurrent neural networks. In: Advances in neural information processing
systems. (2009) 545–552

31. Graves, A., Fernandez, S., Schmidhuber, J.: Multi-dimensional recurrent neural

networks. arXiv preprint arXiv:0705.2011 (2007)

32. Byeon, W., Breuel, T.M., Raue, F., Liwicki, M.: Scene labeling with lstm recurrent
neural networks. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2015) 3547–3555

33. Bell, S., Zitnick, C.L., Bala, K., Girshick, R.:

Inside-outside net: Detecting ob-
jects in context with skip pooling and recurrent neural networks. arXiv preprint
arXiv:1512.04143 (2015)

34. Theis, L., Bethge, M.: Generative image modeling using spatial lstms. In: Advances

in Neural Information Processing Systems. (2015) 1918–1926

35. Oord, A.v.d., Kalchbrenner, N., Kavukcuoglu, K.: Pixel recurrent neural networks.

arXiv preprint arXiv:1601.06759 (2016)

36. Rangarajan, A., Chellappa, R., Manjunath, B.: Markov random ﬁelds and neural

networks with applications to early vision problems. Citeseer (1991)

37. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang,
C., Torr, P.H.: Conditional random ﬁelds as recurrent neural networks. In: Proceed-
ings of the IEEE International Conference on Computer Vision. (2015) 1529–1537

Deep Markov Random Field for Image Modeling

17

38. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. arXiv
preprint arXiv:1412.7062 (2014)

39. Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.: Joint training of a convolutional
network and a graphical model for human pose estimation. In: Advances in neural
information processing systems. (2014) 1799–1807

40. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in neural information processing systems.
(2012) 1097–1105

41. Pearl, J.: Probabilistic reasoning in intelligent systems: networks of plausible in-

42. Li, S.Z.: Markov random ﬁeld modeling in image analysis. Springer Science &

ference. Morgan Kaufmann (2014)

Business Media (2009)

43. Salakhutdinov, R.R.: Learning in markov random ﬁelds using tempered transitions.

In: Advances in neural information processing systems. (2009) 1598–1606

44. Duchi, J., Hazan, E., Singer, Y.: Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Machine Learning Research 12 (2011)
2121–2159

45. Graves, A.: Generating sequences with recurrent neural networks. arXiv preprint

arXiv:1308.0850 (2013)

46. Dempster, A., Laird, N., Rubin, D.: Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical Society 39(1) (1977) 1–38
47. Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. Signal Pro-

cessing, IEEE Transactions on 45(11) (1997) 2673–2681

48. Berglund, M., Raiko, T., Honkala, M., K¨arkk¨ainen, L., Vetek, A., Karhunen, J.:
Bidirectional recurrent neural networks as generative models-reconstructing gaps
in time series. arXiv preprint arXiv:1504.01575 (2015)

49. Brodatz, P., Textures, A.: A photographic album for artists and designers. 1966.

Images downloaded in July (2009)

50. Lab, M.M.: Vision Texture Database. http://http://vismod.media.mit.edu

(2002)

51. Timofte, R., De Smet, V., Van Gool, L.: A+: Adjusted anchored neighborhood
regression for fast super-resolution. In: Computer Vision–ACCV 2014. Springer
(2014) 111–126

52. Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network
In: Computer Vision–ECCV 2014. Springer (2014)

for image super-resolution.
184–199

53. Huang, J.B., Singh, A., Ahuja, N.: Single image super-resolution from transformed
self-exemplars. In: Computer Vision and Pattern Recognition (CVPR), 2015 IEEE
Conference on, IEEE (2015) 5197–5206

54. Wang, Z., Liu, D., Yang, J., Han, W., Huang, T.: Deep networks for image super-
resolution with sparse prior. In: Proceedings of the IEEE International Conference
on Computer Vision. (2015) 370–378

55. Freeman, W., Liu, C.: Markov random ﬁelds for super-resolution and texture
synthesis. Advances in Markov Random Fields for Vision and Image Processing 1
(2011)

56. Bevilacqua, M., Roumy, A., Guillemot, C., Alberi-Morel, M.L.: Low-complexity

single-image super-resolution based on nonnegative neighbor embedding. (2012)

57. Martin, D., Fowlkes, C., Tal, D., Malik, J.: A database of human segmented natural
images and its application to evaluating segmentation algorithms and measuring

18

Zhirong Wu, Dahua Lin, Xiaoou Tang

ecological statistics. In: Proc. 8th Int’l Conf. Computer Vision. Volume 2. (July
2001) 416–423

58. Shotton, J., Winn, J., Rother, C., Criminisi, A.: Textonboost for image understand-
ing: Multi-class object recognition and segmentation by jointly modeling texture,
layout, and context. International Journal of Computer Vision 81(1) (2009) 2–23
59. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database: Large-scale
scene recognition from abbey to zoo. In: Computer vision and pattern recognition
(CVPR), 2010 IEEE conference on, IEEE (2010) 3485–3492

60. Radford, A., Metz, L., Chintala, S.:

ing with deep convolutional generative adversarial networks.
arXiv:1511.06434 (2015)

Unsupervised representation learn-
arXiv preprint

6
1
0
2
 
p
e
S
 
7
 
 
]

V
C
.
s
c
[
 
 
1
v
6
3
0
2
0
.
9
0
6
1
:
v
i
X
r
a

Deep Markov Random Field for Image Modeling

Zhirong Wu

Dahua Lin

Xiaoou Tang

The Chinese University of Hong Kong

Abstract. Markov Random Fields (MRFs), a formulation widely used
in generative image modeling, have long been plagued by the lack of ex-
pressive power. This issue is primarily due to the fact that conventional
MRFs formulations tend to use simplistic factors to capture local pat-
terns. In this paper, we move beyond such limitations, and propose a
novel MRF model that uses fully-connected neurons to express the com-
plex interactions among pixels. Through theoretical analysis, we reveal
an inherent connection between this model and recurrent neural net-
works, and thereon derive an approximated feed-forward network that
couples multiple RNNs along opposite directions. This formulation com-
bines the expressive power of deep neural networks and the cyclic de-
pendency structure of MRF in a uniﬁed model, bringing the modeling
capability to a new level. The feed-forward approximation also allows it
to be eﬃciently learned from data. Experimental results on a variety of
low-level vision tasks show notable improvement over state-of-the-arts.

Keywords: Generative image model, MRF, RNN

1 Introduction

Generative image models play a crucial role in a variety of image processing and
computer vision tasks, such as denoising [1], super-resolution [2], inpainting [3],
and image-based rendering [4]. As repeatedly shown by previous work [5], the
success of image modeling, to a large extent, hinges on whether the model can
successfully capture the spatial relations among pixels.

Existing image models can be roughly categorized as global models and low-
level models. Global models [6,7,8] usually rely on compressed representations
to capture the global structures. Such models are typically used for describing
objects with regular structures, e.g. faces. For generic images, low-level models
are more popular. Thanks to their focus on local patterns instead of global
appearance, low-level models tend to generalize much better, especially when
there can be vast variations in the image content.

Over the past decades, Markov Random Fields (MRFs) have evolved into
one of the most popular models for low-level vision. Speciﬁcally, the clique-based
structure makes them particularly well suited for capturing local relations among
pixels. Whereas MRFs as a generic mathematical framework are very ﬂexible
and provide immense expressive power, the performance of many MRF-based
methods still leaves a lot to be desired when faced with challenging conditions.

2

Zhirong Wu, Dahua Lin, Xiaoou Tang

Fig. 1: We present a new class of markov random ﬁeld models whose potential
functions are expressed by powerful deep neural networks. We show applications
of the model on texture synthesis, image super-resolution and image synthesis.

This occurs due to the widespread use of simplistic potential functions that
largely restrict the expressive power of MRFs.

In recent years, the rise of Deep Neural Networks (DNN) has profoundly
reshaped the landscape of many areas in computer vision. The success of DNNs
is primarily attributed to its unparalleled expressive power, particularly their
strong capability of modeling complex variations. However, DNNs in computer
vision are mostly formulated as end-to-end convolutional networks (CNN) for
classiﬁcation or regression. The modeling of local interactions among pixels,
which is crucial for many low-level vision tasks, has not been suﬃciently explored.
The respective strengths of MRFs and DNNs inspire us to explore a new
approach to low-level image modeling, that is, to bring the expressive power
of DNNs to an MRF formulation. Speciﬁcally, we propose a generative image
model comprised of a grid of hidden states, each corresponding to a pixel. These
latent states are connected to their neighbors – together they form an MRF.
Unlike in classical MRF formulations, we use fully connected layers to express
the relationship among these variables, thus substantially improving the model’s
ability to capture complex patterns.

Through theoretical analysis, we reveal an inherent connection between our
MRF formulation and the RNN [9], which opens an alternative way to MRF
formulation. However, they still diﬀer fundamentally: the dependency structure
of an RNN is acyclic, while that of an MRF is cyclic. Consequently, the hidden
states cannot be inferred in a single feed-forward manner as in a RNN. This posts
a signiﬁcant challenge – how can one derive the back-propagation procedure
without a well-deﬁned forward function?

Our strategy to tackle this diﬃculty is to unroll an iterative inference pro-
cedure into a feed-forward function. This is motivated by the observation that
while the inference is iterative, each cycle of updates is still a feed-forward proce-
dure. Following a carefully devised scheduling policy, which we call the Coupled

Deep Markov Random Field for Image Modeling

3

Acyclic Passes (CAP), the inference can be unrolled into multiple RNNs op-
erating along opposite directions that are coupled together. In this way, local
information can be eﬀectively propagated over the entire network, where each
hidden state can have a complete picture of its context from all directions.

The primary contribution of this work is a new generative model that uniﬁes
MRFs and DNNs in a novel way, as well as a new learning strategy that makes
it possible to learn such a model using mainstream deep learning frameworks.
It is worth noting that the proposed method is generic and can be adapted to a
various problems. In this work, we test it on a variety of low-level vision tasks,
including texture synthesis, image super-resolution, and image synthesis.

2 Related Works

In this paper, we develop a generative image model that incorporates the ex-
pressive power of deep neural networks with an MRF. This work is related to
several streams of research eﬀorts, but moves beyond their respective limitations.

Generative image models. Generative image models generally fall into two
categories: parametric models and non-parametric models. Parametric models
typically use a compressed representation to capture an image’s global appear-
ance. In recent years, deep networks such as autoencoders [10] and adversarial
networks [11,12] have achieved substantial improvement in generating images
with regular structures such as faces or digits. Non-parametric models, including
pixel-based sampling [13,14,15] and patch-based sampling [16,17,18], instead rely
on a large set of exemplars to capture local patterns. Whereas these methods can
produce high quality images with local patterns directly sampled from realistic
images. Exhaustive search over a large exemplar set limits their scalability and
often leads to computational diﬃculties. Our work draws inspiration from both
lines of work. By using DNNs to express local interactions in an MRF, our model
can capture highly complex patterns while maintaining strong scalability.

Markov random ﬁelds. For decades, MRFs have been widely used for low-level
vision tasks, including texture synthesis [19], segmentation [20,21], denoising [1],
and super-resolution [2]. Classical MRF models in earlier work [22] use simple
hand-crafted potentials (e.g., Ising models [23], Gaussian MRFs [24]) to link
neighboring pixels. Later, more ﬂexible models such as FRAME [25] and Fields
of Experts [26] were proposed, which allow the potential functions to be learned
from data. However, in these methods, the potential functions are usually pa-
rameterized as a set of linear ﬁlters, and therefore their expressive power remains
very limited.

Recurrent neural networks. Recurrent neural networks (RNNs), a special
family of deep models, use a chain of nonlinear units to capture sequential rela-
tions. In computer vision, RNNs are primarily used to model sequential changes
in videos [27], visual attention [28,29], and hand-written digit recognition [30].
Previous work explores multi-dimensional RNNs [31] for scene labeling [32] as
well as object detections [33]. The most related work is perhaps the use of 2D

4

Zhirong Wu, Dahua Lin, Xiaoou Tang

Fig. 2: Graphical model of deep MRFs. Left: The hidden states and the pixels
together form an MRF. Right: Each hidden state connects to the neighboring
states, the neighboring pixels, and the pixel at the same location.

RNNs for generating gray-scale textures [34] or color images [35]. A key distinc-
tion of these models from ours is that 2D RNNs rely on an acyclic graphs to
model spatial dependency, e.g. each pixel depends only on its left and upper
neighbors – this severely limits the spatial coherence. Our model, instead, allows
dependencies from all directions via iterative inference unrolling.

MRF and neural networks. Connections between both models have been
discussed long ago [36]. With the rise of deep learning, recent work on image
segmentation [37,38] uses mean ﬁeld method to approximate a conditional ran-
dom ﬁeld (CRF) with CNN layers. A hybrid model of CNN and MRF has also
been proposed for human pose estimation [39]. These works primarily target
prediction problems (e.g. segmentation) and are not as eﬀective at capturing
complex pixel patterns in a purely generative way.

3 Deep Markov Random Field

The primary goal of this work is to develop a generative model for images that
can express complex local relationships among pixels while being tractable for
inference and learning. Formally, we consider an image, denoted by x, as an
undirected graph with a grid structure, as shown in Figure 2 left. Each node u
corresponds to a pixel xu. To capture the interactions among pixels, we intro-
duce, hu, a hidden variable for each pixel denoting the hidden state corresponding
to the pixel xu. In the graph, each node u has a neighborhood, denoted by Nu.
Particularly, we use the 4-connected neighborhood of a 2D grid in this work.

Joint Distribution. We consider three kinds of dependencies: (1) the depen-
dency between a pixel xu and its corresponding hidden state hu, (2) the depen-
dency between a hidden state hu and a neighbor hv with v ∈ Nu, and (3) the
dependency between a hidden state hu and a neighboring pixel xv. They are
respectively captured by factors ζ(xu, hu), φ(hu, hv), and ψ(hu, xv). In addition,
we introduce a regularization factor λ(hu) for each hidden state, which gives us
the leeway to encourage certain distribution over the state values. Bringing these

Deep Markov Random Field for Image Modeling

5

factors together, we formulate an MRF to express the joint distribution:

p(x, h) =

ζ(xu, hu)

(φ(hu, hv)ψ(hu, xv)ψ(hv, xu))

λ(hu). (1)

1
Z

(cid:89)

u∈V

(cid:89)

(u,v)∈E

(cid:89)

u∈V

Here, V and E are respectively the set of vertices and that of the edges in the
image graph, Z is a normalizing constant. Figure 2 shows it structure.

Choices of Factors. Whereas the MRF provides a principled way to express
the dependency structure, the expressive power of the model still largely depends
on the speciﬁc forms of the factors that we choose. For example, the modeling
capacity of classical MRF models are limited by their simplistic factors.

Below, we discuss the factors that we choose for the proposed model. First,
the factor ζ(xu, hu) determines how the pixel values are generated from the hid-
den states. Considering the stochastic nature of natural images, we formalize this
generative process as a Gaussian mixture model (GMM). The rationale behind
is that pixel values are on a low-dimensional space, where a GMM with a small
number of components can usually provide a good approximation to an empir-
ical distribution. Speciﬁcally, we ﬁx the number of components to be K, and
consider the concatenation of component parameters as the linear transform of
the hidden state, hT
u, µc
c=1, where Q is a weight matrix of model
parameters. In this way, the factor ζ(xu, hu) can be written as

u Q = ((πc

u, Σc

u))K

(2)

(3)

(4)

ζ(xu, hu) (cid:44) pGMM(xu|hu) =

uN (xu|µc
πc

u, Σc

u).

K
(cid:88)

c=1

To capture the rich interactions among pixels and their neighbors, we formulate
the relational factors φ(hu, hv) and ψ(hu, xv) with fully connected forms:

φ(hu, hv) = exp (cid:0)hT

u Whv

(cid:1) , ψ(hu, xv) = exp (cid:0)hT

u Rxv

(cid:1) .

Finally, to control the value distribution of the hidden states, we further incor-
porate a regularization term over hu, as

λ(hu) = exp (cid:0)−1T η(hu)(cid:1) = exp

(cid:16)

−η(h(1)

u ) − · · · − η(h(d)
u )

(cid:17)

.

Here, η is an element-wise nonlinear function and d is the dimension of hu. In
summary, the use of GMM in ζ(xu, hu) eﬀectively accounts for the variations
in pixel generation, the fully-connected factors φ(hu, hv) and ψ(hu, xv) enable
the modeling of complex interactions among neighbors, while the regularization
term λ(hu) provides a way to explicitly control the distribution of hidden states.
Together, they substantially increase the capacity of the MRF model.

Inference of Hidden States. With this MRF formulation, the posterior dis-
tribution of the hidden state hu, conditioned on all other variables, is given by

p (hu | xu, xNu , hNu ) ∝ ζ(xu, hu)λ(hu) ·

φ(hu, hv)ψ(hu, xv).

(5)

(cid:89)

v∈Nu

6

Zhirong Wu, Dahua Lin, Xiaoou Tang

h approximation

ReLU

Regularizer

Sigmoid

Regularizer

Fig. 3: Left shows the numerical simulation of approximated inference for the
hidden variables. Right shows the ReLU, sigmoid activation function and their
corresponding regularizations for the hidden variables.

Here, hu depends on its neighboring states, the corresponding pixel values, as
well as that of its neighbors. Since the pixel xu and its neighboring pixels xNu
are highly correlated, to simplify our later computations, we approximate the
posterior distribution as,

p (hu | xu, xNu , hNu ) (cid:39) p (hu | xNu , hNu ) ∝ λ(h)

φ(h, hv)ψ(h, xv).

(6)

(cid:89)

v∈Nu

We performed numerical simulations for this approximation. They are indeed
very close to each other, as illustrated in Figure 3. Consequently, the MAP
estimate of hu can be approximately computed from its neighbors. It turns out
that this optimization problem has an analytic solution given by,

˜hu = σ

Whv + Rxv

.

(cid:33)

(cid:32)

(cid:88)

v∈Nu

(7)

Here, σ is an element-wise function that is related to η as σ−1(z) = η(cid:48)(z), where
η(cid:48) is the ﬁrst-order derivative w.r.t. η, and σ−1 the inverse function of σ.

Connections to RNNs. We observe that Eq.(7) has a form that is similar
to the feed-forward computations in Recurrent Neural Networks (RNN) [9]. In
this sense, we can view the feed-forward RNN as an MAP inference process for
MRF models. Particularly, given the RNN computations in the form of Eq.(7),
one can formulate an MRF as in Eq.(1), where regularization function η can be
derived from σ according to the relation σ−1(z) = η(cid:48)(z), as

η(h) =

σ−1(z)dz + C.

(8)

(cid:90) h

b

Here, b is the minimum of the domain of h, which can be −∞, and C is an
arbitrary constant. This connection provides an alternative way to formulate
an MRF model. More importantly, in this way, RNN models that have been
proven to be successful can be readily transferred to an MRF formulation. Fig-
ure 3 shows the regularization functions η(h) corresponding to popular activation
functions in RNNs, such as sigmoid and ReLU [40].

Deep Markov Random Field for Image Modeling

7

4 Learning via Coupled Recurrent Networks

Except for special cases [41], inference and learning on MRFs is generally in-
tractable. Conventional estimation methods [42,8,43] either take overly long
time to train or tend to yield poor estimates, especially for models with a high-
dimensional parameter space. In this work, we consider an alternative approach
to MRF learning, which allows us to draw on deep learning techniques [44,45]
that have been proven to be highly eﬀective [40].

Variational Learning Principle. Estimation of probabilistic models based on
the maximum likelihood principle is often intractable when the model contains
hidden variables. Expectation-maximization [46] is one of the most widely used
ways to tackle this problem, which iteratively calculates the posterior distribu-
tion of hi (in E-steps) and then optimizes θ (in M-steps) as

ˆθ = argmax

θ

1
n

n
(cid:88)

i=1

Ep(hi|xi,θ) {log p(xi, hi|θ)} .

(9)

Here, θ = {W, Q, R} is the model parameter, xi is the i-th image, and hi is the
corresponding hidden state. As exact computation of this posterior expectation
is intractable, we approximate it based on ˜hi, the MAP estimate of hi, as below:

ˆθ = argmax

θ

1
n

n
(cid:88)

i=1

log p(xi|˜hi, θ), with ˜hi (cid:44) f (xi, θ).

(10)

This is the learning objective of our model. Here, f is the function that approxi-
mately infers the latent state ˜hi given an observed image xi. When the posterior
distribution p(hi|xi, θ) is highly concentrated, which is often the case in vision
tasks, this is a good approximation. For an image x, log p(x|˜h, θ) can be further
expanded as a sum of terms deﬁned on individual pixels:

(cid:88)

u

u((cid:80)

log p(x|˜h, θ) =

log pGMM(xu|˜h) =

πc
uN (xu| ˜µc

u, Σc

u),

(11)

(cid:88)

log

u

K
(cid:88)

c=1

u = µc

u + Σc

where ˜µc
v )R. For our problem, this learning principle can be
interpreted in terms of encoding/decoding – the hidden states ˜h = f (x, θ) can
be understood as an representation that encodes the observed patterns in an
image x, while log p(x|˜h, θ) measures how well ˜h explains the observations.

v hT

Coupled Acyclic Passes. In the proposed model, the dependencies among
neighbors are cyclic. Hence, the MAP estimate ˜h = f (x, θ) cannot be computed
in a single forward pass. Instead, Eq.(7) needs to be applied across the graph
in multiple iterations. Our strategy is to unroll this iterative inference proce-
dure into multiple feed-forward passes along opposite directions, such that these
passes together provide a complete context to each local estimate.

8

Zhirong Wu, Dahua Lin, Xiaoou Tang

Fig. 4: Coupled acyclic passes. We decouple an undirected cyclic graph into two di-
rected acyclic graphs with each one allowing feed-forward computation. Inference is
performed by alternately traversing the two acyclic graphs, while coupling their infor-
mation at each step.

Speciﬁcally, we decompose the underlying dependency graph G = (V, E),
which is undirected, into two acyclic directed graphs Gf = (V, Ef ) and Gb =
(V, Eb), as illustrated in Figure 4, such that each undirected edge {u, v} ∈ E
corresponds uniquely to an edge (u, v) ∈ Ef and an opposite edge (v, u) ∈ Eb.
It can be proved that such a decomposition always exists and that for each node
u ∈ V , the neighborhood Nu can be expressed as Nu = N f (u) ∪ N b(u), where
N f (u) and N b(u) are the set of parents of u respectively along Gf and Gb.

Given such a decomposition, we can derive an iterative computational pro-
cedure, where each cycle couples a forward pass that applies Eq.(7) along Gf
and a backward pass 1 along Gb. After the t-th cycle, the state hu is updated to

h(t)
u = σ

(cid:32)

(cid:88)

(cid:16)

v∈N f (u)

Wh(t−1)
v

+ Rxv

+

Wh(t)

v + Rxv

(cid:17)

(cid:88)

(cid:16)

(cid:17)

(cid:33)
.

(12)

v∈N b(u)

As states above, we have Nu = N f (u) ∪ N b(u). Therefore, over a cycle, the
updated state hu would incorporate information from all its neighbors. Note
that a given graph G can be decomposed in many diﬀerent ways. In this work,
we speciﬁcally choose the one that forms the zigzag path. The advantage over a
simple raster line order is that zigzag path traverses all the nodes continuously,
so that it conserves spatial coherence by making dependence of each node to
all the previous nodes that have been visited before. The forward and backward
passes resulted from such decomposition are shown in Figure 4.

This algorithm has two important properties: First, the acyclic decomposi-
tion allows feed-forward computation as in Eq.(7) to be applied. As a result, the
entire inference procedure can be viewed as a feed-forward network that couples
multiple RNNs operating along diﬀerent directions. Therefore, it can be learned
in a way similar to other deep neural networks, using Stochastic Gradient Descent
(SGD). Second, the feedback mechanism embodied by the backward pass facil-

1 The word forward and backward here means the sequential order in the graph. They
are not feed-forward and back-propagation in the context of deep neural networks.

Deep Markov Random Field for Image Modeling

9

itates the propagation of local information and thus the learning of long-range
dependencies.

Discussions with 2D-RNN. Previous work has explored two-dimensional
extensions of RNN [31], often referred to as 2D-RNN. Such extensions, however,
are formulated upon an acyclic graph, and can be considered as a trimmed down
version of our algorithm. A major drawback of 2D-RNN is that it scans the image
in a raster line order and it is not able to provide a feedback path. Therefore,
the inference of each hidden state can only take into account 1/4 of the context,
and there is no way to recover from a poor inference. As we will show in our
experiments, this may cause undesirable eﬀects. Whereas bidirectional RNNs [47]
may partly mitigate this problem, they decouple the hidden states into multiple
ones that are independent apriori, which would lead to consistency issues. Recent
work [48] also ﬁnds it diﬃcult to use in generative modeling.

Implementation Details For inference and learning, to make the computation
feasible, we just take one forward pass and one backward pass. Thus, each node
is only updated twice while being able to use the information from all possible
contexts. The training patch size varies from 15 to 25 depending on the speciﬁc
experiment. Overall, if we unroll the full inference procedure, our model2 is more
than thousands of layers deep. We use rmsprop [45] for optimization and we don’t
use dropout for regularization, as we ﬁnd it oscillates the training.

5 Experiments

In the following experiments, we test the proposed deep MRF on 3 scenarios for
modeling natural images. We ﬁrst study its basic properties on texture synthesis,
and then we apply it on a prediction problem, image super-resolution. Finally, we
integrate global CNN models with local deep MRF for natural image synthesis.

5.1 Texture Synthesis

The task of texture synthesis is to synthesize new texture images that possess
similar patterns and statistical characteristics as a given texture sample. The
study of this problem originated from graphics [13,14]. The key to successful
texture reproduction, as we learned from previous work, is to eﬀectively capture
the local patterns and variations. Therefore, this task is a perfect testbed to
assess a model’s capability of modeling visual patterns.

Our model works in a purely generative way. Given a sample texture, we train
the model on randomly extracted patches of size 25 × 25, which are larger than
most texels in natural images. We set K = 20, initialize x and h to zeros, and
train the model with back-propagation along the coupled acyclic graph. With
a trained model, we can generate textures by running the RNN to derive the

2 code available at https://github.com/zhirongw/deep-mrf

10

Zhirong Wu, Dahua Lin, Xiaoou Tang

D12

D34

D104

ﬂowers

bark

clouds

t
u
p
n
i

]
4
3
[
N
N
R
D
2

]
3
1
[
s
c
i
h
p
a
r
g

s
r
u
o

Fig. 5: Texture synthesis results.

latent states and at the same time sampling the output pixels. As our model is
stationary, it can generate texture images of arbitrary sizes.

We work on two texture datasets, Brodatz [49] for grayscale images, and
VisTex [50] for color images. From the results shown in Figure 5, our synthesis
visually resembles to high resolution natural images, and the quality is close to
the non-parametric approach [13]. We also compare with the 2D-RNN. [34]. As
we can see, the results obtained using 2D-RNN, which synthesizes based only
on the left and upper regions, exhibit undesirable eﬀects and often evolve into
blacks in the bottom-right parts.

Two fundamental parameters control the behaviors of our texture model.
The training patch size decides the farthest spatial relationships that could be
learned from data. The number of gaussian mixtures control the dynamics of
the texture landscape. We analyze our model by changing the two parameters.
As shown in Figure 6, bigger training patch size and bigger number of mixtures
consistently improves the results. For non-parametric approaches, bigger patch
size would dramatically bring up the computation cost. While for our model,
the inference time holds the same regardless of the patch size that the model
is trained on. Moreover, our parametric model is able to scale to large dataset
without bringing additional computations.

5.2

Image Super-Resolution

Image super-resolution is a task to produce a high resolution image given a single
low resolution one. Whereas previous MRF-based models [2,55] work reasonably,

Deep Markov Random Field for Image Modeling

11

5

10

15

20

25

input

1

2

5

10

20

input

e
z
i
s

h
c
t
a
p

g
n
i
n
i
a
r
t

s
e
r
u
t
x
i
m

f
o

r
e
b
m
u
n

Fig. 6: Texture synthesis by varying the patch size and the number of mixtures.

the quality of their products is inferior to the state-of-the-art models based on
deep learning [52,54]. With deep MRF, we wish to close the gap.

Unlike in texture synthesis, the generation of this task is driven by a low-
resolution image. To incorporate this information, we introduce additional con-
nections between the hidden states and corresponding pixels of the low-resolution
image, as shown in Figure 7. It is noteworthy that we just input a single pixel
(instead of a patch) at each site, and in this way, we can test whether the model
can propagate information across the spatial domain. As the task is determinis-
tic, we use a GMM with a single component and ﬁx its variance. In the testing
stage, we output the mean of the Gaussian component at each location as the
inferred high-resolution pixel. This approach is very generic – the model is not
speciﬁcally tuned for the task and no pre- and post-processing steps are needed.
We train our model on a widely used super-resolution dataset [56] which
contains 91 images, and test it on Set5, Set14, and BSD100 [57]. The training
is on patches of size 16 × 16 and rmsprop with momentum 0.95 is used. We use
PSNR for quantitative evaluation. Following previous work, we only consider the
luminance channel in the YCrCb color space. The two chrominance channels are
upsampled with bicubic interpolation.

As shown in Table 1 and Table 2, our approach outperforms the CNN-based
baseline [52] and compares favorably with the state-of-the-art methods dedicated
to this task [53,54]. One possible explanation for the success is that our model
not only learns the mapping, but also learns the image statistics for high reso-

12

Zhirong Wu, Dahua Lin, Xiaoou Tang

Fig. 7: Adapting deep MRFs to speciﬁc applications. Image super-resolution:
the hidden state receives an additional connection from the low-resolution pixel.
Image synthesis: deep MRF renders the ﬁnal image from a spatial feature map,
which is jointly learned by a variational auto-encoder.

images

2x upscale

baby
bird

Table 1: PSNR (dB) on Set5 dataset with upscale factor 2,3,4.
4x upscale

37.07 38.30 38.48 38.31
36.81 40.40 40.50 40.36

3x upscale
Bicubic CNN SE Ours Bicubic CNN SE Ours Bicubic CNN SE Ours
33.91 35.01 35.22 35.15
31.78 32.98 33.14 32.94
32.58 34.91 35.58 36.14 30.18 31.98 32.54 32.49
butterﬂy 27.43 32.20 31.86 32.74 24.04 27.58 26.86 29.09 22.10 25.07 24.09 25.78
31.59 32.19 32.52 32.41
34.86 35.64 35.69 35.70 32.88 33.55 33.76 33.63
32.14 34.94 35.33 34.84
28.56 30.92 31.36 31.69 26.46 28.21 28.92 28.97
33.66 36.34 36.37 36.38 31.92 32.30 32.56 33.14 28.42 30.09 30.24 30.52

head
women
average

lution images. The training procedure which unrolls the RNN into thousands of
steps that share parameters also reduces the risk of overﬁtting. The results also
demonstrate the particular strength of our model in handling large upscaling
factors and diﬃcult images. Figure 8 shows several examples visually.

5.3 Natural Image Synthesis

Images can be roughly considered as a composition of textures with the guid-
ance of scene and object structures. In this task, we move beyond the synthesis
of homogeneous textures, and try to generate natural images with structural
guidance.

While our model excels in capturing spatial dependencies, learning weak
dependencies across the entire image is both computationally infeasible and an-
alytically ineﬃcient. Instead, we adopt a global model to capture the overall
structure and use it to provide contextual guidance to MRF. Speciﬁcally, we
incorporate the variational auto-encoder (VAE) [10] for this purpose – VAE
generates feature maps at each location and our model uses that feature to ren-
der the ﬁnal image (see Figure 7). Such features may contain information of
scene layouts, objects, and texture categories.

We train the joint model end-to-end from scratch. During each iteration, the
VAE ﬁrst encodes the image into a latent vector, then decodes it to a feature
map with the same size of the input image. We then connect this feature map

Deep Markov Random Field for Image Modeling

13

Table 2: PSNR (dB) on various dataset with upscale factor 3.
Dataset Bicubic A+ [51] CNN [52] SE [53] CSCN [54] Ours
33.14
32.30
29.38
29.00
28.54
28.20

30.39
27.54
BSD100 27.22

33.10
29.41
28.50

32.59
29.13
28.18

32.56
29.16
28.20

Set5
Set14

Original / PSNR

CNN / 31.65 dB

SE / 31.56 dB

Ours / 33.11 dB

Original / PSNR

CNN / 34.91 dB

SE / 35.58 dB

Ours / 36.14 dB

Original / PSNR

CNN / 27.58 dB

SE / 26.86 dB

Ours / 29.09 dB

Fig. 8: Image super resolution results from Set 5 with upscaling factor 3.

to the latent states of the deep MRF. The total loss is deﬁned as the addition of
gaussian mixtures at image space and KL divergence at high-level VAE latent
space. For training, we randomly extracts patches from the feature map. The
gradients from the deep MRF back to the VAE thus only cover the patches being
extracted. During testing, VAE randomly samples from the latent space and
decodes it to generate the global feature maps. The output pixels are sampled
from the GMM with 10 mixtures along the coupled acyclic graph.

We work on the MSRC [58] and SUN database [59] and select some scene cat-
egories with rich natural textures, such as Mountains and Valleys. Each category
contains about a hundred images. As we will see, our approach generalizes much
better than the data-hungry CNN approaches. We train the model on images of
size 64 × 64 with a batch size of 4. For each image, we extract 16 patches of size
15 × 15 for training. Figure 9 shows several images generated from our models,
in comparison with those obtained from the baselines, namely raw VAE [10] and
DCGAN [60]. The CNN architecture is shared for all methods described in the

14

Zhirong Wu, Dahua Lin, Xiaoou Tang

Deep MRF

VAE DCGAN

t
e
e
r
t
s

r
e
v
i
r

n
r
a
b

y
e
l
l
a
v

n
i
a
t
n
u
o
m

y
e
l
l
a

Fig. 9: Image synthesis results.

DCGAN paper [60] to ensure fair comparison. We can see our model successfully
captures a variety of local patterns, such as water, clouds, wall and trees. The
global appearance also looks coherent, real and dynamic. The state-of-the-art
CNN based models, which focuses too much on global structures, often yield
sub-optimal local eﬀects.

6 Conclusions

We present a new class of MRF model whose potential functions are expressed
by powerful fully-connected neurons. Through theoretical analysis, we draw close
connections between probabilistic deep MRFs and end-to-end RNNs. To tackle
the diﬃculty of inference in cyclic graphs, we derive a new framework that decou-
ples a cyclic graph with multiple coupled acyclic passes. Experimental results
show state-of-the-art results on a variety of low-level vision problems, which
demonstrate the strong capability of MRFs with expressive potential functions.

Acknowledgment. This work is supported by the Big Data Collaboration Re-
search grant (CUHK Agreement No. TS1610626) and the Early Career Scheme
(ECS) grant (No: 24204215). We also thank Aditya Khosla for helpful discussions
and comments on a draft of the manuscript.

Deep Markov Random Field for Image Modeling

15

References

1. Portilla, J., Strela, V., Wainwright, M.J., Simoncelli, E.P.: Image denoising us-
ing scale mixtures of gaussians in the wavelet domain. Image Processing, IEEE
Transactions on 12(11) (2003) 1338–1351

2. Freeman, W.T., Pasztor, E.C., Carmichael, O.T.: Learning low-level vision. Inter-

national journal of computer vision 40(1) (2000) 25–47

3. Bertalmio, M., Sapiro, G., Caselles, V., Ballester, C.: Image inpainting. In: Pro-
ceedings of the 27th annual conference on Computer graphics and interactive tech-
niques, ACM Press/Addison-Wesley Publishing Co. (2000) 417–424

4. McMillan, L., Bishop, G.: Plenoptic modeling: An image-based rendering system.
In: Proceedings of the 22nd annual conference on Computer graphics and interac-
tive techniques, ACM (1995) 39–46

5. Huang, J., Mumford, D.: Statistics of natural images and models. In: Computer
Vision and Pattern Recognition, 1999. IEEE Computer Society Conference on.
Volume 1., IEEE (1999)

6. Turk, M.A., Pentland, A.P.: Face recognition using eigenfaces. In: Computer Vision
and Pattern Recognition, 1991. Proceedings CVPR’91., IEEE Computer Society
Conference on, IEEE (1991) 586–591

7. Wright, J., Ma, Y., Mairal, J., Sapiro, G., Huang, T.S., Yan, S.: Sparse representa-
tion for computer vision and pattern recognition. Proceedings of the IEEE 98(6)
(2010) 1031–1044

8. Hinton, G.E.: Training products of experts by minimizing contrastive divergence.

Neural computation 14(8) (2002) 1771–1800

9. Werbos, P.J.: Backpropagation through time: what it does and how to do it.

Proceedings of the IEEE 78(10) (1990) 1550–1560

10. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114 (2013)

11. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
In: Advances in Neural

Courville, A., Bengio, Y.: Generative adversarial nets.
Information Processing Systems. (2014) 2672–2680

12. Denton, E.L., Chintala, S., Fergus, R., et al.: Deep generative image models using
a laplacian pyramid of adversarial networks. In: Advances in Neural Information
Processing Systems. (2015) 1486–1494

13. Efros, A., Leung, T.K., et al.: Texture synthesis by non-parametric sampling.
In: Computer Vision, 1999. The Proceedings of the Seventh IEEE International
Conference on. Volume 2., IEEE (1999) 1033–1038

14. Wei, L.Y., Levoy, M.: Fast texture synthesis using tree-structured vector quanti-
zation. In: Proceedings of the 27th annual conference on Computer graphics and
interactive techniques, ACM Press/Addison-Wesley Publishing Co. (2000) 479–488
15. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-
In: Proceedings of the 28th annual conference on Computer graphics and

gies.
interactive techniques, ACM (2001) 327–340

16. Efros, A.A., Freeman, W.T.: Image quilting for texture synthesis and transfer. In:
Proceedings of the 28th annual conference on Computer graphics and interactive
techniques, ACM (2001) 341–346

17. Hays, J., Efros, A.A.: Scene completion using millions of photographs. ACM

Transactions on Graphics (TOG) 26(3) (2007) 4

18. Lalonde, J.F., Hoiem, D., Efros, A.A., Rother, C., Winn, J., Criminisi, A.: Photo

clip art. ACM transactions on graphics (TOG) 26(3) (2007) 3

16

Zhirong Wu, Dahua Lin, Xiaoou Tang

19. Cross, G.R., Jain, A.K.: Markov random ﬁeld texture models. Pattern Analysis

and Machine Intelligence, IEEE Transactions on (1) (1983) 25–39

20. Boykov, Y.Y., Jolly, M.P.: Interactive graph cuts for optimal boundary & region
segmentation of objects in nd images.
In: Computer Vision, 2001. ICCV 2001.
Proceedings. Eighth IEEE International Conference on. Volume 1., IEEE (2001)
105–112

21. He, X., Zemel, R.S., Carreira-Perpi˜n´an, M. ´A.: Multiscale conditional random ﬁelds
for image labeling. In: Computer vision and pattern recognition, 2004. CVPR 2004.
Proceedings of the 2004 IEEE computer society conference on. Volume 2., IEEE
(2004) II–695

22. Geman, S., Geman, D.: Stochastic relaxation, gibbs distributions, and the bayesian
restoration of images. Pattern Analysis and Machine Intelligence, IEEE Transac-
tions on (6) (1984) 721–741

23. Ising, E.: Beitrag zur theorie des ferromagnetismus. Zeitschrift f¨ur Physik A

Hadrons and Nuclei 31(1) (1925) 253–258

24. Rue, H., Held, L.: Gaussian Markov random ﬁelds: theory and applications. CRC

Press (2005)

25. Zhu, S.C., Wu, Y., Mumford, D.: Filters, random ﬁelds and maximum entropy
(frame): Towards a uniﬁed theory for texture modeling. International Journal of
Computer Vision 27(2) (1998) 107–126

26. Roth, S., Black, M.J.: Fields of experts: A framework for learning image priors.
In: Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
Society Conference on. Volume 2., IEEE (2005) 860–867

27. Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan,
S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual
recognition and description. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. (2015) 2625–2634

28. Mnih, V., Heess, N., Graves, A., et al.: Recurrent models of visual attention. In:

Advances in Neural Information Processing Systems. (2014) 2204–2212

29. Gregor, K., Danihelka, I., Graves, A., Wierstra, D.: Draw: A recurrent neural

network for image generation. arXiv preprint arXiv:1502.04623 (2015)

30. Graves, A., Schmidhuber, J.: Oﬄine handwriting recognition with multidimen-
sional recurrent neural networks. In: Advances in neural information processing
systems. (2009) 545–552

31. Graves, A., Fernandez, S., Schmidhuber, J.: Multi-dimensional recurrent neural

networks. arXiv preprint arXiv:0705.2011 (2007)

32. Byeon, W., Breuel, T.M., Raue, F., Liwicki, M.: Scene labeling with lstm recurrent
neural networks. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. (2015) 3547–3555

33. Bell, S., Zitnick, C.L., Bala, K., Girshick, R.:

Inside-outside net: Detecting ob-
jects in context with skip pooling and recurrent neural networks. arXiv preprint
arXiv:1512.04143 (2015)

34. Theis, L., Bethge, M.: Generative image modeling using spatial lstms. In: Advances

in Neural Information Processing Systems. (2015) 1918–1926

35. Oord, A.v.d., Kalchbrenner, N., Kavukcuoglu, K.: Pixel recurrent neural networks.

arXiv preprint arXiv:1601.06759 (2016)

36. Rangarajan, A., Chellappa, R., Manjunath, B.: Markov random ﬁelds and neural

networks with applications to early vision problems. Citeseer (1991)

37. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang,
C., Torr, P.H.: Conditional random ﬁelds as recurrent neural networks. In: Proceed-
ings of the IEEE International Conference on Computer Vision. (2015) 1529–1537

Deep Markov Random Field for Image Modeling

17

38. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. arXiv
preprint arXiv:1412.7062 (2014)

39. Tompson, J.J., Jain, A., LeCun, Y., Bregler, C.: Joint training of a convolutional
network and a graphical model for human pose estimation. In: Advances in neural
information processing systems. (2014) 1799–1807

40. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in neural information processing systems.
(2012) 1097–1105

41. Pearl, J.: Probabilistic reasoning in intelligent systems: networks of plausible in-

42. Li, S.Z.: Markov random ﬁeld modeling in image analysis. Springer Science &

ference. Morgan Kaufmann (2014)

Business Media (2009)

43. Salakhutdinov, R.R.: Learning in markov random ﬁelds using tempered transitions.

In: Advances in neural information processing systems. (2009) 1598–1606

44. Duchi, J., Hazan, E., Singer, Y.: Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Machine Learning Research 12 (2011)
2121–2159

45. Graves, A.: Generating sequences with recurrent neural networks. arXiv preprint

arXiv:1308.0850 (2013)

46. Dempster, A., Laird, N., Rubin, D.: Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical Society 39(1) (1977) 1–38
47. Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. Signal Pro-

cessing, IEEE Transactions on 45(11) (1997) 2673–2681

48. Berglund, M., Raiko, T., Honkala, M., K¨arkk¨ainen, L., Vetek, A., Karhunen, J.:
Bidirectional recurrent neural networks as generative models-reconstructing gaps
in time series. arXiv preprint arXiv:1504.01575 (2015)

49. Brodatz, P., Textures, A.: A photographic album for artists and designers. 1966.

Images downloaded in July (2009)

50. Lab, M.M.: Vision Texture Database. http://http://vismod.media.mit.edu

(2002)

51. Timofte, R., De Smet, V., Van Gool, L.: A+: Adjusted anchored neighborhood
regression for fast super-resolution. In: Computer Vision–ACCV 2014. Springer
(2014) 111–126

52. Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network
In: Computer Vision–ECCV 2014. Springer (2014)

for image super-resolution.
184–199

53. Huang, J.B., Singh, A., Ahuja, N.: Single image super-resolution from transformed
self-exemplars. In: Computer Vision and Pattern Recognition (CVPR), 2015 IEEE
Conference on, IEEE (2015) 5197–5206

54. Wang, Z., Liu, D., Yang, J., Han, W., Huang, T.: Deep networks for image super-
resolution with sparse prior. In: Proceedings of the IEEE International Conference
on Computer Vision. (2015) 370–378

55. Freeman, W., Liu, C.: Markov random ﬁelds for super-resolution and texture
synthesis. Advances in Markov Random Fields for Vision and Image Processing 1
(2011)

56. Bevilacqua, M., Roumy, A., Guillemot, C., Alberi-Morel, M.L.: Low-complexity

single-image super-resolution based on nonnegative neighbor embedding. (2012)

57. Martin, D., Fowlkes, C., Tal, D., Malik, J.: A database of human segmented natural
images and its application to evaluating segmentation algorithms and measuring

18

Zhirong Wu, Dahua Lin, Xiaoou Tang

ecological statistics. In: Proc. 8th Int’l Conf. Computer Vision. Volume 2. (July
2001) 416–423

58. Shotton, J., Winn, J., Rother, C., Criminisi, A.: Textonboost for image understand-
ing: Multi-class object recognition and segmentation by jointly modeling texture,
layout, and context. International Journal of Computer Vision 81(1) (2009) 2–23
59. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database: Large-scale
scene recognition from abbey to zoo. In: Computer vision and pattern recognition
(CVPR), 2010 IEEE conference on, IEEE (2010) 3485–3492

60. Radford, A., Metz, L., Chintala, S.:

ing with deep convolutional generative adversarial networks.
arXiv:1511.06434 (2015)

Unsupervised representation learn-
arXiv preprint

