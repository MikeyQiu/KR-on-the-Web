More Data, More Relations, More Context and More Openness:
A Review and Outlook for Relation Extraction

Xu Han1∗ , Tianyu Gao1∗, Yankai Lin2∗, Hao Peng1, Yaoliang Yang1, Chaojun Xiao1,
Zhiyuan Liu1, Peng Li2, Maosong Sun1, Jie Zhou2
1 State Key Lab on Intelligent Technology and Systems,
Institute for Artiﬁcial Intelligence,
Department of Computer Science and Technology, Tsinghua University, Beijing, China
2Pattern Recognition Center, WeChat AI, Tencent Inc, China

Abstract

Relational facts are an important component of
human knowledge, which are hidden in vast
amounts of text. In order to extract these facts
from text, people have been working on rela-
tion extraction (RE) for years. From early pat-
tern matching to current neural networks, ex-
isting RE methods have achieved signiﬁcant
progress. Yet with explosion of Web text and
emergence of new relations, human knowl-
edge is increasing drastically, and we thus re-
quire “more” from RE: a more powerful RE
system that can robustly utilize more data, ef-
ﬁciently learn more relations, easily handle
more complicated context, and ﬂexibly gener-
alize to more open domains. In this paper, we
look back at existing RE methods, analyze key
challenges we are facing nowadays, and show
promising directions towards more powerful
RE. We hope our view can advance this ﬁeld
and inspire more efforts in the community.

1

Introduction

Relational facts organize knowledge of the world in
a triplet format. These structured facts act as an im-
port role of human knowledge and are explicitly or
implicitly hidden in the text. For example, “Steve
Jobs co-founded Apple” indicates the fact (Apple
Inc., founded by, Steve Jobs), and we can also
infer the fact (USA, contains, New York) from
“Hamilton made its debut in New York, USA”.

As these structured facts could beneﬁt down-
stream applications, e.g, knowledge graph comple-
tion (Bordes et al., 2013; Wang et al., 2014), search
engine (Xiong et al., 2017; Schlichtkrull et al.,
2018) and question answering (Bordes et al., 2014;
Dong et al., 2015), many efforts have been devoted
to researching relation extraction (RE), which
aims at extracting relational facts from plain text.
More speciﬁcally, after identifying entity mentions

∗ indicates equal contribution

(e.g., USA and New York) in text, the main goal
of RE is to classify relations (e.g., contains)
between these entity mentions from their context.
The pioneering explorations of RE lie in statisti-
cal approaches, such as pattern mining (Huffman,
1995; Califf and Mooney, 1997), feature-based
methods (Kambhatla, 2004) and graphical models
(Roth and Yih, 2002). Recently, with the develop-
ment of deep learning, neural models have been
widely adopted for RE (Zeng et al., 2014; Zhang
et al., 2015) and achieved superior results. These
RE methods have bridged the gap between unstruc-
tured text and structured knowledge, and shown
their effectiveness on several public benchmarks.

Despite the success of existing RE methods,
most of them still work in a simpliﬁed setting.
These methods mainly focus on training models
with large amounts of human annotations to
classify two given entities within one sentence
into pre-deﬁned relations. However, the real
world is much more complicated than this simple
setting: (1) collecting high-quality human annota-
tions is expensive and time-consuming, (2) many
long-tail relations cannot provide large amounts
of training examples, (3) most facts are expressed
by long context consisting of multiple sentences,
and moreover (4) using a pre-deﬁned set to cover
those relations with open-ended growth is difﬁcult.
Hence, to build an effective and robust RE sys-
tem for real-world deployment, there are still some
more complex scenarios to be further investigated.
In this paper, we review existing RE meth-
ods (Section 2) as well as latest RE explorations
(Section 3) targeting more complex RE scenarios.
Those feasible approaches leading to better RE
abilities still require further efforts, and here we
summarize them into four directions:

(1) Utilizing More Data (Section 3.1). Super-
vised RE methods heavily rely on expensive human
annotations, while distant supervision (Mintz et al.,

0
2
0
2
 
r
p
A
 
7
 
 
]
L
C
.
s
c
[
 
 
1
v
6
8
1
3
0
.
4
0
0
2
:
v
i
X
r
a

2009) introduces more auto-labeled data to allevi-
ate this issue. Yet distant methods bring noise ex-
amples and just utilize single sentences mentioning
entity pairs, which signiﬁcantly weaken extraction
performance. Designing schemas to obtain high-
quality and high-coverage data to train robust RE
models still remains a problem to be explored.

(2) Performing More Efﬁcient Learning (Sec-
tion 3.2). Lots of long-tail relations only contain a
handful of training examples. However, it is hard
for conventional RE methods to well generalize re-
lation patterns from limited examples like humans.
Therefore, developing efﬁcient learning schemas
to make better use of limited or few-shot examples
is a potential research direction.

(3) Handling More Complicated Context
(Section 3.3). Many relational facts are expressed
in complicated context (e.g. multiple sentences or
even documents), while most existing RE models
focus on extracting intra-sentence relations. To
cover those complex facts, it is valuable to investi-
gate RE in more complicated context.

(4) Orienting More Open Domains (Sec-
tion 3.4). New relations emerge every day from dif-
ferent domains in the real world, and thus it is hard
to cover all of them by hand. However, conven-
tional RE frameworks are generally designed for
pre-deﬁned relations. Therefore, how to automat-
ically detect undeﬁned relations in open domains
remains an open problem.

Besides the introduction of promising directions,
we also point out two key challenges for existing
methods: (1) learning from text or names (Sec-
tion 4.1) and (2) datasets towards special inter-
ests (Section 4.2). We hope that all these contents
could encourage the community to make further
exploration and breakthrough towards better RE.

2 Background and Existing Work

Information extraction (IE) aims at extracting struc-
tural information from unstructured text, which is
an important ﬁeld in natural language processing
(NLP). Relation extraction (RE), as an important
task in IE, particularly focuses on extracting rela-
tions between entities. A complete relation extrac-
tion system consists of a named entity recognizer to
identify named entities (e.g., people, organizations,
locations) from text, an entity linker to link enti-
ties to existing knowledge graphs (KGs, necessary
when using relation extraction for knowledge graph
completion), and a relational classiﬁer to determine

Figure 1: An example of RE. Given two entities and
one sentence mentioning them, RE models classify the
relation between them within a pre-deﬁned relation set.

relations between entities by given context.

Among these steps, identifying the relation is
the most crucial and difﬁcult task, since it requires
models to well understand the semantics of the con-
text. Hence, RE generally focuses on researching
the classiﬁcation part, which is also known as rela-
tion classiﬁcation. As shown in Figure 1, a typical
RE setting is that given a sentence with two marked
entities, models need to classify the sentence into
one of the pre-deﬁned relations1.

In this section, we introduce the development of
RE methods following the typical supervised set-
ting, from early pattern-based methods, statistical
approaches, to recent neural models.

2.1 Pattern Extraction Models

The pioneering methods use sentence analysis tools
to identify syntactic elements in text, then auto-
matically construct pattern rules from these ele-
ments (Soderland et al., 1995; Kim and Moldovan,
1995; Huffman, 1995; Califf and Mooney, 1997).
In order to extract patterns with better coverage
and accuracy, later work involves larger corpora
(Carlson et al., 2010), more formats of patterns
(Nakashole et al., 2012; Jiang et al., 2017), and
more efﬁcient ways of extraction (Zheng et al.,
2019). As automatically constructed patterns may
have mistakes, most of the above methods require
further examinations from human experts, which is
the main limitation of pattern-based models.

2.2 Statistical Relation Extraction Models

As compared to using pattern rules, statistical meth-
ods bring better coverage and require less human
efforts. Thus statistical relation extraction (SRE)
has been extensively studied.

One typical SRE approach is feature-based
methods (Kambhatla, 2004; Zhou et al., 2005;
Jiang and Zhai, 2007; Nguyen et al., 2007), which
design lexical, syntactic and semantic features for

1Sometimes there is a special class in the relation set in-
dicating that the sentence does not express any pre-speciﬁed
relation (usually named as N/A).

entity pairs and their corresponding context, and
then input these features into relation classiﬁers.

Due to the wide use of support vector machines
(SVM), kernel-based methods have been widely
explored, which design kernel functions for SVM
to measure the similarities between relation rep-
resentations and textual instances (Culotta and
Sorensen, 2004; Bunescu and Mooney, 2005; Zhao
and Grishman, 2005; Mooney and Bunescu, 2006;
Zhang et al., 2006b,a; Wang, 2008).

There are also some other statistical methods
focusing on extracting and inferring the latent in-
formation hidden in the text. Graphical meth-
ods (Roth and Yih, 2002, 2004; Sarawagi and Co-
hen, 2005; Yu and Lam, 2010) abstract the depen-
dencies between entities, text and relations in the
form of directed acyclic graphs, and then use infer-
ence models to identify the correct relations.

Inspired by the success of embedding models in
other NLP tasks (Mikolov et al., 2013a,b), there are
also efforts in encoding text into low-dimensional
semantic spaces and extracting relations from tex-
tual embeddings (Weston et al., 2013; Riedel et al.,
2013; Gormley et al., 2015). Furthermore, Bor-
des et al. (2013),Wang et al. (2014) and Lin et al.
(2015) utilize KG embeddings for RE.

Although SRE has been widely studied, it still
faces some challenges. Feature-based and kernel-
based models require many efforts to design fea-
tures or kernel functions. While graphical and em-
bedding methods can predict relations without too
much human intervention, they are still limited in
model capacities. There are some surveys system-
atically introducing SRE models (Zelenko et al.,
2003; Bach and Badaskar, 2007; Pawar et al., 2017).
In this paper, we do not spend too much space for
SRE and focus more on neural-based models.

2.3 Neural Relation Extraction Models

Neural relation extraction (NRE) models introduce
neural networks to automatically extract semantic
features from text. Compared with SRE models,
NRE methods can effectively capture textual infor-
mation and generalize to wider range of data.

Studies in NRE mainly focus on designing and
utilizing various network architectures to capture
the relational semantics within text, such as recur-
sive neural networks (Socher et al., 2012; Miwa
and Bansal, 2016) that learn compositional repre-
sentations for sentences recursively, convolutional
neural networks (CNNs) (Liu et al., 2013; Zeng

Figure 2: The performance of state-of-the-art RE mod-
els in different years on widely-used dataset SemEval-
2010 Task 8. The adoption of neural models (since
2013) has brought great improvement in performance.

et al., 2014; Santos et al., 2015; Nguyen and Grish-
man, 2015b; Zeng et al., 2015; Huang and Wang,
2017) that effectively model local textual patterns,
recurrent neural networks (RNNs) (Zhang and
Wang, 2015; Nguyen and Grishman, 2015a; Vu
et al., 2016; Zhang et al., 2015) that can better
handle long sequential data, graph neural net-
works (GNNs) (Zhang et al., 2018; Zhu et al.,
2019a) that build word/entity graphs for reason-
ing, and attention-based neural networks (Zhou
et al., 2016; Wang et al., 2016; Xiao and Liu, 2016)
that utilize attention mechanism to aggregate global
relational information.

Different from SRE models, NRE mainly uti-
lizes word embeddings and position embeddings
instead of hand-craft features as inputs. Word
embeddings (Turian et al., 2010; Mikolov et al.,
2013b) are the most used input representations
in NLP, which encode the semantic meaning of
words into vectors. In order to capture the entity
information in text, position embeddings (Zeng
et al., 2014) are introduced to specify the relative
distances between words and entities. Except for
word embeddings and position embeddings, there
are also other works integrating syntactic infor-
mation into NRE models. Xu et al. (2015a) and
Xu et al. (2015b) adopt CNNs and RNNs over
shortest dependency paths respectively. Liu et al.
(2015) propose a recursive neural network based
on augmented dependency paths. Xu et al. (2016)
and Cai et al. (2016) utilize deep RNNs to make
further use of dependency paths. Besides, there
are some efforts combining NRE with universal
schemas (Verga et al., 2016; Verga and McCallum,
2016; Riedel et al., 2013). Recently, Transformers
(Vaswani et al., 2017) and pre-trained language
models (Devlin et al., 2019) have also been ex-
plored for NRE (Du et al., 2018; Verga et al., 2018;

Dataset

#Rel.

#Fact

#Inst.

N/A

NYT-10
Wiki-Distant

53
454

377,980
605,877

694,491
1,108,288

79.43%
47.61%

Table 1: Statistics for NYT-10 and Wiki-Distant. Four
columns stand for numbers of relations, facts and in-
stances, and proportions of N/A instances respectively.

Model

NYT-10 Wiki-Distant

PCNN-ONE
PCNN-ATT
BERT

0.340
0.349
0.458

0.214
0.222
0.361

Table 2: Area under the curve (AUC) of PCNN-ONE
(Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and
BERT (Devlin et al., 2019) on two datasets.

Although DS provides a feasible approach to uti-
lize more data, this automatic labeling mechanism
is inevitably accompanied by the wrong labeling
problem. The reason is that not all sentences men-
tioning the two entities express their relations in
KGs exactly. For example, we may mistakenly la-
bel “Bill Gates retired from Microsoft” with the
relation founder, if (Bill Gates, founder, Mi-
crosoft) is a relational fact in KGs.

The existing methods to alleviate the noise prob-

lem can be divided into three major approaches:

(1) Some methods adopt multi-instance learning
by combining sentences with same entity pairs and
then selecting informative instances from them.
Riedel et al. (2010); Hoffmann et al. (2011); Sur-
deanu et al. (2012) utilize graphical model to infer
the informative sentences, while Zeng et al. (2015)
use a simple heuristic selection strategy. Later on,
Lin et al. (2016); Zhang et al. (2017); Han et al.
(2018c); Li et al. (2019); Zhu et al. (2019c); Hu
et al. (2019) design attention mechanisms to high-
light informative instances for RE.

(2) Incorporating extra context information
to denoise DS data has also been explored, such as
incorporating KGs as external information to guide
instance selection (Ji et al., 2017; Han et al., 2018b;
Zhang et al., 2019a; Qu et al., 2019) and adopting
multi-lingual corpora for the information consis-
tency and complementarity (Verga et al., 2016; Lin
et al., 2017; Wang et al., 2018).

(3) Many methods tend to utilize sophisticated
mechanisms and training strategies to enhance
distantly supervised NRE models. Vu et al. (2016);
Beltagy et al. (2019) combine different architec-
tures and training strategies to construct hybrid

Figure 3: An example of distantly supervised rela-
tion extraction. With the fact (Apple Inc., product,
iPhone), DS ﬁnds all sentences mentioning the two en-
tities and annotates them with the relation product,
which inevitably brings noise labels.

Wu and He, 2019; Baldini Soares et al., 2019) and
have achieved new state-of-the-arts.

By concisely reviewing the above techniques,
we are able to track the development of RE from
pattern and statistical methods to neural models.
Comparing the performance of state-of-the-art RE
models in years (Figure 2), we can see the vast in-
crease since the emergence of NRE, which demon-
strates the power of neural methods.

3

“More” Directions for RE

Although the above-mentioned NRE models have
achieved superior results on benchmarks, they are
still far from solving the problem of RE. Most
of these models utilize abundant human annota-
tions and just aim at extracting pre-deﬁned rela-
tions within single sentences. Hence, it is hard
for them to work well in complex cases. In fact,
there have been various works exploring feasible
approaches that lead to better RE abilities on real-
world scenarios. In this section, we summarize
these exploratory efforts into four directions, and
give our review and outlook about these directions.

3.1 Utilizing More Data

Supervised NRE models suffer from the lack of
large-scale high-quality training data, since manu-
ally labeling data is time-consuming and human-
intensive. To alleviate this issue, distant supervi-
sion (DS) assumption has been used to automati-
cally label data by aligning existing KGs with plain
text (Mintz et al., 2009; Nguyen and Moschitti,
2011; Min et al., 2013). As shown in Figure 3, for
any entity pair in KGs, sentences mentioning both
the entities will be labeled with their corresponding
relations in KGs. Large-scale training examples
can be easily constructed by this heuristic scheme.

frameworks. Liu et al. (2017) incorporate a soft-
label scheme by changing unconﬁdent labels dur-
ing training. Furthermore, reinforcement learn-
ing (Feng et al., 2018; Zeng et al., 2018) and adver-
sarial training (Wu et al., 2017; Wang et al., 2018;
Han et al., 2018a) have also been adopted in DS.

The researchers have formed a consensus that
utilizing more data is a potential way towards more
powerful RE models, and there still remains some
open problems worth exploring:

(1) Existing DS methods focus on denoising
auto-labeled instances and it is certainly mean-
ingful to follow this research direction. Besides,
current DS schemes are still similar to the origi-
nal one in (Mintz et al., 2009), which just covers
the case that the entity pairs are mentioned in the
same sentences. To achieve better coverage and
less noise, exploring better DS schemes for auto-
labeling data is also valuable.

(2) Inspired by recent work in adopting pre-
trained language models (Zhang et al., 2019b; Wu
and He, 2019; Baldini Soares et al., 2019) and ac-
tive learning (Zheng et al., 2019) for RE, to per-
form unsupervised or semi-supervised learning
for utilizing large-scale unlabeled data as well as
using knowledge from KGs and introducing human
experts in the loop is also promising.

Besides addressing existing approaches and fu-
ture directions, we also propose a new DS dataset
to advance this ﬁeld, which will be released once
the paper is published. The most used benchmark
for DS, NYT-10 (Riedel et al., 2010), suffers from
small amount of relations, limited relation domains
and extreme long-tail relation performance. To
alleviate these drawbacks, we utilize Wikipedia
and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) to
construct Wiki-Distant in the same way as Riedel
et al. (2010). As demonstrated in Table 1, Wiki-
Distant covers more relations and possesses more
instances, with a more reasonable N/A proportion.
Comparison results of state-of-the-art models on
these two datasets2 are shown in Table 2, indicating
that Wiki-Distant is more challenging and there is
a long way to resolve distantly supervised RE.

3.2 Performing More Efﬁcient Learning

Real-world relation distributions are long-tail:
Only the common relations obtain sufﬁcient train-
ing instances and most relations have very limited

2Due to the large size, we do not use any denoise mecha-

nism for BERT, which still achieves the best results.

Figure 4: Relation distributions (log-scale) on the train-
ing part of DS datasets NYT-10 and Wiki-Distant,
suggesting that real-world relation distributions suffer
from the long-tail problem.

relational facts and corresponding sentences. We
can see the long-tail relation distributions on two
DS datasets from Figure 4, where many relations
even have less than 10 training instances. This
phenomenon calls for models that can learn long-
tail relations more efﬁciently. Few-shot learning,
which focuses on grasping tasks with only a few
training examples, is a good ﬁt for this need.

To advance this ﬁeld, Han et al. (2018d) ﬁrst
built a large-scale few-shot relation extraction
dataset (FewRel). This benchmark takes the N -
way K-shot setting, where models are given N
random-sampled new relations, along with K train-
ing examples for each relation. With limited infor-
mation, RE models are required to classify query
instances into given relations (Figure 5).

The general idea of few-shot models is to train
good representations of instances or learn ways
of fast adaptation from existing large-scale data,
and then transfer to new tasks. There are mainly
two ways for handling few-shot learning: (1) Met-
ric learning learns a semantic metric on existing
data and classiﬁes queries by comparing them with
training examples (Koch et al., 2015; Vinyals et al.,
2016; Snell et al., 2017; Baldini Soares et al., 2019).

Figure 5: An example of few-shot RE. Give a few
instances for new relation types, few-shot RE models
classify query sentences into one of the given relations.

Figure 6: Few-shot RE results with (A) increasing N
and (B) similar relations. The left ﬁgure shows the ac-
curacy (%) of two models in N -way 1-shot RE. In the
right ﬁgure, “random” stands for the standard few-shot
setting and “similar” stands for evaluating with selected
similar relations.

While most metric learning models perform dis-
tance measurement on sentence-level representa-
tion, Ye and Ling (2019); Gao et al. (2019) uti-
lize token-level attention for ﬁner-grained compari-
son. (2) Meta-learning, also known as “learning
to learn”, aims at grasping the way of parameter ini-
tialization and optimization through the experience
gained on the meta-train data (Ravi and Larochelle,
2017; Finn et al., 2017; Mishra et al., 2018).

Researchers have made great progress in few-
shot RE. However, there remain many challenges
that are important for its applications and have not
yet been discussed. Gao et al. (2019) propose two
problems worth further investigation:

(1) Few-shot domain adaptation studies how
few-shot models can transfer across domains . It
is argued that in the real-world application, the
test domains are typically lacking annotations and
could differ vastly from the training domains. Thus,
it is crucial to evaluate the transferabilities of few-
shot models across domains.

(2) Few-shot none-of-the-above detection is
about detecting query instances that do not belong
to any of the sampled N relations. In the N -way
K-shot setting, it is assumed that all queries ex-
press one of the given relations. However, the real
case is that most sentences are not related to the
relations of our interest. Conventional few-shot
models cannot well handle this problem due to
the difﬁculty to form a good representation for the
none-of-the-above (NOTA) relation. Therefore, it
is crucial to study how to identify NOTA instances.
(3) Besides the above challenges, it is also impor-
tant to see that, the existing evaluation protocol
may over-estimate the progress we made on few-
shot RE. Unlike conventional RE tasks, few-shot

Figure 7: An example of document-level RE. Given a
paragraph with several sentences and multiple entities,
models are required to extract all possible relations be-
tween these entities expressed in the document.

RE randomly samples N relations for each evalua-
tion episode; in this setting, the number of relations
is usually very small (5 or 10) and it is very likely
to sample N distinct relations and thus reduce to a
very easy classiﬁcation task.

We carry out two simple experiments to show
the problems (Figure 6): (A) We evaluate few-shot
models with increasing N and the performance
drops drastically with larger relation numbers. Con-
sidering that the real-world case contains much
more relations, it shows that existing models are
still far from being applied. (B) Instead of ran-
domly sampling N relations, we hand-pick 5 rela-
tions similar in semantics and evaluate few-shot RE
models on them. It is no surprise to observe a sharp
decrease in the results, which suggests that existing
few-shot models may overﬁt simple textual cues
between relations instead of really understanding
the semantics of the context. More details about
the experiments are in Appendix A.

3.3 Handling More Complicated Context

As shown in Figure 7, one document generally
mentions many entities exhibiting complex cross-
sentence relations. Most existing methods focus on
intra-sentence RE and thus are inadequate for col-
lectively identifying these relational facts expressed
in a long paragraph. In fact, most relational facts
can only be extracted from complicated context
like documents rather than single sentences (Yao
et al., 2019), which should not be neglected.

There are already some works proposed to ex-

tract relations across multiple sentences:

(1) Syntactic methods (Wick et al., 2006; Ger-
ber and Chai, 2010; Swampillai and Stevenson,
2011; Yoshikawa et al., 2011; Quirk and Poon,

Figure 8: An example of open information extraction,
which extracts relation arguments (entities) and phrases
without relying on any pre-deﬁned relation types.

2017) rely on textual features extracted from var-
ious syntactic structures, such as coreference an-
notations, dependency parsing trees and discourse
relations, to connect sentences in documents.

(2) Zeng et al. (2017); Christopoulou et al.
(2018) build inter-sentence entity graphs, which
can utilize multi-hop paths between entities for
inferring the correct relations.

(3) Peng et al. (2017); Song et al. (2018); Zhu
et al. (2019b) employ graph-structured neural
networks to model cross-sentence dependencies
for relation extraction, which bring in memory and
reasoning abilities.

To advance this ﬁeld, some document-level RE
datasets have been proposed. Quirk and Poon
(2017); Peng et al. (2017) build datasets by DS.
Li et al. (2016); Peng et al. (2017) propose datasets
for speciﬁc domains. Yao et al. (2019) construct
a general document-level RE dataset annotated
by crowdsourcing workers, suitable for evaluating
general-purpose document-level RE systems.

Although there are some efforts investing into
extracting relations from complicated context (e.g.,
documents), the current RE models for this chal-
lenge are still crude and straightforward. Follow-
ings are some directions worth further investiga-
tion:

(1) Extracting relations from complicated con-
text is a challenging task requiring reading, mem-
orizing and reasoning for discovering relational
facts across multiple sentences. Most of current
RE models are still very weak in these abilities.

(2) Besides documents, more forms of context
is also worth exploring, such as extracting rela-
tional facts across documents, or understanding re-
lational information based on heterogeneous data.

(3) Inspired by Narasimhan et al. (2016), which
utilizes search engines for acquiring external infor-
mation, automatically searching and analysing
context for RE may help RE models identify rela-
tional facts with more coverage and become practi-
cal for daily scenarios.

Figure 9: An example of clustering-based relation dis-
covery, which identifying potential relation types by
clustering unlabeled relational instances.

3.4 Orienting More Open Domains

Most RE systems work within pre-speciﬁed rela-
tion sets designed by human experts. However, our
world undergoes open-ended growth of relations
and it is not possible to handle all these emerging
relation types only by humans. Thus, we need RE
systems that do not rely on pre-deﬁned relation
schemas and can work in open scenarios.

There are already some explorations in handling
open relations: (1) Open information extraction
(Open IE), as shown in Figure 8, extracts relation
phrases and arguments (entities) from text (Banko
et al., 2007; Fader et al., 2011; Mausam et al., 2012;
Del Corro and Gemulla, 2013; Angeli et al., 2015;
Stanovsky and Dagan, 2016; Mausam, 2016; Cui
et al., 2018). Open IE does not rely on speciﬁc
relation types and thus can handle all kinds of re-
lational facts. (2) Relation discovery, as shown
in Figure 9, aims at discovering unseen relation
types from unsupervised data. Yao et al. (2011);
Marcheggiani and Titov (2016) propose to use gen-
erative models and treat these relations as latent
variables, while Shinyama and Sekine (2006); El-
sahar et al. (2017); Wu et al. (2019) cast relation
discovery as a clustering task.

Though relation extraction in open domains has
been widely studied, there are still lots of unsolved
research questions remained to be answered:

(1) Canonicalizing relation phrases and argu-
ments in Open IE is crucial for downstream tasks
(Niklaus et al., 2018). If not canonicalized, the
extracted relational facts could be redundant and
ambiguous. For example, Open IE may extract
two triples (Barack Obama, was born in, Hon-
olulu) and (Obama, place of birth, Hon-
olulu) indicating an identical fact. Thus, normal-
izing extracted results will largely beneﬁt the ap-
plications of Open IE. There are already some pre-
liminary works in this area (Gal´arraga et al., 2014;
Vashishth et al., 2018) and more efforts are needed.
(2) The not applicable (N/A) relation has been

Benchmark

Normal ME

OE

Wiki80 (Acc)
TACRED (F-1)
NYT-10 (AUC)
Wiki-Distant (AUC)

0.861
0.666
0.349
0.222

0.631
0.211
0.216
0.145

0.763
0.412
0.185
0.173

Table 3: Results of state-of-the-arts models on the nor-
mal setting, masked-entity (ME) setting and only-entity
(OE) setting. We report accuracies of BERT on Wiki80,
F-1 scores of BERT on TACRED and AUC of PCNN-
ATT on NYT-10 and Wiki-Distant. All models are
from the OpenNRE package (Han et al., 2019).

hardly addressed in relation discovery. In previ-
ous work, it is usually assumed that the sentence
always expresses a relation between the two enti-
ties (Marcheggiani and Titov, 2016). However, in
the real-world scenario, a large proportion of entity
pairs appearing in a sentence do not have a rela-
tion, and ignoring them or using simple heuristics
to get rid of them may lead to poor results. Thus, it
would be of interest to study how to handle these
N/A instances in relation discovery.

4 Other Challenges

In this section, we analyze two key challenges
faced by RE models, address them with experi-
ments and show their signiﬁcance in the research
and development of RE systems.

4.1 Learning from Text or Names

In the process of RE, both entity names and their
context provide useful information for classiﬁca-
tion. Entity names provide typing information
(e.g., we can easily tell JFK International Airport
is an airport) and help to narrow down the range
of possible relations; In the training process, entity
embeddings may also be formed to help relation
classiﬁcation (like in the link prediction task of
KG). On the other hand, relations can usually be
extracted from the semantics of text around entity
pairs. In some cases, relations can only be inferred
implicitly by reasoning over the context.

Since there are two sources of information, it is
interesting to study how much each of them con-
tributes to the RE performance. Therefore, we
design three different settings for the experiments:
(1) normal setting, where both names and text are
taken as inputs; (2) masked-entity (ME) setting,
where entity names are replaced with a special
token; (3) only-entity (OE) setting, where only
names of the two entities are provided.

Results from Table 3 show that compared to the
normal setting, models suffer a huge performance
drop in both the ME and OE settings. Besides, it is
surprising to see that in most cases, only using en-
tity names outperforms only using text with entities
masked. It suggests that (1) both entity names and
text provide crucial information for RE, and (2) for
existing state-of-the-art models and benchmarks,
entity names contribute even more.

The observation is contrary to human intuition:
we classify the relations between given entities
mainly from the text description, yet models learn
more from their names. To make real progress in
understanding how language expresses relational
facts, this problem should be further investigated
and more efforts are needed.

4.2 RE Datasets towards Special Interests

There are already many datasets that beneﬁt RE
research: For supervised RE, there are MUC (Gr-
ishman and Sundheim, 1996), ACE-2005 (Ntro-
duction, 2005), SemEval-2010 Task 8 (Hendrickx
et al., 2009), KBP37 (Zhang and Wang, 2015) and
TACRED (Zhang et al., 2017); and we have NYT-
10 (Riedel et al., 2010), FewRel (Han et al., 2018d)
and DocRED (Yao et al., 2019) for distant supervi-
sion, few-shot and document-level RE respectively.
However, there are barely datasets targeting
special problems of interest. For example, RE
across sentences (e.g., two entities are mentioned
in two different sentences) is an important problem,
yet there is no speciﬁc datasets that can help re-
searchers study it. Though existing document-level
RE datasets contain instances of this case, it is hard
to analyze the exact performance gain towards this
speciﬁc aspect. Usually, researchers (1) use hand-
crafted sub-sets of general datasets or (2) carry
out case studies to show the effectiveness of their
models in speciﬁc problems, which is lacking of
convincing and quantitative analysis. Therefore, to
further study these problems of great importance in
the development of RE, it is necessary for the com-
munity to construct well-recognized, well-designed
and ﬁne-grained datasets towards special interests.

5 Conclusion

In this paper, we give a comprehensive and de-
tailed review on the development of relation extrac-
tion models, generalize four promising directions
leading to more powerful RE systems (utilizing
more data, performing more efﬁcient learning, han-

dling more complicated context and orienting more
open domains), and further investigate two key
challenges faced by existing RE models. We thor-
oughly survey the previous RE literature as well
as supporting our points with statistics and experi-
ments. Through this paper, we hope to demonstrate
the progress and problems in existing RE research
and encourage more efforts in this area.

References

Gabor Angeli, Melvin Jose Johnson Premkumar, and
Christopher D. Manning. 2015. Leveraging linguis-
tic structure for open domain information extraction.
In Proceedings of ACL-IJCNLP, pages 344–354.

Nguyen Bach and Sameer Badaskar. 2007. A review

of relation extraction.

Livio Baldini Soares, Nicholas FitzGerald, Jeffrey
Ling, and Tom Kwiatkowski. 2019. Matching the
blanks: Distributional similarity for relation learn-
ing. In Proceedings of ACL, pages 2895–2905.

Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of IJCAI, pages 2670–2676.

Iz Beltagy, Kyle Lo, and Waleed Ammar. 2019. Com-
bining distant and direct supervision for neural re-
In Proceedings of NAACL-HLT,
lation extraction.
pages 1858–1867.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
dings. In Proceedings of EMNLP, pages 615–620.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Jason Weston, and Oksana Yakhnenko.
Duran,
2013. Translating embeddings for modeling multi-
relational data. In Proceedings of NIPS, pages 2787–
2795.

Razvan C Bunescu and Raymond J Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of EMNLP, pages 724–731.

Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016.
Bidirectional recurrent convolutional neural network
In Proceedings of ACL,
for relation classiﬁcation.
pages 756–765.

Mary Elaine Califf and Raymond J. Mooney. 1997. Re-
lational learning of pattern-match rules for informa-
tion extraction. In Proceedings of CoNLL.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R Hruschka, and Tom M Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In Proceedings of AAAI.

Fenia Christopoulou, Makoto Miwa, and Sophia Ana-
niadou. 2018. A walk-based model on entity graphs
In Proceedings of ACL,
for relation extraction.
pages 81–88.

Lei Cui, Furu Wei, and Ming Zhou. 2018. Neural
open information extraction. In Proceedings of ACL,
pages 407–413.

Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of ACL, page 423.

Luciano Del Corro and Rainer Gemulla. 2013. Clausie:
In Pro-

clause-based open information extraction.
ceedings of WWW, pages 355–366.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
In Proceedings of NAACL-HLT, pages 4171–
ing.
4186.

Li Dong, Furu Wei, Ming Zhou, and Ke Xu.
2015. Question answering over freebase with multi-
column convolutional neural networks. In Proceed-
ings of ACL-IJCNLP, pages 260–269.

Jinhua Du, Jingguang Han, Andy Way, and Dadong
Wan. 2018. Multi-level structured self-attentions for
distantly supervised relation extraction. In Proceed-
ings of EMNLP, pages 2216–2225.

Hady Elsahar, Elena Demidova, Simon Gottschalk,
Christophe Gravier, and Frederique Laforest. 2017.
Unsupervised open relation extraction. In Proceed-
ings of ESWC, pages 12–16.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
In Proceedings of EMNLP, pages 1535–
traction.
1545.

Jun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xi-
aoyan Zhu. 2018. Reinforcement learning for rela-
tion classiﬁcation from noisy data. In Proceedings
of AAAI, pages 5779–5786.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
In Proceedings of ICML, pages
deep networks.
1126–1135.

Luis Gal´arraga, Geremy Heitz, Kevin Murphy, and
Fabian M Suchanek. 2014. Canonicalizing open
In Proceedings of CIKM, pages
knowledge bases.
1679–1688. ACM.

Tianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng
Li, Maosong Sun, and Jie Zhou. 2019. FewRel 2.0:
Towards more challenging few-shot relation classiﬁ-
In Proceedings of EMNLP-IJCNLP, pages
cation.
6251–6256.

Matthew Gerber and Joyce Chai. 2010. Beyond Nom-
Bank: A study of implicit arguments for nominal
In Proceedings of ACL, pages 1583–
predicates.
1592.

Matthew R Gormley, Mo Yu, and Mark Dredze. 2015.
Improved relation extraction with feature-rich com-
In Proceedings of
positional embedding models.
EMNLP, pages 1774–1784.

Ralph Grishman and Beth Sundheim. 1996. Message
In
understanding conference- 6: A brief history.
Proceedings of COLING.

Xu Han, Tianyu Gao, Yuan Yao, Deming Ye, Zhiyuan
Liu, and Maosong Sun. 2019. OpenNRE: An open
and extensible toolkit for neural relation extraction.
In Proceedings of EMNLP-IJCNLP: System Demon-
strations, pages 169–174.

Scott B Huffman. 1995. Learning information extrac-
tion patterns from examples. In Proceedings of IJ-
CAI, pages 246–260.

Guoliang Ji, Kang Liu, Shizhu He, Jun Zhao, et al.
2017. Distant supervision for relation extraction
with sentence-level attention and entity descriptions.
In AAAI, pages 3060–3066.

Jing Jiang and ChengXiang Zhai. 2007. A systematic
exploration of the feature space for relation extrac-
tion. In Proceedings of NAACL, pages 113–120.

Meng Jiang, Jingbo Shang, Taylor Cassidy, Xiang Ren,
Lance M Kaplan, Timothy P Hanratty, and Jiawei
Han. 2017. Metapad: Meta pattern discovery from
massive text corpora. In Proceedings of KDD, pages
877–886.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2018a. De-
noising distant supervision for relation extraction via
instance-level adversarial training. arXiv preprint
arXiv:1805.10959.

Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for extracting relations. In Proceedings of ACL,
pages 178–181.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2018b. Neu-
ral knowledge acquisition via mutual attention be-
tween knowledge graph and text. In Proceedings of
AAAI.

Xu Han, Pengfei Yu, Zhiyuan Liu, Maosong Sun, and
Peng Li. 2018c. Hierarchical relation extraction
In Proceed-
with coarse-to-ﬁne grained attention.
ings of EMNLP, pages 2236–2245.

Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,
Zhiyuan Liu, and Maosong Sun. 2018d. Fewrel:
A large-scale supervised few-shot relation classiﬁca-
tion dataset with state-of-the-art evaluation. In Pro-
ceedings of EMNLP, pages 4803–4809.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid ´O S´eaghdha, Sebastian
Pad´o, Marco Pennacchiotti, Lorenza Romano, and
SemEval-2010 task 8:
Stan Szpakowicz. 2009.
Multi-way classiﬁcation of semantic relations be-
In Proceedings of the
tween pairs of nominals.
Workshop on Semantic Evaluations: Recent Achieve-
ments and Future Directions (SEW-2009), pages 94–
99.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
In Proceedings of ACL,
of overlapping relations.
pages 541–550.

Linmei Hu, Luhao Zhang, Chuan Shi, Liqiang Nie,
Weili Guan, and Cheng Yang. 2019.
Improving
distantly-supervised relation extraction with joint la-
bel embedding. In Proceedings of EMNLP-IJCNLP,
pages 3812–3820.

Jun-Tae Kim and Dan I. Moldovan. 1995. Acquisition
of linguistic patterns for knowledge-based informa-
tion extraction. TKDE, 7(5):713–724.

Gregory Koch, Richard Zemel, and Ruslan Salakhut-
dinov. 2015. Siamese neural networks for one-shot
image recognition. In Proceedings of the Workshop
of ICML.

Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sci-
aky, Chih-Hsuan Wei, Robert Leaman, Allan Peter
Davis, Carolyn J. Mattingly, Thomas C. Wiegers,
and Zhiyong Lu. 2016. BioCreative V CDR task
corpus: a resource for chemical disease relation ex-
traction. Database, pages 1–10.

Yang Li, Guodong Long, Tao Shen, Tianyi Zhou,
Lina Yao, Huan Huo, and Jing Jiang. 2019. Self-
attention enhanced selective gate with entity-aware
embedding for distantly supervised relation extrac-
tion. arXiv preprint arXiv:1911.11899.

Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2017.
Neural relation extraction with multi-lingual atten-
tion. In Proceedings of ACL, pages 34–43.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu,
and Xuan Zhu. 2015. Learning entity and relation
In
embeddings for knowledge graph completion.
Twenty-ninth AAAI conference on artiﬁcial intelli-
gence.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of ACL, pages 2124–2133.

Yi Yao Huang and William Yang Wang. 2017. Deep
residual learning for weakly-supervised relation ex-
In Proceedings of EMNLP, pages 1803–
traction.
1807.

Chunyang Liu, Wenbo Sun, Wenhan Chao, and Wanx-
iang Che. 2013. Convolution neural network for re-
In Proceedings of ICDM, pages
lation extraction.
231–242.

Tianyu Liu, Kexiang Wang, Baobao Chang, and Zhi-
fang Sui. 2017. A soft-label method for noise-
tolerant distantly supervised relation extraction. In
Proceedings of EMNLP, pages 1790–1795.

Dat PT Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka.
2007. Relation extraction from wikipedia using sub-
tree mining. In Proceedings of AAAI, pages 1414–
1420.

Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,
and WANG Houfeng. 2015. A dependency-based
In Pro-
neural network for relation classiﬁcation.
ceedings of ACL-IJCNLP, pages 285–290.

Thien Huu Nguyen and Ralph Grishman. 2015a.
Combining neural networks and log-linear mod-
els to improve relation extraction. arXiv preprint
arXiv:1511.05926.

Diego Marcheggiani and Ivan Titov. 2016. Discrete-
state variational autoencoders for joint discovery and
factorization of relations. TACL, 4:231–244.

Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
In Proceedings of
ing for information extraction.
EMNLP-CoNLL, pages 523–534.

Mausam Mausam. 2016. Open information extraction
systems and downstream applications. In Proceed-
ings of IJCAI, pages 4074–4077.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efﬁcient estimation of word represen-
tations in vector space. In Proceedings of ICLR.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119.

Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for re-
lation extraction with an incomplete knowledge base.
In Proceedings of NAACL, pages 777–782.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
In Proceedings of ACL-
tion without labeled data.
IJCNLP, pages 1003–1011.

Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and
Pieter Abbeel. 2018. A simple neural attentive meta-
learner. In Proceedings of ICLR.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
In Proceedings of ACL, pages 1105–
structures.
1116.

Raymond J Mooney and Razvan C Bunescu. 2006.
Subsequence kernels for relation extraction. In Pro-
ceedings of NIPS, pages 171–178.

Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. PATTY: A taxonomy of relational
In Proceedings of
patterns with semantic types.
EMNLP-CoNLL, pages 1135–1145.

Karthik Narasimhan, Adam Yala, and Regina Barzilay.
2016. Improving information extraction by acquir-
ing external evidence with reinforcement learning.
In Proceedings of EMNLP, pages 2355–2365.

Thien Huu Nguyen and Ralph Grishman. 2015b. Rela-
tion extraction: Perspective from convolutional neu-
In Proceedings of the NAACL Work-
ral networks.
shop on Vector Space Modeling for NLP, pages 39–
48.

Truc-Vien T Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of ACL, pages 277–282.

Christina Niklaus, Matthias Cetto, Andr´e Freitas, and
Siegfried Handschuh. 2018. A survey on open in-
In Proceedings of COLING,
formation extraction.
pages 3866–3878.

Ii. I Ntroduction. 2005. The ace 2005 ( ace 05 ) eval-
uation plan evaluation of the detection and recogni-
tion of ace entities, values, temporal expression, re-
lations, and events.

Sachin Pawar, Girish K Palshikar, and Pushpak Bhat-
tacharyya. 2017. Relation extraction: A survey.
arXiv preprint arXiv:1712.05191.

Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina
Toutanova, and Wen-tau Yih. 2017. Cross-sentence
n-ary relation extraction with graph LSTMs. TACL,
5:101–115.

Jianfeng Qu, Wen Hua, Dantong Ouyang, Xiaofang
Zhou, and Ximing Li. 2019. A ﬁne-grained and
noise-aware method for neural relation extraction.
In Proceedings of CIKM, pages 659–668.

Chris Quirk and Hoifung Poon. 2017. Distant super-
vision for relation extraction beyond the sentence
In Proceedings of EACL, pages 1171–
boundary.
1182.

Sachin Ravi and Hugo Larochelle. 2017. Optimization
as a model for few-shot learning. In Proceedings of
ICLR.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
In Proceedings of ECML-PKDD,
out labeled text.
pages 148–163.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL, pages 74–84.

Dan Roth and Wen-tau Yih. 2002. Probabilistic reason-
ing for entity & relation recognition. In Proceedings
of COLING.

Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL.

Cicero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
In Proceedings of
convolutional neural networks.
ACL-IJCNLP, pages 626–634.

Sunita Sarawagi and William W Cohen. 2005. Semi-
markov conditional random ﬁelds for information
In Proceedings of NIPS, pages 1185–
extraction.
1192.

Michael Schlichtkrull, Thomas N Kipf, Peter Bloem,
Rianne van den Berg, Ivan Titov, and Max Welling.
2018. Modeling relational data with graph convo-
lutional networks. In Proceedings of ESWC, pages
593–607.

Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
In Proceedings of NAACL, pages
tion discovery.
304–311.

Jake Snell, Kevin Swersky, and Richard Zemel. 2017.
Prototypical networks for few-shot learning. In Pro-
ceedings of NIPS, pages 4077–4087.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of EMNLP, pages 1201–1211.

Stephen Soderland, David Fisher, Jonathan Aseltine,
and Wendy Lehnert. 1995. Crystal inducing a con-
ceptual dictionary. In Proceedings of IJCAI, pages
1314–1319.

Linfeng Song, Yue Zhang, et al. 2018. N-ary relation
extraction using graph-state lstm. In Proceedings of
EMNLP.

Gabriel Stanovsky and Ido Dagan. 2016. Creating a
large benchmark for open information extraction. In
Proceedings of EMNLP, pages 2300–2305.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP, pages 455–465.

Kumutha Swampillai and Mark Stevenson. 2011. Ex-
tracting relations within and across sentences.
In
Proceedings of RANLP.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
In Proceedings of
for semi-supervised learning.
ACL, pages 384–394.

Shikhar Vashishth, Prince Jain, and Partha Talukdar.
2018. Cesi: Canonicalizing open knowledge bases
using embeddings and side information. In Proceed-
ings of WWW, pages 1317–1327.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
In Proceedings of NIPS, pages 5998–
you need.
6008.

Patrick Verga, David Belanger, Emma Strubell, Ben-
jamin Roth, and Andrew McCallum. 2016. Multilin-
gual relation extraction using compositional univer-
sal schema. In Proceedings of NAACL, pages 886–
896.

Patrick Verga and Andrew McCallum. 2016. Row-less
universal schema. In Proceedings of ACL, pages 63–
68.

Patrick Verga, Emma Strubell, and Andrew McCallum.
2018. Simultaneously self-attending to all mentions
for full-abstract biological relation extraction.
In
Proceedings of NAACL-HLT, pages 872–884.

Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan
Wierstra, et al. 2016. Matching networks for one
shot learning. In Proceedings of NIPS, pages 3630–
3638.

Denny Vrandeˇci´c and Markus Kr¨otzsch. 2014. Wiki-
data: a free collaborative knowledgebase. Proceed-
ings of CACM, 57(10):78–85.

Ngoc Thang Vu, Heike Adel, Pankaj Gupta, et al. 2016.
Combining recurrent and convolutional neural net-
works for relation classiﬁcation. In Proceedings of
NAACL, pages 534–539.

Linlin Wang, Zhu Cao, Gerard De Melo, and Zhiyuan
Liu. 2016. Relation classiﬁcation via multi-level at-
tention cnns. In Proceedings of ACL, pages 1298–
1307.

Mengqiu Wang. 2008. A re-examination of depen-
In Pro-

dency path kernels for relation extraction.
ceedings of IJCNLP, pages 841–846.

Xiaozhi Wang, Xu Han, Yankai Lin, Zhiyuan Liu, and
Maosong Sun. 2018. Adversarial multi-lingual neu-
ral relation extraction. In Proceedings of COLING,
pages 1156–1166.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In Proceedings of AAAI.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of EMNLP, pages
1366–1371.

Michael Wick, Aron Culotta, et al. 2006. Learning
ﬁeld compatibilities to extract database records from
unstructured text. In Proceedings of EMNLP.

Ruidong Wu, Yuan Yao, Xu Han, Ruobing Xie,
Zhiyuan Liu, Fen Lin, Leyu Lin, and Maosong Sun.

2019. Open relation extraction: Relational knowl-
edge transfer from supervised data to unsupervised
In Proceedings of EMNLP-IJCNLP, pages
data.
219–228.

Shanchan Wu and Yifan He. 2019.

Enriching
pre-trained language model with entity informa-
arXiv preprint
tion for relation classiﬁcation.
arXiv:1905.08284.

Yi Wu, David Bamman, and Stuart Russell. 2017. Ad-
versarial training for relation extraction. In Proceed-
ings of EMNLP, pages 1778–1783.

Dmitry Zelenko, Chinatsu Aone,

and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Proceedings of JMLR, pages 1083–1106.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
In Pro-
piecewise convolutional neural networks.
ceedings of EMNLP, pages 1753–1762.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classiﬁcation via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Minguang Xiao and Cong Liu. 2016. Semantic rela-
tion classiﬁcation via hierarchical recurrent neural
network with attention. In Proceedings of COLING,
pages 1254–1263.

Wenyuan Zeng, Yankai Lin, Zhiyuan Liu,

and
Incorporating relation paths
In Proceedings of

Maosong Sun. 2017.
in neural relation extraction.
EMNLP, pages 1768–1777.

Chenyan Xiong, Russell Power, and Jamie Callan.
Explicit semantic ranking for academic
2017.
search via knowledge graph embedding. In Proceed-
ings of WWW, pages 1271–1279.

Xiangrong Zeng, Shizhu He, Kang Liu, and Jun Zhao.
2018. Large scaled relation extraction with rein-
forcement learning. In Proceedings of AAAI, pages
5658–5665.

Kun Xu, Yansong Feng, Songfang Huang, and
Dongyan Zhao. 2015a. Semantic relation classiﬁ-
cation via convolutional neural networks with sim-
ple negative sampling. In Proceedings of EMNLP,
pages 536–540.

Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen,
Yangyang Lu, and Zhi Jin. 2016.
Improved rela-
tion classiﬁcation by deep recurrent neural networks
with data augmentation. In Proceedings of COLING,
pages 1461–1470.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015b. Classifying relations via long
short term memory networks along shortest depen-
In Proceedings of EMNLP, pages
dency paths.
1785–1794.

Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
In Proceedings of
ery using generative models.
EMNLP, pages 1456–1466.

Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,
Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou,
and Maosong Sun. 2019. DocRED: A large-scale
In Pro-
document-level relation extraction dataset.
ceedings of ACL, pages 764–777.

Dongxu Zhang and Dong Wang. 2015. Relation classi-
ﬁcation via recurrent neural network. arXiv preprint
arXiv:1508.01006.

Min Zhang, Jie Zhang, and Jian Su. 2006a. Explor-
ing syntactic features for relation extraction using a
convolution tree kernel. In Proceedings of NAACL,
pages 288–295.

Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006b. A composite kernel to extract relations be-
tween entities with both ﬂat and structured features.
In Proceedings of ACL, pages 825–832.

Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guany-
ing Wang, Xi Chen, Wei Zhang, and Huajun Chen.
2019a. Long-tail relation extraction via knowledge
graph embeddings and graph convolution networks.
In Proceedings of NAACL-HLT, pages 3016–3025.

Shu Zhang, Dequan Zheng, Xinchen Hu, and Ming
Yang. 2015. Bidirectional long short-term memory
networks for relation classiﬁcation. In Proceedings
of PACLIC, pages 73–78.

Yuhao Zhang, Peng Qi, and Christopher D. Manning.
2018. Graph convolution over pruned dependency
In Proceedings
trees improves relation extraction.
of EMNLP, pages 2205–2215.

Zhi-Xiu Ye and Zhen-Hua Ling. 2019. Multi-level
matching and aggregation network for few-shot re-
lation classiﬁcation. In Proceedings of ACL, pages
2872–2881.

Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-
geli, and Christopher D Manning. 2017. Position-
aware attention and supervised data improve slot ﬁll-
ing. In Proceedings of EMNLP, pages 35–45.

Katsumasa Yoshikawa, Sebastian Riedel, et al. 2011.
Coreference based event-argument relation extrac-
tion on biomedical text. J. Biomed. Semant.

Xiaofeng Yu and Wai Lam. 2010. Jointly identifying
entities and extracting relations in encyclopedia text
via a graphical model approach. In Proceedings of
ACL, pages 1399–1407.

Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,
Maosong Sun, and Qun Liu. 2019b. ERNIE: En-
hanced language representation with informative en-
tities. In Proceedings of ACL, pages 1441–1451.

Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of ACL, pages 419–426.

Shun Zheng, Xu Han, Yankai Lin, Peilin Yu, Lu Chen,
Ling Huang, Zhiyuan Liu, and Wei Xu. 2019.
DIAG-NRE: A neural pattern diagnosis framework
for distantly supervised neural relation extraction.
In Proceedings of ACL, pages 1419–1429.

Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of ACL, pages 427–434.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li,
Hongwei Hao, and Bo Xu. 2016. Attention-based
bidirectional long short-term memory networks for
relation classiﬁcation. In Proceedings of ACL, pages
207–212.

Hao Zhu, Yankai Lin, Zhiyuan Liu, Jie Fu, Tat-Seng
Chua, and Maosong Sun. 2019a. Graph neural net-
works with generated parameters for relation extrac-
tion. In Proceedings of ACL, pages 1331–1339.

Mengdi Zhu, Zheye Deng, Wenhan Xiong, Mo Yu,
Ming Zhang, and William Yang Wang. 2019b.
Towards open-domain named entity recognition
arXiv preprint
via neural correction models.
arXiv:1909.06058.

Zhangdong Zhu, Jindian Su, and Yang Zhou. 2019c.
Improving distantly supervised relation classiﬁca-
tion with attention and semantic weight. IEEE Ac-
cess, 7:91160–91168.

