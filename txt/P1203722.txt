Exploratory Combinatorial Optimization with Reinforcement Learning

Thomas D. Barrett,1 William R. Clements,2 Jakob N. Foerster,3 Alex I. Lvovsky1,4
1University of Oxford, Oxford, UK
2indust.ai, Paris, France
3Facebook AI Research
4Russian Quantum Center, Moscow, Russia
thomas.barrett@physics.ox.ac.uk, william.clements@indust.ai, jnf@fb.com, alex.lvovsky@physics.ox.ac.uk

0
2
0
2
 
n
a
J
 
1
3
 
 
]

G
L
.
s
c
[
 
 
2
v
3
6
0
4
0
.
9
0
9
1
:
v
i
X
r
a

Abstract

Many real-world problems can be reduced to combinatorial
optimization on a graph, where the subset or ordering of ver-
tices that maximize some objective function must be found.
With such tasks often NP-hard and analytically intractable,
reinforcement learning (RL) has shown promise as a frame-
work with which efﬁcient heuristic methods to tackle these
problems can be learned. Previous works construct the so-
lution subset incrementally, adding one element at a time,
however, the irreversible nature of this approach prevents the
agent from revising its earlier decisions, which may be nec-
essary given the complexity of the optimization task. We in-
stead propose that the agent should seek to continuously im-
prove the solution by learning to explore at test time. Our
approach of exploratory combinatorial optimization (ECO-
DQN) is, in principle, applicable to any combinatorial prob-
lem that can be deﬁned on a graph. Experimentally, we show
our method to produce state-of-the-art RL performance on
the Maximum Cut problem. Moreover, because ECO-DQN
can start from any arbitrary conﬁguration, it can be combined
with other search methods to further improve performance,
which we demonstrate using a simple random search.

1

Introduction

NP-hard combinatorial problems – such as Travelling Sales-
man (Papadimitriou 1977), Minimum Vertex Cover (Dinur
and Safra 2005) and Maximum Cut
(Goemans and
Williamson 1995) – are canonical challenges in computer
science. With practical applications ranging from fundamen-
tal science to industry, efﬁcient methods for approaching
combinatorial optimization are of great interest. However, as
no known algorithms are able to solve NP-hard problems in
polynomial time, exact methods rapidly become intractable.
Approximation algorithms guarantee a worst-case solution
quality, but sufﬁciently strong bounds may not exist and,
even if they do, these algorithms can have limited scalabil-
ity (Williamson and Shmoys 2011). Instead, heuristics are
often deployed that, despite offering no theoretical guaran-
tees, are chosen for high performance.

There are numerous heuristic methods, ranging from
search-based (Benlic and Hao 2013; Banks, Vincent, and
Anyakoha 2008) to physical systems that utilise both quan-
tum and classical effects (Johnson et al. 2011; Yamamoto

Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

et al. 2017) and their simulated counterparts (Kirkpatrick,
Gelatt, and Vecchi 1983; Clements et al. 2017; Tiunov,
Ulanov, and Lvovsky 2019). However, the effectiveness
of general algorithms is dependent on the problem being
considered, and high levels of performance often require
extensive tailoring and domain-speciﬁc knowledge. Ma-
chine learning offers a route to addressing these challenges,
which led to the demonstration of a meta-algorithm, S2V-
DQN (Khalil et al. 2017), that utilises reinforcement learn-
ing (RL) and a deep graph network to automatically learn
good heuristics for various combinatorial problems.

A solution to a combinatorial problem deﬁned on a graph
consists of a subset of vertices that satisﬁes the desired op-
timality criteria. Approaches following S2V-DQN’s frame-
work incrementally construct solutions one element at a time
– reducing the problem to predicting the value of adding any
vertex not currently in the solution to this subset. However,
due to the inherent complexity of many combinatorial prob-
lems, learning a policy that directly produces a single, opti-
mal solution is often impractical, as evidenced by the sub-
optimal performance of such approaches. Instead, we pro-
pose that a natural reformulation is for the agent to explore
the solution space at test time, rather than producing only
a single “best-guess”. Concretely, this means the agent can
add or remove vertices from the solution subset and is tasked
with searching for ever-improving solutions at test time. In
this work we present ECO-DQN (Exploratory Combinato-
rial Optimization DQN), a framework combining RL and
deep graph networks to realise this approach.

Our experimental work considers the Maximum Cut
(Max-Cut) problem as it is a fundamental combinatorial
challenge – in fact over half of the 21 NP-complete problems
enumerated in Karp’s seminal work (Karp 1972) can be re-
duced to Max-Cut – with numerous real-world applications.
The framework we propose can, however, be readily applied
to any graph-based combinatorial problem where solutions
correspond to a subset of vertices and the goal is to optimize
some objective function.

By comparing ECO-DQN to S2V-DQN as a baseline, we
demonstrate that our approach improves on the state-of-the-
art for applying RL to the Max-Cut problem. Suitable abla-
tions show that this performance gap is dependent on both
allowing the agent to reverse its earlier decisions and pro-
viding suitable information and rewards to exploit this free-

dom. Moreover, ECO-DQN can be initialised in any state
(i.e. will look to improve on any proposed solution) and, as
a consequence, has the ﬂexibility to be either deployed in-
dependently or combined with other search heuristics. For
example, we achieve signiﬁcant performance improvements
by simply taking the best solution found across multiple ran-
domly initialised episodes. ECO-DQN also generalises well
to graphs from unseen distributions. We obtain very strong
performance on known benchmarks of up to 2000 vertices,
even when the agent is trained on graphs an order of magni-
tude smaller and with a different structure.

Related Work
A formative demonstration of neural networks for combi-
natorial optimization (CO) was the application of Hopﬁeld
networks to the Travelling Salesman Problem (TSP) by Hop-
ﬁeld and Tank (1985). They mapped N -city problems to
N ×N graphs (networks) with each vertex, vij, a binary vari-
able denoting whether city i is the j-th to be visited, and the
edges connecting vertices proportional to the distance be-
tween cities. Although the results of Hopﬁeld and Tank were
contested by Wilson and Pawley (1988), there followed a pe-
riod active research into neural networks for CO that lasted
for over a decade (Smith 1999). During this time, RL tech-
niques were ﬁrst by applied to CO by Zhang and Diettench
(1995), who considerer the NP-hard job-shop problem (of
which the TSP is a speciﬁc case).

More recently, Bello et al. (2016) used policy gradients to
train pointer networks (Vinyals, Fortunato, and Jaitly 2015),
a recurrent architecture that produces a softmax attention
mechanism (a “pointer”) to select a member of the input
sequence as an output. However, this architecture did not
reﬂect the structure of problems deﬁned over a graph, which
Khalil et al. (2017) addressed with S2V-DQN, a general RL-
based framework for CO that uses a combined graph em-
bedding network and deep Q-network. Mittal et al. (2019)
developed these ideas further by modifying the training pro-
cess: ﬁrst training an embedding graph convolution network
(GCN), and then training a Q-network to predict the ver-
tex (action) values. This is orthogonal to our proposal which
considers the framework itself, rather than the training pro-
cedure, and, in principle, appears to be compatible with
ECO-DQN.

Another current direction is applying graph networks for
CO in combination with a tree search. Li et al. (2018) com-
bined a GCN with a guided tree-search in a supervised set-
ting, i.e. requiring large numbers of pre-solved instances for
training. Very recently, Abe et al. (2019) trained a GCN us-
ing Monte-Carlo tree search as a policy improvement opera-
tor, in a similar manner to AlphaGo Zero (Silver et al. 2017),
however, this work does not consider the Max-Cut problem.

2 Background

Max-Cut Problem
The Max-Cut problem is to ﬁnd a subset of vertices on a
graph that maximises the total number of edges connect-
ing vertices within this subset to vertices not in this subset
(the cut value). In this work we consider the more general

weighted version of this problem, where each edge in the
graph is assigned a weight and the objective is to maximise
the total value of cut edges. Formally, for a graph, G(V, W ),
with vertices V connected by edges W , the Max-Cut prob-
lem is to ﬁnd the subset of vertices S ⊂ V that maximises
C(S, G) = (cid:80)
S wij where wij ∈ W is the weight
i
⊂
of the edge connecting vertices i and j.

S,j

⊂

V

\

This is not simply a mathematical challenge as many real
world applications can be reduced to the Max-Cut prob-
lem, including protein folding (Perdomo-Ortiz et al. 2012),
investment portfolio optimization (Elsokkary et al. 2017;
Venturelli and Kondratyev 2018) (speciﬁcally using the
Markowitz (1952) formulation), and ﬁnding the ground state
of the Ising Hamiltonian in physics (Barahona 1982).

Q-learning
As is standard for RL, we consider the optimization task as
a Markov decision process (MDP) deﬁned by the 5-tuple
(S, A, T , R, γ). Here, S denotes the set of states, A is the
set of actions, T : S × A × S → [0, 1] is the transition
function, R : S → R the reward function and γ ∈ [0, 1] is
the discount factor. A policy, π : S → [0, 1], maps a state
to a probability distribution over actions. The Q-value of a
given state-action pair, (s ∈ S, a ∈ A), is then given by the
discounted sum of immediate and future rewards

Qπ(s, a) = E

γtR(st)

(cid:12)
(cid:12)
(cid:12) s0 = s, a0 = a, π

(cid:21)
,

(1)

(cid:20) ∞(cid:88)

t=0

where s0 and a0 correspond to the initial state and action
taken, with future actions chosen according to the policy, π.
A deep Q-network (Mnih et al. 2015) (DQN) provides
a function Q(s, a; θ), where θ parameterises the network,
which is trained to approximate Q∗(s, a) ≡ maxπQπ(s, a),
the Q-values of each state-action pair when follow-
ing the optimal policy. Once trained, an approxima-
tion of the optimal policy can be obtained simply by
acting greedily with respect
to the predicted Q-values,
π(s; θ) = argmaxa(cid:48)Q(s, a(cid:48); θ).

Message Passing Neural Networks
Our choice of deep Q-network is a message passing neu-
ral network (MPNN) (Gilmer et al. 2017). This is a general
framework of which many common graph networks are spe-
ciﬁc implementations. Each vertex in the graph, v ∈ V , is
represented with an n-dimensional embedding, µk
v, where k
labels the current iteration (network layer). These are ini-
tialised, by some function I, from an input vector of ob-
servations, xv, as µ0
v = I(xv). During the message-passing
phase, the embeddings are repeatedly updated with informa-
tion from neighbouring vertices, N (v), according to
(cid:1),
v, {µk
u}u
∈
(cid:1),
v, mk+1
(3)
v
where Mk and Uk are message and update functions, respec-
tively. After K rounds of message passing, a prediction is
produced by some readout function, R. In our case this pre-
diction is the set of Q-values of the actions corresponding to
“ﬂipping” each vertex, i.e. adding or removing it from the
solution subset S, {Qv}v

mk+1
v = Mk
µk+1
v = Uk

N (v), {wuv}u

(cid:0)µk
(cid:0)µk

V ).

N (v)

(2)

∈

V = R({µK
∈

u }u
∈

3 Exploiting Exploration
One straightforward application of Q-learning to CO over a
graph is to attempt to directly learn the utility of adding any
given vertex to the solution subset. This formalism, which
is followed by S2V-DQN and related works, incrementally
constructs a solution by adding one vertex at a time to the
subset, until no further improvement can be made. However,
the complexity of NP-hard combinatorial problems means
it is challenging to learn a single function approximation of
Q∗(s, a) that generalises across the vast number of possible
graphs. Therefore, as vertices can only be added to the solu-
tion set, policies derived from the learnt Q-values, such as a
typical greedy policy, will likely be sub-optimal.

In this work we present an alternative approach where the
agent is trained to explore the solution space at test time,
seeking ever-improving states. As such, the Q-value of ei-
ther adding or removing a vertex from the solution is con-
tinually re-evaluated in the context of the episode’s history.
Additionally, as all actions can be reversed, the challenge of
predicting the true value of a vertex “ﬂip” does not neces-
sarily result in sub-optimal performance. The fundamental
change distinguishing our approach, ECO-DQN, from pre-
vious works can then be summarised as follows: instead of
learning to construct a single good solution, learn to explore
for improving solutions.

However, simply allowing for revisiting the previously
ﬂipped vertices does not automatically improve perfor-
mance. The agent is not immediately able to make more
informed decisions, nor can it reach previously unobtain-
able solutions. Instead, further modiﬁcations are required to
leverage this freedom for improved performance, which we
now discuss.

Reward Shaping. The objective of our exploring agent
is to ﬁnd the best solution (highest cut-value) at any point
within an episode. Formally, the reward at state st ∈ S
is given by R(st) = max(C(st) − C(s∗), 0)/|V |, where
s∗ ∈ S is the state corresponding to the highest cut value
previously seen within the episode, C(s∗) (note that we im-
plicitly assume the graph, G, and solution subset, S, to be
included in the state). As continued exploration is desired,
even after a good solution is found, there is no punishment
if a chosen action reduces the cut-value. The reward is nor-
malised by the total number of vertices, |V |, to mitigate
the impact of different reward scales across different graph
sizes. We use a discount factor of γ = 0.95 to ensure the
agent actively pursues rewards within a ﬁnite time horizon.
As our environment only provides a reward when a new
best solution is found, after an initial period of exploration,
these extrinsic rewards can be sparse, or absent, for the re-
mainder of the episode. We therefore also provide a small
intermediate reward of 1/|V | whenever the agent reaches a
locally optimal state (one where no action will immediately
increase the cut value) previously unseen within the episode.
In addition to mitigating the effect of sparse extrinsic re-
wards, these intrinsic rewards also shape the exploratory be-
haviour at test time. There are far more states than could be
visited within our ﬁnite episodes, the vast majority of which
are signiﬁcantly sub-optimal, and so it is useful to focus on

a subset of states known to include the global optimum. As
local optima in combinatorial problems are typically close
to each other, the agent learns to “hop” between nearby lo-
cal optima, thereby performing a in-depth local search of the
most promising subspace of the state space (see ﬁgure 2c).

Observations A Q-value for ﬂipping each vertex is calcu-
lated using seven observations, (xv∈R7), derived from the
current state, with a state corresponding to both the target
graph, G(V, W ), and the current subset of vertices assigned
to the solution set, S ⊂ V . These observations are:

1. Vertex state, i.e. if v is currently in the solution set, S.

2. Immediate cut change if vertex state is changed.

3. Steps since the vertex state was last changed.

4. Difference of current cut-value from the best observed.

5. Distance of current solution set from the best observed.

6. Number of available actions that immediately increase the

cut-value.

7. Steps remaining in the episode.

Observations (1-3) are local, which is to say they can be dif-
ferent for each vertex considered, whereas (4-7) are global,
describing the overall state of the graph and the context of
the episode. The general purposes of each of the observa-
tions are: (1-2) provide useful information for determining
the value of selecting an action; (3) provides a simple history
to prevent short looping trajectories; (4-6) ensure the extrin-
sic and intrinsic rewards are Markovian; and (7) accounts for
the ﬁnite episode duration.

Experiments
Experimental details.
In this work we train and test the
(Erd˝os and R´enyi 1960) and
agent on both Erd˝os-R´enyi
(Albert and Barab´asi 2002) graphs with
Barabasi-Albert
edges wij ∈ {0, ±1}, which we refer to as ER and BA
graphs, respectively. Training is performed on randomly
generated graphs from either distribution, with each episode
considering a freshly generated instance. The performance
over training (i.e. all learning curves) is evaluated as the
mean of a ﬁxed set of 50 held-out graphs from the same dis-
tribution. Once trained, the agents are tested on a separate set
of 100 held-out validation graphs from a given distribution.
During training and testing, every action taken demarks a
timestep, t. For agents that are allowed to take the same ac-
tion multiple times (i.e. ECO-DQN and selected ablations),
which for convenience we will refer to as reversible agents,
the episode lengths are set to twice the number of vertices in
the graph, t = 1, 2, . . . , 2|V |. Each episode for such agents
is initialised with a random subset of vertices in the solu-
tion set. By contrast, agents that can only add vertices to
the solution set (irreversible agents, i.e. S2V-DQN and se-
lected ablations) are initialised with an empty solution sub-
set. These agents keep selecting actions greedily even if no
positive Q-values are available until t = |V |, to account for
possible incorrect predictions of the Q-values. In both cases
the best solution obtained at any timestep within the episode
is taken as the ﬁnal result. To facilitate direct comparison,

Figure 1: Performance comparison of ECO-DQN and baselines. (a) Learning curves, averaged over 5 seeds, when training on
40-vertex ER graphs. (b-c) Approximation ratios for ER and BA graphs with different numbers of vertices, |V |. We report the
mean approximation ratios over the 100 validation graphs, along with the distance to the upper and lower quartiles.

ECO-DQN and S2V-DQN are implemented with the same
MPNN architecture, with details provided in the Appendix.

Benchmarking details We compare the performance of
ECO-DQN to a leading RL-based heuristic, S2V-DQN.
To interpret the performance gap, we also consider the
following ablations, which together
for
the differences between our approach and the baseline
(ECO-DQN ≡ S2V-DQN+RevAct+ObsTun+IntRew).
• Reversible Actions (RevAct): Whether the agent is al-
lowed to ﬂip a vertex more than once. For irreversible
agents we follow S2V-DQN and use γ=1.

fully account

• Observation Tuning (ObsTun): Observations (2-7) from
the list above that allow the agent to exploit having re-
versible actions. Also, in the absence of ObsTun, the re-
wards used are simply the (normalised) immediate change
1))/|V |, which is
in cut value, R(st) = (C(st) − C(st
necessary as without observations (4-5) the ECO-DQN
reward structure is non-Markovian.

−

• Intermediate Rewards (IntRew): Whether the agent is pro-
vided with the small intermediate rewards for reaching
new locally optimal solutions.

As an additional benchmark we also implement the Max-
CutApprox (MCA) algorithm. This is a greedy algorithm,
choosing the action (vertex) that provides the greatest im-
mediate increase in cut value until no further improvements
can be made. We consider two modiﬁcations of MCA. The
standard application, which we denote MCA-irrev, is irre-
versible and begins with an empty solution set. The alterna-
tive algorithm, MCA-rev, starts with a random solution set
and allows reversible actions.

We use the approximation ratio – C(s∗)/C(sopt), where
C(sopt) is the cut-value of the true optimum solution – of
each approach as a metric of solution quality. Exact meth-
ods are intractable for many of the graph sizes we use, there-
fore we apply a battery of optimization approaches to each
graph and take the best solution found by any of them as the
“optimum” solution. Speciﬁcally, in addition to ECO-DQN,
S2V-DQN and the MCA algorithms, we use CPLEX, an in-
dustry standard integer programming solver, and a pair of
recently developed simulated annealing heuristics by Tiunov

et al. (2019) and Leleu et al. (2019). Details of these imple-
mentations and a comparison of their efﬁcacy can be found
in the Supplemental Material.

Performance benchmarking. Figure 1a shows learning
curves of agents trained on ER graphs of size |V | = 40,
where it can be seen that ECO-DQN reaches a signiﬁ-
cantly higher average cut than S2V-DQN. Removing either
reversible actions (RevAct) or the additional observations
(ObsTun) reduces the performance below that of S2V-DQN,
underlining our previous assertion that obtaining state-of-
the-art performance requires not only that the agent be al-
lowed to reverse its previous actions, but also that it be suit-
ably informed and rewarded to do so effectively. Intermedi-
ate rewards (IntRew) are seen to speed up and stabilise train-
ing. They also result in a small performance improvement,
however this effect becomes clearer when considering how
the agents generalise to larger graphs (see ﬁgures 3a and 3b).
Figures 1b and 1c show the performance of agents trained
and tested on graphs with up to 200 vertices. We see that
ECO-DQN has superior performance across most consid-
ered graph sizes and structures. Both ECO-DQN and S2V-
DQN have similar computational costs per action, with ex-
tended performance comparisons provided in the appendix.

Intra-episode behaviour. We now consider how this
strong performance is achieved by examining the intra-
episode behaviour of an agent trained and tested on 200-
vertex ER graphs. The larger graph size is chosen as it pro-
vides greater scope for the agent to exhibit non-trivial be-
haviour. Figure 2a highlights the trajectories taken by the
trained agent on graphs from the validation set. Whilst the
overall trend is towards higher cut-values, the ﬂuctuations
show that the agent has learnt to search for improving so-
lutions even when this requires sacriﬁcing cut-value in the
short-term. Further analysis of the agent’s behaviour is pre-
sented in ﬁgures 2b and 2c which show the action prefer-
ences and the types of states visited, respectively, over the
course of an optimization episode.

From ﬁgure 2b, we see that the fully trained agent regu-
larly chooses actions that do not correspond to the greatest
immediate increase in the cut-value (Non-Greedy), or even
that decrease the cut value (Negative). Moreover, the agent

proximation ratios corresponds to the upper and lower quar-
tiles of the performance across all 100 validation graphs.

4 Leveraging Variance
Changing the initial subset of vertices selected to be in the
solution set can result in very different trajectories over the
course of an episode. An immediate result of this stochastic-
ity is that performance can be further improved by running
multiple episodes with a distribution of initialisations, and
selecting the best result from across this set.

Experiments
We optimize every graph using 50 randomly initialised
episodes. At the same time, we make the task more challeng-
ing by testing on graphs that are larger, or that have a differ-
ent structure, from those on which the agent was trained.
This ability to generalise to unseen challenges is important
for the real-world applicability of RL agents to combinato-
rial problems where the distribution of optimization tasks
may be unknown or even change over time.

Generalisation to unseen graph types. Figures 3a and 3b
show the generalisation of agents trained on 40 vertices to
systems with up to 500 vertices for ER and BA graphs, re-
spectively. (Generalisation data for agents trained on graphs
of sizes ranging from |V | = 20 to |V | = 200 can be found in
the Appendix.) ECO-DQN is compared to multiple bench-
marks, with details provided in the caption, however there
are three important observations to emphasise. Firstly, re-
versible agents outperform the irreversible benchmarks on
all tests, with the performance gap widening with increasing
graph size. This is particularly noticeable for BA graphs, for
which the degrees of each vertex in the graph tend to be dis-
tributed over a greater range than for ER graphs, where S2V-
DQN fails to generalise in any meaningful way to |V |≥200.
Secondly, for the reversible agents it is clear that using
multiple randomly initialised episodes provides a signiﬁcant
advantage. As ECO-DQN provides near-optimal solutions
on small graphs within a single episode, it is only on larger
graphs that this becomes relevant. However, it is notewor-
thy that even the simple MCA-rev algorithm, with only a
relatively modest budget of 50 random initialisations, out-
performs a highly trained irreversible heuristic (S2V-DQN).
This further emphasises how stochasticity – which here is
provided by the random episode initialisations and ensures
many regions of the solution space are considered – is a pow-
erful attribute when combined with local optimization.

Finally, we again observe the effect of small intermediate
rewards (IntRew) for ﬁnding locally optimal solutions dur-
ing training upon the ﬁnal performance. For small graphs the
agent performs near-optimally with or without this intrin-
sic motivation, however the difference becomes noticeable
when generalising to larger graphs at test time.

In ﬁgure 3c we observe that ECO-DQN performs well
across a range of graph structures, even if they were not rep-
resented during training, which is a highly desirable char-
acteristic for practical CO. We train the agent on ER graphs
with |V |=40 and then test it on BA graphs of up to |V |=500,
and vice versa. The performance is marginally better when

Figure 2: Intra-episode behaviour averaged across all 100
instances from the validation set for ER graphs with |V | =
200. (a) Mean (dashed) and range (shaded) of all trajectories,
with three examples (solid) shown for reference. (b) The
probability that the chosen action has already been taken
within the episode (Repeats), does not provide the greatest
immediate reward (Non-Greedy) or reduces the cut-value
(Negative). (c) The probability that the current state is lo-
cally optimal (Locally Optimal), has already been visited
within the episode (Revisited), and that the best solution
that will be found within the episode has already been seen
(MC found). The behaviour is shown at three points during
training: when performance is equivalent to that of MCA-
irrev (dotted) or S2V-DQN (dashed), and when fully trained
(solid). (b-c) use a 10-step moving average over all graphs
(trajectories) in the validation set, however, the shaded errors
are only shown for the fully trained agent in (c).

also moves the same vertex in or out of the solution set mul-
tiple times within an episode (Repeats), which suggests the
agent has learnt to explore multiple possible solutions that
may be different from those obtained initially.

This is further emphasised in ﬁgure 2c where we see
that, after an initial period of exploration, the agent searches
through the solution space, repeatedly moving in and out of
locally optimal (Locally Optimal) solutions whilst minimis-
ing the probability that it revisits states (Revisited). By com-
paring the agent at three points during training (fully trained
and when performance level is equivalent to either MCA-
irrev or S2V-DQN), we see that this behaviour is learnt.
Weaker agents from earlier in training revisit the same states
far more often, yet ﬁnd fewer locally optimal states. The
probability that the fully-trained agent has already found the
best solution it will see in the episode (MC found) grows
monotonically, implying that the agent ﬁnds ever better solu-
tions while exploring. Indeed, for this agent, simply increas-
ing the number of timesteps in an episode from 2|V |=400
to 4|V | is seen to increase the average approximation ratio
from 0.98+0.01
0.01. The range quoted for these ap-

0.01 to 0.99+0.01

−

−

Figure 3: Generalisation of agents to unseen graph sizes and structures. (a-b) The performance of agents trained on ER and BA
graphs of size |V |=40 tested on graphs of up to |V |=500 of the same type. ECO-DQN is shown with and without providing the
intermediate rewards (IntRew) for ﬁnding locally optimal solutions during training, and is compared to the MCA-rev algorithm.
Reversible agents are applied with 50 randomly initialised episodes to each of the 100 validation graphs for each type and size.
The ﬁrst marking on each bar is the average across every episode (the expected ‘single-try’ performance), with the upper limit
extending to the average performance across different graphs. The vertical bars denote the 68 % conﬁdence interval of this
upper limit. Irreversible approaches are initialised with empty solution sets, and so only use 1 episode per graph. Note that
S2V-DQN applied to BA graphs with |V |=500 is not visible on these axes but has a value of 0.49+0.12
0.11. (c) A comparison of
how agents trained on only one of either ER or BA graphs with |V |=40, perform on larger graphs from both distributions.

−

testing on graphs from the same distribution as the training
data, however this difference is negligible for |V | ≤ 100.
Furthermore, in every case deploying ECO-DQN with 50
randomly initialised episodes outperforms all other bench-
marks (S2V-DQN and MCA), even when only ECO-DQN
is trained on different graph types to the test data.

Generalization to real-world datasets. Finally, we test
ECO-DQN on publicly available datasets. The “Physics”
dataset consists of ten graphs – with |V |=125, exactly
6 connections per vertex and wij∈{0, ±1} – correspond-
ing to Ising models of physical systems. The GSet is a
well-investigated benchmark collection of graphs (Benlic
and Hao 2013). We separately consider ten graphs, G1-
G10, with |V |=800, and ten larger graphs, G22-G32, with
|V |=2000. For G1-G10 we utilise 50 randomly initialised
episodes per graph, however for G22-G32 we use only a
single episode per graph, due to the increased computational
cost. We apply agents trained on ER graphs with |V |=200.
The results are summarised in table 1, where ECO-DQN
is seen to signiﬁcantly outperform other approaches, even
when restricted to use only a single episode per graph.

Dataset
Physics
G1-10
G22-32

ECO-DQN S2V-DQN MCA-(rev, irrev)

1.000
0.996
0.971

0.928
0.950
0.919

0.879, 0.855
0.947, 0.913
0.883, 0.893

Table 1: Average performance on known benchmarks.

Despite the structure of graphs in the “Physics” dataset
being distinct from the ER graphs on which the agent is
trained, every instance in optimally solved. Averaged across
all graphs, 37.6 % of episodes ﬁnd an optimal solution and
90.4 % of these solutions are unique, demonstrating that, in
conjunction with random initialisations, the agent is capable
of ﬁnding many different optimal trajectories. Importantly,
the structure of the GSet is distinct from that of the training

data, with the ﬁrst ﬁve instances in each tested set have only
positive edges, wij∈{0, 1}.

5 Summary and Outlook

This work introduces ECO-DQN, a new state-of-the-art RL-
based algorithm for the Max-Cut problem that generalises
well to unseen graph sizes and structures. We show that
treating CO as an ongoing exploratory exercise in surpass-
ing the best observed solution is a powerful approach to this
NP-hard problem. In principle, our approach is applicable to
any combinatorial problem deﬁned on a graph.

ECO-DQN can initialise a search from any valid state,
opening the door to combining it with other search heuris-
tics. We obtained further improved performance with a
simple “heuristic” of randomly initialised episodes, how-
ever, one could consider combining ECO-DQN with more
sophisticated episode initialisation policies. Alternatively,
ECO-DQN could also be initialised with solutions found
by other optimization methods to further strengthen them.
Also, we train our agents with highly discounted future
rewards (γ= 0.95), and although this is found to provide
strong performance, the relatively short-term reward hori-
zon likely limits exploration to only local regions of the so-
lution space. As such, it would be interesting to investigate
longer reward-horizons, particularly when training on larger
graphs. A more substantial avenue to explore would be to
use a recurrent architecture to learn a useful representation
of the episode history, as opposed to the hand-crafted repre-
sentation that we describe in section 3.

Whilst

these are paths towards further developing
exploration-based CO, we believe that the strong perfor-
mance already demonstrated would allow our approach to
be applied in numerous practical settings. This is especially
true for settings where many graphs of similar structure need
to be optimized, such as protein folding (Perdomo-Ortiz et
al. 2012) and portfolio optimization (Elsokkary et al. 2017;
Venturelli and Kondratyev 2018).

Test
→Train
↓

|V |=20
|V |=40
|V |=60
|V |=100
|V |=200

|V |=20

|V |=40

|V |=60

|V |=100

|V |=200

|V |=500

|V |=20

|V |=40

|V |=60

|V |=100

|V |=200

|V |=500

ER graphs

0.99+0.01
0.01
−
—
—
—
—

1.00+0.00
0.00
−
1.00+0.00
0.00
−
—
—
—

1.00+0.00
0.00
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−
—
—

1.00+0.00
0.00
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−
—

0.98+0.01
0.01
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−

0.95+0.01
0.01
−
0.98+0.01
0.01
−
0.99+0.01
0.01
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−

BA graphs

1.00+0.00
0.00
−
—
—
—
—

1.00+0.00
0.00
−
1.00+0.00
0.00
−
—
—
—

1.00+0.00
0.00
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−
—
—

1.00+0.00
0.00
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−
—

0.99+0.01
0.01
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−
1.00+0.00
0.00
−
0.99+0.01
0.01
−

0.98+0.01
0.01
−
0.98+0.01
0.01
−
0.99+0.01
0.01
−
0.98+0.01
0.01
−
0.98+0.01
0.01
−

Table 2: Generalisation performance of ECO-DQN, using 50 randomly initialised episodes per graph.

Acknowledgements
The authors would like to thank D. Chermoshentsev and
A. Boev. A.L.’s research is partially supported by Russian
Science Foundation (19-71-10092).

Appendix
Code and graph availability. Source code, including ex-
perimental scripts and all testing and validation graphs, can
be found at https://github.com/tomdbar/eco-dqn.

Graphs were generated with the NetworkX Python pack-
age (Hagberg, Schult, and Swart 2008). For ER graphs, a
connection probability of 0.15 is used. The BA graphs have
an average degree of 4. To produce our target graphs we then
randomly set all non-zero edges to ±1.

MPNN architecture. The initial embedding for each ver-
tex, v, is given by

µ0
v = relu(θ1xv)
(4)
where xv∈Rm is the input vector of observations and
θ1∈Rm
n. We also learn embeddings describing the con-
nections to each vertex v,
(cid:16)

relu(cid:0)θ2[wuv, xu](cid:1), |N (v)|(cid:3)(cid:17)

ξv = relu

(cid:88)

θ3

×

(cid:2)

,

1
N (v)

|

|

u

N (v)

∈

(5)
where θ2∈Rm+1
n and square bracket denote
×
concatenation. The embeddings at each vertex are then up-
dated according to

1, θ3∈Rn
×

−

n

(cid:16)
v = relu

mk+1

(cid:2)

θ4,k

(cid:88)

wuvµk

u, ξv

(cid:3)(cid:17)

,

1
N (v)
|
|

u

∈
v, mk+1
v

(cid:2)µk

N (v)
(cid:3)(cid:17)
,

(cid:16)
µk+1
v = relu

θ5,k

where {θ4,k, θ5,k}∈R2n
n. After K rounds of message
passing, the Q-value for a vertex is read out using the ﬁnal
embeddings across the entire graph,

×

Qv = θ7

(cid:104)

relu(cid:0)θ6

µK
u

(cid:1), µK

v

(cid:105)

,

1
V
|

|

(cid:88)

u

V

∈

with θ6∈Rn

n and θ7∈R2n.

×

In this work we use n=64 dimensional embedding vec-
tors, and have K=3 rounds of message passing. However,
many different MPNN implementations can be used with
good success. In general, what is important is that the net-
work be capable of capturing relevant information about the
local neighbourhood of a vertex.

(6)

(7)

(8)

Training details. The Q-learning algorithm for ECO-
DQN is shown in algorithm 1. All agents are trained with
a minibatch sizes of 64 and k=32 actions per step of gra-
4 and the exploration
dient descent. The learning rate is 10−
rate is linearly decreased from ε=1 to ε=0.05 over the ﬁrst
∼10 % of training. We use the same MPNN for both S2V-
DQN and ECO-DQN. We verify that this network properly
represents S2V-DQN by reproducing its performance on the
‘Physics’ dataset at the level reported in the original work
by Khalil et al. (2017). During the training of an irreversible
agent’s Q-network, the predictions of the Q-values produced
by the target network are clipped to be strictly non-negative.
This clipping is also used by Khalil et al. and is empirically
observed to improve and stabilise training.

Algorithm 1: Q-learning for ECO-DQN
Initialize experience replay memory M.
for each episode do

Sample a graph G(V, W ) from distribution D
Initialise a random solution set, S0 ⊂ V
for each step t in the episode do

(cid:26)choose random v(cid:48) ∈ V,

with prob. ε

V Q(St, v(cid:48); θ), otherwise

vt =

∈

St+1 :=

argmaxv(cid:48)
(cid:26)St ∪ {vt},
if vt /∈ St
if vt ∈ St
St \ {vt},
Add tuple (St, vt, Rt, St+1) to M
if t mod k = 0 then

Sample minibatch B ∼ M
Update θ by SGD for B

end

end

end
return θ

Extended data. ECO-DQN’s generalisation performance
on ER and BA graphs is shown in table 2. ECO-DQN
and S2V-DQN have a similar computational cost per time-
step, which is largely determined by computing the MPNN-
predicted Q-values, with the overall time to solution depend-
ing on the graph size and episode length. This performance
is compared to the speed of simple MCA agents in table 3.

Graph size
|V |=20
|V |=500

ECO-DQN
(0.29 ± 0.06) ms
(6.64 ± 0.09) ms

MCA-(rev, irrev)
(0.10 ± 0.02) ms
(1.38 ± 0.06) ms

Table 3: Time per action for ECO-DQN and the greedy
MCA algorithms across different graph sizes. Experiments
were performed on NVIDIA Tesla M60 GPUs.

References
Abe, K.; Xu, Z.; Sato, I.; and Sugiyama, M. 2019. Solving NP-
Hard Problems on Graphs by Reinforcement Learning without Domain
Knowledge. arXiv:1905.11623.

Albert, R., and Barab´asi, A.-L. 2002. Statistical mechanics of complex
networks. Reviews of Modern Physics 74(1):47.

Banks, A.; Vincent, J.; and Anyakoha, C. 2008. A review of particle
swarm optimization. Part II: hybridisation, combinatorial, multicrite-
ria and constrained optimization, and indicative applications. Natural
Computing 7(1):109–124.

Barahona, F. 1982. On the computational complexity of Ising spin
Journal of Physics A: Mathematical and General
glass models.
15(10):3241.

Bello, I.; Pham, H.; Le, Q. V.; Norouzi, M.; and Bengio, S. 2016.
Neural Combinatorial Optimization with Reinforcement Learning.
arXiv:1611.09940.

Benlic, U., and Hao, J.-K. 2013. Breakout Local Search for the
Max-Cut problem. Engineering Applications of Artiﬁcial Intelligence
26(3):1162 – 1173.

Clements, W. R.; Renema, J. J.; Wen, Y. H.; Chrzanowski, H. M.;
Kolthammer, W. S.; and Walmsley, I. A. 2017. Gaussian Optical Ising
Machines. Phys. Rev. A 96:043850.

Dinur, I., and Safra, S. 2005. On the Hardness of Approximating
Minimum Vertex Cover. Annals of Mathematics 439–485.

Elsokkary, N.; Khan, F. S.; La Torre, D.; Humble, T. S.; and Gottlieb, J.
2017. Financial Portfolio Management using D-Wave Quantum Opti-
mizer: The Case of Abu Dhabi Securities Exchange. Technical report,
Oak Ridge National Lab.(ORNL), Oak Ridge, TN (United States).

Erd˝os, P., and R´enyi, A. 1960. On the Evolution of Random Graphs.
Publ. Math. Inst. Hung. Acad. Sci 5(1):17–60.

Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and Dahl, G. E.
2017. Neural Message Passing for Quantum Chemistry. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70,
1263–1272.

Goemans, M. X., and Williamson, D. P. 1995. Improved Approxima-
tion Algorithms for Maximum Cut and Satisﬁability Problems using
Semideﬁnite Programming. Journal of the ACM (JACM) 42(6):1115–
1145.

Hagberg, A. A.; Schult, D. A.; and Swart, P. J. 2008. Exploring Net-
work Structure, Dynamics, and Function using NetworkX. In In Pro-
ceedings of the 7th Python in Science Conference.

Hopﬁeld, J. J., and Tank, D. W. 1985. “Neural” Computation of De-
cisions in Optimization Problems. Biological Cybernetics 52(3):141–
152.

Johnson, M. W.; Amin, M. H.; Gildert, S.; Lanting, T.; Hamze, F.;
Dickson, N.; Harris, R.; Berkley, A. J.; Johansson, J.; Bunyk, P.;
et al. 2011. Quantum Annealing with Manufactured Spins. Nature
473(7346):194.

Karp, R. M. 1972. Reducibility Among Combinatorial Problems. In
Complexity of Computer Computations. Springer. 85–103.

Khalil, E.; Dai, H.; Zhang, Y.; Dilkina, B.; and Song, L. 2017. Learning
Combinatorial Optimization Algorithms over Graphs. In Advances in
Neural Information Processing Systems, 6348–6358.
Kirkpatrick, S.; Gelatt, C. D.; and Vecchi, M. P. 1983. Optimization
by Simulated Annealing. Science 220(4598):671–680.
Kochenberger, G. A., and Glover, F. 2006. A uniﬁed framework for
modeling and solving combinatorial optimization problems: A tutorial.
In Multiscale Optimization Methods and Applications. Springer. 101–
124.
Leleu, T.; Yamamoto, Y.; McMahon, P. L.; and Aihara, K. 2019. Desta-
bilization of local minima in analog spin systems by correction of am-
plitude heterogeneity. Phys. Rev. Lett. 122:040607.
Li, Z.; Chen, Q.; and Koltun, V. 2018. Combinatorial Optimization
with Graph Convolutional Networks and Guided Tree Search. In Ad-
vances in Neural Information Processing Systems, 539–548.
Markowitz, H. 1952. Portfolio Selection. The Journal of Finance
7(1):77–91.
Mittal, A.; Dhawan, A.; Medya, S.; Ranu, S.; and Singh, A. 2019.
Learning Heuristics over Large Graphs via Deep Reinforcement Learn-
ing. arXiv:1903.03332.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Belle-
mare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski,
G.; et al. 2015. Human-level control through deep reinforcement learn-
ing. Nature 518(7540):529.
Papadimitriou, C. H. 1977. The Euclidean travelling salesman problem
is NP-complete. Theoretical Computer Science 4(3):237–244.
Perdomo-Ortiz, A.; Dickson, N.; Drew-Brook, M.; Rose, G.; and
Aspuru-Guzik, A. 2012. Finding low-energy conformations of lattice
protein models by quantum annealing. Scientiﬁc Reports 2:571.
Silver, D.; Schrittwieser, J.; Simonyan, K.; Antonoglou, I.; Huang, A.;
Guez, A.; Hubert, T.; Baker, L.; Lai, M.; Bolton, A.; et al. 2017.
Mastering the game of Go without Human Knowledge. Nature
550(7676):354.
Smith, K. A. 1999. Neural Networks for Combinatorial Optimization:
A Review of More Than a Decade of Research. INFORMS Journal on
Computing 11(1):15–34.
Tiunov, E. S.; Ulanov, A. E.; and Lvovsky, A. 2019. Annealing by
simulating the coherent Ising machine. Optics Express 27(7):10288–
10295.
Venturelli, D., and Kondratyev, A. 2018. Reverse Quantum Anneal-
ing Approach to Portfolio Optimization Problems. Quantum Machine
Intelligence 1–14.
Vinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer Networks. In
Advances in Neural Information Processing Systems, 2692–2700.
Wigley, P. B.; Everitt, P. J.; van den Hengel, A.; Bastian, J. W.; Sooriya-
bandara, M. A.; McDonald, G. D.; Hardman, K. S.; Quinlivan, C. D.;
Manju, P.; Kuhn, C. C.; et al. 2016. Fast machine-learning online op-
timization of ultra-cold-atom experiments. Scientiﬁc reports 6:25890.
Williamson, D. P., and Shmoys, D. B. 2011. The Design of Approxi-
mation Algorithms. Cambridge University Press.
Wilson, G., and Pawley, G. 1988. On the stability of the Travelling
Salesman Problem algorithm of Hopﬁeld and Tank. Biological Cyber-
netics 58(1):63–70.
Yamamoto, Y.; Aihara, K.; Leleu, T.; Kawarabayashi, K.-i.; Kako, S.;
Fejer, M.; Inoue, K.; and Takesue, H. 2017. Coherent Ising machines—
optical neural networks operating at the quantum limit. npj Quantum
Information 3(1):49.
Zhang, W., and Dietterich, T. G. 1995. A Reinforcement Learning
Approach to Job-Shop Scheduling. In IJCAI, volume 95, 1114–1120.
Citeseer.

Supplemental Material

V

|∈{

20, 40, 60, 100, 200, 500

We apply six different optimization methods to the 100 vali-
dation graphs of each structure (ER or BA graphs) and size
(
). The highest cut value across the
}
|
board is then chosen as the reference point that we refer to as the
“optimum value”. Table 4 compares the performance of these meth-
ods on our validation sets. The number of times each method reaches
these “optimum” solutions is then shown in table 5. Below we give an
overview of each method and summarise their efﬁcacy.

|

|

V

ECO-DQN. The framework introduced and discussed in detail in
the main text. We train distinct agents on every graph structure and
= 200, and then test them on graphs of the same or
size, up to
larger sizes. For each individual agent-graph pair, we run 50 randomly
initialised optimization episodes. Therefore, graphs with
= 20
are subject to only 50 optimization attempts, whereas graphs with
= 500 are optimised with 300 episodes using 5 distinct agents.
V
|
The performance of each agent is summarised in table 2 of the main
text.

V

|

|

|

S2V-DQN. An RL framework for graph-based combinatorial prob-
lems introduced by Khalil et al. (2017). Details of our implementation
can, again, be found in the main text. S2V-DQN agents are trained and
tested equivalently to the ECO-DQN agents. However, as S2V-DQN is
deterministic at test time, only a single optimization episode is used for
every agent-graph pair.

MCA. The ﬁnal optimization method introduced in the main text
is MaxCutApprox (MCA). This is a simple greedy algorithm that can
be appliedeither in the reversible or irreversible setting. We refer to
these as MCA-rev and MCA-irrev, respectively. For every optimiza-
tion episode of ECO-DQN or S2V-DQN, a corresponding MCA-rev or
MCA-irrev episode is also undertaken. We take the best solution found
in any episode by either of these greedy algorithms as the MCA solu-
tion.

We see from tables 4 and 5 that the greedy MCA algorithms ﬁnd
optimal solutions on nearly all graphs of size up to
= 60, but per-
formance rapidly deteriorates thereafter. As the number of possible so-
lution conﬁgurations (states) grows exponentially with the number of
vertices, this simply reﬂects how it quickly becomes intractable to suf-
ﬁciently cover the state-space in our ﬁnite number of episodes.

V

|

|

Approach

V
|

| →
↓

20

40

60

100 200 500

ER graphs
ECO-DQN 0.99 1.00 1.00 1.00 1.00 1.00
S2V-DQN 0.97 0.99 0.99 0.98 0.96 0.95
1.00 1.00 1.00 0.99 0.98 0.96
MCA
CPLEX
1.00 1.00 1.00 0.87 0.46 0.16
SimCIM 1.00 1.00 1.00 1.00 0.99 0.99
Leleu et al. 1.00 1.00 1.00 1.00 1.00 1.00
BA graphs
ECO-DQN 1.00 1.00 1.00 1.00 1.00 0.99
S2V-DQN 0.97 0.98 0.98 0.97 0.96 0.92
MCA
1.00 1.00 1.00 0.99 0.95 0.90
1.00 1.00 1.00 1.00 0.83 0.17
CPLEX
SimCIM 1.00 1.00 1.00 0.99 0.99 0.97
Leleu et al. 1.00 1.00 1.00 1.00 0.94 1.00

CPLEX. A commercial optimizer. We ﬁrst transform the Max-Cut
problem into a QUBO (Quadratic Unconstrained Binary Optimization)
task (Kochenberger and Glover 2006). Strictly, for a graph, G(V, W ),
with vertices V connected by edges W , this task is to minimize the
Hamiltonian

H =

(cid:88)

−

i,j

wij (xi

xj )2,

−

(9)

where xk
subset, S
programming (MIP) by the CPLEX branch-and-bound routine.

V is in the solution
V . This Hamiltonian is then solved using mixed integer

labels whether vertex k

∈ {±
⊂

∈

1

}

For each graph, we take the best solution found within 10 min as the
ﬁnal answer. Within this time budget we ﬁnd the exact solution on all
graphs with up to 60 vertices. Only some of the 100-vertex graphs are
optimally solved, with performance signiﬁcantly dropping for the 200
and 500 vertex graphs due to the unfeasibly large solution space.

}

1

∈ {±

SimCIM. A simulated annealing heuristic proposed by Tiunov et
al. (2019) that models the classical dynamics within a coherent Ising
machine (CIM) (Yamamoto et al. 2017). This approach relaxes the bi-
), associated with labelling a vertex as
nary vertex values (xk
either in or out of the solution subset, to analog values (
1).
Each vertex is initialised to 0, and then subjected to evolution according
to a set of stochastic differential equations that describe the operation of
the CIM. In the process of the evolution, the system eventually settles
with all vertices in near-binary states. Details of both CIM and SimCIM
beyond the high-level description given here can be found in the refer-
enced works. The hyperparameters of SimCIM were optimised using a
differential evolution approach by M-LOOP (Wigley et al. 2016) over
50 runs.

xk

−

≤

≤

1

Leleu et al.. Another recently developed simulated annealing
heuristic that relaxes the binary vertex labels to analog values. A key
feature of this approach is the modiﬁcation of the time-dependent in-
teraction strengths in such a way as to destabilise locally optimal solu-
tions. Details can be found in the work of Leleu et al. (2019). As with
SimCIM, the hyperparameters are adjusted by M-LOOP (Wigley et al.
2016) over 50 runs.

Approach

V
|

| →
↓

20

40

60 100 200 500

ER graphs
ECO-DQN 99 100 100 100 100 50
0
S2V-DQN 76
0
MCA
0
CPLEX
SimCIM 100 92
0
Leleu et al. 100 100 100 98 100 92
BA graphs

67
55
100 100 93
100 100 100
92

18
49
0
71

0
3
0
13

ECO-DQN 100 100 100 100 95
1
S2V-DQN 74
18
60
0
MCA
100 99
24
0
CPLEX
100 100 100 97
12
SimCIM 100 87
60
94
Leleu et al. 100 97
99 100 96

39
85

12
0
0
0
0
97

Table 4: The approximation ratios, averaged across 100
graphs for each graph structure and size, of the different op-
timization methods.

Table 5: The relative contributions of the different optimiza-
tion methods to the “optimum” solutions. Shown is the num-
ber of graphs (out of 100) for which each approach ﬁnds the
best, or equal best, solution.

