8
1
0
2
 
r
a

M
 
8
 
 
]

V
C
.
s
c
[
 
 
3
v
8
7
7
6
0
.
1
1
7
1
:
v
i
X
r
a

Excitation Backprop for RNNs

Sarah Adel Bargal∗1, Andrea Zunino∗ 2, Donghyun Kim1, Jianming Zhang3,
Vittorio Murino2,4, Stan Sclaroff1

1Department of Computer Science, Boston University 2Pattern Analysis & Computer Vision (PAVIS),
Istituto Italiano di Tecnologia 3Adobe Research 4Computer Science Department, Universit`a di Verona
{sbargal,donhk,sclaroff}@bu.edu, {andrea.zunino,vittorio.murino}@iit.it, jianmzha@adobe.com

Figure 1: Our proposed framework spatiotemporally highlights/grounds the evidence that an RNN model used in producing a class label
or caption for a given input video. In this example, by using our proposed back-propagation method, the evidence for the activity class
CliffDiving is highlighted in a video that contains CliffDiving and HorseRiding. Our model employs a single backward pass to produce
saliency maps that highlight the evidence that a given RNN used in generating its outputs.

Abstract

Deep models are state-of-the-art for many vision tasks
including video action recognition and video captioning.
Models are trained to caption or classify activity in videos,
but little is known about the evidence used to make such de-
cisions. Grounding decisions made by deep networks has
been studied in spatial visual content, giving more insight
into model predictions for images. However, such studies
are relatively lacking for models of spatiotemporal visual
content – videos. In this work, we devise a formulation that
simultaneously grounds evidence in space and time, in a
single pass, using top-down saliency. We visualize the spa-
tiotemporal cues that contribute to a deep model’s classiﬁ-
cation/captioning output using the model’s internal repre-
sentation. Based on these spatiotemporal cues, we are able
to localize segments within a video that correspond with a
speciﬁc action, or phrase from a caption, without explicitly
optimizing/training for these tasks.

1. Introduction

∗Equal contribution

To visualize what in a video gives rise to an output of a
deep recurrent network, it is important to consider space and

time saliency, i.e., where and when. The visualization of
what a deep recurrent network ﬁnds salient in an input video
can enable interpretation of the model’s behavior in action
classiﬁcation, video captioning, and other tasks. Moreover,
estimates of the model’s attention (e.g., saliency maps) can
be used directly in localizing a given action within a video
or in localizing the portions of a video that correspond to a
particular concept within a caption.

Several works address visualization of model attention in
Convolutional Neural Networks (CNNs) for image classiﬁ-
cation [1, 31, 21, 30, 18, 34, 15]. These methods produce
saliency maps that visualize the importance of class-speciﬁc
image regions (spatial localization). Analogous methods
for Recurrent Neural Network (RNN)-based models must
handle more complex recurrent, non-linear, spatiotempo-
ral dependencies; thus, progress on RNNs has been limited
to [7, 13]. Karpathy et al. [7] visualize the role of Long
Short Term Memory (LSTM) cells for text input, but not for
visual data. Ramanishka et al. [13] map words to regions
in the video captioning task by dropping out (exhaustively
or by sampling) video frames and/or parts of video frames
to obtain saliency maps. This can be computationally ex-
pensive, and does not consider temporal evolution but only
frame-level saliency.

1

In contrast, we propose the ﬁrst one-pass formulation
for visualizing spatiotemporal attention in RNNs, without
selectively dropping or sampling frames or frame regions.
In our proposed approach, contrastive Excitation Backprop
for RNNs (cEB-R), we address how to ground1 decisions
of deep recurrent networks in space and time simultane-
ously, using top-down saliency. Our approach models the
top-down attention mechanism of deep models to produce
interpretable and useful task-relevant saliency maps. Our
saliency maps are obtained implicitly without the need to
re-train models, unlike models that include explicit atten-
tion layers [26, 27]. Our method does not require a model
trained using explicit spatial (region/bounding box) or tem-
poral (frame) supervision.

Fig. 1 gives an overview of our approach that produces
saliency maps which enable us to visualize where and when
an action/caption is occurring in a video. Given a trained
model, we perform the standard forward pass. In the back-
ward pass, we use cEB-R to compute and propagate win-
ning neuron probabilities normalized over space and time.
This process yields spatiotemporal attention maps. Our
demo code is publicly available 2.

We evaluate our approach on two models from the lit-
erature: a CNN-LSTM trained for video action recogni-
tion, and a CNN-LSTM-LSTM (encoder-decoder) trained
In addition, we show how the spa-
for video captioning.
tiotemporal saliency maps produced for these two models
can be utilized for localization of segments within a video
that correspond to speciﬁed activity classes or noun phrases.

In summary, our contributions are:

• We are the ﬁrst to formulate top-down saliency in deep
recurrent models for space-time grounding of videos.

• We do so using a single contrastive Excitation Backprop

pass of an already trained model.

• Although we are not directly optimizing for localization
(no training is performed on spatial or temporal anno-
tations), we show that the internal representation of the
model can be utilized to perform localization.

2. Related Work

Several works in the literature give more insight into
the evidence behind deep
i.e.,
CNN model predictions,
model predictions. Such approaches are mainly devised
for image understanding and can identify the importance of
class-speciﬁc image regions by means of saliency maps in
a weakly-supervised way.

Spatial Grounding. Ribeiro et al. [?] explained classiﬁ-
cation predictions with applications on images. Fong et al.
[?] addressed spatial grounding in images by exhaustively

1In this work we use the terms ground and localize interchangeably.
2https://github.com/sbargal/Caffe-ExcitationBP-RNNs

perturbing image regions. Guided Backpropagation [21]
and Deconvolution [30, 18] used different variants of the
standard backpropagation error and visualized salient parts
at the image pixel level. In particular, starting from a high-
level feature map, [30] inverted the data ﬂow inside a CNN,
from neuron activations in higher layers down to the im-
age level. Guided Backpropagation [21] introduced an addi-
tional guidance signal to standard backpropagation prevent-
ing backward ﬂow of negative gradients. Simonyan et al.
[18] directly computed the gradient of the class score with
respect to the image pixel to ﬁnd the spatial cues that help
the class predictions in a CNN. CAM [34] removed the last
fully connected layer of a CNN and exploited a weighted
sum of the last convolutional feature maps to obtain the
class activation maps. Zhang et al. [31] generated class
activation maps from any CNN architecture that uses non-
linearities producing non-negative activations. Oquab et al.
[11] used mid-level CNN outputs on overlapping patches,
requiring multiple passes through the network.

Spatiotemporal Grounding. Weakly-supervised visual
saliency is much less explored for temporal architectures.
Karpathy et al. [7] visualized interpretable LSTM cells that
keep track of long-range dependencies such as line lengths,
quotes, and brackets in a character-based model. Li et al.
[?] visualized a unit’s salience for NLP. Selvaraju et al. [15]
qualitatively present grounding for captioning and visual
question answering in images using an RNN. Ramanishka
et al. [13] explored visual saliency guided by captions in an
encoder-decoder model. In contrast, our approach models
the top-down attention mechanism of CNN-RNN models to
produce interpretable and useful task-relevant spatiotempo-
ral saliency maps that can be used for action/caption local-
ization in videos.

3. Background: Excitation Backprop

In this section, a brief background on Excitation Back-
prop (EB) [31] is given. EB was proposed for CNNs in that
work. In general, the forward activation of neuron aj in a
CNN is computed by (cid:98)aj = φ((cid:80)
i wij(cid:98)ai + bi), where (cid:98)ai is
the activation coming from a lower layer, φ is a nonlinear
activation function, wij is the weight from neuron i to neu-
ron j, and bi is the added bias at layer i. The EB framework
makes two key assumptions about the activation (cid:98)aj which
are satisﬁed in the majority of modern CNNs due to wide
usage of the ReLU non-linearity: A1. (cid:98)aj is non-negative,
and A2. (cid:98)aj is a response that is positively correlated with
its conﬁdence of the detection of speciﬁc visual features.

EB realized a probabilistic Winner-Take-All formulation
to efﬁciently compute the probability of each neuron re-
cursively using conditional winning probabilities P (ai|aj),
normalized (cid:98)aiwij (Fig. 2). The top-down signal is a prior
distribution over the output units. EB passes top-down
signals through excitatory connections having non-negative

2

(cid:40)

(cid:40)

P t(ai|aj) =

iwij,

Zj(cid:98)at
0,

if wij ≥ 0,
otherwise.

t

P

(ai|aj) =

iwij,

Zj(cid:98)at
0,

if wij ≥ 0,
otherwise.

i:wij≥0

Zj = 1/ (cid:80)
ˆat
iwij is a normalization factor such that
the sum of all conditional probabilities of the children of
aj (Eqn.s 1, 2) sum to 1; wij ∈ W where W is the set of
model weights and wij is the weight between child neuron
ai and parent neuron aj; wij ∈ W where W is obtained by
negating the model weights at the classiﬁcation layer only.
P

(ai|aj) is only needed for contrastive attention.
We compute the neuron winning probabilities starting
from the prior distribution encoding a given action/caption
as follows:

t

P t(ai) =

P t(ai|aj)P t(aj)

(cid:88)

aj ∈Pi

aj ∈Pi

t

P

(ai) =

(cid:88)

t

P

(ai|aj)P

(aj)

t

where Pi is the set of parent neurons of ai.

Temporal Normalization.

Replacing tanh non-
linearities with ReLU non-linearities to extend EB in time
does not sufﬁce for temporal saliency. EB performs normal-
ization at every layer to maintain a probability distribution.
Hence, for spatiotemporal localization, signals from the de-
sired nth time-step of a T -frame clip should be normal-
ized in both time and space (assuming S neurons in current
layer) before being further backpropagated into the CNN:

N (ai) = P t(ai)/ (cid:80)T
P t

t=1

(cid:80)S

i=1 P t(ai).

(5)

t

t

P

t=1

(6)

(cid:80)S

(ai).

i=1 P

t
N (ai) = P

(ai)/ (cid:80)T
cEB-R computes the difference between the normalized
saliency maps obtained by EB-R starting from O, and EB-R
starting from O using negated weights of the classiﬁcation
layer. cEB-R is more discriminative as it grounds the evi-
dence that is unique to a selected class/word. For example,
cEB-R of Surﬁng will give evidence that is unique to Surf-
ing and not common to other classes used at training time
(see Fig. 5 for an example). This is conducted as follows:

M apt(ai) = P t

N (ai) − P

t
N (ai).

CNN Backward. For every video frame ft at time step

t, we use the backprop of [31] for all CNN layers:

P t(ai|aj) =

(cid:40)

iwij,

Zj(cid:98)at
0,

if wij ≥ 0,
otherwise

M apt(ai) =

P t(ai|aj)M apt(aj)

(cid:88)

aj ∈Pi

(1)

(2)

(3)

(4)

(7)

(8)

(9)

Figure 2:
In Excitation Backprop, excitation probabilities are
propagated in a single backward pass in the CNN. A top-down
signal is a probability distribution over the output units. The prob-
abilities are backpropagated from every parent node to its children
through its excitatory connections. The ﬁgure illustrates the con-
tributions of a single parent neuron to the excitation probabilities
computed at the next layer. Each P (ai) in the saliency map is
computed over the complete parent set Pi. Shading of nodes in
the ﬁgure conveys P (ai) (darker shade = greater P (ai)).

weights, excluding from the competition inhibitory ones.
Recursively propagating the top-down signal and preserv-
ing the sum of backpropagated probabilities layer by layer,
it is possible to compute task-speciﬁc saliency maps from
any intermediate layer in a single backward pass.

To improve the discriminativeness of the saliency maps,
[31] introduced contrastive EB (cEB) which cancels out
common winner neurons and ampliﬁes the class discrimi-
native neurons. To do this, given an output unit oi ∈ O, a
dual unit oi ∈ O is virtually generated, whose input weights
are the negation of those of oi. By subtracting the saliency
map for oi from the one for oi the result better highlights
cues in the image that are unique to the desired class.

4. Our Framework

In this section we explain the details of our spatiotempo-
ral grounding framework: cEB-R. As illustrated in Fig. 1,
we have three main modules: RNN Backward, Temporal
normalization, and CNN Backward.

RNN Backward. This module implements an excitation
backprop formulation for RNNs. Recurrent models such as
LSTMs are well-suited for top-down temporal saliency as
they explicitly propagate information over time. The exten-
sion of EB for Recurrent Networks, EB-R, is not straightfor-
ward since EB must be implemented through the unrolled
time steps of the RNN and since the original RNN formula-
tion contains tanh non-linearities which do not satisfy the
EB assumptions A1 and A2. [3, 5] have conducted an anal-
ysis over variations of the standard RNN formulation, and
discovered that different non-linearities performed similarly
for a variety of tasks. This is also reﬂected in our experi-
ments. Based on this, we use ReLU nonlinearities and cor-
responding derivatives, instead of tanh. This satisﬁes A1
and A2, and gives similar performance on both tasks.

Working backwards from the RNN’s output layer, we
compute the conditional winning probabilities from the set
of output nodes O, and the set of dual output nodes O:

3

where (cid:98)at
i is the activation when frame ft is passed through
the CNN. M apt at the desired CNN layer is the cEB-R
saliency map for ft. Computationally, the complexity of
cEB-R is on the order of a single backward pass. Note that
N (aj) is used instead of M apt(aj) in Eqn. 9.
for EB-R, P t
The general framework applied to both video action recog-
nition and captioning is summarized in Algorithm 1. Details
of each task are discussed in the following two sections.

4.1. Grounding: Video Action Recognition

In this task, we ground the evidence of a speciﬁc action
using a model trained on action recognition. The task takes
as input a video sequence and the action (A) to be local-
ized, and outputs spatiotemporal saliency maps for this ac-
tion in the video. We use the CNN-LSTM implementation
of [2] with VGG-16 [19] for our action grounding in video.
This encodes the temporal information intrinsically present
in the actions we want to localize. The CNN is truncated at
the fc7 layer such that the fc7 features of frames feed into
the recurrent unit. We use a single LSTM layer.

Performing cEB-R results in a sequence of saliency maps
M apt for t = 1, ..., T at conv5 (various layers perform sim-
ilarly [31]). These maps are then used to perform the tem-
poral grounding for action A. Localizing the action, entails
the following sequence of steps. First, the sum of every
saliency map is computed to give a vector S ∈ RT . Second,
we ﬁnd an anchor map with the highest sum. Third, we ex-
tend a window around the anchor map in both directions in
a greedy manner until a saliency map with a negative sum is
found. A negative sum indicates that the map is less relevant
to the action A under consideration. This allows us to de-
termine the start and end points of the temporal grounding,
sA and eA respectively. Fig. 3 depicts the cEB-R pipeline
for the task of action grounding.

4.2. Grounding: Video Captioning

In this task, we ground evidence of word(s) using a
model trained on video captioning. The task takes as input
a video and word(s) to be localized, and outputs spatiotem-
poral saliency maps corresponding to the query word(s).
We use the captioning model of [22] to test our cEB-R ap-
proach. This model consists of a VGG-16, followed by a
mean pooling of the VGG fc7 features, followed by a two-
layer LSTM. Fig. 4 depicts cEB-R for caption grounding.

We backpropagate an indicator vector for the words
to be visualized starting at the time-steps they were pre-
dicted, through time, to the average pooling layer. We then
distribute and backpropagate probabilities among frames -
according to their forward activations (Eqn. 8)- through the
VGG until the conv5 layer where we obtain the correspond-
ing saliency map. Performing cEB-R results in a sequence
of saliency maps M apt for t = 1, ..., T grounding the
words in the video frames. Temporal localization is per-
formed using the steps described in Sec. 4.1.

Figure 3: Grounding Action Recognition. The red arrows de-
pict cEB-R for spatiotemporal grounding of the action CliffDiving.
Starting from the last LSTM time-step, cEB-R backpropagates the
probability distribution through time and through the CNN at ev-
ery time-step. The saliency map for each time-step is used for the
spatial localization. The sum of each saliency map, over time, is
then used for temporal localization of the action within the video,
as described in Sec. 4.1.

Algorithm 1: cEB-R

Input: T -frame video clip, pre-trained CNN-LSTM

model, A: action or word to be localized in the
video.

Output: Spatial saliency maps of A : M apt for

t = 1, ..., T .

Procedure:

1 Set a one-hot vector according to the desired action

class or caption word A at the desired nth time-step;

2 Backprop the indicator vector through time and down
to the fc CNN layer using EB-R obtaining a saliency
map M apt at every time step t;

3 Normalize the resulting frame-wise saliency maps

over time such that (cid:80)T

t=1 M apt = 1;

4 Repeat the above steps, with negated weights at the
top layer to get a second set of T saliency maps;

5 Contrastive Operation: Subtract the resulting maps at
the fc CNN layer to yield cEB for each time step;

6 Continue EB through the CNN to the desired conv

layer to obtain the spatial grounding;

7 The sum of each spatial saliency map over time can be

used to perform temporal grounding for A;

4

5. Experiments: Action Grounding

In this work we ground the decisions made by our deep
models. In order to evaluate this grounding, we compare
it with methods that localize actions. Although our frame-
work is able to jointly localize actions in space and time, we
report results for spatial localization and temporal localiza-
tion separately due to the lack of an action dataset that has
untrimmed videos with spatiotemporal bounding boxes.

5.1. Spatial Localization

In this section we evaluate how well we ground actions in
space. We do this by comparing our grounding results with
ground-truth bounding boxes localizing actions per-frame.
Dataset. THUMOS14 [4] provides per-frame bound-
ing box annotations of humans performing actions for
3207 videos of 24 classes from the UCF101 dataset [20].
UCF101 is a trimmed video dataset containing 13320 ac-
tions belonging to 101 action classes.

Baselines. We compare our formulation against spatial
top-down saliency using a CNN (treating every video frame
as an independent image). We also compare against stan-
dard backpropagation (BP), and BP for RNNs (BP-R).

Models. We use the following CNN model: VGG-16 of
Ma et al. [9] trained on UCF101 video frames and BU101
web images for action recognition with a test accuracy of
83.5%. We use the following CNN-LSTM model: the same
VGG-16 ﬁne-tuned with a one-layer LSTM on UCF101 for
action recognition with a test accuracy of 83.3%.

Setup and Results. We use the bounding box annota-
tions to evaluate our spatial grounding using the pointing
game introduced by Zhang et al. [31]. We locate the point
having maximum value on each top-down saliency map.
Following [31], if a 15-pixel diameter circle around the lo-
cated point intersects the ground-truth bounding-box of the
action category for a frame, we record a hit, otherwise we
record a miss. We measure the spatial action localization
accuracy by Acc = #Hits/(#Hits + #M isses) over all
the annotated frames for each action.

Table 1 reports the results of the spatial pointing game.
Extending top-down saliency in time (-R) consistently im-
proves the accuracy for all three methods, compared to per-
forming top-down saliency separately on every frame of the
video using a CNN. EB-R has the greatest absolute im-
provement of 5.7%.

We note that the non-contrastive versions outperform
their contrastive counterparts. This is because they high-
light discriminative evidence for actions, which may not
necessarily be the humans performing the actions. For ex-
ample, for many actions in UCF101, the human may be in a
standing position, in which case cEB-R will highlight cues
that are discriminative and unique to this action rather than
highlighting the human. These cues may belong to the con-
text in which the activity is performed, or the action classes

Figure 4: Grounding Captioning. The red arrows depict cEB-R
for spatiotemporal caption grounding. The video caption produced
by the model is A man is singing on a stage. Starting from the
time-step corresponding to the word singing, cEB-R backprops
the probability distribution through the previous time-steps and
through the CNN. The saliency map for each time step is used
for spatial localization. The sum of each saliency map, over time,
is then used for temporal localization of the word within the clip.

EB
55.8

EB-R
61.5

Method Acc (%)
cEB-R
39.1

cEB
37.0

BP
37.3

BP-R
39.2

Table 1: Accuracy of the spatial pointing game conducted on
∼3K videos of UCF101 for spatially locating humans perform-
ing actions in videos. The results show that extending top-down
saliency in time (-R) improves the accuracy compared to perform-
ing top-down saliency separately on every frame of the video using
a CNN. The non-contrastive versions work better for reasons de-
scribed in the text.

on which the model was trained. We demonstrate this in
Fig. 5 for the actions Surﬁng and BasketballDunk.

5.2. Temporal Localization

In this section we evaluate how well we ground actions
in time. We do this by comparing our grounding results
with ground-truth action boundaries.

Datasets. We ﬁrst use a simple and controlled setting
to validate our method by creating a synthetic action detec-
tion dataset. We then present results on the THUMOS14
[4] action detection dataset. The synthetic dataset is cre-
ated by concatenating two UCF101 videos uniformly sam-
pled: a ground truth (GT) video, and a random (rand) back-

5

(a) Grounding Surﬁng using EB-R (L) and cEB-R (R)

(a) Grounding of the action TableTennisShot in the video

(b) Grounding BasketballDunk using EB-R (L) and cEB-R (R)

(b) Grounding of the action Skiing in the video

Figure 5: The saliency maps produced by EB-R (left) and cEB-R
(right) together with the THUMOS14 groundtruth bounding box
over the same frame of the actions (a) Surﬁng and (b) Basketball-
Dunk.
In both cases, EB-R highlights the most salient regions
of the frame for this action (human), which matches the bound-
ing box annotation. However, cEB-R highlights the region that is
unique to the ground truth action: the waves for Surﬁng, and the
hoop for BasketballDunk. This is because highlighting the human
region does not provide insightful information to the classiﬁer.

ground video, such that class(GT) (cid:54)= class(rand). The two
actions are concatenated, ﬁrst sequentially (rand + GT or
GT + rand) in 16-frame clips, then inserted at a random
position (rand + GT + rand) in 128-frame clips. We use
all 3783 test videos provided in UCF101, each in com-
bination with a different random background video. The
THUMOS14 dataset consists of 1010 untrimmed validation
videos and 1574 untrimmed test videos of 20 action classes.
Among test videos, we evaluate our framework on the 213
test videos which contain annotations as in [24, 16].

Baselines. For the synthetic experiment, we compare
cEB-R and EB-R with a probability-based approach where
we threshold the predicted probability (to 1 if ≥ 0.5, to −1
if < 0.5) of the GT class at every time-step. For the detec-
tion experiment in THUMOS14 we compare our proposed
method with state-of-the-art approaches.

Models. For the synthetic dataset, we use the same
CNN-LSTM model used for spatial action grounding
(Sec. 5.1). For the THUMOS14 dataset we use a CNN-
the same VGG-16 model used for spatial
LSTM model:
action grounding (Sec. 5.1) ﬁne-tuned with a one-layer
LSTM on UCF101 and trimmed sequences from THU-
MOS14 background and validation sets.

Setup and Results: Synthetic Data. First, we per-
form experiments on the synthetic videos composed of two
sequential actions, where the boundary is the midpoint.
Fig. 6 presents a sample spatiotemporal localization. The
heatmaps produced by cEB-R correctly ground the queried
action. More examples are presented in the supplementary
material. While Fig. 6 presents a qualitative sample, Fig. 7
quantitatively presents results on the entire test set. The ac-

Figure 6: Applying contrastive Excitation Backprop for Recur-
rent Networks (cEB-R) to produce spatiotemporal localization of
actions in sample frames of a video. Demonstrated here is (a)
cEB-R spatiotemporal localization of TableTennisShot in a video
(b) cEB-R spatiotemporal localization of Skiing in the same video.
The video consists of two consecutive actions that are synthetically
concatenated: Skiing followed by TableTennisShot.

tion switches from GT to rand or vice versa midway. It can
be seen that the sum of saliency maps is: positive and in-
creasing as more of the GT action is observed, negative and
decreasing as more of the rand action is observed.

Next, we perform experiments where we vary the length
of the GT action that we want to localize inside a clip. To re-
tain action dynamics, we sample GT and rand from the en-
tire length of their corresponding videos. Table 2 presents
the temporal localization results of our synthetic data. In
the experimental setup with ﬁxed action length we assume
that we know the label and length of the action to be lo-
calized. To localize, we ﬁnd the highest consecutive sum
of attention maps for the desired action length. Regard-
ing the sequences with unknown action lengths, we only
assume the label of the action to be localized and perform
the pipeline described in Sec. 4.1.
In the bottom half of
Table 2 we only report thresholded probabilities and cEB-
R results since our localization procedure assumes negative
values at action boundaries, whereas EB-R is non-negative.
The grounded evidence obtained by cEB-R attains the high-
est detection scores, 73.5% and 62.0%, for action sequences
of known and unknown lengths, respectively, for IoU over-
lap between detections and ground-truth of α = 0.5, despite
the fact that the model is not trained for localization.

Setup and Results: THUMOS14 Pointing Game. We
evaluate the pointing game in time for THUMOS14 -a fair
evaluation for methods that do not optimize for detection.
For processing, we divide a video into 128-frame consecu-
tive clips. We perform the pointing game by pointing [31]
in time to the peak sum of saliency maps. For each ground-
truth annotation we check if the detected peak is within its
boundaries. If yes, we count it as a hit, otherwise, as a miss.
We compare this approach with the peak position of pre-

6

dicted probabilities, and a random point in that clip.

The results of this experiment are presented in Table 3.
Pointing to a random position clearly obtains lowest results
while peak probability (65.8%) and cEB-R (65.1%) have
similar performance. However, peak probability does not
offer spatial localization. Peak probability uses the model
prediction, while cEB-R uses the evidence of that predic-
tion. Moreover, we observe that peak probability and cEB-
R are complementary, yielding 77.4%.

Setup and Results: THUMOS14 Action Detection.
We evaluate how well our grounding does on the more chal-
lenging task of action detection that it was not trained for.
In this experiment, we divide a video into 128-frame con-
secutive clips for processing. Table 4 presents the tempo-
ral detection results of the THUMOS14 dataset. Differently
from the pointing game experiment, we detect the start and
end of the ground-truth action. We note that although our
method is not supervised for the detection task, we achieve
an accuracy of 57.9% when locating a ground truth class
with an overlap α = 0.1 as demonstrated in Table 4.

6. Experiments: Caption Grounding

In this section, we show how cEB-R is also applica-
ble in the context of caption grounding. As observed by
[13], there is an absence of datasets with spatiotemporal an-
notations of frames for captions. Therefore, they propose
the following experimental setup which we follow: qualita-
tive results for the spatiotemporal grounding on videos, and
quantitative results for spatial grounding on images.

Datasets. We use the MSR-VTT [25] dataset for video
captioning and Flickr30kEntities [12] for image captioning.
Models. We use the CNN-LSTM-LSTM video caption-
ing model of [22] trained on MSR-VTT to test our cEB-
R approach for spatiotemporal grounding as described in
Sec. 4.2. We use the same video captioning model, with-
out the average pooling layer, trained on Flickr30kEntities
for image captioning. The models have comparable ME-
TEOR scores to the Caption-Guided Saliency work of [13],
to which we compare our results: 26.5 (vs. 25.9) for video
captioning and 18.0 (vs. 18.3) for image captioning.

Setup and Results. For the MSR-VTT video dataset,
we sample 26 frames per video following [13] and perform
grounding of nouns. Fig. 8 presents the grounding for the
word man and phone in the same video. The man is well lo-
calized only in frames where a man appears, and the phone
is well localized in frames where a phone appears.

We quantitatively evaluate our results of spatial ground-
ing using the pointing game on the Flickr30kEntities and
compare our method to the Caption-Guided Saliency work
of [13], following their evaluation protocol. We use ground
truth captions as an input to our model in order to reproduce
the same captions. Then, we use bounding box annotations
for each noun phrase in the ground truth captions and check

Figure 7: Sum of the saliency maps at fc7 over time, in frames,
for synthetic videos that (blue) have a rand action followed by a
GT action and (green) have a GT followed by a rand action. The
average and standard deviation are reported over all test videos.
cEB-R provides an accurate midway boundary between actions.

Length
(frames)

Probability
(%)

EB-R
(%)

cEB-R
(%)

h

t
g
n
e
L
n
o
i
t
c
A

n
w
o
n
K

n
w
o
n
k
n
U

11
41
65

11
41
65

8.5
28.2
47.7

3.4
9.5
35.7

11.3
38.5
56.3

-
-
-

15.5
53.2
73.5

4.1
47.9
62.0

Table 2: Action detection results on synthetic data, measured by
mAP at IoU threshold α = 0.5. Top part of table: methods assume
that the length and label of the action to be detected are known.
Bottom part of table: methods assume that the label is known, but
the length is unknown. cEB-R attains best performance.

Method
Random
Peak probability
cEB-R
Peak probability + cEB-R

Accuracy (%)
57.3
65.8
65.1
77.4

Table 3: Pointing game in time performed on the THUMOS14
test set. The probability of an action together with the evidence
for presence of the action are complementary and give a great im-
provement in accuracy when combined.

whether the maximum point in a saliency map is inside the
annotated bounding box.

Table 5 shows the results of the spatial pointing game
on Flickr30kEntities. Our approach achieves comparable
performance to [13].
In this experiment, we ground the
ground truth captions to match the experimental setup in
[13]. Although we follow their protocol for fair compari-
son, we note that our method can better highlight evidence
using generated captions (vs. ground truth captions). This
is because the evidence of a ground truth noun that is not

7

Method
Karaman et al. [6]
Wang et al. [23]
Oneata et al. [10]
Richard et al. [14]
Shou et al. [17]
Yeung et al. [28]
Yuan et al. [29]
Xu et al. [24]
Zhao et al. [32]
Kaufman et al. [8]
Ours

mAP (α = 0.1)
4.6
18.2
36.6
39.7
47.7
48.9
51.4
54.5
60.3
61.1
57.9

Table 4: Our approach vs. fully supervised approaches for action
detection on THUMOS14, measured by mAP at IoU threshold α =
0.1. Although our model is not trained for action detection (trained
for recognition), we achieve 57.9%, which is comparable to state-
of-the-art when localizing a ground truth action in a video.

Method

Baseline random
Baseline center
Caption-Guided Saliency [13]
Ours

Avg (Noun Phrases)
0.268
0.492
0.501
0.512

Table 5: Evaluation of spatial saliency on Flickr30kEntities using
cEB-R. Baseline random samples the maximum point uniformly
and Baseline center always picks the center.

predicted may not be sufﬁciently activated in the forward
pass. Fig. 9 presents some visual examples of grounding in
images using the generated captions.

Our approach has a computational advantage over [13].
In order to obtain spatial saliency maps for a word in a
video, c-EB-R requires one forward pass and one back-
ward pass through the CNN-LSTM-LSTM, while [13] re-
quires one forward pass through the CNN part, but m for-
ward passes through the LSTM-LSTM part, where m = 64
is the area of the saliency map (vs. our single backward
pass). Moreover, they require f forward LSTM passes,
where f = 26 is the number of frames, to compute the tem-
poral grounding, whereas ours is implicitly spatiotemporal.

7. Conclusion

In conclusion, we devise a temporal formulation, cEB-R,
that enables us to visualize how recurrent networks ground
their decisions in visual content. We apply this to two video
understanding tasks: video action recognition, and video
captioning. We demonstrate how spatiotemporal top-down
saliency is capable of grounding evidence on several action
and captioning datasets. These datasets provide annotations
for detection and/or localization, to which we have com-
pared the evidence in our generated saliency maps. We
observe the strengths of cEB-R in highlighting discrimina-

(a) grounding of the word man

(b) grounding of the word phone

Figure 8: Comparison of grounding of words man and phone in
the caption A man is talking about a phone of a video from MSR-
VTT using cEB-R. The man is well localized in (a) and the phone
is well localized in (b), as desired.

(a) image caption: A man in a lab coat is working on a microscope.

(b) image caption: A cowboy is riding a bucking horse.

Figure 9: Grounding different words of a caption using cEB-R
for two images from the Flicker30kEntities dataset.

tive evidence, which was particularly beneﬁcial for tempo-
ral grounding. We also observe the strengths of its variant,
EB-R, in highlighting salient evidence, which was particu-
larly beneﬁcial for spatial localization of action subjects.

Acknowledgments

We thank Kate Saenko and Vasili Ramanishka for help-
ful discussions. This work was supported in part by NSF
grants 1551572 and 1029430, an IBM PhD Fellowship,
gifts from Adobe and NVidia, and Intelligence Advanced
Research Projects Activity (IARPA) via Department of In-
terior/ Interior Business Center (DOI/IBC) contract number
D17PC00341. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon. Dis-
claimer: The views and conclusions contained herein are
those of the authors and should not be interpreted as neces-
sarily representing the ofﬁcial policies or endorsements, ei-
ther expressed or implied, of IARPA, DOI/IBC, or the U.S.
Government.

8

References

[1] C. Cao, X. Liu, Y. Yang, Y. Yu, J. Wang, Z. Wang, Y. Huang,
L. Wang, C. Huang, W. Xu, et al. Look and think twice: Cap-
turing top-down visual attention with feedback convolutional
neural networks. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2956–2964, 2015. 1
S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
In Proceedings of the IEEE
recognition and description.
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 2625–2634, 2015. 4

[2] J. Donahue, L. Anne Hendricks,

[3] K. Greff, R. K. Srivastava, J. Koutn´ık, B. R. Steunebrink,
and J. Schmidhuber. LSTM: A search space odyssey. IEEE
transactions on neural networks and learning systems, 2016.
3

[4] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014. 5, 6

[5] R. Jozefowicz, W. Zaremba, and I. Sutskever. An empiri-
cal exploration of recurrent network architectures. In Pro-
ceedings of the 32nd International Conference on Machine
Learning (ICML), pages 2342–2350, 2015. 3

[6] S. Karaman, L. Seidenari, and A. Del Bimbo. Fast saliency
based pooling of Fisher encoded dense trajectories. In ECCV
THUMOS Workshop, volume 1, page 5, 2014. 8

[7] A. Karpathy, J. Johnson, and L. Fei-Fei. Visualizing and
understanding recurrent networks. In ICLR Workshop, 2016.
1, 2

[8] D. Kaufman, G. Levi, T. Hassner, and L. Wolf. Temporal
tessellation: A uniﬁed approach for video analysis. In The
IEEE International Conference on Computer Vision (ICCV),
2017. 8

[9] S. Ma, S. A. Bargal, J. Zhang, L. Sigal, and S. Sclaroff. Do
less and achieve more: Training cnns for action recognition
utilizing action images from the web. Pattern Recognition,
2017. 5

[10] D. Oneata, J. Verbeek, and C. Schmid. The lear submission

at thumos 2014. 2014. 8

[11] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Is object local-
ization for free? - weakly-supervised learning with convo-
lutional neural networks. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2015. 2
[12] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
In Proceedings of the IEEE interna-
to-sentence models.
tional conference on computer vision (ICCV), pages 2641–
2649, 2015. 7

[13] V. Ramanishka, A. Das, J. Zhang, and K. Saenko. Top-down
In Proceedings of the
visual saliency guided by captions.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 1, 2, 7, 8

[14] A. Richard and J. Gall. Temporal action detection using a
statistical language model. In Proceedings of the IEEE Con-

ference on Computer Vision and Pattern Recognition, pages
3131–3140, 2016. 8

[15] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,
D. Parikh, and D. Batra. Grad-cam: Visual explanations
from deep networks via gradient-based localization. See
https://arxiv. org/abs/1610.02391 v3, 2016. 1, 2

[16] Z. Shou, J. Chan, A. Zareian, K. Miyazawa, and S.-F. Chang.
Cdc: Convolutional-de-convolutional networks for precise
temporal action localization in untrimmed videos. arXiv
preprint arXiv:1703.01515, 2017. 6

[17] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-
In Pro-
ization in untrimmed videos via multi-stage cnns.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 1049–1058, 2016. 8
[18] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside
convolutional networks: Visualising image classiﬁcation
models and saliency maps. arXiv preprint arXiv:1312.6034,
2013. 1, 2

[19] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 4

[20] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012. 5

[21] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Ried-
miller. Striving for simplicity: The all convolutional net.
CoRR, abs/1412.6806, 2014. 1, 2

[22] S. Venugopalan, H. Xu,

J. Donahue, M. Rohrbach,
R. Mooney, and K. Saenko. Translating videos to natural
language using deep recurrent neural networks. North Amer-
ican Chapter of the Association for Computational Linguis-
tics Human Language Technologies NAACL-HLT, 2015. 4,
7

[23] L. Wang, Y. Qiao, and X. Tang. Action recognition and de-
tection by combining motion and appearance features. THU-
MOS14 Action Recognition Challenge, 1(2):2, 2014. 8
[24] H. Xu, A. Das, and K. Saenko. R-c3d: Region convolutional
3d network for temporal activity detection. The IEEE Inter-
national Conference on Computer Vision (ICCV), 2017. 6,
8

[25] J. Xu, T. Mei, T. Yao, and Y. Rui. Msr-vtt: A large video
description dataset for bridging video and language. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 5288–5296, 2016. 7
[26] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudi-
nov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural
image caption generation with visual attention. In Interna-
tional Conference on Machine Learning, pages 2048–2057,
2015. 2

[27] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle,
and A. Courville. Describing videos by exploiting temporal
structure. In Proceedings of the IEEE international confer-
ence on computer vision (ICCV), pages 4507–4515, 2015.
2

[28] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In Proceedings of the IEEE Conference on Computer

9

Vision and Pattern Recognition (CVPR), pages 2678–2687,
2016. 8

[29] J. Yuan, B. Ni, X. Yang, and A. A. Kassim. Temporal ac-
tion localization with pyramid of score distribution features.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 3093–3102, 2016. 8
[30] M. D. Zeiler and R. Fergus. Visualizing and understanding
In European conference on com-

convolutional networks.
puter vision (ECCV), pages 818–833. Springer, 2014. 1, 2

[31] J. Zhang, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff. Top-
down neural attention by excitation backprop. In European
Conference on Computer Vision (ECCV), pages 543–559.
Springer, 2016. 1, 2, 3, 4, 5, 7

[32] Y. Zhao, Y. Xiong, L. Wang, Z. Wu, X. Tang, and D. Lin.
Temporal action detection with structured segment networks.
In The IEEE International Conference on Computer Vision
(ICCV), Oct 2017. 8

[33] B. Zhou, A. Khosla, L. A., A. Oliva, and A. Torralba. Learn-
ing Deep Features for Discriminative Localization. The
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2016.

[34] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-
ralba. Learning deep features for discriminative localization.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2921–2929, 2016.
1, 2

10

8
1
0
2
 
r
a

M
 
8
 
 
]

V
C
.
s
c
[
 
 
3
v
8
7
7
6
0
.
1
1
7
1
:
v
i
X
r
a

Excitation Backprop for RNNs

Sarah Adel Bargal∗1, Andrea Zunino∗ 2, Donghyun Kim1, Jianming Zhang3,
Vittorio Murino2,4, Stan Sclaroff1

1Department of Computer Science, Boston University 2Pattern Analysis & Computer Vision (PAVIS),
Istituto Italiano di Tecnologia 3Adobe Research 4Computer Science Department, Universit`a di Verona
{sbargal,donhk,sclaroff}@bu.edu, {andrea.zunino,vittorio.murino}@iit.it, jianmzha@adobe.com

Figure 1: Our proposed framework spatiotemporally highlights/grounds the evidence that an RNN model used in producing a class label
or caption for a given input video. In this example, by using our proposed back-propagation method, the evidence for the activity class
CliffDiving is highlighted in a video that contains CliffDiving and HorseRiding. Our model employs a single backward pass to produce
saliency maps that highlight the evidence that a given RNN used in generating its outputs.

Abstract

Deep models are state-of-the-art for many vision tasks
including video action recognition and video captioning.
Models are trained to caption or classify activity in videos,
but little is known about the evidence used to make such de-
cisions. Grounding decisions made by deep networks has
been studied in spatial visual content, giving more insight
into model predictions for images. However, such studies
are relatively lacking for models of spatiotemporal visual
content – videos. In this work, we devise a formulation that
simultaneously grounds evidence in space and time, in a
single pass, using top-down saliency. We visualize the spa-
tiotemporal cues that contribute to a deep model’s classiﬁ-
cation/captioning output using the model’s internal repre-
sentation. Based on these spatiotemporal cues, we are able
to localize segments within a video that correspond with a
speciﬁc action, or phrase from a caption, without explicitly
optimizing/training for these tasks.

1. Introduction

∗Equal contribution

To visualize what in a video gives rise to an output of a
deep recurrent network, it is important to consider space and

time saliency, i.e., where and when. The visualization of
what a deep recurrent network ﬁnds salient in an input video
can enable interpretation of the model’s behavior in action
classiﬁcation, video captioning, and other tasks. Moreover,
estimates of the model’s attention (e.g., saliency maps) can
be used directly in localizing a given action within a video
or in localizing the portions of a video that correspond to a
particular concept within a caption.

Several works address visualization of model attention in
Convolutional Neural Networks (CNNs) for image classiﬁ-
cation [1, 31, 21, 30, 18, 34, 15]. These methods produce
saliency maps that visualize the importance of class-speciﬁc
image regions (spatial localization). Analogous methods
for Recurrent Neural Network (RNN)-based models must
handle more complex recurrent, non-linear, spatiotempo-
ral dependencies; thus, progress on RNNs has been limited
to [7, 13]. Karpathy et al. [7] visualize the role of Long
Short Term Memory (LSTM) cells for text input, but not for
visual data. Ramanishka et al. [13] map words to regions
in the video captioning task by dropping out (exhaustively
or by sampling) video frames and/or parts of video frames
to obtain saliency maps. This can be computationally ex-
pensive, and does not consider temporal evolution but only
frame-level saliency.

1

In contrast, we propose the ﬁrst one-pass formulation
for visualizing spatiotemporal attention in RNNs, without
selectively dropping or sampling frames or frame regions.
In our proposed approach, contrastive Excitation Backprop
for RNNs (cEB-R), we address how to ground1 decisions
of deep recurrent networks in space and time simultane-
ously, using top-down saliency. Our approach models the
top-down attention mechanism of deep models to produce
interpretable and useful task-relevant saliency maps. Our
saliency maps are obtained implicitly without the need to
re-train models, unlike models that include explicit atten-
tion layers [26, 27]. Our method does not require a model
trained using explicit spatial (region/bounding box) or tem-
poral (frame) supervision.

Fig. 1 gives an overview of our approach that produces
saliency maps which enable us to visualize where and when
an action/caption is occurring in a video. Given a trained
model, we perform the standard forward pass. In the back-
ward pass, we use cEB-R to compute and propagate win-
ning neuron probabilities normalized over space and time.
This process yields spatiotemporal attention maps. Our
demo code is publicly available 2.

We evaluate our approach on two models from the lit-
erature: a CNN-LSTM trained for video action recogni-
tion, and a CNN-LSTM-LSTM (encoder-decoder) trained
In addition, we show how the spa-
for video captioning.
tiotemporal saliency maps produced for these two models
can be utilized for localization of segments within a video
that correspond to speciﬁed activity classes or noun phrases.

In summary, our contributions are:

• We are the ﬁrst to formulate top-down saliency in deep
recurrent models for space-time grounding of videos.

• We do so using a single contrastive Excitation Backprop

pass of an already trained model.

• Although we are not directly optimizing for localization
(no training is performed on spatial or temporal anno-
tations), we show that the internal representation of the
model can be utilized to perform localization.

2. Related Work

Several works in the literature give more insight into
the evidence behind deep
i.e.,
CNN model predictions,
model predictions. Such approaches are mainly devised
for image understanding and can identify the importance of
class-speciﬁc image regions by means of saliency maps in
a weakly-supervised way.

Spatial Grounding. Ribeiro et al. [?] explained classiﬁ-
cation predictions with applications on images. Fong et al.
[?] addressed spatial grounding in images by exhaustively

1In this work we use the terms ground and localize interchangeably.
2https://github.com/sbargal/Caffe-ExcitationBP-RNNs

perturbing image regions. Guided Backpropagation [21]
and Deconvolution [30, 18] used different variants of the
standard backpropagation error and visualized salient parts
at the image pixel level. In particular, starting from a high-
level feature map, [30] inverted the data ﬂow inside a CNN,
from neuron activations in higher layers down to the im-
age level. Guided Backpropagation [21] introduced an addi-
tional guidance signal to standard backpropagation prevent-
ing backward ﬂow of negative gradients. Simonyan et al.
[18] directly computed the gradient of the class score with
respect to the image pixel to ﬁnd the spatial cues that help
the class predictions in a CNN. CAM [34] removed the last
fully connected layer of a CNN and exploited a weighted
sum of the last convolutional feature maps to obtain the
class activation maps. Zhang et al. [31] generated class
activation maps from any CNN architecture that uses non-
linearities producing non-negative activations. Oquab et al.
[11] used mid-level CNN outputs on overlapping patches,
requiring multiple passes through the network.

Spatiotemporal Grounding. Weakly-supervised visual
saliency is much less explored for temporal architectures.
Karpathy et al. [7] visualized interpretable LSTM cells that
keep track of long-range dependencies such as line lengths,
quotes, and brackets in a character-based model. Li et al.
[?] visualized a unit’s salience for NLP. Selvaraju et al. [15]
qualitatively present grounding for captioning and visual
question answering in images using an RNN. Ramanishka
et al. [13] explored visual saliency guided by captions in an
encoder-decoder model. In contrast, our approach models
the top-down attention mechanism of CNN-RNN models to
produce interpretable and useful task-relevant spatiotempo-
ral saliency maps that can be used for action/caption local-
ization in videos.

3. Background: Excitation Backprop

In this section, a brief background on Excitation Back-
prop (EB) [31] is given. EB was proposed for CNNs in that
work. In general, the forward activation of neuron aj in a
CNN is computed by (cid:98)aj = φ((cid:80)
i wij(cid:98)ai + bi), where (cid:98)ai is
the activation coming from a lower layer, φ is a nonlinear
activation function, wij is the weight from neuron i to neu-
ron j, and bi is the added bias at layer i. The EB framework
makes two key assumptions about the activation (cid:98)aj which
are satisﬁed in the majority of modern CNNs due to wide
usage of the ReLU non-linearity: A1. (cid:98)aj is non-negative,
and A2. (cid:98)aj is a response that is positively correlated with
its conﬁdence of the detection of speciﬁc visual features.

EB realized a probabilistic Winner-Take-All formulation
to efﬁciently compute the probability of each neuron re-
cursively using conditional winning probabilities P (ai|aj),
normalized (cid:98)aiwij (Fig. 2). The top-down signal is a prior
distribution over the output units. EB passes top-down
signals through excitatory connections having non-negative

2

(cid:40)

(cid:40)

P t(ai|aj) =

iwij,

Zj(cid:98)at
0,

if wij ≥ 0,
otherwise.

t

P

(ai|aj) =

iwij,

Zj(cid:98)at
0,

if wij ≥ 0,
otherwise.

i:wij≥0

Zj = 1/ (cid:80)
ˆat
iwij is a normalization factor such that
the sum of all conditional probabilities of the children of
aj (Eqn.s 1, 2) sum to 1; wij ∈ W where W is the set of
model weights and wij is the weight between child neuron
ai and parent neuron aj; wij ∈ W where W is obtained by
negating the model weights at the classiﬁcation layer only.
P

(ai|aj) is only needed for contrastive attention.
We compute the neuron winning probabilities starting
from the prior distribution encoding a given action/caption
as follows:

t

P t(ai) =

P t(ai|aj)P t(aj)

(cid:88)

aj ∈Pi

aj ∈Pi

t

P

(ai) =

(cid:88)

t

P

(ai|aj)P

(aj)

t

where Pi is the set of parent neurons of ai.

Temporal Normalization.

Replacing tanh non-
linearities with ReLU non-linearities to extend EB in time
does not sufﬁce for temporal saliency. EB performs normal-
ization at every layer to maintain a probability distribution.
Hence, for spatiotemporal localization, signals from the de-
sired nth time-step of a T -frame clip should be normal-
ized in both time and space (assuming S neurons in current
layer) before being further backpropagated into the CNN:

N (ai) = P t(ai)/ (cid:80)T
P t

t=1

(cid:80)S

i=1 P t(ai).

(5)

t

t

P

t=1

(6)

(cid:80)S

(ai).

i=1 P

t
N (ai) = P

(ai)/ (cid:80)T
cEB-R computes the difference between the normalized
saliency maps obtained by EB-R starting from O, and EB-R
starting from O using negated weights of the classiﬁcation
layer. cEB-R is more discriminative as it grounds the evi-
dence that is unique to a selected class/word. For example,
cEB-R of Surﬁng will give evidence that is unique to Surf-
ing and not common to other classes used at training time
(see Fig. 5 for an example). This is conducted as follows:

M apt(ai) = P t

N (ai) − P

t
N (ai).

CNN Backward. For every video frame ft at time step

t, we use the backprop of [31] for all CNN layers:

P t(ai|aj) =

(cid:40)

iwij,

Zj(cid:98)at
0,

if wij ≥ 0,
otherwise

M apt(ai) =

P t(ai|aj)M apt(aj)

(cid:88)

aj ∈Pi

(1)

(2)

(3)

(4)

(7)

(8)

(9)

Figure 2:
In Excitation Backprop, excitation probabilities are
propagated in a single backward pass in the CNN. A top-down
signal is a probability distribution over the output units. The prob-
abilities are backpropagated from every parent node to its children
through its excitatory connections. The ﬁgure illustrates the con-
tributions of a single parent neuron to the excitation probabilities
computed at the next layer. Each P (ai) in the saliency map is
computed over the complete parent set Pi. Shading of nodes in
the ﬁgure conveys P (ai) (darker shade = greater P (ai)).

weights, excluding from the competition inhibitory ones.
Recursively propagating the top-down signal and preserv-
ing the sum of backpropagated probabilities layer by layer,
it is possible to compute task-speciﬁc saliency maps from
any intermediate layer in a single backward pass.

To improve the discriminativeness of the saliency maps,
[31] introduced contrastive EB (cEB) which cancels out
common winner neurons and ampliﬁes the class discrimi-
native neurons. To do this, given an output unit oi ∈ O, a
dual unit oi ∈ O is virtually generated, whose input weights
are the negation of those of oi. By subtracting the saliency
map for oi from the one for oi the result better highlights
cues in the image that are unique to the desired class.

4. Our Framework

In this section we explain the details of our spatiotempo-
ral grounding framework: cEB-R. As illustrated in Fig. 1,
we have three main modules: RNN Backward, Temporal
normalization, and CNN Backward.

RNN Backward. This module implements an excitation
backprop formulation for RNNs. Recurrent models such as
LSTMs are well-suited for top-down temporal saliency as
they explicitly propagate information over time. The exten-
sion of EB for Recurrent Networks, EB-R, is not straightfor-
ward since EB must be implemented through the unrolled
time steps of the RNN and since the original RNN formula-
tion contains tanh non-linearities which do not satisfy the
EB assumptions A1 and A2. [3, 5] have conducted an anal-
ysis over variations of the standard RNN formulation, and
discovered that different non-linearities performed similarly
for a variety of tasks. This is also reﬂected in our experi-
ments. Based on this, we use ReLU nonlinearities and cor-
responding derivatives, instead of tanh. This satisﬁes A1
and A2, and gives similar performance on both tasks.

Working backwards from the RNN’s output layer, we
compute the conditional winning probabilities from the set
of output nodes O, and the set of dual output nodes O:

3

where (cid:98)at
i is the activation when frame ft is passed through
the CNN. M apt at the desired CNN layer is the cEB-R
saliency map for ft. Computationally, the complexity of
cEB-R is on the order of a single backward pass. Note that
N (aj) is used instead of M apt(aj) in Eqn. 9.
for EB-R, P t
The general framework applied to both video action recog-
nition and captioning is summarized in Algorithm 1. Details
of each task are discussed in the following two sections.

4.1. Grounding: Video Action Recognition

In this task, we ground the evidence of a speciﬁc action
using a model trained on action recognition. The task takes
as input a video sequence and the action (A) to be local-
ized, and outputs spatiotemporal saliency maps for this ac-
tion in the video. We use the CNN-LSTM implementation
of [2] with VGG-16 [19] for our action grounding in video.
This encodes the temporal information intrinsically present
in the actions we want to localize. The CNN is truncated at
the fc7 layer such that the fc7 features of frames feed into
the recurrent unit. We use a single LSTM layer.

Performing cEB-R results in a sequence of saliency maps
M apt for t = 1, ..., T at conv5 (various layers perform sim-
ilarly [31]). These maps are then used to perform the tem-
poral grounding for action A. Localizing the action, entails
the following sequence of steps. First, the sum of every
saliency map is computed to give a vector S ∈ RT . Second,
we ﬁnd an anchor map with the highest sum. Third, we ex-
tend a window around the anchor map in both directions in
a greedy manner until a saliency map with a negative sum is
found. A negative sum indicates that the map is less relevant
to the action A under consideration. This allows us to de-
termine the start and end points of the temporal grounding,
sA and eA respectively. Fig. 3 depicts the cEB-R pipeline
for the task of action grounding.

4.2. Grounding: Video Captioning

In this task, we ground evidence of word(s) using a
model trained on video captioning. The task takes as input
a video and word(s) to be localized, and outputs spatiotem-
poral saliency maps corresponding to the query word(s).
We use the captioning model of [22] to test our cEB-R ap-
proach. This model consists of a VGG-16, followed by a
mean pooling of the VGG fc7 features, followed by a two-
layer LSTM. Fig. 4 depicts cEB-R for caption grounding.

We backpropagate an indicator vector for the words
to be visualized starting at the time-steps they were pre-
dicted, through time, to the average pooling layer. We then
distribute and backpropagate probabilities among frames -
according to their forward activations (Eqn. 8)- through the
VGG until the conv5 layer where we obtain the correspond-
ing saliency map. Performing cEB-R results in a sequence
of saliency maps M apt for t = 1, ..., T grounding the
words in the video frames. Temporal localization is per-
formed using the steps described in Sec. 4.1.

Figure 3: Grounding Action Recognition. The red arrows de-
pict cEB-R for spatiotemporal grounding of the action CliffDiving.
Starting from the last LSTM time-step, cEB-R backpropagates the
probability distribution through time and through the CNN at ev-
ery time-step. The saliency map for each time-step is used for the
spatial localization. The sum of each saliency map, over time, is
then used for temporal localization of the action within the video,
as described in Sec. 4.1.

Algorithm 1: cEB-R

Input: T -frame video clip, pre-trained CNN-LSTM

model, A: action or word to be localized in the
video.

Output: Spatial saliency maps of A : M apt for

t = 1, ..., T .

Procedure:

1 Set a one-hot vector according to the desired action

class or caption word A at the desired nth time-step;

2 Backprop the indicator vector through time and down
to the fc CNN layer using EB-R obtaining a saliency
map M apt at every time step t;

3 Normalize the resulting frame-wise saliency maps

over time such that (cid:80)T

t=1 M apt = 1;

4 Repeat the above steps, with negated weights at the
top layer to get a second set of T saliency maps;

5 Contrastive Operation: Subtract the resulting maps at
the fc CNN layer to yield cEB for each time step;

6 Continue EB through the CNN to the desired conv

layer to obtain the spatial grounding;

7 The sum of each spatial saliency map over time can be

used to perform temporal grounding for A;

4

5. Experiments: Action Grounding

In this work we ground the decisions made by our deep
models. In order to evaluate this grounding, we compare
it with methods that localize actions. Although our frame-
work is able to jointly localize actions in space and time, we
report results for spatial localization and temporal localiza-
tion separately due to the lack of an action dataset that has
untrimmed videos with spatiotemporal bounding boxes.

5.1. Spatial Localization

In this section we evaluate how well we ground actions in
space. We do this by comparing our grounding results with
ground-truth bounding boxes localizing actions per-frame.
Dataset. THUMOS14 [4] provides per-frame bound-
ing box annotations of humans performing actions for
3207 videos of 24 classes from the UCF101 dataset [20].
UCF101 is a trimmed video dataset containing 13320 ac-
tions belonging to 101 action classes.

Baselines. We compare our formulation against spatial
top-down saliency using a CNN (treating every video frame
as an independent image). We also compare against stan-
dard backpropagation (BP), and BP for RNNs (BP-R).

Models. We use the following CNN model: VGG-16 of
Ma et al. [9] trained on UCF101 video frames and BU101
web images for action recognition with a test accuracy of
83.5%. We use the following CNN-LSTM model: the same
VGG-16 ﬁne-tuned with a one-layer LSTM on UCF101 for
action recognition with a test accuracy of 83.3%.

Setup and Results. We use the bounding box annota-
tions to evaluate our spatial grounding using the pointing
game introduced by Zhang et al. [31]. We locate the point
having maximum value on each top-down saliency map.
Following [31], if a 15-pixel diameter circle around the lo-
cated point intersects the ground-truth bounding-box of the
action category for a frame, we record a hit, otherwise we
record a miss. We measure the spatial action localization
accuracy by Acc = #Hits/(#Hits + #M isses) over all
the annotated frames for each action.

Table 1 reports the results of the spatial pointing game.
Extending top-down saliency in time (-R) consistently im-
proves the accuracy for all three methods, compared to per-
forming top-down saliency separately on every frame of the
video using a CNN. EB-R has the greatest absolute im-
provement of 5.7%.

We note that the non-contrastive versions outperform
their contrastive counterparts. This is because they high-
light discriminative evidence for actions, which may not
necessarily be the humans performing the actions. For ex-
ample, for many actions in UCF101, the human may be in a
standing position, in which case cEB-R will highlight cues
that are discriminative and unique to this action rather than
highlighting the human. These cues may belong to the con-
text in which the activity is performed, or the action classes

Figure 4: Grounding Captioning. The red arrows depict cEB-R
for spatiotemporal caption grounding. The video caption produced
by the model is A man is singing on a stage. Starting from the
time-step corresponding to the word singing, cEB-R backprops
the probability distribution through the previous time-steps and
through the CNN. The saliency map for each time step is used
for spatial localization. The sum of each saliency map, over time,
is then used for temporal localization of the word within the clip.

EB
55.8

EB-R
61.5

Method Acc (%)
cEB-R
39.1

cEB
37.0

BP
37.3

BP-R
39.2

Table 1: Accuracy of the spatial pointing game conducted on
∼3K videos of UCF101 for spatially locating humans perform-
ing actions in videos. The results show that extending top-down
saliency in time (-R) improves the accuracy compared to perform-
ing top-down saliency separately on every frame of the video using
a CNN. The non-contrastive versions work better for reasons de-
scribed in the text.

on which the model was trained. We demonstrate this in
Fig. 5 for the actions Surﬁng and BasketballDunk.

5.2. Temporal Localization

In this section we evaluate how well we ground actions
in time. We do this by comparing our grounding results
with ground-truth action boundaries.

Datasets. We ﬁrst use a simple and controlled setting
to validate our method by creating a synthetic action detec-
tion dataset. We then present results on the THUMOS14
[4] action detection dataset. The synthetic dataset is cre-
ated by concatenating two UCF101 videos uniformly sam-
pled: a ground truth (GT) video, and a random (rand) back-

5

(a) Grounding Surﬁng using EB-R (L) and cEB-R (R)

(a) Grounding of the action TableTennisShot in the video

(b) Grounding BasketballDunk using EB-R (L) and cEB-R (R)

(b) Grounding of the action Skiing in the video

Figure 5: The saliency maps produced by EB-R (left) and cEB-R
(right) together with the THUMOS14 groundtruth bounding box
over the same frame of the actions (a) Surﬁng and (b) Basketball-
Dunk.
In both cases, EB-R highlights the most salient regions
of the frame for this action (human), which matches the bound-
ing box annotation. However, cEB-R highlights the region that is
unique to the ground truth action: the waves for Surﬁng, and the
hoop for BasketballDunk. This is because highlighting the human
region does not provide insightful information to the classiﬁer.

ground video, such that class(GT) (cid:54)= class(rand). The two
actions are concatenated, ﬁrst sequentially (rand + GT or
GT + rand) in 16-frame clips, then inserted at a random
position (rand + GT + rand) in 128-frame clips. We use
all 3783 test videos provided in UCF101, each in com-
bination with a different random background video. The
THUMOS14 dataset consists of 1010 untrimmed validation
videos and 1574 untrimmed test videos of 20 action classes.
Among test videos, we evaluate our framework on the 213
test videos which contain annotations as in [24, 16].

Baselines. For the synthetic experiment, we compare
cEB-R and EB-R with a probability-based approach where
we threshold the predicted probability (to 1 if ≥ 0.5, to −1
if < 0.5) of the GT class at every time-step. For the detec-
tion experiment in THUMOS14 we compare our proposed
method with state-of-the-art approaches.

Models. For the synthetic dataset, we use the same
CNN-LSTM model used for spatial action grounding
(Sec. 5.1). For the THUMOS14 dataset we use a CNN-
the same VGG-16 model used for spatial
LSTM model:
action grounding (Sec. 5.1) ﬁne-tuned with a one-layer
LSTM on UCF101 and trimmed sequences from THU-
MOS14 background and validation sets.

Setup and Results: Synthetic Data. First, we per-
form experiments on the synthetic videos composed of two
sequential actions, where the boundary is the midpoint.
Fig. 6 presents a sample spatiotemporal localization. The
heatmaps produced by cEB-R correctly ground the queried
action. More examples are presented in the supplementary
material. While Fig. 6 presents a qualitative sample, Fig. 7
quantitatively presents results on the entire test set. The ac-

Figure 6: Applying contrastive Excitation Backprop for Recur-
rent Networks (cEB-R) to produce spatiotemporal localization of
actions in sample frames of a video. Demonstrated here is (a)
cEB-R spatiotemporal localization of TableTennisShot in a video
(b) cEB-R spatiotemporal localization of Skiing in the same video.
The video consists of two consecutive actions that are synthetically
concatenated: Skiing followed by TableTennisShot.

tion switches from GT to rand or vice versa midway. It can
be seen that the sum of saliency maps is: positive and in-
creasing as more of the GT action is observed, negative and
decreasing as more of the rand action is observed.

Next, we perform experiments where we vary the length
of the GT action that we want to localize inside a clip. To re-
tain action dynamics, we sample GT and rand from the en-
tire length of their corresponding videos. Table 2 presents
the temporal localization results of our synthetic data. In
the experimental setup with ﬁxed action length we assume
that we know the label and length of the action to be lo-
calized. To localize, we ﬁnd the highest consecutive sum
of attention maps for the desired action length. Regard-
ing the sequences with unknown action lengths, we only
assume the label of the action to be localized and perform
the pipeline described in Sec. 4.1.
In the bottom half of
Table 2 we only report thresholded probabilities and cEB-
R results since our localization procedure assumes negative
values at action boundaries, whereas EB-R is non-negative.
The grounded evidence obtained by cEB-R attains the high-
est detection scores, 73.5% and 62.0%, for action sequences
of known and unknown lengths, respectively, for IoU over-
lap between detections and ground-truth of α = 0.5, despite
the fact that the model is not trained for localization.

Setup and Results: THUMOS14 Pointing Game. We
evaluate the pointing game in time for THUMOS14 -a fair
evaluation for methods that do not optimize for detection.
For processing, we divide a video into 128-frame consecu-
tive clips. We perform the pointing game by pointing [31]
in time to the peak sum of saliency maps. For each ground-
truth annotation we check if the detected peak is within its
boundaries. If yes, we count it as a hit, otherwise, as a miss.
We compare this approach with the peak position of pre-

6

dicted probabilities, and a random point in that clip.

The results of this experiment are presented in Table 3.
Pointing to a random position clearly obtains lowest results
while peak probability (65.8%) and cEB-R (65.1%) have
similar performance. However, peak probability does not
offer spatial localization. Peak probability uses the model
prediction, while cEB-R uses the evidence of that predic-
tion. Moreover, we observe that peak probability and cEB-
R are complementary, yielding 77.4%.

Setup and Results: THUMOS14 Action Detection.
We evaluate how well our grounding does on the more chal-
lenging task of action detection that it was not trained for.
In this experiment, we divide a video into 128-frame con-
secutive clips for processing. Table 4 presents the tempo-
ral detection results of the THUMOS14 dataset. Differently
from the pointing game experiment, we detect the start and
end of the ground-truth action. We note that although our
method is not supervised for the detection task, we achieve
an accuracy of 57.9% when locating a ground truth class
with an overlap α = 0.1 as demonstrated in Table 4.

6. Experiments: Caption Grounding

In this section, we show how cEB-R is also applica-
ble in the context of caption grounding. As observed by
[13], there is an absence of datasets with spatiotemporal an-
notations of frames for captions. Therefore, they propose
the following experimental setup which we follow: qualita-
tive results for the spatiotemporal grounding on videos, and
quantitative results for spatial grounding on images.

Datasets. We use the MSR-VTT [25] dataset for video
captioning and Flickr30kEntities [12] for image captioning.
Models. We use the CNN-LSTM-LSTM video caption-
ing model of [22] trained on MSR-VTT to test our cEB-
R approach for spatiotemporal grounding as described in
Sec. 4.2. We use the same video captioning model, with-
out the average pooling layer, trained on Flickr30kEntities
for image captioning. The models have comparable ME-
TEOR scores to the Caption-Guided Saliency work of [13],
to which we compare our results: 26.5 (vs. 25.9) for video
captioning and 18.0 (vs. 18.3) for image captioning.

Setup and Results. For the MSR-VTT video dataset,
we sample 26 frames per video following [13] and perform
grounding of nouns. Fig. 8 presents the grounding for the
word man and phone in the same video. The man is well lo-
calized only in frames where a man appears, and the phone
is well localized in frames where a phone appears.

We quantitatively evaluate our results of spatial ground-
ing using the pointing game on the Flickr30kEntities and
compare our method to the Caption-Guided Saliency work
of [13], following their evaluation protocol. We use ground
truth captions as an input to our model in order to reproduce
the same captions. Then, we use bounding box annotations
for each noun phrase in the ground truth captions and check

Figure 7: Sum of the saliency maps at fc7 over time, in frames,
for synthetic videos that (blue) have a rand action followed by a
GT action and (green) have a GT followed by a rand action. The
average and standard deviation are reported over all test videos.
cEB-R provides an accurate midway boundary between actions.

Length
(frames)

Probability
(%)

EB-R
(%)

cEB-R
(%)

h

t
g
n
e
L
n
o
i
t
c
A

n
w
o
n
K

n
w
o
n
k
n
U

11
41
65

11
41
65

8.5
28.2
47.7

3.4
9.5
35.7

11.3
38.5
56.3

-
-
-

15.5
53.2
73.5

4.1
47.9
62.0

Table 2: Action detection results on synthetic data, measured by
mAP at IoU threshold α = 0.5. Top part of table: methods assume
that the length and label of the action to be detected are known.
Bottom part of table: methods assume that the label is known, but
the length is unknown. cEB-R attains best performance.

Method
Random
Peak probability
cEB-R
Peak probability + cEB-R

Accuracy (%)
57.3
65.8
65.1
77.4

Table 3: Pointing game in time performed on the THUMOS14
test set. The probability of an action together with the evidence
for presence of the action are complementary and give a great im-
provement in accuracy when combined.

whether the maximum point in a saliency map is inside the
annotated bounding box.

Table 5 shows the results of the spatial pointing game
on Flickr30kEntities. Our approach achieves comparable
performance to [13].
In this experiment, we ground the
ground truth captions to match the experimental setup in
[13]. Although we follow their protocol for fair compari-
son, we note that our method can better highlight evidence
using generated captions (vs. ground truth captions). This
is because the evidence of a ground truth noun that is not

7

Method
Karaman et al. [6]
Wang et al. [23]
Oneata et al. [10]
Richard et al. [14]
Shou et al. [17]
Yeung et al. [28]
Yuan et al. [29]
Xu et al. [24]
Zhao et al. [32]
Kaufman et al. [8]
Ours

mAP (α = 0.1)
4.6
18.2
36.6
39.7
47.7
48.9
51.4
54.5
60.3
61.1
57.9

Table 4: Our approach vs. fully supervised approaches for action
detection on THUMOS14, measured by mAP at IoU threshold α =
0.1. Although our model is not trained for action detection (trained
for recognition), we achieve 57.9%, which is comparable to state-
of-the-art when localizing a ground truth action in a video.

Method

Baseline random
Baseline center
Caption-Guided Saliency [13]
Ours

Avg (Noun Phrases)
0.268
0.492
0.501
0.512

Table 5: Evaluation of spatial saliency on Flickr30kEntities using
cEB-R. Baseline random samples the maximum point uniformly
and Baseline center always picks the center.

predicted may not be sufﬁciently activated in the forward
pass. Fig. 9 presents some visual examples of grounding in
images using the generated captions.

Our approach has a computational advantage over [13].
In order to obtain spatial saliency maps for a word in a
video, c-EB-R requires one forward pass and one back-
ward pass through the CNN-LSTM-LSTM, while [13] re-
quires one forward pass through the CNN part, but m for-
ward passes through the LSTM-LSTM part, where m = 64
is the area of the saliency map (vs. our single backward
pass). Moreover, they require f forward LSTM passes,
where f = 26 is the number of frames, to compute the tem-
poral grounding, whereas ours is implicitly spatiotemporal.

7. Conclusion

In conclusion, we devise a temporal formulation, cEB-R,
that enables us to visualize how recurrent networks ground
their decisions in visual content. We apply this to two video
understanding tasks: video action recognition, and video
captioning. We demonstrate how spatiotemporal top-down
saliency is capable of grounding evidence on several action
and captioning datasets. These datasets provide annotations
for detection and/or localization, to which we have com-
pared the evidence in our generated saliency maps. We
observe the strengths of cEB-R in highlighting discrimina-

(a) grounding of the word man

(b) grounding of the word phone

Figure 8: Comparison of grounding of words man and phone in
the caption A man is talking about a phone of a video from MSR-
VTT using cEB-R. The man is well localized in (a) and the phone
is well localized in (b), as desired.

(a) image caption: A man in a lab coat is working on a microscope.

(b) image caption: A cowboy is riding a bucking horse.

Figure 9: Grounding different words of a caption using cEB-R
for two images from the Flicker30kEntities dataset.

tive evidence, which was particularly beneﬁcial for tempo-
ral grounding. We also observe the strengths of its variant,
EB-R, in highlighting salient evidence, which was particu-
larly beneﬁcial for spatial localization of action subjects.

Acknowledgments

We thank Kate Saenko and Vasili Ramanishka for help-
ful discussions. This work was supported in part by NSF
grants 1551572 and 1029430, an IBM PhD Fellowship,
gifts from Adobe and NVidia, and Intelligence Advanced
Research Projects Activity (IARPA) via Department of In-
terior/ Interior Business Center (DOI/IBC) contract number
D17PC00341. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon. Dis-
claimer: The views and conclusions contained herein are
those of the authors and should not be interpreted as neces-
sarily representing the ofﬁcial policies or endorsements, ei-
ther expressed or implied, of IARPA, DOI/IBC, or the U.S.
Government.

8

References

[1] C. Cao, X. Liu, Y. Yang, Y. Yu, J. Wang, Z. Wang, Y. Huang,
L. Wang, C. Huang, W. Xu, et al. Look and think twice: Cap-
turing top-down visual attention with feedback convolutional
neural networks. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2956–2964, 2015. 1
S. Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
In Proceedings of the IEEE
recognition and description.
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 2625–2634, 2015. 4

[2] J. Donahue, L. Anne Hendricks,

[3] K. Greff, R. K. Srivastava, J. Koutn´ık, B. R. Steunebrink,
and J. Schmidhuber. LSTM: A search space odyssey. IEEE
transactions on neural networks and learning systems, 2016.
3

[4] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,
M. Shah, and R. Sukthankar. THUMOS challenge: Ac-
tion recognition with a large number of classes. http:
//crcv.ucf.edu/THUMOS14/, 2014. 5, 6

[5] R. Jozefowicz, W. Zaremba, and I. Sutskever. An empiri-
cal exploration of recurrent network architectures. In Pro-
ceedings of the 32nd International Conference on Machine
Learning (ICML), pages 2342–2350, 2015. 3

[6] S. Karaman, L. Seidenari, and A. Del Bimbo. Fast saliency
based pooling of Fisher encoded dense trajectories. In ECCV
THUMOS Workshop, volume 1, page 5, 2014. 8

[7] A. Karpathy, J. Johnson, and L. Fei-Fei. Visualizing and
understanding recurrent networks. In ICLR Workshop, 2016.
1, 2

[8] D. Kaufman, G. Levi, T. Hassner, and L. Wolf. Temporal
tessellation: A uniﬁed approach for video analysis. In The
IEEE International Conference on Computer Vision (ICCV),
2017. 8

[9] S. Ma, S. A. Bargal, J. Zhang, L. Sigal, and S. Sclaroff. Do
less and achieve more: Training cnns for action recognition
utilizing action images from the web. Pattern Recognition,
2017. 5

[10] D. Oneata, J. Verbeek, and C. Schmid. The lear submission

at thumos 2014. 2014. 8

[11] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Is object local-
ization for free? - weakly-supervised learning with convo-
lutional neural networks. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2015. 2
[12] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
In Proceedings of the IEEE interna-
to-sentence models.
tional conference on computer vision (ICCV), pages 2641–
2649, 2015. 7

[13] V. Ramanishka, A. Das, J. Zhang, and K. Saenko. Top-down
In Proceedings of the
visual saliency guided by captions.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 1, 2, 7, 8

[14] A. Richard and J. Gall. Temporal action detection using a
statistical language model. In Proceedings of the IEEE Con-

ference on Computer Vision and Pattern Recognition, pages
3131–3140, 2016. 8

[15] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,
D. Parikh, and D. Batra. Grad-cam: Visual explanations
from deep networks via gradient-based localization. See
https://arxiv. org/abs/1610.02391 v3, 2016. 1, 2

[16] Z. Shou, J. Chan, A. Zareian, K. Miyazawa, and S.-F. Chang.
Cdc: Convolutional-de-convolutional networks for precise
temporal action localization in untrimmed videos. arXiv
preprint arXiv:1703.01515, 2017. 6

[17] Z. Shou, D. Wang, and S.-F. Chang. Temporal action local-
In Pro-
ization in untrimmed videos via multi-stage cnns.
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 1049–1058, 2016. 8
[18] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside
convolutional networks: Visualising image classiﬁcation
models and saliency maps. arXiv preprint arXiv:1312.6034,
2013. 1, 2

[19] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014. 4

[20] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset
of 101 human actions classes from videos in the wild. arXiv
preprint arXiv:1212.0402, 2012. 5

[21] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. A. Ried-
miller. Striving for simplicity: The all convolutional net.
CoRR, abs/1412.6806, 2014. 1, 2

[22] S. Venugopalan, H. Xu,

J. Donahue, M. Rohrbach,
R. Mooney, and K. Saenko. Translating videos to natural
language using deep recurrent neural networks. North Amer-
ican Chapter of the Association for Computational Linguis-
tics Human Language Technologies NAACL-HLT, 2015. 4,
7

[23] L. Wang, Y. Qiao, and X. Tang. Action recognition and de-
tection by combining motion and appearance features. THU-
MOS14 Action Recognition Challenge, 1(2):2, 2014. 8
[24] H. Xu, A. Das, and K. Saenko. R-c3d: Region convolutional
3d network for temporal activity detection. The IEEE Inter-
national Conference on Computer Vision (ICCV), 2017. 6,
8

[25] J. Xu, T. Mei, T. Yao, and Y. Rui. Msr-vtt: A large video
description dataset for bridging video and language. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 5288–5296, 2016. 7
[26] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudi-
nov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural
image caption generation with visual attention. In Interna-
tional Conference on Machine Learning, pages 2048–2057,
2015. 2

[27] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle,
and A. Courville. Describing videos by exploiting temporal
structure. In Proceedings of the IEEE international confer-
ence on computer vision (ICCV), pages 4507–4515, 2015.
2

[28] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei. End-
to-end learning of action detection from frame glimpses in
videos. In Proceedings of the IEEE Conference on Computer

9

Vision and Pattern Recognition (CVPR), pages 2678–2687,
2016. 8

[29] J. Yuan, B. Ni, X. Yang, and A. A. Kassim. Temporal ac-
tion localization with pyramid of score distribution features.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 3093–3102, 2016. 8
[30] M. D. Zeiler and R. Fergus. Visualizing and understanding
In European conference on com-

convolutional networks.
puter vision (ECCV), pages 818–833. Springer, 2014. 1, 2

[31] J. Zhang, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff. Top-
down neural attention by excitation backprop. In European
Conference on Computer Vision (ECCV), pages 543–559.
Springer, 2016. 1, 2, 3, 4, 5, 7

[32] Y. Zhao, Y. Xiong, L. Wang, Z. Wu, X. Tang, and D. Lin.
Temporal action detection with structured segment networks.
In The IEEE International Conference on Computer Vision
(ICCV), Oct 2017. 8

[33] B. Zhou, A. Khosla, L. A., A. Oliva, and A. Torralba. Learn-
ing Deep Features for Discriminative Localization. The
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2016.

[34] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-
ralba. Learning deep features for discriminative localization.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2921–2929, 2016.
1, 2

10

