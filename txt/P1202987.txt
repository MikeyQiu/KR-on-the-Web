Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation

Yige Xu1 , Xipeng Qiu1 , Ligao Zhou2 and Xuanjing Huang1
1Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
2Huawei Technologies Co., Ltd.
{ygxu18, xpqiu, xjhuang}@fudan.edu.cn, zhouligao@huawei.com,

0
2
0
2
 
b
e
F
 
4
2
 
 
]
L
C
.
s
c
[
 
 
1
v
5
4
3
0
1
.
2
0
0
2
:
v
i
X
r
a

Abstract

Fine-tuning pre-trained language models like BERT
has become an effective way in NLP and yields state-
of-the-art results on many downstream tasks. Recent
studies on adapting BERT to new tasks mainly focus
on modifying the model structure, re-designing the
pre-train tasks, and leveraging external data and
knowledge. The ﬁne-tuning strategy itself has yet
to be fully explored. In this paper, we improve the
ﬁne-tuning of BERT with two effective mechanisms:
self-ensemble and self-distillation. The experiments
on text classiﬁcation and natural language inference
tasks show our proposed methods can signiﬁcantly
improve the adaption of BERT without any external
data or knowledge.

1 Introduction
The pre-trained language models including BERT [Devlin et
al., 2018] and its variants (XLNet [Yang et al., 2019] and
RoBERTa [Liu et al., 2019b]) have been proven beneﬁcial
for many natural language processing (NLP) tasks, such as
text classiﬁcation, question answering [Rajpurkar et al., 2016]
and natural language inference [Bowman et al., 2015]. These
pre-trained models have learned general-purpose language
representations on a large amount of unlabeled data, therefore,
adapting these models to the downstream tasks can bring a
good initialization for and avoid training from scratch. There
are two common ways to utilize these pre-trained models on
downstream tasks: feature extraction (where the pre-trained
parameters are frozen), and ﬁne-tuning (where the pre-trained
parameters are unfrozen and ﬁne-tuned). Although both these
two ways can signiﬁcantly improve the performance of most
of the downstream tasks, the ﬁne-tuning way usually achieves
better results than feature extraction way [Peters et al., 2019].
Therefore, it is worth paying attention to ﬁnd a good ﬁne-
tuning strategy.

As a widely-studied pre-trained language model, the po-
tential of BERT can be further boosted by modifying model
structure [Stickland and Murray, 2019; Houlsby et al., 2019],
re-designing pre-training objectives [Dong et al., 2019;
Yang et al., 2019; Liu et al., 2019b], data augmentation [Raffel
et al., 2019] and optimizing ﬁne-tuning strategies with exter-
nal knowledge [Liu et al., 2019a; Sun et al., 2019]. However,

the ﬁne-tuning strategy itself has yet to be fully explored. Sun
et al. [2019] investigated different ﬁne-tuning strategies and
hyper-parameters of BERT for text classiﬁcation and showed
the “further-pretraining” on the related-domain corpus can fur-
ther improve the ability of BERT. Liu et al. [2019a] ﬁne-tuned
BERT under the multi-task learning framework. The perfor-
mance of BERT on a task could beneﬁt from other related
tasks. Although these methods achieve better performance,
they usually need to leverage external data or knowledge.

In this paper, we investigate how to maximize the utiliza-
tion of BERT by better ﬁne-tuning strategy without utilizing
the external data or knowledge. BERT is usually ﬁne-tuned
by using stochastic gradient descent (SGD) method. In prac-
tice, the performance of ﬁne-tuning BERT is often sensitive
to the different random seeds and orders of training data, es-
pecially when the last training sample is noise. To alleviate
this, the ensemble method is widely-used to combine several
ﬁne-tuned based models since it can reduce the overﬁtting
and improve the model generalization. The ensemble BERT
usually achieves superior performance than the single BERT
model. However, the main disadvantages of the ensemble
method are its model size and training cost. The ensemble
model needs to keep multiple ﬁne-tuned BERTs and has a low
computation efﬁciency and high storage cost.

We improve the ﬁne-tuning strategy of BERT by introducing

two mechanisms: self-ensemble and self-distillation.

(1) Self-Ensemble. Motivated the success of widely-used
ensemble models, we propose a self-ensemble method, in
which the base models are the intermediate BERT models at
different time steps within a single training process [Polyak
and Juditsky, 1992]. To further reduce the model complexity
of the ensemble model, we use a more efﬁcient ensemble
method, which combines several base models with parameter
averaging rather than keeping several base models.

(2) Self-Distillation. Although the self-ensemble can im-
prove the model performance, the training of the base model
is the same as the vanilla ﬁne-tuning strategy and cannot be
affected by the ensemble model. We further use knowledge dis-
tillation [Hinton et al., 2015] to improve ﬁne-tuning efﬁciency.
At each time step in training, the current BERT model (called
student model) is learned with two teachers: the gold labels
and self-ensemble model (called teacher model). The teacher
model is an average of student models at previous time steps.
With the help of the teacher model, the student is more robust

and accurate. Moreover, a better student model further leads
to a better teacher model. A similar idea is also used in semi-
supervised learning, such as Temporal Ensembling [Laine and
Aila, 2016] and Mean Teacher [Tarvainen and Valpola, 2017].
Different from them, our proposed self-distillation aims to
optimize the student model without external data.

The experiments on text classiﬁcation and natural language
inference tasks show our proposed methods can reduce the
test error rate by more than 5.5% on the average on seven
widely-studied datasets.

The contributions of this paper are summarized as follows:

• We show the potential of BERT can be further stimu-
lated by a better ﬁne-tuning strategy without leveraging
external knowledge or data.

• The self-ensemble method with parameter averaging can
improve BERT without signiﬁcantly decreasing the train-
ing efﬁciency.

• With self-distillation, the student and teacher models can
beneﬁt from each other. The distillation loss can also be
regarded as a regularization to improve the generalization
ability of the model.

2 Related Work

We brieﬂy review two kinds of related work: pre-trained lan-
guage models and knowledge distillation.

2.1 Pre-trained Language Models

Pre-training language models on a large amount of unlabeled
data then ﬁne-tuning in downstream tasks has become a new
paradigm for NLP and made a breakthrough in many NLP
tasks. Most of the recent pre-trained language models (e.g.,
BERT [Devlin et al., 2018], XLNet [Yang et al., 2019] and
RoBERTa [Liu et al., 2019b]) are built with Transformer ar-
chitecture [Vaswani et al., 2017].

As a wide-used model, BERT is pre-trained on Masked
Language Model Task and Next Sentence Prediction Task via a
large cross-domain unlabeled corpus. BERT has two different
model size: BERTBASE with a 12-layer Transformer encoder
and BERTLARGE with a 24-layer Transformer encoder. Both
of them take an input of a sequence of no more than 512
tokens and outputs the representation of the sequence. The
sequence has one segment for text classiﬁcation task or two
for text matching task. A special token [CLS] is added before
segments, which contain the special classiﬁcation embedding.
Another special token [SEP] is used for separating segments.

Fine-tuning BERT can deal with different natural language
tasks with task-speciﬁc output layers. For text classiﬁcation
or text matching, BERT takes the ﬁnal hidden state h of the
ﬁrst token [CLS] as the representation of the input sentence
or sentence-pair. A simple softmax classiﬁer is added to the
top of BERT to predict the probability of label y:

p(y|h) = softmax(W h),

(1)

where W is the task-speciﬁc parameter matrix. The cross-
entropy loss is used to ﬁne-tune BERT as well as W jointly.

2.2 Knowledge Distillation for Pre-trained Models

Since the pre-trained language models usually have an ex-
tremely large number of parameters, they are difﬁcult to be
deployed on the resource-restricted devices. Several previous
works leverage the knowledge distillation [Hinton et al., 2015]
approach to reducing model size while maintaining accuracy,
such as TinyBERT [Jiao et al., 2019] and DistilBERT [Sanh et
al., 2019]. Knowledge Distillation aims to transfer the knowl-
edge of a large teacher model to a small student model by
training the student model to reproduce the behaviors of the
teacher model.

The teacher model usually is well-trained and ﬁxed in the
processing knowledge distillation. Unlike the common way of
knowledge distillation, we perform knowledge distillation in
online fashion. The teacher model is an ensemble of several
student models at previous time steps within the ﬁne-tuning
stage.

3 Methodology

The ﬁne-tuning of BERT usually aims to minimize the cross-
entropy loss on a speciﬁc task with stochastic gradient descent
method. Due to the stochastic nature, the performance of ﬁne-
tuning is often affected by the random orderings of training
data, especially when the last training samples is noise.

Our proposed ﬁne-tuning strategy is motivated by the en-
semble method and knowledge distillation. There are two
models in our ﬁne-tuning strategy: a student model is the
ﬁne-tuning BERT and a teacher model is a self-ensemble of
several student models. At each time step, we further distillate
the knowledge of the teacher model to the student model.

3.1 Ensemble BERT

In practice, the ensemble method is usually adopted to further
improve the performance of BERT.

Voted BERT The common ensemble method is voting-
based. We ﬁrst ﬁne-tune multiple BERT with different random
seeds. For each input, we output the best predictions made
by the ﬁne-tuned BERT along with the probability and sum
up the probability of predictions from each model together.
The output of the ensemble model is the prediction with the
highest probability.

Let BERT(x; θk) (1 ≤ k ≤ K) are K BERT models
ﬁne-tuned on a speciﬁc task with different random seeds, the
ensemble model BERTVOTE(x; Θ) is deﬁned as

BERTVOTE(x; Θ) =

BERT(x; θk),

(2)

K
(cid:88)

k=1

where θk denotes the parameters of the k-th model and Θ
denotes the parameters of all the K models.

We call this kind of ensemble model as voted BERT. The
voted BERT can greatly boost the performance of single BERT
in many tasks, such as question answering [Rajpurkar et al.,
2016]. However, a major disadvantage of the voted BERT is it
needs keeping multiple different BERTs and its efﬁciency is
low in computing and storage.

(a) Voted BERT

(b) Averaged BERT

Figure 1: Two methods to ensemble BERT. The averaged BERT has
better computational and memory efﬁciency then voted BERT.

Averaged BERT To reduce the model complexity of ensem-
ble model, we use a parameter-averaging strategy to combine
several BERTs into a single model, called averaged BERT.
The averaged BERT is deﬁned as

BERTAVG(x; ¯θ) = BERT(x;

θk),

(3)

1
K

K
(cid:88)

k=1

where ¯θ is the averaged parameters of K individual ﬁne-tuned
BERTs.

Figure 1 illustrates two kinds of ensemble BERTs. Since
the averaged BERT is indeed a single model and has better
computational and memory efﬁciency than the voted BERT.

3.2 Self-Ensemble BERT
Although the ensemble BERT usually brings better perfor-
mance, it needs to train multiple BERTs and its cost is often
expensive. To reduce the training cost, we use a self-ensemble
model to combine the intermediate models at different time
steps in a single training phase. We regard the BERT at
each time step as a base model and combine them into a
self-ensemble model.

Here we only describe the self-ensemble BERT with pa-
rameter averaging, since the voted version of self-ensemble is
impracticable to keep all the intermediate models.

Let θt denote parameters when ﬁne-tuning BERT at time

step t, the self-ensemble BERT is deﬁned as

BERTSE(x; ¯θ) = BERT(x;

θt),

(4)

1
T

T
(cid:88)

τ =1

where ¯θ is the averaged parameters of BERTs over T time
steps.

Averaging model weights over training steps tends to pro-
duce a more accurate model than using the ﬁnal weights di-
rectly [Polyak and Juditsky, 1992].

Figure 2: The proposed ﬁne-tuning strategy BERTSDA. The teacher
model is the average of student models over recent K time steps, and
K = 3 here. (x, y) is the input training sample. ˆy is the probability
of label y predicted by student model. ˆo and ˜o are the logits output
by student model and teacher model reﬂectively. MSE and CE denote
the mean square error and cross-entropy loss.

vanilla ﬁne-tuning strategy and cannot be affected by the en-
semble model. We further use knowledge distillation [Hinton
et al., 2015] to improve the base model. At each time step
in training, the current BERT model (called student model)
is learned from two teachers: the gold labels and the self-
ensemble model (called teacher model). The teacher model
is an average of student models at previous time steps. With
the help of the teacher model, the student is more robust and
accurate.
Self-Distillation-Averaged (SDA) We ﬁrst denote a ﬁne-
tuning strategy BERTSDA, in which the teacher model is
self-ensemble BERT with parameter averaging.

Let BERT(x, θ) denote the student BERT, the objective of

BERTSDA strategy is
(cid:16)

Lθ(x, y) =CE

BERT(x, θ), y

(cid:17)

+λMSE

(cid:16)

(cid:17)
BERT(x, θ), BERT(x, ¯θ)

,

(5)

where CE and MSE denote the cross-entropy loss and mean
squared error respectively, and λ balances the importance of
two loss functions. The teacher model BERT(x, ¯θ) is a self-
ensemble BERT with recent time steps. At time step t, ¯θ is the
averaged parameters of recent K time steps:

¯θ =

1
K

K
(cid:88)

k=1

θt−k,

(6)

where K is a hyperparameter denoting the teacher size.

Figure 2 shows the training phase of our proposed method.
In the training phase, we can compute ¯θ efﬁciently by moving
average.

Since the teacher model aggregates information of student
models after every time step, it is usually more robust. More-
over, a better student model further leads to better a teacher
model.
Self-Distillation-Voted (SDV) As a comparison, we also
propose an alternative self-distillation method by replacing the
teacher model with self-voted BERT, called BERTSDV. The
objective of BERTSDV strategy is

Lθ(x, y) = CE

BERT(x, θ), y

(cid:16)

(cid:17)

3.3 Self-Distillation BERT
Although the self-ensemble can improve the model perfor-
mance, the base model is trained in the same manner to the

+ λMSE

BERT(x, θ),

BERT(x, θt−k)

(7)

(cid:17)

.

(cid:16)

1
K

K
(cid:88)

k=1

Type

Dataset

Num of Labels

Train samples Dev samples

Test samples

Text Classiﬁcation

NLI

IMDb
AG’s News
DBPedia
Yelp Polarity
Yelp Full

SNLI
MNLI-(m/mm)

2
4
14
2
5

3
3

25,000
120,000
560,000
560,000
650,000

549,367
392,702

-
-
-
-
-

25,000
7,600
70,000
38,000
50,000

9,842
9,815/9,832

9,824
9,796/9,847

Table 1: Summary statistics of seven widely-studied text classiﬁcation and natural language inference (NLI) datasets.

The training efﬁciency of BERTSDV strategy is lower than
BERTSDA strategy since BERTSDV needs to process the
input with recent K student models.

4 Experiments

In this paper, we improve BERT ﬁne-tuning via self-ensemble
and self-distillation. The vanilla ﬁne-tuning method of BERT
is used as our baseline. Then we evaluate our proposed ﬁne-
tuning strategies on seven datasets to demonstrate the feasibil-
ity of our self-distillation model.

4.1 Datasets
Our proposed method is evaluated on ﬁve Text Classiﬁcation
datasets and two Natural Language Inference (NLI) datasets.
The statistics of datasets are shown in Table 1.

Text Classiﬁcation

• IMDb IMDb [Maas et al., 2011] is a binary sentiment
analysis dataset from the Internet Movie Database. The
dataset has 25,000 training examples and 25,000 valida-
tion examples. The task is to predict whether the review
text is positive or negative.

• AG’s News AG’s corpus [Zhang et al., 2015] of the news
articles on the web contains 496,835 categorized news ar-
ticles. The four largest classes from this corpus with only
the title and description ﬁelds were chosen to construct
the AG’s News dataset.

• DBPedia DBPedia [Zhang et al., 2015] is a crowd-
sourced community effort that includes structured infor-
mation from Wikipedia. The DBPedia dataset is con-
structed by picking 14 non-overlapping classes from
DBPedia 2014 with only the title and abstract of each
Wikipedia article.

• Yelp The Yelp dataset is obtained from the Yelp Dataset
Challenge in 2015, built by Zhang et al. [2015]. There
are two classiﬁcation tasks in this dataset: Yelp Full and
Yelp Polarity. Yelp Full predicts the full number of stars
(1 to 5) which given by users, and the other predicts a
polarity label that is positive or negative.

Natural Language Inference

• SNLI The Stanford Natural Language Inference Corpus
[Bowman et al., 2015] is a collection of 570k human-
written English sentence pairs manually labeled for bal-
anced classiﬁcation with the labels entailment, contradic-
tion, or neutral.

• MNLI Multi-Genre Natural Language

Inference
[Williams et al., 2018] corpus is a crowdsourced
entailment classiﬁcation task with about 433k sentence
pairs. The corpus has two test sets: matched (MNLI-m)
and mismatched (MNLI-mm).

4.2 Hyperparameters
All the hyperparameters of our proposed methods are the
same as the ofﬁcial BERT [Devlin et al., 2018] except self-
distillation weight λ and teacher size K. We use AdamW
optimizer with the warm-up proportion of 0.1, base learning
rate for BERT encoder of 2e-5, base learning rate for softmax
layer of 1e-3, dropout probability of 0.1. For sequences of
more than 512 tokens, we truncate them and choose head 512
as model input.

We ﬁne-tune all models on one RTX 2080Ti GPU. For
BERTBASE, the batch size is 4, and the gradient accumulation
steps is 4. For BERTLARGE, the batch size is 1, and the
gradient accumulation steps is 16.

For ensemble BERT (See section 3.1), we run BERTBASE

with 4 different random seeds and save the checkpoint.

4.3 Model Selection
As shown in section 3.3, there are two main hyperparameters
in our ﬁne-tuning methods (BERTSDA and BERTSDV): self-
distillation weight λ and teacher size K.
Self-Distillation Weight We ﬁrst evaluate our methods on
IMDb dataset to investigate the effect of self-distillation
weight λ. Figure 3 shows that λ ∈ [1.0, 1.5] has better results.
This observation is the same as the other datasets. Therefore,
we set λ = 1 in the following experiments.
Teacher Size We choose different teacher size K and evalu-
ate our models in three datasets. Table 2 shows that teacher
size is sensitive to datasets. Therefore, we select the best
teacher size for each dataset in the following experiment.

4.4 Model Analysis
Training Stability Generally, distinct random seeds can
lead to substantially different results when ﬁne-tuning BERT
even with the same hyperparameters. Thus, we conduct exper-
iments to explore the effect of data order on our models.

This experiment is conducted with a set of data order seeds.
One data order can be regarded as one sample from the set
of permutations of the training data. 1,500 labeled examples
from SNLI dataset (500 on each class) are randomly selected
to construct a new training set. With the same initialization

0.1

0.2

0.5

1.0

2.0

3.0

4.0

5.0

1.5
λ

1

2

3

4

5

6

Epoch

Figure 3: Test error on IMDb dataset over different setting of self-
distillation weight λ. T denotes the total number of iterations.

Figure 5: Test error rates(%) on IMDb dataset durning different
epoch. T denotes the total number of iterations.

BERTBASE
BERTSDV(K = 1)
BERTSDV(K = 5)
BERTSDA(K = 5)
BERTSDA(K = T − 1)

)

%
(
e
t
a
r

r
o
r
r
e

t
s
e
T

6.4

6.2

6

5.8

5.6

5.4

5.2

)

%

(

y
c
a
r
u
c
c
A

75

74

73

Model

K

IMDb

SNLI

BERTSDV

BERTSDA

1
2
3
4
5

T − 1
2
3
4
5

5.39
5.44
5.40
5.47
5.35

5.41
5.46
5.48
5.44
5.29

AG’s
News

5.38
5.39
5.50
5.49
5.55

5.29
5.49
5.55
5.52
5.41

91.2
91.1
91.2
91.2
91.1

91.0
91.2
91.1
91.1
91.1

Table 2: Results on IMDb dataset over different teacher sizes.
BERTSDV(K = 1) is same as BERTSDA(K = 1). For IMDb
and AG’s News, we report test error rate (%). For SNLI, we report
accuracy (%). T denotes the total number of iterations.

seeds but different data order seeds, we run 10 times for each
ﬁne-tuning strategy and record the result as Figure 4.

Results show that our strategies have higher accuracy and
smaller variance than the vanilla BERTBASE ﬁne-tuning. This
proves that the ﬁne-tuned BERT with the self-distillation strat-
egy inherits the property of the ensemble model and is less
sensitive to the data order.

B E R T B A S E

B E R T S D A ( K = 1

B E R T S D A ( K = 5

B E R T S D V ( K = 5

)

)

)

Figure 4: Boxplot for training stability. Only 1,500 labeled example
in SNLI training set are used for training. Here BERTSDA(K = 1)
is same as BERTSDV(K = 1).

BERTBASE
BERTSDV(K = 5)
BERTSDA(K = 5)
BERTSDA(K = T − 1)

CE-Loss
MSE-Loss

8

7

6

)

%
(
e
t
a
r

r
o
r
r
e

t
s
e
T

0.4

s
s
o
L

0.6

0.2

0

0.5k 5k

10k 15k 20k 25k 30k
Training steps

37.5k

Figure 6: Loss curve of BERTSDA(K = 1) on IMDb dataset.

Convergence Curves To understand the effects of using
self-distillation, we record the converge curve while training.
The training curves on IMDb dataset are shown in Figure 5.
Fine-tuning BERTBASE cannot get signiﬁcant improvement
in the last 3 epochs (from 6.00% to 5.80%). But with self-
distillation mechanisms, the test error rate can further decrease
to 5.35% (for BERTSDV) and 5.29% (for BERTSDA).

To further analyze the reason for this observation, we also
record the loss curve of cross-entropy (CE) loss and mean-
squared-error (MSE) loss, as shown in Figure 6. When training
begins, the CE loss dominates the optimization objective. In
the last phase of training, the cross-entropy loss becomes small,
and a large proportion of gain also comes from self-distillation.
Therefore, although optimizing the CE loss at the end of the
training phase cannot continue improving the performance of
BERT, self-distillation with ensemble BERT as the teacher
will continuously enhance the generalization and robustness
of BERT.

4.5 Model Performance
In this section, we evaluate our proposed ﬁne-tuning strategies
for the BERT-base and BERT-large models on text classiﬁca-
tion and NLI tasks.
Effects on Fine-tuning BERT-Base Table 3 shows the re-
sults of ﬁne-tuning the BERT-base model on ﬁve text classi-
ﬁcation datasets and two NLI datasets. For ensemble BERT,
both the voted BERT (BERTVOTE) and averaged BERT
(BERTAVG) outperform the single BERT (BERTBASE). The
average improvement of BERTVOTE is 5.44% (for text clas-

Model

ULMFiT [Howard and Ruder, 2018]
BERTBASE [Sun et al., 2019]*

BERTBASE
BERTVOTE (K = 4)
BERTAVG (K = 4)

BERTSE (ours)

BERTSDV (ours)
BERTSDA (ours)

IMDb

DBPedia Yelp P. Yelp F.

Test Error Rate (%)

AG’s
News

5.01
5.25

5.71
5.41
5.53

5.59

5.38
5.29

4.60
5.40

5.80
5.60
5.68

5.82

5.35
5.29

Avg. ∆

Avg. ∆

SNLI

MNLI
(m/mm)
Accuracy (%)

/
/

/
/

/
/

/
/

-
5.44%
4.07%

90.7
91.2
90.8

84.6/83.3
85.3/84.4
85.1/84.2

-
5.50%
3.24%

29.98
30.06

30.37
29.44
30.03

30.48

2.50%

90.8

84.2/83.3

-0.51%

29.88
29.88

91.2
5.65%
6.26% 91.2

85.3/84.3
85.0/84.3

5.30%
4.65%

0.80
0.71

0.71
0.67
0.68

0.65

0.68
0.68

2.16
2.28

2.25
2.03
2.03

2.19

2.05
2.04

Table 3: Effects on ﬁne-tuning the BERT-base model (BERTBASE). ‘*’ indicates using extra ﬁne-tuning strategies and data preprocessing. ‘/’
means no available reported result. We implemented a “BERTBASE” without any extra ﬁne-tuning strategy as our baseline. “BERTVOTE”
and “BERTAVG” means ensemble BERT (See section 3.1). “BERTSE” means self-ensemble BERT (See section 3.2). “BERTSDV” and
“BERTSDA” means self-distillation BERT (See section 3.3). ‘Avg. ∆’ means the average of relative change, respectively. We bold the better
self-distillation results.

siﬁcation) and 5.50% (for NLI), while BERTAVG follows
closely with 4.07% and 3.24%. BERTVOTE outperforms
BERTAVG on all tasks, which adheres to our intuition since
BERTVOTE is more complicated.

The self-ensemble BERT (BERTSE) has a slight improve-
ment in classiﬁcation tasks of 2.50%, but it does not work on
NLI tasks. This is also a reason why we need self-distillation
to improve the base models.

Overall, self-distillation model has signiﬁcant improvement
on both classiﬁcation and NLI tasks. Table 3 shows that
BERTSDA and BERTSDV outperform BERTBASE on all
datasets. Generally speaking, BERTSDA performs better than
BERTSDV on text classiﬁcation tasks with the improvement
of 6.26% vs. 5.65%, but the latter performs better on NLI
tasks (BERTSDA vs. BERTSDV is 4.65% vs. 5.30%).

Our proposed ﬁne-tuning strategies also outperform the pre-
vious method in [Sun et al., 2019] on text classiﬁcation tasks,
which makes extensive efforts to ﬁnd sophisticated hyperpa-
rameters.

Model

IMDb

Avg. ∆ SNLI ∆

AG’s
News

MT-DNN [Liu et al., 2019a]
BERT-L
(our implementation)

/

/

4.98

5.45

/

-

91.6

90.9

/

-

BERT-LSDA(K = 1)
BERT-LSDA(K = T − 1)

4.66
4.58

5.21 5.62% 91.5 6.59%
5.15 7.02% 91.4 5.49%

Table 4: Effects on ﬁne-tuning the BERT-large model (BERT-L).
For IMDb and AG’s News, we report test error rate (%). For SNLI,
we report accuracy (%). MT-DNN ﬁne-tunes BERT with multi-task
learning.

Effects on Fine-tuning BERT-Large We also investigate
whether self-distillation has similar ﬁndings for the BERT-
large model (BERT-L), which contains 24 Transformer layers.
Due to the limitation of our devices, we only conduct an exper-
iment on two text classiﬁcation datasets and one NLI datasets

and evaluate strategy BERTSDA, namely self-distillation with
averaged BERT as a teacher. We set two different teacher sizes
for comparison. As shown in Table 4, self-distillation also
gets a signiﬁcant gain while ﬁne-tuning the BERT-large model.
On two text classiﬁcation tasks, BERT-LSDA(K = T − 1)
gives better results and the average improvement is 7.02%.
For NLI task, BERT-LSDA(K = 1) gives better result and
the improvement is 6.59%.

Moreover, although our self-distillation ﬁne-tuning strategy
does not leverage the external data or knowledge, it also gives
a comparable performance of MT-DNN [Liu et al., 2019a],
which ﬁne-tunes BERT with a speciﬁc projection layer under
the multi-task learning framework.

Discussion In general, BERTSDA has a similar phe-
nomenon compared to BERTSDV, while having better
computational and memory efﬁciency. Considering that
BERT-LSDA(K = 1) is same as BERT-LSDV(K = 1), and
it performs better than BERT-LSDA(K = T − 1). The SDA
models are generally worse than SDV models on NLI tasks.
All the above illustrates that parameter averaging is worse than
logits voting when dealing with difﬁcult tasks such as NLI,
but better on simple tasks such as text classiﬁcation.

5 Conclusion
In this paper, we propose simple but effective ﬁne-tuning strate-
gies for BERT without external knowledge or data. Speciﬁ-
cally, we introduce two mechanisms: self-ensemble and self-
distillation. The self-ensemble method with parameter av-
eraging can improve BERT without signiﬁcantly decreasing
the training efﬁciency. With self-distillation, the student and
teacher models can beneﬁt from each other. Our proposed
strategies are orthogonal to the approaches with external data
and knowledge. Therefore, we believe that our strategies can
be further boosted by more sophisticated hyperparameters and
data augmentation.

In future, we will investigate a better ﬁne-tuning strategy
by integrating our proposed method into an optimization algo-
rithm.

ing. SIAM Journal on Control and Optimization, 30(4):838–
855, 1992.

[Raffel et al., 2019] Colin Raffel, Noam Shazeer, Adam
Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits
of transfer learning with a uniﬁed text-to-text transformer.
arXiv e-prints, 2019.

[Rajpurkar et al., 2016] Pranav Rajpurkar, Jian Zhang, Kon-
stantin Lopyrev, and Percy Liang. Squad: 100,000+ ques-
tions for machine comprehension of text. arXiv preprint
arXiv:1606.05250, 2016.

[Sanh et al., 2019] Victor Sanh, Lysandre Debut, Julien Chau-
mond, and Thomas Wolf. DistilBERT, a distilled version of
BERT: smaller, faster, cheaper and lighter. arXiv preprint
arXiv:1910.01108, 2019.

[Stickland and Murray, 2019] Asa Cooper Stickland and Iain
Murray. BERT and PALs: Projected attention layers for
efﬁcient adaptation in multi-task learning. arXiv preprint
arXiv:1902.02671, 2019.

[Sun et al., 2019] Chi Sun, Xipeng Qiu, Yige Xu, and Xuan-
jing Huang. How to ﬁne-tune BERT for text classiﬁcation?
arXiv preprint arXiv:1905.05583, 2019.

[Tarvainen and Valpola, 2017] Antti Tarvainen and Harri
Valpola. Mean teachers are better role models: Weight-
averaged consistency targets improve semi-supervised deep
learning results. In Advances in neural information pro-
cessing systems, pages 1195–1204, 2017.

[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you
need. In Advances in Neural Information Processing Sys-
tems, pages 5998–6008, 2017.

[Williams et al., 2018] Adina Williams, Nikita Nangia, and
Samuel Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In Proceedings
of the Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, pages 1112–1122, 2018.

[Yang et al., 2019] Zhilin Yang, Zihang Dai, Yiming Yang,
Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.
XLNet: Generalized autoregressive pretraining for lan-
guage understanding. In Advances in Neural Information
Processing Systems 32, pages 5754–5764, 2019.

[Zhang et al., 2015] Xiang Zhang, Junbo Zhao, and Yann Le-
Cun. Character-level convolutional networks for text clas-
siﬁcation. In Advances in neural information processing
systems, pages 649–657, 2015.

References
[Bowman et al., 2015] Samuel R. Bowman, Gabor Angeli,
Christopher Potts, and Christopher D. Manning. A large
annotated corpus for learning natural language inference.
In Proceedings of the Conference on Empirical Methods in
Natural Language Processing, 2015.

[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-
ton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding.
arXiv preprint arXiv:1810.04805, 2018.

[Dong et al., 2019] Li Dong, Nan Yang, Wenhui Wang, Furu
Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon. Uniﬁed language model pre-training
for natural language understanding and generation. arXiv
preprint arXiv:1905.03197, 2019.

[Hinton et al., 2015] Geoffrey Hinton, Oriol Vinyals, and Jeff
Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.

[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanis-
law Jastrzebski, Bruna Morrone, Quentin De Larous-
silhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
Gelly. Parameter-efﬁcient transfer learning for NLP. arXiv
preprint arXiv:1902.00751, 2019.

[Howard and Ruder, 2018] Jeremy Howard and Sebastian
Ruder. Universal language model ﬁne-tuning for text clas-
siﬁcation. arXiv preprint arXiv:1801.06146, 2018.

[Jiao et al., 2019] Xiaoqi Jiao, Yichun Yin, Lifeng Shang,
Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun
Liu. TinyBERT: Distilling BERT for natural language un-
derstanding. arXiv preprint arXiv:1909.10351, 2019.
[Laine and Aila, 2016] Samuli Laine and Timo Aila. Tempo-
ral ensembling for semi-supervised learning. arXiv preprint
arXiv:1610.02242, 2016.

[Liu et al., 2019a] Xiaodong Liu, Pengcheng He, Weizhu
Chen, and Jianfeng Gao. Multi-task deep neural net-
works for natural language understanding. arXiv preprint
arXiv:1901.11504, 2019.

[Liu et al., 2019b] Yinhan Liu, Myle Ott, Naman Goyal,
Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa:
A robustly optimized BERT pretraining approach. arXiv
preprint arXiv:1907.11692, 2019.

[Maas et al., 2011] Andrew L Maas, Raymond E Daly, Pe-
ter T Pham, Dan Huang, Andrew Y Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In
Proceedings of the annual meeting of the association for
computational linguistics: Human language technologies,
pages 142–150, 2011.

[Peters et al., 2019] Matthew Peters, Sebastian Ruder, and
Noah A Smith. To tune or not to tune? adapting pre-
trained representations to diverse tasks. arXiv preprint
arXiv:1903.05987, 2019.

[Polyak and Juditsky, 1992] Boris T Polyak and Anatoli B Ju-
ditsky. Acceleration of stochastic approximation by averag-

