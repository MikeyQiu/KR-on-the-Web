Idiom Paraphrases: Seventh Heaven vs Cloud Nine

Maria Pershina

Yifan He
Computer Science Department
New York University
New York, NY 10003, USA
{pershina,yhe,grishman}@cs.nyu.edu

Ralph Grishman

Abstract

The goal of paraphrase identiﬁcation is
to decide whether two given text frag-
ments have the same meaning. Of par-
ticular interest in this area is the identiﬁ-
cation of paraphrases among short texts,
such as SMS and Twitter.
In this paper,
we present idiomatic expressions as a new
domain for short-text paraphrase identiﬁ-
cation. We propose a technique, utiliz-
ing idiom deﬁnitions and continuous space
word representations that performs com-
petitively on a dataset of 1.4K annotated
idiom paraphrase pairs, which we make
publicly available for the research commu-
nity.

1

Introduction

The task of paraphrase identiﬁcation, i.e. ﬁnding
alternative linguistic expressions of the same or
similar meaning, attracted a great deal of attention
in the research community in recent years (Ban-
nard and Callison-Burch, 2005; Sekine, 2005;
Socher et al., 2011; Guo et al., 2013; Xu et al.,
2013; Wang et al., 2013; Zhang and Weld, 2013;
Xu et al., 2015).

This task was extensively studied in Twitter
data, where millions of user-generated tweets talk
about the same topics and thus present a nat-
ural challenge to resolve redundancy in tweets
for many applications, such as textual entailment
(Zhao et al., 2014), text summarization (Lloret et
al., 2008), ﬁrst story detection (Petrovich, 2012),
search (Zanzotto et al., 2011), question answering
(Celikyilmaz, 2010), etc.

In this paper we explore a new domain for
the task of paraphrase identiﬁcation - idiomatic
expressions, in which the goal is to determine
whether two idioms convey the same idea.

This task is related to previous short-text para-
phrase tasks, but it does not have access to many

of the information sources that can be exploited in
Twitter/short text paraphrasing: unlike tweets, id-
ioms do not have hashtags, which are very strong
topic indictors; unlike SMS, idioms do not have
timestamp or geographical metadata; and unlike
news headlines, there are no real world events that
can serve as anchors to cluster similar expressions.
In addition, an idea, or a moral of the idiom is of-
ten expressed in an indirect way, e.g. the idioms

(1) make a mountain out of a molehill
(2) tempest in a teapot

convey similar ideas1:

(1) If somebody makes a mountain out of a
molehill they exaggerate the importance or
seriousness of a problem.
(2) If people exaggerate the seriousness of a
situation or problem they are making a tempest in
a teapot.

There is a line of research focused on extract-
ing idioms from the text or identifying whether
a particular expression is idiomatic (or a non-
compositional multi-word expression)
(Muzny
and Zettlemoyer, 2013; Shutova et al., 2010; Li
and Sporleder, 2009; Gedigian et al., 2006; Katz
and Giesbrecht, 2006). Without linguistic sources
such as Wiktionary, usingenglish.com, etc, it is
often hard to understand what the meaning of a
particular idiom is.
It is even harder to deter-
mine whether two idioms convey the same idea
or ﬁnd alternative idiomatic expressions. Using
idiom deﬁnitions, given by linguistic resources,
one can view this problem as identifying para-
phrases between deﬁnitions and thus deciding on
paraphrases between corresponding idioms. Efﬁ-
cient techniques for identifying idiom paraphrases
would complement any paraphrase identiﬁcation
system, and thus improve the downstream appli-
cations, such as question answering, summariza-

1Deﬁnitions

these
http://www.usingenglish.com

of

idioms

are

taken

from

tion, opinion mining, information extraction, and
machine translation.

To the best of our knowledge we are the ﬁrst to
address the problem of determining whether two
idioms convey the same idea, and to propose a new
scheme that utilizes idiom deﬁnitions and continu-
ous space word representation (word embedding)
to solve it. By linking word- and sentence-level
semantics our technique outperforms state-of-the-
art paraphrasing approaches on a dataset of 1.4K
annotated idiom pairs that we make publicly avail-
able.

2 Related Work

There is no strict deﬁnition of a paraphrase (Bha-
gat and Hovy, 2013) and in linguistic literature
paraphrases are most often characterized by an
approximate equivalence of meanings across sen-
tences or phrases.

A growing body of research investigates ways
of paraphrase detection in both supervised (Qiu
et al., 2006; Wan et al., 2006; Das and Smith,
2009; Socher et al., 2011; Blacoe and Lapata,
2012; Madnani and Tetreault, 2012; Ji and Eisen-
stein, 2013) and unsupervised settings (Bannard
and Callison-Burch, 2005; Mihalcea et al., 2006;
Rus et al., 2008; Fernando and Stevenson, 2008;
Islam and Inkpen, 2007; Hassan and Mihalcea,
2011). These methods mainly work on large scale
news data. News data is very different from ours
in two aspects: most news text can be interpreted
literally and similar news events (passing a legis-
lation, death of a person, elections) happen repeat-
edly. Therefore, lexical anchors or event anchors
can work well on news text, but not necessarily on
our task.

Millions of tweets generated by Twitter users
every day provide plenty of paraphrase data for
NLP research. An increasing interest in this prob-
lem led to the Paraphrase and Semantic Similar-
ity In Twitter (PIT) task in SemEval-2015 com-
petition (Xu et al., 2015). Existing bias towards
Twitter paraphrases results in sophisticated sys-
tems that exploit character level similarity or meta-
data. But models relying on these insights are
not necessarily applicable to other domains where
misspellings are rare, or metadata is not available.
Idiomatic expressions constitute an essential
part of modern English. They often behave id-
iosyncratically and are therefore a signiﬁcant chal-
lenge for natural language processing systems.

Recognizing when two idiomatic expressions con-
vey similar ideas is crucial to recognizing the sen-
timent of the author, identifying correct triggers
for events, and to translating the idiom properly.
However, although there are several existing mod-
els to identify paraphrases in short text, idioms
have very different characteristics from the data
that those models are built on. In this paper, we
experiment with two state-of-the-art paraphrasing
models that are outperformed on our dataset of id-
iomatic expressions by a simple technique, raising
a question on how well existing paraphrase models
generalize to new data.

3 The Challenge

Identifying idiom paraphrases is an interesting and
challenging problem. Lexical similarity is not a
reliable clue to ﬁnd similar idioms. Some idioms
look very similar, differ in only one or two words,
and convey the same idea. For example, “like
two peas in a pod” vs “like peas in a pod” (“if
people or things are like peas in a pod they look
identical”), but other idioms that look similar can
have very different meaning, e.g. “well oiled” vs
“well oiled machine” (“if someone is well oiled
they have drunk a lot” vs “something that func-
tions very well is a well oiled machine”).

Finally, there are idioms that do not have any
words in common at all and may seem quite differ-
ent for a person not familiar with idiomatic expres-
sions, but still have similar meaning. For exam-
ple, “cross swords” vs “lock horns” (“when peo-
ple cross swords they argue or dispute” vs “when
people lock horns they argue or ﬁght about some-
thing”). Thus, a natural way to identify idiom
paraphrases is to focus on idiom deﬁnitions that
explain meaning of an idiom in a clear and con-
cise way.

4 Lexical vs Semantic Similarities

Our dataset consists of pairs (cid:104)idiom, def inition(cid:105).
We use two types of similarity measures to com-
pute how similar deﬁnitions of different idioms
the lexical similarity is based on a lexical
are:
(word) overlap between two deﬁnitions, and the
semantic similarity captures the overall semantic
meaning of the whole sentence.
Lexical similarity. We compute cosine similarity
between vectors −→v d1 and −→v d2, representing id-
iom descriptions d1 and d2 and weight each word

in these vectors by its tf-idf score:

lexSim(d1, d2) = cosine(−→v d1, −→v d2),
(1)
where −→v d is a |V |-dimensional vector with V
being the vocabulary of all deﬁnition words.

Semantic similarity. To capture the overall mean-
ing of the deﬁnitions d we combine word embed-
dings (Collobert et al., 2011; Turian et al., 2010)
for all words in d using two combination schemes:

• Averaged sum:

−−−−−−→
averagedd =

1
|d|

(cid:88)

−−→
emb(word)

(2)

word∈d

• Weighted sum:
−−−−−−→
weightedd =
1

(cid:80)
word∈d

tﬁdf word

(3)

(cid:88)

tﬁdf word ·

−−→
emb(word)

word∈d

Then semantic similarity is measured as

semSim(d1,d2) = cosine(combd1,combd2)

(4)

−−−→
combd is a 100-dimensional vector com-
where
−−→
emb(word) (Turian
bined from word embeddings
et al., 2010) for words in description d using ei-
ther averaged (2) or weighted (3) combination
schemes. 2

4.1

IdiomSim

There is a tradeoff between the two similarity mea-
sures lexSim and semSim (Section 4): while the
ﬁrst one captures the actual lexical overlap, the
second one can better capture the closeness in se-
mantic meaning. To ﬁnd an optimal balance be-
tween the two we consider their weighted sum

IdiomSim(d1, d2) =

(5)

(1 − α) · lexSim(d1, d2) + α · semSim(d1, d2)
and decide on an α by optimizing for a maximal
F-score on a development dataset.

5 Experiments

We

collected 2,432 idioms

Data.
from
http://www.usingenglish.com, a site for English
learners, where every idiom has a unique de-
scription giving a clear explanation of the idiom’s
meaning. As opposed to tweets there are no hash-
tags, no topics or trends, no timestamps, or any
other default evidence, that two idioms may con-
vey similar ideas. Thus it becomes a challenging

Figure 1: Comparison of IdiomSim with baselines
CosSim, LexicalSim, and state-of-the-art para-
phrasing models: ASOBEK, WTMF.

task itself to construct a dataset of pairs that is
guaranteed to have a certain fraction of true para-
phrases.

We used a simple cosine similarity between all
possible idiom deﬁnitions pairs to have a ranked
list and labeled the top 1.5K pairs. Three annota-
tors were asked to label each pair of idiom deﬁ-
nitions as “similar” (score 2), “have something in
common” (score 1), “not similar” (score 0). 0.1K
pairs received a total score of 4 (either 2+2+0, or
2+1+1), and were further removed as debatable.
The rest of the labeled pairs were randomly split
into 1K for test data and 0.4K for development.
Only pairs that received a total score of 5 or higher
were considered as positive examples. There are
364 and 96 true paraphrases in our test and devel-
opment sets respectively. 3

Baselines. Our baselines are simple and tf-idf
weighted cosine similarity between idiom descrip-
tion sentences: CosSim and LexicalSim.

We compare our method with the deterministic
state-of-the-art ASOBEK model (Eyecioglu and

2We use 100-dimensional Turian word embeddings avail-
at http://metaoptimize.com/projects/

3https://github.com/masha-p/Idiom_

able
wordreprs/

Paraphrases

Idioms
seventh heaven
cloud nine
face only a mother could love when someone has a face only a mother could love they are ugly
stop a clock
take your medicine

Descriptions
if you are in seventh heaven you are extremely happy
if you are on cloud nine you are extremely happy

face the music

well oiled
drunk as a lord
cheap as chips
to be dog cheap
great minds think alike
on the same wavelength

could eat a horse
hungry as a bear
cross swords
lock horns
talk the hind legs off a donkey

talk the legs off an iron pot

a face that could stop a clock is very ugly indeed
if you take your medicine you accept the consequences of something
you have done wrong
if you have to face the music you have to accept
the negative consequences of something you have done wrong
if someone is well oiled they have drunk a lot
someone who is very drunk is as drunk as a lord
if something is very inexpensive it is as cheap as chips
if something is dog cheap it is very cheap indeed
if two people have the same thought at the same time
if people are on the same wavelength they have the same ideas
and opinions about something
if you are very hungry you could eat a horse
if you are hungry as a bear it means that you are really hungry
when people cross swords they argue or dispute
when people lock horns they argue or ﬁght about something
a person who is excessively or extremely talkative
can talk the hind legs off a donkey
somebody who is excessively talkative or is especially convincing
is said to talk the legs off an iron pot

Table 1: Examples of extracted idiom paraphrases.

Keller, 2015) that was ranked ﬁrst among 19 teams
in the Paraphrase in Twitter (PIT) track on the Se-
mEval 2015 shared task (Xu et al., 2015). This
model extracts eight simple and elegant character
and word features from two sentences to train an
SVM with linear kernel. It achieves an F-score of
55.1% on our test set.4

We also compare our method with the state-
of-the-art Weighted Textual Matrix Factorization
model (WTMF) (Guo et al., 2013),5 which is
speciﬁcally developed for short sentences by mod-
eling the semantic space of words, that can be ei-
ther present or absent from the sentences (Guo and
Diab, 2012). This model achieves a maximal F-
score of 61.4% on the test set.

The state-of-the-art model for lexically diver-
gent paraphrases on Twitter (Xu et al., 2015) is
tailored for tweets and requires topic and anchor
words to be present in the sentence, which is not
applicable to idiom deﬁnitions.
Evaluation and Results. To evaluate models we

4We thank Asli Eyecioglu for running her ASOBEK

model on our test data.

5The source code for WTMF is available at http://

www.cs.columbia.edu/˜weiwei/code

plot precision-recall curves for CosSim, WTMF,
LexicalSim, and IdiomSim (for clarity we omit
curves for other models). We also compare maxi-
mal F-score for all models. We observe that simple
cosine similarity (CosSim) achieves a maximal F-
score of 53.7%, LexicalSim is a high baseline and
achieves an F-score of 63.75%. When we add av-
eraged word embeddings the maximal F-score is
64.4% (IdiomSimave). With tﬁdf weighted word
embeddings we achieve F-score of 65.9% (Idiom-
Sim). By ﬁltering out uninformative words such as
“a”, “the”, etc (12 words total) we improve the F-
score to 66.6% (IdiomSim+), outperforming state-
of-the-art paraphrase models by more than 5% ab-
solute (Figure 1). Both IdiomSim and IdiomSim+
outperform WTMF signiﬁcantly according to a
paired t-test with p less than 0.05.
Examples and Discussion. We use threshold,
corresponding to a maximal F-score obtained on
the development dataset, and explore paraphrases
from test dataset scored higher and lower than
this threshold. Examples of extracted idiom para-
phrases are in Table 1. Examples of false positives
and false negatives are in Table 2.

Simple word overlap is not a reliable clue to de-

Idioms
False positives
healthy as a horse
an apple a day keeps
the doctor away
jersey justice
justice is blind
heart of steel

heart of glass
False negatives
like a kid in a candy store

bee in your bonnet

easy as falling off a log
no sweat
hopping mad
off on one

Descriptions

if you are as healthy as a horse you are very healthy
eating healthy food keeps you healthy

jersey justice is a very severe justice
justice is blind means that justice is impartial and objective
when someone has a heart of steel they do not show emotion
or are not affected emotionally
when someone has a heart of glass they are easily affected emotionally

if someone is like a kid in a candy store
they are very excited about something
if someone is very excited about something
they have a bee in their bonnet
something very easy or simple to do is as easy as falling off a log
no sweat means something is easy
if you are hopping mad you are extremely angry
if someone goes off on one they get extremely angry indeed

Table 2: Examples of false positive and false negative paraphrases.

cide on a paraphrase between two idiom descrip-
tions. Since words are main units in the computa-
tion (5) our metric is biased towards lexical simi-
larity. Thus we get a false positive paraphrase be-
tween “healthy as a horse” and “an apple a day”.
The ﬁrst one is rather a statement about someone’s
health while the second one is an advice on how
to be healthy. Moreover, idioms “heart of steel”
vs “heart of glass” convey opposite ideas of be-
ing “not affected emotionally” vs being “easily af-
fected emotionally”. Having “heart” and “affected
emotionally” in both idiom descriptions leads to a
high cosine similarity between them and results in
a false positive decision. For the same reason lexi-
cally divergent idiom descriptions get a lower rank
while convey similar ideas, e.g. “hopping mad” vs
“off on one”.

Combining lexical and sentence similarity via
(5) performs better than lexical similarity alone
(Figure 1) but still does not capture all aspects of
a true paraphrase.

6 Conclusion and Future Work

In this paper we present a new domain for the
paraphrase identiﬁcation task: to ﬁnd paraphrases
among idiomatic expressions. We propose a sim-
ple scheme to compute the similarity of two id-
iom deﬁnitions that outperforms state-of-the-art
paraphrasing models on the dataset of idiom para-
phrases that we make publicly available.

Our future work will be focused on exploring
different strategies to compute semantic similarity
between sentences, developing a comprehensive
idiom similarity measure that will utilize both id-
ioms and their deﬁnitions, and on comparing text
with an idiom and a general text as a realistic sce-
nario for paraphrase identiﬁcation. It is a new and
a challenging task and thus opens up many oppor-
tunities for further research in paraphrase identiﬁ-
cation and all its downstream applications.

Acknowledgments

We thank Thien Huu Nguyen of New York Univer-
sity and Asli Eyecioglu of University of Sussex for
their help and advice.

References

Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. (2012). Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, German Rigau, Larraitz Uria, and Janyce
Wiebe. (2015). Semeval-2015 task 2: Semantic tex-
tual similarity, English, Spanish and Pilot on Inter-
pretability. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval).

Colin Bannard and Chris Callison-Burch.

(2005).
In
Paraphrasing with Bilingual Parallel Corpora.
Proceedings of the 43th Annual Meeting of the As-
sociation for Computational Linguistics (ACL).

Marco Baroni, Georgiana Dinu,

and German
(2014). Don’t count, predict! A
Kruszewski.
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52th Annual Meeting of the Association for
Computational Linguistics (ACL).

Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. (2013).
Linking Tweets to News: A Framework to Enrich
In Proceedings
Short Text Data in Social Media.
of the 51th Annual Meeting of the Association for
Computational Linguistics (ACL).

Samer Hassan and Rada Mihalcea.

(2011). Seman-
tic relatedness using salient semantic analysis.
In
Proceedings of the twenty-ﬁfth Association for the
Advancement of Artiﬁcial Intelligence Conference
(AAAI).

Rahul Bhagat and Eduard Hovy.

(2013). What is a
In Proceedings of the International
paraphrase?
Conference on Computational Linguistics (COL-
ING).

Aminul Islam and Diana Inkpen.

(2007). Semantic
similarity of short texts. In Proceedings of Confer-
ence on Recent Advances in Natural Language Pro-
cessing (RANLP).

Julia Birke and Anoop Sarkar. (2006). A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In Proceedings of the Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL).

William Blacoe and Mirella Lapata. (2012). A com-
parison of vector-based representations for semantic
composition. In Proceedings of EMNLP-CoLNN.

Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur.
(2010). LDA based similarity modeing for question
answering. In Proceedings of the NAACL HLT 2010
Workshop on Semantic Search.

Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa
(2011). Natural Language Processing (Almost)
from Scratch. In Journal of Machine Learning Re-
search (JMLR).

Dipanjan Das and Noah A. Smith. (2009). Paraphrase
identiﬁcation as probabilistic quasi-synchronous
In Proceedings of the Joint Confer-
recognition.
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage (ACL-IJCNLP).

Asli Eyecioglu and Bill Keller.

(2015). ASOBEK:
Twitter Paraphrase Identiﬁcation with Simple Over-
lap Features and SVMs In Proceedings of 9th In-
ternational Workshop on Semantic Evaluation (Se-
mEval).

Samuel Fernando and Mark Stevenson (2008). A se-
mantic similarity approach to paraphrase detection.
Computational Linguistics UK (CLUK) 11th Annual
Research Colloquium.

Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. (2006). Catching metaphors. In Pro-
ceedings of the Third Workshop on Scalable Natural
Language Understanding (ScaNaLU).

Yangfeng Ji and Jacob Eisenstein. (2013). Discrimi-
native improvements to distributional sentence sim-
In Proceedings of the Conference on Em-
ilarity.
pirical Methods in Natural Language Processing
(EMNLP).

Graham Katz and Eugenie Giesbrecht.

(2006). Au-
tomatic identiﬁcation of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties (MWE).

Linlin Li and Caroline Sporleder.

(2009). Classiﬁer
combination for contextual idiom detection without
labeled data. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Elena Lloret, Oscar Ferrandez, Rafael Munoz, and
Manuel Palomar.
(2008). A text summarization
approach under the inﬂuence of textual entailment.
In Proceedings of the 5th International Workshop
on Natural Language Processing and Cognitive Sci-
ence (NLPCS).

(2012).

Nitin Madnani and Joel Tetreault.

Re-
examining machine translation metrics for para-
In Proceedings of the North
phrase identiﬁcation.
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL-HLT).

Rada Mihalcea, Courtney Corley, and Strapparava.
(2006). Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of
the Association for the Advancement of Artiﬁcial In-
telligence Conference (AAAI).

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. (2013). Efﬁcient estimation of word repre-
sentations in vector space. In Proceedings of Work-
shop at the International Conference on Learning
Representations (ICLR).

Weiwei Guo and Mona Diab. (2013). Modeling Sen-
In Proceedings of the
tences in the Latent Space.
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL).

Grace Muzny and Luke Zettlemoyer.

(2013). Auto-
In Pro-
matic Idiom Identiﬁcation in Wiktionary.
ceedings of the Conference on Empirical Methods
on Natural Language Processing (EMNLP).

Wei Xu, Alan Ritter, Chris Callison-Burch, William B.
Dolan, and Yangfeng Ji. (2015). Extracting Lexi-
cally Divergent Paraphrases from Twitter. Transac-
tions of the Association for Computational Linguis-
tics (TACL).

Wei Xu, Alan Ritter, and Ralph Grishman.

(2013).
Gathering and Generating Paraphrases from Twitter
with Application to Normalization. In Proceedings
of the Sixth Workshop on Building and Using Com-
parable Corpora (BUCC).

Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Kostas Tsioutsiouliklis
(2011). Linguistic redun-
dancy in Twitter. In Proceedings of the Conference
on Empirical Methods on Natural Language Pro-
cessing (EMNLP).

Congle Zhang and Daniel S. Weld (2013). Harvest-
ing parallel news streams to generate paraphrases of
event relations. In Proceedings of the Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP).

Jiang Zhao, Man Lan, Zheng-Yu Niu, and Dong-Hong
Ji. (2014). Recognizing cross-lingual textual entail-
ment with co-training using similarity and difference
views. In Proceedings of International Joint Confer-
ence on Neural Networks (IJCNN).

Jiang Zhao, Man Lan, and Jun Feng Tian.

(2015).
ECNU: Using Traditional Similarity Measurements
and Word Embedding for Semantic Textual Similar-
In Proceedings of the 9th Interna-
ity Estimation.
tional Workshop on Semantic Evaluation (SemEval).

Sasa Petrovic, Miles Osborne, and Victor Lavrenko
(2012). Using paraphrases for improving ﬁrst story
In Proceedings of
detection in news and Twitter.
the North American Chapter of the Association for
Computational Linguistics - Human Language Tech-
nologies (NAACL-HLT).

Long Qiu, Min-Yen Kan, and Tat-Seng Chua. (2006).
Paraphrase recognition via dissimilarity signiﬁcance
classiﬁcation. In Proceedings of the Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP).

Vasile Rus, Philip M. McCarthy, Mihai C. Lintean,
Danielle S. McNamara, and Arthur C. Graesser
Paraphrase identiﬁcation with lexico-
(2008).
syntactic graph subsumption. In Proceedings of the
Twenty-First International FLAIRS Conference.

Satoshi Sekine,

(2005). Automatic paraphrase dis-
covery based on context and keywords between NE
pairs. In Proceedings of the 3rd International Work-
shop on Paraphrasing.

Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
(2002). Automatic paraphrase acquisition from
In Proceedings of the 2nd Interna-
news articles.
tional Conference on Human Language Technology
Research (HLT).

Ekaterina Shutova, Lin Sun, and Anna Korhonen.
(2010). Metaphor identiﬁcation using verb and noun
clustering. In Proceedings of the International Con-
ference on Computational Linguistics (COLING).

Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. (2011).
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Proceedings of
Advances in Neural Information Processing Systems
(NIPS).

Joseph Turian, Lev Ratinov, and Yoshua Bengio.
(2010). Word representations: A simple and general
In Proceed-
method for semi-supervised learning.
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL).

Stephen Wan, Mark Dras, Robert Dale, and Cecile
Paris. (2006). Using dependency-based features to
take the parafarce out of paraphrase. In Proceedings
of the Australasian Language Technology Workshop.

Ling Wang, Chris Dyer, Alan W Black, and Isabel
(2013). Paraphrasing 4 microblog nor-
Trancoso.
In Proceedings of the Conference on
malization.
Empirical Methods on Natural Language Process-
ing (EMNLP).

Wei Xu, Chris Callison-Burch, and William B. Dolan.
(2015). SemEval-2015 Task 1: Paraphrase and Se-
mantic Similarity in Twitter (PIT). In Proceedings
of the 9th International Workshop on Semantic Eval-
uation (SemEval).

Idiom Paraphrases: Seventh Heaven vs Cloud Nine

Maria Pershina

Yifan He
Computer Science Department
New York University
New York, NY 10003, USA
{pershina,yhe,grishman}@cs.nyu.edu

Ralph Grishman

Abstract

The goal of paraphrase identiﬁcation is
to decide whether two given text frag-
ments have the same meaning. Of par-
ticular interest in this area is the identiﬁ-
cation of paraphrases among short texts,
such as SMS and Twitter.
In this paper,
we present idiomatic expressions as a new
domain for short-text paraphrase identiﬁ-
cation. We propose a technique, utiliz-
ing idiom deﬁnitions and continuous space
word representations that performs com-
petitively on a dataset of 1.4K annotated
idiom paraphrase pairs, which we make
publicly available for the research commu-
nity.

1

Introduction

The task of paraphrase identiﬁcation, i.e. ﬁnding
alternative linguistic expressions of the same or
similar meaning, attracted a great deal of attention
in the research community in recent years (Ban-
nard and Callison-Burch, 2005; Sekine, 2005;
Socher et al., 2011; Guo et al., 2013; Xu et al.,
2013; Wang et al., 2013; Zhang and Weld, 2013;
Xu et al., 2015).

This task was extensively studied in Twitter
data, where millions of user-generated tweets talk
about the same topics and thus present a nat-
ural challenge to resolve redundancy in tweets
for many applications, such as textual entailment
(Zhao et al., 2014), text summarization (Lloret et
al., 2008), ﬁrst story detection (Petrovich, 2012),
search (Zanzotto et al., 2011), question answering
(Celikyilmaz, 2010), etc.

In this paper we explore a new domain for
the task of paraphrase identiﬁcation - idiomatic
expressions, in which the goal is to determine
whether two idioms convey the same idea.

This task is related to previous short-text para-
phrase tasks, but it does not have access to many

of the information sources that can be exploited in
Twitter/short text paraphrasing: unlike tweets, id-
ioms do not have hashtags, which are very strong
topic indictors; unlike SMS, idioms do not have
timestamp or geographical metadata; and unlike
news headlines, there are no real world events that
can serve as anchors to cluster similar expressions.
In addition, an idea, or a moral of the idiom is of-
ten expressed in an indirect way, e.g. the idioms

(1) make a mountain out of a molehill
(2) tempest in a teapot

convey similar ideas1:

(1) If somebody makes a mountain out of a
molehill they exaggerate the importance or
seriousness of a problem.
(2) If people exaggerate the seriousness of a
situation or problem they are making a tempest in
a teapot.

There is a line of research focused on extract-
ing idioms from the text or identifying whether
a particular expression is idiomatic (or a non-
compositional multi-word expression)
(Muzny
and Zettlemoyer, 2013; Shutova et al., 2010; Li
and Sporleder, 2009; Gedigian et al., 2006; Katz
and Giesbrecht, 2006). Without linguistic sources
such as Wiktionary, usingenglish.com, etc, it is
often hard to understand what the meaning of a
particular idiom is.
It is even harder to deter-
mine whether two idioms convey the same idea
or ﬁnd alternative idiomatic expressions. Using
idiom deﬁnitions, given by linguistic resources,
one can view this problem as identifying para-
phrases between deﬁnitions and thus deciding on
paraphrases between corresponding idioms. Efﬁ-
cient techniques for identifying idiom paraphrases
would complement any paraphrase identiﬁcation
system, and thus improve the downstream appli-
cations, such as question answering, summariza-

1Deﬁnitions

these
http://www.usingenglish.com

of

idioms

are

taken

from

tion, opinion mining, information extraction, and
machine translation.

To the best of our knowledge we are the ﬁrst to
address the problem of determining whether two
idioms convey the same idea, and to propose a new
scheme that utilizes idiom deﬁnitions and continu-
ous space word representation (word embedding)
to solve it. By linking word- and sentence-level
semantics our technique outperforms state-of-the-
art paraphrasing approaches on a dataset of 1.4K
annotated idiom pairs that we make publicly avail-
able.

2 Related Work

There is no strict deﬁnition of a paraphrase (Bha-
gat and Hovy, 2013) and in linguistic literature
paraphrases are most often characterized by an
approximate equivalence of meanings across sen-
tences or phrases.

A growing body of research investigates ways
of paraphrase detection in both supervised (Qiu
et al., 2006; Wan et al., 2006; Das and Smith,
2009; Socher et al., 2011; Blacoe and Lapata,
2012; Madnani and Tetreault, 2012; Ji and Eisen-
stein, 2013) and unsupervised settings (Bannard
and Callison-Burch, 2005; Mihalcea et al., 2006;
Rus et al., 2008; Fernando and Stevenson, 2008;
Islam and Inkpen, 2007; Hassan and Mihalcea,
2011). These methods mainly work on large scale
news data. News data is very different from ours
in two aspects: most news text can be interpreted
literally and similar news events (passing a legis-
lation, death of a person, elections) happen repeat-
edly. Therefore, lexical anchors or event anchors
can work well on news text, but not necessarily on
our task.

Millions of tweets generated by Twitter users
every day provide plenty of paraphrase data for
NLP research. An increasing interest in this prob-
lem led to the Paraphrase and Semantic Similar-
ity In Twitter (PIT) task in SemEval-2015 com-
petition (Xu et al., 2015). Existing bias towards
Twitter paraphrases results in sophisticated sys-
tems that exploit character level similarity or meta-
data. But models relying on these insights are
not necessarily applicable to other domains where
misspellings are rare, or metadata is not available.
Idiomatic expressions constitute an essential
part of modern English. They often behave id-
iosyncratically and are therefore a signiﬁcant chal-
lenge for natural language processing systems.

Recognizing when two idiomatic expressions con-
vey similar ideas is crucial to recognizing the sen-
timent of the author, identifying correct triggers
for events, and to translating the idiom properly.
However, although there are several existing mod-
els to identify paraphrases in short text, idioms
have very different characteristics from the data
that those models are built on. In this paper, we
experiment with two state-of-the-art paraphrasing
models that are outperformed on our dataset of id-
iomatic expressions by a simple technique, raising
a question on how well existing paraphrase models
generalize to new data.

3 The Challenge

Identifying idiom paraphrases is an interesting and
challenging problem. Lexical similarity is not a
reliable clue to ﬁnd similar idioms. Some idioms
look very similar, differ in only one or two words,
and convey the same idea. For example, “like
two peas in a pod” vs “like peas in a pod” (“if
people or things are like peas in a pod they look
identical”), but other idioms that look similar can
have very different meaning, e.g. “well oiled” vs
“well oiled machine” (“if someone is well oiled
they have drunk a lot” vs “something that func-
tions very well is a well oiled machine”).

Finally, there are idioms that do not have any
words in common at all and may seem quite differ-
ent for a person not familiar with idiomatic expres-
sions, but still have similar meaning. For exam-
ple, “cross swords” vs “lock horns” (“when peo-
ple cross swords they argue or dispute” vs “when
people lock horns they argue or ﬁght about some-
thing”). Thus, a natural way to identify idiom
paraphrases is to focus on idiom deﬁnitions that
explain meaning of an idiom in a clear and con-
cise way.

4 Lexical vs Semantic Similarities

Our dataset consists of pairs (cid:104)idiom, def inition(cid:105).
We use two types of similarity measures to com-
pute how similar deﬁnitions of different idioms
the lexical similarity is based on a lexical
are:
(word) overlap between two deﬁnitions, and the
semantic similarity captures the overall semantic
meaning of the whole sentence.
Lexical similarity. We compute cosine similarity
between vectors −→v d1 and −→v d2, representing id-
iom descriptions d1 and d2 and weight each word

in these vectors by its tf-idf score:

lexSim(d1, d2) = cosine(−→v d1, −→v d2),
(1)
where −→v d is a |V |-dimensional vector with V
being the vocabulary of all deﬁnition words.

Semantic similarity. To capture the overall mean-
ing of the deﬁnitions d we combine word embed-
dings (Collobert et al., 2011; Turian et al., 2010)
for all words in d using two combination schemes:

• Averaged sum:

−−−−−−→
averagedd =

1
|d|

(cid:88)

−−→
emb(word)

(2)

word∈d

• Weighted sum:
−−−−−−→
weightedd =
1

(cid:80)
word∈d

tﬁdf word

(3)

(cid:88)

tﬁdf word ·

−−→
emb(word)

word∈d

Then semantic similarity is measured as

semSim(d1,d2) = cosine(combd1,combd2)

(4)

−−−→
combd is a 100-dimensional vector com-
where
−−→
emb(word) (Turian
bined from word embeddings
et al., 2010) for words in description d using ei-
ther averaged (2) or weighted (3) combination
schemes. 2

4.1

IdiomSim

There is a tradeoff between the two similarity mea-
sures lexSim and semSim (Section 4): while the
ﬁrst one captures the actual lexical overlap, the
second one can better capture the closeness in se-
mantic meaning. To ﬁnd an optimal balance be-
tween the two we consider their weighted sum

IdiomSim(d1, d2) =

(5)

(1 − α) · lexSim(d1, d2) + α · semSim(d1, d2)
and decide on an α by optimizing for a maximal
F-score on a development dataset.

5 Experiments

We

collected 2,432 idioms

Data.
from
http://www.usingenglish.com, a site for English
learners, where every idiom has a unique de-
scription giving a clear explanation of the idiom’s
meaning. As opposed to tweets there are no hash-
tags, no topics or trends, no timestamps, or any
other default evidence, that two idioms may con-
vey similar ideas. Thus it becomes a challenging

Figure 1: Comparison of IdiomSim with baselines
CosSim, LexicalSim, and state-of-the-art para-
phrasing models: ASOBEK, WTMF.

task itself to construct a dataset of pairs that is
guaranteed to have a certain fraction of true para-
phrases.

We used a simple cosine similarity between all
possible idiom deﬁnitions pairs to have a ranked
list and labeled the top 1.5K pairs. Three annota-
tors were asked to label each pair of idiom deﬁ-
nitions as “similar” (score 2), “have something in
common” (score 1), “not similar” (score 0). 0.1K
pairs received a total score of 4 (either 2+2+0, or
2+1+1), and were further removed as debatable.
The rest of the labeled pairs were randomly split
into 1K for test data and 0.4K for development.
Only pairs that received a total score of 5 or higher
were considered as positive examples. There are
364 and 96 true paraphrases in our test and devel-
opment sets respectively. 3

Baselines. Our baselines are simple and tf-idf
weighted cosine similarity between idiom descrip-
tion sentences: CosSim and LexicalSim.

We compare our method with the deterministic
state-of-the-art ASOBEK model (Eyecioglu and

2We use 100-dimensional Turian word embeddings avail-
at http://metaoptimize.com/projects/

3https://github.com/masha-p/Idiom_

able
wordreprs/

Paraphrases

Idioms
seventh heaven
cloud nine
face only a mother could love when someone has a face only a mother could love they are ugly
stop a clock
take your medicine

Descriptions
if you are in seventh heaven you are extremely happy
if you are on cloud nine you are extremely happy

face the music

well oiled
drunk as a lord
cheap as chips
to be dog cheap
great minds think alike
on the same wavelength

could eat a horse
hungry as a bear
cross swords
lock horns
talk the hind legs off a donkey

talk the legs off an iron pot

a face that could stop a clock is very ugly indeed
if you take your medicine you accept the consequences of something
you have done wrong
if you have to face the music you have to accept
the negative consequences of something you have done wrong
if someone is well oiled they have drunk a lot
someone who is very drunk is as drunk as a lord
if something is very inexpensive it is as cheap as chips
if something is dog cheap it is very cheap indeed
if two people have the same thought at the same time
if people are on the same wavelength they have the same ideas
and opinions about something
if you are very hungry you could eat a horse
if you are hungry as a bear it means that you are really hungry
when people cross swords they argue or dispute
when people lock horns they argue or ﬁght about something
a person who is excessively or extremely talkative
can talk the hind legs off a donkey
somebody who is excessively talkative or is especially convincing
is said to talk the legs off an iron pot

Table 1: Examples of extracted idiom paraphrases.

Keller, 2015) that was ranked ﬁrst among 19 teams
in the Paraphrase in Twitter (PIT) track on the Se-
mEval 2015 shared task (Xu et al., 2015). This
model extracts eight simple and elegant character
and word features from two sentences to train an
SVM with linear kernel. It achieves an F-score of
55.1% on our test set.4

We also compare our method with the state-
of-the-art Weighted Textual Matrix Factorization
model (WTMF) (Guo et al., 2013),5 which is
speciﬁcally developed for short sentences by mod-
eling the semantic space of words, that can be ei-
ther present or absent from the sentences (Guo and
Diab, 2012). This model achieves a maximal F-
score of 61.4% on the test set.

The state-of-the-art model for lexically diver-
gent paraphrases on Twitter (Xu et al., 2015) is
tailored for tweets and requires topic and anchor
words to be present in the sentence, which is not
applicable to idiom deﬁnitions.
Evaluation and Results. To evaluate models we

4We thank Asli Eyecioglu for running her ASOBEK

model on our test data.

5The source code for WTMF is available at http://

www.cs.columbia.edu/˜weiwei/code

plot precision-recall curves for CosSim, WTMF,
LexicalSim, and IdiomSim (for clarity we omit
curves for other models). We also compare maxi-
mal F-score for all models. We observe that simple
cosine similarity (CosSim) achieves a maximal F-
score of 53.7%, LexicalSim is a high baseline and
achieves an F-score of 63.75%. When we add av-
eraged word embeddings the maximal F-score is
64.4% (IdiomSimave). With tﬁdf weighted word
embeddings we achieve F-score of 65.9% (Idiom-
Sim). By ﬁltering out uninformative words such as
“a”, “the”, etc (12 words total) we improve the F-
score to 66.6% (IdiomSim+), outperforming state-
of-the-art paraphrase models by more than 5% ab-
solute (Figure 1). Both IdiomSim and IdiomSim+
outperform WTMF signiﬁcantly according to a
paired t-test with p less than 0.05.
Examples and Discussion. We use threshold,
corresponding to a maximal F-score obtained on
the development dataset, and explore paraphrases
from test dataset scored higher and lower than
this threshold. Examples of extracted idiom para-
phrases are in Table 1. Examples of false positives
and false negatives are in Table 2.

Simple word overlap is not a reliable clue to de-

Idioms
False positives
healthy as a horse
an apple a day keeps
the doctor away
jersey justice
justice is blind
heart of steel

heart of glass
False negatives
like a kid in a candy store

bee in your bonnet

easy as falling off a log
no sweat
hopping mad
off on one

Descriptions

if you are as healthy as a horse you are very healthy
eating healthy food keeps you healthy

jersey justice is a very severe justice
justice is blind means that justice is impartial and objective
when someone has a heart of steel they do not show emotion
or are not affected emotionally
when someone has a heart of glass they are easily affected emotionally

if someone is like a kid in a candy store
they are very excited about something
if someone is very excited about something
they have a bee in their bonnet
something very easy or simple to do is as easy as falling off a log
no sweat means something is easy
if you are hopping mad you are extremely angry
if someone goes off on one they get extremely angry indeed

Table 2: Examples of false positive and false negative paraphrases.

cide on a paraphrase between two idiom descrip-
tions. Since words are main units in the computa-
tion (5) our metric is biased towards lexical simi-
larity. Thus we get a false positive paraphrase be-
tween “healthy as a horse” and “an apple a day”.
The ﬁrst one is rather a statement about someone’s
health while the second one is an advice on how
to be healthy. Moreover, idioms “heart of steel”
vs “heart of glass” convey opposite ideas of be-
ing “not affected emotionally” vs being “easily af-
fected emotionally”. Having “heart” and “affected
emotionally” in both idiom descriptions leads to a
high cosine similarity between them and results in
a false positive decision. For the same reason lexi-
cally divergent idiom descriptions get a lower rank
while convey similar ideas, e.g. “hopping mad” vs
“off on one”.

Combining lexical and sentence similarity via
(5) performs better than lexical similarity alone
(Figure 1) but still does not capture all aspects of
a true paraphrase.

6 Conclusion and Future Work

In this paper we present a new domain for the
paraphrase identiﬁcation task: to ﬁnd paraphrases
among idiomatic expressions. We propose a sim-
ple scheme to compute the similarity of two id-
iom deﬁnitions that outperforms state-of-the-art
paraphrasing models on the dataset of idiom para-
phrases that we make publicly available.

Our future work will be focused on exploring
different strategies to compute semantic similarity
between sentences, developing a comprehensive
idiom similarity measure that will utilize both id-
ioms and their deﬁnitions, and on comparing text
with an idiom and a general text as a realistic sce-
nario for paraphrase identiﬁcation. It is a new and
a challenging task and thus opens up many oppor-
tunities for further research in paraphrase identiﬁ-
cation and all its downstream applications.

Acknowledgments

We thank Thien Huu Nguyen of New York Univer-
sity and Asli Eyecioglu of University of Sussex for
their help and advice.

References

Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. (2012). Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).

Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, German Rigau, Larraitz Uria, and Janyce
Wiebe. (2015). Semeval-2015 task 2: Semantic tex-
tual similarity, English, Spanish and Pilot on Inter-
pretability. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval).

Colin Bannard and Chris Callison-Burch.

(2005).
In
Paraphrasing with Bilingual Parallel Corpora.
Proceedings of the 43th Annual Meeting of the As-
sociation for Computational Linguistics (ACL).

Marco Baroni, Georgiana Dinu,

and German
(2014). Don’t count, predict! A
Kruszewski.
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52th Annual Meeting of the Association for
Computational Linguistics (ACL).

Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. (2013).
Linking Tweets to News: A Framework to Enrich
In Proceedings
Short Text Data in Social Media.
of the 51th Annual Meeting of the Association for
Computational Linguistics (ACL).

Samer Hassan and Rada Mihalcea.

(2011). Seman-
tic relatedness using salient semantic analysis.
In
Proceedings of the twenty-ﬁfth Association for the
Advancement of Artiﬁcial Intelligence Conference
(AAAI).

Rahul Bhagat and Eduard Hovy.

(2013). What is a
In Proceedings of the International
paraphrase?
Conference on Computational Linguistics (COL-
ING).

Aminul Islam and Diana Inkpen.

(2007). Semantic
similarity of short texts. In Proceedings of Confer-
ence on Recent Advances in Natural Language Pro-
cessing (RANLP).

Julia Birke and Anoop Sarkar. (2006). A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In Proceedings of the Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL).

William Blacoe and Mirella Lapata. (2012). A com-
parison of vector-based representations for semantic
composition. In Proceedings of EMNLP-CoLNN.

Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur.
(2010). LDA based similarity modeing for question
answering. In Proceedings of the NAACL HLT 2010
Workshop on Semantic Search.

Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa
(2011). Natural Language Processing (Almost)
from Scratch. In Journal of Machine Learning Re-
search (JMLR).

Dipanjan Das and Noah A. Smith. (2009). Paraphrase
identiﬁcation as probabilistic quasi-synchronous
In Proceedings of the Joint Confer-
recognition.
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage (ACL-IJCNLP).

Asli Eyecioglu and Bill Keller.

(2015). ASOBEK:
Twitter Paraphrase Identiﬁcation with Simple Over-
lap Features and SVMs In Proceedings of 9th In-
ternational Workshop on Semantic Evaluation (Se-
mEval).

Samuel Fernando and Mark Stevenson (2008). A se-
mantic similarity approach to paraphrase detection.
Computational Linguistics UK (CLUK) 11th Annual
Research Colloquium.

Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. (2006). Catching metaphors. In Pro-
ceedings of the Third Workshop on Scalable Natural
Language Understanding (ScaNaLU).

Yangfeng Ji and Jacob Eisenstein. (2013). Discrimi-
native improvements to distributional sentence sim-
In Proceedings of the Conference on Em-
ilarity.
pirical Methods in Natural Language Processing
(EMNLP).

Graham Katz and Eugenie Giesbrecht.

(2006). Au-
tomatic identiﬁcation of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties (MWE).

Linlin Li and Caroline Sporleder.

(2009). Classiﬁer
combination for contextual idiom detection without
labeled data. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Elena Lloret, Oscar Ferrandez, Rafael Munoz, and
Manuel Palomar.
(2008). A text summarization
approach under the inﬂuence of textual entailment.
In Proceedings of the 5th International Workshop
on Natural Language Processing and Cognitive Sci-
ence (NLPCS).

(2012).

Nitin Madnani and Joel Tetreault.

Re-
examining machine translation metrics for para-
In Proceedings of the North
phrase identiﬁcation.
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL-HLT).

Rada Mihalcea, Courtney Corley, and Strapparava.
(2006). Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of
the Association for the Advancement of Artiﬁcial In-
telligence Conference (AAAI).

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. (2013). Efﬁcient estimation of word repre-
sentations in vector space. In Proceedings of Work-
shop at the International Conference on Learning
Representations (ICLR).

Weiwei Guo and Mona Diab. (2013). Modeling Sen-
In Proceedings of the
tences in the Latent Space.
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL).

Grace Muzny and Luke Zettlemoyer.

(2013). Auto-
In Pro-
matic Idiom Identiﬁcation in Wiktionary.
ceedings of the Conference on Empirical Methods
on Natural Language Processing (EMNLP).

Wei Xu, Alan Ritter, Chris Callison-Burch, William B.
Dolan, and Yangfeng Ji. (2015). Extracting Lexi-
cally Divergent Paraphrases from Twitter. Transac-
tions of the Association for Computational Linguis-
tics (TACL).

Wei Xu, Alan Ritter, and Ralph Grishman.

(2013).
Gathering and Generating Paraphrases from Twitter
with Application to Normalization. In Proceedings
of the Sixth Workshop on Building and Using Com-
parable Corpora (BUCC).

Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Kostas Tsioutsiouliklis
(2011). Linguistic redun-
dancy in Twitter. In Proceedings of the Conference
on Empirical Methods on Natural Language Pro-
cessing (EMNLP).

Congle Zhang and Daniel S. Weld (2013). Harvest-
ing parallel news streams to generate paraphrases of
event relations. In Proceedings of the Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP).

Jiang Zhao, Man Lan, Zheng-Yu Niu, and Dong-Hong
Ji. (2014). Recognizing cross-lingual textual entail-
ment with co-training using similarity and difference
views. In Proceedings of International Joint Confer-
ence on Neural Networks (IJCNN).

Jiang Zhao, Man Lan, and Jun Feng Tian.

(2015).
ECNU: Using Traditional Similarity Measurements
and Word Embedding for Semantic Textual Similar-
In Proceedings of the 9th Interna-
ity Estimation.
tional Workshop on Semantic Evaluation (SemEval).

Sasa Petrovic, Miles Osborne, and Victor Lavrenko
(2012). Using paraphrases for improving ﬁrst story
In Proceedings of
detection in news and Twitter.
the North American Chapter of the Association for
Computational Linguistics - Human Language Tech-
nologies (NAACL-HLT).

Long Qiu, Min-Yen Kan, and Tat-Seng Chua. (2006).
Paraphrase recognition via dissimilarity signiﬁcance
classiﬁcation. In Proceedings of the Conference on
Empirical Methods on Natural Language Process-
ing (EMNLP).

Vasile Rus, Philip M. McCarthy, Mihai C. Lintean,
Danielle S. McNamara, and Arthur C. Graesser
Paraphrase identiﬁcation with lexico-
(2008).
syntactic graph subsumption. In Proceedings of the
Twenty-First International FLAIRS Conference.

Satoshi Sekine,

(2005). Automatic paraphrase dis-
covery based on context and keywords between NE
pairs. In Proceedings of the 3rd International Work-
shop on Paraphrasing.

Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
(2002). Automatic paraphrase acquisition from
In Proceedings of the 2nd Interna-
news articles.
tional Conference on Human Language Technology
Research (HLT).

Ekaterina Shutova, Lin Sun, and Anna Korhonen.
(2010). Metaphor identiﬁcation using verb and noun
clustering. In Proceedings of the International Con-
ference on Computational Linguistics (COLING).

Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. (2011).
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Proceedings of
Advances in Neural Information Processing Systems
(NIPS).

Joseph Turian, Lev Ratinov, and Yoshua Bengio.
(2010). Word representations: A simple and general
In Proceed-
method for semi-supervised learning.
ings of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL).

Stephen Wan, Mark Dras, Robert Dale, and Cecile
Paris. (2006). Using dependency-based features to
take the parafarce out of paraphrase. In Proceedings
of the Australasian Language Technology Workshop.

Ling Wang, Chris Dyer, Alan W Black, and Isabel
(2013). Paraphrasing 4 microblog nor-
Trancoso.
In Proceedings of the Conference on
malization.
Empirical Methods on Natural Language Process-
ing (EMNLP).

Wei Xu, Chris Callison-Burch, and William B. Dolan.
(2015). SemEval-2015 Task 1: Paraphrase and Se-
mantic Similarity in Twitter (PIT). In Proceedings
of the 9th International Workshop on Semantic Eval-
uation (SemEval).

