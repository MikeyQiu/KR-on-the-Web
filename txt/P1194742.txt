6
1
0
2
 
v
o
N
 
0
1

 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
3
2
3
0
.
1
1
6
1
:
v
i
X
r
a

Policy Search with High-Dimensional Context Variables

Voot Tangkaratt1, Herke van Hoof2, Simone Parisi2,
Gerhard Neumann2, Jan Peters2, and Masashi Sugiyama3,4,1

1Department of Computer Science, The University of Tokyo
2Department of Computer Science, Technical University of Darmstadt
3Center for Advanced Integrated Intelligence Research, RIKEN
4Department of Complexity Science and Engineering, The University of Tokyo

Abstract

Direct contextual policy search methods learn to im-
prove policy parameters and simultaneously generalize
these parameters to different context or task variables.
However, learning from high-dimensional context vari-
ables, such as camera images, is still a prominent prob-
lem in many real-world tasks. A naive application of
unsupervised dimensionality reduction methods to the
context variables, such as principal component analysis,
is insufﬁcient as task-relevant input may be ignored. In
this paper, we propose a contextual policy search method
in the model-based relative entropy stochastic search
framework with integrated dimensionality reduction. We
learn a model of the reward that is locally quadratic in
both the policy parameters and the context variables.
Furthermore, we perform supervised linear dimension-
ality reduction on the context variables by nuclear norm
regularization. The experimental results show that the
proposed method outperforms naive dimensionality re-
duction via principal component analysis and a state-of-
the-art contextual policy search method.

Introduction

An autonomous agent often requires different poli-
cies for solving tasks with different contexts. For
instance,
in a ball hitting task the robot has to
adapt his controller according to the ball position,
the context. Direct policy search approaches
i.e.,
(Baxter and Bartlett, 2000; Rosenstein and Barto, 2001;
Deisenroth, Neumann, and Peters, 2013) allow the agent
to learn a separate policy for each context through
learning optimal policies
trial and error. However,
for many large contexts, such as in the presence
of continuous context variables, is impracticable. On
the other hand, direct contextual policy search ap-
proaches (Kober, Oztop, and Peters, 2011; Neumann,
2011; da Silva, Konidaris, and Barto, 2012) represent

the contexts by real-valued vectors and are able to learn
a context-dependent distribution over the policy param-
eters. Such a distribution can generalize across context
values and therefore the agent is able to adapt to unseen
contexts.

Yet, direct policy search methods (both contextual and
plain) usually require a lot of evaluations of the objective
and may converge prematurely. To alleviate these issues,
Abdolmaleki et al. (2015) recently proposed a stochas-
tic search framework called model-based relative en-
tropy stochastic search (MORE). In this framework, the
new search distribution can be computed efﬁciently in
a closed form using a learned model of the objective
function. MORE outperformed state-of-the-art methods
in stochastic optimization problems and single-context
policy search problems, but its application to contextual
policy search has not been explored yet. One of the con-
tributions in this paper is a novel contextual policy search
method in the MORE framework.

However, a naive extension of the original MORE
would still suffer
from high-dimensional contexts.
Learning from high-dimensional variables, in fact, is still
an important problem in statistics and machine learn-
ing (Bishop, 2006). Nowadays, high-dimensional data
(e.g., camera images) can often be obtained quite eas-
ily, but obtaining informative low-dimensional variables
(e.g., exact ball positions) is non-trivial and requires
prior knowledge and/or human guidance.

In this paper, we propose to handle high-dimensional
context variables by learning a low-rank representation
of the objective function. We show that learning a low-
rank representation corresponds to performing linear di-
mensionality reduction on the context variables. Since
optimization with a rank constraint is generally NP-
hard, we minimize the nuclear norm (also called trace
norm), which is a convex surrogate of the rank function
(Recht, Fazel, and Parrilo, 2010). This minimization al-
lows us to learn a low-rank representation in a fully su-
pervised manner by just solving a convex optimization
problem. We evaluate the proposed method on a syn-

1

thetic task with known ground truth and on robotic ball
hitting tasks based on camera images. The evaluation
shows that the proposed method with nuclear norm min-
imization outperforms the methods that naively perform
principal component analysis to reduce the dimensional-
ity of context variables.

Contextual Policy Search

In this section, we formulate the direct contextual policy
search problem and brieﬂy discuss existing methods.

Problem Formulation

The direct contextual policy search is formulated as fol-
Rdc
lows. An agent observes the context variable c
Rdθ from a search distribu-
and draws a parameter θ
tion π(θ
c). Subsequently, the agent executes a policy
with the parameter θ and observes a scalar reward com-
puted by a reward function R(θ, c). The goal is to ﬁnd a
c) maximizing the expected re-
search distribution π(θ
ward

∈

∈

|

|

µ(c)π(θ

c)R(θ, c)dθdc,

(1)

Z Z

|

where µ(c) denotes the context distribution. We assume
that the reward function R(θ, c) itself is unknown, but
the agent can always access the reward value.

Related Work

|

basic

contextual
policy
iteratively collects

search
direct
In
the
samples
the agent
framework,
N
(θn, cn, R(θn, cn))
n=1 using a sampling distribution
{
}
c). Subsequently, it computes a new search distribu-
q(θ
|
tion π(θ
c) such that the expected reward increases or is
maximized. In literature, different approaches have been
used to compute the new search distribution, e.g., evolu-
tionary strategies (Hansen, M¨uller, and Koumoutsakos,
2003),
algo-
expectation-maximization
rithms (Kober, Oztop, and Peters, 2011), or information
theoretic approaches (Deisenroth, Neumann, and Peters,
2013).

Most of the existing direct contextual policy search
methods focus on tasks with low-dimensional con-
text variables. To learn from high-dimensional con-
text variables, usually the problem of learning a low-
dimensional context representation is separated from
the direct policy search by preprocessing the context
space. However, unsupervised linear dimensionality re-
duction techniques are insufﬁcient in problems where
the latent representation contains distractor dimensions
that do not inﬂuence the reward. A prominent exam-
ple is principal component analysis (PCA) (Jolliffe,
1986), that does not take the supervisory signal into

account and therefore cannot discriminate between rel-
evant and irrelevant latent dimensions. On the other
hand, supervised linear dimensionality reduction tech-
niques require a suitable response variable. However,
deﬁning such a variable can be subjective. Moreover,
they often involve non-convex optimization and suffer
from local optima (Fukumizu, Bach, and Jordan, 2009;
Suzuki and Sugiyama, 2013).

In the last years, non-linear dimensionality reduc-
tion techniques based on deep learning have gained
popularity (Bengio, 2009). For instance, Watter et al.
(2015) proposed a generative deep network to learn low-
dimensional representations of images in order to cap-
ture information about the system transition dynamics
and allow optimal control problems to be solved in low-
dimensional spaces. More recently, Silver et al. (2016)
successfully trained a machine to play a high-level game
of go using a deep convolutional network. Although their
work does not directly focus on dimensionality reduc-
tion, the deep convolutional network is known to be able
to extract meaningful representation of data. Thus, the
effect of dimensionality reduction is achieved.

However, deep learning approaches generally require
large datasets that are difﬁcult to obtain in real-world
scenarios (e.g., robotics). Furthermore,
they involve
solving non-convex optimization, which can suffer from
local optima.

In this paper, we tackle the issues raised above. First,
the proposed approach integrates supervised linear di-
mensionality reduction on the context variables by learn-
ing a low-rank representation for the reward model. Sec-
ond, the problem is formalized as a convex optimization
problem and is therefore guaranteed to converge to a
global optimum.

Contextual MORE

The original MORE (Abdolmaleki et al., 2015) ﬁnds a
search distribution (without context) that maximizes the
expected reward while upper-bounding the Kullback-
Leibler (KL) divergence (Kullback and Leibler, 1951)
and lower-bounding the entropy. The KL and the en-
tropy are bounded to control the exploration-exploitation
trade-off. The key insight of MORE is to learn a reward
model to efﬁciently compute a new search distribution in
closed form. Below, we propose our method called con-
textual model-based relative entropy stochastic search
(C-MORE), which is a direct contextual policy search
method in the MORE framework.

Learning the Search Distribution

The goal of C-MORE is to ﬁnd a search distribution
c) that maximizes the expected reward while upper-
π(θ
c)
bounding the expected KL divergence between π(θ

|

|

2

c), and lower-bounding the expected entropy of

Algorithm 1: C-MORE

and q(θ
π(θ

|

|

c). Formally,

max
π

µ(c)π(θ

c)R(θ, c)dθdc,

s.t.

µ(c)π(θ

c) log

π(θ
q(θ

dθdc

c)
|
c)
|
c)dθdc

|

ǫ,

≤

β,

≥

µ(c)π(θ

c) log π(θ

µ(c)π(θ

c)dθdc = 1,

Z Z

Z Z

−

Z Z

Z Z

|

|

|

|

where the KL upper-bound ǫ and the entropy lower-
bound β are parameters speciﬁed by the user. The for-
mer is ﬁxed for the whole learning process. The lat-
ter is adaptively changed according to the percentage
of the relative difference between the sampling policy’s
expected entropy and the minimal entropy, as described
by Abdolmaleki et al. (2015), i.e.,

β = γ(E[H(q)]

H0) + H0,

−
µ(c)q(θ

−

c) log q(θ

where E[H(q)] =
c)dθdc is
the sampling policy’s expected entropy and H0 is the
minimal entropy. In the experiments, we set γ = 0.99
and H0 =
150. The above optimization problem can
be solved by the method of Lagrange multipliers1. The
solution is given by

RR

−

|

|

π(θ

c) = q(θ

c)

η
η+ω exp

|

|

R(θ, c)
η + ω

(cid:18)

exp

(cid:19)

−

(cid:18)

η + ω

γ

−
η + ω

,

(cid:19)

where η > 0 and ω > 0 are the Lagrange multipliers
obtained by minimizing the dual function

where

Input: Parameters ǫ and β, initial distribution

c)

π(θ
1 for k = 1, . . . , K do
2

for n = 1, . . . , N do

|

3

4

5

6

7

8

Observe context cn
∼
Draw parameter θn
∼
Execute task with θn and receive R(θn, cn)

µ(c)
π(θ

cn)

|

Learn the quadratic model
Solve argminη>0,ω>0 g(η, ω) using Eq. (5)
Set new search distribution π(θ

R(θ, c)

b

c) using Eq. (6)

|

Rdθ×dc, r1 ∈
Rdc×dc, D
Rdθ×dθ , B
where A
∈
∈
∈
R are the model parameters.
Rdc, and r0 ∈
Rdθ , r2 ∈
Matrices A and B are symmetric. We also assume the
c) to be Gaussian of the form
sampling distribution q(θ

|

q(θ

c) =

(θ

b + Kc, Q).

|

N

|

(4)

Under these assumptions, the dual function in Eq. (2) can
be expressed by

g(η, ω) = ηǫ

ωβ +

−

+ (η + ω) log

f ⊤F −1f

ηb⊤Q−1b

−

(cid:18)
2πF −1(η + ω)

1
2

|

η log

2πQ
|

|

| −

+

µ(c)

c⊤m +

c⊤M c

dc,

Z

(cid:18)

(cid:19)

1
2

(cid:19)

(5)

g(η, ω) = ηǫ

ωβ + (η + ω)

µ(c)

−

(cid:18)Z

log

q(θ

c)

η
η+ω exp

|

×

Z

R(θ, c)
η + ω

(cid:18)

dθ

dc.

(cid:19)

(cid:19)

(2)

Evaluating the above integral is not trivial due to the in-
η
tegration over q(θ
η+ω , that cannot be approximated
straightforwardly by sample averages. Below, we de-
scribe how to solve this issue and evaluate the dual func-
tion from data.

c)

|

Dual Function Evaluation via the Quadratic
Model

We assume that the reward function R(θ, c) can be ap-
proximated by a quadratic model

R(θ, c) = θ⊤Aθ + c⊤Bc + 2θ⊤Dc

+ θ⊤r1 + c⊤r2 + r0,

(3)

b

1All derivations are given in the supplementary material.

f = ηQ−1b + r1,
F = ηQ−1
2A,
−
m = L⊤F −1f
M = L⊤F −1L
L = ηQ−1K + 2D.

−

−

ηK ⊤Q−1b,
ηK⊤Q−1K,

Since the context distribution µ(c) is unknown, we
approximate the expectation in Eq.
(5) by sam-
ple averages. The dual function can be minimized
by standard non-linear optimization routines such as
IPOPT (W¨achter and Biegler, 2006). Finally, using
Eq. (3) and Eq. (4) the new search distribution π(θ
c)
is computed in closed form as

|

π(θ

c) =

|

N

θ

F −1f + F −1Lc, F −1(η + ω)
(cid:17)

|

(cid:16)

. (6)

To ensure that the covariance F −1(η+ω) is positive deﬁ-
nite, the model parameter A is constrained to be negative
deﬁnite. C-MORE is summarized in Algorithm 1.

3

Learning the Quadratic Model

The performance of C-MORE depends on the accu-
racy of the quadratic model. For many problems, the re-
ward function R(θ, c) is not quadratic and the quadratic
model is not suitable to approximate the entire reward
function. However, the reward function is often smooth
and it can be locally approximated by a quadratic model.
Therefore, we locally approximate the reward function
by learning a new quadratic model for each policy up-
date. The quadratic model can be learned by regression
methods such as ridge regression2 (Bishop, 2006). How-
ever, ridge regression is prone to error when the con-
text is high-dimensional. Below, we address this issue
by ﬁrstly showing that performing linear dimensionality
reduction on the context variables yields a low-rank ma-
trix of parameters. Secondly, we propose a nuclear norm
minimization approach to learn a low-rank matrix with-
out explicitly performing dimensionality reduction.

Dimensionality Reduction and Low-Rank
Representation

Linear dimensionality reduction learns a low-rank ma-
trix W and projects the data onto a lower dimensional
subspace. Performing linear dimensionality reduction
on the context variables yields the following quadratic
model

R(θ, c) = θ⊤Aθ + c⊤W ⊤
+ θ⊤r1 + c⊤W ⊤

DW c

BW c + 2θ⊤
r2 + r0,
e

e

(7)

b

e

∈

B,

D, r1,

Rdz×dc denotes a rank-dz matrix with
where W
r2 and r0
dz < dc. The model parameters A,
can be learned by ridge regression. However, the matrix
B = W ⊤
BW is low-rank, i.e., rank(B) = dz < dc.
Thus, performing linear dimensionality reduction on the
contexts makes B low-rank. Note that the rank of D =
DW depends on θ and is problem dependent. Hence,
we do not consider the rank of D for dimensionality re-
duction.
e

e

e

e

e

(PCA)

There are several

component analysis

learning. However, being unsupervised,

linear dimensionality reduction
methods that can be applied to learn W . Prin-
cipal
(Jolliffe, 1986)
is a common method used in statistics and ma-
chine
it
take the regression targets into account,
does not
i.e.,
the reward. Alternative supervised techniques,
such as KDR (Fukumizu, Bach, and Jordan, 2009)
and LSDR (Suzuki and Sugiyama, 2013), do not
quadratic
take

regression model,

i.e.,

the

the

2After learning the parameters, A is enforced to be negative deﬁnite
by truncating its positive eigenvalues. Subsequently, we re-learn the
remainder parameters. An alternative approach is projected gradient
descend, but it is more computationally demanding and requires step
size tuning.

contrary,

(Friedman and Stuetzle,

into account. On the
regression

in pro-
model,
jection
1981;
Vijayakumar and Schaal, 2000) the model parameters
and the projection matrix are learned simultaneously.
However, applying this approach to the model in Eq. (7)
requires alternately optimizing for the model parame-
ters and the projection matrix and is computationally
expensive.

In the original MORE, Bayesian dimensionality re-
duction (G¨onen, 2013) is applied to perform linear su-
pervised dimensionality reduction on θ, i.e., the algo-
rithm considers a projection W θ. The matrix W is sam-
pled from a prior distribution and the algorithm learns
the model parameters using weighted average over the
sampled W . However, for high-dimensional W , this ap-
proach requires an impractically large amount of sam-
ples W to obtain an accurate model, leading to compu-
tationally expensive updates.

Learning a Low-Rank Matrix with Nuclear
Norm Regularization

The quadratic model in Eq. (3) can be re-written as

R(x) = x⊤Hx,

where the input vector x and the parameter matrix H are
deﬁned as

b

θ
c
1 


x =



, H =

A
D⊤
0.5r⊤
1



D 0.5r1
0.5r2
B
0.5r⊤
r0
2

.






Note that H is symmetric since both A and B are sym-
metric. As discussed in the previous section, we desire
B to be low-rank. Unlike Eq. (7), we do not consider
dimensionality reduction for the linear terms in c, i.e.,
2θ⊤Dc and c⊤r2. Instead, we learn H by solving the
following convex optimization problem



min
H

[

J

(H) + λ∗

B

∗] ,

k

k

s.t. A is negative deﬁnite,

(8)

where

(H) denotes the differentiable part

J

N

n=1
X

(cid:0)

(H) =

J

1
2N

x⊤
n Hxn

−

R(θn, cn)

2

+

λ
2 k

H

2
F,

k

(cid:1)

H

tr(HH ⊤

where λ > 0 and λ∗ > 0 are regularization parame-
ters. The Frobenius norm
k · kF is deﬁned as
kF =
∗ is
). The nuclear norm of a matrix
deﬁned as the ℓ1-norm of its singular values . This op-
q
timization problem can be explained as follows. The
(H) consists of the mean squared error and the
term
(H) corre-
ℓ2-regularization term. Thus, minimizing
sponds to ridge regression. Minimizing the nuclear norm

k · k

J

J

k

4

k

B

∗ shrinks the singular values of B. Thus, the solu-
k
tion tends to have sparse singular values and to be low-
rank. The negative deﬁnite constraint further ensures that
the covariance matrix in Eq. (6) is positive deﬁnite.

b

J

(x⊤

n V xn)2

2g(t) = 1
N

The convexity of this optimization problem can be
veriﬁed by checking the following conditions. First, the
convexity of the mean squared error can be proven
following Boyd and Vandenberghe, 2004, page 74. Let
(Z + tV ) be the mean squared error and
g(t) =
Z and V are symmetric matrices. Then we have that
0. Thus, the mean
∇
squared error is convex. Since the Frobenius norm is
(H) is convex as well. Second, a set of neg-
convex,
ative deﬁnite matrices is convex since y⊤(aX + (1
−
a)Y )y < 0 for any negative deﬁnite matrices X and Y ,
1, and any vector y (Boyd and Vandenberghe,
0
2004). Third, the nuclear norm is a convex function
(Recht, Fazel, and Parrilo, 2010). Note that, since the
(H) is symmetric, H is guaranteed to be
gradient
symmetric as well given that the initial solution is also
symmetric.

∇J

P

≥

≤

≤

J

a

It is also possible to enforce the matrix H (rather
than B) to be low-rank, implying that both θ and c can
be projected onto a common low-dimensional subspace.
However, this is often not the case, and regularizing by
the nuclear norm of H did not perform well in our exper-
iments. We may also directly constrain rank(B) = dz
in Eq. (8) instead of performing nuclear norm regular-
ization. However, minimization problems with rank con-
straints are NP-hard. On the contrary, the nuclear norm is
the convex envelop of the rank function and can be opti-
mized more efﬁciently (Recht, Fazel, and Parrilo, 2010).
For this reason, the nuclear norm has been a popular
surrogate to a low-rank constraint in many applications,
such as matrix completion (Cand`es and Tao, 2010) and
multi-task learning (Pong et al., 2010).

Since the optimization problem in Eq. (8) is con-
vex, any convex optimization method can be used
(Boyd and Vandenberghe, 2004). For our experiments,
we use the accelerated proximal gradient descend
(APG) (Toh and Yun, 2009). The pseudocode of our im-
plementation of APG for solving Eq. (8) is given in the
supplementary material. Note that APG requires com-
puting the SVD of the matrix B. Since computing the
exact SVD of a high-dimensional matrix can be compu-
tationally expensive, we approximate it by randomized
SVD (Halko, Martinsson, and Tropp, 2011).

Experiments

We evaluate the proposed method on three problems.
We start by studying C-MORE behavior in a scenario
where we know the true reward model and the true low-
dimensional context. Subsequently, we focus our atten-
tion on two simulated robotic ball hitting tasks. In the

ﬁrst task, a toy 2-DoF planar robot arm has to hit a ball
placed on a plane. In the second task, a simulated 6-DoF
robot arm has to hit a ball placed in a three-dimensional
space. In both cases, the robots accomplish their task by
using raw camera images as context variables. However,
in the latter case we have limited data and therefore sam-
ple efﬁciency is of primary importance.

The evaluation is performed on three different ver-
sions of C-MORE, according to the model learning ap-
proach: using only ridge regression (C-MORE Ridge),
aided by a low-dimensional context variables learned
by PCA (C-MORE Ridge+PCA) and aided by nuclear
norm regularization (C-MORE Nuc. Norm). We also use
C-REPS (Deisenroth, Neumann, and Peters, 2013) with
PCA as baseline. For the ball hitting tasks, we also tried
to preprocess the context space with an autoencoder.
However, the learned representation performed poorly,
possibly due to the limited amount of data at our dis-
posal, and therefore this method is not reported.

For each case study, ﬁrst, the experiments are pre-
sented and then the results are reported and discussed.
For additional details, we refer to the supplementary ma-
terial.

Quadratic Cost Function Optimization

In the ﬁrst experiment, we want to study the performance
of the algorithms in a setup where we are able to an-
alytically compute both the reward and the true low-
dimensional context. To this aim, we deﬁne the follow-
ing problem

R(θ, c) =

θ

(
||

−

T 1˜c
−
Rdc×dc,

||2)2,
I
∈

2 c,

IT −1
˜c =
Rd˜c×dc, d˜c < dc,

e

e

e

Rdθ ×d˜c, T 2 ∈

T 1 ∈
where I is the identity matrix,
I is a rectangular matrix
with ones in its main diagonal and zeros otherwise, ˜c
is the true low-dimensional context, and T 1 is to match
the dimension of the true context and the parameter θ in
order to compute the reward. This setup is particularly
interesting because only a subset of the observed con-
text inﬂuences the reward. First, the observed context c
is linearly transformed by T −1
2 . Subsequently, thanks to
I, only the ﬁrst d˜c elements are kept to com-
the matrix
pose the true context, while the remainder is treated as
noise. Finally, the reward is computed by linearly trans-
forming the true context by T 1.

e

Setup. We set d˜c = 3, dθ = 10, dc = 25, while the
elements of T 1, T 2 are chosen uniformly randomly in
[0, 1]. The sampling Gaussian distribution is initialized
with random mean and covariance Q = 10, 000I. For
learning, we collect 35 new samples and keeps track
of the samples collected during the last 20 iterations to
stabilize the policy update. The evaluation is performed

5

C-MORE Nuc. Norm
C-MORE Ridge+PCA
C-MORE Ridge
C-REPS PCA

10

0

10

−

d
r
a
w
e
R

-
g
o
L
e
g
a
r
e
v
A

7

5

3

1

1

d
r
a
w
e
R
e
g
a
r
e
v
A

−

∼ 76% Hit Rate

∼ 53% Hit Rate
∼ 18% Hit Rate

∼ 11% Hit Rate

20

40

60
Iteration

80

100

200

400

600

800

1,000

Iteration

Figure 1: Average reward for the quadratic cost func-
tion problem. Shaded area denotes standard deviation
(results are averaged over ten trials). Only C-MORE
Nuc. Norm converges within 100 iterations to an almost
optimal policy.

Figure 2: Averaged reward for the 2-DoF hitting task. C-
REPS outperforms C-MORE early on. However, it prema-
turely converges to suboptimal solutions, while C-MORE
continues to improve and soon outperforms C-REPS.

at each iteration over 1,000 contexts. Each context el-
ement is drawn from a uniform random distribution in
10, 10]. Since we can generate a large amount of data
[
−
in this setting, we pre-train PCA using 10,000 random
context samples and ﬁxed the dimensionality to dz = 20
(chosen by cross-validation). The learning is performed
for a maximum of 100 iterations. If the KL divergence
is lower than 0.1, then the learning is considered to be
converged and the policy is not updated anymore.

Results. As shown in Figure 1 , C-MORE Nuc. Norm
clearly outperforms all the competitors, learning an al-
most optimal policy and being the only one to converge
within the maximum number of iterations. It is also the
only algorithm correctly learning the true context di-
mensionality, as nuclear norm successfully regularizes
B to have rank three. On the contrary, PCA does not
help C-MORE much and yields only slightly better re-
sults than plain ridge regression. PCA cannot in fact de-
termine task-relevant dimensions as non-relevant dimen-
sions have equally-high variance.

Ball Hitting with a 2-DoF Robot Arm

In this task, a simulated
planar robot arm (shown
aside) has to hit a green
virtual ball placed on RGB
camera images of
size
24. The context is
32
deﬁned by the observed
pixels, for a total of 2304
context
variables. The
ball is randomly and uni-
formly placed in the robot
workspace. Noise drawn

×

6

Figure 5: 2-DoF hitting
task. The context observed
by the robot (blue and red
lines) consists of a virtual
green ball and the back-
ground image.

−

from a uniform random
30, 30]
distribution in [
is also added to the context, to simulate different light
conditions. The robot controls the joint accelerations at
each time step by a linear-in-parameter controller with
Gaussian basis functions, for a total of 32 parameters
θ to be learned. The reward R(θ, c) is the negative
cumulative joint accelerations plus the negative distance
between the end-effector and the ball at the ﬁnal time
step.

−

Setup. For learning, the agent collects 50 samples at
each iteration and keeps samples from the last four pre-
vious iterations. The evaluation is performed at each it-
eration over 500 contexts. Pixel values are normalized
1, 1]. The sampling Gaussian distribution is ini-
in [
tialized with random mean and identity covariance. For
both C-MORE Nuc. Norm and C-MORE PCA, we per-
form 5-fold cross-validation every 100 policy updates to
choose the values of λ∗ and dz, respectively, based on the
mean squared error between the collected returns and the
model-predicted ones. For C-REPS PCA, we tried differ-
and selected dz = 10
ent values of dz
which gave the best result.

10, 20, 30, 40

∈ {

}

Results. Figure 2 shows the averaged reward against
the number of iterations. Once again, C-MORE aided by
nuclear norm regularization performs the best, achieving
the highest average reward. At the 1000th iteration, the
learned controller hits the ball with 76% accuracy. The
rank of its learned matrix B is approximately 31, which
shows that the algorithm successfully learns a low-rank
model representation. On the contrary, preprocessing the
context space through PCA still helps C-MORE (the
rank of its learned B is approximately 25), but yields
poor results for C-REPS, which suffers of premature

d
r
a
w
e
R
e
g
a
r
e
v
A

5

4

3

2

∼ 80% Hit Rate

∼ 60% Hit Rate

C-MORE Nuc. Norm
C-MORE Ridge+PCA

100

200

300

400

500

Iteration

Figure 4: 6-DoF hitting task results (averaged over
three trials). Nuclear norm regularization outper-
forms PCA, both in terms of reward and accuracy.

performs PCA. At the 500th iteration, the learned con-
troller hits the ball with 80% accuracy. Considering that
the robot is not able to hit the ball in some contexts and
can achieve a maximum accuracy of 90%, this accuracy
is impressive for the task. The averaged rank of matrix B
learned by the nuclear norm approach is approximately
25, which shows that minimizing the nuclear norm suc-
cessfully learns a low-rank matrix. For PCA, the aver-
aged rank of B is approximately 30.

Conclusion

Learning with high-dimensional context variables is a
challenging and prominent problem in machine learn-
ing. In this paper, we proposed C-MORE, a novel con-
textual policy search method with integrated dimension-
ality reduction. C-MORE learns a reward model that is
locally quadratic in the policy parameters and the con-
text variables. By enforcing the model representation to
be low-rank, we perform supervised linear dimensional-
ity reduction. Unlike existing techniques relying on non-
convex formulations, the nuclear norm allows us to learn
the low-rank representation by solving a convex opti-
mization problem, thus guaranteeing convergence to a
global optimum. The main disadvantage of the proposed
method is that it demands more computation time due
to the nuclear norm regularization. Although we did not
encounter severe problems in our experiments, for very
large dimensional tasks this issue can be mitigated by
using more efﬁcient techniques, such as active subspace
selection (Hsieh and Olsen, 2014).

In this paper, we only focused on linear dimensional-
ity reduction techniques. Recently, non-linear techniques
based on deep network has been showing impressive per-
formance (Bengio, 2009; Watter et al., 2015). In future
work, we will incorporate deep network into C-MORE,
e.g., by using a deep convolutional network to represent
the reward model.

(a)

(b)

Figure 3: The 6-DoF robot as seen from the camera (Figure 3a,
bottom right) and in simulation (Figure 3b). The goal is to con-
trol the robot to hit the real green ball according to camera im-
ages, resized to 32

24.

×

convergence.

Ball Hitting with a 6-DoF Robot Arm

×

×

Similarly to the previous task, here a 6-DoF robotic arm
has to hit a ball placed on a three-dimensional space,
as shown in Figure 3. The context is once again de-
ﬁned by the vectorized pixels of RGB images of size
24, for a total of 2304 context variables. Note that
32
Figure 3a shows an image before we rescale it to size
24. However, unlike the 2-DoF task, the ball is di-
32
rectly recorded by a real camera placed near the phys-
ical robot, and it is not virtually generated on the im-
ages. Furthermore, the robot is controlled by dynamic
motor primitives (Ijspeert, Nakanishi, and Schaal, 2002)
(DMPs), which are non-linear dynamical systems. We
use one DMP per joint, with ﬁve basis functions per
DMP. We also learn the goal attractor of the DMPs, for
a total of 36 parameters θ to be learned. The reward
R(θ, c) is computed as the negative cumulative joint
accelerations and minimum distance between the end-
effector and the ball as well.

−

Setup. The image dataset is collected by taking pic-
tures with the ball placed at 50 different positions. To
increase the number of data, we add a uniform random
30, 30] to the context to simulate different
noise in [
light conditions. Therefore, although some samples de-
termine the same ball position, they are considered dif-
ferent due to the added noise. The search distribution is
initialized by imitation learning using 50 demonstration
samples. For learning, the agent collects 50 samples at
each iteration and always keeps samples from the last
four previous iterations.

Results. We only evaluate C-MORE with nuclear
norm and PCA since they performed well in the previous
evaluation. Figure 4 shows that nuclear norm again out-

7

References

Abdolmaleki, A.; Lioutikov, R.; Peters, J.; Lau, N.; Reis,
L. P.; and Neumann, G. 2015. Model-based relative
entropy stochastic search. In Advances in Neural In-
formation Processing Systems 28, 3537–3545.

Baxter, J., and Bartlett, P. L. 2000. Reinforcement learn-
In The
ing in POMDP’s via direct gradient ascent.
17th International Conference on Machine Learning,
41–48.

Bengio, Y. 2009. Learning deep architectures for AI.
Foundations and Trends in Machine Learning 2(1):1–
127.

Bishop, C. M. 2006. Pattern Recognition and Machine
Learning (Information Science and Statistics). Secau-
cus, NJ, USA: Springer-Verlag New York, Inc.

Boyd, S., and Vandenberghe, L. 2004. Convex Opti-
mization. New York, NY, USA: Cambridge University
Press.

Cand`es, E. J., and Tao, T.

The power
of convex relaxation: near-optimal matrix comple-
IEEE Transactions on Information Theory
tion.
56(5):2053–2080.

2010.

da Silva, B. C.; Konidaris, G.; and Barto, A. G. 2012.
In The 29th Interna-

Learning parameterized skills.
tional Conference on Machine Learning.

Deisenroth, M. P.; Neumann, G.; and Peters, J. 2013. A
survey on policy search for robotics. Foundations and
Trends in Robotics 2(1-2):1–142.

Friedman, J. H., and Stuetzle, W. 1981. Projection pur-
suit regression. Journal of the American Statistical
Association 76(376):817–823.

Fukumizu, K.; Bach, F. R.; and Jordan, M. I. 2009. Ker-
nel dimension reduction in regression. The Annals of
Statistics 37(4):1871–1905.

G¨onen, M. 2013. Bayesian supervised dimensional-
IEEE Transanscation on Cybernetics

ity reduction.
43(6):2179–2189.

Halko, N.; Martinsson, P.; and Tropp, J. A.

2011.
Finding structure with randomness: Probabilistic al-
gorithms for constructing approximate matrix decom-
positions. SIAM Review 53(2):217–288.

Hsieh, C., and Olsen, P. A. 2014. Nuclear norm min-
imization via active subspace selection. In The 31st
International Conference on Machine Learning, 575–
583.

Ijspeert, A. J.; Nakanishi, J.; and Schaal, S. 2002. Learn-
ing attractor landscapes for learning motor primitives.
In Advances in Neural Information Processing Sys-
tems 15, 1523–1530.

Jolliffe, I. T. 1986. Principal Component Analysis.

Springer Verlag.

Kober, J.; Oztop, E.; and Peters, J. 2011. Reinforcement
learning to adjust robot movements to new situations.
In The 22nd International Joint Conference on Artiﬁ-
cial Intelligence, 2650–2655.

Kullback, S., and Leibler, R. A. 1951. On information
and sufﬁciency. The Annals of Mathematical Statistics
22(1):79–86.

Neumann, G. 2011. Variational inference for policy
In The 28th Interna-

search in changing situations.
tional Conference on Machine Learning, 817–824.

Pong, T. K.; Tseng, P.; Ji, S.; and Ye, J. 2010. Trace
norm regularization: Reformulations, algorithms, and
multi-task learning. SIAM Journal on Optimization
20(6):3465–3489.

Recht, B.; Fazel, M.; and Parrilo, P. A. 2010. Guaran-
teed minimum-rank solutions of linear matrix equa-
tions via nuclear norm minimization. SIAM Review
52(3):471–501.

Rosenstein, M. T., and Barto, A. G.

2001. Robot
weightlifting by direct policy search. In The 17th In-
ternational Joint Conference on Artiﬁcial Intelligence,
839–846.

Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.;
Sifre, L.; van den Driessche, G.; Schrittwieser, J.;
Antonoglou, I.; Panneershelvam, V.; Lanctot, M.;
Dieleman, S.; Grewe, D.; Nham, J.; Kalchbrenner, N.;
Sutskever, I.; Lillicrap, T.; Leach, M.; Kavukcuoglu,
K.; Graepel, T.; and Hassabis, D. 2016. Mastering the
game of go with deep neural networks and tree search.
Nature 529:484–503.

Suzuki, T., and Sugiyama, M. 2013. Sufﬁcient dimen-
sion reduction via squared-loss mutual information es-
timation. Neural Computation 25:725–758.

Hansen, N.; M¨uller, S. D.; and Koumoutsakos, P. 2003.
Reducing the time complexity of the derandomized
evolution strategy with covariance matrix adaptation
(CMA-ES). Evolutionary Computation 11(1):1–18.

Toh, K., and Yun, S. 2009. An accelerated proximal
gradient algorithm for nuclear norm regularized least
In International Symposium on
squares problems.
Mathematical Programming.

8

Vijayakumar, S., and Schaal, S. 2000. Locally weighted
projection regression: Incremental real time learning
in high dimensional space. In The 17th International
Conference on Machine Learning, 1079–1086.

W¨achter, A., and Biegler, L. T. 2006. On the implemen-
tation of an interior-point ﬁlter line-search algorithm
for large-scale nonlinear programming. Mathametical
Programming 106(1):25–57.

Watter, M.; Springenberg, J. T.; Boedecker, J.; and Ried-
miller, M. A. 2015. Embed to control: A locally linear
latent dynamics model for control from raw images. In
Advances in Neural Information Processing Systems
28, 2746–2754.

9

—Supplementary Material—

Derivations of C-MORE

In this section, we derive C-MORE in details. C-MORE solves

max
π

µ(c)π(θ

c)R(θ, c)dθdc,

s.t.

µ(c)π(θ

c) log

π(θ
q(θ

c)
|
c)
|

dθdc

ǫ,

≤

µ(c)π(θ

c) log π(θ

c)dθdc

β,

|

|

≥

µ(c)π(θ

c)dθdc = 1,

Z Z

Z Z

−

Z Z

Z Z

|

|

|

by the method of Lagrange multipliers. Firstly, we write the Lagrangian
with the Lagrange
multipliers η > 0, ω > 0, and γ, which correspond to the ﬁrst, second, and third constraints,
respectively

L

(π, η, ω, γ) =

µ(c)π(θ

c)R(θ, c)dθdc + η

L

µ(c)π(θ

c) log

|

|

Z Z
+ ω

−

(cid:18)

Z Z

ǫ
(cid:18)

−

Z Z

−

π(θ
q(θ

c)
|
c)
|

µ(c)π(θ

|

dθdc

(cid:19)
c)dθdc

1

.

−

µ(c)π(θ

c) log π(θ

c)dθdc

β

+ γ

|

|

(cid:19)
(π, η, ω, γ) w.r.t. the primal variable π. The derivative of the

(cid:18)Z Z

(cid:19)

Then, we maximize the Lagrangian
Lagrangian w.r.t. π is

L

∂π

(π, η, ω, γ) =

µ(c)

R(θ, c)

(η + ω) log π(θ

c) + η log q(θ

c)

dθdc

(η + ω

γ).

L

Z Z
By setting this derivative to zero, we have

(cid:16)

−

|

|

(cid:17)

−

−

0 =

µ(c)

R(θ, c)

(η + ω) log π(θ

c) + η log q(θ

c)

dθdc

(η + ω

γ)

−

|

−

−

(η + ω) log π(θ

c) + η log q(θ

c)

(η + ω

γ).

|

(cid:17)
−

|

−

Z Z
= R(θ, c)

(cid:16)
−

This gives us

log π(θ

c) =

R(θ, c)
η + ω

+

η
η + ω

log q(θ

c)

|

−

η + ω

−
η + ω

γ

,

π(θ

c) = q(θ

c)

η+ω exp

|

R(θ, c)
η + ω

exp

η + ω

γ

−
η + ω

−

.

|

|

(cid:18)
The last exponential term in Eq. (9) is the normalization constant for the search distribution π(θ
since it does not depend on θ or c. Thus, we have

(cid:19)

(cid:19)

(cid:18)

|

(9)

c)

|

η

η + ω

γ

−
η + ω

exp

(cid:18)

η + ω

−

=

q(θ

c)

η+ω exp

η

|

Z

(cid:19)
γ = (η + ω) log

R(θ, c)
η + ω

dθ,

(cid:19)

η

c)

η+ω exp

(cid:18)
q(θ

|

R(θ, c)
η + ω

(cid:18)

dθ

.

(cid:19)

(cid:19)

(cid:18)Z

10

(The minus sign in the exponent in Eq. (9) becomes the inverse operator and cancels out). This
normalization term will be used to derive the dual function. Next, we substitute the term log π(θ
c)
back to the Lagrangian

|

(π∗, η, ω, γ) =

L

µ(c)π(θ

c)R(θ, c)dθdc

|

Z Z
η

−

ω

−

+ η

(cid:18)Z Z

(cid:18)Z Z

Z Z

µ(c)π(θ

c)

(cid:20)

R(θ, c)
η + ω
R(θ, c)
η + ω

+

+

η
η + ω
η
η + ω

|

|

log q(θ

c)

log q(θ

c)

η + ω

γ

−
η + ω

η + ω

−
η + ω

(cid:21)

γ

−

−

|

|

µ(c)π(θ

c)

(cid:20)
c) log q(θ

|

µ(c)π(θ

|

dθdc

(cid:19)

dθdc

(cid:19)
+ ηǫ

(cid:21)

1

−

(cid:19)

c)dθdc + γ

µ(c)π(θ

c)dθdc

(cid:18)Z Z

|

ωβ.

−

Most terms cancel out and we only have

(π∗, η, ω, γ) = ηǫ

ωβ

γ +

µ(c) (η + ω) dc

L

= ηǫ

ωβ +

µ(c) (η + ω

γ) dc

−

−

Z

−

−

Z
ωβ + (η + ω)

= ηǫ

−
= g(η, ω).

µ(c) log

Z

(cid:18)Z

η

q(θ

c)

η+ω exp

|

R(θ, c)
η + ω

dθ

dc

(cid:18)

(cid:19)

(cid:19)

The Lagrange multipliers η > 0 and ω > 0 are obtained by minimizing the dual function g(η, ω).
Then, the search distribution in Eq. (9) can be computed using these Lagrange multipliers.

Evaluating the Dual Function

Here, we show how to compute the new search distribution in closed form. Recall that our quadratic
model is

R(θ, c) = θ⊤Aθ + c⊤Bc + 2θ⊤Dc + θ⊤r1 + c⊤r2 + r0,

with symmetric A and B. Also recall that the sampling distribution q(θ

c) is Gaussian

b

|

q(θ

c) =

|

N

(θ
1

|

2πQ
|

|

b + Kc, Q)
1
2

exp

−

1
2

(cid:18)

=

[θ

−

(b + Kc)]⊤Q−1[θ

(b + Kc)]

.

−

(cid:19)

The quadratic model and the Gaussian distribution allow us to compute the dual function from data
as follows. Firstly, we consider the term

η

q(θ

c)

η+ω exp

|

R(θ, c)
η + ω

.

(cid:19)

(cid:18)

11

Using the Gaussian distribution q(θ

c) and replacing R(θ, c) with

R(θ, c) yield

|

η

q(θ

c)

η+ω exp

|

 

R(θ, c)
η + ω !
b
exp

(b + Kc)]⊤Q−1[θ

(b + Kc)]

1

η
2(η+ω)

η
2(η + ω)

[θ

2πQ
|

|

exp

×

(cid:18)

−

−

−
θ⊤Aθ + c⊤Bc + 2θ⊤Dc + θ⊤r1 + c⊤r2 + r0
η + ω
η
2

(cid:19)
(b + Kc)]⊤Q−1[θ

1
η + ω

exp

[θ

−

−

−

η
2(η+ω)

(cid:18)
1

2πQ
|

|
+ θ⊤Aθ + c⊤Bc + 2θ⊤Dc + θ⊤r1 + c⊤r2 + r0

(cid:16)

(cid:18)

(b + Kc)]

−

b

(cid:19)

=

=

=

exp

η
2(η+ω)

1
2(η + ω)

−

ηθ⊤Q−1θ + 2ηθ⊤Q−1b + 2ηθ⊤Q−1Kc

ηb⊤Q−1b

−

(cid:18)
ηc⊤K ⊤Q−1Kc + 2θ⊤Aθ + 2c⊤Bc + 4θ⊤Dc + 2θ⊤r1 + 2c⊤r2 + 2r0

(cid:16)

2πQ
|

|

2ηb⊤Q−1Kc

−

−

(cid:17)(cid:19)

(cid:17)(cid:19)

1

1

ηQ−1 + 2A

θ + θ⊤

2ηQ−1b + 2r1

(cid:1)

(cid:0)

(cid:1)

(θ⊤F θ

2θ⊤f

2θ⊤Lc)

exp

−

−

G
2(η + ω)

,

(cid:19)

(cid:17)(cid:19)

(cid:18)

=

exp

η
2(η+ω)

2πQ
|
2ηQ−1K + 4D

|
+ θ⊤

(cid:18)

1
2(η + ω)

θ⊤
(cid:16)
c + G

−

(cid:0)

exp

η
2(η+ω)

(cid:1)
1
2(η + ω)

(cid:18)

(cid:17)(cid:19)

−

(cid:16)

(cid:0)
1

=

2πQ
|

|
where

−

F = ηQ−1
2A,
f = ηQ−1b + r1,
L = ηQ−1K + 2D,
ηb⊤Q−1b
G =

−

−

2ηb⊤Q−1Kc

ηc⊤K⊤Q−1Kc + 2c⊤Bc + 2c⊤r2 + 2r0.

Next, we “complete the square” by considering the following quadratic term

[θ
−
= θ⊤F θ
−
θ⊤F θ
=

(F −1f + F −1Lc)]⊤F [θ

(F −1f + F −1Lc)]

2θ⊤f
−
2θ⊤f

2θ⊤Lc + (F −1f + F −1Lc)⊤F (F −1f + F −1Lc)
2θ⊤Lc

+ f ⊤F −1f + 2f ⊤F −1Lc + c⊤L⊤F −1Lc.

−

−

(cid:1)

Therefore, we have

(cid:0)

−

−

η

q(θ

c)

η+ω exp

R(θ, c)
η + ω

|

=

(cid:18)

1

exp

η
2(η+ω)

(cid:19)
1
2(η + ω)

([θ

−

−

2πQ
|

|
+ f ⊤F −1f + 2f ⊤F −1Lc + c⊤L⊤F −1Lc)

(cid:16)

(cid:18)

(F −1f + F −1Lc)]⊤F [θ

(F −1f + F −1Lc)]

−

(10)

exp

(cid:17)(cid:19)

(cid:18)

G
2(η + ω)

.

(cid:19)

12

Using the above result, the inner integral term in the dual function is

η

q(θ

c)

η+ω exp

|

 
2πF −1(η + ω)

R(θ, c)
η + ω !
b
exp

1
2

|

Z

= |

dθ

η
2(η+ω)

2πQ
|

|

1
2(η + ω)

(cid:18)

f ⊤F −1f + 2f ⊤F −1Lc + c⊤L⊤F −1Lc
(cid:16)

(cid:17)(cid:19)

exp

G
2(η + ω)

,

(cid:19)

(cid:18)

where the squared exponential term in Eq. (10) depending on θ is “integrated out” and becomes
the inverted normalization term
2 . Plugging this term back to the dual function
yields

2πF −1(η + ω)

|

|

1

g(η, ω) = ηǫ

ωβ +

f ⊤F −1f

ηb⊤Q−1b + (η + ω) log

2πF −1(η + ω)

+

µ(c)

f ⊤F −1L

ηb⊤Q−1K

cdc +

µ(c)c⊤

L⊤F −1L

ηK⊤Q−1K

cdc

−

(cid:19)

(cid:18)

−

f ⊤F −1f

ηb⊤Q−1b + (η + ω) log

2πF −1(η + ω)

−

−

1
2

Z

|

|

η log

2πQ
|

|

| −

η log

2πQ
|

|

| −

(cid:19)

(cid:19)

(cid:19)

(cid:18)

1
2

1
2

−

Z
= ηǫ

(cid:18)
ωβ +

−

+

µ(c)

Z

(cid:18)

(cid:18)
c⊤m +

1
2

c⊤M c

dc,

(cid:19)

where

m = L⊤F −1f
M = L⊤F −1L

−

ηK ⊤Q−1b,
ηK⊤Q−1K.

−
The expectation over µ(c) can be approximated by the context samples. The term 2c⊤Bc +
2c⊤r2 + 2r0 in G does not appear in the dual function since this is constant w.r.t. η and ω. Simi-
larly to the dual function, by using Eq. (9) and Eq. (10) we compute the new search distribution in
closed form as

π(θ

c)

q(θ

c)

η+ω exp

|

∝

|

η

R(θ, c)
η + ω !
b

 

∝

=

exp

N

(cid:16)

(cid:18)
θ

|

1
2(η + ω)
F −1f + F −1Lc, F −1(η + ω)

([θ

−

−

(cid:16)

(F −1f + F −1Lc)]⊤F [θ

(F −1f + F −1Lc)]

−

(cid:17)(cid:19)

.

(cid:17)

Proof of Convexity of

(H)

J

J

(H) = 1
2N

b
R(θn, cn)
is a convex function, we follow the
To show that
(Z+tV ) with symmetric matrices Z
proof in Boyd and Vandenberghe 2004, page 74. Let g(t) =
(cid:1)
P
J
through g. Through simple calculation,
and V and scalar t. Then, we can verify the convexity of
b

n Hxn

−

J

b

(cid:0)

N
n=1

x⊤

2

13

b

we have that

g(t) =

x⊤

n (Z + tV )xn

R(θn, cn)

2

−

=

(x⊤

n Zxn)2 + 2t(x⊤

(cid:1)
n Zxn)(x⊤
n V xn)

1
2N

1
2N

N

n=1
X
N

(cid:0)

n=1 (cid:18)
X

2(x⊤

n Zxn)R(xn)

−

+ t2(x⊤

n V xn)2

2t(x⊤

n V xn)R(xn) + R(xn)2

−

.
(cid:19)

The ﬁrst and second derivatives of g(t) are

N

n=1 (cid:18)
X
N

g(t) =

∇

2g(t) =

∇

1
N

1
N

(x⊤

n V xn)2

0.

≥

(x⊤Zx)(x⊤V x) + t(x⊤

n V xn)2

(x⊤

n V xn)R(xn)

−

,
(cid:19)

n=1
X
Since the second derivative is non-negative, the function

(H) is convex.

Experiments Details

J

b

Here, we provide all additional details regarding the experiments that are not mentioned in the
paper.

Quadratic Cost Function Optimization

The sampling Gaussian distribution mean is initialized uniformly randomly in [0, 5], while the
initial covariance is Q = 10, 000I. For nuclear norm, the regularization parameters are set to
λ∗ = 0.00002 and λ = 0.00001. For PCA, the dimensionality candidates are dz
.
}
Figure 6 shows the performance with different dimensionality. The regularization parameter for
the ℓ2-regularization is set at λ = 0.00001. The step size for accelerated proximal gradient (APG)
is ﬁxed at 0.001. We also normalize the gradient such that the Frobenius norm is 1. The maximum
iteration of APG is set at K = 300. C-REPS and C-MORE KL divergence is set to 0.9.

3, 6, 10, 20

∈ {

Ball Hitting with a 2-DoF Robot Arm

We use a linear-in-parameter controller with Gaussian basis functions to control the joint acceler-
ations ¨q of the robot

¨q =

θ⊤
1 ϕ(q),

θ⊤
2 ϕ(q)

⊤

,

2

⊤

1 , θ⊤
θ⊤

(cid:2)
is the policy parameters vector, q

R2 is the joint angles vector, and ϕ is the
where θ =
Gaussian basis functions vector with 16 Gaussian centers placed at
for
both joints. The number of total parameter θ is 32. The reward R(θ, c) is computed as the negative
cumulative joint accelerations plus the negative distance between the end-effector and the ball at

2 , π, 3π
2 }

2 , π, 3π

2 } × {

0, π

0, π

∈

{

(cid:2)

(cid:3)

(cid:3)

14

dz = 3
dz = 6
dz = 10
dz = 20

d
r
a
w
e
R

-
g
o
L
e
g
a
r
e
v
A

8

−

10

−

12

−

20

40

60

80

100

Iteration

Figure 6: Average reward for the quadratic cost function problem with C-MORE PCA using different dimensionality
dz. Shaded area denotes standard deviation (results are averaged over ten trials). Results do not differ much, since
PCA always fails in reducing the dimensionality of the context variables. Clearly, the more principal components we
keep, the better results we obtain.

the ﬁnal time step

R(θ, c) =

0.05

+ 10 exp

−

40

¨qt|

t=1 |
X

(ballx

eﬀ x)2

(bally

eﬀ y)2

−

−
50

−

,

(cid:19)

−

(cid:18)

where ¨qt denotes the joint accelerations at time step t, ballx and eﬀ x denotes the x coordinate of
the ball and end-effector at the ﬁnal time step, respectively. Variables bally and eﬀ y are also also
deﬁned similarly. We use the following transition dynamics to govern the joint angles

˙q + ∆¨q,
q + ∆ ˙q,

˙q
q

←
←

where ∆ = 0.1. Each arm has length 7.5. The robot is initialized such that the end-effector is at
the bottom position.

|

1

(θ

N
10−7, 1

b + Kc, Q) is initialized by bij

10−7, 5
∈ {
×
10, 20, 30, 40

The sampling Gaussian distribution
∼
(0, 0.012), for each entry (i, j) and Q = I. For nuclear norm, the regularization parameter
N
10−6, 2
candidates are λ∗
. For PCA, the dimensionlaity can-
×
didates are dz
. The regularization parameter for the ℓ2-regularization is set at
λ = 0.0001. The step size for accelerated proximal gradient (APG) is ﬁxed at 0.001. We also
normalize the gradient such that the Frobenius norm is 1. The maximum iteration of APG is ini-
tialize at K = 500 and it is reduced by 2 after each update to reduce the computation time until a
minimum of K = 300. K is reset to 500 when cross-validation is performed.

(0, 1), Kij

10−6

∼ N

∈ {

×

×

}

}

Figure 7 shows the performance of C-REPS with PCA on different dimensionality dz and dif-
ferent KL upper-bound ǫ. The best result of C-REPS is the one reported in the main paper for the
comparison with C-MORE. Figure 8 shows the rank of B against update iteration averaged over
ten trials. C-MORE with nuclear norm is quite unstable early on. However, the rank stabilizes after
some iterations and the rank converges to 31.

15

Figure 7: Results of C-REPS with different ǫ and dz for the 2-DoF hitting task.

Ball Hitting with a 6-DoF Robot Arm

In this task, the reward function is deﬁned as

R(θ, c) =

0.05
0.05

−
−

(

P
P

qt|
+ 10
|
qt + 10

exp(

∗
exp(

distance2),
−
distance2) + 20, otherwise,

if distance > 0.1 cm.

∗

−

where distance denotes the minimum distance between the ball and the end-effector along the tra-
jectory. APG and PCA setup is the same as in the 2-DoF robot experiment. Figure 9 and Figure 10
show the average reward and hit accuracy averaged over 50 contexts on three individual trial, re-
spectively. On all trials, the nuclear norm performs better than PCA and consistently achieves 80%
hit accuracy.

16

Figure 8: Rank of B for the 2-DoF hitting task. At the 1000th iteration the rank of B learned by nuclear norm is
approximately 31, while PCA is approximately 25.

Figure 9: Average reward of three individual trials on the 6-DoF ball hitting task.

1

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
a
 
t
i

H

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
a
 
t
i

H

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
a
 
t
i

H

Nuclear norm
PCA

Nuclear norm
PCA

Nuclear norm
PCA

0

0

100

300

200
Iteration

400

500

0

0

100

300

200
Iteration

400

500

0

0

100

300

200
Iteration

400

500

Figure 10: Hit accuracy of three individual trials on the 6-DoF ball hitting task.

17

Algorithm 2: APG for solving the nuclear norm minimization problem
Input: Parameters λ and λ∗, gradient step size τ , maximum number of iteration K, initial solution H 0

1 Initialize H −1 = H 0 and t−1 = t0 = 1
2 for k = 1, . . . , K do
3

Set intermediate point

4

Do gradient descent using the differentiable term

where

Y k = H k +

1

tk−1 −
tk

(H k

H k−1)

−

Y + = Y k + τ

(Y k),

∇J

A+
D⊤
+
0.5r⊤

D+
B+
1+ 0.5r⊤

2+

0.5r1+
0.5r2+
r0+





Y + =





B∗ = U max(Σ

λ∗I, 0)V ⊤,

−

Shrink singular values of B+

where B+ = U ΣV ⊤

is the SVD of B+

Truncate positive eigenvalues of A+

A∗ = P min(Λ, 0)P ⊤,

where A+ = P ΛP ⊤

is the eigendecomposition of A+

7

Update solution

H k+1 =





A∗
D⊤
+
0.5r⊤

D+
B∗
1+ 0.5r⊤

2+

0.5r1+
0.5r2+
r0+





Update parameter tk+1 =

1+√1+4(tk)2
2

if stopping criterion is met then

return

5

6

8

9

10

18

