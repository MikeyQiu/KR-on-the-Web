8
1
0
2
 
p
e
S
 
9
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
8
1
9
2
0
.
9
0
8
1
:
v
i
X
r
a

Towards Query Efficient Black-box Attacks:
An Input-free Perspective

Yali Du∗123 Meng Fang∗4 Jinfeng Yi5 Jun Cheng26 Dacheng Tao3
1 Centre for Artificial Intelligence, FEIT, University of Technology Sydney, Australia
2 Guangdong Key Lab of Robotics and Intelligent System, Shenzhen Institutes of Advanced Technology, CAS, China
3 UBTECH Sydney AI Centre, School of IT, FEIT, the University of Sydney, Australia
4 Tencent AI Lab, Shenzhen, China 5 JD AI Research, Beijing, China 6 The Chinese University of Hong Kong
yali.du@student.uts.edu.au, mfang@tencent.com,yijinfeng@jd.com,
jun.cheng@siat.ac.cn, dacheng.tao@sydney.edu.au

ABSTRACT
Recent studies have highlighted that deep neural networks (DNNs)
are vulnerable to adversarial attacks, even in a black-box scenario.
However, most of the existing black-box attack algorithms need
to make a huge amount of queries to perform attacks, which is
not practical in the real world. We note one of the main reasons
for the massive queries is that the adversarial example is required
to be visually similar to the original image, but in many cases,
how adversarial examples look like does not matter much. It in-
spires us to introduce a new attack called input-free attack, under
which an adversary can choose an arbitrary image to start with
and is allowed to add perceptible perturbations on it. Following
this approach, we propose two techniques to significantly reduce
the query complexity. First, we initialize an adversarial example
with a gray color image on which every pixel has roughly the same
importance for the target model. Then we shrink the dimension of
the attack space by perturbing a small region and tiling it to cover
the input image. To make our algorithm more effective, we stabilize
a projected gradient ascent algorithm with momentum, and also
propose a heuristic approach for region size selection. Through
extensive experiments, we show that with only 1,701 queries on av-
erage, we can perturb a gray image to any target class of ImageNet
with a 100% success rate on InceptionV3. Besides, our algorithm
has successfully defeated two real-world systems, the Clarifai food
detection API and the Baidu Animal Identification API.

KEYWORDS
adversarial learning; black-box attack; input-free attack; region
attack; neural network

∗ Contributed equally.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
AISec ’18, October 19, 2018, Toronto, ON, Canada
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-6004-3/18/10. . . $15.00
https://doi.org/10.1145/3270101.3270106

1 INTRODUCTION
Given a legitimate example, adversarial attacks aim to generate
adversarial examples by adding human-imperceptible perturba-
tions [5, 10, 15]. Depending on how much information an adversary
can access to, attacks can be classified as white-box or black-box.
When performing a white-box attack, the adversary has full control
and complete access to the target model [5, 10, 15]. While perform-
ing a black-box attack, the adversary has no access to the explicit
knowledge of the underlying model, but it can make queries to
obtain corresponding outputs [4, 6, 11–13, 24].

Compared with white-box attacks, black-box attacks are more
realistic and more challenging. Although many black-box attack
methods have been proposed, most of them suffer from high query
complexity which is proportional to the size of the image. For ex-
ample, attacking a 299 × 299 × 3 ImageNet image can usually take
hundreds of thousands of queries, or even more. Indeed, most of
these approaches would fail by simply setting an upper limit on the
number of queries one can make. Note that an important reason
for the high query complexity is that the adversarial example is
required to be visually similar to the original image. This is because
each pixel on the original image usually plays a significantly dif-
ferent role on the model’s outputs, and the adversary may need to
query all of the pixels to understand their roles. However, a primary
goal of black-box attack is to break down the underlying target
model with as little cost as possible. When this goal is achieved, how
adversarial examples look like does not matter too much. For this
reason, we re-evaluate the necessity of the common practice that
adversarial examples should be visually similar to the meaningful
input images.

In this work, we focus on a specific type of attack, which we
refer to as “input-free" attack. With this attack, adversaries are free
to choose any initial image to perform attacks. A typical initial
image is a gray color image with each pixel value equal to 128.
One significant advantage of this kind of initial image is that all of
the pixels have exactly the same intensity, thus they should have
the same importance. Note that many black-box adversarial attack
algorithms need to make a large number of queries to recognize
the importance of each pixel, starting from a gray color image can
greatly reduce the query complexity. Besides, “input-free" attacks
are more desirable than traditional adversarial attacks when benign
examples are not accessible. For instance, an adversary may have
no access to any rightful faces that can unlock a face recognition
system.

Under input-free settings, the adversarial attack becomes slightly
simpler since the distortion constraint is removed. In other words,
an input-free attack can choose an arbitrary image to start with and
is allowed to add perceptible perturbations on it. The effectiveness
of an input-free attack is measured by the cost spent during the
attack. The cost is usually represented by the number of queries,
and the lower the number is, the more effective the attack is. We
note that [16] also lies in the scope of input-free attack, in which
the adversarial examples were generated by starting from a ran-
dom image. However, [16] doesn’t focus on improving the query
complexity of black-box attacks, thus is different from our concern.
Recently, Ilyas et al. [12] proposed to adopt Natural Evolutionary
Strategies (NES) [25] to optimize black-box attacks. It shows that
fewer queries are needed for the gradient estimation than the finite-
difference methods [6]. Based on the NES approach [12, 25], we
propose two techniques to further improve the query complex-
ity, namely gray image attack and region-based attack, which are
explained as follows.

Gray image attack. First, we choose a gray color image with
uniform pixel values as the starting point, and then keep adding
noises on it until it is successfully classified into the target class. The
gray image does not contain any human perceivable information,
which is different from the traditional perturbation constraint on the
adversarial examples. In other words, this kind of attack abandons
the similarity constraint that most standard attacks used [6, 12].
Since all of the pixels are initialized to be the same value, they would
be almost equally important for the network output. In this case, a
gray image evades the strength of convolution neural networks in
extracting representational features from input images, rendering
it easier to be classified into other classes. Note that the gray color
image is not the only choice for input-free attack. We have tried
other natural images such as rocks and woods and they all lead to
satisfactory performance.

Region-based attack. The equivalence of pixels leads us to con-
sider whether it is necessary to query the whole image to synthesize
an adversarial example. When developing an image recognition
system, the model’s sensitivity to common object deformations
such as translation and scaling are implicitly considered in general.
Inspired by this fact, we do not need to query the whole image.
Instead, we perturb a much smaller region and then tile it to cover
the whole image to match the input dimension. This operation
can significantly reduce the dimension of attack space, and thus
improve the query complexity.

We name the proposed attack method as Region Attack, and
evaluate it on InceptionV3 [22], a popular classifier trained on
ImageNet [7]. Figure 1 shows one example of the proposed attack
algorithm. The adversarial example is initialized by a gray image,
and after only 735 queries, the output image is misclassified to the
target class Stingray. Our empirical study shows that on average,
the proposed attack algorithm only needs 1,701 queries to reach
100% success rate with target attack on ImageNet. Furthermore, We
test the proposed Region Attack on some challenging real-world
systems, including Clarifai food detection API1 and Baidu Animal

Identification API2. These two APIs only return a top list of labels
and scores that do not sum to one. We show that Region Attack
can still succeed within a small amount of queries.

Our main contributions are summarized as follows:

• We show that the query complexity of black-box attacks
can be significantly improved by starting from a gray image.
This opens up a prominent direction of performing efficient
attacks by designing proper input images.

• We shrink the dimension of the attack space by perturbing
a region and tiling it to the whole image. Compared with
the baseline models, this technique reduces average query
requirement by at least a factor of two. Besides, we also
propose a heuristic algorithm for region size selection.
• We show that momentum-based stochastic gradient descent
(SGD) can stabilize the update of the search distribution,
which was also examined to boost traditional gradient-based
adversarial attacks [5, 8].

• We show that the proposed model could successfully at-
tack commercial online classification systems. To the best
of our knowledge, the proposed attack model is one of the
pioneering works that reduces query complexity from the
perspective of input-free attack.

2 RELATED WORKS
The research of adversarial attacks is rooted in the need for un-
derstanding the security of machine learning models and hence
deploying reliable systems in real-world applications [2, 3]. After
Szegedy et al. [23] pointed out the existence of adversarial exam-
ples, there is a long line of works [4–6, 9, 10, 12, 15, 17, 18] that
produce adversarial examples in different settings.

The adversarial attack approaches fall into two categories: white-
box and black-box attacks. White-box attacks [5, 10, 15, 18, 23]
explore the target model’s architecture and parameter weights
to find the gradient direction to update adversarial perturbations.
However, when target models do not release their architectures to
the public, it is impossible for white-box attacks to back propagate
the gradients of the network output with regard to the input image.
Black-box attacks do not require the architecture or parameters
of the target model, but need to query the target model to obtain
feedback on the adversarial examples. The transferability of ad-
versarial examples [23], [17] can construct a substitute model to
perform white-box attacks and take the query results from the tar-
get model to refine the substitute model’s classification boundary.
Since training a substitute model is cumbersome and adversarial ex-
amples do not always transfer well, black-box attacks based solely
on queries are in highly demand.

When black-box attacks have only query access to a target model
[6, 11–13], to imitate gradient information, [6, 12] performed black-
box attacks by zeroth order optimization. [6] proposed to estimate
gradients via coordinate-wise finite differences based on probability
vector of different classes. [12] maximized the expected value of
loss function by a search distribution, and approximated gradients
by Natural Evolution Strategies [25] through a Gaussian search
distribution. These methods rely on probabilities show the relative
goodness of adversarial examples. [4] does not require probability to

1https://clarifai.com/models

2https://cloud.baidu.com/product/imagerecognition/fine−grained

Figure 1: Illustration of adversarial examples on ImageNet for Stringray class with 735 queries. The perturbations show re-
peated patterns of noises and the resulting image contains the texture and edge informaiton of Stringray class.

estimate gradients but only labels of the target model. It initialized
the adversarial example by an image in target class and greedily
reduced the distance between the adversarial example and the
image from the target class, and preserved the prediction results of
the adversarial example in the target class. The number of queries
of this algorithm was as high as 400,000 for falsifying an adversarial
example for a simple neural classifier on CIFAR10.

Existing research under input-free attack is scarce. Nguyen et al.
[16] evolved from uniform random noise and compressible patterns
to fool deep neural networks. [16] aimed to produce adversarial
examples with high confidence, but the efficiency of the algorithm
was not guaranteed. Ilyas et al. [12] claimed that maximizing the
expected value of fitness function by Natural Evolution Strategies
requires fewer queries than typical finite-difference methods. How-
ever, it still needs a median of more than 10,000 queries to attack
InceptionV3 classifier on ImageNet. [9] studied rotations and trans-
lations for adversarial attacks which supposedly requires fewer
queries, but the success of its attacks was not guaranteed.

The proposed region-based attack restricts the dimension of the
attack space; in this respect, [6] adopted an attack-space dimension
technique which first queries a small-sized image to generate adver-
sarial noises and then upscales it to the dimension of the original
image. [21] performed attacks by modifying one or few pixels but
the success rate of targeted attacks was low.

3 QUERY EFFICIENT BLACK-BOX ATTACKS

BASED ON NATURAL EVOLUTION
STRATEGIES
3.1 Preliminaries

Deep Neural Networks and Adversarial Attacks. Following the
naming conventions in [5], we denote x ∈ [0, 1]d as an input image,
F (x) as the output of the softmax layer and Z (x) as the logit layer
which is the output of the last layer before softmax. C(x) gives final
label prediction. Specifically:

F (x) = softmax(Z (x)),
C(x) = arg max
[F (x)]i .

i
Assume original prediction on x satisfies that C(x) = y, an adversary
aims to either reduce the classifier’s confidence on correct labels
or alter the classifier’s prediction to any specified class by adding

spuriously crafted noise on the image x. In untargeted attacks, an
adversary intends to perturb the input x with maliciously crafted
noises δ such that

C(x + δ ) (cid:44) y and ∥δ ∥ < c,

where ∥ · ∥ is a norm chosen by the attacker, such as L2, L∞ and
c is a pre-specified constant. Targeted attacks craft adversarial
perturbations that explicitly classify an input image into another
specified class y ′, which is formulated as follows:

C(x + δ ) = y ′, y ′ (cid:44) y, and ∥δ ∥ < c.

(1)

The formulation in Eq. (1) can not be directly solved by existing
algorithms, as the equality constraint C(x + δ ) = y ′ is highly non-
linear. Accordingly, one can express it in an equivalent form that is
more suitable for optimization [5]:

f (x, y ′) = max{max
t (cid:44)y′

log[F (x)]t − log[F (x)]y′, −κ},

(2)

where κ is a suitable constant. Eq. (2) is more suitable for optimiza-
tion and has been applied to both white-box and black-box attacks.
When performing white-box attacks, one can directly take gradient
derivation to image x to minimize the loss function f (x, y ′). When
performing black-box attacks, gradient can be approximated by
pixel-wise finite differences.

Black-box optimization by Natural Evolution Strategies. To per-
form a black-box attack, one needs to search the input space for
adversarial noise which satisfies Eq. (1). Existing works mainly
solve Eq. (2) through minimizing the objective function. In this
work we adopt Natural Evolutionary Strategies [25] which is used
in [12] to estimate gradients from probabilities of predictions for
derivative-free optimization. Instead of maximizing the objective
function directly, NES optimize the expected value of a fitness func-
tion under a search distribution.

The procedure of optimization by NES can be summarized as
four steps from [25]: (i) one samples a batch of search points from
the chosen parameterized search distribution and evaluates the
fitness function on them; (ii) NES estimates search gradients for the
parameters of the search distribution; (iii) NES performs gradient
ascent step along the search gradient direction; (iv) NES repeats
the above three steps until a stopping criterion is satisfied. Denote
the fitness function as J (θ ) = P(y|θ ) and x as parameters for search

distribution, the formulation of search gradients can be derived as
follows, per [12, 25]:

E

π (θ |x )[J (θ )] =

J (θ )π (θ |x)dθ

∫

∫

∇x E

π (θ |x )[J (θ )] = ∇x
∫

J (θ )π (θ |x)dθ

=

J (θ )∇x π (θ |x)dθ .

Applying the “log-likelihood” trick yields:

∇x E

π (θ |x )[J (θ )] =

J (θ )

π (θ |x)
π (θ |x)

∇x π (θ |x)dθ

∫

∫

=

= E

π (θ |x)J (θ )∇x log π (θ |x)dθ
π (θ |x )[J (θ )∇x log π (θ |x)].

(3)

(4)

The derivations in Eq. (3) gives mathematical foundations for steps
(ii) and (iii). From samples θ1, ..., θn , the search gradient for the loss
function J (θ ) is reduced to this form:

∇x E[J (θ )] ≈

J (θk )∇θ

log π (θk |x).

1

n

n
(cid:213)

k =1

To avoid non-smoothness of the fitness function, local reparam-
eterization trick [12, 20] can be used by setting θ = x + σδ with
δ ∼ N (0, I ). In this case, sampling θ is equal to sampling δ , and
at each step, we are literally searching the neighborhood of x for
a better solution. Denote n as batch size which is the number of
search points used in each step for estimating gradients, ∇x E[J (θ )]
is approximated by:

1

n
(cid:213)

σn

∇x E[J (θ )] ≈

J (θ + σδk ) · δk .

(5)

k =1
Rather than sampling n search points independently, we follow
[12, 20, 25] and implement an antithetic sampling strategy such
that δk ∼ N (0, I ), for k = 1, 2, ..., n
= −δn−k +1, for k =
n
2 + 1, ..., n. The search gradient under Gaussian distribution can
now be estimated by:

2 , and δk

∇x E[J (θ )] ≈

(δk J (θ + σδk ) − δk J (θ − σδk )).

(6)

1

σn

n/2
(cid:213)

k =1

3.2 The Proposed Attacks by Gray Images
With the proposed black-box attack, the adversary does not have
access to any natural input, which is different from the existing
attack algorithms. By normalizing the pixel values within [0, 1], we
initialize the input to the target model with x = [xi j ]n×m, xi j =
0.5 ∀(i, j) and call it “gray image”. This initialization leaves us much
flexibility since we can perturb the pixel values to be either brighter
(closer to 1) or darker (closer to 0) to falsify an example into target
class. One can choose different methods to initialize the input other
than a gray image.

We first define a fitness function which satisfies that its maximum
is obtained when C(x + δ ) = y ′ and it is dependent on the input to
the target model. Denote input θ = x + δ , the fitness function is
defined as:

J (θ ) = P(y ′|θ ) ≈ [F (θ )]y′ .

(7)

We adopt NES optimization and maximize P(y ′|θ ) directly until
C(x + δ ) = y ′. We choose Gaussian as search distribution for θ
which provides a simple form for gradient estimation [12, 20]. For a
Gaussian distribution θ ∼ N (ψd , σd ×d ), parameters to be optimized
. By performing gradient
are the mean ψd
ascent for parameters, the expected value of fitness function is
maximized.

and covariance σd ×d

Aside from Gaussian, other distributions whose derivatives of
its log-density with regard to their parameters are attainable can
be used as search distributions, such as the Laplace distribution. By
setting θ = x + bδ with δ ∼ Laplace(0, 1), the search gradient of
the fitness function can be estimated in the following form:

∇x E[J (θ )] ≈

J (θ + bδk ) · sign(δk ).

(8)

1

n
(cid:213)

bn

k =1

Note that we adopt a region-based attack algorithm, so gradients
are dependent on regions. We denote the gradient as д(h, w) with
h, w denoting region size. The details will be expanded in Section
3.3. After evaluating the gradients, we perform projected gradient
ascent as in [12, 14] by using the sign of the obtained gradients to
update x:

x t = Π[0,1](x t −1 + γ sign(дt (h, w)),
(9)
where γ denotes the learning rate and Π[0,1] denotes the projection
operator that clips pixel values into [0, 1]. The adversarial image
is updated by the search gradients until the adversarial goal is
achieved.

We show the overall process of Region Attack in Fig. 2. The
adversarial example is initialized with a gray image. We produce
search gradients in a small region indicated by the red box and tile it
to the whole image; thus, the gradient information for each region
in the image is exactly the same. After black-box optimization,
we obtain the adversarial example with clear texture and edging
information showing in each region.

Several issues require our attention for implementing Region
Attacks. On one hand, how to set up an appropriate region size?
On the other hand, since gradients are estimated based on the
finite sampling of search points, the variance will be introduced
and prohibits convergence of the algorithm. How to tame this
challenge? First, we summarize the general procedures for the gray
image attack in Algorithm 1, and then discuss the key steps in the
remaining section. Step 1, 3, and 4 in Algorithm 1 will be further
explained in Section 3.3, 3.4, and 3.5.

Algorithm 1 Canonical Procedures for Query Efficient Attack
Input: target class y ′, gray image x, max iteration maxiter
Output: adversarial image x ′ such at C(x ′) = y ′

1: set up the region size (h, w)
2: for t = 1, 2, ..., maxiter do
3: estimate gradient дt (h, w) by region mutation
4: x t = Π[0,1](x t −1 + γ sign(дt (h, w)))
5: if C(x t ) = y ′ :

success = True
return xi

6: end if
7: end for

Figure 2: Overview of the proposed Region Attack algorithm. The input is a gray image. By querying the black-box classifier,
we obtain the gradients to update a region on the image in the red box. This process is repeated until we reach an adversarial
example that is identified to be the target class.

3.3 Gradient Estimation based on Region

size as:

Mutation

We denote a perturbed region as R = (p1, p2, h, w), where (p1, p2) is
the position of the pixel at the top left corner of the chosen region,
and h, w indicate the height and width of the perturbed region. We
tile the region to the whole image such that every region of size
h × w has the same pixel values. Now the dimension of the attack
space we need to optimize is h × w; in fact, adopting a small region
size will lead to a reduction on dimension of the attack space. If
h ×w cannot be divided by d
⌋ regions in the middle
of the image and adopt symmetric padding to complete the whole
image.

2, we place ⌊ d 2
hw

Based on the above discussion, the gradient estimation over the
whole image is aggregated by gradients on different regions. We
summarize the gradient estimation procedures in Algorithm 2 based
on antithetic sampling [12, 20], which will be used to replace Step
3 in Algorithm 1.

Algorithm 2 NES Gradient Estimation for Region Mutation
Input: target class t, image x of dimension d, classifier P(y|x),

batch size n, region size h, w
Output: gradient estimation д

1: д ← 0d
2: for i = 1, 2, ..., n
3: sample ϵk ∼ N (0
by ⌈ d 2
4: repeat ϵk
hw
5: д = д + 1
σ P(y|x + σ · ˜ϵk ) · ˜ϵk
6: д = д − 1
σ P(y|x − σ · ˜ϵk ) · ˜ϵk
7: end for
8: return д
n

2 do
hw , δhw ×hw )
⌉ times thus obtain ˜ϵk ∈ Rd

3.4 Active Selection on Region Sizes
We propose a heuristic approach based on a limited query budget.
We use the variation of entropy to measure whether a chosen size
is effective for Region Attack on the target model. We set the batch
size as n0, and take t0 iterations in this warm-up section. Define
the entropy of the target model’s prediction under current region

h(F (x)) =

[F (x)]i log[F (x)]i .

(10)

n
(cid:213)

i=1

We use its variation ∆h to measure the effectiveness of current
selected size, which is defined as

(11)

∆h = ht0 − min(hj ), j ∈ {1, ..., t0},
where hj denote the entropy calculated after j−th updating step.
The larger the variation of entropy is, the more capable we are
to alter the prediction of the original input. This ensures that we
are more likely to reach the target goal under the same number
of queries. In fact, most of the targeted attacks would experience
such two stages that the entropy hj of predicted probability vector
first goes down and then goes up. In the first stage, Region Attack
reduces the labeling information of the original prediction on the
gray image, which leads to the decrease of hj . In the second stage,
Region Attack raises the labeling information of the target class
which corresponds to the increase of hj . The steps of this approach
are listed as follows:

(1) we perform momentum gradient ascent for t0 iterations and

record the output of softmax layer F (x);

(2) we calculate the entropy of F (x) after each iteration to obtain

hj ;

(3) we estimate the variation of entropies of these t0 iterations
and return the size that corresponds to the largest variance.
The region size used in Step 1 in Algorithm 1 can be set up by these
procedures.

3.5 Momentum-based Gradient Ascent
Since one can not search the whole distribution to calculate the
accurate gradient, one always samples a batch of points from search
distribution to estimate gradients. Due to the limited sample size
for one batch, the variance would be introduced into the gradients
and hurts the convergence of the algorithm. Increasing the sample
size would reduce the variance for each estimation of the gradient.
However, monotonically increasing the sample size for one draw
will increase query complexity. Note that the momentum-based
gradient descent has been verified in [5, 8] to be effective in stabiliz-
ing update directions of objective function which is mentioned but
not discussed in [12]. Following this route, we use the approach of

momentum-based gradient ascent to update the mean of the search
distribution, which is exactly the adversarial image:

дt = ηдt −1 + (1 − η)∇θ J (θ )
x t = Π[0,1](x t −1 + γ sign(дt )).

(12)

One thing to mention is that we do not follow the convention to set
η to 0.9 or a similar value, we instead set it to be around 0.5. Since
we use the sign of gradients to update adversarial perturbations,
setting η = 0.9 will lead to the situation that the algorithm keeps
exploiting in the same direction without exploring new directions.

4 PARTIAL-INFORMATION ATTACK
In addition to the discussion in Section 3, we note that many open
cloud APIs provide only TOP-k scores of detected labels that are
not summed to 1. This situation is difficult since neither the number
of labels nor the probability of all labels is given. Furthermore, in
our procedure, an attacker does not have access to benign inputs,
and a random initialization might lead to the prediction as “non-
object” or “background”. This makes it more challenging than the
partial-information attack discussed in [12].

We deploy a two stage process for efficient attacks. Firstly, we
treat a target model as a binary classifier which produces the label
of either “non-object” or “is object”. Now it is possible to perform an
untargeted attack by minimizing the probability of P(non-object|x).
We minimize P(non-object|x) until it falls out of TOP-K predic-
tions. Secondly, we choose a legal label from TOP-K{P(·|x)} as a
target class y ′ and maximize P(y ′|x) until the target model confi-
dently recognizes x ′ as an legal object. In the first stage, we reverse
gradient ascent steps in Section 3.3 to gradient descent to mini-
mize P(non-object|x). In the second stage, we follow the two-step
optimization procedure used in [4, 12] which alternates between
maximizing probability of y ′ as P(y ′|x) and retaining y ′ in the TOP-
K{P(·|x)} predictions by shrinking the learning rate. The general
procedures are summarized into the following four steps.

(1) initialize an input x0 and its label y0 = C(x0) and
specify a classifier P(·|x) that returns TOP-K labels
and scores;

(2) minimize P(y0|x) until y0 (cid:60)TOP-K{P(·|x)};
(3) choose y ′ ∈ TOP-K{P(·|x)} as the target class;
(4) alternate between the following two steps:

(a) maximizing the score of the target class: x t =
arg maxx P(y ′|x);
(b) adjusting the learning rate γ such that: γ =
max γ ′ s.t. rank(y ′|x t ) ≤ k;

(5) quit when P(y ′|x) > c where c is a specified constant.

5 EXPERIMENTS
We examine the proposed algorithm’s efficiency in generating tar-
get adversarial examples and reducing query complexity. We would
like to show that the proposed algorithm achieves a high success
rate and leads to a significant reduction in query complexity. Fur-
thermore, we demonstrate that Region Attack can fool many open
Cloud APIs with affordable number of queries.

5.1 Baselines
We adopt two recent attacks ZOO [6] and Query-Limited (QL) [12]
as baselines for comparison. Since the similarity between images is
not the main issue in our settings, to be fair, by removing the simi-
larity constrain in objective functions, we construct two stronger
baselines ZOO-L2 and QL-L∞ with necessary modifications based
on ZOO and QL where “-” should be read as “minus”.

• ZOO [6] estimates gradients by pixel-by-pixel finite differ-
ences and adopts Adam optimizer to optimize the adversarial
perturbations. It includes L2 norm as distance loss to con-
strain adversarial examples to not be far away from the
initialization. We use its public implementation.

• ZOO-L2 is constructed by slightly modifying ZOO. Since the
proposed setting does not consider distance loss, to make
fair comparisons, we remove the distance loss in ZOO and
construct a more effective baseline denoted as ZOO-L2.
• QL [12] applies Natural Evolutionary Strategies to perform
black-box optimization and adopts L∞ to constrain the dis-
tance between adversarial and benign inputs. It reports 1∼2
orders of magnitude reduction on query complexity com-
pared to ZOO. We use its implementation without modifica-
tions.

• QL-L∞ is a slightly modified version of QL with L∞ distance
removed, which is supposed to be more query-efficient than
QL.

To this end, we have four strong baselines. Since both ZOO and QL
implemented attacks on InceptionV3, we will report attack results
on this classifier.

5.2 Setup
We choose InceptionV3 as the target model with default input
size 299 × 299 × 3, which is consistent with the settings in ZOO
and QL. InceptionV3 [1, 22] achieves 78% top-1 accuracy for the
ImageNet dataset [19]. All attacks are restricted to have access only
to the probability vectors of all possible classes. The default search
distribution used through experiments is the Gaussian distribution.
We produce a gray color image with size 299 × 299 × 3 and pixel
values equal to 128, which are normalized to 0.5 for optimization
as the initialized input for all baselines in different targeted attacks.
For this same gray image x, we choose different target class ti , i =
1, ..., 1000, and construct the test set S = {(x, t1), ..., (x, t1000)}.
Note that in our experiments, we always use a square region by
setting w = h for the object. We will only specify the height of the
region in the following.

Metrics. Denote Ω as the set of successfully attacked target
classes and qi as the query required for attacking i-th target class,
we define two measurements: success rate r and average number
of queries qavд:

r =

∥Ω∥
1000 ,
(cid:205)

qavд =

i ∈Ω qi
∥Ω∥

.

Success rate indicates the capability of an algorithm to cheat a clas-
sifier. Average number of queries indicates the cost of an algorithm

to attack. The fewer the number of queries are, the more efficient
an algorithm is.

Since ZOO and QL punish L2 and L∞ distortion of adversarial
examples, we report the averaged L2 and L∞ distortions in Section
5.3. Denote the L2 and L∞ distortions for i-th attack as δi,2, δi,∞,
the averaged L2, L∞ distortion are defined as:

δavд,2 =

δavд,∞ =

(cid:205)

(cid:205)

,

i ∈Ω δi,2
∥Ω∥
i ∈Ω δi,∞
∥Ω∥

.

Average distortion means the average distance between initial gray
image and adversarial examples when an attack succeeds. Averaged
distortions are reported for readers’ reference and are not consid-
ered to be the main measurement since we consider input-free
attacks. The implementation code is publicly available.3

5.3 Results on Query Efficient Attack
For Region Attack, we choose different region sizes and momentum
factors, then we run the proposed threat model and four baselines
on the test set S. It is important to note that the algorithms and
Region Attack are tested under the same gray image and target class
set. The results are reported in Table 1. Some adversarial examples,
adversarial noises, and ground-truth images will be shown in the
appendix. The attack is not restricted to gray images. In fact, we
tried other natural images like rocks and woods and also obtained
good performance. Since texture patterns are not explicitly shown
in these perturbed natural images, they are not displayed in the
paper.

In Table 1, the proposed algorithm obtains 100% success rate
which is the same as QL and QL-L∞, but higher than ZOO and
ZOO-L2. It is shown that for the proposed algorithm with the fixed
region size w = h = 64, the query complexity is only around 1700
on average, which is 60 times more efficient than ZOO and ZOO-L2,
and 2 times faster than QL and QL-L∞.

To evaluate distance loss’s influence on query complexity, we
compare ZOO with ZOO-L2 and QL with QL-L∞. We observe that
without L2 distance loss between benign and adversarial images,
ZOO-L2 requires fewer queries and achieves comparable success
rate compared to ZOO. Removing L∞ norm also reduces QL-L∞’s
query needs, and as a result, produces larger distortions than QL.
We remark that removing the perturbation constraint will help in
query reduction. Note that comparing to the query needs reported
by Chen et al. [6], ZOO-L2 reduces the query counts from more than
2 millions to 77,513, which is 1 ∼ 2 orders of magnitude. Compared
with QL-L∞, which uses the same black-box optimization as Region
Attack, we reduce the query needs by half. This indicates that only
removing the distance loss in existing attacks does not produce
efficient attacks; our Region Attack reduces the dimension of the
attack space and allows fewer queries in gradient estimation.

5.3.1 Query distribution comparison. Since QL-L∞ is the strongest
baseline method, we compare its query distributions with Region
Attack in Fig. 3. Note that the maximal and minimal number of
queries for QL-L∞ and Region Attack are 19431 and 51 respectively;
we thus divide queries of 1000 attacks into 39 bins with each bin

3https://github.com/yalidu/input-free-attack

Figure 3: Query distributions of Region attack and QL-L∞
methods on 1000 targeted attacks with η = 0.7, h = 64. The
queries are divided into 39 bins with each bin spanning a
width of 500 queries. Blue bar shows the query distribution
for Region Attack; Orange bar shows queries for QL-L∞.

Figure 4: Query comparison of different region sizes un-
der 20 targeted attacks. Centerline represents the median.
Box extents show 25th and 75th percentile. Whisker ex-
tents illustrate maximum and minimum values. Black cir-
cles show outliers that are too far from the median such as
6000 queries for target class tench under size 32.

spanning a width of 500 queries. The highest blue bar shows that 252
targeted attacks are completed within queries [500, 1000). Dividing
the histogram by query counts of 2500, 83.6% attacks of Region
Attack are less than 2500 while it is only 31.8% for QL-L∞.

Influence of size selection on query complexity. We compare
5.3.2
the active size selection algorithm with fixed sizes chosen from
{32, 64, 128, 256} and report query complexity on attacking the
first 20 classes of ImageNet which are tench, goldfish, great white
shark, tiger shark, hammerhead shark, electric ray, stingray, cock, hen,
ostrich, brambling, goldfinch, house finch, junco, indigo bird, robin,
bulbul, jay, magpie, chickadee.

The average results are shown in Table 2. The active selection
costs are set to be 200 queries for each targeted attack. In Table 2,
we find that the active selection strategy leads to lower average
query counts than using a fixed size selected from {32,128,256}, but

Table 1: Comparisons between different attacks on success rate, average queries, L2 and L∞ distortions against InceptionV3. η
stands for momentum factors for momentum based SGD. Line 5 shows Region Attack results based on standard SGD.

ZOO
ZOO-L2
QL
QL-L∞
Region Attack
(h = 64 with SGD )
Region Attack
(h = 64; η = 0.4)
Region Attack
(h = 64; η = 0.7)
Region Attack
(h = 32; η = 0.7)
Region Attack
(size selection; η = 0.4)
Region Attack
(size selection;η = 0.7)

success rate
99.7%
99.6%
100%
100%

avg. queries
111,218
77,513
3,948
3,770

avg. L2
1.666
2.035
14.357
18.908

avg. L∞ size selection cost
0.0223
0.0238
0.050
0.169

-
-
-
-

100%

100%

100%

100%

100%

100%

1,959

18.222

0.169

1,701

24.323

0.167

1,734

35.320

0.208

2,941

32.605

0.180

1,945

31.976

0.173

1909

37.590

0.211

-

-

-

-

200

200

Table 2: Comparisons between different region sizes on suc-
cess rate, average queries against InceptionV3 on 20 targeted
attacks with η = 0.4.

(w, h)=32
(w, h)=64
(w, h)=128
(w, h)=256
size selection

success rate
100%
100%
100%
100%
100%

avg. queries
1344.00
1012.20
1408.05
2297.40
1033.25

size selection cost
-
-
-
-
200

marginally higher average query counts than using size 64. This
observation is consistent with Table 1 in which the active selection
strategy reports lower average query needs than attack with a fixed
size w, h = 32. This indicates that when one does not have prior
knowledge about the suitable size for an attack, our size selection
strategy will provide a satisfactory choice.

Besides, the average number of queries required for region size
64 is 1012, 2 times faster than region size 256. This observation
validates our opinion that the target model is robust to the size of the
object and a smaller but suitable region will lead to query reduction.
Moreover, Fig. 4 shows the detailed property of comparisons among
different sizes. Observing from Fig. 4, we remark that excessively
small region size such as w = h = 32 will lead to unstable results.

5.3.3 Comparisons of SGD with and without Momentum. We im-
plement attacks to test momentum method’s influence on SGD in
optimization. In Fig. 5a, we compare queries on 20 targeted attacks.
The blue line shows the query needs for gradient ascent without
momentum, which is above the yellow line showing query needs
with the momentum factor η = 0.7 most of the time. This obser-
vation corroborates that momentum method plays an important
role in stabilizing the optimization thus leading to faster conver-
gence. Furthermore, we examine different momentum factor η’s

influence on query complexity. For η ∈ {0, 0.1, ..., 0.9}, we report
the average number of queries over 20 targeted attacks in Fig. 5b.
One can find that the query count first goes down with the increase
of momentum factor η and then goes up when η is larger than 0.7.
As we analyzed before, a large momentum factor will hinder the
algorithm from exploring a new ascent direction, while a small
momentum factor merely has the power to maintain the correct
ascent direction.

5.4 Results on Partial-information Attack
In this battery of experiments, we evaluate the performance of
Region Attack under partial-information settings. Since Region
Attack and QL adopt different initialization methods, Region Attack
uses random initialization and QL uses examples from target class,
we can not compare them directly.

We first evaluate Region Attack on InceptionV3. Attacks under
partial information are more difficult than the one under full infor-
mation since we have to carefully choose small learning rate for
updating the adversarial image and mode σ of search distribution
to avoid stepping out of the TOP-K list. Assume we have access to
the TOP-20 predictions, the fifth predictions of InceptionV3 on the
gray image is whistle with confidence 1.21%. We set momentum
factor η = 0.1, region size h = 64 and batch size n = 40 and maxi-
mize its score until it is placed on the 1st place. In the experiments,
we find that Region Attack under the Gaussian search distribution
converges slowly with vibrations. This implies that adding small
noises to pixels does not effectively help search the useful gradi-
ent direction. We replace it with the Laplace distribution which
is sharper compared to Gaussian and observe better performance.
Within 10820 queries, label whistle is ranked on the first place. The
Laplace distribution will be used in the followed online experiments.

(a) Average query for different momentum

(a) Initial results

(b) Queries for twenty attacks

Figure 5: Query complexity comparisons under different
momentum factor. (a) Average queries for 20 targeted at-
tacks under different momentum factor. (b) Comparisons of
query for each attack with gradient ascent and momentum
gradient ascent at η = 0.7 .

5.5 Real-world Attack on Clarifai and Baidu

Cloud APIs

We test Region Attack against real-world image recognition systems
in this section. We choose food detection API from Clarifai which
offers label detection service to public, and animal identification
API from Baidu AI platform which provides fine-grained object
identification service as target models.

5.5.1 Results on Clarifai. Under partial-information settings, at-
tacking Clarifai is more challenging than attacking InceptionV3.
On one hand, the total number of labels is large and unknown. On
the other hand, Clarifai provides multi-label prediction results, and
the scores of returned list of labels are not summed to be 1. Further-
more, queries on Clarifai are expensive US$1.2 for 1,000 queries.
We show that Region Attack reports promising results even under
this hard scenario.

We implement the partial-information attack discussed in Sec-
tion 4. Since we cannot access to any natural input example and

(b) Targeted attack on apple

Figure 6: The Clarifai food detection API’s prediction results
on an adversarial image produced by Region Attack with η =
0.4 and query need as 5,000.

legal label, we select the gray image as the initialization of adversar-
ial examples and obtain the initial results shown in Fig. 6a. In Fig. 6a,
original image are labeled as vegetable by Clarifai with confidence
87.7%. We choose 13th label apple as target label to perform targeted
attack. With around 5,000 queries to the food API, we obtain the
adversarial image that is labeled as apple with 90% confidence as
shown in Fig. 6b.

5.5.2 Results on Baidu AI platform. We test the animal identifica-
tion API on Baidu Cloud and report the results in Fig. 7. Fig. 7a
gives the original prediction of the gray image, which is non-animal.
We treat it as a binary classification problem by first minimizing
the score of non-animal until it falls out of TOP-5 predictions. Fig.
7b shows identification result when non-animal falls out of TOP-5.
We pick Purple heron as our target label which has the lowest prob-
ability among TOP-5 predictions and maximize its score until its

score is raised to TOP-1. Fig. 7c gives predictions when adversarial
example are classified into Purple heron with high probability.

6 CONCLUSION
In this work, we introduce a new attack approach called input-free
attack. Under this setting, we propose Region Attack, a black-box
attack algorithm that can significantly reduce the query complexity.
Our extensive experiments show that our algorithm is quite effec-
tive: it can achieve a 100% targeted-attack success rate with less
than 2,000 queries on average on ImageNet dataset. Besides, it can
also defeat some real-world commercial classification systems. Our
results clearly suggest that, compared with traditional adversarial
attacks that require the adversarial example to be visually similar
to the original example, the input-free attack is more effective and
practical in query-limited settings.

ACKNOWLEDGMENTS
The authors would like to thank Sam Bretheim and the anony-
mous reviewers for their valuable feedback and helpful suggestions.
The work is supported by Australian Research Council Projects
FL-170100117 and DP-180103424, National Natural Science Foun-
dation of China (61772508, U1713213), Guangdong Technology
Project (2016B010108010, 2016B010125003, 2017B010110007), CAS
Key Technology Talent Program, Shenzhen Engineering Laboratory
for 3D Content Generating Technologies (NO. [2017]476).

REFERENCES
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
2016. TensorFlow: a system for large-scale machine learning. In Proceedings of
the 12th USENIX conference on Operating Systems Design and Implementation.
USENIX Association, 265–283.

[2] Marco Barreno, Blaine Nelson, Anthony D Joseph, and JD Tygar. 2010. The

security of machine learning. Machine Learning 81, 2 (2010), 121–148.

[3] Marco Barreno, Blaine Nelson, Russell Sears, Anthony D Joseph, and J Doug
Tygar. 2006. Can machine learning be secure?. In Proceedings of the 2006 ACM
Symposium on Information, computer and communications security. ACM, 16–25.
[4] Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2017. Decision-Based Ad-
versarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.
arXiv preprint arXiv:1712.04248 (2017).

[5] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of
neural networks. In IEEE Symposium on Security and Privacy (SP). IEEE, 39–57.
[6] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017.
Zoo: Zeroth order optimization based black-box attacks to deep neural networks
without training substitute models. In Proceedings of the 10th ACM Workshop on
Artificial Intelligence and Security. ACM, 15–26.

[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:
A large-scale hierarchical image database. In IEEE Conference on Computer Vision
and Pattern Recognition. IEEE, 248–255.

[8] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and
Jianguo Li. 2018. Boosting adversarial attacks with momentum. arXiv preprint
(2018).

[9] Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. 2017.
A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations.
arXiv preprint arXiv:1712.02779 (2017).

[10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).
[11] Jamie Hayes and George Danezis. 2017. Machine learning as an adversarial
service: Learning black-box adversarial examples. arXiv preprint arXiv:1708.05207
(2017).

[12] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-
box Adversarial Attacks with Limited Queries and Information. arXiv preprint
arXiv:1804.08598 (2018).

[13] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2016. Delving into transfer-
able adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770
(2016).

(a) Initial results

(b) Intermediate adversarial example

(c) Targeted attack on Purple heron

Figure 7: Region Attack on Baidu AI Platform with η=0.4. (a):
original prediction on gray image; (b): the intermediate ad-
versarial example when the label non-animal is perturbed
out of TOP-5 predictions with 1500 queries; (c): extra 2400
queries are needed to raise target label Purple heron to TOP-
1 prediction with score 0.0421. With further 3,000 queries,
the score for Purple heron is increased to 0.243, which is ten-
folds larger than that of second label.

[14] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2017. Towards deep learning models resistant to adversarial attacks.
arXiv preprint arXiv:1706.06083 (2017).

[15] Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.
Deepfool: a simple and accurate method to fool deep neural networks. In Pro-
ceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR).

[16] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep neural networks are easily
fooled: High confidence predictions for unrecognizable images. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. 427–436.
[17] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security. ACM, 506–519.

[18] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,
and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In IEEE European Symposium on Security and Privacy (EuroS&P). IEEE,
372–387.

[19] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean
Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
2015. Imagenet large scale visual recognition challenge. International Journal of

Computer Vision 115, 3 (2015), 211–252.

[20] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017.
Evolution strategies as a scalable alternative to reinforcement learning. arXiv
preprint arXiv:1703.03864 (2017).

[21] Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi. 2017. One pixel
attack for fooling deep neural networks. arXiv preprint arXiv:1710.08864 (2017).
[22] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
Wojna. 2016. Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
2818–2826.

[23] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
In ICLR. Citeseer.

[24] Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi,
Cho-Jui Hsieh, and Shin-Ming Cheng. 2018. AutoZOOM: Autoencoder-based
Zeroth Order Optimization Method for Attacking Black-box Neural Networks.
arXiv preprint arXiv:1805.11770 (2018).

[25] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Jürgen
Schmidhuber. 2014. Natural evolution strategies. Journal of Machine Learning
Research 15, 1 (2014), 949–980.

Figure 8: Adversarial noises, examples for InceptionV3 and ground-truth examples from ImageNet. Adversarial examples are
obtained by putting the gray image and adversarial noises together. The number before a label indicates the index of the label
in ImageNet dataset.

APPENDIX A. ADVERSARIAL EXAMPLES
Figure 8 shows some adversarial examples for targeted attacks
in ImageNet where clear texture patterns of target classes can be
observed from the adversarial examples.

8
1
0
2
 
p
e
S
 
9
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
8
1
9
2
0
.
9
0
8
1
:
v
i
X
r
a

Towards Query Efficient Black-box Attacks:
An Input-free Perspective

Yali Du∗123 Meng Fang∗4 Jinfeng Yi5 Jun Cheng26 Dacheng Tao3
1 Centre for Artificial Intelligence, FEIT, University of Technology Sydney, Australia
2 Guangdong Key Lab of Robotics and Intelligent System, Shenzhen Institutes of Advanced Technology, CAS, China
3 UBTECH Sydney AI Centre, School of IT, FEIT, the University of Sydney, Australia
4 Tencent AI Lab, Shenzhen, China 5 JD AI Research, Beijing, China 6 The Chinese University of Hong Kong
yali.du@student.uts.edu.au, mfang@tencent.com,yijinfeng@jd.com,
jun.cheng@siat.ac.cn, dacheng.tao@sydney.edu.au

ABSTRACT
Recent studies have highlighted that deep neural networks (DNNs)
are vulnerable to adversarial attacks, even in a black-box scenario.
However, most of the existing black-box attack algorithms need
to make a huge amount of queries to perform attacks, which is
not practical in the real world. We note one of the main reasons
for the massive queries is that the adversarial example is required
to be visually similar to the original image, but in many cases,
how adversarial examples look like does not matter much. It in-
spires us to introduce a new attack called input-free attack, under
which an adversary can choose an arbitrary image to start with
and is allowed to add perceptible perturbations on it. Following
this approach, we propose two techniques to significantly reduce
the query complexity. First, we initialize an adversarial example
with a gray color image on which every pixel has roughly the same
importance for the target model. Then we shrink the dimension of
the attack space by perturbing a small region and tiling it to cover
the input image. To make our algorithm more effective, we stabilize
a projected gradient ascent algorithm with momentum, and also
propose a heuristic approach for region size selection. Through
extensive experiments, we show that with only 1,701 queries on av-
erage, we can perturb a gray image to any target class of ImageNet
with a 100% success rate on InceptionV3. Besides, our algorithm
has successfully defeated two real-world systems, the Clarifai food
detection API and the Baidu Animal Identification API.

KEYWORDS
adversarial learning; black-box attack; input-free attack; region
attack; neural network

∗ Contributed equally.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
AISec ’18, October 19, 2018, Toronto, ON, Canada
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-6004-3/18/10. . . $15.00
https://doi.org/10.1145/3270101.3270106

1 INTRODUCTION
Given a legitimate example, adversarial attacks aim to generate
adversarial examples by adding human-imperceptible perturba-
tions [5, 10, 15]. Depending on how much information an adversary
can access to, attacks can be classified as white-box or black-box.
When performing a white-box attack, the adversary has full control
and complete access to the target model [5, 10, 15]. While perform-
ing a black-box attack, the adversary has no access to the explicit
knowledge of the underlying model, but it can make queries to
obtain corresponding outputs [4, 6, 11–13, 24].

Compared with white-box attacks, black-box attacks are more
realistic and more challenging. Although many black-box attack
methods have been proposed, most of them suffer from high query
complexity which is proportional to the size of the image. For ex-
ample, attacking a 299 × 299 × 3 ImageNet image can usually take
hundreds of thousands of queries, or even more. Indeed, most of
these approaches would fail by simply setting an upper limit on the
number of queries one can make. Note that an important reason
for the high query complexity is that the adversarial example is
required to be visually similar to the original image. This is because
each pixel on the original image usually plays a significantly dif-
ferent role on the model’s outputs, and the adversary may need to
query all of the pixels to understand their roles. However, a primary
goal of black-box attack is to break down the underlying target
model with as little cost as possible. When this goal is achieved, how
adversarial examples look like does not matter too much. For this
reason, we re-evaluate the necessity of the common practice that
adversarial examples should be visually similar to the meaningful
input images.

In this work, we focus on a specific type of attack, which we
refer to as “input-free" attack. With this attack, adversaries are free
to choose any initial image to perform attacks. A typical initial
image is a gray color image with each pixel value equal to 128.
One significant advantage of this kind of initial image is that all of
the pixels have exactly the same intensity, thus they should have
the same importance. Note that many black-box adversarial attack
algorithms need to make a large number of queries to recognize
the importance of each pixel, starting from a gray color image can
greatly reduce the query complexity. Besides, “input-free" attacks
are more desirable than traditional adversarial attacks when benign
examples are not accessible. For instance, an adversary may have
no access to any rightful faces that can unlock a face recognition
system.

Under input-free settings, the adversarial attack becomes slightly
simpler since the distortion constraint is removed. In other words,
an input-free attack can choose an arbitrary image to start with and
is allowed to add perceptible perturbations on it. The effectiveness
of an input-free attack is measured by the cost spent during the
attack. The cost is usually represented by the number of queries,
and the lower the number is, the more effective the attack is. We
note that [16] also lies in the scope of input-free attack, in which
the adversarial examples were generated by starting from a ran-
dom image. However, [16] doesn’t focus on improving the query
complexity of black-box attacks, thus is different from our concern.
Recently, Ilyas et al. [12] proposed to adopt Natural Evolutionary
Strategies (NES) [25] to optimize black-box attacks. It shows that
fewer queries are needed for the gradient estimation than the finite-
difference methods [6]. Based on the NES approach [12, 25], we
propose two techniques to further improve the query complex-
ity, namely gray image attack and region-based attack, which are
explained as follows.

Gray image attack. First, we choose a gray color image with
uniform pixel values as the starting point, and then keep adding
noises on it until it is successfully classified into the target class. The
gray image does not contain any human perceivable information,
which is different from the traditional perturbation constraint on the
adversarial examples. In other words, this kind of attack abandons
the similarity constraint that most standard attacks used [6, 12].
Since all of the pixels are initialized to be the same value, they would
be almost equally important for the network output. In this case, a
gray image evades the strength of convolution neural networks in
extracting representational features from input images, rendering
it easier to be classified into other classes. Note that the gray color
image is not the only choice for input-free attack. We have tried
other natural images such as rocks and woods and they all lead to
satisfactory performance.

Region-based attack. The equivalence of pixels leads us to con-
sider whether it is necessary to query the whole image to synthesize
an adversarial example. When developing an image recognition
system, the model’s sensitivity to common object deformations
such as translation and scaling are implicitly considered in general.
Inspired by this fact, we do not need to query the whole image.
Instead, we perturb a much smaller region and then tile it to cover
the whole image to match the input dimension. This operation
can significantly reduce the dimension of attack space, and thus
improve the query complexity.

We name the proposed attack method as Region Attack, and
evaluate it on InceptionV3 [22], a popular classifier trained on
ImageNet [7]. Figure 1 shows one example of the proposed attack
algorithm. The adversarial example is initialized by a gray image,
and after only 735 queries, the output image is misclassified to the
target class Stingray. Our empirical study shows that on average,
the proposed attack algorithm only needs 1,701 queries to reach
100% success rate with target attack on ImageNet. Furthermore, We
test the proposed Region Attack on some challenging real-world
systems, including Clarifai food detection API1 and Baidu Animal

Identification API2. These two APIs only return a top list of labels
and scores that do not sum to one. We show that Region Attack
can still succeed within a small amount of queries.

Our main contributions are summarized as follows:

• We show that the query complexity of black-box attacks
can be significantly improved by starting from a gray image.
This opens up a prominent direction of performing efficient
attacks by designing proper input images.

• We shrink the dimension of the attack space by perturbing
a region and tiling it to the whole image. Compared with
the baseline models, this technique reduces average query
requirement by at least a factor of two. Besides, we also
propose a heuristic algorithm for region size selection.
• We show that momentum-based stochastic gradient descent
(SGD) can stabilize the update of the search distribution,
which was also examined to boost traditional gradient-based
adversarial attacks [5, 8].

• We show that the proposed model could successfully at-
tack commercial online classification systems. To the best
of our knowledge, the proposed attack model is one of the
pioneering works that reduces query complexity from the
perspective of input-free attack.

2 RELATED WORKS
The research of adversarial attacks is rooted in the need for un-
derstanding the security of machine learning models and hence
deploying reliable systems in real-world applications [2, 3]. After
Szegedy et al. [23] pointed out the existence of adversarial exam-
ples, there is a long line of works [4–6, 9, 10, 12, 15, 17, 18] that
produce adversarial examples in different settings.

The adversarial attack approaches fall into two categories: white-
box and black-box attacks. White-box attacks [5, 10, 15, 18, 23]
explore the target model’s architecture and parameter weights
to find the gradient direction to update adversarial perturbations.
However, when target models do not release their architectures to
the public, it is impossible for white-box attacks to back propagate
the gradients of the network output with regard to the input image.
Black-box attacks do not require the architecture or parameters
of the target model, but need to query the target model to obtain
feedback on the adversarial examples. The transferability of ad-
versarial examples [23], [17] can construct a substitute model to
perform white-box attacks and take the query results from the tar-
get model to refine the substitute model’s classification boundary.
Since training a substitute model is cumbersome and adversarial ex-
amples do not always transfer well, black-box attacks based solely
on queries are in highly demand.

When black-box attacks have only query access to a target model
[6, 11–13], to imitate gradient information, [6, 12] performed black-
box attacks by zeroth order optimization. [6] proposed to estimate
gradients via coordinate-wise finite differences based on probability
vector of different classes. [12] maximized the expected value of
loss function by a search distribution, and approximated gradients
by Natural Evolution Strategies [25] through a Gaussian search
distribution. These methods rely on probabilities show the relative
goodness of adversarial examples. [4] does not require probability to

1https://clarifai.com/models

2https://cloud.baidu.com/product/imagerecognition/fine−grained

Figure 1: Illustration of adversarial examples on ImageNet for Stringray class with 735 queries. The perturbations show re-
peated patterns of noises and the resulting image contains the texture and edge informaiton of Stringray class.

estimate gradients but only labels of the target model. It initialized
the adversarial example by an image in target class and greedily
reduced the distance between the adversarial example and the
image from the target class, and preserved the prediction results of
the adversarial example in the target class. The number of queries
of this algorithm was as high as 400,000 for falsifying an adversarial
example for a simple neural classifier on CIFAR10.

Existing research under input-free attack is scarce. Nguyen et al.
[16] evolved from uniform random noise and compressible patterns
to fool deep neural networks. [16] aimed to produce adversarial
examples with high confidence, but the efficiency of the algorithm
was not guaranteed. Ilyas et al. [12] claimed that maximizing the
expected value of fitness function by Natural Evolution Strategies
requires fewer queries than typical finite-difference methods. How-
ever, it still needs a median of more than 10,000 queries to attack
InceptionV3 classifier on ImageNet. [9] studied rotations and trans-
lations for adversarial attacks which supposedly requires fewer
queries, but the success of its attacks was not guaranteed.

The proposed region-based attack restricts the dimension of the
attack space; in this respect, [6] adopted an attack-space dimension
technique which first queries a small-sized image to generate adver-
sarial noises and then upscales it to the dimension of the original
image. [21] performed attacks by modifying one or few pixels but
the success rate of targeted attacks was low.

3 QUERY EFFICIENT BLACK-BOX ATTACKS

BASED ON NATURAL EVOLUTION
STRATEGIES
3.1 Preliminaries

Deep Neural Networks and Adversarial Attacks. Following the
naming conventions in [5], we denote x ∈ [0, 1]d as an input image,
F (x) as the output of the softmax layer and Z (x) as the logit layer
which is the output of the last layer before softmax. C(x) gives final
label prediction. Specifically:

F (x) = softmax(Z (x)),
C(x) = arg max
[F (x)]i .

i
Assume original prediction on x satisfies that C(x) = y, an adversary
aims to either reduce the classifier’s confidence on correct labels
or alter the classifier’s prediction to any specified class by adding

spuriously crafted noise on the image x. In untargeted attacks, an
adversary intends to perturb the input x with maliciously crafted
noises δ such that

C(x + δ ) (cid:44) y and ∥δ ∥ < c,

where ∥ · ∥ is a norm chosen by the attacker, such as L2, L∞ and
c is a pre-specified constant. Targeted attacks craft adversarial
perturbations that explicitly classify an input image into another
specified class y ′, which is formulated as follows:

C(x + δ ) = y ′, y ′ (cid:44) y, and ∥δ ∥ < c.

(1)

The formulation in Eq. (1) can not be directly solved by existing
algorithms, as the equality constraint C(x + δ ) = y ′ is highly non-
linear. Accordingly, one can express it in an equivalent form that is
more suitable for optimization [5]:

f (x, y ′) = max{max
t (cid:44)y′

log[F (x)]t − log[F (x)]y′, −κ},

(2)

where κ is a suitable constant. Eq. (2) is more suitable for optimiza-
tion and has been applied to both white-box and black-box attacks.
When performing white-box attacks, one can directly take gradient
derivation to image x to minimize the loss function f (x, y ′). When
performing black-box attacks, gradient can be approximated by
pixel-wise finite differences.

Black-box optimization by Natural Evolution Strategies. To per-
form a black-box attack, one needs to search the input space for
adversarial noise which satisfies Eq. (1). Existing works mainly
solve Eq. (2) through minimizing the objective function. In this
work we adopt Natural Evolutionary Strategies [25] which is used
in [12] to estimate gradients from probabilities of predictions for
derivative-free optimization. Instead of maximizing the objective
function directly, NES optimize the expected value of a fitness func-
tion under a search distribution.

The procedure of optimization by NES can be summarized as
four steps from [25]: (i) one samples a batch of search points from
the chosen parameterized search distribution and evaluates the
fitness function on them; (ii) NES estimates search gradients for the
parameters of the search distribution; (iii) NES performs gradient
ascent step along the search gradient direction; (iv) NES repeats
the above three steps until a stopping criterion is satisfied. Denote
the fitness function as J (θ ) = P(y|θ ) and x as parameters for search

distribution, the formulation of search gradients can be derived as
follows, per [12, 25]:

E

π (θ |x )[J (θ )] =

J (θ )π (θ |x)dθ

∫

∫

∇x E

π (θ |x )[J (θ )] = ∇x
∫

J (θ )π (θ |x)dθ

=

J (θ )∇x π (θ |x)dθ .

Applying the “log-likelihood” trick yields:

∇x E

π (θ |x )[J (θ )] =

J (θ )

π (θ |x)
π (θ |x)

∇x π (θ |x)dθ

∫

∫

=

= E

π (θ |x)J (θ )∇x log π (θ |x)dθ
π (θ |x )[J (θ )∇x log π (θ |x)].

(3)

(4)

The derivations in Eq. (3) gives mathematical foundations for steps
(ii) and (iii). From samples θ1, ..., θn , the search gradient for the loss
function J (θ ) is reduced to this form:

∇x E[J (θ )] ≈

J (θk )∇θ

log π (θk |x).

1

n

n
(cid:213)

k =1

To avoid non-smoothness of the fitness function, local reparam-
eterization trick [12, 20] can be used by setting θ = x + σδ with
δ ∼ N (0, I ). In this case, sampling θ is equal to sampling δ , and
at each step, we are literally searching the neighborhood of x for
a better solution. Denote n as batch size which is the number of
search points used in each step for estimating gradients, ∇x E[J (θ )]
is approximated by:

1

n
(cid:213)

σn

∇x E[J (θ )] ≈

J (θ + σδk ) · δk .

(5)

k =1
Rather than sampling n search points independently, we follow
[12, 20, 25] and implement an antithetic sampling strategy such
that δk ∼ N (0, I ), for k = 1, 2, ..., n
= −δn−k +1, for k =
n
2 + 1, ..., n. The search gradient under Gaussian distribution can
now be estimated by:

2 , and δk

∇x E[J (θ )] ≈

(δk J (θ + σδk ) − δk J (θ − σδk )).

(6)

1

σn

n/2
(cid:213)

k =1

3.2 The Proposed Attacks by Gray Images
With the proposed black-box attack, the adversary does not have
access to any natural input, which is different from the existing
attack algorithms. By normalizing the pixel values within [0, 1], we
initialize the input to the target model with x = [xi j ]n×m, xi j =
0.5 ∀(i, j) and call it “gray image”. This initialization leaves us much
flexibility since we can perturb the pixel values to be either brighter
(closer to 1) or darker (closer to 0) to falsify an example into target
class. One can choose different methods to initialize the input other
than a gray image.

We first define a fitness function which satisfies that its maximum
is obtained when C(x + δ ) = y ′ and it is dependent on the input to
the target model. Denote input θ = x + δ , the fitness function is
defined as:

J (θ ) = P(y ′|θ ) ≈ [F (θ )]y′ .

(7)

We adopt NES optimization and maximize P(y ′|θ ) directly until
C(x + δ ) = y ′. We choose Gaussian as search distribution for θ
which provides a simple form for gradient estimation [12, 20]. For a
Gaussian distribution θ ∼ N (ψd , σd ×d ), parameters to be optimized
. By performing gradient
are the mean ψd
ascent for parameters, the expected value of fitness function is
maximized.

and covariance σd ×d

Aside from Gaussian, other distributions whose derivatives of
its log-density with regard to their parameters are attainable can
be used as search distributions, such as the Laplace distribution. By
setting θ = x + bδ with δ ∼ Laplace(0, 1), the search gradient of
the fitness function can be estimated in the following form:

∇x E[J (θ )] ≈

J (θ + bδk ) · sign(δk ).

(8)

1

n
(cid:213)

bn

k =1

Note that we adopt a region-based attack algorithm, so gradients
are dependent on regions. We denote the gradient as д(h, w) with
h, w denoting region size. The details will be expanded in Section
3.3. After evaluating the gradients, we perform projected gradient
ascent as in [12, 14] by using the sign of the obtained gradients to
update x:

x t = Π[0,1](x t −1 + γ sign(дt (h, w)),
(9)
where γ denotes the learning rate and Π[0,1] denotes the projection
operator that clips pixel values into [0, 1]. The adversarial image
is updated by the search gradients until the adversarial goal is
achieved.

We show the overall process of Region Attack in Fig. 2. The
adversarial example is initialized with a gray image. We produce
search gradients in a small region indicated by the red box and tile it
to the whole image; thus, the gradient information for each region
in the image is exactly the same. After black-box optimization,
we obtain the adversarial example with clear texture and edging
information showing in each region.

Several issues require our attention for implementing Region
Attacks. On one hand, how to set up an appropriate region size?
On the other hand, since gradients are estimated based on the
finite sampling of search points, the variance will be introduced
and prohibits convergence of the algorithm. How to tame this
challenge? First, we summarize the general procedures for the gray
image attack in Algorithm 1, and then discuss the key steps in the
remaining section. Step 1, 3, and 4 in Algorithm 1 will be further
explained in Section 3.3, 3.4, and 3.5.

Algorithm 1 Canonical Procedures for Query Efficient Attack
Input: target class y ′, gray image x, max iteration maxiter
Output: adversarial image x ′ such at C(x ′) = y ′

1: set up the region size (h, w)
2: for t = 1, 2, ..., maxiter do
3: estimate gradient дt (h, w) by region mutation
4: x t = Π[0,1](x t −1 + γ sign(дt (h, w)))
5: if C(x t ) = y ′ :

success = True
return xi

6: end if
7: end for

Figure 2: Overview of the proposed Region Attack algorithm. The input is a gray image. By querying the black-box classifier,
we obtain the gradients to update a region on the image in the red box. This process is repeated until we reach an adversarial
example that is identified to be the target class.

3.3 Gradient Estimation based on Region

size as:

Mutation

We denote a perturbed region as R = (p1, p2, h, w), where (p1, p2) is
the position of the pixel at the top left corner of the chosen region,
and h, w indicate the height and width of the perturbed region. We
tile the region to the whole image such that every region of size
h × w has the same pixel values. Now the dimension of the attack
space we need to optimize is h × w; in fact, adopting a small region
size will lead to a reduction on dimension of the attack space. If
h ×w cannot be divided by d
⌋ regions in the middle
of the image and adopt symmetric padding to complete the whole
image.

2, we place ⌊ d 2
hw

Based on the above discussion, the gradient estimation over the
whole image is aggregated by gradients on different regions. We
summarize the gradient estimation procedures in Algorithm 2 based
on antithetic sampling [12, 20], which will be used to replace Step
3 in Algorithm 1.

Algorithm 2 NES Gradient Estimation for Region Mutation
Input: target class t, image x of dimension d, classifier P(y|x),

batch size n, region size h, w
Output: gradient estimation д

1: д ← 0d
2: for i = 1, 2, ..., n
3: sample ϵk ∼ N (0
by ⌈ d 2
4: repeat ϵk
hw
5: д = д + 1
σ P(y|x + σ · ˜ϵk ) · ˜ϵk
6: д = д − 1
σ P(y|x − σ · ˜ϵk ) · ˜ϵk
7: end for
8: return д
n

2 do
hw , δhw ×hw )
⌉ times thus obtain ˜ϵk ∈ Rd

3.4 Active Selection on Region Sizes
We propose a heuristic approach based on a limited query budget.
We use the variation of entropy to measure whether a chosen size
is effective for Region Attack on the target model. We set the batch
size as n0, and take t0 iterations in this warm-up section. Define
the entropy of the target model’s prediction under current region

h(F (x)) =

[F (x)]i log[F (x)]i .

(10)

n
(cid:213)

i=1

We use its variation ∆h to measure the effectiveness of current
selected size, which is defined as

(11)

∆h = ht0 − min(hj ), j ∈ {1, ..., t0},
where hj denote the entropy calculated after j−th updating step.
The larger the variation of entropy is, the more capable we are
to alter the prediction of the original input. This ensures that we
are more likely to reach the target goal under the same number
of queries. In fact, most of the targeted attacks would experience
such two stages that the entropy hj of predicted probability vector
first goes down and then goes up. In the first stage, Region Attack
reduces the labeling information of the original prediction on the
gray image, which leads to the decrease of hj . In the second stage,
Region Attack raises the labeling information of the target class
which corresponds to the increase of hj . The steps of this approach
are listed as follows:

(1) we perform momentum gradient ascent for t0 iterations and

record the output of softmax layer F (x);

(2) we calculate the entropy of F (x) after each iteration to obtain

hj ;

(3) we estimate the variation of entropies of these t0 iterations
and return the size that corresponds to the largest variance.
The region size used in Step 1 in Algorithm 1 can be set up by these
procedures.

3.5 Momentum-based Gradient Ascent
Since one can not search the whole distribution to calculate the
accurate gradient, one always samples a batch of points from search
distribution to estimate gradients. Due to the limited sample size
for one batch, the variance would be introduced into the gradients
and hurts the convergence of the algorithm. Increasing the sample
size would reduce the variance for each estimation of the gradient.
However, monotonically increasing the sample size for one draw
will increase query complexity. Note that the momentum-based
gradient descent has been verified in [5, 8] to be effective in stabiliz-
ing update directions of objective function which is mentioned but
not discussed in [12]. Following this route, we use the approach of

momentum-based gradient ascent to update the mean of the search
distribution, which is exactly the adversarial image:

дt = ηдt −1 + (1 − η)∇θ J (θ )
x t = Π[0,1](x t −1 + γ sign(дt )).

(12)

One thing to mention is that we do not follow the convention to set
η to 0.9 or a similar value, we instead set it to be around 0.5. Since
we use the sign of gradients to update adversarial perturbations,
setting η = 0.9 will lead to the situation that the algorithm keeps
exploiting in the same direction without exploring new directions.

4 PARTIAL-INFORMATION ATTACK
In addition to the discussion in Section 3, we note that many open
cloud APIs provide only TOP-k scores of detected labels that are
not summed to 1. This situation is difficult since neither the number
of labels nor the probability of all labels is given. Furthermore, in
our procedure, an attacker does not have access to benign inputs,
and a random initialization might lead to the prediction as “non-
object” or “background”. This makes it more challenging than the
partial-information attack discussed in [12].

We deploy a two stage process for efficient attacks. Firstly, we
treat a target model as a binary classifier which produces the label
of either “non-object” or “is object”. Now it is possible to perform an
untargeted attack by minimizing the probability of P(non-object|x).
We minimize P(non-object|x) until it falls out of TOP-K predic-
tions. Secondly, we choose a legal label from TOP-K{P(·|x)} as a
target class y ′ and maximize P(y ′|x) until the target model confi-
dently recognizes x ′ as an legal object. In the first stage, we reverse
gradient ascent steps in Section 3.3 to gradient descent to mini-
mize P(non-object|x). In the second stage, we follow the two-step
optimization procedure used in [4, 12] which alternates between
maximizing probability of y ′ as P(y ′|x) and retaining y ′ in the TOP-
K{P(·|x)} predictions by shrinking the learning rate. The general
procedures are summarized into the following four steps.

(1) initialize an input x0 and its label y0 = C(x0) and
specify a classifier P(·|x) that returns TOP-K labels
and scores;

(2) minimize P(y0|x) until y0 (cid:60)TOP-K{P(·|x)};
(3) choose y ′ ∈ TOP-K{P(·|x)} as the target class;
(4) alternate between the following two steps:

(a) maximizing the score of the target class: x t =
arg maxx P(y ′|x);
(b) adjusting the learning rate γ such that: γ =
max γ ′ s.t. rank(y ′|x t ) ≤ k;

(5) quit when P(y ′|x) > c where c is a specified constant.

5 EXPERIMENTS
We examine the proposed algorithm’s efficiency in generating tar-
get adversarial examples and reducing query complexity. We would
like to show that the proposed algorithm achieves a high success
rate and leads to a significant reduction in query complexity. Fur-
thermore, we demonstrate that Region Attack can fool many open
Cloud APIs with affordable number of queries.

5.1 Baselines
We adopt two recent attacks ZOO [6] and Query-Limited (QL) [12]
as baselines for comparison. Since the similarity between images is
not the main issue in our settings, to be fair, by removing the simi-
larity constrain in objective functions, we construct two stronger
baselines ZOO-L2 and QL-L∞ with necessary modifications based
on ZOO and QL where “-” should be read as “minus”.

• ZOO [6] estimates gradients by pixel-by-pixel finite differ-
ences and adopts Adam optimizer to optimize the adversarial
perturbations. It includes L2 norm as distance loss to con-
strain adversarial examples to not be far away from the
initialization. We use its public implementation.

• ZOO-L2 is constructed by slightly modifying ZOO. Since the
proposed setting does not consider distance loss, to make
fair comparisons, we remove the distance loss in ZOO and
construct a more effective baseline denoted as ZOO-L2.
• QL [12] applies Natural Evolutionary Strategies to perform
black-box optimization and adopts L∞ to constrain the dis-
tance between adversarial and benign inputs. It reports 1∼2
orders of magnitude reduction on query complexity com-
pared to ZOO. We use its implementation without modifica-
tions.

• QL-L∞ is a slightly modified version of QL with L∞ distance
removed, which is supposed to be more query-efficient than
QL.

To this end, we have four strong baselines. Since both ZOO and QL
implemented attacks on InceptionV3, we will report attack results
on this classifier.

5.2 Setup
We choose InceptionV3 as the target model with default input
size 299 × 299 × 3, which is consistent with the settings in ZOO
and QL. InceptionV3 [1, 22] achieves 78% top-1 accuracy for the
ImageNet dataset [19]. All attacks are restricted to have access only
to the probability vectors of all possible classes. The default search
distribution used through experiments is the Gaussian distribution.
We produce a gray color image with size 299 × 299 × 3 and pixel
values equal to 128, which are normalized to 0.5 for optimization
as the initialized input for all baselines in different targeted attacks.
For this same gray image x, we choose different target class ti , i =
1, ..., 1000, and construct the test set S = {(x, t1), ..., (x, t1000)}.
Note that in our experiments, we always use a square region by
setting w = h for the object. We will only specify the height of the
region in the following.

Metrics. Denote Ω as the set of successfully attacked target
classes and qi as the query required for attacking i-th target class,
we define two measurements: success rate r and average number
of queries qavд:

r =

∥Ω∥
1000 ,
(cid:205)

qavд =

i ∈Ω qi
∥Ω∥

.

Success rate indicates the capability of an algorithm to cheat a clas-
sifier. Average number of queries indicates the cost of an algorithm

to attack. The fewer the number of queries are, the more efficient
an algorithm is.

Since ZOO and QL punish L2 and L∞ distortion of adversarial
examples, we report the averaged L2 and L∞ distortions in Section
5.3. Denote the L2 and L∞ distortions for i-th attack as δi,2, δi,∞,
the averaged L2, L∞ distortion are defined as:

δavд,2 =

δavд,∞ =

(cid:205)

(cid:205)

,

i ∈Ω δi,2
∥Ω∥
i ∈Ω δi,∞
∥Ω∥

.

Average distortion means the average distance between initial gray
image and adversarial examples when an attack succeeds. Averaged
distortions are reported for readers’ reference and are not consid-
ered to be the main measurement since we consider input-free
attacks. The implementation code is publicly available.3

5.3 Results on Query Efficient Attack
For Region Attack, we choose different region sizes and momentum
factors, then we run the proposed threat model and four baselines
on the test set S. It is important to note that the algorithms and
Region Attack are tested under the same gray image and target class
set. The results are reported in Table 1. Some adversarial examples,
adversarial noises, and ground-truth images will be shown in the
appendix. The attack is not restricted to gray images. In fact, we
tried other natural images like rocks and woods and also obtained
good performance. Since texture patterns are not explicitly shown
in these perturbed natural images, they are not displayed in the
paper.

In Table 1, the proposed algorithm obtains 100% success rate
which is the same as QL and QL-L∞, but higher than ZOO and
ZOO-L2. It is shown that for the proposed algorithm with the fixed
region size w = h = 64, the query complexity is only around 1700
on average, which is 60 times more efficient than ZOO and ZOO-L2,
and 2 times faster than QL and QL-L∞.

To evaluate distance loss’s influence on query complexity, we
compare ZOO with ZOO-L2 and QL with QL-L∞. We observe that
without L2 distance loss between benign and adversarial images,
ZOO-L2 requires fewer queries and achieves comparable success
rate compared to ZOO. Removing L∞ norm also reduces QL-L∞’s
query needs, and as a result, produces larger distortions than QL.
We remark that removing the perturbation constraint will help in
query reduction. Note that comparing to the query needs reported
by Chen et al. [6], ZOO-L2 reduces the query counts from more than
2 millions to 77,513, which is 1 ∼ 2 orders of magnitude. Compared
with QL-L∞, which uses the same black-box optimization as Region
Attack, we reduce the query needs by half. This indicates that only
removing the distance loss in existing attacks does not produce
efficient attacks; our Region Attack reduces the dimension of the
attack space and allows fewer queries in gradient estimation.

5.3.1 Query distribution comparison. Since QL-L∞ is the strongest
baseline method, we compare its query distributions with Region
Attack in Fig. 3. Note that the maximal and minimal number of
queries for QL-L∞ and Region Attack are 19431 and 51 respectively;
we thus divide queries of 1000 attacks into 39 bins with each bin

3https://github.com/yalidu/input-free-attack

Figure 3: Query distributions of Region attack and QL-L∞
methods on 1000 targeted attacks with η = 0.7, h = 64. The
queries are divided into 39 bins with each bin spanning a
width of 500 queries. Blue bar shows the query distribution
for Region Attack; Orange bar shows queries for QL-L∞.

Figure 4: Query comparison of different region sizes un-
der 20 targeted attacks. Centerline represents the median.
Box extents show 25th and 75th percentile. Whisker ex-
tents illustrate maximum and minimum values. Black cir-
cles show outliers that are too far from the median such as
6000 queries for target class tench under size 32.

spanning a width of 500 queries. The highest blue bar shows that 252
targeted attacks are completed within queries [500, 1000). Dividing
the histogram by query counts of 2500, 83.6% attacks of Region
Attack are less than 2500 while it is only 31.8% for QL-L∞.

Influence of size selection on query complexity. We compare
5.3.2
the active size selection algorithm with fixed sizes chosen from
{32, 64, 128, 256} and report query complexity on attacking the
first 20 classes of ImageNet which are tench, goldfish, great white
shark, tiger shark, hammerhead shark, electric ray, stingray, cock, hen,
ostrich, brambling, goldfinch, house finch, junco, indigo bird, robin,
bulbul, jay, magpie, chickadee.

The average results are shown in Table 2. The active selection
costs are set to be 200 queries for each targeted attack. In Table 2,
we find that the active selection strategy leads to lower average
query counts than using a fixed size selected from {32,128,256}, but

Table 1: Comparisons between different attacks on success rate, average queries, L2 and L∞ distortions against InceptionV3. η
stands for momentum factors for momentum based SGD. Line 5 shows Region Attack results based on standard SGD.

ZOO
ZOO-L2
QL
QL-L∞
Region Attack
(h = 64 with SGD )
Region Attack
(h = 64; η = 0.4)
Region Attack
(h = 64; η = 0.7)
Region Attack
(h = 32; η = 0.7)
Region Attack
(size selection; η = 0.4)
Region Attack
(size selection;η = 0.7)

success rate
99.7%
99.6%
100%
100%

avg. queries
111,218
77,513
3,948
3,770

avg. L2
1.666
2.035
14.357
18.908

avg. L∞ size selection cost
0.0223
0.0238
0.050
0.169

-
-
-
-

100%

100%

100%

100%

100%

100%

1,959

18.222

0.169

1,701

24.323

0.167

1,734

35.320

0.208

2,941

32.605

0.180

1,945

31.976

0.173

1909

37.590

0.211

-

-

-

-

200

200

Table 2: Comparisons between different region sizes on suc-
cess rate, average queries against InceptionV3 on 20 targeted
attacks with η = 0.4.

(w, h)=32
(w, h)=64
(w, h)=128
(w, h)=256
size selection

success rate
100%
100%
100%
100%
100%

avg. queries
1344.00
1012.20
1408.05
2297.40
1033.25

size selection cost
-
-
-
-
200

marginally higher average query counts than using size 64. This
observation is consistent with Table 1 in which the active selection
strategy reports lower average query needs than attack with a fixed
size w, h = 32. This indicates that when one does not have prior
knowledge about the suitable size for an attack, our size selection
strategy will provide a satisfactory choice.

Besides, the average number of queries required for region size
64 is 1012, 2 times faster than region size 256. This observation
validates our opinion that the target model is robust to the size of the
object and a smaller but suitable region will lead to query reduction.
Moreover, Fig. 4 shows the detailed property of comparisons among
different sizes. Observing from Fig. 4, we remark that excessively
small region size such as w = h = 32 will lead to unstable results.

5.3.3 Comparisons of SGD with and without Momentum. We im-
plement attacks to test momentum method’s influence on SGD in
optimization. In Fig. 5a, we compare queries on 20 targeted attacks.
The blue line shows the query needs for gradient ascent without
momentum, which is above the yellow line showing query needs
with the momentum factor η = 0.7 most of the time. This obser-
vation corroborates that momentum method plays an important
role in stabilizing the optimization thus leading to faster conver-
gence. Furthermore, we examine different momentum factor η’s

influence on query complexity. For η ∈ {0, 0.1, ..., 0.9}, we report
the average number of queries over 20 targeted attacks in Fig. 5b.
One can find that the query count first goes down with the increase
of momentum factor η and then goes up when η is larger than 0.7.
As we analyzed before, a large momentum factor will hinder the
algorithm from exploring a new ascent direction, while a small
momentum factor merely has the power to maintain the correct
ascent direction.

5.4 Results on Partial-information Attack
In this battery of experiments, we evaluate the performance of
Region Attack under partial-information settings. Since Region
Attack and QL adopt different initialization methods, Region Attack
uses random initialization and QL uses examples from target class,
we can not compare them directly.

We first evaluate Region Attack on InceptionV3. Attacks under
partial information are more difficult than the one under full infor-
mation since we have to carefully choose small learning rate for
updating the adversarial image and mode σ of search distribution
to avoid stepping out of the TOP-K list. Assume we have access to
the TOP-20 predictions, the fifth predictions of InceptionV3 on the
gray image is whistle with confidence 1.21%. We set momentum
factor η = 0.1, region size h = 64 and batch size n = 40 and maxi-
mize its score until it is placed on the 1st place. In the experiments,
we find that Region Attack under the Gaussian search distribution
converges slowly with vibrations. This implies that adding small
noises to pixels does not effectively help search the useful gradi-
ent direction. We replace it with the Laplace distribution which
is sharper compared to Gaussian and observe better performance.
Within 10820 queries, label whistle is ranked on the first place. The
Laplace distribution will be used in the followed online experiments.

(a) Average query for different momentum

(a) Initial results

(b) Queries for twenty attacks

Figure 5: Query complexity comparisons under different
momentum factor. (a) Average queries for 20 targeted at-
tacks under different momentum factor. (b) Comparisons of
query for each attack with gradient ascent and momentum
gradient ascent at η = 0.7 .

5.5 Real-world Attack on Clarifai and Baidu

Cloud APIs

We test Region Attack against real-world image recognition systems
in this section. We choose food detection API from Clarifai which
offers label detection service to public, and animal identification
API from Baidu AI platform which provides fine-grained object
identification service as target models.

5.5.1 Results on Clarifai. Under partial-information settings, at-
tacking Clarifai is more challenging than attacking InceptionV3.
On one hand, the total number of labels is large and unknown. On
the other hand, Clarifai provides multi-label prediction results, and
the scores of returned list of labels are not summed to be 1. Further-
more, queries on Clarifai are expensive US$1.2 for 1,000 queries.
We show that Region Attack reports promising results even under
this hard scenario.

We implement the partial-information attack discussed in Sec-
tion 4. Since we cannot access to any natural input example and

(b) Targeted attack on apple

Figure 6: The Clarifai food detection API’s prediction results
on an adversarial image produced by Region Attack with η =
0.4 and query need as 5,000.

legal label, we select the gray image as the initialization of adversar-
ial examples and obtain the initial results shown in Fig. 6a. In Fig. 6a,
original image are labeled as vegetable by Clarifai with confidence
87.7%. We choose 13th label apple as target label to perform targeted
attack. With around 5,000 queries to the food API, we obtain the
adversarial image that is labeled as apple with 90% confidence as
shown in Fig. 6b.

5.5.2 Results on Baidu AI platform. We test the animal identifica-
tion API on Baidu Cloud and report the results in Fig. 7. Fig. 7a
gives the original prediction of the gray image, which is non-animal.
We treat it as a binary classification problem by first minimizing
the score of non-animal until it falls out of TOP-5 predictions. Fig.
7b shows identification result when non-animal falls out of TOP-5.
We pick Purple heron as our target label which has the lowest prob-
ability among TOP-5 predictions and maximize its score until its

score is raised to TOP-1. Fig. 7c gives predictions when adversarial
example are classified into Purple heron with high probability.

6 CONCLUSION
In this work, we introduce a new attack approach called input-free
attack. Under this setting, we propose Region Attack, a black-box
attack algorithm that can significantly reduce the query complexity.
Our extensive experiments show that our algorithm is quite effec-
tive: it can achieve a 100% targeted-attack success rate with less
than 2,000 queries on average on ImageNet dataset. Besides, it can
also defeat some real-world commercial classification systems. Our
results clearly suggest that, compared with traditional adversarial
attacks that require the adversarial example to be visually similar
to the original example, the input-free attack is more effective and
practical in query-limited settings.

ACKNOWLEDGMENTS
The authors would like to thank Sam Bretheim and the anony-
mous reviewers for their valuable feedback and helpful suggestions.
The work is supported by Australian Research Council Projects
FL-170100117 and DP-180103424, National Natural Science Foun-
dation of China (61772508, U1713213), Guangdong Technology
Project (2016B010108010, 2016B010125003, 2017B010110007), CAS
Key Technology Talent Program, Shenzhen Engineering Laboratory
for 3D Content Generating Technologies (NO. [2017]476).

REFERENCES
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
2016. TensorFlow: a system for large-scale machine learning. In Proceedings of
the 12th USENIX conference on Operating Systems Design and Implementation.
USENIX Association, 265–283.

[2] Marco Barreno, Blaine Nelson, Anthony D Joseph, and JD Tygar. 2010. The

security of machine learning. Machine Learning 81, 2 (2010), 121–148.

[3] Marco Barreno, Blaine Nelson, Russell Sears, Anthony D Joseph, and J Doug
Tygar. 2006. Can machine learning be secure?. In Proceedings of the 2006 ACM
Symposium on Information, computer and communications security. ACM, 16–25.
[4] Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2017. Decision-Based Ad-
versarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.
arXiv preprint arXiv:1712.04248 (2017).

[5] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of
neural networks. In IEEE Symposium on Security and Privacy (SP). IEEE, 39–57.
[6] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017.
Zoo: Zeroth order optimization based black-box attacks to deep neural networks
without training substitute models. In Proceedings of the 10th ACM Workshop on
Artificial Intelligence and Security. ACM, 15–26.

[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:
A large-scale hierarchical image database. In IEEE Conference on Computer Vision
and Pattern Recognition. IEEE, 248–255.

[8] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and
Jianguo Li. 2018. Boosting adversarial attacks with momentum. arXiv preprint
(2018).

[9] Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. 2017.
A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations.
arXiv preprint arXiv:1712.02779 (2017).

[10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).
[11] Jamie Hayes and George Danezis. 2017. Machine learning as an adversarial
service: Learning black-box adversarial examples. arXiv preprint arXiv:1708.05207
(2017).

[12] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-
box Adversarial Attacks with Limited Queries and Information. arXiv preprint
arXiv:1804.08598 (2018).

[13] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. 2016. Delving into transfer-
able adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770
(2016).

(a) Initial results

(b) Intermediate adversarial example

(c) Targeted attack on Purple heron

Figure 7: Region Attack on Baidu AI Platform with η=0.4. (a):
original prediction on gray image; (b): the intermediate ad-
versarial example when the label non-animal is perturbed
out of TOP-5 predictions with 1500 queries; (c): extra 2400
queries are needed to raise target label Purple heron to TOP-
1 prediction with score 0.0421. With further 3,000 queries,
the score for Purple heron is increased to 0.243, which is ten-
folds larger than that of second label.

[14] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2017. Towards deep learning models resistant to adversarial attacks.
arXiv preprint arXiv:1706.06083 (2017).

[15] Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.
Deepfool: a simple and accurate method to fool deep neural networks. In Pro-
ceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR).

[16] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep neural networks are easily
fooled: High confidence predictions for unrecognizable images. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. 427–436.
[17] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security. ACM, 506–519.

[18] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,
and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In IEEE European Symposium on Security and Privacy (EuroS&P). IEEE,
372–387.

[19] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean
Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
2015. Imagenet large scale visual recognition challenge. International Journal of

Computer Vision 115, 3 (2015), 211–252.

[20] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017.
Evolution strategies as a scalable alternative to reinforcement learning. arXiv
preprint arXiv:1703.03864 (2017).

[21] Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi. 2017. One pixel
attack for fooling deep neural networks. arXiv preprint arXiv:1710.08864 (2017).
[22] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
Wojna. 2016. Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
2818–2826.

[23] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
In ICLR. Citeseer.

[24] Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi,
Cho-Jui Hsieh, and Shin-Ming Cheng. 2018. AutoZOOM: Autoencoder-based
Zeroth Order Optimization Method for Attacking Black-box Neural Networks.
arXiv preprint arXiv:1805.11770 (2018).

[25] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Jürgen
Schmidhuber. 2014. Natural evolution strategies. Journal of Machine Learning
Research 15, 1 (2014), 949–980.

Figure 8: Adversarial noises, examples for InceptionV3 and ground-truth examples from ImageNet. Adversarial examples are
obtained by putting the gray image and adversarial noises together. The number before a label indicates the index of the label
in ImageNet dataset.

APPENDIX A. ADVERSARIAL EXAMPLES
Figure 8 shows some adversarial examples for targeted attacks
in ImageNet where clear texture patterns of target classes can be
observed from the adversarial examples.

