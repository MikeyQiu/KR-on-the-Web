4
1
0
2
 
r
a

M
 
4
 
 
]
L
C
.
s
c
[
 
 
3
v
5
0
0
3
.
2
1
3
1
:
v
i
X
r
a

One Billion Word Benchmark for Measuring Progress in
Statistical Language Modeling

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants
Google
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA

Phillipp Koehn
University of Edinburgh
10 Crichton Street, Room 4.19
Edinburgh, EH8 9AB, UK

Tony Robinson
Cantab Research Ltd
St Johns Innovation Centre
Cowley Road, Cambridge, CB4 0WS, UK

Abstract

We propose a new benchmark corpus to be
used for measuring progress in statistical lan-
guage modeling. With almost one billion
words of training data, we hope this bench-
mark will be useful to quickly evaluate novel
language modeling techniques, and to com-
pare their contribution when combined with
other advanced techniques. We show perfor-
mance of several well-known types of lan-
guage models, with the best results achieved
with a recurrent neural network based lan-
guage model. The baseline unpruned Kneser-
Ney 5-gram model achieves perplexity 67.6.
A combination of techniques leads to 35%
reduction in perplexity, or 10% reduction in
cross-entropy (bits), over that baseline.

is

as

besides

available

a
benchmark
The
code.google.com project;
the
scripts needed to rebuild the training/held-out
data, it also makes available log-probability
values for each word in each of ten held-out
data sets, for each of the baseline n-gram
models.

1 Introduction

Statistical language modeling has been applied to a
wide range of applications and domains with great
success. To name a few, automatic speech recogni-
tion, machine translation, spelling correction, touch-
screen “soft” keyboards and many natural language
processing applications depend on the quality of lan-
guage models (LMs).

The performance of LMs is determined mostly by
several factors: the amount of training data, quality

and match of the training data to the test data, and
choice of modeling technique for estimation from
the data.
It is widely accepted that the amount of
data, and the ability of a given estimation algorithm
to accomodate large amounts of training are very im-
portant in providing a solution that competes suc-
cessfully with the entrenched n-gram LMs. At the
same time, scaling up a novel algorithm to a large
amount of data involves a large amount of work, and
provides a signiﬁcant barrier to entry for new mod-
eling techniques. By choosing one billion words as
the amount of training data we hope to strike a bal-
ance between the relevance of the benchmark in the
world of abundant data, and the ease with which any
researcher can evaluate a given modeling approach.
This follows the work of Goodman (2001a), who
explored performance of various language modeling
techniques when applied to large data sets. One of
the key contributions of our work is that the experi-
ments presented in this paper can be reproduced by
virtually anybody with an interest in LM, as we use
a data set that is freely available on the web.

Another contribution is that we provide strong
baseline results with the currently very popular neu-
ral network LM (Bengio et al., 2003). This should
allow researchers who work on competitive tech-
niques to quickly compare their results to the current
state of the art.

The paper is organized as follows: Section 2 de-
scribes how the training data was obtained; Section
3 provides a short overview of the language model-
ing techniques evaluated; ﬁnally, Section 4 presents
results obtained and Section 5 concludes the paper.

Model

Training Time

Perplexity

Interpolated KN 5-gram, 1.1B n-grams (KN)
Katz 5-gram, 1.1B n-grams
Stupid Backoff 5-gram (SBO)
Interpolated KN 5-gram, 15M n-grams
Katz 5-gram, 15M n-grams
Binary MaxEnt 5-gram (n-gram features)
Binary MaxEnt 5-gram (n-gram + skip-1 features)
Hierarchical Softmax MaxEnt 4-gram (HME)
Recurrent NN-256 + MaxEnt 9-gram
Recurrent NN-512 + MaxEnt 9-gram
Recurrent NN-1024 + MaxEnt 9-gram

Num. Params
[billions]
1.76
1.74
1.13
0.03
0.03
1.13
1.8
6
20
20
20

[hours]
3
2
0.4
3
2
1
1.25
3
60
120
240

[CPUs]
100
100
200
100
100
5000
5000
1
24
24
24

67.6
79.9
87.9
243.2
127.5
115.4
107.1
101.3
58.3
54.5
51.3

Table 1: Results on the 1B Word Benchmark test set with various types of language models.

2 Description of the Benchmark Data

In the following experiments, we used text data ob-
tained from the WMT11 website1. The data prepa-
ration process was performed as follows:

• All training monolingual/English corpora were

selected

• Normalization and tokenization was performed
using scripts distributed from the WMT11 site,
slightly augmented to normalize various UTF-8
variants for common punctuation, e.g. ’

• Duplicate sentences were removed, dropping
the number of words from about 2.9 bil-
lion to about 0.8 billion (829250940, more
exactly, counting sentence boundary markers
<S>, <\S>)

• Vocabulary (793471 words including sentence
boundary markers <S>, <\S>) was con-
structed by discarding all words with count be-
low 3

• Words outside of the vocabulary were mapped
to <UNK> token, also part of the vocabulary
• Sentence order was randomized, and the data

was split into 100 disjoint partitions

• One such partition (1%) of the data was chosen

as the held-out set

• The held-out set was then randomly shufﬂed
and split again into 50 disjoint partitions to be
used as development/test data

• One such resulting partition (2%, amounting to
159658 words without counting the sentence
beginning marker <S> which is never predicted
by the language model) of the held-out data
were used as test data in our experiments; the
remaining partitions are reserved for future ex-
periments

• The out-of-vocabulary (OoV) rate on the test

set was 0.28%.

The

benchmark

is
code.google.com

available
project:

a

as
https://code.google.com/p/1-billion-word-langua
the
needed
to
scripts
the
Besides
training/held-out data,
it also makes available
log-probability values for each word in each of ten
held-out data sets, for each of the baseline n-gram
models.

rebuild

Because the original data had already randomized
sentence order, the benchmark is not useful for ex-
periments with models that capture long context de-
pendencies across sentence boundaries.

3 Baseline Language Models

As baselines we chose to use (Katz, 1995), and
Interpolated (Kneser and Ney, 1995) (KN) 5-gram
LMs, as they are the most prevalent.
Since in
practice these models are pruned, often quite ag-
gressivley, we also illustrate the negative effect of
(Stolcke, 1998) entropy pruning on both models,
In particular KN
similar to (Chelba et al., 2010).

1http://statmt.org/wmt11/training-monolingual.tgz

smoothing degrades much more rapidly than Katz,
calling for a discerning choice in a given applica-
tion.

4 Advanced Language Modeling

Techniques

The number of advanced techniques for statistical
language modeling is very large. It is out of scope
of this paper to provide their detailed description,
but we mention some of the most popular ones:

• N-grams with Modiﬁed Kneser-Ney smooth-

ing (Chen and Goodman, 1996)

• Cache (Jelinek et al., 1991)
• Class-based (Brown et al., 1992)
• Maximum entropy (Rosenfeld, 1994)
• Structured (Chelba and Jelinek, 2000)
• Neural net based (Bengio et al., 2003)
• Discriminative (Roark et al., 2004)
• Random forest (Xu, 2005)
• Bayesian (Teh, 2006)

Below, we provide a short description of models
that we used in our comparison using the benchmark
data.

4.1 Normalized Stupid Backoff

Stupid

Backoff

LM was

The
proposed
in (Brants et al., 2007) as a simpliﬁed version
of backoff LM, suited to client-server architectures
in a distributed computing environment.It does not
apply any discounting to relative frequencies, and
it uses a single backoff weight instead of context-
dependent backoff weights. As a result, the Stupid
Backoff model does not generate normalized prob-
abilities. For the purpose of computing perplexity
as reported in Table 1, values output by the model
were normalized over the entire LM vocabulary.

4.2 Binary Maximum Entropy Language

Model

The Binary MaxEnt model was
proposed
in (Xu et al., 2011) and aims to avoid the ex-
pensive probability normalization during training by
using independent binary predictors. Each predictor
is trained using all the positive examples, but the
negative examples are dramatically down-sampled.

This type of model is attractive for parallel training,
thus we explored its performance further.

We trained two models with a sampling rate of
0.001 for negative examples, one uses n-gram fea-
tures only and the other uses n-gram and skip-1 n-
gram features. We separated the phases of generat-
ing negative examples and tuning model parameters
such that the output of the ﬁrst phase can be shared
by two models. The generation of the negative ex-
amples took 7.25 hours using 500 machines, while
tuning the parameters using 5000 machines took 50
minutes, and 70 minutes for the two models, respec-
tively.

4.3 Maximum Entropy Language Model with

Hierarchical Softmax

Another option to reduce training complexity of
the MaxEnt models is to use a hierarchical soft-
max (Goodman, 2001b; Morin and Bengio, 2005).
The idea is to estimate probabilities of groups of
words,
like in a class based model – only the
classes that contain the positive examples need to
In our case, we explored a bi-
be evaluated.
nary Huffman tree representation of the vocabu-
lary, such that evaluation of frequent words takes
less time. The idea of using frequencies of words
for a hierarchical softmax was presented previously
in (Mikolov et al., 2011a).

4.4 Recurrent Neural Network Language

Model

The Recurrent Neural Network (RNN) based LM
have recently achieved outstanding performance
It was
on a number of tasks (Mikolov, 2012).
shown that RNN LM signiﬁcantly outperforms
many other language modeling techniques on the
Penn Treebank data set (Mikolov et al., 2011b). It
was also shown that RNN models scale very
well
to data sets with hundreds of millions of
words (Mikolov et al., 2011c), although the reported
training times for the largest models were in the or-
der of weeks.

We cut down training times by a factor of 20-50
for large problems using a number of techniques,
which allow RNN training in typically 1-10 days
with billions of words, > 1M vocabularies and up to
20B parameters on a single standard machine with-
out GPUs.

Model
Interpolated KN 5-gram, 1.1B n-grams
All models

Perplexity
67.6
43.8

Table 2: Model combination on the 1B Word Benchmark test set. The weights were tuned to minimize perplexity on
held-out data. The optimal interpolation weights for the KN, rnn1024, rnn512, rnn256, SBO, HME were, respectively:
0.06, 0.61, 0.13, 0.00, 0.20, 0.00.

These techniques were in order of importance:
a) Parallelization of training across available CPU
threads, b) Making use of SIMD instructions where
possible, c) Reducing number of output parameters
by 90%, d) Running a Maximum Entropy model in
parallel to the RNN. Because of space limitations
we cannot describe the exact details of the speed-ups
here – they will be reported in an upcoming paper.

We trained several models with varying number
of neurons (Table 1) using regular SGD with a
learning rate of 0.05 to 0.001 using 10 iterations over
the data. The MaxEnt models running in parallel
to the RNN capture a history of 9 previous words,
and the models use as additional features the previ-
ous 15 words independently of order. While train-
ing times approach 2 weeks for the most complex
model, slightly worse models can be trained in a few
days. Note that we didn’t optimize for model size
nor training speed, only test performance.

5 Results

5.1 Performance of Individual Models

Results achieved on the benchmark data with vari-
ous types of LM are reported in Table 1. We fo-
cused on minimizing the perplexity when choosing
hyper-parameters, however we also report the time
required to train the models. Training times are not
necessarily comparable as they depend on the under-
lying implementation. Mapreduces can potentially
process larger data sets than single-machine imple-
mentations, but come with a large overhead of com-
munication and ﬁle I/O. Discussing details of the im-
plementations is outside the scope as this paper.

5.2 Model Combination

The best perplexity results were achieved by linearly
interpolating together probabilities from all models.
However, only some models had signiﬁcant weight
in the combination; the weights were tuned on the

held-out data. As can be seen in Table 2, the best
perplexity is about 35% lower than the baseline - the
modiﬁed Kneser-Ney smoothed 5-gram model with
no count cutoffs. This corresponds to about 10%
reduction of cross-entropy (bits).

Somewhat surprisingly the SBO model receives
a relatively high weight in the linear combination
of models, despite its poor performance in perplex-
ity, whereas the KN baseline receives a fairly small
weight relative to the other models in the combina-
tion.

6 Conclusion

We introduced a new data set for measuring re-
search progress in statistical
language modeling.
The benchmark data set is based on resources that
are freely available on the web, thus fair comparison
of various techniques is easily possible. The impor-
tance of such effort is unquestionable:
it has been
seen many times in the history of research that sig-
niﬁcant progress can be achieved when various ap-
proaches are measurable, reproducible, and the bar-
rier to entry is low.

The choice of approximately one billion words
might seem somewhat restrictive. Indeed, it can be
hardly expected that new techniques will be immedi-
ately competitive on a large data set. Computation-
ally expensive techniques can still be compared us-
ing for example just the ﬁrst or the ﬁrst 10 partitions
of this new dataset, corresponding to approx. 10 mil-
lion and 100 million words. However, to achieve
impactful results in domains such as speech recogni-
tion and machine translation, the language modeling
techniques need to be scaled up to large data sets.

Another contribution of this paper is the com-
parison of a few novel modeling approaches when
trained on a large data set. As far as we know, we
were able to train the largest recurrent neural net-
work language model ever reported. The perfor-

mance gain is very promising; the perplexity reduc-
tion of 35% is large enough to let us hope for signif-
icant improvements in various applications.

In the future, we would like to encourage other
researchers to participate in our efforts to make lan-
guage modeling research more transparent. This
would greatly help to transfer the latest discover-
In the spirit of a
ies into real-world applications.
benchmark our ﬁrst goal was to achieve the best pos-
sible test perplexities regardless of model sizes or
training time. However, this was a relatively lim-
ited collaborative effort, and some well known tech-
niques are still missing. We invite other researchers
to complete the picture by evaluating new, and well-
known techniques on this corpus. Ideally the bench-
mark would also contain ASR or SMT lattices/N-
best lists, such that one can evaluate application spe-
ciﬁc performance as well.

References

[Bengio et al.2003] Y. Bengio, R. Ducharme, and P. Vin-
cent. 2003. A neural probabilistic language model.
Journal of Machine Learning Research, 3:1137-1155.
[Brants et al.2007] T. Brants, A. C. Popat, P. Xu, F. J. Och,
and J. Dean. 2007. Large language models in machine
translation. In Proceedings of EMNLP.

[Brown et al.1992] P. F. Brown, P. V. deSouza, R. L. Mer-
cer, V. J. Della Pietra, and J. C. Lai. 1992. Class-
Based n-gram Models of Natural Language. Compu-
tational Linguistics, 18, 467-479.

[Elman1990] J. Elman. 1990. Finding Structure in Time.

Cognitive Science, 14, 179-211.

[Emami2006] A. Emami. 2006. A Neural Syntactic Lan-
guage Model. Ph.D. thesis, Johns Hopkins University.
A bit of
progress in language modeling, extended version.
Technical report MSR-TR-2001-72.

[Goodman2001a] J. T. Goodman.

2001a.

[Goodman2001b] J. T. Goodman. 2001b. Classes for
In Proceedings of

fast maximum entropy training.
ICASSP.

[Jelinek et al.1991] F. Jelinek, B. Merialdo, S. Roukos,
and M. Strauss. 1991. A Dynamic Language Model
for Speech Recognition. In Proceedings of the DARPA
Workshop on Speech and Natural Language.

[Chelba and Jelinek2000] C. Chelba and F. Jelinek. 2000.
Structured language modeling. Computer Speech &
Language.

[Chelba et al.2010] C. Chelba, T. Brants, W. Neveitt, and
P. Xu. 2010. Study on Interaction between Entropy

Pruning and Kneser-Ney Smoothing. In Proceedings
of Interspeech.

[Chen and Goodman1996] S. F. Chen and J. T. Goodman.
1996. An empirical study of smoothing techniques for
language modeling. In Proceedings of ACL.

[Chen2009] S. F. Chen. 2009. Shrinking exponential lan-
guage models. In Proceedings of NAACL-HLT.
[Katz1995] S. Katz. 1987. Estimation of probabilities
from sparse data for the language model component of
a speech recognizer. In IEEE Transactions on Acous-
tics, Speech and Signal Processing.

[Kneser and Ney1995] R. Kneser and H. Ney. 1995. Im-
proved Backing-Off For M-Gram Language Modeling.
In Proceedings of ICASSP.

[Mikolov et al.2010] T. Mikolov, M. Karaﬁ´at, L. Burget,
J. ˇCernock´y, and S. Khudanpur. 2010. Recurrent neu-
ral network based language model. In Proceedings of
Interspeech.

[Mikolov et al.2011a] T. Mikolov, S. Kombrink, L. Bur-
get, J. ˇCernock´y, and S. Khudanpur. 2011. Exten-
sions oﬂ Recurrent Neural Network Language Model.
In Proceedings of ICASSP.

[Mikolov et al.2011b] T. Mikolov, A. Deoras, S. Kom-
brink, L. Burget, and J. ˇCernock´y. 2011a. Empirical
Evaluation and Combination of Advanced Language
Modeling Techniques. In Proceedings of Interspeech.
[Mikolov et al.2011c] T. Mikolov, A. Deoras, D. Povey,
L. Burget, and J. ˇCernock´y. 2011b. Strategies for
Training Large Scale Neural Network Language Mod-
els. In Proceedings of ASRU.

[Mikolov2012] T. Mikolov. 2012. Statistical Language
Models based on Neural Networks. Ph.D. thesis, Brno
University of Technology.

[Mnih and Hinton2007] A. Mnih and G. Hinton. 2007.
Three new graphical models for statistical language
modelling. In Proceedings of ICML.

[Morin and Bengio2005] F. Morin and Y. Bengio. 2005.
Hierarchical Probabilistic Neural Network Language
Model. In Proceedings of AISTATS.

[Roark et al.2004] B. Roark, M. Saralar, M. Collins, and
M. Johnson. 2004. Discriminative language model-
ing with conditional random ﬁelds and the perceptron
algorithm. In Proceedings of ACL.

[Rosenfeld1994] R. Rosenfeld. 1994. Adaptive Statis-
tical Language Modeling: A Maximum Entropy Ap-
proach. Ph.D. thesis, Carnegie Mellon University.
[Rumelhart et al.1986] D. E. Rumelhart, G. E. Hinton,
and R. J. Williams. 1986. Learning internal represen-
tations by back-propagating errors. Nature, 323:533-
536.

[Schwenk2007] H. Schwenk. 2007. Continuous space
language models. Computer Speech and Language,
vol. 21.

[Stolcke1998] A. Stolcke. 1998. Entropy-based Pruning
of Back-off Language Models. In Proceedings of News
Transcription and Understanding Workshop.

[Sundermeyer et al.2012] M. Sundermeyer, R. Schluter,
and H. Ney. 2012. LSTM Neural Networks for Lan-
guage Modeling. In Proceedings of Interspeech.
[Teh2006] Y. W. Teh. 2006. A hierarchical Bayesian lan-
guage model based on PitmanYor processes. In Pro-
ceedings of Coling/ACL.

[Wu et al.2012] Y. Wu, H. Yamamoto, X. Lu, S. Matsuda,
C. Hori, and H. Kashioka. 2012. Factored Recur-
rent Neural Network Language Model in TED Lecture
Transcription. In Proceedings of IWSLT.

[Xu2005] Peng Xu. 2005. Random forests and the data
sparseness problem in language modeling. Ph.D. the-
sis, Johns Hopkins University.

[Xu et al.2011] Puyang Xu, A. Gunawardana, and S.
Khudanpur. 2011. Efﬁcient Subsampling for Train-
ing Complex Language Models.
In Proceedings of
EMNLP.

[Zweig and Makarychev2013] G.

2013.

Makarychev.
Optimality in Word Classing.
ICASSP.

and

Zweig

K.
Speed Regularization and
In Proceedings of

