7
1
0
2
 
y
a
M
 
5
 
 
]
I

A
.
s
c
[
 
 
3
v
0
4
9
6
0
.
0
1
6
1
:
v
i
X
r
a

Safety Veriﬁcation of Deep Neural Networks(cid:63)

Xiaowei Huang, Marta Kwiatkowska, Sen Wang and Min Wu

Department of Computer Science, University of Oxford

Abstract. Deep neural networks have achieved impressive experimental results
in image classiﬁcation, but can surprisingly be unstable with respect to adversar-
ial perturbations, that is, minimal changes to the input image that cause the net-
work to misclassify it. With potential applications including perception modules
and end-to-end controllers for self-driving cars, this raises concerns about their
safety. We develop a novel automated veriﬁcation framework for feed-forward
multi-layer neural networks based on Satisﬁability Modulo Theory (SMT). We
focus on safety of image classiﬁcation decisions with respect to image manipu-
lations, such as scratches or changes to camera angle or lighting conditions that
would result in the same class being assigned by a human, and deﬁne safety
for an individual decision in terms of invariance of the classiﬁcation within a
small neighbourhood of the original image. We enable exhaustive search of the
region by employing discretisation, and propagate the analysis layer by layer. Our
method works directly with the network code and, in contrast to existing meth-
ods, can guarantee that adversarial examples, if they exist, are found for the given
region and family of manipulations. If found, adversarial examples can be shown
to human testers and/or used to ﬁne-tune the network. We implement the tech-
niques using Z3 and evaluate them on state-of-the-art networks, including regu-
larised and deep learning networks. We also compare against existing techniques
to search for adversarial examples and estimate network robustness.

1

Introduction

Deep neural networks have achieved impressive experimental results in image classiﬁ-
cation, matching the cognitive ability of humans [23] in complex tasks with thousands
of classes. Many applications are envisaged, including their use as perception modules
and end-to-end controllers for self-driving cars [15]. Let Rn be a vector space of images
(points) that we wish to classify and assume that f : Rn → C, where C is a (ﬁnite) set of
class labels, models the human perception capability, then a neural network classiﬁer is
a function ˆf (x) which approximates f (x) from M training examples {(xi, ci)}i=1,..,M. For
example, a perception module of a self-driving car may input an image from a camera
and must correctly classify the type of object in its view, irrespective of aspects such
as the angle of its vision and image imperfections. Therefore, though they clearly in-
clude imperfections, all four pairs of images in Figure 1 should arguably be classiﬁed
as automobiles, since they appear so to a human eye.

(cid:63) This work is

supported by the EPSRC Programme Grant on Mobile Autonomy
(EP/M019918/1). Part of this work was done while MK was visiting the Simons Institute for
the Theory of Computing.

Classiﬁers employed in vision tasks are typically multi-layer networks, which prop-
agate the input image through a series of linear and non-linear operators. They are
high-dimensional, often with millions of dimensions, non-linear and potentially dis-
continuous: even a small network, such as that trained to classify hand-written images
of digits 0-9, has over 60,000 real-valued parameters and 21,632 neurons (dimensions)
in its ﬁrst layer. At the same time, the networks are trained on a ﬁnite data set and
expected to generalise to previously unseen images. To increase the probability of cor-
rectly classifying such an image, regularisation techniques such as dropout are typically
used, which improves the smoothness of the classiﬁers, in the sense that images that are
close (within (cid:15) distance) to a training point are assigned the same class label.

automobile to bird

automobile to frog

automobile to airplane automobile to horse

Fig. 1. Automobile images (classiﬁed correctly) and their perturbed images (classiﬁed
wrongly)

Unfortunately, it has been observed in [13,36] that deep neural networks, includ-
ing highly trained and smooth networks optimised for vision tasks, are unstable with
respect to so called adversarial perturbations. Such adversarial perturbations are (min-
imal) changes to the input image, often imperceptible to the human eye, that cause the
network to misclassify the image. Examples include not only artiﬁcially generated ran-
dom perturbations, but also (more worryingly) modiﬁcations of camera images [22] that
correspond to resizing, cropping or change in lighting conditions. They can be devised
without access to the training set [29] and are transferable [19], in the sense that an ex-
ample misclassiﬁed by one network is also misclassiﬁed by a network with a diﬀerent
architecture, even if it is trained on diﬀerent data. Figure 1 gives adversarial pertur-
bations of automobile images that are misclassiﬁed as a bird, frog, airplane or horse
by a highly trained state-of-the-art network. This obviously raises potential safety con-
cerns for applications such as autonomous driving and calls for automated veriﬁcation
techniques that can verify the correctness of their decisions.

Safety of AI systems is receiving increasing attention, to mention [33,10], in view
of their potential to cause harm in safety-critical situations such as autonomous driving.
Typically, decision making in such systems is either solely based on machine learning,
through end-to-end controllers, or involves some combination of logic-based reasoning
and machine learning components, where an image classiﬁer produces a classiﬁcation,
say speed limit or a stop sign, that serves as input to a controller. A recent trend towards
“explainable AI” has led to approaches that learn not only how to assign the classiﬁca-
tion labels, but also additional explanations of the model, which can take the form of
a justiﬁcation explanation (why this decision has been reached, for example identify-
ing the features that supported the decision) [17,31]. In all these cases, the safety of a
decision can be reduced to ensuring the correct behaviour of a machine learning com-

ponent. However, safety assurance and veriﬁcation methodologies for machine learning
are little studied.

The main diﬃculty with image classiﬁcation tasks, which play a critical role in per-
ception modules of autonomous driving controllers, is that they do not have a formal
speciﬁcation in the usual sense: ideally, the performance of a classiﬁer should match
the perception ability and class labels assigned by a human. Traditionally, the correct-
ness of a neural network classiﬁer is expressed in terms of risk [37], deﬁned as the
probability of misclassiﬁcation of a given image, weighted with respect to the input
distribution µ of images. Similar (statistical) robustness properties of deep neural net-
work classiﬁers, which compute the average minimum distance to a misclassiﬁcation
and are independent of the data point, have been studied and can be estimated using
tools such as DeepFool [25] and cleverhans [27]. However, we are interested in the
safety of an individual decision, and to this end focus on the key property of the clas-
siﬁer being invariant to perturbations at a given point. This notion is also known as
pointwise robustness [18,12] or local adversarial robustness [21].

Contributions. In this paper we propose a general framework for automated veriﬁ-
cation of safety of classiﬁcation decisions made by feed-forward deep neural networks.
Although we work concretely with image classiﬁers, the techniques can be generalised
to other settings. For a given image x (a point in a vector space), we assume that there
is a (possibly inﬁnite) region η around that point that incontrovertibly supports the de-
cision, in the sense that all points in this region must have the same class. This region is
speciﬁed by the user and can be given as a small diameter, or the set of all points whose
salient features are of the same type. We next assume that there is a family of operations
∆, which we call manipulations, that specify modiﬁcations to the image under which the
classiﬁcation decision should remain invariant in the region η. Such manipulations can
represent, for example, camera imprecisions, change of camera angle, or replacement of
a feature. We deﬁne a network decision to be safe for input x and region η with respect
to the set of manipulations ∆ if applying the manipulations on x will not result in a class
change for η. We employ discretisation to enable a ﬁnite exhaustive search of the high-
dimensional region η for adversarial misclassiﬁcations. The discretisation approach is
justiﬁed in the case of image classiﬁers since they are typically represented as vectors of
discrete pixels (vectors of 8 bit RGB colours). To achieve scalability, we propagate the
analysis layer by layer, mapping the region and manipulations to the deeper layers. We
show that this propagation is sound, and is complete under the additional assumption of
minimality of manipulations, which holds in discretised settings. In contrast to existing
approaches [36,28], our framework can guarantee that a misclassiﬁcation is found if it
exists. Since we reduce veriﬁcation to a search for adversarial examples, we can achieve
safety veriﬁcation (if no misclassiﬁcations are found for all layers) or falsiﬁcation (in
which case the adversarial examples can be used to ﬁne-tune the network or shown to a
human tester).

We implement the techniques using Z3 [8] in a tool called DLV (Deep Learning Ver-
iﬁcation) [2] and evaluate them on state-of-the-art networks, including regularised and
deep learning networks. This includes image classiﬁcation networks trained for clas-
sifying hand-written images of digits 0-9 (MNIST), 10 classes of small colour images
(CIFAR10), 43 classes of the German Traﬃc Sign Recognition Benchmark (GTSRB)

[35] and 1000 classes of colour images used for the well-known imageNet large-scale
visual recognition challenge (ILSVRC) [4]. We also perform a comparison of the DLV
falsiﬁcation functionality on the MNIST dataset against the methods of [36] and [28],
focusing on the search strategies and statistical robustness estimation. The perturbed
images in Figure 1 are found automatically using our tool for the network trained on
the CIFAR10 dataset.

This invited paper is an extended and improved version of [20], where an extended

version including appendices can also be found.

2 Background on Neural Networks

We consider feed-forward multi-layer neural networks [14], henceforth abbreviated as
neural networks. Perceptrons (neurons) in a neural network are arranged in disjoint
layers, with each perceptron in one layer connected to the next layer, but no connection
between perceptrons in the same layer. Each layer Lk of a network is associated with
an nk-dimensional vector space DLk ⊆ Rnk , in which each dimension corresponds to
a perceptron. We write Pk for the set of perceptrons in layer Lk and nk = |Pk| is the
number of perceptrons (dimensions) in layer Lk.

Formally, a (feed-forward and deep) neural network N is a tuple (L, T, Φ), where
L = {Lk | k ∈ {0, ..., n}} is a set of layers such that layer L0 is the input layer and Ln
is the output layer, T ⊆ L × L is a set of sequential connections between layers such
that, except for the input and output layers, each layer has an incoming connection and
an outgoing connection, and Φ = {φk | k ∈ {1, ..., n}} is a set of activation functions
φk : DLk−1 → DLk , one for each non-input layer. Layers other than input and output
layers are called the hidden layers.

The network is fed an input x (point in DL0 ) through its input layer, which is then
propagated through the layers by successive application of the activation functions. An
activation for point x in layer k is the value of the corresponding function, denoted
αx,k = φk(φk−1(...φ1(x))) ∈ DLk , where αx,0 = x. For perceptron p ∈ Pk we write
αx,k(p) for the value of its activation on input x. For every activation αx,k and layer
k(cid:48) < k, we deﬁne Prek(cid:48) (αx,k) = {αy,k(cid:48) ∈ DLk(cid:48)
| αy,k = αx,k} to be the set of activations in
layer k(cid:48) whose corresponding activation in layer Lk is αx,k. The classiﬁcation decision
is made based on the activations in the output layer by, e.g., assigning to x the class
arg maxp∈Pn αx,n(p). For simplicity, we use αx,n to denote the class assigned to input x,
and thus αx,n = αy,n expresses that two inputs x and y have the same class.

The neural network classiﬁer N represents a function ˆf (x) which approximates
f (x) : DL0 → C, a function that models the human perception capability in labelling im-
ages with labels from C, from M training examples {(xi, ci)}i=1,..,M. Image classiﬁcation
networks, for example convolutional networks, may contain many layers, which can
be non-linear, and work in high dimensions, which for the image classiﬁcation prob-
lems can be of the order of millions. Digital images are represented as 3D tensors of
pixels (width, height and depth, the latter to represent colour), where each pixel is a dis-
crete value in the range 0..255. The training process determines real values for weights
used as ﬁlters that are convolved with the activation functions. Since it is diﬃcult to
approximate f with few samples in the sparsely populated high-dimensional space, to

increase the probability of classifying correctly a previously unseen image, various reg-
ularisation techniques such as dropout are employed. They improve the smoothness of
the classiﬁer, in the sense that points that are (cid:15)-close to a training point (potentially
inﬁnitely many of them) classify the same.

In this paper, we work with the code of the network and its trained weights.

3 Safety Analysis of Classiﬁcation Decisions

In this section we deﬁne our notion of safety of classiﬁcation decisions for a neural net-
work, based on the concept of a manipulation of an image, essentially perturbations that
a human observer would classify the same as the original image. Safety is deﬁned for
an individual classiﬁcation decision and is parameterised by the class of manipulations
and a neighbouring region around a given image. To ensure ﬁniteness of the search of
the region for adversarial misclassiﬁcations, we introduce so called “ladders”, nonde-
terministically branching and iterated application of successive manipulations, and state
the conditions under which the search is exhaustive.

Safety and Robustness Our method assumes the existence of a (possibly inﬁnite)
region η around a data point (image) x such that all points in the region are indistin-
guishable by a human, and therefore have the same true class. This region is understood
as supporting the classiﬁcation decision and can usually be inferred from the type of
the classiﬁcation problem. For simplicity, we identify such a region via its diameter d
with respect to some user-speciﬁed norm, which intuitively measures the closeness to
the point x. As deﬁned in [18], a network ˆf approximating human capability f is said
to be not robust at x if there exists a point y in the region η = {z ∈ DL0 | ||z − x|| ≤ d}
of the input layer such that ˆf (x) (cid:44) ˆf (y). The point y, at a minimal distance from x, is
known as an adversarial example. Our deﬁnition of safety for a classiﬁcation decision
(abbreviated safety at a point) follows he same intuition, except that we work layer
by layer, and therefore will identify such a region ηk, a subspace of DLk , at each layer
Lk, for k ∈ {0, ..., n}, and successively reﬁne the regions through the deeper layers. We
justify this choice based on the observation [11,23,24] that deep neural networks are
thought to compute progressively more powerful invariants as the depth increases. In
other words, they gradually transform images into a representation in which the classes
are separable by a linear classiﬁer.

Assumption 1 For each activation αx,k of point x in layer Lk, the region ηk(αx,k) con-
tains activations that the human observer believes to be so close to αx,k that they should
be classiﬁed the same as x.

Intuitively, safety for network N at a point x means that the classiﬁcation decision is
robust at x against perturbations within the region ηk(αx,k). Note that, while the pertur-
bation is applied in layer Lk, the classiﬁcation decision is based on the activation in the
output layer Ln.

Deﬁnition 1. [General Safety] Let ηk(αx,k) be a region in layer Lk of a neural network
N such that αx,k ∈ ηk(αx,k). We say that N is safe for input x and region ηk(αx,k), written
as N, ηk |= x, if for all activations αy,k in ηk(αx,k) we have αy,n = αx,n.

We remark that, unlike the notions of risk [37] and robustness of [18,12], we work
with safety for a speciﬁc point and do not account for the input distribution, but such
expectation measures can be considered, see Section 6 for comparison.

Manipulations A key concept of our framework is the notion of a manipulation, an
operator that intuitively models image perturbations, for example bad angles, scratches
or weather conditions, the idea being that the classiﬁcation decisions in a region of im-
ages close to it should be invariant under such manipulations. The choice of the type of
manipulation is dependent on the application and user-deﬁned, reﬂecting knowledge of
the classiﬁcation problem to model perturbations that should or should not be allowed.
Judicious choice of families of such manipulations and appropriate distance metrics is
particularly important. For simplicity, we work with operators δk : DLk → DLk over the
activations in the vector space of layer k, and consider the Euclidean (L2) and Manhattan
(L1) norms to measure the distance between an image and its perturbation through δk,
but the techniques generalise to other norms discussed in [18,19,12]. More speciﬁcally,
applying a manipulation δk(αx,k) to an activation αx,k will result in another activation
such that the values of some or all dimensions are changed. We therefore represent a
manipulation as a hyper-rectangle, deﬁned for two activations αx,k and αy,k of layer Lk
by rec(αx,k, αy,k) = ×p∈Pk [min(αx,k(p), αy,k(p)), max(αx,k(p), αy,k(p))]. The main chal-
lenge for veriﬁcation is the fact that the region ηk contains potentially an uncountable
number of activations. Our approach relies on discretisation in order to enable a ﬁnite
exploration of the region to discover and/or rule out adversarial perturbations.

For an activation αx,k and a set ∆ of manipulations, we denote by rec(∆, αx,k) the
polyhedron which includes all hyper-rectangles that result from applying some manip-
ulation in ∆ on αx,k, i.e., rec(∆, αx,k) = (cid:83)
δ∈∆ rec(αx,k, δ(αx,k)). Let ∆k be the set of all
possible manipulations for layer Lk. To ensure region coverage, we deﬁne valid manip-
ulation as follows.

Deﬁnition 2. Given an activation αx,k, a set of manipulations V(αx,k) ⊆ ∆k is valid if
αx,k is an interior point of rec(V(αx,k), αx,k), i.e., αx,k is in rec(V(αx,k), αx,k) and does
not belong to the boundary of rec(V(αx,k), αx,k).

Figure 2 presents an example of valid manipulations in two-dimensional space: each
arrow represents a manipulation, each dashed box represents a (hyper-)rectangle of the
corresponding manipulation, and activation αx,k is an interior point of the space from
the dashed boxes.

Since we work with discretised spaces, which is a reasonable assumption for im-
ages, we introduce the notion of a minimal manipulation. If applying a minimal manip-
ulation, it suﬃces to check for misclassiﬁcation just at the end points, that is, αx,k and
δk(αx,k). This allows an exhaustive, albeit impractical, exploration of the region in unit
steps.

A manipulation δ1

k(αx,k), written as δ1

k(αy,k) is ﬁner than δ2

k(αx,k), if any
activation in the hyper-rectangle of the former is also in the hyper-rectangle of the latter.
It is implied in this deﬁnition that αy,k is an activation in the hyper-rectangle of δ2
k(αx,k).
Moreover, we write δk,k(cid:48) (αx,k) for φk(cid:48) (...φk+1(δk(αx,k))), representing the corresponding
activation in layer k(cid:48) ≥ k after applying manipulation δk on the activation αx,k, where
δk,k(αx,k) = δk(αx,k).

k(αy,k) ≤ δ2

Fig. 2. Example of a set {δ1, δ2, δ3, δ4} of valid manipulations in a 2-dimensional space

Deﬁnition 3. A manipulation δk on an activation αx,k is minimal if there does not exist
k(αx,k) ≤ δk(αx,k), αy,k =
k and an activation αy,k such that δ1
manipulations δ1
k(αy,k), and αy,n (cid:44) αx,n and αy,n (cid:44) δk,n(αx,k).
k(αx,k), δk(αx,k) = δ2
δ1

k and δ2

Intuitively, a minimal manipulation does not have a ﬁner manipulation that results in
a diﬀerent classiﬁcation. However, it is possible to have diﬀerent classiﬁcations before
and after applying the minimal manipulation, i.e., it is possible that δk,n(αx,k) (cid:44) αx,n. It
is not hard to see that the minimality of a manipulation implies that the class change in
its associated hyper-rectangle can be detected by checking the class of the end points
αx,k and δk(αx,k).

Bounded Variation Recall that we apply manipulations in layer Lk, but check the
classiﬁcation decisions in the output layer. To ensure ﬁnite, exhaustive coverage of
the region, we introduce a continuity assumption on the mapping from space DLk to
the output space DLn , adapted from the concept of bounded variation [9]. Given an
activation αx,k with its associated region ηk(αx,k), we deﬁne a “ladder” on ηk(αx,k) to
be a set ld of activations containing αx,k and ﬁnitely many, possibly zero, activations
from ηk(αx,k). The activations in a ladder can be arranged into an increasing order
αx,k = αx0,k < αx1,k < ... < αx j,k such that every activation αxt,k ∈ ld appears once and
has a successor αxt+1,k such that αxt+1,k = δk(αxt,k) for some manipulation δk ∈ V(αxt,k).
For the greatest element αx j,k, its successor should be outside the region ηk(αx,k), i.e.,
αx j+1,k (cid:60) ηk(αx,k). Given a ladder ld, we write ld(t) for its t + 1-th activation, ld[0..t] for
the preﬁx of ld up to the t + 1-th activation, and last(ld) for the greatest element of ld.
Figure 3 gives a diagrammatic explanation on the ladders.

Deﬁnition 4. Let L(ηk(αx,k)) be the set of ladders in ηk(αx,k). Then the total variation
of the region ηk(αx,k) on the neural network with respect to L(ηk(αx,k)) is

V(N; ηk(αx,k)) =

sup
ld∈L(ηk(αx,k))

(cid:88)

diﬀn(αxt,n, αxt+1,n)

αxt ,k∈ld\{last(ld)}

where diﬀn : DLn × DLn → {0, 1} is given by diﬀn(αx,n, αy,n) = 0 if αx,n = αy,n and 1
otherwise. We say that the region ηk(αx,k) is a bounded variation if V(N; ηk(αx,k)) < ∞,
and are particularly interested in the case when V(N; rk(αy,k)) = 0, which is called a
0-variation.

Fig. 3. Examples of ladders in region ηk(αx,k). Starting from αx,k = αx0,k, the activations
αx1,k...αx j,k form a ladder such that each consecutive activation results from some valid
manipulation δk applied to a previous activation, and the ﬁnal activation αx j,k is outside
the region ηk(αx,k).

The set L(ηk(αx,k)) is complete if, for any ladder ld ∈ L(ηk(αx,k)) of j+1 activations,
any element ld(t) for 0 ≤ t ≤ j, and any manipulation δk ∈ V(ld(t)), there exists a ladder
ld(cid:48) ∈ L(ηk(αx,k)) such that ld(cid:48)[0..t] = ld[0..t] and ld(cid:48)(t + 1) = δk(ld(t)). Intuitively, a
complete ladder is a complete tree, on which each node represents an activation and
each branch of a node corresponds to a valid manipulation. From the root αx,k, every
path of the tree leading to a leaf is a ladder. Moreover, the set L(ηk(αx,k)) is covering if
the polyhedra of all activations in it cover the region ηk(αx,k), i.e.,
(cid:91)

(cid:91)

ηk(αx,k) ⊆

rec(V(αxt,k), αxt,k).

(1)

ld∈L(ηk(αx,k))

αxt ,k∈ld\{last(ld)}

Based on the above, we have the following deﬁnition of safety with respect to a set
of manipulations. Intuitively, we iteratively and nondeterministically apply manipula-
tions to explore the region ηk(αx,k), and safety means that no class change is observed
by successive application of such manipulations.

Deﬁnition 5. [Safety wrt Manipulations] Given a neural network N, an input x and a
set ∆k of manipulations, we say that N is safe for input x with respect to the region ηk
and manipulations ∆k, written as N, ηk, ∆k |= x, if the region ηk(αx,k) is a 0-variation for
the set L(ηk(αx,k)) of its ladders, which is complete and covering.

It is straightforward to note that general safety in the sense of Deﬁnition 1 implies

safety wrt manipulations, in the sense of Deﬁnition 5.

Theorem 1. Given a neural network N, an input x, and a region ηk, we have that
N, ηk |= x implies N, ηk, ∆k |= x for any set of manipulations ∆k.

In the opposite direction, we require the minimality assumption on manipulations.

Theorem 2. Given a neural network N, an input x, a region ηk(αx,k) and a set ∆k of
manipulations, we have that N, ηk, ∆k |= x implies N, ηk |= x if the manipulations in ∆k
are minimal.

Theorem 2 means that, under the minimality assumption over the manipulations, an
exhaustive search through the complete and covering ladder tree from L(ηk(αx,k)) can
ﬁnd adversarial examples, if any, and enable us to conclude that the network is safe
at a given point if none are found. Though computing minimal manipulations is not
practical, in discrete spaces by iterating over increasingly reﬁned manipulations we are
able to rule out the existence of adversarial examples in the region. This contrasts with
partial exploration according to, e.g., [25,12]; for comparison see Section 7.

4 The Veriﬁcation Framework

In this section we propose a novel framework for automated veriﬁcation of safety of
classiﬁcation decisions, which is based on search for an adversarial misclassiﬁcation
within a given region. The key distinctive distinctive features of our framework com-
pared to existing work are: a guarantee that a misclassiﬁcation is found if it exists; the
propagation of the analysis layer by layer; and working with hidden layers, in addi-
tion to input and output layers. Since we reduce veriﬁcation to a search for adversarial
examples, we can achieve safety veriﬁcation (if no misclassiﬁcations are found for all
layers) or falsiﬁcation (in which case the adversarial examples can be used to ﬁne-tune
the network or shown to a human tester).

4.1 Layer-by-Layer Analysis

We ﬁrst consider how to propagate the analysis layer by layer, which will involve reﬁn-
ing manipulations through the hidden layers. To facilitate such analysis, in addition to
the activation function φk : DLk−1 → DLk we also require a mapping ψk : DLk → DLk−1
in the opposite direction, to represent how a manipulated activation of layer Lk aﬀects
the activations of layer Lk−1. We can simply take ψk as the inverse function of φk. In
order to propagate safety of regions ηk(αx,k) at a point x into deeper layers, we assume
the existence of functions ηk that map activations to regions, and impose the following
restrictions on the functions φk and ψk, shown diagrammatically in Figure 4.

Deﬁnition 6. The functions {η0, η1, ..., ηn} and {ψ1, ..., ψn} mapping activations to re-
gions are such that

1. ηk(αx,k) ⊆ DLk , for k = 0, ..., n,
2. αx,k ∈ ηk(αx,k), for k = 0, ..., n, and
3. ηk−1(αi,k−1) ⊆ ψk(ηk(αx,k)) for all k = 1, ..., n.

Intuitively, the ﬁrst two conditions state that each function ηk assigns a region around
the activation αx,k, and the last condition that mapping the region ηk from layer Lk to

Lk−1 via ψk should cover the region ηk−1. The aim is to compute functions ηk+1, ..., ηn
based on ηk and the neural network.

The size and complexity of a deep neural network generally means that determining
whether a given set ∆k of manipulations is minimal is intractable. To partially counter
this, we deﬁne a reﬁnement relation between safety wrt manipulations for consecutive
layers in the sense that N, ηk, ∆k |= x is a reﬁnement of N, ηk−1, ∆k−1 |= x if all ma-
nipulations δk−1 in ∆k−1 are reﬁned by a sequence of manipulations δk from the set ∆k.
Therefore, although we cannot theoretically conﬁrm the minimality of ∆k, they are re-
ﬁned layer by layer and, in discrete settings, this process can be bounded from below
by the unit step. Moreover, we can work gradually from a speciﬁc layer inwards until
an adversarial example is found, ﬁnishing processing when reaching the output layer.

The reﬁnement framework is given in Figure 5. The arrows represent the implication

Fig. 4. Layer by layer analysis according to Deﬁnition 6

relations between the safety notions and are labelled with conditions if needed. The
goal of the reﬁnements is to ﬁnd a chain of implications to justify N, η0 |= x. The
fact that N, ηk |= x implies N, ηk−1 |= x is due to the constraints in Deﬁnition 6 when
ψk = φ−1
k . The fact that N, ηk |= x implies N, ηk, ∆k |= x follows from Theorem 1. The
implication from N, ηk, ∆k |= x to N, ηk |= x under the condition that ∆k is minimal is
due to Theorem 2.

We now deﬁne the notion of reﬁnability of manipulations between layers. Intu-
itively, a manipulation in layer Lk−1 is reﬁnable in layer Lk if there exists a sequence of
manipulations in layer Lk that implements the manipulation in layer Lk−1.

Deﬁnition 7. A manipulation δk−1(αy,k−1) is reﬁnable in layer Lk if there exist activa-
tions αx0,k, ..., αx j,k ∈ DLk and valid manipulations δ1
k ∈ V(αx j−1,k) such
that αy,k = αx0,k, δk−1,k(αy,k−1) = αx j,k, and αxt,k = δt
k(αxt−1,k) for 1 ≤ t ≤ j. Given a
neural network N and an input x, the manipulations ∆k are a reﬁnement by layer of
ηk−1, ∆k−1 and ηk if, for all αy,k−1 ∈ ηk−1(αz,k−1), all its valid manipulations δk−1(αy,k−1)
are reﬁnable in layer Lk.

k ∈ V(αx0,k), ..., δ j

Fig. 5. Reﬁnement framework

We have the following theorem stating that the reﬁnement of safety notions is im-

plied by the “reﬁnement by layer” relation.

Theorem 3. Assume a neural network N and an input x. For all layers k ≥ 1, if manip-
ulations ∆k are reﬁnement by layer of ηk−1, ∆k−1 and ηk, then we have that N, ηk, ∆k |= x
implies N, ηk−1, ∆k−1 |= x.

We note that any adversarial example of safety wrt manipulations N, ηk, ∆k |= x
is also an adversarial example for general safety N, ηk |= x. However, an adversarial
example αx,k for N, ηk |= x at layer k needs to be checked to see if it is an adversarial
example of N, η0 |= x, i.e. for the input layer. Recall that Prek(cid:48) (αx,k) is not necessarily
unique. This is equivalent to checking the emptiness of Pre0(αx,k) ∩ η0(αx,0). If we start
the analysis with a hidden layer k > 0 and there is no speciﬁcation for η0, we can instead
consider checking the emptiness of {αy,0 ∈ Pre0(αx,k) | αy,n (cid:44) αx,n}.

4.2 The Veriﬁcation Method

We summarise the theory developed thus far as a search-based recursive veriﬁcation
procedure given below. The method is parameterised by the region ηk around a given
point and a family of manipulations ∆k. The manipulations are speciﬁed by the user
for the classiﬁcation problem at hand, or alternatively can be selected automatically, as
described in Section 4.4. The vector norm to identify the region can also be speciﬁed
by the user and can vary by layer. The method can start in any layer, with analysis
propagated into deeper layers, and terminates when a misclassiﬁcation is found. If an
adversarial example is found by manipulating a hidden layer, it can be mapped back to
the input layer, see Section 4.5.

Algorithm 1 Given a neural network N and an input x, recursively perform the fol-
lowing steps, starting from some layer l ≥ 0. Let k ≥ l be the current layer under
consideration.

1. determine a region ηk such that if k > l then ηk and ηk−1 satisfy Deﬁnition 6;
2. determine a manipulation set ∆k such that if k > l then ∆k is a reﬁnement by layer

of ηk−1, ∆k−1 and ηk according to Deﬁnition 7;

3. verify whether N, ηk, ∆k |= x,
(a) if N, ηk, ∆k |= x then

i. report that N is safe at x with respect to ηk(αx,k) and ∆k, and
ii. continue to layer k + 1;

(b) if N, ηk, ∆k (cid:54)|= x, then report an adversarial example.

We implement Algorithm 1 by utilising satisﬁability modulo theory (SMT) solvers.
The SMT problem is a decision problem for logical formulas with respect to combina-
tions of background theories expressed in classical ﬁrst-order logic with equality. For
checking reﬁnement by layer, we use the theory of linear real arithmetic with existen-
tial and universal quantiﬁers, and for veriﬁcation within a layer (0-variation) we use the
same theory but without universal quantiﬁcation. The details of the encoding and the ap-
proach taken to compute the regions and manipulations are included in Section 4.4. To
enable practical veriﬁcation of deep neural networks, we employ a number of heuristics
described in the remainder of this section.

4.3 Feature Decomposition and Discovery

While Theorem 1 and 2 provide a ﬁnite way to verify safety of neural network clas-
siﬁcation decisions, the high-dimensionality of the region ηk(αx,k) can make any com-
putational approach impractical. We therefore use the concept of a feature to parti-
tion the region ηk(αx,k) into a set of features, and exploit their independence and low-
dimensionality. This allows us to work with state-of-the-art networks that have hun-
dreds, and even thousands, of dimensions.

Intuitively, a feature deﬁnes for each point in the high-dimensional space DLk the
most explicit salient feature it has, e.g., the red-coloured frame of a street sign in Fig-
ure 10. Formally, for each layer Lk, a feature function fk : DLk → P(DLk ) assigns a small
region for each activation αx,k in the space DLk , where P(DLk ) is the set of subspaces of
DLk . The region fk(αx,k) may have lower dimension than that of Dk. It has been argued,
in e.g. [16] for natural images, that natural data, for example natural images and sound,
forms a high-dimensional manifold, which embeds tangled manifolds to represent their
features. Feature manifolds usually have lower dimension than the data manifold, and a
classiﬁcation algorithm is to separate a set of tangled manifolds. By assuming that the
appearance of features is independent, we can manipulate them one by one regardless
of the manipulation order, and thus reduce the problem of size O(2d1+...+dm ) into a set of
smaller problems of size O(2d1 ), ..., O(2dm ).

The analysis of activations in hidden layers, as performed by our method, provides
an opportunity to discover the features automatically. Moreover, deﬁning the feature
fk on each activation as a single region corresponding to a speciﬁc feature is without
loss of generality: although an activation may include multiple features, the indepen-
dence relation between features suggests the existence of a total relation between these
features. The function fk essentially deﬁnes for each activation one particular feature,
subject to certain criteria such as explicit knowledge, but features can also be explored
in parallel.

Every feature fk(αy,k) is identiﬁed by a pre-speciﬁed number dimsk, f of dimensions.
Let dimsk( fk(αy,k)) be the set of dimensions selected according to some heuristic. Then
we have that

(cid:40)

fk(αy,k)(p) =

ηk(αx,k)(p),
[αy,k(p), αy,k(p)] otherwise.

if p ∈ dimsk( fk(αy,k))

(2)

Moreover, we need a set of features to partition the region ηk(αx,k) as follows.

Deﬁnition 8. A set { f1, ..., fm} of regions is a partition of ηk(αx,k), written as π(ηk(αx,k)),
if dimsk, f ( fi) ∩ dimsk, f ( f j) = ∅ for i, j ∈ {1, ..., m} and ηk(αx,k) = ×m

i=1 fi.
Given such a partition π(ηk(αx,k)), we deﬁne a function acts(x, k) by

acts(x, k) = {αy,k ∈ x | x ∈ π(ηk(αx,k))}

(3)

which contains one point for each feature. Then, we reduce the checking of 0-variation
of a region ηk(αx,k) to the following problems:

– checking whether the points in acts(x, k) have the same class as αx,k, and
– checking the 0-variation of all features in π(ηk(αx,k)).

In the above procedure, the checking of points in acts(x, k) can be conducted ei-
ther by following a pre-speciﬁed sequential order (single-path search) or by exhaus-
tively searching all possible orders (multi-path search). In Section 5 we demonstrate
that single-path search according to the prominence of features can enable us to ﬁnd
adversarial examples, while multi-path search may ﬁnd other examples whose distance
to the original input image is smaller.

4.4 Selection of Regions and Manipulations

The procedure summarised in Algorithm 1 is typically invoked for a given image in the
input layer, but, providing insight about hidden layers is available, it can start from any
layer Ll in the network. The selection of regions can be automated, as described below.
For the ﬁrst layer to be considered, i.e., k = l, the region ηk(αx,k) is deﬁned by ﬁrst
selecting the subset of dimsk dimensions from Pk whose activation values are furthest
away from the average activation value of the layer1. Intuitively, the knowledge repre-
sented by these activations is more explicit than the knowledge represented by the other
dimensions, and manipulations over more explicit knowledge are more likely to result
in a class change. Let avgk = ((cid:80)
p∈Pk αx,k(p))/nk be the average activation value of layer
Lk. We let dimsk(ηk(αx,k)) be the ﬁrst dimsk dimensions p ∈ Pk with the greatest values
|αx,k(p) − avg| among all dimensions, and then deﬁne

ηk(αx,k) = ×p∈dimsk(ηk(αx,k))[αx,k(p) − sp ∗ mp, αx,k(p) + sp ∗ mp]

(4)

i.e., a dimsk-polytope containing the activation αx,k, where sp represents a small span
and mp represents the number of such spans. Let Vk = {sp, mp | p ∈ dimsk(ηk(αx,k))} be
a set of variables.

Let d be a function mapping from dimsk(ηk(αx,k)) to {−1, 0, +1} such that {d(p) (cid:44)
0 | p ∈ dimsk(ηk(αx,k))} (cid:44) ∅, and D(dimsk(ηk(αx,k))) be the set of such functions. Let a
manipulation δd

k be

k (αy,k)(p) =
δd





αy,k(p) − sp if d(p) = −1
if d(p) = 0
αy,k(p)
αy,k(p) + sp if d(p) = +1

(5)

1 We also considered other approaches, including computing derivatives up to several layers, but

for the experiments we conduct they are less eﬀective.

for activation αy,k ∈ ηk(αx,k). That is, each manipulation changes a subset of the dimen-
sions by the span sp, according to the directions given in d. The set ∆k is deﬁned by col-
lecting the set of all such manipulations. Based on this, we can deﬁne a set L(ηk(αx,k))
of ladders, which is complete and covering.

Determining the region ηk according to ηk−1 Given ηk−1(αx,k−1) and the functions φk
and ψk, we can automatically determine a region ηk(αx,k) satisfying Deﬁnition 6 using
the following approach. According to the function φk, the activation value αx,k(p) of
perceptron p ∈ Pk is computed from activation values of a subset of perceptrons in
Pk−1. We let Vars(p) ⊆ Pk−1 be such a set of perceptrons. The selection of dimensions
in dimsk(ηk(αx,k)) depends on dimsk−1(ηk−1(αx,k−1)) and φk, by requiring that, for every
p(cid:48) ∈ dimsk−1(ηk−1(αx,k−1)), there is at least one dimension p ∈ dimsk(ηk(αx,k)) such that
p(cid:48) ∈ Vars(p). We let

dimsk(ηk(αx,k)) = {arg max

{ |αx,k(p) − avgk| | p(cid:48) ∈ Vars(p)} | p(cid:48) ∈ dimsk−1(ηk−1(αx,k−1))}

p∈Pk

(6)
Therefore, the restriction of Deﬁnition 6 can be expressed with the following formula:

∀αy,k−1 ∈ ηk(αx,k−1) : αy,k−1 ∈ ψk(ηk(αx,k)).

(7)

We omit the details of rewriting αy,k−1 ∈ ηk(αx,k−1) and αy,k−1 ∈ ψk(ηk(αx,k)) into
Boolean expressions, which follow from standard techniques. Note that this expres-
sion includes variables in Vk, Vk−1 and αy,k−1. The variables in Vk−1 are ﬁxed for a given
ηk−1(αx,k−1). Because such a region ηk(αx,k) always exists, a simple iterative procedure
can be invoked to gradually increase the size of the region represented with variables in
Vk to eventually satisfy the expression.

Determining the manipulation set ∆k according to ηk(αx,k), ηk−1(αx,k−1), and ∆k−1
The values of the variables Vk obtained from the satisﬁability of Eqn (7) yield a deﬁni-
tion of manipulations using Eqn (5). However, the obtained values for span variables sp
do not necessarily satisfy the “reﬁnement by layer” relation as deﬁned in Deﬁnition 7.
Therefore, we need to adapt the values for the variables Vk while, at the same time,
retaining the region ηk(αx,k). To do so, we could rewrite the constraint in Deﬁnition 7
into a formula, which can then be solved by an SMT solver. But, in practice, we notice
that such precise computations easily lead to overly small spans sp, which in turn result
in an unacceptable amount of computation needed to verify the relation N, ηk, ∆k |= x.
To reduce computational cost, we work with a weaker “reﬁnable in layer Lk” notion,
parameterised with respect to precision ε. Given two activations αy,k and αm,k, we use
dist(αy,k, αm,k) to represent their distance.

Deﬁnition 9. A manipulation δk−1(αy,k−1) is reﬁnable in layer Lk with precision ε > 0
if there exists a sequence of activations αx0,k, ..., αx j,k ∈ DLk and valid manipulations
k ∈ V(αx j−1,k) such that αy,k = αx0,k, δk−1,k(αy,k−1) ∈ rec(αx j−1,k, αx j,k),
k ∈ V(αx0,k), ..., δd
δ1
dist(αx j−1,k, αx j,k) ≤ (cid:15), and αxt,k = δt
k(αxt−1,k) for 1 ≤ t ≤ j. Given a neural network N
and an input x, the manipulations ∆k are a reﬁnement by layer of ηk, ηk−1, ∆k−1 with

precision ε if, for all αy,k−1 ∈ ηk−1(αx,k−1), all its legal manipulations δk−1(αy,k−1) are
reﬁnable in layer Lk with precision ε.

Comparing with Deﬁnition 7, the above deﬁnition replaces δk−1,k(αy,k−1) = αx j,k
with δk−1,k(αy,k−1) ∈ rec(αx j−1,k, αx j,k) and dist(αx j−1,k, αx j,k) ≤ ε. Intuitively, instead of
requiring a manipulation to reach the activation δk−1,k(αy,k−1) precisely, this deﬁnition
allows for each δk−1,k(αy,k−1) to be within the hyper-rectangle rec(αx j−1,k, αx j,k). To ﬁnd
suitable values for Vk according to the approximate “reﬁnement-by-layer” relation, we
use a variable h to represent the maximal number of manipulations of layer Lk used to
express a manipulation in layer k − 1. The value of h (and variables sp and np in Vk)
are automatically adapted to ensure the satisﬁability of the following formula, which
expresses the constraints of Deﬁnition 9:

∀αy,k−1 ∈ ηk(αx,k−1)∀d ∈ D(dimsk(ηk(αx,k−1)))∀δd
∃αy0,k, ..., αyh,k ∈ ηk(αx,k) : αy0,k = αy,k ∧ (cid:86)h−1
(cid:87)h−1

k−1 ∈ Vk−1(αy,k−1)
k (αyt,k)∧

t=0 αyt+1,k = δd

t=0 (δd

k−1,k(αy,k) ∈ rec(αyt,k, αyt+1,k) ∧ dist(αyt,k, αyt+1,k) ≤ ε).
It is noted that sp and mp for p ∈ dimsk(ηk(αx,k)) are employed when expressing δd
k .
The manipulation δd
k−1 by considering the corresponding relation
between dimensions in dimsk(ηk(αx,k)) and dimsk−1(ηk−1(αx,k−1)).

k is obtained from δd

(8)

Adversarial examples shown in Figures 8, 9, and 10 were found using single-path

search and automatic selection of regions and manipulations.

4.5 Mapping Back to Input Layer

When manipulating the hidden layers, we may need to map back an activation in layer
k to the input layer to obtain an input image that resulted in misclassiﬁcation, which
involves computation of Pre0(αy,k) described next. To check the 0-variation of a region
ηk(αx,k), we need to compute diﬀn(αx,n, αy,n) for many points αy,x in ηk(αx,k), where
diﬀn : DLn × DLn → {0, 1} is given by diﬀn(αx,n, αy,n) = 0 if αx,n = αy,n and 1 otherwise.
Because αx,n is known, we only need to compute αy,n. We can compute αy,n by ﬁnding
a point αy,0 ∈ Pre0(αy,k) and then using the neural network to predict the value αy,n. It
should be noted that, although Pre0(αy,k) may include more than one point, all points
have the same class, so any point in Pre0(αy,k) is suﬃcient for our purpose.

To compute αy,0 from αy,k, we use functions ψk, ψk−1, ..., ψ1 and compute points

αy,k−1, αy,k−2, ..., αy,0 such that

αy, j−1 = ψ j(αy, j) ∧ αy, j−1 ∈ η j−1(αx, j−1)

for 1 ≤ j ≤ k. The computation relies on an SMT solver to encode the functions
ψk, ψk−1, ..., ψ1 if they are piecewise linear functions, and by taking the corresponding
inverse functions directly if they are sigmoid functions. It is possible that, for some
1 ≤ j ≤ k, no point can be found by SMT solver, which means that the point αy,k does
not have any corresponding point in the input layer. We can safely discard these points.
The maxpooling function ψ j selects from every m ∗ m dimensions the maximal element
for some m > 0. The computation of the maxpooling layer ψ j−1 is combined with the
computation of the next layer ψ j, that is, ﬁnding αy, j−2 with the following expression
∃αx, j−1 : αy, j−2 = ψ j−1(ψ j(αy, j)) ∧ αy, j−1 ∈ η j−1(αx, j−1) ∧ αy, j−2 ∈ η j−2(αx, j−2)

This is to ensure that in the expression αy, j−2 = ψ j−1(ψ j(αy, j)) we can reuse m ∗ m − 1
elements in αx, j−2 and only need to replace the maximal element.

Figures 8, 9, and 10 show images obtained by mapping back from the ﬁrst hidden

layer to the input layer.

5 Experimental Results

The proposed framework has been implemented as a software tool called DLV (Deep
Learning Veriﬁcation) [2] written in Python, see Appendix of [20] for details of input
parameters and how to use the tool. The SMT solver we employ is Z3 [8], which has
Python APIs. The neural networks are built from a widely-used neural networks library
Keras [3] with a deep learning package Theano [6] as its backend.

We validate DLV on a set of experiments performed for neural networks trained for
classiﬁcation based on a predeﬁned multi-dimensional surface (small size networks),
as well as image classiﬁcation (medium size networks). These networks respectively
use two representative types of layers: fully connected layers and convolutional layers.
They may also use other types of layers, e.g., the ReLU layer, the pooling layer, the
zero-padding layer, and the dropout layer. The ﬁrst three demonstrate the single-path
search functionality on the Euclidean (L2) norm, whereas the fourth (GTSRB) multi-
path search for the L1 and L2 norms.

The experiments are conducted on a MacBook Pro laptop, with 2.7 GHz Intel Core

i5 CPU and 8 GB memory.

Two-Dimensional Point Classiﬁcation Network To demonstrate exhaustive veriﬁca-
tion facilitated by our framework, we consider a neural network trained for classifying
points above and below a two-dimensional curve shown in red in Figure 6 and Figure 7.
The network has three fully-connected hidden layers with the ReLU activation func-
tion. The input layer has two perceptrons, every hidden layer has 20 perceptrons, and
the output layer has two perceptrons. The network is trained with 5,000 points sampled
from the provided two-dimensional space, and has an accuracy of more than 99%.

For a given input x = (3.59, 1.11), we start from the input layer and deﬁne a region

around this point by taking unit steps in both directions

η0(αx,0) = [3.59 − 1.0, 3.59 + 1.0] × [1.11 − 1.0, 1.11 + 1.0] = [2.59, 4.59] × [0.11, 2.11]

The manipulation set ∆0 is shown in Figure 6: there are 9 points, of which the point in
the middle represents the activation αx,0 and the other 8 points represent the activations
resulting from applying one of the manipulations in ∆0 on αx,0. Note that, although there
are class changes in the region η0(αx,0), the manipulation set ∆0 is not able to detect such
changes. Therefore, we have that N, η0, ∆0 |= x.

Now consider layer k = 1. To obtain the region η1(αx,1), the tool selects two dimen-

sions p1,17, p1,19 ∈ P1 in layer L1 with indices 17 and 19 and computes

η1(αx,1) = [αx,1(p1,17) − 3.6, αx,1(p1,17) + 3.6] × [αx,1(p1,19) − 3.52, αx,1(p1,19) + 3.52]

The manipulation set ∆1, after mapping back to the input layer with function ψ1, is given
as Figure 7. Note that η1 and η0 satisfy Deﬁnition 6, and ∆1 is a reﬁnement by layer of

Fig. 6. Input layer

Fig. 7. First hidden layer

η0, ∆0 and η1. We can see that a class change can be detected (represented as the red
coloured point). Therefore, we have that N, η1, ∆1 (cid:54)|= x.

Image Classiﬁcation Network for the MNIST Handwritten Image Dataset The
well-known MNIST image dataset contains images of size 28 × 28 and one channel
and the network is trained with the source code given in [5]. The trained network is of
medium size with 600,810 parameters, has an accuracy of more than 99%, and is state-
of-the-art. It has 12 layers, within which there are 2 convolutional layers, as well as
layers such as ReLU, dropout, fully-connected layers and a softmax layer. The images
are preprocessed to make the value of each pixel within the bound [0, 1].

Given an image x, we start with layer k = 1 and the parameter set to at most 150
dimensions (there are 21632 dimensions in layer L1). All ηk, ∆k for k ≥ 2 are computed
according to the simple heuristic mentioned in Section 4.2 and satisfy Deﬁnition 6 and
Deﬁnition 7. For the region η1(αx,1), we allow changes to the activation value of each
selected dimension that are within [-1,1]. The set ∆1 includes manipulations that can
change the activation value for a subset of the 150 dimensions, by incrementing or
decrementing the value for each dimension by 1. The experimental results show that
for most of the examples we can ﬁnd a class change within 100 dimensional changes
in layer L1, by comparing the number of pixels that have changed, and some of them
can have less than 30 dimensional changes. Figure 8 presents examples of such class
changes for layer L1. We also experiment on images with up to 40 dimensional changes
in layer L1; the tool is able to check the entire network, reaching the output layer and
claiming that N, ηk, ∆k |= x for all k ≥ 1. While training of the network takes half an
hour, ﬁnding an adversarial example takes up to several minutes.

Image Classiﬁcation Network for the CIFAR-10 Small Image Dataset We work
with a medium size neural network, trained with the source code from [1] for more than
12 hours on the well-known CIFAR10 dataset. The inputs to the network are images
of size 32 × 32 with three channels. The trained network has 1,250,858 real-valued pa-

8 to 0

2 to 1

4 to 2

2 to 3

9 to 4

6 to 5

4 to 6

9 to 7

0 to 8

7 to 9

Fig. 8. Adversarial examples for a neural network trained on MNIST

rameters and includes convolutional layers, ReLU layers, max-pooling layers, dropout
layers, fully-connected layers, and a softmax layer.

As an illustration of the type of perturbations that we are investigating, consider the
images in Figure 9, which correspond to the parameter setting of up to 25, 45, 65, 85,
105, 125, 145 dimensions, respectively, for layer k = 1. The manipulations change the
activation values of these dimensions. Each image is obtained by mapping back from
the ﬁrst hidden layer and represents a point close to the boundary of the correspond-
ing region. The relation N, η1, ∆1 |= x holds for the ﬁrst 7 images, but fails for the last
one and the image is classiﬁed as a truck. Intuitively, our choice of the region η1(αx,1)
identiﬁes the subset of dimensions with most extreme activations, taking advantage of
the analytical capability of the ﬁrst hidden layer. A higher number of selected dimen-
sions implies a larger region in which we apply manipulations, and, more importantly,
suggests a more dramatic change to the knowledge represented by the activations when
moving to the boundary of the region.

Fig. 9. An illustrative example of mapping back to input layer from the Cifar-10
mataset: the last image classiﬁes as a truck.

We also work with 500 dimensions and otherwise the same experimental parameters
as for MNIST. Figure 13 in Appendix of [20] gives 16 pairs of original images (clas-
siﬁed correctly) and perturbed images (classiﬁed wrongly). We found that, while the
manipulations lead to human-recognisable modiﬁcations to the images, the perturbed
images can be classiﬁed wrongly by the network. For each image, ﬁnding an adversarial
example ranges from seconds to 20 minutes.

Image Classiﬁcation Network for the ImageNet Dataset We also conduct experi-
ments on a large image classiﬁcation network trained on the popular ImageNet dataset.
The images are of size 224 × 224 and have three channels. The network is the model
of the 16-layer network [34], called VGG16, used by the VGG team in the ILSVRC-

2014 competition, downloaded from [7]. The trained network has 138,357,544 real-
valued parameters and includes convolutional layers, ReLU layers, zero-padding lay-
ers, dropout layers, max-pooling layers, fully-connected layers, and a softmax layer.
The experimental parameters are the same as for the previous two experiments, except
that we work with 20,000 dimensions.

Several additional pairs of original and perturbed images are included in Figure 14
in Appendix of [20]. In Figure 10 we also give two examples of street sign images. The
image on the left is reported unsafe for the second layer with 6346 dimensional changes
(0.2% of the 3,211,264 dimensions of layer L2). The one on the right is reported safe
for 20,000 dimensional changes of layer L2. It appears that more complex manipula-
tions, involving more dimensions (perceptrons), are needed in this case to cause a class
change.

Fig. 10. Street sign images. Found an adversarial example for the left image (class
changed into bird house), but cannot ﬁnd an adversarial example for the right image
for 20,000 dimensions.

5.1 The German Traﬃc Sign Recognition Benchmark (GTSRB)

We evaluate DLV on the GTSRB dataset (by resizing images into size 32*32), which
has 43 classes. Figure 11 presents the results for the multi-path search. The ﬁrst case
(approx. 20 minutes to manipulate) is a stop sign (conﬁdence 1.0) changed into a speed
limit of 30 miles, with an L1 distance of 0.045 and L2 distance of 0.19. The conﬁdence
of the manipulated image is 0.79. The second, easy, case (seconds to manipulate) is a
speed limit of 80 miles (conﬁdence 0.999964) changed into a speed limit of 30 miles,
with an L1 distance of 0.004 and L2 distance of 0.06. The conﬁdence of the manipulated
image is 0.99 (a very high conﬁdence of misclassiﬁcation). Also, a “go right” sign can
be easily manipulated into a sign classiﬁed as “go straight”.

Figure 16 in [20] presents additional adversarial examples obtained when selecting

single-path search.

6 Comparison

We compare our approach with two existing approaches for ﬁnding adversarial exam-
ples, i.e., fast gradient sign method (FGSM) [36] and Jacobian saliency map algorithm

“stop”
to “30m speed limit”

“80m speed limit”
to “30m speed limit”

“go right”
to “go straight”

Fig. 11. Adversarial examples for the network trained on the GTSRB dataset by multi-
path search

(JSMA) [28]. FGSM calculates the optimal attack for a linear approximation of the net-
work cost, whereas DLV explores a proportion of dimensions in the feature space in the
input or hidden layers. JSMA ﬁnds a set of dimensions in the input layer to manipulate,
according to the linear approximation (by computing the Jacobian matrix) of the model
from current output to a nominated target output. Intuitively, the diﬀerence between
DLV’s manipulation and JSMA is that DLV manipulates over features discovered in
the activations of the hidden layer, while JSMA manipulates according to the partial
derivatives, which depend on the parameters of the network.

Experiment 1. We randomly select an image from the MNIST dataset. Figure 12
shows some intermediate and ﬁnal images obtained by running the three approaches:
FGSM, JSMA and DLV. FGSM has a single parameter, (cid:15), where a greater (cid:15) represents a
greater perturbation along the gradient of cost function. Given an (cid:15), for each input exam-
ple a perturbed example is returned and we test whether it is an adversarial example by
checking for misclassiﬁcation against the original image. We gradually increase the pa-
rameter (cid:15) = 0.05, 0.1, 0.2, 0.3, 0.4, with the last image (i.e., (cid:15) = 0.4) witnessing a class
change, see the images in the top row of Figure 12. FGSM can eﬃciently manipulate a
set of images, but it requires a relatively large manipulation to ﬁnd a misclassiﬁcation.
For the JSMA approach, we conduct the experiment on a setting with parameters
(cid:15) = 0.1 and θ = 1.0. The parameter (cid:15) = 0.1 means that we only consider adversarial ex-
amples changing no more than 10% of all the pixels, which is suﬃcient here. As stated
in [29], the parameter θ = 1.0, which allows a maximum change to every pixel, can en-
sure that fewer pixels need to be changed. The approach takes a series of manipulations
to gradually lead to a misclassiﬁcation, see the images in the middle row of Figure 12.
The misclassiﬁed image has an L2 (Euclidean) distance of 0.17 and an L1 (Manhattan)
distance of 0.03 from the original image. While JSMA can ﬁnd adversarial examples
with smaller distance from the original image, it takes longer to manipulate a set of
images.

Both FGSM and JSMA follow their speciﬁc heuristics to deterministically explore
the space of images. However, in some cases, the heuristics may omit better adversarial
examples. In the experiment for DLV, instead of giving features a speciﬁc order and
manipulating them sequentially, we allow the program to nondeterministically choose
features. This is currently done by MCTS (Monte Carlo Tree Search), which has a theo-
retical guarantee of convergence for inﬁnite sampling. Therefore, the high-dimensional
space is explored by following many diﬀerent paths. By taking the same manipulation

Fig. 12. FGSM vs. JSMA vs. DLV, where FGSM and JSMA search a single path and
DLV multiple paths. Top row: Original image (7) perturbed deterministically by FGSM
with (cid:15) = 0.05, 0.1, 0.2, 0.3, 0.4, with the ﬁnal image (i.e., (cid:15) = 0.4) misclassiﬁed as
9. Middle row: Original image (7) perturbed deterministically by JSMA with (cid:15) = 0.1
and θ = 1.0. We show even numbered images of the 12 produced by JSMA, with the
ﬁnal image misclassiﬁed as 3. Bottom row: Original image (7) perturbed nondetermin-
istically by DLV, for the same manipulation on a single pixel as that of JSMA (i.e.,
sp ∗ mp = 1.0) and working in the input layer, with the ﬁnal image misclassiﬁed as 3.

on a single pixel as that of JSMA (i.e., sp ∗ mp = 1.0) and working on the input layer,
DLV is able to ﬁnd another perturbed image that is also classiﬁed as 3 but has a smaller
distance (L2 distance is 0.14 and L1 distance is 0.02) from the original image, see the
images in the last row of Figure 12. In terms of the time taken to ﬁnd an adversarial
example, DLV may take longer than JSMA, since it searches over many diﬀerent paths.

L2
L1
%

FGSM ((cid:15) = 0.1)
0.08
0.06
17.5%

(0.4) DLV (dimsl = 75) (150) (450) JSMA (θ = 0.1) (0.4)
(0.2)
0.11
0.22 0.27
0.32
0.15
0.02
0.06 0.09
0.12
0.25
99%
79% 98%
70.9% 97.2%

0.19
0.04
52.3%
Table 1. FGSM vs. DLV (on a single path) vs. JSMA

0.11
0.02
92%

Experiment 2. Table 1 gives a comparison of robustness evaluation of the three
appraoches on the MNIST dataset. For FGSM, we vary the input parameter (cid:15) accord-
ing to the values {0.1, 0.2, 0.4}. For DLV, we select regions as deﬁned in Section 4.4
on a single path (by deﬁning a speciﬁc order on the features and manipulating them
sequentially) for the ﬁrst hidden layer. The experiment is parameterised by varying the
maximal number of dimensions to be changed, i.e., dimsl ∈ {75, 150, 450}. For each
input image, an adversarial example is returned, if found, by manipulating fewer than
the maximal number of dimensions. When the maximal number has been reached, DLV
will report failure and return the last perturbed example. For JSMA, the experiment is
conducted by letting θ take the value in the set {0.1, 0.4} and setting (cid:15) to 1.0.

We collect three statistics, i.e., the average L1 distance over the adversarial exam-
ples, the average L2 distance over the adversarial examples, and the success rate of

ﬁnding adversary examples. Let Ld(x, δ(x)) for d ∈ {1, 2} be the distance between an
input x and the returned perturbed image δ(x), and diﬀ(x, δ(x)) ∈ {0, 1} be a Boolean
value representing whether x and δ(x) have diﬀerent classes. We let

and

(cid:80)

Ld =

x in test set diﬀ(x, δ(x)) × Ld(x, δ(x))
x in test set diﬀ(x, δ(x))

(cid:80)

% =

(cid:80)

x in test set diﬀ(x, δ(x))
the number of examples in test set

We note that the approaches yield diﬀerent perturbed examples δ(x).

The test set size is 500 images selected randomly. DLV takes 1-2 minutes to manip-
ulate each input image in MNIST. JSMA takes about 10 minutes for each image, but it
works for 10 classes, so the running time is similar to that of DLV. FGSM works with a
set of images, so it is the fastest per image.

For the case when the success rates are very high, i.e., 97.2% for FGSM with (cid:15) =
0.4, 98% for DLV with dimsl = 450, and 99% for JSMA with θ = 0.4, JSMA has the
smallest average distances, followed by DLV, which has smaller average distances than
FGSM on both L1 and L2 distances.

We mention that a smaller distance leading to a misclassiﬁcation may result in a
lower rate of transferability [29], meaning that a misclassiﬁcation can be harder to wit-
ness on another model trained on the same (or a small subset of) data-set.

7 Related Work

AI safety is recognised an an important problem, see e.g., [33,10]. An early veriﬁcation
approach for neural networks was proposed in [30], where, using the notation of this
paper, safety is deﬁned as the existence, for all inputs in a region η0 ∈ DL0 , of a corre-
sponding output in another region ηn ⊆ DLn . They encode the entire network as a set of
constraints, approximating the sigmoid using constraints, which can then be solved by a
SAT solver, but their approach only works with 6 neurons (3 hidden neurons). A similar
idea is presented in [32]. In contrast, we work layer by layer and obtain much greater
scalability. Since the ﬁrst version of this paper appeared [20], another constraint-based
method has been proposed in [21] which improves on [30]. While they consider more
general correctness properties than this paper, they can only handle the ReLU activation
functions, by extending the Simplex method to work with the piecewise linear ReLU
functions that cannot be expressed using linear programming. This necessitates a search
tree (instead of a search path as in Simplex), for which a heuristic search is proposed
and shown to be complete. The approach is demonstrated on networks with 300 ReLU
nodes, but as it encodes the full network it is unclear whether it can be scaled to work
with practical deep neural networks: for example, the MNIST network has 630,016
ReLU nodes. They also handle continuous spaces directly without discretisation, the
beneﬁts of which are not yet clear, since it is argued in [19] that linear behaviour in
high-dimensional spaces is suﬃcient to cause adversarial examples.

Concerns about the instability of neural networks to adversarial examples were ﬁrst
raised in [13,36], where optimisation is used to identify misclassiﬁcations. A method

for computing the perturbations is also proposed, which is based on box-constrained
optimisation and is approximate in view of non-convexity of the search space. This
work is followed by [19], which introduced the much faster FGSM method, and [22],
which employed a compromise between the two (iterative, but with a smaller number
of iterations than [36]). In our notation, [19] uses a deterministic, iterative manipula-
tion δ(x) = x + (cid:15) sign((cid:79)x J(x, αx,n)), where x is an image in matrix representation, (cid:15) is a
hyper-parameter that can be tuned to get diﬀerent manipulated images, and J(x, αx,n) is
the cross-entropy cost function of the neural network on input x and class αx,n. There-
fore, their approach will test a set of discrete points in the region η0(αx,0) of the input
layer. Therefore these manipulations will test a lasso-type ladder tree (i.e., a ladder tree
without branches) L(ηk(αx,k)), which does not satisfy the covering property. In [26],
instead of working with a single image, an evolutionary algorithm is employed for a
population of images. For each individual image in the current population, the manip-
ulation is the mutation and/or crossover. While mutations can be nondeterministic, the
manipulations of an individual image are also following a lasso-type ladder tree which
is not covering. We also mention that [38] uses several distortions such as JPEG com-
pression, thumbnail resizing, random cropping, etc, to test the robustness of the trained
network. These distortions can be understood as manipulations. All these attacks do not
leverage any speciﬁc properties of the model family, and do not guarantee that they will
ﬁnd a misclassiﬁed image in the constraint region, even if such an image exists.

The notion of robustness studied in [18] has some similarities to our deﬁnition of
safety, except that the authors work with values averaged over the input distribution µ,
which is diﬃcult to estimate accurately in high dimensions. As in [36,22], they use opti-
misation without convergence guarantees, as a result computing only an approximation
to the minimal perturbation. In [12] pointwise robustness is adopted, which corresponds
to our general safety; they also use a constraint solver but represent the full constraint
system by reduction to a convex LP problem, and only verify an approximation of the
property. In contrast, we work directly with activations rather than an encoding of ac-
tivation functions, and our method exhaustively searches through the complete ladder
tree for an adversarial example by iterative and nondeterministic application of manip-
ulations. Further, our deﬁnition of a manipulation is more ﬂexible, since it allows us to
select a subset of dimensions, and each such subset can have a diﬀerent region diameter
computed with respect to a diﬀerent norm.

8 Conclusions

This paper presents an automated veriﬁcation framework for checking safety of deep
neural networks that is based on a systematic exploration of a region around a data point
to search for adversarial manipulations of a given type, and propagating the analysis into
deeper layers. Though we focus on the classiﬁcation task, the approach also generalises
to other types of networks. We have implemented the approach using SMT and vali-
dated it on several state-of-the-art neural network classiﬁers for realistic images. The
results are encouraging, with adversarial examples found in some cases in a matter of
seconds when working with few dimensions, but the veriﬁcation process itself is expo-
nential in the number of features and has prohibitive complexity for larger images. The

performance and scalability of our method can be signiﬁcantly improved through par-
allelisation. It would be interesting to see if the notions of regularity suggested in [24]
permit a symbolic approach, and whether an abstraction reﬁnement framework can be
formulated to improve the scalability and computational performance.

Acknowledgements. This paper has greatly beneﬁted from discussions with sev-
eral researchers. We are particularly grateful to Martin Fraenzle, Ian Goodfellow and
Nicolas Papernot.

References

1. CIFAR10 model for Keras. https://github.com/fchollet/keras/blob/master/examples/cifar10 cnn.py.
2. DLV. https://github.com/verideep/dlv.
3. Keras. https://keras.io.
4. Large scale visual recognition challenge. http://www.image-net.org/challenges/LSVRC/.
5. MNIST CNN network. https://github.com/fchollet/keras/blob/master/examples/mnist cnn.py.
6. Theano. http://deeplearning.net/software/theano/.
7. VGG16 model for Keras. https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3.
8. Z3. http://rise4fun.com/z3.
9. Luigi Ambrosio, Nicola Fusco, and Diego Pallara. Functions of bounded variation and free
discontinuity problems. Oxford Mathematical Monographs. Oxford University Press, 2000.
10. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dario
Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e.
Concrete problems in AI safety. CoRR, abs/1606.06565, 2016.

11. Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso
Poggio. Unsupervised learning of invariant representations. Theoretical Computer Science,
633:112–121, 2016.

12. Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori,
CoRR,

and Antonio Criminisi. Measuring neural net robustness with constraints.
abs/1605.07262, 2016. To appear in NIPS.

13. Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov,
Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In
ECML/PKDD 2013, pages 387–402, 2013.

14. Christopher M Bishop. Neural networks for pattern recognition. Oxford university press,

1995.

15. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp,
Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang,
Jake Zhao, and Karol Zieba. End to end learning for self-driving cars. arXiv:1604.07316,
2016.

16. Gunnar E. Carlsson, Tigran Ishkhanov, Vin de Silva, and Afra Zomorodian. On the local
behavior of spaces of natural images. International Journal of Computer Vision, 76(1), 2008.
17. Lisa Anne Hendricks Dong Huk Park, Zeynep Akata, Bernt Schiele, Trevor Darrell, and
Marcus Rohrbach. Attentive explanations: Justifying decisions and pointing to the evidence.
arxiv.org/abs/1612.04757, 2016.

18. Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classiﬁers’ robustness to

adversarial perturbations. CoRR, abs/1502.02590, 2015.

19. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing ad-

versarial examples. CoRR, abs/1412.6572, 2014.

20. Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety veriﬁcation of deep

neural networks. https://arxiv.org/abs/1610.06940, 2016.

21. Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An
eﬃcient SMT solver for verifying deep neural networks. In CAV 2017, 2017. To appear.
22. Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical

world. arXiv:1607.02533, 2016.

23. Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, 521:436–444,

24. St´ephane Mallat. Understanding deep convolutional networks. Philosohical Transactions of

2015.

the Royal Society A, 2016.

25. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple

and accurate method to fool deep neural networks. CoRR, abs/1511.04599, 2015.

26. Anh Nguyen, Jason Yosinski, and Jeﬀ Clune. Deep neural networks are easily fooled: High
conﬁdence predictions for unrecognizable images. In Computer Vision and Pattern Recog-
nition (CVPR ’15), 2015.

27. Nicolas Papernot, Ian Goodfellow, Ryan Sheatsley, Reuben Feinman, and Patrick McDaniel.
cleverhans v1.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768,
2016.

28. Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and
Ananthram Swami. The limitations of deep learning in adversarial settings. In Proceedings
of the 1st IEEE European Symposium on Security and Privacy, 2015.

29. Nicolas Papernot, Patrick Drew McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik,
and Ananthram Swami. Practical black-box attacks against deep learning systems using
adversarial examples. CoRR, abs/1602.02697, 2016.

30. Luca Pulina and Armando Tacchella. An abstraction-reﬁnement approach to veriﬁcation of

artiﬁcial neural networks. In CAV 2010, pages 243–257, 2010.

31. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust you?”: Ex-
In ACM SIGKDD International Conference on

plaining the predictions of any classiﬁer.
Knowledge Discovery and Data Mining (KDD2016), 2016.

32. Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker. Towards veriﬁcation
of artiﬁcial neural networks. In 18th Workshop on Methoden und Beschreibungssprachen
zur Modellierung und Veriﬁkation von Schaltungen und Systemen” (MBMV), pages 30–40,
2015.

33. Sanjit A. Seshia and Dorsa Sadigh.

Towards veriﬁed artiﬁcial intelligence. CoRR,

abs/1606.08514, 2016.

34. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv:1409.1556, 2014.

35. J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking
machine learning algorithms for traﬃc sign recognition. Neural Networks, 32:323–332,
2012.

36. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
In International

Intriguing properties of neural networks.

Goodfellow, and Rob Fergus.
Conference on Learning Representations (ICLR-2014), 2014.

37. Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in Neural
Information Processing Systems 4, [NIPS Conference, Denver, Colorado, USA, December
2-5, 1991], pages 831–838, 1991.

38. Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness

of deep neural networks via stability training. In CVPR 2016, 2016.

A Input Parameters and Experimental Setup

The DLV tool accepts as input a network N and an image x, and has the following input
parameters:

– an integer l ∈ [0, n] indicating the starting layer Ll,
– an integer dimsl ≥ 1 indicating the maximal number of dimensions that need to be

considered in layer Ll,

– the values of variables sp and mp in Vl; for simplicity, we ask that, for all dimensions
p that will be selected by the automated procedure, sp and mp have the same values,

– the precision ε ∈ [0, ∞),
– an integer dimsk, f indicating the number of dimensions for each feature; for sim-
plicity, we ask that every feature has the same number of dimensions and dimsk, f =
dimsk(cid:48), f for all layers k and k(cid:48), and

– type of search: either heuristic (single-path) or Monte Carlo Tree Search (MCTS)

(multi-path).

A.1 Two-Dimensional Point Classiﬁcation Network

A.2 Network for the MNIST Dataset

– l = 0
– dimsl = 2,
– sp = 1.0 and mp = 1.0,
– ε = 0.1, and
– dimsk, f = 2

– l = 1
– dimsl = 150,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

– l = 1
– dimsl = 500,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

– l = 1
– dimsl = 1000,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

A.3 Network for the CIFAR-10 Dataset

A.4 Network for the GTSRB Dataset

A.5 Network for the ImageNet Dataset

– l = 2
– dimsl = 20, 000,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

B Additional Adversarial Examples Found for the CIFAR-10,

ImageNet, and MNIST Networks

Figure 13 and Figure 14 present additional adversarial examples for the CIFAR-10 and
ImageNet networks by single-path search. Figure 15 presents adversarial examples for
the MNIST network by multi-path search.

automobile to bird

automobile to frog

automobile to airplane automobile to horse

airplane to dog

airplane to deer

airplane to truck

airplane to cat

truck to frog

truck to cat

ship to bird

ship to airplane

ship to truck

horse to cat

horse to automobile

horse to truck

Fig. 13. Adversarial examples for a neural network trained on the CIFAR-10 dataset by
single-path search

C Additional Adversarial Examples for the German Traﬃc Sign

Recognition Benchmark (GTSRB)

Figure 16 presents adversarial examples obtained when selecting single-path search.

labrador to life boat

rhodesian ridgeback to malinois

boxer to rhodesian ridgeback

great pyrenees to kuvasz

Fig. 14. Adversarial Examples for the VGG16 Network Trained on the imageNet
Dataset By Single-Path Search

9 to 4

8 to 3

5 to 3

4 to 9

5 to 3

7 to 3

9 to 4

9 to 4

2 to 3

1 to 8

8 to 5

0 to 3

7 to 2

8 to 3

3 to 2

9 to 7

3 to 2

4 to 9

6 to 4

3 to 5

9 to 4

0 to 2

2 to 3

9 to 8

4 to 2

Fig. 15. Adversarial examples for the network trained on the MNIST dataset by multi-
path search

speed limit 50 (pro-
hibitory) to speed limit
80 (prohibitory)

restriction ends (other)
to restriction ends (80)

no overtaking (trucks)
(prohibitory)
to speed
limit 80 (prohibitory)

give way (other)
priority road (other)

to

priority road (other) to
speed limit 30 (pro-
hibitory)

speed limit 70 (pro-
hibitory) to speed limit
120 (prohibitory)

(pro-
overtaking
no
hibitory) to go straight
(mandatory)

speed limit 50 (pro-
hibitory) to stop (other)

road narrows (danger)
to construction (danger)

ends

restriction
80
(other) to speed limit 80
(prohibitory)

no overtaking (trucks)
(prohibitory)
to speed
limit 80 (prohibitory)

overtaking

no
hibitory)
ends
(trucks)) (other)

(pro-
to restriction
(overtaking

priority at next intersec-
tion (danger) to speed
limit 30 (prohibitory)

uneven road (danger) to
traﬃc signal (danger)

danger
to
school crossing (danger)

(danger)

Fig. 16. Adversarial examples for the GTSRB dataset by single-path search

D Architectures of Neural Networks

Figure 17, Figure 18, Figure 19, and Figure 20 present architectures of the networks we
work with in this paper. The network for the ImageNet dataset is from [34].

Fig. 17. Architecture of the neural network for two-dimensional point classiﬁcation

Fig. 18. Architecture of the neural network for the MNIST dataset

Fig. 19. Architecture of the neural network for the CIFAR-10 dataset

Fig. 20. Architecture of the neural network for the GTSRB dataset

7
1
0
2
 
y
a
M
 
5
 
 
]
I

A
.
s
c
[
 
 
3
v
0
4
9
6
0
.
0
1
6
1
:
v
i
X
r
a

Safety Veriﬁcation of Deep Neural Networks(cid:63)

Xiaowei Huang, Marta Kwiatkowska, Sen Wang and Min Wu

Department of Computer Science, University of Oxford

Abstract. Deep neural networks have achieved impressive experimental results
in image classiﬁcation, but can surprisingly be unstable with respect to adversar-
ial perturbations, that is, minimal changes to the input image that cause the net-
work to misclassify it. With potential applications including perception modules
and end-to-end controllers for self-driving cars, this raises concerns about their
safety. We develop a novel automated veriﬁcation framework for feed-forward
multi-layer neural networks based on Satisﬁability Modulo Theory (SMT). We
focus on safety of image classiﬁcation decisions with respect to image manipu-
lations, such as scratches or changes to camera angle or lighting conditions that
would result in the same class being assigned by a human, and deﬁne safety
for an individual decision in terms of invariance of the classiﬁcation within a
small neighbourhood of the original image. We enable exhaustive search of the
region by employing discretisation, and propagate the analysis layer by layer. Our
method works directly with the network code and, in contrast to existing meth-
ods, can guarantee that adversarial examples, if they exist, are found for the given
region and family of manipulations. If found, adversarial examples can be shown
to human testers and/or used to ﬁne-tune the network. We implement the tech-
niques using Z3 and evaluate them on state-of-the-art networks, including regu-
larised and deep learning networks. We also compare against existing techniques
to search for adversarial examples and estimate network robustness.

1

Introduction

Deep neural networks have achieved impressive experimental results in image classiﬁ-
cation, matching the cognitive ability of humans [23] in complex tasks with thousands
of classes. Many applications are envisaged, including their use as perception modules
and end-to-end controllers for self-driving cars [15]. Let Rn be a vector space of images
(points) that we wish to classify and assume that f : Rn → C, where C is a (ﬁnite) set of
class labels, models the human perception capability, then a neural network classiﬁer is
a function ˆf (x) which approximates f (x) from M training examples {(xi, ci)}i=1,..,M. For
example, a perception module of a self-driving car may input an image from a camera
and must correctly classify the type of object in its view, irrespective of aspects such
as the angle of its vision and image imperfections. Therefore, though they clearly in-
clude imperfections, all four pairs of images in Figure 1 should arguably be classiﬁed
as automobiles, since they appear so to a human eye.

(cid:63) This work is

supported by the EPSRC Programme Grant on Mobile Autonomy
(EP/M019918/1). Part of this work was done while MK was visiting the Simons Institute for
the Theory of Computing.

Classiﬁers employed in vision tasks are typically multi-layer networks, which prop-
agate the input image through a series of linear and non-linear operators. They are
high-dimensional, often with millions of dimensions, non-linear and potentially dis-
continuous: even a small network, such as that trained to classify hand-written images
of digits 0-9, has over 60,000 real-valued parameters and 21,632 neurons (dimensions)
in its ﬁrst layer. At the same time, the networks are trained on a ﬁnite data set and
expected to generalise to previously unseen images. To increase the probability of cor-
rectly classifying such an image, regularisation techniques such as dropout are typically
used, which improves the smoothness of the classiﬁers, in the sense that images that are
close (within (cid:15) distance) to a training point are assigned the same class label.

automobile to bird

automobile to frog

automobile to airplane automobile to horse

Fig. 1. Automobile images (classiﬁed correctly) and their perturbed images (classiﬁed
wrongly)

Unfortunately, it has been observed in [13,36] that deep neural networks, includ-
ing highly trained and smooth networks optimised for vision tasks, are unstable with
respect to so called adversarial perturbations. Such adversarial perturbations are (min-
imal) changes to the input image, often imperceptible to the human eye, that cause the
network to misclassify the image. Examples include not only artiﬁcially generated ran-
dom perturbations, but also (more worryingly) modiﬁcations of camera images [22] that
correspond to resizing, cropping or change in lighting conditions. They can be devised
without access to the training set [29] and are transferable [19], in the sense that an ex-
ample misclassiﬁed by one network is also misclassiﬁed by a network with a diﬀerent
architecture, even if it is trained on diﬀerent data. Figure 1 gives adversarial pertur-
bations of automobile images that are misclassiﬁed as a bird, frog, airplane or horse
by a highly trained state-of-the-art network. This obviously raises potential safety con-
cerns for applications such as autonomous driving and calls for automated veriﬁcation
techniques that can verify the correctness of their decisions.

Safety of AI systems is receiving increasing attention, to mention [33,10], in view
of their potential to cause harm in safety-critical situations such as autonomous driving.
Typically, decision making in such systems is either solely based on machine learning,
through end-to-end controllers, or involves some combination of logic-based reasoning
and machine learning components, where an image classiﬁer produces a classiﬁcation,
say speed limit or a stop sign, that serves as input to a controller. A recent trend towards
“explainable AI” has led to approaches that learn not only how to assign the classiﬁca-
tion labels, but also additional explanations of the model, which can take the form of
a justiﬁcation explanation (why this decision has been reached, for example identify-
ing the features that supported the decision) [17,31]. In all these cases, the safety of a
decision can be reduced to ensuring the correct behaviour of a machine learning com-

ponent. However, safety assurance and veriﬁcation methodologies for machine learning
are little studied.

The main diﬃculty with image classiﬁcation tasks, which play a critical role in per-
ception modules of autonomous driving controllers, is that they do not have a formal
speciﬁcation in the usual sense: ideally, the performance of a classiﬁer should match
the perception ability and class labels assigned by a human. Traditionally, the correct-
ness of a neural network classiﬁer is expressed in terms of risk [37], deﬁned as the
probability of misclassiﬁcation of a given image, weighted with respect to the input
distribution µ of images. Similar (statistical) robustness properties of deep neural net-
work classiﬁers, which compute the average minimum distance to a misclassiﬁcation
and are independent of the data point, have been studied and can be estimated using
tools such as DeepFool [25] and cleverhans [27]. However, we are interested in the
safety of an individual decision, and to this end focus on the key property of the clas-
siﬁer being invariant to perturbations at a given point. This notion is also known as
pointwise robustness [18,12] or local adversarial robustness [21].

Contributions. In this paper we propose a general framework for automated veriﬁ-
cation of safety of classiﬁcation decisions made by feed-forward deep neural networks.
Although we work concretely with image classiﬁers, the techniques can be generalised
to other settings. For a given image x (a point in a vector space), we assume that there
is a (possibly inﬁnite) region η around that point that incontrovertibly supports the de-
cision, in the sense that all points in this region must have the same class. This region is
speciﬁed by the user and can be given as a small diameter, or the set of all points whose
salient features are of the same type. We next assume that there is a family of operations
∆, which we call manipulations, that specify modiﬁcations to the image under which the
classiﬁcation decision should remain invariant in the region η. Such manipulations can
represent, for example, camera imprecisions, change of camera angle, or replacement of
a feature. We deﬁne a network decision to be safe for input x and region η with respect
to the set of manipulations ∆ if applying the manipulations on x will not result in a class
change for η. We employ discretisation to enable a ﬁnite exhaustive search of the high-
dimensional region η for adversarial misclassiﬁcations. The discretisation approach is
justiﬁed in the case of image classiﬁers since they are typically represented as vectors of
discrete pixels (vectors of 8 bit RGB colours). To achieve scalability, we propagate the
analysis layer by layer, mapping the region and manipulations to the deeper layers. We
show that this propagation is sound, and is complete under the additional assumption of
minimality of manipulations, which holds in discretised settings. In contrast to existing
approaches [36,28], our framework can guarantee that a misclassiﬁcation is found if it
exists. Since we reduce veriﬁcation to a search for adversarial examples, we can achieve
safety veriﬁcation (if no misclassiﬁcations are found for all layers) or falsiﬁcation (in
which case the adversarial examples can be used to ﬁne-tune the network or shown to a
human tester).

We implement the techniques using Z3 [8] in a tool called DLV (Deep Learning Ver-
iﬁcation) [2] and evaluate them on state-of-the-art networks, including regularised and
deep learning networks. This includes image classiﬁcation networks trained for clas-
sifying hand-written images of digits 0-9 (MNIST), 10 classes of small colour images
(CIFAR10), 43 classes of the German Traﬃc Sign Recognition Benchmark (GTSRB)

[35] and 1000 classes of colour images used for the well-known imageNet large-scale
visual recognition challenge (ILSVRC) [4]. We also perform a comparison of the DLV
falsiﬁcation functionality on the MNIST dataset against the methods of [36] and [28],
focusing on the search strategies and statistical robustness estimation. The perturbed
images in Figure 1 are found automatically using our tool for the network trained on
the CIFAR10 dataset.

This invited paper is an extended and improved version of [20], where an extended

version including appendices can also be found.

2 Background on Neural Networks

We consider feed-forward multi-layer neural networks [14], henceforth abbreviated as
neural networks. Perceptrons (neurons) in a neural network are arranged in disjoint
layers, with each perceptron in one layer connected to the next layer, but no connection
between perceptrons in the same layer. Each layer Lk of a network is associated with
an nk-dimensional vector space DLk ⊆ Rnk , in which each dimension corresponds to
a perceptron. We write Pk for the set of perceptrons in layer Lk and nk = |Pk| is the
number of perceptrons (dimensions) in layer Lk.

Formally, a (feed-forward and deep) neural network N is a tuple (L, T, Φ), where
L = {Lk | k ∈ {0, ..., n}} is a set of layers such that layer L0 is the input layer and Ln
is the output layer, T ⊆ L × L is a set of sequential connections between layers such
that, except for the input and output layers, each layer has an incoming connection and
an outgoing connection, and Φ = {φk | k ∈ {1, ..., n}} is a set of activation functions
φk : DLk−1 → DLk , one for each non-input layer. Layers other than input and output
layers are called the hidden layers.

The network is fed an input x (point in DL0 ) through its input layer, which is then
propagated through the layers by successive application of the activation functions. An
activation for point x in layer k is the value of the corresponding function, denoted
αx,k = φk(φk−1(...φ1(x))) ∈ DLk , where αx,0 = x. For perceptron p ∈ Pk we write
αx,k(p) for the value of its activation on input x. For every activation αx,k and layer
k(cid:48) < k, we deﬁne Prek(cid:48) (αx,k) = {αy,k(cid:48) ∈ DLk(cid:48)
| αy,k = αx,k} to be the set of activations in
layer k(cid:48) whose corresponding activation in layer Lk is αx,k. The classiﬁcation decision
is made based on the activations in the output layer by, e.g., assigning to x the class
arg maxp∈Pn αx,n(p). For simplicity, we use αx,n to denote the class assigned to input x,
and thus αx,n = αy,n expresses that two inputs x and y have the same class.

The neural network classiﬁer N represents a function ˆf (x) which approximates
f (x) : DL0 → C, a function that models the human perception capability in labelling im-
ages with labels from C, from M training examples {(xi, ci)}i=1,..,M. Image classiﬁcation
networks, for example convolutional networks, may contain many layers, which can
be non-linear, and work in high dimensions, which for the image classiﬁcation prob-
lems can be of the order of millions. Digital images are represented as 3D tensors of
pixels (width, height and depth, the latter to represent colour), where each pixel is a dis-
crete value in the range 0..255. The training process determines real values for weights
used as ﬁlters that are convolved with the activation functions. Since it is diﬃcult to
approximate f with few samples in the sparsely populated high-dimensional space, to

increase the probability of classifying correctly a previously unseen image, various reg-
ularisation techniques such as dropout are employed. They improve the smoothness of
the classiﬁer, in the sense that points that are (cid:15)-close to a training point (potentially
inﬁnitely many of them) classify the same.

In this paper, we work with the code of the network and its trained weights.

3 Safety Analysis of Classiﬁcation Decisions

In this section we deﬁne our notion of safety of classiﬁcation decisions for a neural net-
work, based on the concept of a manipulation of an image, essentially perturbations that
a human observer would classify the same as the original image. Safety is deﬁned for
an individual classiﬁcation decision and is parameterised by the class of manipulations
and a neighbouring region around a given image. To ensure ﬁniteness of the search of
the region for adversarial misclassiﬁcations, we introduce so called “ladders”, nonde-
terministically branching and iterated application of successive manipulations, and state
the conditions under which the search is exhaustive.

Safety and Robustness Our method assumes the existence of a (possibly inﬁnite)
region η around a data point (image) x such that all points in the region are indistin-
guishable by a human, and therefore have the same true class. This region is understood
as supporting the classiﬁcation decision and can usually be inferred from the type of
the classiﬁcation problem. For simplicity, we identify such a region via its diameter d
with respect to some user-speciﬁed norm, which intuitively measures the closeness to
the point x. As deﬁned in [18], a network ˆf approximating human capability f is said
to be not robust at x if there exists a point y in the region η = {z ∈ DL0 | ||z − x|| ≤ d}
of the input layer such that ˆf (x) (cid:44) ˆf (y). The point y, at a minimal distance from x, is
known as an adversarial example. Our deﬁnition of safety for a classiﬁcation decision
(abbreviated safety at a point) follows he same intuition, except that we work layer
by layer, and therefore will identify such a region ηk, a subspace of DLk , at each layer
Lk, for k ∈ {0, ..., n}, and successively reﬁne the regions through the deeper layers. We
justify this choice based on the observation [11,23,24] that deep neural networks are
thought to compute progressively more powerful invariants as the depth increases. In
other words, they gradually transform images into a representation in which the classes
are separable by a linear classiﬁer.

Assumption 1 For each activation αx,k of point x in layer Lk, the region ηk(αx,k) con-
tains activations that the human observer believes to be so close to αx,k that they should
be classiﬁed the same as x.

Intuitively, safety for network N at a point x means that the classiﬁcation decision is
robust at x against perturbations within the region ηk(αx,k). Note that, while the pertur-
bation is applied in layer Lk, the classiﬁcation decision is based on the activation in the
output layer Ln.

Deﬁnition 1. [General Safety] Let ηk(αx,k) be a region in layer Lk of a neural network
N such that αx,k ∈ ηk(αx,k). We say that N is safe for input x and region ηk(αx,k), written
as N, ηk |= x, if for all activations αy,k in ηk(αx,k) we have αy,n = αx,n.

We remark that, unlike the notions of risk [37] and robustness of [18,12], we work
with safety for a speciﬁc point and do not account for the input distribution, but such
expectation measures can be considered, see Section 6 for comparison.

Manipulations A key concept of our framework is the notion of a manipulation, an
operator that intuitively models image perturbations, for example bad angles, scratches
or weather conditions, the idea being that the classiﬁcation decisions in a region of im-
ages close to it should be invariant under such manipulations. The choice of the type of
manipulation is dependent on the application and user-deﬁned, reﬂecting knowledge of
the classiﬁcation problem to model perturbations that should or should not be allowed.
Judicious choice of families of such manipulations and appropriate distance metrics is
particularly important. For simplicity, we work with operators δk : DLk → DLk over the
activations in the vector space of layer k, and consider the Euclidean (L2) and Manhattan
(L1) norms to measure the distance between an image and its perturbation through δk,
but the techniques generalise to other norms discussed in [18,19,12]. More speciﬁcally,
applying a manipulation δk(αx,k) to an activation αx,k will result in another activation
such that the values of some or all dimensions are changed. We therefore represent a
manipulation as a hyper-rectangle, deﬁned for two activations αx,k and αy,k of layer Lk
by rec(αx,k, αy,k) = ×p∈Pk [min(αx,k(p), αy,k(p)), max(αx,k(p), αy,k(p))]. The main chal-
lenge for veriﬁcation is the fact that the region ηk contains potentially an uncountable
number of activations. Our approach relies on discretisation in order to enable a ﬁnite
exploration of the region to discover and/or rule out adversarial perturbations.

For an activation αx,k and a set ∆ of manipulations, we denote by rec(∆, αx,k) the
polyhedron which includes all hyper-rectangles that result from applying some manip-
ulation in ∆ on αx,k, i.e., rec(∆, αx,k) = (cid:83)
δ∈∆ rec(αx,k, δ(αx,k)). Let ∆k be the set of all
possible manipulations for layer Lk. To ensure region coverage, we deﬁne valid manip-
ulation as follows.

Deﬁnition 2. Given an activation αx,k, a set of manipulations V(αx,k) ⊆ ∆k is valid if
αx,k is an interior point of rec(V(αx,k), αx,k), i.e., αx,k is in rec(V(αx,k), αx,k) and does
not belong to the boundary of rec(V(αx,k), αx,k).

Figure 2 presents an example of valid manipulations in two-dimensional space: each
arrow represents a manipulation, each dashed box represents a (hyper-)rectangle of the
corresponding manipulation, and activation αx,k is an interior point of the space from
the dashed boxes.

Since we work with discretised spaces, which is a reasonable assumption for im-
ages, we introduce the notion of a minimal manipulation. If applying a minimal manip-
ulation, it suﬃces to check for misclassiﬁcation just at the end points, that is, αx,k and
δk(αx,k). This allows an exhaustive, albeit impractical, exploration of the region in unit
steps.

A manipulation δ1

k(αx,k), written as δ1

k(αy,k) is ﬁner than δ2

k(αx,k), if any
activation in the hyper-rectangle of the former is also in the hyper-rectangle of the latter.
It is implied in this deﬁnition that αy,k is an activation in the hyper-rectangle of δ2
k(αx,k).
Moreover, we write δk,k(cid:48) (αx,k) for φk(cid:48) (...φk+1(δk(αx,k))), representing the corresponding
activation in layer k(cid:48) ≥ k after applying manipulation δk on the activation αx,k, where
δk,k(αx,k) = δk(αx,k).

k(αy,k) ≤ δ2

Fig. 2. Example of a set {δ1, δ2, δ3, δ4} of valid manipulations in a 2-dimensional space

Deﬁnition 3. A manipulation δk on an activation αx,k is minimal if there does not exist
k(αx,k) ≤ δk(αx,k), αy,k =
k and an activation αy,k such that δ1
manipulations δ1
k(αy,k), and αy,n (cid:44) αx,n and αy,n (cid:44) δk,n(αx,k).
k(αx,k), δk(αx,k) = δ2
δ1

k and δ2

Intuitively, a minimal manipulation does not have a ﬁner manipulation that results in
a diﬀerent classiﬁcation. However, it is possible to have diﬀerent classiﬁcations before
and after applying the minimal manipulation, i.e., it is possible that δk,n(αx,k) (cid:44) αx,n. It
is not hard to see that the minimality of a manipulation implies that the class change in
its associated hyper-rectangle can be detected by checking the class of the end points
αx,k and δk(αx,k).

Bounded Variation Recall that we apply manipulations in layer Lk, but check the
classiﬁcation decisions in the output layer. To ensure ﬁnite, exhaustive coverage of
the region, we introduce a continuity assumption on the mapping from space DLk to
the output space DLn , adapted from the concept of bounded variation [9]. Given an
activation αx,k with its associated region ηk(αx,k), we deﬁne a “ladder” on ηk(αx,k) to
be a set ld of activations containing αx,k and ﬁnitely many, possibly zero, activations
from ηk(αx,k). The activations in a ladder can be arranged into an increasing order
αx,k = αx0,k < αx1,k < ... < αx j,k such that every activation αxt,k ∈ ld appears once and
has a successor αxt+1,k such that αxt+1,k = δk(αxt,k) for some manipulation δk ∈ V(αxt,k).
For the greatest element αx j,k, its successor should be outside the region ηk(αx,k), i.e.,
αx j+1,k (cid:60) ηk(αx,k). Given a ladder ld, we write ld(t) for its t + 1-th activation, ld[0..t] for
the preﬁx of ld up to the t + 1-th activation, and last(ld) for the greatest element of ld.
Figure 3 gives a diagrammatic explanation on the ladders.

Deﬁnition 4. Let L(ηk(αx,k)) be the set of ladders in ηk(αx,k). Then the total variation
of the region ηk(αx,k) on the neural network with respect to L(ηk(αx,k)) is

V(N; ηk(αx,k)) =

sup
ld∈L(ηk(αx,k))

(cid:88)

diﬀn(αxt,n, αxt+1,n)

αxt ,k∈ld\{last(ld)}

where diﬀn : DLn × DLn → {0, 1} is given by diﬀn(αx,n, αy,n) = 0 if αx,n = αy,n and 1
otherwise. We say that the region ηk(αx,k) is a bounded variation if V(N; ηk(αx,k)) < ∞,
and are particularly interested in the case when V(N; rk(αy,k)) = 0, which is called a
0-variation.

Fig. 3. Examples of ladders in region ηk(αx,k). Starting from αx,k = αx0,k, the activations
αx1,k...αx j,k form a ladder such that each consecutive activation results from some valid
manipulation δk applied to a previous activation, and the ﬁnal activation αx j,k is outside
the region ηk(αx,k).

The set L(ηk(αx,k)) is complete if, for any ladder ld ∈ L(ηk(αx,k)) of j+1 activations,
any element ld(t) for 0 ≤ t ≤ j, and any manipulation δk ∈ V(ld(t)), there exists a ladder
ld(cid:48) ∈ L(ηk(αx,k)) such that ld(cid:48)[0..t] = ld[0..t] and ld(cid:48)(t + 1) = δk(ld(t)). Intuitively, a
complete ladder is a complete tree, on which each node represents an activation and
each branch of a node corresponds to a valid manipulation. From the root αx,k, every
path of the tree leading to a leaf is a ladder. Moreover, the set L(ηk(αx,k)) is covering if
the polyhedra of all activations in it cover the region ηk(αx,k), i.e.,
(cid:91)

(cid:91)

ηk(αx,k) ⊆

rec(V(αxt,k), αxt,k).

(1)

ld∈L(ηk(αx,k))

αxt ,k∈ld\{last(ld)}

Based on the above, we have the following deﬁnition of safety with respect to a set
of manipulations. Intuitively, we iteratively and nondeterministically apply manipula-
tions to explore the region ηk(αx,k), and safety means that no class change is observed
by successive application of such manipulations.

Deﬁnition 5. [Safety wrt Manipulations] Given a neural network N, an input x and a
set ∆k of manipulations, we say that N is safe for input x with respect to the region ηk
and manipulations ∆k, written as N, ηk, ∆k |= x, if the region ηk(αx,k) is a 0-variation for
the set L(ηk(αx,k)) of its ladders, which is complete and covering.

It is straightforward to note that general safety in the sense of Deﬁnition 1 implies

safety wrt manipulations, in the sense of Deﬁnition 5.

Theorem 1. Given a neural network N, an input x, and a region ηk, we have that
N, ηk |= x implies N, ηk, ∆k |= x for any set of manipulations ∆k.

In the opposite direction, we require the minimality assumption on manipulations.

Theorem 2. Given a neural network N, an input x, a region ηk(αx,k) and a set ∆k of
manipulations, we have that N, ηk, ∆k |= x implies N, ηk |= x if the manipulations in ∆k
are minimal.

Theorem 2 means that, under the minimality assumption over the manipulations, an
exhaustive search through the complete and covering ladder tree from L(ηk(αx,k)) can
ﬁnd adversarial examples, if any, and enable us to conclude that the network is safe
at a given point if none are found. Though computing minimal manipulations is not
practical, in discrete spaces by iterating over increasingly reﬁned manipulations we are
able to rule out the existence of adversarial examples in the region. This contrasts with
partial exploration according to, e.g., [25,12]; for comparison see Section 7.

4 The Veriﬁcation Framework

In this section we propose a novel framework for automated veriﬁcation of safety of
classiﬁcation decisions, which is based on search for an adversarial misclassiﬁcation
within a given region. The key distinctive distinctive features of our framework com-
pared to existing work are: a guarantee that a misclassiﬁcation is found if it exists; the
propagation of the analysis layer by layer; and working with hidden layers, in addi-
tion to input and output layers. Since we reduce veriﬁcation to a search for adversarial
examples, we can achieve safety veriﬁcation (if no misclassiﬁcations are found for all
layers) or falsiﬁcation (in which case the adversarial examples can be used to ﬁne-tune
the network or shown to a human tester).

4.1 Layer-by-Layer Analysis

We ﬁrst consider how to propagate the analysis layer by layer, which will involve reﬁn-
ing manipulations through the hidden layers. To facilitate such analysis, in addition to
the activation function φk : DLk−1 → DLk we also require a mapping ψk : DLk → DLk−1
in the opposite direction, to represent how a manipulated activation of layer Lk aﬀects
the activations of layer Lk−1. We can simply take ψk as the inverse function of φk. In
order to propagate safety of regions ηk(αx,k) at a point x into deeper layers, we assume
the existence of functions ηk that map activations to regions, and impose the following
restrictions on the functions φk and ψk, shown diagrammatically in Figure 4.

Deﬁnition 6. The functions {η0, η1, ..., ηn} and {ψ1, ..., ψn} mapping activations to re-
gions are such that

1. ηk(αx,k) ⊆ DLk , for k = 0, ..., n,
2. αx,k ∈ ηk(αx,k), for k = 0, ..., n, and
3. ηk−1(αi,k−1) ⊆ ψk(ηk(αx,k)) for all k = 1, ..., n.

Intuitively, the ﬁrst two conditions state that each function ηk assigns a region around
the activation αx,k, and the last condition that mapping the region ηk from layer Lk to

Lk−1 via ψk should cover the region ηk−1. The aim is to compute functions ηk+1, ..., ηn
based on ηk and the neural network.

The size and complexity of a deep neural network generally means that determining
whether a given set ∆k of manipulations is minimal is intractable. To partially counter
this, we deﬁne a reﬁnement relation between safety wrt manipulations for consecutive
layers in the sense that N, ηk, ∆k |= x is a reﬁnement of N, ηk−1, ∆k−1 |= x if all ma-
nipulations δk−1 in ∆k−1 are reﬁned by a sequence of manipulations δk from the set ∆k.
Therefore, although we cannot theoretically conﬁrm the minimality of ∆k, they are re-
ﬁned layer by layer and, in discrete settings, this process can be bounded from below
by the unit step. Moreover, we can work gradually from a speciﬁc layer inwards until
an adversarial example is found, ﬁnishing processing when reaching the output layer.

The reﬁnement framework is given in Figure 5. The arrows represent the implication

Fig. 4. Layer by layer analysis according to Deﬁnition 6

relations between the safety notions and are labelled with conditions if needed. The
goal of the reﬁnements is to ﬁnd a chain of implications to justify N, η0 |= x. The
fact that N, ηk |= x implies N, ηk−1 |= x is due to the constraints in Deﬁnition 6 when
ψk = φ−1
k . The fact that N, ηk |= x implies N, ηk, ∆k |= x follows from Theorem 1. The
implication from N, ηk, ∆k |= x to N, ηk |= x under the condition that ∆k is minimal is
due to Theorem 2.

We now deﬁne the notion of reﬁnability of manipulations between layers. Intu-
itively, a manipulation in layer Lk−1 is reﬁnable in layer Lk if there exists a sequence of
manipulations in layer Lk that implements the manipulation in layer Lk−1.

Deﬁnition 7. A manipulation δk−1(αy,k−1) is reﬁnable in layer Lk if there exist activa-
tions αx0,k, ..., αx j,k ∈ DLk and valid manipulations δ1
k ∈ V(αx j−1,k) such
that αy,k = αx0,k, δk−1,k(αy,k−1) = αx j,k, and αxt,k = δt
k(αxt−1,k) for 1 ≤ t ≤ j. Given a
neural network N and an input x, the manipulations ∆k are a reﬁnement by layer of
ηk−1, ∆k−1 and ηk if, for all αy,k−1 ∈ ηk−1(αz,k−1), all its valid manipulations δk−1(αy,k−1)
are reﬁnable in layer Lk.

k ∈ V(αx0,k), ..., δ j

Fig. 5. Reﬁnement framework

We have the following theorem stating that the reﬁnement of safety notions is im-

plied by the “reﬁnement by layer” relation.

Theorem 3. Assume a neural network N and an input x. For all layers k ≥ 1, if manip-
ulations ∆k are reﬁnement by layer of ηk−1, ∆k−1 and ηk, then we have that N, ηk, ∆k |= x
implies N, ηk−1, ∆k−1 |= x.

We note that any adversarial example of safety wrt manipulations N, ηk, ∆k |= x
is also an adversarial example for general safety N, ηk |= x. However, an adversarial
example αx,k for N, ηk |= x at layer k needs to be checked to see if it is an adversarial
example of N, η0 |= x, i.e. for the input layer. Recall that Prek(cid:48) (αx,k) is not necessarily
unique. This is equivalent to checking the emptiness of Pre0(αx,k) ∩ η0(αx,0). If we start
the analysis with a hidden layer k > 0 and there is no speciﬁcation for η0, we can instead
consider checking the emptiness of {αy,0 ∈ Pre0(αx,k) | αy,n (cid:44) αx,n}.

4.2 The Veriﬁcation Method

We summarise the theory developed thus far as a search-based recursive veriﬁcation
procedure given below. The method is parameterised by the region ηk around a given
point and a family of manipulations ∆k. The manipulations are speciﬁed by the user
for the classiﬁcation problem at hand, or alternatively can be selected automatically, as
described in Section 4.4. The vector norm to identify the region can also be speciﬁed
by the user and can vary by layer. The method can start in any layer, with analysis
propagated into deeper layers, and terminates when a misclassiﬁcation is found. If an
adversarial example is found by manipulating a hidden layer, it can be mapped back to
the input layer, see Section 4.5.

Algorithm 1 Given a neural network N and an input x, recursively perform the fol-
lowing steps, starting from some layer l ≥ 0. Let k ≥ l be the current layer under
consideration.

1. determine a region ηk such that if k > l then ηk and ηk−1 satisfy Deﬁnition 6;
2. determine a manipulation set ∆k such that if k > l then ∆k is a reﬁnement by layer

of ηk−1, ∆k−1 and ηk according to Deﬁnition 7;

3. verify whether N, ηk, ∆k |= x,
(a) if N, ηk, ∆k |= x then

i. report that N is safe at x with respect to ηk(αx,k) and ∆k, and
ii. continue to layer k + 1;

(b) if N, ηk, ∆k (cid:54)|= x, then report an adversarial example.

We implement Algorithm 1 by utilising satisﬁability modulo theory (SMT) solvers.
The SMT problem is a decision problem for logical formulas with respect to combina-
tions of background theories expressed in classical ﬁrst-order logic with equality. For
checking reﬁnement by layer, we use the theory of linear real arithmetic with existen-
tial and universal quantiﬁers, and for veriﬁcation within a layer (0-variation) we use the
same theory but without universal quantiﬁcation. The details of the encoding and the ap-
proach taken to compute the regions and manipulations are included in Section 4.4. To
enable practical veriﬁcation of deep neural networks, we employ a number of heuristics
described in the remainder of this section.

4.3 Feature Decomposition and Discovery

While Theorem 1 and 2 provide a ﬁnite way to verify safety of neural network clas-
siﬁcation decisions, the high-dimensionality of the region ηk(αx,k) can make any com-
putational approach impractical. We therefore use the concept of a feature to parti-
tion the region ηk(αx,k) into a set of features, and exploit their independence and low-
dimensionality. This allows us to work with state-of-the-art networks that have hun-
dreds, and even thousands, of dimensions.

Intuitively, a feature deﬁnes for each point in the high-dimensional space DLk the
most explicit salient feature it has, e.g., the red-coloured frame of a street sign in Fig-
ure 10. Formally, for each layer Lk, a feature function fk : DLk → P(DLk ) assigns a small
region for each activation αx,k in the space DLk , where P(DLk ) is the set of subspaces of
DLk . The region fk(αx,k) may have lower dimension than that of Dk. It has been argued,
in e.g. [16] for natural images, that natural data, for example natural images and sound,
forms a high-dimensional manifold, which embeds tangled manifolds to represent their
features. Feature manifolds usually have lower dimension than the data manifold, and a
classiﬁcation algorithm is to separate a set of tangled manifolds. By assuming that the
appearance of features is independent, we can manipulate them one by one regardless
of the manipulation order, and thus reduce the problem of size O(2d1+...+dm ) into a set of
smaller problems of size O(2d1 ), ..., O(2dm ).

The analysis of activations in hidden layers, as performed by our method, provides
an opportunity to discover the features automatically. Moreover, deﬁning the feature
fk on each activation as a single region corresponding to a speciﬁc feature is without
loss of generality: although an activation may include multiple features, the indepen-
dence relation between features suggests the existence of a total relation between these
features. The function fk essentially deﬁnes for each activation one particular feature,
subject to certain criteria such as explicit knowledge, but features can also be explored
in parallel.

Every feature fk(αy,k) is identiﬁed by a pre-speciﬁed number dimsk, f of dimensions.
Let dimsk( fk(αy,k)) be the set of dimensions selected according to some heuristic. Then
we have that

(cid:40)

fk(αy,k)(p) =

ηk(αx,k)(p),
[αy,k(p), αy,k(p)] otherwise.

if p ∈ dimsk( fk(αy,k))

(2)

Moreover, we need a set of features to partition the region ηk(αx,k) as follows.

Deﬁnition 8. A set { f1, ..., fm} of regions is a partition of ηk(αx,k), written as π(ηk(αx,k)),
if dimsk, f ( fi) ∩ dimsk, f ( f j) = ∅ for i, j ∈ {1, ..., m} and ηk(αx,k) = ×m

i=1 fi.
Given such a partition π(ηk(αx,k)), we deﬁne a function acts(x, k) by

acts(x, k) = {αy,k ∈ x | x ∈ π(ηk(αx,k))}

(3)

which contains one point for each feature. Then, we reduce the checking of 0-variation
of a region ηk(αx,k) to the following problems:

– checking whether the points in acts(x, k) have the same class as αx,k, and
– checking the 0-variation of all features in π(ηk(αx,k)).

In the above procedure, the checking of points in acts(x, k) can be conducted ei-
ther by following a pre-speciﬁed sequential order (single-path search) or by exhaus-
tively searching all possible orders (multi-path search). In Section 5 we demonstrate
that single-path search according to the prominence of features can enable us to ﬁnd
adversarial examples, while multi-path search may ﬁnd other examples whose distance
to the original input image is smaller.

4.4 Selection of Regions and Manipulations

The procedure summarised in Algorithm 1 is typically invoked for a given image in the
input layer, but, providing insight about hidden layers is available, it can start from any
layer Ll in the network. The selection of regions can be automated, as described below.
For the ﬁrst layer to be considered, i.e., k = l, the region ηk(αx,k) is deﬁned by ﬁrst
selecting the subset of dimsk dimensions from Pk whose activation values are furthest
away from the average activation value of the layer1. Intuitively, the knowledge repre-
sented by these activations is more explicit than the knowledge represented by the other
dimensions, and manipulations over more explicit knowledge are more likely to result
in a class change. Let avgk = ((cid:80)
p∈Pk αx,k(p))/nk be the average activation value of layer
Lk. We let dimsk(ηk(αx,k)) be the ﬁrst dimsk dimensions p ∈ Pk with the greatest values
|αx,k(p) − avg| among all dimensions, and then deﬁne

ηk(αx,k) = ×p∈dimsk(ηk(αx,k))[αx,k(p) − sp ∗ mp, αx,k(p) + sp ∗ mp]

(4)

i.e., a dimsk-polytope containing the activation αx,k, where sp represents a small span
and mp represents the number of such spans. Let Vk = {sp, mp | p ∈ dimsk(ηk(αx,k))} be
a set of variables.

Let d be a function mapping from dimsk(ηk(αx,k)) to {−1, 0, +1} such that {d(p) (cid:44)
0 | p ∈ dimsk(ηk(αx,k))} (cid:44) ∅, and D(dimsk(ηk(αx,k))) be the set of such functions. Let a
manipulation δd

k be

k (αy,k)(p) =
δd





αy,k(p) − sp if d(p) = −1
if d(p) = 0
αy,k(p)
αy,k(p) + sp if d(p) = +1

(5)

1 We also considered other approaches, including computing derivatives up to several layers, but

for the experiments we conduct they are less eﬀective.

for activation αy,k ∈ ηk(αx,k). That is, each manipulation changes a subset of the dimen-
sions by the span sp, according to the directions given in d. The set ∆k is deﬁned by col-
lecting the set of all such manipulations. Based on this, we can deﬁne a set L(ηk(αx,k))
of ladders, which is complete and covering.

Determining the region ηk according to ηk−1 Given ηk−1(αx,k−1) and the functions φk
and ψk, we can automatically determine a region ηk(αx,k) satisfying Deﬁnition 6 using
the following approach. According to the function φk, the activation value αx,k(p) of
perceptron p ∈ Pk is computed from activation values of a subset of perceptrons in
Pk−1. We let Vars(p) ⊆ Pk−1 be such a set of perceptrons. The selection of dimensions
in dimsk(ηk(αx,k)) depends on dimsk−1(ηk−1(αx,k−1)) and φk, by requiring that, for every
p(cid:48) ∈ dimsk−1(ηk−1(αx,k−1)), there is at least one dimension p ∈ dimsk(ηk(αx,k)) such that
p(cid:48) ∈ Vars(p). We let

dimsk(ηk(αx,k)) = {arg max

{ |αx,k(p) − avgk| | p(cid:48) ∈ Vars(p)} | p(cid:48) ∈ dimsk−1(ηk−1(αx,k−1))}

p∈Pk

(6)
Therefore, the restriction of Deﬁnition 6 can be expressed with the following formula:

∀αy,k−1 ∈ ηk(αx,k−1) : αy,k−1 ∈ ψk(ηk(αx,k)).

(7)

We omit the details of rewriting αy,k−1 ∈ ηk(αx,k−1) and αy,k−1 ∈ ψk(ηk(αx,k)) into
Boolean expressions, which follow from standard techniques. Note that this expres-
sion includes variables in Vk, Vk−1 and αy,k−1. The variables in Vk−1 are ﬁxed for a given
ηk−1(αx,k−1). Because such a region ηk(αx,k) always exists, a simple iterative procedure
can be invoked to gradually increase the size of the region represented with variables in
Vk to eventually satisfy the expression.

Determining the manipulation set ∆k according to ηk(αx,k), ηk−1(αx,k−1), and ∆k−1
The values of the variables Vk obtained from the satisﬁability of Eqn (7) yield a deﬁni-
tion of manipulations using Eqn (5). However, the obtained values for span variables sp
do not necessarily satisfy the “reﬁnement by layer” relation as deﬁned in Deﬁnition 7.
Therefore, we need to adapt the values for the variables Vk while, at the same time,
retaining the region ηk(αx,k). To do so, we could rewrite the constraint in Deﬁnition 7
into a formula, which can then be solved by an SMT solver. But, in practice, we notice
that such precise computations easily lead to overly small spans sp, which in turn result
in an unacceptable amount of computation needed to verify the relation N, ηk, ∆k |= x.
To reduce computational cost, we work with a weaker “reﬁnable in layer Lk” notion,
parameterised with respect to precision ε. Given two activations αy,k and αm,k, we use
dist(αy,k, αm,k) to represent their distance.

Deﬁnition 9. A manipulation δk−1(αy,k−1) is reﬁnable in layer Lk with precision ε > 0
if there exists a sequence of activations αx0,k, ..., αx j,k ∈ DLk and valid manipulations
k ∈ V(αx j−1,k) such that αy,k = αx0,k, δk−1,k(αy,k−1) ∈ rec(αx j−1,k, αx j,k),
k ∈ V(αx0,k), ..., δd
δ1
dist(αx j−1,k, αx j,k) ≤ (cid:15), and αxt,k = δt
k(αxt−1,k) for 1 ≤ t ≤ j. Given a neural network N
and an input x, the manipulations ∆k are a reﬁnement by layer of ηk, ηk−1, ∆k−1 with

precision ε if, for all αy,k−1 ∈ ηk−1(αx,k−1), all its legal manipulations δk−1(αy,k−1) are
reﬁnable in layer Lk with precision ε.

Comparing with Deﬁnition 7, the above deﬁnition replaces δk−1,k(αy,k−1) = αx j,k
with δk−1,k(αy,k−1) ∈ rec(αx j−1,k, αx j,k) and dist(αx j−1,k, αx j,k) ≤ ε. Intuitively, instead of
requiring a manipulation to reach the activation δk−1,k(αy,k−1) precisely, this deﬁnition
allows for each δk−1,k(αy,k−1) to be within the hyper-rectangle rec(αx j−1,k, αx j,k). To ﬁnd
suitable values for Vk according to the approximate “reﬁnement-by-layer” relation, we
use a variable h to represent the maximal number of manipulations of layer Lk used to
express a manipulation in layer k − 1. The value of h (and variables sp and np in Vk)
are automatically adapted to ensure the satisﬁability of the following formula, which
expresses the constraints of Deﬁnition 9:

∀αy,k−1 ∈ ηk(αx,k−1)∀d ∈ D(dimsk(ηk(αx,k−1)))∀δd
∃αy0,k, ..., αyh,k ∈ ηk(αx,k) : αy0,k = αy,k ∧ (cid:86)h−1
(cid:87)h−1

k−1 ∈ Vk−1(αy,k−1)
k (αyt,k)∧

t=0 αyt+1,k = δd

t=0 (δd

k−1,k(αy,k) ∈ rec(αyt,k, αyt+1,k) ∧ dist(αyt,k, αyt+1,k) ≤ ε).
It is noted that sp and mp for p ∈ dimsk(ηk(αx,k)) are employed when expressing δd
k .
The manipulation δd
k−1 by considering the corresponding relation
between dimensions in dimsk(ηk(αx,k)) and dimsk−1(ηk−1(αx,k−1)).

k is obtained from δd

(8)

Adversarial examples shown in Figures 8, 9, and 10 were found using single-path

search and automatic selection of regions and manipulations.

4.5 Mapping Back to Input Layer

When manipulating the hidden layers, we may need to map back an activation in layer
k to the input layer to obtain an input image that resulted in misclassiﬁcation, which
involves computation of Pre0(αy,k) described next. To check the 0-variation of a region
ηk(αx,k), we need to compute diﬀn(αx,n, αy,n) for many points αy,x in ηk(αx,k), where
diﬀn : DLn × DLn → {0, 1} is given by diﬀn(αx,n, αy,n) = 0 if αx,n = αy,n and 1 otherwise.
Because αx,n is known, we only need to compute αy,n. We can compute αy,n by ﬁnding
a point αy,0 ∈ Pre0(αy,k) and then using the neural network to predict the value αy,n. It
should be noted that, although Pre0(αy,k) may include more than one point, all points
have the same class, so any point in Pre0(αy,k) is suﬃcient for our purpose.

To compute αy,0 from αy,k, we use functions ψk, ψk−1, ..., ψ1 and compute points

αy,k−1, αy,k−2, ..., αy,0 such that

αy, j−1 = ψ j(αy, j) ∧ αy, j−1 ∈ η j−1(αx, j−1)

for 1 ≤ j ≤ k. The computation relies on an SMT solver to encode the functions
ψk, ψk−1, ..., ψ1 if they are piecewise linear functions, and by taking the corresponding
inverse functions directly if they are sigmoid functions. It is possible that, for some
1 ≤ j ≤ k, no point can be found by SMT solver, which means that the point αy,k does
not have any corresponding point in the input layer. We can safely discard these points.
The maxpooling function ψ j selects from every m ∗ m dimensions the maximal element
for some m > 0. The computation of the maxpooling layer ψ j−1 is combined with the
computation of the next layer ψ j, that is, ﬁnding αy, j−2 with the following expression
∃αx, j−1 : αy, j−2 = ψ j−1(ψ j(αy, j)) ∧ αy, j−1 ∈ η j−1(αx, j−1) ∧ αy, j−2 ∈ η j−2(αx, j−2)

This is to ensure that in the expression αy, j−2 = ψ j−1(ψ j(αy, j)) we can reuse m ∗ m − 1
elements in αx, j−2 and only need to replace the maximal element.

Figures 8, 9, and 10 show images obtained by mapping back from the ﬁrst hidden

layer to the input layer.

5 Experimental Results

The proposed framework has been implemented as a software tool called DLV (Deep
Learning Veriﬁcation) [2] written in Python, see Appendix of [20] for details of input
parameters and how to use the tool. The SMT solver we employ is Z3 [8], which has
Python APIs. The neural networks are built from a widely-used neural networks library
Keras [3] with a deep learning package Theano [6] as its backend.

We validate DLV on a set of experiments performed for neural networks trained for
classiﬁcation based on a predeﬁned multi-dimensional surface (small size networks),
as well as image classiﬁcation (medium size networks). These networks respectively
use two representative types of layers: fully connected layers and convolutional layers.
They may also use other types of layers, e.g., the ReLU layer, the pooling layer, the
zero-padding layer, and the dropout layer. The ﬁrst three demonstrate the single-path
search functionality on the Euclidean (L2) norm, whereas the fourth (GTSRB) multi-
path search for the L1 and L2 norms.

The experiments are conducted on a MacBook Pro laptop, with 2.7 GHz Intel Core

i5 CPU and 8 GB memory.

Two-Dimensional Point Classiﬁcation Network To demonstrate exhaustive veriﬁca-
tion facilitated by our framework, we consider a neural network trained for classifying
points above and below a two-dimensional curve shown in red in Figure 6 and Figure 7.
The network has three fully-connected hidden layers with the ReLU activation func-
tion. The input layer has two perceptrons, every hidden layer has 20 perceptrons, and
the output layer has two perceptrons. The network is trained with 5,000 points sampled
from the provided two-dimensional space, and has an accuracy of more than 99%.

For a given input x = (3.59, 1.11), we start from the input layer and deﬁne a region

around this point by taking unit steps in both directions

η0(αx,0) = [3.59 − 1.0, 3.59 + 1.0] × [1.11 − 1.0, 1.11 + 1.0] = [2.59, 4.59] × [0.11, 2.11]

The manipulation set ∆0 is shown in Figure 6: there are 9 points, of which the point in
the middle represents the activation αx,0 and the other 8 points represent the activations
resulting from applying one of the manipulations in ∆0 on αx,0. Note that, although there
are class changes in the region η0(αx,0), the manipulation set ∆0 is not able to detect such
changes. Therefore, we have that N, η0, ∆0 |= x.

Now consider layer k = 1. To obtain the region η1(αx,1), the tool selects two dimen-

sions p1,17, p1,19 ∈ P1 in layer L1 with indices 17 and 19 and computes

η1(αx,1) = [αx,1(p1,17) − 3.6, αx,1(p1,17) + 3.6] × [αx,1(p1,19) − 3.52, αx,1(p1,19) + 3.52]

The manipulation set ∆1, after mapping back to the input layer with function ψ1, is given
as Figure 7. Note that η1 and η0 satisfy Deﬁnition 6, and ∆1 is a reﬁnement by layer of

Fig. 6. Input layer

Fig. 7. First hidden layer

η0, ∆0 and η1. We can see that a class change can be detected (represented as the red
coloured point). Therefore, we have that N, η1, ∆1 (cid:54)|= x.

Image Classiﬁcation Network for the MNIST Handwritten Image Dataset The
well-known MNIST image dataset contains images of size 28 × 28 and one channel
and the network is trained with the source code given in [5]. The trained network is of
medium size with 600,810 parameters, has an accuracy of more than 99%, and is state-
of-the-art. It has 12 layers, within which there are 2 convolutional layers, as well as
layers such as ReLU, dropout, fully-connected layers and a softmax layer. The images
are preprocessed to make the value of each pixel within the bound [0, 1].

Given an image x, we start with layer k = 1 and the parameter set to at most 150
dimensions (there are 21632 dimensions in layer L1). All ηk, ∆k for k ≥ 2 are computed
according to the simple heuristic mentioned in Section 4.2 and satisfy Deﬁnition 6 and
Deﬁnition 7. For the region η1(αx,1), we allow changes to the activation value of each
selected dimension that are within [-1,1]. The set ∆1 includes manipulations that can
change the activation value for a subset of the 150 dimensions, by incrementing or
decrementing the value for each dimension by 1. The experimental results show that
for most of the examples we can ﬁnd a class change within 100 dimensional changes
in layer L1, by comparing the number of pixels that have changed, and some of them
can have less than 30 dimensional changes. Figure 8 presents examples of such class
changes for layer L1. We also experiment on images with up to 40 dimensional changes
in layer L1; the tool is able to check the entire network, reaching the output layer and
claiming that N, ηk, ∆k |= x for all k ≥ 1. While training of the network takes half an
hour, ﬁnding an adversarial example takes up to several minutes.

Image Classiﬁcation Network for the CIFAR-10 Small Image Dataset We work
with a medium size neural network, trained with the source code from [1] for more than
12 hours on the well-known CIFAR10 dataset. The inputs to the network are images
of size 32 × 32 with three channels. The trained network has 1,250,858 real-valued pa-

8 to 0

2 to 1

4 to 2

2 to 3

9 to 4

6 to 5

4 to 6

9 to 7

0 to 8

7 to 9

Fig. 8. Adversarial examples for a neural network trained on MNIST

rameters and includes convolutional layers, ReLU layers, max-pooling layers, dropout
layers, fully-connected layers, and a softmax layer.

As an illustration of the type of perturbations that we are investigating, consider the
images in Figure 9, which correspond to the parameter setting of up to 25, 45, 65, 85,
105, 125, 145 dimensions, respectively, for layer k = 1. The manipulations change the
activation values of these dimensions. Each image is obtained by mapping back from
the ﬁrst hidden layer and represents a point close to the boundary of the correspond-
ing region. The relation N, η1, ∆1 |= x holds for the ﬁrst 7 images, but fails for the last
one and the image is classiﬁed as a truck. Intuitively, our choice of the region η1(αx,1)
identiﬁes the subset of dimensions with most extreme activations, taking advantage of
the analytical capability of the ﬁrst hidden layer. A higher number of selected dimen-
sions implies a larger region in which we apply manipulations, and, more importantly,
suggests a more dramatic change to the knowledge represented by the activations when
moving to the boundary of the region.

Fig. 9. An illustrative example of mapping back to input layer from the Cifar-10
mataset: the last image classiﬁes as a truck.

We also work with 500 dimensions and otherwise the same experimental parameters
as for MNIST. Figure 13 in Appendix of [20] gives 16 pairs of original images (clas-
siﬁed correctly) and perturbed images (classiﬁed wrongly). We found that, while the
manipulations lead to human-recognisable modiﬁcations to the images, the perturbed
images can be classiﬁed wrongly by the network. For each image, ﬁnding an adversarial
example ranges from seconds to 20 minutes.

Image Classiﬁcation Network for the ImageNet Dataset We also conduct experi-
ments on a large image classiﬁcation network trained on the popular ImageNet dataset.
The images are of size 224 × 224 and have three channels. The network is the model
of the 16-layer network [34], called VGG16, used by the VGG team in the ILSVRC-

2014 competition, downloaded from [7]. The trained network has 138,357,544 real-
valued parameters and includes convolutional layers, ReLU layers, zero-padding lay-
ers, dropout layers, max-pooling layers, fully-connected layers, and a softmax layer.
The experimental parameters are the same as for the previous two experiments, except
that we work with 20,000 dimensions.

Several additional pairs of original and perturbed images are included in Figure 14
in Appendix of [20]. In Figure 10 we also give two examples of street sign images. The
image on the left is reported unsafe for the second layer with 6346 dimensional changes
(0.2% of the 3,211,264 dimensions of layer L2). The one on the right is reported safe
for 20,000 dimensional changes of layer L2. It appears that more complex manipula-
tions, involving more dimensions (perceptrons), are needed in this case to cause a class
change.

Fig. 10. Street sign images. Found an adversarial example for the left image (class
changed into bird house), but cannot ﬁnd an adversarial example for the right image
for 20,000 dimensions.

5.1 The German Traﬃc Sign Recognition Benchmark (GTSRB)

We evaluate DLV on the GTSRB dataset (by resizing images into size 32*32), which
has 43 classes. Figure 11 presents the results for the multi-path search. The ﬁrst case
(approx. 20 minutes to manipulate) is a stop sign (conﬁdence 1.0) changed into a speed
limit of 30 miles, with an L1 distance of 0.045 and L2 distance of 0.19. The conﬁdence
of the manipulated image is 0.79. The second, easy, case (seconds to manipulate) is a
speed limit of 80 miles (conﬁdence 0.999964) changed into a speed limit of 30 miles,
with an L1 distance of 0.004 and L2 distance of 0.06. The conﬁdence of the manipulated
image is 0.99 (a very high conﬁdence of misclassiﬁcation). Also, a “go right” sign can
be easily manipulated into a sign classiﬁed as “go straight”.

Figure 16 in [20] presents additional adversarial examples obtained when selecting

single-path search.

6 Comparison

We compare our approach with two existing approaches for ﬁnding adversarial exam-
ples, i.e., fast gradient sign method (FGSM) [36] and Jacobian saliency map algorithm

“stop”
to “30m speed limit”

“80m speed limit”
to “30m speed limit”

“go right”
to “go straight”

Fig. 11. Adversarial examples for the network trained on the GTSRB dataset by multi-
path search

(JSMA) [28]. FGSM calculates the optimal attack for a linear approximation of the net-
work cost, whereas DLV explores a proportion of dimensions in the feature space in the
input or hidden layers. JSMA ﬁnds a set of dimensions in the input layer to manipulate,
according to the linear approximation (by computing the Jacobian matrix) of the model
from current output to a nominated target output. Intuitively, the diﬀerence between
DLV’s manipulation and JSMA is that DLV manipulates over features discovered in
the activations of the hidden layer, while JSMA manipulates according to the partial
derivatives, which depend on the parameters of the network.

Experiment 1. We randomly select an image from the MNIST dataset. Figure 12
shows some intermediate and ﬁnal images obtained by running the three approaches:
FGSM, JSMA and DLV. FGSM has a single parameter, (cid:15), where a greater (cid:15) represents a
greater perturbation along the gradient of cost function. Given an (cid:15), for each input exam-
ple a perturbed example is returned and we test whether it is an adversarial example by
checking for misclassiﬁcation against the original image. We gradually increase the pa-
rameter (cid:15) = 0.05, 0.1, 0.2, 0.3, 0.4, with the last image (i.e., (cid:15) = 0.4) witnessing a class
change, see the images in the top row of Figure 12. FGSM can eﬃciently manipulate a
set of images, but it requires a relatively large manipulation to ﬁnd a misclassiﬁcation.
For the JSMA approach, we conduct the experiment on a setting with parameters
(cid:15) = 0.1 and θ = 1.0. The parameter (cid:15) = 0.1 means that we only consider adversarial ex-
amples changing no more than 10% of all the pixels, which is suﬃcient here. As stated
in [29], the parameter θ = 1.0, which allows a maximum change to every pixel, can en-
sure that fewer pixels need to be changed. The approach takes a series of manipulations
to gradually lead to a misclassiﬁcation, see the images in the middle row of Figure 12.
The misclassiﬁed image has an L2 (Euclidean) distance of 0.17 and an L1 (Manhattan)
distance of 0.03 from the original image. While JSMA can ﬁnd adversarial examples
with smaller distance from the original image, it takes longer to manipulate a set of
images.

Both FGSM and JSMA follow their speciﬁc heuristics to deterministically explore
the space of images. However, in some cases, the heuristics may omit better adversarial
examples. In the experiment for DLV, instead of giving features a speciﬁc order and
manipulating them sequentially, we allow the program to nondeterministically choose
features. This is currently done by MCTS (Monte Carlo Tree Search), which has a theo-
retical guarantee of convergence for inﬁnite sampling. Therefore, the high-dimensional
space is explored by following many diﬀerent paths. By taking the same manipulation

Fig. 12. FGSM vs. JSMA vs. DLV, where FGSM and JSMA search a single path and
DLV multiple paths. Top row: Original image (7) perturbed deterministically by FGSM
with (cid:15) = 0.05, 0.1, 0.2, 0.3, 0.4, with the ﬁnal image (i.e., (cid:15) = 0.4) misclassiﬁed as
9. Middle row: Original image (7) perturbed deterministically by JSMA with (cid:15) = 0.1
and θ = 1.0. We show even numbered images of the 12 produced by JSMA, with the
ﬁnal image misclassiﬁed as 3. Bottom row: Original image (7) perturbed nondetermin-
istically by DLV, for the same manipulation on a single pixel as that of JSMA (i.e.,
sp ∗ mp = 1.0) and working in the input layer, with the ﬁnal image misclassiﬁed as 3.

on a single pixel as that of JSMA (i.e., sp ∗ mp = 1.0) and working on the input layer,
DLV is able to ﬁnd another perturbed image that is also classiﬁed as 3 but has a smaller
distance (L2 distance is 0.14 and L1 distance is 0.02) from the original image, see the
images in the last row of Figure 12. In terms of the time taken to ﬁnd an adversarial
example, DLV may take longer than JSMA, since it searches over many diﬀerent paths.

L2
L1
%

FGSM ((cid:15) = 0.1)
0.08
0.06
17.5%

(0.4) DLV (dimsl = 75) (150) (450) JSMA (θ = 0.1) (0.4)
(0.2)
0.11
0.22 0.27
0.32
0.15
0.02
0.06 0.09
0.12
0.25
99%
79% 98%
70.9% 97.2%

0.19
0.04
52.3%
Table 1. FGSM vs. DLV (on a single path) vs. JSMA

0.11
0.02
92%

Experiment 2. Table 1 gives a comparison of robustness evaluation of the three
appraoches on the MNIST dataset. For FGSM, we vary the input parameter (cid:15) accord-
ing to the values {0.1, 0.2, 0.4}. For DLV, we select regions as deﬁned in Section 4.4
on a single path (by deﬁning a speciﬁc order on the features and manipulating them
sequentially) for the ﬁrst hidden layer. The experiment is parameterised by varying the
maximal number of dimensions to be changed, i.e., dimsl ∈ {75, 150, 450}. For each
input image, an adversarial example is returned, if found, by manipulating fewer than
the maximal number of dimensions. When the maximal number has been reached, DLV
will report failure and return the last perturbed example. For JSMA, the experiment is
conducted by letting θ take the value in the set {0.1, 0.4} and setting (cid:15) to 1.0.

We collect three statistics, i.e., the average L1 distance over the adversarial exam-
ples, the average L2 distance over the adversarial examples, and the success rate of

ﬁnding adversary examples. Let Ld(x, δ(x)) for d ∈ {1, 2} be the distance between an
input x and the returned perturbed image δ(x), and diﬀ(x, δ(x)) ∈ {0, 1} be a Boolean
value representing whether x and δ(x) have diﬀerent classes. We let

and

(cid:80)

Ld =

x in test set diﬀ(x, δ(x)) × Ld(x, δ(x))
x in test set diﬀ(x, δ(x))

(cid:80)

% =

(cid:80)

x in test set diﬀ(x, δ(x))
the number of examples in test set

We note that the approaches yield diﬀerent perturbed examples δ(x).

The test set size is 500 images selected randomly. DLV takes 1-2 minutes to manip-
ulate each input image in MNIST. JSMA takes about 10 minutes for each image, but it
works for 10 classes, so the running time is similar to that of DLV. FGSM works with a
set of images, so it is the fastest per image.

For the case when the success rates are very high, i.e., 97.2% for FGSM with (cid:15) =
0.4, 98% for DLV with dimsl = 450, and 99% for JSMA with θ = 0.4, JSMA has the
smallest average distances, followed by DLV, which has smaller average distances than
FGSM on both L1 and L2 distances.

We mention that a smaller distance leading to a misclassiﬁcation may result in a
lower rate of transferability [29], meaning that a misclassiﬁcation can be harder to wit-
ness on another model trained on the same (or a small subset of) data-set.

7 Related Work

AI safety is recognised an an important problem, see e.g., [33,10]. An early veriﬁcation
approach for neural networks was proposed in [30], where, using the notation of this
paper, safety is deﬁned as the existence, for all inputs in a region η0 ∈ DL0 , of a corre-
sponding output in another region ηn ⊆ DLn . They encode the entire network as a set of
constraints, approximating the sigmoid using constraints, which can then be solved by a
SAT solver, but their approach only works with 6 neurons (3 hidden neurons). A similar
idea is presented in [32]. In contrast, we work layer by layer and obtain much greater
scalability. Since the ﬁrst version of this paper appeared [20], another constraint-based
method has been proposed in [21] which improves on [30]. While they consider more
general correctness properties than this paper, they can only handle the ReLU activation
functions, by extending the Simplex method to work with the piecewise linear ReLU
functions that cannot be expressed using linear programming. This necessitates a search
tree (instead of a search path as in Simplex), for which a heuristic search is proposed
and shown to be complete. The approach is demonstrated on networks with 300 ReLU
nodes, but as it encodes the full network it is unclear whether it can be scaled to work
with practical deep neural networks: for example, the MNIST network has 630,016
ReLU nodes. They also handle continuous spaces directly without discretisation, the
beneﬁts of which are not yet clear, since it is argued in [19] that linear behaviour in
high-dimensional spaces is suﬃcient to cause adversarial examples.

Concerns about the instability of neural networks to adversarial examples were ﬁrst
raised in [13,36], where optimisation is used to identify misclassiﬁcations. A method

for computing the perturbations is also proposed, which is based on box-constrained
optimisation and is approximate in view of non-convexity of the search space. This
work is followed by [19], which introduced the much faster FGSM method, and [22],
which employed a compromise between the two (iterative, but with a smaller number
of iterations than [36]). In our notation, [19] uses a deterministic, iterative manipula-
tion δ(x) = x + (cid:15) sign((cid:79)x J(x, αx,n)), where x is an image in matrix representation, (cid:15) is a
hyper-parameter that can be tuned to get diﬀerent manipulated images, and J(x, αx,n) is
the cross-entropy cost function of the neural network on input x and class αx,n. There-
fore, their approach will test a set of discrete points in the region η0(αx,0) of the input
layer. Therefore these manipulations will test a lasso-type ladder tree (i.e., a ladder tree
without branches) L(ηk(αx,k)), which does not satisfy the covering property. In [26],
instead of working with a single image, an evolutionary algorithm is employed for a
population of images. For each individual image in the current population, the manip-
ulation is the mutation and/or crossover. While mutations can be nondeterministic, the
manipulations of an individual image are also following a lasso-type ladder tree which
is not covering. We also mention that [38] uses several distortions such as JPEG com-
pression, thumbnail resizing, random cropping, etc, to test the robustness of the trained
network. These distortions can be understood as manipulations. All these attacks do not
leverage any speciﬁc properties of the model family, and do not guarantee that they will
ﬁnd a misclassiﬁed image in the constraint region, even if such an image exists.

The notion of robustness studied in [18] has some similarities to our deﬁnition of
safety, except that the authors work with values averaged over the input distribution µ,
which is diﬃcult to estimate accurately in high dimensions. As in [36,22], they use opti-
misation without convergence guarantees, as a result computing only an approximation
to the minimal perturbation. In [12] pointwise robustness is adopted, which corresponds
to our general safety; they also use a constraint solver but represent the full constraint
system by reduction to a convex LP problem, and only verify an approximation of the
property. In contrast, we work directly with activations rather than an encoding of ac-
tivation functions, and our method exhaustively searches through the complete ladder
tree for an adversarial example by iterative and nondeterministic application of manip-
ulations. Further, our deﬁnition of a manipulation is more ﬂexible, since it allows us to
select a subset of dimensions, and each such subset can have a diﬀerent region diameter
computed with respect to a diﬀerent norm.

8 Conclusions

This paper presents an automated veriﬁcation framework for checking safety of deep
neural networks that is based on a systematic exploration of a region around a data point
to search for adversarial manipulations of a given type, and propagating the analysis into
deeper layers. Though we focus on the classiﬁcation task, the approach also generalises
to other types of networks. We have implemented the approach using SMT and vali-
dated it on several state-of-the-art neural network classiﬁers for realistic images. The
results are encouraging, with adversarial examples found in some cases in a matter of
seconds when working with few dimensions, but the veriﬁcation process itself is expo-
nential in the number of features and has prohibitive complexity for larger images. The

performance and scalability of our method can be signiﬁcantly improved through par-
allelisation. It would be interesting to see if the notions of regularity suggested in [24]
permit a symbolic approach, and whether an abstraction reﬁnement framework can be
formulated to improve the scalability and computational performance.

Acknowledgements. This paper has greatly beneﬁted from discussions with sev-
eral researchers. We are particularly grateful to Martin Fraenzle, Ian Goodfellow and
Nicolas Papernot.

References

1. CIFAR10 model for Keras. https://github.com/fchollet/keras/blob/master/examples/cifar10 cnn.py.
2. DLV. https://github.com/verideep/dlv.
3. Keras. https://keras.io.
4. Large scale visual recognition challenge. http://www.image-net.org/challenges/LSVRC/.
5. MNIST CNN network. https://github.com/fchollet/keras/blob/master/examples/mnist cnn.py.
6. Theano. http://deeplearning.net/software/theano/.
7. VGG16 model for Keras. https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3.
8. Z3. http://rise4fun.com/z3.
9. Luigi Ambrosio, Nicola Fusco, and Diego Pallara. Functions of bounded variation and free
discontinuity problems. Oxford Mathematical Monographs. Oxford University Press, 2000.
10. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dario
Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e.
Concrete problems in AI safety. CoRR, abs/1606.06565, 2016.

11. Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso
Poggio. Unsupervised learning of invariant representations. Theoretical Computer Science,
633:112–121, 2016.

12. Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori,
CoRR,

and Antonio Criminisi. Measuring neural net robustness with constraints.
abs/1605.07262, 2016. To appear in NIPS.

13. Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov,
Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In
ECML/PKDD 2013, pages 387–402, 2013.

14. Christopher M Bishop. Neural networks for pattern recognition. Oxford university press,

1995.

15. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp,
Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang,
Jake Zhao, and Karol Zieba. End to end learning for self-driving cars. arXiv:1604.07316,
2016.

16. Gunnar E. Carlsson, Tigran Ishkhanov, Vin de Silva, and Afra Zomorodian. On the local
behavior of spaces of natural images. International Journal of Computer Vision, 76(1), 2008.
17. Lisa Anne Hendricks Dong Huk Park, Zeynep Akata, Bernt Schiele, Trevor Darrell, and
Marcus Rohrbach. Attentive explanations: Justifying decisions and pointing to the evidence.
arxiv.org/abs/1612.04757, 2016.

18. Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classiﬁers’ robustness to

adversarial perturbations. CoRR, abs/1502.02590, 2015.

19. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing ad-

versarial examples. CoRR, abs/1412.6572, 2014.

20. Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety veriﬁcation of deep

neural networks. https://arxiv.org/abs/1610.06940, 2016.

21. Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An
eﬃcient SMT solver for verifying deep neural networks. In CAV 2017, 2017. To appear.
22. Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical

world. arXiv:1607.02533, 2016.

23. Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, 521:436–444,

24. St´ephane Mallat. Understanding deep convolutional networks. Philosohical Transactions of

2015.

the Royal Society A, 2016.

25. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple

and accurate method to fool deep neural networks. CoRR, abs/1511.04599, 2015.

26. Anh Nguyen, Jason Yosinski, and Jeﬀ Clune. Deep neural networks are easily fooled: High
conﬁdence predictions for unrecognizable images. In Computer Vision and Pattern Recog-
nition (CVPR ’15), 2015.

27. Nicolas Papernot, Ian Goodfellow, Ryan Sheatsley, Reuben Feinman, and Patrick McDaniel.
cleverhans v1.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768,
2016.

28. Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and
Ananthram Swami. The limitations of deep learning in adversarial settings. In Proceedings
of the 1st IEEE European Symposium on Security and Privacy, 2015.

29. Nicolas Papernot, Patrick Drew McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik,
and Ananthram Swami. Practical black-box attacks against deep learning systems using
adversarial examples. CoRR, abs/1602.02697, 2016.

30. Luca Pulina and Armando Tacchella. An abstraction-reﬁnement approach to veriﬁcation of

artiﬁcial neural networks. In CAV 2010, pages 243–257, 2010.

31. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust you?”: Ex-
In ACM SIGKDD International Conference on

plaining the predictions of any classiﬁer.
Knowledge Discovery and Data Mining (KDD2016), 2016.

32. Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker. Towards veriﬁcation
of artiﬁcial neural networks. In 18th Workshop on Methoden und Beschreibungssprachen
zur Modellierung und Veriﬁkation von Schaltungen und Systemen” (MBMV), pages 30–40,
2015.

33. Sanjit A. Seshia and Dorsa Sadigh.

Towards veriﬁed artiﬁcial intelligence. CoRR,

abs/1606.08514, 2016.

34. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv:1409.1556, 2014.

35. J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking
machine learning algorithms for traﬃc sign recognition. Neural Networks, 32:323–332,
2012.

36. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
In International

Intriguing properties of neural networks.

Goodfellow, and Rob Fergus.
Conference on Learning Representations (ICLR-2014), 2014.

37. Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in Neural
Information Processing Systems 4, [NIPS Conference, Denver, Colorado, USA, December
2-5, 1991], pages 831–838, 1991.

38. Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness

of deep neural networks via stability training. In CVPR 2016, 2016.

A Input Parameters and Experimental Setup

The DLV tool accepts as input a network N and an image x, and has the following input
parameters:

– an integer l ∈ [0, n] indicating the starting layer Ll,
– an integer dimsl ≥ 1 indicating the maximal number of dimensions that need to be

considered in layer Ll,

– the values of variables sp and mp in Vl; for simplicity, we ask that, for all dimensions
p that will be selected by the automated procedure, sp and mp have the same values,

– the precision ε ∈ [0, ∞),
– an integer dimsk, f indicating the number of dimensions for each feature; for sim-
plicity, we ask that every feature has the same number of dimensions and dimsk, f =
dimsk(cid:48), f for all layers k and k(cid:48), and

– type of search: either heuristic (single-path) or Monte Carlo Tree Search (MCTS)

(multi-path).

A.1 Two-Dimensional Point Classiﬁcation Network

A.2 Network for the MNIST Dataset

– l = 0
– dimsl = 2,
– sp = 1.0 and mp = 1.0,
– ε = 0.1, and
– dimsk, f = 2

– l = 1
– dimsl = 150,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

– l = 1
– dimsl = 500,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

– l = 1
– dimsl = 1000,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

A.3 Network for the CIFAR-10 Dataset

A.4 Network for the GTSRB Dataset

A.5 Network for the ImageNet Dataset

– l = 2
– dimsl = 20, 000,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

B Additional Adversarial Examples Found for the CIFAR-10,

ImageNet, and MNIST Networks

Figure 13 and Figure 14 present additional adversarial examples for the CIFAR-10 and
ImageNet networks by single-path search. Figure 15 presents adversarial examples for
the MNIST network by multi-path search.

automobile to bird

automobile to frog

automobile to airplane automobile to horse

airplane to dog

airplane to deer

airplane to truck

airplane to cat

truck to frog

truck to cat

ship to bird

ship to airplane

ship to truck

horse to cat

horse to automobile

horse to truck

Fig. 13. Adversarial examples for a neural network trained on the CIFAR-10 dataset by
single-path search

C Additional Adversarial Examples for the German Traﬃc Sign

Recognition Benchmark (GTSRB)

Figure 16 presents adversarial examples obtained when selecting single-path search.

labrador to life boat

rhodesian ridgeback to malinois

boxer to rhodesian ridgeback

great pyrenees to kuvasz

Fig. 14. Adversarial Examples for the VGG16 Network Trained on the imageNet
Dataset By Single-Path Search

9 to 4

8 to 3

5 to 3

4 to 9

5 to 3

7 to 3

9 to 4

9 to 4

2 to 3

1 to 8

8 to 5

0 to 3

7 to 2

8 to 3

3 to 2

9 to 7

3 to 2

4 to 9

6 to 4

3 to 5

9 to 4

0 to 2

2 to 3

9 to 8

4 to 2

Fig. 15. Adversarial examples for the network trained on the MNIST dataset by multi-
path search

speed limit 50 (pro-
hibitory) to speed limit
80 (prohibitory)

restriction ends (other)
to restriction ends (80)

no overtaking (trucks)
(prohibitory)
to speed
limit 80 (prohibitory)

give way (other)
priority road (other)

to

priority road (other) to
speed limit 30 (pro-
hibitory)

speed limit 70 (pro-
hibitory) to speed limit
120 (prohibitory)

(pro-
overtaking
no
hibitory) to go straight
(mandatory)

speed limit 50 (pro-
hibitory) to stop (other)

road narrows (danger)
to construction (danger)

ends

restriction
80
(other) to speed limit 80
(prohibitory)

no overtaking (trucks)
(prohibitory)
to speed
limit 80 (prohibitory)

overtaking

no
hibitory)
ends
(trucks)) (other)

(pro-
to restriction
(overtaking

priority at next intersec-
tion (danger) to speed
limit 30 (prohibitory)

uneven road (danger) to
traﬃc signal (danger)

danger
to
school crossing (danger)

(danger)

Fig. 16. Adversarial examples for the GTSRB dataset by single-path search

D Architectures of Neural Networks

Figure 17, Figure 18, Figure 19, and Figure 20 present architectures of the networks we
work with in this paper. The network for the ImageNet dataset is from [34].

Fig. 17. Architecture of the neural network for two-dimensional point classiﬁcation

Fig. 18. Architecture of the neural network for the MNIST dataset

Fig. 19. Architecture of the neural network for the CIFAR-10 dataset

Fig. 20. Architecture of the neural network for the GTSRB dataset

7
1
0
2
 
y
a
M
 
5
 
 
]
I

A
.
s
c
[
 
 
3
v
0
4
9
6
0
.
0
1
6
1
:
v
i
X
r
a

Safety Veriﬁcation of Deep Neural Networks(cid:63)

Xiaowei Huang, Marta Kwiatkowska, Sen Wang and Min Wu

Department of Computer Science, University of Oxford

Abstract. Deep neural networks have achieved impressive experimental results
in image classiﬁcation, but can surprisingly be unstable with respect to adversar-
ial perturbations, that is, minimal changes to the input image that cause the net-
work to misclassify it. With potential applications including perception modules
and end-to-end controllers for self-driving cars, this raises concerns about their
safety. We develop a novel automated veriﬁcation framework for feed-forward
multi-layer neural networks based on Satisﬁability Modulo Theory (SMT). We
focus on safety of image classiﬁcation decisions with respect to image manipu-
lations, such as scratches or changes to camera angle or lighting conditions that
would result in the same class being assigned by a human, and deﬁne safety
for an individual decision in terms of invariance of the classiﬁcation within a
small neighbourhood of the original image. We enable exhaustive search of the
region by employing discretisation, and propagate the analysis layer by layer. Our
method works directly with the network code and, in contrast to existing meth-
ods, can guarantee that adversarial examples, if they exist, are found for the given
region and family of manipulations. If found, adversarial examples can be shown
to human testers and/or used to ﬁne-tune the network. We implement the tech-
niques using Z3 and evaluate them on state-of-the-art networks, including regu-
larised and deep learning networks. We also compare against existing techniques
to search for adversarial examples and estimate network robustness.

1

Introduction

Deep neural networks have achieved impressive experimental results in image classiﬁ-
cation, matching the cognitive ability of humans [23] in complex tasks with thousands
of classes. Many applications are envisaged, including their use as perception modules
and end-to-end controllers for self-driving cars [15]. Let Rn be a vector space of images
(points) that we wish to classify and assume that f : Rn → C, where C is a (ﬁnite) set of
class labels, models the human perception capability, then a neural network classiﬁer is
a function ˆf (x) which approximates f (x) from M training examples {(xi, ci)}i=1,..,M. For
example, a perception module of a self-driving car may input an image from a camera
and must correctly classify the type of object in its view, irrespective of aspects such
as the angle of its vision and image imperfections. Therefore, though they clearly in-
clude imperfections, all four pairs of images in Figure 1 should arguably be classiﬁed
as automobiles, since they appear so to a human eye.

(cid:63) This work is

supported by the EPSRC Programme Grant on Mobile Autonomy
(EP/M019918/1). Part of this work was done while MK was visiting the Simons Institute for
the Theory of Computing.

Classiﬁers employed in vision tasks are typically multi-layer networks, which prop-
agate the input image through a series of linear and non-linear operators. They are
high-dimensional, often with millions of dimensions, non-linear and potentially dis-
continuous: even a small network, such as that trained to classify hand-written images
of digits 0-9, has over 60,000 real-valued parameters and 21,632 neurons (dimensions)
in its ﬁrst layer. At the same time, the networks are trained on a ﬁnite data set and
expected to generalise to previously unseen images. To increase the probability of cor-
rectly classifying such an image, regularisation techniques such as dropout are typically
used, which improves the smoothness of the classiﬁers, in the sense that images that are
close (within (cid:15) distance) to a training point are assigned the same class label.

automobile to bird

automobile to frog

automobile to airplane automobile to horse

Fig. 1. Automobile images (classiﬁed correctly) and their perturbed images (classiﬁed
wrongly)

Unfortunately, it has been observed in [13,36] that deep neural networks, includ-
ing highly trained and smooth networks optimised for vision tasks, are unstable with
respect to so called adversarial perturbations. Such adversarial perturbations are (min-
imal) changes to the input image, often imperceptible to the human eye, that cause the
network to misclassify the image. Examples include not only artiﬁcially generated ran-
dom perturbations, but also (more worryingly) modiﬁcations of camera images [22] that
correspond to resizing, cropping or change in lighting conditions. They can be devised
without access to the training set [29] and are transferable [19], in the sense that an ex-
ample misclassiﬁed by one network is also misclassiﬁed by a network with a diﬀerent
architecture, even if it is trained on diﬀerent data. Figure 1 gives adversarial pertur-
bations of automobile images that are misclassiﬁed as a bird, frog, airplane or horse
by a highly trained state-of-the-art network. This obviously raises potential safety con-
cerns for applications such as autonomous driving and calls for automated veriﬁcation
techniques that can verify the correctness of their decisions.

Safety of AI systems is receiving increasing attention, to mention [33,10], in view
of their potential to cause harm in safety-critical situations such as autonomous driving.
Typically, decision making in such systems is either solely based on machine learning,
through end-to-end controllers, or involves some combination of logic-based reasoning
and machine learning components, where an image classiﬁer produces a classiﬁcation,
say speed limit or a stop sign, that serves as input to a controller. A recent trend towards
“explainable AI” has led to approaches that learn not only how to assign the classiﬁca-
tion labels, but also additional explanations of the model, which can take the form of
a justiﬁcation explanation (why this decision has been reached, for example identify-
ing the features that supported the decision) [17,31]. In all these cases, the safety of a
decision can be reduced to ensuring the correct behaviour of a machine learning com-

ponent. However, safety assurance and veriﬁcation methodologies for machine learning
are little studied.

The main diﬃculty with image classiﬁcation tasks, which play a critical role in per-
ception modules of autonomous driving controllers, is that they do not have a formal
speciﬁcation in the usual sense: ideally, the performance of a classiﬁer should match
the perception ability and class labels assigned by a human. Traditionally, the correct-
ness of a neural network classiﬁer is expressed in terms of risk [37], deﬁned as the
probability of misclassiﬁcation of a given image, weighted with respect to the input
distribution µ of images. Similar (statistical) robustness properties of deep neural net-
work classiﬁers, which compute the average minimum distance to a misclassiﬁcation
and are independent of the data point, have been studied and can be estimated using
tools such as DeepFool [25] and cleverhans [27]. However, we are interested in the
safety of an individual decision, and to this end focus on the key property of the clas-
siﬁer being invariant to perturbations at a given point. This notion is also known as
pointwise robustness [18,12] or local adversarial robustness [21].

Contributions. In this paper we propose a general framework for automated veriﬁ-
cation of safety of classiﬁcation decisions made by feed-forward deep neural networks.
Although we work concretely with image classiﬁers, the techniques can be generalised
to other settings. For a given image x (a point in a vector space), we assume that there
is a (possibly inﬁnite) region η around that point that incontrovertibly supports the de-
cision, in the sense that all points in this region must have the same class. This region is
speciﬁed by the user and can be given as a small diameter, or the set of all points whose
salient features are of the same type. We next assume that there is a family of operations
∆, which we call manipulations, that specify modiﬁcations to the image under which the
classiﬁcation decision should remain invariant in the region η. Such manipulations can
represent, for example, camera imprecisions, change of camera angle, or replacement of
a feature. We deﬁne a network decision to be safe for input x and region η with respect
to the set of manipulations ∆ if applying the manipulations on x will not result in a class
change for η. We employ discretisation to enable a ﬁnite exhaustive search of the high-
dimensional region η for adversarial misclassiﬁcations. The discretisation approach is
justiﬁed in the case of image classiﬁers since they are typically represented as vectors of
discrete pixels (vectors of 8 bit RGB colours). To achieve scalability, we propagate the
analysis layer by layer, mapping the region and manipulations to the deeper layers. We
show that this propagation is sound, and is complete under the additional assumption of
minimality of manipulations, which holds in discretised settings. In contrast to existing
approaches [36,28], our framework can guarantee that a misclassiﬁcation is found if it
exists. Since we reduce veriﬁcation to a search for adversarial examples, we can achieve
safety veriﬁcation (if no misclassiﬁcations are found for all layers) or falsiﬁcation (in
which case the adversarial examples can be used to ﬁne-tune the network or shown to a
human tester).

We implement the techniques using Z3 [8] in a tool called DLV (Deep Learning Ver-
iﬁcation) [2] and evaluate them on state-of-the-art networks, including regularised and
deep learning networks. This includes image classiﬁcation networks trained for clas-
sifying hand-written images of digits 0-9 (MNIST), 10 classes of small colour images
(CIFAR10), 43 classes of the German Traﬃc Sign Recognition Benchmark (GTSRB)

[35] and 1000 classes of colour images used for the well-known imageNet large-scale
visual recognition challenge (ILSVRC) [4]. We also perform a comparison of the DLV
falsiﬁcation functionality on the MNIST dataset against the methods of [36] and [28],
focusing on the search strategies and statistical robustness estimation. The perturbed
images in Figure 1 are found automatically using our tool for the network trained on
the CIFAR10 dataset.

This invited paper is an extended and improved version of [20], where an extended

version including appendices can also be found.

2 Background on Neural Networks

We consider feed-forward multi-layer neural networks [14], henceforth abbreviated as
neural networks. Perceptrons (neurons) in a neural network are arranged in disjoint
layers, with each perceptron in one layer connected to the next layer, but no connection
between perceptrons in the same layer. Each layer Lk of a network is associated with
an nk-dimensional vector space DLk ⊆ Rnk , in which each dimension corresponds to
a perceptron. We write Pk for the set of perceptrons in layer Lk and nk = |Pk| is the
number of perceptrons (dimensions) in layer Lk.

Formally, a (feed-forward and deep) neural network N is a tuple (L, T, Φ), where
L = {Lk | k ∈ {0, ..., n}} is a set of layers such that layer L0 is the input layer and Ln
is the output layer, T ⊆ L × L is a set of sequential connections between layers such
that, except for the input and output layers, each layer has an incoming connection and
an outgoing connection, and Φ = {φk | k ∈ {1, ..., n}} is a set of activation functions
φk : DLk−1 → DLk , one for each non-input layer. Layers other than input and output
layers are called the hidden layers.

The network is fed an input x (point in DL0 ) through its input layer, which is then
propagated through the layers by successive application of the activation functions. An
activation for point x in layer k is the value of the corresponding function, denoted
αx,k = φk(φk−1(...φ1(x))) ∈ DLk , where αx,0 = x. For perceptron p ∈ Pk we write
αx,k(p) for the value of its activation on input x. For every activation αx,k and layer
k(cid:48) < k, we deﬁne Prek(cid:48) (αx,k) = {αy,k(cid:48) ∈ DLk(cid:48)
| αy,k = αx,k} to be the set of activations in
layer k(cid:48) whose corresponding activation in layer Lk is αx,k. The classiﬁcation decision
is made based on the activations in the output layer by, e.g., assigning to x the class
arg maxp∈Pn αx,n(p). For simplicity, we use αx,n to denote the class assigned to input x,
and thus αx,n = αy,n expresses that two inputs x and y have the same class.

The neural network classiﬁer N represents a function ˆf (x) which approximates
f (x) : DL0 → C, a function that models the human perception capability in labelling im-
ages with labels from C, from M training examples {(xi, ci)}i=1,..,M. Image classiﬁcation
networks, for example convolutional networks, may contain many layers, which can
be non-linear, and work in high dimensions, which for the image classiﬁcation prob-
lems can be of the order of millions. Digital images are represented as 3D tensors of
pixels (width, height and depth, the latter to represent colour), where each pixel is a dis-
crete value in the range 0..255. The training process determines real values for weights
used as ﬁlters that are convolved with the activation functions. Since it is diﬃcult to
approximate f with few samples in the sparsely populated high-dimensional space, to

increase the probability of classifying correctly a previously unseen image, various reg-
ularisation techniques such as dropout are employed. They improve the smoothness of
the classiﬁer, in the sense that points that are (cid:15)-close to a training point (potentially
inﬁnitely many of them) classify the same.

In this paper, we work with the code of the network and its trained weights.

3 Safety Analysis of Classiﬁcation Decisions

In this section we deﬁne our notion of safety of classiﬁcation decisions for a neural net-
work, based on the concept of a manipulation of an image, essentially perturbations that
a human observer would classify the same as the original image. Safety is deﬁned for
an individual classiﬁcation decision and is parameterised by the class of manipulations
and a neighbouring region around a given image. To ensure ﬁniteness of the search of
the region for adversarial misclassiﬁcations, we introduce so called “ladders”, nonde-
terministically branching and iterated application of successive manipulations, and state
the conditions under which the search is exhaustive.

Safety and Robustness Our method assumes the existence of a (possibly inﬁnite)
region η around a data point (image) x such that all points in the region are indistin-
guishable by a human, and therefore have the same true class. This region is understood
as supporting the classiﬁcation decision and can usually be inferred from the type of
the classiﬁcation problem. For simplicity, we identify such a region via its diameter d
with respect to some user-speciﬁed norm, which intuitively measures the closeness to
the point x. As deﬁned in [18], a network ˆf approximating human capability f is said
to be not robust at x if there exists a point y in the region η = {z ∈ DL0 | ||z − x|| ≤ d}
of the input layer such that ˆf (x) (cid:44) ˆf (y). The point y, at a minimal distance from x, is
known as an adversarial example. Our deﬁnition of safety for a classiﬁcation decision
(abbreviated safety at a point) follows he same intuition, except that we work layer
by layer, and therefore will identify such a region ηk, a subspace of DLk , at each layer
Lk, for k ∈ {0, ..., n}, and successively reﬁne the regions through the deeper layers. We
justify this choice based on the observation [11,23,24] that deep neural networks are
thought to compute progressively more powerful invariants as the depth increases. In
other words, they gradually transform images into a representation in which the classes
are separable by a linear classiﬁer.

Assumption 1 For each activation αx,k of point x in layer Lk, the region ηk(αx,k) con-
tains activations that the human observer believes to be so close to αx,k that they should
be classiﬁed the same as x.

Intuitively, safety for network N at a point x means that the classiﬁcation decision is
robust at x against perturbations within the region ηk(αx,k). Note that, while the pertur-
bation is applied in layer Lk, the classiﬁcation decision is based on the activation in the
output layer Ln.

Deﬁnition 1. [General Safety] Let ηk(αx,k) be a region in layer Lk of a neural network
N such that αx,k ∈ ηk(αx,k). We say that N is safe for input x and region ηk(αx,k), written
as N, ηk |= x, if for all activations αy,k in ηk(αx,k) we have αy,n = αx,n.

We remark that, unlike the notions of risk [37] and robustness of [18,12], we work
with safety for a speciﬁc point and do not account for the input distribution, but such
expectation measures can be considered, see Section 6 for comparison.

Manipulations A key concept of our framework is the notion of a manipulation, an
operator that intuitively models image perturbations, for example bad angles, scratches
or weather conditions, the idea being that the classiﬁcation decisions in a region of im-
ages close to it should be invariant under such manipulations. The choice of the type of
manipulation is dependent on the application and user-deﬁned, reﬂecting knowledge of
the classiﬁcation problem to model perturbations that should or should not be allowed.
Judicious choice of families of such manipulations and appropriate distance metrics is
particularly important. For simplicity, we work with operators δk : DLk → DLk over the
activations in the vector space of layer k, and consider the Euclidean (L2) and Manhattan
(L1) norms to measure the distance between an image and its perturbation through δk,
but the techniques generalise to other norms discussed in [18,19,12]. More speciﬁcally,
applying a manipulation δk(αx,k) to an activation αx,k will result in another activation
such that the values of some or all dimensions are changed. We therefore represent a
manipulation as a hyper-rectangle, deﬁned for two activations αx,k and αy,k of layer Lk
by rec(αx,k, αy,k) = ×p∈Pk [min(αx,k(p), αy,k(p)), max(αx,k(p), αy,k(p))]. The main chal-
lenge for veriﬁcation is the fact that the region ηk contains potentially an uncountable
number of activations. Our approach relies on discretisation in order to enable a ﬁnite
exploration of the region to discover and/or rule out adversarial perturbations.

For an activation αx,k and a set ∆ of manipulations, we denote by rec(∆, αx,k) the
polyhedron which includes all hyper-rectangles that result from applying some manip-
ulation in ∆ on αx,k, i.e., rec(∆, αx,k) = (cid:83)
δ∈∆ rec(αx,k, δ(αx,k)). Let ∆k be the set of all
possible manipulations for layer Lk. To ensure region coverage, we deﬁne valid manip-
ulation as follows.

Deﬁnition 2. Given an activation αx,k, a set of manipulations V(αx,k) ⊆ ∆k is valid if
αx,k is an interior point of rec(V(αx,k), αx,k), i.e., αx,k is in rec(V(αx,k), αx,k) and does
not belong to the boundary of rec(V(αx,k), αx,k).

Figure 2 presents an example of valid manipulations in two-dimensional space: each
arrow represents a manipulation, each dashed box represents a (hyper-)rectangle of the
corresponding manipulation, and activation αx,k is an interior point of the space from
the dashed boxes.

Since we work with discretised spaces, which is a reasonable assumption for im-
ages, we introduce the notion of a minimal manipulation. If applying a minimal manip-
ulation, it suﬃces to check for misclassiﬁcation just at the end points, that is, αx,k and
δk(αx,k). This allows an exhaustive, albeit impractical, exploration of the region in unit
steps.

A manipulation δ1

k(αx,k), written as δ1

k(αy,k) is ﬁner than δ2

k(αx,k), if any
activation in the hyper-rectangle of the former is also in the hyper-rectangle of the latter.
It is implied in this deﬁnition that αy,k is an activation in the hyper-rectangle of δ2
k(αx,k).
Moreover, we write δk,k(cid:48) (αx,k) for φk(cid:48) (...φk+1(δk(αx,k))), representing the corresponding
activation in layer k(cid:48) ≥ k after applying manipulation δk on the activation αx,k, where
δk,k(αx,k) = δk(αx,k).

k(αy,k) ≤ δ2

Fig. 2. Example of a set {δ1, δ2, δ3, δ4} of valid manipulations in a 2-dimensional space

Deﬁnition 3. A manipulation δk on an activation αx,k is minimal if there does not exist
k(αx,k) ≤ δk(αx,k), αy,k =
k and an activation αy,k such that δ1
manipulations δ1
k(αy,k), and αy,n (cid:44) αx,n and αy,n (cid:44) δk,n(αx,k).
k(αx,k), δk(αx,k) = δ2
δ1

k and δ2

Intuitively, a minimal manipulation does not have a ﬁner manipulation that results in
a diﬀerent classiﬁcation. However, it is possible to have diﬀerent classiﬁcations before
and after applying the minimal manipulation, i.e., it is possible that δk,n(αx,k) (cid:44) αx,n. It
is not hard to see that the minimality of a manipulation implies that the class change in
its associated hyper-rectangle can be detected by checking the class of the end points
αx,k and δk(αx,k).

Bounded Variation Recall that we apply manipulations in layer Lk, but check the
classiﬁcation decisions in the output layer. To ensure ﬁnite, exhaustive coverage of
the region, we introduce a continuity assumption on the mapping from space DLk to
the output space DLn , adapted from the concept of bounded variation [9]. Given an
activation αx,k with its associated region ηk(αx,k), we deﬁne a “ladder” on ηk(αx,k) to
be a set ld of activations containing αx,k and ﬁnitely many, possibly zero, activations
from ηk(αx,k). The activations in a ladder can be arranged into an increasing order
αx,k = αx0,k < αx1,k < ... < αx j,k such that every activation αxt,k ∈ ld appears once and
has a successor αxt+1,k such that αxt+1,k = δk(αxt,k) for some manipulation δk ∈ V(αxt,k).
For the greatest element αx j,k, its successor should be outside the region ηk(αx,k), i.e.,
αx j+1,k (cid:60) ηk(αx,k). Given a ladder ld, we write ld(t) for its t + 1-th activation, ld[0..t] for
the preﬁx of ld up to the t + 1-th activation, and last(ld) for the greatest element of ld.
Figure 3 gives a diagrammatic explanation on the ladders.

Deﬁnition 4. Let L(ηk(αx,k)) be the set of ladders in ηk(αx,k). Then the total variation
of the region ηk(αx,k) on the neural network with respect to L(ηk(αx,k)) is

V(N; ηk(αx,k)) =

sup
ld∈L(ηk(αx,k))

(cid:88)

diﬀn(αxt,n, αxt+1,n)

αxt ,k∈ld\{last(ld)}

where diﬀn : DLn × DLn → {0, 1} is given by diﬀn(αx,n, αy,n) = 0 if αx,n = αy,n and 1
otherwise. We say that the region ηk(αx,k) is a bounded variation if V(N; ηk(αx,k)) < ∞,
and are particularly interested in the case when V(N; rk(αy,k)) = 0, which is called a
0-variation.

Fig. 3. Examples of ladders in region ηk(αx,k). Starting from αx,k = αx0,k, the activations
αx1,k...αx j,k form a ladder such that each consecutive activation results from some valid
manipulation δk applied to a previous activation, and the ﬁnal activation αx j,k is outside
the region ηk(αx,k).

The set L(ηk(αx,k)) is complete if, for any ladder ld ∈ L(ηk(αx,k)) of j+1 activations,
any element ld(t) for 0 ≤ t ≤ j, and any manipulation δk ∈ V(ld(t)), there exists a ladder
ld(cid:48) ∈ L(ηk(αx,k)) such that ld(cid:48)[0..t] = ld[0..t] and ld(cid:48)(t + 1) = δk(ld(t)). Intuitively, a
complete ladder is a complete tree, on which each node represents an activation and
each branch of a node corresponds to a valid manipulation. From the root αx,k, every
path of the tree leading to a leaf is a ladder. Moreover, the set L(ηk(αx,k)) is covering if
the polyhedra of all activations in it cover the region ηk(αx,k), i.e.,
(cid:91)

(cid:91)

ηk(αx,k) ⊆

rec(V(αxt,k), αxt,k).

(1)

ld∈L(ηk(αx,k))

αxt ,k∈ld\{last(ld)}

Based on the above, we have the following deﬁnition of safety with respect to a set
of manipulations. Intuitively, we iteratively and nondeterministically apply manipula-
tions to explore the region ηk(αx,k), and safety means that no class change is observed
by successive application of such manipulations.

Deﬁnition 5. [Safety wrt Manipulations] Given a neural network N, an input x and a
set ∆k of manipulations, we say that N is safe for input x with respect to the region ηk
and manipulations ∆k, written as N, ηk, ∆k |= x, if the region ηk(αx,k) is a 0-variation for
the set L(ηk(αx,k)) of its ladders, which is complete and covering.

It is straightforward to note that general safety in the sense of Deﬁnition 1 implies

safety wrt manipulations, in the sense of Deﬁnition 5.

Theorem 1. Given a neural network N, an input x, and a region ηk, we have that
N, ηk |= x implies N, ηk, ∆k |= x for any set of manipulations ∆k.

In the opposite direction, we require the minimality assumption on manipulations.

Theorem 2. Given a neural network N, an input x, a region ηk(αx,k) and a set ∆k of
manipulations, we have that N, ηk, ∆k |= x implies N, ηk |= x if the manipulations in ∆k
are minimal.

Theorem 2 means that, under the minimality assumption over the manipulations, an
exhaustive search through the complete and covering ladder tree from L(ηk(αx,k)) can
ﬁnd adversarial examples, if any, and enable us to conclude that the network is safe
at a given point if none are found. Though computing minimal manipulations is not
practical, in discrete spaces by iterating over increasingly reﬁned manipulations we are
able to rule out the existence of adversarial examples in the region. This contrasts with
partial exploration according to, e.g., [25,12]; for comparison see Section 7.

4 The Veriﬁcation Framework

In this section we propose a novel framework for automated veriﬁcation of safety of
classiﬁcation decisions, which is based on search for an adversarial misclassiﬁcation
within a given region. The key distinctive distinctive features of our framework com-
pared to existing work are: a guarantee that a misclassiﬁcation is found if it exists; the
propagation of the analysis layer by layer; and working with hidden layers, in addi-
tion to input and output layers. Since we reduce veriﬁcation to a search for adversarial
examples, we can achieve safety veriﬁcation (if no misclassiﬁcations are found for all
layers) or falsiﬁcation (in which case the adversarial examples can be used to ﬁne-tune
the network or shown to a human tester).

4.1 Layer-by-Layer Analysis

We ﬁrst consider how to propagate the analysis layer by layer, which will involve reﬁn-
ing manipulations through the hidden layers. To facilitate such analysis, in addition to
the activation function φk : DLk−1 → DLk we also require a mapping ψk : DLk → DLk−1
in the opposite direction, to represent how a manipulated activation of layer Lk aﬀects
the activations of layer Lk−1. We can simply take ψk as the inverse function of φk. In
order to propagate safety of regions ηk(αx,k) at a point x into deeper layers, we assume
the existence of functions ηk that map activations to regions, and impose the following
restrictions on the functions φk and ψk, shown diagrammatically in Figure 4.

Deﬁnition 6. The functions {η0, η1, ..., ηn} and {ψ1, ..., ψn} mapping activations to re-
gions are such that

1. ηk(αx,k) ⊆ DLk , for k = 0, ..., n,
2. αx,k ∈ ηk(αx,k), for k = 0, ..., n, and
3. ηk−1(αi,k−1) ⊆ ψk(ηk(αx,k)) for all k = 1, ..., n.

Intuitively, the ﬁrst two conditions state that each function ηk assigns a region around
the activation αx,k, and the last condition that mapping the region ηk from layer Lk to

Lk−1 via ψk should cover the region ηk−1. The aim is to compute functions ηk+1, ..., ηn
based on ηk and the neural network.

The size and complexity of a deep neural network generally means that determining
whether a given set ∆k of manipulations is minimal is intractable. To partially counter
this, we deﬁne a reﬁnement relation between safety wrt manipulations for consecutive
layers in the sense that N, ηk, ∆k |= x is a reﬁnement of N, ηk−1, ∆k−1 |= x if all ma-
nipulations δk−1 in ∆k−1 are reﬁned by a sequence of manipulations δk from the set ∆k.
Therefore, although we cannot theoretically conﬁrm the minimality of ∆k, they are re-
ﬁned layer by layer and, in discrete settings, this process can be bounded from below
by the unit step. Moreover, we can work gradually from a speciﬁc layer inwards until
an adversarial example is found, ﬁnishing processing when reaching the output layer.

The reﬁnement framework is given in Figure 5. The arrows represent the implication

Fig. 4. Layer by layer analysis according to Deﬁnition 6

relations between the safety notions and are labelled with conditions if needed. The
goal of the reﬁnements is to ﬁnd a chain of implications to justify N, η0 |= x. The
fact that N, ηk |= x implies N, ηk−1 |= x is due to the constraints in Deﬁnition 6 when
ψk = φ−1
k . The fact that N, ηk |= x implies N, ηk, ∆k |= x follows from Theorem 1. The
implication from N, ηk, ∆k |= x to N, ηk |= x under the condition that ∆k is minimal is
due to Theorem 2.

We now deﬁne the notion of reﬁnability of manipulations between layers. Intu-
itively, a manipulation in layer Lk−1 is reﬁnable in layer Lk if there exists a sequence of
manipulations in layer Lk that implements the manipulation in layer Lk−1.

Deﬁnition 7. A manipulation δk−1(αy,k−1) is reﬁnable in layer Lk if there exist activa-
tions αx0,k, ..., αx j,k ∈ DLk and valid manipulations δ1
k ∈ V(αx j−1,k) such
that αy,k = αx0,k, δk−1,k(αy,k−1) = αx j,k, and αxt,k = δt
k(αxt−1,k) for 1 ≤ t ≤ j. Given a
neural network N and an input x, the manipulations ∆k are a reﬁnement by layer of
ηk−1, ∆k−1 and ηk if, for all αy,k−1 ∈ ηk−1(αz,k−1), all its valid manipulations δk−1(αy,k−1)
are reﬁnable in layer Lk.

k ∈ V(αx0,k), ..., δ j

Fig. 5. Reﬁnement framework

We have the following theorem stating that the reﬁnement of safety notions is im-

plied by the “reﬁnement by layer” relation.

Theorem 3. Assume a neural network N and an input x. For all layers k ≥ 1, if manip-
ulations ∆k are reﬁnement by layer of ηk−1, ∆k−1 and ηk, then we have that N, ηk, ∆k |= x
implies N, ηk−1, ∆k−1 |= x.

We note that any adversarial example of safety wrt manipulations N, ηk, ∆k |= x
is also an adversarial example for general safety N, ηk |= x. However, an adversarial
example αx,k for N, ηk |= x at layer k needs to be checked to see if it is an adversarial
example of N, η0 |= x, i.e. for the input layer. Recall that Prek(cid:48) (αx,k) is not necessarily
unique. This is equivalent to checking the emptiness of Pre0(αx,k) ∩ η0(αx,0). If we start
the analysis with a hidden layer k > 0 and there is no speciﬁcation for η0, we can instead
consider checking the emptiness of {αy,0 ∈ Pre0(αx,k) | αy,n (cid:44) αx,n}.

4.2 The Veriﬁcation Method

We summarise the theory developed thus far as a search-based recursive veriﬁcation
procedure given below. The method is parameterised by the region ηk around a given
point and a family of manipulations ∆k. The manipulations are speciﬁed by the user
for the classiﬁcation problem at hand, or alternatively can be selected automatically, as
described in Section 4.4. The vector norm to identify the region can also be speciﬁed
by the user and can vary by layer. The method can start in any layer, with analysis
propagated into deeper layers, and terminates when a misclassiﬁcation is found. If an
adversarial example is found by manipulating a hidden layer, it can be mapped back to
the input layer, see Section 4.5.

Algorithm 1 Given a neural network N and an input x, recursively perform the fol-
lowing steps, starting from some layer l ≥ 0. Let k ≥ l be the current layer under
consideration.

1. determine a region ηk such that if k > l then ηk and ηk−1 satisfy Deﬁnition 6;
2. determine a manipulation set ∆k such that if k > l then ∆k is a reﬁnement by layer

of ηk−1, ∆k−1 and ηk according to Deﬁnition 7;

3. verify whether N, ηk, ∆k |= x,
(a) if N, ηk, ∆k |= x then

i. report that N is safe at x with respect to ηk(αx,k) and ∆k, and
ii. continue to layer k + 1;

(b) if N, ηk, ∆k (cid:54)|= x, then report an adversarial example.

We implement Algorithm 1 by utilising satisﬁability modulo theory (SMT) solvers.
The SMT problem is a decision problem for logical formulas with respect to combina-
tions of background theories expressed in classical ﬁrst-order logic with equality. For
checking reﬁnement by layer, we use the theory of linear real arithmetic with existen-
tial and universal quantiﬁers, and for veriﬁcation within a layer (0-variation) we use the
same theory but without universal quantiﬁcation. The details of the encoding and the ap-
proach taken to compute the regions and manipulations are included in Section 4.4. To
enable practical veriﬁcation of deep neural networks, we employ a number of heuristics
described in the remainder of this section.

4.3 Feature Decomposition and Discovery

While Theorem 1 and 2 provide a ﬁnite way to verify safety of neural network clas-
siﬁcation decisions, the high-dimensionality of the region ηk(αx,k) can make any com-
putational approach impractical. We therefore use the concept of a feature to parti-
tion the region ηk(αx,k) into a set of features, and exploit their independence and low-
dimensionality. This allows us to work with state-of-the-art networks that have hun-
dreds, and even thousands, of dimensions.

Intuitively, a feature deﬁnes for each point in the high-dimensional space DLk the
most explicit salient feature it has, e.g., the red-coloured frame of a street sign in Fig-
ure 10. Formally, for each layer Lk, a feature function fk : DLk → P(DLk ) assigns a small
region for each activation αx,k in the space DLk , where P(DLk ) is the set of subspaces of
DLk . The region fk(αx,k) may have lower dimension than that of Dk. It has been argued,
in e.g. [16] for natural images, that natural data, for example natural images and sound,
forms a high-dimensional manifold, which embeds tangled manifolds to represent their
features. Feature manifolds usually have lower dimension than the data manifold, and a
classiﬁcation algorithm is to separate a set of tangled manifolds. By assuming that the
appearance of features is independent, we can manipulate them one by one regardless
of the manipulation order, and thus reduce the problem of size O(2d1+...+dm ) into a set of
smaller problems of size O(2d1 ), ..., O(2dm ).

The analysis of activations in hidden layers, as performed by our method, provides
an opportunity to discover the features automatically. Moreover, deﬁning the feature
fk on each activation as a single region corresponding to a speciﬁc feature is without
loss of generality: although an activation may include multiple features, the indepen-
dence relation between features suggests the existence of a total relation between these
features. The function fk essentially deﬁnes for each activation one particular feature,
subject to certain criteria such as explicit knowledge, but features can also be explored
in parallel.

Every feature fk(αy,k) is identiﬁed by a pre-speciﬁed number dimsk, f of dimensions.
Let dimsk( fk(αy,k)) be the set of dimensions selected according to some heuristic. Then
we have that

(cid:40)

fk(αy,k)(p) =

ηk(αx,k)(p),
[αy,k(p), αy,k(p)] otherwise.

if p ∈ dimsk( fk(αy,k))

(2)

Moreover, we need a set of features to partition the region ηk(αx,k) as follows.

Deﬁnition 8. A set { f1, ..., fm} of regions is a partition of ηk(αx,k), written as π(ηk(αx,k)),
if dimsk, f ( fi) ∩ dimsk, f ( f j) = ∅ for i, j ∈ {1, ..., m} and ηk(αx,k) = ×m

i=1 fi.
Given such a partition π(ηk(αx,k)), we deﬁne a function acts(x, k) by

acts(x, k) = {αy,k ∈ x | x ∈ π(ηk(αx,k))}

(3)

which contains one point for each feature. Then, we reduce the checking of 0-variation
of a region ηk(αx,k) to the following problems:

– checking whether the points in acts(x, k) have the same class as αx,k, and
– checking the 0-variation of all features in π(ηk(αx,k)).

In the above procedure, the checking of points in acts(x, k) can be conducted ei-
ther by following a pre-speciﬁed sequential order (single-path search) or by exhaus-
tively searching all possible orders (multi-path search). In Section 5 we demonstrate
that single-path search according to the prominence of features can enable us to ﬁnd
adversarial examples, while multi-path search may ﬁnd other examples whose distance
to the original input image is smaller.

4.4 Selection of Regions and Manipulations

The procedure summarised in Algorithm 1 is typically invoked for a given image in the
input layer, but, providing insight about hidden layers is available, it can start from any
layer Ll in the network. The selection of regions can be automated, as described below.
For the ﬁrst layer to be considered, i.e., k = l, the region ηk(αx,k) is deﬁned by ﬁrst
selecting the subset of dimsk dimensions from Pk whose activation values are furthest
away from the average activation value of the layer1. Intuitively, the knowledge repre-
sented by these activations is more explicit than the knowledge represented by the other
dimensions, and manipulations over more explicit knowledge are more likely to result
in a class change. Let avgk = ((cid:80)
p∈Pk αx,k(p))/nk be the average activation value of layer
Lk. We let dimsk(ηk(αx,k)) be the ﬁrst dimsk dimensions p ∈ Pk with the greatest values
|αx,k(p) − avg| among all dimensions, and then deﬁne

ηk(αx,k) = ×p∈dimsk(ηk(αx,k))[αx,k(p) − sp ∗ mp, αx,k(p) + sp ∗ mp]

(4)

i.e., a dimsk-polytope containing the activation αx,k, where sp represents a small span
and mp represents the number of such spans. Let Vk = {sp, mp | p ∈ dimsk(ηk(αx,k))} be
a set of variables.

Let d be a function mapping from dimsk(ηk(αx,k)) to {−1, 0, +1} such that {d(p) (cid:44)
0 | p ∈ dimsk(ηk(αx,k))} (cid:44) ∅, and D(dimsk(ηk(αx,k))) be the set of such functions. Let a
manipulation δd

k be

k (αy,k)(p) =
δd





αy,k(p) − sp if d(p) = −1
if d(p) = 0
αy,k(p)
αy,k(p) + sp if d(p) = +1

(5)

1 We also considered other approaches, including computing derivatives up to several layers, but

for the experiments we conduct they are less eﬀective.

for activation αy,k ∈ ηk(αx,k). That is, each manipulation changes a subset of the dimen-
sions by the span sp, according to the directions given in d. The set ∆k is deﬁned by col-
lecting the set of all such manipulations. Based on this, we can deﬁne a set L(ηk(αx,k))
of ladders, which is complete and covering.

Determining the region ηk according to ηk−1 Given ηk−1(αx,k−1) and the functions φk
and ψk, we can automatically determine a region ηk(αx,k) satisfying Deﬁnition 6 using
the following approach. According to the function φk, the activation value αx,k(p) of
perceptron p ∈ Pk is computed from activation values of a subset of perceptrons in
Pk−1. We let Vars(p) ⊆ Pk−1 be such a set of perceptrons. The selection of dimensions
in dimsk(ηk(αx,k)) depends on dimsk−1(ηk−1(αx,k−1)) and φk, by requiring that, for every
p(cid:48) ∈ dimsk−1(ηk−1(αx,k−1)), there is at least one dimension p ∈ dimsk(ηk(αx,k)) such that
p(cid:48) ∈ Vars(p). We let

dimsk(ηk(αx,k)) = {arg max

{ |αx,k(p) − avgk| | p(cid:48) ∈ Vars(p)} | p(cid:48) ∈ dimsk−1(ηk−1(αx,k−1))}

p∈Pk

(6)
Therefore, the restriction of Deﬁnition 6 can be expressed with the following formula:

∀αy,k−1 ∈ ηk(αx,k−1) : αy,k−1 ∈ ψk(ηk(αx,k)).

(7)

We omit the details of rewriting αy,k−1 ∈ ηk(αx,k−1) and αy,k−1 ∈ ψk(ηk(αx,k)) into
Boolean expressions, which follow from standard techniques. Note that this expres-
sion includes variables in Vk, Vk−1 and αy,k−1. The variables in Vk−1 are ﬁxed for a given
ηk−1(αx,k−1). Because such a region ηk(αx,k) always exists, a simple iterative procedure
can be invoked to gradually increase the size of the region represented with variables in
Vk to eventually satisfy the expression.

Determining the manipulation set ∆k according to ηk(αx,k), ηk−1(αx,k−1), and ∆k−1
The values of the variables Vk obtained from the satisﬁability of Eqn (7) yield a deﬁni-
tion of manipulations using Eqn (5). However, the obtained values for span variables sp
do not necessarily satisfy the “reﬁnement by layer” relation as deﬁned in Deﬁnition 7.
Therefore, we need to adapt the values for the variables Vk while, at the same time,
retaining the region ηk(αx,k). To do so, we could rewrite the constraint in Deﬁnition 7
into a formula, which can then be solved by an SMT solver. But, in practice, we notice
that such precise computations easily lead to overly small spans sp, which in turn result
in an unacceptable amount of computation needed to verify the relation N, ηk, ∆k |= x.
To reduce computational cost, we work with a weaker “reﬁnable in layer Lk” notion,
parameterised with respect to precision ε. Given two activations αy,k and αm,k, we use
dist(αy,k, αm,k) to represent their distance.

Deﬁnition 9. A manipulation δk−1(αy,k−1) is reﬁnable in layer Lk with precision ε > 0
if there exists a sequence of activations αx0,k, ..., αx j,k ∈ DLk and valid manipulations
k ∈ V(αx j−1,k) such that αy,k = αx0,k, δk−1,k(αy,k−1) ∈ rec(αx j−1,k, αx j,k),
k ∈ V(αx0,k), ..., δd
δ1
dist(αx j−1,k, αx j,k) ≤ (cid:15), and αxt,k = δt
k(αxt−1,k) for 1 ≤ t ≤ j. Given a neural network N
and an input x, the manipulations ∆k are a reﬁnement by layer of ηk, ηk−1, ∆k−1 with

precision ε if, for all αy,k−1 ∈ ηk−1(αx,k−1), all its legal manipulations δk−1(αy,k−1) are
reﬁnable in layer Lk with precision ε.

Comparing with Deﬁnition 7, the above deﬁnition replaces δk−1,k(αy,k−1) = αx j,k
with δk−1,k(αy,k−1) ∈ rec(αx j−1,k, αx j,k) and dist(αx j−1,k, αx j,k) ≤ ε. Intuitively, instead of
requiring a manipulation to reach the activation δk−1,k(αy,k−1) precisely, this deﬁnition
allows for each δk−1,k(αy,k−1) to be within the hyper-rectangle rec(αx j−1,k, αx j,k). To ﬁnd
suitable values for Vk according to the approximate “reﬁnement-by-layer” relation, we
use a variable h to represent the maximal number of manipulations of layer Lk used to
express a manipulation in layer k − 1. The value of h (and variables sp and np in Vk)
are automatically adapted to ensure the satisﬁability of the following formula, which
expresses the constraints of Deﬁnition 9:

∀αy,k−1 ∈ ηk(αx,k−1)∀d ∈ D(dimsk(ηk(αx,k−1)))∀δd
∃αy0,k, ..., αyh,k ∈ ηk(αx,k) : αy0,k = αy,k ∧ (cid:86)h−1
(cid:87)h−1

k−1 ∈ Vk−1(αy,k−1)
k (αyt,k)∧

t=0 αyt+1,k = δd

t=0 (δd

k−1,k(αy,k) ∈ rec(αyt,k, αyt+1,k) ∧ dist(αyt,k, αyt+1,k) ≤ ε).
It is noted that sp and mp for p ∈ dimsk(ηk(αx,k)) are employed when expressing δd
k .
The manipulation δd
k−1 by considering the corresponding relation
between dimensions in dimsk(ηk(αx,k)) and dimsk−1(ηk−1(αx,k−1)).

k is obtained from δd

(8)

Adversarial examples shown in Figures 8, 9, and 10 were found using single-path

search and automatic selection of regions and manipulations.

4.5 Mapping Back to Input Layer

When manipulating the hidden layers, we may need to map back an activation in layer
k to the input layer to obtain an input image that resulted in misclassiﬁcation, which
involves computation of Pre0(αy,k) described next. To check the 0-variation of a region
ηk(αx,k), we need to compute diﬀn(αx,n, αy,n) for many points αy,x in ηk(αx,k), where
diﬀn : DLn × DLn → {0, 1} is given by diﬀn(αx,n, αy,n) = 0 if αx,n = αy,n and 1 otherwise.
Because αx,n is known, we only need to compute αy,n. We can compute αy,n by ﬁnding
a point αy,0 ∈ Pre0(αy,k) and then using the neural network to predict the value αy,n. It
should be noted that, although Pre0(αy,k) may include more than one point, all points
have the same class, so any point in Pre0(αy,k) is suﬃcient for our purpose.

To compute αy,0 from αy,k, we use functions ψk, ψk−1, ..., ψ1 and compute points

αy,k−1, αy,k−2, ..., αy,0 such that

αy, j−1 = ψ j(αy, j) ∧ αy, j−1 ∈ η j−1(αx, j−1)

for 1 ≤ j ≤ k. The computation relies on an SMT solver to encode the functions
ψk, ψk−1, ..., ψ1 if they are piecewise linear functions, and by taking the corresponding
inverse functions directly if they are sigmoid functions. It is possible that, for some
1 ≤ j ≤ k, no point can be found by SMT solver, which means that the point αy,k does
not have any corresponding point in the input layer. We can safely discard these points.
The maxpooling function ψ j selects from every m ∗ m dimensions the maximal element
for some m > 0. The computation of the maxpooling layer ψ j−1 is combined with the
computation of the next layer ψ j, that is, ﬁnding αy, j−2 with the following expression
∃αx, j−1 : αy, j−2 = ψ j−1(ψ j(αy, j)) ∧ αy, j−1 ∈ η j−1(αx, j−1) ∧ αy, j−2 ∈ η j−2(αx, j−2)

This is to ensure that in the expression αy, j−2 = ψ j−1(ψ j(αy, j)) we can reuse m ∗ m − 1
elements in αx, j−2 and only need to replace the maximal element.

Figures 8, 9, and 10 show images obtained by mapping back from the ﬁrst hidden

layer to the input layer.

5 Experimental Results

The proposed framework has been implemented as a software tool called DLV (Deep
Learning Veriﬁcation) [2] written in Python, see Appendix of [20] for details of input
parameters and how to use the tool. The SMT solver we employ is Z3 [8], which has
Python APIs. The neural networks are built from a widely-used neural networks library
Keras [3] with a deep learning package Theano [6] as its backend.

We validate DLV on a set of experiments performed for neural networks trained for
classiﬁcation based on a predeﬁned multi-dimensional surface (small size networks),
as well as image classiﬁcation (medium size networks). These networks respectively
use two representative types of layers: fully connected layers and convolutional layers.
They may also use other types of layers, e.g., the ReLU layer, the pooling layer, the
zero-padding layer, and the dropout layer. The ﬁrst three demonstrate the single-path
search functionality on the Euclidean (L2) norm, whereas the fourth (GTSRB) multi-
path search for the L1 and L2 norms.

The experiments are conducted on a MacBook Pro laptop, with 2.7 GHz Intel Core

i5 CPU and 8 GB memory.

Two-Dimensional Point Classiﬁcation Network To demonstrate exhaustive veriﬁca-
tion facilitated by our framework, we consider a neural network trained for classifying
points above and below a two-dimensional curve shown in red in Figure 6 and Figure 7.
The network has three fully-connected hidden layers with the ReLU activation func-
tion. The input layer has two perceptrons, every hidden layer has 20 perceptrons, and
the output layer has two perceptrons. The network is trained with 5,000 points sampled
from the provided two-dimensional space, and has an accuracy of more than 99%.

For a given input x = (3.59, 1.11), we start from the input layer and deﬁne a region

around this point by taking unit steps in both directions

η0(αx,0) = [3.59 − 1.0, 3.59 + 1.0] × [1.11 − 1.0, 1.11 + 1.0] = [2.59, 4.59] × [0.11, 2.11]

The manipulation set ∆0 is shown in Figure 6: there are 9 points, of which the point in
the middle represents the activation αx,0 and the other 8 points represent the activations
resulting from applying one of the manipulations in ∆0 on αx,0. Note that, although there
are class changes in the region η0(αx,0), the manipulation set ∆0 is not able to detect such
changes. Therefore, we have that N, η0, ∆0 |= x.

Now consider layer k = 1. To obtain the region η1(αx,1), the tool selects two dimen-

sions p1,17, p1,19 ∈ P1 in layer L1 with indices 17 and 19 and computes

η1(αx,1) = [αx,1(p1,17) − 3.6, αx,1(p1,17) + 3.6] × [αx,1(p1,19) − 3.52, αx,1(p1,19) + 3.52]

The manipulation set ∆1, after mapping back to the input layer with function ψ1, is given
as Figure 7. Note that η1 and η0 satisfy Deﬁnition 6, and ∆1 is a reﬁnement by layer of

Fig. 6. Input layer

Fig. 7. First hidden layer

η0, ∆0 and η1. We can see that a class change can be detected (represented as the red
coloured point). Therefore, we have that N, η1, ∆1 (cid:54)|= x.

Image Classiﬁcation Network for the MNIST Handwritten Image Dataset The
well-known MNIST image dataset contains images of size 28 × 28 and one channel
and the network is trained with the source code given in [5]. The trained network is of
medium size with 600,810 parameters, has an accuracy of more than 99%, and is state-
of-the-art. It has 12 layers, within which there are 2 convolutional layers, as well as
layers such as ReLU, dropout, fully-connected layers and a softmax layer. The images
are preprocessed to make the value of each pixel within the bound [0, 1].

Given an image x, we start with layer k = 1 and the parameter set to at most 150
dimensions (there are 21632 dimensions in layer L1). All ηk, ∆k for k ≥ 2 are computed
according to the simple heuristic mentioned in Section 4.2 and satisfy Deﬁnition 6 and
Deﬁnition 7. For the region η1(αx,1), we allow changes to the activation value of each
selected dimension that are within [-1,1]. The set ∆1 includes manipulations that can
change the activation value for a subset of the 150 dimensions, by incrementing or
decrementing the value for each dimension by 1. The experimental results show that
for most of the examples we can ﬁnd a class change within 100 dimensional changes
in layer L1, by comparing the number of pixels that have changed, and some of them
can have less than 30 dimensional changes. Figure 8 presents examples of such class
changes for layer L1. We also experiment on images with up to 40 dimensional changes
in layer L1; the tool is able to check the entire network, reaching the output layer and
claiming that N, ηk, ∆k |= x for all k ≥ 1. While training of the network takes half an
hour, ﬁnding an adversarial example takes up to several minutes.

Image Classiﬁcation Network for the CIFAR-10 Small Image Dataset We work
with a medium size neural network, trained with the source code from [1] for more than
12 hours on the well-known CIFAR10 dataset. The inputs to the network are images
of size 32 × 32 with three channels. The trained network has 1,250,858 real-valued pa-

8 to 0

2 to 1

4 to 2

2 to 3

9 to 4

6 to 5

4 to 6

9 to 7

0 to 8

7 to 9

Fig. 8. Adversarial examples for a neural network trained on MNIST

rameters and includes convolutional layers, ReLU layers, max-pooling layers, dropout
layers, fully-connected layers, and a softmax layer.

As an illustration of the type of perturbations that we are investigating, consider the
images in Figure 9, which correspond to the parameter setting of up to 25, 45, 65, 85,
105, 125, 145 dimensions, respectively, for layer k = 1. The manipulations change the
activation values of these dimensions. Each image is obtained by mapping back from
the ﬁrst hidden layer and represents a point close to the boundary of the correspond-
ing region. The relation N, η1, ∆1 |= x holds for the ﬁrst 7 images, but fails for the last
one and the image is classiﬁed as a truck. Intuitively, our choice of the region η1(αx,1)
identiﬁes the subset of dimensions with most extreme activations, taking advantage of
the analytical capability of the ﬁrst hidden layer. A higher number of selected dimen-
sions implies a larger region in which we apply manipulations, and, more importantly,
suggests a more dramatic change to the knowledge represented by the activations when
moving to the boundary of the region.

Fig. 9. An illustrative example of mapping back to input layer from the Cifar-10
mataset: the last image classiﬁes as a truck.

We also work with 500 dimensions and otherwise the same experimental parameters
as for MNIST. Figure 13 in Appendix of [20] gives 16 pairs of original images (clas-
siﬁed correctly) and perturbed images (classiﬁed wrongly). We found that, while the
manipulations lead to human-recognisable modiﬁcations to the images, the perturbed
images can be classiﬁed wrongly by the network. For each image, ﬁnding an adversarial
example ranges from seconds to 20 minutes.

Image Classiﬁcation Network for the ImageNet Dataset We also conduct experi-
ments on a large image classiﬁcation network trained on the popular ImageNet dataset.
The images are of size 224 × 224 and have three channels. The network is the model
of the 16-layer network [34], called VGG16, used by the VGG team in the ILSVRC-

2014 competition, downloaded from [7]. The trained network has 138,357,544 real-
valued parameters and includes convolutional layers, ReLU layers, zero-padding lay-
ers, dropout layers, max-pooling layers, fully-connected layers, and a softmax layer.
The experimental parameters are the same as for the previous two experiments, except
that we work with 20,000 dimensions.

Several additional pairs of original and perturbed images are included in Figure 14
in Appendix of [20]. In Figure 10 we also give two examples of street sign images. The
image on the left is reported unsafe for the second layer with 6346 dimensional changes
(0.2% of the 3,211,264 dimensions of layer L2). The one on the right is reported safe
for 20,000 dimensional changes of layer L2. It appears that more complex manipula-
tions, involving more dimensions (perceptrons), are needed in this case to cause a class
change.

Fig. 10. Street sign images. Found an adversarial example for the left image (class
changed into bird house), but cannot ﬁnd an adversarial example for the right image
for 20,000 dimensions.

5.1 The German Traﬃc Sign Recognition Benchmark (GTSRB)

We evaluate DLV on the GTSRB dataset (by resizing images into size 32*32), which
has 43 classes. Figure 11 presents the results for the multi-path search. The ﬁrst case
(approx. 20 minutes to manipulate) is a stop sign (conﬁdence 1.0) changed into a speed
limit of 30 miles, with an L1 distance of 0.045 and L2 distance of 0.19. The conﬁdence
of the manipulated image is 0.79. The second, easy, case (seconds to manipulate) is a
speed limit of 80 miles (conﬁdence 0.999964) changed into a speed limit of 30 miles,
with an L1 distance of 0.004 and L2 distance of 0.06. The conﬁdence of the manipulated
image is 0.99 (a very high conﬁdence of misclassiﬁcation). Also, a “go right” sign can
be easily manipulated into a sign classiﬁed as “go straight”.

Figure 16 in [20] presents additional adversarial examples obtained when selecting

single-path search.

6 Comparison

We compare our approach with two existing approaches for ﬁnding adversarial exam-
ples, i.e., fast gradient sign method (FGSM) [36] and Jacobian saliency map algorithm

“stop”
to “30m speed limit”

“80m speed limit”
to “30m speed limit”

“go right”
to “go straight”

Fig. 11. Adversarial examples for the network trained on the GTSRB dataset by multi-
path search

(JSMA) [28]. FGSM calculates the optimal attack for a linear approximation of the net-
work cost, whereas DLV explores a proportion of dimensions in the feature space in the
input or hidden layers. JSMA ﬁnds a set of dimensions in the input layer to manipulate,
according to the linear approximation (by computing the Jacobian matrix) of the model
from current output to a nominated target output. Intuitively, the diﬀerence between
DLV’s manipulation and JSMA is that DLV manipulates over features discovered in
the activations of the hidden layer, while JSMA manipulates according to the partial
derivatives, which depend on the parameters of the network.

Experiment 1. We randomly select an image from the MNIST dataset. Figure 12
shows some intermediate and ﬁnal images obtained by running the three approaches:
FGSM, JSMA and DLV. FGSM has a single parameter, (cid:15), where a greater (cid:15) represents a
greater perturbation along the gradient of cost function. Given an (cid:15), for each input exam-
ple a perturbed example is returned and we test whether it is an adversarial example by
checking for misclassiﬁcation against the original image. We gradually increase the pa-
rameter (cid:15) = 0.05, 0.1, 0.2, 0.3, 0.4, with the last image (i.e., (cid:15) = 0.4) witnessing a class
change, see the images in the top row of Figure 12. FGSM can eﬃciently manipulate a
set of images, but it requires a relatively large manipulation to ﬁnd a misclassiﬁcation.
For the JSMA approach, we conduct the experiment on a setting with parameters
(cid:15) = 0.1 and θ = 1.0. The parameter (cid:15) = 0.1 means that we only consider adversarial ex-
amples changing no more than 10% of all the pixels, which is suﬃcient here. As stated
in [29], the parameter θ = 1.0, which allows a maximum change to every pixel, can en-
sure that fewer pixels need to be changed. The approach takes a series of manipulations
to gradually lead to a misclassiﬁcation, see the images in the middle row of Figure 12.
The misclassiﬁed image has an L2 (Euclidean) distance of 0.17 and an L1 (Manhattan)
distance of 0.03 from the original image. While JSMA can ﬁnd adversarial examples
with smaller distance from the original image, it takes longer to manipulate a set of
images.

Both FGSM and JSMA follow their speciﬁc heuristics to deterministically explore
the space of images. However, in some cases, the heuristics may omit better adversarial
examples. In the experiment for DLV, instead of giving features a speciﬁc order and
manipulating them sequentially, we allow the program to nondeterministically choose
features. This is currently done by MCTS (Monte Carlo Tree Search), which has a theo-
retical guarantee of convergence for inﬁnite sampling. Therefore, the high-dimensional
space is explored by following many diﬀerent paths. By taking the same manipulation

Fig. 12. FGSM vs. JSMA vs. DLV, where FGSM and JSMA search a single path and
DLV multiple paths. Top row: Original image (7) perturbed deterministically by FGSM
with (cid:15) = 0.05, 0.1, 0.2, 0.3, 0.4, with the ﬁnal image (i.e., (cid:15) = 0.4) misclassiﬁed as
9. Middle row: Original image (7) perturbed deterministically by JSMA with (cid:15) = 0.1
and θ = 1.0. We show even numbered images of the 12 produced by JSMA, with the
ﬁnal image misclassiﬁed as 3. Bottom row: Original image (7) perturbed nondetermin-
istically by DLV, for the same manipulation on a single pixel as that of JSMA (i.e.,
sp ∗ mp = 1.0) and working in the input layer, with the ﬁnal image misclassiﬁed as 3.

on a single pixel as that of JSMA (i.e., sp ∗ mp = 1.0) and working on the input layer,
DLV is able to ﬁnd another perturbed image that is also classiﬁed as 3 but has a smaller
distance (L2 distance is 0.14 and L1 distance is 0.02) from the original image, see the
images in the last row of Figure 12. In terms of the time taken to ﬁnd an adversarial
example, DLV may take longer than JSMA, since it searches over many diﬀerent paths.

L2
L1
%

FGSM ((cid:15) = 0.1)
0.08
0.06
17.5%

(0.4) DLV (dimsl = 75) (150) (450) JSMA (θ = 0.1) (0.4)
(0.2)
0.11
0.22 0.27
0.32
0.15
0.02
0.06 0.09
0.12
0.25
99%
79% 98%
70.9% 97.2%

0.19
0.04
52.3%
Table 1. FGSM vs. DLV (on a single path) vs. JSMA

0.11
0.02
92%

Experiment 2. Table 1 gives a comparison of robustness evaluation of the three
appraoches on the MNIST dataset. For FGSM, we vary the input parameter (cid:15) accord-
ing to the values {0.1, 0.2, 0.4}. For DLV, we select regions as deﬁned in Section 4.4
on a single path (by deﬁning a speciﬁc order on the features and manipulating them
sequentially) for the ﬁrst hidden layer. The experiment is parameterised by varying the
maximal number of dimensions to be changed, i.e., dimsl ∈ {75, 150, 450}. For each
input image, an adversarial example is returned, if found, by manipulating fewer than
the maximal number of dimensions. When the maximal number has been reached, DLV
will report failure and return the last perturbed example. For JSMA, the experiment is
conducted by letting θ take the value in the set {0.1, 0.4} and setting (cid:15) to 1.0.

We collect three statistics, i.e., the average L1 distance over the adversarial exam-
ples, the average L2 distance over the adversarial examples, and the success rate of

ﬁnding adversary examples. Let Ld(x, δ(x)) for d ∈ {1, 2} be the distance between an
input x and the returned perturbed image δ(x), and diﬀ(x, δ(x)) ∈ {0, 1} be a Boolean
value representing whether x and δ(x) have diﬀerent classes. We let

and

(cid:80)

Ld =

x in test set diﬀ(x, δ(x)) × Ld(x, δ(x))
x in test set diﬀ(x, δ(x))

(cid:80)

% =

(cid:80)

x in test set diﬀ(x, δ(x))
the number of examples in test set

We note that the approaches yield diﬀerent perturbed examples δ(x).

The test set size is 500 images selected randomly. DLV takes 1-2 minutes to manip-
ulate each input image in MNIST. JSMA takes about 10 minutes for each image, but it
works for 10 classes, so the running time is similar to that of DLV. FGSM works with a
set of images, so it is the fastest per image.

For the case when the success rates are very high, i.e., 97.2% for FGSM with (cid:15) =
0.4, 98% for DLV with dimsl = 450, and 99% for JSMA with θ = 0.4, JSMA has the
smallest average distances, followed by DLV, which has smaller average distances than
FGSM on both L1 and L2 distances.

We mention that a smaller distance leading to a misclassiﬁcation may result in a
lower rate of transferability [29], meaning that a misclassiﬁcation can be harder to wit-
ness on another model trained on the same (or a small subset of) data-set.

7 Related Work

AI safety is recognised an an important problem, see e.g., [33,10]. An early veriﬁcation
approach for neural networks was proposed in [30], where, using the notation of this
paper, safety is deﬁned as the existence, for all inputs in a region η0 ∈ DL0 , of a corre-
sponding output in another region ηn ⊆ DLn . They encode the entire network as a set of
constraints, approximating the sigmoid using constraints, which can then be solved by a
SAT solver, but their approach only works with 6 neurons (3 hidden neurons). A similar
idea is presented in [32]. In contrast, we work layer by layer and obtain much greater
scalability. Since the ﬁrst version of this paper appeared [20], another constraint-based
method has been proposed in [21] which improves on [30]. While they consider more
general correctness properties than this paper, they can only handle the ReLU activation
functions, by extending the Simplex method to work with the piecewise linear ReLU
functions that cannot be expressed using linear programming. This necessitates a search
tree (instead of a search path as in Simplex), for which a heuristic search is proposed
and shown to be complete. The approach is demonstrated on networks with 300 ReLU
nodes, but as it encodes the full network it is unclear whether it can be scaled to work
with practical deep neural networks: for example, the MNIST network has 630,016
ReLU nodes. They also handle continuous spaces directly without discretisation, the
beneﬁts of which are not yet clear, since it is argued in [19] that linear behaviour in
high-dimensional spaces is suﬃcient to cause adversarial examples.

Concerns about the instability of neural networks to adversarial examples were ﬁrst
raised in [13,36], where optimisation is used to identify misclassiﬁcations. A method

for computing the perturbations is also proposed, which is based on box-constrained
optimisation and is approximate in view of non-convexity of the search space. This
work is followed by [19], which introduced the much faster FGSM method, and [22],
which employed a compromise between the two (iterative, but with a smaller number
of iterations than [36]). In our notation, [19] uses a deterministic, iterative manipula-
tion δ(x) = x + (cid:15) sign((cid:79)x J(x, αx,n)), where x is an image in matrix representation, (cid:15) is a
hyper-parameter that can be tuned to get diﬀerent manipulated images, and J(x, αx,n) is
the cross-entropy cost function of the neural network on input x and class αx,n. There-
fore, their approach will test a set of discrete points in the region η0(αx,0) of the input
layer. Therefore these manipulations will test a lasso-type ladder tree (i.e., a ladder tree
without branches) L(ηk(αx,k)), which does not satisfy the covering property. In [26],
instead of working with a single image, an evolutionary algorithm is employed for a
population of images. For each individual image in the current population, the manip-
ulation is the mutation and/or crossover. While mutations can be nondeterministic, the
manipulations of an individual image are also following a lasso-type ladder tree which
is not covering. We also mention that [38] uses several distortions such as JPEG com-
pression, thumbnail resizing, random cropping, etc, to test the robustness of the trained
network. These distortions can be understood as manipulations. All these attacks do not
leverage any speciﬁc properties of the model family, and do not guarantee that they will
ﬁnd a misclassiﬁed image in the constraint region, even if such an image exists.

The notion of robustness studied in [18] has some similarities to our deﬁnition of
safety, except that the authors work with values averaged over the input distribution µ,
which is diﬃcult to estimate accurately in high dimensions. As in [36,22], they use opti-
misation without convergence guarantees, as a result computing only an approximation
to the minimal perturbation. In [12] pointwise robustness is adopted, which corresponds
to our general safety; they also use a constraint solver but represent the full constraint
system by reduction to a convex LP problem, and only verify an approximation of the
property. In contrast, we work directly with activations rather than an encoding of ac-
tivation functions, and our method exhaustively searches through the complete ladder
tree for an adversarial example by iterative and nondeterministic application of manip-
ulations. Further, our deﬁnition of a manipulation is more ﬂexible, since it allows us to
select a subset of dimensions, and each such subset can have a diﬀerent region diameter
computed with respect to a diﬀerent norm.

8 Conclusions

This paper presents an automated veriﬁcation framework for checking safety of deep
neural networks that is based on a systematic exploration of a region around a data point
to search for adversarial manipulations of a given type, and propagating the analysis into
deeper layers. Though we focus on the classiﬁcation task, the approach also generalises
to other types of networks. We have implemented the approach using SMT and vali-
dated it on several state-of-the-art neural network classiﬁers for realistic images. The
results are encouraging, with adversarial examples found in some cases in a matter of
seconds when working with few dimensions, but the veriﬁcation process itself is expo-
nential in the number of features and has prohibitive complexity for larger images. The

performance and scalability of our method can be signiﬁcantly improved through par-
allelisation. It would be interesting to see if the notions of regularity suggested in [24]
permit a symbolic approach, and whether an abstraction reﬁnement framework can be
formulated to improve the scalability and computational performance.

Acknowledgements. This paper has greatly beneﬁted from discussions with sev-
eral researchers. We are particularly grateful to Martin Fraenzle, Ian Goodfellow and
Nicolas Papernot.

References

1. CIFAR10 model for Keras. https://github.com/fchollet/keras/blob/master/examples/cifar10 cnn.py.
2. DLV. https://github.com/verideep/dlv.
3. Keras. https://keras.io.
4. Large scale visual recognition challenge. http://www.image-net.org/challenges/LSVRC/.
5. MNIST CNN network. https://github.com/fchollet/keras/blob/master/examples/mnist cnn.py.
6. Theano. http://deeplearning.net/software/theano/.
7. VGG16 model for Keras. https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3.
8. Z3. http://rise4fun.com/z3.
9. Luigi Ambrosio, Nicola Fusco, and Diego Pallara. Functions of bounded variation and free
discontinuity problems. Oxford Mathematical Monographs. Oxford University Press, 2000.
10. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dario
Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e.
Concrete problems in AI safety. CoRR, abs/1606.06565, 2016.

11. Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso
Poggio. Unsupervised learning of invariant representations. Theoretical Computer Science,
633:112–121, 2016.

12. Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori,
CoRR,

and Antonio Criminisi. Measuring neural net robustness with constraints.
abs/1605.07262, 2016. To appear in NIPS.

13. Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov,
Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In
ECML/PKDD 2013, pages 387–402, 2013.

14. Christopher M Bishop. Neural networks for pattern recognition. Oxford university press,

1995.

15. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp,
Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang,
Jake Zhao, and Karol Zieba. End to end learning for self-driving cars. arXiv:1604.07316,
2016.

16. Gunnar E. Carlsson, Tigran Ishkhanov, Vin de Silva, and Afra Zomorodian. On the local
behavior of spaces of natural images. International Journal of Computer Vision, 76(1), 2008.
17. Lisa Anne Hendricks Dong Huk Park, Zeynep Akata, Bernt Schiele, Trevor Darrell, and
Marcus Rohrbach. Attentive explanations: Justifying decisions and pointing to the evidence.
arxiv.org/abs/1612.04757, 2016.

18. Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classiﬁers’ robustness to

adversarial perturbations. CoRR, abs/1502.02590, 2015.

19. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing ad-

versarial examples. CoRR, abs/1412.6572, 2014.

20. Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety veriﬁcation of deep

neural networks. https://arxiv.org/abs/1610.06940, 2016.

21. Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An
eﬃcient SMT solver for verifying deep neural networks. In CAV 2017, 2017. To appear.
22. Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical

world. arXiv:1607.02533, 2016.

23. Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, 521:436–444,

24. St´ephane Mallat. Understanding deep convolutional networks. Philosohical Transactions of

2015.

the Royal Society A, 2016.

25. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple

and accurate method to fool deep neural networks. CoRR, abs/1511.04599, 2015.

26. Anh Nguyen, Jason Yosinski, and Jeﬀ Clune. Deep neural networks are easily fooled: High
conﬁdence predictions for unrecognizable images. In Computer Vision and Pattern Recog-
nition (CVPR ’15), 2015.

27. Nicolas Papernot, Ian Goodfellow, Ryan Sheatsley, Reuben Feinman, and Patrick McDaniel.
cleverhans v1.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768,
2016.

28. Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and
Ananthram Swami. The limitations of deep learning in adversarial settings. In Proceedings
of the 1st IEEE European Symposium on Security and Privacy, 2015.

29. Nicolas Papernot, Patrick Drew McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik,
and Ananthram Swami. Practical black-box attacks against deep learning systems using
adversarial examples. CoRR, abs/1602.02697, 2016.

30. Luca Pulina and Armando Tacchella. An abstraction-reﬁnement approach to veriﬁcation of

artiﬁcial neural networks. In CAV 2010, pages 243–257, 2010.

31. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust you?”: Ex-
In ACM SIGKDD International Conference on

plaining the predictions of any classiﬁer.
Knowledge Discovery and Data Mining (KDD2016), 2016.

32. Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker. Towards veriﬁcation
of artiﬁcial neural networks. In 18th Workshop on Methoden und Beschreibungssprachen
zur Modellierung und Veriﬁkation von Schaltungen und Systemen” (MBMV), pages 30–40,
2015.

33. Sanjit A. Seshia and Dorsa Sadigh.

Towards veriﬁed artiﬁcial intelligence. CoRR,

abs/1606.08514, 2016.

34. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv:1409.1556, 2014.

35. J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking
machine learning algorithms for traﬃc sign recognition. Neural Networks, 32:323–332,
2012.

36. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
In International

Intriguing properties of neural networks.

Goodfellow, and Rob Fergus.
Conference on Learning Representations (ICLR-2014), 2014.

37. Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in Neural
Information Processing Systems 4, [NIPS Conference, Denver, Colorado, USA, December
2-5, 1991], pages 831–838, 1991.

38. Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness

of deep neural networks via stability training. In CVPR 2016, 2016.

A Input Parameters and Experimental Setup

The DLV tool accepts as input a network N and an image x, and has the following input
parameters:

– an integer l ∈ [0, n] indicating the starting layer Ll,
– an integer dimsl ≥ 1 indicating the maximal number of dimensions that need to be

considered in layer Ll,

– the values of variables sp and mp in Vl; for simplicity, we ask that, for all dimensions
p that will be selected by the automated procedure, sp and mp have the same values,

– the precision ε ∈ [0, ∞),
– an integer dimsk, f indicating the number of dimensions for each feature; for sim-
plicity, we ask that every feature has the same number of dimensions and dimsk, f =
dimsk(cid:48), f for all layers k and k(cid:48), and

– type of search: either heuristic (single-path) or Monte Carlo Tree Search (MCTS)

(multi-path).

A.1 Two-Dimensional Point Classiﬁcation Network

A.2 Network for the MNIST Dataset

– l = 0
– dimsl = 2,
– sp = 1.0 and mp = 1.0,
– ε = 0.1, and
– dimsk, f = 2

– l = 1
– dimsl = 150,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

– l = 1
– dimsl = 500,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

– l = 1
– dimsl = 1000,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

A.3 Network for the CIFAR-10 Dataset

A.4 Network for the GTSRB Dataset

A.5 Network for the ImageNet Dataset

– l = 2
– dimsl = 20, 000,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

B Additional Adversarial Examples Found for the CIFAR-10,

ImageNet, and MNIST Networks

Figure 13 and Figure 14 present additional adversarial examples for the CIFAR-10 and
ImageNet networks by single-path search. Figure 15 presents adversarial examples for
the MNIST network by multi-path search.

automobile to bird

automobile to frog

automobile to airplane automobile to horse

airplane to dog

airplane to deer

airplane to truck

airplane to cat

truck to frog

truck to cat

ship to bird

ship to airplane

ship to truck

horse to cat

horse to automobile

horse to truck

Fig. 13. Adversarial examples for a neural network trained on the CIFAR-10 dataset by
single-path search

C Additional Adversarial Examples for the German Traﬃc Sign

Recognition Benchmark (GTSRB)

Figure 16 presents adversarial examples obtained when selecting single-path search.

labrador to life boat

rhodesian ridgeback to malinois

boxer to rhodesian ridgeback

great pyrenees to kuvasz

Fig. 14. Adversarial Examples for the VGG16 Network Trained on the imageNet
Dataset By Single-Path Search

9 to 4

8 to 3

5 to 3

4 to 9

5 to 3

7 to 3

9 to 4

9 to 4

2 to 3

1 to 8

8 to 5

0 to 3

7 to 2

8 to 3

3 to 2

9 to 7

3 to 2

4 to 9

6 to 4

3 to 5

9 to 4

0 to 2

2 to 3

9 to 8

4 to 2

Fig. 15. Adversarial examples for the network trained on the MNIST dataset by multi-
path search

speed limit 50 (pro-
hibitory) to speed limit
80 (prohibitory)

restriction ends (other)
to restriction ends (80)

no overtaking (trucks)
(prohibitory)
to speed
limit 80 (prohibitory)

give way (other)
priority road (other)

to

priority road (other) to
speed limit 30 (pro-
hibitory)

speed limit 70 (pro-
hibitory) to speed limit
120 (prohibitory)

(pro-
overtaking
no
hibitory) to go straight
(mandatory)

speed limit 50 (pro-
hibitory) to stop (other)

road narrows (danger)
to construction (danger)

ends

restriction
80
(other) to speed limit 80
(prohibitory)

no overtaking (trucks)
(prohibitory)
to speed
limit 80 (prohibitory)

overtaking

no
hibitory)
ends
(trucks)) (other)

(pro-
to restriction
(overtaking

priority at next intersec-
tion (danger) to speed
limit 30 (prohibitory)

uneven road (danger) to
traﬃc signal (danger)

danger
to
school crossing (danger)

(danger)

Fig. 16. Adversarial examples for the GTSRB dataset by single-path search

D Architectures of Neural Networks

Figure 17, Figure 18, Figure 19, and Figure 20 present architectures of the networks we
work with in this paper. The network for the ImageNet dataset is from [34].

Fig. 17. Architecture of the neural network for two-dimensional point classiﬁcation

Fig. 18. Architecture of the neural network for the MNIST dataset

Fig. 19. Architecture of the neural network for the CIFAR-10 dataset

Fig. 20. Architecture of the neural network for the GTSRB dataset

7
1
0
2
 
y
a
M
 
5
 
 
]
I

A
.
s
c
[
 
 
3
v
0
4
9
6
0
.
0
1
6
1
:
v
i
X
r
a

Safety Veriﬁcation of Deep Neural Networks(cid:63)

Xiaowei Huang, Marta Kwiatkowska, Sen Wang and Min Wu

Department of Computer Science, University of Oxford

Abstract. Deep neural networks have achieved impressive experimental results
in image classiﬁcation, but can surprisingly be unstable with respect to adversar-
ial perturbations, that is, minimal changes to the input image that cause the net-
work to misclassify it. With potential applications including perception modules
and end-to-end controllers for self-driving cars, this raises concerns about their
safety. We develop a novel automated veriﬁcation framework for feed-forward
multi-layer neural networks based on Satisﬁability Modulo Theory (SMT). We
focus on safety of image classiﬁcation decisions with respect to image manipu-
lations, such as scratches or changes to camera angle or lighting conditions that
would result in the same class being assigned by a human, and deﬁne safety
for an individual decision in terms of invariance of the classiﬁcation within a
small neighbourhood of the original image. We enable exhaustive search of the
region by employing discretisation, and propagate the analysis layer by layer. Our
method works directly with the network code and, in contrast to existing meth-
ods, can guarantee that adversarial examples, if they exist, are found for the given
region and family of manipulations. If found, adversarial examples can be shown
to human testers and/or used to ﬁne-tune the network. We implement the tech-
niques using Z3 and evaluate them on state-of-the-art networks, including regu-
larised and deep learning networks. We also compare against existing techniques
to search for adversarial examples and estimate network robustness.

1

Introduction

Deep neural networks have achieved impressive experimental results in image classiﬁ-
cation, matching the cognitive ability of humans [23] in complex tasks with thousands
of classes. Many applications are envisaged, including their use as perception modules
and end-to-end controllers for self-driving cars [15]. Let Rn be a vector space of images
(points) that we wish to classify and assume that f : Rn → C, where C is a (ﬁnite) set of
class labels, models the human perception capability, then a neural network classiﬁer is
a function ˆf (x) which approximates f (x) from M training examples {(xi, ci)}i=1,..,M. For
example, a perception module of a self-driving car may input an image from a camera
and must correctly classify the type of object in its view, irrespective of aspects such
as the angle of its vision and image imperfections. Therefore, though they clearly in-
clude imperfections, all four pairs of images in Figure 1 should arguably be classiﬁed
as automobiles, since they appear so to a human eye.

(cid:63) This work is

supported by the EPSRC Programme Grant on Mobile Autonomy
(EP/M019918/1). Part of this work was done while MK was visiting the Simons Institute for
the Theory of Computing.

Classiﬁers employed in vision tasks are typically multi-layer networks, which prop-
agate the input image through a series of linear and non-linear operators. They are
high-dimensional, often with millions of dimensions, non-linear and potentially dis-
continuous: even a small network, such as that trained to classify hand-written images
of digits 0-9, has over 60,000 real-valued parameters and 21,632 neurons (dimensions)
in its ﬁrst layer. At the same time, the networks are trained on a ﬁnite data set and
expected to generalise to previously unseen images. To increase the probability of cor-
rectly classifying such an image, regularisation techniques such as dropout are typically
used, which improves the smoothness of the classiﬁers, in the sense that images that are
close (within (cid:15) distance) to a training point are assigned the same class label.

automobile to bird

automobile to frog

automobile to airplane automobile to horse

Fig. 1. Automobile images (classiﬁed correctly) and their perturbed images (classiﬁed
wrongly)

Unfortunately, it has been observed in [13,36] that deep neural networks, includ-
ing highly trained and smooth networks optimised for vision tasks, are unstable with
respect to so called adversarial perturbations. Such adversarial perturbations are (min-
imal) changes to the input image, often imperceptible to the human eye, that cause the
network to misclassify the image. Examples include not only artiﬁcially generated ran-
dom perturbations, but also (more worryingly) modiﬁcations of camera images [22] that
correspond to resizing, cropping or change in lighting conditions. They can be devised
without access to the training set [29] and are transferable [19], in the sense that an ex-
ample misclassiﬁed by one network is also misclassiﬁed by a network with a diﬀerent
architecture, even if it is trained on diﬀerent data. Figure 1 gives adversarial pertur-
bations of automobile images that are misclassiﬁed as a bird, frog, airplane or horse
by a highly trained state-of-the-art network. This obviously raises potential safety con-
cerns for applications such as autonomous driving and calls for automated veriﬁcation
techniques that can verify the correctness of their decisions.

Safety of AI systems is receiving increasing attention, to mention [33,10], in view
of their potential to cause harm in safety-critical situations such as autonomous driving.
Typically, decision making in such systems is either solely based on machine learning,
through end-to-end controllers, or involves some combination of logic-based reasoning
and machine learning components, where an image classiﬁer produces a classiﬁcation,
say speed limit or a stop sign, that serves as input to a controller. A recent trend towards
“explainable AI” has led to approaches that learn not only how to assign the classiﬁca-
tion labels, but also additional explanations of the model, which can take the form of
a justiﬁcation explanation (why this decision has been reached, for example identify-
ing the features that supported the decision) [17,31]. In all these cases, the safety of a
decision can be reduced to ensuring the correct behaviour of a machine learning com-

ponent. However, safety assurance and veriﬁcation methodologies for machine learning
are little studied.

The main diﬃculty with image classiﬁcation tasks, which play a critical role in per-
ception modules of autonomous driving controllers, is that they do not have a formal
speciﬁcation in the usual sense: ideally, the performance of a classiﬁer should match
the perception ability and class labels assigned by a human. Traditionally, the correct-
ness of a neural network classiﬁer is expressed in terms of risk [37], deﬁned as the
probability of misclassiﬁcation of a given image, weighted with respect to the input
distribution µ of images. Similar (statistical) robustness properties of deep neural net-
work classiﬁers, which compute the average minimum distance to a misclassiﬁcation
and are independent of the data point, have been studied and can be estimated using
tools such as DeepFool [25] and cleverhans [27]. However, we are interested in the
safety of an individual decision, and to this end focus on the key property of the clas-
siﬁer being invariant to perturbations at a given point. This notion is also known as
pointwise robustness [18,12] or local adversarial robustness [21].

Contributions. In this paper we propose a general framework for automated veriﬁ-
cation of safety of classiﬁcation decisions made by feed-forward deep neural networks.
Although we work concretely with image classiﬁers, the techniques can be generalised
to other settings. For a given image x (a point in a vector space), we assume that there
is a (possibly inﬁnite) region η around that point that incontrovertibly supports the de-
cision, in the sense that all points in this region must have the same class. This region is
speciﬁed by the user and can be given as a small diameter, or the set of all points whose
salient features are of the same type. We next assume that there is a family of operations
∆, which we call manipulations, that specify modiﬁcations to the image under which the
classiﬁcation decision should remain invariant in the region η. Such manipulations can
represent, for example, camera imprecisions, change of camera angle, or replacement of
a feature. We deﬁne a network decision to be safe for input x and region η with respect
to the set of manipulations ∆ if applying the manipulations on x will not result in a class
change for η. We employ discretisation to enable a ﬁnite exhaustive search of the high-
dimensional region η for adversarial misclassiﬁcations. The discretisation approach is
justiﬁed in the case of image classiﬁers since they are typically represented as vectors of
discrete pixels (vectors of 8 bit RGB colours). To achieve scalability, we propagate the
analysis layer by layer, mapping the region and manipulations to the deeper layers. We
show that this propagation is sound, and is complete under the additional assumption of
minimality of manipulations, which holds in discretised settings. In contrast to existing
approaches [36,28], our framework can guarantee that a misclassiﬁcation is found if it
exists. Since we reduce veriﬁcation to a search for adversarial examples, we can achieve
safety veriﬁcation (if no misclassiﬁcations are found for all layers) or falsiﬁcation (in
which case the adversarial examples can be used to ﬁne-tune the network or shown to a
human tester).

We implement the techniques using Z3 [8] in a tool called DLV (Deep Learning Ver-
iﬁcation) [2] and evaluate them on state-of-the-art networks, including regularised and
deep learning networks. This includes image classiﬁcation networks trained for clas-
sifying hand-written images of digits 0-9 (MNIST), 10 classes of small colour images
(CIFAR10), 43 classes of the German Traﬃc Sign Recognition Benchmark (GTSRB)

[35] and 1000 classes of colour images used for the well-known imageNet large-scale
visual recognition challenge (ILSVRC) [4]. We also perform a comparison of the DLV
falsiﬁcation functionality on the MNIST dataset against the methods of [36] and [28],
focusing on the search strategies and statistical robustness estimation. The perturbed
images in Figure 1 are found automatically using our tool for the network trained on
the CIFAR10 dataset.

This invited paper is an extended and improved version of [20], where an extended

version including appendices can also be found.

2 Background on Neural Networks

We consider feed-forward multi-layer neural networks [14], henceforth abbreviated as
neural networks. Perceptrons (neurons) in a neural network are arranged in disjoint
layers, with each perceptron in one layer connected to the next layer, but no connection
between perceptrons in the same layer. Each layer Lk of a network is associated with
an nk-dimensional vector space DLk ⊆ Rnk , in which each dimension corresponds to
a perceptron. We write Pk for the set of perceptrons in layer Lk and nk = |Pk| is the
number of perceptrons (dimensions) in layer Lk.

Formally, a (feed-forward and deep) neural network N is a tuple (L, T, Φ), where
L = {Lk | k ∈ {0, ..., n}} is a set of layers such that layer L0 is the input layer and Ln
is the output layer, T ⊆ L × L is a set of sequential connections between layers such
that, except for the input and output layers, each layer has an incoming connection and
an outgoing connection, and Φ = {φk | k ∈ {1, ..., n}} is a set of activation functions
φk : DLk−1 → DLk , one for each non-input layer. Layers other than input and output
layers are called the hidden layers.

The network is fed an input x (point in DL0 ) through its input layer, which is then
propagated through the layers by successive application of the activation functions. An
activation for point x in layer k is the value of the corresponding function, denoted
αx,k = φk(φk−1(...φ1(x))) ∈ DLk , where αx,0 = x. For perceptron p ∈ Pk we write
αx,k(p) for the value of its activation on input x. For every activation αx,k and layer
k(cid:48) < k, we deﬁne Prek(cid:48) (αx,k) = {αy,k(cid:48) ∈ DLk(cid:48)
| αy,k = αx,k} to be the set of activations in
layer k(cid:48) whose corresponding activation in layer Lk is αx,k. The classiﬁcation decision
is made based on the activations in the output layer by, e.g., assigning to x the class
arg maxp∈Pn αx,n(p). For simplicity, we use αx,n to denote the class assigned to input x,
and thus αx,n = αy,n expresses that two inputs x and y have the same class.

The neural network classiﬁer N represents a function ˆf (x) which approximates
f (x) : DL0 → C, a function that models the human perception capability in labelling im-
ages with labels from C, from M training examples {(xi, ci)}i=1,..,M. Image classiﬁcation
networks, for example convolutional networks, may contain many layers, which can
be non-linear, and work in high dimensions, which for the image classiﬁcation prob-
lems can be of the order of millions. Digital images are represented as 3D tensors of
pixels (width, height and depth, the latter to represent colour), where each pixel is a dis-
crete value in the range 0..255. The training process determines real values for weights
used as ﬁlters that are convolved with the activation functions. Since it is diﬃcult to
approximate f with few samples in the sparsely populated high-dimensional space, to

increase the probability of classifying correctly a previously unseen image, various reg-
ularisation techniques such as dropout are employed. They improve the smoothness of
the classiﬁer, in the sense that points that are (cid:15)-close to a training point (potentially
inﬁnitely many of them) classify the same.

In this paper, we work with the code of the network and its trained weights.

3 Safety Analysis of Classiﬁcation Decisions

In this section we deﬁne our notion of safety of classiﬁcation decisions for a neural net-
work, based on the concept of a manipulation of an image, essentially perturbations that
a human observer would classify the same as the original image. Safety is deﬁned for
an individual classiﬁcation decision and is parameterised by the class of manipulations
and a neighbouring region around a given image. To ensure ﬁniteness of the search of
the region for adversarial misclassiﬁcations, we introduce so called “ladders”, nonde-
terministically branching and iterated application of successive manipulations, and state
the conditions under which the search is exhaustive.

Safety and Robustness Our method assumes the existence of a (possibly inﬁnite)
region η around a data point (image) x such that all points in the region are indistin-
guishable by a human, and therefore have the same true class. This region is understood
as supporting the classiﬁcation decision and can usually be inferred from the type of
the classiﬁcation problem. For simplicity, we identify such a region via its diameter d
with respect to some user-speciﬁed norm, which intuitively measures the closeness to
the point x. As deﬁned in [18], a network ˆf approximating human capability f is said
to be not robust at x if there exists a point y in the region η = {z ∈ DL0 | ||z − x|| ≤ d}
of the input layer such that ˆf (x) (cid:44) ˆf (y). The point y, at a minimal distance from x, is
known as an adversarial example. Our deﬁnition of safety for a classiﬁcation decision
(abbreviated safety at a point) follows he same intuition, except that we work layer
by layer, and therefore will identify such a region ηk, a subspace of DLk , at each layer
Lk, for k ∈ {0, ..., n}, and successively reﬁne the regions through the deeper layers. We
justify this choice based on the observation [11,23,24] that deep neural networks are
thought to compute progressively more powerful invariants as the depth increases. In
other words, they gradually transform images into a representation in which the classes
are separable by a linear classiﬁer.

Assumption 1 For each activation αx,k of point x in layer Lk, the region ηk(αx,k) con-
tains activations that the human observer believes to be so close to αx,k that they should
be classiﬁed the same as x.

Intuitively, safety for network N at a point x means that the classiﬁcation decision is
robust at x against perturbations within the region ηk(αx,k). Note that, while the pertur-
bation is applied in layer Lk, the classiﬁcation decision is based on the activation in the
output layer Ln.

Deﬁnition 1. [General Safety] Let ηk(αx,k) be a region in layer Lk of a neural network
N such that αx,k ∈ ηk(αx,k). We say that N is safe for input x and region ηk(αx,k), written
as N, ηk |= x, if for all activations αy,k in ηk(αx,k) we have αy,n = αx,n.

We remark that, unlike the notions of risk [37] and robustness of [18,12], we work
with safety for a speciﬁc point and do not account for the input distribution, but such
expectation measures can be considered, see Section 6 for comparison.

Manipulations A key concept of our framework is the notion of a manipulation, an
operator that intuitively models image perturbations, for example bad angles, scratches
or weather conditions, the idea being that the classiﬁcation decisions in a region of im-
ages close to it should be invariant under such manipulations. The choice of the type of
manipulation is dependent on the application and user-deﬁned, reﬂecting knowledge of
the classiﬁcation problem to model perturbations that should or should not be allowed.
Judicious choice of families of such manipulations and appropriate distance metrics is
particularly important. For simplicity, we work with operators δk : DLk → DLk over the
activations in the vector space of layer k, and consider the Euclidean (L2) and Manhattan
(L1) norms to measure the distance between an image and its perturbation through δk,
but the techniques generalise to other norms discussed in [18,19,12]. More speciﬁcally,
applying a manipulation δk(αx,k) to an activation αx,k will result in another activation
such that the values of some or all dimensions are changed. We therefore represent a
manipulation as a hyper-rectangle, deﬁned for two activations αx,k and αy,k of layer Lk
by rec(αx,k, αy,k) = ×p∈Pk [min(αx,k(p), αy,k(p)), max(αx,k(p), αy,k(p))]. The main chal-
lenge for veriﬁcation is the fact that the region ηk contains potentially an uncountable
number of activations. Our approach relies on discretisation in order to enable a ﬁnite
exploration of the region to discover and/or rule out adversarial perturbations.

For an activation αx,k and a set ∆ of manipulations, we denote by rec(∆, αx,k) the
polyhedron which includes all hyper-rectangles that result from applying some manip-
ulation in ∆ on αx,k, i.e., rec(∆, αx,k) = (cid:83)
δ∈∆ rec(αx,k, δ(αx,k)). Let ∆k be the set of all
possible manipulations for layer Lk. To ensure region coverage, we deﬁne valid manip-
ulation as follows.

Deﬁnition 2. Given an activation αx,k, a set of manipulations V(αx,k) ⊆ ∆k is valid if
αx,k is an interior point of rec(V(αx,k), αx,k), i.e., αx,k is in rec(V(αx,k), αx,k) and does
not belong to the boundary of rec(V(αx,k), αx,k).

Figure 2 presents an example of valid manipulations in two-dimensional space: each
arrow represents a manipulation, each dashed box represents a (hyper-)rectangle of the
corresponding manipulation, and activation αx,k is an interior point of the space from
the dashed boxes.

Since we work with discretised spaces, which is a reasonable assumption for im-
ages, we introduce the notion of a minimal manipulation. If applying a minimal manip-
ulation, it suﬃces to check for misclassiﬁcation just at the end points, that is, αx,k and
δk(αx,k). This allows an exhaustive, albeit impractical, exploration of the region in unit
steps.

A manipulation δ1

k(αx,k), written as δ1

k(αy,k) is ﬁner than δ2

k(αx,k), if any
activation in the hyper-rectangle of the former is also in the hyper-rectangle of the latter.
It is implied in this deﬁnition that αy,k is an activation in the hyper-rectangle of δ2
k(αx,k).
Moreover, we write δk,k(cid:48) (αx,k) for φk(cid:48) (...φk+1(δk(αx,k))), representing the corresponding
activation in layer k(cid:48) ≥ k after applying manipulation δk on the activation αx,k, where
δk,k(αx,k) = δk(αx,k).

k(αy,k) ≤ δ2

Fig. 2. Example of a set {δ1, δ2, δ3, δ4} of valid manipulations in a 2-dimensional space

Deﬁnition 3. A manipulation δk on an activation αx,k is minimal if there does not exist
k(αx,k) ≤ δk(αx,k), αy,k =
k and an activation αy,k such that δ1
manipulations δ1
k(αy,k), and αy,n (cid:44) αx,n and αy,n (cid:44) δk,n(αx,k).
k(αx,k), δk(αx,k) = δ2
δ1

k and δ2

Intuitively, a minimal manipulation does not have a ﬁner manipulation that results in
a diﬀerent classiﬁcation. However, it is possible to have diﬀerent classiﬁcations before
and after applying the minimal manipulation, i.e., it is possible that δk,n(αx,k) (cid:44) αx,n. It
is not hard to see that the minimality of a manipulation implies that the class change in
its associated hyper-rectangle can be detected by checking the class of the end points
αx,k and δk(αx,k).

Bounded Variation Recall that we apply manipulations in layer Lk, but check the
classiﬁcation decisions in the output layer. To ensure ﬁnite, exhaustive coverage of
the region, we introduce a continuity assumption on the mapping from space DLk to
the output space DLn , adapted from the concept of bounded variation [9]. Given an
activation αx,k with its associated region ηk(αx,k), we deﬁne a “ladder” on ηk(αx,k) to
be a set ld of activations containing αx,k and ﬁnitely many, possibly zero, activations
from ηk(αx,k). The activations in a ladder can be arranged into an increasing order
αx,k = αx0,k < αx1,k < ... < αx j,k such that every activation αxt,k ∈ ld appears once and
has a successor αxt+1,k such that αxt+1,k = δk(αxt,k) for some manipulation δk ∈ V(αxt,k).
For the greatest element αx j,k, its successor should be outside the region ηk(αx,k), i.e.,
αx j+1,k (cid:60) ηk(αx,k). Given a ladder ld, we write ld(t) for its t + 1-th activation, ld[0..t] for
the preﬁx of ld up to the t + 1-th activation, and last(ld) for the greatest element of ld.
Figure 3 gives a diagrammatic explanation on the ladders.

Deﬁnition 4. Let L(ηk(αx,k)) be the set of ladders in ηk(αx,k). Then the total variation
of the region ηk(αx,k) on the neural network with respect to L(ηk(αx,k)) is

V(N; ηk(αx,k)) =

sup
ld∈L(ηk(αx,k))

(cid:88)

diﬀn(αxt,n, αxt+1,n)

αxt ,k∈ld\{last(ld)}

where diﬀn : DLn × DLn → {0, 1} is given by diﬀn(αx,n, αy,n) = 0 if αx,n = αy,n and 1
otherwise. We say that the region ηk(αx,k) is a bounded variation if V(N; ηk(αx,k)) < ∞,
and are particularly interested in the case when V(N; rk(αy,k)) = 0, which is called a
0-variation.

Fig. 3. Examples of ladders in region ηk(αx,k). Starting from αx,k = αx0,k, the activations
αx1,k...αx j,k form a ladder such that each consecutive activation results from some valid
manipulation δk applied to a previous activation, and the ﬁnal activation αx j,k is outside
the region ηk(αx,k).

The set L(ηk(αx,k)) is complete if, for any ladder ld ∈ L(ηk(αx,k)) of j+1 activations,
any element ld(t) for 0 ≤ t ≤ j, and any manipulation δk ∈ V(ld(t)), there exists a ladder
ld(cid:48) ∈ L(ηk(αx,k)) such that ld(cid:48)[0..t] = ld[0..t] and ld(cid:48)(t + 1) = δk(ld(t)). Intuitively, a
complete ladder is a complete tree, on which each node represents an activation and
each branch of a node corresponds to a valid manipulation. From the root αx,k, every
path of the tree leading to a leaf is a ladder. Moreover, the set L(ηk(αx,k)) is covering if
the polyhedra of all activations in it cover the region ηk(αx,k), i.e.,
(cid:91)

(cid:91)

ηk(αx,k) ⊆

rec(V(αxt,k), αxt,k).

(1)

ld∈L(ηk(αx,k))

αxt ,k∈ld\{last(ld)}

Based on the above, we have the following deﬁnition of safety with respect to a set
of manipulations. Intuitively, we iteratively and nondeterministically apply manipula-
tions to explore the region ηk(αx,k), and safety means that no class change is observed
by successive application of such manipulations.

Deﬁnition 5. [Safety wrt Manipulations] Given a neural network N, an input x and a
set ∆k of manipulations, we say that N is safe for input x with respect to the region ηk
and manipulations ∆k, written as N, ηk, ∆k |= x, if the region ηk(αx,k) is a 0-variation for
the set L(ηk(αx,k)) of its ladders, which is complete and covering.

It is straightforward to note that general safety in the sense of Deﬁnition 1 implies

safety wrt manipulations, in the sense of Deﬁnition 5.

Theorem 1. Given a neural network N, an input x, and a region ηk, we have that
N, ηk |= x implies N, ηk, ∆k |= x for any set of manipulations ∆k.

In the opposite direction, we require the minimality assumption on manipulations.

Theorem 2. Given a neural network N, an input x, a region ηk(αx,k) and a set ∆k of
manipulations, we have that N, ηk, ∆k |= x implies N, ηk |= x if the manipulations in ∆k
are minimal.

Theorem 2 means that, under the minimality assumption over the manipulations, an
exhaustive search through the complete and covering ladder tree from L(ηk(αx,k)) can
ﬁnd adversarial examples, if any, and enable us to conclude that the network is safe
at a given point if none are found. Though computing minimal manipulations is not
practical, in discrete spaces by iterating over increasingly reﬁned manipulations we are
able to rule out the existence of adversarial examples in the region. This contrasts with
partial exploration according to, e.g., [25,12]; for comparison see Section 7.

4 The Veriﬁcation Framework

In this section we propose a novel framework for automated veriﬁcation of safety of
classiﬁcation decisions, which is based on search for an adversarial misclassiﬁcation
within a given region. The key distinctive distinctive features of our framework com-
pared to existing work are: a guarantee that a misclassiﬁcation is found if it exists; the
propagation of the analysis layer by layer; and working with hidden layers, in addi-
tion to input and output layers. Since we reduce veriﬁcation to a search for adversarial
examples, we can achieve safety veriﬁcation (if no misclassiﬁcations are found for all
layers) or falsiﬁcation (in which case the adversarial examples can be used to ﬁne-tune
the network or shown to a human tester).

4.1 Layer-by-Layer Analysis

We ﬁrst consider how to propagate the analysis layer by layer, which will involve reﬁn-
ing manipulations through the hidden layers. To facilitate such analysis, in addition to
the activation function φk : DLk−1 → DLk we also require a mapping ψk : DLk → DLk−1
in the opposite direction, to represent how a manipulated activation of layer Lk aﬀects
the activations of layer Lk−1. We can simply take ψk as the inverse function of φk. In
order to propagate safety of regions ηk(αx,k) at a point x into deeper layers, we assume
the existence of functions ηk that map activations to regions, and impose the following
restrictions on the functions φk and ψk, shown diagrammatically in Figure 4.

Deﬁnition 6. The functions {η0, η1, ..., ηn} and {ψ1, ..., ψn} mapping activations to re-
gions are such that

1. ηk(αx,k) ⊆ DLk , for k = 0, ..., n,
2. αx,k ∈ ηk(αx,k), for k = 0, ..., n, and
3. ηk−1(αi,k−1) ⊆ ψk(ηk(αx,k)) for all k = 1, ..., n.

Intuitively, the ﬁrst two conditions state that each function ηk assigns a region around
the activation αx,k, and the last condition that mapping the region ηk from layer Lk to

Lk−1 via ψk should cover the region ηk−1. The aim is to compute functions ηk+1, ..., ηn
based on ηk and the neural network.

The size and complexity of a deep neural network generally means that determining
whether a given set ∆k of manipulations is minimal is intractable. To partially counter
this, we deﬁne a reﬁnement relation between safety wrt manipulations for consecutive
layers in the sense that N, ηk, ∆k |= x is a reﬁnement of N, ηk−1, ∆k−1 |= x if all ma-
nipulations δk−1 in ∆k−1 are reﬁned by a sequence of manipulations δk from the set ∆k.
Therefore, although we cannot theoretically conﬁrm the minimality of ∆k, they are re-
ﬁned layer by layer and, in discrete settings, this process can be bounded from below
by the unit step. Moreover, we can work gradually from a speciﬁc layer inwards until
an adversarial example is found, ﬁnishing processing when reaching the output layer.

The reﬁnement framework is given in Figure 5. The arrows represent the implication

Fig. 4. Layer by layer analysis according to Deﬁnition 6

relations between the safety notions and are labelled with conditions if needed. The
goal of the reﬁnements is to ﬁnd a chain of implications to justify N, η0 |= x. The
fact that N, ηk |= x implies N, ηk−1 |= x is due to the constraints in Deﬁnition 6 when
ψk = φ−1
k . The fact that N, ηk |= x implies N, ηk, ∆k |= x follows from Theorem 1. The
implication from N, ηk, ∆k |= x to N, ηk |= x under the condition that ∆k is minimal is
due to Theorem 2.

We now deﬁne the notion of reﬁnability of manipulations between layers. Intu-
itively, a manipulation in layer Lk−1 is reﬁnable in layer Lk if there exists a sequence of
manipulations in layer Lk that implements the manipulation in layer Lk−1.

Deﬁnition 7. A manipulation δk−1(αy,k−1) is reﬁnable in layer Lk if there exist activa-
tions αx0,k, ..., αx j,k ∈ DLk and valid manipulations δ1
k ∈ V(αx j−1,k) such
that αy,k = αx0,k, δk−1,k(αy,k−1) = αx j,k, and αxt,k = δt
k(αxt−1,k) for 1 ≤ t ≤ j. Given a
neural network N and an input x, the manipulations ∆k are a reﬁnement by layer of
ηk−1, ∆k−1 and ηk if, for all αy,k−1 ∈ ηk−1(αz,k−1), all its valid manipulations δk−1(αy,k−1)
are reﬁnable in layer Lk.

k ∈ V(αx0,k), ..., δ j

Fig. 5. Reﬁnement framework

We have the following theorem stating that the reﬁnement of safety notions is im-

plied by the “reﬁnement by layer” relation.

Theorem 3. Assume a neural network N and an input x. For all layers k ≥ 1, if manip-
ulations ∆k are reﬁnement by layer of ηk−1, ∆k−1 and ηk, then we have that N, ηk, ∆k |= x
implies N, ηk−1, ∆k−1 |= x.

We note that any adversarial example of safety wrt manipulations N, ηk, ∆k |= x
is also an adversarial example for general safety N, ηk |= x. However, an adversarial
example αx,k for N, ηk |= x at layer k needs to be checked to see if it is an adversarial
example of N, η0 |= x, i.e. for the input layer. Recall that Prek(cid:48) (αx,k) is not necessarily
unique. This is equivalent to checking the emptiness of Pre0(αx,k) ∩ η0(αx,0). If we start
the analysis with a hidden layer k > 0 and there is no speciﬁcation for η0, we can instead
consider checking the emptiness of {αy,0 ∈ Pre0(αx,k) | αy,n (cid:44) αx,n}.

4.2 The Veriﬁcation Method

We summarise the theory developed thus far as a search-based recursive veriﬁcation
procedure given below. The method is parameterised by the region ηk around a given
point and a family of manipulations ∆k. The manipulations are speciﬁed by the user
for the classiﬁcation problem at hand, or alternatively can be selected automatically, as
described in Section 4.4. The vector norm to identify the region can also be speciﬁed
by the user and can vary by layer. The method can start in any layer, with analysis
propagated into deeper layers, and terminates when a misclassiﬁcation is found. If an
adversarial example is found by manipulating a hidden layer, it can be mapped back to
the input layer, see Section 4.5.

Algorithm 1 Given a neural network N and an input x, recursively perform the fol-
lowing steps, starting from some layer l ≥ 0. Let k ≥ l be the current layer under
consideration.

1. determine a region ηk such that if k > l then ηk and ηk−1 satisfy Deﬁnition 6;
2. determine a manipulation set ∆k such that if k > l then ∆k is a reﬁnement by layer

of ηk−1, ∆k−1 and ηk according to Deﬁnition 7;

3. verify whether N, ηk, ∆k |= x,
(a) if N, ηk, ∆k |= x then

i. report that N is safe at x with respect to ηk(αx,k) and ∆k, and
ii. continue to layer k + 1;

(b) if N, ηk, ∆k (cid:54)|= x, then report an adversarial example.

We implement Algorithm 1 by utilising satisﬁability modulo theory (SMT) solvers.
The SMT problem is a decision problem for logical formulas with respect to combina-
tions of background theories expressed in classical ﬁrst-order logic with equality. For
checking reﬁnement by layer, we use the theory of linear real arithmetic with existen-
tial and universal quantiﬁers, and for veriﬁcation within a layer (0-variation) we use the
same theory but without universal quantiﬁcation. The details of the encoding and the ap-
proach taken to compute the regions and manipulations are included in Section 4.4. To
enable practical veriﬁcation of deep neural networks, we employ a number of heuristics
described in the remainder of this section.

4.3 Feature Decomposition and Discovery

While Theorem 1 and 2 provide a ﬁnite way to verify safety of neural network clas-
siﬁcation decisions, the high-dimensionality of the region ηk(αx,k) can make any com-
putational approach impractical. We therefore use the concept of a feature to parti-
tion the region ηk(αx,k) into a set of features, and exploit their independence and low-
dimensionality. This allows us to work with state-of-the-art networks that have hun-
dreds, and even thousands, of dimensions.

Intuitively, a feature deﬁnes for each point in the high-dimensional space DLk the
most explicit salient feature it has, e.g., the red-coloured frame of a street sign in Fig-
ure 10. Formally, for each layer Lk, a feature function fk : DLk → P(DLk ) assigns a small
region for each activation αx,k in the space DLk , where P(DLk ) is the set of subspaces of
DLk . The region fk(αx,k) may have lower dimension than that of Dk. It has been argued,
in e.g. [16] for natural images, that natural data, for example natural images and sound,
forms a high-dimensional manifold, which embeds tangled manifolds to represent their
features. Feature manifolds usually have lower dimension than the data manifold, and a
classiﬁcation algorithm is to separate a set of tangled manifolds. By assuming that the
appearance of features is independent, we can manipulate them one by one regardless
of the manipulation order, and thus reduce the problem of size O(2d1+...+dm ) into a set of
smaller problems of size O(2d1 ), ..., O(2dm ).

The analysis of activations in hidden layers, as performed by our method, provides
an opportunity to discover the features automatically. Moreover, deﬁning the feature
fk on each activation as a single region corresponding to a speciﬁc feature is without
loss of generality: although an activation may include multiple features, the indepen-
dence relation between features suggests the existence of a total relation between these
features. The function fk essentially deﬁnes for each activation one particular feature,
subject to certain criteria such as explicit knowledge, but features can also be explored
in parallel.

Every feature fk(αy,k) is identiﬁed by a pre-speciﬁed number dimsk, f of dimensions.
Let dimsk( fk(αy,k)) be the set of dimensions selected according to some heuristic. Then
we have that

(cid:40)

fk(αy,k)(p) =

ηk(αx,k)(p),
[αy,k(p), αy,k(p)] otherwise.

if p ∈ dimsk( fk(αy,k))

(2)

Moreover, we need a set of features to partition the region ηk(αx,k) as follows.

Deﬁnition 8. A set { f1, ..., fm} of regions is a partition of ηk(αx,k), written as π(ηk(αx,k)),
if dimsk, f ( fi) ∩ dimsk, f ( f j) = ∅ for i, j ∈ {1, ..., m} and ηk(αx,k) = ×m

i=1 fi.
Given such a partition π(ηk(αx,k)), we deﬁne a function acts(x, k) by

acts(x, k) = {αy,k ∈ x | x ∈ π(ηk(αx,k))}

(3)

which contains one point for each feature. Then, we reduce the checking of 0-variation
of a region ηk(αx,k) to the following problems:

– checking whether the points in acts(x, k) have the same class as αx,k, and
– checking the 0-variation of all features in π(ηk(αx,k)).

In the above procedure, the checking of points in acts(x, k) can be conducted ei-
ther by following a pre-speciﬁed sequential order (single-path search) or by exhaus-
tively searching all possible orders (multi-path search). In Section 5 we demonstrate
that single-path search according to the prominence of features can enable us to ﬁnd
adversarial examples, while multi-path search may ﬁnd other examples whose distance
to the original input image is smaller.

4.4 Selection of Regions and Manipulations

The procedure summarised in Algorithm 1 is typically invoked for a given image in the
input layer, but, providing insight about hidden layers is available, it can start from any
layer Ll in the network. The selection of regions can be automated, as described below.
For the ﬁrst layer to be considered, i.e., k = l, the region ηk(αx,k) is deﬁned by ﬁrst
selecting the subset of dimsk dimensions from Pk whose activation values are furthest
away from the average activation value of the layer1. Intuitively, the knowledge repre-
sented by these activations is more explicit than the knowledge represented by the other
dimensions, and manipulations over more explicit knowledge are more likely to result
in a class change. Let avgk = ((cid:80)
p∈Pk αx,k(p))/nk be the average activation value of layer
Lk. We let dimsk(ηk(αx,k)) be the ﬁrst dimsk dimensions p ∈ Pk with the greatest values
|αx,k(p) − avg| among all dimensions, and then deﬁne

ηk(αx,k) = ×p∈dimsk(ηk(αx,k))[αx,k(p) − sp ∗ mp, αx,k(p) + sp ∗ mp]

(4)

i.e., a dimsk-polytope containing the activation αx,k, where sp represents a small span
and mp represents the number of such spans. Let Vk = {sp, mp | p ∈ dimsk(ηk(αx,k))} be
a set of variables.

Let d be a function mapping from dimsk(ηk(αx,k)) to {−1, 0, +1} such that {d(p) (cid:44)
0 | p ∈ dimsk(ηk(αx,k))} (cid:44) ∅, and D(dimsk(ηk(αx,k))) be the set of such functions. Let a
manipulation δd

k be

k (αy,k)(p) =
δd





αy,k(p) − sp if d(p) = −1
if d(p) = 0
αy,k(p)
αy,k(p) + sp if d(p) = +1

(5)

1 We also considered other approaches, including computing derivatives up to several layers, but

for the experiments we conduct they are less eﬀective.

for activation αy,k ∈ ηk(αx,k). That is, each manipulation changes a subset of the dimen-
sions by the span sp, according to the directions given in d. The set ∆k is deﬁned by col-
lecting the set of all such manipulations. Based on this, we can deﬁne a set L(ηk(αx,k))
of ladders, which is complete and covering.

Determining the region ηk according to ηk−1 Given ηk−1(αx,k−1) and the functions φk
and ψk, we can automatically determine a region ηk(αx,k) satisfying Deﬁnition 6 using
the following approach. According to the function φk, the activation value αx,k(p) of
perceptron p ∈ Pk is computed from activation values of a subset of perceptrons in
Pk−1. We let Vars(p) ⊆ Pk−1 be such a set of perceptrons. The selection of dimensions
in dimsk(ηk(αx,k)) depends on dimsk−1(ηk−1(αx,k−1)) and φk, by requiring that, for every
p(cid:48) ∈ dimsk−1(ηk−1(αx,k−1)), there is at least one dimension p ∈ dimsk(ηk(αx,k)) such that
p(cid:48) ∈ Vars(p). We let

dimsk(ηk(αx,k)) = {arg max

{ |αx,k(p) − avgk| | p(cid:48) ∈ Vars(p)} | p(cid:48) ∈ dimsk−1(ηk−1(αx,k−1))}

p∈Pk

(6)
Therefore, the restriction of Deﬁnition 6 can be expressed with the following formula:

∀αy,k−1 ∈ ηk(αx,k−1) : αy,k−1 ∈ ψk(ηk(αx,k)).

(7)

We omit the details of rewriting αy,k−1 ∈ ηk(αx,k−1) and αy,k−1 ∈ ψk(ηk(αx,k)) into
Boolean expressions, which follow from standard techniques. Note that this expres-
sion includes variables in Vk, Vk−1 and αy,k−1. The variables in Vk−1 are ﬁxed for a given
ηk−1(αx,k−1). Because such a region ηk(αx,k) always exists, a simple iterative procedure
can be invoked to gradually increase the size of the region represented with variables in
Vk to eventually satisfy the expression.

Determining the manipulation set ∆k according to ηk(αx,k), ηk−1(αx,k−1), and ∆k−1
The values of the variables Vk obtained from the satisﬁability of Eqn (7) yield a deﬁni-
tion of manipulations using Eqn (5). However, the obtained values for span variables sp
do not necessarily satisfy the “reﬁnement by layer” relation as deﬁned in Deﬁnition 7.
Therefore, we need to adapt the values for the variables Vk while, at the same time,
retaining the region ηk(αx,k). To do so, we could rewrite the constraint in Deﬁnition 7
into a formula, which can then be solved by an SMT solver. But, in practice, we notice
that such precise computations easily lead to overly small spans sp, which in turn result
in an unacceptable amount of computation needed to verify the relation N, ηk, ∆k |= x.
To reduce computational cost, we work with a weaker “reﬁnable in layer Lk” notion,
parameterised with respect to precision ε. Given two activations αy,k and αm,k, we use
dist(αy,k, αm,k) to represent their distance.

Deﬁnition 9. A manipulation δk−1(αy,k−1) is reﬁnable in layer Lk with precision ε > 0
if there exists a sequence of activations αx0,k, ..., αx j,k ∈ DLk and valid manipulations
k ∈ V(αx j−1,k) such that αy,k = αx0,k, δk−1,k(αy,k−1) ∈ rec(αx j−1,k, αx j,k),
k ∈ V(αx0,k), ..., δd
δ1
dist(αx j−1,k, αx j,k) ≤ (cid:15), and αxt,k = δt
k(αxt−1,k) for 1 ≤ t ≤ j. Given a neural network N
and an input x, the manipulations ∆k are a reﬁnement by layer of ηk, ηk−1, ∆k−1 with

precision ε if, for all αy,k−1 ∈ ηk−1(αx,k−1), all its legal manipulations δk−1(αy,k−1) are
reﬁnable in layer Lk with precision ε.

Comparing with Deﬁnition 7, the above deﬁnition replaces δk−1,k(αy,k−1) = αx j,k
with δk−1,k(αy,k−1) ∈ rec(αx j−1,k, αx j,k) and dist(αx j−1,k, αx j,k) ≤ ε. Intuitively, instead of
requiring a manipulation to reach the activation δk−1,k(αy,k−1) precisely, this deﬁnition
allows for each δk−1,k(αy,k−1) to be within the hyper-rectangle rec(αx j−1,k, αx j,k). To ﬁnd
suitable values for Vk according to the approximate “reﬁnement-by-layer” relation, we
use a variable h to represent the maximal number of manipulations of layer Lk used to
express a manipulation in layer k − 1. The value of h (and variables sp and np in Vk)
are automatically adapted to ensure the satisﬁability of the following formula, which
expresses the constraints of Deﬁnition 9:

∀αy,k−1 ∈ ηk(αx,k−1)∀d ∈ D(dimsk(ηk(αx,k−1)))∀δd
∃αy0,k, ..., αyh,k ∈ ηk(αx,k) : αy0,k = αy,k ∧ (cid:86)h−1
(cid:87)h−1

k−1 ∈ Vk−1(αy,k−1)
k (αyt,k)∧

t=0 αyt+1,k = δd

t=0 (δd

k−1,k(αy,k) ∈ rec(αyt,k, αyt+1,k) ∧ dist(αyt,k, αyt+1,k) ≤ ε).
It is noted that sp and mp for p ∈ dimsk(ηk(αx,k)) are employed when expressing δd
k .
The manipulation δd
k−1 by considering the corresponding relation
between dimensions in dimsk(ηk(αx,k)) and dimsk−1(ηk−1(αx,k−1)).

k is obtained from δd

(8)

Adversarial examples shown in Figures 8, 9, and 10 were found using single-path

search and automatic selection of regions and manipulations.

4.5 Mapping Back to Input Layer

When manipulating the hidden layers, we may need to map back an activation in layer
k to the input layer to obtain an input image that resulted in misclassiﬁcation, which
involves computation of Pre0(αy,k) described next. To check the 0-variation of a region
ηk(αx,k), we need to compute diﬀn(αx,n, αy,n) for many points αy,x in ηk(αx,k), where
diﬀn : DLn × DLn → {0, 1} is given by diﬀn(αx,n, αy,n) = 0 if αx,n = αy,n and 1 otherwise.
Because αx,n is known, we only need to compute αy,n. We can compute αy,n by ﬁnding
a point αy,0 ∈ Pre0(αy,k) and then using the neural network to predict the value αy,n. It
should be noted that, although Pre0(αy,k) may include more than one point, all points
have the same class, so any point in Pre0(αy,k) is suﬃcient for our purpose.

To compute αy,0 from αy,k, we use functions ψk, ψk−1, ..., ψ1 and compute points

αy,k−1, αy,k−2, ..., αy,0 such that

αy, j−1 = ψ j(αy, j) ∧ αy, j−1 ∈ η j−1(αx, j−1)

for 1 ≤ j ≤ k. The computation relies on an SMT solver to encode the functions
ψk, ψk−1, ..., ψ1 if they are piecewise linear functions, and by taking the corresponding
inverse functions directly if they are sigmoid functions. It is possible that, for some
1 ≤ j ≤ k, no point can be found by SMT solver, which means that the point αy,k does
not have any corresponding point in the input layer. We can safely discard these points.
The maxpooling function ψ j selects from every m ∗ m dimensions the maximal element
for some m > 0. The computation of the maxpooling layer ψ j−1 is combined with the
computation of the next layer ψ j, that is, ﬁnding αy, j−2 with the following expression
∃αx, j−1 : αy, j−2 = ψ j−1(ψ j(αy, j)) ∧ αy, j−1 ∈ η j−1(αx, j−1) ∧ αy, j−2 ∈ η j−2(αx, j−2)

This is to ensure that in the expression αy, j−2 = ψ j−1(ψ j(αy, j)) we can reuse m ∗ m − 1
elements in αx, j−2 and only need to replace the maximal element.

Figures 8, 9, and 10 show images obtained by mapping back from the ﬁrst hidden

layer to the input layer.

5 Experimental Results

The proposed framework has been implemented as a software tool called DLV (Deep
Learning Veriﬁcation) [2] written in Python, see Appendix of [20] for details of input
parameters and how to use the tool. The SMT solver we employ is Z3 [8], which has
Python APIs. The neural networks are built from a widely-used neural networks library
Keras [3] with a deep learning package Theano [6] as its backend.

We validate DLV on a set of experiments performed for neural networks trained for
classiﬁcation based on a predeﬁned multi-dimensional surface (small size networks),
as well as image classiﬁcation (medium size networks). These networks respectively
use two representative types of layers: fully connected layers and convolutional layers.
They may also use other types of layers, e.g., the ReLU layer, the pooling layer, the
zero-padding layer, and the dropout layer. The ﬁrst three demonstrate the single-path
search functionality on the Euclidean (L2) norm, whereas the fourth (GTSRB) multi-
path search for the L1 and L2 norms.

The experiments are conducted on a MacBook Pro laptop, with 2.7 GHz Intel Core

i5 CPU and 8 GB memory.

Two-Dimensional Point Classiﬁcation Network To demonstrate exhaustive veriﬁca-
tion facilitated by our framework, we consider a neural network trained for classifying
points above and below a two-dimensional curve shown in red in Figure 6 and Figure 7.
The network has three fully-connected hidden layers with the ReLU activation func-
tion. The input layer has two perceptrons, every hidden layer has 20 perceptrons, and
the output layer has two perceptrons. The network is trained with 5,000 points sampled
from the provided two-dimensional space, and has an accuracy of more than 99%.

For a given input x = (3.59, 1.11), we start from the input layer and deﬁne a region

around this point by taking unit steps in both directions

η0(αx,0) = [3.59 − 1.0, 3.59 + 1.0] × [1.11 − 1.0, 1.11 + 1.0] = [2.59, 4.59] × [0.11, 2.11]

The manipulation set ∆0 is shown in Figure 6: there are 9 points, of which the point in
the middle represents the activation αx,0 and the other 8 points represent the activations
resulting from applying one of the manipulations in ∆0 on αx,0. Note that, although there
are class changes in the region η0(αx,0), the manipulation set ∆0 is not able to detect such
changes. Therefore, we have that N, η0, ∆0 |= x.

Now consider layer k = 1. To obtain the region η1(αx,1), the tool selects two dimen-

sions p1,17, p1,19 ∈ P1 in layer L1 with indices 17 and 19 and computes

η1(αx,1) = [αx,1(p1,17) − 3.6, αx,1(p1,17) + 3.6] × [αx,1(p1,19) − 3.52, αx,1(p1,19) + 3.52]

The manipulation set ∆1, after mapping back to the input layer with function ψ1, is given
as Figure 7. Note that η1 and η0 satisfy Deﬁnition 6, and ∆1 is a reﬁnement by layer of

Fig. 6. Input layer

Fig. 7. First hidden layer

η0, ∆0 and η1. We can see that a class change can be detected (represented as the red
coloured point). Therefore, we have that N, η1, ∆1 (cid:54)|= x.

Image Classiﬁcation Network for the MNIST Handwritten Image Dataset The
well-known MNIST image dataset contains images of size 28 × 28 and one channel
and the network is trained with the source code given in [5]. The trained network is of
medium size with 600,810 parameters, has an accuracy of more than 99%, and is state-
of-the-art. It has 12 layers, within which there are 2 convolutional layers, as well as
layers such as ReLU, dropout, fully-connected layers and a softmax layer. The images
are preprocessed to make the value of each pixel within the bound [0, 1].

Given an image x, we start with layer k = 1 and the parameter set to at most 150
dimensions (there are 21632 dimensions in layer L1). All ηk, ∆k for k ≥ 2 are computed
according to the simple heuristic mentioned in Section 4.2 and satisfy Deﬁnition 6 and
Deﬁnition 7. For the region η1(αx,1), we allow changes to the activation value of each
selected dimension that are within [-1,1]. The set ∆1 includes manipulations that can
change the activation value for a subset of the 150 dimensions, by incrementing or
decrementing the value for each dimension by 1. The experimental results show that
for most of the examples we can ﬁnd a class change within 100 dimensional changes
in layer L1, by comparing the number of pixels that have changed, and some of them
can have less than 30 dimensional changes. Figure 8 presents examples of such class
changes for layer L1. We also experiment on images with up to 40 dimensional changes
in layer L1; the tool is able to check the entire network, reaching the output layer and
claiming that N, ηk, ∆k |= x for all k ≥ 1. While training of the network takes half an
hour, ﬁnding an adversarial example takes up to several minutes.

Image Classiﬁcation Network for the CIFAR-10 Small Image Dataset We work
with a medium size neural network, trained with the source code from [1] for more than
12 hours on the well-known CIFAR10 dataset. The inputs to the network are images
of size 32 × 32 with three channels. The trained network has 1,250,858 real-valued pa-

8 to 0

2 to 1

4 to 2

2 to 3

9 to 4

6 to 5

4 to 6

9 to 7

0 to 8

7 to 9

Fig. 8. Adversarial examples for a neural network trained on MNIST

rameters and includes convolutional layers, ReLU layers, max-pooling layers, dropout
layers, fully-connected layers, and a softmax layer.

As an illustration of the type of perturbations that we are investigating, consider the
images in Figure 9, which correspond to the parameter setting of up to 25, 45, 65, 85,
105, 125, 145 dimensions, respectively, for layer k = 1. The manipulations change the
activation values of these dimensions. Each image is obtained by mapping back from
the ﬁrst hidden layer and represents a point close to the boundary of the correspond-
ing region. The relation N, η1, ∆1 |= x holds for the ﬁrst 7 images, but fails for the last
one and the image is classiﬁed as a truck. Intuitively, our choice of the region η1(αx,1)
identiﬁes the subset of dimensions with most extreme activations, taking advantage of
the analytical capability of the ﬁrst hidden layer. A higher number of selected dimen-
sions implies a larger region in which we apply manipulations, and, more importantly,
suggests a more dramatic change to the knowledge represented by the activations when
moving to the boundary of the region.

Fig. 9. An illustrative example of mapping back to input layer from the Cifar-10
mataset: the last image classiﬁes as a truck.

We also work with 500 dimensions and otherwise the same experimental parameters
as for MNIST. Figure 13 in Appendix of [20] gives 16 pairs of original images (clas-
siﬁed correctly) and perturbed images (classiﬁed wrongly). We found that, while the
manipulations lead to human-recognisable modiﬁcations to the images, the perturbed
images can be classiﬁed wrongly by the network. For each image, ﬁnding an adversarial
example ranges from seconds to 20 minutes.

Image Classiﬁcation Network for the ImageNet Dataset We also conduct experi-
ments on a large image classiﬁcation network trained on the popular ImageNet dataset.
The images are of size 224 × 224 and have three channels. The network is the model
of the 16-layer network [34], called VGG16, used by the VGG team in the ILSVRC-

2014 competition, downloaded from [7]. The trained network has 138,357,544 real-
valued parameters and includes convolutional layers, ReLU layers, zero-padding lay-
ers, dropout layers, max-pooling layers, fully-connected layers, and a softmax layer.
The experimental parameters are the same as for the previous two experiments, except
that we work with 20,000 dimensions.

Several additional pairs of original and perturbed images are included in Figure 14
in Appendix of [20]. In Figure 10 we also give two examples of street sign images. The
image on the left is reported unsafe for the second layer with 6346 dimensional changes
(0.2% of the 3,211,264 dimensions of layer L2). The one on the right is reported safe
for 20,000 dimensional changes of layer L2. It appears that more complex manipula-
tions, involving more dimensions (perceptrons), are needed in this case to cause a class
change.

Fig. 10. Street sign images. Found an adversarial example for the left image (class
changed into bird house), but cannot ﬁnd an adversarial example for the right image
for 20,000 dimensions.

5.1 The German Traﬃc Sign Recognition Benchmark (GTSRB)

We evaluate DLV on the GTSRB dataset (by resizing images into size 32*32), which
has 43 classes. Figure 11 presents the results for the multi-path search. The ﬁrst case
(approx. 20 minutes to manipulate) is a stop sign (conﬁdence 1.0) changed into a speed
limit of 30 miles, with an L1 distance of 0.045 and L2 distance of 0.19. The conﬁdence
of the manipulated image is 0.79. The second, easy, case (seconds to manipulate) is a
speed limit of 80 miles (conﬁdence 0.999964) changed into a speed limit of 30 miles,
with an L1 distance of 0.004 and L2 distance of 0.06. The conﬁdence of the manipulated
image is 0.99 (a very high conﬁdence of misclassiﬁcation). Also, a “go right” sign can
be easily manipulated into a sign classiﬁed as “go straight”.

Figure 16 in [20] presents additional adversarial examples obtained when selecting

single-path search.

6 Comparison

We compare our approach with two existing approaches for ﬁnding adversarial exam-
ples, i.e., fast gradient sign method (FGSM) [36] and Jacobian saliency map algorithm

“stop”
to “30m speed limit”

“80m speed limit”
to “30m speed limit”

“go right”
to “go straight”

Fig. 11. Adversarial examples for the network trained on the GTSRB dataset by multi-
path search

(JSMA) [28]. FGSM calculates the optimal attack for a linear approximation of the net-
work cost, whereas DLV explores a proportion of dimensions in the feature space in the
input or hidden layers. JSMA ﬁnds a set of dimensions in the input layer to manipulate,
according to the linear approximation (by computing the Jacobian matrix) of the model
from current output to a nominated target output. Intuitively, the diﬀerence between
DLV’s manipulation and JSMA is that DLV manipulates over features discovered in
the activations of the hidden layer, while JSMA manipulates according to the partial
derivatives, which depend on the parameters of the network.

Experiment 1. We randomly select an image from the MNIST dataset. Figure 12
shows some intermediate and ﬁnal images obtained by running the three approaches:
FGSM, JSMA and DLV. FGSM has a single parameter, (cid:15), where a greater (cid:15) represents a
greater perturbation along the gradient of cost function. Given an (cid:15), for each input exam-
ple a perturbed example is returned and we test whether it is an adversarial example by
checking for misclassiﬁcation against the original image. We gradually increase the pa-
rameter (cid:15) = 0.05, 0.1, 0.2, 0.3, 0.4, with the last image (i.e., (cid:15) = 0.4) witnessing a class
change, see the images in the top row of Figure 12. FGSM can eﬃciently manipulate a
set of images, but it requires a relatively large manipulation to ﬁnd a misclassiﬁcation.
For the JSMA approach, we conduct the experiment on a setting with parameters
(cid:15) = 0.1 and θ = 1.0. The parameter (cid:15) = 0.1 means that we only consider adversarial ex-
amples changing no more than 10% of all the pixels, which is suﬃcient here. As stated
in [29], the parameter θ = 1.0, which allows a maximum change to every pixel, can en-
sure that fewer pixels need to be changed. The approach takes a series of manipulations
to gradually lead to a misclassiﬁcation, see the images in the middle row of Figure 12.
The misclassiﬁed image has an L2 (Euclidean) distance of 0.17 and an L1 (Manhattan)
distance of 0.03 from the original image. While JSMA can ﬁnd adversarial examples
with smaller distance from the original image, it takes longer to manipulate a set of
images.

Both FGSM and JSMA follow their speciﬁc heuristics to deterministically explore
the space of images. However, in some cases, the heuristics may omit better adversarial
examples. In the experiment for DLV, instead of giving features a speciﬁc order and
manipulating them sequentially, we allow the program to nondeterministically choose
features. This is currently done by MCTS (Monte Carlo Tree Search), which has a theo-
retical guarantee of convergence for inﬁnite sampling. Therefore, the high-dimensional
space is explored by following many diﬀerent paths. By taking the same manipulation

Fig. 12. FGSM vs. JSMA vs. DLV, where FGSM and JSMA search a single path and
DLV multiple paths. Top row: Original image (7) perturbed deterministically by FGSM
with (cid:15) = 0.05, 0.1, 0.2, 0.3, 0.4, with the ﬁnal image (i.e., (cid:15) = 0.4) misclassiﬁed as
9. Middle row: Original image (7) perturbed deterministically by JSMA with (cid:15) = 0.1
and θ = 1.0. We show even numbered images of the 12 produced by JSMA, with the
ﬁnal image misclassiﬁed as 3. Bottom row: Original image (7) perturbed nondetermin-
istically by DLV, for the same manipulation on a single pixel as that of JSMA (i.e.,
sp ∗ mp = 1.0) and working in the input layer, with the ﬁnal image misclassiﬁed as 3.

on a single pixel as that of JSMA (i.e., sp ∗ mp = 1.0) and working on the input layer,
DLV is able to ﬁnd another perturbed image that is also classiﬁed as 3 but has a smaller
distance (L2 distance is 0.14 and L1 distance is 0.02) from the original image, see the
images in the last row of Figure 12. In terms of the time taken to ﬁnd an adversarial
example, DLV may take longer than JSMA, since it searches over many diﬀerent paths.

L2
L1
%

FGSM ((cid:15) = 0.1)
0.08
0.06
17.5%

(0.4) DLV (dimsl = 75) (150) (450) JSMA (θ = 0.1) (0.4)
(0.2)
0.11
0.22 0.27
0.32
0.15
0.02
0.06 0.09
0.12
0.25
99%
79% 98%
70.9% 97.2%

0.19
0.04
52.3%
Table 1. FGSM vs. DLV (on a single path) vs. JSMA

0.11
0.02
92%

Experiment 2. Table 1 gives a comparison of robustness evaluation of the three
appraoches on the MNIST dataset. For FGSM, we vary the input parameter (cid:15) accord-
ing to the values {0.1, 0.2, 0.4}. For DLV, we select regions as deﬁned in Section 4.4
on a single path (by deﬁning a speciﬁc order on the features and manipulating them
sequentially) for the ﬁrst hidden layer. The experiment is parameterised by varying the
maximal number of dimensions to be changed, i.e., dimsl ∈ {75, 150, 450}. For each
input image, an adversarial example is returned, if found, by manipulating fewer than
the maximal number of dimensions. When the maximal number has been reached, DLV
will report failure and return the last perturbed example. For JSMA, the experiment is
conducted by letting θ take the value in the set {0.1, 0.4} and setting (cid:15) to 1.0.

We collect three statistics, i.e., the average L1 distance over the adversarial exam-
ples, the average L2 distance over the adversarial examples, and the success rate of

ﬁnding adversary examples. Let Ld(x, δ(x)) for d ∈ {1, 2} be the distance between an
input x and the returned perturbed image δ(x), and diﬀ(x, δ(x)) ∈ {0, 1} be a Boolean
value representing whether x and δ(x) have diﬀerent classes. We let

and

(cid:80)

Ld =

x in test set diﬀ(x, δ(x)) × Ld(x, δ(x))
x in test set diﬀ(x, δ(x))

(cid:80)

% =

(cid:80)

x in test set diﬀ(x, δ(x))
the number of examples in test set

We note that the approaches yield diﬀerent perturbed examples δ(x).

The test set size is 500 images selected randomly. DLV takes 1-2 minutes to manip-
ulate each input image in MNIST. JSMA takes about 10 minutes for each image, but it
works for 10 classes, so the running time is similar to that of DLV. FGSM works with a
set of images, so it is the fastest per image.

For the case when the success rates are very high, i.e., 97.2% for FGSM with (cid:15) =
0.4, 98% for DLV with dimsl = 450, and 99% for JSMA with θ = 0.4, JSMA has the
smallest average distances, followed by DLV, which has smaller average distances than
FGSM on both L1 and L2 distances.

We mention that a smaller distance leading to a misclassiﬁcation may result in a
lower rate of transferability [29], meaning that a misclassiﬁcation can be harder to wit-
ness on another model trained on the same (or a small subset of) data-set.

7 Related Work

AI safety is recognised an an important problem, see e.g., [33,10]. An early veriﬁcation
approach for neural networks was proposed in [30], where, using the notation of this
paper, safety is deﬁned as the existence, for all inputs in a region η0 ∈ DL0 , of a corre-
sponding output in another region ηn ⊆ DLn . They encode the entire network as a set of
constraints, approximating the sigmoid using constraints, which can then be solved by a
SAT solver, but their approach only works with 6 neurons (3 hidden neurons). A similar
idea is presented in [32]. In contrast, we work layer by layer and obtain much greater
scalability. Since the ﬁrst version of this paper appeared [20], another constraint-based
method has been proposed in [21] which improves on [30]. While they consider more
general correctness properties than this paper, they can only handle the ReLU activation
functions, by extending the Simplex method to work with the piecewise linear ReLU
functions that cannot be expressed using linear programming. This necessitates a search
tree (instead of a search path as in Simplex), for which a heuristic search is proposed
and shown to be complete. The approach is demonstrated on networks with 300 ReLU
nodes, but as it encodes the full network it is unclear whether it can be scaled to work
with practical deep neural networks: for example, the MNIST network has 630,016
ReLU nodes. They also handle continuous spaces directly without discretisation, the
beneﬁts of which are not yet clear, since it is argued in [19] that linear behaviour in
high-dimensional spaces is suﬃcient to cause adversarial examples.

Concerns about the instability of neural networks to adversarial examples were ﬁrst
raised in [13,36], where optimisation is used to identify misclassiﬁcations. A method

for computing the perturbations is also proposed, which is based on box-constrained
optimisation and is approximate in view of non-convexity of the search space. This
work is followed by [19], which introduced the much faster FGSM method, and [22],
which employed a compromise between the two (iterative, but with a smaller number
of iterations than [36]). In our notation, [19] uses a deterministic, iterative manipula-
tion δ(x) = x + (cid:15) sign((cid:79)x J(x, αx,n)), where x is an image in matrix representation, (cid:15) is a
hyper-parameter that can be tuned to get diﬀerent manipulated images, and J(x, αx,n) is
the cross-entropy cost function of the neural network on input x and class αx,n. There-
fore, their approach will test a set of discrete points in the region η0(αx,0) of the input
layer. Therefore these manipulations will test a lasso-type ladder tree (i.e., a ladder tree
without branches) L(ηk(αx,k)), which does not satisfy the covering property. In [26],
instead of working with a single image, an evolutionary algorithm is employed for a
population of images. For each individual image in the current population, the manip-
ulation is the mutation and/or crossover. While mutations can be nondeterministic, the
manipulations of an individual image are also following a lasso-type ladder tree which
is not covering. We also mention that [38] uses several distortions such as JPEG com-
pression, thumbnail resizing, random cropping, etc, to test the robustness of the trained
network. These distortions can be understood as manipulations. All these attacks do not
leverage any speciﬁc properties of the model family, and do not guarantee that they will
ﬁnd a misclassiﬁed image in the constraint region, even if such an image exists.

The notion of robustness studied in [18] has some similarities to our deﬁnition of
safety, except that the authors work with values averaged over the input distribution µ,
which is diﬃcult to estimate accurately in high dimensions. As in [36,22], they use opti-
misation without convergence guarantees, as a result computing only an approximation
to the minimal perturbation. In [12] pointwise robustness is adopted, which corresponds
to our general safety; they also use a constraint solver but represent the full constraint
system by reduction to a convex LP problem, and only verify an approximation of the
property. In contrast, we work directly with activations rather than an encoding of ac-
tivation functions, and our method exhaustively searches through the complete ladder
tree for an adversarial example by iterative and nondeterministic application of manip-
ulations. Further, our deﬁnition of a manipulation is more ﬂexible, since it allows us to
select a subset of dimensions, and each such subset can have a diﬀerent region diameter
computed with respect to a diﬀerent norm.

8 Conclusions

This paper presents an automated veriﬁcation framework for checking safety of deep
neural networks that is based on a systematic exploration of a region around a data point
to search for adversarial manipulations of a given type, and propagating the analysis into
deeper layers. Though we focus on the classiﬁcation task, the approach also generalises
to other types of networks. We have implemented the approach using SMT and vali-
dated it on several state-of-the-art neural network classiﬁers for realistic images. The
results are encouraging, with adversarial examples found in some cases in a matter of
seconds when working with few dimensions, but the veriﬁcation process itself is expo-
nential in the number of features and has prohibitive complexity for larger images. The

performance and scalability of our method can be signiﬁcantly improved through par-
allelisation. It would be interesting to see if the notions of regularity suggested in [24]
permit a symbolic approach, and whether an abstraction reﬁnement framework can be
formulated to improve the scalability and computational performance.

Acknowledgements. This paper has greatly beneﬁted from discussions with sev-
eral researchers. We are particularly grateful to Martin Fraenzle, Ian Goodfellow and
Nicolas Papernot.

References

1. CIFAR10 model for Keras. https://github.com/fchollet/keras/blob/master/examples/cifar10 cnn.py.
2. DLV. https://github.com/verideep/dlv.
3. Keras. https://keras.io.
4. Large scale visual recognition challenge. http://www.image-net.org/challenges/LSVRC/.
5. MNIST CNN network. https://github.com/fchollet/keras/blob/master/examples/mnist cnn.py.
6. Theano. http://deeplearning.net/software/theano/.
7. VGG16 model for Keras. https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3.
8. Z3. http://rise4fun.com/z3.
9. Luigi Ambrosio, Nicola Fusco, and Diego Pallara. Functions of bounded variation and free
discontinuity problems. Oxford Mathematical Monographs. Oxford University Press, 2000.
10. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dario
Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e.
Concrete problems in AI safety. CoRR, abs/1606.06565, 2016.

11. Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, and Tomaso
Poggio. Unsupervised learning of invariant representations. Theoretical Computer Science,
633:112–121, 2016.

12. Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori,
CoRR,

and Antonio Criminisi. Measuring neural net robustness with constraints.
abs/1605.07262, 2016. To appear in NIPS.

13. Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov,
Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In
ECML/PKDD 2013, pages 387–402, 2013.

14. Christopher M Bishop. Neural networks for pattern recognition. Oxford university press,

1995.

15. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp,
Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang,
Jake Zhao, and Karol Zieba. End to end learning for self-driving cars. arXiv:1604.07316,
2016.

16. Gunnar E. Carlsson, Tigran Ishkhanov, Vin de Silva, and Afra Zomorodian. On the local
behavior of spaces of natural images. International Journal of Computer Vision, 76(1), 2008.
17. Lisa Anne Hendricks Dong Huk Park, Zeynep Akata, Bernt Schiele, Trevor Darrell, and
Marcus Rohrbach. Attentive explanations: Justifying decisions and pointing to the evidence.
arxiv.org/abs/1612.04757, 2016.

18. Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classiﬁers’ robustness to

adversarial perturbations. CoRR, abs/1502.02590, 2015.

19. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing ad-

versarial examples. CoRR, abs/1412.6572, 2014.

20. Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety veriﬁcation of deep

neural networks. https://arxiv.org/abs/1610.06940, 2016.

21. Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer. Reluplex: An
eﬃcient SMT solver for verifying deep neural networks. In CAV 2017, 2017. To appear.
22. Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical

world. arXiv:1607.02533, 2016.

23. Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. Nature, 521:436–444,

24. St´ephane Mallat. Understanding deep convolutional networks. Philosohical Transactions of

2015.

the Royal Society A, 2016.

25. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple

and accurate method to fool deep neural networks. CoRR, abs/1511.04599, 2015.

26. Anh Nguyen, Jason Yosinski, and Jeﬀ Clune. Deep neural networks are easily fooled: High
conﬁdence predictions for unrecognizable images. In Computer Vision and Pattern Recog-
nition (CVPR ’15), 2015.

27. Nicolas Papernot, Ian Goodfellow, Ryan Sheatsley, Reuben Feinman, and Patrick McDaniel.
cleverhans v1.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768,
2016.

28. Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and
Ananthram Swami. The limitations of deep learning in adversarial settings. In Proceedings
of the 1st IEEE European Symposium on Security and Privacy, 2015.

29. Nicolas Papernot, Patrick Drew McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik,
and Ananthram Swami. Practical black-box attacks against deep learning systems using
adversarial examples. CoRR, abs/1602.02697, 2016.

30. Luca Pulina and Armando Tacchella. An abstraction-reﬁnement approach to veriﬁcation of

artiﬁcial neural networks. In CAV 2010, pages 243–257, 2010.

31. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust you?”: Ex-
In ACM SIGKDD International Conference on

plaining the predictions of any classiﬁer.
Knowledge Discovery and Data Mining (KDD2016), 2016.

32. Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker. Towards veriﬁcation
of artiﬁcial neural networks. In 18th Workshop on Methoden und Beschreibungssprachen
zur Modellierung und Veriﬁkation von Schaltungen und Systemen” (MBMV), pages 30–40,
2015.

33. Sanjit A. Seshia and Dorsa Sadigh.

Towards veriﬁed artiﬁcial intelligence. CoRR,

abs/1606.08514, 2016.

34. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv:1409.1556, 2014.

35. J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking
machine learning algorithms for traﬃc sign recognition. Neural Networks, 32:323–332,
2012.

36. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian
In International

Intriguing properties of neural networks.

Goodfellow, and Rob Fergus.
Conference on Learning Representations (ICLR-2014), 2014.

37. Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in Neural
Information Processing Systems 4, [NIPS Conference, Denver, Colorado, USA, December
2-5, 1991], pages 831–838, 1991.

38. Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness

of deep neural networks via stability training. In CVPR 2016, 2016.

A Input Parameters and Experimental Setup

The DLV tool accepts as input a network N and an image x, and has the following input
parameters:

– an integer l ∈ [0, n] indicating the starting layer Ll,
– an integer dimsl ≥ 1 indicating the maximal number of dimensions that need to be

considered in layer Ll,

– the values of variables sp and mp in Vl; for simplicity, we ask that, for all dimensions
p that will be selected by the automated procedure, sp and mp have the same values,

– the precision ε ∈ [0, ∞),
– an integer dimsk, f indicating the number of dimensions for each feature; for sim-
plicity, we ask that every feature has the same number of dimensions and dimsk, f =
dimsk(cid:48), f for all layers k and k(cid:48), and

– type of search: either heuristic (single-path) or Monte Carlo Tree Search (MCTS)

(multi-path).

A.1 Two-Dimensional Point Classiﬁcation Network

A.2 Network for the MNIST Dataset

– l = 0
– dimsl = 2,
– sp = 1.0 and mp = 1.0,
– ε = 0.1, and
– dimsk, f = 2

– l = 1
– dimsl = 150,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

– l = 1
– dimsl = 500,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

– l = 1
– dimsl = 1000,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

A.3 Network for the CIFAR-10 Dataset

A.4 Network for the GTSRB Dataset

A.5 Network for the ImageNet Dataset

– l = 2
– dimsl = 20, 000,
– sp = 1.0 and mp = 1.0,
– ε = 1.0, and
– dimsk, f = 5

B Additional Adversarial Examples Found for the CIFAR-10,

ImageNet, and MNIST Networks

Figure 13 and Figure 14 present additional adversarial examples for the CIFAR-10 and
ImageNet networks by single-path search. Figure 15 presents adversarial examples for
the MNIST network by multi-path search.

automobile to bird

automobile to frog

automobile to airplane automobile to horse

airplane to dog

airplane to deer

airplane to truck

airplane to cat

truck to frog

truck to cat

ship to bird

ship to airplane

ship to truck

horse to cat

horse to automobile

horse to truck

Fig. 13. Adversarial examples for a neural network trained on the CIFAR-10 dataset by
single-path search

C Additional Adversarial Examples for the German Traﬃc Sign

Recognition Benchmark (GTSRB)

Figure 16 presents adversarial examples obtained when selecting single-path search.

labrador to life boat

rhodesian ridgeback to malinois

boxer to rhodesian ridgeback

great pyrenees to kuvasz

Fig. 14. Adversarial Examples for the VGG16 Network Trained on the imageNet
Dataset By Single-Path Search

9 to 4

8 to 3

5 to 3

4 to 9

5 to 3

7 to 3

9 to 4

9 to 4

2 to 3

1 to 8

8 to 5

0 to 3

7 to 2

8 to 3

3 to 2

9 to 7

3 to 2

4 to 9

6 to 4

3 to 5

9 to 4

0 to 2

2 to 3

9 to 8

4 to 2

Fig. 15. Adversarial examples for the network trained on the MNIST dataset by multi-
path search

speed limit 50 (pro-
hibitory) to speed limit
80 (prohibitory)

restriction ends (other)
to restriction ends (80)

no overtaking (trucks)
(prohibitory)
to speed
limit 80 (prohibitory)

give way (other)
priority road (other)

to

priority road (other) to
speed limit 30 (pro-
hibitory)

speed limit 70 (pro-
hibitory) to speed limit
120 (prohibitory)

(pro-
overtaking
no
hibitory) to go straight
(mandatory)

speed limit 50 (pro-
hibitory) to stop (other)

road narrows (danger)
to construction (danger)

ends

restriction
80
(other) to speed limit 80
(prohibitory)

no overtaking (trucks)
(prohibitory)
to speed
limit 80 (prohibitory)

overtaking

no
hibitory)
ends
(trucks)) (other)

(pro-
to restriction
(overtaking

priority at next intersec-
tion (danger) to speed
limit 30 (prohibitory)

uneven road (danger) to
traﬃc signal (danger)

danger
to
school crossing (danger)

(danger)

Fig. 16. Adversarial examples for the GTSRB dataset by single-path search

D Architectures of Neural Networks

Figure 17, Figure 18, Figure 19, and Figure 20 present architectures of the networks we
work with in this paper. The network for the ImageNet dataset is from [34].

Fig. 17. Architecture of the neural network for two-dimensional point classiﬁcation

Fig. 18. Architecture of the neural network for the MNIST dataset

Fig. 19. Architecture of the neural network for the CIFAR-10 dataset

Fig. 20. Architecture of the neural network for the GTSRB dataset

