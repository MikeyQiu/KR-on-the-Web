6
1
0
2
 
v
o
N
 
4
1
 
 
]
T
S
.
h
t
a
m

[
 
 
2
v
8
8
9
8
0
.
5
0
6
1
:
v
i
X
r
a

On Explore-Then-Commit Strategies

Aurélien Garivier∗
Institut de Mathématiques de Toulouse; UMR5219
Université de Toulouse; CNRS
UPS IMT, F-31062 Toulouse Cedex 9, France
aurelien.garivier@math.univ-toulouse.fr

Emilie Kaufmann
Univ. Lille, CNRS, Centrale Lille, Inria SequeL
UMR 9189, CRIStAL - Centre de Recherche en Informatique Signal et Automatique de Lille
F-59000 Lille, France
emilie.kaufmann@univ-lille1.fr

Tor Lattimore
University of Alberta
116 St & 85 Ave, Edmonton, AB T6G 2R3, Canada
tor.lattimore@gmail.com

Abstract

We study the problem of minimising regret in two-armed bandit problems with
Gaussian rewards. Our objective is to use this simple setting to illustrate that
strategies based on an exploration phase (up to a stopping time) followed by
exploitation are necessarily suboptimal. The results hold regardless of whether
or not the difference in means between the two arms is known. Besides the
main message, we also reﬁne existing deviation inequalities, which allow us to
design fully sequential strategies with ﬁnite-time regret guarantees that are (a)
asymptotically optimal as the horizon grows and (b) order-optimal in the minimax
sense. Furthermore we provide empirical evidence that the theory also holds in
practice and discuss extensions to non-gaussian and multiple-armed case.

1

Introduction

It is now a very frequent issue for companies to optimise their daily proﬁts by choosing between
one of two possible website layouts. A natural approach is to start with a period of A/B Testing
(exploration) during which the two versions are uniformly presented to users. Once the testing is
complete, the company displays the version believed to generate the most proﬁt for the rest of the
month (exploitation). The time spent exploring may be chosen adaptively based on past observations,
but could also be ﬁxed in advance. Our contribution is to show that strategies of this form are
much worse than if the company is allowed to dynamically select which website to display without
restrictions for the whole month.

Our analysis focuses on a simple sequential decision problem played over T time-steps. In time-step
t ∈ 1, 2, . . . , T the agent chooses an action At ∈ {1, 2} and receives a normally distributed reward

∗This work was partially supported by the CIMI (Centre International de Mathématiques et d’Informatique)
Excellence program while Emilie Kaufmann visited Toulouse in November 2015. The authors acknowledge the
support of the French Agence Nationale de la Recherche (ANR), under grants ANR-13-BS01-0005 (project
SPADRO) and ANR-13-CORD-0020 (project ALICIA).

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Zt ∼ N (µAt, 1) where µ1, µ2 ∈ R are the unknown mean rewards for actions 1 and 2 respectively.
The goal is to ﬁnd a strategy π (a way of choosing each action At based on past observation) that
maximises the cumulative reward over T steps in expectation, or equivalently minimises the regret

Rπ

µ(T ) = T max {µ1, µ2} − Eµ

(1)

(cid:35)

µAt

.

(cid:34) T

(cid:88)

t=1

This framework is known as the multi-armed bandit problem, which has many applications and
has been studied for almost a century [Thompson, 1933]. Although this setting is now quite well
understood, the purpose of this article is to show that strategies based on distinct phases of exploration
and exploitation are necessarily suboptimal. This is an important message because exploration
followed by exploitation is the most natural approach and is often implemented in applications
(including the website optimisation problem described above). Moreover, strategies of this kind
have been proposed in the literature for more complicated settings [Auer and Ortner, 2010, Perchet
and Rigollet, 2013, Perchet et al., 2015]. Recent progress on optimal exploration policies (e.g., by
Garivier and Kaufmann [2016]) could have suggested that well-tuned variants of two-phase strategies
might be near-optimal. We show, on the contrary, that optimal strategies for multi-armed bandit
problems must be fully-sequential, and in particular should mix exploration and exploitation. It is
known since the work of Wald [1945] on simple hypothese testing that sequential procedures can lead
to signiﬁcant gains. Here, the superiority of fully sequential procedures is consistent with intuition: if
one arm ﬁrst appears to be better, but if subsequent observations are disappointing, the obligation to
commit at some point can be restrictive. In this paper, we give a crisp and precise description of how
restrictive it is: it leads to regret asympotically twice as large on average. The proof of this result
combines some classical techniques of sequential analysis and of the bandit literature.

We study two settings, one when the gap ∆ = |µ1 − µ2| is known and the other when it is not.
The most straight-forward strategy in the former case is to explore each action a ﬁxed number
of times n and subsequently exploit by choosing the action that appeared best while exploring.
It is easy to calculate the optimal n and consequently show that this strategy suffers a regret of
Rπ
µ(T ) ∼ 4 log(T )/∆. A more general approach is to use a so-called Explore-Then-Commit (ETC)
strategy, following a nomenclature introduced by Perchet et al. [2015]. An ETC strategy explores
each action alternately until some data-dependent stopping time and subsequently commits to a single
action for the remaining time-steps. We show in Theorem 2 that by using a sequential probability ratio
test (SPRT) it is possible to design an ETC strategy for which Rπ
µ(T ) ∼ log(T )/∆, which improves
on the above result by a factor of 4. We also prove a lower bound showing that no ETC strategy can
improve on this result. Surprisingly it is possible to do even better by using a fully sequential strategy
inspired by the UCB algorithm for multi-armed bandits [Katehakis and Robbins, 1995]. We design a
new strategy for which Rπ
µ(T ) ∼ log(T )/(2∆), which improves on the ﬁxed-design strategy by a
factor of 8 and on SPRT by a factor of 2. Again we prove a lower bound showing that no strategy can
improve on this result.

For the case where ∆ is unknown, ﬁxed-design strategies are hopeless because there is no reasonable
tuning for the exploration budget n. However, it is possible to design an ETC strategy for unknown
gaps. Our approach uses a modiﬁed ﬁxed-budget best arm identiﬁcation (BAI) algorithm in its
exploration phase (see e.g., Even-Dar et al. [2006], Garivier and Kaufmann [2016]) and chooses the
recommended arm for the remaining time-steps. In Theorem 5 we show that a strategy based on
this idea satisﬁes Rπ
µ(T ) ∼ 4 log(T )/∆, which again we show is optimal within the class of ETC
strategies. As before, strategies based on ETC are suboptimal by a factor of 2 relative to the optimal
rates achieved by fully sequential strategies such as UCB, which satisﬁes Rπ
µ(T ) ∼ 2 log(T )/∆
[Katehakis and Robbins, 1995].

In a nutshell, strategies based on ﬁxed-design or ETC are necessarily suboptimal. That this failure
occurs even in the simple setting considered here is a strong indicator that they are suboptimal in
more complicated settings. Our main contribution, presented in more details in Section 2, is to fully
characterise the achievable asymptotic regret when ∆ is either known or unknown and the strategies
are either ﬁxed-design, ETC or fully sequential. All upper bounds have explicit ﬁnite-time forms,
which allow us to derive optimal minimax guarantees. For the lower bounds we give a novel and
generic proof of all results. All proofs contain new, original ideas that we believe are fundamental to
the understanding of sequential analysis.

2

2 Notation and Summary of Results

s=1

We assume that the horizon T is known to the agent. The optimal action is a∗ = arg max(µ1, µ2), its
mean reward is µ∗ = µa∗ , and the gap between the means is ∆ = |µ1 − µ2|. Let H = R2 be the set
of all possible pairs of means, and H∆ = (cid:8)µ ∈ R2 : |µ1 − µ2| = ∆(cid:9). For i ∈ {1, 2} and n ∈ N let
ˆµi,n be the empirical mean of the ith action based on the ﬁrst n samples. Let At be the action chosen
in time-step t and Ni(t) = (cid:80)t
1 {As = i} be the number of times the ith action has been chosen
after time-step t. We denote by ˆµi(t) = ˆµi,Ni(t) the empirical mean of the ith arm after time-step t.
A strategy is denoted by π, which is a function from past actions/rewards to a distribution over the
next actions. An ETC strategy is governed by a sampling rule (which determines which arm to sample
at each step), a stopping rule (which speciﬁes when to stop the exploration phase) and a decision
rule indicating which arm is chosen in the exploitation phase. As we consider two-armed, Gaussian
bandits with equal variances, we focus here on uniform sampling rules, which have been shown
in Kaufmann et al. [2014] to be optimal in that setting. For this reason, we deﬁne an ETC strategy as
a pair (τ, ˆa), where τ is an even stopping time with respect to the ﬁltration (Ft = σ(Z1, . . . , Zt))t
and ˆa ∈ {1, 2} is Fτ -measurable. In all the ETC strategies presented in this paper, the stopping time
τ depends on the horizon T (although this is not reﬂected in the notation). At time t, the action

picked by the ETC strategy is At =






if t ≤ τ and t is odd ,
if t ≤ τ and t is even ,

1
2
ˆa otherwise .

The regret for strategy π, given in Eq. (1), depends on T and µ. Assuming, for example that µ1 =
2 + (T − τ )+1 {ˆa = 2}
µ2 + ∆, then an ETC strategy π chooses the suboptimal arm N2(T ) = τ ∧T
times, and the regret Rπ

µ(T ) = ∆Eµ[N2(T )] thus satisﬁes

∆Eµ[(τ ∧ T )/2] ≤ Rπ

µ(T ) ≤ (∆/2)Eµ[τ ∧ T ] + ∆T Pµ(τ ≤ T, ˆa (cid:54)= a∗) .

(2)

We denote the set of all ETC strategies by ΠETC. A ﬁxed-design strategy is and ETC strategy for
which there exists an integer n such that τ = 2n almost surely, and the set of all such strategies is
denoted by ΠDETC. The set of all strategies is denoted by ΠALL. For S ∈ {H, H∆}, we are interested
in strategies π that are uniformly efﬁcient on S, in the sense that

∀µ ∈ S, ∀α > 0, Rπ

µ(T ) = o(T α).

(3)

We show in this paper that any uniformly efﬁcient strategy in Π
has a regret at least equal to C Π
S log(T )/|µ1 − µ2|(1 − oT (1))
for every parameter µ ∈ S, where C Π
S is given in the adjacent
table. Furthermore, we prove that these results are tight. In
each case, we propose a uniformly efﬁcient strategy matching
this bound. In addition, we prove a tight and non-asymptotic regret bound which also implies, in
particular, minimax rate-optimality.

ΠALL ΠETC ΠDETC
2

H
H∆ 1/2

NA

1

4

4

The paper is organised as follows. First we consider ETC and ﬁxed-design strategies when ∆ known
and unknown (Section 3). We then analyse fully sequential strategies that interleave exploration and
exploitation in an optimal way (Section 4). For known ∆ we present a novel algorithm that exploits
the additional information to improve the regret. For unknown ∆ we brieﬂy recall the well-known
results, but also propose a new regret analysis of the UCB* algorithm, a variant of UCB that can
be traced back to Lai [1987], for which we also obtain order-optimal minimax regret. Numerical
experiments illustrate and empirically support our results in Section 5. We conclude with a short
discussion on non-uniform exploration, and on models with more than 2 arms, possibly non Gaussian.
All the proofs are given in the supplementary material. In particular, our simple, uniﬁed proof for all
the lower bounds is given in Appendix A.

3 Explore-Then-Commit Strategies

Fixed Design Strategies for Known Gaps. As a warm-up we start with the ﬁxed-design ETC
setting where ∆ is known and where the agent chooses each action n times before committing for the
remainder.

3

The optimal decision rule is obviously ˆa =
arg maxi ˆµi,n with ties broken arbitrarily. The formal
description of the strategy is given in Algorithm 1,
where W denotes the Lambert function implicitly de-
ﬁned for y > 0 by W (y) exp(W (y)) = y. We denote
the regret associated to the choice of n by Rn
µ(T ). The
following theorem is not especially remarkable except
that the bound is sufﬁciently reﬁned to show certain
negative lower-order terms that would otherwise not
be apparent.
Theorem 1. Let µ ∈ H∆, and let

2W (cid:0)T 2∆4/(32π)(cid:1)/∆2(cid:109)
(cid:108)

input: T and ∆
n :=
for k ∈ {1, . . . , n} do

choose A2k−1 = 1 and A2k = 2

end for
ˆa := arg maxi ˆµi,n
for t ∈ {2n + 1, . . . , T } do

choose At = ˆa

end for
Algorithm 1: FB-ETC algorithm

n =

(cid:24) 2
∆2 W

(cid:19)(cid:25)

(cid:18) T 2∆4
32π
√

.

Then Rn

µ(T ) ≤

4
∆

log

(cid:19)

(cid:18) T ∆2
4.46

2
∆

−

log log

(cid:19)

(cid:18) T ∆2
√
2π
4

+ ∆

√

µ(T ) ≤ 2.04

T + ∆.

whenever T ∆2 > 4
Furthermore, for all ε > 0, T ≥ 1 and n ≤ 4(1 − ε) log(T )/∆2,
(cid:18)

2πe, and Rn

µ(T ) ≤ T ∆/2 + ∆ otherwise. In all cases, Rn

(cid:19) (cid:18)

(cid:19)

Rn

µ(T ) ≥

1 −

2
n∆2

1 −

8 log(T )
∆2T

∆T ε
2(cid:112)π log(T )

.

As Rn

µ(T ) ≥ n∆, this entails that

inf
1≤n≤T

Rn

µ(T ) ∼ 4 log(T )/∆.

The proof of Theorem 1 is in Appendix B. Note that the "asymptotic lower bound" 4 log(T )/∆ is
actually not a lower bound, even up to an additive constant: Rn
µ(T ) − 4 log(T )/∆ → −∞ when
T → ∞. Actually, the same phenomenon applies many other cases, and it should be no surprise that,
in numerical experiments, some algorithm reach a regret smaller than Lai and Robbins asymptotic
lower bound, as was already observed in several articles (see e.g. Garivier et al. [2016]). Also note
that the term ∆ at the end of the upper bound is necessary: if ∆ is large, the problem is statistically
so simple that one single observation is sufﬁcient to identify the best arm; but that observation cannot
be avoided.

Explore-Then-Commit Strategies for Known Gaps. We now show the existence of ETC strategies
that improve on the optimal ﬁxed-design strategy. Surprisingly, the gain is signiﬁcant. We describe
an algorithm inspired by ideas from hypothesis testing and prove an upper bound on its regret that is
minimax optimal and that asymptotically matches our lower bound.

Let P be the law of X − Y , where X (resp. Y ) is a reward from arm 1 (resp. arm 2). As ∆ is
known, the exploration phase of an ETC algorithm can be viewed as a statistical test of the hypothesis
H1 : (P = N (∆, 2)) against H2 : (P = N (−∆, 2)). The work of Wald [1945] shows that a
signiﬁcant gain in terms of expected number of samples can be obtained by using a sequential rather
than a batch test. Indeed, for a batch test, a sample size of n ∼ (4/∆2) log(1/δ) is necessary to
guarantee that both type I and type II errors are upper bounded by δ. In contrast, when a random
number of samples is permitted, there exists a sequential probability ratio test (SPRT) with the same
guarantees that stops after a random number N of samples with expectation E[N ] ∼ log(1/δ)/∆2
under both H1 and H2. The SPRT stops when the absolute value of the log-likelihood ratio between
H1 and H2 exceeds some threshold. Asymptotic upper bound on the expected number of samples
used by a SPRT, as well as the (asymptotic) optimality of such procedures among the class of all
sequential tests can be found in [Wald, 1945, Siegmund, 1985].

Algorithm 2 is an ETC strategy that explores
each action alternately, halting when sufﬁcient
conﬁdence is reached according to a SPRT. The
threshold depends on the gap ∆ and the horizon
T corresponding to a risk of δ = 1/(T ∆2). The
exploration phase ends at the stopping time

τ = inf

(cid:110)
t = 2n : (cid:12)

(cid:12)ˆµ1,n−ˆµ2,n

(cid:12)
(cid:12) ≥

log(T ∆2)
n∆

(cid:111)
.

If τ < T then the empirical best arm ˆa at time τ
is played until time T . If T ∆2 ≤ 1, then τ = 1

input: T and ∆
A1 = 1, A2 = 2, t := 2
while (t/2)∆ |ˆµ1(t) − ˆµ2(t)| < log (cid:0)T ∆2(cid:1) do

choose At+1 = 1 and At+2 = 2,
t := t + 2

end while
ˆa := arg maxi ˆµi(t)
while t ≤ T do

choose At = ˆa,
t := t + 1

end while

Algorithm 2: SPRT ETC algorithm

4

(one could even deﬁne τ = 0 and pick a random arm). The following theorem gives a non-asymptotic
upper bound on the regret of the algorithm. The results rely on non-asymptotic upper bounds on the
expectation of τ , which are interesting in their own right.
Theorem 2. If T ∆2 ≥ 1, then the regret of the SPRT-ETC algorithm is upper-bounded as

RSPRT-ETC

(T ) ≤

µ

log(eT ∆2)
∆

+

4(cid:112)log(T ∆2) + 4
∆

+ ∆ .

Otherwise it is upper bounded by T ∆/2+∆, and for all T and ∆ the regret is less than 10(cid:112)T /e+∆.

The proof of Theorem 2 is given in Appendix C. The following lower bound shows that no uniformly
efﬁcient ETC strategy can improve on the asymptotic regret of Algorithm 2. The proof is given in
Section A together with the other lower bounds.
Theorem 3. Let π be an ETC strategy that is uniformly efﬁcient on H∆. Then for all µ ∈ H∆,
Rπ
µ(T )
log(T )

lim inf
T →∞

1
∆

≥

.

Explore-Then-Commit Strategies for Unknown Gaps. When the gap is unknown it is not possible
to tune a ﬁxed-design strategy that achieves logarithmic regret. ETC strategies can enjoy logarithmic
regret and these are now analysed. We start with the asymptotic lower bound.
Theorem 4. Let π be a uniformly efﬁcient ETC strategy on H. For all µ ∈ H, if ∆ = |µ1 − µ2| then
Rπ
µ(T )
log(T )

lim inf
T →∞

4
∆

≥

.

A simple idea for constructing an algorithm that matches the lower bound is to use a (ﬁxed-conﬁdence)
best arm identiﬁcation algorithm for the exploration phase. Given a risk parameter δ, a δ-PAC BAI
algorithm consists of a sampling rule (At), a stopping rule τ and a recommendation rule ˆa which
is Fτ measurable and satisﬁes, for all µ ∈ H such that µ1 (cid:54)= µ2, Pµ(ˆa = a∗) ≥ 1 − δ. In a bandit
model with two Gaussian arms, Kaufmann et al. [2014] propose a δ-PAC algorithm using a uniform
sampling rule and a stopping rule τδ that asymptotically attains the minimal sample complexity
Eµ[τδ] ∼ (8/∆2) log(1/δ). Using the regret decomposition (2), it is easy to show that the ETC
algorithm using the stopping rule τδ for δ = 1/T matches the lower bound of Theorem 4.

Algorithm 3 is a slight variant of this optimal BAI
algorithm, based on the stopping time
(cid:115)

τ = inf

t = 2n : |ˆµ1,n − ˆµ2,n|>

4 log (cid:0)T /(2n)(cid:1)
n






.






input: T (≥ 3)
A1 = 1, A2 = 2, t := 2
while |ˆµ1(t) − ˆµ2(t)| <

(cid:113) 8 log(T /t)
t

do

choose At+1 = 1 and At+2 = 2
t := t + 2

end while
ˆa := arg maxi ˆµi(t)
while t ≤ T do

The motivation for the difference (which comes from
a more carefully tuned threshold featuring log(T /2n)
in place of log(T )) is that the conﬁdence level should
depend on the unknown gap ∆, which determines the
regret when a mis-identiﬁcation occurs. The improve-
ment only appears in the non-asymptotic regime where
we are able to prove both asymptotic optimality and
order-optimal minimax regret. The latter would not be possible using a ﬁxed-conﬁdence BAI strategy.
The proof of this result can be found in Appendix D. The main difﬁculty is developing a sufﬁciently
strong deviation bound, which we do in Appendix G, and that may be of independent interest. Note
that a similar strategy was proposed and analysed by Lai et al. [1983], but in the continuous time
framework and with asymptotic analysis only.
Theorem 5. If T ∆2 > 4e2, the regret of the BAI-ETC algorithm is upper bounded as

Algorithm 3: BAI-ETC algorithm

choose At = ˆa
t := t + 1

end while

RBAI-ETC
µ

(T ) ≤

∆

4 log

(cid:17)

(cid:16) T ∆2
4

(cid:113)

334

(cid:1)

log (cid:0) T ∆2
∆

4

+

178
∆

+ 2∆.

It is upper bounded by T ∆ otherwise, and by 32

T + 2∆ in any case.

+

√

5

4 Fully Sequential Strategies for Known and Unknown Gaps

In the previous section we saw that allowing a random stopping time leads to a factor of 4 improvement
in terms of the asymptotic regret relative to the naive ﬁxed-design strategy. We now turn our attention
to fully sequential strategies when ∆ is known and unknown. The latter case is the classic 2-armed
bandit problem and is now quite well understood. Our modest contribution in that case is the ﬁrst
algorithm that is simultaneously asymptotically optimal and order optimal in the minimax sense. For
the former case, we are not aware of any previous research where the gap is known except the line of
work by Bubeck et al. [2013], Bubeck and Liu [2013], where different questions are treated. In both
cases we see that fully sequential strategies improve on the best ETC strategies by a factor of 2.

Known Gaps. We start by stating the lower bound (proved in Section A), which is a straightforward
generalisation of Lai and Robbins’ lower bound.
Theorem 6. Let π be a strategy that is uniformly efﬁcient on H∆. Then for all µ ∈ H∆,

lim inf
T →∞

Rπ
µ(T )
log T

≥

1
2∆

We are not aware of any existing algorithm matching this lower bound, which motivates us to
introduce a new strategy called ∆-UCB that exploits the knowledge of ∆ to improve the performance
of UCB. In each round the algorithm chooses the arm that has been played most often so far unless
the other arm has an upper conﬁdence bound that is close to ∆ larger than the empirical estimate of
the most played arm. Like ETC strategies, ∆-UCB is not anytime in the sense that it requires the
knowledge of both the horizon T and the gap ∆.

5:

if ˆµAt,min (t − 1) +

≥ ˆµAt,max(t − 1) + ∆ − 2εT then

Ni(t − 1) and At,max = 3 − At,min

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:17)

(cid:16)

2 log

T
NAt,min (t−1)
NAt,min(t − 1)

1: input: T and ∆
2: εT = ∆ log− 1
3: for t ∈ {1, . . . , T } do
4:

let At,min := arg min

8 (e + T ∆2)/4

i∈1,2

choose At = At,min

choose At = At,max

else

6:
7:
8:
9:
10: end for

end if

Algorithm 4: ∆-UCB

Theorem 7. If T (2∆ − 3εT )2 ≥ 2 and T ε2
bounded as

T ≥ e2, the regret of the ∆-UCB algorithm is upper

R∆-UCB
µ

(T ) ≤

log (cid:0)2T ∆2(cid:1)
2∆(1 − 3εT /(2∆))2 +
T T )

30e(cid:112)log(ε2
ε2
T

+ ∆

(cid:34)

(cid:112)π log (2T ∆2)
2∆(1 − 3εT /∆)2

+

+

80
ε2
T

2
(2∆ − 3εT )2

(cid:35)

+ 5∆.

Moreover lim supT →∞ R∆-UCB

µ

(T )/ log(T ) ≤ (2∆)−1 and ∀µ ∈ H∆, R∆-UCB

µ

(T ) ≤ 328

T + 5∆.

√

The proof may be found in Appendix E.

Unknown Gaps. In the classical bandit setting where ∆ is unknown, UCB by Katehakis and Robbins
[1995] is known to be asymptotically optimal: RUCB
µ (T ) ∼ 2 log(T )/∆, which matches the lower
bound of Lai and Robbins [1985]. Non-asymptotic regret bounds are given for example by Auer
et al. [2002], Cappé et al. [2013]. Unfortunately, UCB is not optimal in the minimax sense, which
is so far only achieved by algorithms that are not asymptotically optimal [Audibert and Bubeck,
2009, Lattimore, 2015]. Here, with only two arms, we are able to show that Algorithm 5 below is

6

simultaneously minimax order-optimal and asymptotically optimal. The strategy is essentially the
same as suggested by Lai [1987], but with a fractionally smaller conﬁdence bound. The proof of
Theorem 8 is given in Appendix F. Empirically the smaller conﬁdence bonus used by UCB∗ leads to
a signiﬁcant improvement relative to UCB.

1: input: T
2: for t ∈ {1, . . . , T } do

3:

At = arg max
i∈{1,2}

ˆµi(t − 1) +

4: end for

(cid:115)

2
Ni(t − 1)

(cid:18)

log

(cid:19)

T
Ni(t − 1)

Algorithm 5: UCB∗

Theorem 8. For all ε ∈ (0, ∆), if T (∆ − ε)2 ≥ 2 and T ε2 ≥ e2, the regret of the UCB∗ strategy is
upper bounded as

∗

RUCB
µ

(T ) ≤

(cid:16) T ∆2
2

2 log
∆ (cid:0)1 − ε

(cid:17)
(cid:1)2 +

∆

(cid:113)
2

π log (cid:0) T ∆2

∆ (cid:0)1 − ε

∆

(cid:32)

2

(cid:1)
(cid:1)2 + ∆

30e(cid:112)log(ε2T ) + 16e
ε2

(cid:33)

+

Moreover, lim supT →∞ Rπ

µ(T )/ log(T ) = 2/∆ and for all µ ∈ H, Rπ

µ(T ) ≤ 33

(cid:1)2 + ∆.

2
∆ (cid:0)1 − ε
√

∆

T + ∆.

Note that if there are K > 2 arms, then the strategy above is still asymptotically optimal, but suffers
a minimax regret of Ω((cid:112)T K log(K)), which is a factor of (cid:112)log(K) suboptimal.

5 Numerical Experiments

We represent here the regret of the ﬁve strategies presented in this article on a bandit problem
with ∆ = 1/5, for different values of the horizon. The regret is estimated by 4.105 Monte-Carlo
replications. In the legend, the estimated slopes of ∆Rπ(T ) (in logarithmic scale) are indicated after
the policy names.

The experimental behavior of the algorithms reﬂects the theoretical results presented above: the
regret asymptotically grows as the logarithm of the horizon, the experimental coefﬁcients correspond
approximately to theory, and the relative ordering of the policies is respected. However, it should
be noted that for short horizons the hierarchy is not quite the same, and the growth rate is not
logarithmic; this question is raised in Garivier et al. [2016]. In particular, on short horizons the
Best-Arm Identiﬁcation procedure performs very well with respect to the others, and starts to be
beaten (even by the gap-aware strategies) only when T ∆2 is much larger that 10.

7

6 Conclusion: Beyond Uniform Exploration, Two Arms and Gaussian

distributions

It is worth emphasising the impossibility of non-trivial lower bounds on the regret of ETC strategies
using any possible (non-uniform) sampling rule. Indeed, using UCB as a sampling rule together with
an a.s. inﬁnite stopping rule deﬁnes an artiﬁcial but formally valid ETC strategy that achieves the best
possible rate for general strategies. This strategy is not a faithful counter-example to our claim that
ETC strategies are sub-optimal, because UCB is not a satisfying exploration rule. If exploration is the
objective, then uniform sampling is known to be optimal in the two-armed Gaussian case [Kaufmann
et al., 2014], which justiﬁes the uniform sampling assumption.

The use of ETC strategies for regret minimisation (e.g., as presented by Perchet and Rigollet [2013])
is certainly not limited to bandit models with 2 arms. The extension to multiple arms is based on the
successive elimination idea in which a set of active arms is maintained with arms chosen according
to a round robin within the active set. Arms are eliminated from the active set once their optimality
becomes implausible and the exploration phase terminates when the active set contains only a single
arm (an example is by Auer and Ortner [2010]). The Successive Elimination algorithm has been
introduced by Even-Dar et al. [2006] for best-arm identiﬁcation in the ﬁxed-conﬁdence setting. It was
shown to be rate-optimal, and thus a good compromise for both minimizing regret and ﬁnding the
best arm. If one looks more precisely at mutliplicative constants, however, Garivier and Kaufmann
[2016] showed that it is suboptimal for the best arm identiﬁcation task in almost all settings except
two-armed Gaussian bandits. Regarding regret minimization, the present paper shows that it is
sub-optimal by a factor 2 on every two-armed Gaussian problem.

It is therefore interesting to investigate the performance in terms of regret of an ETC algorithm
using an optimal BAI algorithm. This is actually possible not only for Gaussian distributions, but
more generally for one-parameter exponential families, for which Garivier and Kaufmann [2016]
propose the asymptotically optimal Track-and-Stop strategy. Denoting d(µ, µ(cid:48)) = KL(νµ, νµ(cid:48)) the
Kullback-Leibler divergence between two distributions parameterised by µ and µ(cid:48), they provide
results which can be adapted to obtain the following bound.
Proposition 1. For µ such that µ1 > maxa(cid:54)=1 µa, the regret of the ETC strategy using Track-and-Stop
exploration with risk 1/T satisﬁes

lim sup
T →∞

µ (T )
RTaS
log T

≤ T ∗(µ)

w∗

a(µ)(µ1 − µa)

,

(cid:33)

(cid:32) K
(cid:88)

a=2

where T ∗(µ) (resp. w∗(µ)) is the the maximum (resp. maximiser) of the optimisation problem

max
w∈ΣK

inf
a(cid:54)=1

(cid:18)

(cid:20)
w1d

µ1,

w1µ1 + waµa
w1 + wa

(cid:19)

(cid:18)

+ wad

µa,

waµ1 + waµa
w1 + wa

(cid:19)(cid:21)

,

where ΣK is the set of probability distributions on {1, . . . , K}.

In general, it is not easy to quantify the difference to the lower bound of Lai and Robbins

lim inf
T →∞

Rπ
µ(T )
log T

≥

K
(cid:88)

a=2

µ1 − µa
d(µa, µ1)

.

Even for Gaussian distributions, there is no general closed-form formula for T ∗(µ) and w∗(µ) except
when K = 2. However, we conjecture that the worst case is when µ1 and µ2 are much larger than
the other means: then, the regret is almost the same as in the 2-arm case, and ETC strategies are
suboptimal by a factor 2. On the other hand, the most favourable case (in terms of relative efﬁciency)
seems to be when µ2 = · · · = µK: then

√

K − 1
√

K − 1 +

K − 1

,

w∗

2(µ) = · · · = w∗

K(µ) =

1
√
K − 1 +

K − 1

w∗

1(µ) =
√

and T ∗ = 2(

K − 1 + 1)2/∆2, leading to
RTaS
µ (T )
log(T )

lim sup
T →∞

(cid:18)

≤

1 +

√

1
K − 1

(cid:19) 2(K − 1)
∆

,

while Lai and Robbins’ lower bound yields 2(K − 1)/∆. Thus, the difference grows with K as
√
2

K − 1 log(T )/∆ , but the relative difference decreases.

8

References

Jean-Yves Audibert and Sébastien Bubeck. Minimax policies for adversarial and stochastic bandits.

In Proceedings of Conference on Learning Theory (COLT), pages 217–226, 2009.

Peter Auer and Ronald Ortner. UCB revisited: Improved regret bounds for the stochastic multi-armed

bandit problem. Periodica Mathematica Hungarica, 61(1-2):55–65, 2010.

Peter Auer, Nicoló Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit

problem. Machine Learning, 47:235–256, 2002.

Sébastien Bubeck and Che-Yu Liu. Prior-free and prior-dependent regret bounds for thompson

sampling. In Advances in Neural Information Processing Systems, pages 638–646, 2013.

Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet. Bounded regret in stochastic multi-armed

bandits. In Proceedings of the 26th Conference On Learning Theory, pages 122–134, 2013.

Olivier Cappé, Aurélien Garivier, Odalric-Ambrym Maillard, Rémi Munos, and Gilles Stoltz.
Kullback–Leibler upper conﬁdence bounds for optimal sequential allocation. The Annals of
Statistics, 41(3):1516–1541, 2013.

Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action Elimination and Stopping Conditions for
the Multi-Armed Bandit and Reinforcement Learning Problems. Journal of Machine Learning
Research, 7:1079–1105, 2006.

Aurélien Garivier and Emilie Kaufmann. Optimal best arm identiﬁcation with ﬁxed conﬁdence. In

Proceedings of the 29th Conference On Learning Theory (to appear), 2016.

Aurélien Garivier, Pierre Ménard, and Gilles Stoltz. Explore ﬁrst, exploit next: The true shape of

regret in bandit problems. arXiv preprint arXiv:1602.07182, 2016.

Abdolhossein Hoorfar and Mehdi Hassani. Inequalities on the lambert w function and hyperpower

function. J. Inequal. Pure and Appl. Math, 9(2):5–9, 2008.

Michael N Katehakis and Herbert Robbins. Sequential choice from several populations. Proceedings

of the National Academy of Sciences of the United States of America, 92(19):8584, 1995.

Emilie Kaufmann, Olivier Cappé, and Aurélien Garivier. On the Complexity of A/B Testing. In

Proceedings of the 27th Conference On Learning Theory, 2014.

Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of

Statistics, pages 1091–1114, 1987.

Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

applied mathematics, 6(1):4–22, 1985.

Tze Leung Lai, Herbert Robbins, and David Siegmund. Sequential design of comparative clinical
trials. In M. Haseeb Rizvi, Jagdish Rustagi, and David Siegmund, editors, Recent advances in
statistics: papers in honor of Herman Chernoff on his sixtieth birthday, pages 51–68. Academic
Press, 1983.

Tor Lattimore. Optimally conﬁdent UCB: Improved regret for ﬁnite-armed bandits. Technical report,

2015. URL http://arxiv.org/abs/1507.07880.

Peter Mörters and Yuval Peres. Brownian motion, volume 30. Cambridge University Press, 2010.

Vianney Perchet and Philippe Rigollet. The multi-armed bandit with covariates. The Annals of

Statistics, 2013.

Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Eric Snowberg. Batched bandit problems.

In Proceedings of the 28th Conference On Learning Theory, 2015.

David Siegmund. Sequential Analysis. Springer-Verlag, 1985.

William Thompson. On the likelihood that one unknown probability exceeds another in view of the

evidence of two samples. Biometrika, 25(3/4):285–294, 1933.

Abraham Wald. Sequential Tests of Statistical Hypotheses. Annals of Mathematical Statistics, 16(2):

117–186, 1945.

9

Notation for the Proofs

We denote by (Xs) and (Ys) the sequence of successive observations from arm 1 and arm 2, so that

ˆµ1,s =

Xi

and

ˆµ2,s =

1
s

s
(cid:88)

i=1

1
s

s
(cid:88)

i=1

Yi

In the proofs of all regret upper bounds given in Appendix B to F we assume without loss of generality
that µ is such that µ1 > µ2 and let ∆ = µ1 − µ2.

A Proof of the Lower Bounds (Theorems 3, 4, 6 and Lai&Robbins)

Let π be a uniformly efﬁcient strategy on some class S, as deﬁned in (3), and let λ ∈ H. If
µ(T )] = |λ1 − λ2|Eλ[Nm(λ)(T )] this implies in particular that
m(λ) = arg min{λ1, λ2}, as Eλ[Rπ

∀α ∈]0, 1], Eλ[Nm(λ)(T )] = o(T α).

Without loss of generality we assume that µ1 = µ2 + ∆ with ∆ > 0. All the lower bounds are based
1, µ(cid:48)
on a change of measure argument, which involves considering an alternative reward vector (µ(cid:48)
2)
that is “not too far” from (µ1, µ2), but for which the expected behaviour of the algorithm is very
different. This is the same approach used by Lai and Robbins [1985], but rewritten and generalised in
a more powerful way (in particular regarding the ETC strategies). The improvements come thanks to
Inequality 4 in [Garivier et al., 2016], which states that for every (µ(cid:48)
2) ∈ H and for every stopping
time σ such that N2(T ) is Fσ-measurable,

1, µ(cid:48)

(cid:2)N1(σ)(cid:3) (µ(cid:48)

Eµ

1 − µ1)2
2

+ Eµ

(cid:2)N2(σ)(cid:3) (µ(cid:48)

2 − µ2)2
2

(cid:18)

≥ kl

Eµ

(cid:20) N2(T )
T

(cid:21)
, Eµ(cid:48)

(cid:20) N2(T )
T

(cid:21)(cid:19)

,

where kl(p, q) is the relative entropy between Bernoulli distributions with parameters p, q ∈ [0, 1]
respectively. Since kl(p, q) ≥ (1 − p) log(1/(1 − q)) − log(2) for all p, q ∈ (0, 1), one obtains

Eµ

(cid:2)N1(σ)(cid:3) (µ(cid:48)
(cid:18)

1 − µ1)2
2

≥

1 −

+ Eµ
Eµ[N2(T )]
T

(cid:2)N2(σ)(cid:3) (µ(cid:48)
(cid:19)

(cid:18)

log

2 − µ2)2
2
T
Eµ(cid:48)[N1(T )]

(cid:19)

− log(2) .

(4)

For µ(cid:48) ∈ S such that µ(cid:48)

2, Eµ[N2(T )] = o(T α) and Eµ(cid:48)[N1(T )] = o(T α) for all α ∈]0, 1], thus

1 < µ(cid:48)
Eµ

lim inf
T →∞

(cid:2)N1(σ)(cid:3)(µ(cid:48)

1 − µ1)2/2 + Eµ

(cid:2)N2(σ)(cid:3)(µ(cid:48)

2 − µ2)2/2

≥ 1 .

log T

Let us now draw the conclusions in each setting. Observe that while this argument is now routine for
general policies, we show here how to apply it very nicely to ETC strategies as well.

Known gap, General strategy: S = H∆. By choos-
ing σ = T , µ(cid:48)
2 = µ1 + ∆ = µ2 + 2∆,
we obtain

1 = µ1 and µ(cid:48)

Eµ

(cid:2)N2(T )(cid:3)
log T

≥

1
(2∆)2/2

.

lim inf
T →∞

Known gap, ETC strategy: S = H∆. For an ETC
strategy π with a stopping rule τ , one has N1(τ ∧T ) =
N2(τ ∧ T ) = (τ ∧ T )/2. Besides, N2(T ) is indeed
Fτ ∧T -measurable: after τ ∧ T draws, the agent knows
whether she will draw arm 2 for the last T − τ ∧ T
steps or not. With µ(cid:48)
1 = µ2, µ(cid:48)
2 = µ1, Inequality (4)
thus yields:

(cid:3)

Eµ

(cid:2) τ ∧T
2
log T

≥

1
2∆2/2

.

lim inf
T →∞

10

Unknown gap, General
strategy:
S = H. We use the same choices, ex-
cept µ(cid:48)
2 = µ1 + ε for some ε > 0:

Eµ

(cid:2)N2(T )(cid:3)
log T

lim inf
T →∞

≥

1
(∆ + ε)2 /2

.

Unknown gap, ETC strategy: S = H.
Choosing this time µ(cid:48)
1 = (µ1 + µ2 −
ε)/2 and µ(cid:48)
2 = (µ1 + µ2 + ε)/2, for
some ε > 0 yields

(cid:3)

Eµ

(cid:2) τ ∧T
2
log T

≥

lim inf
T →∞

1
(cid:0) ∆+ε
2

(cid:1)2 .

µ(T ) = ∆Eµ[N2(T )] for general strategies, while Equation (2) shows that Rπ
Rπ
T )/2] for ETC strategies. Therefore letting ε go to zero when needed shows that

µ(T ) ≥ ∆Eµ[(τ ∧

lim inf
T →∞

Rπ
µ(T )
log(T )

≥

C Π
S
∆

,

for the value C Π

S given at the end of Section 2.

Note that the asymptotic lower bound of Theorem 1 can also be proved by similar arguments, if one
really wants to bring in an elephant to kill a mouse. Moreover, note that this proof may also lead to
(not-so-simple) non-asymptotic lower-bounds, as shown in Garivier et al. [2016] for example.

B Proof of Theorem 1

Let n ≤ T /2. The number of draws of the suboptimal arm 2 is N2 = n + (T − 2n)1{Sn ≤ 0},
where Sn = (X1 − Y1) + · · · + (Xn − Yn) ∼ N (n∆, 2n). The expected regret of the strategy using
2n exploration steps is

µ(T ) = ∆Eµ[N2] = ∆(cid:0)n + (T − 2n)Pµ(Sn ≤ 0)(cid:1) .
Rn

(5)

But

Pµ(Sn ≤ 0) = Pµ

(cid:18) Sn − n∆
√

≤

−n∆
√
2n

= −∆

(cid:19)

(cid:114) n
2

.

2n
Denote by Φ (resp. φ) the pdf (resp. cdf) of the standard Gaussian distribution, and recall that W is the
Lambert function deﬁned for all y > 0 by W (y) exp(W (y)) = y. The regret is thus upper-bounded
µ(T ) ≤ ∆g(n) where, for all x > 0, g(x) = x + T Φ(−∆(cid:112)x/2). By differentiating g, one can
as Rn
see that its maximum is reached at x∗ such that
(cid:114) x∗
2

∆(cid:112)x∗/2
T ∆2/4

(cid:18) T 2∆4
32π

and thus x∗ =

2
∆2 W

∆

(cid:32)

(cid:33)

=

(cid:19)

φ

.

,

By choosing n = (cid:100)x∗(cid:101), we obtain

Rn

µ(T ) ≤

2
∆

W

(cid:18) T 2∆4
32π

(cid:19)

+ ∆ + ∆T Φ

−∆

(cid:32)

(cid:33)

(cid:114) x∗
2

.

As g(n) ≤ g(x∗) + 1 ≤ g(0) + 1 = T /2 + 1, Rn
T ∆2 ≤ 4
y ≥ e (see Hoorfar and Hassani [2008]) entails

2πe. If T ∆2 ≥ 4

T + ∆ for
2πe, the inequality W (y) ≤ log (cid:0)(1 + e−1)y/ log(y)(cid:1) valid for all

µ(T ) ≤ (T /2 + 1)∆ ≤ 2.04

√

√

W

(cid:19)

(cid:18) T 2∆4
32π

≤ log

(cid:32)

(1 + e−1) T 2∆4
32π
(cid:1)
log (cid:0) T 2∆4

32π

(cid:33)

= 2 log

(cid:114)



1
8





1 + e−1
π

(cid:114)

T ∆2
(cid:16) T ∆2
√
2π
4

log

(cid:17)

√







In addition, the classic bound on the Gaussian tail Φ(−y) ≤ φ(y)/y yields by deﬁnition of x∗:

(cid:32)

Φ

−∆

(cid:33)

(cid:114) x∗
2

≤

(cid:16)

φ

−∆

(cid:17)

(cid:113) x∗
2

(cid:113) x∗
2

∆

=

4
T ∆2 .

Hence, for T ∆2 ≥ 4


√

2πe,

Rn

µ(T ) ≤

(cid:114)

4
∆

log

e
8





1 + e−1
π

(cid:114)







T ∆2
(cid:16) T ∆2
√
2π
4

log

(cid:17)

+ ∆ <

log

4
∆







T ∆2

(cid:114)

4.46

log

(cid:17)

(cid:16) T ∆2
√
2π
4







+ ∆ .

To complete the proof of the uniform upper-bound, we start from

Rn

µ(T ) ≤

2
∆

W

(cid:18) T 2∆4
32π

(cid:19)

4
∆

+

+ ∆ .

11

Figure 1: Regret of the Fixed-Budget ETC algorithm with optimal n, for ∆ = 1/5 (solid line). The
linear upper bound T ∆/2 and the logarithmic bound of Theorem 1 are dashed, bold lines. The thin,
dotted line is 2.04

T + ∆. The bold, dotted line is ∆g(n), with g and n as in the proof.

√

Denoting by r ≈ 1.09 the root of (4r − 1)W (r) = 2, the maximum of 2W
reached at ∆ = (cid:0)32πr/T 2(cid:1)1/4

, and is equal to

(cid:17)

(cid:16) T 2∆4
32π

/∆ + 4/∆ is

2
∆

W

(cid:18) T 2∆4
32π

(cid:19)

+

=

4
∆

8r3/4
(4r − 1)(2π)1/4

√

√

T < 2

T .

Let us now prove that choosing n too small leads to catastrophic regret. If n ≤ 4(1 − ε) log(T )/∆2,
then Equation (5) yields

Pµ(Sn ≤ 0) ≥

(cid:18)

1 −

1
√
πn

2
n∆2

(cid:19)

(cid:18)

exp

−

(cid:19)

∆2n
4

∆
(cid:18)

(cid:18)

≥

1 −

≥

1 −

(cid:19)

(cid:19)

2
n∆2

2
n∆2

1
2(cid:112)π(1 − ε) log(T )

T ε−1
2(cid:112)π log(T )

.

exp (cid:0) − (1 − ε) log(T )(cid:1)

Thus, the expected regret is lower-bounded as

Rn

µ(T ) ≥ ∆(T − 2n)Pµ(Sn ≤ 0) ≥ ∆

1 −

(cid:18)

(cid:19) (cid:18)

1 −

2
n∆2

(cid:19)

8 log(T )
∆2T

T ε
2(cid:112)π log(T )

.

Let us now turn to the last statement of the theorem. The previous inequality shows that for all ε > 0,

lim inf
T

inf
∆2 <n≤ 4(1−ε) log(T )

∆2

3

Rn
µ(T )
log(T )

= +∞ .

12

For n ≤ 3/∆2, we have that

Pµ(Sn ≤ 0) ≥ Pµ

(cid:32)

Sn − n∆
2n

√

(cid:33)

(cid:114) 3
2

≤ −

> 0 ,

and hence that lim inf T inf n≤3/∆2 Rn

µ(T )/T > 0. As Rn

µ(T ) ≥ n∆, the result follows.

C Proof of Theorem 2

Recall that µ1 > µ2. Using (2), one has

Rπ

µ(T ) ≤ ∆Eµ

+ T ∆ Pµ(τ < T, ˆa = 2) .

(cid:105)

(cid:104) τ
2

If T ∆2 ≤ 1, then τ = 2 and ˆa is based on a single sample from each action. Therefore ˆa = 2 with
probability less than 1/2 and the regret is upper-bounded by ∆ + T ∆/2. Otherwise, let S0 = 0,
Sn = (X1 − Y1) + · · · + (Xn − Yn) for every n ≥ 1. For every u > 0, let nu = (log(T ∆2) + u)/∆2.
Observe that

(cid:110) τ
2
Moreover, if nu ∈ N then Snu ∼ N (nu∆, 2nu) and

(cid:24) log(T ∆2) + u
∆2

(cid:25) (cid:111)

⊂

≥

(cid:110)

S(cid:100)nu(cid:101) ≤

log(T ∆2)
∆

(cid:111)

.

(cid:18)

Pµ

Snu ≤

log(T ∆2)
∆
(cid:32)

= Pµ

(cid:32)

(cid:19)

= Pµ

Snu − nu∆
2nu

√

≤

Snu − nu∆
2nu

√

≤

−u
(cid:112)2(log(T ∆2) + u)

(cid:33)

log(T ∆2)/∆ − ∆(log(T ∆2) + u)/∆2
(cid:112)2(log(T ∆2) + u)/∆2
(cid:33)

(cid:32)

(cid:33)

≤ exp

−

u2
4(cid:0) log(T ∆2) + u(cid:1)

.

Hence, for a = 2(cid:112)log(T ∆2),
(cid:90) ∞

(cid:17)

− 1 ≥ v

dv =

Pµ

(cid:16) τ
2

na

(cid:90) ∞

a
(cid:90) ∞

Pµ

Pµ

(cid:18) τ
2
(cid:18) τ
2

a

1
∆2

1
∆2

(cid:90) ∞

(cid:90) ∞

a

a

− 1 ≥

log(T ∆2) + u
∆2

(cid:24) log(T ∆2) + u
∆2

(cid:19) du
∆2
(cid:19)(cid:25) du
∆2
(cid:33)

exp

−

u2
4(cid:0) log(T ∆2) + u(cid:1)

du

≥

(cid:32)

(cid:32)

exp

−

u
2(cid:112)log(T ∆2) + 4

u
2(cid:112)log(T ∆2) + 4

(cid:32)

(cid:90) ∞

0

−

exp

1
∆2
2(cid:112)log(T ∆2) + 4
∆2

(cid:33)

(cid:33)

du

du

≤

≤

≤

≤

=

as log(T ∆2) ≤ u(cid:112)log(∆2T )/2

and

Eµ

(cid:105)

(cid:104) τ
2

≤ 1 + na +

− 1 ≥ v

dv ≤ 1 +

(cid:90) ∞

na

Pµ

(cid:16) τ
2

log(T ∆2) + 4(cid:112)log(T ∆2) + 4
∆2

.

To conclude the proof of the ﬁrst statement, it remains to show that P(τ < T, ˆa = 2) ≤ 1/(T ∆2).
Since X1 − Y1 ∼ N (∆, 2), Eµ[exp(−∆(X1 − Y1))] = exp(−∆2 + 2∆2/2) = 1 and Mn =
exp(−∆Sn) is a martingale. Let τ2 = T ∧ inf{n ≥ 1 : Sn ≤ − log(T ∆2)/∆}, and observe that

(cid:8)τ < T, ˆa = 2(cid:9) ⊂

∃n < T : Sn ≤ −

(cid:26)

(cid:27)

log(T ∆2)
∆

= (cid:8)τ2 < T (cid:9) .

,

(cid:17)

13

Doob’s optional stopping theorem yields Eµ[Mτ2] = Eµ[M0] = 1. But as Mτ2 = exp(−∆Sτ2 ) ≥
exp(∆ log(T ∆2)/∆) = T ∆2 on the event {τ2 < T },

Pµ(τ < T, ˆa = 2) ≤ Pµ(τ2 < T ) ≤ Eµ

1{τ2 < T }

(cid:20)

(cid:21)

≤

Mτ2
T ∆2

Eµ

(cid:2)Mτ2
(cid:3)
T ∆2 =

1
T ∆2 .

√

The last statement Theorem 2 is obtained by maximising the bound (except for the ∆ summand). Let
e) + 2u(cid:112)− log(u). Denoting (cid:96) = − log(u), f (cid:48)(u) = (cid:96) −
u = 1/(∆
√
√
T (cid:1) = 1/2
(cid:96), thus the maximum is reached at (cid:96) = log (cid:0)∆
1/2+2
and ∆ = (cid:112)e/T . Re-injecting this value into the bound, we obtain that for every ∆ > 0,

T ) and f (u) = −u log(u/

(cid:96)+2)((cid:96)−1/2)/

(cid:96)−1/

(cid:96) = (

√

√

√

√

RSPRT-ETC

(T ) ≤

µ

√

2 + 4

1 + 4

(cid:112)e/T

= 10

(cid:114)

T
e

.

D Proof of Theorem 5

2, which means that W1, W2, . . .
Recall that µ1 = µ2 + ∆ with ∆ > 0. Let Ws = (Xs − Ys − ∆)/
are i.i.d. standard Gaussian random variables. Introducing F = (ˆa (cid:54)= 1, τ < T ), one has by (2) that

√

RBAI-ETC
µ

(T ) ≤ T ∆Pµ(F) + (∆/2)Eµ[τ ∧ T ].

From the deﬁnition of τ and Lemma 1.(c) in Appendix G, assuming that T ∆2 ≥ 4e2, one obtains

Pµ (F) ≤ Pµ

∃s : 2s ≤ T, ˆµ1,s − ˆµ2,s ≤ −

log

(cid:115)

4
s

(cid:19)(cid:33)

(cid:18) T
2s

= Pµ

∃s ≤ T /2 :

(cid:80)s

i=1 Wi
s

(cid:115)

2
s

≤ −

log

(cid:33)

(cid:19)

(cid:18) T /2
s

−

∆
√
2

120e(cid:112)log(∆2T /4)
∆2T

≤

+

64e
∆2T

.

The last step is bounding Eµ[τ ∧ T ] for which as T ∆2 ≥ 4, Lemma 1.(b) yields

(cid:32)

(cid:32)

T
(cid:88)

t=1

Eµ[τ ∧ T ] =

Pµ (τ ≥ t) ≤ 2 + 2

Pµ (τ ≥ 2s + 1)

T /2
(cid:88)

s=1

T /2
(cid:88)

Pµ

s=1
(cid:16) T ∆2
4

(cid:17)

(cid:32) (cid:80)s

i=1 Wi
s
(cid:113)
8

+

≤ 2 + 2

8 log

≤

∆2

(cid:115)

≤

log

(cid:33)

(cid:19)

(cid:18) T /2
s

−

∆
√
2

2
s

(cid:1)

π log (cid:0) T ∆2
∆2

4

+

8
∆2 + 4 ,

Therefore, if T ∆2 ≥ 4e2,

RBAI-ETC
µ

(T ) ≤

4 log

4 log

≤

(cid:17)

(cid:16) T ∆2
4

∆
(cid:16) T ∆2
4

(cid:17)

∆

+

+

(cid:113)

4

334

(cid:1)

(cid:1)

4

π log (cid:0) T ∆2
∆
log (cid:0) T ∆2
∆

(cid:113)

4

+

+ 2∆ +

4
∆

+

178
∆

+ 2∆.

(cid:113)

120e

(cid:1)

log (cid:0) ∆2T
∆

4

+

64e
∆

(T )/ log(T ) ≤ 4. Noting that
(T ) ≤ ∆T and taking the minimum of this bound and the ﬁnite-time bound given above

Taking the limit as T → ∞ shows that lim supT →∞ RBAI-ETC
RBAI-ETC
µ
leads arduously to RBAI-ETC

(T ) ≤ 2∆ + 32

T for all µ.

√

µ

µ

14

E Proof of Theorem 7

Deﬁne random time τ = max {τ1, τ2} where τi is given by

(cid:26)

(cid:27)

τi = min

t ≤ T : sup
s≥t

|ˆµi,s − µi| < εT

.

By the concentration Lemma 1.(a) in Appendix G we have Eµ[τi] ≤ 1 + 9/ε2
Eµ[τ1 + τ2] ≤ 2 + 18/ε2
expected number of draws of the suboptimal arm may be bounded by

T and so Eµ[τ ] ≤
T . For t > 2τ we have |ˆµAt,max(t − 1) − µAt,max | < εT . Therefore the

Eµ[N2(T )] = Eµ

(cid:34) T

(cid:88)

(cid:35)
1 {It = 2}

≤ Eµ[2τ ] + Eµ

1 {It = 2}

(cid:35)

(cid:34) T

(cid:88)

t=2τ +1

(cid:40)

t=1
(cid:34) T

(cid:88)

1

t=1

(cid:115)

2 log(T /N2(t − 1))
N2(t − 1)

≤ Eµ[2τ ] + Eµ

ˆµ2(t − 1) +

≥ µ1 + ∆ − 3εT and It = 2

(cid:40)

(cid:34) T

(cid:88)

1

+ Eµ

t=1

(cid:115)

2 log(T /N1(t − 1))
N1(t − 1)

ˆµ1(t − 1) +

≤ µ2 + ∆ − εT = µ1 − εT

(6)

(cid:41)(cid:35)

By Lemma 1.(b), whenever T (2∆ − 3εT )2 ≥ 2,

(cid:40)

(cid:34) T

(cid:88)

1

Eµ

ˆµ2(t − 1) +

(cid:115)

2 log(T /N2(t − 1))
N2(t − 1)

t=1

≤

T
(cid:88)

s=1

≤

(cid:32)

Pµ

ˆµ2,s − µ2 +

(cid:114)

2 log(T /s)
s

(cid:33)

≥ 2∆ − 3εT

2 log

(cid:16) T (2∆−3εT )2
2

(cid:17)

(cid:114)
2

π log

(cid:16) T (2∆−3εT )2
2

(cid:17)

(2∆ − 3εT )2

(2∆ − 3εT )2

2

+

(2∆ − 3εT )2 + 1.

≥ µ1 + ∆ − 3εT = µ2 + 2∆ − 3εT and It = 2

(cid:41)(cid:35)

(cid:41)(cid:35)

For the second term in (6) we apply Lemma 1.(c) to obtain, whenever T ε2

T ≥ e2,

ˆµ1(t − 1) +

(cid:40)

(cid:34) T

(cid:88)

1

Eµ

t=1

(cid:32)

≤ T Pµ

2 log(T /N1(t − 1))
N1(t − 1)
(cid:114) 2
s

∃s ≤ T : ˆµ1,s +

log(T /s) ≤ µ1 − εT

≤

(cid:41)(cid:35)

≤ µ1 − εT

(cid:33)

30e(cid:112)log(ε2
ε2
T

T T )

+

16e
ε2
T

.

Therefore, if T (2∆ − 3εT )2 ≥ 2 and T ε2

T ≥ e2,

Eµ[N2(T )] ≤

2 log

(cid:16) T (2∆−3εT )2
2

(cid:17)

+

(2∆ − 3εT )2
80
ε2
T

+

+

2

(2∆ − 3εT )2 + 5.

(cid:114)

2

π log

(cid:16) T (2∆−3εT )2
2

(cid:17)

(2∆ − 3εT )2

+

30e(cid:112)log(ε2
ε2
T

T T )

is R∆-UCB
The ﬁrst result follows since the regret
For the second
µ
it easily noted that for the choice of εT given in the deﬁnition of the algorithm that
Eµ[N2(T )]/ log(T ) ≤ 1/(2∆2). Therefore lim supT →∞ R∆-UCB
lim supT →∞
(T )/ log(T ) ≤
1/(2∆). The third result follows from a laborious optimisation step to upper-bound the minimum of
T ∆ and the ﬁnite-time regret bound above.

(T ) = ∆Eµ[N2(T )].

µ

+

(cid:115)

15

F Proof of Theorem 8

For any ε ∈ (0, ∆) we have

Eµ[N2(T )] = Eµ

1 {At = 2}

(cid:35)

(cid:34) T

(cid:88)

t=1
(cid:34) T

(cid:88)

t=1
(cid:32)

(cid:40)

1

≤ Eµ

At = 2 and ˆµ2(t − 1) +

(cid:115)

2
N2(t − 1)

(cid:18)

log

(cid:19)

T
N2(t − 1)

(cid:41)(cid:35)

≥ µ1 − ε

+ T Pµ

∃s ≤ T : ˆµ1,s +

(cid:115)

2
s

log

(cid:19)

(cid:18) T
s

(cid:33)

≤ µ1 − ε

.

By the concentration Lemma 1.(b) in Appendix G we have, whenever T (∆ − ε)2 ≥ 2,

(cid:40)

(cid:34) T

(cid:88)

1

Eµ

At = 2 and ˆµ2(t − 1) +

(cid:115)

2
N2(t − 1)

(cid:19)

(cid:18)

T
N2(t − 1)

(cid:41)(cid:35)

≥ µ1 − ε

log

(cid:33)

t=1

T
(cid:88)

s=1

≤

(cid:32)

Pµ

ˆµ2,s − µ2 +

(cid:115)

2
s

log

(cid:19)

(cid:18) T
s

≥ ∆ − ε

2 log

(cid:16) T (∆−ε)2
2

(cid:17)

(cid:114)
2

π log

(cid:16) T (∆−ε)2
2

(cid:17)

≤

(∆ − ε)2

(∆ − ε)2

+

2

+

(∆ − ε)2 + 1

For the second term we apply Lemma 1.(c) to obtain, whenever T ε2 ≥ e2,

(cid:32)

T Pµ

∃s ≤ T : ˆµ1,s +

(cid:115)

2
s

log

(cid:19)

(cid:18) T
s

(cid:33)

≤ µ1 − ε

≤

30e(cid:112)log(ε2T )
ε2

+

16e
ε2 .

Finally, if T (∆ − ε)2 ≥ 2 and T ε2 ≥ e2,

RUCB

µ (T ) ≤ ∆Eµ[N2(T )]
(cid:16) T (∆−ε)2
2 log
2
∆ (1 − ε/∆)2 +

(cid:17)

2

≤

(cid:114)

π log

(cid:16) T (∆−ε)2
2

(cid:17)

∆ (1 − ε/∆)2

+

∆ (1 − ε/∆)2 + ∆ + ∆

2

(cid:32)

30e(cid:112)log(ε2T )
ε2

+

16e
ε2

(cid:33)

The asymptotic result follows by taking the limit as T tends to inﬁnity and choosing ε = log− 1
8 (T )
while the minimax result follows by ﬁnding the minimum of the ﬁnite-time bound given above and
the naive RUCB

µ (T ) ≤ T ∆.

G Deviation Inequalities

As was already the case in the proof of Theorem 1, we heavily rely on the following well-known
inequality on the tail of a Gaussian distribution: if X ∼ N (0, 1), then for all x > 0

P (X ≥ x) ≤ min

1,

exp (cid:0)−x2/2(cid:1) .

(cid:26)

(cid:27)

1
√
2π

x

Lemma 1 gathers some more speciﬁc results that are useful in our regret analyses, and that we believe
to be of a certain interest on their own.

16

Lemma 1. Let ε > 0 and ∆ > 0 and W1, W2, . . . be standard i.i.d. Gaussian random variables and
ˆµt = (cid:80)t

s=1 Ws/t. Then the following hold:
(cid:20)

(cid:27)(cid:21)

(cid:26)

(a). E

min

t : sup
s≥t

|ˆµs| ≥ ε

≤ 1 + 9/ε2

(b).

if T ∆2 ≥ 2 then

(cid:32)

T
(cid:88)

P

ˆµn +

(cid:115)

n=1
(cid:32)

(cid:19)

(cid:18) T
n

2
n

log

(cid:115)

2
s

log

(cid:19)

(cid:18) T
s

(cid:33)

2 log

(cid:17)

(cid:16) T ∆2
2

(cid:113)

2

≥ ∆

≤

+

(cid:1)

π log (cid:0) T ∆2
∆2

2

+

2
∆2 + 1

∆2

(cid:33)

30e(cid:112)log(ε2T )
ε2T

+

16e
ε2T

(c).

if T ε2 ≥ e2 then P

∃s ≤ T : ˆµs +

+ ε ≤ 0

≤

The proof of Lemma 1 follows from standard peeling techniques and inequalities for Gaussian sums
of random variables. So far we do not know if this statement holds for subgaussian random variables
where slightly weaker results may be shown by adding log log terms to the conﬁdence interval, but
unfortunately by doing this one sacriﬁces minimax optimality.

Proof of Lemma 1.(a). We use a standard peeling argument and the maximal inequality.

P (∃s ≥ t : |ˆµs| ≥ ε) ≤

P (∃s ∈ [kt, (k + 1)t] : |ˆµs| ≥ ε)

∞
(cid:88)

k=1
∞
(cid:88)

k=1
∞
(cid:88)

k=1
∞
(cid:88)

k=1

≤

≤

≤

P (∃s ≤ (k + 1)t : |sˆµs| ≥ ktε)

2 exp

−

=

2 exp

−

(cid:19)

(ktε)2
2(k + 1)t

∞
(cid:88)

k=1

(cid:18)

(cid:19)

ktε2
2 (1 + 1/k)

(cid:18)

(cid:18)

2 exp

−

(cid:19)

ktε2
4

Therefore

E[τ ] ≤ 1 +

P (τ ≥ t) ≤ 1 +

min

1, 2

exp

−

∞
(cid:88)

t=1

∞
(cid:88)

k=1

(cid:40)

(cid:27)

(cid:18)

(cid:19)(cid:41)

ktε2
4

(cid:26)

∞
(cid:88)

t=1
∞
(cid:88)

= 1 +

min

1,

t=1
4 log(4)

ε2 +

(cid:90) ∞

≤ 1 +

(cid:26)

2
exp (tε2/4) − 1

≤ 1 +

min

1,

(cid:90) ∞

0

2
exp (tε2/4) − 1

(cid:27)

dt

8
3 exp (tε2/4)

dt = 1 +

4 log(4)

ε2 +

8
3ε2 ≤ 1 +

9
ε2 .

4/ε2 log(4)

Proof of Lemma 1.(b). Let ν be the solution of (cid:112)2 log(T /n)/n = ∆, that is ν = 2W (cid:0)∆2T /2(cid:1)/∆2.
Then

(cid:32)

T
(cid:88)

P

n=1

(cid:115)

2
n

log

(cid:19)

(cid:18) T
n

ˆµn +

(cid:33)

T
(cid:88)

(cid:32)

P

n=(cid:100)ν(cid:101)

≥ ∆

≤ ν +

ˆµn ≥ ∆ −

(cid:115)

2
n

log

(cid:18) T
n

(cid:19)(cid:33)

.

As for all n ≥ ν

2
n

log

≤

log

T
n

2
ν

(cid:18) T
ν

(cid:19) ν
n

= ∆2 ν
n

,

17

(cid:32)

T
(cid:88)

P

n=(cid:100)ν(cid:101)

ˆµn ≥ ∆ −

(cid:115)

2
n

log

(cid:18) T
n

(cid:19)(cid:33)

∞
(cid:88)

(cid:18)

P

≤

(cid:18)

ˆµn ≥ ∆

1 −

(cid:19)(cid:19)

(cid:114) ν
n

n=(cid:100)ν(cid:101)
(cid:19)2(cid:33)

≤

=

∞
(cid:88)

n=(cid:100)ν(cid:101)

∞
(cid:88)

n=(cid:100)ν(cid:101)

(cid:32)

(cid:18)

exp

−

n∆2
2

(cid:18)

1 −

(cid:114) ν
n

exp

−

∆2
2

(cid:0)√

n −

√

ν(cid:1)2(cid:19)

(cid:90) ∞

(cid:18)

≤ 1 +

exp

−

(cid:0)√

x −

√

ν(cid:1)2(cid:19)

dx

= 1 +

+

ν

exp

−

dy

(cid:17)

(cid:18)

(cid:19)

y2
2

ν
2
∆

(cid:90) ∞

(cid:16) y
∆

0

√

∆2
2
√

= 1 +

2πν
∆

2
∆2 +
Hence, as ν ≤ 2 log(∆2T /2)/∆2 whenever T ∆2 ≥ 2,
(cid:18) T
n

2
∆2

ˆµn +

T
(cid:88)

≥ ∆

2
n

log

log

(cid:115)

(cid:32)

(cid:32)

(cid:33)

≤

(cid:19)

P

.

n=1

(cid:19)

(cid:18) T ∆2
2

(cid:115)

+ 1 +

π log

(cid:19)(cid:33)

(cid:18) T ∆2
2

+ 1 .

Proof of Lemma 1.(c). Let x > 0 and n ∈ N, then by the reﬂection principle (eg., Mörters and Peres
[2010]) it holds that

P (∃s ≤ n : sˆµs + x ≤ 0) = 2P (nˆµn + x ≤ 0) ≤ 2 min

(cid:114) n
2π
We prepare to use the peeling technique with a carefully chosen grid. Let

(cid:26) 1
x

(cid:27)

(cid:18)

, 1

exp

−

(cid:19)

x2
2n

(7)

η =

log(ε2T )
log(ε2T ) − 1

and Gk = [ηk, ηk+1[ .

As T ε2 > e2, one has η ∈]1, 2[. Moreover, our choice of η leads to the following inequality, that will
be useful in the sequel

1
η ≤ e (x/T )

(8)

(x/T )
Using a union bound and then Eq. (7), one can write
(cid:115)

∀x ≥ ε−2,

(cid:32)

(cid:33)

P

∃s ≤ T : ˆµs +

+ ε ≤ 0

2
s

log

(cid:19)

(cid:18) T
s
(cid:115)

(cid:18)

1

(cid:32)

∞
(cid:88)

P

≤

∃s ∈ Gk : sˆµs +

2ηk log

1 ∨

(cid:19)

T
ηk+1

(cid:33)

+ ηkε ≤ 0

k=0

∞
(cid:88)

k=0

∞
(cid:88)

k=0






(cid:114) η
4π

k

≤

2 min

1,

(cid:114)

(cid:16)

log

1 ∨ T

ηk+1

(cid:17)

+ ε(cid:112)ηk/2

≤

f (k) ≤ 2 max

f (k) +

f (u)du ,

(cid:90) ∞

0






(cid:18) ηk+1
T

(cid:19) 1

η

(cid:18)

exp

−

(cid:19)

ηk−1ε2
2

where the function f is deﬁned on [0, +∞[ by

f (u) = 2 min

1,

(cid:115)






4π log






(cid:18) ηu+1
T

η
(cid:16) T

ηu+1

(cid:17)

(cid:19) 1

η

(cid:18)

exp

−

ηu−1ε2
2

(cid:19)

,

18

with the convention that

(cid:17) = +∞ for u such that T /ηu+1 < 1.

(cid:114)

η
(cid:16) T

ηu+1

4π log

The last inequality relies on the fact that f can be checked to be unimodal, which permits to upper
bound the sum with an integral. The maximum of f is easily upper bounded as follows, using notably
(8):

max
k

f (k) ≤ 2 sup
k≥0

(cid:18) ηk+1
T

(cid:19) 1

η

(cid:18)

exp

−

(cid:19)

ηk−1ε2
2

= 2 exp(−1/η)

(cid:19) 1

η

(cid:18) 2η
ε2T

≤

8e
ε2T

.

The remainder of the proof is spent bounding the integral, which will be split into three disjoint
intervals with boundaries at constants k1 < k2 given by

k1 = log(1/ε2)/ log(η)

k2 = 1 +

log

(cid:17)

(cid:16) − log log(η)
ε2
log(η)

.

These are chosen such that

ηk1 = ε−2

First, one has

(cid:18)

exp

−

(cid:19)

ηk2−1ε2
2

= (cid:112)log(η) .

(cid:18) ηu+1
T

(cid:17)

(cid:19) 1

η

(cid:18)

exp

−

ηu−1ε2
2

(cid:19)

du

I1 :=

f (u)du ≤

(cid:90) k1

0

√

2
(cid:16) T

≤

(cid:114)

π log

(cid:90) k1

(cid:114)

0

√

2
(cid:16) T

ηu+1

π log

(cid:90) k1

(cid:17)

0

(cid:19) 1

η

(cid:18) ηu+1
T

ηk1+1
√
e
(cid:16) ε2T
η

2η2
(cid:17)

≤

(cid:114)

≤

(cid:114)

π log

log(η)ε2T

π log

√

2
(cid:16) ε2T
η

π log

log(ε2T )
ε2T

,

(cid:17)

du ≤

(cid:114)

√
e

2η2
(cid:16) ε2T
η

η
log(η)

(cid:16) η
ε2T

(cid:17)

(cid:17) 1

η

where the last inequality follows from the fact that (log(η))−1 ≤ η/(η − 1) = log (cid:0)ε2T (cid:1) .
Secondly,

I2 :=

f (u)du ≤

(cid:90) k2

k1

(cid:90) k2

(cid:114)

k1

√

2
(cid:16) T

ηu+1

π log

(cid:18) ηu+1
T

(cid:17)

(cid:19) 1

η

(cid:18)

exp

−

ηu−1ε2
2

(cid:19)

du

(cid:90) ∞

(cid:17)

k1

ηu+1
T

(cid:18)

exp

−

(cid:19)

ηu−1ε2
2

du =

(cid:114)

η2
ε2T log(η)

(cid:17)

≤

(cid:114)

√

2η2

2e
(cid:16)

π log

ε2T
−η2 log log(η)

√

e

2
(cid:16) T

π log

ηk2+1

log(ε2T )
ε2T

(cid:17)

2η2 exp

(cid:17)

(cid:16)

− ηk1−1ε2
2
ε2T log(η)

(cid:17)

√

e

2
(cid:16) T

√

2
(cid:16) T

≤

(cid:114)

π log

ηk2+1

≤

(cid:114)

2e

π log

≤

(cid:114)

ηk2+1
√

2η2

2e
(cid:16)

log(ε2T )
ε2T

(cid:17)

π log

ε2T
η2 log log(ε2T )

The second inequality follows from (8), since by deﬁnition of k1, one has ηu+1 ≥ ε−2 for u ≥ k1,
whereas the last inequalities use again that (log(η))−1 ≤ log(ε2T ). Using similar arguments for the

19

third term, one has

I3 :=

f (u)du ≤ 2

(cid:90) ∞

k2

(cid:90) ∞

≤ 2e

ηu+1
T

k2
4eη2
ε2T (cid:112)log(η)

=

exp

−

≤

4eη2
ε2T

(cid:90) ∞

k2

(cid:18)

(cid:18) ηu+1
T

(cid:19) 1

η

(cid:18)

exp

−

ηu−1ε2
2

(cid:19)

du

(cid:19)

ηu−1ε2
2

du =

4eη2 exp

(cid:17)

(cid:16)

− ηk2−1ε2
2
ε2T log(η)

(cid:112)log(ε2T )

Combining the three upper bounds yield

(cid:90) ∞

0

f (u)du ≤ I1 + I2 + I3

≤

eη2 log(ε2T )
ε2T

≤

4e log(ε2T )
ε2T







√

2
(cid:16) ε2T
η

(cid:114)

π log







√

(cid:113)

2
π log (cid:0) ε2T

2

(cid:1)

√
2

2

+

(cid:114)

(cid:17)

(cid:16)

π log

ε2T
η2 log log(ε2T )



+

(cid:17)

4
(cid:112)log(ε2T )





+

(cid:114)

√

2

2
(cid:16)

π log

ε2T
4 log log(ε2T )



+

(cid:17)

4
(cid:112)log(ε2T )





It can be shown, using notably the inequality log u ≤ u/e, that for all ε2T ≥ e2,

(cid:18)

log

log(ε2T /2) ≥ (1/2) log(ε2T )
ε2T
4 log log(ε2T )

4
e2

1 −

≥

(cid:18)

(cid:19)

(cid:19)

log(ε2T )

and one obtains

(cid:90) ∞

0

f (u)du ≤

4e log(ε2T )
ε2T

×

2 + 2e

√

√

e2 − 4) + 4

2/(
(cid:112)π log(ε2T )

√

π

≤

30e
ε2T

(cid:112)log(ε2T ).

Finally,

(cid:32)

P

∃s ≤ T : ˆµs +

(cid:115)

2
s

log

(cid:19)

(cid:18) T
s

(cid:33)

(cid:90) ∞

+ ε ≤ 0

≤

f (u)du + 2 max

f (k) ≤

0

k

30e
ε2T

(cid:112)log(ε2T ) +

16e
ε2T

.

20

