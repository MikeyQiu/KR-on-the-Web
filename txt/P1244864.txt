8
1
0
2
 
l
u
J
 
2
1
 
 
]

V
C
.
s
c
[
 
 
2
v
8
5
7
2
0
.
7
0
8
1
:
v
i
X
r
a

Image Super-Resolution Using Very Deep
Residual Channel Attention Networks

Yulun Zhang1, Kunpeng Li1, Kai Li1, Lichen Wang1,
Bineng Zhong1, and Yun Fu1,2

1Department of ECE, Northeastern University, Boston, USA
2College of Computer and Information Science, Northeastern University, Boston, USA
{yulun100,li.kai.gml,wanglichenxj}@gmail.com,
bnzhong@hqu.edu.cn, {kunpengli,yunfu}@ece.neu.edu

Abstract. Convolutional neural network (CNN) depth is of crucial im-
portance for image super-resolution (SR). However, we observe that
deeper networks for image SR are more diﬃcult to train. The low-
resolution inputs and features contain abundant low-frequency informa-
tion, which is treated equally across channels, hence hindering the rep-
resentational ability of CNNs. To solve these problems, we propose the
very deep residual channel attention networks (RCAN). Speciﬁcally, we
propose a residual in residual (RIR) structure to form very deep network,
which consists of several residual groups with long skip connections. Each
residual group contains some residual blocks with short skip connec-
tions. Meanwhile, RIR allows abundant low-frequency information to be
bypassed through multiple skip connections, making the main network
focus on learning high-frequency information. Furthermore, we propose a
channel attention mechanism to adaptively rescale channel-wise features
by considering interdependencies among channels. Extensive experiments
show that our RCAN achieves better accuracy and visual improvements
against state-of-the-art methods.

Keywords: Super-Resolution, Residual in Residual, Channel Attention

1 Introduction

We address the problem of reconstructing an accurate high-resolution (HR) im-
age given its low-resolution (LR) counterpart, usually referred as single image
super-resolution (SR) [12]. Image SR is used in various computer vision applica-
tions, ranging from security and surveillance imaging [13], medical imaging [14]
to object recognition [8]. However, image SR is an ill-posed problem, since there
exists multiple solutions for any LR input. To tackle such an inverse problem, nu-
merous learning based methods have been proposed to learn mappings between
LR and HR image pairs.

Recently, deep convolutional neural network (CNN) based methods [1–11,
15–17] have achieved signiﬁcant improvements over conventional SR methods.
Among them, Dong et al. [18] proposed SRCNN by ﬁrstly introducing a three-
layer CNN for image SR. Kim et al. increased the network depth to 20 in

2

Yulun Zhang et al.

HR

Bicubic

SRCNN [1]

FSRCNN [2]

SCN [3]

VDSR [4]

DRRN [5]

LapSRN [6] MSLapSRN [7] ENet-PAT [8] MemNet [9]

EDSR [10]

SRMDNF [11] RCAN (ours)

Fig. 1. Visual results with Bicubic (BI) degradation (4×) on “img 074” from Urban100

VDSR [4] and DRCN [19], achieving notable improvements over SRCNN. Net-
work depth was demonstrated to be of central importance for many visual recog-
nition tasks, especially when He at al. [20] proposed residual net (ResNet), which
reaches 1,000 layers with residual blocks. Such eﬀective residual learning strategy
was then introduced in many other CNN-based image SR methods [5, 8–10, 21].
Lim et al. [10] built a very wide network EDSR and a very deep one MDSR
(about 165 layers) by using simpliﬁed residual blocks. The great improvements
on performance of EDSR and MDSR indicate that the depth of representation
is of crucial importance for image SR. However, to the best of our knowledge,
simply stacking residual blocks to construct deeper networks can hardly obtain
better improvements. Whether deeper networks can further contribute to image
SR and how to construct very deep trainable networks remains to be explored.

On the other hand, most recent CNN-based methods [1–11] treat channel-
wise features equally, which lacks ﬂexibility in dealing with diﬀerent types of in-
formation (e.g., low- and high-frequency information). Image SR can be viewed
as a process, where we try to recover as more high-frequency information as pos-
sible. The LR images contain most low-frequency information, which can directly
forwarded to the ﬁnal HR outputs and don’t need too much computation. While,
the leading CNN-based methods (e.g., EDSR [10]) would extract features from
the original LR inputs and treat each channel-wise feature equally. Such process
would wastes unnecessary computations for abundant low-frequency features,
lacks discriminative learning ability across feature channels, and ﬁnally hinders
the representational power of deep networks.

To practically resolve these problems, we propose a residual channel attention
network (RCAN) to obtain very deep trainable network and adaptively learn
more useful channel-wise features simultaneously. To ease the training of very
deep networks (e.g., over 400 layers), we propose residual in residual (RIR)
structure, where the residual group (RG) serves as the basic module and long skip
connection (LSC) allows residual learning in a coarse level. In each RG module,
we stack several simpliﬁed residual block [10] with short skip connection (SSC).
The long and short skip connection as well as the short-cut in residual block allow
abundant low-frequency information to be bypassed through these identity-based
skip connections, which can ease the ﬂow of information. To make a further
step, we propose channel attention (CA) mechanism to adaptively rescale each
channel-wise feature by modeling the interdependencies across feature channels.
Such CA mechanism allows our proposed network to concentrate on more useful

Image Super-Resolution Using Very Deep RCAN

3

channels and enhance discriminative learning ability. As shown in Figure 1, our
RCAN achieves better visual SR result compared with state-of-the-art methods.
Overall, our contributions are three-fold: (1) We propose the very deep resid-
ual channel attention networks (RCAN) for highly accurate image SR. Our
RCAN can reach much deeper than previous CNN-based methods and obtains
much better SR performance. (2) We propose residual in residual (RIR) struc-
ture to construct very deep trainable networks. The long and short skip connec-
tions in RIR help to bypass abundant low-frequency information and make the
main network learn more eﬀective information. (3) We propose channel attention
(CA) mechanism to adaptively rescale features by considering interdependencies
among feature channels. Such CA mechanism further improves the representa-
tional ability of the network.

2 Related Work
Numerous image SR methods have been studied in the computer vision commu-
nity [1–11, 22]. Attention mechanism is popular in high-level vision tasks, but is
seldom investigated in low-level vision applications [23]. Due to space limitation,
here we focus on works related to CNN-based methods and attention mechanism.
Deep CNN for SR. The pioneer work was done by Dong et al. [18], who
proposed SRCNN for image SR and achieved superior performance against pre-
vious works. By introducing residual learning to ease the training diﬃculty, Kim
et al. proposed VDSR [4] and DRCN [19] with 20 layers and achieved signif-
icant improvement in accuracy. Tai et al. later introduced recursive blocks in
DRRN [5] and memory block in MemNet [9]. These methods would have to ﬁrst
interpolate the LR inputs to the desired size, which inevitably loses some details
and increases computation greatly.

Extracting features from the original LR inputs and upscaling spatial reso-
lution at the network tail then became the main choice for deep architecture. A
faster network structure FSRCNN [2] was proposed to accelerate the training and
testing of SRCNN. Ledig et al. [21] introduced ResNet [20] to construct a deeper
network, SRResNet, for image SR. They also proposed SRGAN with perceptual
losses [24] and generative adversarial network (GAN) [25] for photo-realistic SR.
Such GAN based model was then introduced in EnhanceNet [8], which com-
bines automated texture synthesis and perceptual loss. Although SRGAN and
Enhancenet can alleviate the blurring and oversmoothing artifacts to some de-
gree, their predicted results may not be faithfully reconstructed and produce
unpleasing artifacts. By removing unnecessary modules in conventional residual
networks, Lim et al. [10] proposed EDSR and MDSR, which achieve signiﬁ-
cant improvement. However, most of these methods have limited network depth,
which has demonstrated to be very important in visual recognition tasks [20] and
can reach to about 1,000 layers. Simply stacking residual blocks in MDSR [10],
very deep networks can hardly achieved improvements. Furthermore, most of
these methods treat the channel-wise features equally, hindering better discrim-
inative ability for diﬀerent types of features.

Attention mechanism. Generally, attention can be viewed as a guidance to
bias the allocation of available processing resources towards the most informative

4

Yulun Zhang et al.

Fig. 2. Network architecture of our residual channel attention network (RCAN)

components of an input [23]. Recently, tentative works have been proposed to
apply attention into deep neural networks [23, 26, 27], ranging from localization
and understanding in images [28, 29] to sequence-based networks [30, 31]. It’s
usually combined with a gating function (e.g., sigmoid) to rescale the feature
maps. Wang et al. [26] proposed residual attention network for image classi-
ﬁcation with a trunk-and-mask attention mechanism. Hu et al. [23] proposed
squeeze-and-excitation (SE) block to model channel-wise relationships to ob-
tain signiﬁcant performance improvement for image classiﬁcation. However, few
works have been proposed to investigate the eﬀect of attention for low-level vision
tasks (e.g., image SR).

In image SR, high-frequency channel-wise features are more informative for
HR reconstruction. If our network pays more attention to such channel-wise
features, it should be promising to obtain improvements. To investigate such
mechanism in very deep CNN, we propose very deep residual channel attention
networks (RCAN), which we will detail in next section.

3 Residual Channel Attention Network (RCAN)

3.1 Network Architecture

As shown in Figure 2, our RCAN mainly consists four parts: shallow feature
extraction, residual in residual (RIR) deep feature extraction, upscale module,
and reconstruction part. Let’s denote ILR and ISR as the input and output of
RCAN. As investigated in [10, 21], we use only one convolutional layer (Conv)
to extract the shallow feature F0 from the LR input

where HSF (·) denotes convolution operation. F0 is then used for deep feature
extraction with RIR module. So we can further have

F0 = HSF (ILR) ,

FDF = HRIR (F0) ,

(1)

(2)

where HRIR (·) denotes our proposed very deep residual in residual structure,
which contains G residual groups (RG). To the best of our knowledge, our pro-
posed RIR achieves the largest depth so far and provides very large receptive

Image Super-Resolution Using Very Deep RCAN

5

ﬁeld size. So we treat its output as deep feature, which is then upscaled via a
upscale module

FU P = HU P (FDF ) ,

(3)

where HU P (·) and FU P denote a upscale module and upscaled feature respec-
tively.

There’re several choices to serve as upscale modules, such as deconvolution
layer (also known as transposed convolution) [2], nearest-neighbor upsampling +
convolution [32], and ESPCN [33]. Such post-upscaling strategy has been demon-
strated to be more eﬃcient for both computation complexity and achieve higher
performance than pre-upscaling SR methods (e.g., DRRN [5] and MemNet [9]).
The upscaled feature is then reconstructed via one Conv layer

ISR = HREC (FU P ) = HRCAN (ILR) ,

(4)

where HREC (·) and HRCAN (·) denote the reconstruction layer and the function
of our RCAN respectively.

Then RCAN is optimized with loss function. Several loss functions have been
investigated, such as L2 [1–5,8,9,11,16], L1 [6,7,10,17], perceptual and adversar-
ial losses [8, 21]. To show the eﬀectiveness of our RCAN, we choose to optimize
same loss function as previous works (e.g., L1 loss function). Given a training
set (cid:8)I i
(cid:9)N
i=1, which contains N LR inputs and their HR counterparts. The
goal of training RCAN is to minimize the L1 loss function

LR, I i

HR

L (Θ) =

(cid:13)
(cid:13)HRCAN

(cid:0)I i

LR

(cid:1) − I i

HR

(cid:13)
(cid:13)1 ,

(5)

1
N

N
(cid:88)

i=1

where Θ denotes the parameter set of our network. The loss function is optimized
by using stochastic gradient descent. More details of training would be shown
in Section 4.1. As we choose the shallow feature extraction HSF (·), upscaling
module HU P (·), and reconstruction part HU P (·) as similar as previous works
(e.g., EDSR [10] and RDN [17]), we pay more attention to our proposed RIR,
CA, and the basic module RCAB.

3.2 Residual in Residual (RIR)

We now give more details about our proposed RIR structure (see Figure 2),
which contains G residual groups (RG) and long skip connection (LSC). Each
RG further contains B residual channel attention blocks (RCAB) with short skip
connection (SSC). Such residual in residual structure allows to train very deep
CNN (over 400 layers) for image SR with high performance.

It has been demonstrated that stacked residual blocks and LSC can be used
to construct deep CNN in [10]. In visual recognition, residual blocks [20] can be
stacked to achieve more than 1,000-layer trainable networks. However, in image
SR, very deep network built in such way would suﬀer from training diﬃculty

6

Yulun Zhang et al.

Fig. 3. Channel attention (CA). ⊗ denotes element-wise product

and can hardly achieve more performance gain. Inspired by previous works in
SRRestNet [21] and EDSR [10], we proposed residual group (RG) as the basic
module for deeper networks. A RG in the g-th group is formulated as

Fg = Hg (Fg−1) = Hg (Hg−1 (· · · H1 (F0) · · · )) ,

(6)

where Hg denotes the function of g-th RG. Fg−1 and Fg are the input and output
for g-th RG. We observe that simply stacking many RGs would fail to achieve
better performance. To solve the problem, the long skip connection (LSC) is
further introduced in RIR to stabilize the training of very deep network. LSC
also makes better performance possible with residual learning via

FDF = F0 + WLSCFG = F0 + WLSCHg (Hg−1 (· · · H1 (F0) · · · )) ,

(7)

where WLSC is the weight set to the Conv layer at the tail of RIR. The bias
term is omitted for simplicity. LSC can not only ease the ﬂow of information
across RGs, but only make it possible for RIR to learning residual information
in a coarse level.

As discussed in Section 1, there are lots of abundant information in the
LR inputs and features and the goal of SR network is to recover more useful
information. The abundant low-frequency information can be bypassed through
identity-based skip connection. To make a further step towards residual learning,
we stack B residual channel attention blocks in each RG. The b-th residual
channel attention block (RCAB) in g-th RG can be formulated as

Fg,b = Hg,b (Fg,b−1) = Hg,b (Hg,b−1 (· · · Hg,1 (Fg−1) · · · )) ,

(8)

where Fg,b−1 and Fg,b are the input and output of the b-th RCAB in g-th RG.
The corresponding function is denoted with Hg,b. To make the main network
pay more attention to more informative features, a short skip connection (SSC)
is introduced to obtain the block output via

Fg = Fg−1 + WgFg,B = Fg−1 + WgHg,B (Hg,B−1 (· · · Hg,1 (Fg−1) · · · )) ,

(9)

where Wg is the weight set to the Conv layer at the tail of g-th RG. The SSC
further allows the main parts of network to learn residual information. With LSC
and SSC, more abundant low-frequency information is easier bypassed in the
training process. To make a further step towards more discriminative learning,
we pay more attention to channel-wise feature rescaling with channel attention.

Image Super-Resolution Using Very Deep RCAN

7

3.3 Channel Attention (CA)

Previous CNN-based SR methods treat LR channel-wise features equally, which
is not ﬂexible for the real cases. In order to make the network focus on more
informative features, we exploit the interdependencies among feature channels,
resulting in a channel attention (CA) mechanism (see Figure 3).

How to generate diﬀerent attention for each channel-wise feature is a key
step. Here we mainly have two concerns: First, information in the LR space
has abundant low-frequency and valuable high-frequency components. The low-
frequency parts seem to be more complanate. The high-frequency components
would usually be regions, being full of edges, texture, and other details. On the
other hand, each ﬁlter in Conv layer operates with a local receptive ﬁeld. Conse-
quently, the output after convolution is unable to exploit contextual information
outside of the local region.

Based on these analyses, we take the channel-wise global spatial information
into a channel descriptor by using global average pooling. As shown in Figure 3,
let X = [x1, · · · , xc, · · · , xC] be an input, which has C feature maps with size
of H × W . The channel-wise statistic z ∈ RC can be obtained by shrinking X
through spatial dimensions H × W . Then the c-th element of z is determined by

zc = HGP (xc) =

xc (i, j) ,

(10)

1
H × W

H
(cid:88)

W
(cid:88)

i=1

j=1

where xc (i, j) is the value at position (i, j) of c-th feature xc. HGP (·) denotes the
global pooling function. Such channel statistic can be viewed as a collection of
the local descriptors, whose statistics contribute to express the whole image [23].
Except for global average pooling, more sophisticated aggregation techniques
could also be introduced here.

To fully capture channel-wise dependencies from the aggregated informa-
tion by global average pooling, we introduce a gating mechanism. As discussed
in [23], the gating mechanism should meet two criteria: First, it must be able
to learn nonlinear interactions between channels. Second, as multiple channel-
wise features can be emphasized opposed to one-hot activation, it must learn
a non-mututually-exclusive relationship. Here, we opt to exploit simple gating
mechanism with sigmoid function

s = f (WU δ (WDz)) ,

(11)

where f (·) and δ (·) denote the sigmoid gating and ReLU [34] function, respec-
tively. WD is the weight set of a Conv layer, which acts as channel-downscaling
with reduction ratio r. After being activated by ReLU, the low-dimension signal
is then increased with ratio r by a channel-upscaling layer, whose weight set is
WU . Then we obtain the ﬁnal channel statistics s, which is used to rescale the
input xc

(cid:98)xc = sc · xc,
where sc and xc are the scaling factor and feature map in the c-th channel. With
channel attention, the residual component in the RCAB is adaptively rescaled.

(12)

8

Yulun Zhang et al.

Fig. 4. Residual channel attention block (RCAB)

3.4 Residual Channel Attention Block (RCAB)
As discussed above, residual groups and long skip connection allow the main
parts of network to focus on more informative components of the LR features.
Channel attention extracts the channel statistic among channels to further en-
hance the discriminative ability of the network.

At the same time, inspired by the success of residual blocks (RB) in [10], we
integrate CA into RB and propose residual channel attention block (RCAB) (
see Figure 4). For the b-th RB in g-th RG, we have

Fg,b = Fg,b−1 + Rg,b (Xg,b) · Xg,b,

where Rg,b denotes the function of channel attention. Fg,b and Fg,b−1 are the
input and output of RCAB, which learns the residual Xg,b from the input. The
residual component is mainly obtained by two stacked Conv layers

Xg,b = W 2

g,bδ (cid:0)W 1

g,bFg,b−1

(cid:1) ,

(13)

(14)

where W 1

g,b and W 2

g,b are weight sets the two stacked Conv layers in RCAB.

We further show the relationships between our proposed RCAB and residual
block (RB) in [10]. We ﬁnd that the RBs used in MDSR and EDSR [10] can be
viewed as special cases of our RCAB. For RB in MDSR, there is no rescaling
operation. It is the same as RCAB, where we set Rg,b (·) as constant 1. For RB
with constant rescaling (e.g., 0.1) in EDSR, it is the same as RCAB with Rg,b (·)
set to be 0.1. Although the channel-wise feature rescaling is introduced to train
a very wide network, the interdependencies among channels are not considered
in EDSR. In these cases, the CA is not considered.

Based on residual channel attention block (RCAB) and RIR structure, we
construct a very deep RCAN for highly accurate image SR and achieve no-
table performance improvements over previous leading methods. More discus-
sions about the eﬀects of each proposed component are shown in Section 4.2.

Implementation Details

3.5
Now we specify the implementation details of our proposed RCAN. We set RG
number as G=10 in the RIR structure. In each RG, we set RCAB number as
20. We set 3×3 as the size of all Conv layers except for that in the channel-
downscaling and channel-upscaling, whose kernel size is 1×1. For Conv layers
with kernel size 3×3, zero-padding strategy is used to keep size ﬁxed. Conv
layers in shallow feature extraction and RIR structure have C=64 ﬁlters, except
for that in the channel-downscaling. Conv layer in channel-downscaling has C
r =4
ﬁlters, where the reduction ratio r is set as 16. For upscaling module HU P (·), we
follow [10, 17,33] and use ESPCNN [33] to upscale the coarse resolution features
to ﬁne ones. The ﬁnal Conv layer has 3 ﬁlters, as we output color images. While,
our network can also process gray images.

Image Super-Resolution Using Very Deep RCAN

9

Table 1. Investigations of RIR (including LSC and SSC) and CA. We observe the best
PSNR (dB) values on Set5 (2×) in 5×104 iterations

Residual in Residual (RIR)

Channel attention (CA)
PSNR on Set5 (2×)

LSC % ! % ! % ! % !
SSC % % ! ! % % ! !
% % % % ! ! ! !

37.45 37.77 37.81 37.87 37.52 37.85 37.86 37.90

4 Experiments
4.1 Settings
We clarify the experimental settings about datasets, degradation models, evalu-
ation metric, and training settings.

Datasets and degradation models. Following [10, 11, 17, 35], we use 800
training images from DIV2K dataset [35] as training set. For testing, we use ﬁve
standard benchmark datasets: Set5 [36], Set14 [37], B100 [38], Urban100 [22], and
Manga109 [39]. We conduct experiments with Bicubic (BI) and blur-downscale
(BD) degradation models [11, 15, 17].

Evaluation metrics. The SR results are evaluated with PSNR and SSIM [40]
on Y channel (i.e., luminance) of transformed YCbCr space. We also provide
performance (e.g., top-1 and top-5 recognition errors) comparisons on object
recognition by several leading SR methods.

Training settings. Data augmentation is performed on the 800 training
images, which are randomly rotated by 90◦, 180◦, 270◦ and ﬂipped horizontally.
In each training batch, 16 LR color patches with the size of 48 × 48 are extracted
as inputs. Our model is trained by ADAM optimizor [41] with β1 = 0.9, β2 =
0.999, and (cid:15) = 10−8. The initial leaning rate is set to 10−4 and then decreases
to half every 2 × 105 iterations of back-propagation. We use PyTorch [42] to
implement our models with a Titan Xp GPU.1

4.2 Eﬀects of RIR and CA
We study the eﬀects of residual in residual (RIR) and channel attention (CA).
Residual in residual (RIR). To demonstrate the eﬀect of our proposed
residual in residual structure, we remove long skip connection (LSC) or/and
short skip connection (SSC) from very deep networks. Speciﬁcally, we set the
number of residual block as 200, namely 10 residual groups, resulting in very
deep networks with over 400 Conv layers. In Table 1, when both LSC and SSC
are removed, the PSNR value on Set5 (×2) is relatively low, no matter channel
attention (CA) is used or not. For example, in the ﬁrst column, the PSNR is 37.45
dB. After adding RIR, the performance reaches 37.87 dB. When CA is added,
the performance can be improved from 37.52 dB to 37.90 dB by using RIR.
This indicates that simply stacking residual blocks is not applicable to achieve
very deep and powerful networks for image SR. The performance would increase
with LSC or SSC and can obtain better results by using both of them. These
comparisons show that LSC and SSC are essential for very deep networks. They
also demonstrate the eﬀectiveness of our proposed residual in residual (RIR)
structure for very deep networks.

1 The RCAN source code is available at https://github.com/yulunzhang/RCAN.

10

Yulun Zhang et al.

Channel attention (CA). We further show the eﬀect of channel atten-
tion (CA) based on the observations and discussions above. When we compare
the results of ﬁrst 4 columns and last 4 columns, we ﬁnd that networks with
CA would perform better than those without CA. Beneﬁtting from very large
network depth, the very deep trainable networks can achieve a very high per-
formance. It’s hard to obtain further improvements from such deep networks,
but we obtain improvements with CA. Even without RIR, CA can improve the
performance from 37.45 dB to 37.52 dB. These comparisons ﬁrmly demonstrate
the eﬀectiveness of CA and indicate adaptive attentions to channel-wise features
really improves the performance.

Table 2. Quantitative results with BI degradation model. Best and second best results
are highlighted and underlined

Set5

Scale

Set14

Method

Urban100

Manga109

B100
PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM
33.66 0.9299 30.24 0.8688 29.56 0.8431 26.88 0.8403 30.80 0.9339
×2
Bicubic
36.66 0.9542 32.45 0.9067 31.36 0.8879 29.50 0.8946 35.60 0.9663
×2
SRCNN [1]
37.05 0.9560 32.66 0.9090 31.53 0.8920 29.88 0.9020 36.67 0.9710
×2
FSRCNN [2]
37.53 0.9590 33.05 0.9130 31.90 0.8960 30.77 0.9140 37.22 0.9750
×2
VDSR [4]
37.52 0.9591 33.08 0.9130 31.08 0.8950 30.41 0.9101 37.27 0.9740
×2
LapSRN [6]
37.78 0.9597 33.28 0.9142 32.08 0.8978 31.31 0.9195 37.72 0.9740
×2
MemNet [9]
38.11 0.9602 33.92 0.9195 32.32 0.9013 32.93 0.9351 39.10 0.9773
EDSR [10]
×2
37.79 0.9601 33.32 0.9159 32.05 0.8985 31.33 0.9204 38.07 0.9761
SRMDNF [11] ×2
38.09 0.9600 33.85 0.9190 32.27 0.9000 32.55 0.9324 38.89 0.9775
×2
D-DBPN [16]
38.24 0.9614 34.01 0.9212 32.34 0.9017 32.89 0.9353 39.18 0.9780
×2
RDN [17]
RCAN (ours)
38.27 0.9614 34.12 0.9216 32.41 0.9027 33.34 0.9384 39.44 0.9786
×2
RCAN+ (ours) ×2 38.33 0.9617 34.23 0.9225 32.46 0.9031 33.54 0.9399 39.61 0.9788
×3
30.39 0.8682 27.55 0.7742 27.21 0.7385 24.46 0.7349 26.95 0.8556
Bicubic
×3
32.75 0.9090 29.30 0.8215 28.41 0.7863 26.24 0.7989 30.48 0.9117
SRCNN [1]
×3
33.18 0.9140 29.37 0.8240 28.53 0.7910 26.43 0.8080 31.10 0.9210
FSRCNN [2]
×3
33.67 0.9210 29.78 0.8320 28.83 0.7990 27.14 0.8290 32.01 0.9340
VDSR [4]
×3
33.82 0.9227 29.87 0.8320 28.82 0.7980 27.07 0.8280 32.21 0.9350
LapSRN [6]
34.09 0.9248 30.00 0.8350 28.96 0.8001 27.56 0.8376 32.51 0.9369
×3
MemNet [9]
34.65 0.9280 30.52 0.8462 29.25 0.8093 28.80 0.8653 34.17 0.9476
EDSR [10]
×3
34.12 0.9254 30.04 0.8382 28.97 0.8025 27.57 0.8398 33.00 0.9403
SRMDNF [11] ×3
34.71 0.9296 30.57 0.8468 29.26 0.8093 28.80 0.8653 34.13 0.9484
×3
RDN [17]
RCAN (ours)
34.74 0.9299 30.65 0.8482 29.32 0.8111 29.09 0.8702 34.44 0.9499
×3
RCAN+ (ours) ×3 34.85 0.9305 30.76 0.8494 29.39 0.8122 29.31 0.8736 34.76 0.9513
×4
28.42 0.8104 26.00 0.7027 25.96 0.6675 23.14 0.6577 24.89 0.7866
Bicubic
×4
30.48 0.8628 27.50 0.7513 26.90 0.7101 24.52 0.7221 27.58 0.8555
SRCNN [1]
×4
30.72 0.8660 27.61 0.7550 26.98 0.7150 24.62 0.7280 27.90 0.8610
FSRCNN [2]
×4
31.35 0.8830 28.02 0.7680 27.29 0.0726 25.18 0.7540 28.83 0.8870
VDSR [4]
×4
31.54 0.8850 28.19 0.7720 27.32 0.7270 25.21 0.7560 29.09 0.8900
LapSRN [6]
31.74 0.8893 28.26 0.7723 27.40 0.7281 25.50 0.7630 29.42 0.8942
×4
MemNet [9]
32.46 0.8968 28.80 0.7876 27.71 0.7420 26.64 0.8033 31.02 0.9148
EDSR [10]
×4
31.96 0.8925 28.35 0.7787 27.49 0.7337 25.68 0.7731 30.09 0.9024
SRMDNF [11] ×4
32.47 0.8980 28.82 0.7860 27.72 0.7400 26.38 0.7946 30.91 0.9137
×4
D-DBPN [16]
32.47 0.8990 28.81 0.7871 27.72 0.7419 26.61 0.8028 31.00 0.9151
×4
RDN [17]
RCAN (ours)
32.63 0.9002 28.87 0.7889 27.77 0.7436 26.82 0.8087 31.22 0.9173
×4
RCAN+ (ours) ×4 32.73 0.9013 28.98 0.7910 27.85 0.7455 27.10 0.8142 31.65 0.9208
×8
24.40 0.6580 23.10 0.5660 23.67 0.5480 20.74 0.5160 21.47 0.6500
Bicubic
×8
25.33 0.6900 23.76 0.5910 24.13 0.5660 21.29 0.5440 22.46 0.6950
SRCNN [1]
×8
20.13 0.5520 19.75 0.4820 24.21 0.5680 21.32 0.5380 22.39 0.6730
FSRCNN [2]
×8
25.59 0.7071 24.02 0.6028 24.30 0.5698 21.52 0.5571 22.68 0.6963
SCN [3]
×8
25.93 0.7240 24.26 0.6140 24.49 0.5830 21.70 0.5710 23.16 0.7250
VDSR [4]
26.15 0.7380 24.35 0.6200 24.54 0.5860 21.81 0.5810 23.39 0.7350
×8
LapSRN [6]
26.16 0.7414 24.38 0.6199 24.58 0.5842 21.89 0.5825 23.56 0.7387
MemNet [9]
×8
26.34 0.7558 24.57 0.6273 24.65 0.5895 22.06 0.5963 23.90 0.7564
MSLapSRN [7] ×8
26.96 0.7762 24.91 0.6420 24.81 0.5985 22.51 0.6221 24.69 0.7841
×8
EDSR [10]
27.21 0.7840 25.13 0.6480 24.88 0.6010 22.73 0.6312 25.14 0.7987
×8
D-DBPN [16]
RCAN (ours)
27.31 0.7878 25.23 0.6511 24.98 0.6058 23.00 0.6452 25.24 0.8029
×8
RCAN+ (ours) ×8 27.47 0.7913 25.40 0.6553 25.05 0.6077 23.22 0.6524 25.58 0.8092

Image Super-Resolution Using Very Deep RCAN

11

HR
PSNR/SSIM

Bicubic
21.08/0.6788

SRCNN [1]
22.13/0.7635

FSRCNN [2]
22.02/0.7628

VDSR [4]
22.37/0.7939

Urban100 (4×):
img 004

LapSRN [6]
22.41/0.7984

MemNet [9]
22.35/0.7992

EDSR [10]
24.07/0.8591

SRMDNF [11]
22.93/0.8207

RCAN
25.64/0.8830

HR
PSNR/SSIM

Bicubic
19.48/0.4371

SRCNN [1]
19.94/0.5124

FSRCNN [2]
19.88/0.5158

VDSR [4]
19.88/0.5229

Urban100 (4×):
img 073

LapSRN [6]
19.76/0.5250

MemNet [9]
19.71/0.5213

EDSR [10]
20.42/0.6028

SRMDNF [11]
19.88/0.5425

RCAN
21.26/0.6298

HR
PSNR/SSIM

Bicubic
24.66/0.7849

SRCNN [1]
26.22/0.8464

FSRCNN [2]
26.38/0.8496

VDSR [4]
26.89/0.8703

Manga109 (4×):
YumeiroCooking

LapSRN [6]
26.92/0.8739

MemNet [9]
27.09/0.8811

EDSR [10]
29.04/0.9230

SRMDNF [11]
27.53/0.8901

RCAN
29.85/0.9368

Fig. 5. Visual comparison for 4× SR with BI model on Urban100 and Manga109
datasets. The best results are highlighted

4.3 Results with Bicubic (BI) Degradation Model

We compare our method with 11 state-of-the-art methods: SRCNN [1], FSR-
CNN [2], SCN [3], VDSR [4], LapSRN [6], MemNet [9], EDSR [10], SRMDNF [11],
D-DBPN [16], and RDN [17]. Similar to [10, 17, 43], we also introduce self-
ensemble strategy to further improve our RCAN and denote the self-ensembled
one as RCAN+. More comparisons are provided in supplementary material.

Quantitative results by PSNR/SSIM. Table 2 shows quantitative com-
parisons for ×2, ×3, ×4, and ×8 SR. The results of D-DBPN [16] are cited from
their paper. When compared with all previous methods, our RCAN+ performs
the best on all the datasets with all scaling factors. Even without self-ensemble,
our RCAN also outperforms other compared methods.

On the other hand, when the scaling factor become larger (e.g., 8), the gains
of our RCAN over EDSR also becomes larger. For Urban100 and Manga109, the
PSNR gains of RCAN over EDSR are 0.49 dB and 0.55 dB. EDSR has much
larger number of parameters (43 M) than ours (16 M), but our RCAN obtains
much better performance. Instead of constantly rescaling the features in EDSR,
our RCAN adaptively rescales features with channel attention (CA). CA allows
our network to further focus on more informative features. This observation
indicates that very large network depth and CA improve the performance.

Visual results. In Figure 5, we show visual comparisons on scale ×4. For im-
age “img 004”, we observe that most of the compared methods cannot recover
the lattices and would suﬀer from blurring artifacts. In contrast, our RCAN
can alleviate the blurring artifacts better and recover more details. For image

12

Yulun Zhang et al.

HR
PSNR/SSIM

Bicubic
15.89/0.4595

SRCNN [1]
17.48/0.5927

SCN [3]
17.64/0.6410

VDSR [4]
17.59/0.6612

Urban100 (8×):
img 040

LapSRN [6]
18.27/0.7182

MemNet [9] MSLapSRN [7]
18.17/0.7190

18.52/0.7525

EDSR [10]
19.53/0.7857

RCAN
22.43/0.8607

HR
PSNR/SSIM

Bicubic
24.89/0.7572

SRCNN [1]
25.58/0.6993

SCN [3]
26.62/0.8035

VDSR [4]
26.33/0.8091

Manga109 (8×):
TaiyouNiSmash

LapSRN [6]
27.26/0.8278

MemNet [9] MSLapSRN [7]
27.47/0.8353

28.02/0.8532

EDSR [10]
29.44/0.8746

RCAN
30.67/0.8961

Fig. 6. Visual comparison for 8× SR with BI model on Urban100 and Manga109
datasets. The best results are highlighted

“img 073”, most of the compared methods produce blurring artifacts along the
horizontal lines. What’s worse, for the right parts of the cropped images, FSR-
CNN cannot recover lines. Other methods would generate some lines with wrong
directions. Only our RCAN produces more faithful results. For image “Yumeiro-
Cooking”, the cropped part is full of textures. As we can see, all the compared
methods suﬀer from heavy blurring artifacts, failing to recover more details.
While, our RCAN can recover them obviously, being more faithful to the ground
truth. Such obvious comparisons demonstrate that networks with more power-
ful representational ability can extract more sophisticated features from the LR
space.

To further illustrate the analyses above, we show visual comparisons for 8×
SR in Figure 6. For image “img 040”, due to very large scaling factor, the result
by Bicubic would lose the structures and produce diﬀerent structures. This wrong
pre-scaling result would also lead some state-of-the-art methods (e.g., SRCNN,
VDSR, and MemNet) to generate totally wrong structures. Even starting from
the original LR input, other methods cannot recover the right structure either.
While, our RCAN can recover them correctly. For smaller details, like the net in
image “TaiyouNiSmash”, the tiny lines can be lost in the LR image. When the
scaling factor is very large (e.g., 8), LR images contain very limited information
for SR. Losing most high-frequency information makes it very diﬃcult for SR
methods to reconstruct informative results. Most of compared methods cannot
achieve this goal and produce serious blurring artifacts. However, our RCAN can
obtain more useful information and produce ﬁner results.

As we have discussed above, in BI degradation model, the reconstruction of
high-frequency information is very important and diﬃcult, especially with large
scaling factor (e.g., 8). Our proposed RIR structure makes the main network
learn residual information. Channel attention (CA) is further used to enhance
the representational ability of the network by adaptively rescaling channel-wise
features.

Image Super-Resolution Using Very Deep RCAN

13

Table 3. Quantitative results with BD degradation model. Best and second best results
are highlighted and underlined

Set5

Scale

Set14

Method

Urban100

B100
PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM
28.78 0.8308 26.38 0.7271 26.33 0.6918 23.52 0.6862 25.46 0.8149
×3
Bicubic
32.21 0.9001 28.89 0.8105 28.13 0.7740 25.84 0.7856 29.64 0.9003
×3
SPMSR [44]
32.05 0.8944 28.80 0.8074 28.13 0.7736 25.70 0.7770 29.47 0.8924
×3
SRCNN [1]
26.23 0.8124 24.44 0.7106 24.86 0.6832 22.04 0.6745 23.04 0.7927
×3
FSRCNN [2]
33.25 0.9150 29.46 0.8244 28.57 0.7893 26.61 0.8136 31.06 0.9234
×3
VDSR [4]
33.38 0.9182 29.63 0.8281 28.65 0.7922 26.77 0.8154 31.15 0.9245
IRCNN [15]
×3
34.01 0.9242 30.11 0.8364 28.98 0.8009 27.50 0.8370 32.97 0.9391
SRMDNF [11] ×3
34.58 0.9280 30.53 0.8447 29.23 0.8079 28.46 0.8582 33.97 0.9465
×3
RDN [17]
RCAN (ours)
34.70 0.9288 30.63 0.8462 29.32 0.8093 28.81 0.8647 34.38 0.9483
×3
RCAN+ (ours) ×3 34.83 0.9296 30.76 0.8479 29.39 0.8106 29.04 0.8682 34.76 0.9502

Manga109

HR
PSNR/SSIM

Bicubic
20.20/0.6737

SPMSR [44]
21.72/0.7923

SRCNN [1]
21.74/0.7882

FSRCNN [2]
19.30/0.6960

Urban100 (3×):
img 062

VDSR [4]
22.36/0.8351

IRCNN [15]
22.32/0.8292

SRMDNF [11]
23.11/0.8662

RDN [17]
24.42/0.9052

RCAN
25.73/0.9238

HR
PSNR/SSIM

Bicubic
26.10/0.7032

SPMSR [44]
28.06/0.7950

SRCNN [1]
27.91/0.7874

FSRCNN [2]
24.34/0.6711

Urban100 (3×):
img 078

VDSR [4]
28.34/0.8166

IRCNN [15]
28.57/0.8184

SRMDNF [11]
29.08/0.8342

RDN [17]
29.94/0.8513

RCAN
30.65/0.8624

Fig. 7. Visual comparison for 3× SR with BD model on Urban100 dataset. The best
results are highlighted

4.4 Results with Blur-downscale (BD) Degradation Model

We further apply our method to super-resolve images with blur-down (BD)
degradation model, which is also commonly used recently [11, 15, 17].

Quantitative results by PSNR/SSIM. Here, we compare 3× SR re-
sults with 7 state-of-the-art methods: SPMSR [44], SRCNN [1], FSRCNN [2],
VDSR [4], IRCNN [15], SRMDNF [11], and RDN [17]. As shown in Table 3,
RDN has achieved very high performance on each dataset. While, our RCAN
can obtain notable gains over RDN. Using self-ensemble, RCAN+ achieves even
better results. Compared with fully using hierarchical features in RDN, a much
deeper network with channel attention in RCAN achieves better performance.
This comparison also indicates that there has promising potential to investigate
much deeper networks for image SR.

Visual Results. We also show visual comparisons in Figure 7. For challeng-
ing details in images “img 062” and “img 078”, most methods suﬀer from heavy
blurring artifacts. RDN alleviates it to some degree and can recover more details.
In contrast, our RCAN obtains much better results by recovering more informa-
tive components. These comparisons indicate that very deep channel attention
guided network would alleviate the blurring artifacts. It also demonstrates the
strong ability of RCAN for BD degradation model.

14

Yulun Zhang et al.

Table 4. ResNet object recognition performance. The best results are highlighted
Evaluation Bicubic DRCN [19] FSRCNN [2] PSyCo [45] ENet-E [8] RCAN Baseline
Top-1 error 0.506
Top-5 error 0.266

0.393 0.260
0.167 0.072

0.477
0.242

0.437
0.196

0.454
0.224

0.449
0.214

(a) Results on Set5 (4×)

(b) Results on Set5 (8×)

Fig. 8. Performance and number of parameters. Results are evaluated on Set5

4.5 Object Recognition Performance
Image SR also serves as pre-processing step for high-level visual tasks (e.g.,
object recognition). We evaluate the object recognition performance to further
demonstrate the eﬀectiveness of our RCAN.

Here we use the same settings as ENet [8]. We use ResNet-50 [20] as the
evaluation model and use the ﬁrst 1,000 images from ImageNet CLS-LOC val-
idation dataset for evaluation. The original cropped 224×224 images are used
for baseline and downscaled to 56×56 for SR methods. We use 4 stat-of-the-art
methods (e.g., DRCN [19], FSRCNN [2], PSyCo [45], and ENet-E [8]) to upscale
the LR images and then calculate their accuracies. As shown in Table 4, our
RCAN achieves the lowest top-1 and top-5 errors. These comparisons further
demonstrate the highly powerful representational ability of our RCAN.
4.6 Model Size Analyses
We show comparisons about model size and performance in Figure 8. Although
our RCAN is the deepest network, it has less parameter number than that of
EDSR and RDN. Our RCAN and RCAN+ achieve higher performance, having a
better tradeoﬀ between model size and performance. It also indicates that deeper
networks may be easier to achieve better performance than wider networks.
5 Conclusions
We propose very deep residual channel attention networks (RCAN) for highly
accurate image SR. Speciﬁcally, the residual in residual (RIR) structure allows
RCAN to reach very large depth with LSC and SSC. Meanwhile, RIR allows
abundant low-frequency information to be bypassed through multiple skip con-
nections, making the main network focus on learning high-frequency information.
Furthermore, to improve ability of the network, we propose channel attention
(CA) mechanism to adaptively rescale channel-wise features by considering in-
terdependencies among channels. Extensive experiments on SR with BI and BD
models demonstrate the eﬀectiveness of our proposed RCAN. RCAN also shows
promissing results for object recognition.
Acknowledgements: This research is supported in part by the NSF IIS award
1651902, ONR Young Investigator Award N00014-14-1-0484, and U.S. Army
Research Oﬃce Award W911NF-17-1-0367.

Image Super-Resolution Using Very Deep RCAN

15

References

1. Dong, C., Loy, C.C., He, K., Tang, X.: Image super-resolution using deep convo-

2. Dong, C., Loy, C.C., Tang, X.: Accelerating the super-resolution convolutional

lutional networks. TPAMI (2016)

neural network. In: ECCV. (2016)

3. Wang, Z., Liu, D., Yang, J., Han, W., Huang, T.: Deep networks for image super-

resolution with sparse prior. In: ICCV. (2015)

4. Kim, J., Kwon Lee, J., Mu Lee, K.: Accurate image super-resolution using very

deep convolutional networks. In: CVPR. (2016)

Image super-resolution via deep recursive residual

5. Tai, Y., Yang, J., Liu, X.:
network. In: CVPR. (2017)

6. Lai, W.S., Huang, J.B., Ahuja, N., Yang, M.H.: Deep laplacian pyramid networks

for fast and accurate super-resolution. In: CVPR. (2017)

7. Lai, W.S., Huang, J.B., Ahuja, N., Yang, M.H.: Fast and accurate image super-

resolution with deep laplacian pyramid networks. arXiv:1710.01992 (2017)

8. Sajjadi, M.S., Sch¨olkopf, B., Hirsch, M.: Enhancenet: Single image super-resolution

through automated texture synthesis. In: ICCV. (2017)

9. Tai, Y., Yang, J., Liu, X., Xu, C.: Memnet: A persistent memory network for

image restoration. In: ICCV. (2017)

10. Lim, B., Son, S., Kim, H., Nah, S., Lee, K.M.: Enhanced deep residual networks

for single image super-resolution. In: CVPRW. (2017)

11. Zhang, K., Zuo, W., Zhang, L.: Learning a single convolutional super-resolution

network for multiple degradations. In: CVPR. (2018)

12. Freeman, W.T., Pasztor, E.C., Carmichael, O.T.: Learning low-level vision. IJCV

(2000)

13. Zou, W.W., Yuen, P.C.: Very low resolution face recognition problem. TIP (2012)
14. Shi, W., Caballero, J., Ledig, C., Zhuang, X., Bai, W., Bhatia, K., de Marvao,
A.M.S.M., Dawes, T., ORegan, D., Rueckert, D.: Cardiac image super-resolution
with global correspondence using multi-atlas patchmatch. In: MICCAI. (2013)
15. Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep cnn denoiser prior for image

16. Haris, M., Shakhnarovich, G., Ukita, N.: Deep back-projection networks for super-

restoration. In: CVPR. (2017)

resolution. In: CVPR. (2018)

image super-resolution. In: CVPR. (2018)

image super-resolution. In: ECCV. (2014)

image super-resolution. In: CVPR. (2016)

In: CVPR. (2016)

18. Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for

19. Kim, J., Kwon Lee, J., Mu Lee, K.: Deeply-recursive convolutional network for

20. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

21. Ledig, C., Theis, L., Husz´ar, F., Caballero, J., Cunningham, A., Acosta, A., Aitken,
A., Tejani, A., Totz, J., Wang, Z., Shi, W.: Photo-realistic single image super-
resolution using a generative adversarial network. In: CVPR. (2017)

22. Huang, J.B., Singh, A., Ahuja, N.: Single image super-resolution from transformed

23. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks.

arXiv preprint

self-exemplars. In: CVPR. (2015)

arXiv:1709.01507 (2017)

17. Zhang, Y., Tian, Y., Kong, Y., Zhong, B., Fu, Y.: Residual dense network for

16

Yulun Zhang et al.

24. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer

and super-resolution. In: ECCV. (2016)

25. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)

26. Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang, X.:

Residual attention network for image classiﬁcation. In: CVPR. (2017)

27. Li, K., Wu, Z., Peng, K.C., Ernst, J., Fu, Y.: Tell me where to look: Guided

attention inference network. In: CVPR. (2018)

28. Cao, C., Liu, X., Yang, Y., Yu, Y., Wang, J., Wang, Z., Huang, Y., Wang, L.,
Huang, C., Xu, W., Ramanan, D., Huang, T.S.: Look and think twice: Capturing
top-down visual attention with feedback convolutional neural networks. In: ICCV.
(2015)

29. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial transformer

networks. In: NIPS. (2015)

30. Bluche, T.: Joint line segmentation and transcription for end-to-end handwritten

paragraph recognition. In: NIPS. (2016)

31. Miech, A., Laptev, I., Sivic, J.: Learnable pooling with context gating for video

classiﬁcation. arXiv preprint arXiv:1706.06905 (2017)

32. Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style.

In: ICLR. (2017)

33. Shi, W., Caballero, J., Husz´ar, F., Totz, J., Aitken, A.P., Bishop, R., Rueckert,
D., Wang, Z.: Real-time single image and video super-resolution using an eﬃcient
sub-pixel convolutional neural network. In: CVPR. (2016)

34. Nair, V., Hinton, G.E.: Rectiﬁed linear units improve restricted boltzmann ma-

chines. In: ICML. (2010)

35. Timofte, R., Agustsson, E., Van Gool, L., Yang, M.H., Zhang, L., Lim, B., Son,
S., Kim, H., Nah, S., Lee, K.M., et al.: Ntire 2017 challenge on single image
super-resolution: Methods and results. In: CVPRW. (2017)

36. Bevilacqua, M., Roumy, A., Guillemot, C., Alberi-Morel, M.L.: Low-complexity
In:
single-image super-resolution based on nonnegative neighbor embedding.
BMVC. (2012)

37. Zeyde, R., Elad, M., Protter, M.: On single image scale-up using sparse-

representations. In: Proc. 7th Int. Conf. Curves Surf. (2010)

38. Martin, D., Fowlkes, C., Tal, D., Malik, J.: A database of human segmented natural
images and its application to evaluating segmentation algorithms and measuring
ecological statistics. In: ICCV. (2001)

39. Matsui, Y., Ito, K., Aramaki, Y., Fujimoto, A., Ogawa, T., Yamasaki, T., Aizawa,
K.: Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and
Applications (2017)

40. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:

from error visibility to structural similarity. TIP (2004)

41. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. In: ICLR. (2014)
42. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch. (2017)
43. Timofte, R., Rothe, R., Van Gool, L.: Seven ways to improve example-based single

image super resolution. In: CVPR. (2016)

44. Peleg, T., Elad, M.: A statistical prediction model based on sparse representations

for single image super-resolution. TIP (2014)

45. P´erez-Pellitero, E., Salvador, J., Ruiz-Hidalgo, J., Rosenhahn, B.: Psyco: Manifold

span reduction for super resolution. In: CVPR. (2016)

8
1
0
2
 
l
u
J
 
2
1
 
 
]

V
C
.
s
c
[
 
 
2
v
8
5
7
2
0
.
7
0
8
1
:
v
i
X
r
a

Image Super-Resolution Using Very Deep
Residual Channel Attention Networks

Yulun Zhang1, Kunpeng Li1, Kai Li1, Lichen Wang1,
Bineng Zhong1, and Yun Fu1,2

1Department of ECE, Northeastern University, Boston, USA
2College of Computer and Information Science, Northeastern University, Boston, USA
{yulun100,li.kai.gml,wanglichenxj}@gmail.com,
bnzhong@hqu.edu.cn, {kunpengli,yunfu}@ece.neu.edu

Abstract. Convolutional neural network (CNN) depth is of crucial im-
portance for image super-resolution (SR). However, we observe that
deeper networks for image SR are more diﬃcult to train. The low-
resolution inputs and features contain abundant low-frequency informa-
tion, which is treated equally across channels, hence hindering the rep-
resentational ability of CNNs. To solve these problems, we propose the
very deep residual channel attention networks (RCAN). Speciﬁcally, we
propose a residual in residual (RIR) structure to form very deep network,
which consists of several residual groups with long skip connections. Each
residual group contains some residual blocks with short skip connec-
tions. Meanwhile, RIR allows abundant low-frequency information to be
bypassed through multiple skip connections, making the main network
focus on learning high-frequency information. Furthermore, we propose a
channel attention mechanism to adaptively rescale channel-wise features
by considering interdependencies among channels. Extensive experiments
show that our RCAN achieves better accuracy and visual improvements
against state-of-the-art methods.

Keywords: Super-Resolution, Residual in Residual, Channel Attention

1 Introduction

We address the problem of reconstructing an accurate high-resolution (HR) im-
age given its low-resolution (LR) counterpart, usually referred as single image
super-resolution (SR) [12]. Image SR is used in various computer vision applica-
tions, ranging from security and surveillance imaging [13], medical imaging [14]
to object recognition [8]. However, image SR is an ill-posed problem, since there
exists multiple solutions for any LR input. To tackle such an inverse problem, nu-
merous learning based methods have been proposed to learn mappings between
LR and HR image pairs.

Recently, deep convolutional neural network (CNN) based methods [1–11,
15–17] have achieved signiﬁcant improvements over conventional SR methods.
Among them, Dong et al. [18] proposed SRCNN by ﬁrstly introducing a three-
layer CNN for image SR. Kim et al. increased the network depth to 20 in

2

Yulun Zhang et al.

HR

Bicubic

SRCNN [1]

FSRCNN [2]

SCN [3]

VDSR [4]

DRRN [5]

LapSRN [6] MSLapSRN [7] ENet-PAT [8] MemNet [9]

EDSR [10]

SRMDNF [11] RCAN (ours)

Fig. 1. Visual results with Bicubic (BI) degradation (4×) on “img 074” from Urban100

VDSR [4] and DRCN [19], achieving notable improvements over SRCNN. Net-
work depth was demonstrated to be of central importance for many visual recog-
nition tasks, especially when He at al. [20] proposed residual net (ResNet), which
reaches 1,000 layers with residual blocks. Such eﬀective residual learning strategy
was then introduced in many other CNN-based image SR methods [5, 8–10, 21].
Lim et al. [10] built a very wide network EDSR and a very deep one MDSR
(about 165 layers) by using simpliﬁed residual blocks. The great improvements
on performance of EDSR and MDSR indicate that the depth of representation
is of crucial importance for image SR. However, to the best of our knowledge,
simply stacking residual blocks to construct deeper networks can hardly obtain
better improvements. Whether deeper networks can further contribute to image
SR and how to construct very deep trainable networks remains to be explored.

On the other hand, most recent CNN-based methods [1–11] treat channel-
wise features equally, which lacks ﬂexibility in dealing with diﬀerent types of in-
formation (e.g., low- and high-frequency information). Image SR can be viewed
as a process, where we try to recover as more high-frequency information as pos-
sible. The LR images contain most low-frequency information, which can directly
forwarded to the ﬁnal HR outputs and don’t need too much computation. While,
the leading CNN-based methods (e.g., EDSR [10]) would extract features from
the original LR inputs and treat each channel-wise feature equally. Such process
would wastes unnecessary computations for abundant low-frequency features,
lacks discriminative learning ability across feature channels, and ﬁnally hinders
the representational power of deep networks.

To practically resolve these problems, we propose a residual channel attention
network (RCAN) to obtain very deep trainable network and adaptively learn
more useful channel-wise features simultaneously. To ease the training of very
deep networks (e.g., over 400 layers), we propose residual in residual (RIR)
structure, where the residual group (RG) serves as the basic module and long skip
connection (LSC) allows residual learning in a coarse level. In each RG module,
we stack several simpliﬁed residual block [10] with short skip connection (SSC).
The long and short skip connection as well as the short-cut in residual block allow
abundant low-frequency information to be bypassed through these identity-based
skip connections, which can ease the ﬂow of information. To make a further
step, we propose channel attention (CA) mechanism to adaptively rescale each
channel-wise feature by modeling the interdependencies across feature channels.
Such CA mechanism allows our proposed network to concentrate on more useful

Image Super-Resolution Using Very Deep RCAN

3

channels and enhance discriminative learning ability. As shown in Figure 1, our
RCAN achieves better visual SR result compared with state-of-the-art methods.
Overall, our contributions are three-fold: (1) We propose the very deep resid-
ual channel attention networks (RCAN) for highly accurate image SR. Our
RCAN can reach much deeper than previous CNN-based methods and obtains
much better SR performance. (2) We propose residual in residual (RIR) struc-
ture to construct very deep trainable networks. The long and short skip connec-
tions in RIR help to bypass abundant low-frequency information and make the
main network learn more eﬀective information. (3) We propose channel attention
(CA) mechanism to adaptively rescale features by considering interdependencies
among feature channels. Such CA mechanism further improves the representa-
tional ability of the network.

2 Related Work
Numerous image SR methods have been studied in the computer vision commu-
nity [1–11, 22]. Attention mechanism is popular in high-level vision tasks, but is
seldom investigated in low-level vision applications [23]. Due to space limitation,
here we focus on works related to CNN-based methods and attention mechanism.
Deep CNN for SR. The pioneer work was done by Dong et al. [18], who
proposed SRCNN for image SR and achieved superior performance against pre-
vious works. By introducing residual learning to ease the training diﬃculty, Kim
et al. proposed VDSR [4] and DRCN [19] with 20 layers and achieved signif-
icant improvement in accuracy. Tai et al. later introduced recursive blocks in
DRRN [5] and memory block in MemNet [9]. These methods would have to ﬁrst
interpolate the LR inputs to the desired size, which inevitably loses some details
and increases computation greatly.

Extracting features from the original LR inputs and upscaling spatial reso-
lution at the network tail then became the main choice for deep architecture. A
faster network structure FSRCNN [2] was proposed to accelerate the training and
testing of SRCNN. Ledig et al. [21] introduced ResNet [20] to construct a deeper
network, SRResNet, for image SR. They also proposed SRGAN with perceptual
losses [24] and generative adversarial network (GAN) [25] for photo-realistic SR.
Such GAN based model was then introduced in EnhanceNet [8], which com-
bines automated texture synthesis and perceptual loss. Although SRGAN and
Enhancenet can alleviate the blurring and oversmoothing artifacts to some de-
gree, their predicted results may not be faithfully reconstructed and produce
unpleasing artifacts. By removing unnecessary modules in conventional residual
networks, Lim et al. [10] proposed EDSR and MDSR, which achieve signiﬁ-
cant improvement. However, most of these methods have limited network depth,
which has demonstrated to be very important in visual recognition tasks [20] and
can reach to about 1,000 layers. Simply stacking residual blocks in MDSR [10],
very deep networks can hardly achieved improvements. Furthermore, most of
these methods treat the channel-wise features equally, hindering better discrim-
inative ability for diﬀerent types of features.

Attention mechanism. Generally, attention can be viewed as a guidance to
bias the allocation of available processing resources towards the most informative

4

Yulun Zhang et al.

Fig. 2. Network architecture of our residual channel attention network (RCAN)

components of an input [23]. Recently, tentative works have been proposed to
apply attention into deep neural networks [23, 26, 27], ranging from localization
and understanding in images [28, 29] to sequence-based networks [30, 31]. It’s
usually combined with a gating function (e.g., sigmoid) to rescale the feature
maps. Wang et al. [26] proposed residual attention network for image classi-
ﬁcation with a trunk-and-mask attention mechanism. Hu et al. [23] proposed
squeeze-and-excitation (SE) block to model channel-wise relationships to ob-
tain signiﬁcant performance improvement for image classiﬁcation. However, few
works have been proposed to investigate the eﬀect of attention for low-level vision
tasks (e.g., image SR).

In image SR, high-frequency channel-wise features are more informative for
HR reconstruction. If our network pays more attention to such channel-wise
features, it should be promising to obtain improvements. To investigate such
mechanism in very deep CNN, we propose very deep residual channel attention
networks (RCAN), which we will detail in next section.

3 Residual Channel Attention Network (RCAN)

3.1 Network Architecture

As shown in Figure 2, our RCAN mainly consists four parts: shallow feature
extraction, residual in residual (RIR) deep feature extraction, upscale module,
and reconstruction part. Let’s denote ILR and ISR as the input and output of
RCAN. As investigated in [10, 21], we use only one convolutional layer (Conv)
to extract the shallow feature F0 from the LR input

where HSF (·) denotes convolution operation. F0 is then used for deep feature
extraction with RIR module. So we can further have

F0 = HSF (ILR) ,

FDF = HRIR (F0) ,

(1)

(2)

where HRIR (·) denotes our proposed very deep residual in residual structure,
which contains G residual groups (RG). To the best of our knowledge, our pro-
posed RIR achieves the largest depth so far and provides very large receptive

Image Super-Resolution Using Very Deep RCAN

5

ﬁeld size. So we treat its output as deep feature, which is then upscaled via a
upscale module

FU P = HU P (FDF ) ,

(3)

where HU P (·) and FU P denote a upscale module and upscaled feature respec-
tively.

There’re several choices to serve as upscale modules, such as deconvolution
layer (also known as transposed convolution) [2], nearest-neighbor upsampling +
convolution [32], and ESPCN [33]. Such post-upscaling strategy has been demon-
strated to be more eﬃcient for both computation complexity and achieve higher
performance than pre-upscaling SR methods (e.g., DRRN [5] and MemNet [9]).
The upscaled feature is then reconstructed via one Conv layer

ISR = HREC (FU P ) = HRCAN (ILR) ,

(4)

where HREC (·) and HRCAN (·) denote the reconstruction layer and the function
of our RCAN respectively.

Then RCAN is optimized with loss function. Several loss functions have been
investigated, such as L2 [1–5,8,9,11,16], L1 [6,7,10,17], perceptual and adversar-
ial losses [8, 21]. To show the eﬀectiveness of our RCAN, we choose to optimize
same loss function as previous works (e.g., L1 loss function). Given a training
set (cid:8)I i
(cid:9)N
i=1, which contains N LR inputs and their HR counterparts. The
goal of training RCAN is to minimize the L1 loss function

LR, I i

HR

L (Θ) =

(cid:13)
(cid:13)HRCAN

(cid:0)I i

LR

(cid:1) − I i

HR

(cid:13)
(cid:13)1 ,

(5)

1
N

N
(cid:88)

i=1

where Θ denotes the parameter set of our network. The loss function is optimized
by using stochastic gradient descent. More details of training would be shown
in Section 4.1. As we choose the shallow feature extraction HSF (·), upscaling
module HU P (·), and reconstruction part HU P (·) as similar as previous works
(e.g., EDSR [10] and RDN [17]), we pay more attention to our proposed RIR,
CA, and the basic module RCAB.

3.2 Residual in Residual (RIR)

We now give more details about our proposed RIR structure (see Figure 2),
which contains G residual groups (RG) and long skip connection (LSC). Each
RG further contains B residual channel attention blocks (RCAB) with short skip
connection (SSC). Such residual in residual structure allows to train very deep
CNN (over 400 layers) for image SR with high performance.

It has been demonstrated that stacked residual blocks and LSC can be used
to construct deep CNN in [10]. In visual recognition, residual blocks [20] can be
stacked to achieve more than 1,000-layer trainable networks. However, in image
SR, very deep network built in such way would suﬀer from training diﬃculty

6

Yulun Zhang et al.

Fig. 3. Channel attention (CA). ⊗ denotes element-wise product

and can hardly achieve more performance gain. Inspired by previous works in
SRRestNet [21] and EDSR [10], we proposed residual group (RG) as the basic
module for deeper networks. A RG in the g-th group is formulated as

Fg = Hg (Fg−1) = Hg (Hg−1 (· · · H1 (F0) · · · )) ,

(6)

where Hg denotes the function of g-th RG. Fg−1 and Fg are the input and output
for g-th RG. We observe that simply stacking many RGs would fail to achieve
better performance. To solve the problem, the long skip connection (LSC) is
further introduced in RIR to stabilize the training of very deep network. LSC
also makes better performance possible with residual learning via

FDF = F0 + WLSCFG = F0 + WLSCHg (Hg−1 (· · · H1 (F0) · · · )) ,

(7)

where WLSC is the weight set to the Conv layer at the tail of RIR. The bias
term is omitted for simplicity. LSC can not only ease the ﬂow of information
across RGs, but only make it possible for RIR to learning residual information
in a coarse level.

As discussed in Section 1, there are lots of abundant information in the
LR inputs and features and the goal of SR network is to recover more useful
information. The abundant low-frequency information can be bypassed through
identity-based skip connection. To make a further step towards residual learning,
we stack B residual channel attention blocks in each RG. The b-th residual
channel attention block (RCAB) in g-th RG can be formulated as

Fg,b = Hg,b (Fg,b−1) = Hg,b (Hg,b−1 (· · · Hg,1 (Fg−1) · · · )) ,

(8)

where Fg,b−1 and Fg,b are the input and output of the b-th RCAB in g-th RG.
The corresponding function is denoted with Hg,b. To make the main network
pay more attention to more informative features, a short skip connection (SSC)
is introduced to obtain the block output via

Fg = Fg−1 + WgFg,B = Fg−1 + WgHg,B (Hg,B−1 (· · · Hg,1 (Fg−1) · · · )) ,

(9)

where Wg is the weight set to the Conv layer at the tail of g-th RG. The SSC
further allows the main parts of network to learn residual information. With LSC
and SSC, more abundant low-frequency information is easier bypassed in the
training process. To make a further step towards more discriminative learning,
we pay more attention to channel-wise feature rescaling with channel attention.

Image Super-Resolution Using Very Deep RCAN

7

3.3 Channel Attention (CA)

Previous CNN-based SR methods treat LR channel-wise features equally, which
is not ﬂexible for the real cases. In order to make the network focus on more
informative features, we exploit the interdependencies among feature channels,
resulting in a channel attention (CA) mechanism (see Figure 3).

How to generate diﬀerent attention for each channel-wise feature is a key
step. Here we mainly have two concerns: First, information in the LR space
has abundant low-frequency and valuable high-frequency components. The low-
frequency parts seem to be more complanate. The high-frequency components
would usually be regions, being full of edges, texture, and other details. On the
other hand, each ﬁlter in Conv layer operates with a local receptive ﬁeld. Conse-
quently, the output after convolution is unable to exploit contextual information
outside of the local region.

Based on these analyses, we take the channel-wise global spatial information
into a channel descriptor by using global average pooling. As shown in Figure 3,
let X = [x1, · · · , xc, · · · , xC] be an input, which has C feature maps with size
of H × W . The channel-wise statistic z ∈ RC can be obtained by shrinking X
through spatial dimensions H × W . Then the c-th element of z is determined by

zc = HGP (xc) =

xc (i, j) ,

(10)

1
H × W

H
(cid:88)

W
(cid:88)

i=1

j=1

where xc (i, j) is the value at position (i, j) of c-th feature xc. HGP (·) denotes the
global pooling function. Such channel statistic can be viewed as a collection of
the local descriptors, whose statistics contribute to express the whole image [23].
Except for global average pooling, more sophisticated aggregation techniques
could also be introduced here.

To fully capture channel-wise dependencies from the aggregated informa-
tion by global average pooling, we introduce a gating mechanism. As discussed
in [23], the gating mechanism should meet two criteria: First, it must be able
to learn nonlinear interactions between channels. Second, as multiple channel-
wise features can be emphasized opposed to one-hot activation, it must learn
a non-mututually-exclusive relationship. Here, we opt to exploit simple gating
mechanism with sigmoid function

s = f (WU δ (WDz)) ,

(11)

where f (·) and δ (·) denote the sigmoid gating and ReLU [34] function, respec-
tively. WD is the weight set of a Conv layer, which acts as channel-downscaling
with reduction ratio r. After being activated by ReLU, the low-dimension signal
is then increased with ratio r by a channel-upscaling layer, whose weight set is
WU . Then we obtain the ﬁnal channel statistics s, which is used to rescale the
input xc

(cid:98)xc = sc · xc,
where sc and xc are the scaling factor and feature map in the c-th channel. With
channel attention, the residual component in the RCAB is adaptively rescaled.

(12)

8

Yulun Zhang et al.

Fig. 4. Residual channel attention block (RCAB)

3.4 Residual Channel Attention Block (RCAB)
As discussed above, residual groups and long skip connection allow the main
parts of network to focus on more informative components of the LR features.
Channel attention extracts the channel statistic among channels to further en-
hance the discriminative ability of the network.

At the same time, inspired by the success of residual blocks (RB) in [10], we
integrate CA into RB and propose residual channel attention block (RCAB) (
see Figure 4). For the b-th RB in g-th RG, we have

Fg,b = Fg,b−1 + Rg,b (Xg,b) · Xg,b,

where Rg,b denotes the function of channel attention. Fg,b and Fg,b−1 are the
input and output of RCAB, which learns the residual Xg,b from the input. The
residual component is mainly obtained by two stacked Conv layers

Xg,b = W 2

g,bδ (cid:0)W 1

g,bFg,b−1

(cid:1) ,

(13)

(14)

where W 1

g,b and W 2

g,b are weight sets the two stacked Conv layers in RCAB.

We further show the relationships between our proposed RCAB and residual
block (RB) in [10]. We ﬁnd that the RBs used in MDSR and EDSR [10] can be
viewed as special cases of our RCAB. For RB in MDSR, there is no rescaling
operation. It is the same as RCAB, where we set Rg,b (·) as constant 1. For RB
with constant rescaling (e.g., 0.1) in EDSR, it is the same as RCAB with Rg,b (·)
set to be 0.1. Although the channel-wise feature rescaling is introduced to train
a very wide network, the interdependencies among channels are not considered
in EDSR. In these cases, the CA is not considered.

Based on residual channel attention block (RCAB) and RIR structure, we
construct a very deep RCAN for highly accurate image SR and achieve no-
table performance improvements over previous leading methods. More discus-
sions about the eﬀects of each proposed component are shown in Section 4.2.

Implementation Details

3.5
Now we specify the implementation details of our proposed RCAN. We set RG
number as G=10 in the RIR structure. In each RG, we set RCAB number as
20. We set 3×3 as the size of all Conv layers except for that in the channel-
downscaling and channel-upscaling, whose kernel size is 1×1. For Conv layers
with kernel size 3×3, zero-padding strategy is used to keep size ﬁxed. Conv
layers in shallow feature extraction and RIR structure have C=64 ﬁlters, except
for that in the channel-downscaling. Conv layer in channel-downscaling has C
r =4
ﬁlters, where the reduction ratio r is set as 16. For upscaling module HU P (·), we
follow [10, 17,33] and use ESPCNN [33] to upscale the coarse resolution features
to ﬁne ones. The ﬁnal Conv layer has 3 ﬁlters, as we output color images. While,
our network can also process gray images.

Image Super-Resolution Using Very Deep RCAN

9

Table 1. Investigations of RIR (including LSC and SSC) and CA. We observe the best
PSNR (dB) values on Set5 (2×) in 5×104 iterations

Residual in Residual (RIR)

Channel attention (CA)
PSNR on Set5 (2×)

LSC % ! % ! % ! % !
SSC % % ! ! % % ! !
% % % % ! ! ! !

37.45 37.77 37.81 37.87 37.52 37.85 37.86 37.90

4 Experiments
4.1 Settings
We clarify the experimental settings about datasets, degradation models, evalu-
ation metric, and training settings.

Datasets and degradation models. Following [10, 11, 17, 35], we use 800
training images from DIV2K dataset [35] as training set. For testing, we use ﬁve
standard benchmark datasets: Set5 [36], Set14 [37], B100 [38], Urban100 [22], and
Manga109 [39]. We conduct experiments with Bicubic (BI) and blur-downscale
(BD) degradation models [11, 15, 17].

Evaluation metrics. The SR results are evaluated with PSNR and SSIM [40]
on Y channel (i.e., luminance) of transformed YCbCr space. We also provide
performance (e.g., top-1 and top-5 recognition errors) comparisons on object
recognition by several leading SR methods.

Training settings. Data augmentation is performed on the 800 training
images, which are randomly rotated by 90◦, 180◦, 270◦ and ﬂipped horizontally.
In each training batch, 16 LR color patches with the size of 48 × 48 are extracted
as inputs. Our model is trained by ADAM optimizor [41] with β1 = 0.9, β2 =
0.999, and (cid:15) = 10−8. The initial leaning rate is set to 10−4 and then decreases
to half every 2 × 105 iterations of back-propagation. We use PyTorch [42] to
implement our models with a Titan Xp GPU.1

4.2 Eﬀects of RIR and CA
We study the eﬀects of residual in residual (RIR) and channel attention (CA).
Residual in residual (RIR). To demonstrate the eﬀect of our proposed
residual in residual structure, we remove long skip connection (LSC) or/and
short skip connection (SSC) from very deep networks. Speciﬁcally, we set the
number of residual block as 200, namely 10 residual groups, resulting in very
deep networks with over 400 Conv layers. In Table 1, when both LSC and SSC
are removed, the PSNR value on Set5 (×2) is relatively low, no matter channel
attention (CA) is used or not. For example, in the ﬁrst column, the PSNR is 37.45
dB. After adding RIR, the performance reaches 37.87 dB. When CA is added,
the performance can be improved from 37.52 dB to 37.90 dB by using RIR.
This indicates that simply stacking residual blocks is not applicable to achieve
very deep and powerful networks for image SR. The performance would increase
with LSC or SSC and can obtain better results by using both of them. These
comparisons show that LSC and SSC are essential for very deep networks. They
also demonstrate the eﬀectiveness of our proposed residual in residual (RIR)
structure for very deep networks.

1 The RCAN source code is available at https://github.com/yulunzhang/RCAN.

10

Yulun Zhang et al.

Channel attention (CA). We further show the eﬀect of channel atten-
tion (CA) based on the observations and discussions above. When we compare
the results of ﬁrst 4 columns and last 4 columns, we ﬁnd that networks with
CA would perform better than those without CA. Beneﬁtting from very large
network depth, the very deep trainable networks can achieve a very high per-
formance. It’s hard to obtain further improvements from such deep networks,
but we obtain improvements with CA. Even without RIR, CA can improve the
performance from 37.45 dB to 37.52 dB. These comparisons ﬁrmly demonstrate
the eﬀectiveness of CA and indicate adaptive attentions to channel-wise features
really improves the performance.

Table 2. Quantitative results with BI degradation model. Best and second best results
are highlighted and underlined

Set5

Scale

Set14

Method

Urban100

Manga109

B100
PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM
33.66 0.9299 30.24 0.8688 29.56 0.8431 26.88 0.8403 30.80 0.9339
×2
Bicubic
36.66 0.9542 32.45 0.9067 31.36 0.8879 29.50 0.8946 35.60 0.9663
×2
SRCNN [1]
37.05 0.9560 32.66 0.9090 31.53 0.8920 29.88 0.9020 36.67 0.9710
×2
FSRCNN [2]
37.53 0.9590 33.05 0.9130 31.90 0.8960 30.77 0.9140 37.22 0.9750
×2
VDSR [4]
37.52 0.9591 33.08 0.9130 31.08 0.8950 30.41 0.9101 37.27 0.9740
×2
LapSRN [6]
37.78 0.9597 33.28 0.9142 32.08 0.8978 31.31 0.9195 37.72 0.9740
×2
MemNet [9]
38.11 0.9602 33.92 0.9195 32.32 0.9013 32.93 0.9351 39.10 0.9773
EDSR [10]
×2
37.79 0.9601 33.32 0.9159 32.05 0.8985 31.33 0.9204 38.07 0.9761
SRMDNF [11] ×2
38.09 0.9600 33.85 0.9190 32.27 0.9000 32.55 0.9324 38.89 0.9775
×2
D-DBPN [16]
38.24 0.9614 34.01 0.9212 32.34 0.9017 32.89 0.9353 39.18 0.9780
×2
RDN [17]
RCAN (ours)
38.27 0.9614 34.12 0.9216 32.41 0.9027 33.34 0.9384 39.44 0.9786
×2
RCAN+ (ours) ×2 38.33 0.9617 34.23 0.9225 32.46 0.9031 33.54 0.9399 39.61 0.9788
×3
30.39 0.8682 27.55 0.7742 27.21 0.7385 24.46 0.7349 26.95 0.8556
Bicubic
×3
32.75 0.9090 29.30 0.8215 28.41 0.7863 26.24 0.7989 30.48 0.9117
SRCNN [1]
×3
33.18 0.9140 29.37 0.8240 28.53 0.7910 26.43 0.8080 31.10 0.9210
FSRCNN [2]
×3
33.67 0.9210 29.78 0.8320 28.83 0.7990 27.14 0.8290 32.01 0.9340
VDSR [4]
×3
33.82 0.9227 29.87 0.8320 28.82 0.7980 27.07 0.8280 32.21 0.9350
LapSRN [6]
34.09 0.9248 30.00 0.8350 28.96 0.8001 27.56 0.8376 32.51 0.9369
×3
MemNet [9]
34.65 0.9280 30.52 0.8462 29.25 0.8093 28.80 0.8653 34.17 0.9476
EDSR [10]
×3
34.12 0.9254 30.04 0.8382 28.97 0.8025 27.57 0.8398 33.00 0.9403
SRMDNF [11] ×3
34.71 0.9296 30.57 0.8468 29.26 0.8093 28.80 0.8653 34.13 0.9484
×3
RDN [17]
RCAN (ours)
34.74 0.9299 30.65 0.8482 29.32 0.8111 29.09 0.8702 34.44 0.9499
×3
RCAN+ (ours) ×3 34.85 0.9305 30.76 0.8494 29.39 0.8122 29.31 0.8736 34.76 0.9513
×4
28.42 0.8104 26.00 0.7027 25.96 0.6675 23.14 0.6577 24.89 0.7866
Bicubic
×4
30.48 0.8628 27.50 0.7513 26.90 0.7101 24.52 0.7221 27.58 0.8555
SRCNN [1]
×4
30.72 0.8660 27.61 0.7550 26.98 0.7150 24.62 0.7280 27.90 0.8610
FSRCNN [2]
×4
31.35 0.8830 28.02 0.7680 27.29 0.0726 25.18 0.7540 28.83 0.8870
VDSR [4]
×4
31.54 0.8850 28.19 0.7720 27.32 0.7270 25.21 0.7560 29.09 0.8900
LapSRN [6]
31.74 0.8893 28.26 0.7723 27.40 0.7281 25.50 0.7630 29.42 0.8942
×4
MemNet [9]
32.46 0.8968 28.80 0.7876 27.71 0.7420 26.64 0.8033 31.02 0.9148
EDSR [10]
×4
31.96 0.8925 28.35 0.7787 27.49 0.7337 25.68 0.7731 30.09 0.9024
SRMDNF [11] ×4
32.47 0.8980 28.82 0.7860 27.72 0.7400 26.38 0.7946 30.91 0.9137
×4
D-DBPN [16]
32.47 0.8990 28.81 0.7871 27.72 0.7419 26.61 0.8028 31.00 0.9151
×4
RDN [17]
RCAN (ours)
32.63 0.9002 28.87 0.7889 27.77 0.7436 26.82 0.8087 31.22 0.9173
×4
RCAN+ (ours) ×4 32.73 0.9013 28.98 0.7910 27.85 0.7455 27.10 0.8142 31.65 0.9208
×8
24.40 0.6580 23.10 0.5660 23.67 0.5480 20.74 0.5160 21.47 0.6500
Bicubic
×8
25.33 0.6900 23.76 0.5910 24.13 0.5660 21.29 0.5440 22.46 0.6950
SRCNN [1]
×8
20.13 0.5520 19.75 0.4820 24.21 0.5680 21.32 0.5380 22.39 0.6730
FSRCNN [2]
×8
25.59 0.7071 24.02 0.6028 24.30 0.5698 21.52 0.5571 22.68 0.6963
SCN [3]
×8
25.93 0.7240 24.26 0.6140 24.49 0.5830 21.70 0.5710 23.16 0.7250
VDSR [4]
26.15 0.7380 24.35 0.6200 24.54 0.5860 21.81 0.5810 23.39 0.7350
×8
LapSRN [6]
26.16 0.7414 24.38 0.6199 24.58 0.5842 21.89 0.5825 23.56 0.7387
MemNet [9]
×8
26.34 0.7558 24.57 0.6273 24.65 0.5895 22.06 0.5963 23.90 0.7564
MSLapSRN [7] ×8
26.96 0.7762 24.91 0.6420 24.81 0.5985 22.51 0.6221 24.69 0.7841
×8
EDSR [10]
27.21 0.7840 25.13 0.6480 24.88 0.6010 22.73 0.6312 25.14 0.7987
×8
D-DBPN [16]
RCAN (ours)
27.31 0.7878 25.23 0.6511 24.98 0.6058 23.00 0.6452 25.24 0.8029
×8
RCAN+ (ours) ×8 27.47 0.7913 25.40 0.6553 25.05 0.6077 23.22 0.6524 25.58 0.8092

Image Super-Resolution Using Very Deep RCAN

11

HR
PSNR/SSIM

Bicubic
21.08/0.6788

SRCNN [1]
22.13/0.7635

FSRCNN [2]
22.02/0.7628

VDSR [4]
22.37/0.7939

Urban100 (4×):
img 004

LapSRN [6]
22.41/0.7984

MemNet [9]
22.35/0.7992

EDSR [10]
24.07/0.8591

SRMDNF [11]
22.93/0.8207

RCAN
25.64/0.8830

HR
PSNR/SSIM

Bicubic
19.48/0.4371

SRCNN [1]
19.94/0.5124

FSRCNN [2]
19.88/0.5158

VDSR [4]
19.88/0.5229

Urban100 (4×):
img 073

LapSRN [6]
19.76/0.5250

MemNet [9]
19.71/0.5213

EDSR [10]
20.42/0.6028

SRMDNF [11]
19.88/0.5425

RCAN
21.26/0.6298

HR
PSNR/SSIM

Bicubic
24.66/0.7849

SRCNN [1]
26.22/0.8464

FSRCNN [2]
26.38/0.8496

VDSR [4]
26.89/0.8703

Manga109 (4×):
YumeiroCooking

LapSRN [6]
26.92/0.8739

MemNet [9]
27.09/0.8811

EDSR [10]
29.04/0.9230

SRMDNF [11]
27.53/0.8901

RCAN
29.85/0.9368

Fig. 5. Visual comparison for 4× SR with BI model on Urban100 and Manga109
datasets. The best results are highlighted

4.3 Results with Bicubic (BI) Degradation Model

We compare our method with 11 state-of-the-art methods: SRCNN [1], FSR-
CNN [2], SCN [3], VDSR [4], LapSRN [6], MemNet [9], EDSR [10], SRMDNF [11],
D-DBPN [16], and RDN [17]. Similar to [10, 17, 43], we also introduce self-
ensemble strategy to further improve our RCAN and denote the self-ensembled
one as RCAN+. More comparisons are provided in supplementary material.

Quantitative results by PSNR/SSIM. Table 2 shows quantitative com-
parisons for ×2, ×3, ×4, and ×8 SR. The results of D-DBPN [16] are cited from
their paper. When compared with all previous methods, our RCAN+ performs
the best on all the datasets with all scaling factors. Even without self-ensemble,
our RCAN also outperforms other compared methods.

On the other hand, when the scaling factor become larger (e.g., 8), the gains
of our RCAN over EDSR also becomes larger. For Urban100 and Manga109, the
PSNR gains of RCAN over EDSR are 0.49 dB and 0.55 dB. EDSR has much
larger number of parameters (43 M) than ours (16 M), but our RCAN obtains
much better performance. Instead of constantly rescaling the features in EDSR,
our RCAN adaptively rescales features with channel attention (CA). CA allows
our network to further focus on more informative features. This observation
indicates that very large network depth and CA improve the performance.

Visual results. In Figure 5, we show visual comparisons on scale ×4. For im-
age “img 004”, we observe that most of the compared methods cannot recover
the lattices and would suﬀer from blurring artifacts. In contrast, our RCAN
can alleviate the blurring artifacts better and recover more details. For image

12

Yulun Zhang et al.

HR
PSNR/SSIM

Bicubic
15.89/0.4595

SRCNN [1]
17.48/0.5927

SCN [3]
17.64/0.6410

VDSR [4]
17.59/0.6612

Urban100 (8×):
img 040

LapSRN [6]
18.27/0.7182

MemNet [9] MSLapSRN [7]
18.17/0.7190

18.52/0.7525

EDSR [10]
19.53/0.7857

RCAN
22.43/0.8607

HR
PSNR/SSIM

Bicubic
24.89/0.7572

SRCNN [1]
25.58/0.6993

SCN [3]
26.62/0.8035

VDSR [4]
26.33/0.8091

Manga109 (8×):
TaiyouNiSmash

LapSRN [6]
27.26/0.8278

MemNet [9] MSLapSRN [7]
27.47/0.8353

28.02/0.8532

EDSR [10]
29.44/0.8746

RCAN
30.67/0.8961

Fig. 6. Visual comparison for 8× SR with BI model on Urban100 and Manga109
datasets. The best results are highlighted

“img 073”, most of the compared methods produce blurring artifacts along the
horizontal lines. What’s worse, for the right parts of the cropped images, FSR-
CNN cannot recover lines. Other methods would generate some lines with wrong
directions. Only our RCAN produces more faithful results. For image “Yumeiro-
Cooking”, the cropped part is full of textures. As we can see, all the compared
methods suﬀer from heavy blurring artifacts, failing to recover more details.
While, our RCAN can recover them obviously, being more faithful to the ground
truth. Such obvious comparisons demonstrate that networks with more power-
ful representational ability can extract more sophisticated features from the LR
space.

To further illustrate the analyses above, we show visual comparisons for 8×
SR in Figure 6. For image “img 040”, due to very large scaling factor, the result
by Bicubic would lose the structures and produce diﬀerent structures. This wrong
pre-scaling result would also lead some state-of-the-art methods (e.g., SRCNN,
VDSR, and MemNet) to generate totally wrong structures. Even starting from
the original LR input, other methods cannot recover the right structure either.
While, our RCAN can recover them correctly. For smaller details, like the net in
image “TaiyouNiSmash”, the tiny lines can be lost in the LR image. When the
scaling factor is very large (e.g., 8), LR images contain very limited information
for SR. Losing most high-frequency information makes it very diﬃcult for SR
methods to reconstruct informative results. Most of compared methods cannot
achieve this goal and produce serious blurring artifacts. However, our RCAN can
obtain more useful information and produce ﬁner results.

As we have discussed above, in BI degradation model, the reconstruction of
high-frequency information is very important and diﬃcult, especially with large
scaling factor (e.g., 8). Our proposed RIR structure makes the main network
learn residual information. Channel attention (CA) is further used to enhance
the representational ability of the network by adaptively rescaling channel-wise
features.

Image Super-Resolution Using Very Deep RCAN

13

Table 3. Quantitative results with BD degradation model. Best and second best results
are highlighted and underlined

Set5

Scale

Set14

Method

Urban100

B100
PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM
28.78 0.8308 26.38 0.7271 26.33 0.6918 23.52 0.6862 25.46 0.8149
×3
Bicubic
32.21 0.9001 28.89 0.8105 28.13 0.7740 25.84 0.7856 29.64 0.9003
×3
SPMSR [44]
32.05 0.8944 28.80 0.8074 28.13 0.7736 25.70 0.7770 29.47 0.8924
×3
SRCNN [1]
26.23 0.8124 24.44 0.7106 24.86 0.6832 22.04 0.6745 23.04 0.7927
×3
FSRCNN [2]
33.25 0.9150 29.46 0.8244 28.57 0.7893 26.61 0.8136 31.06 0.9234
×3
VDSR [4]
33.38 0.9182 29.63 0.8281 28.65 0.7922 26.77 0.8154 31.15 0.9245
IRCNN [15]
×3
34.01 0.9242 30.11 0.8364 28.98 0.8009 27.50 0.8370 32.97 0.9391
SRMDNF [11] ×3
34.58 0.9280 30.53 0.8447 29.23 0.8079 28.46 0.8582 33.97 0.9465
×3
RDN [17]
RCAN (ours)
34.70 0.9288 30.63 0.8462 29.32 0.8093 28.81 0.8647 34.38 0.9483
×3
RCAN+ (ours) ×3 34.83 0.9296 30.76 0.8479 29.39 0.8106 29.04 0.8682 34.76 0.9502

Manga109

HR
PSNR/SSIM

Bicubic
20.20/0.6737

SPMSR [44]
21.72/0.7923

SRCNN [1]
21.74/0.7882

FSRCNN [2]
19.30/0.6960

Urban100 (3×):
img 062

VDSR [4]
22.36/0.8351

IRCNN [15]
22.32/0.8292

SRMDNF [11]
23.11/0.8662

RDN [17]
24.42/0.9052

RCAN
25.73/0.9238

HR
PSNR/SSIM

Bicubic
26.10/0.7032

SPMSR [44]
28.06/0.7950

SRCNN [1]
27.91/0.7874

FSRCNN [2]
24.34/0.6711

Urban100 (3×):
img 078

VDSR [4]
28.34/0.8166

IRCNN [15]
28.57/0.8184

SRMDNF [11]
29.08/0.8342

RDN [17]
29.94/0.8513

RCAN
30.65/0.8624

Fig. 7. Visual comparison for 3× SR with BD model on Urban100 dataset. The best
results are highlighted

4.4 Results with Blur-downscale (BD) Degradation Model

We further apply our method to super-resolve images with blur-down (BD)
degradation model, which is also commonly used recently [11, 15, 17].

Quantitative results by PSNR/SSIM. Here, we compare 3× SR re-
sults with 7 state-of-the-art methods: SPMSR [44], SRCNN [1], FSRCNN [2],
VDSR [4], IRCNN [15], SRMDNF [11], and RDN [17]. As shown in Table 3,
RDN has achieved very high performance on each dataset. While, our RCAN
can obtain notable gains over RDN. Using self-ensemble, RCAN+ achieves even
better results. Compared with fully using hierarchical features in RDN, a much
deeper network with channel attention in RCAN achieves better performance.
This comparison also indicates that there has promising potential to investigate
much deeper networks for image SR.

Visual Results. We also show visual comparisons in Figure 7. For challeng-
ing details in images “img 062” and “img 078”, most methods suﬀer from heavy
blurring artifacts. RDN alleviates it to some degree and can recover more details.
In contrast, our RCAN obtains much better results by recovering more informa-
tive components. These comparisons indicate that very deep channel attention
guided network would alleviate the blurring artifacts. It also demonstrates the
strong ability of RCAN for BD degradation model.

14

Yulun Zhang et al.

Table 4. ResNet object recognition performance. The best results are highlighted
Evaluation Bicubic DRCN [19] FSRCNN [2] PSyCo [45] ENet-E [8] RCAN Baseline
Top-1 error 0.506
Top-5 error 0.266

0.393 0.260
0.167 0.072

0.477
0.242

0.437
0.196

0.449
0.214

0.454
0.224

(a) Results on Set5 (4×)

(b) Results on Set5 (8×)

Fig. 8. Performance and number of parameters. Results are evaluated on Set5

4.5 Object Recognition Performance
Image SR also serves as pre-processing step for high-level visual tasks (e.g.,
object recognition). We evaluate the object recognition performance to further
demonstrate the eﬀectiveness of our RCAN.

Here we use the same settings as ENet [8]. We use ResNet-50 [20] as the
evaluation model and use the ﬁrst 1,000 images from ImageNet CLS-LOC val-
idation dataset for evaluation. The original cropped 224×224 images are used
for baseline and downscaled to 56×56 for SR methods. We use 4 stat-of-the-art
methods (e.g., DRCN [19], FSRCNN [2], PSyCo [45], and ENet-E [8]) to upscale
the LR images and then calculate their accuracies. As shown in Table 4, our
RCAN achieves the lowest top-1 and top-5 errors. These comparisons further
demonstrate the highly powerful representational ability of our RCAN.
4.6 Model Size Analyses
We show comparisons about model size and performance in Figure 8. Although
our RCAN is the deepest network, it has less parameter number than that of
EDSR and RDN. Our RCAN and RCAN+ achieve higher performance, having a
better tradeoﬀ between model size and performance. It also indicates that deeper
networks may be easier to achieve better performance than wider networks.
5 Conclusions
We propose very deep residual channel attention networks (RCAN) for highly
accurate image SR. Speciﬁcally, the residual in residual (RIR) structure allows
RCAN to reach very large depth with LSC and SSC. Meanwhile, RIR allows
abundant low-frequency information to be bypassed through multiple skip con-
nections, making the main network focus on learning high-frequency information.
Furthermore, to improve ability of the network, we propose channel attention
(CA) mechanism to adaptively rescale channel-wise features by considering in-
terdependencies among channels. Extensive experiments on SR with BI and BD
models demonstrate the eﬀectiveness of our proposed RCAN. RCAN also shows
promissing results for object recognition.
Acknowledgements: This research is supported in part by the NSF IIS award
1651902, ONR Young Investigator Award N00014-14-1-0484, and U.S. Army
Research Oﬃce Award W911NF-17-1-0367.

Image Super-Resolution Using Very Deep RCAN

15

References

1. Dong, C., Loy, C.C., He, K., Tang, X.: Image super-resolution using deep convo-

2. Dong, C., Loy, C.C., Tang, X.: Accelerating the super-resolution convolutional

lutional networks. TPAMI (2016)

neural network. In: ECCV. (2016)

3. Wang, Z., Liu, D., Yang, J., Han, W., Huang, T.: Deep networks for image super-

resolution with sparse prior. In: ICCV. (2015)

4. Kim, J., Kwon Lee, J., Mu Lee, K.: Accurate image super-resolution using very

deep convolutional networks. In: CVPR. (2016)

Image super-resolution via deep recursive residual

5. Tai, Y., Yang, J., Liu, X.:
network. In: CVPR. (2017)

6. Lai, W.S., Huang, J.B., Ahuja, N., Yang, M.H.: Deep laplacian pyramid networks

for fast and accurate super-resolution. In: CVPR. (2017)

7. Lai, W.S., Huang, J.B., Ahuja, N., Yang, M.H.: Fast and accurate image super-

resolution with deep laplacian pyramid networks. arXiv:1710.01992 (2017)

8. Sajjadi, M.S., Sch¨olkopf, B., Hirsch, M.: Enhancenet: Single image super-resolution

through automated texture synthesis. In: ICCV. (2017)

9. Tai, Y., Yang, J., Liu, X., Xu, C.: Memnet: A persistent memory network for

image restoration. In: ICCV. (2017)

10. Lim, B., Son, S., Kim, H., Nah, S., Lee, K.M.: Enhanced deep residual networks

for single image super-resolution. In: CVPRW. (2017)

11. Zhang, K., Zuo, W., Zhang, L.: Learning a single convolutional super-resolution

network for multiple degradations. In: CVPR. (2018)

12. Freeman, W.T., Pasztor, E.C., Carmichael, O.T.: Learning low-level vision. IJCV

(2000)

13. Zou, W.W., Yuen, P.C.: Very low resolution face recognition problem. TIP (2012)
14. Shi, W., Caballero, J., Ledig, C., Zhuang, X., Bai, W., Bhatia, K., de Marvao,
A.M.S.M., Dawes, T., ORegan, D., Rueckert, D.: Cardiac image super-resolution
with global correspondence using multi-atlas patchmatch. In: MICCAI. (2013)
15. Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep cnn denoiser prior for image

16. Haris, M., Shakhnarovich, G., Ukita, N.: Deep back-projection networks for super-

17. Zhang, Y., Tian, Y., Kong, Y., Zhong, B., Fu, Y.: Residual dense network for

18. Dong, C., Loy, C.C., He, K., Tang, X.: Learning a deep convolutional network for

19. Kim, J., Kwon Lee, J., Mu Lee, K.: Deeply-recursive convolutional network for

restoration. In: CVPR. (2017)

resolution. In: CVPR. (2018)

image super-resolution. In: CVPR. (2018)

image super-resolution. In: ECCV. (2014)

image super-resolution. In: CVPR. (2016)

In: CVPR. (2016)

20. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

21. Ledig, C., Theis, L., Husz´ar, F., Caballero, J., Cunningham, A., Acosta, A., Aitken,
A., Tejani, A., Totz, J., Wang, Z., Shi, W.: Photo-realistic single image super-
resolution using a generative adversarial network. In: CVPR. (2017)

22. Huang, J.B., Singh, A., Ahuja, N.: Single image super-resolution from transformed

23. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks.

arXiv preprint

self-exemplars. In: CVPR. (2015)

arXiv:1709.01507 (2017)

16

Yulun Zhang et al.

24. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer

and super-resolution. In: ECCV. (2016)

25. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)

26. Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang, X.:

Residual attention network for image classiﬁcation. In: CVPR. (2017)

27. Li, K., Wu, Z., Peng, K.C., Ernst, J., Fu, Y.: Tell me where to look: Guided

attention inference network. In: CVPR. (2018)

28. Cao, C., Liu, X., Yang, Y., Yu, Y., Wang, J., Wang, Z., Huang, Y., Wang, L.,
Huang, C., Xu, W., Ramanan, D., Huang, T.S.: Look and think twice: Capturing
top-down visual attention with feedback convolutional neural networks. In: ICCV.
(2015)

29. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial transformer

networks. In: NIPS. (2015)

30. Bluche, T.: Joint line segmentation and transcription for end-to-end handwritten

paragraph recognition. In: NIPS. (2016)

31. Miech, A., Laptev, I., Sivic, J.: Learnable pooling with context gating for video

classiﬁcation. arXiv preprint arXiv:1706.06905 (2017)

32. Dumoulin, V., Shlens, J., Kudlur, M.: A learned representation for artistic style.

In: ICLR. (2017)

33. Shi, W., Caballero, J., Husz´ar, F., Totz, J., Aitken, A.P., Bishop, R., Rueckert,
D., Wang, Z.: Real-time single image and video super-resolution using an eﬃcient
sub-pixel convolutional neural network. In: CVPR. (2016)

34. Nair, V., Hinton, G.E.: Rectiﬁed linear units improve restricted boltzmann ma-

chines. In: ICML. (2010)

35. Timofte, R., Agustsson, E., Van Gool, L., Yang, M.H., Zhang, L., Lim, B., Son,
S., Kim, H., Nah, S., Lee, K.M., et al.: Ntire 2017 challenge on single image
super-resolution: Methods and results. In: CVPRW. (2017)

36. Bevilacqua, M., Roumy, A., Guillemot, C., Alberi-Morel, M.L.: Low-complexity
In:
single-image super-resolution based on nonnegative neighbor embedding.
BMVC. (2012)

37. Zeyde, R., Elad, M., Protter, M.: On single image scale-up using sparse-

representations. In: Proc. 7th Int. Conf. Curves Surf. (2010)

38. Martin, D., Fowlkes, C., Tal, D., Malik, J.: A database of human segmented natural
images and its application to evaluating segmentation algorithms and measuring
ecological statistics. In: ICCV. (2001)

39. Matsui, Y., Ito, K., Aramaki, Y., Fujimoto, A., Ogawa, T., Yamasaki, T., Aizawa,
K.: Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and
Applications (2017)

40. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:

from error visibility to structural similarity. TIP (2004)

41. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. In: ICLR. (2014)
42. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch. (2017)
43. Timofte, R., Rothe, R., Van Gool, L.: Seven ways to improve example-based single

image super resolution. In: CVPR. (2016)

44. Peleg, T., Elad, M.: A statistical prediction model based on sparse representations

for single image super-resolution. TIP (2014)

45. P´erez-Pellitero, E., Salvador, J., Ruiz-Hidalgo, J., Rosenhahn, B.: Psyco: Manifold

span reduction for super resolution. In: CVPR. (2016)

