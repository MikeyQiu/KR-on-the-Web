8
1
0
2
 
r
a

M
 
6
2
 
 
]

V
C
.
s
c
[
 
 
2
v
1
6
7
6
0
.
1
0
8
1
:
v
i
X
r
a

PU-Net: Point Cloud Upsampling Network

Lequan Yu∗ 1,3 Xianzhi Li∗1 Chi-Wing Fu1,3 Daniel Cohen-Or2 Pheng-Ann Heng1,3

1The Chinese University of Hong Kong

2 Tel Aviv University

3Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology,
Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China

{lqyu,xzli,cwfu,pheng}@cse.cuhk.edu.hk

dcor@mail.tau.ac.il

Abstract

Learning and analyzing 3D point clouds with deep net-
works is challenging due to the sparseness and irregularity
of the data. In this paper, we present a data-driven point
cloud upsampling technique. The key idea is to learn multi-
level features per point and expand the point set via a multi-
branch convolution unit implicitly in feature space. The ex-
panded feature is then split to a multitude of features, which
are then reconstructed to an upsampled point set. Our net-
work is applied at a patch-level, with a joint loss function
that encourages the upsampled points to remain on the un-
derlying surface with a uniform distribution. We conduct
various experiments using synthesis and scan data to eval-
uate our method and demonstrate its superiority over some
baseline methods and an optimization-based method. Re-
sults show that our upsampled points have better uniformity
and are located closer to the underlying surfaces.

1. Introduction

Point cloud is a fundamental 3D representation that has
drawn increasing attention due to the popularity of various
depth scanning devices. Recently, pioneering works [29,
30, 18] began to explore the possibility of reasoning point
clouds by means of deep networks for understanding geom-
etry and recognizing 3D structures. In these works, the deep
networks directly extract features from the raw 3D point co-
ordinates without using traditional features, e.g., normal and
curvature. These works present impressive results for 3D
object classiﬁcation and semantic scene segmentation.

In this work we are interested in an upsampling problem:
given a set of points, generate a denser set of points to de-
scribe the underlying geometry by learning the geometry of
a training dataset. This upsampling problem is similar in
spirit to the image super-resolution problem [33, 20]; how-
ever, dealing with 3D points rather than a 2D grid of pixels

∗Equal contribution.

poses new challenges. First, unlike the image space, which
is represented by a regular grid, point clouds do not have
any spatial order and regular structure. Second, the gener-
ated points should describe the underlying geometry of a
latent target object, meaning that they should roughly lie on
the target object surface. Third, the generated points should
be informative and should not clutter together. Having said
that, the generated output point set should be more uniform
on the target object surface. Thus, simple interpolation be-
tween input points cannot produce satisfactory results.

To meet the above challenges, we present a data-driven
point cloud upsampling network. Our network is applied
at a patch-level, with a joint loss function that encourages
the upsampled points to remain on the underlying surface
with a uniform distribution. The key idea is to learn multi-
level features per point, and then expand the point set via
a multi-branch convolution unit implicitly in feature space.
The expanded feature is then split to a multitude of features,
which are then reconstructed to an upsampled point set.

Our network, namely PU-Net, learns geometry seman-
tics of point-based patches from 3D models, and applies
the learned knowledge to upsample a given point cloud. It
should be noted that unlike previous network-based meth-
ods designed for 3D point sets [29, 30, 18], the number of
input and output points in our network are not the same.

We formulate two metrics, distribution uniformity and
distance deviation from underlying surfaces, to quantita-
tively evaluate the upsampled point set, and test our method
using variety of synthetic and real-scanned data. We also
evaluate the performance of our method, and compare it
to baseline and state-of-the-art optimization-based methods.
Results show that our upsampled points have better unifor-
mity, and are located closer to the underlying surfaces.

Related work: optimization-based methods. An early
work by Alexa et al. [2] upsamples a point set by interpo-
lating points at vertices of a Voronoi diagram in the local
tangent space. Lipman et al. [24] present a locally optimal
projection (LOP) operator for points resampling and surface

1

Figure 1. The architecture of PU-Net (better view in color). The input has N points, while the output has rN points, where r is the
upsampling rate. Ci, ˜C and ˜Ci represent the feature channel number. We restore different level features for the original N points with
interpolation, and reduce all level features to a ﬁxed dimension C with a convolution. The red color in the point feature embedding
component shows the original and progressively subsampled points in hierarchical feature learning, while the green color indicates the
restored features. We jointly adopt the reconstruction loss and repulsion loss in the end-to-end training of PU-Net.

reconstruction based on an L1 median. The operator works
well even when the input point set contains noise and out-
liers. Successively, Huang et al. [14] propose an improved
weighted LOP to address the point set density problem.

Although these works have demonstrated good results,
they make a strong assumption that the underlying sur-
face is smooth, thus restricting the method’s scope. Then,
Huang et al. [15] introduce an edge-aware point set resam-
pling method by ﬁrst resampling away from edges and then
progressively approaching edges and corners. However, the
quality of their results heavily relies on the accuracy of the
normals at given points and careful parameter tuning. It is
worth mentioning that Wu et al. [35] propose a deep points
representation method to fuse consolidation and completion
in one coherent step. Since its main focus is on ﬁlling large
holes, global smoothness is, however, not enforced, so the
method is sensitive to large noise. Overall, the above meth-
ods are not data-driven, thus heavily relying on priors.

Related work: deep-learning-based methods. Points in
a point cloud do not have any speciﬁc order nor follow any
regular grid structure, so only a few recent works adopt a
deep learning model to directly process point clouds. Most
existing works convert a point cloud into some other 3D rep-
resentations such as the volumetric grids [27, 36, 31, 6] and
geometric graphs [3, 26] for processing. Qi et al. [29, 30]
ﬁrstly introduced a deep learning network for point cloud
classiﬁcation and segmentation;
in particular, the Point-
Net++ uses a hierarchical feature learning architecture to
capture both local and global geometry context. Subse-
quently, many other networks were proposed for high-level

analysis problems with point clouds [18, 13, 21, 34, 28].
However, they all focus on global or mid-level attributes
of point clouds. In another work, Guerrero et al. [10] de-
veloped a network to estimate the local shape properties in
point clouds, including normal and curvature. Other rel-
evant networks focus on 3D reconstruction from 2D im-
ages [8, 23, 9]. To the best of our knowledge, there are
no prior works focusing on point cloud upsampling.

2. Network Architecture

Given a 3D point cloud with point coordinates in nonuni-
form distributions, our network aims to output a denser
point cloud that follows the underlying surface of the tar-
get object while being uniform in distribution. Our network
architecture (see Fig. 1) has four components: patch extrac-
tion, point feature embedding, feature expansion, and co-
ordinate reconstruction. First, we extract patches of points
in varying scales and distributions from a given set of prior
3D models (Sec. 2.1). Then, the point feature embedding
component maps the raw 3D coordinates to a feature space
by hierarchical feature learning and multi-level feature ag-
gregation (Sec. 2.2). After that, we expand the number of
features using the feature expansion component (Sec. 2.3)
and reconstruct the 3D coordinates of the output point cloud
via a series of fully connected layers in the coordinate re-
construction component (Sec. 2.4).

2.1. Patch Extraction

We collect a set of 3D objects as prior information for
training. These objects cover a rich variety of shapes, from

smooth surface to shapes with sharp edges and corners.
Essentially, for our network to upsample a point cloud, it
should learn local geometry patterns from the objects. This
motivates us to take a patch-based approach to train the net-
work and to learn the geometry semantics.

In detail, we randomly select M points on the surface
of these objects. From each selected point, we grow a sur-
face patch on the object, such that any point on the patch
is within a certain geodesic distance (d) from the selected
point over the surface. Then, we use Poisson disk sampling
to randomly generate ˆN points on each patch as the refer-
enced ground truth point distribution on the patch. In our
upsampling task, both local and global context contribute to
a smooth and uniform output. Hence, we set d with varying
sizes, so that we can extract patches of points on the prior
objects with varying scale and density.

2.2. Point Feature Embedding

To learn both local and global geometry context from
the patches, we consider the following two feature learning
strategies, whose beneﬁts complement each other:

Hierarchical feature learning. Progressively capturing
features of growing scales in a hierarchy has been proved to
be an effective strategy for extracting local and global fea-
tures. Hence, we adopt the recently proposed hierarchical
feature learning mechanism in PointNet++ [30] as the very
frontal part in our network. To adopt hierarchical feature
learning for point cloud upsampling, we speciﬁcally use a
relatively small grouping radius in each level, since gener-
ating new points usually involves more of the local context
than the high-level recognition tasks in [30].

Multi-level feature aggregation. Lower layers in a net-
work generally correspond to local features in smaller
scales, and vice versa.
For better upsampling results,
we should optimally aggregate features in different levels.
Some previous works adopt skip-connections for cascaded
multi-level feature aggregation [25, 32, 30]. However, we
found by experiments that such top-down propagation is not
very efﬁcient for aggregating features in our upsampling
problem. Therefore, we propose to directly combine fea-
tures from different levels and let the network learn the im-
portance of each level [11, 38, 12].

Since the input point set on each patch (see point fea-
ture embedding in Fig. 1) is subsampled gradually in hierar-
chical feature extraction, we concatenate the point features
from each level by ﬁrst restoring the features of all original
points from the downsampled point features by the interpo-
lation method in PointNet++ [30]. Speciﬁcally, the features
of an interpolated point x in level (cid:96) is calculated by:

where wi(x)=1/d(x, xi), which is an inverse distance
weight, and xi, x2, x3 are the three nearest neighbors of
x in level (cid:96). We then use a 1×1 convolution to reduce the
interpolated feature in different level to the same dimension,
i.e., C. Finally, we concatenate the features from each level
as the embedded point feature f .

2.3. Feature Expansion

After the point feature embedding component, we ex-
pand the number of features in the feature space. This is
equivalent to expanding the number of points, since points
and features are interchangeable. Suppose the dimension
of f is N × ˜C, N is the number of input points and ˜C is the
feature dimension of the concatenated embedded feature.
The feature expansion operation would output a feature f (cid:48)
with dimension rN × ˜C2, where r is the upsampling rate
and ˜C2 is the new feature dimension. Essentially, this is
similar to feature upsampling in image-related tasks, which
can be done by deconvolution [25] (also known as trans-
posed convolution) or interpolation [7]. However, it is non-
trivial to apply these operations to point clouds due to the
non-regularity and unordered properties of points.

We therefore propose an efﬁcient feature expansion op-
eration based on the sub-pixel convolution layer [33]. This
operation can be represented as:

f (cid:48) = RS( [ C2

1 (C1

1 (f )) , ... , C2

r (C1

r (f )) ] ) ,

(2)

i (·) and C2

where C1
i (·) are two sets of separate 1×1 con-
volutions, and RS(·) is a reshape operation to convert an
N × r ˜C2 tensor to a tensor of size rN × ˜C2. We emphasize
that the feature in the embedding space has already encap-
sulated the relative spatial information from the neighbor-
hood points via the efﬁcient multi-level feature aggregation,
so we do not need to explicitly consider the spatial informa-
tion when performing this feature expansion operation.

It is worth mentioning that the r feature sets generated
from the ﬁrst convolution C1
i (·) in each set have a high cor-
relation, and this would cause the ﬁnal reconstructed 3D
points to be located too close to one another. Hence, we
further add another convolution (with separate weights) for
each feature set. Since we train the network to learn the r
different convolutions for the r feature sets, these new fea-
tures can include more diverse information, thus reducing
their correlations. This feature expansion operation can be
implemented by applying separated convolutions to the r
feature sets; see Fig. 1. It can also be implemented by more
computation efﬁcient grouped convolution [19, 37, 39].

2.4. Coordinate Reconstruction

f ((cid:96))(x) =

(cid:80)3

i=1 wi(x)f ((cid:96))(xi)
(cid:80)3
i=1 wi(x)

,

(1)

In this part, we reconstruct the 3D coordinates of output
points from the expanded feature with the size of rN × ˜C2.
Speciﬁcally, we regress the 3D coordinates via a series of

fully connected layers on the feature of each point, and the
ﬁnal output is the upsampled point coordinates rN × 3.

3. End-to-End Network Training

3.1. Training Data Generation

Point cloud upsampling is an ill-posed problem due to
the uncertainty or ambiguity of upsampled point clouds.
Given a sparse input point cloud, there are many feasible
output point distributions. Therefore, we do not have the
notion of “correct pairs” of input and ground truth. To al-
leviate this problem, we propose an on-the-ﬂy input gen-
eration scheme. Speciﬁcally, the referenced ground truth
point distribution of a training patch is ﬁxed, whereas the
input points are randomly sampled from the ground truth
point set with a downsampling rate of r at each training
epoch. Intuitively, this scheme is equivalent to simulating
many feasible output point distributions for a given sparse
input point distribution. Additionally, this scheme can fur-
ther enlarge the training dataset, allowing us to depend on a
relatively small dataset for training.

3.2. Joint Loss Function

We propose a novel joint loss function to train the net-
work in an end-to-end fashion. As we mentioned earlier,
the function should encourage the generated points to be lo-
cated on the underlying object surfaces in a more uniform
distribution. Therefore, we design a joint loss function that
combines the reconstruction loss and repulsion loss.

Reconstruction loss. To put points on the underlying ob-
ject surfaces, we propose to use the Earth Mover’s distance
(EMD) [8] as our reconstruction loss to evaluate the simi-
larity between the predicted point cloud Sp ⊆ R3 and the
referenced ground truth point cloud Sgt ⊆ R3:

Lrec = dEM D(Sp, Sgt) = min

(cid:107)xi − φ(xi)(cid:107)2,

φ:Sp→Sgt

(cid:88)

xi∈Sp

(3)

where φ : Sp → Sgt indicates the bijection mapping.

Actually, Chamfer Distance (CD) is another candidate
for evaluating the similarity between two point sets. How-
ever, compared with CD, EMD can better capture the shape
(see [8] for more details) to encourage the output points to
be located close to the underlying object surfaces. Hence,
we choose to use EMD in our reconstruction loss.

Repulsion loss. Although training with the reconstruction
loss can generate points on the underlying object surfaces,
the generated points tend to be located near the original
points. To distribute the generated points more uniformly,
we design the repulsion loss, which is represented as:

Lrep =

η((cid:107)xi(cid:48) − xi(cid:107))w((cid:107)xi(cid:48) − xi(cid:107)) ,

(4)

ˆN
(cid:88)

(cid:88)

i=0

i(cid:48)∈K(i)

where ˆN = rN is the number of output points, K(i) is the
index set of the k-nearest neighbors of point xi, and (cid:107) · (cid:107) is
the L2-norm. η(r) = −r is called the repulsion term, which
is a decreasing function to penalize xi if xi is located too
close to other points in K(i). To penalize xi only when it is
too close to its neighboring points, we add two restrictions:
(i) we only consider points xi(cid:48) in the k-nearest neighbor-
hood of xi; and (ii) we use the fast-decaying weight func-
tion w(r) in the repulsion loss; that is, we follow [24, 14] to
set w(r) = e−r2/h2
in our experiments.
Altogether, we train the network in an end-to-end man-

ner by minimizing the following joint loss function:

L(θ) = Lrec + αLrep + β(cid:107)θ(cid:107)2,

(5)

where θ indicates the parameters in our network, α balances
the reconstruction loss and repulsion loss, and β denotes the
multiplier of weight decay. For simplicity, we ignore the
index of each training sample.

4. Experiments

4.1. Datasets

Since there are no public benchmarks for point cloud up-
sampling, we collect a dataset of 60 different models from
the Visionair repository [1], ranging from smooth non-rigid
objects (e.g., Bunny) to steep rigid objects (e.g., Chair).
Among them, we randomly select 40 for training, and use
the rest for testing1. We crop 100 patches for each training
object, and we use M =4000 patches to train the network in
total. For testing objects, we use Monte-Carlo random sam-
pling approach to sample 5000 points on each object as in-
put. To further demonstrate the generalization ability of our
network, we directly test our well-trained network on the
SHREC15 [22] dataset, which contains 1200 shapes from
50 categories.
In detail, we randomly choose one model
from each category for testing, considering that each cate-
gory contains 24 similar objects in various poses. As for
ModelNet40 [36] and ShapeNet [4], we found it hard to ex-
tract patches from those objects due to the low mesh quality
(e.g., holes, self-intersection, etc.). Therefore, we use them
for testing; see the supplementary material for the results.

4.2. Implementation Details

The default point number ˆN of each patch is 4096, and
the upsampling rate r is 4. Therefore, each input patch has
1024 points. To avoid overﬁtting, we augment the data by
randomly rotating, shifting and scaling the data. We use 4
levels with grouping radii 0.05, 0.1, 0.2 and 0.3 in the point
feature embedding component, and the dimension C of the
restored feature is 64. For details on other network archi-
tecture parameters, please see our supplementary material.

1The complete object list can be found in the supplementary material.

Figure 2. Example point distributions with corresponding normal-
ized uniformity coefﬁcients (NUC) computed with p = 0.2%.

Parameters k and h in repulsion loss are set as 5 and 0.03,
respectively. The balancing weights α and β are set as 0.01
and β = 10−5, respectively. The implementation is based
on TensorFlow2. For the optimization, we train the network
for 120 epoch using the Adam [17] algorithm with a mini-
batch size of 28 and a learning rate of 0.001. Generally, the
training took about 4.5h on the NVIDIA TITAN Xp GPU.

4.3. Evaluation Metric

To quantitatively evaluate the quality of the output point
sets, we formulate two metrics to measure the deviation be-
tween the output points and the ground truth meshes, as well
as the distribution uniformity of the output points. For sur-
face deviation, we ﬁnd the closest point ˆxi on the mesh for
each predicted point xi, and calculate the distance between
them. Then we compute the mean and standard deviation
over all the points as one of our metrics.

As for the uniformity metric, we randomly put D equal-
size disks on the object surface (D = 9000 in our experi-
ments) and calculate the standard deviation of the number
of points inside the disks. We further normalize the den-
sity of each object and then compute the overall uniformity
of the point sets over all the objects in the testing dataset.
Therefore, we deﬁne the normalized uniformity coefﬁcient
(NUC) with disk area percentage p as:

avg =

1
K ∗ D

K
(cid:88)

D
(cid:88)

k=1

i=1

nk
i
N k ∗ p

,

(cid:118)
(cid:117)
(cid:117)
(cid:116)

N U C =

1
K ∗ D

K
(cid:88)

D
(cid:88)

k=1

i=1

(

nk
i
N k ∗ p

− avg)2,

(6)

where nk
is the number of points within the i-th disk of
i
the k-th object, N k is the total number of points on the k-
th object, K is the total number of test objects, and p is the
percentage of the disk area over the total object surface area.
Note that we use geodesic distance rather than Euclidean
distance to form the disks. Fig. 2 shows three different point
distributions with their corresponding NUC values. As we

2 https://github.com/yulequan/PU-Net

Figure 3. Comparison with the EAR method [15]. We color-code
all point clouds to show the deviation from the ground truth mesh.
For the EAR method, the radius of the Chair model is 0.1 and
0.182, while the radius of the Spider model is 0.106 and 0.155.

can see, the proposed NUC metric can effectively reveal the
point set uniformity: the lower the UNC value, the more
uniform the point set distribution is.

4.4. Comparisons with Other Methods

Comparison with an optimization-based method. We
compare our method with the Edge Aware Resampling
(EAR) method [15], which is a state-of-art method for point
cloud upsampling. The results are shown in Fig. 3, where
the Chair is from our collected testing dataset and the Spider
is from SHREC15. We color-code the point clouds to show
the deviation from the ground truth meshes. There are 1024
points in the input and we do a 4X upsampling. Since EAR
relies on the normal information, to be fair, we calculate the
normal according to the ground truth mesh. We show two
results of EAR with increasing radius, while setting other
parameters to their default values. As we can see, the radius
parameter has a great inﬂuence on EAR’s performance. For
relatively small radius, the output has low surface deviation
but the added points are not uniform, while more outliers
are introduced if the radius is large. In contrast, our method
can better balance the deviation and uniformity without the
need to carefully tune the parameters.

Comparison with deep learning-based methods. As far
as we know, we are not aware of any deep learning-based
method for point cloud upsampling, so we design some
baseline methods for comparison. Since PointNet [29] and
PointNet++ [30] are pioneers for 3D point cloud reason-
ing with deep learning techniques, we design the baselines
based on them. Speciﬁcally, we adopt the semantic segmen-
tation network architecture for point feature embedding and
use one set of convolutions for feature expansion. Note that
we consider two versions of PointNet++: basic PointNet++
and PointNet++ with multi-scale grouping (MSG) for han-
dling non-uniform sampling density; hence, we have three

Table 1. Quantitative comparison on our collected dataset.

NUC with different p

Deviation (10−2)

Time (s)

Method

Input
PointNet [29]
PointNet++ [30]
PointNet++(MSG) [30]
PU-Net (Ours)

0.2% 0.4% 0.6% 0.8% 1.0% 1.2% mean
0.164
0.316
0.270
0.409
0.150
0.215
0.143
0.208
0.115
0.174

0.224
0.334
0.178
0.169
0.138

0.185
0.295
0.160
0.152
0.122

0.150
0.252
0.143
0.137
0.112

0.142
0.239
0.139
0.134
0.110

-
2.27
1.01
0.78
0.63

Table 2. Quantitative comparison on SHREC15 dataset.

NUC with different p

Deviation (10−2)

Time (s)

Method

Input
PointNet [29]
PointNet++ [30]
PointNet++(MSG) [30]
PU-Net (Ours)

0.2% 0.4% 0.6% 0.8% 1.0% 1.2% mean
0.165
0.315
0.330
0.490
0.185
0.259
0.161
0.250
0.144
0.219

0.222
0.405
0.218
0.199
0.173

0.184
0.360
0.197
0.175
0.154

0.146
0.293
0.170
0.148
0.137

0.153
0.309
0.176
0.153
0.140

-
2.03
0.88
0.69
0.52

std
-
3.18
0.83
0.61
0.53

std
-
2.94
0.75
0.59
0.45

-
0.05
0.16
0.45
0.15

-
0.05
0.16
0.45
0.15

baselines in total, and we train them only with the recon-
struction loss. Please refer to the supplemental material for
details of the baseline network architectures. Although we
modify the PointNet, PointNet++, and PointNet++(MSG)
architectures for the upsampling problem, for convenience,
we still call the baselines by their original names.

Tables 1 and 2 list the quantitative comparison results
on our collected dataset and the SHREC15 dataset, respec-
tively. Note that measuring NUC with small p shows local
distribution uniformity in small regions, while measuring
NUC with large p shows more global uniformity. Among
the baselines, PointNet performs the worst, since it cannot
capture local structure information. Compared with Point-
Net++, PointNet++(MSG) can slightly improve the unifor-
mity due to the explicit multi-scale information grouping.
However, it involves more parameters, thus signiﬁcantly
prolonging the training and testing time. Overall, our PU-
Net achieves the best performance with the lowest deviation
from surface and the best distribution uniformity compared
to the baselines, especially on the local uniformity.

Fig. 4 shows results for visual comparison, where points
are colored by their distance deviations from surface. As
we can see, the point clouds predicted by our method better
match the underlying surface with lower deviations.

4.5. Architecture Design Analysis

Analyzing the feature expansion. We compare our pro-
posed feature expansion scheme with two interpolation-like
schemes. The ﬁrst one is similar to a naive point interpola-
tion (denoted as interp1). After extracting the point feature
of each point, we combine its own features and the features
from the nearest neighboring points to generate the upsam-
pled features. The second one introduces more randomness

Table 3. Architecture design analysis on our collect dataset.

NUC with different p

Deviation (10−2)

Methods

interp1+EMD 0.153
interp2+EMD 0.144
0.185
CD
0.140
EMD
0.138
Ours

0.4% 0.6% 0.8% 1.0% mean
0.67
0.126
0.71
0.122
0.87
0.154
0.68
0.119
0.63
0.115

0.121
0.118
0.147
0.116
0.112

0.136
0.129
0.167
0.126
0.122

std
0.54
0.57
0.69
0.58
0.53

(denoted as interp2).
Instead of using the features from
the nearest neighbors, we use a radius based ball query to
ﬁnd the neighborhood and combine the features from these
points to generate the upsampled features. We train these
two networks with the reconstruction loss (also named as
the EMD loss) and the results are listed in the top two rows
of Table 3. For fair comparison, we also train our network
only with the EMD loss. The results are shown in the fourth
row. Comparing with these two interpolation-like schemes,
our proposed scheme can generate more uniform outputs
with comparable surface distance deviation.

Comparing different loss functions. As mentioned above,
the EMD can better capture the object shape than CD. Com-
paring their performance in Table 3, we can see that the
EMD loss improves the output uniformity with low surface
distance deviation when comparing with the CD loss, mean-
ing that the EMD loss can better encourage the output points
to lie on the underlying surface. Furthermore, by comparing
the last two rows in Table 3, we can see that the repulsion
loss can further improve the uniformity of the output.

4.6. More Experiments

Surface reconstruction from upsampled point sets. An
important application of point cloud upsampling is to im-
prove the surface reconstruction quality. Hence, we com-

Figure 4. Visual comparison on samples from our collected dataset (top row) and SHREC (bottom row). The colors on points (see color
map) reveal the surface distance errors, where blue indicates low error and red indicates high error.

Figure 5. Surface reconstruction results from the upsampled point clouds.

pare the reconstruction results of different methods with the
direct Poisson surface reconstruction method [16] provided
in MeshLab [5]; see Fig. 5. We can observe that the re-
construction result from our method is the closest to the
ground truth, while other methods either miss certain struc-
tures (e.g., the leg of the Horse) or overﬁll the hole.

Results of iterative upsampling. To study the ability of
our network to handle varying number of input points, we
design an iterative upsampling experiment, which takes the
output of the previous iteration as the input of the next iter-
ation. Fig. 6 shows the results. The initial input point cloud
has 1024 points and we increase fourfold in each iteration.
From the results, we can see that our network can produce
reasonable results for different number of input points. Fur-
thermore, this iterative upsampling experiment also shows

Figure 6. Results of iterative upsampling. We color-code points by
the depth information. Blue points are closer to us.

Figure 7. Surface reconstruction results from noisy input points.

5. Conclusion

the anti-noise ability of our network to resist the accumu-
lated errors introduced in the iterative upsampling.

Results from noisy input point sets. Fig. 7 shows the sur-
face reconstruction results from noisy point clouds (Gaus-
sian noise of 0.5% and 1% of object bounding box diago-
nal), which demonstrate that our network facilitates the pro-
duction of better surfaces even with noisy inputs.

Results on real-scanned point clouds. Lastly, we eval-
uated the ability of our network to upsample real-scanned
point clouds, which were downloaded from Visionair [1]. In
Fig. 8, the left-most column presents the real-scanned point
clouds. Even though each real-scanned point cloud contains
millions of points, the phenomenon of inhomogeneity still
exists. For better visualization, we cut small patches from
the original point clouds and show the patches in the middle
column. We can observe that the real-scanned points tend to
have line structures, while our network still has the ability
to uniformly add points in the sparse regions.

In this paper, we present a deep network for point cloud
upsampling, with the goal of generating a denser and uni-
form set of points from a sparser set of points. Our network
is trained at a patch-level using a multi-level feature aggre-
gation manner, thus capturing both local and global infor-
mation. The design of our network bypasses the need for
a prescribed order among the points, by operating on in-
dividual features that contain non-local geometry to allow
a context-aware upsampling. Our experiments demonstrate
the effectiveness of our method. As the ﬁrst attempt using
deep networks, our method still has a number of limitations.
Firstly, it is not designed for completion, so our network can
not ﬁll large holes and missing parts. Besides, our network
may not be able to add meaningful points for tiny structures
that are severely undersampled.

In the future, we would like to investigate and develop
more means to handle irregular and sparse data, both for re-
gression purposes and for synthesis. One immediate step is
to develop a downsampling method. Although, downsam-
pling seems like a simpler problem, there is room to devise
proper losses and architecture that maximize the preserva-
tion of information in the decimated point set. We believe
that in general, the development of deep learning methods
for irregular structures is a viable research direction.

Acknowledgments. We thank anonymous reviewers for the
comments and suggestions. The work is supported in part
by the National Basic Program of China, the 973 Program
(Project No. 2015CB351706), the Research Grants Council
of the Hong Kong Special Administrative Region (Project
no. CUHK 14225616), the Shenzhen Science and Tech-
nology Program (No. JCYJ20170413162617606), and the
CUHK strategic recruitment fund.

Figure 8. Results on real-scanned point clouds (Screw nut & Tur-
tle). We color-code input patches and upsampling results to show
the depth information. Blue points are closer to viewpoint.

References

[1] Visionair. [Online; accessed on 14-November-2017]. 4, 8
[2] M. Alexa, J. Behr, D. Cohen-Or, S. Fleishman, D. Levin,
and C. T. Silva. Computing and rendering point set surfaces.
IEEE Trans. Vis. & Comp. Graphics, 9(1):3–15, 2003. 1
[3] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral
networks and locally connected networks on graphs. In Int.
Conf. on Learning Representations (ICLR), 2014. 2

[4] A. X. Chang, T. Funkhouser, L. J. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. ShapeNet: An information-rich 3D model repository.
arXiv preprint arXiv:1512.03012, 2015. 4

[5] P. Cignoni, M. Callieri, M. Corsini, M. Dellepiane, F. Ganov-
elli, and G. Ranzuglia. MeshLab: an open-source mesh pro-
cessing tool. In Eurographics Italian Chapter Conference,
2008. 7

[6] A. Dai, C. R. Qi, and M. Niessner. Shape completion using
3D-Encoder-Predictor CNNs and shape synthesis. In IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
2017. 2

[7] C. Dong, C. C. Loy, K. He, and X. Tang.

Image super-
resolution using deep convolutional networks. IEEE Trans.
Pattern Anal. & Mach. Intell., 38(2):295–307, 2016. 3
[8] H. Fan, H. Su, and L. J. Guibas. A point set generation
network for 3D object reconstruction from a single image.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2017. 2, 4

[9] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and
M. Aubry. AtlasNet: A papier-mˆach´e approach to learning
3D surface generation. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2018. to appear. 2
[10] P. Guerrero, Y. Kleiman, M. Ovsjanikov, and N. J. Mitra.
PCPNet: Learning local shape properties from raw point
clouds. Computer Graphics Forum (Eurographics), 2018.
to appear. 2

[11] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In IEEE Conf. on Computer Vision and Pattern Recog-
nition (CVPR), 2015. 3

[12] Q. Hou, M. Cheng, X. Hu, A. Borji, Z. Tu, and P. Torr.
Deeply supervised salient object detection with short con-
In IEEE Conf. on Computer Vision and Pattern
nections.
Recognition (CVPR), 2017. 3

[13] B.-S. Hua, M.-K. Tran, and S.-K. Yeung. Pointwise convo-
lutional neural networks. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2018. to appear. 2
[14] H. Huang, D. Li, H. Zhang, U. Ascher, and D. Cohen-Or.
Consolidation of unorganized point clouds for surface re-
construction. ACM Trans. on Graphics (SIGGRAPH Asia),
28(5):176:1–8, 2009. 2, 4

[15] H. Huang, S. Wu, M. Gong, D. Cohen-Or, U. Ascher, and
H. R. Zhang. Edge-aware point set resampling. ACM Trans.
on Graphics, 32(9):1–12, 2013. 2, 5

[16] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface
In Eurographics Symposium on Geometry

reconstruction.
Processing (SGP), 2006. 7

[17] D. Kingma and J. Ba. Adam: A method for stochastic opti-
mization. In Int. Conf. on Learning Representations (ICLR),
2015. 5

[18] R. Klokov and V. Lempitsky. Escape from cells: deep Kd-
Networks for the recognition of 3D point cloud models. In
IEEE Int. Conf. on Computer Vision (ICCV), 2017. 1, 2

[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

ImageNet
classiﬁcation with deep convolutional neural networks.
In
Int. Conf. on Neural Information Processing Systems (NIPS),
2012. 3

[20] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a genera-
tive adversarial network. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2017. 1

[21] Y. Li, R. Bu, M. Sun, and B. Chen. PointCNN. arXiv preprint

arXiv:1801.07791, 2018. 2

[22] Z. Lian, J. Zhang, S. Choi, H. ElNaghy, J. El-Sana, T. Fu-
ruya, A. Giachetti, R. A. Guler, L. Lai, C. Li, H. Li,
F. A. Limberger, R. Martin, R. U. Nakanishi, A. P. Neto,
L. G. Nonato, R. Ohbuchi, K. Pevzner, D. Pickup, P. Rosin,
A. Sharf, L. Sun, X. Sun, S. Tari, G. Unal, and R. C. Wilson.
In Proc. of the 2015 Euro-
Non-rigid 3D shape retrieval.
graphics Workshop on 3D Object Retrieval, 2015. 4

[23] C.-H. Lin, C. Kong, and S. Lucey. Learning efﬁcient point
cloud generation for dense 3D object reconstruction. In AAAI
Conf. on Artiﬁcial Intell. (AAAI), 2017. 2

[24] Y. Lipman, D. Cohen-Or, D. Levin, and H. Tal-Ezer.
Parameterization-free projection for geometry reconstruc-
tion. ACM Trans. on Graphics (SIGGRAPH), 26(3):22:1–5,
2007. 1, 4

[25] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In IEEE Conf. on Com-
puter Vision and Pattern Recognition (CVPR), 2015. 3
[26] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst.
Geodesic convolutional neural networks on Riemannian
manifolds. In IEEE Int. Conf. on Computer Vision (ICCV)
workshops, 2015. 2

[27] D. Maturana and S. Scherer. Voxnet: A 3D convolutional
neural network for real-time object recognition. In Int. Conf.
on Intell. Robots and Systems (IROS), 2015. 2

[28] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas. Frus-
tum PointNets for 3D object detection from RGB-D data.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2018. to appear. 2

[29] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3D classiﬁcation and segmentation.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2017. 1, 2, 5, 6

[30] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. PointNet++:
Deep hierarchical feature learning on point sets in a metric
space. In Int. Conf. on Neural Information Processing Sys-
tems (NIPS), 2017. 1, 2, 3, 5, 6

[31] G. Riegler, A. O. Ulusoys, and A. Geiger. OctNet: Learning
deep 3D representations at high resolutions. In IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), 2017.
2

[32] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. In Int.
Conf. on MICCAI, 2015. 3

[33] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2016. 1, 3

[34] W. Wang, R. Yu, Q. Huang, and U. Neumann. SGPN: Sim-
ilarity group proposal network for 3D point cloud instance
segmentation. In IEEE Conf. on Computer Vision and Pat-
tern Recognition (CVPR), 2018. to appear. 2

[35] S. Wu, H. Huang, M. Gong, M. Zwicker, and D. Cohen-
Or. Deep points consolidation. ACM Trans. on Graphics
(SIGGRAPH Asia), 34(6):176:1–13, 2015. 2

[36] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3D ShapeNets: A deep representation for volumet-
ric shapes. In IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2015. 2, 4

[37] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. arXiv
preprint arXiv:1611.05431, 2016. 3

[38] S. Xie and Z. Tu. Holistically-nested edge detection. In IEEE

Int. Conf. on Computer Vision (ICCV), 2015. 3

[39] X. Zhang, X. Zhou, M. Lin, and J. Sun. ShufﬂeNet: An
extremely efﬁcient convolutional neural network for mobile
devices. arXiv preprint arXiv:1707.01083, 2017. 3

A. Overview

In this supplementary material, we ﬁrst provide more details about our collected dataset in Section B. Then, we show the

details of our network architecture as well as the baseline networks employed in the experiments in Section C.

B. Details of our Collected Dataset

We collect 60 different 3D models to form our training and testing datasets. The speciﬁc name of each model is shown in
Table 4. We also present the shapes of some training and testing 3D models in our dataset in Fig. 9 and Fig. 10, respectively.
As we can see, our collected datasets have a large variation in geometry shapes, containing 3D models with smooth surface
regions (ﬁrst row) and 3D models with sharp corners and edges (second row). There is also a large variation between training
and testing 3D models, indicating a good generalization ability of our proposed method.

Table 4. The complete name list of the 3D models in our training and testing datasets.

Model Names

Training

Testing

Armadillo, Boy1, Boy2, Bumpy torus, Bunny, Cad, Cylinder, Child1,
Child2, Chinese lion, Cone, Cup, Dino, Egea, Ellipsoid, Eros, Fish, Fo-
cal octa, Gargoyle, Girl1, Girl2, Hand, Joint, Julius, Nicolo, Octa ﬂower,
Pierrot, Pulley, Pyramid1, Pyramid2, Retinal, Rolling stage, Screwdriver,
Sharp sphere, Special cube, Star1, Turbine, Twirl, Vaselion
Camel, Casting, Chair, Cover rear, Cow, Duck, Eight, Elephant, Elk, Fan-
disk, Genus, Horse, Icosahedron, Kitten, Moai, Octahedron, Pig, Quadric,
Sculpt, Star2

Armadillo

Bunny

Dino

Julius

Pierrot

Vaselion

Block

Cad

Focal octa

Joint

Pulley

Twirl

Figure 9. Examples 3D models in our training dataset. The ﬁrst row shows 3D models with smooth surface regions, while the
second row shows 3D models with sharp corners and edges.

Camel

Elephant

Elk

Horse

Kitten

Moai

Casting

Chair

Fandisk

Icosahedron

Quadric

Sculpt

Figure 10. Examples 3D models in our testing dataset. The ﬁrst row shows 3D models with smooth surface regions, while the
second row shows 3D models with sharp corners and edges.

C. Details of Network Architectures

The details of our network architecture are listed as follows.

• In the hierarchical feature learning component, we use four levels to extract local features. Following the notations
in PointNet++, we use (K, r, [l1, ..., ld]) to represent a level with K local regions of ball radius r, and [l1, ..., ld]
the d fully connected layers with width li (i = 1, ..., d). Therefore, the parameters we use are (N, 0.05, [32, 32, 64]),
(N/2, 0.1, [64, 64, 128]), (N/4, 0.2, [128, 128, 256]) and (N/8, 0.3, [256, 256, 512]).

• In the multi-level feature aggregation component, we use interpolation to restore the feature of each level and use a

convolution to reduce the restored feature to 64 dimensions. Therefore, ˜C = 259 in our experiments.

• In the feature expansion component, the output feature channel numbers ˜C1 and ˜C2 are 256 and 128, respectively.

• In the coordinate reconstruction component, we use two fully connected layers with 64 and 3 output channels, respec-

tively.

The details of the baseline architectures are illustrated in Fig. 11, Fig. 12 and Fig. 13.
All the convolution layers and fully connected layers in the above networks are followed by the ReLU operator, except for

the last coordinate regression layer.

Figure 11. The network architecture of PointNet for point cloud upsampling.

Figure 12. The network architecture of PointNet++ for point cloud upsampling.

Figure 13. The network architecture of PointNet++(MSG) for point cloud upsampling.

8
1
0
2
 
r
a

M
 
6
2
 
 
]

V
C
.
s
c
[
 
 
2
v
1
6
7
6
0
.
1
0
8
1
:
v
i
X
r
a

PU-Net: Point Cloud Upsampling Network

Lequan Yu∗ 1,3 Xianzhi Li∗1 Chi-Wing Fu1,3 Daniel Cohen-Or2 Pheng-Ann Heng1,3

1The Chinese University of Hong Kong

2 Tel Aviv University

3Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology,
Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China

{lqyu,xzli,cwfu,pheng}@cse.cuhk.edu.hk

dcor@mail.tau.ac.il

Abstract

Learning and analyzing 3D point clouds with deep net-
works is challenging due to the sparseness and irregularity
of the data. In this paper, we present a data-driven point
cloud upsampling technique. The key idea is to learn multi-
level features per point and expand the point set via a multi-
branch convolution unit implicitly in feature space. The ex-
panded feature is then split to a multitude of features, which
are then reconstructed to an upsampled point set. Our net-
work is applied at a patch-level, with a joint loss function
that encourages the upsampled points to remain on the un-
derlying surface with a uniform distribution. We conduct
various experiments using synthesis and scan data to eval-
uate our method and demonstrate its superiority over some
baseline methods and an optimization-based method. Re-
sults show that our upsampled points have better uniformity
and are located closer to the underlying surfaces.

1. Introduction

Point cloud is a fundamental 3D representation that has
drawn increasing attention due to the popularity of various
depth scanning devices. Recently, pioneering works [29,
30, 18] began to explore the possibility of reasoning point
clouds by means of deep networks for understanding geom-
etry and recognizing 3D structures. In these works, the deep
networks directly extract features from the raw 3D point co-
ordinates without using traditional features, e.g., normal and
curvature. These works present impressive results for 3D
object classiﬁcation and semantic scene segmentation.

In this work we are interested in an upsampling problem:
given a set of points, generate a denser set of points to de-
scribe the underlying geometry by learning the geometry of
a training dataset. This upsampling problem is similar in
spirit to the image super-resolution problem [33, 20]; how-
ever, dealing with 3D points rather than a 2D grid of pixels

∗Equal contribution.

poses new challenges. First, unlike the image space, which
is represented by a regular grid, point clouds do not have
any spatial order and regular structure. Second, the gener-
ated points should describe the underlying geometry of a
latent target object, meaning that they should roughly lie on
the target object surface. Third, the generated points should
be informative and should not clutter together. Having said
that, the generated output point set should be more uniform
on the target object surface. Thus, simple interpolation be-
tween input points cannot produce satisfactory results.

To meet the above challenges, we present a data-driven
point cloud upsampling network. Our network is applied
at a patch-level, with a joint loss function that encourages
the upsampled points to remain on the underlying surface
with a uniform distribution. The key idea is to learn multi-
level features per point, and then expand the point set via
a multi-branch convolution unit implicitly in feature space.
The expanded feature is then split to a multitude of features,
which are then reconstructed to an upsampled point set.

Our network, namely PU-Net, learns geometry seman-
tics of point-based patches from 3D models, and applies
the learned knowledge to upsample a given point cloud. It
should be noted that unlike previous network-based meth-
ods designed for 3D point sets [29, 30, 18], the number of
input and output points in our network are not the same.

We formulate two metrics, distribution uniformity and
distance deviation from underlying surfaces, to quantita-
tively evaluate the upsampled point set, and test our method
using variety of synthetic and real-scanned data. We also
evaluate the performance of our method, and compare it
to baseline and state-of-the-art optimization-based methods.
Results show that our upsampled points have better unifor-
mity, and are located closer to the underlying surfaces.

Related work: optimization-based methods. An early
work by Alexa et al. [2] upsamples a point set by interpo-
lating points at vertices of a Voronoi diagram in the local
tangent space. Lipman et al. [24] present a locally optimal
projection (LOP) operator for points resampling and surface

1

Figure 1. The architecture of PU-Net (better view in color). The input has N points, while the output has rN points, where r is the
upsampling rate. Ci, ˜C and ˜Ci represent the feature channel number. We restore different level features for the original N points with
interpolation, and reduce all level features to a ﬁxed dimension C with a convolution. The red color in the point feature embedding
component shows the original and progressively subsampled points in hierarchical feature learning, while the green color indicates the
restored features. We jointly adopt the reconstruction loss and repulsion loss in the end-to-end training of PU-Net.

reconstruction based on an L1 median. The operator works
well even when the input point set contains noise and out-
liers. Successively, Huang et al. [14] propose an improved
weighted LOP to address the point set density problem.

Although these works have demonstrated good results,
they make a strong assumption that the underlying sur-
face is smooth, thus restricting the method’s scope. Then,
Huang et al. [15] introduce an edge-aware point set resam-
pling method by ﬁrst resampling away from edges and then
progressively approaching edges and corners. However, the
quality of their results heavily relies on the accuracy of the
normals at given points and careful parameter tuning. It is
worth mentioning that Wu et al. [35] propose a deep points
representation method to fuse consolidation and completion
in one coherent step. Since its main focus is on ﬁlling large
holes, global smoothness is, however, not enforced, so the
method is sensitive to large noise. Overall, the above meth-
ods are not data-driven, thus heavily relying on priors.

Related work: deep-learning-based methods. Points in
a point cloud do not have any speciﬁc order nor follow any
regular grid structure, so only a few recent works adopt a
deep learning model to directly process point clouds. Most
existing works convert a point cloud into some other 3D rep-
resentations such as the volumetric grids [27, 36, 31, 6] and
geometric graphs [3, 26] for processing. Qi et al. [29, 30]
ﬁrstly introduced a deep learning network for point cloud
classiﬁcation and segmentation;
in particular, the Point-
Net++ uses a hierarchical feature learning architecture to
capture both local and global geometry context. Subse-
quently, many other networks were proposed for high-level

analysis problems with point clouds [18, 13, 21, 34, 28].
However, they all focus on global or mid-level attributes
of point clouds. In another work, Guerrero et al. [10] de-
veloped a network to estimate the local shape properties in
point clouds, including normal and curvature. Other rel-
evant networks focus on 3D reconstruction from 2D im-
ages [8, 23, 9]. To the best of our knowledge, there are
no prior works focusing on point cloud upsampling.

2. Network Architecture

Given a 3D point cloud with point coordinates in nonuni-
form distributions, our network aims to output a denser
point cloud that follows the underlying surface of the tar-
get object while being uniform in distribution. Our network
architecture (see Fig. 1) has four components: patch extrac-
tion, point feature embedding, feature expansion, and co-
ordinate reconstruction. First, we extract patches of points
in varying scales and distributions from a given set of prior
3D models (Sec. 2.1). Then, the point feature embedding
component maps the raw 3D coordinates to a feature space
by hierarchical feature learning and multi-level feature ag-
gregation (Sec. 2.2). After that, we expand the number of
features using the feature expansion component (Sec. 2.3)
and reconstruct the 3D coordinates of the output point cloud
via a series of fully connected layers in the coordinate re-
construction component (Sec. 2.4).

2.1. Patch Extraction

We collect a set of 3D objects as prior information for
training. These objects cover a rich variety of shapes, from

smooth surface to shapes with sharp edges and corners.
Essentially, for our network to upsample a point cloud, it
should learn local geometry patterns from the objects. This
motivates us to take a patch-based approach to train the net-
work and to learn the geometry semantics.

In detail, we randomly select M points on the surface
of these objects. From each selected point, we grow a sur-
face patch on the object, such that any point on the patch
is within a certain geodesic distance (d) from the selected
point over the surface. Then, we use Poisson disk sampling
to randomly generate ˆN points on each patch as the refer-
enced ground truth point distribution on the patch. In our
upsampling task, both local and global context contribute to
a smooth and uniform output. Hence, we set d with varying
sizes, so that we can extract patches of points on the prior
objects with varying scale and density.

2.2. Point Feature Embedding

To learn both local and global geometry context from
the patches, we consider the following two feature learning
strategies, whose beneﬁts complement each other:

Hierarchical feature learning. Progressively capturing
features of growing scales in a hierarchy has been proved to
be an effective strategy for extracting local and global fea-
tures. Hence, we adopt the recently proposed hierarchical
feature learning mechanism in PointNet++ [30] as the very
frontal part in our network. To adopt hierarchical feature
learning for point cloud upsampling, we speciﬁcally use a
relatively small grouping radius in each level, since gener-
ating new points usually involves more of the local context
than the high-level recognition tasks in [30].

Multi-level feature aggregation. Lower layers in a net-
work generally correspond to local features in smaller
scales, and vice versa.
For better upsampling results,
we should optimally aggregate features in different levels.
Some previous works adopt skip-connections for cascaded
multi-level feature aggregation [25, 32, 30]. However, we
found by experiments that such top-down propagation is not
very efﬁcient for aggregating features in our upsampling
problem. Therefore, we propose to directly combine fea-
tures from different levels and let the network learn the im-
portance of each level [11, 38, 12].

Since the input point set on each patch (see point fea-
ture embedding in Fig. 1) is subsampled gradually in hierar-
chical feature extraction, we concatenate the point features
from each level by ﬁrst restoring the features of all original
points from the downsampled point features by the interpo-
lation method in PointNet++ [30]. Speciﬁcally, the features
of an interpolated point x in level (cid:96) is calculated by:

where wi(x)=1/d(x, xi), which is an inverse distance
weight, and xi, x2, x3 are the three nearest neighbors of
x in level (cid:96). We then use a 1×1 convolution to reduce the
interpolated feature in different level to the same dimension,
i.e., C. Finally, we concatenate the features from each level
as the embedded point feature f .

2.3. Feature Expansion

After the point feature embedding component, we ex-
pand the number of features in the feature space. This is
equivalent to expanding the number of points, since points
and features are interchangeable. Suppose the dimension
of f is N × ˜C, N is the number of input points and ˜C is the
feature dimension of the concatenated embedded feature.
The feature expansion operation would output a feature f (cid:48)
with dimension rN × ˜C2, where r is the upsampling rate
and ˜C2 is the new feature dimension. Essentially, this is
similar to feature upsampling in image-related tasks, which
can be done by deconvolution [25] (also known as trans-
posed convolution) or interpolation [7]. However, it is non-
trivial to apply these operations to point clouds due to the
non-regularity and unordered properties of points.

We therefore propose an efﬁcient feature expansion op-
eration based on the sub-pixel convolution layer [33]. This
operation can be represented as:

f (cid:48) = RS( [ C2

1 (C1

1 (f )) , ... , C2

r (C1

r (f )) ] ) ,

(2)

i (·) and C2

where C1
i (·) are two sets of separate 1×1 con-
volutions, and RS(·) is a reshape operation to convert an
N × r ˜C2 tensor to a tensor of size rN × ˜C2. We emphasize
that the feature in the embedding space has already encap-
sulated the relative spatial information from the neighbor-
hood points via the efﬁcient multi-level feature aggregation,
so we do not need to explicitly consider the spatial informa-
tion when performing this feature expansion operation.

It is worth mentioning that the r feature sets generated
from the ﬁrst convolution C1
i (·) in each set have a high cor-
relation, and this would cause the ﬁnal reconstructed 3D
points to be located too close to one another. Hence, we
further add another convolution (with separate weights) for
each feature set. Since we train the network to learn the r
different convolutions for the r feature sets, these new fea-
tures can include more diverse information, thus reducing
their correlations. This feature expansion operation can be
implemented by applying separated convolutions to the r
feature sets; see Fig. 1. It can also be implemented by more
computation efﬁcient grouped convolution [19, 37, 39].

2.4. Coordinate Reconstruction

f ((cid:96))(x) =

(cid:80)3

i=1 wi(x)f ((cid:96))(xi)
(cid:80)3
i=1 wi(x)

,

(1)

In this part, we reconstruct the 3D coordinates of output
points from the expanded feature with the size of rN × ˜C2.
Speciﬁcally, we regress the 3D coordinates via a series of

fully connected layers on the feature of each point, and the
ﬁnal output is the upsampled point coordinates rN × 3.

3. End-to-End Network Training

3.1. Training Data Generation

Point cloud upsampling is an ill-posed problem due to
the uncertainty or ambiguity of upsampled point clouds.
Given a sparse input point cloud, there are many feasible
output point distributions. Therefore, we do not have the
notion of “correct pairs” of input and ground truth. To al-
leviate this problem, we propose an on-the-ﬂy input gen-
eration scheme. Speciﬁcally, the referenced ground truth
point distribution of a training patch is ﬁxed, whereas the
input points are randomly sampled from the ground truth
point set with a downsampling rate of r at each training
epoch. Intuitively, this scheme is equivalent to simulating
many feasible output point distributions for a given sparse
input point distribution. Additionally, this scheme can fur-
ther enlarge the training dataset, allowing us to depend on a
relatively small dataset for training.

3.2. Joint Loss Function

We propose a novel joint loss function to train the net-
work in an end-to-end fashion. As we mentioned earlier,
the function should encourage the generated points to be lo-
cated on the underlying object surfaces in a more uniform
distribution. Therefore, we design a joint loss function that
combines the reconstruction loss and repulsion loss.

Reconstruction loss. To put points on the underlying ob-
ject surfaces, we propose to use the Earth Mover’s distance
(EMD) [8] as our reconstruction loss to evaluate the simi-
larity between the predicted point cloud Sp ⊆ R3 and the
referenced ground truth point cloud Sgt ⊆ R3:

Lrec = dEM D(Sp, Sgt) = min

(cid:107)xi − φ(xi)(cid:107)2,

φ:Sp→Sgt

(cid:88)

xi∈Sp

(3)

where φ : Sp → Sgt indicates the bijection mapping.

Actually, Chamfer Distance (CD) is another candidate
for evaluating the similarity between two point sets. How-
ever, compared with CD, EMD can better capture the shape
(see [8] for more details) to encourage the output points to
be located close to the underlying object surfaces. Hence,
we choose to use EMD in our reconstruction loss.

Repulsion loss. Although training with the reconstruction
loss can generate points on the underlying object surfaces,
the generated points tend to be located near the original
points. To distribute the generated points more uniformly,
we design the repulsion loss, which is represented as:

Lrep =

η((cid:107)xi(cid:48) − xi(cid:107))w((cid:107)xi(cid:48) − xi(cid:107)) ,

(4)

ˆN
(cid:88)

(cid:88)

i=0

i(cid:48)∈K(i)

where ˆN = rN is the number of output points, K(i) is the
index set of the k-nearest neighbors of point xi, and (cid:107) · (cid:107) is
the L2-norm. η(r) = −r is called the repulsion term, which
is a decreasing function to penalize xi if xi is located too
close to other points in K(i). To penalize xi only when it is
too close to its neighboring points, we add two restrictions:
(i) we only consider points xi(cid:48) in the k-nearest neighbor-
hood of xi; and (ii) we use the fast-decaying weight func-
tion w(r) in the repulsion loss; that is, we follow [24, 14] to
set w(r) = e−r2/h2
in our experiments.
Altogether, we train the network in an end-to-end man-

ner by minimizing the following joint loss function:

L(θ) = Lrec + αLrep + β(cid:107)θ(cid:107)2,

(5)

where θ indicates the parameters in our network, α balances
the reconstruction loss and repulsion loss, and β denotes the
multiplier of weight decay. For simplicity, we ignore the
index of each training sample.

4. Experiments

4.1. Datasets

Since there are no public benchmarks for point cloud up-
sampling, we collect a dataset of 60 different models from
the Visionair repository [1], ranging from smooth non-rigid
objects (e.g., Bunny) to steep rigid objects (e.g., Chair).
Among them, we randomly select 40 for training, and use
the rest for testing1. We crop 100 patches for each training
object, and we use M =4000 patches to train the network in
total. For testing objects, we use Monte-Carlo random sam-
pling approach to sample 5000 points on each object as in-
put. To further demonstrate the generalization ability of our
network, we directly test our well-trained network on the
SHREC15 [22] dataset, which contains 1200 shapes from
50 categories.
In detail, we randomly choose one model
from each category for testing, considering that each cate-
gory contains 24 similar objects in various poses. As for
ModelNet40 [36] and ShapeNet [4], we found it hard to ex-
tract patches from those objects due to the low mesh quality
(e.g., holes, self-intersection, etc.). Therefore, we use them
for testing; see the supplementary material for the results.

4.2. Implementation Details

The default point number ˆN of each patch is 4096, and
the upsampling rate r is 4. Therefore, each input patch has
1024 points. To avoid overﬁtting, we augment the data by
randomly rotating, shifting and scaling the data. We use 4
levels with grouping radii 0.05, 0.1, 0.2 and 0.3 in the point
feature embedding component, and the dimension C of the
restored feature is 64. For details on other network archi-
tecture parameters, please see our supplementary material.

1The complete object list can be found in the supplementary material.

Figure 2. Example point distributions with corresponding normal-
ized uniformity coefﬁcients (NUC) computed with p = 0.2%.

Parameters k and h in repulsion loss are set as 5 and 0.03,
respectively. The balancing weights α and β are set as 0.01
and β = 10−5, respectively. The implementation is based
on TensorFlow2. For the optimization, we train the network
for 120 epoch using the Adam [17] algorithm with a mini-
batch size of 28 and a learning rate of 0.001. Generally, the
training took about 4.5h on the NVIDIA TITAN Xp GPU.

4.3. Evaluation Metric

To quantitatively evaluate the quality of the output point
sets, we formulate two metrics to measure the deviation be-
tween the output points and the ground truth meshes, as well
as the distribution uniformity of the output points. For sur-
face deviation, we ﬁnd the closest point ˆxi on the mesh for
each predicted point xi, and calculate the distance between
them. Then we compute the mean and standard deviation
over all the points as one of our metrics.

As for the uniformity metric, we randomly put D equal-
size disks on the object surface (D = 9000 in our experi-
ments) and calculate the standard deviation of the number
of points inside the disks. We further normalize the den-
sity of each object and then compute the overall uniformity
of the point sets over all the objects in the testing dataset.
Therefore, we deﬁne the normalized uniformity coefﬁcient
(NUC) with disk area percentage p as:

avg =

1
K ∗ D

K
(cid:88)

D
(cid:88)

k=1

i=1

nk
i
N k ∗ p

,

(cid:118)
(cid:117)
(cid:117)
(cid:116)

N U C =

1
K ∗ D

K
(cid:88)

D
(cid:88)

k=1

i=1

(

nk
i
N k ∗ p

− avg)2,

(6)

where nk
is the number of points within the i-th disk of
i
the k-th object, N k is the total number of points on the k-
th object, K is the total number of test objects, and p is the
percentage of the disk area over the total object surface area.
Note that we use geodesic distance rather than Euclidean
distance to form the disks. Fig. 2 shows three different point
distributions with their corresponding NUC values. As we

2 https://github.com/yulequan/PU-Net

Figure 3. Comparison with the EAR method [15]. We color-code
all point clouds to show the deviation from the ground truth mesh.
For the EAR method, the radius of the Chair model is 0.1 and
0.182, while the radius of the Spider model is 0.106 and 0.155.

can see, the proposed NUC metric can effectively reveal the
point set uniformity: the lower the UNC value, the more
uniform the point set distribution is.

4.4. Comparisons with Other Methods

Comparison with an optimization-based method. We
compare our method with the Edge Aware Resampling
(EAR) method [15], which is a state-of-art method for point
cloud upsampling. The results are shown in Fig. 3, where
the Chair is from our collected testing dataset and the Spider
is from SHREC15. We color-code the point clouds to show
the deviation from the ground truth meshes. There are 1024
points in the input and we do a 4X upsampling. Since EAR
relies on the normal information, to be fair, we calculate the
normal according to the ground truth mesh. We show two
results of EAR with increasing radius, while setting other
parameters to their default values. As we can see, the radius
parameter has a great inﬂuence on EAR’s performance. For
relatively small radius, the output has low surface deviation
but the added points are not uniform, while more outliers
are introduced if the radius is large. In contrast, our method
can better balance the deviation and uniformity without the
need to carefully tune the parameters.

Comparison with deep learning-based methods. As far
as we know, we are not aware of any deep learning-based
method for point cloud upsampling, so we design some
baseline methods for comparison. Since PointNet [29] and
PointNet++ [30] are pioneers for 3D point cloud reason-
ing with deep learning techniques, we design the baselines
based on them. Speciﬁcally, we adopt the semantic segmen-
tation network architecture for point feature embedding and
use one set of convolutions for feature expansion. Note that
we consider two versions of PointNet++: basic PointNet++
and PointNet++ with multi-scale grouping (MSG) for han-
dling non-uniform sampling density; hence, we have three

Table 1. Quantitative comparison on our collected dataset.

NUC with different p

Deviation (10−2)

Time (s)

Method

Input
PointNet [29]
PointNet++ [30]
PointNet++(MSG) [30]
PU-Net (Ours)

0.2% 0.4% 0.6% 0.8% 1.0% 1.2% mean
0.164
0.316
0.270
0.409
0.150
0.215
0.143
0.208
0.115
0.174

0.224
0.334
0.178
0.169
0.138

0.185
0.295
0.160
0.152
0.122

0.150
0.252
0.143
0.137
0.112

0.142
0.239
0.139
0.134
0.110

-
2.27
1.01
0.78
0.63

Table 2. Quantitative comparison on SHREC15 dataset.

NUC with different p

Deviation (10−2)

Time (s)

Method

Input
PointNet [29]
PointNet++ [30]
PointNet++(MSG) [30]
PU-Net (Ours)

0.2% 0.4% 0.6% 0.8% 1.0% 1.2% mean
0.165
0.315
0.330
0.490
0.185
0.259
0.161
0.250
0.144
0.219

0.222
0.405
0.218
0.199
0.173

0.184
0.360
0.197
0.175
0.154

0.146
0.293
0.170
0.148
0.137

0.153
0.309
0.176
0.153
0.140

-
2.03
0.88
0.69
0.52

std
-
3.18
0.83
0.61
0.53

std
-
2.94
0.75
0.59
0.45

-
0.05
0.16
0.45
0.15

-
0.05
0.16
0.45
0.15

baselines in total, and we train them only with the recon-
struction loss. Please refer to the supplemental material for
details of the baseline network architectures. Although we
modify the PointNet, PointNet++, and PointNet++(MSG)
architectures for the upsampling problem, for convenience,
we still call the baselines by their original names.

Tables 1 and 2 list the quantitative comparison results
on our collected dataset and the SHREC15 dataset, respec-
tively. Note that measuring NUC with small p shows local
distribution uniformity in small regions, while measuring
NUC with large p shows more global uniformity. Among
the baselines, PointNet performs the worst, since it cannot
capture local structure information. Compared with Point-
Net++, PointNet++(MSG) can slightly improve the unifor-
mity due to the explicit multi-scale information grouping.
However, it involves more parameters, thus signiﬁcantly
prolonging the training and testing time. Overall, our PU-
Net achieves the best performance with the lowest deviation
from surface and the best distribution uniformity compared
to the baselines, especially on the local uniformity.

Fig. 4 shows results for visual comparison, where points
are colored by their distance deviations from surface. As
we can see, the point clouds predicted by our method better
match the underlying surface with lower deviations.

4.5. Architecture Design Analysis

Analyzing the feature expansion. We compare our pro-
posed feature expansion scheme with two interpolation-like
schemes. The ﬁrst one is similar to a naive point interpola-
tion (denoted as interp1). After extracting the point feature
of each point, we combine its own features and the features
from the nearest neighboring points to generate the upsam-
pled features. The second one introduces more randomness

Table 3. Architecture design analysis on our collect dataset.

NUC with different p

Deviation (10−2)

Methods

interp1+EMD 0.153
interp2+EMD 0.144
0.185
CD
0.140
EMD
0.138
Ours

0.4% 0.6% 0.8% 1.0% mean
0.67
0.126
0.71
0.122
0.87
0.154
0.68
0.119
0.63
0.115

0.136
0.129
0.167
0.126
0.122

0.121
0.118
0.147
0.116
0.112

std
0.54
0.57
0.69
0.58
0.53

(denoted as interp2).
Instead of using the features from
the nearest neighbors, we use a radius based ball query to
ﬁnd the neighborhood and combine the features from these
points to generate the upsampled features. We train these
two networks with the reconstruction loss (also named as
the EMD loss) and the results are listed in the top two rows
of Table 3. For fair comparison, we also train our network
only with the EMD loss. The results are shown in the fourth
row. Comparing with these two interpolation-like schemes,
our proposed scheme can generate more uniform outputs
with comparable surface distance deviation.

Comparing different loss functions. As mentioned above,
the EMD can better capture the object shape than CD. Com-
paring their performance in Table 3, we can see that the
EMD loss improves the output uniformity with low surface
distance deviation when comparing with the CD loss, mean-
ing that the EMD loss can better encourage the output points
to lie on the underlying surface. Furthermore, by comparing
the last two rows in Table 3, we can see that the repulsion
loss can further improve the uniformity of the output.

4.6. More Experiments

Surface reconstruction from upsampled point sets. An
important application of point cloud upsampling is to im-
prove the surface reconstruction quality. Hence, we com-

Figure 4. Visual comparison on samples from our collected dataset (top row) and SHREC (bottom row). The colors on points (see color
map) reveal the surface distance errors, where blue indicates low error and red indicates high error.

Figure 5. Surface reconstruction results from the upsampled point clouds.

pare the reconstruction results of different methods with the
direct Poisson surface reconstruction method [16] provided
in MeshLab [5]; see Fig. 5. We can observe that the re-
construction result from our method is the closest to the
ground truth, while other methods either miss certain struc-
tures (e.g., the leg of the Horse) or overﬁll the hole.

Results of iterative upsampling. To study the ability of
our network to handle varying number of input points, we
design an iterative upsampling experiment, which takes the
output of the previous iteration as the input of the next iter-
ation. Fig. 6 shows the results. The initial input point cloud
has 1024 points and we increase fourfold in each iteration.
From the results, we can see that our network can produce
reasonable results for different number of input points. Fur-
thermore, this iterative upsampling experiment also shows

Figure 6. Results of iterative upsampling. We color-code points by
the depth information. Blue points are closer to us.

Figure 7. Surface reconstruction results from noisy input points.

5. Conclusion

the anti-noise ability of our network to resist the accumu-
lated errors introduced in the iterative upsampling.

Results from noisy input point sets. Fig. 7 shows the sur-
face reconstruction results from noisy point clouds (Gaus-
sian noise of 0.5% and 1% of object bounding box diago-
nal), which demonstrate that our network facilitates the pro-
duction of better surfaces even with noisy inputs.

Results on real-scanned point clouds. Lastly, we eval-
uated the ability of our network to upsample real-scanned
point clouds, which were downloaded from Visionair [1]. In
Fig. 8, the left-most column presents the real-scanned point
clouds. Even though each real-scanned point cloud contains
millions of points, the phenomenon of inhomogeneity still
exists. For better visualization, we cut small patches from
the original point clouds and show the patches in the middle
column. We can observe that the real-scanned points tend to
have line structures, while our network still has the ability
to uniformly add points in the sparse regions.

In this paper, we present a deep network for point cloud
upsampling, with the goal of generating a denser and uni-
form set of points from a sparser set of points. Our network
is trained at a patch-level using a multi-level feature aggre-
gation manner, thus capturing both local and global infor-
mation. The design of our network bypasses the need for
a prescribed order among the points, by operating on in-
dividual features that contain non-local geometry to allow
a context-aware upsampling. Our experiments demonstrate
the effectiveness of our method. As the ﬁrst attempt using
deep networks, our method still has a number of limitations.
Firstly, it is not designed for completion, so our network can
not ﬁll large holes and missing parts. Besides, our network
may not be able to add meaningful points for tiny structures
that are severely undersampled.

In the future, we would like to investigate and develop
more means to handle irregular and sparse data, both for re-
gression purposes and for synthesis. One immediate step is
to develop a downsampling method. Although, downsam-
pling seems like a simpler problem, there is room to devise
proper losses and architecture that maximize the preserva-
tion of information in the decimated point set. We believe
that in general, the development of deep learning methods
for irregular structures is a viable research direction.

Acknowledgments. We thank anonymous reviewers for the
comments and suggestions. The work is supported in part
by the National Basic Program of China, the 973 Program
(Project No. 2015CB351706), the Research Grants Council
of the Hong Kong Special Administrative Region (Project
no. CUHK 14225616), the Shenzhen Science and Tech-
nology Program (No. JCYJ20170413162617606), and the
CUHK strategic recruitment fund.

Figure 8. Results on real-scanned point clouds (Screw nut & Tur-
tle). We color-code input patches and upsampling results to show
the depth information. Blue points are closer to viewpoint.

References

[1] Visionair. [Online; accessed on 14-November-2017]. 4, 8
[2] M. Alexa, J. Behr, D. Cohen-Or, S. Fleishman, D. Levin,
and C. T. Silva. Computing and rendering point set surfaces.
IEEE Trans. Vis. & Comp. Graphics, 9(1):3–15, 2003. 1
[3] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral
networks and locally connected networks on graphs. In Int.
Conf. on Learning Representations (ICLR), 2014. 2

[4] A. X. Chang, T. Funkhouser, L. J. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. ShapeNet: An information-rich 3D model repository.
arXiv preprint arXiv:1512.03012, 2015. 4

[5] P. Cignoni, M. Callieri, M. Corsini, M. Dellepiane, F. Ganov-
elli, and G. Ranzuglia. MeshLab: an open-source mesh pro-
cessing tool. In Eurographics Italian Chapter Conference,
2008. 7

[6] A. Dai, C. R. Qi, and M. Niessner. Shape completion using
3D-Encoder-Predictor CNNs and shape synthesis. In IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
2017. 2

[7] C. Dong, C. C. Loy, K. He, and X. Tang.

Image super-
resolution using deep convolutional networks. IEEE Trans.
Pattern Anal. & Mach. Intell., 38(2):295–307, 2016. 3
[8] H. Fan, H. Su, and L. J. Guibas. A point set generation
network for 3D object reconstruction from a single image.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2017. 2, 4

[9] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and
M. Aubry. AtlasNet: A papier-mˆach´e approach to learning
3D surface generation. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2018. to appear. 2
[10] P. Guerrero, Y. Kleiman, M. Ovsjanikov, and N. J. Mitra.
PCPNet: Learning local shape properties from raw point
clouds. Computer Graphics Forum (Eurographics), 2018.
to appear. 2

[11] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In IEEE Conf. on Computer Vision and Pattern Recog-
nition (CVPR), 2015. 3

[12] Q. Hou, M. Cheng, X. Hu, A. Borji, Z. Tu, and P. Torr.
Deeply supervised salient object detection with short con-
In IEEE Conf. on Computer Vision and Pattern
nections.
Recognition (CVPR), 2017. 3

[13] B.-S. Hua, M.-K. Tran, and S.-K. Yeung. Pointwise convo-
lutional neural networks. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2018. to appear. 2
[14] H. Huang, D. Li, H. Zhang, U. Ascher, and D. Cohen-Or.
Consolidation of unorganized point clouds for surface re-
construction. ACM Trans. on Graphics (SIGGRAPH Asia),
28(5):176:1–8, 2009. 2, 4

[15] H. Huang, S. Wu, M. Gong, D. Cohen-Or, U. Ascher, and
H. R. Zhang. Edge-aware point set resampling. ACM Trans.
on Graphics, 32(9):1–12, 2013. 2, 5

[16] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface
In Eurographics Symposium on Geometry

reconstruction.
Processing (SGP), 2006. 7

[17] D. Kingma and J. Ba. Adam: A method for stochastic opti-
mization. In Int. Conf. on Learning Representations (ICLR),
2015. 5

[18] R. Klokov and V. Lempitsky. Escape from cells: deep Kd-
Networks for the recognition of 3D point cloud models. In
IEEE Int. Conf. on Computer Vision (ICCV), 2017. 1, 2

[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

ImageNet
classiﬁcation with deep convolutional neural networks.
In
Int. Conf. on Neural Information Processing Systems (NIPS),
2012. 3

[20] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a genera-
tive adversarial network. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2017. 1

[21] Y. Li, R. Bu, M. Sun, and B. Chen. PointCNN. arXiv preprint

arXiv:1801.07791, 2018. 2

[22] Z. Lian, J. Zhang, S. Choi, H. ElNaghy, J. El-Sana, T. Fu-
ruya, A. Giachetti, R. A. Guler, L. Lai, C. Li, H. Li,
F. A. Limberger, R. Martin, R. U. Nakanishi, A. P. Neto,
L. G. Nonato, R. Ohbuchi, K. Pevzner, D. Pickup, P. Rosin,
A. Sharf, L. Sun, X. Sun, S. Tari, G. Unal, and R. C. Wilson.
In Proc. of the 2015 Euro-
Non-rigid 3D shape retrieval.
graphics Workshop on 3D Object Retrieval, 2015. 4

[23] C.-H. Lin, C. Kong, and S. Lucey. Learning efﬁcient point
cloud generation for dense 3D object reconstruction. In AAAI
Conf. on Artiﬁcial Intell. (AAAI), 2017. 2

[24] Y. Lipman, D. Cohen-Or, D. Levin, and H. Tal-Ezer.
Parameterization-free projection for geometry reconstruc-
tion. ACM Trans. on Graphics (SIGGRAPH), 26(3):22:1–5,
2007. 1, 4

[25] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In IEEE Conf. on Com-
puter Vision and Pattern Recognition (CVPR), 2015. 3
[26] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst.
Geodesic convolutional neural networks on Riemannian
manifolds. In IEEE Int. Conf. on Computer Vision (ICCV)
workshops, 2015. 2

[27] D. Maturana and S. Scherer. Voxnet: A 3D convolutional
neural network for real-time object recognition. In Int. Conf.
on Intell. Robots and Systems (IROS), 2015. 2

[28] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas. Frus-
tum PointNets for 3D object detection from RGB-D data.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2018. to appear. 2

[29] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3D classiﬁcation and segmentation.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2017. 1, 2, 5, 6

[30] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. PointNet++:
Deep hierarchical feature learning on point sets in a metric
space. In Int. Conf. on Neural Information Processing Sys-
tems (NIPS), 2017. 1, 2, 3, 5, 6

[31] G. Riegler, A. O. Ulusoys, and A. Geiger. OctNet: Learning
deep 3D representations at high resolutions. In IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), 2017.
2

[32] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. In Int.
Conf. on MICCAI, 2015. 3

[33] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2016. 1, 3

[34] W. Wang, R. Yu, Q. Huang, and U. Neumann. SGPN: Sim-
ilarity group proposal network for 3D point cloud instance
segmentation. In IEEE Conf. on Computer Vision and Pat-
tern Recognition (CVPR), 2018. to appear. 2

[35] S. Wu, H. Huang, M. Gong, M. Zwicker, and D. Cohen-
Or. Deep points consolidation. ACM Trans. on Graphics
(SIGGRAPH Asia), 34(6):176:1–13, 2015. 2

[36] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3D ShapeNets: A deep representation for volumet-
ric shapes. In IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2015. 2, 4

[37] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. arXiv
preprint arXiv:1611.05431, 2016. 3

[38] S. Xie and Z. Tu. Holistically-nested edge detection. In IEEE

Int. Conf. on Computer Vision (ICCV), 2015. 3

[39] X. Zhang, X. Zhou, M. Lin, and J. Sun. ShufﬂeNet: An
extremely efﬁcient convolutional neural network for mobile
devices. arXiv preprint arXiv:1707.01083, 2017. 3

A. Overview

In this supplementary material, we ﬁrst provide more details about our collected dataset in Section B. Then, we show the

details of our network architecture as well as the baseline networks employed in the experiments in Section C.

B. Details of our Collected Dataset

We collect 60 different 3D models to form our training and testing datasets. The speciﬁc name of each model is shown in
Table 4. We also present the shapes of some training and testing 3D models in our dataset in Fig. 9 and Fig. 10, respectively.
As we can see, our collected datasets have a large variation in geometry shapes, containing 3D models with smooth surface
regions (ﬁrst row) and 3D models with sharp corners and edges (second row). There is also a large variation between training
and testing 3D models, indicating a good generalization ability of our proposed method.

Table 4. The complete name list of the 3D models in our training and testing datasets.

Model Names

Training

Testing

Armadillo, Boy1, Boy2, Bumpy torus, Bunny, Cad, Cylinder, Child1,
Child2, Chinese lion, Cone, Cup, Dino, Egea, Ellipsoid, Eros, Fish, Fo-
cal octa, Gargoyle, Girl1, Girl2, Hand, Joint, Julius, Nicolo, Octa ﬂower,
Pierrot, Pulley, Pyramid1, Pyramid2, Retinal, Rolling stage, Screwdriver,
Sharp sphere, Special cube, Star1, Turbine, Twirl, Vaselion
Camel, Casting, Chair, Cover rear, Cow, Duck, Eight, Elephant, Elk, Fan-
disk, Genus, Horse, Icosahedron, Kitten, Moai, Octahedron, Pig, Quadric,
Sculpt, Star2

Armadillo

Bunny

Dino

Julius

Pierrot

Vaselion

Block

Cad

Focal octa

Joint

Pulley

Twirl

Figure 9. Examples 3D models in our training dataset. The ﬁrst row shows 3D models with smooth surface regions, while the
second row shows 3D models with sharp corners and edges.

Camel

Elephant

Elk

Horse

Kitten

Moai

Casting

Chair

Fandisk

Icosahedron

Quadric

Sculpt

Figure 10. Examples 3D models in our testing dataset. The ﬁrst row shows 3D models with smooth surface regions, while the
second row shows 3D models with sharp corners and edges.

C. Details of Network Architectures

The details of our network architecture are listed as follows.

• In the hierarchical feature learning component, we use four levels to extract local features. Following the notations
in PointNet++, we use (K, r, [l1, ..., ld]) to represent a level with K local regions of ball radius r, and [l1, ..., ld]
the d fully connected layers with width li (i = 1, ..., d). Therefore, the parameters we use are (N, 0.05, [32, 32, 64]),
(N/2, 0.1, [64, 64, 128]), (N/4, 0.2, [128, 128, 256]) and (N/8, 0.3, [256, 256, 512]).

• In the multi-level feature aggregation component, we use interpolation to restore the feature of each level and use a

convolution to reduce the restored feature to 64 dimensions. Therefore, ˜C = 259 in our experiments.

• In the feature expansion component, the output feature channel numbers ˜C1 and ˜C2 are 256 and 128, respectively.

• In the coordinate reconstruction component, we use two fully connected layers with 64 and 3 output channels, respec-

tively.

The details of the baseline architectures are illustrated in Fig. 11, Fig. 12 and Fig. 13.
All the convolution layers and fully connected layers in the above networks are followed by the ReLU operator, except for

the last coordinate regression layer.

Figure 11. The network architecture of PointNet for point cloud upsampling.

Figure 12. The network architecture of PointNet++ for point cloud upsampling.

Figure 13. The network architecture of PointNet++(MSG) for point cloud upsampling.

8
1
0
2
 
r
a

M
 
6
2
 
 
]

V
C
.
s
c
[
 
 
2
v
1
6
7
6
0
.
1
0
8
1
:
v
i
X
r
a

PU-Net: Point Cloud Upsampling Network

Lequan Yu∗ 1,3 Xianzhi Li∗1 Chi-Wing Fu1,3 Daniel Cohen-Or2 Pheng-Ann Heng1,3

1The Chinese University of Hong Kong

2 Tel Aviv University

3Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology,
Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China

{lqyu,xzli,cwfu,pheng}@cse.cuhk.edu.hk

dcor@mail.tau.ac.il

Abstract

Learning and analyzing 3D point clouds with deep net-
works is challenging due to the sparseness and irregularity
of the data. In this paper, we present a data-driven point
cloud upsampling technique. The key idea is to learn multi-
level features per point and expand the point set via a multi-
branch convolution unit implicitly in feature space. The ex-
panded feature is then split to a multitude of features, which
are then reconstructed to an upsampled point set. Our net-
work is applied at a patch-level, with a joint loss function
that encourages the upsampled points to remain on the un-
derlying surface with a uniform distribution. We conduct
various experiments using synthesis and scan data to eval-
uate our method and demonstrate its superiority over some
baseline methods and an optimization-based method. Re-
sults show that our upsampled points have better uniformity
and are located closer to the underlying surfaces.

1. Introduction

Point cloud is a fundamental 3D representation that has
drawn increasing attention due to the popularity of various
depth scanning devices. Recently, pioneering works [29,
30, 18] began to explore the possibility of reasoning point
clouds by means of deep networks for understanding geom-
etry and recognizing 3D structures. In these works, the deep
networks directly extract features from the raw 3D point co-
ordinates without using traditional features, e.g., normal and
curvature. These works present impressive results for 3D
object classiﬁcation and semantic scene segmentation.

In this work we are interested in an upsampling problem:
given a set of points, generate a denser set of points to de-
scribe the underlying geometry by learning the geometry of
a training dataset. This upsampling problem is similar in
spirit to the image super-resolution problem [33, 20]; how-
ever, dealing with 3D points rather than a 2D grid of pixels

∗Equal contribution.

poses new challenges. First, unlike the image space, which
is represented by a regular grid, point clouds do not have
any spatial order and regular structure. Second, the gener-
ated points should describe the underlying geometry of a
latent target object, meaning that they should roughly lie on
the target object surface. Third, the generated points should
be informative and should not clutter together. Having said
that, the generated output point set should be more uniform
on the target object surface. Thus, simple interpolation be-
tween input points cannot produce satisfactory results.

To meet the above challenges, we present a data-driven
point cloud upsampling network. Our network is applied
at a patch-level, with a joint loss function that encourages
the upsampled points to remain on the underlying surface
with a uniform distribution. The key idea is to learn multi-
level features per point, and then expand the point set via
a multi-branch convolution unit implicitly in feature space.
The expanded feature is then split to a multitude of features,
which are then reconstructed to an upsampled point set.

Our network, namely PU-Net, learns geometry seman-
tics of point-based patches from 3D models, and applies
the learned knowledge to upsample a given point cloud. It
should be noted that unlike previous network-based meth-
ods designed for 3D point sets [29, 30, 18], the number of
input and output points in our network are not the same.

We formulate two metrics, distribution uniformity and
distance deviation from underlying surfaces, to quantita-
tively evaluate the upsampled point set, and test our method
using variety of synthetic and real-scanned data. We also
evaluate the performance of our method, and compare it
to baseline and state-of-the-art optimization-based methods.
Results show that our upsampled points have better unifor-
mity, and are located closer to the underlying surfaces.

Related work: optimization-based methods. An early
work by Alexa et al. [2] upsamples a point set by interpo-
lating points at vertices of a Voronoi diagram in the local
tangent space. Lipman et al. [24] present a locally optimal
projection (LOP) operator for points resampling and surface

1

Figure 1. The architecture of PU-Net (better view in color). The input has N points, while the output has rN points, where r is the
upsampling rate. Ci, ˜C and ˜Ci represent the feature channel number. We restore different level features for the original N points with
interpolation, and reduce all level features to a ﬁxed dimension C with a convolution. The red color in the point feature embedding
component shows the original and progressively subsampled points in hierarchical feature learning, while the green color indicates the
restored features. We jointly adopt the reconstruction loss and repulsion loss in the end-to-end training of PU-Net.

reconstruction based on an L1 median. The operator works
well even when the input point set contains noise and out-
liers. Successively, Huang et al. [14] propose an improved
weighted LOP to address the point set density problem.

Although these works have demonstrated good results,
they make a strong assumption that the underlying sur-
face is smooth, thus restricting the method’s scope. Then,
Huang et al. [15] introduce an edge-aware point set resam-
pling method by ﬁrst resampling away from edges and then
progressively approaching edges and corners. However, the
quality of their results heavily relies on the accuracy of the
normals at given points and careful parameter tuning. It is
worth mentioning that Wu et al. [35] propose a deep points
representation method to fuse consolidation and completion
in one coherent step. Since its main focus is on ﬁlling large
holes, global smoothness is, however, not enforced, so the
method is sensitive to large noise. Overall, the above meth-
ods are not data-driven, thus heavily relying on priors.

Related work: deep-learning-based methods. Points in
a point cloud do not have any speciﬁc order nor follow any
regular grid structure, so only a few recent works adopt a
deep learning model to directly process point clouds. Most
existing works convert a point cloud into some other 3D rep-
resentations such as the volumetric grids [27, 36, 31, 6] and
geometric graphs [3, 26] for processing. Qi et al. [29, 30]
ﬁrstly introduced a deep learning network for point cloud
classiﬁcation and segmentation;
in particular, the Point-
Net++ uses a hierarchical feature learning architecture to
capture both local and global geometry context. Subse-
quently, many other networks were proposed for high-level

analysis problems with point clouds [18, 13, 21, 34, 28].
However, they all focus on global or mid-level attributes
of point clouds. In another work, Guerrero et al. [10] de-
veloped a network to estimate the local shape properties in
point clouds, including normal and curvature. Other rel-
evant networks focus on 3D reconstruction from 2D im-
ages [8, 23, 9]. To the best of our knowledge, there are
no prior works focusing on point cloud upsampling.

2. Network Architecture

Given a 3D point cloud with point coordinates in nonuni-
form distributions, our network aims to output a denser
point cloud that follows the underlying surface of the tar-
get object while being uniform in distribution. Our network
architecture (see Fig. 1) has four components: patch extrac-
tion, point feature embedding, feature expansion, and co-
ordinate reconstruction. First, we extract patches of points
in varying scales and distributions from a given set of prior
3D models (Sec. 2.1). Then, the point feature embedding
component maps the raw 3D coordinates to a feature space
by hierarchical feature learning and multi-level feature ag-
gregation (Sec. 2.2). After that, we expand the number of
features using the feature expansion component (Sec. 2.3)
and reconstruct the 3D coordinates of the output point cloud
via a series of fully connected layers in the coordinate re-
construction component (Sec. 2.4).

2.1. Patch Extraction

We collect a set of 3D objects as prior information for
training. These objects cover a rich variety of shapes, from

smooth surface to shapes with sharp edges and corners.
Essentially, for our network to upsample a point cloud, it
should learn local geometry patterns from the objects. This
motivates us to take a patch-based approach to train the net-
work and to learn the geometry semantics.

In detail, we randomly select M points on the surface
of these objects. From each selected point, we grow a sur-
face patch on the object, such that any point on the patch
is within a certain geodesic distance (d) from the selected
point over the surface. Then, we use Poisson disk sampling
to randomly generate ˆN points on each patch as the refer-
enced ground truth point distribution on the patch. In our
upsampling task, both local and global context contribute to
a smooth and uniform output. Hence, we set d with varying
sizes, so that we can extract patches of points on the prior
objects with varying scale and density.

2.2. Point Feature Embedding

To learn both local and global geometry context from
the patches, we consider the following two feature learning
strategies, whose beneﬁts complement each other:

Hierarchical feature learning. Progressively capturing
features of growing scales in a hierarchy has been proved to
be an effective strategy for extracting local and global fea-
tures. Hence, we adopt the recently proposed hierarchical
feature learning mechanism in PointNet++ [30] as the very
frontal part in our network. To adopt hierarchical feature
learning for point cloud upsampling, we speciﬁcally use a
relatively small grouping radius in each level, since gener-
ating new points usually involves more of the local context
than the high-level recognition tasks in [30].

Multi-level feature aggregation. Lower layers in a net-
work generally correspond to local features in smaller
scales, and vice versa.
For better upsampling results,
we should optimally aggregate features in different levels.
Some previous works adopt skip-connections for cascaded
multi-level feature aggregation [25, 32, 30]. However, we
found by experiments that such top-down propagation is not
very efﬁcient for aggregating features in our upsampling
problem. Therefore, we propose to directly combine fea-
tures from different levels and let the network learn the im-
portance of each level [11, 38, 12].

Since the input point set on each patch (see point fea-
ture embedding in Fig. 1) is subsampled gradually in hierar-
chical feature extraction, we concatenate the point features
from each level by ﬁrst restoring the features of all original
points from the downsampled point features by the interpo-
lation method in PointNet++ [30]. Speciﬁcally, the features
of an interpolated point x in level (cid:96) is calculated by:

where wi(x)=1/d(x, xi), which is an inverse distance
weight, and xi, x2, x3 are the three nearest neighbors of
x in level (cid:96). We then use a 1×1 convolution to reduce the
interpolated feature in different level to the same dimension,
i.e., C. Finally, we concatenate the features from each level
as the embedded point feature f .

2.3. Feature Expansion

After the point feature embedding component, we ex-
pand the number of features in the feature space. This is
equivalent to expanding the number of points, since points
and features are interchangeable. Suppose the dimension
of f is N × ˜C, N is the number of input points and ˜C is the
feature dimension of the concatenated embedded feature.
The feature expansion operation would output a feature f (cid:48)
with dimension rN × ˜C2, where r is the upsampling rate
and ˜C2 is the new feature dimension. Essentially, this is
similar to feature upsampling in image-related tasks, which
can be done by deconvolution [25] (also known as trans-
posed convolution) or interpolation [7]. However, it is non-
trivial to apply these operations to point clouds due to the
non-regularity and unordered properties of points.

We therefore propose an efﬁcient feature expansion op-
eration based on the sub-pixel convolution layer [33]. This
operation can be represented as:

f (cid:48) = RS( [ C2

1 (C1

1 (f )) , ... , C2

r (C1

r (f )) ] ) ,

(2)

i (·) and C2

where C1
i (·) are two sets of separate 1×1 con-
volutions, and RS(·) is a reshape operation to convert an
N × r ˜C2 tensor to a tensor of size rN × ˜C2. We emphasize
that the feature in the embedding space has already encap-
sulated the relative spatial information from the neighbor-
hood points via the efﬁcient multi-level feature aggregation,
so we do not need to explicitly consider the spatial informa-
tion when performing this feature expansion operation.

It is worth mentioning that the r feature sets generated
from the ﬁrst convolution C1
i (·) in each set have a high cor-
relation, and this would cause the ﬁnal reconstructed 3D
points to be located too close to one another. Hence, we
further add another convolution (with separate weights) for
each feature set. Since we train the network to learn the r
different convolutions for the r feature sets, these new fea-
tures can include more diverse information, thus reducing
their correlations. This feature expansion operation can be
implemented by applying separated convolutions to the r
feature sets; see Fig. 1. It can also be implemented by more
computation efﬁcient grouped convolution [19, 37, 39].

2.4. Coordinate Reconstruction

f ((cid:96))(x) =

(cid:80)3

i=1 wi(x)f ((cid:96))(xi)
(cid:80)3
i=1 wi(x)

,

(1)

In this part, we reconstruct the 3D coordinates of output
points from the expanded feature with the size of rN × ˜C2.
Speciﬁcally, we regress the 3D coordinates via a series of

fully connected layers on the feature of each point, and the
ﬁnal output is the upsampled point coordinates rN × 3.

3. End-to-End Network Training

3.1. Training Data Generation

Point cloud upsampling is an ill-posed problem due to
the uncertainty or ambiguity of upsampled point clouds.
Given a sparse input point cloud, there are many feasible
output point distributions. Therefore, we do not have the
notion of “correct pairs” of input and ground truth. To al-
leviate this problem, we propose an on-the-ﬂy input gen-
eration scheme. Speciﬁcally, the referenced ground truth
point distribution of a training patch is ﬁxed, whereas the
input points are randomly sampled from the ground truth
point set with a downsampling rate of r at each training
epoch. Intuitively, this scheme is equivalent to simulating
many feasible output point distributions for a given sparse
input point distribution. Additionally, this scheme can fur-
ther enlarge the training dataset, allowing us to depend on a
relatively small dataset for training.

3.2. Joint Loss Function

We propose a novel joint loss function to train the net-
work in an end-to-end fashion. As we mentioned earlier,
the function should encourage the generated points to be lo-
cated on the underlying object surfaces in a more uniform
distribution. Therefore, we design a joint loss function that
combines the reconstruction loss and repulsion loss.

Reconstruction loss. To put points on the underlying ob-
ject surfaces, we propose to use the Earth Mover’s distance
(EMD) [8] as our reconstruction loss to evaluate the simi-
larity between the predicted point cloud Sp ⊆ R3 and the
referenced ground truth point cloud Sgt ⊆ R3:

Lrec = dEM D(Sp, Sgt) = min

(cid:107)xi − φ(xi)(cid:107)2,

φ:Sp→Sgt

(cid:88)

xi∈Sp

(3)

where φ : Sp → Sgt indicates the bijection mapping.

Actually, Chamfer Distance (CD) is another candidate
for evaluating the similarity between two point sets. How-
ever, compared with CD, EMD can better capture the shape
(see [8] for more details) to encourage the output points to
be located close to the underlying object surfaces. Hence,
we choose to use EMD in our reconstruction loss.

Repulsion loss. Although training with the reconstruction
loss can generate points on the underlying object surfaces,
the generated points tend to be located near the original
points. To distribute the generated points more uniformly,
we design the repulsion loss, which is represented as:

Lrep =

η((cid:107)xi(cid:48) − xi(cid:107))w((cid:107)xi(cid:48) − xi(cid:107)) ,

(4)

ˆN
(cid:88)

(cid:88)

i=0

i(cid:48)∈K(i)

where ˆN = rN is the number of output points, K(i) is the
index set of the k-nearest neighbors of point xi, and (cid:107) · (cid:107) is
the L2-norm. η(r) = −r is called the repulsion term, which
is a decreasing function to penalize xi if xi is located too
close to other points in K(i). To penalize xi only when it is
too close to its neighboring points, we add two restrictions:
(i) we only consider points xi(cid:48) in the k-nearest neighbor-
hood of xi; and (ii) we use the fast-decaying weight func-
tion w(r) in the repulsion loss; that is, we follow [24, 14] to
set w(r) = e−r2/h2
in our experiments.
Altogether, we train the network in an end-to-end man-

ner by minimizing the following joint loss function:

L(θ) = Lrec + αLrep + β(cid:107)θ(cid:107)2,

(5)

where θ indicates the parameters in our network, α balances
the reconstruction loss and repulsion loss, and β denotes the
multiplier of weight decay. For simplicity, we ignore the
index of each training sample.

4. Experiments

4.1. Datasets

Since there are no public benchmarks for point cloud up-
sampling, we collect a dataset of 60 different models from
the Visionair repository [1], ranging from smooth non-rigid
objects (e.g., Bunny) to steep rigid objects (e.g., Chair).
Among them, we randomly select 40 for training, and use
the rest for testing1. We crop 100 patches for each training
object, and we use M =4000 patches to train the network in
total. For testing objects, we use Monte-Carlo random sam-
pling approach to sample 5000 points on each object as in-
put. To further demonstrate the generalization ability of our
network, we directly test our well-trained network on the
SHREC15 [22] dataset, which contains 1200 shapes from
50 categories.
In detail, we randomly choose one model
from each category for testing, considering that each cate-
gory contains 24 similar objects in various poses. As for
ModelNet40 [36] and ShapeNet [4], we found it hard to ex-
tract patches from those objects due to the low mesh quality
(e.g., holes, self-intersection, etc.). Therefore, we use them
for testing; see the supplementary material for the results.

4.2. Implementation Details

The default point number ˆN of each patch is 4096, and
the upsampling rate r is 4. Therefore, each input patch has
1024 points. To avoid overﬁtting, we augment the data by
randomly rotating, shifting and scaling the data. We use 4
levels with grouping radii 0.05, 0.1, 0.2 and 0.3 in the point
feature embedding component, and the dimension C of the
restored feature is 64. For details on other network archi-
tecture parameters, please see our supplementary material.

1The complete object list can be found in the supplementary material.

Figure 2. Example point distributions with corresponding normal-
ized uniformity coefﬁcients (NUC) computed with p = 0.2%.

Parameters k and h in repulsion loss are set as 5 and 0.03,
respectively. The balancing weights α and β are set as 0.01
and β = 10−5, respectively. The implementation is based
on TensorFlow2. For the optimization, we train the network
for 120 epoch using the Adam [17] algorithm with a mini-
batch size of 28 and a learning rate of 0.001. Generally, the
training took about 4.5h on the NVIDIA TITAN Xp GPU.

4.3. Evaluation Metric

To quantitatively evaluate the quality of the output point
sets, we formulate two metrics to measure the deviation be-
tween the output points and the ground truth meshes, as well
as the distribution uniformity of the output points. For sur-
face deviation, we ﬁnd the closest point ˆxi on the mesh for
each predicted point xi, and calculate the distance between
them. Then we compute the mean and standard deviation
over all the points as one of our metrics.

As for the uniformity metric, we randomly put D equal-
size disks on the object surface (D = 9000 in our experi-
ments) and calculate the standard deviation of the number
of points inside the disks. We further normalize the den-
sity of each object and then compute the overall uniformity
of the point sets over all the objects in the testing dataset.
Therefore, we deﬁne the normalized uniformity coefﬁcient
(NUC) with disk area percentage p as:

avg =

1
K ∗ D

K
(cid:88)

D
(cid:88)

k=1

i=1

nk
i
N k ∗ p

,

(cid:118)
(cid:117)
(cid:117)
(cid:116)

N U C =

1
K ∗ D

K
(cid:88)

D
(cid:88)

k=1

i=1

(

nk
i
N k ∗ p

− avg)2,

(6)

where nk
is the number of points within the i-th disk of
i
the k-th object, N k is the total number of points on the k-
th object, K is the total number of test objects, and p is the
percentage of the disk area over the total object surface area.
Note that we use geodesic distance rather than Euclidean
distance to form the disks. Fig. 2 shows three different point
distributions with their corresponding NUC values. As we

2 https://github.com/yulequan/PU-Net

Figure 3. Comparison with the EAR method [15]. We color-code
all point clouds to show the deviation from the ground truth mesh.
For the EAR method, the radius of the Chair model is 0.1 and
0.182, while the radius of the Spider model is 0.106 and 0.155.

can see, the proposed NUC metric can effectively reveal the
point set uniformity: the lower the UNC value, the more
uniform the point set distribution is.

4.4. Comparisons with Other Methods

Comparison with an optimization-based method. We
compare our method with the Edge Aware Resampling
(EAR) method [15], which is a state-of-art method for point
cloud upsampling. The results are shown in Fig. 3, where
the Chair is from our collected testing dataset and the Spider
is from SHREC15. We color-code the point clouds to show
the deviation from the ground truth meshes. There are 1024
points in the input and we do a 4X upsampling. Since EAR
relies on the normal information, to be fair, we calculate the
normal according to the ground truth mesh. We show two
results of EAR with increasing radius, while setting other
parameters to their default values. As we can see, the radius
parameter has a great inﬂuence on EAR’s performance. For
relatively small radius, the output has low surface deviation
but the added points are not uniform, while more outliers
are introduced if the radius is large. In contrast, our method
can better balance the deviation and uniformity without the
need to carefully tune the parameters.

Comparison with deep learning-based methods. As far
as we know, we are not aware of any deep learning-based
method for point cloud upsampling, so we design some
baseline methods for comparison. Since PointNet [29] and
PointNet++ [30] are pioneers for 3D point cloud reason-
ing with deep learning techniques, we design the baselines
based on them. Speciﬁcally, we adopt the semantic segmen-
tation network architecture for point feature embedding and
use one set of convolutions for feature expansion. Note that
we consider two versions of PointNet++: basic PointNet++
and PointNet++ with multi-scale grouping (MSG) for han-
dling non-uniform sampling density; hence, we have three

Table 1. Quantitative comparison on our collected dataset.

NUC with different p

Deviation (10−2)

Time (s)

Method

Input
PointNet [29]
PointNet++ [30]
PointNet++(MSG) [30]
PU-Net (Ours)

0.2% 0.4% 0.6% 0.8% 1.0% 1.2% mean
0.164
0.316
0.270
0.409
0.150
0.215
0.143
0.208
0.115
0.174

0.224
0.334
0.178
0.169
0.138

0.185
0.295
0.160
0.152
0.122

0.150
0.252
0.143
0.137
0.112

0.142
0.239
0.139
0.134
0.110

-
2.27
1.01
0.78
0.63

Table 2. Quantitative comparison on SHREC15 dataset.

NUC with different p

Deviation (10−2)

Time (s)

Method

Input
PointNet [29]
PointNet++ [30]
PointNet++(MSG) [30]
PU-Net (Ours)

0.2% 0.4% 0.6% 0.8% 1.0% 1.2% mean
0.165
0.315
0.330
0.490
0.185
0.259
0.161
0.250
0.144
0.219

0.222
0.405
0.218
0.199
0.173

0.153
0.309
0.176
0.153
0.140

0.146
0.293
0.170
0.148
0.137

0.184
0.360
0.197
0.175
0.154

-
2.03
0.88
0.69
0.52

std
-
3.18
0.83
0.61
0.53

std
-
2.94
0.75
0.59
0.45

-
0.05
0.16
0.45
0.15

-
0.05
0.16
0.45
0.15

baselines in total, and we train them only with the recon-
struction loss. Please refer to the supplemental material for
details of the baseline network architectures. Although we
modify the PointNet, PointNet++, and PointNet++(MSG)
architectures for the upsampling problem, for convenience,
we still call the baselines by their original names.

Tables 1 and 2 list the quantitative comparison results
on our collected dataset and the SHREC15 dataset, respec-
tively. Note that measuring NUC with small p shows local
distribution uniformity in small regions, while measuring
NUC with large p shows more global uniformity. Among
the baselines, PointNet performs the worst, since it cannot
capture local structure information. Compared with Point-
Net++, PointNet++(MSG) can slightly improve the unifor-
mity due to the explicit multi-scale information grouping.
However, it involves more parameters, thus signiﬁcantly
prolonging the training and testing time. Overall, our PU-
Net achieves the best performance with the lowest deviation
from surface and the best distribution uniformity compared
to the baselines, especially on the local uniformity.

Fig. 4 shows results for visual comparison, where points
are colored by their distance deviations from surface. As
we can see, the point clouds predicted by our method better
match the underlying surface with lower deviations.

4.5. Architecture Design Analysis

Analyzing the feature expansion. We compare our pro-
posed feature expansion scheme with two interpolation-like
schemes. The ﬁrst one is similar to a naive point interpola-
tion (denoted as interp1). After extracting the point feature
of each point, we combine its own features and the features
from the nearest neighboring points to generate the upsam-
pled features. The second one introduces more randomness

Table 3. Architecture design analysis on our collect dataset.

NUC with different p

Deviation (10−2)

Methods

interp1+EMD 0.153
interp2+EMD 0.144
0.185
CD
0.140
EMD
0.138
Ours

0.4% 0.6% 0.8% 1.0% mean
0.67
0.126
0.71
0.122
0.87
0.154
0.68
0.119
0.63
0.115

0.136
0.129
0.167
0.126
0.122

0.121
0.118
0.147
0.116
0.112

std
0.54
0.57
0.69
0.58
0.53

(denoted as interp2).
Instead of using the features from
the nearest neighbors, we use a radius based ball query to
ﬁnd the neighborhood and combine the features from these
points to generate the upsampled features. We train these
two networks with the reconstruction loss (also named as
the EMD loss) and the results are listed in the top two rows
of Table 3. For fair comparison, we also train our network
only with the EMD loss. The results are shown in the fourth
row. Comparing with these two interpolation-like schemes,
our proposed scheme can generate more uniform outputs
with comparable surface distance deviation.

Comparing different loss functions. As mentioned above,
the EMD can better capture the object shape than CD. Com-
paring their performance in Table 3, we can see that the
EMD loss improves the output uniformity with low surface
distance deviation when comparing with the CD loss, mean-
ing that the EMD loss can better encourage the output points
to lie on the underlying surface. Furthermore, by comparing
the last two rows in Table 3, we can see that the repulsion
loss can further improve the uniformity of the output.

4.6. More Experiments

Surface reconstruction from upsampled point sets. An
important application of point cloud upsampling is to im-
prove the surface reconstruction quality. Hence, we com-

Figure 4. Visual comparison on samples from our collected dataset (top row) and SHREC (bottom row). The colors on points (see color
map) reveal the surface distance errors, where blue indicates low error and red indicates high error.

Figure 5. Surface reconstruction results from the upsampled point clouds.

pare the reconstruction results of different methods with the
direct Poisson surface reconstruction method [16] provided
in MeshLab [5]; see Fig. 5. We can observe that the re-
construction result from our method is the closest to the
ground truth, while other methods either miss certain struc-
tures (e.g., the leg of the Horse) or overﬁll the hole.

Results of iterative upsampling. To study the ability of
our network to handle varying number of input points, we
design an iterative upsampling experiment, which takes the
output of the previous iteration as the input of the next iter-
ation. Fig. 6 shows the results. The initial input point cloud
has 1024 points and we increase fourfold in each iteration.
From the results, we can see that our network can produce
reasonable results for different number of input points. Fur-
thermore, this iterative upsampling experiment also shows

Figure 6. Results of iterative upsampling. We color-code points by
the depth information. Blue points are closer to us.

Figure 7. Surface reconstruction results from noisy input points.

5. Conclusion

the anti-noise ability of our network to resist the accumu-
lated errors introduced in the iterative upsampling.

Results from noisy input point sets. Fig. 7 shows the sur-
face reconstruction results from noisy point clouds (Gaus-
sian noise of 0.5% and 1% of object bounding box diago-
nal), which demonstrate that our network facilitates the pro-
duction of better surfaces even with noisy inputs.

Results on real-scanned point clouds. Lastly, we eval-
uated the ability of our network to upsample real-scanned
point clouds, which were downloaded from Visionair [1]. In
Fig. 8, the left-most column presents the real-scanned point
clouds. Even though each real-scanned point cloud contains
millions of points, the phenomenon of inhomogeneity still
exists. For better visualization, we cut small patches from
the original point clouds and show the patches in the middle
column. We can observe that the real-scanned points tend to
have line structures, while our network still has the ability
to uniformly add points in the sparse regions.

In this paper, we present a deep network for point cloud
upsampling, with the goal of generating a denser and uni-
form set of points from a sparser set of points. Our network
is trained at a patch-level using a multi-level feature aggre-
gation manner, thus capturing both local and global infor-
mation. The design of our network bypasses the need for
a prescribed order among the points, by operating on in-
dividual features that contain non-local geometry to allow
a context-aware upsampling. Our experiments demonstrate
the effectiveness of our method. As the ﬁrst attempt using
deep networks, our method still has a number of limitations.
Firstly, it is not designed for completion, so our network can
not ﬁll large holes and missing parts. Besides, our network
may not be able to add meaningful points for tiny structures
that are severely undersampled.

In the future, we would like to investigate and develop
more means to handle irregular and sparse data, both for re-
gression purposes and for synthesis. One immediate step is
to develop a downsampling method. Although, downsam-
pling seems like a simpler problem, there is room to devise
proper losses and architecture that maximize the preserva-
tion of information in the decimated point set. We believe
that in general, the development of deep learning methods
for irregular structures is a viable research direction.

Acknowledgments. We thank anonymous reviewers for the
comments and suggestions. The work is supported in part
by the National Basic Program of China, the 973 Program
(Project No. 2015CB351706), the Research Grants Council
of the Hong Kong Special Administrative Region (Project
no. CUHK 14225616), the Shenzhen Science and Tech-
nology Program (No. JCYJ20170413162617606), and the
CUHK strategic recruitment fund.

Figure 8. Results on real-scanned point clouds (Screw nut & Tur-
tle). We color-code input patches and upsampling results to show
the depth information. Blue points are closer to viewpoint.

References

[1] Visionair. [Online; accessed on 14-November-2017]. 4, 8
[2] M. Alexa, J. Behr, D. Cohen-Or, S. Fleishman, D. Levin,
and C. T. Silva. Computing and rendering point set surfaces.
IEEE Trans. Vis. & Comp. Graphics, 9(1):3–15, 2003. 1
[3] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral
networks and locally connected networks on graphs. In Int.
Conf. on Learning Representations (ICLR), 2014. 2

[4] A. X. Chang, T. Funkhouser, L. J. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. ShapeNet: An information-rich 3D model repository.
arXiv preprint arXiv:1512.03012, 2015. 4

[5] P. Cignoni, M. Callieri, M. Corsini, M. Dellepiane, F. Ganov-
elli, and G. Ranzuglia. MeshLab: an open-source mesh pro-
cessing tool. In Eurographics Italian Chapter Conference,
2008. 7

[6] A. Dai, C. R. Qi, and M. Niessner. Shape completion using
3D-Encoder-Predictor CNNs and shape synthesis. In IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
2017. 2

[7] C. Dong, C. C. Loy, K. He, and X. Tang.

Image super-
resolution using deep convolutional networks. IEEE Trans.
Pattern Anal. & Mach. Intell., 38(2):295–307, 2016. 3
[8] H. Fan, H. Su, and L. J. Guibas. A point set generation
network for 3D object reconstruction from a single image.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2017. 2, 4

[9] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and
M. Aubry. AtlasNet: A papier-mˆach´e approach to learning
3D surface generation. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2018. to appear. 2
[10] P. Guerrero, Y. Kleiman, M. Ovsjanikov, and N. J. Mitra.
PCPNet: Learning local shape properties from raw point
clouds. Computer Graphics Forum (Eurographics), 2018.
to appear. 2

[11] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In IEEE Conf. on Computer Vision and Pattern Recog-
nition (CVPR), 2015. 3

[12] Q. Hou, M. Cheng, X. Hu, A. Borji, Z. Tu, and P. Torr.
Deeply supervised salient object detection with short con-
In IEEE Conf. on Computer Vision and Pattern
nections.
Recognition (CVPR), 2017. 3

[13] B.-S. Hua, M.-K. Tran, and S.-K. Yeung. Pointwise convo-
lutional neural networks. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2018. to appear. 2
[14] H. Huang, D. Li, H. Zhang, U. Ascher, and D. Cohen-Or.
Consolidation of unorganized point clouds for surface re-
construction. ACM Trans. on Graphics (SIGGRAPH Asia),
28(5):176:1–8, 2009. 2, 4

[15] H. Huang, S. Wu, M. Gong, D. Cohen-Or, U. Ascher, and
H. R. Zhang. Edge-aware point set resampling. ACM Trans.
on Graphics, 32(9):1–12, 2013. 2, 5

[16] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface
In Eurographics Symposium on Geometry

reconstruction.
Processing (SGP), 2006. 7

[17] D. Kingma and J. Ba. Adam: A method for stochastic opti-
mization. In Int. Conf. on Learning Representations (ICLR),
2015. 5

[18] R. Klokov and V. Lempitsky. Escape from cells: deep Kd-
Networks for the recognition of 3D point cloud models. In
IEEE Int. Conf. on Computer Vision (ICCV), 2017. 1, 2

[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

ImageNet
classiﬁcation with deep convolutional neural networks.
In
Int. Conf. on Neural Information Processing Systems (NIPS),
2012. 3

[20] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a genera-
tive adversarial network. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2017. 1

[21] Y. Li, R. Bu, M. Sun, and B. Chen. PointCNN. arXiv preprint

arXiv:1801.07791, 2018. 2

[22] Z. Lian, J. Zhang, S. Choi, H. ElNaghy, J. El-Sana, T. Fu-
ruya, A. Giachetti, R. A. Guler, L. Lai, C. Li, H. Li,
F. A. Limberger, R. Martin, R. U. Nakanishi, A. P. Neto,
L. G. Nonato, R. Ohbuchi, K. Pevzner, D. Pickup, P. Rosin,
A. Sharf, L. Sun, X. Sun, S. Tari, G. Unal, and R. C. Wilson.
In Proc. of the 2015 Euro-
Non-rigid 3D shape retrieval.
graphics Workshop on 3D Object Retrieval, 2015. 4

[23] C.-H. Lin, C. Kong, and S. Lucey. Learning efﬁcient point
cloud generation for dense 3D object reconstruction. In AAAI
Conf. on Artiﬁcial Intell. (AAAI), 2017. 2

[24] Y. Lipman, D. Cohen-Or, D. Levin, and H. Tal-Ezer.
Parameterization-free projection for geometry reconstruc-
tion. ACM Trans. on Graphics (SIGGRAPH), 26(3):22:1–5,
2007. 1, 4

[25] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In IEEE Conf. on Com-
puter Vision and Pattern Recognition (CVPR), 2015. 3
[26] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst.
Geodesic convolutional neural networks on Riemannian
manifolds. In IEEE Int. Conf. on Computer Vision (ICCV)
workshops, 2015. 2

[27] D. Maturana and S. Scherer. Voxnet: A 3D convolutional
neural network for real-time object recognition. In Int. Conf.
on Intell. Robots and Systems (IROS), 2015. 2

[28] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas. Frus-
tum PointNets for 3D object detection from RGB-D data.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2018. to appear. 2

[29] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3D classiﬁcation and segmentation.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2017. 1, 2, 5, 6

[30] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. PointNet++:
Deep hierarchical feature learning on point sets in a metric
space. In Int. Conf. on Neural Information Processing Sys-
tems (NIPS), 2017. 1, 2, 3, 5, 6

[31] G. Riegler, A. O. Ulusoys, and A. Geiger. OctNet: Learning
deep 3D representations at high resolutions. In IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), 2017.
2

[32] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. In Int.
Conf. on MICCAI, 2015. 3

[33] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2016. 1, 3

[34] W. Wang, R. Yu, Q. Huang, and U. Neumann. SGPN: Sim-
ilarity group proposal network for 3D point cloud instance
segmentation. In IEEE Conf. on Computer Vision and Pat-
tern Recognition (CVPR), 2018. to appear. 2

[35] S. Wu, H. Huang, M. Gong, M. Zwicker, and D. Cohen-
Or. Deep points consolidation. ACM Trans. on Graphics
(SIGGRAPH Asia), 34(6):176:1–13, 2015. 2

[36] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3D ShapeNets: A deep representation for volumet-
ric shapes. In IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2015. 2, 4

[37] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. arXiv
preprint arXiv:1611.05431, 2016. 3

[38] S. Xie and Z. Tu. Holistically-nested edge detection. In IEEE

Int. Conf. on Computer Vision (ICCV), 2015. 3

[39] X. Zhang, X. Zhou, M. Lin, and J. Sun. ShufﬂeNet: An
extremely efﬁcient convolutional neural network for mobile
devices. arXiv preprint arXiv:1707.01083, 2017. 3

A. Overview

In this supplementary material, we ﬁrst provide more details about our collected dataset in Section B. Then, we show the

details of our network architecture as well as the baseline networks employed in the experiments in Section C.

B. Details of our Collected Dataset

We collect 60 different 3D models to form our training and testing datasets. The speciﬁc name of each model is shown in
Table 4. We also present the shapes of some training and testing 3D models in our dataset in Fig. 9 and Fig. 10, respectively.
As we can see, our collected datasets have a large variation in geometry shapes, containing 3D models with smooth surface
regions (ﬁrst row) and 3D models with sharp corners and edges (second row). There is also a large variation between training
and testing 3D models, indicating a good generalization ability of our proposed method.

Table 4. The complete name list of the 3D models in our training and testing datasets.

Model Names

Training

Testing

Armadillo, Boy1, Boy2, Bumpy torus, Bunny, Cad, Cylinder, Child1,
Child2, Chinese lion, Cone, Cup, Dino, Egea, Ellipsoid, Eros, Fish, Fo-
cal octa, Gargoyle, Girl1, Girl2, Hand, Joint, Julius, Nicolo, Octa ﬂower,
Pierrot, Pulley, Pyramid1, Pyramid2, Retinal, Rolling stage, Screwdriver,
Sharp sphere, Special cube, Star1, Turbine, Twirl, Vaselion
Camel, Casting, Chair, Cover rear, Cow, Duck, Eight, Elephant, Elk, Fan-
disk, Genus, Horse, Icosahedron, Kitten, Moai, Octahedron, Pig, Quadric,
Sculpt, Star2

Armadillo

Bunny

Dino

Julius

Pierrot

Vaselion

Block

Cad

Focal octa

Joint

Pulley

Twirl

Figure 9. Examples 3D models in our training dataset. The ﬁrst row shows 3D models with smooth surface regions, while the
second row shows 3D models with sharp corners and edges.

Camel

Elephant

Elk

Horse

Kitten

Moai

Casting

Chair

Fandisk

Icosahedron

Quadric

Sculpt

Figure 10. Examples 3D models in our testing dataset. The ﬁrst row shows 3D models with smooth surface regions, while the
second row shows 3D models with sharp corners and edges.

C. Details of Network Architectures

The details of our network architecture are listed as follows.

• In the hierarchical feature learning component, we use four levels to extract local features. Following the notations
in PointNet++, we use (K, r, [l1, ..., ld]) to represent a level with K local regions of ball radius r, and [l1, ..., ld]
the d fully connected layers with width li (i = 1, ..., d). Therefore, the parameters we use are (N, 0.05, [32, 32, 64]),
(N/2, 0.1, [64, 64, 128]), (N/4, 0.2, [128, 128, 256]) and (N/8, 0.3, [256, 256, 512]).

• In the multi-level feature aggregation component, we use interpolation to restore the feature of each level and use a

convolution to reduce the restored feature to 64 dimensions. Therefore, ˜C = 259 in our experiments.

• In the feature expansion component, the output feature channel numbers ˜C1 and ˜C2 are 256 and 128, respectively.

• In the coordinate reconstruction component, we use two fully connected layers with 64 and 3 output channels, respec-

tively.

The details of the baseline architectures are illustrated in Fig. 11, Fig. 12 and Fig. 13.
All the convolution layers and fully connected layers in the above networks are followed by the ReLU operator, except for

the last coordinate regression layer.

Figure 11. The network architecture of PointNet for point cloud upsampling.

Figure 12. The network architecture of PointNet++ for point cloud upsampling.

Figure 13. The network architecture of PointNet++(MSG) for point cloud upsampling.

8
1
0
2
 
r
a

M
 
6
2
 
 
]

V
C
.
s
c
[
 
 
2
v
1
6
7
6
0
.
1
0
8
1
:
v
i
X
r
a

PU-Net: Point Cloud Upsampling Network

Lequan Yu∗ 1,3 Xianzhi Li∗1 Chi-Wing Fu1,3 Daniel Cohen-Or2 Pheng-Ann Heng1,3

1The Chinese University of Hong Kong

2 Tel Aviv University

3Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology,
Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China

{lqyu,xzli,cwfu,pheng}@cse.cuhk.edu.hk

dcor@mail.tau.ac.il

Abstract

Learning and analyzing 3D point clouds with deep net-
works is challenging due to the sparseness and irregularity
of the data. In this paper, we present a data-driven point
cloud upsampling technique. The key idea is to learn multi-
level features per point and expand the point set via a multi-
branch convolution unit implicitly in feature space. The ex-
panded feature is then split to a multitude of features, which
are then reconstructed to an upsampled point set. Our net-
work is applied at a patch-level, with a joint loss function
that encourages the upsampled points to remain on the un-
derlying surface with a uniform distribution. We conduct
various experiments using synthesis and scan data to eval-
uate our method and demonstrate its superiority over some
baseline methods and an optimization-based method. Re-
sults show that our upsampled points have better uniformity
and are located closer to the underlying surfaces.

1. Introduction

Point cloud is a fundamental 3D representation that has
drawn increasing attention due to the popularity of various
depth scanning devices. Recently, pioneering works [29,
30, 18] began to explore the possibility of reasoning point
clouds by means of deep networks for understanding geom-
etry and recognizing 3D structures. In these works, the deep
networks directly extract features from the raw 3D point co-
ordinates without using traditional features, e.g., normal and
curvature. These works present impressive results for 3D
object classiﬁcation and semantic scene segmentation.

In this work we are interested in an upsampling problem:
given a set of points, generate a denser set of points to de-
scribe the underlying geometry by learning the geometry of
a training dataset. This upsampling problem is similar in
spirit to the image super-resolution problem [33, 20]; how-
ever, dealing with 3D points rather than a 2D grid of pixels

∗Equal contribution.

poses new challenges. First, unlike the image space, which
is represented by a regular grid, point clouds do not have
any spatial order and regular structure. Second, the gener-
ated points should describe the underlying geometry of a
latent target object, meaning that they should roughly lie on
the target object surface. Third, the generated points should
be informative and should not clutter together. Having said
that, the generated output point set should be more uniform
on the target object surface. Thus, simple interpolation be-
tween input points cannot produce satisfactory results.

To meet the above challenges, we present a data-driven
point cloud upsampling network. Our network is applied
at a patch-level, with a joint loss function that encourages
the upsampled points to remain on the underlying surface
with a uniform distribution. The key idea is to learn multi-
level features per point, and then expand the point set via
a multi-branch convolution unit implicitly in feature space.
The expanded feature is then split to a multitude of features,
which are then reconstructed to an upsampled point set.

Our network, namely PU-Net, learns geometry seman-
tics of point-based patches from 3D models, and applies
the learned knowledge to upsample a given point cloud. It
should be noted that unlike previous network-based meth-
ods designed for 3D point sets [29, 30, 18], the number of
input and output points in our network are not the same.

We formulate two metrics, distribution uniformity and
distance deviation from underlying surfaces, to quantita-
tively evaluate the upsampled point set, and test our method
using variety of synthetic and real-scanned data. We also
evaluate the performance of our method, and compare it
to baseline and state-of-the-art optimization-based methods.
Results show that our upsampled points have better unifor-
mity, and are located closer to the underlying surfaces.

Related work: optimization-based methods. An early
work by Alexa et al. [2] upsamples a point set by interpo-
lating points at vertices of a Voronoi diagram in the local
tangent space. Lipman et al. [24] present a locally optimal
projection (LOP) operator for points resampling and surface

1

Figure 1. The architecture of PU-Net (better view in color). The input has N points, while the output has rN points, where r is the
upsampling rate. Ci, ˜C and ˜Ci represent the feature channel number. We restore different level features for the original N points with
interpolation, and reduce all level features to a ﬁxed dimension C with a convolution. The red color in the point feature embedding
component shows the original and progressively subsampled points in hierarchical feature learning, while the green color indicates the
restored features. We jointly adopt the reconstruction loss and repulsion loss in the end-to-end training of PU-Net.

reconstruction based on an L1 median. The operator works
well even when the input point set contains noise and out-
liers. Successively, Huang et al. [14] propose an improved
weighted LOP to address the point set density problem.

Although these works have demonstrated good results,
they make a strong assumption that the underlying sur-
face is smooth, thus restricting the method’s scope. Then,
Huang et al. [15] introduce an edge-aware point set resam-
pling method by ﬁrst resampling away from edges and then
progressively approaching edges and corners. However, the
quality of their results heavily relies on the accuracy of the
normals at given points and careful parameter tuning. It is
worth mentioning that Wu et al. [35] propose a deep points
representation method to fuse consolidation and completion
in one coherent step. Since its main focus is on ﬁlling large
holes, global smoothness is, however, not enforced, so the
method is sensitive to large noise. Overall, the above meth-
ods are not data-driven, thus heavily relying on priors.

Related work: deep-learning-based methods. Points in
a point cloud do not have any speciﬁc order nor follow any
regular grid structure, so only a few recent works adopt a
deep learning model to directly process point clouds. Most
existing works convert a point cloud into some other 3D rep-
resentations such as the volumetric grids [27, 36, 31, 6] and
geometric graphs [3, 26] for processing. Qi et al. [29, 30]
ﬁrstly introduced a deep learning network for point cloud
classiﬁcation and segmentation;
in particular, the Point-
Net++ uses a hierarchical feature learning architecture to
capture both local and global geometry context. Subse-
quently, many other networks were proposed for high-level

analysis problems with point clouds [18, 13, 21, 34, 28].
However, they all focus on global or mid-level attributes
of point clouds. In another work, Guerrero et al. [10] de-
veloped a network to estimate the local shape properties in
point clouds, including normal and curvature. Other rel-
evant networks focus on 3D reconstruction from 2D im-
ages [8, 23, 9]. To the best of our knowledge, there are
no prior works focusing on point cloud upsampling.

2. Network Architecture

Given a 3D point cloud with point coordinates in nonuni-
form distributions, our network aims to output a denser
point cloud that follows the underlying surface of the tar-
get object while being uniform in distribution. Our network
architecture (see Fig. 1) has four components: patch extrac-
tion, point feature embedding, feature expansion, and co-
ordinate reconstruction. First, we extract patches of points
in varying scales and distributions from a given set of prior
3D models (Sec. 2.1). Then, the point feature embedding
component maps the raw 3D coordinates to a feature space
by hierarchical feature learning and multi-level feature ag-
gregation (Sec. 2.2). After that, we expand the number of
features using the feature expansion component (Sec. 2.3)
and reconstruct the 3D coordinates of the output point cloud
via a series of fully connected layers in the coordinate re-
construction component (Sec. 2.4).

2.1. Patch Extraction

We collect a set of 3D objects as prior information for
training. These objects cover a rich variety of shapes, from

smooth surface to shapes with sharp edges and corners.
Essentially, for our network to upsample a point cloud, it
should learn local geometry patterns from the objects. This
motivates us to take a patch-based approach to train the net-
work and to learn the geometry semantics.

In detail, we randomly select M points on the surface
of these objects. From each selected point, we grow a sur-
face patch on the object, such that any point on the patch
is within a certain geodesic distance (d) from the selected
point over the surface. Then, we use Poisson disk sampling
to randomly generate ˆN points on each patch as the refer-
enced ground truth point distribution on the patch. In our
upsampling task, both local and global context contribute to
a smooth and uniform output. Hence, we set d with varying
sizes, so that we can extract patches of points on the prior
objects with varying scale and density.

2.2. Point Feature Embedding

To learn both local and global geometry context from
the patches, we consider the following two feature learning
strategies, whose beneﬁts complement each other:

Hierarchical feature learning. Progressively capturing
features of growing scales in a hierarchy has been proved to
be an effective strategy for extracting local and global fea-
tures. Hence, we adopt the recently proposed hierarchical
feature learning mechanism in PointNet++ [30] as the very
frontal part in our network. To adopt hierarchical feature
learning for point cloud upsampling, we speciﬁcally use a
relatively small grouping radius in each level, since gener-
ating new points usually involves more of the local context
than the high-level recognition tasks in [30].

Multi-level feature aggregation. Lower layers in a net-
work generally correspond to local features in smaller
scales, and vice versa.
For better upsampling results,
we should optimally aggregate features in different levels.
Some previous works adopt skip-connections for cascaded
multi-level feature aggregation [25, 32, 30]. However, we
found by experiments that such top-down propagation is not
very efﬁcient for aggregating features in our upsampling
problem. Therefore, we propose to directly combine fea-
tures from different levels and let the network learn the im-
portance of each level [11, 38, 12].

Since the input point set on each patch (see point fea-
ture embedding in Fig. 1) is subsampled gradually in hierar-
chical feature extraction, we concatenate the point features
from each level by ﬁrst restoring the features of all original
points from the downsampled point features by the interpo-
lation method in PointNet++ [30]. Speciﬁcally, the features
of an interpolated point x in level (cid:96) is calculated by:

where wi(x)=1/d(x, xi), which is an inverse distance
weight, and xi, x2, x3 are the three nearest neighbors of
x in level (cid:96). We then use a 1×1 convolution to reduce the
interpolated feature in different level to the same dimension,
i.e., C. Finally, we concatenate the features from each level
as the embedded point feature f .

2.3. Feature Expansion

After the point feature embedding component, we ex-
pand the number of features in the feature space. This is
equivalent to expanding the number of points, since points
and features are interchangeable. Suppose the dimension
of f is N × ˜C, N is the number of input points and ˜C is the
feature dimension of the concatenated embedded feature.
The feature expansion operation would output a feature f (cid:48)
with dimension rN × ˜C2, where r is the upsampling rate
and ˜C2 is the new feature dimension. Essentially, this is
similar to feature upsampling in image-related tasks, which
can be done by deconvolution [25] (also known as trans-
posed convolution) or interpolation [7]. However, it is non-
trivial to apply these operations to point clouds due to the
non-regularity and unordered properties of points.

We therefore propose an efﬁcient feature expansion op-
eration based on the sub-pixel convolution layer [33]. This
operation can be represented as:

f (cid:48) = RS( [ C2

1 (C1

1 (f )) , ... , C2

r (C1

r (f )) ] ) ,

(2)

i (·) and C2

where C1
i (·) are two sets of separate 1×1 con-
volutions, and RS(·) is a reshape operation to convert an
N × r ˜C2 tensor to a tensor of size rN × ˜C2. We emphasize
that the feature in the embedding space has already encap-
sulated the relative spatial information from the neighbor-
hood points via the efﬁcient multi-level feature aggregation,
so we do not need to explicitly consider the spatial informa-
tion when performing this feature expansion operation.

It is worth mentioning that the r feature sets generated
from the ﬁrst convolution C1
i (·) in each set have a high cor-
relation, and this would cause the ﬁnal reconstructed 3D
points to be located too close to one another. Hence, we
further add another convolution (with separate weights) for
each feature set. Since we train the network to learn the r
different convolutions for the r feature sets, these new fea-
tures can include more diverse information, thus reducing
their correlations. This feature expansion operation can be
implemented by applying separated convolutions to the r
feature sets; see Fig. 1. It can also be implemented by more
computation efﬁcient grouped convolution [19, 37, 39].

2.4. Coordinate Reconstruction

f ((cid:96))(x) =

(cid:80)3

i=1 wi(x)f ((cid:96))(xi)
(cid:80)3
i=1 wi(x)

,

(1)

In this part, we reconstruct the 3D coordinates of output
points from the expanded feature with the size of rN × ˜C2.
Speciﬁcally, we regress the 3D coordinates via a series of

fully connected layers on the feature of each point, and the
ﬁnal output is the upsampled point coordinates rN × 3.

3. End-to-End Network Training

3.1. Training Data Generation

Point cloud upsampling is an ill-posed problem due to
the uncertainty or ambiguity of upsampled point clouds.
Given a sparse input point cloud, there are many feasible
output point distributions. Therefore, we do not have the
notion of “correct pairs” of input and ground truth. To al-
leviate this problem, we propose an on-the-ﬂy input gen-
eration scheme. Speciﬁcally, the referenced ground truth
point distribution of a training patch is ﬁxed, whereas the
input points are randomly sampled from the ground truth
point set with a downsampling rate of r at each training
epoch. Intuitively, this scheme is equivalent to simulating
many feasible output point distributions for a given sparse
input point distribution. Additionally, this scheme can fur-
ther enlarge the training dataset, allowing us to depend on a
relatively small dataset for training.

3.2. Joint Loss Function

We propose a novel joint loss function to train the net-
work in an end-to-end fashion. As we mentioned earlier,
the function should encourage the generated points to be lo-
cated on the underlying object surfaces in a more uniform
distribution. Therefore, we design a joint loss function that
combines the reconstruction loss and repulsion loss.

Reconstruction loss. To put points on the underlying ob-
ject surfaces, we propose to use the Earth Mover’s distance
(EMD) [8] as our reconstruction loss to evaluate the simi-
larity between the predicted point cloud Sp ⊆ R3 and the
referenced ground truth point cloud Sgt ⊆ R3:

Lrec = dEM D(Sp, Sgt) = min

(cid:107)xi − φ(xi)(cid:107)2,

φ:Sp→Sgt

(cid:88)

xi∈Sp

(3)

where φ : Sp → Sgt indicates the bijection mapping.

Actually, Chamfer Distance (CD) is another candidate
for evaluating the similarity between two point sets. How-
ever, compared with CD, EMD can better capture the shape
(see [8] for more details) to encourage the output points to
be located close to the underlying object surfaces. Hence,
we choose to use EMD in our reconstruction loss.

Repulsion loss. Although training with the reconstruction
loss can generate points on the underlying object surfaces,
the generated points tend to be located near the original
points. To distribute the generated points more uniformly,
we design the repulsion loss, which is represented as:

Lrep =

η((cid:107)xi(cid:48) − xi(cid:107))w((cid:107)xi(cid:48) − xi(cid:107)) ,

(4)

ˆN
(cid:88)

(cid:88)

i=0

i(cid:48)∈K(i)

where ˆN = rN is the number of output points, K(i) is the
index set of the k-nearest neighbors of point xi, and (cid:107) · (cid:107) is
the L2-norm. η(r) = −r is called the repulsion term, which
is a decreasing function to penalize xi if xi is located too
close to other points in K(i). To penalize xi only when it is
too close to its neighboring points, we add two restrictions:
(i) we only consider points xi(cid:48) in the k-nearest neighbor-
hood of xi; and (ii) we use the fast-decaying weight func-
tion w(r) in the repulsion loss; that is, we follow [24, 14] to
set w(r) = e−r2/h2
in our experiments.
Altogether, we train the network in an end-to-end man-

ner by minimizing the following joint loss function:

L(θ) = Lrec + αLrep + β(cid:107)θ(cid:107)2,

(5)

where θ indicates the parameters in our network, α balances
the reconstruction loss and repulsion loss, and β denotes the
multiplier of weight decay. For simplicity, we ignore the
index of each training sample.

4. Experiments

4.1. Datasets

Since there are no public benchmarks for point cloud up-
sampling, we collect a dataset of 60 different models from
the Visionair repository [1], ranging from smooth non-rigid
objects (e.g., Bunny) to steep rigid objects (e.g., Chair).
Among them, we randomly select 40 for training, and use
the rest for testing1. We crop 100 patches for each training
object, and we use M =4000 patches to train the network in
total. For testing objects, we use Monte-Carlo random sam-
pling approach to sample 5000 points on each object as in-
put. To further demonstrate the generalization ability of our
network, we directly test our well-trained network on the
SHREC15 [22] dataset, which contains 1200 shapes from
50 categories.
In detail, we randomly choose one model
from each category for testing, considering that each cate-
gory contains 24 similar objects in various poses. As for
ModelNet40 [36] and ShapeNet [4], we found it hard to ex-
tract patches from those objects due to the low mesh quality
(e.g., holes, self-intersection, etc.). Therefore, we use them
for testing; see the supplementary material for the results.

4.2. Implementation Details

The default point number ˆN of each patch is 4096, and
the upsampling rate r is 4. Therefore, each input patch has
1024 points. To avoid overﬁtting, we augment the data by
randomly rotating, shifting and scaling the data. We use 4
levels with grouping radii 0.05, 0.1, 0.2 and 0.3 in the point
feature embedding component, and the dimension C of the
restored feature is 64. For details on other network archi-
tecture parameters, please see our supplementary material.

1The complete object list can be found in the supplementary material.

Figure 2. Example point distributions with corresponding normal-
ized uniformity coefﬁcients (NUC) computed with p = 0.2%.

Parameters k and h in repulsion loss are set as 5 and 0.03,
respectively. The balancing weights α and β are set as 0.01
and β = 10−5, respectively. The implementation is based
on TensorFlow2. For the optimization, we train the network
for 120 epoch using the Adam [17] algorithm with a mini-
batch size of 28 and a learning rate of 0.001. Generally, the
training took about 4.5h on the NVIDIA TITAN Xp GPU.

4.3. Evaluation Metric

To quantitatively evaluate the quality of the output point
sets, we formulate two metrics to measure the deviation be-
tween the output points and the ground truth meshes, as well
as the distribution uniformity of the output points. For sur-
face deviation, we ﬁnd the closest point ˆxi on the mesh for
each predicted point xi, and calculate the distance between
them. Then we compute the mean and standard deviation
over all the points as one of our metrics.

As for the uniformity metric, we randomly put D equal-
size disks on the object surface (D = 9000 in our experi-
ments) and calculate the standard deviation of the number
of points inside the disks. We further normalize the den-
sity of each object and then compute the overall uniformity
of the point sets over all the objects in the testing dataset.
Therefore, we deﬁne the normalized uniformity coefﬁcient
(NUC) with disk area percentage p as:

avg =

1
K ∗ D

K
(cid:88)

D
(cid:88)

k=1

i=1

nk
i
N k ∗ p

,

(cid:118)
(cid:117)
(cid:117)
(cid:116)

N U C =

1
K ∗ D

K
(cid:88)

D
(cid:88)

k=1

i=1

(

nk
i
N k ∗ p

− avg)2,

(6)

where nk
is the number of points within the i-th disk of
i
the k-th object, N k is the total number of points on the k-
th object, K is the total number of test objects, and p is the
percentage of the disk area over the total object surface area.
Note that we use geodesic distance rather than Euclidean
distance to form the disks. Fig. 2 shows three different point
distributions with their corresponding NUC values. As we

2 https://github.com/yulequan/PU-Net

Figure 3. Comparison with the EAR method [15]. We color-code
all point clouds to show the deviation from the ground truth mesh.
For the EAR method, the radius of the Chair model is 0.1 and
0.182, while the radius of the Spider model is 0.106 and 0.155.

can see, the proposed NUC metric can effectively reveal the
point set uniformity: the lower the UNC value, the more
uniform the point set distribution is.

4.4. Comparisons with Other Methods

Comparison with an optimization-based method. We
compare our method with the Edge Aware Resampling
(EAR) method [15], which is a state-of-art method for point
cloud upsampling. The results are shown in Fig. 3, where
the Chair is from our collected testing dataset and the Spider
is from SHREC15. We color-code the point clouds to show
the deviation from the ground truth meshes. There are 1024
points in the input and we do a 4X upsampling. Since EAR
relies on the normal information, to be fair, we calculate the
normal according to the ground truth mesh. We show two
results of EAR with increasing radius, while setting other
parameters to their default values. As we can see, the radius
parameter has a great inﬂuence on EAR’s performance. For
relatively small radius, the output has low surface deviation
but the added points are not uniform, while more outliers
are introduced if the radius is large. In contrast, our method
can better balance the deviation and uniformity without the
need to carefully tune the parameters.

Comparison with deep learning-based methods. As far
as we know, we are not aware of any deep learning-based
method for point cloud upsampling, so we design some
baseline methods for comparison. Since PointNet [29] and
PointNet++ [30] are pioneers for 3D point cloud reason-
ing with deep learning techniques, we design the baselines
based on them. Speciﬁcally, we adopt the semantic segmen-
tation network architecture for point feature embedding and
use one set of convolutions for feature expansion. Note that
we consider two versions of PointNet++: basic PointNet++
and PointNet++ with multi-scale grouping (MSG) for han-
dling non-uniform sampling density; hence, we have three

Table 1. Quantitative comparison on our collected dataset.

NUC with different p

Deviation (10−2)

Time (s)

Method

Input
PointNet [29]
PointNet++ [30]
PointNet++(MSG) [30]
PU-Net (Ours)

0.2% 0.4% 0.6% 0.8% 1.0% 1.2% mean
0.164
0.316
0.270
0.409
0.150
0.215
0.143
0.208
0.115
0.174

0.224
0.334
0.178
0.169
0.138

0.185
0.295
0.160
0.152
0.122

0.150
0.252
0.143
0.137
0.112

0.142
0.239
0.139
0.134
0.110

-
2.27
1.01
0.78
0.63

Table 2. Quantitative comparison on SHREC15 dataset.

NUC with different p

Deviation (10−2)

Time (s)

Method

Input
PointNet [29]
PointNet++ [30]
PointNet++(MSG) [30]
PU-Net (Ours)

0.2% 0.4% 0.6% 0.8% 1.0% 1.2% mean
0.165
0.315
0.330
0.490
0.185
0.259
0.161
0.250
0.144
0.219

0.222
0.405
0.218
0.199
0.173

0.184
0.360
0.197
0.175
0.154

0.146
0.293
0.170
0.148
0.137

0.153
0.309
0.176
0.153
0.140

-
2.03
0.88
0.69
0.52

std
-
3.18
0.83
0.61
0.53

std
-
2.94
0.75
0.59
0.45

-
0.05
0.16
0.45
0.15

-
0.05
0.16
0.45
0.15

baselines in total, and we train them only with the recon-
struction loss. Please refer to the supplemental material for
details of the baseline network architectures. Although we
modify the PointNet, PointNet++, and PointNet++(MSG)
architectures for the upsampling problem, for convenience,
we still call the baselines by their original names.

Tables 1 and 2 list the quantitative comparison results
on our collected dataset and the SHREC15 dataset, respec-
tively. Note that measuring NUC with small p shows local
distribution uniformity in small regions, while measuring
NUC with large p shows more global uniformity. Among
the baselines, PointNet performs the worst, since it cannot
capture local structure information. Compared with Point-
Net++, PointNet++(MSG) can slightly improve the unifor-
mity due to the explicit multi-scale information grouping.
However, it involves more parameters, thus signiﬁcantly
prolonging the training and testing time. Overall, our PU-
Net achieves the best performance with the lowest deviation
from surface and the best distribution uniformity compared
to the baselines, especially on the local uniformity.

Fig. 4 shows results for visual comparison, where points
are colored by their distance deviations from surface. As
we can see, the point clouds predicted by our method better
match the underlying surface with lower deviations.

4.5. Architecture Design Analysis

Analyzing the feature expansion. We compare our pro-
posed feature expansion scheme with two interpolation-like
schemes. The ﬁrst one is similar to a naive point interpola-
tion (denoted as interp1). After extracting the point feature
of each point, we combine its own features and the features
from the nearest neighboring points to generate the upsam-
pled features. The second one introduces more randomness

Table 3. Architecture design analysis on our collect dataset.

NUC with different p

Deviation (10−2)

Methods

interp1+EMD 0.153
interp2+EMD 0.144
0.185
CD
0.140
EMD
0.138
Ours

0.4% 0.6% 0.8% 1.0% mean
0.67
0.126
0.71
0.122
0.87
0.154
0.68
0.119
0.63
0.115

0.121
0.118
0.147
0.116
0.112

0.136
0.129
0.167
0.126
0.122

std
0.54
0.57
0.69
0.58
0.53

(denoted as interp2).
Instead of using the features from
the nearest neighbors, we use a radius based ball query to
ﬁnd the neighborhood and combine the features from these
points to generate the upsampled features. We train these
two networks with the reconstruction loss (also named as
the EMD loss) and the results are listed in the top two rows
of Table 3. For fair comparison, we also train our network
only with the EMD loss. The results are shown in the fourth
row. Comparing with these two interpolation-like schemes,
our proposed scheme can generate more uniform outputs
with comparable surface distance deviation.

Comparing different loss functions. As mentioned above,
the EMD can better capture the object shape than CD. Com-
paring their performance in Table 3, we can see that the
EMD loss improves the output uniformity with low surface
distance deviation when comparing with the CD loss, mean-
ing that the EMD loss can better encourage the output points
to lie on the underlying surface. Furthermore, by comparing
the last two rows in Table 3, we can see that the repulsion
loss can further improve the uniformity of the output.

4.6. More Experiments

Surface reconstruction from upsampled point sets. An
important application of point cloud upsampling is to im-
prove the surface reconstruction quality. Hence, we com-

Figure 4. Visual comparison on samples from our collected dataset (top row) and SHREC (bottom row). The colors on points (see color
map) reveal the surface distance errors, where blue indicates low error and red indicates high error.

Figure 5. Surface reconstruction results from the upsampled point clouds.

pare the reconstruction results of different methods with the
direct Poisson surface reconstruction method [16] provided
in MeshLab [5]; see Fig. 5. We can observe that the re-
construction result from our method is the closest to the
ground truth, while other methods either miss certain struc-
tures (e.g., the leg of the Horse) or overﬁll the hole.

Results of iterative upsampling. To study the ability of
our network to handle varying number of input points, we
design an iterative upsampling experiment, which takes the
output of the previous iteration as the input of the next iter-
ation. Fig. 6 shows the results. The initial input point cloud
has 1024 points and we increase fourfold in each iteration.
From the results, we can see that our network can produce
reasonable results for different number of input points. Fur-
thermore, this iterative upsampling experiment also shows

Figure 6. Results of iterative upsampling. We color-code points by
the depth information. Blue points are closer to us.

Figure 7. Surface reconstruction results from noisy input points.

5. Conclusion

the anti-noise ability of our network to resist the accumu-
lated errors introduced in the iterative upsampling.

Results from noisy input point sets. Fig. 7 shows the sur-
face reconstruction results from noisy point clouds (Gaus-
sian noise of 0.5% and 1% of object bounding box diago-
nal), which demonstrate that our network facilitates the pro-
duction of better surfaces even with noisy inputs.

Results on real-scanned point clouds. Lastly, we eval-
uated the ability of our network to upsample real-scanned
point clouds, which were downloaded from Visionair [1]. In
Fig. 8, the left-most column presents the real-scanned point
clouds. Even though each real-scanned point cloud contains
millions of points, the phenomenon of inhomogeneity still
exists. For better visualization, we cut small patches from
the original point clouds and show the patches in the middle
column. We can observe that the real-scanned points tend to
have line structures, while our network still has the ability
to uniformly add points in the sparse regions.

In this paper, we present a deep network for point cloud
upsampling, with the goal of generating a denser and uni-
form set of points from a sparser set of points. Our network
is trained at a patch-level using a multi-level feature aggre-
gation manner, thus capturing both local and global infor-
mation. The design of our network bypasses the need for
a prescribed order among the points, by operating on in-
dividual features that contain non-local geometry to allow
a context-aware upsampling. Our experiments demonstrate
the effectiveness of our method. As the ﬁrst attempt using
deep networks, our method still has a number of limitations.
Firstly, it is not designed for completion, so our network can
not ﬁll large holes and missing parts. Besides, our network
may not be able to add meaningful points for tiny structures
that are severely undersampled.

In the future, we would like to investigate and develop
more means to handle irregular and sparse data, both for re-
gression purposes and for synthesis. One immediate step is
to develop a downsampling method. Although, downsam-
pling seems like a simpler problem, there is room to devise
proper losses and architecture that maximize the preserva-
tion of information in the decimated point set. We believe
that in general, the development of deep learning methods
for irregular structures is a viable research direction.

Acknowledgments. We thank anonymous reviewers for the
comments and suggestions. The work is supported in part
by the National Basic Program of China, the 973 Program
(Project No. 2015CB351706), the Research Grants Council
of the Hong Kong Special Administrative Region (Project
no. CUHK 14225616), the Shenzhen Science and Tech-
nology Program (No. JCYJ20170413162617606), and the
CUHK strategic recruitment fund.

Figure 8. Results on real-scanned point clouds (Screw nut & Tur-
tle). We color-code input patches and upsampling results to show
the depth information. Blue points are closer to viewpoint.

References

[1] Visionair. [Online; accessed on 14-November-2017]. 4, 8
[2] M. Alexa, J. Behr, D. Cohen-Or, S. Fleishman, D. Levin,
and C. T. Silva. Computing and rendering point set surfaces.
IEEE Trans. Vis. & Comp. Graphics, 9(1):3–15, 2003. 1
[3] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral
networks and locally connected networks on graphs. In Int.
Conf. on Learning Representations (ICLR), 2014. 2

[4] A. X. Chang, T. Funkhouser, L. J. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. ShapeNet: An information-rich 3D model repository.
arXiv preprint arXiv:1512.03012, 2015. 4

[5] P. Cignoni, M. Callieri, M. Corsini, M. Dellepiane, F. Ganov-
elli, and G. Ranzuglia. MeshLab: an open-source mesh pro-
cessing tool. In Eurographics Italian Chapter Conference,
2008. 7

[6] A. Dai, C. R. Qi, and M. Niessner. Shape completion using
3D-Encoder-Predictor CNNs and shape synthesis. In IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
2017. 2

[7] C. Dong, C. C. Loy, K. He, and X. Tang.

Image super-
resolution using deep convolutional networks. IEEE Trans.
Pattern Anal. & Mach. Intell., 38(2):295–307, 2016. 3
[8] H. Fan, H. Su, and L. J. Guibas. A point set generation
network for 3D object reconstruction from a single image.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2017. 2, 4

[9] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and
M. Aubry. AtlasNet: A papier-mˆach´e approach to learning
3D surface generation. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2018. to appear. 2
[10] P. Guerrero, Y. Kleiman, M. Ovsjanikov, and N. J. Mitra.
PCPNet: Learning local shape properties from raw point
clouds. Computer Graphics Forum (Eurographics), 2018.
to appear. 2

[11] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Hyper-
columns for object segmentation and ﬁne-grained localiza-
tion. In IEEE Conf. on Computer Vision and Pattern Recog-
nition (CVPR), 2015. 3

[12] Q. Hou, M. Cheng, X. Hu, A. Borji, Z. Tu, and P. Torr.
Deeply supervised salient object detection with short con-
In IEEE Conf. on Computer Vision and Pattern
nections.
Recognition (CVPR), 2017. 3

[13] B.-S. Hua, M.-K. Tran, and S.-K. Yeung. Pointwise convo-
lutional neural networks. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2018. to appear. 2
[14] H. Huang, D. Li, H. Zhang, U. Ascher, and D. Cohen-Or.
Consolidation of unorganized point clouds for surface re-
construction. ACM Trans. on Graphics (SIGGRAPH Asia),
28(5):176:1–8, 2009. 2, 4

[15] H. Huang, S. Wu, M. Gong, D. Cohen-Or, U. Ascher, and
H. R. Zhang. Edge-aware point set resampling. ACM Trans.
on Graphics, 32(9):1–12, 2013. 2, 5

[16] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface
In Eurographics Symposium on Geometry

reconstruction.
Processing (SGP), 2006. 7

[17] D. Kingma and J. Ba. Adam: A method for stochastic opti-
mization. In Int. Conf. on Learning Representations (ICLR),
2015. 5

[18] R. Klokov and V. Lempitsky. Escape from cells: deep Kd-
Networks for the recognition of 3D point cloud models. In
IEEE Int. Conf. on Computer Vision (ICCV), 2017. 1, 2

[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

ImageNet
classiﬁcation with deep convolutional neural networks.
In
Int. Conf. on Neural Information Processing Systems (NIPS),
2012. 3

[20] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a genera-
tive adversarial network. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2017. 1

[21] Y. Li, R. Bu, M. Sun, and B. Chen. PointCNN. arXiv preprint

arXiv:1801.07791, 2018. 2

[22] Z. Lian, J. Zhang, S. Choi, H. ElNaghy, J. El-Sana, T. Fu-
ruya, A. Giachetti, R. A. Guler, L. Lai, C. Li, H. Li,
F. A. Limberger, R. Martin, R. U. Nakanishi, A. P. Neto,
L. G. Nonato, R. Ohbuchi, K. Pevzner, D. Pickup, P. Rosin,
A. Sharf, L. Sun, X. Sun, S. Tari, G. Unal, and R. C. Wilson.
In Proc. of the 2015 Euro-
Non-rigid 3D shape retrieval.
graphics Workshop on 3D Object Retrieval, 2015. 4

[23] C.-H. Lin, C. Kong, and S. Lucey. Learning efﬁcient point
cloud generation for dense 3D object reconstruction. In AAAI
Conf. on Artiﬁcial Intell. (AAAI), 2017. 2

[24] Y. Lipman, D. Cohen-Or, D. Levin, and H. Tal-Ezer.
Parameterization-free projection for geometry reconstruc-
tion. ACM Trans. on Graphics (SIGGRAPH), 26(3):22:1–5,
2007. 1, 4

[25] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. In IEEE Conf. on Com-
puter Vision and Pattern Recognition (CVPR), 2015. 3
[26] J. Masci, D. Boscaini, M. Bronstein, and P. Vandergheynst.
Geodesic convolutional neural networks on Riemannian
manifolds. In IEEE Int. Conf. on Computer Vision (ICCV)
workshops, 2015. 2

[27] D. Maturana and S. Scherer. Voxnet: A 3D convolutional
neural network for real-time object recognition. In Int. Conf.
on Intell. Robots and Systems (IROS), 2015. 2

[28] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas. Frus-
tum PointNets for 3D object detection from RGB-D data.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2018. to appear. 2

[29] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3D classiﬁcation and segmentation.
In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2017. 1, 2, 5, 6

[30] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. PointNet++:
Deep hierarchical feature learning on point sets in a metric
space. In Int. Conf. on Neural Information Processing Sys-
tems (NIPS), 2017. 1, 2, 3, 5, 6

[31] G. Riegler, A. O. Ulusoys, and A. Geiger. OctNet: Learning
deep 3D representations at high resolutions. In IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), 2017.
2

[32] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolu-
tional networks for biomedical image segmentation. In Int.
Conf. on MICCAI, 2015. 3

[33] W. Shi, J. Caballero, F. Husz´ar, J. Totz, A. P. Aitken,
R. Bishop, D. Rueckert, and Z. Wang. Real-time single im-
age and video super-resolution using an efﬁcient sub-pixel
convolutional neural network. In IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2016. 1, 3

[34] W. Wang, R. Yu, Q. Huang, and U. Neumann. SGPN: Sim-
ilarity group proposal network for 3D point cloud instance
segmentation. In IEEE Conf. on Computer Vision and Pat-
tern Recognition (CVPR), 2018. to appear. 2

[35] S. Wu, H. Huang, M. Gong, M. Zwicker, and D. Cohen-
Or. Deep points consolidation. ACM Trans. on Graphics
(SIGGRAPH Asia), 34(6):176:1–13, 2015. 2

[36] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3D ShapeNets: A deep representation for volumet-
ric shapes. In IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2015. 2, 4

[37] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated
residual transformations for deep neural networks. arXiv
preprint arXiv:1611.05431, 2016. 3

[38] S. Xie and Z. Tu. Holistically-nested edge detection. In IEEE

Int. Conf. on Computer Vision (ICCV), 2015. 3

[39] X. Zhang, X. Zhou, M. Lin, and J. Sun. ShufﬂeNet: An
extremely efﬁcient convolutional neural network for mobile
devices. arXiv preprint arXiv:1707.01083, 2017. 3

A. Overview

In this supplementary material, we ﬁrst provide more details about our collected dataset in Section B. Then, we show the

details of our network architecture as well as the baseline networks employed in the experiments in Section C.

B. Details of our Collected Dataset

We collect 60 different 3D models to form our training and testing datasets. The speciﬁc name of each model is shown in
Table 4. We also present the shapes of some training and testing 3D models in our dataset in Fig. 9 and Fig. 10, respectively.
As we can see, our collected datasets have a large variation in geometry shapes, containing 3D models with smooth surface
regions (ﬁrst row) and 3D models with sharp corners and edges (second row). There is also a large variation between training
and testing 3D models, indicating a good generalization ability of our proposed method.

Table 4. The complete name list of the 3D models in our training and testing datasets.

Model Names

Training

Testing

Armadillo, Boy1, Boy2, Bumpy torus, Bunny, Cad, Cylinder, Child1,
Child2, Chinese lion, Cone, Cup, Dino, Egea, Ellipsoid, Eros, Fish, Fo-
cal octa, Gargoyle, Girl1, Girl2, Hand, Joint, Julius, Nicolo, Octa ﬂower,
Pierrot, Pulley, Pyramid1, Pyramid2, Retinal, Rolling stage, Screwdriver,
Sharp sphere, Special cube, Star1, Turbine, Twirl, Vaselion
Camel, Casting, Chair, Cover rear, Cow, Duck, Eight, Elephant, Elk, Fan-
disk, Genus, Horse, Icosahedron, Kitten, Moai, Octahedron, Pig, Quadric,
Sculpt, Star2

Armadillo

Bunny

Dino

Julius

Pierrot

Vaselion

Block

Cad

Focal octa

Joint

Pulley

Twirl

Figure 9. Examples 3D models in our training dataset. The ﬁrst row shows 3D models with smooth surface regions, while the
second row shows 3D models with sharp corners and edges.

Camel

Elephant

Elk

Horse

Kitten

Moai

Casting

Chair

Fandisk

Icosahedron

Quadric

Sculpt

Figure 10. Examples 3D models in our testing dataset. The ﬁrst row shows 3D models with smooth surface regions, while the
second row shows 3D models with sharp corners and edges.

C. Details of Network Architectures

The details of our network architecture are listed as follows.

• In the hierarchical feature learning component, we use four levels to extract local features. Following the notations
in PointNet++, we use (K, r, [l1, ..., ld]) to represent a level with K local regions of ball radius r, and [l1, ..., ld]
the d fully connected layers with width li (i = 1, ..., d). Therefore, the parameters we use are (N, 0.05, [32, 32, 64]),
(N/2, 0.1, [64, 64, 128]), (N/4, 0.2, [128, 128, 256]) and (N/8, 0.3, [256, 256, 512]).

• In the multi-level feature aggregation component, we use interpolation to restore the feature of each level and use a

convolution to reduce the restored feature to 64 dimensions. Therefore, ˜C = 259 in our experiments.

• In the feature expansion component, the output feature channel numbers ˜C1 and ˜C2 are 256 and 128, respectively.

• In the coordinate reconstruction component, we use two fully connected layers with 64 and 3 output channels, respec-

tively.

The details of the baseline architectures are illustrated in Fig. 11, Fig. 12 and Fig. 13.
All the convolution layers and fully connected layers in the above networks are followed by the ReLU operator, except for

the last coordinate regression layer.

Figure 11. The network architecture of PointNet for point cloud upsampling.

Figure 12. The network architecture of PointNet++ for point cloud upsampling.

Figure 13. The network architecture of PointNet++(MSG) for point cloud upsampling.

