7
1
0
2
 
b
e
F
 
3
1
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
4
9
9
3
0
.
2
0
7
1
:
v
i
X
r
a

Running head: METBOOST

1

metboost: Exploratory regression analysis with hierarchically clustered data

Patrick J. Miller1, Daniel B. McArtor1, Gitta H. Lubke1,2

1 University of Notre Dame, 2 VU University Amsterdam

Author Note

Patrick J. Miller expresses his appreciation to Daniel B. McArtor for his critical

insight that motivated the method.

Gitta H. Lubke is supported by NIDA R37 DA-018673. The computational work was

done on clusters acquired through NSF MRI BCS-1229450.

Correspondence concerning this abstract should be addressed to Patrick J. Miller, 110

Haggar Hall, University of Notre Dame, Notre Dame, IN 46656. Email: pmille13@nd.edu.

METBOOST

2

Abstract

As data collections become larger, exploratory regression analysis becomes more important

but more challenging. When observations are hierarchically clustered the problem is even

more challenging because model selection with mixed eﬀect models can produce misleading

results when nonlinear eﬀects are not included into the model (Bauer and Cai, 2009). A

machine learning method called boosted decision trees (Friedman, 2001) is a good

approach for exploratory regression analysis in real data sets because it can detect

predictors with nonlinear and interaction eﬀects while also accounting for missing data. We

propose an extension to boosted decision decision trees called metboost for hierarchically

clustered data. It works by constraining the structure of each tree to be the same across

groups, but allowing the terminal node means to diﬀer. This allows predictors and split

points to lead to diﬀerent predictions within each group, and approximates nonlinear group

speciﬁc eﬀects. Importantly, metboost remains computationally feasible for thousands of

observations and hundreds of predictors that may contain missing values. We apply the

method to predict math performance for 15,240 students from 751 schools in data collected

in the Educational Longitudinal Study 2002 (Ingels et al., 2007), allowing 76 predictors to

have unique eﬀects for each school. When comparing results to boosted decision trees,

metboost has 15% improved prediction performance. Results of a large simulation study

show that metboost has up to 70% improved variable selection performance and up to 30%

improved prediction performance compared to boosted decision trees when group sizes are

small.

Keywords: exploratory data analysis, boosting, decision trees, mixed eﬀect

METBOOST

3

metboost: Exploratory regression analysis with hierarchically clustered data

It can be challenging to make predictions and ﬁnd structure in hierarchically clustered

data with many predictors. Hierarchical clustering is frequently observed in psychology.

For example, individuals can be nested within schools, organizations, regions, or families.

Many studies initially consider a large number of variables. This could range from tens of

thousands to simply a large number that is inconvenient to include in a parametric model

simultaneously (e.g. 50 or 100). For instance, data might be collected using sensors or

computer logs, collected from virtual environments like websites or online textbooks, or

compiled from large publicly funded surveys. In these studies, research questions are often

broad, specifying only which variable is the outcome and which variables are plausible

predictors. Initially the goal is the data exploration: understanding how the predictors

relate to the outcome while making as few assumptions as possible (Tukey, 1969). Using a

part of the data for exploration can suggest further research questions, testable hypotheses,

and appropriate models to ﬁt to the data. It can also enhance our ability to make

predictions for student learning, depression risk, or employee well-being.

While the beneﬁts of exploratory regression analysis are well known, many intuitive

approaches require strong assumptions or do not scale to a large number of variables.

Common methods are plots (scatter plots, regression diagnostics), estimating pearson

correlations between the outcome and all predictors, and stepwise or best subsets

regression. While data and model visualization can highlight nonlinear eﬀects, these

approaches do not work well with a large number of predictors. Linear models (pearson

correlation, stepwise regression) can work with a large number of predictors but require

assuming that the relationship between the outcome and predictors is linear. Thus linear

models can fail to detect predictors with nonlinear and interaction eﬀects unless these

terms are included a priori. Furthermore, stepwise and best subsets regression are also not

guaranteed to ﬁnd the best possible model, even if all eﬀects are linear (Thompson, 1995).

Finally, none of these approaches explicitly account for hierarchical clustering.

METBOOST

4

Decision trees are a compelling alternative for exploratory regression analysis because

they allow an arbitrary large number of predictors, can approximate nonlinear eﬀects

without speciﬁcation, and handle missing data. A decision tree is a set of binary decision

rules built in a data driven way. For example, a decision tree predicting math score on a

standardized test might ﬁrst split on the variable GPA < 3.2. Within each of the two

resulting regions, (GPA < 3.2, GPA ≥ 3.2), the search through all predictors is carried out

again. Within one region, a split might be found on homework completion rate < 80%.

The predictions of the tree are the mean math performance in each region. In this region,

for example, the prediction would be the math performance of students with GPA < 3.2

and homework completion rate > 80%. The problem with a single decision tree is that the

structure of the trees can vary considerably from sample to sample (i.e. it has high

variance). So while interpretation of the tree as a set of decision rules is straightforward,

the high sample to sample variability of any individual tree does not instill conﬁdence in

any given interpretation. For example, ﬁtting a tree in a diﬀerent sample could result in

homework completion being the ﬁrst splitting variable, which potentially leads to diﬀerent

subsequent splits and ultimately a completely diﬀerent conceptual interpretation of the

tree.

Boosted decision trees, or boosting, is a method that reduces the variance of

individual decision trees by combining the predictions of many trees in an ensemble. In

boosting, each tree is ﬁt by increasing the weight of observations that are most poorly

predicted by the previous trees in the model. In our example, the ﬁrst tree with splits on

GPA and homework completion rate would not explain variability in math performance

due to additional factors like teacher expectations or math self eﬃcacy. Using boosting,

these predictors could be included in subsequent trees that are ﬁtted to the residuals of the

previous trees. The resulting ensemble has higher prediction accuracy and less variability

than individual trees, but is no longer easy to interpret as a set of decision rules. Instead,

boosting models are typically interpreted by ranking how often each predictor is used for

METBOOST

5

splitting and how eﬀective the corresponding splits were. In our example, GPA might rank

highest among all the variables considered. Nonlinear eﬀects can be detected by examining

plots of the eﬀects of individual predictor eﬀects implied by the model.

Generally, a boosted decision tree ensemble can be viewed as an approximation of a

highly complex or nonlinear function relating many predictors to an outcome. This

property is ideal for exploratory regression analysis because it allows exploration of the

regression relationship between an outcome and many predictors while making few

assumptions about the structure of this relationship. However, it is not clear how best to

incorporate hierarchically clustered data in a boosted decision tree ensemble.

In psychology, observations are often hierarchically clustered into groups. We refer to

the variables that induce clustering within observations simply as grouping variables:

school, family, organization, team, or therapist are some common examples. Throughout

this paper, we will use a data set in education as a concrete example of these hierarchical

structures, with students clustered within schools. When students are clustered within

schools, the eﬀect of school is commonly modeled as a random eﬀect in a mixed eﬀect

model (Harville, 1977; Henderson, 1975). A mixed eﬀects model deﬁnes a set of ﬁxed

eﬀects which are the same across all groups, and a set of random eﬀects that are unique to

each group (Laird & Ware, 1982). The regression intercept and slope for certain predictors

are often allowed to vary by group, which allows each group to have unique predictor

eﬀects. The levels of a grouping variable are assumed to be sampled randomly from a

population of all possible levels. For example, the observed schools in the sample are a

random sample of all schools in a state. Accounting for the random eﬀect of the grouping

variable corrects the standard errors of parameter estimates used in hypothesis tests, which

are overly liberal if the clustering is ignored (Henderson, 1975).

For prediction, the advantage of a mixed-eﬀect models is that they provide shrunken

(or empirical bayes) estimates of the random eﬀects (Skrondal & Rabe-Hesketh, 2009). For

a random intercept model, these estimates are closer to the within-group mean or grand

METBOOST

6

mean depending on the within-group variance and the sample size within each group

(Afshartous & de Leeuw, 2005; Gelman & Hill, 2006). In ﬁnite samples, shrunken or

empirical bayes estimates of the random eﬀects are more eﬃcient than simply using the

group means alone (Afshartous & de Leeuw, 2005).

A natural approach for exploratory regression analysis with large mixed eﬀect models

is to ﬁt a set of alternative models followed by model selection. However, model selection is

problematic because it is diﬃcult to both detect and disambiguate nonlinear predictor

eﬀects from random predictor eﬀects. Others have recommended allowing all predictors to

have group speciﬁc eﬀects (Barr, Levy, Scheepers, & Tily, 2013). However, such a strategy

is statistically and computationally problematic with ﬁnite samples. Critically, it has also

been shown that nonlinear eﬀects not included in the model can be incorrectly

approximated by a random slope (Bauer & Cai, 2009). Thus, exploratory regression

analysis with mixed eﬀect models by model selection can result in an incorrect ﬁnal model

with a misleading interpretation.

As an alternative to model selection, we propose a procedure speciﬁcally designed for

exploratory regression analysis with mixed eﬀect models. This is done by extending

boosted decision trees to take hierarchical clustering into account. We call the procedure

metboost (mixed eﬀects tree boosting) because it combines boosted decision trees with

mixed eﬀects models. Our approach is implemented in the R package mvtboost (Miller,

Lubke, McArtor, & Bergeman, 2016) and is freely available on CRAN. Building on boosted

decision trees capitalizes on their ﬂexibility for exploratory data analysis and their ability

to detect nonlinear eﬀects without speciﬁcation, while simultaneously handling missing

data in the predictors. The resulting model can still be interpreted by ranking predictors

by their contribution to the model and by plotting their model-implied eﬀects. Combining

boosted decision trees with mixed eﬀects models improves the eﬃciency of predictions

because the predictions are shrunken according to group size. For example, the predicted

values for a school with a small number of students are shrunken toward the average across

METBOOST

7

all schools. Furthermore, the interpretation of metboost is improved because the levels of

grouping variable are treated as a sample of all possible levels (e.g. a sample of all schools

in a state).

The essence of our approach is to constrain the split points of each tree to be the

same across groups, but to allow the terminal node means of the tree (the predictions) to

vary by group. In the education example, the tree split point GPA < 3.2 is the same across

schools, but the math performance in each region is speciﬁc to each school and depends on

the distribution of GPA within the school. This approach is implemented in metboost by

representing each tree in the ensemble as a fully observed design matrix of indicator

variables denoting terminal node membership. Each column in the new design matrix is

allowed to vary by group using the R package lme4 (Bates, Mächler, Bolker, & Walker,

2015). This novel representation allows metboost to eﬃciently explore hierarchically

clustered data with many predictors while handling missing data. However, incorporating

random eﬀects with boosted decision trees does incur computational costs, with an analysis

taking on the order of minutes for medium data sets (1K observations) or hours for large

data sets (>10K observations).

Many parametric, semi-parametric, and recursive partitioning approaches have been

proposed in the last 10 years for exploratory regression analysis with mixed eﬀect models.

Exploratory parametric methods penalize the random and/or ﬁxed parameters of linear

mixed models so that the number of parameters can be larger than the number of samples

(Fan & Li, 2012; Ni, Zhang, & Zhang, 2010; Rohart, San Cristobal, & Laurent, 2014;

Schelldorfer, Bühlmann, & Van Der Geer, 2011; Schelldorfer, Meier, & Bühlmann, 2014).

Similarly, regularization of linear mixed models with many parameters can also be achieved

by updating coeﬃcients of the linear mixed eﬀect model stepwise through likelihood based

boosting (Groll & Tutz, 2014; Tutz & Groll, 2010). The primary drawback to these

approaches for exploratory regression analysis is that all eﬀects are assumed to be linear.

To account for nonlinear eﬀects, several semi-parametric methods combine additive

METBOOST

8

models and mixed eﬀect models to capture nonlinear eﬀects with splines (Wood, 2006).

Two examples include the gamm function in the R package mgcv (Wood, 2006), which is

based on nlme (Pinheiro & Bates, 2006), and the gamm4 function (Wood & Scheipl, 2014)

based on the R package lme4. While these approaches can capture nonlinear eﬀects, these

models can be slow to estimate with a large number of predictors and samples.

Recently, recursive partitioning approaches have been used to capture nonlinear ﬁxed

eﬀects. These approaches are faster to estimate with a large number of predictors.

Examples include ﬁtting linear mixed models within each region of a decision tree

(Fokkema, Smits, Zeileis, Hothorn, & Kelderman, 2015), and modeling ﬁxed eﬀects with a

decision tree while accounting for random eﬀects with a linear mixed model in a separate

step (Hajjem, Bellavance, & Larocque, 2014; Sela & Simonoﬀ, 2012). The interpretability

of the resulting trees is helpful for exploratory regression analysis, but requires speciﬁcation

of the mixed eﬀects model a priori. The most recent approach uses splines to allow group

speciﬁc nonlinear eﬀects for a single covariate in boosted decision trees (Pande et al.,

2016). However, only a single covariate can have group speciﬁc eﬀects.

Compared to existing parametric, semi-parametric and recursive partitioning

procedures, metboost is a more general solution for exploratory regression analysis with

many predictors in the presence of hierarchical clustering. Our approach is useful because

it has all of the following features:

• Can be used for selection of predictors with random eﬀects without a priori model

speciﬁcation or model selection.

• Allows group-speciﬁc (non)linear eﬀects to be detected and visualized without

including additional terms in the model a priori.

• Provides a built-in imputation procedure without a separate imputation step.

• Is computationally feasible with several hundred predictors and thousands of

observations.

METBOOST

9

In some cases, all four goals may not be necessary for exploratory regression analysis (e.g.

if the number of predictors is small, or the model is already known). In this case, one or

more procedures described above will be more appropriate and interpretable for

exploratory regression analysis than metboost.

The only other method that has all four features are tree ensembles (such as boosted

decision trees) with the grouping variable included as a candidate for splitting. This

eﬀectively treats the grouping variable as ﬁxed rather than random. Treating the grouping

variable as ﬁxed is appropriate if all possible levels of the variable are observed in the

sample, group sizes are large, and the research questions address the speciﬁc levels

observed in the sample. In education, this might be true if the number of schools of

interest is ﬁnite and all appear in the sample. However, treating the grouping variable as

ﬁxed is not appropriate when the research goal is to make predictions for observations at

unobserved levels of the grouping variable, or to identify important predictors while

generalizing to the population of possible levels of the grouping variable. In this case, the

grouping variable should be treated as random, e.g. if the schools in the sample are drawn

from a population of schools in a state. Importantly, treating the grouping variable as ﬁxed

does not make the most statistically eﬃcient predictions when group sizes are small. The

critical diﬀerence between metboost and boosted decision trees is that the eﬀect of the

grouping variable is incorporated in every tree in metboost with predictions shrunken

proportional to group size, but chosen in a data driven way and unshrunk in boosting.

While metboost is good for exploratory regression analysis, there are two primary

limitations. The ﬁrst is that while metboost is more general than many of the existing

approaches, it is less interpretable than (semi)-parametric models. However, when the

research questions are exploratory rather than conﬁrmatory, this tradeoﬀ is useful because

it informs the appropriate model to ﬁt to the data in a second step. The second limitation

to metboost (and many other machine learning methods) is that using the method

eﬀectively for variable selection and prediction requires ﬁnding optimal values for

METBOOST

10

meta-parameters: the number of trees, the step size, and tree-depth. Tuning all three by

cross-validation is critical for achieving optimal prediction accuracy and variable selection

performance, but adds additional computation time and complexity. In our

implementation, we made it easy to tune these models and improved computation time

using parallelization.

In the rest of the paper, we describe the metboost model and how it is estimated,

tuned, and interpreted. To illustrate the advantages of treating the grouping variable as

random, we compare the performance of metboost to boosted decision trees in a Monte

Carlo simulation. Finally, we describe an empirical example where the math ability of

15,240 students from 751 high schools is predicted from 76 variables using data from the

Educational Longitudinal Study 2002 (Ingels et al., 2007). We begin with a brief overview

of decision trees, boosting, and mixed eﬀect models.

Decision Trees and Boosting

Decision Trees

represented as:

A general way to view a decision or classiﬁcation tree is as a piecewise function

approximation of an unknown function of the predictors. Notationally, they can be

yi = f (Xi) + (cid:15)i

f (Xi) = T (Xi, ν, γ) =

IXi∈Rj γj ,

J
X

j=1

where the outcome variable yi for observation i is some function f of the vector of p

predictors Xi. A decision tree is most easily visualized and interpreted as a tree diagram

(Figure 1), but it can also be represented by the function T (Xi, ν, γ). The J terminal

nodes of the decision tree are represented by regions Rj. The vector ν contains the split

points and predictors that deﬁne regions Rj, and γ is a vector of length J containing the

METBOOST

11

means of the j = 1, . . . , J terminal nodes. We use the indicator function IXi∈Rj to denote

that observation i is within a given region, that is, IXi∈Rj = 1 if subject i is in terminal

node Rj. Continuing the education example for a tree with a single split point and J = 4

terminal nodes, the predictor xGP A might have split point ν1 = 3.2, and predictor xHW has

split point ν2 = ν3 = .8.1 Four regions (or terminal nodes) R1, . . . , R4 are deﬁned by the

split points {xGP A < 3.2, xHW < .8}, {xGP A < 3.2, xHW ≥ .8}, {xGP A ≥ 3.2, xHW < .8},

and {xGP A ≥ 3.2, xHW ≥ .8}. The observations falling into each node are then

IxGP A<3.2,xHW <.8 and so forth. The predictions of the tree are simply the mean math

performance in each node. For the ﬁrst node, ˆγ1 = 1
n1

P

i∈xGP A<3.2,xHW <.8 yi, where n1 is the

number of observations in the ﬁrst node.

In general, there is no maximum likelihood or deterministic procedure for estimating

the optimal partitions of predictors into regions (Hyaﬁl & Rivest, 1976). Instead, a

data-driven procedure called recursive partitioning is used. In the ﬁrst step, the predictor

with the largest main eﬀect is selected (e.g. GPA), and the sample is split into two more

homogenous regions based on a split point on that predictor (e.g. GPA < 3.2). The

procedure continues (recursively) by selecting a new splitting variable and split point for

each terminal node (e.g. homework completion rate < 80%), and partitioning within each

of the nodes. This recursive partitioning continues until a stopping criterion is reached.

Common stopping criteria are (a) reaching a minimum number of observations within a

node; and/or (b) carrying out a maximum number of splits. After a tree is fully grown, it

can also be pruned to avoid overﬁtting (Hastie, Tibshirani, & Friedman, 2013). Decision

trees are readily interpretable in terms of tree-diagrams, where the predictors and split

points of the predictors are shown as branches (Figure 1). For further details see e.g.

Breiman, Friedman, Stone, and Olshen (1984), or Strobl, Malley, and Tutz (2009).

Research in decision trees has further improved their performance, interpretability,

and applicability to psychological data. Recursive partitioning can be improved by using

1These split points are chosen for simplicity. It is rare to ﬁnd that subsequent splits in each region are

on the same variable and have the same cutpoint, especially when the number of predictors is large.

METBOOST

12

statistical tests as criteria for splits and controlling Type-I error (Hothorn, Hornik, &

Zeileis, 2006; Strobl, Boulesteix, Zeileis, & Hothorn, 2007). A very popular extension to

decision trees is to estimate parametric models within each node. In general, this is called

model-based recursive partitioning (Zeileis, Hothorn, & Hornik, 2008). Predictors and split

points are chosen to select models with maximally diﬀerent parameter estimates within

each node. Examples include generalized linear models (Zeileis et al., 2008), item-response

models (De Boeck & Partchev, 2012), structural equation models (Brandmaier, Prindle,

McArdle, & Lindenberger, 2016; Brandmaier, von Oertzen, McArdle, & Lindenberger,

2013), and generalized mixed-eﬀect models (Fokkema et al., 2015). Applications of

model-based recursive partitioning in psychology include data-driven detection of

diﬀerential item functioning (De Boeck & Partchev, 2012), measurement diﬀerences over

time or across groups (Brandmaier et al., 2013), and estimating diﬀerent treatment eﬀects

(Fokkema et al., 2015). In general, these approaches are appropriate when the general

structure of the interrelations between variables is known and can be expressed in a model,

and partitioning only serves to ﬁnd group speciﬁc parameter estimates. If such a model is

not known a priori, or the distributional assumptions of the model do not hold within each

node, using boosted decision trees provides a useful and elegant alternative.

Boosted Decision Trees

In general, boosting is a method for estimating complex models with many

parameters stagewise by gradient descent (Bühlmann & Hothorn, 2007; Bühlmann & Yu,

2003; Elith, Leathwick, & Hastie, 2008; Friedman, 2001, 2002; Friedman, Hastie, &

Tibshirani, 2000; Hofner, Mayr, Robinzonov, & Schmid, 2014). Stagewise estimation

proceeds by updating the model one parameter at a time without changing the previous

estimates. To estimate a model stagewise by gradient descent, parameters are chosen that

minimize the ﬁrst derivative of the loss function (the gradient) at each step. In boosted

decision trees, gradient descent is used to estimate an additive model (or ensemble) of

13

(1)

(2)

METBOOST

decision trees (Friedman, 2001, 2002; Friedman et al., 2000). With a continuous outcome

and squared error loss L(y, ˆy) = 1
n

Pn

i=1(y − ˆy)2, the gradient is the vector of residuals.

Thus individual trees are simply ﬁt to the residuals of the previous trees. The additive

model of trees an be represented as:

yi = f (Xi) + (cid:15)i

f (Xi) =

Tm(Xi, νm, γm),

M
X

m=1

where the complex, nonlinear function f (Xi) is approximated by M trees with unique

regions and predictions.

Estimation of an additive model of decision trees by gradient descent is necessary

because there is no maximum likelihood or deterministic procedure for estimating model

parameters (Friedman, 2001). Further, an optimal step size is unknown because derivatives

cannot be taken with respect to model parameters, so that both the number of trees and

the step size become meta-parameters that need to be chosen. It has been shown that

stagewise gradient descent can be improved by ﬁtting trees to a subsample of the

observations at each iteration (Friedman, 2002). This diminishes overﬁtting caused by

outlying observations while speeding up computation time. An algorithm for estimating an

additive model of trees by stochastic gradient descent is shown as Algorithm 1.

Algorithm 1: Boosted Decision Trees with Squared error Loss (Friedman, 2001)

ri,0 = yi − ¯y
for m = 1, . . . , M steps (trees) do

Let s(i) be a subsample of i ∈ {1, . . . , N }

(cid:18)

( ˆν, ˆγ)m = minν,γ
ri,m = ri,m−1 − ˆTm(Xi, ˆνm, ˆγm)λ

s(i)

P

rs(i),m−1 − T (Xs(i), ν, γ)

(cid:19)2

end for

Like a single decision tree, the ensemble or additive model of M decision trees is used

to approximate an unknown function of the predictors. The ensemble of trees results in

METBOOST

14

improved prediction performance compared to single trees while sacriﬁcing the ability to

interpret trees as a set of decision rules. For more discussion, see Miller et al. (2016). For

computational and theoretical details, see (Friedman, 2001, 2002).

Several R, Python, and C++ implementations exist for ﬁtting models of boosted

decision trees (Chen & Guestrin, 2016; Hickey et al., 2016; Hothorn, Buehlmann, Kneib,

Schmid, & Hofner, 2016; Pedregosa et al., 2011). In R, one of the most popular is the

package gbm (Hickey et al., 2016) which allows users to ﬁt boosted decision tree ensembles

based on diﬀerent loss functions for continuous, multi-nomial, and survival outcomes. The

R package mvtboost (Miller et al., 2016) extends gbm to continuous multivariate outcomes.

In this paper, we again build on the gbm package which contains computationally eﬃcient

procedures for ﬁtting individual decision trees.

Tuning meta-parameters to avoid under- and overﬁtting. A critical aspect

of model ﬁtting is selecting meta-parameters (number of trees, step size, and tree-depth) to

avoid under- or overﬁtting the data. The model can easily overﬁt with a large number of

trees, or can underﬁt if a tiny step size is speciﬁed and the maximum number of trees is too

small. A minimally suﬃcient strategy is to choose a maximum number of trees and

tree-depth corresponding to the available computation time, and then choose both the step

size and the number of trees that minimizes prediction error.

It is important to tune the maximum depth of the trees or the minimum number of

observations in each node because it governs the complexity of the interactions that can be

approximated by each tree. For example, a tree-depth of one (a single split) will only

capture main eﬀects, while a tree-depth of two can capture two-way interactions, etc.

However, a tree-depth of two is not guaranteed to capture any particular two-way

interactions because split points are chosen by selecting predictors with the largest main

eﬀects ﬁrst (within each partition). Two-way and higher order interactions are typically

approximated by multiple trees. In practice, choosing a tree-depth of ﬁve or ten is common.

However, it is critical to tune tree-depth along with step-size and the number of trees

METBOOST

15

jointly to maximize predictive accuracy and variable selection performance of the model.

The meta-parameters (number of trees, step size, and tree-depth) can be chosen by

minimizing test-error or cross-validation error. The test-error is the error of model

predictions on the subset of data not used for training the model. It is an accurate estimate

of the prediction error, but is not eﬃcient with small samples. Most commonly, the

prediction error is estimated via K-fold cross validation. The K-fold cross-validation error

is computed by splitting the observations into k = 1, . . . , K sets or folds. The model is then

trained K times on the observations that are not in fold k, and the prediction error is

computed for the observations within fold k. An estimate of the prediction error is obtained

by averaging over all K folds. Cross-validation is usually the best approach in practice

because it uses all available data while protecting against overﬁtting and estimating the K

models can easily be carried out in parallel. Importantly, the number of trees should never

be chosen to minimize training error (the prediction error for the subset of data used for

training the model) because training error will always decrease with each additional tree.

As a practical note, the default values of the number of trees, step-size, and

tree-depth provided in many implementations are primarily useful only for checking that

the data format is suitable for analysis. Additionally, the commonly recommended strategy

of choosing the number of trees given one step size and one tree-depth (e.g. 5 or 10) is

rarely optimal. For best results, we recommend tuning the number of trees over many

combinations of tree-depth, step size, and even the minimum number of observations in a

node by grid search. We provide a function gbm_grid for extensive tuning by grid search in

the mvtboost package.

Interpretation. The ﬁnal model is often interpreted by ranking the relative

inﬂuence of predictors and visualizing their eﬀects using partial dependence plots.

Predictors can be ranked using relative inﬂuence, which is the contribution of each

predictor to the model relative to the other predictors. For continuous outcomes, the

relative inﬂuence is deﬁned as the reductions in the sum of squared error of the outcome

METBOOST

16

(SSE) attributable to splits on predictors over all M trees. The inﬂuence score is usually

expressed as a percent of the total reductions in SSE due to splits on all predictors. It has

been shown that predictor selection by relative inﬂuence provides good variable selection

performance that balances true and false positive rates when the predictors are on the

same scale (Miller et al., 2016). However, the naive relative inﬂuence is known to favor

predictors with many categories (Strobl et al., 2007). A conditional importance framework

is promising alternative, which uses p-values testing the association of an outcome with a

proposed split as the split criterion (Hothorn et al., 2006; Strobl, Boulesteix, Kneib,

Augustin, & Zeileis, 2008). This approach has not yet been implemented or evaluated for

boosted decision trees.

Another way that the additive model of trees can be interpreted is through the use of

partial dependence plots. A partial dependence plot illustrates the model-implied eﬀect of

an individual predictor, averaging over the eﬀects of other predictors. See Friedman and

Meulman (2003) for further details and Goldstein, Kapelner, Bleich, and Pitkin (2015) for

independent conditional expectation plots. Though ensembles of decision trees are models

of interactions (each split is conditional or depends on the previous split), it is diﬃcult to

visualize sets of many interacting predictors. Current methods for detecting sets of

interacting variables from the model are limited to two-way interactions and are only

heuristics (Elith et al., 2008; Miller et al., 2016). The performance of these heuristics for

detecting two-way interactions is unknown.

Linear mixed eﬀect model

Next, we brieﬂy introduce the linear mixed eﬀect model. Let there be i = 1, . . . , g

groups with j = 1, . . . , ni observations in each group. Let yi be a vector of length ni,

containing observations from group i, and let N = P

i ni be the total number of samples. A

general mixed eﬀect model for yi is given by (Bates et al., 2015; Pinheiro & Bates, 2006):

METBOOST

17

(3)

yi = Xiβ + Zibi + (cid:15)i

bi

iid∼ M N (0, Ψ)

(cid:15)i

iid∼ M N (0, σ2Ini)

Where yi is an ni × 1 vector of responses for group i, Xi is a ni × p matrix of predictors,

and β is a p × 1 vector of ﬁxed eﬀects for p covariates. Zi is the ni × q design matrix for

the random eﬀects, with the q × 1 vector of weights bi for the random eﬀects for each group

i = 1, . . . , g. The (cid:15)i is the ni × 1 vector of errors. The q × q matrix Ψ is the variance

covariance matrix of the random eﬀects. In matrix form, the model can be expressed as

y = Xβ + Zb + (cid:15),

(4)

where y is an N × 1 vector of responses from the entire sample, X is an N × p matrix of

predictors (including intercept), and β is a p × 1 vector of ﬁxed eﬀects. The matrix Z is an

N × (gq) matrix of random eﬀects with (gq) × 1 vector of weights b.

Estimation

When the variance components Ψ are known, the model (3) can be estimated as the

solution to Henderson’s mixed model equations (Henderson, 1975).







X 0X X 0Z

Z 0X Z 0Z + Ψ



















β

b







X 0y

Z 0y







=

(5)

It can be shown that these estimates are the best linear unbiased predictions of the random

eﬀects (Henderson, 1975; Searle, 1997). When the variance components must also be

estimated, the model is usually estimated by restricted maximum likelihood. See Bates et

al. (2015) for estimation details speciﬁc to lme4.

METBOOST

18

The estimated random eﬀects from the mixed eﬀect model are shrunken toward 0

when group sizes are small or when the within group variance is large. To illustrate,

consider the simple random intercept model yi = µ + αi + ei where yi ∼ N (αi, σ2) and

αi ∼ N (µ, σ2

α). The variance component σ2

α is the between group variance, and σ2 is the

within group variance. Then the mixed eﬀect model estimates ˆα∗

i of the group-speciﬁc

parameters αi are ¯yi − µ, which are shrunk toward the grand mean µ by a factor ω (Searle,

1997):

ˆα∗

i =

σ2
α
α + σ2
σ2
ni

(¯yi − µ) = ω(¯yi − µ)

(6)

As ω → 1, the between group variance σ2

α is much larger than the within group variance

σ2, and the mixed eﬀect model estimates ˆα∗

i approach the deviations of the group means

from the grand mean. As ω → 0, the within group variance σ2 is much larger than the

between group group variance σ2

α, and the mixed eﬀect model estimates ˆα∗

i → 0. In ﬁnite

samples with unknown variances, the additional ni term is incorporated to account for the

precision of the estimate of the within group variance σ2. Small group sizes result in lower

precision, which in turn results in lower ω and more shrinkage of ˆα∗

i toward 0.

metboost: mixed eﬀect tree boosting

Next, we describe a procedure for extending boosted decision trees to allow predictors

to have random eﬀects. This procedure is called metboost, and combines decision trees

with mixed eﬀect models by constraining the split points to be the same across groups (e.g.

GPA < 3.2 across all schools) but allowing the terminal node means of the tree to vary by

group (GPA < 3.2 has diﬀerent implications for math performance for diﬀerent schools).

Nonlinear eﬀects are approximated by the terminal node means of each tree. Adding a

grouping variable allows group speciﬁc terminal node means to approximate group speciﬁc

nonlinear eﬀects.

In addition to approximating group speciﬁc nonlinear eﬀects, treating grouping

variables as random rather than ﬁxed addresses common research questions in psychology.

METBOOST

19

It also increases the eﬃciency of the predictions from each tree when group sizes are small.

Additionally, metboost is helpful when the number of predictors is large and when it is

unclear which predictor eﬀects should be allowed to vary by group. Simply allowing all

predictors to vary by group is computationally intensive, and it is also challenging to

estimate such a large number of variance components unless the group sizes are very large.

To address this problem, metboost iteratively selects predictors with random eﬀects by

gradient descent.

The procedure works by ﬁtting an additive model of trees, where the split points of

each tree are the same for each group, but the terminal node means diﬀer. This is

accomplished at each iteration in the following way. First a decision tree is ﬁt to the data

ignoring the grouping and variable, and this tree is then represented as a design matrix.

Each column of the design matrix is an indicator variable denoting terminal node

membership. The eﬀects of the indicator variables are then allowed to vary by group in a

mixed eﬀect model. Finally, the ﬁtted values from this mixed eﬀects decision tree are used

to update the predictions at each iteration.

To illustrate this approach, consider again the example of predicting math

performance on a standardized test from 8 high school seniors from i = 1, 2 schools. A

single tree is ﬁt to this data and has 4 terminal nodes (Figure 1). For example, the ﬁrst

split is GPA < 3.2. Within each node, a second split is chosen for homework completion

rate < 80%. These splits can be represented as a design matrix ˜X with k = 4 columns and

8 rows (Figure 1). The full tree can be represented by ˜Xβ, where β contains the mean

math performance for each node (Figure 1). The tree captures nonlinear eﬀects of

predictors that are the same across schools by allowing the patterns of means in the

terminal nodes to take any functional form. To allow the eﬀect of GPA and homework

completion rate to diﬀer by school, the indicators (columns) in this design matrix are

allowed to vary by group. In terms of the formula syntax from lme4, such a model is

represented by:

METBOOST

1 MATH ~ X1 + X2 + X3 + X4 + ( X1 + X2 + X3 + X4 | SCHOOL )

In this model, the random eﬀects are the group speciﬁc terminal node means (see e.g.

Figure 2). In terms of the linear mixed eﬀect model matrices (Bates et al., 2015), the

random eﬀects design matrix ˜Zi for schools one and two in Figure 2 would be as in (7)















1 0 0 0

0 0 1 0

1 0 0 0

0 0 1 0

˜Z1 =

˜Z2 =

0 1 0 0

0 0 0 1

0 1 0 0

0 0 0 1











































,

20

(7)

where the random eﬀects bi were assigned as b1 = [−.3, −.5, 0, 0]0, b2 = [0, 0, .3, .5]. These

random eﬀects allow the splits GPA < 3.2 and homework completion rate < 80% to have

diﬀerent implications for math performance in each school. The limitation is that only a

small number of predictor eﬀects are accounted for in any single tree. In this example, only

the eﬀects of GPA and homework completion are accounted for. Thus to allow for the

model to account for nonlinear group speciﬁc eﬀects of many predictors, many of these

trees can be ﬁt by gradient descent (i.e. boosting).

The desirable properties of metboost are highlighted in this example. First, group

speciﬁc nonlinear eﬀects are detected with each tree. Group speciﬁc nonlinear eﬀects for all

predictors are accounted for in the ensemble of trees. Second, the design matrix ˜X is

guaranteed to be fully observed because it is a matrix of indicators assigning observations

to terminal nodes. Observations with missing values on a splitting variable are assigned to

nodes based on a split on a surrogate. Finally, the model is faster to estimate than a mixed

eﬀect model containing random eﬀects for p predictors because the number of random

eﬀects is constrained to be the number of terminal nodes k. In our example, the original

sample could have had as many as 50 or 100 predictors, but the design matrix for any

given tree is restricted to 4 columns, the number of terminal nodes. This constraint results

METBOOST

in a signiﬁcant improvement in computation time.

More generally, the model can be understood as follows:

yi = f (Xi) + gi(Xi) + (cid:15)i

f (Xi) =

Tm(Xi, νm, γm)

= ˜Xi,mβm

M
X

m=1

M
X

m=1
M
X

m=1

=

˜Zi,mbi,m,

gi(Xi) =

Tm(Xi, νm, γm,i)

21

(8)

(9)

(10)

where f is a function of predictors constrained to be the same across groups, while gi are

unknown functions of the predictors unique to each group. The functions gi are

approximated by allowing the means of the terminal nodes in tree m to vary by group, γm,i

(9).

Allowing the means of the terminal nodes to vary by group is accomplished by

representing tree m as a special mixed eﬀect model with design matrices ˜Xi,m, ˜Zi,m (10). A

tree can be viewed as a categorical variable mapping individuals to terminal nodes. This

categorical variable is equivalently represented by a design matrix of indicator variables

(Figure 1). We denote this mapping of the partitions in a tree to a design matrix of

indicators by the operator D

. The resulting design matrix is denoted ˜Xi. For
a single decision tree with k terminal regions, ˜Xi has k columns of indicator variables, and

(cid:16)

(cid:17)
T (Xi, ν, γ)

ni rows (Figure 1).

The matrix ˜Zi allows the terminal node means of the tree to vary by group. It is
constructed by multiplying each column in ˜Xi by vectors of group indicators (Bates et al.,

2015). If Ji is the ni × k matrix of indicator variables for group membership, the design
matrix for the random eﬀects ˜Zi is the ni × k matrix obtained by element wise
multiplication of each column in ˜Xi by each column in Ji. Thus, ˜Xi,m is ni × k design

METBOOST

22

matrix implied by the decision tree m, and ˜Zi,m is the ni × k design matrix allowing the

means of the nodes γm to vary by group. The weights βm are a k × 1 vector of means for

each terminal node implied by the tree m, and the bi,m is a q × 1 vector of group speciﬁc

deviations from the terminal node mean. A summary of the matrices in this model appears

in Table 1.

For example, the model matrices in (8) from the data in Figures 1 and 2 is given by:

˜Xβ + ˜Zb =

(cid:21)

(cid:20)
β

+


˜Z1




0



















b1

b2

0

˜Z2








˜X1



˜X2






























1 0 0 0

1 0 0 0

0 1 0 0

0 1 0 0

0 0 1 0

0 0 1 0

0 0 0 1

0 0 0 1































=





























2.7

4.6

7.2

9.8

+































1 0 0 0

1 0 0 0

0 1 0 0

0 1 0 0



























































































−.5

−3

0

0

0

0

.3

.5

0 0 1 0

0 0 1 0

0 0 0 1

0 0 0 1

(11)

(12)

The columns in ˜X are comprised of k = 4 columns of indicators assigning individuals to

terminal nodes in a tree using the ﬁrst node as a reference. The terminal node means of

the tree are β = γ. These represent, for instance, the average math performance on a test

for all students with GPA < 2.3 and homework completion rate < 80%. The matrix ˜Z is

partitioned by group with blocks ˜Zi. The matrix ˜Z is given by a multiplication of each

column in ˜X by the group indicators j0

1 = [1, 1, 1, 1, 0, 0, 0, 0], j0

2 = [0, 0, 0, 0, 1, 1, 1, 1]. The

vector of random eﬀects parameters b allows terminal node means parameterized by β to

vary by group membership. In our example, this allows the mean math performance of

students with GPA > 2.3 to vary by school.

METBOOST

23

Estimation by gradient descent. This model (8) can be estimated iteratively by

gradient descent. In step m a single tree Tm(Xi, νm, γm) is ﬁt, and its design matrix

computed. Next, the means within nodes γ, are allowed to vary by group by ﬁtting a

mixed eﬀect model with tree m as the design matrix to the residuals at step m as follows:

ri,m = ˜Xi,mβm + ˜Zi,mbi,m + (cid:15)i,m

(13)

Where ri,m is the ni × 1 vector of residuals at iteration m, ˜Xi,m = D

(cid:16)

(cid:17)
Tm(Xi, νm, γm)

is

the ni × k tree design matrix, βm is the k × 1 vector of node means, and bi,m is the k × 1

vector of deviations within each group from the terminal node mean. The estimates of the

parameters βm and bi,m can be obtained using general purpose mixed eﬀect modeling

software packages, such as the R package lme4 (Bates et al., 2015). The estimates for

f (Xi) and gi(Zi) at iteration m are given by:

ˆfm(Xi) = Tm(Xi, ˆνm, ˆγm)

ˆgi,m(Zi) = Tm( ˜Xi,m, ˆνm, ˆγi,m)

= ˜Xi

ˆβm

= ˜Zi,m

ˆbi,m

(14)

(15)

(16)

Estimating both ˆf , ˆgi using squared error loss by gradient descent proceeds iteratively by

ﬁtting a new tree to the residuals of the previous ﬁt as follows:

ri,m = ri,m−1 − λ

(cid:18)
ˆfm(Xi) + ˆgi,m(Zi)

(cid:19)

Where ri,m is the ni × 1 vector of residuals at iteration m, and λ is a ﬁxed step size at

some small value, e.g. .01. In the ﬁrst iteration, ri,0 = yi − ¯y. Like with univariate

boosting with squared error loss (Algorithm 1), subsampling from each group at each

METBOOST

24

iteration can improve performance. The resulting stochastic gradient descent algorithm is

summarized in Algorithm 2.

Algorithm 2: Stochastic gradient descent for boosted decision trees with hierarchically clus-
tered data

r0 = y − ¯y
for m = 1, . . . , M steps (trees) do

Let s(N ) = sub_sample(1, . . . , N ) be a subsample of all observations

T (X, ˆνm, ˆγm)

(cid:16)

ˆνm, ˆγm = minν,γ
˜Xm = D
ˆβm, ˆbm = minβ,b
ˆfm(X) = ˜X ˆβ,
rm = rm−1 − λ

(cid:18)

(cid:18)

P

s(N )

(cid:19)2

rm−1 − T (X, ν, γ)
(cid:17)

(cid:18)

P

(cid:19)
rm, ˜Xmβ + ˜Zmb
s(N ) L
ˆbm
ˆgm(Z) = ˜Zm
(cid:19)
ˆfm(Xi) + ˆgm(Z)

. (Algorithm 1)

. Mapping tree to design matrix

. (13)

. (14, 15)

. (16)

end for

Interpretation

The two methods of interpreting the resulting model are through ranking predictors

according to their inﬂuence on the model and by plotting the model implied eﬀects of

individual predictors. For a continuous outcome variable with squared error loss, the

inﬂuence of predictor j = 1, . . . , p is the sum of the reductions in SSE due to splits on this

predictor, summed over all trees in the model (Friedman, 2001). The inﬂuence scores for

all p predictors are often scaled to sum to 100, reﬂecting the percent in total reductions in

SSE due to each predictor. Deﬁning the inﬂuence in this way captures the ﬁxed eﬀects of

predictors, because split points in each tree are the same in each group. To visualize

potentially nonlinear eﬀects of predictors, the model predictions ˆy can be plotted against

one or two predictors of choice at a time. These plots show the the marginal eﬀect of each

predictor on the model predictions. If the grouping variable is of direct interest, separate

plots can be created showing how the marginal eﬀect of a predictor varies by group.

METBOOST

Implementation

25

An implementation of metboost is provided in the R package mvtboost (Miller et al.,

2016).2 This implementation provides easy meta-parameter tuning by cross validation in

parallel, and is built on the newest version of the R package gbm (Hickey et al., 2016).

Fitting the model with cross-validated meta-parameter tuning in parallel can be done as

follows:

metboost ( y = y , X = X , id = " id " ,

n . trees = 2500 ,

interaction . depth = c (3 , 5 , 8) ,

shrinkage = c (.005 , .01 , .025) ,

bag . fraction = .5 ,

cv . folds = 3 ,

subset = 1:500 ,

mc . cores = 12)

1

2

3

4

5

6

7

8

In this example y is a vector of outcomes with no missing data, X is a matrix of

predictors optionally containing missing values, and id is name of the grouping variable in

X. Following the speciﬁcation of the data, the other key meta-parameters are listed,

including the number of trees, the tree-depth, and shrinkage values. These

meta-parameters are the same as in gbm and similar to other boosting algorithms. Setting

cv.folds > 1 allows cross validation over the grid implied by the vectors of values given to

n.trees, interaction.depth, and shrinkage (step-size). Setting mc.cores carries out

the cross-validation in parallel for a given number of cores. The quantities ˆy, X ˆβ, and Z ˆu

for the best number of trees and best set of meta-parameters with the lowest

cross-validation error are returned.

In addition, to the metboost function, an influence function is provided for

2The metboost method from mvtboost is not yet on CRAN, but will be before publication. For review

purposes, a development version can be installed from github.com/patr1ckm/mvtboost.

METBOOST

26

computing the inﬂuence of important variables. A plot function is provided that plots the

predicted values against predictors to help detect the presence of nonlinear eﬀects. A

predict function is also provided that computes predicted values at a given number of

trees. However, it is faster and more memory eﬃcient to simply compute predicted values

along with training the model by indicating which observations are used for training and

testing by the subset argument.

Estimates of computation time for metboost are shown in Figure 3 for a range of

sample sizes and number of predictors for 1000 trees. The group size was always 50. These

timings were carried out on Quad 16 core 2.4 GHz AMD Opteron processors with up to

128 GB of RAM. For small data sets (n < 1000 and p < 100), metboost can be run in 1-4

minutes. For moderate data sets, (n < 10, 000 and p < 1000), analyses can be carried out

in a few hours or less. However, very large samples (n > 100, 000) or a very large number

of predictors (p > 10, 000) can require days or weeks of computing time. For a computation

time estimate with more trees, the estimates in Figure 3 can be extrapolated linearly.

To illustrate the advantage of taking the hierarchical structure of data into account,

we compare metboost to gbm in a simulation study next. Following the simulation study,

we use the method to predict math ability using data from the Educational Longitudinal

Study 2002.

Simulation Study: Prediction Error and Variable Selection Performance

In this simulation study, the variable selection and prediction performance of

metboost and gbm were compared while varying the following factors when generating the

data: the number of predictors, the type of eﬀect (linear only or nonlinear), the number of

predictors with random eﬀects, average group size, and the ICC (intra-class correlation

coeﬃcient). In the next sections, details are provided on how the artiﬁcial data sets were

generated, the overall simulation design, and how the methods were compared.

METBOOST

Simulation Design

eﬀect model (3, repeated):

Data generation. Artiﬁcial data sets were simulated under the following mixed

The predictors were independent, normally distributed variables with mean 0 and variance

1. Of the total number of predictors P , a smaller number p was chosen to have nonzero

ﬁxed eﬀects. Of these p predictors, a subset q were chosen to have eﬀects that varied by

group. With the variance of the intercepts and slopes ﬁxed to 1, the error variance σ2 was

chosen so that random eﬀects had a chosen ICC as follows:

The ﬁxed eﬀect βj for predictor j = 1, . . . , p was chosen to control for a given ˜R2 as follows:

y = Xβ + Zb + e

b iid∼ M N (0, Iq)

e iid∼ M N (0, σ2IN )

ICC =

σ2 =

1
1 + σ2
1 − ICC
ICC

˜R2 =

βj =

V ar(Xβ)
V ar(Xβ) + V ar(Zb) + σ2
˜R2(Z 0Z + σ2)
(1 − ˜R2)p

To generate nonlinear ﬁxed eﬀects, the design matrix X ∗ was used in place of X, and was

created by transforming predictor j = 1, . . . , p as x∗

j = f (xj). The function was chosen

randomly from the following set: f0(x) = x, f1(x) = x2, f2(x) =

|x|, f3(x) = cos(πx),

q

f4(x) = | sin(πx)|. Functions f1, f2 were chosen so that the predictor x∗

j would have a main

eﬀect of 0. Functions f3, f4 were chosen to simulate periodic behavior, while constraining

27

(17)

(18)

(19)

(20)

METBOOST

28

predictors to have a main eﬀect of 0. Function f0 was included to incorporate predictors

with truly linear or predictors with eﬀects that can be linearly approximated (e.g. ex, x3).

To generate group speciﬁc linear or nonlinear eﬀects for q predictors, the matrix Z

was generated as

Z = X ∗

1,...,q ∗ J

Where J is the indicator matrix denoting group membership (i.e. column ji = 1 for

observations in group i). X ∗

1,...,q is the subset of q columns in X ∗ (nonlinear) or X (linear)

that have group speciﬁc eﬀects. The operator ∗ denotes the column-wise product. Finally,

group speciﬁc linear or nonlinear functions were generated by multiplying Zb, with unique

weights b iid∼ M N (0, Iq). Examples of the simulated group speciﬁc nonlinear eﬀects are

shown in Figure 4.

Simulation Factors. The levels of the ﬁve factors used in the study are

summarized in Table 2. As in the applied example, both metboost and gbm were ﬁt to each

generated data set. The best number of trees and shrinkage values were chosen by 3-fold

cross-validation in the training set over a grid of meta-parameter values. The values of the

meta-parameters used for gbm and metboost are summarized in Table 3. The candidate

values for each method were chosen based on pilot simulations, and to ensure reasonable

computation time. For each method, the relative inﬂuence scores for each predictor and

predicted values for observations in the test set are based on the best number of trees was

chosen from the set of meta-parameters that achieved the lowest cross-validation error.

The methods were compared in terms of their mean squared prediction error on a

test set of equal size as the training set (ntrain = ntest = 1000). The methods were also

compared on how well they correctly selected important variables, which is referred to as

their variable selection performance. Variable selection performance is quantiﬁed by the

area under the ROC curve (AUC) for variable selection, which is based on the relative

inﬂuence scores from each predictor calculated for each model. The inﬂuence scores for

each predictor are computed from the trees in both metboost and gbm as usual. However,

METBOOST

29

the relative inﬂuence scores for gbm were calculated without including the inﬂuence of the

grouping variable. Without this adjustment, the inﬂuence of many of the other predictors

from gbm would be close to 0. This is has been previously observed in practice (Strobl et

al., 2007).

The AUC ranges from .5 to 1, with .5 being chance variable selection and 1 being

perfect variable selection. It can be computed as follows. Given a vector of inﬂuence scores

for p predictors and a vector of known truly signiﬁcant predictors, predictors can be

selected with inﬂuence greater than some threshold τ . This results in a 2 × 2 confusion

matrix containing counts of all correct (true positives, true negatives) and incorrect

decisions (false positives, false negatives) in the cells. For any given τ , the true and false

positive rates for variable selection can be computed. A ROC curve is then used to plot the

true positive rate against the false positive rate for all possible thresholds. An optimal

ROC curve has an AUC of 1. In this case the true positive rate is equal to 1 regardless of

the false positive rate as thresholds for selection become more lenient. An AUC of .5

corresponds to chance variable selection. In this case the false positive rate increases

linearly with the true positive rate as thresholds become more lenient.

Simulation Results

The percent change in prediction and variable selection performance for metboost

relative to gbm is shown in Figures 5 and 6 at ICC = .5. Performance of both methods

improved with larger ICCs. However, the ICC did not discriminate between the methods

or interact with the other factors. Simulation results show that metboost has uniformly

higher variable selection performance over gbm across all conditions, achieving up to 70%

better variable selection performance when group sizes are small and eﬀects are linear

(Figure 6, top row). When eﬀects are nonlinear and group sizes are small, metboost

achieves up to a 20% improvement in variable selection performance (Figure 6, bottom

row). When group sizes were large, metboost had 1-5% improvement over gbm when the

METBOOST

30

number of predictors increased (Figure 6).

The results also show that metboost can achieve improvements in prediction

performance of 10-30% over gbm when the average group size is small and when there are

group speciﬁc nonlinear eﬀects (Figure 5, bottom row). When eﬀects were exactly linear,

metboost could achieve 25% improvements or more when group sizes were small Figure 5,

top row). However, when group sizes were large, eﬀects were linear, and the number of

predictors is between 5-50, gbm outperforms metboost by 30-50% (Figure 5, top row). This

is larger than expected, but diminishes to a 0-10% improvement over metboost as the

number of predictors increase. We expect that this result will not be critical in practice

because the random and ﬁxed eﬀects of many predictors are unlikely to be exactly linear.

Further, if a small number of predictors were known to have exactly linear eﬀects, the true

model is the linear mixed eﬀect model and no exploratory regression analysis is necessary.

Next, we compare metboost and gbm to a large data set from education. This data

set illustrates the motivating factors for using metboost for exploratory regression analysis:

predictors may have group speciﬁc eﬀects, the eﬀects could be nonlinear, many predictors

have missing data, and there are a large number of predictors and samples.

Application of metboost to Education Longitudinal Study 2002

A natural application of metboost is for exploratory analyses of data sets in

education, where the eﬀects of student level predictors can often be expected to diﬀer

between schools. To illustrate this application, a data set on performance on a

standardized math test was obtained from the Education Longitudinal Study from the base

year 2002 (ELS), from the National Center for Education Statistics (Ingels et al., 2007).

This nationally representative sample included responses from 17,591 students, 7,135

teachers, and 743 principals from 751 schools (Ingels et al., 2007). For this analysis, the

student is the primary unit of analysis. Of the original 17,591 students, 15,240 had an

observed math standardized score. Students from 751 unique schools were included in the

METBOOST

31

dataset, and the number of students in each school ranged from 2-50 with median 20. The

dependent variable of interest in this analysis is the performance of students on a math

standardized test included in the ELS. This was the ability estimate from a 2-parameter

item response model, which was re-normalized to a z-score with 5.33% of the scores

imputed. Details of the imputation procedure are provided in Ingels et al. (2007).

Of the 4937 variables collected in the survey (including original items, item

aggregates, and survey weights), 109 publicly available items and aggregates of items were

selected for evaluation of their utility in predicting the math ability estimates. Groups of

variables are summarized in Table 4. Seven new aggregate scores were created as sum or

factor scores from multiple items. This was done for interpretability, to reduce

measurement error, to decrease collinearity, to reduce the impact of missing values on

narrowly worded items, and to improve computation time at the expense of some

predictive accuracy. Of the new aggregate scores, 3 aggregates were simple counts of the

number of math competency exams in grades 8-12, number of awards or participations in

competitions, and the number of attendance problems respectively. One variable was

added indicating dropout from the high school before graduation. Next, 3 factor scores

were generated from their respective items: Enjoys Math, Teacher Rated Attendance, and

Teacher Rated Tardiness. The items used to form these 7 aggregate scores (39 in total)

were dropped from the analysis, leaving 76 predictors plus the school identiﬁer for inclusion

in the prediction models (Table 4).

Analysis

The metboost algorithm was used to predict the math standardized score from the 76

predictors shown in Table 4. The eﬀects of these predictors were allowed to vary by school.

metboost was trained on 80% of the sample (ntrain = 12, 192 observations), with 2500 trees

over the following grid of meta-parameters: tree-depth = (3, 5, 8), shrinkage = (.005, .01,

.025, .05, .1) for a total of 15 unique combinations of meta-parameters. The best set of

METBOOST

32

meta-parameters was chosen by 3-fold cross-validation within the training set, and the best

number of trees chosen to from that set to minimize cross-validation error.

We compare the results of predicting the math standardized score using metboost to

the results of gbm including the school id variable as a candidate for splitting. Like

metboost, gbm was tuned by 3-fold cross-validation over the following grid of

meta-parameters: tree-depth = (5, 10, 25, 49), shrinkage = (.005, .01, .025, .05, .1),

minimum number of observations in each node = (10, 25, 50). The number of trees was

5000. This resulted in 60 total conﬁgurations of meta-parameters. As with metboost, the

best set of meta-parameters was chosen by 3-fold cross-validation within the training set,

and the best number of trees chosen to from that set to minimize cross-validation error.

Results

(15%).

The best set of meta-parameters for metboost was tree-depth = 3, shrinkage = .025,

and the best number of trees was 2388. For gbm, the best set of meta-parameters was

interaction.depth = 5, shrinkage = .025, minimum number of observations in each node =

50, and the best number of trees was 4115. The mean squared prediction error in the test

set (ntest = 3048) achieved by metboost was .40, compared to .47 for gbm, an improvement

of 15%. In metboost, school accounted for 8% of the variance in predictions. The

improvement in prediction performance of metboost in the test set is consistent with

simulation results when group sizes are relatively large and nonlinear eﬀects are present

The relative inﬂuence scores for the top 10 predictors from metboost and gbm are

shown in Figure 7. For the relative inﬂuence scores to be comparable between gbm and

metboost, the inﬂuence of school was set to 0 in gbm. Of the 76 predictors, the most

important predictor was GPA, followed by highest degree expected (teacher) and math self

eﬃcacy. The rank correlation between the inﬂuence scores for all predictors from gbm and

metboost was ρ = .89, S = 7757.1, p < 2.2e−16. The only discrepancy in rank between the

METBOOST

33

top 10 predictors was that highest degree expected (student) is ranked 4th for metboost

but unranked for gbm, while ‘has disability’ (rated by the student’s english teacher) is

ranked 5th for gbm but unranked for metboost.

The model implied eﬀects of GPA and highest degree expected (teacher) are shown in

Figures 8 and 9. These plots suggest that the eﬀects of GPA and highest degree expected

(teacher) on math performance are approximately linear. These model implied eﬀects are

shown for nine schools with the most discrepant predictions between gbm and metboost.

While no consistent patterns of diﬀerences between the predictions emerged, the metboost

predictions are more theoretically informative because school is treated as random.

Discussion

Exploratory regression analysis with mixed eﬀect models is an important but diﬃcult

problem. As the number of variables in psychological research increase (such as with a

large educational study), exploratory questions becomes more important. Which predictors

should be included in the model? Which predictor eﬀects should be random or ﬁxed? A

good exploratory regression analysis should be able to identify predictors with nonlinear

eﬀects and predictor eﬀects that vary by group, handle missing data, while remaining

computationally feasible for a large number of predictors and samples. Plots and model

selection can address some of these questions, but they require strong assumptions, a

separate data imputation step if missing data are present, or they cannot handle a large

number of predictors or samples. There are many existing parametric and semi-parametric

models that can approximate group speciﬁc nonlinear eﬀects, but almost all require a priori

model speciﬁcation or selection. Additionally, few existing methods are computationally

feasible when the number of predictors is large and work well with missing data.

To address this gap, we have proposed a method called metboost for exploratory

regression analysis, which combines the strengths of boosted decision trees with mixed

eﬀect models. Built on boosted decision trees, metboost yields excellent prediction and

METBOOST

34

variable selection performance for predictors with nonlinear eﬀects even when observations

are missing on some predictors. Group-speciﬁc nonlinear eﬀects are included in the model

by constraining the split points of each tree to be the same across groups, but allowing the

terminal node means to diﬀer. This feature allows splits (e.g. GPA < 3.2) to have the same

cut point but diﬀerent implications for the outcome (e.g. math performance for each

school). These modiﬁed trees are then ﬁt in an ensemble by gradient descent (boosting),

which increases variable selection and prediction performance compared to individual trees.

The only other method that achieves all the goals of exploratory regression analysis are

decision tree ensembles such as boosted decision trees (Friedman, 2001) with the grouping

variable as a candidate for splitting, in which group speciﬁc eﬀects are chosen in a data

driven way.

As a demonstration, metboost was used to predict math performance on a

standardized test in a large educational data set from the Educational Longitudinal Survey

2002 (Ingels et al., 2007). This data set had a moderate number of samples (> 10, 000) and

predictors (76), many of which contained missing values. Collectively, all 76 included

predictors achieved a 40% prediction error in a test set, which was a 15% improvement in

prediction error relative to gbm. In the context of machine learning, such large gains are

diﬃcult to achieve. Both metboost and gbm selected GPA and highest degree expected by

teacher as being the most important predictors, and had consistent predictor ranks overall.

Importantly, math performance varied signiﬁcantly across the schools considered in the

study, with school accounting for 8% of the variance in the predictions.

In the more general settings considered the simulation, results showed that metboost

had uniformly better variable selection performance compared to gbm across group size,

number of predictors, and number of predictors with random eﬀects (up to 70% better).

These results indicated that even though variable selection was similar in the Educational

Longitudinal Survey data, metboost can more consistently select a correct set of variables

to be included in a subsequent model regardless of the inclusion threshold chosen. In terms

METBOOST

35

of prediction performance, metboost had up to 50% improved prediction over gbm when

group sizes were small. When group sizes were large, gbm and metboost had similar

prediction error in the presence of nonlinear eﬀects. There was a single case when gbm

outperformed metboost: when the number of predictors was small (5-50), the number of

predictors with group speciﬁc eﬀects was small (5-10), group sizes were large (> 20), and

eﬀects were exactly linear. In this case, gbm had 25-50% better prediction performance

than metboost. However, in this situation, even better performance can be achieved by

simply using a linear mixed eﬀect model (the true model). In such cases, a nonparametric

exploratory procedure is not necessary.

There are several theoretical limitations of the method which could be addressed in

future research. First, metboost does not produce a deﬁnitive conclusion regarding which

predictors should be treated as random or ﬁxed, and only indicates a subset of candidate

predictors that could be considered random. We note, however, that identifying such a

subset is diﬃcult using existing methods when the number of predictors is large. A second

limitation is that many of the mixed models ﬁt in the ensemble do not converge. The eﬀect

of convergence on variable selection or prediction error could be explored, as well as

diﬀerent solutions. A third limitation is in interpretation. For example, predictors can only

be ranked by their inﬂuence score. However, this is essentially true of all exploratory

procedures (even model selection), and is advantageous compared to interpreting p-values

after model selection: results from the exploration are reproducible, and are clearly

exploratory rather than conﬁrmatory. Hypotheses suggested from metboost can then be

tested by ﬁtting a single model and testing coeﬃcients in that model in a separate sample.

The generality and ﬂexibility of metboost comes at the cost of computation time,

which also limits the sample size and number of predictors that can be used for the

method. Currently, a few thousand predictors and sample sizes <10K are practically

feasible in terms of memory use and computation time. This limits applications for high

dimensional data from genetics or neuroimaging, which often involve millions of predictors.

METBOOST

36

However, metboost could be feasibly run in a few hours for many data sets in psychology.

Computation time also limits further improvements to the method or its interpretation,

such as handling multiple grouping variables, obtaining true partial dependence plots, or in

obtaining estimates of uncertainty around the predictions. Meta-parameter tuning is

critical, but also adds computation time. Our implementation provides easy speciﬁcation

for cross-validated grid tuning for the important metboost meta-parameters, and makes it

easy to carry out the tuning in parallel. To carry out the analysis, we suggest utilizing large

computing clusters provided by many universities or other cloud computing providers.

An important application for metboost that we left unexplored is for analyses of

longitudinal data with many subjects and relatively few measurement occasions. With the

data in long form, the unique subject identiﬁer is the grouping variable and time can be

included as a predictor in the model, along with other covariates. In this setting, metboost

approximates subject speciﬁc nonlinear trajectories that are conditional on the included

covariates. This approach is similar to using an additive model for time-varying

autoregression (Bringmann et al., 2016), but is fully nonparametric and more exploratory.

The simulation results carried out in this paper apply to the longitudinal setting by noting

that the number of groups is the number of measurement occasions. The results suggest

that metboost can have improved prediction error and variable selection performance

compared to gbm when the average number of measurement occasions is small, which is

often the case in psychology.

In the future, we hope to extend metboost in several ways. For example, when groups

are unknown or appear in the test set but not in training, it may be possible to classify

unknown groups by their similarity to groups that are known by clustering. In addition, we

hope to generalize metboost for classiﬁcation or survival outcomes, similar to existing

implementations of boosting. Finally, gains in computation time might be realized by using

a faster language for scientiﬁc computing such as Julia (Bezanson, Edelman, Karpinski, &

Shah, 2017).

METBOOST

37

In conclusion, metboost is a powerful tool for exploratory regression analysis with

hierarchically clustered data. We hope that researchers in psychology and other social

sciences can use this tool to answer exploratory questions, improve predictions, highlight

potential nonlinear eﬀects, and to narrow the number of variables to include in subsequent

mixed eﬀects models.

METBOOST

38

References

Afshartous, D., & de Leeuw, J. (2005). Prediction in Multilevel Models. Journal of

Educational and Behavioral Statistics, 30 (2), 109–139. doi:

10.3102/10769986030002109

Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random eﬀects structure for

conﬁrmatory hypothesis testing: Keep it maximal. Journal of Memory and

Language, 68 (3), 255–278. doi: 10.1016/j.jml.2012.11.001

Bates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting Linear Mixed-Eﬀects

Models Using lme4. Journal of Statistical Software, 67 (1), 1–48. doi:

Bauer, D. J., & Cai, L. (2009). Consequences of unmodeled nonlinear eﬀects in multilevel

models. Journal of Educational and Behavioral Statistics, 34 (1), 97–114. doi:

10.18637/jss.v067.i01

10.3102/1076998607310504

Bezanson, J., Edelman, A., Karpinski, S., & Shah, V. (2017). Julia: A Fresh Approach to

Numerical Computing. SIAM Review, 59 (1), 65–98. doi: 10.1137/141000671

Brandmaier, A. M., Prindle, J. J., McArdle, J. J., & Lindenberger, U. (2016).

Theory-guided exploration with structural equation model forests. Psychological

Methods, 21 (4), 566–582. doi: 10.1037/met0000090

Brandmaier, A. M., von Oertzen, T., McArdle, J. J., & Lindenberger, U. (2013). Structural

equation model trees. Psychological methods, 18 (1), 71–86. doi: 10.1037/a0030001

Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). Classiﬁcation and

regression trees. London: CRC press.

Bringmann, L. F., Hamaker, E. L., Vigo, D. E., Aubert, A., Borsboom, D., & Tuerlinckx,

F. (2016). Changing dynamics: Time-varying autoregressive models using generalized

additive modeling. Psychological methods. doi: 10.1037/met0000085

Bühlmann, P., & Hothorn, T. (2007). Boosting Algorithms: Regularization, Prediction and

Model Fitting. Statistical Science, 22 (4), 477–505. doi: 10.1214/07-STS242

METBOOST

39

Bühlmann, P., & Yu, B. (2003). Boosting with the L2 loss: Regression and classiﬁcation.

Journal of the American Statistical Association, 98 (462), 324–339.

Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. In

Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge

Discovery and Data Mining (pp. 785–794). New York, NY, USA: ACM. doi:

10.1145/2939672.2939785

De Boeck, P., & Partchev, I. (2012). IRTrees: Tree-based item response models of the

GLMM family. Journal of Statistical Software, 48 (1), 1–28. doi: 10.1.1.302.6429

Elith, J., Leathwick, J. R., & Hastie, T. (2008). A working guide to boosted regression

trees. Journal of Animal Ecology, 77 (4), 802–813. doi:

10.1111/j.1365-2656.2008.01390.x

Fan, Y., & Li, R. (2012). Variable selection in linear mixed eﬀects models. The Annals of

Statistics, 40 (4), 2043–2068. doi: 10.1214/12-AOS1028

Fokkema, M., Smits, N., Zeileis, A., Hothorn, T., & Kelderman, H. (2015). Detecting

treatment-subgroup interactions in clustered data with generalized linear mixed-eﬀects

model trees (Working Papers). Faculty of Economics and Statistics, University of

Innsbruck.

Friedman, J. (2001). Greedy function approximation: A gradient boosting machine.

Annals of Statistics, 29 (5), 1189–1232. doi: 10.1214/aos/1013203451

Friedman, J. (2002). Stochastic gradient boosting. Computational Statistics & Data

Analysis, 38 (4), 367–378. doi: 10.1016/S0167-9473(01)00065-2

Friedman, J., Hastie, T., & Tibshirani, R. (2000). Additive logistic regression: A statistical

view of boosting (With discussion and a rejoinder by the authors). The Annals of

Statistics, 28 (2), 337–407.

Friedman, J., & Meulman, J. (2003). Multiple additive regression trees with application in

epidemiology. Statistics in Medicine, 22 (9), 1365–1381. doi: 10.1002/sim.1501

Gelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical

METBOOST

40

models. New York, NY, USA: Cambridge University Press.

Goldstein, A., Kapelner, A., Bleich, J., & Pitkin, E. (2015). Peeking inside the black box:

Visualizing statistical learning with plots of individual conditional expectation.

Journal of Computational and Graphical Statistics, 24 (1), 44–65. doi:

10.1080/10618600.2014.907095

Groll, A., & Tutz, G. (2014). Variable selection for generalized linear mixed models by

L1-penalized estimation. Statistics and Computing, 24 (2), 137–154. doi:

10.1007/s11222-012-9359-z

Hajjem, A., Bellavance, F., & Larocque, D. (2014). Mixed-eﬀects random forest for

clustered data. Journal of Statistical Computation and Simulation, 84 (6), 1313–1328.

doi: 10.1080/00949655.2012.741599

Harville, D. A. (1977). Maximum likelihood approaches to variance component estimation

and to related problems. Journal of the American Statistical Association, 72 (358),

Hastie, T., Tibshirani, R., & Friedman, J. (2013). The elements of statistical learning. New

320–338. doi: 10.2307/2286796

York: Springer.

Henderson, C. R. (1975). Best linear unbiased estimation and prediction under a selection

model. Biometrics, 31 (2), 423–447. doi: 10.2307/2529430

Hickey, J., Mtcalfe, P., Ridgeway, G., Schroedl, S., Southworth, H., & Therneau, T. (2016).

Gbm: Generalized Boosted Regression Models.

Hofner, B., Mayr, A., Robinzonov, N., & Schmid, M. (2014). Model-based boosting in R:

A hands-on tutorial using the R package mboost. Computational Statistics, 29 (1-2),

Hothorn, T., Buehlmann, P., Kneib, T., Schmid, M., & Hofner, B. (2016). Mboost:

3–35. doi: 10.1007/s00180-012-0382-5

Model-Based Boosting.

Hothorn, T., Hornik, K., & Zeileis, A. (2006). Unbiased recursive partitioning: A

conditional inference framework. Journal of Computational and Graphical Statistics,

METBOOST

41

15 (3), 651–674. doi: 10.1198/106186006X133933

Hyaﬁl, L., & Rivest, R. L. (1976). Constructing optimal binary decision trees is

NP-complete. Information Processing Letters, 5 (1), 15–17. doi:

10.1016/0020-0190(76)90095-8

Ingels, S. J., Pratt, D. J., Wilson, D., Burns, L. J., Currivan, D., Rogers, J. E., &

Hubbard-Bednasz, S. (2007). Education Longitudinal Study of 2002 (ELS: 2002):

Base-Year to Second Follow-Up Data File Documentation. NCES 2008-347. National

Center for Education Statistics.

Laird, N. M., & Ware, J. H. (1982). Random-Eﬀects Models for Longitudinal Data.

Biometrics, 38 (4), 963–974. doi: 10.2307/2529876

Miller, P. J., Lubke, G. H., McArtor, D. B., & Bergeman, C. S. (2016). Finding structure

in data using multivariate tree boosting. Psychological Methods, 21 (4), 583–602. doi:

10.1037/met0000087

Ni, X., Zhang, D., & Zhang, H. H. (2010). Variable selection for semiparametric mixed

models in longitudinal studies. Biometrics, 66 (1), 79–88. doi:

10.1111/j.1541-0420.2009.01240.x

Pande, A., Li, L., Rajeswaran, J., Ehrlinger, J., Kogalur, U. B., Blackstone, E. H., &

Ishwaran, H. (2016). Boosted multivariate trees for longitudinal data. Machine

Learning, 106 (2), 277–305. doi: 10.1007/s10994-016-5597-1

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., . . . others

(2011). Scikit-learn: Machine learning in Python. The Journal of Machine Learning

Pinheiro, J., & Bates, D. (2006). Mixed-eﬀects models in S and S-PLUS. New York, NY:

Research, 12 , 2825–2830.

Springer Science & Business Media.

Rohart, F., San Cristobal, M., & Laurent, B. (2014). Selection of ﬁxed eﬀects in high

dimensional linear mixed models using a multicycle ECM algorithm. Computational

Statistics & Data Analysis, 80 , 209–222. doi: 10.1016/j.csda.2014.06.022

METBOOST

42

Schelldorfer, J., Bühlmann, P., & Van Der Geer, S. (2011). Estimation for

high-dimensional linear mixed-eﬀects models using L1-penalization. Scandinavian

Journal of Statistics, 38 (2), 197–214. doi: 10.1111/j.1467-9469.2011.00740.x

Schelldorfer, J., Meier, L., & Bühlmann, P. (2014). GLMMLasso: An algorithm for

high-dimensional generalized linear mixed models using L1-penalization. Journal of

Computational and Graphical Statistics, 23 (2), 460–477. doi:

10.1080/10618600.2013.773239

Searle, S. R. (1997). The matrix handling of BLUE and BLUP in the mixed linear model.

Linear Algebra and its Applications, 264 (6), 291–311. doi:

10.1016/S0024-3795(96)00400-4

Sela, R. J., & Simonoﬀ, J. S. (2012). RE-EM trees: A data mining approach for

longitudinal and clustered data. Machine Learning, 86 (2), 169–207. doi:

10.1007/s10994-011-5258-3

Skrondal, A., & Rabe-Hesketh, S. (2009). Prediction in multilevel generalized linear

models. Journal of the Royal Statistical Society: Series A (Statistics in Society),

172 (3), 659–687. doi: 10.1111/j.1467-985X.2009.00587.x

Strobl, C., Boulesteix, A.-L., Kneib, T., Augustin, T., & Zeileis, A. (2008). Conditional

variable importance for random forests. BMC Bioinformatics, 9 (1), 307. doi:

10.1186/1471-2105-9-307

Strobl, C., Boulesteix, A.-L., Zeileis, A., & Hothorn, T. (2007). Bias in random forest

variable importance measures: Illustrations, sources and a solution. BMC

bioinformatics, 8 (1), 25. doi: 10.1186/1471-2105-8-25

Strobl, C., Malley, J., & Tutz, G. (2009). An introduction to recursive partitioning:

Rationale, application and characteristics of classiﬁcation and regression trees,

bagging and random forests. Psychological methods, 14 (4), 323–348. doi:

10.1037/a0016973

Thompson, B. (1995). Stepwise Regression and Stepwise Discriminant Analysis Need Not

METBOOST

43

Apply here: A Guidelines Editorial. Educational and Psychological Measurement,

55 (4), 525–534. doi: 10.1177/0013164495055004001

Tukey, J. W. (1969). Analyzing data: Sanctiﬁcation or detective work? American

Psychologist, 24 (2), 83–91. doi: 10.1037/h0027108

Tutz, G., & Groll, A. (2010). Generalized linear mixed models based on boosting. In

T. Kneib & G. Tutz (Eds.), Statistical Modelling and Regression Structures (pp.

Wood, S. (2006). Generalized additive models: An introduction with R. Boca Raton, FL:

Wood, S., & Scheipl, F. (2014). Gamm4: Generalized additive mixed models using mgcv

197–215). Physica-Verlag HD.

CRC press.

and lme4. (R package version 0.2-3)

Zeileis, A., Hothorn, T., & Hornik, K. (2008). Model-based recursive partitioning. Journal

of Computational and Graphical Statistics, 17 (2), 492–514. doi:

10.1198/106186008X319331

44

METBOOST

Table 1

i ni

g
ni
N = P
p
k
q
Xi, X
Ji, J
Zi, Z
˜Xi, ˜X
˜Zi, ˜Z

Description of matrices in linear mixed eﬀect models and mixed tree models.

Symbol

Dimensions

Meaning

Number of groups in the grouping variable
Number of observations within group i
Total number of observations
Number of predictors including intercept
Number of terminal nodes
Number of random eﬀects
Design matrix of ﬁxed eﬀects plus intercept
Matrix of indicator variables denoting group membership

ni × p, N × p
ni × g, N × g
ni × q, N × (gq) Design matrix of random eﬀects
ni × k, N × k
ni × k, N × (gk) Design matrix of random tree eﬀects

Design matrix of ﬁxed tree eﬀects

METBOOST

Table 2

Simulation Design.

45

Factor

Number of predictors
Eﬀect type
Predictors with random eﬀects
Average group size
ICC
Total eﬀect size
Fraction of predictors nonzero eﬀects
N

Levels

5, 25, 50, 100, 250
Linear, Nonlinear
5, 10, 25, 50
4, 5, 8, 10, 20, 40
.3, .5, .8
.5
25
1,000

Note: Five varying factors were setup in a factorial experiment, resulting in 720 simulation con-
ditions. Replicated data sets were generated from (17). The error variance was chosen to control
the ICC (18), and the ﬁxed eﬀect size for predictors with nonzero eﬀects was chosen to control the
total eﬀect size (20). The number of replications was 50.

METBOOST

Table 3

46

Simulation meta-parameter grids for gbm and metboost.

Meta-parameter

gbm

Number of trees
Shrinkage
Tree-Depth
Minimum number of observations in node

10,000
.005, .01, .05, .1
5, 10, 25
˜ni, 20

metboost

2,500
.01, .025, .05, .1
3, 5
20

Note: gbm was run with 24 unique combinations of meta-parameters, while metboost was run over
8 unique combinations. metboost requires a much smaller tree-depth, and fewer number of trees
than gbm. In addition to shrinkage and tree-depth, gbm was also tuned over the minimum number
of observations in each terminal node. These values were the default (20) and the average group
size, ˜ni.

METBOOST

Table 4

47

Summary of included variables from the Education Longitudinal Survey 2002.

Rater

Name of Group

Student

N Vars

51

Representative Items

Math Class Format
12
Demographics
9
Characteristics
7
Beliefs & Attitudes
7
Extra-Curricular
5
School/Homework
5
Motivation
2
2
Teacher Assessment
Beliefs & Values of Friends 2

problem solving in class, Class Preparation Scale
sex, parent ed, SES, In IB/AP Program
awards, attention problems, GPA, dropout, no. of risk factors
highest degree expected, math self eﬃcacy, Enjoys Math
no. of school activities, held job for pay
homework hours in school, hours spent on homework
enjoys math, instrumental motivation
teacher quality, teacher/student relationship
friends consider grades important, friends dropped out

highest degree expected, student falling behind, ‘has disability’
total years teaching, highest graduate degree held
can learn to be good at math, would teach math again
sex, days missed
diﬃculty of class

Teacher

Ratings of student
Education & Experience
Philosophy & Motivation
Characteristics
Classroom

Principal

Total

17

6
5
3
2
1

8

6
2

76

School Characteristics
Safety & Climate

public/private, region, % of 10th graders with free lunch
school safety, academic climate

Note: Variables are grouped by rater, and then by face validity (1st column).

METBOOST

48

Figure 1 . Representing a decision tree with four terminal nodes and two observations per
node as a design matrix ˜X multiplied by a weight vector β. Each column of ˜X is an
indicator that assigns an observation to a terminal node. The weight vector β contains the
terminal node means.

METBOOST

49

Figure 2 . Predictions for decision tree with four terminal nodes. The decision tree
predictions averaged across schools is shown as a solid line. School speciﬁc predictions are
shown as dotted lines.

METBOOST

50

Figure 3 . Computation time for metboost as a function of sample size and the number of
predictors for 1000 trees. These timings were carried out on Quad 16 core 2.4 GHz AMD
Opteron processors with up to 128 GB of RAM. Trees were ﬁt with 8 threads. Timings do
not include cross-validation over a grid of meta-parameters. For small to moderate data
sets, analyses can be carried out in a few hours. When both the number of samples and the
number of predictors is large, computation time can take a week or longer.

METBOOST

51

Figure 4 . Example group speciﬁc nonlinear eﬀects generated for a single covariate and
three groups using four nonlinear functions.

METBOOST

52

Figure 5 . Percent improvement in prediction error of metboost compared to gbm (y-axis),
plotted as a function of the number of predictors (x-axis), the number of random eﬀects
(columns), the kind of eﬀect (rows), and average group size. Conditions with scores above
0 are favorable for metboost, and are indicated with solid lines. Results are shown for
n = 1000, ICC = .5, and with predictors explaining ≈ 50% of the variance in the outcome.
Results show that gbm performs better than metboost only with large group sizes, a small
number of predictors, and explicitly linear eﬀects (top left panels), otherwise metboost
performs better. metboost can perform as much as 35% better when group sizes are small,
and almost always performs better when group speciﬁc eﬀects are nonlinear.

METBOOST

53

Figure 6 . Percent improvement in variable selection performance of metboost compared to
gbm (y-axis) plotted as a function of the number of predictors (x-axis), the number of
random eﬀects (columns), the kind of eﬀect (rows), and average group size. Conditions
with scores above 0 are favorable for metboost, and are indicated with solid lines. Results
are shown for n = 1000, ICC = .5, and with predictors explaining ≈ 50% of the variance in
the outcome. Results show that metboost has uniformly better variable selection
performance across this range of conditions.

METBOOST

54

Figure 7 . The relative inﬂuence (x-axis) of the top ten (out of 76) predictors on the math
ability estimate. The relative inﬂuence scores are compared between metboost (ﬁlled) and
gbm (open). The gbm inﬂuence score was computed removing the inﬂuence of school.
Results are similar in rank except that highest degree expected (student) is ranked 4th for
metboost but not gbm, while ‘has disability’ (rated by the student’s english teacher) is
ranked 5th for gbm but not for metboost.

METBOOST

55

Figure 8 . The model implied eﬀect of GPA on the math standardized score for nine schools.
These nine schools chosen had the most discrepant predictions between gbm and metboost.

METBOOST

56

Figure 9 . The model implied eﬀect of highest degree expected (teacher) on the math
standardized score for nine schools. These nine schools chosen had the most discrepant
predictions between gbm and metboost.

