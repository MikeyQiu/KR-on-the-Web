Unsupervised Generative Modeling Using Matrix Product States

Zhao-Yu Han,1,

∗ Jun Wang,1,

∗ Heng Fan,2 Lei Wang,2,

† and Pan Zhang3,

‡

1School of Physics, Peking University, Beijing 100871, China
2Institute of Physics, Chinese Academy of Sciences, Beijing 100190, China
3Institute of Theoretical Physics, Chinese Academy of Sciences, Beijing 100190, China

Generative modeling, which learns joint probability distribution from data and generates samples according
to it, is an important task in machine learning and artiﬁcial intelligence. Inspired by probabilistic interpretation
of quantum physics, we propose a generative model using matrix product states, which is a tensor network
originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys
eﬃcient learning analogous to the density matrix renormalization group method, which allows dynamically
adjusting dimensions of the tensors and oﬀers an eﬃcient direct sampling approach for generative tasks. We
apply our method to generative modeling of several standard datasets including the Bars and Stripes random
binary patterns and the MNIST handwritten digits to illustrate the abilities, features and drawbacks of our
model over popular generative models such as Hopﬁeld model, Boltzmann machines and generative adversarial
networks. Our work sheds light on many interesting directions of future exploration on the development of
quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to be realized
on quantum devices.

I.

INTRODUCTION

Generative modeling, a typical example of unsupervised
learning that makes use of huge amount of unlabeled data,
lies in the heart of rapid development of modern machine
learning techniques [1]. Diﬀerent from discriminative tasks
such as pattern recognition, the goal of generative modeling
is to model the probability distribution of data and thus be
able to generate new samples according to the distribution.
At the research frontier of generative modeling, it is used for
ﬁnding good data representation and dealing with tasks with
missing data. Popular generative machine learning models in-
clude Boltzmann Machines (BM) [2, 3] and their generaliza-
tions [4], variational autoencoders (VAE) [5], autoregressive
models [6, 7], normalizing ﬂows [8–10], and generative ad-
versarial networks (GAN) [11]. For generative model design,
one tries to balance the representational power and eﬃciency
of learning and sampling.

There is a long history of the interplay between generative
modeling and statistical physics. Some celebrated models,
such as Hopﬁeld model [12] and Boltzmann machine [2, 3],
are closely related to the Ising model and its inverse version
which learns couplings in the model based on given training
conﬁgurations [13, 14].

The task of generative modeling also shares similarities
with quantum physics in the sense that both of them try to
model probability distributions in an immense space. Pre-
cisely speaking, it is the wavefunctions that are modeled in
quantum physics, and probability distributions are given by
their squared norm according to Born’s statistical interpreta-
tion. Modeling probability distributions in this way is funda-
mentally diﬀerent from the traditional statistical physics per-
spective. Hence we may refer probability models which ex-

∗ These two authors contributed equally
† wanglei@iphy.ac.cn
‡ panzhang@itp.ac.cn

ploit quantum state representations as “Born Machines”. Var-
ious ansatz have been developed to express quantum states,
such as the variational Monte Carlo [15], the tensor network
(TN) states and recently artiﬁcial neural networks [16] . In
fact physical systems like quantum circuits are also promising
candidates for implementing Born Machines.

In the past decades, tensor network states and algorithms
have been shown to be an incredibly potent tool set for mod-
eling many-body quantum states [17, 18]. The success of TN
description can be theoretically justiﬁed from a quantum in-
formation perspective [19, 20]. In parallel to quantum physics
applications, tensor decomposition and tensor networks have
also been applied in a broader context by the machine learning
community for feature extraction, dimensionality reduction
and analyzing the expressibility of deep neural networks [21–
26].

In particular, matrix product state (MPS) is a kind of TN
where the tensors are arranged in a one-dimensional geome-
try [27]. The same representation is referred as tensor train
decomposition in the applied math community [28]. De-
spite its simple structure, MPS can represent a large num-
ber of quantum states extremely well. MPS representation
of ground states has been proven to be eﬃcient for one-
dimensional gapped local Hamiltonian [29]. In practice, opti-
mization schemes for MPS such as density-matrix renormal-
ization group (DMRG) [30] have been successful even for
some quantum systems in higher dimension [31]. Some recent
works extended the application of MPS to machine learning
tasks like pattern recognition [32], classiﬁcation [33] and lan-
guage modeling [34]. Eﬀorts also drew connection between
Boltzmann Machines and tensor networks [35].

In this paper, building on the connection between unsu-
pervised generative modeling and quantum physics, we em-
ploy MPS as a model to learn probability distribution of given
data with an algorithm which resembles DMRG [30]. Com-
pared with statistical-physics based models such as the Hop-
ﬁeld model [12] and the inverse Ising model, MPS exhibits
much stronger ability of learning, which adaptively grows by
increasing bond dimensions of the MPS. The MPS model also

8
1
0
2
 
l
u
J
 
9
1
 
 
]
h
c
e
m

-
t
a
t
s
.
t
a
m
-
d
n
o
c
[
 
 
3
v
2
6
6
1
0
.

9
0
7
1
:
v
i
X
r
a

enjoys a direct sampling method [36] much more eﬃcient than
the Boltzmann machines, which require Markov Chain Monte
Carlo (MCMC) process for data generation. When compared
with popular generative models such as GAN, our model of-
fers a more eﬃcient way to reconstruct and denoise from an
initial (noisy) input using the direct sampling algorithm, as
opposed to GAN where mapping a noisy image to its input is
not straightforward.

The rest of the paper is organized as follow.

In Sec. II
we present our model, training algorithm and direct sampling
method.
In Sec. III we apply our model to three datasets:
Bars-and-stripes for a proof-of-principle demonstration, ran-
dom binary patterns for capacity illustration and the MNIST
handwritten digits for showing the generalization ability of
the MPS model in unsupervised tasks such as reconstruc-
tionof images. Finally, Sec IV discusses future prospects of
the generative modeling using more general tensor networks
and quantum circuits.

II. MPS FOR UNSUPERVISED LEARNING

The goal of unsupervised generative modeling is to model
the joint probability distribution of given data. With the
trained model, one can then generate new samples from the
learned probability distribution. Generative modeling ﬁnds
wide applications such as dimensional reduction, feature de-
tection, clustering and recommendation systems [37]. In this
consisting of binary strings
paper, we consider a data set
N, which are potentially repeated and can be
v
}⊗
mapped to basis vectors of a Hilbert space of dimension 2N.

∈ V

0, 1

T

=

{

The probabilistic interpretation of quantum mechanics [38]
naturally suggests modeling data distribution with a quantum
state. Suppose we encode the probability distribution into
a quantum wavefunction Ψ(v), measurement will collapse it
and generate a result v = (v1, v2,
, vN) with a probability
2 Inspired by the generative aspects of
proportional to
quantum mechanics, we represent the model probability dis-
tribution by

Ψ(v)
|
|

· · ·

P(v) = |

2

Ψ(v)
|
Z

,

(1)

v
∈V |

Ψ(v)
|

where Z = (cid:80)
2 is the normalization factor. We also
refer it as the partition function to draw an analogy with the
energy based models [39]. In general the wavefunction Ψ(v)
can be complex valued, but in this work we restrict it to be
real valued. Representing probability density using square of
a function was also put forward by former works [32, 40,
41]. These approaches ensure the positivity of probability and
naturally admit a quantum mechanical interpretation.

A. Matrix Product States

Quantum physicists and chemists have developed many ef-
ﬁcient classical representations of quantum wavefunctions.
A number of these developed representations and algorithms

2

can be adopted for eﬃcient probabilistic modeling. Here, we
parametrize the wave function using MPS:

Ψ(v1, v2,

, vN) = Tr

A(1)v1 A(2)v2

· · ·

(cid:16)

A(N)vN

(cid:17)

,

· · ·

(2)

−

(cid:80)N

Dk

k=1 Dk

1 by
−

D0 =

where each A(k)vk is a
DN is
Dk matrix, and
demanded to close the trace. For the the case considered here,
there are 2
1Dk parameters on the right-hand-side of
Eq. (2). The representational power of MPS is related to Von
Neumann entanglement entropy of the quantum state, which
Tr(ρA ln ρA). Here we divide the variables
is deﬁned as S =
into two groups v = (vA, vB) and ρA = (cid:80)
−
vB Ψ(vA, vB)Ψ(v(cid:48)A, vB)
is the reduced density matrix of a subsystem. The entangle-
ment entropy sets a lower bound for the bond dimension at the
Dk). Any probability distribution of a N-bit
division S
system can be described by an MPS as long as its bond dimen-
sions are free from any restriction. The inductive bias using
MPS with limited bond dimensions comes from dropping oﬀ
the minor components of entanglement spectrum. Therefore
as the bond dimension increases, an MPS enhances its ability
of parameterizing complicated functions. See [17] and [18]
for recent reviews on MPS and its applications on quantum
many-body systems.

ln(

≤

In practice, it is convenient to use MPS with

DN = 1
D0 =
and consequently reduce the left and right most matrices to
vectors [30]. In this case, Eq. (2) reads schematically

=

.

(3)

Here the blocks denote the tensors and the connected lines in-
dicate tensor contraction over virtual indices. The dangling
vertical bonds denote physical indices. We refer to [17, 18]
for an introduction to these graphical notations of TN. Hence-
forth, we shall present formulae with more intuitive graphical
notations wherever possible.

The MPS representation has gauge degrees of freedom,
which allows one to restrict the tensors with canonical con-
ditions. We remark that in our setting of generative modeling,
the canonical form signiﬁcantly beneﬁts computing the exact
partition function Z. More details about the canonical condi-
tion and the calculation of Z can be found in Appendix A.

B. Learning MPS from Data

Once the MPS form of wavefunction Ψ(v) is chosen, learn-
ing can be achieved by adjusting parameters of the wave
function such that the distribution represented by Born’s rule
Eq. (1) is as close as possible to the data distribution. A stan-
dard learning method is called Maximum Likelihood Estima-
tion which deﬁnes a (negative) log-likelihood function and op-
timizes it by adjusting the parameters of the model. In our
case, the negative log-likelihood (NLL) is deﬁned as

=

L

−

1

(cid:88)

|T |

v

∈T

ln P(v),

(4)

|T |

where

denotes the size of the training set. Minimizing
the NLL reduces the dissimilarity between the model proba-
bility distribution P(v) and the empirical distribution deﬁned
by the training set.
is
equivalent to minimizing the Kullback-Leibler divergence be-
tween the two distributions [42].

It is well-known that minimizing

L

Armed with canonical form, we are able to diﬀerentiate the
negative log-likelihood (4) with respect to the components of
an order-4 tensor A(k,k+1), which is obtained by contracting
two adjacent tensors A(k) and A(k+1). The gradient reads

∂
L
∂A(k,k+1)wkwk+1
1ik+1
−

ik

=

Z(cid:48)
Z −

2

(cid:88)

|T |

v

∈T

Ψ(cid:48)(v)
Ψ(v)

,

(5)

(cid:80)

v
∈V

where Ψ(cid:48)(v) denotes the derivative of the MPS with respect
to the tensor element of A(k,k+1), and Z(cid:48) = 2
Ψ(cid:48)(v)Ψ(v).
Note that although Z and Z(cid:48) involve summations over an ex-
ponentially large number of terms, they are tractable in the
MPS model via eﬃcient contraction schemes [17]. In partic-
ular, if the MPS is in the mixed-canonical form [17], Z(cid:48) can
be signiﬁcantly simpliﬁed to Z(cid:48) = 2A(k,k+1)wkwk+1
. The calcula-
tion of the gradient, as well as variant techniques in gradient
descent such as stochastic gradient descent (SGD) and adap-
tive learning rate, are detailed in Appendix B. After gradient
descent, the merged order-4 tensor is decomposed into two
order-3 tensors, and then the procedure is repeated for each
pair of adjacent tensors.

1ik+1
−

ik

The derived algorithm is quite similar to the celebrated
DMRG method with two-site update, which allows us to ad-
just dynamically the bond dimensions during the optimiza-
tion and to allocate computational resources to the important
bonds which represent essential features of data. However we
emphasize that there are key diﬀerences between our algo-
rithm and DMRG:

•

•

•

The loss function of classic DMRG method is usu-
ally the energy, while our loss function, the averaged
NLL (4), is a function of data.

With a huge amount of data, the landscape of the loss
function is typically very complicated so that modern
optimizers developed in the machine learning commu-
nity, such as stochastic gradient descent and learning
rate adapting techniques [43], are important to our al-
gorithm. Since the ultimate goal of learning is optimiz-
ing the performance on the test data, we do not really
need to ﬁnd the optimal parameters minimizing the loss
on the training data. One usually stops training before
reaching the actual minima to prevent overﬁtting.

Our algorithm is data-oriented. It is straightforward to
parallelize over the samples since the operations ap-
plied to them are identical and independent.
In fact,
it is a common practice in modern deep learning frame-
work to parallelize over this so-called ”batch” dimen-
sion [37]. As a concrete example, the GPU implemen-
tation of our algorithm is at least 100 times faster than
the CPU implementation on the full MNIST dataset.

C. Generative Sampling

3

After training, samples can be generated independently ac-
cording to Eq. (1). In other popular generative models, espe-
cially the energy based model such as restricted Boltzmann
machine (RBM) [3], generating new samples is often accom-
plished by running MCMC from an initial conﬁguration, due
to the intractability of the partition function. In our model,
one convenience is that the partition function can be exactly
computed with complexity linear in system size. Our model
enjoys a direct sampling method which generates a sample bit
by bit from one end of the MPS to the other [36]. The detailed
generating process is as follow:

1
−

v1,v2,...,vN

It starts from one end, say the N-th bit. One directly
samples this bit from the marginal probability P(vN) =
(cid:80)
P(v). It is clear that this can be easily performed if
we have gauged all the tensors except A(N) to be left-canonical
because P(vN) =
= A(N)vN
xvN
iN
|
and the normalization factor reads Z = (cid:80)
2. Given
|
the value of the N-th bit, one can then move on to sam-
ple the (N
1)-th bit. More generally, given the bit values
−
vk, vk+1,
, vN, the (k
1)-th bit is sampled according to the
conditional probability

2/Z, where we deﬁne xvN
iN
xvN

vN ∈{

· · ·

0,1

} |

−

1
−

1
−

|

P(vk

vk, vk+1, . . . , vN) =
1|
−

P(vk
1, vk, . . . , vN)
−
P(vk, vk+1 . . . , vN)

.

As a result of the canonical condition, the marginal probability
can be simply expressed as

P(vk, vk+1, . . . , vN) =

xvk,vk+1,...,vN

2/Z.
|

= (cid:80)

1
−

xvk,vk+1,...,vN
1 A(k)vk
has been
1ik
ik
ik
−
settled since the k-th bit is sampled. Schematically, its squared
norm reads

A(N)vN
iN

ik,ik+1,

· · ·

,iN

1
−

···

−

|
A(k+1)vk+1
ikik+1

(6)

(7)

xvk,vk+1,...,vN

2 =

|

|

.

(8)

Multiplying the matrix A(k
ing the squared norm of the resulting vector xvk
ik
(cid:80)
ik

1 from the left, and calculat-
−
=

1,vk,...,vN
−
2
−

, one obtains

1,...,vN
−

1)vk

1
−

−

xvk,vk
ik

1

1 A(k
ik

1)vk
−
2ik
1
−
−

−

−

P(vk

1, vk, ..., vN) =
−

|

xvk

1,vk,...,vN
−

2/Z.

|

(9)

Combining (7, 9) one can compute the conditional probability
(6) and sample the bit vk
1 accordingly. In this way, all the
bit values are successively drawn from the conditional prob-
abilities given all the bits on the right. This procedure gives
a sample strictly obeying the probability distribution of the
MPS.

−

This sampling approach is not limited to generating sam-
ples from scratch in a sequential order. It is also capable of
inference tasks when part of the bits are given. In that case,
the canonicalization trick may not help greatly if there is a
segment of unknown bits sitting between given bits. Neverthe-
less, the marginal probabilities are still tractable because one

can also contract ladder-shaped TN eﬃciently [17, 18]. As
what will be shown in Sec. III, given these ﬂexibilities of the
sampling approach, MPS-based probabilistic modeling can be
applied to image reconstruction and denoising.

D. Features of the model and algorithms

We highlight several salient features of the MPS genera-
tive model and compare it to other popular generative mod-
els. Most signiﬁcantly, MPS has an explicit tractable probabil-
ity density, while still allows eﬃcient learning and inference.
For a system sized N, with prescribed maximal bond dimen-
sion
Dmax, the complexity of training on a dataset of size
|T |
3
N
max). The scaling of generative sampling from a
(
is
|T |
O
2
canonical MPS is
max) if all the bits to be sampled are
connected to the boundaries, otherwise given some segments
the conditional sampling scales as

(N

(N

D

D

O

3
max).

O

D

1. Theoretical Understanding of the Expressive Power

The expressibility of MPS was intensively studied in the
context of quantum physics. The bond dimensions of MPS
put an upper bound on its ability of capturing entanglement
entropy. These solid theoretical understandings of the rep-
resentational power of MPS [17, 18] makes it an appealing
model for generative tasks.

Considering the success of MPS for quantum systems, we
expect a polynomial scaling of the computational resources
for datasets with short-range correlations. Treating dataset of
two dimensional images using MPS is analogous to the appli-
cation of DMRG to two dimensional quantum systems [31].
Although in principle an exact representation of the image
dataset may require exponentially large bond dimensions as
the image resolution increases, at computationally aﬀordable
bond dimensions the MPS may already serve as a good ap-
proximation which captures dominant features of the distribu-
tion.

2. Adaptive Adjustment of Expressibility

Performing optimizations for the two-site tensor instead of
for each tensor individually, allows one to dynamically adjust
the bond dimensions during the learning process. Since for re-
alistic datasets the required bond dimensions are likely to be
inhomogeneous, adjusting them dynamically allocates com-
putational resources in an optimal manner. This situation will
be illustrated clearly using the MNIST data set in Sec. III C,
and in Fig. 4.

Adjustment of the bond dimensions follows the distribution
of singular values in (B5), which is related to the low entan-
glement inductive bias of the MPS representation. Adaptive
adjustment of MPS is advantageous compared to most other
generative models. Because in most cases, the architecture
(which is the main limiting factor of the expressibility of the

4

model) is ﬁxed during the learning procedure, only the param-
eters are tuned. By adaptively tuning the bond dimensions, the
representational power of MPS can grow as it gets more ac-
quainted with the training data. In this sense, adaptive adjust-
ment of expressibility is analogous to the structural learning
of probabilistic graphical models, which is, however, a chal-
lenging task due to discreteness of the structural information.

3. Eﬃcient Computation of Exact Gradients and Log-likelihood

Another advantage of MPS compared to the standard en-
ergy based model is that training can be done with high ef-
ﬁciency. The two terms contributing to the gradient (5) are
analogous to the negative and positive phases in the training of
energy based models [39], where the visible variables are un-
clamped and clamped respectively. In the energy based mod-
els, such as RBM, typical evaluation of the ﬁrst term requires
approximated MCMC sampling [44], or sophisticated mean-
ﬁeld approximations e.g. Thouless-Anderson-Palmer equa-
tions [45]. Fortunately, the normalization factor and its gradi-
ent can be calculated exactly and straightforwardly for MPS.
The exact evaluation of gradients guarantees the associated
stochastic gradient descent unbiased.

In addition to eﬃciency in computing gradients, the unbi-
ased estimate of the log-likelihood and its gradients beneﬁts
signiﬁcantly when compared with classic generative models
such as RBM, where the gradients are approximated due to
the intractability of partition function. First, with MPS we can
optimize the NLL directly, while with RBM the approximate
algorithms such as Contrastive Divergence (CD) is essentially
optimizing a loss function other than NLL. This results in a
fact that some region of conﬁguration space could never be
considered during training RBM and a subsequently poor per-
formance on e.g. denoising and reconstruction. Second, with
MPS we can monitor the training process easily using exact
NLL instead of other quantities such as reconstruction error
or pseudo-lilelihood for RBM, which introduce bias to moni-
toring [46].

4. Eﬃcient Direct Sampling

The approach introduced in Sec. II C allows direct sampling
from the learned probability distribution. This completely
avoids the slowing mixing problem in the MCMC sampling
of energy based models. MCMC randomly ﬂip the bits and
compare the probability ratios for accepting and rejecting the
samples. However, the random walks in the state space can
get stuck in a local minimum, which may bring unexpected
ﬂuctuations of long time correlation to the samples. Some-
times this raises issues to the samplings. As a concrete exam-
ple, consider the case where all training samples are exactly
memorized by both MPS and RBM. This is to say that NLL
, and only training samples
of both models are exactly ln
have ﬁnite probability in both models. While other samples,
even with only one bit diﬀerent, have zero probability. It is
easy to check that our MPS model can generate samples which

|T |

is identical to one of the training samples using approach in-
troduced in Sec. II C. However, RBM will not work at all in
generating samples, as there is no direction that MCMC could
follow for increasing the probability of samplings.

It is known that when graphical models have an appropriate
structure (such as a chain or a tree), the inference can be done
eﬃciently [47, 48], while these structural constraints also limit
the application of graphical models with intractable partition
functions. The MPS model, however, enjoys both the advan-
tages of eﬃcient direct sampling and a tractable partition func-
tion. The sampling algorithm is formally similar to the ones
of autoregressive models [6, 7] though, being able to dynam-
ically adjust its expressibility makes the MPS a more ﬂexible
generative model.

Unlike GAN [11] or VAE [5], the MPS can explicitly gives
tractable probability, which may enable more unsupervised
learning tasks. Moreover, the sampling in MPS works with ar-
bitrary prior information of samples, such as ﬁxed bits, which
supports applications like image reconstruction and denois-
ing. We note that this oﬀers an advantage over the popular
GAN, which easily maps a random vector in the latent space
to the image space, but having diﬃculties in the reverse di-
rection — mapping a vector in the images space to the latent
space as a prior information to sampling.

III. APPLICATIONS

In this section, to demonstrate the ability and features of
the MPS generative modeling, we apply it to several standard
datasets. As a proof of principle, we ﬁrst apply our method
to the toy data set of Bars and Stripes, where some proper-
ties of our model can be characterized analytically. Then we
train MPS as an associative memory to learn random binary
patterns to study properties such as capacity and length depen-
dences. Finally we test our model on the Modiﬁed National
Institute of Standards and Technology database (MNIST) to
illustrate its generalization ability for generating and recon-
structing images of handwritten digits. [49]

A. Bars and Stripes

Bars and Stripes (BS) [50] is a data set containing 4

4 bi-
nary images. Each image has either four-pixel-length vertical
bars or horizontal stripes, but not both. In total there are 30
diﬀerent images in the dataset out of all 216 possible ones, as
shown in Fig. 1a. These images appear with equal probability
in the dataset.This toy problem allows a detailed analysis and
reveals key characteristics of the MPS probabilistic model.

×

×

To use MPS for modeling, we unfold the 4

4 images into
one dimensional vectors as shown in Fig. 1b. After being
trained over 4 loops of batch gradient descent training the cost
function converges to its minimum value, which equals to the
= ln(30), within an ac-
Shannon entropy of the BS dataset
10. Here what the MPS has accomplished is
curacy of 1
memorizing the thirty images rigidly, by increasing the proba-
bility of the instances appeared in the dataset, and suppressing

10−

S

×

5

(a)

(b)

FIG. 1: (a) The Bars and Stripes dataset. (b) Ordering of the
pixels when transforming the image into one dimensional
vector. The numbers between pixels indicate the bond
dimensions of the well-trained MPS.

the probability of not-shown instances towards zero. We have
checked that the result is insensitive to the choice of hyperpa-
rameters.

The bond dimensions of the learned MPS have been anno-
tated in Fig. 1b. It is clear that part of the symmetry of the data
set has been preserved. For instance, the 180 rotation around
the center or the transposition of the second and the third rows
would change neither the data set nor the bond dimension dis-
tribution. Open boundary condition results in the decrease of
bond dimensions at both ends. In fact when conducting SVD
at bond k, there are at most 2min(k,N
k) non-zero singular val-
−
ues because the two parts linked by bond k have their Hilbert
spaces of dimension 2k, 2N
k. In addition, the turnings bonds
−
D12 = 15)
have slightly smaller bond dimension (
than others inside the second row and the third row, which can
be explained qualitatively as these bonds carrying less entan-
glement than the bonds in the bulk.

D4 =

D8 =

One can directly write down the exact “quantum wave func-
tion” of the BS dataset, which has ﬁnite and uniform ampli-
tudes for the training images and zero amplitude for other im-
ages. For division on each bond, one can construct the re-
duced density matrix whose eigenvalues are the square of the
singular values. Analyzed in this way, it is conﬁrmed that the
trained MPS achieves the minimal number of required bond
dimension to exactly describe the BS dataset.

We have generated Ns = 106 independent samples from the
learned MPS. All these samples are training images shown in
Fig. 1a. Carrying out likelihood ratio test [51], we got the
n j
log-likelihood ratio statistic G2 = 2NsDKL(
) = 22.0,
p j}
Ns }||{
{
n j
5. The reason for
) = 1.10
equivalently DKL(
10−
Ns }||{
adopting this statistic is that it is asymptotically χ2-distributed
[51]. The p-value of this test is 0.820, which indicates a
high probability that the uniform distribution holds true for
the sampling outcomes.

p j}

×

{

n j
Ns }||{

{

p j}

Note that DKL(

) quantiﬁes the deviation from the
expected distribution to the sampling outcomes, so it reﬂects
the performance of sampling method rather than merely the
training performance.
In contrast to our model, for energy
based models one typically has to resort to MCMC method for

sampling new patterns. It suﬀers from slow mixing problem
since various patterns in the BS dataset diﬀers substantially
and it requires many MCMC steps to obtain one independent
pattern.

B. Random patterns

Capacity represents how much about data could be learned
by the model. Usually it is evaluated using randomly gener-
ated patterns as data. For the classic Hopﬁeld model [12] with
pairwise interactions given by Hebb’s rule among N
→ ∞
variables, it has been shown [52] that in the low-temperature
region at the thermodynamic limit there is the retrieval phase
|T |c = 0.14N random binary patterns could
where at most
be remembered. In this sense, each sample generated by the
model has a large overlap with one of the training pattern.
If the number of patterns in the Hopﬁeld model is larger than
|T |c, the model would enter the spin glass state where samples
generated by the model are not correlated with any training
pattern.

Thanks to the tractable evaluation of the partition function
Z in MPS, we are able to evaluate exactly the likelihood of
every training pattern. Thus the capability of the model can
be easily characterized by the mean negative log-likelihood
with varying

. In this section we focus on the behavior of

L
number of training samples and varying system sizes.

L

L

L

|T |

In Fig. 2a we plot

as a function of number of patterns
Dmax.
used for training for several maximal bond dimension
= ln
for training set
The ﬁgure shows that we obtain
no larger than
Dmax. As what has been shown in the previous
section, this means that all training patterns are remembered
exactly. As the number of training patterns increases, MPS
with a ﬁxed
Dmax will eventually fail in remembering exactly
all the training patterns, resulting to
. In this regime
L
generations of the model usually deviate from training pat-
terns (as illustrated in Fig. 3 on the MNIST dataset). We no-
increasing, the curves in the ﬁgure deviate
tice that with
continuously. We note this is very diﬀerent from
from ln
the Hopﬁeld model where the overlap between the generation
and training samples changes abruptly due to the ﬁrst order
transition from the retrieval phase to spin glass phase.

> ln

|T |

|T |

|T |

|T |

Fig. 2a also shows that a larger

Dmax enables MPS to re-
member exactly more patterns, and produce smaller
with
the number of patterns
ﬁxed. This is quite natural because
Dmax amounts to the increase of parameter number
enlarging
of the model, hence enhances the capacity of the model. In
Dmax =
principle if
our model has inﬁnite capacity, since
arbitrary quantum states can be decomposed into MPS [17].
Clearly this is an advantage of our model over the Hopﬁeld
model and inverse Ising model [14], whose maximal model
capacity is proportional to system size.

∞

L

Careful readers may complain that the inverse Ising model
is not the correct model to compare with, because its varia-
tion with hidden variables, i.e. Boltzmann machines, do have
inﬁnite representation power. Indeed, increasing the bond di-
mensions in MPS has similar eﬀects to increasing the number
of hidden variables in other generative models.

6

(a)

(b)

FIG. 2: NLL averaged as a function of: (a) number of
random patterns used for training, with system size N = 20.
= 100 random patterns.
(b) system size N, trained using
In both (a) and (b), diﬀerent symbols correspond to diﬀerent
Dmax. Each data point is
values of maximal bond dimension
averaged over 10 random instances (i.e. sets of random
patterns), error bars are also plotted, although they are much
smaller than symbol size. The black dashed lines in ﬁgures

|T |

denote

= ln

.

|T |

L

L

L

|T |

In Fig. 2b we plot

as a function of system size N, trained
= 100 random patterns. As shown in the ﬁgure that
on
increases linearly with system size N,
with
Dmax ﬁxed
which indicates that our model gives worse memory capability
with a larger system size. This is due to the fact that keeping
joint-distribution of variables becomes more and more diﬃ-
cult for MPS when the number of variables increases, espe-
cially for long-range correlated data. This is a drawback of our
model when compared with fully pairwise-connected models
such as the inverse Ising model, which is able to capture long-
distance correlations of the training data easily. Fortunately
Fig. 2b also shows that the decay of memory capability with
system size can be compensated by increasing

Dmax.

C. MNIST dataset of handwritten digits

In this subsection we perform experiments on the MNIST
dataset [53]. In preparation we turn the grayscale images into
binary numbers by threshold binarization and ﬂattened the im-
ages row by row into a vector. For the purpose of unsupervised
generative modeling we do not need the labels of the digits.
Here we further test the capacity of the MPS for this larger-
scale and more meaningful dataset. Then we investigate its
generalization ability via examining its performance on a sep-
arated test set, which is crucial for generative modeling.

1. Model Capacity

|T |

Having chosen

= 1000 MNIST images, we train the
MPS with diﬀerent maximal bond dimensions
Dmax, as shown
decreases to its mini-
Dmax increases, the ﬁnal
in Fig. 3. As
mum ln
, and the images generated become more and more
clear. It is interesting that with a relatively small maximum
Dmax = 100, some crucial features show
bond dimension, e.g.

|T |

L

7

FIG. 3: NLL averaged of a MPS trained using

= 1000

×

MNIST images of size 28

dimension
Shannon entropy of the training set ln
minimal value of
MPS’ trained with diﬀerent

28, with varying maximum bond
Dmax. The horizontal dashed line indicates the
, which is also the
|T |
. The inset images are generated by the
Dmax (annoted by the arrows).

L

|T |

up, though some of the images were not as clear as the orig-
inal ones. For instance the hooks and the loops that partly
resembled to “2”, “3” and “9” emerge. These clear charac-
ters of handwritten digits illustrate that the MPS has learned
many “prototypes”. Similar feature-to-prototype transition in
pattern recognitions could also be observed by using a many-
body interaction in the Hopﬁeld model, or equivalently using
a higher-order rectiﬁed polynomial activation function in the
deep neural networks [54]. It is remarkable that in our model
this can be achieved by simply adjusting the maximum bond
dimension of the MPS.

Next we train another model with the restriction of

Dmax =
800. The NLL on the training dataset reach 16.8, and many
bonds have reached maximal dimension
Dmax. Fig. 4 shows
the distribution of bond dimensions. Large bond dimensions
concentrated in the center of the image, where the variation of
the pixels is complex. The bond dimensions around the top
and bottom edge of the image remain small, because those
pixels are always inactivated in the images. They carry no in-
formation and has no correlations with the remaining part of
the image. Remarkably, although the pixels on the left and
right edges are also white, they also have large bond dimen-
sions because these bonds learn to mediate the correlations
between the rows of the images.

The samples directly generated after training are shown in
Fig. 5a. We also show a few original samples from the training
set in Fig. 5b for comparison. Although many of the generated
images cannot be recognized as digits, some aspects of the re-
sult are worth mentioning. Firstly, the MPS learned to leave
margins blank, which is the most obvious common feature in
MNIST database. Secondly, the activated pixels compose pen
strokes that can be extracted from the digits. Finally, a few of
the samples could already be recognized as digits. Unlike the
discriminative learning task carried out in [32], it seems we
need to use much larger bond dimensions to achieve a good
performance in the unsupervised task. We postulate the rea-

FIG. 4: Bond dimensions of the MPS trained with
= 1000 MNIST samples, constrained to

Dmax = 800.
|T |
Final average NLL reaches 16.8. Each pixel in this ﬁgure
corresponds to bond dimension of the right leg of the tensor
associated to the identical coordinate in the original image.

son to be that in the classiﬁcation task, local features of an
image are suﬃcient for predicting the label.Thus MPS is not
required to remember longer-range correlation between pix-
els. For generative modeling, however, it is necessary because
learning the joint distribution from the data consists of (but
not limited to) learning two-point correlations between pairs
of variables that could be far from each other.

With the MPS restricted to

Dmax = 800 and trained with
1000, we carry out image restoration experiments. As shown
in Fig. 6 we remove part of the images in Fig. 5b and then
reconstruct the removed pixels (in yellow) using conditional
direct sampling. For column reconstruction, its performance
is remarkable. The reconstructed images in Fig. 6a are almost
identical to the original ones in Fig. 5b. On the other hand, for
row reconstruction in Fig. 6b, it makes interesting but reason-
able deviations. For instance, the rightmost in the ﬁrst row, an
“1” has been bent to “7”.

(a) Generated

(b) Original

FIG. 5: (a) Images generated from the same MPS as in Fig. 4.
(b) Original images randomly selected from the training set.

8

,

×

L

|T |

|T |

|T |

104, test

For diﬀerent

for training images always decrease
monotonically to diﬀerent minima, and with a ﬁxed
Dmax it
for
is easier for the MPS to ﬁt fewer training images. The
L
= 103,
test images, however, behaves quite diﬀerently: for
decreases to about 40.26 then starts climbing quickly,
test
L
= 104 the test
decreases to 33.65 then in-
while for
L
creases slowly to 34.18. For
= 6
kept de-
|T |
creasing in 75 loops. The behavior shown in Fig. 7 is quite
typical in machine learning problems. When training data is
not enough, the model quickly overﬁts the training data, giv-
ing worse and worse generalization to the unseen test data. An
extreme example is that if our model is able to decrease train-
ing
, i.e. completely overﬁts the training data, then
all other images, even the images with only one pixel diﬀer-
ent from one of the training images, have zero probability in
. We also observe that the best test
the model hence
NLL decreases as training set volume enlarges, which means
the tendency of memorizing is constrained and that of gener-
alization is enhanced.

to ln

|T |

∞

L

L

L

=

The histograms of log-likelihoods for all training and test
images are shown in Fig. 8. Notice that if the model just
memorized some of the images and ignored the others, the
histograms would be bi-modal. It is not the case, as shown
in the ﬁgure, where all distributions are centered around. This
indicates that the model learns all images well rather than con-
centrates on some images while completely ignoring the oth-
ers. In the bottom panel we show the detailed
histogram by
categories. For some digits, such as “1” and “9”, the diﬀer-
ence between training and test log-likelihood distribution is
insigniﬁcant, which suggests that the model has particularly
great generalization ability to these images.

L

FIG. 7: Evolution of the average negative log-likelihood
L
for both training images (blue, bottom lines) and 104 test
images (red, top lines) during training. From left to right,
are 103, 104, and
number of images in the training set
6

|T |
104 respectively.

×

IV. SUMMARY AND OUTLOOK

We have presented a tensor-network-based unsupervised
model, which aims at modeling the probability distribution
of samples in given unlabeled data. The probabilistic model
is structured as a matrix product state, which brings several

(a) column reconstruction on
training images

(b) row reconstruction on training
images

(c) column reconstruction on test
images

(d) row reconstruction on test
images

FIG. 6: Image reconstruction from partial images by direct
sampling with the same MPS as in Fig. 4. (a,b) Restoration
of images in Fig. 5b which are selected from the training set .
(c,d) Reconstruction of 16 images chosen from the test set.
The test set contains images from the MNIST database that
were not used for training. The given parts are in black and
the reconstructed parts are in yellow. The reconstructed parts
are: 12 columns from either (a,c) the left or the right, and
(b,d) the top or the bottom.

2. Generalization Ability

In a glimpse of its generalization ability, we also tried re-
constructing MNIST images other than the training images,
as shown in Fig. 6c, 6d. These results indicate that the MPS
has learned crucial features of the dataset, rather than merely
memorizing the training instances. In fact, even as early as
only 11 loops trained, the MPS could perform column recon-
struction with similar image quality, but its row reconstruction
performance was much worse than that trained over 251 loops.
It is reﬂected that the MPS has learned about short range pat-
terns within each row earlier than those with long range cor-
relations between diﬀerent rows, since the images have been
ﬂattened into a one dimensional vector row by row.

L

To further illustrate our model’s generalization ability, in
for the same 104 test images after training
Fig. 7 we plotted
on diﬀerent numbers of images. To save computing time we
14. The rescaling
worked on rescaled images of size 14
has also been adopted by past works, and it is shown that the
classiﬁcation on the rescaled images is still comparable with
those obtained using other popular methods [32].

×

9

wardly generalized for higher dimensional data. One can also
follow [32, 58] to use a local feature map to lift continuous
variables to a spinor space for continuous data modeling. The
ability and eﬃciency of this approach may also depend on the
speciﬁc way of performing the mapping, so in terms of contin-
uous input there are still a lot to be explored on this algorithm.
Moreover, for colored images one can encode the RGB values
to three physical legs of each MPS tensor.

Similar to using MPS for studying two-dimensional quan-
tum lattice problems [31], modeling images with MPS faces
the problem of introducing long range correlations for some
neighboring pixels in two dimension. An obvious general-
ization of the present approach is to use more expressive TN
with more complex structures. In particular, the projected en-
tangled pair states (PEPS) [59] is particularly suitable for im-
ages, because it takes care of correlation between pixels in
two-dimension. Similar to the studies of quantum systems
in 2D, however, this advantage of PEPS is partially compen-
sated by the diﬃculty of contracting the network and the lose
of convenient canonical forms. Exact contraction of a PEPS
is #P hard [60]. Nevertheless, one can employ tensor renor-
malization group methods for approximated contraction of
PEPS [61–64]. Thus, it remains to be seen whether judicious
combination of these techniques really brings a better perfor-
mance to generative modeling.

In the end, we would like to remark that perhaps the most
exciting feature of quantum-inspired generative models is the
possibility of being implemented by quantum devices [65],
rather than merely being simulated in classical computers. In
that way, neither the large bond dimension nor the high com-
putational complexity of tensor contraction, would be a prob-
lem. The tensor network representation of probability may
facilitate quantum generative modeling because some of the
tensor network states can be prepared eﬃciently on a quan-
tum computer [66, 67].

FIG. 8: (Top) Distribution of

ln p of 60000 training images

−
and 10000 test images given by a trained MPS with
Dmax = 500. The training negative log likelihood
Ltest = 30.3. (Bottom)
Ltrain = 24.2, and the test
Distributions for each digit.

advantages as discussed in Sec. II D, such as adaptive and ef-
ﬁcient learning and direct sampling.

Since we use square of the TN states to represent probabil-
ity, the sign is redundant for probabilistic modeling besides
the gauge freedom of MPS. It is likely that during the opti-
mization MPS develops diﬀerent signs for diﬀerent conﬁgu-
rations. The sign variation may unnecessarily increase the en-
tanglement in MPS and therefore the bond dimensions [55].
However, restricting the sign of MPS may also impair the ex-
pressibility of the model. One probable approach to obtain a
low entanglement representation is adding a penalty term in
the target function, for instance, a term proportional to R´enyi
entanglement entropy as in our further work on quantum to-
mography [56]. In light of these discussions, we would like
point to future research on the diﬀerences and connections of
MPS with non-negative matrix entries [57] and the probabilis-
tic graphical models such as the hidden Markov model.

Binary data modeling links closely to quantum many-body
systems with spin-1/2 constituents and could be straightfor-

ACKNOWLEDGMENTS

We thank Liang-Zhu Mu, Hong-Ye Hu, Song Cheng, Jing
Chen, Wei Li, Zhengzhi Sun and E. Miles Stoudenmire for
inspiring discussions. We also acknowledge suggestions from
anonymous reviewers. P.Z. acknowledges Swarm Club work-
shop on “Geometry, Complex Network and Machine Learn-
ing” sponsored by Kai Feng Foundation in 2016. L.W. is sup-
ported by the Ministry of Science and Technology of China
under the Grant No. 2016YFA0300603 and National Natural
Science Foundation of China under the Grant No. 11774398.
J.W. is supported by National Training Program of Innovation
for Undergraduates of China. P.Z. is supported by Key Re-
search Program of Frontier SciencesCASGrant No. QYZDB-
SSW-SYS032 and Project 11747601 of National Natural Sci-
ence Foundation of China. Part of the computation was car-
ried out at the High Performance Computational Cluster of
ITP, CAS.

10

[1] Yann LeCun, Yoshua Bengio,

and Geoﬀrey Hinton, “Deep

learning,” Nature 521, 436–444 (2015).
[2] David H. Ackley, Geoﬀrey E. Hinton,

and Terrence J. Se-
jnowski, “A Learning Algorithm for Boltzmann Machines,”
Cognitive Science 9, 147–169 (1985).

[3] P Smolensky, “Information processing in dynamical systems:
foundations of harmony theory,” in Parallel distributed process-
ing: explorations in the microstructure of cognition, vol. 1 (MIT
Press, 1986) pp. 194–281.

[4] Ruslan Salakhutdinov, “Learning deep generative models,” An-
nual Review of Statistics and Its Application 2, 361–385 (2015).
[5] Diederik P Kingma and Max Welling, “Auto-encoding varia-

tional bayes,” arXiv:1312.6114 (2013).

[6] Benigno Uria, Marc-Alexandre Cˆot´e, Karol Gregor, Iain Mur-
ray, and Hugo Larochelle, “Neural autoregressive distribution
estimation,” Journal of Machine Learning Research 17, 1–37
(2016).

[7] Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu,
“Pixel recurrent neural networks,” in Proceedings of The 33rd
International Conference on Machine Learning, Proceedings of
Machine Learning Research, Vol. 48 (PMLR, New York, New
York, USA, 2016) pp. 1747–1756.
[8] Laurent Dinh, David Krueger,

and Yoshua Bengio,
independent components estimation,”

“NICE: non-linear
arXiv:1410.8516 (2014).

[9] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio, “Den-
sity estimation using real NVP,” arXiv:1605.08803 (2016).
[10] Danilo Rezende and Shakir Mohamed, “Variational inference
with normalizing ﬂows,” in Proceedings of the 32nd Interna-
tional Conference on Machine Learning, Proceedings of Ma-
chine Learning Research, Vol. 37 (PMLR, Lille, France, 2015)
pp. 1530–1538.

[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,
and
David Warde-Farley, Sherjil Ozair, Aaron Courville,
Yoshua Bengio, “Generative adversarial nets,” in Advances in
Neural Information Processing Systems 27 (Curran Associates,
Inc., 2014) pp. 2672–2680.

[12] J. J. Hopﬁeld, “Neural networks and physical systems with

emergent collective computational abilities,” (1982) p. 2554.

[13] Hilbert J. Kappen and Francisco de Borja Rodr´ıguez Ortiz,
“Boltzmann machine learning using mean ﬁeld theory and lin-
ear response correction,” in Advances in Neural Information
Processing Systems 10 (MIT Press, 1998) pp. 280–286.
[14] Y. Roudi, J. Tyrcha, and J. Hertz, “Ising model for neural data:
Model quality and approximate methods for extracting func-
tional connectivity,” Phys. Rev. E 79, 051915 (2009).

[15] W. M. C. Foulkes, L. Mitas, R. J. Needs, and G. Rajagopal,
“Quantum monte carlo simulations of solids,” Rev. Mod. Phys.
73, 33–83 (2001).

[16] Giuseppe Carleo and Matthias Troyer, “Solving the quantum
many-body problem with artiﬁcial neural networks,” Science
355, 602–606 (2017).

[17] Ulrich Schollw¨ock, “The density-matrix renormalization group
in the age of matrix product states,” Annals of Physics 326, 96–
192 (2011).

[18] Rom´an Or´us, “A practical introduction to tensor networks: Ma-
trix product states and projected entangled pair states,” Annals
of Physics 349, 117 – 158 (2014).

[19] M B Hastings, “An area law for one-dimensional quantum sys-
tems,” Journal of Statistical Mechanics: Theory and Experi-
ment 2007, P08024–P08024 (2007).

[20] J. Eisert, M. Cramer, and M. B. Plenio, “Colloquium: Area
laws for the entanglement entropy,” Reviews of Modern Physics
82, 277–306 (2010), arXiv:0808.3773.

[21] Johann A. Bengua, Ho N. Phien, and Hoang Duong Tuan, “Op-
timal feature extraction and classiﬁcation of tensors via matrix
product state decomposition,” arXiv:1503.00516 (2015).
[22] Alexander Novikov, Anton Rodomanov, Anton Osokin, and
Dmitry Vetrov, “Putting MRFs on a tensor train,” International
Conference on Machine Learning,

, 811–819 (2014).

[23] Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and
Dmitry P Vetrov, “Tensorizing neural networks,” in Advances
in Neural Information Processing Systems (2015) pp. 442–450.
and Amnon Shashua, “On the
expressive power of deep learning: A tensor analysis,”
arXiv:1509.05009 (2015).

[24] Nadav Cohen, Or Sharir,

[25] Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh-Huy Phan,
Qibin Zhao, Danilo P Mandic, et al., “Tensor networks for
dimensionality reduction and large-scale optimization: Part 1
in
low-rank tensor decompositions,” Foundations and Trends R
(cid:13)
Machine Learning 9, 249–429 (2016).

[26] Y. Levine, D. Yakira, N. Cohen,

and A. Shashua, “Deep
Learning and Quantum Entanglement: Fundamental Connec-
tions with Implications to Network Design,” arXiv:1704.01552
(2017).

[27] D Perez-Garcia, F Verstraete, M M Wolf, and J I Cirac, “Ma-
trix Product State Representations,” Quantum Info. Comput. 7,
401–430 (2007).

[28] Ivan V Oseledets, “Tensor-train decomposition,” SIAM Journal

on Scientiﬁc Computing 33, 2295–2317 (2011).

[29] Zeph Landau, Umesh Vazirani, and Thomas Vidick, “A poly-
nomial time algorithm for the ground state of 1d gapped local
hamiltonians,” Nature Physics 11, 566 (2015).

[30] Steven R. White, “Density matrix formulation for quan-
tum renormalization groups,” Phys. Rev. Lett. 69, 2863–2866
(1992).

[31] E.M. Stoudenmire and Steven R. White, “Studying two-
dimensional systems with the density matrix renormalization
group,” Annual Review of Condensed Matter Physics 3, 111–
128 (2012).

[32] E. Miles Stoudenmire and David J. Schwab, “Supervised
Learning with Quantum-Inspired Tensor Networks,” Advances
in Neural Information Processing Systems 29, 4799 (2016),
arXiv:1605.05775.

[33] Alexander Novikov, Mikhail Troﬁmov, and Ivan Oseledets,

“Exponential machines,” arXiv:1605.05775 (2016).

[34] Angel J Gallego and Roman Orus, “The physical structure
of grammatical correlations: equivalences, formalizations and
consequences,” arXiv:1708.01525 (2017).

[35] Jing Chen, Song Cheng, Haidong Xie, Lei Wang, and Tao Xi-
ang, “Equivalence of restricted boltzmann machines and tensor
network states,” Phys. Rev. B 97, 085104 (2018).

[36] Andrew J. Ferris and Guifre Vidal, “Perfect sampling with uni-
tary tensor networks,” Phys. Rev. B 85, 165146 (2012).

[37] Ian Goodfellow, Yoshua Bengio,
Press,

Deep
Learning
(MIT
deeplearningbook.org.

and Aaron Courville,
http://www.
2016)

[38] M. Born,

“Zur Quantenmechanik

der Stoßvorg¨ange,”

Zeitschrift fur Physik 37, 863–867 (1926).

[39] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ran-
and F Huang, “A tutorial on energy-based learn-
(2006), http://yann.lecun.com/exdb/publis/

zato,
ing,”

orig/lecun-06.pdf (Accessed on 2-September-2017).
[40] Ming-Jie Zhao and Herbert Jaeger, “Norm-observable operator
models,” Neural Computation 22, 1927–1959 (2010), pMID:
20141473, https://doi.org/10.1162/neco.2010.03-09-983.
[41] Raphael Bailly, “Quadratic weighted automata:spectral algo-
rithm and likelihood maximization,” in Proceedings of the
Asian Conference on Machine Learning, Proceedings of Ma-
chine Learning Research, Vol. 20, edited by Chun-Nan Hsu
and Wee Sun Lee (PMLR, South Garden Hotels and Resorts,
Taoyuan, Taiwain, 2011) pp. 147–163.

[42] S. Kullback and R. A. Leibler, “On Information and Suf-
ﬁciency,” The Annals of Mathematical Statistics 22, 79–86
(1951).

[43] Diederik P Kingma and Jimmy Ba, “Adam: A method
for stochastic optimization,” arXiv preprint arXiv:1412.6980
(2014).

[44] Geoﬀrey E Hinton, “A practical guide to training restricted
boltzmann machines,” in Neural networks: Tricks of the trade
(Springer, 2012) pp. 599–619.

[45] Marylou Gabrie, Eric W Tramel, and Florent Krzakala, “Train-
ing restricted boltzmann machine via the thouless-anderson-
palmer free energy,” in Advances in Neural Information Pro-
cessing Systems 28, edited by C. Cortes, N. D. Lawrence, D. D.
Lee, M. Sugiyama, and R. Garnett (Curran Associates, Inc.,
2015) pp. 640–648.

[46] Aapo Hyv¨arinen, “Consistency of pseudolikelihood estimation
of fully visible boltzmann machines,” Neural Computation 18,
2283–2292 (2006).

[47] Martin J Wainwright and Michael I Jordan, “Graphical mod-
els, exponential families, and variational inference,” Founda-
tions and Trends R
(cid:13)

in Machine Learning 1, 1–305 (2008).

[48] Daphne Koller and Nir Friedman, Probabilistic Graphical
Models, Principles and Techniques (The MIT Press, 2009).
[49] The code of these experiments have been posted at https://

github.com/congzlwag/UnsupGenModbyMPS.

[50] David JC MacKay, Information theory, inference and learning

algorithms (Cambridge University Press, 2003).

[51] S. S. Wilks, “The large-sample distribution of the likelihood
ratio for testing composite hypotheses,” Ann. Math. Statist. 9,
60–62 (1938).

[52] D.J. Amit, H. Gutfreund,

and H. Sompolinsky, “Spin-glass

models of neural networks,” Phys. Rev. A 32, 1007 (1985).
[53] Yann LeCun, Corinnai Cortes, and Christopher J.C. Burges,
“The mnist database of handwritten digits,” (1998), http://
yann.lecun.com/exdb/mnist (Accessed on 2-September-
2017).

[54] Dmitry Krotov and John J Hopﬁeld, “Dense associative mem-
ory for pattern recognition,” in Advances in Neural Information
Processing Systems (2016) pp. 1172–1180.

[55] Yi Zhang, Tarun Grover, and Ashvin Vishwanath, “Entangle-
ment entropy of critical spin liquids,” Phys. Rev. Lett. 107,
067202 (2011).

[56] Han Zhao-Yu, Wang Jun, Wang Song-Bo, Li Ze-Yang,
and Lei Wang, “Eﬃcient quan-
Mu Liang-Zhu, Fan Heng,
tum tomography with ﬁdelity estimation,” arXiv:1712.03213
(2017).

[57] Kristan Temme and Frank Verstraete, “Stochastic matrix prod-

uct states,” Phys. Rev. Lett. 104, 210502 (2010).

[58] Edwin Miles Stoudenmire, “Learning relevant features of data
with multi-scale tensor networks,” Quantum Science and Tech-
nology (2018).

[59] Frank Verstraete and Juan Ignacio Cirac, “Renormalization al-
gorithms for quantum-many body systems in two and higher
dimensions,” arXiv preprint cond-mat/0407066 (2004).

11

[60] Norbert Schuch, Michael M. Wolf, Frank Verstraete,

and
Juan I. Cirac, “Computational complexity of projected entan-
gled pair states,” Phys. Rev. Lett. 98, 140506 (2007).

[61] Michael Levin and Cody P. Nave, “Tensor renormalization
group approach to two-dimensional classical lattice models,”
Phys. Rev. Lett. 99, 120601 (2007).

[62] Z. Y. Xie, H. C. Jiang, Q. N. Chen, Z. Y. Weng, and T. Xiang,
“Second renormalization of tensor-network states,” Phys. Rev.
Lett. 103, 160601 (2009).

[63] Chuang Wang, Shao-Meng Qin, and Hai-Jun Zhou, “Topo-
logically invariant tensor renormalization group method for
the edwards-anderson spin glasses model,” Phys. Rev. B 90,
174201 (2014).

[64] G. Evenbly and G. Vidal, “Tensor network renormalization,”

Phys. Rev. Lett. 115, 180405 (2015).

[65] Alejandro Perdomo-Ortiz, Marcello Benedetti, John Realpe-
Gmez, and Rupak Biswas, “Opportunities and challenges for
quantum-assisted machine learning in near-term quantum com-
puters,” Quantum Science and Technology 3, 030502 (2018).

[66] Martin Schwarz, Kristan Temme,

and Frank Verstraete,
“Preparing projected entangled pair states on a quantum com-
puter,” Phys. Rev. Lett. 108, 110502 (2012).

[67] William Huggins, Piyush Patel, K Birgitta Whaley, and E Miles
Stoudenmire, “Towards quantum machine learning with tensor
networks,” arXiv preprint arXiv:1803.11537 (2018).

[68] Setting

Dk = 1 for all bonds makes the bond dimension diﬃcult
to grow in the initial training phase. Since the rank of two site
1 and the number of nonzero singular value
tensor is 1
Dk = 1 with
is at most 2, which is likely to be truncated back to
small cutoﬀ.

×

×

×

2

2

Appendix A: Canonical conditions for MPS and computation of
the partition function

The MPS representation has gauge degrees of freedom,
which means that the state is invariant after inserting identity
1 on each bond (M can be diﬀerent on each bond).
I = MM−
Exploiting the gauge degrees of freedom, one can bring the
MPS into its canonical form: for example, the tensor A(k) is
(cid:17)
† A(k)vk = I.
called left-canonical if it satisﬁes
In diagrammatic notation, the left-canonical condition reads

A(k)vk

vk∈{

(cid:80)

0,1

(cid:16)

}

=

(A1)

The right-canonical condition is deﬁned analogously. Canoni-
calization of each tensor can be done locally and only involves
the single tensor at consideration [17, 18].

Each tensor in the MPS can be in a diﬀerent canonical
form. For example, given a speciﬁc site k, one can con-
duct gauge transformation to make all the tensors on the left,
A(i)
, left-canonical and tensors on the right,
{
A(i)
, right-canonical, while leaving
{
A(k) neither left-canonical nor right-canonical. This is called
mixed-canonical form of the MPS [17]. The normalization of
the MPS is particularly easy to compute in the canonical from.

i = 1, 2,
1
}
· · ·
i = k + 1, k + 2,
· · ·

, N

, k

−

|
|

}

In the graphical notation, it reads

Z =

=

.

(A2)

We note that even if the MPS is not in the canonical form, its
normalization factor Z can be still computed eﬃciently if one
pays attention to the order of contraction [17, 18].

Appendix B: DMRG-like Gradient Descent algorithm for
learning

A standard way of minimization of the cost function (4)
is done by performing the gradient descent algorithm on the
MPS tensor elements. Crucially, our method allows dynam-
ical adjustment of the bond dimension during the optimiza-
tion, thus being able to allocate resources to the spatial regions
where correlations among the physical variables are stronger.
Initially, we set the MPS with random tensors with small
bond dimensions. For example, all the bond dimension are
Dk = 2 except those on the boundaries [68]. We then
set to
carry out the canonicalization procedure so that all the ten-
sors except the rightmost one A(N) are left-canonical. Then,
we sweep through the matrices back and forth to tune the el-
ements of the tensors, i.e.
the parameters of the MPS. The
procedure is similar to the DMRG algorithm with two-site up-
date where one optimizes two adjacent tensors at a time [30].
At each step, we ﬁrstly merge two adjacent tensors into an
order-4 tensor,

=

,

(B1)

followed by adjusting its elements in order to decrease the cost
2. It is straight forward to
function
|
|
check that its gradient with respect to an element of the tensor
(B1) reads

= ln Z

Ψ(v)

v
∈T

(cid:80)

ln

L

−

|T |

1

∂
L
∂A(k,k+1)wkwk+1
1ik+1
−

ik

=

Z(cid:48)
Z −

2

(cid:88)

|T |

v

∈T

Ψ(cid:48)(v)
Ψ(v)

,

where Ψ(cid:48)(v) denotes the derivative of the MPS with respect
Ψ(cid:48)(v)Ψ(v). In diagram
to the tensor (B1), and Z(cid:48) = 2
language, they read

v
∈V

(cid:80)

Ψ(cid:48)(v) =

Z(cid:48)
2

=

=

The direct vertical connections of wk, vk and wk+1, vk+1 in (B3)
stand for Kronecker delta functions δwkvk and δwk+1vk+1 respec-
tively, meaning that only those input data with pattern vkvk+1

12

contribute to the gradient with respect to the tensor elements
A(k,k+1)vkvk+1 . Note that although Z and Z(cid:48) involve summations
over an exponentially large number of terms, they are tractable
in MPS via eﬃcient contraction schemes [17]. In particular, if
the MPS is in the mixed canonical form, the computation only
involves local manipulations illustrated in (B4).

Next, we carry out gradient descent to update the compo-
nents of the merged tensor. The update is ﬂexible and is open
to various gradient descent techniques. Firstly, stochastic gra-
dient descent is considerable. Instead of averaging the gra-
dient over the whole dataset, the second term of the gradi-
ent (B2) can be estimated by a randomly chosen mini-batches
of samples, where the size of the mini-batch mbatch plays a
role of hyperparameter in the training. Secondly, on a speciﬁc
contracted tensor one can conduct several steps of gradient
descent. Note that although the local update of A(k,k+1) does
not change its environment, the shifting of A(k,k+1) makes a
diﬀerence between ndes steps of update with learning rate η
η. Thirdly, especially
and one update step with η(cid:48) = ndes ×
when several steps are conducted on each contracted tensor,
the learning rate (the ratio of the update to the gradient) can
be adaptively tuned by meta-algorithms that such as RMSProp
and Adam [43].

In practice it is observed that sometimes the gradients be-
come very small while it is not in the vicinity of any local
minimum of the landscape. In that case a plateau or a saddle
point may have been encountered, and we simply increase the
learning rate so that the norm of the update is a function of the
dimensions of the contracted tensor.

After updating the order-4 tensor (B1), it is decomposed by
unfolding the tensor to a matrix, subsequently applying sin-
gular value decomposition (SVD), and ﬁnally unfolding ob-
tained two matrices back to two order-3 tensors.

=

=

≈

(B2)

(B3)

(B4)

,

(B5)

where U, V are unitary matrices and Λ is a diagonal ma-
trix containing singular values on the diagonal. The num-
ber of non-vanishing singular values will generally increase
compared to the original value in Eq. (B1) because the MPS
observes correlations in the data and try to capture them.
We truncate those singular values whose ratios to the largest
one are smaller than a prescribed hyperparameter cutoﬀ (cid:15)cut,
along with their corresponding row vectors and column vec-
tors deleted in U and V †.

If the next bond to train on is the (k+1)-th bond on the right,
take A(k) = U so that it is left-canonical, and consequently
A(k+1) = ΛV †. While if the MPS is about to be trained on
1)-th bond, analogously A(k+1) = V † will be right-
the (k
canonical and A(k) = UΛ. This keeps the MPS in mixed-
canonical form.

−

The whole training process consists of many loops.
In
each loop the training starts from the rightmost bond (between

A(N
the rightmost.

1) and A(N)) and sweeps to the leftmost A(1), then back to
−

13

Unsupervised Generative Modeling Using Matrix Product States

Zhao-Yu Han,1,

∗ Jun Wang,1,

∗ Heng Fan,2 Lei Wang,2,

† and Pan Zhang3,

‡

1School of Physics, Peking University, Beijing 100871, China
2Institute of Physics, Chinese Academy of Sciences, Beijing 100190, China
3Institute of Theoretical Physics, Chinese Academy of Sciences, Beijing 100190, China

Generative modeling, which learns joint probability distribution from data and generates samples according
to it, is an important task in machine learning and artiﬁcial intelligence. Inspired by probabilistic interpretation
of quantum physics, we propose a generative model using matrix product states, which is a tensor network
originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys
eﬃcient learning analogous to the density matrix renormalization group method, which allows dynamically
adjusting dimensions of the tensors and oﬀers an eﬃcient direct sampling approach for generative tasks. We
apply our method to generative modeling of several standard datasets including the Bars and Stripes random
binary patterns and the MNIST handwritten digits to illustrate the abilities, features and drawbacks of our
model over popular generative models such as Hopﬁeld model, Boltzmann machines and generative adversarial
networks. Our work sheds light on many interesting directions of future exploration on the development of
quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to be realized
on quantum devices.

I.

INTRODUCTION

Generative modeling, a typical example of unsupervised
learning that makes use of huge amount of unlabeled data,
lies in the heart of rapid development of modern machine
learning techniques [1]. Diﬀerent from discriminative tasks
such as pattern recognition, the goal of generative modeling
is to model the probability distribution of data and thus be
able to generate new samples according to the distribution.
At the research frontier of generative modeling, it is used for
ﬁnding good data representation and dealing with tasks with
missing data. Popular generative machine learning models in-
clude Boltzmann Machines (BM) [2, 3] and their generaliza-
tions [4], variational autoencoders (VAE) [5], autoregressive
models [6, 7], normalizing ﬂows [8–10], and generative ad-
versarial networks (GAN) [11]. For generative model design,
one tries to balance the representational power and eﬃciency
of learning and sampling.

There is a long history of the interplay between generative
modeling and statistical physics. Some celebrated models,
such as Hopﬁeld model [12] and Boltzmann machine [2, 3],
are closely related to the Ising model and its inverse version
which learns couplings in the model based on given training
conﬁgurations [13, 14].

The task of generative modeling also shares similarities
with quantum physics in the sense that both of them try to
model probability distributions in an immense space. Pre-
cisely speaking, it is the wavefunctions that are modeled in
quantum physics, and probability distributions are given by
their squared norm according to Born’s statistical interpreta-
tion. Modeling probability distributions in this way is funda-
mentally diﬀerent from the traditional statistical physics per-
spective. Hence we may refer probability models which ex-

∗ These two authors contributed equally
† wanglei@iphy.ac.cn
‡ panzhang@itp.ac.cn

ploit quantum state representations as “Born Machines”. Var-
ious ansatz have been developed to express quantum states,
such as the variational Monte Carlo [15], the tensor network
(TN) states and recently artiﬁcial neural networks [16] . In
fact physical systems like quantum circuits are also promising
candidates for implementing Born Machines.

In the past decades, tensor network states and algorithms
have been shown to be an incredibly potent tool set for mod-
eling many-body quantum states [17, 18]. The success of TN
description can be theoretically justiﬁed from a quantum in-
formation perspective [19, 20]. In parallel to quantum physics
applications, tensor decomposition and tensor networks have
also been applied in a broader context by the machine learning
community for feature extraction, dimensionality reduction
and analyzing the expressibility of deep neural networks [21–
26].

In particular, matrix product state (MPS) is a kind of TN
where the tensors are arranged in a one-dimensional geome-
try [27]. The same representation is referred as tensor train
decomposition in the applied math community [28]. De-
spite its simple structure, MPS can represent a large num-
ber of quantum states extremely well. MPS representation
of ground states has been proven to be eﬃcient for one-
dimensional gapped local Hamiltonian [29]. In practice, opti-
mization schemes for MPS such as density-matrix renormal-
ization group (DMRG) [30] have been successful even for
some quantum systems in higher dimension [31]. Some recent
works extended the application of MPS to machine learning
tasks like pattern recognition [32], classiﬁcation [33] and lan-
guage modeling [34]. Eﬀorts also drew connection between
Boltzmann Machines and tensor networks [35].

In this paper, building on the connection between unsu-
pervised generative modeling and quantum physics, we em-
ploy MPS as a model to learn probability distribution of given
data with an algorithm which resembles DMRG [30]. Com-
pared with statistical-physics based models such as the Hop-
ﬁeld model [12] and the inverse Ising model, MPS exhibits
much stronger ability of learning, which adaptively grows by
increasing bond dimensions of the MPS. The MPS model also

8
1
0
2
 
l
u
J
 
9
1
 
 
]
h
c
e
m

-
t
a
t
s
.
t
a
m
-
d
n
o
c
[
 
 
3
v
2
6
6
1
0
.

9
0
7
1
:
v
i
X
r
a

enjoys a direct sampling method [36] much more eﬃcient than
the Boltzmann machines, which require Markov Chain Monte
Carlo (MCMC) process for data generation. When compared
with popular generative models such as GAN, our model of-
fers a more eﬃcient way to reconstruct and denoise from an
initial (noisy) input using the direct sampling algorithm, as
opposed to GAN where mapping a noisy image to its input is
not straightforward.

The rest of the paper is organized as follow.

In Sec. II
we present our model, training algorithm and direct sampling
method.
In Sec. III we apply our model to three datasets:
Bars-and-stripes for a proof-of-principle demonstration, ran-
dom binary patterns for capacity illustration and the MNIST
handwritten digits for showing the generalization ability of
the MPS model in unsupervised tasks such as reconstruc-
tionof images. Finally, Sec IV discusses future prospects of
the generative modeling using more general tensor networks
and quantum circuits.

II. MPS FOR UNSUPERVISED LEARNING

The goal of unsupervised generative modeling is to model
the joint probability distribution of given data. With the
trained model, one can then generate new samples from the
learned probability distribution. Generative modeling ﬁnds
wide applications such as dimensional reduction, feature de-
tection, clustering and recommendation systems [37]. In this
consisting of binary strings
paper, we consider a data set
N, which are potentially repeated and can be
v
}⊗
mapped to basis vectors of a Hilbert space of dimension 2N.

∈ V

0, 1

T

=

{

The probabilistic interpretation of quantum mechanics [38]
naturally suggests modeling data distribution with a quantum
state. Suppose we encode the probability distribution into
a quantum wavefunction Ψ(v), measurement will collapse it
and generate a result v = (v1, v2,
, vN) with a probability
2 Inspired by the generative aspects of
proportional to
quantum mechanics, we represent the model probability dis-
tribution by

Ψ(v)
|
|

· · ·

P(v) = |

2

Ψ(v)
|
Z

,

(1)

v
∈V |

Ψ(v)
|

where Z = (cid:80)
2 is the normalization factor. We also
refer it as the partition function to draw an analogy with the
energy based models [39]. In general the wavefunction Ψ(v)
can be complex valued, but in this work we restrict it to be
real valued. Representing probability density using square of
a function was also put forward by former works [32, 40,
41]. These approaches ensure the positivity of probability and
naturally admit a quantum mechanical interpretation.

A. Matrix Product States

Quantum physicists and chemists have developed many ef-
ﬁcient classical representations of quantum wavefunctions.
A number of these developed representations and algorithms

2

can be adopted for eﬃcient probabilistic modeling. Here, we
parametrize the wave function using MPS:

Ψ(v1, v2,

, vN) = Tr

A(1)v1 A(2)v2

· · ·

(cid:16)

A(N)vN

(cid:17)

,

· · ·

(2)

−

(cid:80)N

Dk

k=1 Dk

1 by
−

D0 =

where each A(k)vk is a
DN is
Dk matrix, and
demanded to close the trace. For the the case considered here,
there are 2
1Dk parameters on the right-hand-side of
Eq. (2). The representational power of MPS is related to Von
Neumann entanglement entropy of the quantum state, which
Tr(ρA ln ρA). Here we divide the variables
is deﬁned as S =
into two groups v = (vA, vB) and ρA = (cid:80)
−
vB Ψ(vA, vB)Ψ(v(cid:48)A, vB)
is the reduced density matrix of a subsystem. The entangle-
ment entropy sets a lower bound for the bond dimension at the
Dk). Any probability distribution of a N-bit
division S
system can be described by an MPS as long as its bond dimen-
sions are free from any restriction. The inductive bias using
MPS with limited bond dimensions comes from dropping oﬀ
the minor components of entanglement spectrum. Therefore
as the bond dimension increases, an MPS enhances its ability
of parameterizing complicated functions. See [17] and [18]
for recent reviews on MPS and its applications on quantum
many-body systems.

ln(

≤

In practice, it is convenient to use MPS with

DN = 1
D0 =
and consequently reduce the left and right most matrices to
vectors [30]. In this case, Eq. (2) reads schematically

=

.

(3)

Here the blocks denote the tensors and the connected lines in-
dicate tensor contraction over virtual indices. The dangling
vertical bonds denote physical indices. We refer to [17, 18]
for an introduction to these graphical notations of TN. Hence-
forth, we shall present formulae with more intuitive graphical
notations wherever possible.

The MPS representation has gauge degrees of freedom,
which allows one to restrict the tensors with canonical con-
ditions. We remark that in our setting of generative modeling,
the canonical form signiﬁcantly beneﬁts computing the exact
partition function Z. More details about the canonical condi-
tion and the calculation of Z can be found in Appendix A.

B. Learning MPS from Data

Once the MPS form of wavefunction Ψ(v) is chosen, learn-
ing can be achieved by adjusting parameters of the wave
function such that the distribution represented by Born’s rule
Eq. (1) is as close as possible to the data distribution. A stan-
dard learning method is called Maximum Likelihood Estima-
tion which deﬁnes a (negative) log-likelihood function and op-
timizes it by adjusting the parameters of the model. In our
case, the negative log-likelihood (NLL) is deﬁned as

=

L

−

1

(cid:88)

|T |

v

∈T

ln P(v),

(4)

|T |

where

denotes the size of the training set. Minimizing
the NLL reduces the dissimilarity between the model proba-
bility distribution P(v) and the empirical distribution deﬁned
by the training set.
is
equivalent to minimizing the Kullback-Leibler divergence be-
tween the two distributions [42].

It is well-known that minimizing

L

Armed with canonical form, we are able to diﬀerentiate the
negative log-likelihood (4) with respect to the components of
an order-4 tensor A(k,k+1), which is obtained by contracting
two adjacent tensors A(k) and A(k+1). The gradient reads

∂
L
∂A(k,k+1)wkwk+1
1ik+1
−

ik

=

Z(cid:48)
Z −

2

(cid:88)

|T |

v

∈T

Ψ(cid:48)(v)
Ψ(v)

,

(5)

(cid:80)

v
∈V

where Ψ(cid:48)(v) denotes the derivative of the MPS with respect
to the tensor element of A(k,k+1), and Z(cid:48) = 2
Ψ(cid:48)(v)Ψ(v).
Note that although Z and Z(cid:48) involve summations over an ex-
ponentially large number of terms, they are tractable in the
MPS model via eﬃcient contraction schemes [17]. In partic-
ular, if the MPS is in the mixed-canonical form [17], Z(cid:48) can
be signiﬁcantly simpliﬁed to Z(cid:48) = 2A(k,k+1)wkwk+1
. The calcula-
tion of the gradient, as well as variant techniques in gradient
descent such as stochastic gradient descent (SGD) and adap-
tive learning rate, are detailed in Appendix B. After gradient
descent, the merged order-4 tensor is decomposed into two
order-3 tensors, and then the procedure is repeated for each
pair of adjacent tensors.

1ik+1
−

ik

The derived algorithm is quite similar to the celebrated
DMRG method with two-site update, which allows us to ad-
just dynamically the bond dimensions during the optimiza-
tion and to allocate computational resources to the important
bonds which represent essential features of data. However we
emphasize that there are key diﬀerences between our algo-
rithm and DMRG:

•

•

•

The loss function of classic DMRG method is usu-
ally the energy, while our loss function, the averaged
NLL (4), is a function of data.

With a huge amount of data, the landscape of the loss
function is typically very complicated so that modern
optimizers developed in the machine learning commu-
nity, such as stochastic gradient descent and learning
rate adapting techniques [43], are important to our al-
gorithm. Since the ultimate goal of learning is optimiz-
ing the performance on the test data, we do not really
need to ﬁnd the optimal parameters minimizing the loss
on the training data. One usually stops training before
reaching the actual minima to prevent overﬁtting.

Our algorithm is data-oriented. It is straightforward to
parallelize over the samples since the operations ap-
plied to them are identical and independent.
In fact,
it is a common practice in modern deep learning frame-
work to parallelize over this so-called ”batch” dimen-
sion [37]. As a concrete example, the GPU implemen-
tation of our algorithm is at least 100 times faster than
the CPU implementation on the full MNIST dataset.

C. Generative Sampling

3

After training, samples can be generated independently ac-
cording to Eq. (1). In other popular generative models, espe-
cially the energy based model such as restricted Boltzmann
machine (RBM) [3], generating new samples is often accom-
plished by running MCMC from an initial conﬁguration, due
to the intractability of the partition function. In our model,
one convenience is that the partition function can be exactly
computed with complexity linear in system size. Our model
enjoys a direct sampling method which generates a sample bit
by bit from one end of the MPS to the other [36]. The detailed
generating process is as follow:

1
−

v1,v2,...,vN

It starts from one end, say the N-th bit. One directly
samples this bit from the marginal probability P(vN) =
(cid:80)
P(v). It is clear that this can be easily performed if
we have gauged all the tensors except A(N) to be left-canonical
because P(vN) =
= A(N)vN
xvN
iN
|
and the normalization factor reads Z = (cid:80)
2. Given
|
the value of the N-th bit, one can then move on to sam-
ple the (N
1)-th bit. More generally, given the bit values
−
vk, vk+1,
, vN, the (k
1)-th bit is sampled according to the
conditional probability

2/Z, where we deﬁne xvN
iN
xvN

vN ∈{

· · ·

0,1

} |

−

1
−

1
−

|

P(vk

vk, vk+1, . . . , vN) =
1|
−

P(vk
1, vk, . . . , vN)
−
P(vk, vk+1 . . . , vN)

.

As a result of the canonical condition, the marginal probability
can be simply expressed as

P(vk, vk+1, . . . , vN) =

xvk,vk+1,...,vN

2/Z.
|

= (cid:80)

1
−

xvk,vk+1,...,vN
1 A(k)vk
has been
1ik
ik
ik
−
settled since the k-th bit is sampled. Schematically, its squared
norm reads

A(N)vN
iN

ik,ik+1,

· · ·

,iN

1
−

···

−

|
A(k+1)vk+1
ikik+1

(6)

(7)

xvk,vk+1,...,vN

2 =

|

|

.

(8)

Multiplying the matrix A(k
ing the squared norm of the resulting vector xvk
ik
(cid:80)
ik

1 from the left, and calculat-
−
=

1,vk,...,vN
−
2
−

, one obtains

1,...,vN
−

1)vk

1
−

−

xvk,vk
ik

1

1 A(k
ik

1)vk
−
2ik
1
−
−

−

−

P(vk

1, vk, ..., vN) =
−

|

xvk

1,vk,...,vN
−

2/Z.

|

(9)

Combining (7, 9) one can compute the conditional probability
(6) and sample the bit vk
1 accordingly. In this way, all the
bit values are successively drawn from the conditional prob-
abilities given all the bits on the right. This procedure gives
a sample strictly obeying the probability distribution of the
MPS.

−

This sampling approach is not limited to generating sam-
ples from scratch in a sequential order. It is also capable of
inference tasks when part of the bits are given. In that case,
the canonicalization trick may not help greatly if there is a
segment of unknown bits sitting between given bits. Neverthe-
less, the marginal probabilities are still tractable because one

can also contract ladder-shaped TN eﬃciently [17, 18]. As
what will be shown in Sec. III, given these ﬂexibilities of the
sampling approach, MPS-based probabilistic modeling can be
applied to image reconstruction and denoising.

D. Features of the model and algorithms

We highlight several salient features of the MPS genera-
tive model and compare it to other popular generative mod-
els. Most signiﬁcantly, MPS has an explicit tractable probabil-
ity density, while still allows eﬃcient learning and inference.
For a system sized N, with prescribed maximal bond dimen-
sion
Dmax, the complexity of training on a dataset of size
|T |
3
N
max). The scaling of generative sampling from a
(
is
|T |
O
2
canonical MPS is
max) if all the bits to be sampled are
connected to the boundaries, otherwise given some segments
the conditional sampling scales as

(N

(N

D

D

O

3
max).

O

D

1. Theoretical Understanding of the Expressive Power

The expressibility of MPS was intensively studied in the
context of quantum physics. The bond dimensions of MPS
put an upper bound on its ability of capturing entanglement
entropy. These solid theoretical understandings of the rep-
resentational power of MPS [17, 18] makes it an appealing
model for generative tasks.

Considering the success of MPS for quantum systems, we
expect a polynomial scaling of the computational resources
for datasets with short-range correlations. Treating dataset of
two dimensional images using MPS is analogous to the appli-
cation of DMRG to two dimensional quantum systems [31].
Although in principle an exact representation of the image
dataset may require exponentially large bond dimensions as
the image resolution increases, at computationally aﬀordable
bond dimensions the MPS may already serve as a good ap-
proximation which captures dominant features of the distribu-
tion.

2. Adaptive Adjustment of Expressibility

Performing optimizations for the two-site tensor instead of
for each tensor individually, allows one to dynamically adjust
the bond dimensions during the learning process. Since for re-
alistic datasets the required bond dimensions are likely to be
inhomogeneous, adjusting them dynamically allocates com-
putational resources in an optimal manner. This situation will
be illustrated clearly using the MNIST data set in Sec. III C,
and in Fig. 4.

Adjustment of the bond dimensions follows the distribution
of singular values in (B5), which is related to the low entan-
glement inductive bias of the MPS representation. Adaptive
adjustment of MPS is advantageous compared to most other
generative models. Because in most cases, the architecture
(which is the main limiting factor of the expressibility of the

4

model) is ﬁxed during the learning procedure, only the param-
eters are tuned. By adaptively tuning the bond dimensions, the
representational power of MPS can grow as it gets more ac-
quainted with the training data. In this sense, adaptive adjust-
ment of expressibility is analogous to the structural learning
of probabilistic graphical models, which is, however, a chal-
lenging task due to discreteness of the structural information.

3. Eﬃcient Computation of Exact Gradients and Log-likelihood

Another advantage of MPS compared to the standard en-
ergy based model is that training can be done with high ef-
ﬁciency. The two terms contributing to the gradient (5) are
analogous to the negative and positive phases in the training of
energy based models [39], where the visible variables are un-
clamped and clamped respectively. In the energy based mod-
els, such as RBM, typical evaluation of the ﬁrst term requires
approximated MCMC sampling [44], or sophisticated mean-
ﬁeld approximations e.g. Thouless-Anderson-Palmer equa-
tions [45]. Fortunately, the normalization factor and its gradi-
ent can be calculated exactly and straightforwardly for MPS.
The exact evaluation of gradients guarantees the associated
stochastic gradient descent unbiased.

In addition to eﬃciency in computing gradients, the unbi-
ased estimate of the log-likelihood and its gradients beneﬁts
signiﬁcantly when compared with classic generative models
such as RBM, where the gradients are approximated due to
the intractability of partition function. First, with MPS we can
optimize the NLL directly, while with RBM the approximate
algorithms such as Contrastive Divergence (CD) is essentially
optimizing a loss function other than NLL. This results in a
fact that some region of conﬁguration space could never be
considered during training RBM and a subsequently poor per-
formance on e.g. denoising and reconstruction. Second, with
MPS we can monitor the training process easily using exact
NLL instead of other quantities such as reconstruction error
or pseudo-lilelihood for RBM, which introduce bias to moni-
toring [46].

4. Eﬃcient Direct Sampling

The approach introduced in Sec. II C allows direct sampling
from the learned probability distribution. This completely
avoids the slowing mixing problem in the MCMC sampling
of energy based models. MCMC randomly ﬂip the bits and
compare the probability ratios for accepting and rejecting the
samples. However, the random walks in the state space can
get stuck in a local minimum, which may bring unexpected
ﬂuctuations of long time correlation to the samples. Some-
times this raises issues to the samplings. As a concrete exam-
ple, consider the case where all training samples are exactly
memorized by both MPS and RBM. This is to say that NLL
, and only training samples
of both models are exactly ln
have ﬁnite probability in both models. While other samples,
even with only one bit diﬀerent, have zero probability. It is
easy to check that our MPS model can generate samples which

|T |

is identical to one of the training samples using approach in-
troduced in Sec. II C. However, RBM will not work at all in
generating samples, as there is no direction that MCMC could
follow for increasing the probability of samplings.

It is known that when graphical models have an appropriate
structure (such as a chain or a tree), the inference can be done
eﬃciently [47, 48], while these structural constraints also limit
the application of graphical models with intractable partition
functions. The MPS model, however, enjoys both the advan-
tages of eﬃcient direct sampling and a tractable partition func-
tion. The sampling algorithm is formally similar to the ones
of autoregressive models [6, 7] though, being able to dynam-
ically adjust its expressibility makes the MPS a more ﬂexible
generative model.

Unlike GAN [11] or VAE [5], the MPS can explicitly gives
tractable probability, which may enable more unsupervised
learning tasks. Moreover, the sampling in MPS works with ar-
bitrary prior information of samples, such as ﬁxed bits, which
supports applications like image reconstruction and denois-
ing. We note that this oﬀers an advantage over the popular
GAN, which easily maps a random vector in the latent space
to the image space, but having diﬃculties in the reverse di-
rection — mapping a vector in the images space to the latent
space as a prior information to sampling.

III. APPLICATIONS

In this section, to demonstrate the ability and features of
the MPS generative modeling, we apply it to several standard
datasets. As a proof of principle, we ﬁrst apply our method
to the toy data set of Bars and Stripes, where some proper-
ties of our model can be characterized analytically. Then we
train MPS as an associative memory to learn random binary
patterns to study properties such as capacity and length depen-
dences. Finally we test our model on the Modiﬁed National
Institute of Standards and Technology database (MNIST) to
illustrate its generalization ability for generating and recon-
structing images of handwritten digits. [49]

A. Bars and Stripes

Bars and Stripes (BS) [50] is a data set containing 4

4 bi-
nary images. Each image has either four-pixel-length vertical
bars or horizontal stripes, but not both. In total there are 30
diﬀerent images in the dataset out of all 216 possible ones, as
shown in Fig. 1a. These images appear with equal probability
in the dataset.This toy problem allows a detailed analysis and
reveals key characteristics of the MPS probabilistic model.

×

×

To use MPS for modeling, we unfold the 4

4 images into
one dimensional vectors as shown in Fig. 1b. After being
trained over 4 loops of batch gradient descent training the cost
function converges to its minimum value, which equals to the
= ln(30), within an ac-
Shannon entropy of the BS dataset
10. Here what the MPS has accomplished is
curacy of 1
memorizing the thirty images rigidly, by increasing the proba-
bility of the instances appeared in the dataset, and suppressing

10−

S

×

5

(a)

(b)

FIG. 1: (a) The Bars and Stripes dataset. (b) Ordering of the
pixels when transforming the image into one dimensional
vector. The numbers between pixels indicate the bond
dimensions of the well-trained MPS.

the probability of not-shown instances towards zero. We have
checked that the result is insensitive to the choice of hyperpa-
rameters.

The bond dimensions of the learned MPS have been anno-
tated in Fig. 1b. It is clear that part of the symmetry of the data
set has been preserved. For instance, the 180 rotation around
the center or the transposition of the second and the third rows
would change neither the data set nor the bond dimension dis-
tribution. Open boundary condition results in the decrease of
bond dimensions at both ends. In fact when conducting SVD
at bond k, there are at most 2min(k,N
k) non-zero singular val-
−
ues because the two parts linked by bond k have their Hilbert
spaces of dimension 2k, 2N
k. In addition, the turnings bonds
−
D12 = 15)
have slightly smaller bond dimension (
than others inside the second row and the third row, which can
be explained qualitatively as these bonds carrying less entan-
glement than the bonds in the bulk.

D4 =

D8 =

One can directly write down the exact “quantum wave func-
tion” of the BS dataset, which has ﬁnite and uniform ampli-
tudes for the training images and zero amplitude for other im-
ages. For division on each bond, one can construct the re-
duced density matrix whose eigenvalues are the square of the
singular values. Analyzed in this way, it is conﬁrmed that the
trained MPS achieves the minimal number of required bond
dimension to exactly describe the BS dataset.

We have generated Ns = 106 independent samples from the
learned MPS. All these samples are training images shown in
Fig. 1a. Carrying out likelihood ratio test [51], we got the
n j
log-likelihood ratio statistic G2 = 2NsDKL(
) = 22.0,
p j}
Ns }||{
{
n j
5. The reason for
) = 1.10
equivalently DKL(
10−
Ns }||{
adopting this statistic is that it is asymptotically χ2-distributed
[51]. The p-value of this test is 0.820, which indicates a
high probability that the uniform distribution holds true for
the sampling outcomes.

p j}

×

{

n j
Ns }||{

{

p j}

Note that DKL(

) quantiﬁes the deviation from the
expected distribution to the sampling outcomes, so it reﬂects
the performance of sampling method rather than merely the
training performance.
In contrast to our model, for energy
based models one typically has to resort to MCMC method for

sampling new patterns. It suﬀers from slow mixing problem
since various patterns in the BS dataset diﬀers substantially
and it requires many MCMC steps to obtain one independent
pattern.

B. Random patterns

Capacity represents how much about data could be learned
by the model. Usually it is evaluated using randomly gener-
ated patterns as data. For the classic Hopﬁeld model [12] with
pairwise interactions given by Hebb’s rule among N
→ ∞
variables, it has been shown [52] that in the low-temperature
region at the thermodynamic limit there is the retrieval phase
|T |c = 0.14N random binary patterns could
where at most
be remembered. In this sense, each sample generated by the
model has a large overlap with one of the training pattern.
If the number of patterns in the Hopﬁeld model is larger than
|T |c, the model would enter the spin glass state where samples
generated by the model are not correlated with any training
pattern.

Thanks to the tractable evaluation of the partition function
Z in MPS, we are able to evaluate exactly the likelihood of
every training pattern. Thus the capability of the model can
be easily characterized by the mean negative log-likelihood
with varying

. In this section we focus on the behavior of

L
number of training samples and varying system sizes.

L

L

L

|T |

In Fig. 2a we plot

as a function of number of patterns
Dmax.
used for training for several maximal bond dimension
= ln
for training set
The ﬁgure shows that we obtain
no larger than
Dmax. As what has been shown in the previous
section, this means that all training patterns are remembered
exactly. As the number of training patterns increases, MPS
with a ﬁxed
Dmax will eventually fail in remembering exactly
all the training patterns, resulting to
. In this regime
L
generations of the model usually deviate from training pat-
terns (as illustrated in Fig. 3 on the MNIST dataset). We no-
increasing, the curves in the ﬁgure deviate
tice that with
continuously. We note this is very diﬀerent from
from ln
the Hopﬁeld model where the overlap between the generation
and training samples changes abruptly due to the ﬁrst order
transition from the retrieval phase to spin glass phase.

> ln

|T |

|T |

|T |

|T |

Fig. 2a also shows that a larger

Dmax enables MPS to re-
member exactly more patterns, and produce smaller
with
the number of patterns
ﬁxed. This is quite natural because
Dmax amounts to the increase of parameter number
enlarging
of the model, hence enhances the capacity of the model. In
Dmax =
principle if
our model has inﬁnite capacity, since
arbitrary quantum states can be decomposed into MPS [17].
Clearly this is an advantage of our model over the Hopﬁeld
model and inverse Ising model [14], whose maximal model
capacity is proportional to system size.

∞

L

Careful readers may complain that the inverse Ising model
is not the correct model to compare with, because its varia-
tion with hidden variables, i.e. Boltzmann machines, do have
inﬁnite representation power. Indeed, increasing the bond di-
mensions in MPS has similar eﬀects to increasing the number
of hidden variables in other generative models.

6

(a)

(b)

FIG. 2: NLL averaged as a function of: (a) number of
random patterns used for training, with system size N = 20.
= 100 random patterns.
(b) system size N, trained using
In both (a) and (b), diﬀerent symbols correspond to diﬀerent
Dmax. Each data point is
values of maximal bond dimension
averaged over 10 random instances (i.e. sets of random
patterns), error bars are also plotted, although they are much
smaller than symbol size. The black dashed lines in ﬁgures

|T |

denote

= ln

.

|T |

L

L

L

|T |

In Fig. 2b we plot

as a function of system size N, trained
= 100 random patterns. As shown in the ﬁgure that
on
increases linearly with system size N,
with
Dmax ﬁxed
which indicates that our model gives worse memory capability
with a larger system size. This is due to the fact that keeping
joint-distribution of variables becomes more and more diﬃ-
cult for MPS when the number of variables increases, espe-
cially for long-range correlated data. This is a drawback of our
model when compared with fully pairwise-connected models
such as the inverse Ising model, which is able to capture long-
distance correlations of the training data easily. Fortunately
Fig. 2b also shows that the decay of memory capability with
system size can be compensated by increasing

Dmax.

C. MNIST dataset of handwritten digits

In this subsection we perform experiments on the MNIST
dataset [53]. In preparation we turn the grayscale images into
binary numbers by threshold binarization and ﬂattened the im-
ages row by row into a vector. For the purpose of unsupervised
generative modeling we do not need the labels of the digits.
Here we further test the capacity of the MPS for this larger-
scale and more meaningful dataset. Then we investigate its
generalization ability via examining its performance on a sep-
arated test set, which is crucial for generative modeling.

1. Model Capacity

|T |

Having chosen

= 1000 MNIST images, we train the
MPS with diﬀerent maximal bond dimensions
Dmax, as shown
decreases to its mini-
Dmax increases, the ﬁnal
in Fig. 3. As
mum ln
, and the images generated become more and more
clear. It is interesting that with a relatively small maximum
Dmax = 100, some crucial features show
bond dimension, e.g.

|T |

L

7

FIG. 3: NLL averaged of a MPS trained using

= 1000

×

MNIST images of size 28

dimension
Shannon entropy of the training set ln
minimal value of
MPS’ trained with diﬀerent

28, with varying maximum bond
Dmax. The horizontal dashed line indicates the
, which is also the
|T |
. The inset images are generated by the
Dmax (annoted by the arrows).

L

|T |

up, though some of the images were not as clear as the orig-
inal ones. For instance the hooks and the loops that partly
resembled to “2”, “3” and “9” emerge. These clear charac-
ters of handwritten digits illustrate that the MPS has learned
many “prototypes”. Similar feature-to-prototype transition in
pattern recognitions could also be observed by using a many-
body interaction in the Hopﬁeld model, or equivalently using
a higher-order rectiﬁed polynomial activation function in the
deep neural networks [54]. It is remarkable that in our model
this can be achieved by simply adjusting the maximum bond
dimension of the MPS.

Next we train another model with the restriction of

Dmax =
800. The NLL on the training dataset reach 16.8, and many
bonds have reached maximal dimension
Dmax. Fig. 4 shows
the distribution of bond dimensions. Large bond dimensions
concentrated in the center of the image, where the variation of
the pixels is complex. The bond dimensions around the top
and bottom edge of the image remain small, because those
pixels are always inactivated in the images. They carry no in-
formation and has no correlations with the remaining part of
the image. Remarkably, although the pixels on the left and
right edges are also white, they also have large bond dimen-
sions because these bonds learn to mediate the correlations
between the rows of the images.

The samples directly generated after training are shown in
Fig. 5a. We also show a few original samples from the training
set in Fig. 5b for comparison. Although many of the generated
images cannot be recognized as digits, some aspects of the re-
sult are worth mentioning. Firstly, the MPS learned to leave
margins blank, which is the most obvious common feature in
MNIST database. Secondly, the activated pixels compose pen
strokes that can be extracted from the digits. Finally, a few of
the samples could already be recognized as digits. Unlike the
discriminative learning task carried out in [32], it seems we
need to use much larger bond dimensions to achieve a good
performance in the unsupervised task. We postulate the rea-

FIG. 4: Bond dimensions of the MPS trained with
= 1000 MNIST samples, constrained to

Dmax = 800.
|T |
Final average NLL reaches 16.8. Each pixel in this ﬁgure
corresponds to bond dimension of the right leg of the tensor
associated to the identical coordinate in the original image.

son to be that in the classiﬁcation task, local features of an
image are suﬃcient for predicting the label.Thus MPS is not
required to remember longer-range correlation between pix-
els. For generative modeling, however, it is necessary because
learning the joint distribution from the data consists of (but
not limited to) learning two-point correlations between pairs
of variables that could be far from each other.

With the MPS restricted to

Dmax = 800 and trained with
1000, we carry out image restoration experiments. As shown
in Fig. 6 we remove part of the images in Fig. 5b and then
reconstruct the removed pixels (in yellow) using conditional
direct sampling. For column reconstruction, its performance
is remarkable. The reconstructed images in Fig. 6a are almost
identical to the original ones in Fig. 5b. On the other hand, for
row reconstruction in Fig. 6b, it makes interesting but reason-
able deviations. For instance, the rightmost in the ﬁrst row, an
“1” has been bent to “7”.

(a) Generated

(b) Original

FIG. 5: (a) Images generated from the same MPS as in Fig. 4.
(b) Original images randomly selected from the training set.

8

,

×

L

|T |

|T |

|T |

104, test

For diﬀerent

for training images always decrease
monotonically to diﬀerent minima, and with a ﬁxed
Dmax it
for
is easier for the MPS to ﬁt fewer training images. The
L
= 103,
test images, however, behaves quite diﬀerently: for
decreases to about 40.26 then starts climbing quickly,
test
L
= 104 the test
decreases to 33.65 then in-
while for
L
creases slowly to 34.18. For
= 6
kept de-
|T |
creasing in 75 loops. The behavior shown in Fig. 7 is quite
typical in machine learning problems. When training data is
not enough, the model quickly overﬁts the training data, giv-
ing worse and worse generalization to the unseen test data. An
extreme example is that if our model is able to decrease train-
ing
, i.e. completely overﬁts the training data, then
all other images, even the images with only one pixel diﬀer-
ent from one of the training images, have zero probability in
. We also observe that the best test
the model hence
NLL decreases as training set volume enlarges, which means
the tendency of memorizing is constrained and that of gener-
alization is enhanced.

to ln

|T |

∞

L

L

L

=

The histograms of log-likelihoods for all training and test
images are shown in Fig. 8. Notice that if the model just
memorized some of the images and ignored the others, the
histograms would be bi-modal. It is not the case, as shown
in the ﬁgure, where all distributions are centered around. This
indicates that the model learns all images well rather than con-
centrates on some images while completely ignoring the oth-
ers. In the bottom panel we show the detailed
histogram by
categories. For some digits, such as “1” and “9”, the diﬀer-
ence between training and test log-likelihood distribution is
insigniﬁcant, which suggests that the model has particularly
great generalization ability to these images.

L

FIG. 7: Evolution of the average negative log-likelihood
L
for both training images (blue, bottom lines) and 104 test
images (red, top lines) during training. From left to right,
are 103, 104, and
number of images in the training set
6

|T |
104 respectively.

×

IV. SUMMARY AND OUTLOOK

We have presented a tensor-network-based unsupervised
model, which aims at modeling the probability distribution
of samples in given unlabeled data. The probabilistic model
is structured as a matrix product state, which brings several

(a) column reconstruction on
training images

(b) row reconstruction on training
images

(c) column reconstruction on test
images

(d) row reconstruction on test
images

FIG. 6: Image reconstruction from partial images by direct
sampling with the same MPS as in Fig. 4. (a,b) Restoration
of images in Fig. 5b which are selected from the training set .
(c,d) Reconstruction of 16 images chosen from the test set.
The test set contains images from the MNIST database that
were not used for training. The given parts are in black and
the reconstructed parts are in yellow. The reconstructed parts
are: 12 columns from either (a,c) the left or the right, and
(b,d) the top or the bottom.

2. Generalization Ability

In a glimpse of its generalization ability, we also tried re-
constructing MNIST images other than the training images,
as shown in Fig. 6c, 6d. These results indicate that the MPS
has learned crucial features of the dataset, rather than merely
memorizing the training instances. In fact, even as early as
only 11 loops trained, the MPS could perform column recon-
struction with similar image quality, but its row reconstruction
performance was much worse than that trained over 251 loops.
It is reﬂected that the MPS has learned about short range pat-
terns within each row earlier than those with long range cor-
relations between diﬀerent rows, since the images have been
ﬂattened into a one dimensional vector row by row.

L

To further illustrate our model’s generalization ability, in
for the same 104 test images after training
Fig. 7 we plotted
on diﬀerent numbers of images. To save computing time we
14. The rescaling
worked on rescaled images of size 14
has also been adopted by past works, and it is shown that the
classiﬁcation on the rescaled images is still comparable with
those obtained using other popular methods [32].

×

9

wardly generalized for higher dimensional data. One can also
follow [32, 58] to use a local feature map to lift continuous
variables to a spinor space for continuous data modeling. The
ability and eﬃciency of this approach may also depend on the
speciﬁc way of performing the mapping, so in terms of contin-
uous input there are still a lot to be explored on this algorithm.
Moreover, for colored images one can encode the RGB values
to three physical legs of each MPS tensor.

Similar to using MPS for studying two-dimensional quan-
tum lattice problems [31], modeling images with MPS faces
the problem of introducing long range correlations for some
neighboring pixels in two dimension. An obvious general-
ization of the present approach is to use more expressive TN
with more complex structures. In particular, the projected en-
tangled pair states (PEPS) [59] is particularly suitable for im-
ages, because it takes care of correlation between pixels in
two-dimension. Similar to the studies of quantum systems
in 2D, however, this advantage of PEPS is partially compen-
sated by the diﬃculty of contracting the network and the lose
of convenient canonical forms. Exact contraction of a PEPS
is #P hard [60]. Nevertheless, one can employ tensor renor-
malization group methods for approximated contraction of
PEPS [61–64]. Thus, it remains to be seen whether judicious
combination of these techniques really brings a better perfor-
mance to generative modeling.

In the end, we would like to remark that perhaps the most
exciting feature of quantum-inspired generative models is the
possibility of being implemented by quantum devices [65],
rather than merely being simulated in classical computers. In
that way, neither the large bond dimension nor the high com-
putational complexity of tensor contraction, would be a prob-
lem. The tensor network representation of probability may
facilitate quantum generative modeling because some of the
tensor network states can be prepared eﬃciently on a quan-
tum computer [66, 67].

FIG. 8: (Top) Distribution of

ln p of 60000 training images

−
and 10000 test images given by a trained MPS with
Dmax = 500. The training negative log likelihood
Ltest = 30.3. (Bottom)
Ltrain = 24.2, and the test
Distributions for each digit.

advantages as discussed in Sec. II D, such as adaptive and ef-
ﬁcient learning and direct sampling.

Since we use square of the TN states to represent probabil-
ity, the sign is redundant for probabilistic modeling besides
the gauge freedom of MPS. It is likely that during the opti-
mization MPS develops diﬀerent signs for diﬀerent conﬁgu-
rations. The sign variation may unnecessarily increase the en-
tanglement in MPS and therefore the bond dimensions [55].
However, restricting the sign of MPS may also impair the ex-
pressibility of the model. One probable approach to obtain a
low entanglement representation is adding a penalty term in
the target function, for instance, a term proportional to R´enyi
entanglement entropy as in our further work on quantum to-
mography [56]. In light of these discussions, we would like
point to future research on the diﬀerences and connections of
MPS with non-negative matrix entries [57] and the probabilis-
tic graphical models such as the hidden Markov model.

Binary data modeling links closely to quantum many-body
systems with spin-1/2 constituents and could be straightfor-

ACKNOWLEDGMENTS

We thank Liang-Zhu Mu, Hong-Ye Hu, Song Cheng, Jing
Chen, Wei Li, Zhengzhi Sun and E. Miles Stoudenmire for
inspiring discussions. We also acknowledge suggestions from
anonymous reviewers. P.Z. acknowledges Swarm Club work-
shop on “Geometry, Complex Network and Machine Learn-
ing” sponsored by Kai Feng Foundation in 2016. L.W. is sup-
ported by the Ministry of Science and Technology of China
under the Grant No. 2016YFA0300603 and National Natural
Science Foundation of China under the Grant No. 11774398.
J.W. is supported by National Training Program of Innovation
for Undergraduates of China. P.Z. is supported by Key Re-
search Program of Frontier SciencesCASGrant No. QYZDB-
SSW-SYS032 and Project 11747601 of National Natural Sci-
ence Foundation of China. Part of the computation was car-
ried out at the High Performance Computational Cluster of
ITP, CAS.

10

[1] Yann LeCun, Yoshua Bengio,

and Geoﬀrey Hinton, “Deep

learning,” Nature 521, 436–444 (2015).
[2] David H. Ackley, Geoﬀrey E. Hinton,

and Terrence J. Se-
jnowski, “A Learning Algorithm for Boltzmann Machines,”
Cognitive Science 9, 147–169 (1985).

[3] P Smolensky, “Information processing in dynamical systems:
foundations of harmony theory,” in Parallel distributed process-
ing: explorations in the microstructure of cognition, vol. 1 (MIT
Press, 1986) pp. 194–281.

[4] Ruslan Salakhutdinov, “Learning deep generative models,” An-
nual Review of Statistics and Its Application 2, 361–385 (2015).
[5] Diederik P Kingma and Max Welling, “Auto-encoding varia-

tional bayes,” arXiv:1312.6114 (2013).

[6] Benigno Uria, Marc-Alexandre Cˆot´e, Karol Gregor, Iain Mur-
ray, and Hugo Larochelle, “Neural autoregressive distribution
estimation,” Journal of Machine Learning Research 17, 1–37
(2016).

[7] Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu,
“Pixel recurrent neural networks,” in Proceedings of The 33rd
International Conference on Machine Learning, Proceedings of
Machine Learning Research, Vol. 48 (PMLR, New York, New
York, USA, 2016) pp. 1747–1756.
[8] Laurent Dinh, David Krueger,

and Yoshua Bengio,
independent components estimation,”

“NICE: non-linear
arXiv:1410.8516 (2014).

[9] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio, “Den-
sity estimation using real NVP,” arXiv:1605.08803 (2016).
[10] Danilo Rezende and Shakir Mohamed, “Variational inference
with normalizing ﬂows,” in Proceedings of the 32nd Interna-
tional Conference on Machine Learning, Proceedings of Ma-
chine Learning Research, Vol. 37 (PMLR, Lille, France, 2015)
pp. 1530–1538.

[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,
and
David Warde-Farley, Sherjil Ozair, Aaron Courville,
Yoshua Bengio, “Generative adversarial nets,” in Advances in
Neural Information Processing Systems 27 (Curran Associates,
Inc., 2014) pp. 2672–2680.

[12] J. J. Hopﬁeld, “Neural networks and physical systems with

emergent collective computational abilities,” (1982) p. 2554.

[13] Hilbert J. Kappen and Francisco de Borja Rodr´ıguez Ortiz,
“Boltzmann machine learning using mean ﬁeld theory and lin-
ear response correction,” in Advances in Neural Information
Processing Systems 10 (MIT Press, 1998) pp. 280–286.
[14] Y. Roudi, J. Tyrcha, and J. Hertz, “Ising model for neural data:
Model quality and approximate methods for extracting func-
tional connectivity,” Phys. Rev. E 79, 051915 (2009).

[15] W. M. C. Foulkes, L. Mitas, R. J. Needs, and G. Rajagopal,
“Quantum monte carlo simulations of solids,” Rev. Mod. Phys.
73, 33–83 (2001).

[16] Giuseppe Carleo and Matthias Troyer, “Solving the quantum
many-body problem with artiﬁcial neural networks,” Science
355, 602–606 (2017).

[17] Ulrich Schollw¨ock, “The density-matrix renormalization group
in the age of matrix product states,” Annals of Physics 326, 96–
192 (2011).

[18] Rom´an Or´us, “A practical introduction to tensor networks: Ma-
trix product states and projected entangled pair states,” Annals
of Physics 349, 117 – 158 (2014).

[19] M B Hastings, “An area law for one-dimensional quantum sys-
tems,” Journal of Statistical Mechanics: Theory and Experi-
ment 2007, P08024–P08024 (2007).

[20] J. Eisert, M. Cramer, and M. B. Plenio, “Colloquium: Area
laws for the entanglement entropy,” Reviews of Modern Physics
82, 277–306 (2010), arXiv:0808.3773.

[21] Johann A. Bengua, Ho N. Phien, and Hoang Duong Tuan, “Op-
timal feature extraction and classiﬁcation of tensors via matrix
product state decomposition,” arXiv:1503.00516 (2015).
[22] Alexander Novikov, Anton Rodomanov, Anton Osokin, and
Dmitry Vetrov, “Putting MRFs on a tensor train,” International
Conference on Machine Learning,

, 811–819 (2014).

[23] Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and
Dmitry P Vetrov, “Tensorizing neural networks,” in Advances
in Neural Information Processing Systems (2015) pp. 442–450.
and Amnon Shashua, “On the
expressive power of deep learning: A tensor analysis,”
arXiv:1509.05009 (2015).

[24] Nadav Cohen, Or Sharir,

[25] Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh-Huy Phan,
Qibin Zhao, Danilo P Mandic, et al., “Tensor networks for
dimensionality reduction and large-scale optimization: Part 1
in
low-rank tensor decompositions,” Foundations and Trends R
(cid:13)
Machine Learning 9, 249–429 (2016).

[26] Y. Levine, D. Yakira, N. Cohen,

and A. Shashua, “Deep
Learning and Quantum Entanglement: Fundamental Connec-
tions with Implications to Network Design,” arXiv:1704.01552
(2017).

[27] D Perez-Garcia, F Verstraete, M M Wolf, and J I Cirac, “Ma-
trix Product State Representations,” Quantum Info. Comput. 7,
401–430 (2007).

[28] Ivan V Oseledets, “Tensor-train decomposition,” SIAM Journal

on Scientiﬁc Computing 33, 2295–2317 (2011).

[29] Zeph Landau, Umesh Vazirani, and Thomas Vidick, “A poly-
nomial time algorithm for the ground state of 1d gapped local
hamiltonians,” Nature Physics 11, 566 (2015).

[30] Steven R. White, “Density matrix formulation for quan-
tum renormalization groups,” Phys. Rev. Lett. 69, 2863–2866
(1992).

[31] E.M. Stoudenmire and Steven R. White, “Studying two-
dimensional systems with the density matrix renormalization
group,” Annual Review of Condensed Matter Physics 3, 111–
128 (2012).

[32] E. Miles Stoudenmire and David J. Schwab, “Supervised
Learning with Quantum-Inspired Tensor Networks,” Advances
in Neural Information Processing Systems 29, 4799 (2016),
arXiv:1605.05775.

[33] Alexander Novikov, Mikhail Troﬁmov, and Ivan Oseledets,

“Exponential machines,” arXiv:1605.05775 (2016).

[34] Angel J Gallego and Roman Orus, “The physical structure
of grammatical correlations: equivalences, formalizations and
consequences,” arXiv:1708.01525 (2017).

[35] Jing Chen, Song Cheng, Haidong Xie, Lei Wang, and Tao Xi-
ang, “Equivalence of restricted boltzmann machines and tensor
network states,” Phys. Rev. B 97, 085104 (2018).

[36] Andrew J. Ferris and Guifre Vidal, “Perfect sampling with uni-
tary tensor networks,” Phys. Rev. B 85, 165146 (2012).

[37] Ian Goodfellow, Yoshua Bengio,
Press,

Deep
Learning
(MIT
deeplearningbook.org.

and Aaron Courville,
http://www.
2016)

[38] M. Born,

“Zur Quantenmechanik

der Stoßvorg¨ange,”

Zeitschrift fur Physik 37, 863–867 (1926).

[39] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ran-
and F Huang, “A tutorial on energy-based learn-
(2006), http://yann.lecun.com/exdb/publis/

zato,
ing,”

orig/lecun-06.pdf (Accessed on 2-September-2017).
[40] Ming-Jie Zhao and Herbert Jaeger, “Norm-observable operator
models,” Neural Computation 22, 1927–1959 (2010), pMID:
20141473, https://doi.org/10.1162/neco.2010.03-09-983.
[41] Raphael Bailly, “Quadratic weighted automata:spectral algo-
rithm and likelihood maximization,” in Proceedings of the
Asian Conference on Machine Learning, Proceedings of Ma-
chine Learning Research, Vol. 20, edited by Chun-Nan Hsu
and Wee Sun Lee (PMLR, South Garden Hotels and Resorts,
Taoyuan, Taiwain, 2011) pp. 147–163.

[42] S. Kullback and R. A. Leibler, “On Information and Suf-
ﬁciency,” The Annals of Mathematical Statistics 22, 79–86
(1951).

[43] Diederik P Kingma and Jimmy Ba, “Adam: A method
for stochastic optimization,” arXiv preprint arXiv:1412.6980
(2014).

[44] Geoﬀrey E Hinton, “A practical guide to training restricted
boltzmann machines,” in Neural networks: Tricks of the trade
(Springer, 2012) pp. 599–619.

[45] Marylou Gabrie, Eric W Tramel, and Florent Krzakala, “Train-
ing restricted boltzmann machine via the thouless-anderson-
palmer free energy,” in Advances in Neural Information Pro-
cessing Systems 28, edited by C. Cortes, N. D. Lawrence, D. D.
Lee, M. Sugiyama, and R. Garnett (Curran Associates, Inc.,
2015) pp. 640–648.

[46] Aapo Hyv¨arinen, “Consistency of pseudolikelihood estimation
of fully visible boltzmann machines,” Neural Computation 18,
2283–2292 (2006).

[47] Martin J Wainwright and Michael I Jordan, “Graphical mod-
els, exponential families, and variational inference,” Founda-
tions and Trends R
(cid:13)

in Machine Learning 1, 1–305 (2008).

[48] Daphne Koller and Nir Friedman, Probabilistic Graphical
Models, Principles and Techniques (The MIT Press, 2009).
[49] The code of these experiments have been posted at https://

github.com/congzlwag/UnsupGenModbyMPS.

[50] David JC MacKay, Information theory, inference and learning

algorithms (Cambridge University Press, 2003).

[51] S. S. Wilks, “The large-sample distribution of the likelihood
ratio for testing composite hypotheses,” Ann. Math. Statist. 9,
60–62 (1938).

[52] D.J. Amit, H. Gutfreund,

and H. Sompolinsky, “Spin-glass

models of neural networks,” Phys. Rev. A 32, 1007 (1985).
[53] Yann LeCun, Corinnai Cortes, and Christopher J.C. Burges,
“The mnist database of handwritten digits,” (1998), http://
yann.lecun.com/exdb/mnist (Accessed on 2-September-
2017).

[54] Dmitry Krotov and John J Hopﬁeld, “Dense associative mem-
ory for pattern recognition,” in Advances in Neural Information
Processing Systems (2016) pp. 1172–1180.

[55] Yi Zhang, Tarun Grover, and Ashvin Vishwanath, “Entangle-
ment entropy of critical spin liquids,” Phys. Rev. Lett. 107,
067202 (2011).

[56] Han Zhao-Yu, Wang Jun, Wang Song-Bo, Li Ze-Yang,
and Lei Wang, “Eﬃcient quan-
Mu Liang-Zhu, Fan Heng,
tum tomography with ﬁdelity estimation,” arXiv:1712.03213
(2017).

[57] Kristan Temme and Frank Verstraete, “Stochastic matrix prod-

uct states,” Phys. Rev. Lett. 104, 210502 (2010).

[58] Edwin Miles Stoudenmire, “Learning relevant features of data
with multi-scale tensor networks,” Quantum Science and Tech-
nology (2018).

[59] Frank Verstraete and Juan Ignacio Cirac, “Renormalization al-
gorithms for quantum-many body systems in two and higher
dimensions,” arXiv preprint cond-mat/0407066 (2004).

11

[60] Norbert Schuch, Michael M. Wolf, Frank Verstraete,

and
Juan I. Cirac, “Computational complexity of projected entan-
gled pair states,” Phys. Rev. Lett. 98, 140506 (2007).

[61] Michael Levin and Cody P. Nave, “Tensor renormalization
group approach to two-dimensional classical lattice models,”
Phys. Rev. Lett. 99, 120601 (2007).

[62] Z. Y. Xie, H. C. Jiang, Q. N. Chen, Z. Y. Weng, and T. Xiang,
“Second renormalization of tensor-network states,” Phys. Rev.
Lett. 103, 160601 (2009).

[63] Chuang Wang, Shao-Meng Qin, and Hai-Jun Zhou, “Topo-
logically invariant tensor renormalization group method for
the edwards-anderson spin glasses model,” Phys. Rev. B 90,
174201 (2014).

[64] G. Evenbly and G. Vidal, “Tensor network renormalization,”

Phys. Rev. Lett. 115, 180405 (2015).

[65] Alejandro Perdomo-Ortiz, Marcello Benedetti, John Realpe-
Gmez, and Rupak Biswas, “Opportunities and challenges for
quantum-assisted machine learning in near-term quantum com-
puters,” Quantum Science and Technology 3, 030502 (2018).

[66] Martin Schwarz, Kristan Temme,

and Frank Verstraete,
“Preparing projected entangled pair states on a quantum com-
puter,” Phys. Rev. Lett. 108, 110502 (2012).

[67] William Huggins, Piyush Patel, K Birgitta Whaley, and E Miles
Stoudenmire, “Towards quantum machine learning with tensor
networks,” arXiv preprint arXiv:1803.11537 (2018).

[68] Setting

Dk = 1 for all bonds makes the bond dimension diﬃcult
to grow in the initial training phase. Since the rank of two site
1 and the number of nonzero singular value
tensor is 1
Dk = 1 with
is at most 2, which is likely to be truncated back to
small cutoﬀ.

×

×

×

2

2

Appendix A: Canonical conditions for MPS and computation of
the partition function

The MPS representation has gauge degrees of freedom,
which means that the state is invariant after inserting identity
1 on each bond (M can be diﬀerent on each bond).
I = MM−
Exploiting the gauge degrees of freedom, one can bring the
MPS into its canonical form: for example, the tensor A(k) is
(cid:17)
† A(k)vk = I.
called left-canonical if it satisﬁes
In diagrammatic notation, the left-canonical condition reads

A(k)vk

vk∈{

(cid:80)

0,1

(cid:16)

}

=

(A1)

The right-canonical condition is deﬁned analogously. Canoni-
calization of each tensor can be done locally and only involves
the single tensor at consideration [17, 18].

Each tensor in the MPS can be in a diﬀerent canonical
form. For example, given a speciﬁc site k, one can con-
duct gauge transformation to make all the tensors on the left,
A(i)
, left-canonical and tensors on the right,
{
A(i)
, right-canonical, while leaving
{
A(k) neither left-canonical nor right-canonical. This is called
mixed-canonical form of the MPS [17]. The normalization of
the MPS is particularly easy to compute in the canonical from.

i = 1, 2,
1
}
· · ·
i = k + 1, k + 2,
· · ·

, N

, k

−

|
|

}

In the graphical notation, it reads

Z =

=

.

(A2)

We note that even if the MPS is not in the canonical form, its
normalization factor Z can be still computed eﬃciently if one
pays attention to the order of contraction [17, 18].

Appendix B: DMRG-like Gradient Descent algorithm for
learning

A standard way of minimization of the cost function (4)
is done by performing the gradient descent algorithm on the
MPS tensor elements. Crucially, our method allows dynam-
ical adjustment of the bond dimension during the optimiza-
tion, thus being able to allocate resources to the spatial regions
where correlations among the physical variables are stronger.
Initially, we set the MPS with random tensors with small
bond dimensions. For example, all the bond dimension are
Dk = 2 except those on the boundaries [68]. We then
set to
carry out the canonicalization procedure so that all the ten-
sors except the rightmost one A(N) are left-canonical. Then,
we sweep through the matrices back and forth to tune the el-
ements of the tensors, i.e.
the parameters of the MPS. The
procedure is similar to the DMRG algorithm with two-site up-
date where one optimizes two adjacent tensors at a time [30].
At each step, we ﬁrstly merge two adjacent tensors into an
order-4 tensor,

=

,

(B1)

followed by adjusting its elements in order to decrease the cost
2. It is straight forward to
function
|
|
check that its gradient with respect to an element of the tensor
(B1) reads

= ln Z

Ψ(v)

v
∈T

(cid:80)

ln

L

−

|T |

1

∂
L
∂A(k,k+1)wkwk+1
1ik+1
−

ik

=

Z(cid:48)
Z −

2

(cid:88)

|T |

v

∈T

Ψ(cid:48)(v)
Ψ(v)

,

where Ψ(cid:48)(v) denotes the derivative of the MPS with respect
Ψ(cid:48)(v)Ψ(v). In diagram
to the tensor (B1), and Z(cid:48) = 2
language, they read

v
∈V

(cid:80)

Ψ(cid:48)(v) =

Z(cid:48)
2

=

=

The direct vertical connections of wk, vk and wk+1, vk+1 in (B3)
stand for Kronecker delta functions δwkvk and δwk+1vk+1 respec-
tively, meaning that only those input data with pattern vkvk+1

12

contribute to the gradient with respect to the tensor elements
A(k,k+1)vkvk+1 . Note that although Z and Z(cid:48) involve summations
over an exponentially large number of terms, they are tractable
in MPS via eﬃcient contraction schemes [17]. In particular, if
the MPS is in the mixed canonical form, the computation only
involves local manipulations illustrated in (B4).

Next, we carry out gradient descent to update the compo-
nents of the merged tensor. The update is ﬂexible and is open
to various gradient descent techniques. Firstly, stochastic gra-
dient descent is considerable. Instead of averaging the gra-
dient over the whole dataset, the second term of the gradi-
ent (B2) can be estimated by a randomly chosen mini-batches
of samples, where the size of the mini-batch mbatch plays a
role of hyperparameter in the training. Secondly, on a speciﬁc
contracted tensor one can conduct several steps of gradient
descent. Note that although the local update of A(k,k+1) does
not change its environment, the shifting of A(k,k+1) makes a
diﬀerence between ndes steps of update with learning rate η
η. Thirdly, especially
and one update step with η(cid:48) = ndes ×
when several steps are conducted on each contracted tensor,
the learning rate (the ratio of the update to the gradient) can
be adaptively tuned by meta-algorithms that such as RMSProp
and Adam [43].

In practice it is observed that sometimes the gradients be-
come very small while it is not in the vicinity of any local
minimum of the landscape. In that case a plateau or a saddle
point may have been encountered, and we simply increase the
learning rate so that the norm of the update is a function of the
dimensions of the contracted tensor.

After updating the order-4 tensor (B1), it is decomposed by
unfolding the tensor to a matrix, subsequently applying sin-
gular value decomposition (SVD), and ﬁnally unfolding ob-
tained two matrices back to two order-3 tensors.

=

=

≈

(B2)

(B3)

(B4)

,

(B5)

where U, V are unitary matrices and Λ is a diagonal ma-
trix containing singular values on the diagonal. The num-
ber of non-vanishing singular values will generally increase
compared to the original value in Eq. (B1) because the MPS
observes correlations in the data and try to capture them.
We truncate those singular values whose ratios to the largest
one are smaller than a prescribed hyperparameter cutoﬀ (cid:15)cut,
along with their corresponding row vectors and column vec-
tors deleted in U and V †.

If the next bond to train on is the (k+1)-th bond on the right,
take A(k) = U so that it is left-canonical, and consequently
A(k+1) = ΛV †. While if the MPS is about to be trained on
1)-th bond, analogously A(k+1) = V † will be right-
the (k
canonical and A(k) = UΛ. This keeps the MPS in mixed-
canonical form.

−

The whole training process consists of many loops.
In
each loop the training starts from the rightmost bond (between

A(N
the rightmost.

1) and A(N)) and sweeps to the leftmost A(1), then back to
−

13

