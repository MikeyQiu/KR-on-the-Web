6
1
0
2
 
t
c
O
 
5
 
 
]
L
C
.
s
c
[
 
 
2
v
8
3
7
7
0
.
8
0
6
1
:
v
i
X
r
a

Testing APSyn against Vector Cosine on
Similarity Estimation

Enrico Santus[1], Emmanuele Chersoni[2], Alessandro Lenci[3], Chu-Ren Huang[1], Philippe Blache[2]
[1] The Hong Kong Polytechnic University, Hong Kong
[2] Aix-Marseille University
[3] University of Pisa
{esantus, emmanuelechersoni}@gmail.com
alessandro.lenci@unipi.it
churen.huang@polyu.edu.hk
blache@lpl-aix.fr

Abstract

In Distributional Semantic Models (DSMs),
Vector Cosine is widely used to estimate sim-
ilarity between word vectors, although this
measure was noticed to suffer from several
shortcomings. The recent literature has pro-
posed other methods which attempt to miti-
gate such biases. In this paper, we intend to in-
vestigate APSyn, a measure that computes the
extent of the intersection between the most as-
sociated contexts of two target words, weight-
ing it by context relevance. We evaluated this
metric in a similarity estimation task on sev-
eral popular test sets, and our results show that
APSyn is in fact highly competitive, even with
respect to the results reported in the literature
for word embeddings. On top of it, APSyn
addresses some of the weaknesses of Vector
Cosine, performing well also on genuine sim-
ilarity estimation.

1

Introduction

Word similarity is one of the most important and
most studied problems in Natural Language Pro-
cessing (NLP), as it is fundamental for a wide
range of tasks, such as Word Sense Disambigua-
tion (WSD), Information Extraction (IE), Para-
phrase Generation (PG), as well as the automatic
creation of semantic resources. Most of the cur-
rent approaches to word similarity estimation rely
on some version of the Distributional Hypothesis
(DH), which claims that words occurring in the
same contexts tend to have similar meanings (Har-
ris, 1954; Firth, 1957; Sahlgren, 2008). Such hy-
pothesis provides the theoretical ground for Distri-

butional Semantic Models (DSMs), which represent
word meaning by means of high-dimensional vec-
tors encoding corpus-extracted co-occurrences be-
tween targets and their linguistic contexts (Turney
and Pantel, 2010).
initialize vectors with co-
Traditional DSMs
occurrence frequencies. Statistical measures, such
as Positive Pointwise Mutual Information (PPMI)
or its variants (Church and Hanks, 1990; Bulli-
naria and Levy, 2012; Levy et al., 2015), have been
adopted to normalize these values. Also, these mod-
els have exploited the power of dimensionality re-
duction techniques, such as Singular Value Decom-
position (SVD; Landauer and Dumais, 1997) and
Random Indexing (Sahlgren, 2005).
These ﬁrst-generation models are currently referred
to as count-based, as distinguished from the context-
predicting ones, which have been recently proposed
in the literature (Bengio et al., 2006; Collobert
and Weston, 2008; Turian et al., 2010; Huang
et al., 2012; Mikolov et al., 2013). More com-
monly known as word embeddings, these second-
generation models learn meaning representations
through neural network training: the vectors dimen-
sions are set to maximize the probability for the con-
texts that typically occur with the target word.
Vector Cosine is generally adopted by both types of
models as a similarity measure. However, this met-
ric has been found to suffer from several problems
(Li and Han, 2013; Faruqui et al., 2016), such as
a bias towards features with higher values and the
inability of considering how many features are ac-
tually shared by the vectors. Finally, Cosine is af-
fected by the hubness effect (Dinu et al., 2014; Schn-

abel et al., 2015), i.e. the fact that words with high
frequency tend to be universal neighbours. Even
though other measures have been proposed in the
literature (Deza and Deza, 2009), Vector Cosine is
still by far the most popular one (Turney and Pan-
tel, 2010). However, in a recent paper of Santus et
al.
(2016b), the authors have claimed that Vector
Cosine is outperformed by APSyn (Average Preci-
sion for Synonymy), a metric based on the extent
of the intersection between the most salient con-
texts of two target words. The measure, tested on
a window-based DSM, outperformed Vector Cosine
on the ESL and on the TOEFL datasets.
In the present work, we perform a systematic eval-
uation of APSyn, testing it on the most popular test
sets for similarity estimation - namely WordSim-353
(Finkelstein et al., 2001), MEN (Bruni et al., 2014)
and SimLex-999 (Hill et al., 2015). For comparison,
Vector Cosine is also calculated on several count-
based DSMs. We implement a total of twenty-eight
models with different parameters settings, each of
which differs according to corpus size, context win-
dow width, weighting scheme and SVD application.
The new metric is shown to outperform Vector Co-
sine in most settings, except when the latter metric is
applied on a PPMI-SVD reduced matrix (Bullinaria
and Levy, 2012), against which APSyn still obtains
competitive performances. The results are also dis-
cussed in relation to the state-of-the-art DSMs, as
reported in Hill et al. (2015). In such comparison,
the best settings of our models outperform the word
embeddings in almost all datasets. A pilot study
was also carried out to investigate whether APSyn
is scalable. Results prove its high performance also
when calculated on large corpora, such as those used
by Baroni et al. (2014).
On top of the performance, APSyn seems not to be
subject to some of the biases that affect Vector Co-
sine. Finally, considering the debate about the abil-
ity of DSMs to calculate genuine similarity as op-
posed to word relatedness (Turney, 2001; Agirre et
al., 2009; Hill et al., 2015), we test the ability of the
models to quantify genuine semantic similarity.

2 Background

2.1 DSMs, Measures of Association and

Dimensionality Reduction

Count-based DSMs are built in an unsupervised
way. Starting from large preprocessed corpora, a
matrix M(m×n) is built, in which each row is a vec-
tor representing a target word in a vocabulary of size
m, and each column is one of the n potential contexts
(Turney and Pantel, 2010; Levy et al., 2015). The
vector dimensions are counters recording how many
times the contexts co-occur with the target words.
Since raw frequency is highly skewed, most DSMs
have adopted more sophisticated association mea-
sures, such as Positive PMI (PPMI; Church and
Hanks, 1990; Bullinaria and Levy, 2012; Levy et al.,
2015) and Local Mutual Information (LMI; Evert,
2005). PPMI compares the observed joint probabil-
ity of co-occurrence of w and c with their probability
of co-occurrence assuming statistical indipendence.
It is deﬁned as:

P P M I(w, c) = max(P M I(w, c), 0)

(1)

P M I(w, c) = log

(cid:18) P (w, c)
P (w)P (c)

(cid:19)

= log

(cid:19)

(cid:18) |w, c|D
|w||c|

(2)
where w is the target word, c is the given context,
P(w,c) is the probability of co-occurrence, and D is
the collection of observed word-context pairs.
Unlike frequency, PPMI was found to have a bias
towards rare events. LMI could therefore be used
to reduce such bias and it consists in multiplying
the PPMI of the pair by its co-occurrence frequency.
Since target words may occur in hundreds of thou-
sands contexts, most of which are not informative,
methods for dimensionality reduction have been in-
vestigated, such as truncated SVD (Deerwester et
al., 1990; Landauer and Dumais, 1997; Turney and
Pantel, 2010; Levy et al., 2015). SVD has been re-
garded as a method for noise reduction and for the
discovery of latent dimensions of meaning, and it
has been shown to improve similarity measurements
when combined with PPMI (Bullinaria and Levy,
2012; Levy et al., 2015). As we will see in the
next section, APSyn applies another type of reduc-
tion, which consists in selecting only the top-ranked

contexts in a relevance sorted context list for each
word vector. Such reduction complies with the prin-
ciple of cognitive economy (i.e. only the most rele-
vant contexts are elaborated; see Finton, 2002) and
with the results of behavioural studies, which sup-
ported feature saliency (Smith et al., 1974). Since
APSyn was deﬁned for linguistic contexts (Santus
et al., 2016b), we did not test it on SVD-reduced
spaces, leaving such test to further studies.

2.2 Similarity Measures

Vector Cosine, by far the most common distribu-
tional similarity metric (Turney and Pantel, 2010;
Landauer and Dumais, 1997; Jarmasz and Szpakow-
icz, 2004; Mikolov et al., 2013; Levy et al., 2015),
looks at the normalized correlation between the di-
mensions of two word vectors, w1 and w2 and re-
turns a score between -1 and 1. It is described by the
following equation:

(3)

(cid:80)n
i=1 f1i × f2i
i=1 f1i × (cid:112)(cid:80)n

(cid:112)(cid:80)n

cos(w1, w2) =

i=1 f2i
where fix is the i-th dimension in the vector x.
Despite its extensive usage, Vector Cosine has been
recently criticized for its hyper sensibility to fea-
tures with high values and for the inability of iden-
tifying the actual feature intersection (Li and Han,
2013; Schnabel et al., 2015). Recalling an exam-
ple by Li and Han (2013), the Vector Cosine for
the toy-vectors a = [1, 2, 0] and b = [0, 1, 0] (i.e.
0.8944) is unexpectedly higher than the one for a
and c = [2, 1, 0] (i.e. 0.8000), and even higher
than the one for the toy-vectors a and d = [1, 2, 1]
(i.e. 0.6325), which instead share a larger feature
intersection. Since the Vector Cosine is a distance
measure, it is also subject to the hubness problem,
which was shown by Radovanovic et al. (2010) to
be an inherent property of data distributions in high-
dimensional vector space. The problem consists in
the fact that vectors with high frequency tend to get
high scores with a large number of other vectors,
thus becoming universal nearest neighbours (Dinu
et al., 2014; Schnabel et al., 2015; Faruqui et al.,
2016).
Another measure of word similarity named APSyn 1

1Scripts

information
and
https://github.com/esantus/APSyn

can

be

found

at

has been recently introduced in Santus et al. (2016a)
and Santus et al. (2016b), and it was shown to out-
perform the vector cosine on the TOEFL (Landauer
and Dumais, 1997) and on the ESL (Turney, 2001)
test sets. This measure is based on the hypothe-
sis that words carrying similar meanings share their
most relevant contexts in higher proportion com-
pared to less similar words. The authors deﬁne AP-
Syn as the extent of the weighted intersection be-
tween the top most salient contexts of the target
words, weighting it by the average rank of the in-
tersected features in the PPMI-sorted contexts lists
of the target words:

AP Syn(w1, w2) =

(cid:88)

1
(rank1(f ) + rank2(f ))/2

f (cid:15)N (F1)∩N (F2)

(4)

meaning: for every feature f included in the inter-
section between the top N features of w1 and the
top of w2 (i.e. N (f1) and N (f2)), add 1 divided by
the average rank of the feature in the PPMI-ranked
features of w1 (i.e. rank1) and w2 (i.e. rank2). Ac-
cording to the authors, N is a parameter, generally
ranging between 100 and 1000. Results are shown
to be relatively stable when N varies in this range,
while become worst if bigger N are used, as low
informative features are also introduced. Santus et
al. (2016a) have also used LMI instead of PPMI as
weighting function, but achieving lower results.
With respect to the limitations mentioned above for
the Vector Cosine, APSyn has some advantages.
First of all, it is by deﬁnition able to identify the
extent of the intersection. Second, its sensibility to
features with high values can be kept under control
by tuning the value of N. On top of it, feature values
(i.e. their weights) do not affect directly the similar-
ity score, as they are only used to build the feature
rank. With reference to the toy-vectors presented
above, APSyn would assign in fact completely dif-
ferent scores. The higher score would be assigned
to a and d, as they share two relevant features out of
three. The second higher score would be assigned
to a and c, for the same reason as above. The lower
score would be instead assigned to a and b, as they
only share one non-salient feature. In section 3.4,
we brieﬂy discuss the hubness problem.

2.3 Datasets

For our evaluation, we used three widely popular
datasets: WordSim-353 (Finkelstein et al., 2001),
MEN (Bruni et al., 2014), SimLex-999 (Hill et al.,
2015). These datasets have a different history, but
all of them consist in word pairs with an associ-
ated score, that should either represent word asso-
ciation or word similarity. WordSim-353 (Finkel-
stein et al., 2001) was proposed as a word similarity
dataset containing 353 pairs annotated with scores
(2015)
between 0 and 10. However, Hill et al.
claimed that the instructions to the annotators were
ambiguous with respect to similarity and associa-
tion, so that the subjects assigned high similarity
scores to entities that are only related by virtue of
frequent association (e.g. coffee and cup; movie and
theater). On top of it, WordSim-353 does not pro-
vide the POS-tags for the 439 words that it contains,
forcing the users to decide which POS to assign to
the ambiguous words (e.g. [white, rabbit] and [run,
marathon]). An extension of this dataset resulted
from the subclassiﬁcation carried out by Agirre et al.
(2009), which discriminated between similar and as-
sociated word pairs. Such discrimination was done
by asking annotators to classify all pairs according
identical,
to the semantic relation they hold (i.e.
synonymy, antonymy, hypernymy, meronymy and
none-of-the-above). The annotation was then used
to group the pairs in three categories: similar pairs
(those classiﬁed as identical, synonyms, antonyms
and hypernyms), associated pairs (those classiﬁed
as meronyms and none-of-the-above, with an aver-
age similarity greater than 5), and non-associated
pairs (those classiﬁed as none-of-the-above, with an
average similarity below or equal to 5). Two gold
standard were ﬁnally produced: i) one for similarity,
containing 203 word pairs resulting from the union
of similar and non-associated pairs; ii) one for relat-
edness, containing 252 word pairs resulting from the
union of associated and non-associated pairs. Even
though such a classiﬁcation made a clear distinc-
tion between the two types of relations (i.e. simi-
larity and association), Hill et al. (2015) argue that
these gold standards still carry the scores they had in
WordSim-353, which are known to be ambiguous in
this regard.
The MEN Test Collection (Bruni et al., 2014) in-

cludes 3,000 word pairs divided in two sets (one
for training and one for testing) together with hu-
man judgments, obtained through Amazon Mechan-
ical Turk. The construction was performed by ask-
ing subjects to rate which pair - among two of them -
was the more related one (i.e. the most associated).
Every pairs-couple was proposed only once, and a
ﬁnal score out of 50 was attributed to each pair, ac-
cording to how many times it was rated as the most
related. According to Hill et al. (2015), the major
weakness of this dataset is that it does not encode
word similarity, but a more general notion of associ-
ation.
SimLex-999 is the dataset introduced by Hill et al.
(2015) to address the above mentioned criticisms of
confusion between similarity and association. The
dataset consists of 999 pairs containing 1,028 words,
which were also evaluated in terms of POS-tags and
concreteness. The pairs were annotated with a score
between 0 and 10, and the instructions were strictly
requiring the identiﬁcation of word similarity, rather
than word association. Hill et al. (2015) claim that
differently from other datasets, SimLex-999 inter-
annotator agreement has not been surpassed by any
automatic approach.

2.4 State of the Art Vector Space Models

In order to compare our results with state-of-the-art
DSMs, we report the scores for the Vector Cosines
calculated on the neural language models (NLM) by
Hill et al. (2015), who used the code (or directly the
embeddings) shared by the original authors. As we
trained our models on almost the same corpora used
by Hill and colleagues, the results are perfectly com-
parable.
The three models we compare our results to are:
i) the convolutional neural network of Collobert
and Weston (2008), which was trained on 852 mil-
lion words of Wikipedia; ii) the neural network of
Huang et al. (2012), which was trained on 990 mil-
lion words of Wikipedia; and iii) the word2vec of
Mikolov et al. (2013), which was trained on 1000
million words of Wikipedia and on the RCV Vol. 1
Corpus (Lewis et al., 2004).

Dataset
Window
Cos Freq
Cos LMI
Cos PPMI
Cos SVD-Freq300
Cos SVD-LMI300
Cos SVD-PPMI300
APSynLMI-1000
APSynLMI-500
APSynLMI-100
APSynPPMI-1000
APSynPPMI-500
APSynPPMI-100

Mikolov et al.

SimLex-999 WordSim-353

MEN

2
0.149
0.248
0.284
0.128
0.19
0.386
0.18
0.199
0.206
0.254
0.295
0.332

3
0.133
0.259
0.267
0.127
0.21
0.382
0.163
0.164
0.182
0.304
0.32
0.328

2
0.172
0.321
0.41
0.169
0.299
0.485
0.254
0.283
0.304
0.399
0.455
0.425

3
0.148
0.32
0.407
0.172
0.29
0.47
0.237
0.265
0.265
0.453
0.468
0.422

2
0.089
0.336
0.424
0.076
0.275
0.509
0.205
0.226
0.23
0.369
0.423
0.481

3
0.096
0.364
0.433
0.084
0.286
0.538
0.196
0.214
0.209
0.415
0.478
0.513

State of the Art
0.282

0.442

0.433

Table 1: Spearman correlation scores for our eight models trained on RCV Vol. 1, in the three datasets
Simlex-999, WordSim-353 and MEN. In the bottom the performance of the state-of-the-art model of
Mikolov et al. (2013), as reported in Hill et al. (2015).

3 Experiments

In this section, we describe our experiments, starting
from the training corpora (Section 3.1), to move to
the implementation of twenty-eight DSMs (Section
3.2), following with the application and evaluation
of the measures (Section 3.3), up to the performance
analysis (Section 3.4) and the scalability test (Sec-
tion 3.5).

3.1 Corpora and Preprocessing

We used two different corpora for our experiments:
RCV vol. 1 (Lewis et al., 2004) and the Wikipedia
corpus (Baroni et al., 2009), respectively containing
150 and 820 million words. The RCV Vol. 1 and
Wikipedia were automatically tagged, respectively,
with the POS tagger described in Dell’Orletta (2009)
and with the TreeTagger (Schmid, 1994).

3.2 DSMs

For our experiments, we implemented twenty-eight
DSMs, but for reasons of space only sixteen of them
are reported in the tables. All of them include the
pos-tagged target words used in the three datasets
(i.e. MEN, WordSim-353 and SimLex-999) and the
pos-tagged contexts having frequency above 100
in the two corpora. We considered as contexts the

content words (i.e. nouns, verbs and adjectives)
within a window of 2, 3 and 5, even though the
latter was given up for its poor performances.
As for SVD factorization, we found out that the best
results were always achieved when the number of
latent dimensions was between 300 and 500. We
report here only the scores for k = 300, since 300 is
one of the most common choices for the dimension-
ality of SVD-reduced spaces and it is always close
to be an optimal value for the parameter.
Fourteen out of twenty-eight models were devel-
oped for RCV1, while the others were developed
for Wikipedia. For each corpus, the models differed
according to the window size (i.e. 2 and 3), to the
statistical association measure used as a weighting
none, PPMI and LMI) and to the
scheme (i.e.
application of SVD to the previous combinations.

3.3 Measuring Word Similarity and

Relatedness

Given the twenty-eight DSMs, for each dataset we
have measured the Vector Cosine and APSyn be-
tween the words in the test pairs.

SimLex-999 WordSim-353

MEN

Dataset
Window
Cos Freq
Cos LMI
Cos PPMI
Cos SVD-Freq300
Cos SVD-LMI300
Cos SVD-PPMI300
APSynLMI-1000
APSynLMI-500
APSynLMI-100
APSynPPMI-1000
APSynPPMI-500
APSynPPMI-100

Huang et al.
Collobert & Weston
Mikolov et al.

2
0.148
0.367
0.395
0.157
0.327
0.477
0.343
0.339
0.303
0.434
0.442
0.316

3
0.159
0.374
0.364
0.184
0.329
0.464
0.344
0.342
0.31
0.419
0.423
0.281

2
0.199
0.489
0.605
0.159
0.368
0.533
0.449
0.438
0.392
0.599
0.602
0.58
State of the Art
0.098
0.268
0.414

3
0.207
0.529
0.622
0.172
0.408
0.562
0.477
0.47
0.428
0.643
0.653
0.608

2
0.178
0.59
0.733
0.197
0.524
0.769
0.586
0.58
0.48
0.749
0.757
0.703

3
0.197
0.63
0.74
0.226
0.563
0.779
0.597
0.588
0.498
0.772
0.773
0.722

0.3
0.494
0.655

0.433
0.575
0.699

Table 2: Spearman correlation scores for our eight models trained on Wikipedia, in the three datasets Simlex-
999, WordSim-353 and MEN. In the bottom the performance of the state-of-the-art models of Collobert and
Weston (2008), Huang et al. (2012), Mikolov et al. (2013), as reported in Hill et al. (2015).

The Spearman correlation between our scores and
the gold standard was then computed for every
model and it is reported in Table 1 and Table 2.
In particular, Table 1 describes the performances on
SimLex-999, WordSim-353 and MEN for the mea-
sures applied on RCV Vol. 1 models. Table 2, in-
stead, describes the performances of the measures
on the three datasets for the Wikipedia models. Con-
currently, Table 3 and Table 4 describe the perfor-
mances of the measures respectively on the RCV
Vol. 1 and Wikipedia models, tested on the subsets
of WordSim-353 extracted by Agirre et al. (2009).

3.4 Performance Analysis

Table 1 shows the Spearman correlation scores for
Vector Cosine and APSyn on the three datasets
for the eight most representative DSMs built using
RCV Vol. 1. Table 2 does the same for the DSMs
built using Wikipedia. For the sake of comparison,
we also report the results of the state-of-the-art
DSMs mentioned in Hill et al. (2015) (see Section
2.5).
With a glance at
noticed that

it can be easily
the measures perform particularly

the tables,

in two models:

well
i) APSyn, when applied
on the PPMI-weighted DSM (henceforth, AP-
SynPPMI); ii) Vector Cosine, when applied on the
SVD-reduced PPMI-weighted matrix (henceforth,
CosSVDPPMI). These two models perform consis-
tently and in a comparable way across the datasets,
generally outperforming the state-of-the-art DSMs,
with an exception for the Wikipedia-trained models
in WordSim-353.
i) corpus size
Some further observations are:
strongly affects the results; ii) PPMI strongly out-
performs LMI for both Vector Cosine and APSyn;
iii) SVD boosts the Vector Cosine, especially when
it is combined with PPMI; iv) N has some impact
on the performance of APSyn, which generally
achieves the best results for N=500. As a note about
iii),
the results of using SVD jointly with LMI
spaces are less predictable than when combining it
with PPMI.
Also, we can notice that
the smaller window
(i.e. 2) does not always perform better than the
larger one (i.e. 3). The former appears to perform
better on SimLex-999, while the latter seems to
have some advantages on the other datasets. This

Dataset
Window
Cos Freq
Cos LMI
Cos PPMI
Cos SVD-Freq300
Cos SVD-LMI300
Cos SVD-PPMI300
APSynLMI-1000
APSynLMI-500
APSynLMI-100
APSynPPMI-1000
APSynPPMI-500
PMI APSynPPMI-100

WSim (SIM) WSim (REL)

2
0.208
0.416
0.52
0.240
0.418
0.550
0.32
0.355
0.388
0.519
0.564
0.562

3
0.158
0.395
0.496
0.214
0.393
0.522
0.29
0.319
0.335
0.525
0.546
0.553

2
0.167
0.251
0.378
0.051
0.141
0.325
0.259
0.261
0.233
0.337
0.361
0.287

3
0.175
0.269
0.396
0.084
0.151
0.323
0.241
0.284
0.27
0.397
0.382
0.309

Table 3: Spearman correlation scores for our eight models trained on RCV1, in the two subsets of WordSim-
353.

might depend on the different type of similarity
encoded in SimLex-999 (i.e. genuine similarity).
On top of it, despite Hill et al. (2015)’s claim that
no evidence supports the hypothesis that smaller
context windows improve the ability of models to
capture similarity (Agirre et al., 2009; Kiela and
Clark, 2014), we need to mention that window 5
was abandoned because of its low performance.
With reference to the hubness effect, we have
conducted a pilot study inspired to the one carried
out by Schnabel et al. (2015), using the words of the
SimLex-999 dataset as query words and collecting
for each of them the top 1000 nearest neighbors.
Given all the neighbors at rank r, we have checked
their rank in the frequency list extracted from our
corpora. Figure 1 shows the relation between the
rank in the nearest neighbor list and the rank in
the frequency list.
It can be easily noticed that
the highest ranked nearest neighbors tend to have
higher rank also in the frequency list, supporting
the idea that frequent words are more likely to be
nearest neighbors. APSyn does not seem to be able
to overcome such bias, which seems to be in fact
an inherent property of the DSMs (Radovanovic
et al., 2010). Further investigation is needed to
see whether variations of APSyn can tackle this
problem.

Figure 1: Rank in the corpus-derived frequency list
for the top 1000 nearest neighbors of the terms in
SimLex-999, computed with Cosine (red) and AP-
Syn (blue). The smoothing chart in the bottom uses
the Generalized Additive Model (GAM) from the
mgcv package in R.

Finally, few words need to be spent with regard
to the ability of calculating genuine similarity, as
distinguished from word relatedness (Turney, 2001;
Agirre et al., 2009; Hill et al., 2015). Table 3 and
Table 4 show the Spearman correlation scores for
the two measures calculated on the models respec-
tively trained on RCV1 and Wikipedia, tested on
the subsets of WordSim-353 extracted by Agirre et
al.
It can be easily noticed that our best
models work better on the similarity subset. In par-
ticular, APSynPPMI performs about 20-30% bet-
ter for the similarity subset than for the relatedness
one (see Table 3), as well as both APSynPPMI and
CosSVDPPMI do in Wikipedia (see Table 4).

(2009).

Dataset
Window
Cos Freq
Cos LMI
Cos PPMI
Cos SVD-Freq300
Cos SVD-LMI300
Cos SVD-PPMI300
APSynLMI-1000
APSynLMI-500
APSynLMI-100
APSynPPMI-1000
APSynPPMI-500
APSynPPMI-100

WSim (SIM) WSim (REL)

2
0.335
0.638
0.672
0.35
0.604
0.72
0.609
0.599
0.566
0.692
0.699
0.66

3
0.334
0.663
0.675
0.363
0.626
0.725
0.609
0.601
0.574
0.726
0.742
0.692

2
0.03
0.293
0.441
-0.013
0.222
0.444
0.317
0.289
0.215
0.507
0.508
0.482

3
0.05
0.34
0.446
0.001
0.286
0.486
0.36
0.344
0.271
0.568
0.571
0.516

Table 4: Spearman correlation results for our eight models trained on Wikipedia, in the subsets of WordSim-
353.

3.5 Scalability

In order to evaluate the scalability of APSyn, we
have performed a pilot test on WordSim-353 and
MEN with the same corpus used by Baroni et
al.
(2014), which consists of about 2.8B words
(i.e. about 3 times Wikipedia and almost 20 times
RCV1). The best scores were obtained with APSyn,
N=1000, on a 2-window PPMI-weighted DSM. In
such setting, we obtain a Spearman correlation of
0.72 on WordSim and 0.77 on MEN. These results
are much higher than those reported by Baroni et
al. (2014) for the count-based models (i.e. 0.62 on
WordSim and 0.72 on MEN) and slightly lower than
those reported for the predicting ones (i.e. 0.75 on
WordSim and 0.80 on MEN).

4 Conclusions

In this paper, we have presented the ﬁrst systematic
evaluation of APSyn, comparing it to Vector Co-
sine in the task of word similarity identiﬁcation. We
developed twenty-eight count-based DSMs, each
of which implementing different hyperparameters.
PPMI emerged as the most efﬁcient association mea-
sure: it works particularly well with Vector Cosine,
when combined with SVD, and it boosts APSyn.
APSyn showed extremely promising results, despite
its conceptual simplicity. It outperforms the Vector
Cosine in almost all settings, except when the lat-

ter is used on a PPMI-weighed SVD-reduced DSM.
Even in this case, anyway, its performance is very
competitive. Interestingly, our best models achieve
results that are comparable to - or even better than
- those reported by Hill et al. (2015) for the state-
of-the-art word embeddings models. In Section 3.5
we show that APSyn is scalable, outperforming the
state-of-the-art count-based models reported in Ba-
roni et al. (2014). On top of it, APSyn does not suf-
fer from some of the problems reported for the Vec-
tor Cosine, such as the inability of identifying the
number of shared features. It still however seems to
be affected by the hubness issue, and more research
should be carried out to tackle it. Concerning the dis-
crimination between similarity and association, the
good performance of APSyn on SimLex-999 (which
was built with a speciﬁc attention to genuine similar-
ity) and the large difference in performance between
the two subsets of WordSim-353 described in Table
3 and Table 4 make us conclude that APSyn is in-
deed efﬁcient in quantifying genuine similarity.
To conclude, being a linguistically and cognitively
grounded metric, APSyn offers the possibility for
further improvements, by simply combining it to
other properties that were not yet considered in its
deﬁnition. A natural extension would be to ver-
ify whether APSyn hypothesis and implementation
holds on SVD reduced matrices and word embed-
dings.

Acknowledgments

This paper is partially supported by HK PhD Fellow-
ship Scheme, under PF12-13656. Emmanuele Cher-
soni’s research is funded by a grant of the University
Foundation A*MIDEX. Thanks to Davis Ozols for
the support with R.

References

Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 19–27. As-
sociation for Computational Linguistics.

Luis Von Ahn and Laura Dabbish. 2004. Labeling im-
In Proceedings of the
ages with a computer game.
SIGCHI conference on Human factors in computing
systems, pages 319–326. ACM.

Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673–
721.

Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta.
2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language resources and evaluation,
43(3):209–226.

Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski.
2014. Don’t count, predict! a systematic compari-
son of context-counting vs. context-predicting seman-
tic vectors. In ACL (1), pages 238–247.

Yoshua Bengio, Holger Schwenk,

Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
In
2006. Neural probabilistic language models.
Innovations in Machine Learning, pages 137–186.
Springer.

Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. J. Artif. Intell.
Res.(JAIR), 49(1-47).

John Bullinaria and Joe Levy. 2012. Extracting Semantic
Representations from Word Co-occurrence Statistics:
Stop-lists, Stemming and SVD. Behavior Research
Methods, 44(890-907).

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational linguistics, 16(1):22–29.

Ronan Collobert and Jason Weston. 2008. A uniﬁed ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceedings

of the 25th international conference on Machine learn-
ing, pages 160–167. ACM.

Scott Deerwester, Susan T Dumais, George W Furnas,
Thomas K Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American society for information science, 41(6):391.
Felice DellOrletta. 2009. Ensemble system for part-of-

speech tagging. Proceedings of EVALITA, 9.

Michel Marie Deza and Elena Deza. 2009. Encyclopedia

of distances. Springer.

Georgiana Dinu, Angeliki Lazaridou and Marco Baroni
2014. Improving zero-shot learning by mitigating the
hubness problem. arXiv preprint arXiv:1603.09054.
Stefan Evert. 2005. The statistics of word cooccurrences:

word pairs and collocations.

Manaal Faruqui, Yulia Tsvetkov, Pushpendre Ratogi, and
Chris Dyer. 2016. Problems With Evaluation of Word
arXiv
Embeddings Using Word Similarity Tasks.
preprint arXiv:1301.3781..

Christiane Fellbaum. 1998. WordNet. Wiley Online Li-

brary.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: The con-
cept revisited. In Proceedings of the 10th international
conference on World Wide Web, pages 406–414. ACM.
David Finton. 2002. Cognitive economy and the role of
representation in on-line learning. Doctoral disserta-
tion. University of Wisconsin-Madison.

John Rupert Firth. 1957. Papers in linguistics, 1934-

1951. Oxford University Press.

Zellig S Harris. 1954. Distributional structure. Word,

10(2-3):146–162.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics.

Eric H Huang, Richard Socher, Christopher D Manning,
and Andrew Y Ng. 2012. Improving word representa-
tions via global context and multiple word prototypes.
In Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers-
Volume 1, pages 873–882. Association for Computa-
tional Linguistics.

Mario Jarmasz and Stan Szpakowicz.

2004. Rogets
thesaurus and semantic similarity1. Recent Advances
in Natural Language Processing III: Selected Papers
from RANLP, 2003:111.

Douwe Kiela and Stephen Clark. 2014. A systematic
study of semantic vector space model parameters. In
Proceedings of the 2nd Workshop on Continuous Vec-
tor Space Models and their Compositionality (CVSC)
at EACL, pages 21–30.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
annual meeting of the association for computational
linguistics, pages 384–394. Association for Computa-
tional Linguistics.

Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of artiﬁcial intelligence research, 37(1):141–
188.

Peter D Turney. 2001. Mining the web for synonyms:

Pmi-ir versus lsa on toeﬂ.

Thomas K Landauer and Susan T Dumais. 1997. A so-
lution to plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, 104(2):211.

Thomas K Landauer, Peter W Foltz, and Darrell Laham.
1998. An introduction to latent semantic analysis.
Discourse processes, 25(2-3):259–284.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associa-
tion for Computational Linguistics, 3:211–225.

David D Lewis, Yiming Yang, Tony G Rose, and Fan Li.
2004. Rcv1: A new benchmark collection for text cat-
egorization research. The Journal of Machine Learn-
ing Research, 5:361–397.
Baoli Li and Liping Han.

2013. Distance weighted
In-
cosine similarity measure for text classiﬁcation..
telligent Data Engineering and Automated Learning
-IDEAL 2013: 611-618.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Milos Radovanovic, Alexandros Nanopoulos and Mir-
jana Ivanovic.
2010. On the existence of obsti-
nate results in vector space models. Proceedings of
SIGIR:186-193.

Magnus Sahlgren. 2005. An introduction to random in-
dexing. In Methods and applications of semantic in-
dexing workshop at the 7th international conference
on terminology and knowledge engineering, TKE, vol-
ume 5.

Magnus Sahlgren. 2008. The distributional hypothesis.

Italian Journal of Linguistics, 20(1):33–54.

Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro
Lenci, and Chu-ren Huang.
2016. Unsupervised
Measure of Word Similarity: How to Outperform
Co-Occurrence and Vector Cosine in VSMs. arXiv
preprint arXiv:1603.09054.

Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro
Lenci, and Chu-ren Huang. 2016. What a Nerd! Beat-
ing Students and Vector Cosine in the ESL and TOEFL
Datasets. In Proceedings of LREC.

Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the inter-
national conference on new methods in language pro-
cessing, volume 12, pages 44–49. Citeseer.

Tobias Schnabel,

Igor Labutov, David Mimmo and
Thorsten Joachims. 2015. Evaluation methods for
In Proceedings of
unsupervised word embeddings.
EMNLP.

Edward Smith, Edward Shoben and Lance Rips. 1974.
Structure and process in semantic memory: A featural
model for semantic decisions. Psychological Review,
81(3).

