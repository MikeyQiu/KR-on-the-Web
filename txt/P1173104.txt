6
1
0
2
 
c
e
D
 
7
1
 
 
]

R

I
.
s
c
[
 
 
1
v
9
2
7
5
0
.
2
1
6
1
:
v
i
X
r
a

Exploiting sparsity to build eﬃcient kernel based
collaborative ﬁltering for top-N item recommendation

University of Padova - Department of Mathematics

Via Trieste, 63, 35121 Padova - Italy

Mirko Polato

Fabio Aiolli

University of Padova - Department of Mathematics
Via Trieste, 63, 35121 Padova - Italy

Abstract

The increasing availability of implicit feedback datasets has raised the interest

in developing eﬀective collaborative ﬁltering techniques able to deal asymmet-

rically with unambiguous positive feedback and ambiguous negative feedback.

In this paper, we propose a principled kernel-based collaborative ﬁltering

method for top-N item recommendation with implicit feedback. We present an

eﬃcient implementation using the linear kernel, and we show how to generalize

it to kernels of the dot product family preserving the eﬃciency.

We also investigate on the elements which inﬂuence the sparsity of a stan-

dard cosine kernel. This analysis shows that the sparsity of the kernel strongly

depends on the properties of the dataset, in particular on the long tail distribu-

tion. We compare our method with state-of-the-art algorithms achieving good

results both in terms of eﬃciency and eﬀectiveness.

Keywords: Top-N recommendation, Kernel, Collaborative Filtering, Large

Scale

1. Introduction

Collaborative ﬁltering (CF) techniques can make recommendation to a user

exploiting information provided by similar users. The typical CF setting consists

of a set U of n users, a set I of m items, and the so-called rating matrix

Preprint submitted to Elsevier

December 20, 2016

R = {rui} ∈ Rn×m. Ratings represent, numerically, the interactions between

users and items. These interactions can be of two diﬀerent types: explicit

feedbacks and implicit feedbacks. The former are interactions performed by

users in a direct way, such as, by giving a one-to-ﬁve star rate, thumbs-up

versus thumbs-down, and so on. The latter, instead, are actions performed by

users without the awareness of giving some kind of feedback to the system, e.g.,

clicks, views, elapsed time on a web page and so on.

The explicit setting have got most of the attention from the CF research

community and a lot of rating prediction algorithms have been proposed. More

recently however, the focus is constantly shifting towards the implicit feed-

back scenario since users are not always willing to give their opinion explicitly.

Furthermore, implicit interactions are easier to collect: for an already existing

system, it is suﬃcient to store the operations done by a user in a session without

changing the front-end and consequently there are no additional burden for the

user.

One problem arising in implicit feedback is that typically the feedback is

asymmetric, that is, while there can be evidence that a certain user interacted

and hence showed some interest towards a product (unambiguous feedback), the

opposite is not true, that is the missing evidence of interaction does not imply

that a given user dislikes that item.

In this paper we focus on the implicit feedback scenario, and so we assume

binary ratings, rui ∈ {0, 1}, where rui = 1 means that user u interacted with

the item i (unambiguous feedback) and rui = 0 means there is no evidence that

user u interacted with the item i (ambiguous feedback).

Speciﬁcally, the main contribution of this paper is a CF framework for top-N

item recommendation based on the seminal work described in [1]. Starting from

the original convex optimization problem, we propose an eﬃcient variation of

that formulation which makes the algorithm able to manage very large scale

datasets, by preserving the eﬀectiveness of the original algorithm. This new

formulation is then extended to be used with general kernels thus augmenting

the expressiveness of the representation in collaborative ﬁltering.

2

It is well known that kernels are not suited for large datasets since they

have a prohibitive computational complexity. Datasets used in CF domains are

usually large: rating matrices R have tens of thousands of rows and columns

and so the application of kernels seems to be diﬃcult. Nevertheless, we can

observe that the complexity of the computation of a kernel strictly depends on

its sparsity and hence being able to control the sparsity makes the application

of kernel methods to very large datasets feasible. Unfortunately, kernels are

usually dense unless we are working on very large and sparse feature spaces and

this is in contrast with what we would like to have.

Exploiting a well known result from harmonic theory [2], we are able to

demonstrate how using dot product kernels, a generalization of normalized ho-

mogeneous polynomial kernels, kernels as sparse as the standard cosine kernel

can be obtained without changing the solution of the problem.

However, it should be noted that there is no guarantees about the sparsity

of a cosine kernel because it is closely related to the distribution of the non-zero

values in the matrix R. CF datasets are known to be very sparse, and also they

are usually extracted from e-services which makes them subject to the long tail

phenomenon that is often associated with the power law distribution.

Even if not every long tail is a power law [3], this tailed distribution over

the item ratings in R poses a strong bias on the density of the resulting kernel.

In fact, assuming the long tail, in R there will be few dense columns and many

sparse ones, which intuitively leads to a rather sparse kernel. On the other side,

a long tailed distribution over the user ratings can lead to dense kernels matrices

since diﬀerent items tend to be rated by mostly the same users. We provide a

deep analysis of this phenomenon showing theoretically and empirically which

are the conditions for which a dataset is likely to produce a sparse dot product

kernel. It is worth to notice that these results are not only applicable to CF

contexts, but they apply on every other context where the data distribution is

long tailed.

Finally, in the experimental section we compare our framework with two

3

state-of-the-art methods in top-N recommendation. The empirical work shows

how our method can achieve good results in terms of AUC (Area Under the

ROC Curve) and also in terms of eﬃciency.

Summarizing, the contribution of this paper is 3-fold:

1. Starting from the seminal work described in [1] we propose an optimized

CF framework for top-N recommendation. Speciﬁcally, our proposed

method results far more eﬃcient of the original enabling the application of

the method to large and very large datasets (e.g. in the order of 1 million

of users/items and 50 million of rates) while preserving the state-of-the-art

eﬀectiveness of the original algorithm.

2. The formulation depicted above is generalized to be used with non-linear

kernels as dot-product kernels, that is kernel functions in the form k(x, y) =

f (x · y) where f is a non-linear function admitting a Maclaurin expansion

with non-negative coeﬃcients. A sparsiﬁcation method for dot-product

kernels is also provided making the sparsity of any dot-product kernel

equal to the sparsity of the linear or cosine kernel.

3. A theoretical and empirical analysis concerning the sparsity of the stan-

dard cosine kernel is proposed. This analysis shows that the sparsity of

the kernel produced depends on the properties of the user activity and

item popularity long-tails. In particular, long-tails observed on item pop-

ularity improve the sparsity of the kernels while long-tails observed on

user activity is detrimental for the sparsity of the kernels.

This paper is an extended version of a preliminary paper presented at ESANN

2016 [4]. In particular, preliminary work about the contribution 1 and 2 above

was already presented in that work while contribution 3 is novel. Moreover, the

extensive experimental work of this paper was not present in the preliminary

version.

The rest of the paper is organized as follows. In Section 2 we will introduce

the notation used throughout the paper. Section 3 presents related works on

4

top-N recommendation for implicit feedback. Sections 4 and 5 describes our

framework with a particular focus on the applicability of our proposed kernel

method. Finally, Sections 6 and 7 show the experimental results and which

directions our research can follow in the future.

2. Notation

In this section we provide some useful notation used throughout the paper.

Recommender algorithms are thought to give suggestions to users about items.

We call the set of users as U such that |U| = n, the set of items as I such that

|I| = m and the set of ratings R = {(u, i)}.

We refer to the binary rating matrix with R = {rui} ∈ Rn×m, where users

are on the rows and items on the columns. We add a subscription to both user

and item sets to indicate, respectively, the set of items rated by a user u (Iu)

and the set of users who rated the item i (Ui).

3. Related works

Top-N recommendation ﬁnds application in many diﬀerent domains such as

TV and movies [5], books [1], music [6, 7], social media [8] and so on.

Top-N recommendation methods can be divided into two macro categories.

The ﬁrst is the neighbourhood-based CF algorithms [9], in which the rec-

ommendation for a target user is made by using the ratings of the most similar

users. This category comprises the so called memory-based methods that do

not need the construction of a model, but they directly use the data inside the

rating matrix.

Despite, in general, these methods suﬀer from low accuracy, in 2013 the win-

ner of the remarkable challenge organized by Kaggle, the Million Songs Dataset

challenge [10], was an extension of the well known item-based nearest-neighbors

(NN) algorithm [11]. This extension [6] (here called MSDW) introduced an

5

asymmetric similarity measure, called asymmetric cosine.

In a classic item-

based CF method, the scoring function for a user-item pair (u, i) is computed

by a weighted sum over the items liked by u in the past, that is:

ˆrui =

wijruj =

wij,

(cid:88)

j∈I

(cid:88)

j∈Iu

where wij expresses the similarity between item i and item j.

One of the main contribution, presented in [6], is the asymmetric cosine

(asymC) similarity. The intuition behind asymC comes from the observation

that the cosine similarity can be expressed, over a pair of items (a, b), as the

square root of the product of the reciprocal conditional probabilities. Let a, b ∈

{0, 1}n be respectively the binary vector representations of items a and b. The

idea of asymmetric cosine similarity is to give diﬀerent weights to the conditional

probabilities, that is

Sα(a, b) =

(cid:107)a(cid:107)2α(cid:107)b(cid:107)2(1−α) = P (a|b)αP (b|a)1−α,

a(cid:62)b

with 0 ≤ α ≤ 1.

In case of binary rates this asymmetric similarity can be

computed as in the following. Let Ui represents the set of users who rated the

item i, then the asymC between item i and item j is deﬁned by:

wij = Sα(i, j) =

|Ui ∩ Uj|
|Ui|α|Uj|1−α .

Besides its outstanding performance in terms of mAP@500, the MSD winning

solution is also easily scalable to very large datasets. However, one drawback of

this solution is that it is not theoretically well founded.

The second category is the model-based CF techniques, which construct a

model of the information contained in the rating matrix R.

In the last two

decades many diﬀerent model-based approaches have been proposed. A partic-

ular attention has been devoted to latent factor models which try to factorize

the rating matrix into two low-rank matrices, R = WX, which represent user-

factors (W) and item-factors (X). These factors are “meta-features” that deﬁne

the user tastes and how much of these features represent an item. Usually these

methods are referred to as matrix factorization methods. The prediction for a

6

user-item pair is simply done by a dot product of the corresponding row and

column in the factorized matrices.

One of the most used matrix factorization approach for implicit feedback

is presented in [5] (WRMF: Weighted Regularized Matrix Factorization).

In

this work Hu et al. propose an adaptation of the classic SVD (Singular Value

Decomposition) method in which they minimize the square-loss using two reg-

ularization terms in order to avoid overﬁtting. Their optimization criterion is

deﬁned as:

(cid:88)

(cid:88)

u∈U

i∈I

cui(w(cid:62)

u xi − 1)2 + λ(cid:107)W(cid:107)2 + λ(cid:107)X(cid:107)2,

where cui are a-priori weights for each pair (u, i) such that positive feedbacks

have higher weights. Actually, this method uses information about the rating

values (not binary) to give more importance to user-item interactions with high

rating values, in fact, cui is calculated by: cui = 1 + αrui. In their experiments

the best performances has been achieved with α = 40, but with binary rating

matrices this parameter losses a lot of its importance. As we will see in the

experimental section, changing the value of α does not change the performance

of the algorithm. A similar approach has been used by Wang et al. [12] where

they proposed a framework for broadcast email prioritization based on a novel

active learning method.

In [13] the top-N recommendation problem has been formulated as a ranking

problem. Rendle et al. proposed a Bayesian Personalized Ranking (BPR) crite-

rion that is the maximum posterior estimator derived from a Bayesian analysis.

They also provide a learning method based on stochastic gradient descent with

bootstrap sampling. They ﬁnally show how to adopt this criterion for kNN

(BPRkNN) and MF methods (BPRMF).

In [14] Ning et al. presented SLIM (Sparse LInear Method) which generates

top-N recommendation by aggregating from user rating proﬁles. SLIM learns

an aggregation coeﬃcient matrix by solving a regularized optimization problem.

More recently, a new principled algorithm for CF, which explicitly optimizes

the AUC, has been proposed with very nice performances on the MovieLens

7

dataset [1]. Since this algorithm represents the seminal work for our framework,

we will discuss about it in details in the next section.

A more direct approach in order to build a good ranking over the items is

the so called learning to rank [15]. Learning to rank methods exploit supervised

machine learning to solve ranking problems. These techniques can be divided

into three categories: pointwise approaches [8, 16, 17] in which for each query-

document (i.e., user-item) pair a score is predicted and then used to build the

ranking; pairwise approaches [18, 19] face the ranking problem as a binary

classiﬁcation one (positive document versus negative ones) in which they try to

minimize the number of inversions in the ranking; listwise approaches [20, 21] try

to directly optimize one of the ranking evaluation measures. The big challenge

here is the fact that most of the measures are not continuous function w.r.t. the

model’s parameters and for this reason approximations have to be used.

4. CF-OMD framework

4.1. CF-OMD

In this section we present the seminal CF algorithm, called CF-OMD (Op-

timization of the Margin Distribution) [1], for top-N recommendation inspired

by preference learning [22][23], and designed to explicitly maximize the AUC

(Area Under the ROC Curve).

Consider the matrix W ∈ Rn×k be the embeddings of users in a latent factor
space and X ∈ Rk×m be the embeddings of items in the space. Given a user,
a ranking over items can be induced by the factorization ˆR = WX, where
ˆrui = w(cid:62)

u xi with the constraint (cid:107)wu(cid:107) = (cid:107)xi(cid:107) = 1. The model parameters (i.e.,

W, X) are generally computed minimizing a regularized loss function over the

ground truth matrix R. Let now ﬁx the item representation as xi = ri/(cid:107)ri(cid:107),
and let ρ(i ≺u j) = (ˆrui − ˆruj)/2 = w(cid:62)

u (xi −xj)/2 be the margin for an item pair

(i, j) for user u. Let also deﬁne the probability distribution over the positive

and negative items for u,

Au = {αu ∈ Rm
+ |

αui = 1,

αui = 1}.

(cid:88)

i /∈Iu

(cid:88)

i∈Iu

8

In [1] it is proposed an approach to maximize the minimum margin inspired by

preference learning where the ranking task is posed as a two-player zero-sum

game. Let Pmax and Pmin be the players: on each round of the game, Pmin

picks a preference i ≺u j and, simultaneously, Pmax picks an hypothesis wu

with the aim of maximizing the margin ρ(i ≺u j). The value of the game, i.e.,

the expected margin, is computed by:

Eα[ρ] =

w(cid:62)

u XYuαu,

1
2

where Yu is a diagonal matrix, Yu = diag(yu), such that yui = 1 if i ∈ Iu, −1
otherwise. It can be demonstrated that the w∗

u maximizing the expected margin

is equal to w∗

u = XYuαu normalized. Finally, the best strategy for Pmin can

be expressed as a convex quadratic optimization problem:

α∗

u = argmin
αu∈Au

α(cid:62)
u

(cid:0)YuX(cid:62)XYu + Λ(cid:1) αu,

(1)

in which Λ is a diagonal matrix such that Λii = λp if i ∈ Iu, otherwise Λii = λn,

where λp and λn are regularization parameters (λp, λn ≥ 0).

Although this algorithm has shown state-of-the-art results in terms of AUC,

it is not suitable to deal with large datasets.

In fact, let assume that each

optimization problem can be solved by an algorithm with a complexity quadratic

on the number of parameters. Then the global complexity would be O(ntsm2),

where nts is the number of users in the test set, and for the MSD it would be
O(1019).

4.2. Eﬃcient CF-OMD

The main issue of CF-OMD, in terms of eﬃciency, is the number of param-

eters which is equal to the cardinality of the item set. Analyzing the results

reported in [1], we noticed that high values of λn did not particularly aﬀect the

results, because it tends to ﬂatten the contribution of the ambiguous negative

feedbacks toward the average, mitigating the relevance of noisy information.

In CF contexts the data sparsity is particularly high, this means, on average,

that the number of ambiguous negative feedbacks is orders of magnitude greater

9

than the number of positive feedbacks. Formally, given a user u, let m+

u = |Iu|

and m−

u = |I \ Iu| then m = m−

u + m+

u , where m+

u (cid:28) m−

u , and generally

O(m) = O(m−

u ).

On the basis of this observation, we can simplify the optimization problem

(1), by ﬁxing λn = +∞, which means that ∀i /∈ Iu, αui = 1/m−
u :

α∗

u = argmin

(cid:107)α(cid:62)

u+Xu+ − µ−

u (cid:107)2 + λp(cid:107)αu+(cid:107)2

αu

αu

= argmin
αu+ ∈Au

= argmin

(cid:107)α(cid:62)

u+Xu+(cid:107)2 − (cid:107)µ−

u (cid:107)2 − 2α(cid:62)

u+X(cid:62)

u+µ−

u + λp(cid:107)αu+(cid:107)2

α(cid:62)

u+X(cid:62)

u+Xu+αu+ + λp(cid:107)αu+(cid:107)2 − 2α(cid:62)

u+X(cid:62)

u+µ−
u ,

(2)

(3)

(4)

where

µ−

u =

1
m−
u

(cid:88)

xi

i /∈Iu

is the centroid of the convex hull spanned by the negative items and αu+ are

the probabilities associated with the positive items, Xu+ is the sub-matrix of X

containing only the columns corresponding to the positive items. The number
u and hence the complexity from O(ntsm2) drops to
u = E[|Iu|] is the expected cardinality of the positive item

), where m+

2

of parameters in (4) is m+
O(ntsm+
u
set. In MSD m+

u ≈ 47.46 which leads to a complexity O(108).

4.2.1. Implementation trick

Notwithstanding the huge improvement in terms of complexity, a na¨ıve im-

plementation would have an additional cost due to the calculation of µ−
all users in the test set the cost would be O(ntsnm−

u . For
u = E[|I \ Iu|],

u ), where m−

and it can be approximated with O(ntsnm).

To overcome this bottleneck, we propose an eﬃcient incremental way of

calculating µ−

u . Consider the mean over all items

then, for a given user u, we can express

µ =

1
m

(cid:88)

i∈I

xi,

µ−

u =

1
m−
u

(cid:32)

m · µ −

(cid:33)

xi

.

(cid:88)

i∈Iu

10

From a computational point of view, it is suﬃcient to compute the sum (cid:80)
once (i.e., m · µ) and then, for every µ−

i∈I xi
u , subtract the sum of the positive items.

Using this simple trick, the overall complexity drops to O(nm) + O(n2

tsm+

u ).

In the experimental section we successfully applied this algorithm to the

MSD achieving competitive results against the state-of-the-art method but with

higher eﬃciency.

5. Kernelized CF-OMD

The method proposed in Section 4.2, can be seen as a particular case of a

kernel method. In fact, X(cid:62)
corresponding (linear) kernel function K : Rm+

u+Xu+ is a kernel matrix, let call it Ku+ with the
u → R. Given K we can

u × Rm+

reformulate (4) as:

α∗

u+ = argmin
αu+ ∈Au

α(cid:62)

u+Ku+αu+ + λp(cid:107)αu+(cid:107)2 − 2α(cid:62)

u+qu,

(5)

where elements of the vector qu ∈ Rm+

u are deﬁned as

qui =

K(xi, xj).

1
m−
u

(cid:88)

j /∈Iu

Actually, inside the optimization problem (5) we can plug any kernel func-

tion. Throughout the paper we will refer to this method as CF-KOMD. Gen-

erally speaking, the application of kernel methods on a huge dataset have an

intractable computational complexity. Without any shrewdness the proposed

method would not be applicable because of the computational cost of the kernel

matrix and qu.

An important observation is that the complexity is strictly connected with

the sparsity of the kernel matrix which is, unfortunately, commonly dense. How-

ever, we can leverage on a well known result from harmonic theory [2] to keep

the kernel as sparse as possible without changing the solution of CF-KOMD.

Theorem 1. A function f : R → R deﬁnes a positive deﬁnite kernel k :

B(0, 1) × B(0, 1) as k : (x, y) (cid:55)→ f (x · y) iﬀ f is an analytic function admitting a
Maclaurin expansion with non-negative coeﬃcients, f (x) = (cid:80)∞
s=0 asxs, as ≥ 0.

11

As emphasized in [2, 24], many kernels used in practice [25] satisfy the above-

mentioned condition. These kernels are called dot product kernels because they

are deﬁned as a function of the dot product of the input vectors. Table 1 gives

some example of these kind of kernels.

Kernel

Deﬁnition

Coeﬃcients as

Linear

x · y

1, s = 1

Polynomial

(x · y + c)d

(cid:0)d
n

(cid:1)c(d−s), ∀s ∈ [0, d]

RBF

e−γ(cid:107)x−y(cid:107)2

e−2γ (2γ)2s

s!

, ∀s

Tanimoto

x·y
(cid:107)x(cid:107)+(cid:107)y(cid:107)−x·y

2−s, ∀s > 0

Table 1: Some examples of dot product kernels.

We can observe that the kernel matrices induced by these kernels are, in

general, dense due to the zero degree term (i.e., s = 0) which is a constant

added to all the entries. Adding a constant to a whole matrix means a space

translation and we can demonstrate that this operation does not aﬀect the

margin in CF-KOMD (this is valid also in the generic formulation of CF-OMD).

Proof. Let K = K0+ ˆK be a dot product kernel matrix where K0 is the constant

matrix induced by the 0 degree term of the MacLaurin expansion (i.e. s = 0).

Let also qu be consequently deﬁned as:

qui =

1
m−
u

j /∈Iu

m−

=

1
m−
u

(cid:88)

(cid:16)

(cid:17)
K0(xi, xj) + ˆK(xi, xj)

u · k0 +

ˆK(xi, xj)

(cid:88)

j /∈Iu





= k0 + ˆqui ⇒ qu = k0 + ˆqu.

(6)

(7)

(8)

12

We can rewrite the optimization problem (5) as (we omit the u+ subscription

for brevity):

α∗ = argmin
α∈Au

= argmin
α∈Au

α(cid:62)(K0 + ˆK)α + λp(cid:107)α(cid:107)2 − 2α(cid:62)(k0 + ˆqu)

(9)

α(cid:62)K0α + α(cid:62) ˆKα + λp(cid:107)α(cid:107)2 − 2α(cid:62)k0 − 2α(cid:62) ˆqu

(10)

where both α(cid:62)K0α and −2α(cid:62)k0 are constant values independent from α:

α(cid:62)K0α = k0

αiαj = k0

αj = k0;

(cid:88)

(cid:88)

i∈Iu

j∈Iu

(cid:88)

i∈Iu

(cid:88)

(cid:88)

αi

i∈Iu

j∈Iu

(cid:88)

i∈Iu

−2α(cid:62)k0 = −2

k0αi = −2k0

αi = −2k0;

and hence the solution of the optimization problem does not depend on K0:

α∗ = argmin
α∈Au

α(cid:62) ˆKα + λp(cid:107)α(cid:107)2 − 2α(cid:62) ˆqu.

and it is the same as in (5).

For this reason we can “sparsify” these kernels (we will call theme RDP

Kernels: Reduced Dot Product Kernels) by removing the zero degree factor

obtaining kernel matrices whose sparsities depend only on the distribution of

the input data since they are deﬁned as linear combination of powers of dot

products. This also implies that the sparsity of a RDP kernel is exactly the

same as in the simple linear kernel (i.e., K = X(cid:62)X).

5.1. Sparsity and long tail distribution

As mentioned in Section 4.2, CF datasets are, in most of the cases, very

sparse and in general the distribution of the ratings has a long tail form from

the items perspective [26]. This means that a small set of items, the most

popular ones, receive great part of the whole set of ratings.

What we need to understand are the conditions under which a RDP kernel

remains suﬃciently sparse. To study this phenomenon we use the linear kernel,

but results also apply for every other RDP kernel as well because they contain

exactly the same zero entries as the linear one (which is in fact a special case of

13

RDP kernel).

Let K = X(cid:62)X (X ∈ Rn×m) be a kernel matrix and let P(Kij (cid:54)= 0) be

the probability that the entry Kij is not zero. Given an a-priori probability

distribution over the ratings, and assuming the independence of the ratings, we

can estimate the probability of having a value diﬀerent from zero in the kernel

matrix with:

P(Kij (cid:54)= 0) = 1 − P(Kij = 0)

= 1 −

P(xih · xjh = 0)

(cid:89)

h
(cid:89)

h
(cid:89)

h

= 1 −

(1 − P(xih · xjh (cid:54)= 0))

= 1 −

(1 − P(xih (cid:54)= 0)P(xjh (cid:54)= 0))

= 1 − (1 − P(xih (cid:54)= 0) · P(xjh (cid:54)= 0))n

(11)

(12)

(13)

(14)

(15)

where P(xih (cid:54)= 0) and P(xjh (cid:54)= 0) are the probability of having a non zero

entry in the rating matrix. It is worth to notice that this probability, in the

uniform case, is actually the density of the matrix. However, it does not take

into account the fact that all the elements in the diagonal of the kernel are for

sure non zero. With this consideration in mind we can deﬁne an estimate of the
kernel density d(K), with K ∈ Rm×m, as follows:

d(K) =

(cid:2)m + (m2 − m)P(Kij (cid:54)= 0)(cid:3) .

1
m2

Intuitively, we can argue that anytime both xi and xj are popular items,
i.e., P(xih (cid:54)= 0) and P(xjh (cid:54)= 0) are close to 1, then P(Kij (cid:54)= 0) tends to be high

and hence K is likely to be dense. On the contrary, when one of the two vectors
represents an unpopular item, then the probability P(Kij (cid:54)= 0) is likely close to

zero. Since we are assuming a long tail distribution over the items, most of the

kernel entries result from a dot product of two unpopular items with very few

users that rated them and so the kernel tends to be sparse.

However, up to now, we are assuming a uniform distribution over the users

14

and in real datasets this is often not the case. Any other probability distribution

would highly aﬀect our estimation d(K). This is because, if we assume a long

tail distribution over the users, the probability of having at least one user in

common between two items would be generally high, since the ratings for an

item are likely to be concentrated to the (few) most active users. Considering

that a mathematical proof of this intuition is quite complicated, in the next

section we provide an empirical analysis on real CF datasets.

5.1.1. Empirical analysis of CF datasets

In order to validate our thesis, we empirically analyze a set of famous CF

datasets comparing the theoretical sparsity with uniform ratings distribution

with the sparsity of the linear kernel. We expect that the long tail distribution

of the ratings will tend to lower the likelihood of having a non zero value in the

kernel matrix, with respect to the d(K) estimate.

The empirical analysis has been performed as follows: for each dataset, we

build the corresponding rating matrix R, we calculate the expected density
using (11) by ﬁxing P(xih (cid:54)= 0) equals to the density of R; we calculate the
linear kernel K = R(cid:62)R and ﬁnally we compare the sparsity of the kernel with

d(K). Table 2 summarizes the results.

Dataset

Density R Density K d(K)

Book-Crossing

0.003%

0.687%

0.011%

Delicious

LastFM

Movielens 10M

Netﬂix

Ciao

FilmTrust

0.08%

0.28%

1.34%

0.99%

0.025%

1.13%

0.128%

0.85%

0.13%

1.48%

85.56%

99.99%

98.03%

99.99%

2.51%

0.12%

11.11%

17.74%

Table 2: Analysis of the sparsity of the linear kernel.

Although our intuition seems to work with most of the datasets, we can

notice that with the Ciao and Book Crossing datasets the kernels are more

15

dense than the estimation d(K).

The reason why these datasets behave diﬀerently is clearly depicted in the

plots 1 - 7. Every pair of plots show the distribution of the item popularity

and the user activity. The plots have a loglog scale and the blue line represents

the best ﬁtting power low function. The ﬁtting has been made using the least

square method.

(a) Item Popularity

(b) User Activity

Figure 1: Book Crossing. The plots are in loglog scale.

(a) Item Popularity

(b) User Activity

Figure 2: Ciao. The plots are in loglog scale.

From the plots we can observe that:

• none of the item distributions follows exactly a power low, especially in

the head of the distribution;

16

(a) Item Popularity

(b) User Activity

Figure 3: Delicious. The plots are in loglog scale.

(a) Item Popularity

(b) User Activity

Figure 4: Film Trust. The plots are in loglog scale.

(a) Item Popularity

(b) User Activity

Figure 5: LastFM. The plots are in loglog scale.

17

(a) Item Popularity

(b) User Activity

Figure 6: Movielens. The plots are in loglog scale.

(a) Item Popularity

(b) User Activity

Figure 7: Netﬂix. The plots are in loglog scale.

• datasets with a very dense kernel tend to have shorter tails: the right part

of the plot exceed the power low line;

• generally items are long tailed while users tend to be more uniform;

• users distributions are in general not well ﬁtted by a power law, with the

exception of Ciao and Book Crossing datasets.

The observations listed above point out that Ciao and Book Crossing are

the only two datasets with a well deﬁned long tail distribution over both users

and items. This conﬁrm our intuition about the likelihood of having a denser

kernel with both long tailed distributions.

18

In conclusion, the long tail distribution over the items keeps the RDP kernels

sparse while a long tail distribution over the users increase the density.

5.2. Approximation of qu

Using the RDP kernels, we can further optimize the complexity by providing

a good approximation of qu that can be computed only once, instead of nts
times. The idea consists in replacing every qui with an estimate of E[K(xi, x)]

which is the expected value of the kernel between the item i and every other

items. Formally, consider, without any loss of generality, a normalized kernel

function K and let the approximation of qu be ˜q such that:

˜qi =

K(xi, xj).

1
m

(cid:88)

j∈I

At each component of ˆq, the approximation error is bounded by 2m+

u

m , which is

linear on the sparsity of the dataset.

Proof.

|ˆqi − qui| =

K(xi, xj) −

K(xi, xj)

(16)

K(xi, xj) +

K(xi, xj)

 −

K(xi, xj)

(17)

(cid:88)

j∈I


(cid:88)



j∈Iu

(cid:88)

j∈Iu

(cid:88)

1
m

1
m

1
m

1
m

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

=

≤

≤

(cid:12)
(cid:12)
(cid:12)
K(xi, xj)
(cid:12)
(cid:12)
(cid:12)

+

m − m−
u
m · m−
u

m−
u

j∈Iu
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

m+
u
m

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)



1
m−
u

(cid:88)

j /∈Iu

(cid:88)

j /∈Iu

(cid:88)

j /∈Iu

1
m−
u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

j /∈Iu

m − m−
u
m · m−
u
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m − m−
u
m · m−
u

m+

≤

(cid:88)

(cid:12)
(cid:12)
(cid:12)
K(xi, xj)
(cid:12)
(cid:12)
(cid:12)
m+
u
m

= 2

.

j /∈Iu
u + m − m−
u
m

K(xi, xj) −

K(xi, xj)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(18)

(19)

(20)

6. Experiments and Results

Experiments have been performed comparing the proposed methods against

the state-of-the-art method on MSD (MSDW) with respect to the ranking qual-

19

ity and computational performance.

We also compared our framework, in terms of AUC, against other state-

of-the-art methods on top-N recommendation with implicit feedback, namely

Our framework and MSDW are both implemented in Python123, while for

WRMF and BPR we used the java implementation provided by the open source

WRMF and BPR.

LibRec library 4.

6.1. Datasets

In this section we introduce the datasets used in the experiments. Table 3

shows a brief description of the datasets.

Dataset

Item type

|U|

|I|

MovieLens

Movies

6040

3706

|R|

1M

MSD

Ciao

Netﬂix

Music

1.2M 380K

48M

General

17615

16121

72664

93705

3561

3.3M

FilmTrust

1508

2071

35496

Movie

Movie

Table 3: Brief description of the used datasets.

The MSD dataset is used only to demonstrate the applicability of our kernel

method to huge datasets. All the other datasets are used to compare our frame-

work with the state-of-the-art method in top-N recommendation with implicit

feedback.

6.2. Experimental setting

Experiments have been performed 5 times for each dataset. Datasets have

been pre-processed as described in the following:

1We used CVXOPT package to solve the optimization problem
2The MSDW implementation is available at http://www.math.unipd.it/ aiolli/CODE/MSD/
3The framework implementation is available at https://github.com/makgyver/pyros
4http://www.librec.net/

20

1. we split randomly the users in 5 sets of the same dimension;

2. for each user in a set we further split its ratings in two halves;

3. at each round test, we use all the ratings in 4 sets of users plus the ﬁrst

half of ratings of the remaining set as training set, and the rest as test set.

This setting avoids situations of cold start for users, because in the training

phase we have at least a rating for every user. We also force users with less

than 5 ratings to be in the training set. The results reported below are the

averages (with its standard deviations) over the 5 folds.

6.3. Evaluation measures

The rankings’ evaluation metric used to compare the performances of the

methods is the AUC (Area Under the receiver operating characteristic Curve)

deﬁned as in the following:

AUC =

1
|U|

(cid:88)

u∈U

1
|Iu| · |I \ Iu|

(cid:88)

(cid:88)

i∈Iu

j /∈Iu

I[ˆrui > ˆruj]

where I : Bool → {0, 1} is the indicator function which returns 1 if the predicates

is true 0 otherwise.

In the experiments which use the MSD dataset we also

compared the algorithms using the same metric as in the challenge, that is the

Mean Average Precision at 500 (mAP@500), which is the mean over all users

of the average precision at N (N = 500). Formally is deﬁned as:

AP (πu)@N =

P (πu)@k · ˆruπu(k),

1
min(|Iu|, N )

N
(cid:88)

k=1

where Iu is the set of positive associated items with the user u, πu is the items

ranking for user u, such that πu(k) = i means that item i is ranked at position

k, and P (πu)@k is the precision at k:

P (πu)@k =

ˆruπu(p).

1
k

k
(cid:88)

p=1

21

6.4. Results

6.4.1. MSD

We used MSD as described in the Kaggle challenge5: the training set is

composed by 1M users (plus 10K users as validation set) with all their listening

history and for the rest (i.e., 100K users) only the ﬁrst half of the history is

provided, while the other half constitutes the test set. In these experiments we

ﬁxed the λp parameter to 0.01.

Results are presented in Table 4. In this case MSDW maintains its record

performance in terms of mAP@500, while for the AUC all methods have very

good results. This underline the fact that both ECF-OMD and CF-KOMD

(polynomial kernel with c = 1) try to optimize the AUC rather than the mAP.

MSDW (α, q) ECF-OMD (λp) CF-K (λp)

0.15, 3

mAP@500

0.16881

AUC

0.97342

0.1

0.16391

0.97034

0.1

0.15967

0.97065

Table 4: Ranking accuracy on MSD using AUC and mAP@500.

The computational costs on this dataset are reported in Figure 8.

Figure 8: Average computational time in hours for 1K users.

The results are the average computing time over 1K test users. All methods

run on a machine with 150Gb of RAM and 2 x Eight-Core Intel(R) Xeon(R)

CPU E5-2680 0 @ 2.70GHz. Actually the times in Figure 8 have a constant

overhead due to read operations. Results show that ECF-OMD and CF-K

5https://www.kaggle.com/c/msdchallenge

22

(abbreviation for CF-KOMD) are almost 5 time faster than MSDW even though

they require more RAM to store the kernel matrix. It is worth to notice that

CF-K has a computational time very close to ECF-OMD, and this highlights

the positive eﬀects of the complexity optimization presented in this paper.

6.4.2. Other datasets

This section shows the performance for the top-N recommendation task, in

terms of AUC, achieved by our framework. We compared our methods with

some state-of-the-art techniques. In particular, we used the following settings

for each of the competing algorithm:

ECF-OMD : we ﬁxed the regularization parameter λp = 0.01;

CF-KP : it is the CF-KOMD method with the polynomial kernel. We tested

diﬀerent values for c ∈ {0.5, 1, 2, 4} and we ﬁxed λp = 0.01;

CF-KT : it is the CF-KOMD method with the tanimoto kernel. We ﬁxed the

regularization parameter λp = 0.01;

MSDW : tests have been performed varying the value of the α parameter in

the real range [0,1] with a step of 0.25;

WRMF : we reported only the performance achieved with α = 1 since the

results obtained with diﬀerent α were substantially the same. We ﬁxed

the maximum number of iteration to 30, the learning rate ρ = 0.001, the

regularization term λ = 0.001 and the number of factor k = 100;

BPR : we do not have free parameter. We ﬁxed, as in WRMF, the maximum

number of iteration to 30, the learning rate ρ = 0.001 and the regulariza-

tion term λ = 0.001.

Table 5 summarizes the obtained results.

On the MovieLens dataset, methods of our framework have signiﬁcant better

performance against all the other, achieving an AUC of 0.896 with CF-KOMD

with the polynomial kernel (c = 4). CF-KOMD has the higher AUC (0.964)

23

MovieLens

Netﬂix

FilmTrust

Ciao

MSDW α = 0.00

0.867±0.001

0.939±0.0002

0.961±0.004

0.824±0.008

MSDW α = 0.25

0.870±0.001

0.939±0.0002

0.960±0.005

0.813±0.01

MSDW α = 0.50

0.875±0.001

0.936±0.0006

0.959±0.005

0.805±0.01

MSDW α = 0.75

0.862±0.001

0.912±0.0001

0.955±0.005

0.793±0.008

MSDW α = 1.00

0.458±0.007

0.409±0.019

0.833±0.011

0.784±0.009

ECF-OMD

0.895±0.0003

0.943±0.001

0.961±0.005

0.718±0.006

CF-KP

c = 0.50

0.893±0.0003

0.937±0.001

0.961±0.006

0.731±0.003

CF-KP

c = 1.00

0.893±0.0003

0.937±0.001

0.960±0.006

0.730±0.003

CF-KP

c = 2.00

0.894±0.0003

0.938±0.001

0.959±0.006

0.721±0.003

CF-KP

c = 4.00

0.896±0.0003

0.938±0.001

0.958±0.006

0.718±0.003

CF-KT

0.895±0.0004

0.941±0.001

0.964±0.005

0.734±0.004

WRMF α = 1.00

0.870±0.006

0.804±0.002

0.947±0.007

0.565±0.004

BPR

0.854±0.005

0.843±0.001

0.954±0.008

0.549±0.004

-

-

-

Table 5: AUC results of our framework against state-of-the-art methods.

also in the FilmTrust dataset, but this time with the Tanimoto kernel. In this

dataset, anyway, the polynomial achieved result comparable with ECF-OMD.

Good results are also achieved by the MSDW. On the Netﬂix dataset, the best

AUC is 0.943 by ECF-OMD. We got similar result with the Tanimoto kernel.

Surprisingly, with the Ciao dataset, MSDW obtained very good result while all

the other approaches are quite behind,n particular, WRMF and BPR have a

very poor performances.

We also compared the execution time of the algorithm. All these experiments

has been made on a MacBook Pro late 2012 with 16GB of RAM and CPU

Intel(R) Core i7 @ 2.70GHz. The results are shown in Figure 9. The ﬁrst thing

we can notice is that the WRMF algorithm is always the most time consuming

one. To keep the plot readable, we cut the bars longer than 100 minutes.

Actually, in both Netﬂix and Ciao datasets WRMF took more than 7 hours

24

Figure 9: Execution time took by the tested methods.

(i.e., 420 minutes). The two kernel based methods, CF-KP and CF-KT took

almost the same time and they are a little bit slower than the linear (ECF-

OMD) method which is often the fastest one. MSDW is in general quite fast,

but it seems to suﬀer when the number of items increase (e.g., Ciao dataset).

These results show how our framework have, both in eﬃcacy and eﬃciency,

performances at the state-of-the-art.

7. Conclusions

In this paper we have proposed a collaborative ﬁltering kernel-based method

for the top-N recommendation. The method belongs to a more general frame-

work, inspired by preference learning and designed to explicitly maximize the

AUC. We have also proposed a strategy for the “sparsiﬁcation” of the dot prod-

uct kernels and in which conditions this strategy works.

25

Our analysis, conducted over CF datasets, have shown the eﬀect of the long

tail distribution on the sparsity of the kernel. Since this kind of distribution is

very common, our results can apply in many domains other than CF. Finally,

the experiments we reported have shown that the proposed kernel-based method

achieve good result in terms of AUC and it is also eﬃcient even with large scale

datasets.

Acknowledge

project BIOINFOGEN.

References

This work was supported by the University of Padova under the strategic

[1] F. Aiolli, Convex AUC optimization for top-N recommendation with im-

plicit feedback, in: ACM Recommender Systems Conference, New York,

USA, 2014, pp. 293–296.

[2] P. Kar, H. Karnick, Random feature maps for dot product kernels, in:

N. D. Lawrence, M. A. Girolami (Eds.), Proceedings of the Fifteenth Inter-

national Conference on Artiﬁcial Intelligence and Statistics (AISTATS-12),

Vol. 22, 2012, pp. 583–591.

[3] P. Singer, Not every long tail is power law! (2013).

URL http://www.philippsinger.info/?p=247

[4] M. Polato, F. Aiolli, Kernel based collaborative ﬁltering for very large scale

top-n item recommendation, in: Proceedings of the European Symposium

on Artiﬁcial Neural Networks, Computational Intelligence and Machine

Learning (ESANN), 2016, pp. 11–16.

[5] Y. Hu, Y. Koren, C. Volinsky, Collaborative ﬁltering for implicit feedback

datasets, in: ICDM, 2008, pp. 263–272.

26

[6] F. Aiolli, Eﬃcient top-N recommendation for very large scale binary rated

datasets, in: ACM Recommender Systems Conference, Hong Kong, China,

2013, pp. 273–280.

2011, pp. 213–237.

[7] S. Tan, J. Bu, C. Chen, X. He, Using Rich Social Media Information for

Music Recommendation via Hypergraph Model, Springer London, London,

[8] B. Wang, C. Wang, J. Bu, C. Chen, W. V. Zhang, D. Cai, X. He, Whom

to mention: Expand the diﬀusion of tweets by recommendation on micro-

blogging systems, in: Proceedings of the 22Nd International Conference

on World Wide Web, WWW ’13, ACM, New York, NY, USA, 2013, pp.

1331–1340.

[9] B. M. Sarwar, G. Karypis, J. A. Konstan, J. Riedl, Item-based collaborative

ﬁltering recommendation algorithms, in: WWW, 2001, pp. 285–295.

[10] B. McFee, T. Bertin-Mahieux, D. P. Ellis, G. R. Lanckriet, The million

song dataset challenge, in: Proceedings of the 21st international conference

companion on World Wide Web, WWW ’12 Companion, ACM, New York,

NY, USA, 2012, pp. 909–916. doi:10.1145/2187980.2188222.

URL http://doi.acm.org/10.1145/2187980.2188222

[11] M. Deshpande, G. Karypis, Item-based top-n recommendation algorithms,

ACM Trans. Inf. Syst. 22 (1) (2004) 143–177.

[12] B. Wang, M. Ester, J. Bu, Y. Zhu, Z. Guan, D. Cai, Which to view:

Personalized prioritization for broadcast emails, in: Proceedings of the 25th

International Conference on World Wide Web, WWW ’16, International

World Wide Web Conferences Steering Committee, Republic and Canton

of Geneva, Switzerland, 2016, pp. 1181–1190.

[13] S. Rendle, C. Freudenthaler, Z. Gantner, L. Schmidt-Thieme, Bpr:

Bayesian personalized ranking from implicit feedback, in: Proceedings of

27

the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, UAI

’09, AUAI Press, Arlington, Virginia, United States, 2009, pp. 452–461.

[14] X. Ning, G. Karypis, SLIM: sparse linear methods for top-n recommender

systems, in: 11th IEEE International Conference on Data Mining, ICDM

2011, Vancouver, BC, Canada, December 11-14, 2011, 2011, pp. 497–506.

doi:10.1109/ICDM.2011.134.

[15] C. Zhe, Q. Tao, L. Tie-Yan, T. Ming-Feng, L. Hang, Learning to rank:

From pairwise approach to listwise approach, Tech. rep. (April 2007).

[16] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton,

G. Hullender, Learning to rank using gradient descent, in: Proceedings of

the 22Nd International Conference on Machine Learning, ICML ’05, ACM,

New York, NY, USA, 2005, pp. 89–96.

[17] V. C. Ostuni, T. Di Noia, E. Di Sciascio, R. Mirizzi, Top-n recommenda-

tions from implicit feedback leveraging linked open data, in: Proceedings

of the 7th ACM Conference on Recommender Systems, RecSys ’13, ACM,

New York, NY, USA, 2013, pp. 85–92.

[18] H. Zhong, W. Pan, C. Xu, Z. Yin, Z. Ming, Adaptive pairwise prefer-

ence learning for collaborative recommendation with implicit feedbacks, in:

Proceedings of the 23rd ACM International Conference on Conference on

Information and Knowledge Management, CIKM ’14, ACM, New York,

NY, USA, 2014, pp. 1999–2002.

[19] S. Rendle, C. Freudenthaler, Improving pairwise learning for item recom-

mendation from implicit feedback, in: Proceedings of the 7th ACM Inter-

national Conference on Web Search and Data Mining, WSDM ’14, ACM,

New York, NY, USA, 2014, pp. 273–282.

[20] Y. Shi, M. Larson, A. Hanjalic, List-wise learning to rank with matrix

factorization for collaborative ﬁltering, in: Proceedings of the Fourth ACM

28

Conference on Recommender Systems, RecSys ’10, ACM, New York, NY,

USA, 2010, pp. 269–272.

[21] S. Huang, S. Wang, T.-Y. Liu, J. Ma, Z. Chen, J. Veijalainen, Listwise col-

laborative ﬁltering, in: Proceedings of the 38th International ACM SIGIR

Conference on Research and Development in Information Retrieval, SIGIR

’15, ACM, New York, NY, USA, 2015, pp. 343–352.

[22] F. Aiolli, A preference model for structured supervised learning tasks, IEEE

International Conference on Data Mining (2005) 557–560.

[23] F. Aiolli, G. Da San Martino, A. Sperduti, A Kernel Method for the Op-

timization of the Margin Distribution, Springer Berlin Heidelberg, Berlin,

Heidelberg, 2008, pp. 305–314.

[24] M. Donini, F. Aiolli, Learning deep kernels in the space of dot product

polynomials, in: Machine Learning, 2017 (in press).

[25] B. Scholkopf, A. J. Smola, Learning with Kernels: Support Vector Ma-

chines, Regularization, Optimization, and Beyond, MIT Press, Cambridge,

MA, USA, 2001.

of More, Hyperion, 2006.

[26] C. Anderson, The Long Tail: Why the Future of Business Is Selling Less

29

