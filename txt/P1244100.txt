Towards a Seamless Integration of Word Senses
into Downstream NLP Applications

Mohammad Taher Pilehvar2, Jose Camacho-Collados1,
Roberto Navigli1 and Nigel Collier2

1Department of Computer Science, Sapienza University of Rome
2Department of Theoretical and Applied Linguistics, University of Cambridge
1{collados,navigli}@di.uniroma1.it
2{mp792,nhc30}@cam.ac.uk

Abstract

Lexical ambiguity can impede NLP sys-
tems from accurate understanding of se-
mantics. Despite its potential beneﬁts, the
integration of sense-level information into
NLP systems has remained understudied.
By incorporating a novel disambiguation
algorithm into a state-of-the-art classiﬁca-
tion model, we create a pipeline to inte-
grate sense-level information into down-
stream NLP applications. We show that
a simple disambiguation of the input text
can lead to consistent performance im-
provement on multiple topic categoriza-
tion and polarity detection datasets, par-
ticularly when the ﬁne granularity of the
underlying sense inventory is reduced and
the document is sufﬁciently large. Our re-
sults also point to the need for sense rep-
resentation research to focus more on in
vivo evaluations which target the perfor-
mance in downstream NLP applications
rather than artiﬁcial benchmarks.

1

Introduction

As a general trend, most current Natural Language
Processing (NLP) systems function at the word
level, i.e.
individual words constitute the most
ﬁne-grained meaning-bearing elements of their in-
put. The word level functionality can affect the
performance of these systems in two ways: (1)
it can hamper their efﬁciency in handling words
that are not encountered frequently during train-
ing, such as multiwords, inﬂections and deriva-
tions, and (2) it can restrict their semantic under-
standing to the level of words, with all their am-
biguities, and thereby prevent accurate capture of
the intended meanings.

The ﬁrst issue has recently been alleviated by

techniques that aim to boost the generalisation
power of NLP systems by resorting to sub-word
or character-level information (Ballesteros et al.,
2015; Kim et al., 2016). The second limitation,
however, has not yet been studied sufﬁciently. A
reasonable way to handle word ambiguity, and
hence to tackle the second issue, is to semantify
the input text: transform it from its surface-level
semantics to the deeper level of word senses, i.e.
their intended meanings. We take a step in this di-
rection by designing a pipeline that enables seam-
less integration of word senses into downstream
NLP applications, while beneﬁting from knowl-
edge extracted from semantic networks. To this
end, we propose a quick graph-based Word Sense
Disambiguation (WSD) algorithm which allows
high conﬁdence disambiguation of words without
much computation overload on the system. We
evaluate the pipeline in two downstream NLP ap-
plications: polarity detection and topic categoriza-
tion. Speciﬁcally, we use a classiﬁcation model
based on Convolutional Neural Networks which
has been shown to be very effective in various
text classiﬁcation tasks (Kalchbrenner et al., 2014;
Kim, 2014; Johnson and Zhang, 2015; Tang et al.,
2015; Xiao and Cho, 2016). We show that a simple
disambiguation of input can lead to performance
improvement of a state-of-the-art text classiﬁca-
tion system on multiple datasets, particularly for
long inputs and when the granularity of the sense
inventory is reduced. Our pipeline is quite ﬂexible
and modular, as it permits the integration of differ-
ent WSD and sense representation techniques.

2 Motivation

With the help of an example news article from the
BBC, shown in Figure 1, we highlight some of the
potential deﬁciencies of word-based models.

7
1
0
2
 
t
c
O
 
8
1
 
 
]
L
C
.
s
c
[
 
 
1
v
2
3
6
6
0
.
0
1
7
1
:
v
i
X
r
a

Algorithm 1 Disambiguation algorithm
Input: Input text T and semantic network N
Output: Set of disambiguated senses ˆS
1: Graph representation of T : (S, E) ← getGraph(T, N )
2: ˆS ← ∅
3: for each iteration i ∈ {1, ..., len(T )}
4:
5:
6:
7:
8:
9:
10:
11: return Disambiguation output ˆS

ˆs = argmaxs∈S |{(s, s(cid:48)) ∈ E : s(cid:48) ∈ S}|
maxDeg = |{(ˆs, s(cid:48)) ∈ E : s(cid:48) ∈ S}|
if maxDeg < θ|S| / 100 then

ˆS ← ˆS ∪ {ˆs}
E ← E \ {(s, s(cid:48)) : s ∨ s(cid:48) ∈ getLex(ˆs)}

break

else

towards resolving ambiguities, but it brings about
other advantages mentioned in the previous sec-
tion. The aim is to provide the system with an
input of reduced ambiguity which can facilitate its
decision making.

To this end, we developed a simple graph-based
joint disambiguation and entity linking algorithm
which can take any arbitrary semantic network
as input. The gist of our disambiguation tech-
nique lies in its speed and scalability. Conven-
tional knowledge-based disambiguation systems
(Hoffart et al., 2012; Agirre et al., 2014; Moro
et al., 2014; Ling et al., 2015; Pilehvar and Nav-
igli, 2014) often rely on computationally expen-
sive graph algorithms, which limits their applica-
tion to on-the-ﬂy processing of large number of
text documents, as is the case in our experiments.
Moreover, unlike supervised WSD and entity link-
ing techniques (Zhong and Ng, 2010; Cheng and
Roth, 2013; Melamud et al., 2016; Limsopatham
and Collier, 2016), our algorithm relies only on
semantic networks and does not require any sense-
annotated data, which is limited to English and al-
most non-existent for other languages.

Algorithm 1 shows our procedure for disam-
biguating an input document T . First, we retrieve
from our semantic network the list of candidate
senses1 for each content word, as well as seman-
tic relationships among them. As a result, we ob-
tain a graph representation (S, E) of the input text,
where S is the set of candidate senses and E is
the set of edges among different senses in S. The
graph is, in fact, a small sub-graph of the input se-
mantic network, N . Our algorithm then selects the
best candidates iteratively. In each iteration, the

1As deﬁned in the underlying sense inventory, up to tri-
grams. We used Stanford CoreNLP (Manning et al., 2014)
for tokenization, Part-of-Speech (PoS) tagging and lemmati-
zation.

Figure 1: Excerpt of a news article from the BBC.

Ambiguity. Language is inherently ambiguous.
For instance, Mercedes, race, Hamilton and For-
mula can refer to several different entities or mean-
ings. Current neural models have managed to
successfully represent complex semantic associ-
ations by effectively analyzing large amounts of
data. However, the word-level functionality of
these systems is still a barrier to the depth of their
natural language understanding. Our proposal is
particularly tailored towards addressing this issue.

Multiword expressions (MWE). MWE are lex-
ical units made up of two or more words which
are idiosyncratic in nature (Sag et al., 2002), e.g,
Lewis Hamilton, Nico Rosberg and Formula 1.
Most existing word-based models ignore the in-
terdependency between MWE’s subunits and treat
them as individual units. Handling MWE has
been a long-standing problem in NLP and has re-
cently received a considerable amount of interest
(Tsvetkov and Wintner, 2014; Salehi et al., 2015).
Our pipeline facilitates this goal.

Co-reference. Co-reference resolution of con-
cepts and entities is not explicitly tackled by our
approach. However, thanks to the fact that words
that refer to the same meaning in context, e.g., For-
mula 1-F1 or German Grand Prix-German GP-
Hockenheim, are all disambiguated to the same
concept, the co-reference issue is also partly ad-
dressed by our pipeline.

3 Disambiguation Algorithm

Our proposal relies on a seamless integration of
word senses in word-based systems. The goal is
to semantify the text prior to its being fed into the
system by transforming its individual units from
word surface form to the deeper level of word
senses. The semantiﬁcation step is mainly tailored

Figure 2: Simpliﬁed graph-based representation of
a sample sentence.

candidate sense that has the highest graph degree
maxDeg is chosen as the winning sense:

maxDeg = max
s∈S

|{(s, s(cid:48)) ∈ E : s(cid:48) ∈ S}|

(1)

After each iteration, when a candidate sense ˆs
is selected, all the possible candidate senses of the
corresponding word (i.e. getLex(ˆs)) are removed
from E (line 10 in the algorithm).

Figure 2 shows a simpliﬁed version of the graph
for a sample sentence. The algorithm would dis-
ambiguate the content words in this sentence as
follows. It ﬁrst associates Oasis with its rock band
sense, since its corresponding node has the high-
est degree, i.e. 3. On the basis of this, the desert
sense of Oasis and its link to the stone sense of
rock are removed from the graph. In the second it-
eration, rock band is disambiguated as music band
given that its degree is 2.2 Finally, Manchester is
associated with its city sense (with a degree of 1).
In order to enable disambiguating at differ-
ent conﬁdence levels, we introduce a threshold θ
which determines the stopping criterion of the al-
gorithm.
Iteration continues until the following
condition is fulﬁlled: maxDeg < θ|S| / 100. This
ensures that the system will only disambiguate
those words for which it has a high conﬁdence and
backs off to the word form otherwise, avoiding the
introduction of unwanted noise in the data for un-
certain cases or for word senses that are not de-
ﬁned in the inventory.

2For bigrams and trigrams whose individual words might
also be disambiguated (such as rock and band in rock band),
the longest unit has the highest priority (i.e. rock band).

Figure 3: Text classiﬁcation model architecture.

4 Classiﬁcation Model

In our experiments, we use a standard neural net-
work based classiﬁcation approach which is simi-
lar to the Convolution Neural Network classiﬁer of
Kim (2014) and the pioneering model of Collobert
et al. (2011). Figure 3 depicts the architecture of
the model. The network receives the concatenated
vector representations of the input words, v1:n =
v1⊕v2⊕· · ·⊕vn, and applies (convolves) ﬁlters F
on windows of h words, mi = f (F.vi:i+h−1 + b),
where b is a bias term and f () is a non-linear func-
tion, for which we use ReLU (Nair and Hinton,
2010). The convolution transforms the input text
to a feature map m = [m1, m2, . . . , mn−h+1].
A max pooling operation then selects the most
salient feature ˆm = max{m} for each ﬁlter.

In the network of Kim (2014), the pooled fea-
tures are directly passed to a fully connected soft-
max layer whose outputs are class probabilities.
However, we add a recurrent layer before soft-
max in order to enable better capturing of long-
distance dependencies. It has been shown by Xiao
and Cho (2016) that a recurrent layer can replace
multiple layers of convolution and be beneﬁcial,
particularly when the length of input text grows.
Speciﬁcally, we use a Long Short-Term Memory
(Hochreiter and Schmidhuber, 1997, LSTM) as
our recurrent layer which was originally proposed
to avoid the vanishing gradient problem and has
proven its abilities in capturing distant dependen-
cies. The LSTM unit computes three gate vectors

(forget, input, and output) as follows:

ft = σ(Wf gt + Uf ht−1 + bf ),
it = σ(Wi gt + Ui ht−1 + bi),
ot = σ(Wo gt + Uo ht−1 + bo),

(2)

where W, U, and b are model parameters and
g and h are input and output sequences, respec-
tively. The cell state vector ct is then computed as
ct = ft ct−1 + it tanh(˜ct) where ˜ct = Wc gt +
Uc ht−1. Finally, the output sequence is computed
as ht = ot tanh(ct). As for regularization, we
used dropout (Hinton et al., 2012) after the em-
bedding layer.

We perform experiments with two conﬁgura-
tions of the embedding layer: (1) Random, initial-
ized randomly and updated during training, and
(2) Pre-trained, initialized by pre-trained repre-
sentations and updated during training. In the fol-
lowing section we describe the pre-trained word
and sense representation used for the initialization
of the second conﬁguration.

4.1 Pre-trained Word and Sense Embeddings

One of the main advantages of neural models
is that they usually represent the input words as
dense vectors. This can signiﬁcantly boost a
system’s generalisation power and results in im-
proved performance (Zou et al., 2013; Bordes
et al., 2014; Kim, 2014; Weiss et al., 2015, inter-
alia). This feature also enables us to directly plug
in pre-trained sense representations and check
them in a downstream application.

In our experiments we generate a set of sense
embeddings by extending DeConf, a recent tech-
nique with state-of-the-art performance on multi-
ple semantic similarity benchmarks (Pilehvar and
Collier, 2016). We leave the evaluation of other
representations to future work. DeConf gets a
pre-trained set of word embeddings and computes
sense embeddings in the same semantic space. To
this end, the approach exploits the semantic net-
work of WordNet (Miller, 1995), using the Person-
alized PageRank (Haveliwala, 2002) algorithm,
and obtains a set of sense biasing words Bs for
a word sense s. The sense representation of s is
then obtained using the following formula:

ˆv(s) =

|Bs|
(cid:88)

1
|Bs|

−i

e

δ v(wi),

(3)

i=1
where δ is a decay parameter and v(wi) is the em-
the ith word in the sense bi-
bedding of wi, i.e.

asing list of s, i.e. Bs. We follow Pilehvar and
Collier (2016) and set δ = 5. Finally, the vector
for sense s is calculated as the average of ˆv(s) and
the embedding of its corresponding word.

Owing to its reliance on WordNet’s semantic
network, DeConf is limited to generating only
those word senses that are covered by this lexical
resource. We propose to use Wikipedia in order
to expand the vocabulary of the computed word
senses. Wikipedia provides a high coverage of
named entities and domain-speciﬁc terms in many
languages, while at the same time also beneﬁting
from a continuous update by collaborators. More-
over, it can easily be viewed as a sense inventory
where individual articles are word senses arranged
through hyperlinks and redirections.

Camacho-Collados et al.

(2016b) proposed
NASARI3, a technique to compute the most salient
words for each Wikipedia page. These salient
words were computed by exploiting the struc-
ture and content of Wikipedia and proved effec-
tive in tasks such as Word Sense Disambiguation
(Tripodi and Pelillo, 2017; Camacho-Collados
et al., 2016a), knowledge-base construction (Li-
eto et al., 2016), domain-adapted hypernym dis-
covery (Espinosa-Anke et al., 2016; Camacho-
Collados and Navigli, 2017) or object recogni-
tion (Young et al., 2016). We view these lists
as biasing words for individual Wikipedia pages,
and then leverage the exponential decay function
(Equation 3) to compute new sense embeddings
in the same semantic space.
In order to repre-
sent both WordNet and Wikipedia sense represen-
tations in the same space, we rely on the WordNet-
Wikipedia mapping provided by BabelNet4 (Nav-
igli and Ponzetto, 2012). For the WordNet synsets
which are mapped to Wikipedia pages in Babel-
Net, we average the corresponding Wikipedia-
based and WordNet-based sense embeddings.

4.2 Pre-trained Supersense Embeddings

It has been argued that WordNet sense distinctions
are too ﬁne-grained for many NLP applications
(Hovy et al., 2013). The issue can be tackled by
grouping together similar senses of the same word,
either using automatic clustering techniques (Nav-
igli, 2006; Agirre and Lopez, 2003; Snow et al.,
2007) or with the help of WordNet’s lexicographer

3We downloaded the salient words for Wikipedia pages
(NASARI English lexical vectors, version 3.0) from http://lcl.
uniroma1.it/nasari/

4We used the Java API from http://babelnet.org

ﬁles5. Various applications have been shown to
improve upon moving from senses to supersenses
(R¨ud et al., 2011; Severyn et al., 2013; Flekova
and Gurevych, 2016). In WordNet’s lexicographer
ﬁles there are a total of 44 sense clusters, referred
to as supersenses, for categories such as event, ani-
mal, and quantity. In our experiments we use these
supersenses in order to reduce granularity of our
WordNet and Wikipedia senses. To generate su-
persense embeddings, we simply average the em-
beddings of senses in the corresponding cluster.

5 Evaluation

We evaluated our model on two classiﬁcation
tasks: topic categorization (Section 5.2) and po-
larity detection (Section 5.3). In the following sec-
tion we present the common experimental setup.

5.1 Experimental setup

Classiﬁcation model. Throughout all the exper-
iments we used the classiﬁcation model described
in Section 4. The general architecture of the model
was the same for both tasks, with slight variations
in hyperparameters given the different natures of
the tasks, following the values suggested by Kim
(2014) and Xiao and Cho (2016) for the two tasks.
Hyperparameters were ﬁxed across all conﬁgura-
tions in the corresponding tasks. The embedding
layer was ﬁxed to 300 dimensions, irrespective of
the conﬁguration, i.e. Random and Pre-trained.
For both tasks the evaluation was carried out by
10-fold cross-validation unless standard training-
testing splits were available. The disambiguation
threshold θ (cf. Section 3) was tuned on the train-
ing portion of the corresponding data, over seven
values in [0,3] in steps of 0.5.6 We used Keras
(Chollet, 2015) and Theano (Team, 2016) for our
model implementations.

Semantic network. The integration of senses
was carried out as described in Section 3. For
disambiguating with both WordNet and Wikipedia
senses we relied on the joint semantic network of
Wikipedia hyperlinks and WordNet via the map-
ping provided by BabelNet.7

5https://wordnet.princeton.edu/man/lexnames.5WN.html
6We observed that values higher than 3 led to very few dis-
ambiguations. While the best results were generally achieved
in the [1.5,2.5] range, performance differences across thresh-
old values were not statistically signiﬁcant in most cases.

7For simplicity we refer to this joint sense inventory as

all

the

experiments we

Pre-trained word and sense
embeddings.
used
Throughout
Word2vec (Mikolov et al., 2013) embeddings,
trained on the Google News corpus.8 We trun-
cated this set to its 250K most frequent words.
We also used WordNet 3.0 (Fellbaum, 1998)
and the Wikipedia dump of November 2014 to
compute the sense embeddings (see Section 4.1).
As a result, we obtained a set of 757,262 sense
embeddings in the same space as the pre-trained
Word2vec word embeddings. We used DeConf
(Pilehvar and Collier, 2016) as our pre-trained
WordNet sense embeddings. All vectors had a
ﬁxed dimensionality of 300.

Supersenses.
In addition to WordNet senses, we
experimented with supersenses (see Section 4.2)
to check how reducing granularity would affect
system performance. For obtaining supersenses
in a given text we relied on our disambiguation
pipeline and simply clustered together senses be-
longing to the same WordNet supersense.

Evaluation measures. We report the results in
terms of standard accuracy and F1 measures.9

5.2 Topic Categorization

The task of topic categorization consists of assign-
ing a label (i.e. topic) to a given document from a
pre-deﬁned set of labels.

5.2.1 Datasets

For this task we used two newswire and one med-
ical topic categorization datasets. Table 1 sum-
marizes the statistics of each dataset.10 The BBC
news dataset11 (Greene and Cunningham, 2006)
comprises news articles taken from BBC, divided
into ﬁve topics: business, entertainment, politics,
sport and tech. Newsgroups (Lang, 1995) is a col-
lection of 11,314 documents for training and 7532
for testing12 divided into six topics: computing,
sport and motor vehicles, science, politics, reli-

8https://code.google.com/archive/p/word2vec/
9Since all models in our experiments provide full cover-
age, accuracy and F1 denote micro- and macro-averaged F1,
respectively (Yang, 1999).

10The coverage of the datasets was computed using the
250K top words in the Google News Word2vec embeddings.

11http://mlg.ucd.ie/datasets/bbc.html
12We used the train-test partition available at http://qwone.

Wikipedia, but note that WordNet senses are also covered.

com/∼jason/20Newsgroups/

Dataset

Domain No. of classes No. of docs Avg. doc. size Size of vocab. Coverage

Evaluation

BBC
News
Newsgroups News
Ohsumed

Medical

5
6
23

2,225
18,846
23,166

439.5
394.0
201.2

35,628
225,046
65,323

87.4%
83.4%
79.3%

10 cross valid.
Train-Test
Train-Test

Table 1: Statistics of the topic categorization datasets.

Initialization

Input type

BBC News
Acc
93.0

F1
92.8

Newsgroups
F1
Acc
85.6
87.7

Ohsumed

Random

Pre-trained

Word

Sense

WordNet
Wikipedia
Supersense WordNet
Wikipedia

Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

93.5
92.7
93.6
94.6∗

97.6
97.3

96.3
96.8
96.9

93.3
92.5
93.4
94.4

97.5
97.1

96.2
96.7
96.9

88.1
86.7
90.1∗
88.5

91.1
90.2
89.6†
89.6
88.6

86.9
84.9
89.0
85.8

90.6
88.6

88.9
88.9
87.4

Acc
30.1
27.2†
29.7
31.8∗
31.1

29.4
30.2

32.4
29.5
30.6∗

F1
20.7

18.3
20.9
22.0
21.3

20.1
20.4

22.3
19.9
20.3

Table 2: Classiﬁcation performance at the word, sense, and supersense levels with random and pre-
trained embedding initialization. We show in bold those settings that improve the word-based model.

gion and sales.13 Finally, Ohsumed14 is a col-
lection of medical abstracts from MEDLINE, an
online medical information database, categorized
according to 23 cardiovascular diseases. For our
experiments we used the partition split of 10,433
documents for training and 12,733 for testing.15

5.2.2 Results

Table 2 shows the results of our classiﬁcation
model and its variants on the three datasets.16
When the embedding layer is initialized randomly,
the model integrated with word senses consistently
improves over the word-based model, particularly
when the ﬁne-granularity of the underlying sense
inventory is reduced using supersenses (with sta-
tistically signiﬁcant gains on the three datasets).
This highlights the fact that a simple disambigua-
tion of the input can bring about performance gain
for a state-of-the-art classiﬁcation system. Also,

13The dataset has 20 ﬁne-grained categories clustered into
six general topics. We used the coarse-grained labels for their
clearer distinction and consistency with BBC topics.

14ftp://medir.ohsu.edu/pub/ohsumed
15http://disi.unitn.it/moschitti/corpora.htm
16Symbols ∗ and † indicate the sense-based model with the
smallest margin to the word-based model whose accuracy is
statistically signiﬁcant at 0.95 conﬁdence level according to
unpaired t-test (∗ for positive and † for negative change).

the better performance of supersenses suggests
that the sense distinctions of WordNet are too ﬁne-
grained for the topic categorization task. How-
ever, when pre-trained representations are used to
initialize the embedding layer, no improvement is
observed over the word-based model. This can
be attributed to the quality of the representations,
as the model utilizing them was unable to beneﬁt
from the advantage offered by sense distinctions.
Our results suggest that research in sense represen-
tation should put special emphasis on real-world
evaluations on benchmarks for downstream appli-
cations, rather than on artiﬁcial tasks such as word
similarity. In fact, research has previously shown
that word similarity might not constitute a reliable
proxy to measure the performance of word embed-
dings in downstream applications (Tsvetkov et al.,
2015; Chiu et al., 2016).

Among the three datasets, Ohsumed proves to
be the most challenging one, mainly for its larger
number of classes (i.e. 23) and its domain-speciﬁc
nature (i.e. medicine). Interestingly, unlike for the
other two datasets, the introduction of pre-trained
word embeddings to the system results in reduced
performance on Ohsumed. This suggests that gen-
eral domain embeddings might not be beneﬁcial

in specialized domains, which corroborates previ-
ous ﬁndings by Yadav et al. (2017) on a different
task, i.e. entity extraction. This performance drop
may also be due to diachronic issues (Ohsumed
the
dates back to the 1980s) and low coverage:
pre-trained Word2vec embeddings cover 79.3% of
the words in Ohsumed (see Table 1), in contrast
to the higher coverage on the newswire datasets,
i.e. Newsgroups (83.4%) and BBC (87.4%). How-
ever, also note that the best overall performance
is attained when our pre-trained Wikipedia sense
embeddings are used. This highlights the effec-
tiveness of Wikipedia in handling domain-speciﬁc
entities, thanks to its broad sense inventory.

5.3 Polarity Detection

Polarity detection is the most popular evaluation
framework for sentiment analysis (Dong et al.,
2015). The task is essentially a binary classiﬁca-
tion which determines if the sentiment of a given
sentence or document is negative or positive.

5.3.1 Datasets

For the polarity detection task we used ﬁve stan-
dard evaluation datasets. Table 1 summarizes
statistics. PL04 (Pang and Lee, 2004) is a polar-
ity detection dataset composed of full movie re-
views. PL0518 (Pang and Lee, 2005), instead, is
composed of short snippets from movie reviews.
RTC contains critic reviews from Rotten Toma-
toes19, divided into 436,000 training and 2,000
test instances. IMDB (Maas et al., 2011) includes
50,000 movie reviews, split evenly between train-
ing and test. Finally, we used the Stanford Sen-
timent dataset (Socher et al., 2013), which asso-
ciates each review with a value that denotes its
sentiment. To be consistent with the binary classi-
ﬁcation of the other datasets, we removed the neu-
tral phrases according to the dataset’s scale (be-
tween 0.4 and 0.6) and considered the reviews
whose values were below 0.4 as negative and
above 0.6 as positive. This resulted in a binary po-
larity dataset of 119,783 phrases. Unlike the previ-
ous four datasets, this dataset does not contain an
even distribution of positive and negative labels.

5.3.2 Results

Table 4 lists accuracy performance of our classi-
ﬁcation model and all its variants on ﬁve polar-

18Both PL04 and PL05 were downloaded from http://

www.cs.cornell.edu/people/pabo/movie-review-data/

19http://www.rottentomatoes.com

BBC

Ohsumed

PL04

IMDB

Newsgroups

n
i
a
g

y
c
a
r
u
c
c
A

1.5

0.5

1

0

−0.5

−1

−1.5

RTC
Stanford

PL05

0

100

200

300

400

500

600

700

800

Average document size

Figure 4: Relation between average document size
and performance improvement using Wikipedia
supersenses with random initialization.

ity detection datasets. Results are generally better
than those of Kim (2014), showing that the addi-
tion of the recurrent layer to the model (cf. Section
4) was beneﬁcial. However, interestingly, no con-
sistent performance gain is observed in the polar-
ity detection task, when the model is provided with
disambiguated input, particularly for datasets with
relatively short reviews. We attribute this to the
nature of the task. Firstly, given that words rarely
happen to be ambiguous with respect to their senti-
ment, the semantic sense distinctions provided by
the disambiguation stage do not assist the classiﬁer
in better decision making, and instead introduce
data sparsity. Secondly, since the datasets mostly
contain short texts, e.g., sentences or snippets, the
disambiguation algorithm does not have sufﬁcient
context to make high-conﬁdence judgements, re-
sulting in fewer disambiguations or less reliable
ones. In the following section we perform a more
in-depth analysis of the impact of document size
on the performance of our sense-based models.

5.4 Analysis

Document size. A detailed analysis revealed a
relation between document size (the number of
tokens) and performance gain of our sense-level
model. We show in Figure 4 how these two
vary for our most consistent conﬁguration, i.e.
Wikipedia supersenses, with random initialization.
Interestingly, as a general trend, the performance
gain increases with average document size, irre-

19Stanford is the only unbalanced dataset, but F1 results

were almost identical to accuracy.

Dataset

Type

No. of docs

Avg. doc. size

Vocabulary size

Coverage

Evaluation

RTC
IMDB
PL05
PL04
Stanford

Snippets
Reviews
Snippets
Reviews
Phrases

438,000
50,000
10,662
2,000
119,783

23.4
268.8
21.5
762.1
10.0

128,056
140,172
19,825
45,077
19,400

81.3%
82.5%
81.3%
82.4%
81.6%

Train-Test
Train-Test
10 cross valid.
10 cross valid.
10 cross valid.

Table 3: Statistics of the polarity detection datasets.

Initialization

Random

Pre-trained

Input type
Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

RTC
83.6
83.2

83.1

84.4
83.1

85.5

83.4

83.8

85.2

84.2

IMDB
87.7
87.4

88.0

88.0
88.4∗

88.3

88.3
87.0†
88.8

87.9

PL05
77.3
76.6
75.9†
75.9
75.8

80.2

79.2

79.2

79.5
78.3†

PL04
67.9
67.4

67.1

66.2
69.3∗

72.5
69.7†
73.1

73.8

72.6

Stanford
91.8
91.3

91.0
91.4†
91.0

93.1

92.6

92.3
92.7†
92.2

Table 4: Accuracy performance on ﬁve polarity detection datasets. Given that polarity datasets are
balanced17, we do not report F1 which would have been identical to accuracy.

spective of the classiﬁcation task. We attribute this
to two main factors:

1. Sparsity: Splitting a word into multiple word
senses can have the negative side effect that
the corresponding training data for that word
is distributed among multiple independent
senses. This reduces the training instances
per word sense, which might affect the classi-
ﬁer’s performance, particularly when senses
are semantically related (in comparison to
ﬁne-grained senses, supersenses address this
issue to some extent).

2. Disambiguation quality: As also mentioned
previously, our disambiguation algorithm re-
quires the input text to be sufﬁciently large so
as to create a graph with an adequate num-
ber of coherent connections to function ef-
fectively. In fact, for topic categorization, in
which the documents are relatively long, our
algorithm manages to disambiguate a larger
proportion of words in documents with high
conﬁdence. The lower performance of graph-
based disambiguation algorithms on short

texts is a known issue (Moro et al., 2014; Ra-
ganato et al., 2017), the tackling of which re-
mains an area of exploration.

Senses granularity. Our results showed that re-
ducing ﬁne-granularity of sense distinctions can
irrespective of the
be beneﬁcial to both tasks,
i.e. WordNet or
underlying sense inventory,
Wikipedia, which corroborates previous ﬁndings
(Hovy et al., 2013; Flekova and Gurevych, 2016).
This suggests that text classiﬁcation does not re-
In this
quire ﬁne-grained semantic distinctions.
work we used a simple technique based on Word-
Net’s lexicographer ﬁles for coarsening senses in
this sense inventory as well as in Wikipedia. We
leave the exploration of this promising area as well
as the evaluation of other granularity reduction
techniques for WordNet (Snow et al., 2007; Bhag-
wani et al., 2013) and Wikipedia (Dandala et al.,
2013) sense inventories to future work.

6 Related Work

The past few years have witnessed a growing re-
search interest in semantic representation, mainly
as a consequence of the word embedding tsunami

(Mikolov et al., 2013; Pennington et al., 2014).
Soon after their introduction, word embeddings
were integrated into different NLP applications,
thanks to the migration of the ﬁeld to deep learning
and the fact that most deep learning models view
words as dense vectors. The waves of the word
embedding tsunami have also lapped on the shores
of sense representation. Several techniques have
been proposed that either extend word embedding
models to cluster contexts and induce senses, usu-
ally referred to as unsupervised sense represen-
tations (Sch¨utze, 1998; Reisinger and Mooney,
2010; Huang et al., 2012; Neelakantan et al., 2014;
Guo et al., 2014; Tian et al., 2014; ˇSuster et al.,
2016; Ettinger et al., 2016; Qiu et al., 2016) or
exploit external sense inventories and lexical re-
sources for generating sense representations for
individual meanings of words (Chen et al., 2014;
Johansson and Pina, 2015; Jauhar et al., 2015; Ia-
cobacci et al., 2015; Rothe and Sch¨utze, 2015;
Camacho-Collados et al., 2016b; Mancini et al.,
2016; Pilehvar and Collier, 2016).

However, the integration of sense representa-
tions into deep learning models has not been so
straightforward, and research in this ﬁeld has of-
ten opted for alternative evaluation benchmarks
such as WSD, or artiﬁcial tasks, such as word
similarity. Consequently, the problem of integrat-
ing sense representations into downstream NLP
applications has remained understudied, despite
the potential beneﬁts it can have. Li and Juraf-
sky (2015) proposed a “multi-sense embedding”
pipeline to check the beneﬁt that can be gained
by replacing word embeddings with sense embed-
dings in multiple tasks. With the help of two
simple disambiguation algorithms, unsupervised
sense embeddings were integrated into various
downstream applications, with varying degrees of
success. Given the interdependency of sense rep-
resentation and disambiguation in this model, it is
very difﬁcult to introduce alternative algorithms
into its pipeline, either to beneﬁt from the state
of the art, or to carry out an evaluation. Instead,
our pipeline provides the advantage of being mod-
thanks to its use of disambiguation in the
ular:
pre-processing stage and use of sense representa-
tions that are linked to external sense inventories,
different WSD techniques and sense representa-
tions can be easily plugged in and checked. Along
the same lines, Flekova and Gurevych (2016) pro-
posed a technique for learning supersense rep-

resentations, using automatically-annotated cor-
pora. Coupled with a supersense tagger, the rep-
resentations were fed into a neural network clas-
siﬁer as additional features to the word-based in-
put. Through a set of experiments, Flekova and
Gurevych (2016) showed that the supersense en-
richment can be beneﬁcial to a range of binary
classiﬁcation tasks. Our proposal is different in
that it focuses directly on the beneﬁts that can
be gained by semantifying the input,
re-
ducing lexical ambiguity in the input text, rather
than assisting the model with additional sources
of knowledge.

i.e.

7 Conclusion and Future Work

We proposed a pipeline for the integration of sense
level knowledge into a state-of-the-art text clas-
siﬁer. We showed that a simple disambiguation
of the input can lead to consistent performance
gain, particularly for longer documents and when
the granularity of the underlying sense inventory
is reduced. Our pipeline is modular and can be
used as an in vivo evaluation framework for WSD
and sense representation techniques. We release
our code and data to reproduce our experiments
(including pre-trained sense and supersense em-
beddings) at https://github.com/pilehvar/sensecnn
to allow further checking of the choice of hyperpa-
rameters and to allow further analysis and compar-
ison. We hope that our work will foster future re-
search on the integration of sense-level knowledge
into downstream applications. As future work, we
plan to investigate the extension of the approach
to other languages and applications. Also, given
the promising results observed for supersenses, we
will investigate task-speciﬁc coarsening of sense
inventories, particularly Wikipedia, or the use of
SentiWordNet (Baccianella et al., 2010), which
could be more suitable for polarity detection.

Acknowledgments

The authors gratefully acknowledge the sup-
port of the MRC grant No. MR/M025160/1
for PheneBank and ERC Consolidator Grant
MOUSSE No. 726487. Jose Camacho-Collados
is supported by a Google Doctoral Fellowship in
Natural Language Processing. Nigel Collier is
supported by EPSRC Grant No. EP/M005089/1.
We thank Jim McManus for his suggestions on the
manuscript and the anonymous reviewers for their
helpful comments.

References

Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics
40(1):57–84.

Eneko Agirre and Oier Lopez. 2003. Clustering Word-
In Proceedings of Recent Ad-
Net word senses.
vances in Natural Language Processing. Borovets,
Bulgaria, pages 121–130.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC. volume 10, pages 2200–2204.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res. 12:2493–2537.

Bharath Dandala, Chris Hokamp, Rada Mihalcea, and
Razvan C. Bunescu. 2013. Sense clustering us-
ing Wikipedia. In Proceedings of Recent Advances
in Natural Language Processing. Hissar, Bulgaria,
pages 164–171.

Li Dong, Furu Wei, Shujie Liu, Ming Zhou, and
Ke Xu. 2015. A statistical parsing framework for
sentiment classiﬁcation. Computational Linguistics
41(2):293–336.

Miguel Ballesteros, Chris Dyer, and Noah A Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of EMNLP.

Luis Espinosa-Anke, Jose Camacho-Collados, Claudio
Delli Bovi, and Horacio Saggion. 2016. Supervised
distributional hypernym discovery via domain adap-
tation. In Proceedings of EMNLP. pages 424–435.

Sumit Bhagwani, Shrutiranjan Satapathy, and Harish
In Pro-
Karnick. 2013. Merging word senses.
ceedings of TextGraphs-8 Graph-based Methods for
Natural Language Processing. Seattle, Washington,
USA, pages 11–19.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
dings. In EMNLP.

Jos´e Camacho-Collados, Claudio Delli Bovi, Alessan-
dro Raganato, and Roberto Navigli. 2016a. A
of
Large-Scale Multilingual Disambiguation
In Proceedings of LREC. Portoroz,
Glosses.
Slovenia, pages 1701–1708.

Jose Camacho-Collados and Roberto Navigli. 2017.
BabelDomains: Large-Scale Domain Labeling of
Lexical Resources. In Proceedings of EACL (2). Va-
lencia, Spain.

Jos´e Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2016b. Nasari: Integrating ex-
plicit knowledge and corpus statistics for a multilin-
gual representation of concepts and entities. Artiﬁ-
cial Intelligence 240:36–64.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A uniﬁed model for word sense representation and
disambiguation. In Proceedings of EMNLP. Doha,
Qatar, pages 1025–1035.

Xiao Cheng and Dan Roth. 2013. Relational inference
for wikiﬁcation. In Proceedings of EMNLP. Seattle,
Washington, pages 1787–1796.

Billy Chiu, Anna Korhonen, and Sampo Pyysalo. 2016.
Intrinsic evaluation of word vectors fails to predict
extrinsic performance. In Proceedings of the Work-
shop on Evaluating Vector Space Representations
for NLP, ACL.

Franc¸ois Chollet. 2015. Keras.

https://github.com/

fchollet/keras.

Allyson Ettinger, Philip Resnik, and Marine Carpuat.
2016. Retroﬁtting sense-speciﬁc word vectors using
In Proceedings of NAACL-HLT. San
parallel text.
Diego, California, pages 1378–1383.

Christiane Fellbaum, editor. 1998. WordNet: An Elec-

tronic Database. MIT Press, Cambridge, MA.

Lucie Flekova and Iryna Gurevych. 2016. Supersense
embeddings: A uniﬁed model for supersense inter-
pretation, prediction, and utilization. In Proceedings
of ACL.

Derek Greene and P´adraig Cunningham. 2006. Practi-
cal solutions to the problem of diagonal dominance
in kernel document clustering. In Proceedings of the
23rd International conference on Machine learning.
ACM, pages 377–384.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-speciﬁc word embed-
In COL-
dings by exploiting bilingual resources.
ING. pages 497–507.

Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of the 11th International Conference
on World Wide Web. Hawaii, USA, pages 517–526.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
Improving neural networks by
dinov. 2012.
preventing co-adaptation of feature detectors. CoRR
abs/1207.0580.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen,
Martin Theobald, and Gerhard Weikum. 2012.
Kore: keyphrase overlap relatedness for entity dis-
ambiguation. In Proceedings of CIKM. pages 545–
554.

Eduard H. Hovy, Roberto Navigli, and Simone Paolo
semi-
Ponzetto. 2013.
structured content and Artiﬁcial Intelligence: The
story so far. Artiﬁcial Intelligence 194:2–27.

Collaboratively built

Xiao Ling, Sameer Singh, and Daniel S Weld. 2015.
Design challenges for entity linking. Transactions
of the Association for Computational Linguistics
3:315–328.

Eric H. Huang, Richard Socher, Christopher D. Man-
Improving word
ning, and Andrew Y. Ng. 2012.
representations via global context and multiple word
prototypes. In Proceedings of ACL. Jeju Island, Ko-
rea, pages 873–882.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of ACL-HLT. Portland, Oregon,
USA, pages 142–150.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning sense
embeddings for word and relational similarity.
In
Proceedings of ACL. Beijing, China, pages 95–105.

Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy.
2015. Ontologically grounded multi-sense repre-
sentation learning for semantic vector space models.
In Proceedings of NAACL. Denver, Colorado, pages
683–693.

Richard Johansson and Luis Nieto Pina. 2015. Embed-
ding a semantic network in a word space. In Pro-
ceedings of NAACL. Denver, Colorado, pages 1428–
1433.

Rie Johnson and Tong Zhang. 2015. Effective use
of word order for text categorization with convolu-
tional neural networks. In Proceedings of NAACL.
Denver, Colorado, pages 103–112.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL. Bal-
timore, USA, pages 655–665.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of EMNLP.
Doha, Qatar, pages 1746–1751.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proceedings of AAAI. Phoenix, Arizona,
pages 2741–2749.

Ken Lang. 1995. Newsweeder: Learning to ﬁlter net-
news. In Proceedings of the 12th International Con-
ference on Machine Learning. Tahoe City, Califor-
nia, pages 331–339.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
In Proceedings of EMNLP. Lisbon, Portugal, pages
683–693.

Antonio Lieto, Enrico Mensa, and Daniele P Radicioni.
2016. A resource-driven approach for anchoring lin-
In AI* IA
guistic resources to conceptual spaces.
2016 Advances in Artiﬁcial Intelligence, Springer,
pages 435–449.

Nut Limsopatham and Nigel Collier. 2016. Normalis-
ing medical concepts in social media texts by learn-
ing semantic representation. In Proceedings of ACL.
Berlin, Germany, pages 1014–1023.

Massimiliano Mancini,

Jose Camacho-Collados,
Ignacio Iacobacci, and Roberto Navigli. 2016.
via
Embedding words
CoRR
joint
abs/1612.02703. http://arxiv.org/abs/1612.02703.

knowledge-enhanced

training.

together

senses

and

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations.
pages 55–60.

Oren Melamud, Jacob Goldberger, and Ido Dagan.
context2vec: Learning generic context
2016.
In Proceed-
embedding with bidirectional lstm.
ings of The 20th SIGNLL Conference on Compu-
tational Natural Language Learning. Berlin, Ger-
many, pages 51–61.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient estimation of word represen-
tations in vector space. CoRR abs/1301.3781.

George A Miller. 1995.

a lexical
database for english. Communications of the ACM
38(11):39–41.

WordNet:

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Disam-
biguation: a Uniﬁed Approach. Transactions of the
Association for Computational Linguistics (TACL)
2:231–244.

Vinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning. pages 807–814.

Roberto Navigli. 2006. Meaningful clustering of
senses helps boost Word Sense Disambiguation per-
In Proceedings of COLING-ACL. Syd-
formance.
ney, Australia, pages 105–112.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artiﬁcial Intelligence 193:217–
250.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efﬁcient non-
parametric estimation of multiple embeddings per
In Proceedings of EMNLP.
word in vector space.
Doha, Qatar, pages 1059–1069.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
ACL. Barcelona, Spain, pages 51–61.

Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. Learning semantic textual similar-
In Proceedings
ity with structural representations.
of ACL (2). Soﬁa, Bulgaria, pages 714–718.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL.
Ann Arbor, Michigan, pages 115–124.

Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-
drew Y. Ng. 2007. Learning to merge word senses.
In Proceedings of EMNLP. Prague, Czech Republic,
pages 1005–1014.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
In Proceedings of EMNLP. pages
representation.
1532–1543.

Mohammad Taher Pilehvar and Nigel Collier. 2016.
De-conﬂated semantic representations. In Proceed-
ings of EMNLP. Austin, TX, pages 1680–1690.

Mohammad Taher Pilehvar and Roberto Navigli. 2014.
A large-scale pseudoword-based evaluation frame-
work for state-of-the-art Word Sense Disambigua-
tion. Computational Linguistics 40(4).

Lin Qiu, Kewei Tu, and Yong Yu. 2016. Context-
In Proceedings of

dependent sense embedding.
EMNLP. Austin, Texas, pages 183–191.

Alessandro Raganato, Jose Camacho-Collados, and
Roberto Navigli. 2017. Word sense disambiguation:
A uniﬁed evaluation framework and empirical com-
parison. In Proceedings of EACL. Valencia, Spain,
pages 99–110.

Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceedings of ACL. pages 109–117.

Sascha Rothe and Hinrich Sch¨utze. 2015. Autoex-
tend: Extending word embeddings to embeddings
In Proceedings of ACL.
for synsets and lexemes.
Beijing, China, pages 1793–1803.

Stefan R¨ud, Massimiliano Ciaramita, Jens M¨uller, and
Hinrich Sch¨utze. 2011. Piggyback: Using search
engines for robust cross-domain named entity recog-
nition. In Proceedings of ACL-HLT. Portland, Ore-
gon, USA, pages 965–975.

Ivan A Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
In Inter-
expressions: A pain in the neck for nlp.
national Conference on Intelligent Text Processing
and Computational Linguistics. Mexico City, Mex-
ico, pages 1–15.

Bahar Salehi, Paul Cook, and Timothy Baldwin. 2015.
A word embedding approach to predicting the com-
positionality of multiword expressions. In NAACL-
HTL. Denver, Colorado, pages 977–983.

Hinrich Sch¨utze. 1998. Automatic word sense discrim-
ination. Computational linguistics 24(1):97–123.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher Manning, Andrew Ng, and
Christopher Potts. 2013. Parsing with compositional
vector grammars. In Proceedings of EMNLP. Soﬁa,
Bulgaria, pages 455–465.

Simon ˇSuster, Ivan Titov, and Gertjan van Noord. 2016.
Bilingual learning of multi-sense embeddings with
In Proceedings of NAACL-
discrete autoencoders.
HLT. San Diego, California, pages 1346–1356.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
sentiment classiﬁcation. In Porceedings of EMNLP.
Lisbon, Portugal, pages 1422–1432.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints abs/1605.02688.

Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In COLING. pages 151–160.

Rocco Tripodi and Marcello Pelillo. 2017. A game-
theoretic approach to word sense disambiguation.
Computational Linguistics 43(1):31–70.

Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
laume Lample, and Chris Dyer. 2015. Evaluation
of word vector representations by subspace align-
ment. In Proceedings of EMNLP (2). Lisbon, Por-
tugal, pages 2049–2054.

Yulia Tsvetkov and Shuly Wintner. 2014.

Identiﬁca-
tion of multiword expressions by combining multi-
ple linguistic information sources. Computational
Linguistics 40(2):449–468.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural network
In Proceedings of ACL.
transition-based parsing.
Beijing, China, pages 323–333.

Yijun Xiao and Kyunghyun Cho. 2016.

Efﬁcient
character-level document classiﬁcation by com-
bining convolution and recurrent layers. CoRR
abs/1602.00367.

Shweta Yadav, Asif Ekbal, Sriparna Saha, and Pushpak
Bhattacharyya. 2017. Entity extraction in biomedi-
cal corpora: An approach to evaluate word embed-
ding features with pso based feature selection.
In
Proceedings of EACL. Valencia, Spain, pages 1159–
1170.

Yiming Yang. 1999. An evaluation of statistical ap-
Information re-

proaches to text categorization.
trieval 1(1-2):69–90.

Jay Young, Valerio Basile, Lars Kunze, Elena Cabrio,
and Nick Hawes. 2016. Towards lifelong object
learning by integrating situated robot perception and
In Proceedings of the Eu-
semantic web mining.
ropean Conference on Artiﬁcial Intelligence confer-
ence. The Hague, Netherland, pages 1458–1466.

Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage Word Sense Disambiguation sys-
tem for free text. In Proceedings of the ACL System
Demonstrations. Uppsala, Sweden, pages 78–83.

Will Y. Zou, Richard Socher, Daniel M. Cer, and
Christopher D. Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation.
In
Proceedings of EMNLP. Seattle, USA, pages 1393–
1398.

Towards a Seamless Integration of Word Senses
into Downstream NLP Applications

Mohammad Taher Pilehvar2, Jose Camacho-Collados1,
Roberto Navigli1 and Nigel Collier2

1Department of Computer Science, Sapienza University of Rome
2Department of Theoretical and Applied Linguistics, University of Cambridge
1{collados,navigli}@di.uniroma1.it
2{mp792,nhc30}@cam.ac.uk

Abstract

Lexical ambiguity can impede NLP sys-
tems from accurate understanding of se-
mantics. Despite its potential beneﬁts, the
integration of sense-level information into
NLP systems has remained understudied.
By incorporating a novel disambiguation
algorithm into a state-of-the-art classiﬁca-
tion model, we create a pipeline to inte-
grate sense-level information into down-
stream NLP applications. We show that
a simple disambiguation of the input text
can lead to consistent performance im-
provement on multiple topic categoriza-
tion and polarity detection datasets, par-
ticularly when the ﬁne granularity of the
underlying sense inventory is reduced and
the document is sufﬁciently large. Our re-
sults also point to the need for sense rep-
resentation research to focus more on in
vivo evaluations which target the perfor-
mance in downstream NLP applications
rather than artiﬁcial benchmarks.

1

Introduction

As a general trend, most current Natural Language
Processing (NLP) systems function at the word
level, i.e.
individual words constitute the most
ﬁne-grained meaning-bearing elements of their in-
put. The word level functionality can affect the
performance of these systems in two ways: (1)
it can hamper their efﬁciency in handling words
that are not encountered frequently during train-
ing, such as multiwords, inﬂections and deriva-
tions, and (2) it can restrict their semantic under-
standing to the level of words, with all their am-
biguities, and thereby prevent accurate capture of
the intended meanings.

The ﬁrst issue has recently been alleviated by

techniques that aim to boost the generalisation
power of NLP systems by resorting to sub-word
or character-level information (Ballesteros et al.,
2015; Kim et al., 2016). The second limitation,
however, has not yet been studied sufﬁciently. A
reasonable way to handle word ambiguity, and
hence to tackle the second issue, is to semantify
the input text: transform it from its surface-level
semantics to the deeper level of word senses, i.e.
their intended meanings. We take a step in this di-
rection by designing a pipeline that enables seam-
less integration of word senses into downstream
NLP applications, while beneﬁting from knowl-
edge extracted from semantic networks. To this
end, we propose a quick graph-based Word Sense
Disambiguation (WSD) algorithm which allows
high conﬁdence disambiguation of words without
much computation overload on the system. We
evaluate the pipeline in two downstream NLP ap-
plications: polarity detection and topic categoriza-
tion. Speciﬁcally, we use a classiﬁcation model
based on Convolutional Neural Networks which
has been shown to be very effective in various
text classiﬁcation tasks (Kalchbrenner et al., 2014;
Kim, 2014; Johnson and Zhang, 2015; Tang et al.,
2015; Xiao and Cho, 2016). We show that a simple
disambiguation of input can lead to performance
improvement of a state-of-the-art text classiﬁca-
tion system on multiple datasets, particularly for
long inputs and when the granularity of the sense
inventory is reduced. Our pipeline is quite ﬂexible
and modular, as it permits the integration of differ-
ent WSD and sense representation techniques.

2 Motivation

With the help of an example news article from the
BBC, shown in Figure 1, we highlight some of the
potential deﬁciencies of word-based models.

7
1
0
2
 
t
c
O
 
8
1
 
 
]
L
C
.
s
c
[
 
 
1
v
2
3
6
6
0
.
0
1
7
1
:
v
i
X
r
a

Algorithm 1 Disambiguation algorithm
Input: Input text T and semantic network N
Output: Set of disambiguated senses ˆS
1: Graph representation of T : (S, E) ← getGraph(T, N )
2: ˆS ← ∅
3: for each iteration i ∈ {1, ..., len(T )}
4:
5:
6:
7:
8:
9:
10:
11: return Disambiguation output ˆS

ˆs = argmaxs∈S |{(s, s(cid:48)) ∈ E : s(cid:48) ∈ S}|
maxDeg = |{(ˆs, s(cid:48)) ∈ E : s(cid:48) ∈ S}|
if maxDeg < θ|S| / 100 then

ˆS ← ˆS ∪ {ˆs}
E ← E \ {(s, s(cid:48)) : s ∨ s(cid:48) ∈ getLex(ˆs)}

break

else

towards resolving ambiguities, but it brings about
other advantages mentioned in the previous sec-
tion. The aim is to provide the system with an
input of reduced ambiguity which can facilitate its
decision making.

To this end, we developed a simple graph-based
joint disambiguation and entity linking algorithm
which can take any arbitrary semantic network
as input. The gist of our disambiguation tech-
nique lies in its speed and scalability. Conven-
tional knowledge-based disambiguation systems
(Hoffart et al., 2012; Agirre et al., 2014; Moro
et al., 2014; Ling et al., 2015; Pilehvar and Nav-
igli, 2014) often rely on computationally expen-
sive graph algorithms, which limits their applica-
tion to on-the-ﬂy processing of large number of
text documents, as is the case in our experiments.
Moreover, unlike supervised WSD and entity link-
ing techniques (Zhong and Ng, 2010; Cheng and
Roth, 2013; Melamud et al., 2016; Limsopatham
and Collier, 2016), our algorithm relies only on
semantic networks and does not require any sense-
annotated data, which is limited to English and al-
most non-existent for other languages.

Algorithm 1 shows our procedure for disam-
biguating an input document T . First, we retrieve
from our semantic network the list of candidate
senses1 for each content word, as well as seman-
tic relationships among them. As a result, we ob-
tain a graph representation (S, E) of the input text,
where S is the set of candidate senses and E is
the set of edges among different senses in S. The
graph is, in fact, a small sub-graph of the input se-
mantic network, N . Our algorithm then selects the
best candidates iteratively. In each iteration, the

1As deﬁned in the underlying sense inventory, up to tri-
grams. We used Stanford CoreNLP (Manning et al., 2014)
for tokenization, Part-of-Speech (PoS) tagging and lemmati-
zation.

Figure 1: Excerpt of a news article from the BBC.

Ambiguity. Language is inherently ambiguous.
For instance, Mercedes, race, Hamilton and For-
mula can refer to several different entities or mean-
ings. Current neural models have managed to
successfully represent complex semantic associ-
ations by effectively analyzing large amounts of
data. However, the word-level functionality of
these systems is still a barrier to the depth of their
natural language understanding. Our proposal is
particularly tailored towards addressing this issue.

Multiword expressions (MWE). MWE are lex-
ical units made up of two or more words which
are idiosyncratic in nature (Sag et al., 2002), e.g,
Lewis Hamilton, Nico Rosberg and Formula 1.
Most existing word-based models ignore the in-
terdependency between MWE’s subunits and treat
them as individual units. Handling MWE has
been a long-standing problem in NLP and has re-
cently received a considerable amount of interest
(Tsvetkov and Wintner, 2014; Salehi et al., 2015).
Our pipeline facilitates this goal.

Co-reference. Co-reference resolution of con-
cepts and entities is not explicitly tackled by our
approach. However, thanks to the fact that words
that refer to the same meaning in context, e.g., For-
mula 1-F1 or German Grand Prix-German GP-
Hockenheim, are all disambiguated to the same
concept, the co-reference issue is also partly ad-
dressed by our pipeline.

3 Disambiguation Algorithm

Our proposal relies on a seamless integration of
word senses in word-based systems. The goal is
to semantify the text prior to its being fed into the
system by transforming its individual units from
word surface form to the deeper level of word
senses. The semantiﬁcation step is mainly tailored

Figure 2: Simpliﬁed graph-based representation of
a sample sentence.

candidate sense that has the highest graph degree
maxDeg is chosen as the winning sense:

maxDeg = max
s∈S

|{(s, s(cid:48)) ∈ E : s(cid:48) ∈ S}|

(1)

After each iteration, when a candidate sense ˆs
is selected, all the possible candidate senses of the
corresponding word (i.e. getLex(ˆs)) are removed
from E (line 10 in the algorithm).

Figure 2 shows a simpliﬁed version of the graph
for a sample sentence. The algorithm would dis-
ambiguate the content words in this sentence as
follows. It ﬁrst associates Oasis with its rock band
sense, since its corresponding node has the high-
est degree, i.e. 3. On the basis of this, the desert
sense of Oasis and its link to the stone sense of
rock are removed from the graph. In the second it-
eration, rock band is disambiguated as music band
given that its degree is 2.2 Finally, Manchester is
associated with its city sense (with a degree of 1).
In order to enable disambiguating at differ-
ent conﬁdence levels, we introduce a threshold θ
which determines the stopping criterion of the al-
gorithm.
Iteration continues until the following
condition is fulﬁlled: maxDeg < θ|S| / 100. This
ensures that the system will only disambiguate
those words for which it has a high conﬁdence and
backs off to the word form otherwise, avoiding the
introduction of unwanted noise in the data for un-
certain cases or for word senses that are not de-
ﬁned in the inventory.

2For bigrams and trigrams whose individual words might
also be disambiguated (such as rock and band in rock band),
the longest unit has the highest priority (i.e. rock band).

Figure 3: Text classiﬁcation model architecture.

4 Classiﬁcation Model

In our experiments, we use a standard neural net-
work based classiﬁcation approach which is simi-
lar to the Convolution Neural Network classiﬁer of
Kim (2014) and the pioneering model of Collobert
et al. (2011). Figure 3 depicts the architecture of
the model. The network receives the concatenated
vector representations of the input words, v1:n =
v1⊕v2⊕· · ·⊕vn, and applies (convolves) ﬁlters F
on windows of h words, mi = f (F.vi:i+h−1 + b),
where b is a bias term and f () is a non-linear func-
tion, for which we use ReLU (Nair and Hinton,
2010). The convolution transforms the input text
to a feature map m = [m1, m2, . . . , mn−h+1].
A max pooling operation then selects the most
salient feature ˆm = max{m} for each ﬁlter.

In the network of Kim (2014), the pooled fea-
tures are directly passed to a fully connected soft-
max layer whose outputs are class probabilities.
However, we add a recurrent layer before soft-
max in order to enable better capturing of long-
distance dependencies. It has been shown by Xiao
and Cho (2016) that a recurrent layer can replace
multiple layers of convolution and be beneﬁcial,
particularly when the length of input text grows.
Speciﬁcally, we use a Long Short-Term Memory
(Hochreiter and Schmidhuber, 1997, LSTM) as
our recurrent layer which was originally proposed
to avoid the vanishing gradient problem and has
proven its abilities in capturing distant dependen-
cies. The LSTM unit computes three gate vectors

(forget, input, and output) as follows:

ft = σ(Wf gt + Uf ht−1 + bf ),
it = σ(Wi gt + Ui ht−1 + bi),
ot = σ(Wo gt + Uo ht−1 + bo),

(2)

where W, U, and b are model parameters and
g and h are input and output sequences, respec-
tively. The cell state vector ct is then computed as
ct = ft ct−1 + it tanh(˜ct) where ˜ct = Wc gt +
Uc ht−1. Finally, the output sequence is computed
as ht = ot tanh(ct). As for regularization, we
used dropout (Hinton et al., 2012) after the em-
bedding layer.

We perform experiments with two conﬁgura-
tions of the embedding layer: (1) Random, initial-
ized randomly and updated during training, and
(2) Pre-trained, initialized by pre-trained repre-
sentations and updated during training. In the fol-
lowing section we describe the pre-trained word
and sense representation used for the initialization
of the second conﬁguration.

4.1 Pre-trained Word and Sense Embeddings

One of the main advantages of neural models
is that they usually represent the input words as
dense vectors. This can signiﬁcantly boost a
system’s generalisation power and results in im-
proved performance (Zou et al., 2013; Bordes
et al., 2014; Kim, 2014; Weiss et al., 2015, inter-
alia). This feature also enables us to directly plug
in pre-trained sense representations and check
them in a downstream application.

In our experiments we generate a set of sense
embeddings by extending DeConf, a recent tech-
nique with state-of-the-art performance on multi-
ple semantic similarity benchmarks (Pilehvar and
Collier, 2016). We leave the evaluation of other
representations to future work. DeConf gets a
pre-trained set of word embeddings and computes
sense embeddings in the same semantic space. To
this end, the approach exploits the semantic net-
work of WordNet (Miller, 1995), using the Person-
alized PageRank (Haveliwala, 2002) algorithm,
and obtains a set of sense biasing words Bs for
a word sense s. The sense representation of s is
then obtained using the following formula:

ˆv(s) =

|Bs|
(cid:88)

1
|Bs|

−i

e

δ v(wi),

(3)

i=1
where δ is a decay parameter and v(wi) is the em-
the ith word in the sense bi-
bedding of wi, i.e.

asing list of s, i.e. Bs. We follow Pilehvar and
Collier (2016) and set δ = 5. Finally, the vector
for sense s is calculated as the average of ˆv(s) and
the embedding of its corresponding word.

Owing to its reliance on WordNet’s semantic
network, DeConf is limited to generating only
those word senses that are covered by this lexical
resource. We propose to use Wikipedia in order
to expand the vocabulary of the computed word
senses. Wikipedia provides a high coverage of
named entities and domain-speciﬁc terms in many
languages, while at the same time also beneﬁting
from a continuous update by collaborators. More-
over, it can easily be viewed as a sense inventory
where individual articles are word senses arranged
through hyperlinks and redirections.

Camacho-Collados et al.

(2016b) proposed
NASARI3, a technique to compute the most salient
words for each Wikipedia page. These salient
words were computed by exploiting the struc-
ture and content of Wikipedia and proved effec-
tive in tasks such as Word Sense Disambiguation
(Tripodi and Pelillo, 2017; Camacho-Collados
et al., 2016a), knowledge-base construction (Li-
eto et al., 2016), domain-adapted hypernym dis-
covery (Espinosa-Anke et al., 2016; Camacho-
Collados and Navigli, 2017) or object recogni-
tion (Young et al., 2016). We view these lists
as biasing words for individual Wikipedia pages,
and then leverage the exponential decay function
(Equation 3) to compute new sense embeddings
in the same semantic space.
In order to repre-
sent both WordNet and Wikipedia sense represen-
tations in the same space, we rely on the WordNet-
Wikipedia mapping provided by BabelNet4 (Nav-
igli and Ponzetto, 2012). For the WordNet synsets
which are mapped to Wikipedia pages in Babel-
Net, we average the corresponding Wikipedia-
based and WordNet-based sense embeddings.

4.2 Pre-trained Supersense Embeddings

It has been argued that WordNet sense distinctions
are too ﬁne-grained for many NLP applications
(Hovy et al., 2013). The issue can be tackled by
grouping together similar senses of the same word,
either using automatic clustering techniques (Nav-
igli, 2006; Agirre and Lopez, 2003; Snow et al.,
2007) or with the help of WordNet’s lexicographer

3We downloaded the salient words for Wikipedia pages
(NASARI English lexical vectors, version 3.0) from http://lcl.
uniroma1.it/nasari/

4We used the Java API from http://babelnet.org

ﬁles5. Various applications have been shown to
improve upon moving from senses to supersenses
(R¨ud et al., 2011; Severyn et al., 2013; Flekova
and Gurevych, 2016). In WordNet’s lexicographer
ﬁles there are a total of 44 sense clusters, referred
to as supersenses, for categories such as event, ani-
mal, and quantity. In our experiments we use these
supersenses in order to reduce granularity of our
WordNet and Wikipedia senses. To generate su-
persense embeddings, we simply average the em-
beddings of senses in the corresponding cluster.

5 Evaluation

We evaluated our model on two classiﬁcation
tasks: topic categorization (Section 5.2) and po-
larity detection (Section 5.3). In the following sec-
tion we present the common experimental setup.

5.1 Experimental setup

Classiﬁcation model. Throughout all the exper-
iments we used the classiﬁcation model described
in Section 4. The general architecture of the model
was the same for both tasks, with slight variations
in hyperparameters given the different natures of
the tasks, following the values suggested by Kim
(2014) and Xiao and Cho (2016) for the two tasks.
Hyperparameters were ﬁxed across all conﬁgura-
tions in the corresponding tasks. The embedding
layer was ﬁxed to 300 dimensions, irrespective of
the conﬁguration, i.e. Random and Pre-trained.
For both tasks the evaluation was carried out by
10-fold cross-validation unless standard training-
testing splits were available. The disambiguation
threshold θ (cf. Section 3) was tuned on the train-
ing portion of the corresponding data, over seven
values in [0,3] in steps of 0.5.6 We used Keras
(Chollet, 2015) and Theano (Team, 2016) for our
model implementations.

Semantic network. The integration of senses
was carried out as described in Section 3. For
disambiguating with both WordNet and Wikipedia
senses we relied on the joint semantic network of
Wikipedia hyperlinks and WordNet via the map-
ping provided by BabelNet.7

5https://wordnet.princeton.edu/man/lexnames.5WN.html
6We observed that values higher than 3 led to very few dis-
ambiguations. While the best results were generally achieved
in the [1.5,2.5] range, performance differences across thresh-
old values were not statistically signiﬁcant in most cases.

7For simplicity we refer to this joint sense inventory as

all

the

experiments we

Pre-trained word and sense
embeddings.
used
Throughout
Word2vec (Mikolov et al., 2013) embeddings,
trained on the Google News corpus.8 We trun-
cated this set to its 250K most frequent words.
We also used WordNet 3.0 (Fellbaum, 1998)
and the Wikipedia dump of November 2014 to
compute the sense embeddings (see Section 4.1).
As a result, we obtained a set of 757,262 sense
embeddings in the same space as the pre-trained
Word2vec word embeddings. We used DeConf
(Pilehvar and Collier, 2016) as our pre-trained
WordNet sense embeddings. All vectors had a
ﬁxed dimensionality of 300.

Supersenses.
In addition to WordNet senses, we
experimented with supersenses (see Section 4.2)
to check how reducing granularity would affect
system performance. For obtaining supersenses
in a given text we relied on our disambiguation
pipeline and simply clustered together senses be-
longing to the same WordNet supersense.

Evaluation measures. We report the results in
terms of standard accuracy and F1 measures.9

5.2 Topic Categorization

The task of topic categorization consists of assign-
ing a label (i.e. topic) to a given document from a
pre-deﬁned set of labels.

5.2.1 Datasets

For this task we used two newswire and one med-
ical topic categorization datasets. Table 1 sum-
marizes the statistics of each dataset.10 The BBC
news dataset11 (Greene and Cunningham, 2006)
comprises news articles taken from BBC, divided
into ﬁve topics: business, entertainment, politics,
sport and tech. Newsgroups (Lang, 1995) is a col-
lection of 11,314 documents for training and 7532
for testing12 divided into six topics: computing,
sport and motor vehicles, science, politics, reli-

8https://code.google.com/archive/p/word2vec/
9Since all models in our experiments provide full cover-
age, accuracy and F1 denote micro- and macro-averaged F1,
respectively (Yang, 1999).

10The coverage of the datasets was computed using the
250K top words in the Google News Word2vec embeddings.

11http://mlg.ucd.ie/datasets/bbc.html
12We used the train-test partition available at http://qwone.

Wikipedia, but note that WordNet senses are also covered.

com/∼jason/20Newsgroups/

Dataset

Domain No. of classes No. of docs Avg. doc. size Size of vocab. Coverage

Evaluation

BBC
News
Newsgroups News
Ohsumed

Medical

5
6
23

2,225
18,846
23,166

439.5
394.0
201.2

35,628
225,046
65,323

87.4%
83.4%
79.3%

10 cross valid.
Train-Test
Train-Test

Table 1: Statistics of the topic categorization datasets.

Initialization

Input type

BBC News
Acc
93.0

F1
92.8

Newsgroups
F1
Acc
85.6
87.7

Ohsumed

Random

Pre-trained

Word

Sense

WordNet
Wikipedia
Supersense WordNet
Wikipedia

Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

93.5
92.7
93.6
94.6∗

97.6
97.3

96.3
96.8
96.9

93.3
92.5
93.4
94.4

97.5
97.1

96.2
96.7
96.9

88.1
86.7
90.1∗
88.5

91.1
90.2
89.6†
89.6
88.6

86.9
84.9
89.0
85.8

90.6
88.6

88.9
88.9
87.4

Acc
30.1
27.2†
29.7
31.8∗
31.1

29.4
30.2

32.4
29.5
30.6∗

F1
20.7

18.3
20.9
22.0
21.3

20.1
20.4

22.3
19.9
20.3

Table 2: Classiﬁcation performance at the word, sense, and supersense levels with random and pre-
trained embedding initialization. We show in bold those settings that improve the word-based model.

gion and sales.13 Finally, Ohsumed14 is a col-
lection of medical abstracts from MEDLINE, an
online medical information database, categorized
according to 23 cardiovascular diseases. For our
experiments we used the partition split of 10,433
documents for training and 12,733 for testing.15

5.2.2 Results

Table 2 shows the results of our classiﬁcation
model and its variants on the three datasets.16
When the embedding layer is initialized randomly,
the model integrated with word senses consistently
improves over the word-based model, particularly
when the ﬁne-granularity of the underlying sense
inventory is reduced using supersenses (with sta-
tistically signiﬁcant gains on the three datasets).
This highlights the fact that a simple disambigua-
tion of the input can bring about performance gain
for a state-of-the-art classiﬁcation system. Also,

13The dataset has 20 ﬁne-grained categories clustered into
six general topics. We used the coarse-grained labels for their
clearer distinction and consistency with BBC topics.

14ftp://medir.ohsu.edu/pub/ohsumed
15http://disi.unitn.it/moschitti/corpora.htm
16Symbols ∗ and † indicate the sense-based model with the
smallest margin to the word-based model whose accuracy is
statistically signiﬁcant at 0.95 conﬁdence level according to
unpaired t-test (∗ for positive and † for negative change).

the better performance of supersenses suggests
that the sense distinctions of WordNet are too ﬁne-
grained for the topic categorization task. How-
ever, when pre-trained representations are used to
initialize the embedding layer, no improvement is
observed over the word-based model. This can
be attributed to the quality of the representations,
as the model utilizing them was unable to beneﬁt
from the advantage offered by sense distinctions.
Our results suggest that research in sense represen-
tation should put special emphasis on real-world
evaluations on benchmarks for downstream appli-
cations, rather than on artiﬁcial tasks such as word
similarity. In fact, research has previously shown
that word similarity might not constitute a reliable
proxy to measure the performance of word embed-
dings in downstream applications (Tsvetkov et al.,
2015; Chiu et al., 2016).

Among the three datasets, Ohsumed proves to
be the most challenging one, mainly for its larger
number of classes (i.e. 23) and its domain-speciﬁc
nature (i.e. medicine). Interestingly, unlike for the
other two datasets, the introduction of pre-trained
word embeddings to the system results in reduced
performance on Ohsumed. This suggests that gen-
eral domain embeddings might not be beneﬁcial

in specialized domains, which corroborates previ-
ous ﬁndings by Yadav et al. (2017) on a different
task, i.e. entity extraction. This performance drop
may also be due to diachronic issues (Ohsumed
the
dates back to the 1980s) and low coverage:
pre-trained Word2vec embeddings cover 79.3% of
the words in Ohsumed (see Table 1), in contrast
to the higher coverage on the newswire datasets,
i.e. Newsgroups (83.4%) and BBC (87.4%). How-
ever, also note that the best overall performance
is attained when our pre-trained Wikipedia sense
embeddings are used. This highlights the effec-
tiveness of Wikipedia in handling domain-speciﬁc
entities, thanks to its broad sense inventory.

5.3 Polarity Detection

Polarity detection is the most popular evaluation
framework for sentiment analysis (Dong et al.,
2015). The task is essentially a binary classiﬁca-
tion which determines if the sentiment of a given
sentence or document is negative or positive.

5.3.1 Datasets

For the polarity detection task we used ﬁve stan-
dard evaluation datasets. Table 1 summarizes
statistics. PL04 (Pang and Lee, 2004) is a polar-
ity detection dataset composed of full movie re-
views. PL0518 (Pang and Lee, 2005), instead, is
composed of short snippets from movie reviews.
RTC contains critic reviews from Rotten Toma-
toes19, divided into 436,000 training and 2,000
test instances. IMDB (Maas et al., 2011) includes
50,000 movie reviews, split evenly between train-
ing and test. Finally, we used the Stanford Sen-
timent dataset (Socher et al., 2013), which asso-
ciates each review with a value that denotes its
sentiment. To be consistent with the binary classi-
ﬁcation of the other datasets, we removed the neu-
tral phrases according to the dataset’s scale (be-
tween 0.4 and 0.6) and considered the reviews
whose values were below 0.4 as negative and
above 0.6 as positive. This resulted in a binary po-
larity dataset of 119,783 phrases. Unlike the previ-
ous four datasets, this dataset does not contain an
even distribution of positive and negative labels.

5.3.2 Results

Table 4 lists accuracy performance of our classi-
ﬁcation model and all its variants on ﬁve polar-

18Both PL04 and PL05 were downloaded from http://

www.cs.cornell.edu/people/pabo/movie-review-data/

19http://www.rottentomatoes.com

BBC

Ohsumed

PL04

IMDB

Newsgroups

n
i
a
g

y
c
a
r
u
c
c
A

1.5

0.5

1

0

−0.5

−1

−1.5

RTC
Stanford

PL05

0

100

200

300

400

500

600

700

800

Average document size

Figure 4: Relation between average document size
and performance improvement using Wikipedia
supersenses with random initialization.

ity detection datasets. Results are generally better
than those of Kim (2014), showing that the addi-
tion of the recurrent layer to the model (cf. Section
4) was beneﬁcial. However, interestingly, no con-
sistent performance gain is observed in the polar-
ity detection task, when the model is provided with
disambiguated input, particularly for datasets with
relatively short reviews. We attribute this to the
nature of the task. Firstly, given that words rarely
happen to be ambiguous with respect to their senti-
ment, the semantic sense distinctions provided by
the disambiguation stage do not assist the classiﬁer
in better decision making, and instead introduce
data sparsity. Secondly, since the datasets mostly
contain short texts, e.g., sentences or snippets, the
disambiguation algorithm does not have sufﬁcient
context to make high-conﬁdence judgements, re-
sulting in fewer disambiguations or less reliable
ones. In the following section we perform a more
in-depth analysis of the impact of document size
on the performance of our sense-based models.

5.4 Analysis

Document size. A detailed analysis revealed a
relation between document size (the number of
tokens) and performance gain of our sense-level
model. We show in Figure 4 how these two
vary for our most consistent conﬁguration, i.e.
Wikipedia supersenses, with random initialization.
Interestingly, as a general trend, the performance
gain increases with average document size, irre-

19Stanford is the only unbalanced dataset, but F1 results

were almost identical to accuracy.

Dataset

Type

No. of docs

Avg. doc. size

Vocabulary size

Coverage

Evaluation

RTC
IMDB
PL05
PL04
Stanford

Snippets
Reviews
Snippets
Reviews
Phrases

438,000
50,000
10,662
2,000
119,783

23.4
268.8
21.5
762.1
10.0

128,056
140,172
19,825
45,077
19,400

81.3%
82.5%
81.3%
82.4%
81.6%

Train-Test
Train-Test
10 cross valid.
10 cross valid.
10 cross valid.

Table 3: Statistics of the polarity detection datasets.

Initialization

Random

Pre-trained

Input type
Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

RTC
83.6
83.2

83.1

84.4
83.1

85.5

83.4

83.8

85.2

84.2

IMDB
87.7
87.4

88.0

88.0
88.4∗

88.3

88.3
87.0†
88.8

87.9

PL05
77.3
76.6
75.9†
75.9
75.8

80.2

79.2

79.2

79.5
78.3†

PL04
67.9
67.4

67.1

66.2
69.3∗

72.5
69.7†
73.1

73.8

72.6

Stanford
91.8
91.3

91.0
91.4†
91.0

93.1

92.6

92.3
92.7†
92.2

Table 4: Accuracy performance on ﬁve polarity detection datasets. Given that polarity datasets are
balanced17, we do not report F1 which would have been identical to accuracy.

spective of the classiﬁcation task. We attribute this
to two main factors:

1. Sparsity: Splitting a word into multiple word
senses can have the negative side effect that
the corresponding training data for that word
is distributed among multiple independent
senses. This reduces the training instances
per word sense, which might affect the classi-
ﬁer’s performance, particularly when senses
are semantically related (in comparison to
ﬁne-grained senses, supersenses address this
issue to some extent).

2. Disambiguation quality: As also mentioned
previously, our disambiguation algorithm re-
quires the input text to be sufﬁciently large so
as to create a graph with an adequate num-
ber of coherent connections to function ef-
fectively. In fact, for topic categorization, in
which the documents are relatively long, our
algorithm manages to disambiguate a larger
proportion of words in documents with high
conﬁdence. The lower performance of graph-
based disambiguation algorithms on short

texts is a known issue (Moro et al., 2014; Ra-
ganato et al., 2017), the tackling of which re-
mains an area of exploration.

Senses granularity. Our results showed that re-
ducing ﬁne-granularity of sense distinctions can
irrespective of the
be beneﬁcial to both tasks,
i.e. WordNet or
underlying sense inventory,
Wikipedia, which corroborates previous ﬁndings
(Hovy et al., 2013; Flekova and Gurevych, 2016).
This suggests that text classiﬁcation does not re-
In this
quire ﬁne-grained semantic distinctions.
work we used a simple technique based on Word-
Net’s lexicographer ﬁles for coarsening senses in
this sense inventory as well as in Wikipedia. We
leave the exploration of this promising area as well
as the evaluation of other granularity reduction
techniques for WordNet (Snow et al., 2007; Bhag-
wani et al., 2013) and Wikipedia (Dandala et al.,
2013) sense inventories to future work.

6 Related Work

The past few years have witnessed a growing re-
search interest in semantic representation, mainly
as a consequence of the word embedding tsunami

(Mikolov et al., 2013; Pennington et al., 2014).
Soon after their introduction, word embeddings
were integrated into different NLP applications,
thanks to the migration of the ﬁeld to deep learning
and the fact that most deep learning models view
words as dense vectors. The waves of the word
embedding tsunami have also lapped on the shores
of sense representation. Several techniques have
been proposed that either extend word embedding
models to cluster contexts and induce senses, usu-
ally referred to as unsupervised sense represen-
tations (Sch¨utze, 1998; Reisinger and Mooney,
2010; Huang et al., 2012; Neelakantan et al., 2014;
Guo et al., 2014; Tian et al., 2014; ˇSuster et al.,
2016; Ettinger et al., 2016; Qiu et al., 2016) or
exploit external sense inventories and lexical re-
sources for generating sense representations for
individual meanings of words (Chen et al., 2014;
Johansson and Pina, 2015; Jauhar et al., 2015; Ia-
cobacci et al., 2015; Rothe and Sch¨utze, 2015;
Camacho-Collados et al., 2016b; Mancini et al.,
2016; Pilehvar and Collier, 2016).

However, the integration of sense representa-
tions into deep learning models has not been so
straightforward, and research in this ﬁeld has of-
ten opted for alternative evaluation benchmarks
such as WSD, or artiﬁcial tasks, such as word
similarity. Consequently, the problem of integrat-
ing sense representations into downstream NLP
applications has remained understudied, despite
the potential beneﬁts it can have. Li and Juraf-
sky (2015) proposed a “multi-sense embedding”
pipeline to check the beneﬁt that can be gained
by replacing word embeddings with sense embed-
dings in multiple tasks. With the help of two
simple disambiguation algorithms, unsupervised
sense embeddings were integrated into various
downstream applications, with varying degrees of
success. Given the interdependency of sense rep-
resentation and disambiguation in this model, it is
very difﬁcult to introduce alternative algorithms
into its pipeline, either to beneﬁt from the state
of the art, or to carry out an evaluation. Instead,
our pipeline provides the advantage of being mod-
thanks to its use of disambiguation in the
ular:
pre-processing stage and use of sense representa-
tions that are linked to external sense inventories,
different WSD techniques and sense representa-
tions can be easily plugged in and checked. Along
the same lines, Flekova and Gurevych (2016) pro-
posed a technique for learning supersense rep-

resentations, using automatically-annotated cor-
pora. Coupled with a supersense tagger, the rep-
resentations were fed into a neural network clas-
siﬁer as additional features to the word-based in-
put. Through a set of experiments, Flekova and
Gurevych (2016) showed that the supersense en-
richment can be beneﬁcial to a range of binary
classiﬁcation tasks. Our proposal is different in
that it focuses directly on the beneﬁts that can
be gained by semantifying the input,
re-
ducing lexical ambiguity in the input text, rather
than assisting the model with additional sources
of knowledge.

i.e.

7 Conclusion and Future Work

We proposed a pipeline for the integration of sense
level knowledge into a state-of-the-art text clas-
siﬁer. We showed that a simple disambiguation
of the input can lead to consistent performance
gain, particularly for longer documents and when
the granularity of the underlying sense inventory
is reduced. Our pipeline is modular and can be
used as an in vivo evaluation framework for WSD
and sense representation techniques. We release
our code and data to reproduce our experiments
(including pre-trained sense and supersense em-
beddings) at https://github.com/pilehvar/sensecnn
to allow further checking of the choice of hyperpa-
rameters and to allow further analysis and compar-
ison. We hope that our work will foster future re-
search on the integration of sense-level knowledge
into downstream applications. As future work, we
plan to investigate the extension of the approach
to other languages and applications. Also, given
the promising results observed for supersenses, we
will investigate task-speciﬁc coarsening of sense
inventories, particularly Wikipedia, or the use of
SentiWordNet (Baccianella et al., 2010), which
could be more suitable for polarity detection.

Acknowledgments

The authors gratefully acknowledge the sup-
port of the MRC grant No. MR/M025160/1
for PheneBank and ERC Consolidator Grant
MOUSSE No. 726487. Jose Camacho-Collados
is supported by a Google Doctoral Fellowship in
Natural Language Processing. Nigel Collier is
supported by EPSRC Grant No. EP/M005089/1.
We thank Jim McManus for his suggestions on the
manuscript and the anonymous reviewers for their
helpful comments.

References

Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics
40(1):57–84.

Eneko Agirre and Oier Lopez. 2003. Clustering Word-
In Proceedings of Recent Ad-
Net word senses.
vances in Natural Language Processing. Borovets,
Bulgaria, pages 121–130.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC. volume 10, pages 2200–2204.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res. 12:2493–2537.

Bharath Dandala, Chris Hokamp, Rada Mihalcea, and
Razvan C. Bunescu. 2013. Sense clustering us-
ing Wikipedia. In Proceedings of Recent Advances
in Natural Language Processing. Hissar, Bulgaria,
pages 164–171.

Li Dong, Furu Wei, Shujie Liu, Ming Zhou, and
Ke Xu. 2015. A statistical parsing framework for
sentiment classiﬁcation. Computational Linguistics
41(2):293–336.

Miguel Ballesteros, Chris Dyer, and Noah A Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of EMNLP.

Luis Espinosa-Anke, Jose Camacho-Collados, Claudio
Delli Bovi, and Horacio Saggion. 2016. Supervised
distributional hypernym discovery via domain adap-
tation. In Proceedings of EMNLP. pages 424–435.

Sumit Bhagwani, Shrutiranjan Satapathy, and Harish
In Pro-
Karnick. 2013. Merging word senses.
ceedings of TextGraphs-8 Graph-based Methods for
Natural Language Processing. Seattle, Washington,
USA, pages 11–19.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
dings. In EMNLP.

Jos´e Camacho-Collados, Claudio Delli Bovi, Alessan-
dro Raganato, and Roberto Navigli. 2016a. A
of
Large-Scale Multilingual Disambiguation
In Proceedings of LREC. Portoroz,
Glosses.
Slovenia, pages 1701–1708.

Jose Camacho-Collados and Roberto Navigli. 2017.
BabelDomains: Large-Scale Domain Labeling of
Lexical Resources. In Proceedings of EACL (2). Va-
lencia, Spain.

Jos´e Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2016b. Nasari: Integrating ex-
plicit knowledge and corpus statistics for a multilin-
gual representation of concepts and entities. Artiﬁ-
cial Intelligence 240:36–64.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A uniﬁed model for word sense representation and
disambiguation. In Proceedings of EMNLP. Doha,
Qatar, pages 1025–1035.

Xiao Cheng and Dan Roth. 2013. Relational inference
for wikiﬁcation. In Proceedings of EMNLP. Seattle,
Washington, pages 1787–1796.

Billy Chiu, Anna Korhonen, and Sampo Pyysalo. 2016.
Intrinsic evaluation of word vectors fails to predict
extrinsic performance. In Proceedings of the Work-
shop on Evaluating Vector Space Representations
for NLP, ACL.

Franc¸ois Chollet. 2015. Keras.

https://github.com/

fchollet/keras.

Allyson Ettinger, Philip Resnik, and Marine Carpuat.
2016. Retroﬁtting sense-speciﬁc word vectors using
In Proceedings of NAACL-HLT. San
parallel text.
Diego, California, pages 1378–1383.

Christiane Fellbaum, editor. 1998. WordNet: An Elec-

tronic Database. MIT Press, Cambridge, MA.

Lucie Flekova and Iryna Gurevych. 2016. Supersense
embeddings: A uniﬁed model for supersense inter-
pretation, prediction, and utilization. In Proceedings
of ACL.

Derek Greene and P´adraig Cunningham. 2006. Practi-
cal solutions to the problem of diagonal dominance
in kernel document clustering. In Proceedings of the
23rd International conference on Machine learning.
ACM, pages 377–384.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-speciﬁc word embed-
In COL-
dings by exploiting bilingual resources.
ING. pages 497–507.

Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of the 11th International Conference
on World Wide Web. Hawaii, USA, pages 517–526.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
Improving neural networks by
dinov. 2012.
preventing co-adaptation of feature detectors. CoRR
abs/1207.0580.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen,
Martin Theobald, and Gerhard Weikum. 2012.
Kore: keyphrase overlap relatedness for entity dis-
ambiguation. In Proceedings of CIKM. pages 545–
554.

Eduard H. Hovy, Roberto Navigli, and Simone Paolo
semi-
Ponzetto. 2013.
structured content and Artiﬁcial Intelligence: The
story so far. Artiﬁcial Intelligence 194:2–27.

Collaboratively built

Xiao Ling, Sameer Singh, and Daniel S Weld. 2015.
Design challenges for entity linking. Transactions
of the Association for Computational Linguistics
3:315–328.

Eric H. Huang, Richard Socher, Christopher D. Man-
Improving word
ning, and Andrew Y. Ng. 2012.
representations via global context and multiple word
prototypes. In Proceedings of ACL. Jeju Island, Ko-
rea, pages 873–882.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of ACL-HLT. Portland, Oregon,
USA, pages 142–150.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning sense
embeddings for word and relational similarity.
In
Proceedings of ACL. Beijing, China, pages 95–105.

Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy.
2015. Ontologically grounded multi-sense repre-
sentation learning for semantic vector space models.
In Proceedings of NAACL. Denver, Colorado, pages
683–693.

Richard Johansson and Luis Nieto Pina. 2015. Embed-
ding a semantic network in a word space. In Pro-
ceedings of NAACL. Denver, Colorado, pages 1428–
1433.

Rie Johnson and Tong Zhang. 2015. Effective use
of word order for text categorization with convolu-
tional neural networks. In Proceedings of NAACL.
Denver, Colorado, pages 103–112.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL. Bal-
timore, USA, pages 655–665.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of EMNLP.
Doha, Qatar, pages 1746–1751.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proceedings of AAAI. Phoenix, Arizona,
pages 2741–2749.

Ken Lang. 1995. Newsweeder: Learning to ﬁlter net-
news. In Proceedings of the 12th International Con-
ference on Machine Learning. Tahoe City, Califor-
nia, pages 331–339.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
In Proceedings of EMNLP. Lisbon, Portugal, pages
683–693.

Antonio Lieto, Enrico Mensa, and Daniele P Radicioni.
2016. A resource-driven approach for anchoring lin-
In AI* IA
guistic resources to conceptual spaces.
2016 Advances in Artiﬁcial Intelligence, Springer,
pages 435–449.

Nut Limsopatham and Nigel Collier. 2016. Normalis-
ing medical concepts in social media texts by learn-
ing semantic representation. In Proceedings of ACL.
Berlin, Germany, pages 1014–1023.

Massimiliano Mancini,

Jose Camacho-Collados,
Ignacio Iacobacci, and Roberto Navigli. 2016.
via
Embedding words
CoRR
joint
abs/1612.02703. http://arxiv.org/abs/1612.02703.

knowledge-enhanced

training.

together

senses

and

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations.
pages 55–60.

Oren Melamud, Jacob Goldberger, and Ido Dagan.
context2vec: Learning generic context
2016.
In Proceed-
embedding with bidirectional lstm.
ings of The 20th SIGNLL Conference on Compu-
tational Natural Language Learning. Berlin, Ger-
many, pages 51–61.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient estimation of word represen-
tations in vector space. CoRR abs/1301.3781.

George A Miller. 1995.

a lexical
database for english. Communications of the ACM
38(11):39–41.

WordNet:

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Disam-
biguation: a Uniﬁed Approach. Transactions of the
Association for Computational Linguistics (TACL)
2:231–244.

Vinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning. pages 807–814.

Roberto Navigli. 2006. Meaningful clustering of
senses helps boost Word Sense Disambiguation per-
In Proceedings of COLING-ACL. Syd-
formance.
ney, Australia, pages 105–112.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artiﬁcial Intelligence 193:217–
250.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efﬁcient non-
parametric estimation of multiple embeddings per
In Proceedings of EMNLP.
word in vector space.
Doha, Qatar, pages 1059–1069.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
ACL. Barcelona, Spain, pages 51–61.

Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. Learning semantic textual similar-
In Proceedings
ity with structural representations.
of ACL (2). Soﬁa, Bulgaria, pages 714–718.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL.
Ann Arbor, Michigan, pages 115–124.

Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-
drew Y. Ng. 2007. Learning to merge word senses.
In Proceedings of EMNLP. Prague, Czech Republic,
pages 1005–1014.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
In Proceedings of EMNLP. pages
representation.
1532–1543.

Mohammad Taher Pilehvar and Nigel Collier. 2016.
De-conﬂated semantic representations. In Proceed-
ings of EMNLP. Austin, TX, pages 1680–1690.

Mohammad Taher Pilehvar and Roberto Navigli. 2014.
A large-scale pseudoword-based evaluation frame-
work for state-of-the-art Word Sense Disambigua-
tion. Computational Linguistics 40(4).

Lin Qiu, Kewei Tu, and Yong Yu. 2016. Context-
In Proceedings of

dependent sense embedding.
EMNLP. Austin, Texas, pages 183–191.

Alessandro Raganato, Jose Camacho-Collados, and
Roberto Navigli. 2017. Word sense disambiguation:
A uniﬁed evaluation framework and empirical com-
parison. In Proceedings of EACL. Valencia, Spain,
pages 99–110.

Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceedings of ACL. pages 109–117.

Sascha Rothe and Hinrich Sch¨utze. 2015. Autoex-
tend: Extending word embeddings to embeddings
In Proceedings of ACL.
for synsets and lexemes.
Beijing, China, pages 1793–1803.

Stefan R¨ud, Massimiliano Ciaramita, Jens M¨uller, and
Hinrich Sch¨utze. 2011. Piggyback: Using search
engines for robust cross-domain named entity recog-
nition. In Proceedings of ACL-HLT. Portland, Ore-
gon, USA, pages 965–975.

Ivan A Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
In Inter-
expressions: A pain in the neck for nlp.
national Conference on Intelligent Text Processing
and Computational Linguistics. Mexico City, Mex-
ico, pages 1–15.

Bahar Salehi, Paul Cook, and Timothy Baldwin. 2015.
A word embedding approach to predicting the com-
positionality of multiword expressions. In NAACL-
HTL. Denver, Colorado, pages 977–983.

Hinrich Sch¨utze. 1998. Automatic word sense discrim-
ination. Computational linguistics 24(1):97–123.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher Manning, Andrew Ng, and
Christopher Potts. 2013. Parsing with compositional
vector grammars. In Proceedings of EMNLP. Soﬁa,
Bulgaria, pages 455–465.

Simon ˇSuster, Ivan Titov, and Gertjan van Noord. 2016.
Bilingual learning of multi-sense embeddings with
In Proceedings of NAACL-
discrete autoencoders.
HLT. San Diego, California, pages 1346–1356.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
sentiment classiﬁcation. In Porceedings of EMNLP.
Lisbon, Portugal, pages 1422–1432.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints abs/1605.02688.

Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In COLING. pages 151–160.

Rocco Tripodi and Marcello Pelillo. 2017. A game-
theoretic approach to word sense disambiguation.
Computational Linguistics 43(1):31–70.

Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
laume Lample, and Chris Dyer. 2015. Evaluation
of word vector representations by subspace align-
ment. In Proceedings of EMNLP (2). Lisbon, Por-
tugal, pages 2049–2054.

Yulia Tsvetkov and Shuly Wintner. 2014.

Identiﬁca-
tion of multiword expressions by combining multi-
ple linguistic information sources. Computational
Linguistics 40(2):449–468.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural network
In Proceedings of ACL.
transition-based parsing.
Beijing, China, pages 323–333.

Yijun Xiao and Kyunghyun Cho. 2016.

Efﬁcient
character-level document classiﬁcation by com-
bining convolution and recurrent layers. CoRR
abs/1602.00367.

Shweta Yadav, Asif Ekbal, Sriparna Saha, and Pushpak
Bhattacharyya. 2017. Entity extraction in biomedi-
cal corpora: An approach to evaluate word embed-
ding features with pso based feature selection.
In
Proceedings of EACL. Valencia, Spain, pages 1159–
1170.

Yiming Yang. 1999. An evaluation of statistical ap-
Information re-

proaches to text categorization.
trieval 1(1-2):69–90.

Jay Young, Valerio Basile, Lars Kunze, Elena Cabrio,
and Nick Hawes. 2016. Towards lifelong object
learning by integrating situated robot perception and
In Proceedings of the Eu-
semantic web mining.
ropean Conference on Artiﬁcial Intelligence confer-
ence. The Hague, Netherland, pages 1458–1466.

Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage Word Sense Disambiguation sys-
tem for free text. In Proceedings of the ACL System
Demonstrations. Uppsala, Sweden, pages 78–83.

Will Y. Zou, Richard Socher, Daniel M. Cer, and
Christopher D. Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation.
In
Proceedings of EMNLP. Seattle, USA, pages 1393–
1398.

Towards a Seamless Integration of Word Senses
into Downstream NLP Applications

Mohammad Taher Pilehvar2, Jose Camacho-Collados1,
Roberto Navigli1 and Nigel Collier2

1Department of Computer Science, Sapienza University of Rome
2Department of Theoretical and Applied Linguistics, University of Cambridge
1{collados,navigli}@di.uniroma1.it
2{mp792,nhc30}@cam.ac.uk

Abstract

Lexical ambiguity can impede NLP sys-
tems from accurate understanding of se-
mantics. Despite its potential beneﬁts, the
integration of sense-level information into
NLP systems has remained understudied.
By incorporating a novel disambiguation
algorithm into a state-of-the-art classiﬁca-
tion model, we create a pipeline to inte-
grate sense-level information into down-
stream NLP applications. We show that
a simple disambiguation of the input text
can lead to consistent performance im-
provement on multiple topic categoriza-
tion and polarity detection datasets, par-
ticularly when the ﬁne granularity of the
underlying sense inventory is reduced and
the document is sufﬁciently large. Our re-
sults also point to the need for sense rep-
resentation research to focus more on in
vivo evaluations which target the perfor-
mance in downstream NLP applications
rather than artiﬁcial benchmarks.

1

Introduction

As a general trend, most current Natural Language
Processing (NLP) systems function at the word
level, i.e.
individual words constitute the most
ﬁne-grained meaning-bearing elements of their in-
put. The word level functionality can affect the
performance of these systems in two ways: (1)
it can hamper their efﬁciency in handling words
that are not encountered frequently during train-
ing, such as multiwords, inﬂections and deriva-
tions, and (2) it can restrict their semantic under-
standing to the level of words, with all their am-
biguities, and thereby prevent accurate capture of
the intended meanings.

The ﬁrst issue has recently been alleviated by

techniques that aim to boost the generalisation
power of NLP systems by resorting to sub-word
or character-level information (Ballesteros et al.,
2015; Kim et al., 2016). The second limitation,
however, has not yet been studied sufﬁciently. A
reasonable way to handle word ambiguity, and
hence to tackle the second issue, is to semantify
the input text: transform it from its surface-level
semantics to the deeper level of word senses, i.e.
their intended meanings. We take a step in this di-
rection by designing a pipeline that enables seam-
less integration of word senses into downstream
NLP applications, while beneﬁting from knowl-
edge extracted from semantic networks. To this
end, we propose a quick graph-based Word Sense
Disambiguation (WSD) algorithm which allows
high conﬁdence disambiguation of words without
much computation overload on the system. We
evaluate the pipeline in two downstream NLP ap-
plications: polarity detection and topic categoriza-
tion. Speciﬁcally, we use a classiﬁcation model
based on Convolutional Neural Networks which
has been shown to be very effective in various
text classiﬁcation tasks (Kalchbrenner et al., 2014;
Kim, 2014; Johnson and Zhang, 2015; Tang et al.,
2015; Xiao and Cho, 2016). We show that a simple
disambiguation of input can lead to performance
improvement of a state-of-the-art text classiﬁca-
tion system on multiple datasets, particularly for
long inputs and when the granularity of the sense
inventory is reduced. Our pipeline is quite ﬂexible
and modular, as it permits the integration of differ-
ent WSD and sense representation techniques.

2 Motivation

With the help of an example news article from the
BBC, shown in Figure 1, we highlight some of the
potential deﬁciencies of word-based models.

7
1
0
2
 
t
c
O
 
8
1
 
 
]
L
C
.
s
c
[
 
 
1
v
2
3
6
6
0
.
0
1
7
1
:
v
i
X
r
a

Algorithm 1 Disambiguation algorithm
Input: Input text T and semantic network N
Output: Set of disambiguated senses ˆS
1: Graph representation of T : (S, E) ← getGraph(T, N )
2: ˆS ← ∅
3: for each iteration i ∈ {1, ..., len(T )}
4:
5:
6:
7:
8:
9:
10:
11: return Disambiguation output ˆS

ˆs = argmaxs∈S |{(s, s(cid:48)) ∈ E : s(cid:48) ∈ S}|
maxDeg = |{(ˆs, s(cid:48)) ∈ E : s(cid:48) ∈ S}|
if maxDeg < θ|S| / 100 then

ˆS ← ˆS ∪ {ˆs}
E ← E \ {(s, s(cid:48)) : s ∨ s(cid:48) ∈ getLex(ˆs)}

break

else

towards resolving ambiguities, but it brings about
other advantages mentioned in the previous sec-
tion. The aim is to provide the system with an
input of reduced ambiguity which can facilitate its
decision making.

To this end, we developed a simple graph-based
joint disambiguation and entity linking algorithm
which can take any arbitrary semantic network
as input. The gist of our disambiguation tech-
nique lies in its speed and scalability. Conven-
tional knowledge-based disambiguation systems
(Hoffart et al., 2012; Agirre et al., 2014; Moro
et al., 2014; Ling et al., 2015; Pilehvar and Nav-
igli, 2014) often rely on computationally expen-
sive graph algorithms, which limits their applica-
tion to on-the-ﬂy processing of large number of
text documents, as is the case in our experiments.
Moreover, unlike supervised WSD and entity link-
ing techniques (Zhong and Ng, 2010; Cheng and
Roth, 2013; Melamud et al., 2016; Limsopatham
and Collier, 2016), our algorithm relies only on
semantic networks and does not require any sense-
annotated data, which is limited to English and al-
most non-existent for other languages.

Algorithm 1 shows our procedure for disam-
biguating an input document T . First, we retrieve
from our semantic network the list of candidate
senses1 for each content word, as well as seman-
tic relationships among them. As a result, we ob-
tain a graph representation (S, E) of the input text,
where S is the set of candidate senses and E is
the set of edges among different senses in S. The
graph is, in fact, a small sub-graph of the input se-
mantic network, N . Our algorithm then selects the
best candidates iteratively. In each iteration, the

1As deﬁned in the underlying sense inventory, up to tri-
grams. We used Stanford CoreNLP (Manning et al., 2014)
for tokenization, Part-of-Speech (PoS) tagging and lemmati-
zation.

Figure 1: Excerpt of a news article from the BBC.

Ambiguity. Language is inherently ambiguous.
For instance, Mercedes, race, Hamilton and For-
mula can refer to several different entities or mean-
ings. Current neural models have managed to
successfully represent complex semantic associ-
ations by effectively analyzing large amounts of
data. However, the word-level functionality of
these systems is still a barrier to the depth of their
natural language understanding. Our proposal is
particularly tailored towards addressing this issue.

Multiword expressions (MWE). MWE are lex-
ical units made up of two or more words which
are idiosyncratic in nature (Sag et al., 2002), e.g,
Lewis Hamilton, Nico Rosberg and Formula 1.
Most existing word-based models ignore the in-
terdependency between MWE’s subunits and treat
them as individual units. Handling MWE has
been a long-standing problem in NLP and has re-
cently received a considerable amount of interest
(Tsvetkov and Wintner, 2014; Salehi et al., 2015).
Our pipeline facilitates this goal.

Co-reference. Co-reference resolution of con-
cepts and entities is not explicitly tackled by our
approach. However, thanks to the fact that words
that refer to the same meaning in context, e.g., For-
mula 1-F1 or German Grand Prix-German GP-
Hockenheim, are all disambiguated to the same
concept, the co-reference issue is also partly ad-
dressed by our pipeline.

3 Disambiguation Algorithm

Our proposal relies on a seamless integration of
word senses in word-based systems. The goal is
to semantify the text prior to its being fed into the
system by transforming its individual units from
word surface form to the deeper level of word
senses. The semantiﬁcation step is mainly tailored

Figure 2: Simpliﬁed graph-based representation of
a sample sentence.

candidate sense that has the highest graph degree
maxDeg is chosen as the winning sense:

maxDeg = max
s∈S

|{(s, s(cid:48)) ∈ E : s(cid:48) ∈ S}|

(1)

After each iteration, when a candidate sense ˆs
is selected, all the possible candidate senses of the
corresponding word (i.e. getLex(ˆs)) are removed
from E (line 10 in the algorithm).

Figure 2 shows a simpliﬁed version of the graph
for a sample sentence. The algorithm would dis-
ambiguate the content words in this sentence as
follows. It ﬁrst associates Oasis with its rock band
sense, since its corresponding node has the high-
est degree, i.e. 3. On the basis of this, the desert
sense of Oasis and its link to the stone sense of
rock are removed from the graph. In the second it-
eration, rock band is disambiguated as music band
given that its degree is 2.2 Finally, Manchester is
associated with its city sense (with a degree of 1).
In order to enable disambiguating at differ-
ent conﬁdence levels, we introduce a threshold θ
which determines the stopping criterion of the al-
gorithm.
Iteration continues until the following
condition is fulﬁlled: maxDeg < θ|S| / 100. This
ensures that the system will only disambiguate
those words for which it has a high conﬁdence and
backs off to the word form otherwise, avoiding the
introduction of unwanted noise in the data for un-
certain cases or for word senses that are not de-
ﬁned in the inventory.

2For bigrams and trigrams whose individual words might
also be disambiguated (such as rock and band in rock band),
the longest unit has the highest priority (i.e. rock band).

Figure 3: Text classiﬁcation model architecture.

4 Classiﬁcation Model

In our experiments, we use a standard neural net-
work based classiﬁcation approach which is simi-
lar to the Convolution Neural Network classiﬁer of
Kim (2014) and the pioneering model of Collobert
et al. (2011). Figure 3 depicts the architecture of
the model. The network receives the concatenated
vector representations of the input words, v1:n =
v1⊕v2⊕· · ·⊕vn, and applies (convolves) ﬁlters F
on windows of h words, mi = f (F.vi:i+h−1 + b),
where b is a bias term and f () is a non-linear func-
tion, for which we use ReLU (Nair and Hinton,
2010). The convolution transforms the input text
to a feature map m = [m1, m2, . . . , mn−h+1].
A max pooling operation then selects the most
salient feature ˆm = max{m} for each ﬁlter.

In the network of Kim (2014), the pooled fea-
tures are directly passed to a fully connected soft-
max layer whose outputs are class probabilities.
However, we add a recurrent layer before soft-
max in order to enable better capturing of long-
distance dependencies. It has been shown by Xiao
and Cho (2016) that a recurrent layer can replace
multiple layers of convolution and be beneﬁcial,
particularly when the length of input text grows.
Speciﬁcally, we use a Long Short-Term Memory
(Hochreiter and Schmidhuber, 1997, LSTM) as
our recurrent layer which was originally proposed
to avoid the vanishing gradient problem and has
proven its abilities in capturing distant dependen-
cies. The LSTM unit computes three gate vectors

(forget, input, and output) as follows:

ft = σ(Wf gt + Uf ht−1 + bf ),
it = σ(Wi gt + Ui ht−1 + bi),
ot = σ(Wo gt + Uo ht−1 + bo),

(2)

where W, U, and b are model parameters and
g and h are input and output sequences, respec-
tively. The cell state vector ct is then computed as
ct = ft ct−1 + it tanh(˜ct) where ˜ct = Wc gt +
Uc ht−1. Finally, the output sequence is computed
as ht = ot tanh(ct). As for regularization, we
used dropout (Hinton et al., 2012) after the em-
bedding layer.

We perform experiments with two conﬁgura-
tions of the embedding layer: (1) Random, initial-
ized randomly and updated during training, and
(2) Pre-trained, initialized by pre-trained repre-
sentations and updated during training. In the fol-
lowing section we describe the pre-trained word
and sense representation used for the initialization
of the second conﬁguration.

4.1 Pre-trained Word and Sense Embeddings

One of the main advantages of neural models
is that they usually represent the input words as
dense vectors. This can signiﬁcantly boost a
system’s generalisation power and results in im-
proved performance (Zou et al., 2013; Bordes
et al., 2014; Kim, 2014; Weiss et al., 2015, inter-
alia). This feature also enables us to directly plug
in pre-trained sense representations and check
them in a downstream application.

In our experiments we generate a set of sense
embeddings by extending DeConf, a recent tech-
nique with state-of-the-art performance on multi-
ple semantic similarity benchmarks (Pilehvar and
Collier, 2016). We leave the evaluation of other
representations to future work. DeConf gets a
pre-trained set of word embeddings and computes
sense embeddings in the same semantic space. To
this end, the approach exploits the semantic net-
work of WordNet (Miller, 1995), using the Person-
alized PageRank (Haveliwala, 2002) algorithm,
and obtains a set of sense biasing words Bs for
a word sense s. The sense representation of s is
then obtained using the following formula:

ˆv(s) =

|Bs|
(cid:88)

1
|Bs|

−i

e

δ v(wi),

(3)

i=1
where δ is a decay parameter and v(wi) is the em-
the ith word in the sense bi-
bedding of wi, i.e.

asing list of s, i.e. Bs. We follow Pilehvar and
Collier (2016) and set δ = 5. Finally, the vector
for sense s is calculated as the average of ˆv(s) and
the embedding of its corresponding word.

Owing to its reliance on WordNet’s semantic
network, DeConf is limited to generating only
those word senses that are covered by this lexical
resource. We propose to use Wikipedia in order
to expand the vocabulary of the computed word
senses. Wikipedia provides a high coverage of
named entities and domain-speciﬁc terms in many
languages, while at the same time also beneﬁting
from a continuous update by collaborators. More-
over, it can easily be viewed as a sense inventory
where individual articles are word senses arranged
through hyperlinks and redirections.

Camacho-Collados et al.

(2016b) proposed
NASARI3, a technique to compute the most salient
words for each Wikipedia page. These salient
words were computed by exploiting the struc-
ture and content of Wikipedia and proved effec-
tive in tasks such as Word Sense Disambiguation
(Tripodi and Pelillo, 2017; Camacho-Collados
et al., 2016a), knowledge-base construction (Li-
eto et al., 2016), domain-adapted hypernym dis-
covery (Espinosa-Anke et al., 2016; Camacho-
Collados and Navigli, 2017) or object recogni-
tion (Young et al., 2016). We view these lists
as biasing words for individual Wikipedia pages,
and then leverage the exponential decay function
(Equation 3) to compute new sense embeddings
in the same semantic space.
In order to repre-
sent both WordNet and Wikipedia sense represen-
tations in the same space, we rely on the WordNet-
Wikipedia mapping provided by BabelNet4 (Nav-
igli and Ponzetto, 2012). For the WordNet synsets
which are mapped to Wikipedia pages in Babel-
Net, we average the corresponding Wikipedia-
based and WordNet-based sense embeddings.

4.2 Pre-trained Supersense Embeddings

It has been argued that WordNet sense distinctions
are too ﬁne-grained for many NLP applications
(Hovy et al., 2013). The issue can be tackled by
grouping together similar senses of the same word,
either using automatic clustering techniques (Nav-
igli, 2006; Agirre and Lopez, 2003; Snow et al.,
2007) or with the help of WordNet’s lexicographer

3We downloaded the salient words for Wikipedia pages
(NASARI English lexical vectors, version 3.0) from http://lcl.
uniroma1.it/nasari/

4We used the Java API from http://babelnet.org

ﬁles5. Various applications have been shown to
improve upon moving from senses to supersenses
(R¨ud et al., 2011; Severyn et al., 2013; Flekova
and Gurevych, 2016). In WordNet’s lexicographer
ﬁles there are a total of 44 sense clusters, referred
to as supersenses, for categories such as event, ani-
mal, and quantity. In our experiments we use these
supersenses in order to reduce granularity of our
WordNet and Wikipedia senses. To generate su-
persense embeddings, we simply average the em-
beddings of senses in the corresponding cluster.

5 Evaluation

We evaluated our model on two classiﬁcation
tasks: topic categorization (Section 5.2) and po-
larity detection (Section 5.3). In the following sec-
tion we present the common experimental setup.

5.1 Experimental setup

Classiﬁcation model. Throughout all the exper-
iments we used the classiﬁcation model described
in Section 4. The general architecture of the model
was the same for both tasks, with slight variations
in hyperparameters given the different natures of
the tasks, following the values suggested by Kim
(2014) and Xiao and Cho (2016) for the two tasks.
Hyperparameters were ﬁxed across all conﬁgura-
tions in the corresponding tasks. The embedding
layer was ﬁxed to 300 dimensions, irrespective of
the conﬁguration, i.e. Random and Pre-trained.
For both tasks the evaluation was carried out by
10-fold cross-validation unless standard training-
testing splits were available. The disambiguation
threshold θ (cf. Section 3) was tuned on the train-
ing portion of the corresponding data, over seven
values in [0,3] in steps of 0.5.6 We used Keras
(Chollet, 2015) and Theano (Team, 2016) for our
model implementations.

Semantic network. The integration of senses
was carried out as described in Section 3. For
disambiguating with both WordNet and Wikipedia
senses we relied on the joint semantic network of
Wikipedia hyperlinks and WordNet via the map-
ping provided by BabelNet.7

5https://wordnet.princeton.edu/man/lexnames.5WN.html
6We observed that values higher than 3 led to very few dis-
ambiguations. While the best results were generally achieved
in the [1.5,2.5] range, performance differences across thresh-
old values were not statistically signiﬁcant in most cases.

7For simplicity we refer to this joint sense inventory as

all

the

experiments we

Pre-trained word and sense
embeddings.
used
Throughout
Word2vec (Mikolov et al., 2013) embeddings,
trained on the Google News corpus.8 We trun-
cated this set to its 250K most frequent words.
We also used WordNet 3.0 (Fellbaum, 1998)
and the Wikipedia dump of November 2014 to
compute the sense embeddings (see Section 4.1).
As a result, we obtained a set of 757,262 sense
embeddings in the same space as the pre-trained
Word2vec word embeddings. We used DeConf
(Pilehvar and Collier, 2016) as our pre-trained
WordNet sense embeddings. All vectors had a
ﬁxed dimensionality of 300.

Supersenses.
In addition to WordNet senses, we
experimented with supersenses (see Section 4.2)
to check how reducing granularity would affect
system performance. For obtaining supersenses
in a given text we relied on our disambiguation
pipeline and simply clustered together senses be-
longing to the same WordNet supersense.

Evaluation measures. We report the results in
terms of standard accuracy and F1 measures.9

5.2 Topic Categorization

The task of topic categorization consists of assign-
ing a label (i.e. topic) to a given document from a
pre-deﬁned set of labels.

5.2.1 Datasets

For this task we used two newswire and one med-
ical topic categorization datasets. Table 1 sum-
marizes the statistics of each dataset.10 The BBC
news dataset11 (Greene and Cunningham, 2006)
comprises news articles taken from BBC, divided
into ﬁve topics: business, entertainment, politics,
sport and tech. Newsgroups (Lang, 1995) is a col-
lection of 11,314 documents for training and 7532
for testing12 divided into six topics: computing,
sport and motor vehicles, science, politics, reli-

8https://code.google.com/archive/p/word2vec/
9Since all models in our experiments provide full cover-
age, accuracy and F1 denote micro- and macro-averaged F1,
respectively (Yang, 1999).

10The coverage of the datasets was computed using the
250K top words in the Google News Word2vec embeddings.

11http://mlg.ucd.ie/datasets/bbc.html
12We used the train-test partition available at http://qwone.

Wikipedia, but note that WordNet senses are also covered.

com/∼jason/20Newsgroups/

Dataset

Domain No. of classes No. of docs Avg. doc. size Size of vocab. Coverage

Evaluation

BBC
News
Newsgroups News
Ohsumed

Medical

5
6
23

2,225
18,846
23,166

439.5
394.0
201.2

35,628
225,046
65,323

87.4%
83.4%
79.3%

10 cross valid.
Train-Test
Train-Test

Table 1: Statistics of the topic categorization datasets.

Initialization

Input type

BBC News
Acc
93.0

F1
92.8

Newsgroups
F1
Acc
85.6
87.7

Ohsumed

Random

Pre-trained

Word

Sense

WordNet
Wikipedia
Supersense WordNet
Wikipedia

Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

93.5
92.7
93.6
94.6∗

97.6
97.3

96.3
96.8
96.9

93.3
92.5
93.4
94.4

97.5
97.1

96.2
96.7
96.9

88.1
86.7
90.1∗
88.5

91.1
90.2
89.6†
89.6
88.6

86.9
84.9
89.0
85.8

90.6
88.6

88.9
88.9
87.4

Acc
30.1
27.2†
29.7
31.8∗
31.1

29.4
30.2

32.4
29.5
30.6∗

F1
20.7

18.3
20.9
22.0
21.3

20.1
20.4

22.3
19.9
20.3

Table 2: Classiﬁcation performance at the word, sense, and supersense levels with random and pre-
trained embedding initialization. We show in bold those settings that improve the word-based model.

gion and sales.13 Finally, Ohsumed14 is a col-
lection of medical abstracts from MEDLINE, an
online medical information database, categorized
according to 23 cardiovascular diseases. For our
experiments we used the partition split of 10,433
documents for training and 12,733 for testing.15

5.2.2 Results

Table 2 shows the results of our classiﬁcation
model and its variants on the three datasets.16
When the embedding layer is initialized randomly,
the model integrated with word senses consistently
improves over the word-based model, particularly
when the ﬁne-granularity of the underlying sense
inventory is reduced using supersenses (with sta-
tistically signiﬁcant gains on the three datasets).
This highlights the fact that a simple disambigua-
tion of the input can bring about performance gain
for a state-of-the-art classiﬁcation system. Also,

13The dataset has 20 ﬁne-grained categories clustered into
six general topics. We used the coarse-grained labels for their
clearer distinction and consistency with BBC topics.

14ftp://medir.ohsu.edu/pub/ohsumed
15http://disi.unitn.it/moschitti/corpora.htm
16Symbols ∗ and † indicate the sense-based model with the
smallest margin to the word-based model whose accuracy is
statistically signiﬁcant at 0.95 conﬁdence level according to
unpaired t-test (∗ for positive and † for negative change).

the better performance of supersenses suggests
that the sense distinctions of WordNet are too ﬁne-
grained for the topic categorization task. How-
ever, when pre-trained representations are used to
initialize the embedding layer, no improvement is
observed over the word-based model. This can
be attributed to the quality of the representations,
as the model utilizing them was unable to beneﬁt
from the advantage offered by sense distinctions.
Our results suggest that research in sense represen-
tation should put special emphasis on real-world
evaluations on benchmarks for downstream appli-
cations, rather than on artiﬁcial tasks such as word
similarity. In fact, research has previously shown
that word similarity might not constitute a reliable
proxy to measure the performance of word embed-
dings in downstream applications (Tsvetkov et al.,
2015; Chiu et al., 2016).

Among the three datasets, Ohsumed proves to
be the most challenging one, mainly for its larger
number of classes (i.e. 23) and its domain-speciﬁc
nature (i.e. medicine). Interestingly, unlike for the
other two datasets, the introduction of pre-trained
word embeddings to the system results in reduced
performance on Ohsumed. This suggests that gen-
eral domain embeddings might not be beneﬁcial

in specialized domains, which corroborates previ-
ous ﬁndings by Yadav et al. (2017) on a different
task, i.e. entity extraction. This performance drop
may also be due to diachronic issues (Ohsumed
the
dates back to the 1980s) and low coverage:
pre-trained Word2vec embeddings cover 79.3% of
the words in Ohsumed (see Table 1), in contrast
to the higher coverage on the newswire datasets,
i.e. Newsgroups (83.4%) and BBC (87.4%). How-
ever, also note that the best overall performance
is attained when our pre-trained Wikipedia sense
embeddings are used. This highlights the effec-
tiveness of Wikipedia in handling domain-speciﬁc
entities, thanks to its broad sense inventory.

5.3 Polarity Detection

Polarity detection is the most popular evaluation
framework for sentiment analysis (Dong et al.,
2015). The task is essentially a binary classiﬁca-
tion which determines if the sentiment of a given
sentence or document is negative or positive.

5.3.1 Datasets

For the polarity detection task we used ﬁve stan-
dard evaluation datasets. Table 1 summarizes
statistics. PL04 (Pang and Lee, 2004) is a polar-
ity detection dataset composed of full movie re-
views. PL0518 (Pang and Lee, 2005), instead, is
composed of short snippets from movie reviews.
RTC contains critic reviews from Rotten Toma-
toes19, divided into 436,000 training and 2,000
test instances. IMDB (Maas et al., 2011) includes
50,000 movie reviews, split evenly between train-
ing and test. Finally, we used the Stanford Sen-
timent dataset (Socher et al., 2013), which asso-
ciates each review with a value that denotes its
sentiment. To be consistent with the binary classi-
ﬁcation of the other datasets, we removed the neu-
tral phrases according to the dataset’s scale (be-
tween 0.4 and 0.6) and considered the reviews
whose values were below 0.4 as negative and
above 0.6 as positive. This resulted in a binary po-
larity dataset of 119,783 phrases. Unlike the previ-
ous four datasets, this dataset does not contain an
even distribution of positive and negative labels.

5.3.2 Results

Table 4 lists accuracy performance of our classi-
ﬁcation model and all its variants on ﬁve polar-

18Both PL04 and PL05 were downloaded from http://

www.cs.cornell.edu/people/pabo/movie-review-data/

19http://www.rottentomatoes.com

BBC

Ohsumed

PL04

IMDB

Newsgroups

n
i
a
g

y
c
a
r
u
c
c
A

1.5

0.5

1

0

−0.5

−1

−1.5

RTC
Stanford

PL05

0

100

200

300

400

500

600

700

800

Average document size

Figure 4: Relation between average document size
and performance improvement using Wikipedia
supersenses with random initialization.

ity detection datasets. Results are generally better
than those of Kim (2014), showing that the addi-
tion of the recurrent layer to the model (cf. Section
4) was beneﬁcial. However, interestingly, no con-
sistent performance gain is observed in the polar-
ity detection task, when the model is provided with
disambiguated input, particularly for datasets with
relatively short reviews. We attribute this to the
nature of the task. Firstly, given that words rarely
happen to be ambiguous with respect to their senti-
ment, the semantic sense distinctions provided by
the disambiguation stage do not assist the classiﬁer
in better decision making, and instead introduce
data sparsity. Secondly, since the datasets mostly
contain short texts, e.g., sentences or snippets, the
disambiguation algorithm does not have sufﬁcient
context to make high-conﬁdence judgements, re-
sulting in fewer disambiguations or less reliable
ones. In the following section we perform a more
in-depth analysis of the impact of document size
on the performance of our sense-based models.

5.4 Analysis

Document size. A detailed analysis revealed a
relation between document size (the number of
tokens) and performance gain of our sense-level
model. We show in Figure 4 how these two
vary for our most consistent conﬁguration, i.e.
Wikipedia supersenses, with random initialization.
Interestingly, as a general trend, the performance
gain increases with average document size, irre-

19Stanford is the only unbalanced dataset, but F1 results

were almost identical to accuracy.

Dataset

Type

No. of docs

Avg. doc. size

Vocabulary size

Coverage

Evaluation

RTC
IMDB
PL05
PL04
Stanford

Snippets
Reviews
Snippets
Reviews
Phrases

438,000
50,000
10,662
2,000
119,783

23.4
268.8
21.5
762.1
10.0

128,056
140,172
19,825
45,077
19,400

81.3%
82.5%
81.3%
82.4%
81.6%

Train-Test
Train-Test
10 cross valid.
10 cross valid.
10 cross valid.

Table 3: Statistics of the polarity detection datasets.

Initialization

Random

Pre-trained

Input type
Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

RTC
83.6
83.2

83.1

84.4
83.1

85.5

83.4

83.8

85.2

84.2

IMDB
87.7
87.4

88.0

88.0
88.4∗

88.3

88.3
87.0†
88.8

87.9

PL05
77.3
76.6
75.9†
75.9
75.8

80.2

79.2

79.2

79.5
78.3†

PL04
67.9
67.4

67.1

66.2
69.3∗

72.5
69.7†
73.1

73.8

72.6

Stanford
91.8
91.3

91.0
91.4†
91.0

93.1

92.6

92.3
92.7†
92.2

Table 4: Accuracy performance on ﬁve polarity detection datasets. Given that polarity datasets are
balanced17, we do not report F1 which would have been identical to accuracy.

spective of the classiﬁcation task. We attribute this
to two main factors:

1. Sparsity: Splitting a word into multiple word
senses can have the negative side effect that
the corresponding training data for that word
is distributed among multiple independent
senses. This reduces the training instances
per word sense, which might affect the classi-
ﬁer’s performance, particularly when senses
are semantically related (in comparison to
ﬁne-grained senses, supersenses address this
issue to some extent).

2. Disambiguation quality: As also mentioned
previously, our disambiguation algorithm re-
quires the input text to be sufﬁciently large so
as to create a graph with an adequate num-
ber of coherent connections to function ef-
fectively. In fact, for topic categorization, in
which the documents are relatively long, our
algorithm manages to disambiguate a larger
proportion of words in documents with high
conﬁdence. The lower performance of graph-
based disambiguation algorithms on short

texts is a known issue (Moro et al., 2014; Ra-
ganato et al., 2017), the tackling of which re-
mains an area of exploration.

Senses granularity. Our results showed that re-
ducing ﬁne-granularity of sense distinctions can
irrespective of the
be beneﬁcial to both tasks,
i.e. WordNet or
underlying sense inventory,
Wikipedia, which corroborates previous ﬁndings
(Hovy et al., 2013; Flekova and Gurevych, 2016).
This suggests that text classiﬁcation does not re-
In this
quire ﬁne-grained semantic distinctions.
work we used a simple technique based on Word-
Net’s lexicographer ﬁles for coarsening senses in
this sense inventory as well as in Wikipedia. We
leave the exploration of this promising area as well
as the evaluation of other granularity reduction
techniques for WordNet (Snow et al., 2007; Bhag-
wani et al., 2013) and Wikipedia (Dandala et al.,
2013) sense inventories to future work.

6 Related Work

The past few years have witnessed a growing re-
search interest in semantic representation, mainly
as a consequence of the word embedding tsunami

(Mikolov et al., 2013; Pennington et al., 2014).
Soon after their introduction, word embeddings
were integrated into different NLP applications,
thanks to the migration of the ﬁeld to deep learning
and the fact that most deep learning models view
words as dense vectors. The waves of the word
embedding tsunami have also lapped on the shores
of sense representation. Several techniques have
been proposed that either extend word embedding
models to cluster contexts and induce senses, usu-
ally referred to as unsupervised sense represen-
tations (Sch¨utze, 1998; Reisinger and Mooney,
2010; Huang et al., 2012; Neelakantan et al., 2014;
Guo et al., 2014; Tian et al., 2014; ˇSuster et al.,
2016; Ettinger et al., 2016; Qiu et al., 2016) or
exploit external sense inventories and lexical re-
sources for generating sense representations for
individual meanings of words (Chen et al., 2014;
Johansson and Pina, 2015; Jauhar et al., 2015; Ia-
cobacci et al., 2015; Rothe and Sch¨utze, 2015;
Camacho-Collados et al., 2016b; Mancini et al.,
2016; Pilehvar and Collier, 2016).

However, the integration of sense representa-
tions into deep learning models has not been so
straightforward, and research in this ﬁeld has of-
ten opted for alternative evaluation benchmarks
such as WSD, or artiﬁcial tasks, such as word
similarity. Consequently, the problem of integrat-
ing sense representations into downstream NLP
applications has remained understudied, despite
the potential beneﬁts it can have. Li and Juraf-
sky (2015) proposed a “multi-sense embedding”
pipeline to check the beneﬁt that can be gained
by replacing word embeddings with sense embed-
dings in multiple tasks. With the help of two
simple disambiguation algorithms, unsupervised
sense embeddings were integrated into various
downstream applications, with varying degrees of
success. Given the interdependency of sense rep-
resentation and disambiguation in this model, it is
very difﬁcult to introduce alternative algorithms
into its pipeline, either to beneﬁt from the state
of the art, or to carry out an evaluation. Instead,
our pipeline provides the advantage of being mod-
thanks to its use of disambiguation in the
ular:
pre-processing stage and use of sense representa-
tions that are linked to external sense inventories,
different WSD techniques and sense representa-
tions can be easily plugged in and checked. Along
the same lines, Flekova and Gurevych (2016) pro-
posed a technique for learning supersense rep-

resentations, using automatically-annotated cor-
pora. Coupled with a supersense tagger, the rep-
resentations were fed into a neural network clas-
siﬁer as additional features to the word-based in-
put. Through a set of experiments, Flekova and
Gurevych (2016) showed that the supersense en-
richment can be beneﬁcial to a range of binary
classiﬁcation tasks. Our proposal is different in
that it focuses directly on the beneﬁts that can
be gained by semantifying the input,
re-
ducing lexical ambiguity in the input text, rather
than assisting the model with additional sources
of knowledge.

i.e.

7 Conclusion and Future Work

We proposed a pipeline for the integration of sense
level knowledge into a state-of-the-art text clas-
siﬁer. We showed that a simple disambiguation
of the input can lead to consistent performance
gain, particularly for longer documents and when
the granularity of the underlying sense inventory
is reduced. Our pipeline is modular and can be
used as an in vivo evaluation framework for WSD
and sense representation techniques. We release
our code and data to reproduce our experiments
(including pre-trained sense and supersense em-
beddings) at https://github.com/pilehvar/sensecnn
to allow further checking of the choice of hyperpa-
rameters and to allow further analysis and compar-
ison. We hope that our work will foster future re-
search on the integration of sense-level knowledge
into downstream applications. As future work, we
plan to investigate the extension of the approach
to other languages and applications. Also, given
the promising results observed for supersenses, we
will investigate task-speciﬁc coarsening of sense
inventories, particularly Wikipedia, or the use of
SentiWordNet (Baccianella et al., 2010), which
could be more suitable for polarity detection.

Acknowledgments

The authors gratefully acknowledge the sup-
port of the MRC grant No. MR/M025160/1
for PheneBank and ERC Consolidator Grant
MOUSSE No. 726487. Jose Camacho-Collados
is supported by a Google Doctoral Fellowship in
Natural Language Processing. Nigel Collier is
supported by EPSRC Grant No. EP/M005089/1.
We thank Jim McManus for his suggestions on the
manuscript and the anonymous reviewers for their
helpful comments.

References

Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics
40(1):57–84.

Eneko Agirre and Oier Lopez. 2003. Clustering Word-
In Proceedings of Recent Ad-
Net word senses.
vances in Natural Language Processing. Borovets,
Bulgaria, pages 121–130.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC. volume 10, pages 2200–2204.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res. 12:2493–2537.

Bharath Dandala, Chris Hokamp, Rada Mihalcea, and
Razvan C. Bunescu. 2013. Sense clustering us-
ing Wikipedia. In Proceedings of Recent Advances
in Natural Language Processing. Hissar, Bulgaria,
pages 164–171.

Li Dong, Furu Wei, Shujie Liu, Ming Zhou, and
Ke Xu. 2015. A statistical parsing framework for
sentiment classiﬁcation. Computational Linguistics
41(2):293–336.

Miguel Ballesteros, Chris Dyer, and Noah A Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of EMNLP.

Luis Espinosa-Anke, Jose Camacho-Collados, Claudio
Delli Bovi, and Horacio Saggion. 2016. Supervised
distributional hypernym discovery via domain adap-
tation. In Proceedings of EMNLP. pages 424–435.

Sumit Bhagwani, Shrutiranjan Satapathy, and Harish
In Pro-
Karnick. 2013. Merging word senses.
ceedings of TextGraphs-8 Graph-based Methods for
Natural Language Processing. Seattle, Washington,
USA, pages 11–19.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
dings. In EMNLP.

Jos´e Camacho-Collados, Claudio Delli Bovi, Alessan-
dro Raganato, and Roberto Navigli. 2016a. A
of
Large-Scale Multilingual Disambiguation
In Proceedings of LREC. Portoroz,
Glosses.
Slovenia, pages 1701–1708.

Jose Camacho-Collados and Roberto Navigli. 2017.
BabelDomains: Large-Scale Domain Labeling of
Lexical Resources. In Proceedings of EACL (2). Va-
lencia, Spain.

Jos´e Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2016b. Nasari: Integrating ex-
plicit knowledge and corpus statistics for a multilin-
gual representation of concepts and entities. Artiﬁ-
cial Intelligence 240:36–64.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A uniﬁed model for word sense representation and
disambiguation. In Proceedings of EMNLP. Doha,
Qatar, pages 1025–1035.

Xiao Cheng and Dan Roth. 2013. Relational inference
for wikiﬁcation. In Proceedings of EMNLP. Seattle,
Washington, pages 1787–1796.

Billy Chiu, Anna Korhonen, and Sampo Pyysalo. 2016.
Intrinsic evaluation of word vectors fails to predict
extrinsic performance. In Proceedings of the Work-
shop on Evaluating Vector Space Representations
for NLP, ACL.

Franc¸ois Chollet. 2015. Keras.

https://github.com/

fchollet/keras.

Allyson Ettinger, Philip Resnik, and Marine Carpuat.
2016. Retroﬁtting sense-speciﬁc word vectors using
In Proceedings of NAACL-HLT. San
parallel text.
Diego, California, pages 1378–1383.

Christiane Fellbaum, editor. 1998. WordNet: An Elec-

tronic Database. MIT Press, Cambridge, MA.

Lucie Flekova and Iryna Gurevych. 2016. Supersense
embeddings: A uniﬁed model for supersense inter-
pretation, prediction, and utilization. In Proceedings
of ACL.

Derek Greene and P´adraig Cunningham. 2006. Practi-
cal solutions to the problem of diagonal dominance
in kernel document clustering. In Proceedings of the
23rd International conference on Machine learning.
ACM, pages 377–384.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-speciﬁc word embed-
In COL-
dings by exploiting bilingual resources.
ING. pages 497–507.

Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of the 11th International Conference
on World Wide Web. Hawaii, USA, pages 517–526.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
Improving neural networks by
dinov. 2012.
preventing co-adaptation of feature detectors. CoRR
abs/1207.0580.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen,
Martin Theobald, and Gerhard Weikum. 2012.
Kore: keyphrase overlap relatedness for entity dis-
ambiguation. In Proceedings of CIKM. pages 545–
554.

Eduard H. Hovy, Roberto Navigli, and Simone Paolo
semi-
Ponzetto. 2013.
structured content and Artiﬁcial Intelligence: The
story so far. Artiﬁcial Intelligence 194:2–27.

Collaboratively built

Xiao Ling, Sameer Singh, and Daniel S Weld. 2015.
Design challenges for entity linking. Transactions
of the Association for Computational Linguistics
3:315–328.

Eric H. Huang, Richard Socher, Christopher D. Man-
Improving word
ning, and Andrew Y. Ng. 2012.
representations via global context and multiple word
prototypes. In Proceedings of ACL. Jeju Island, Ko-
rea, pages 873–882.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of ACL-HLT. Portland, Oregon,
USA, pages 142–150.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning sense
embeddings for word and relational similarity.
In
Proceedings of ACL. Beijing, China, pages 95–105.

Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy.
2015. Ontologically grounded multi-sense repre-
sentation learning for semantic vector space models.
In Proceedings of NAACL. Denver, Colorado, pages
683–693.

Richard Johansson and Luis Nieto Pina. 2015. Embed-
ding a semantic network in a word space. In Pro-
ceedings of NAACL. Denver, Colorado, pages 1428–
1433.

Rie Johnson and Tong Zhang. 2015. Effective use
of word order for text categorization with convolu-
tional neural networks. In Proceedings of NAACL.
Denver, Colorado, pages 103–112.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL. Bal-
timore, USA, pages 655–665.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of EMNLP.
Doha, Qatar, pages 1746–1751.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proceedings of AAAI. Phoenix, Arizona,
pages 2741–2749.

Ken Lang. 1995. Newsweeder: Learning to ﬁlter net-
news. In Proceedings of the 12th International Con-
ference on Machine Learning. Tahoe City, Califor-
nia, pages 331–339.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
In Proceedings of EMNLP. Lisbon, Portugal, pages
683–693.

Antonio Lieto, Enrico Mensa, and Daniele P Radicioni.
2016. A resource-driven approach for anchoring lin-
In AI* IA
guistic resources to conceptual spaces.
2016 Advances in Artiﬁcial Intelligence, Springer,
pages 435–449.

Nut Limsopatham and Nigel Collier. 2016. Normalis-
ing medical concepts in social media texts by learn-
ing semantic representation. In Proceedings of ACL.
Berlin, Germany, pages 1014–1023.

Massimiliano Mancini,

Jose Camacho-Collados,
Ignacio Iacobacci, and Roberto Navigli. 2016.
via
Embedding words
CoRR
joint
abs/1612.02703. http://arxiv.org/abs/1612.02703.

knowledge-enhanced

training.

together

senses

and

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations.
pages 55–60.

Oren Melamud, Jacob Goldberger, and Ido Dagan.
context2vec: Learning generic context
2016.
In Proceed-
embedding with bidirectional lstm.
ings of The 20th SIGNLL Conference on Compu-
tational Natural Language Learning. Berlin, Ger-
many, pages 51–61.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient estimation of word represen-
tations in vector space. CoRR abs/1301.3781.

George A Miller. 1995.

a lexical
database for english. Communications of the ACM
38(11):39–41.

WordNet:

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Disam-
biguation: a Uniﬁed Approach. Transactions of the
Association for Computational Linguistics (TACL)
2:231–244.

Vinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning. pages 807–814.

Roberto Navigli. 2006. Meaningful clustering of
senses helps boost Word Sense Disambiguation per-
In Proceedings of COLING-ACL. Syd-
formance.
ney, Australia, pages 105–112.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artiﬁcial Intelligence 193:217–
250.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efﬁcient non-
parametric estimation of multiple embeddings per
In Proceedings of EMNLP.
word in vector space.
Doha, Qatar, pages 1059–1069.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
ACL. Barcelona, Spain, pages 51–61.

Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. Learning semantic textual similar-
In Proceedings
ity with structural representations.
of ACL (2). Soﬁa, Bulgaria, pages 714–718.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL.
Ann Arbor, Michigan, pages 115–124.

Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-
drew Y. Ng. 2007. Learning to merge word senses.
In Proceedings of EMNLP. Prague, Czech Republic,
pages 1005–1014.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
In Proceedings of EMNLP. pages
representation.
1532–1543.

Mohammad Taher Pilehvar and Nigel Collier. 2016.
De-conﬂated semantic representations. In Proceed-
ings of EMNLP. Austin, TX, pages 1680–1690.

Mohammad Taher Pilehvar and Roberto Navigli. 2014.
A large-scale pseudoword-based evaluation frame-
work for state-of-the-art Word Sense Disambigua-
tion. Computational Linguistics 40(4).

Lin Qiu, Kewei Tu, and Yong Yu. 2016. Context-
In Proceedings of

dependent sense embedding.
EMNLP. Austin, Texas, pages 183–191.

Alessandro Raganato, Jose Camacho-Collados, and
Roberto Navigli. 2017. Word sense disambiguation:
A uniﬁed evaluation framework and empirical com-
parison. In Proceedings of EACL. Valencia, Spain,
pages 99–110.

Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceedings of ACL. pages 109–117.

Sascha Rothe and Hinrich Sch¨utze. 2015. Autoex-
tend: Extending word embeddings to embeddings
In Proceedings of ACL.
for synsets and lexemes.
Beijing, China, pages 1793–1803.

Stefan R¨ud, Massimiliano Ciaramita, Jens M¨uller, and
Hinrich Sch¨utze. 2011. Piggyback: Using search
engines for robust cross-domain named entity recog-
nition. In Proceedings of ACL-HLT. Portland, Ore-
gon, USA, pages 965–975.

Ivan A Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
In Inter-
expressions: A pain in the neck for nlp.
national Conference on Intelligent Text Processing
and Computational Linguistics. Mexico City, Mex-
ico, pages 1–15.

Bahar Salehi, Paul Cook, and Timothy Baldwin. 2015.
A word embedding approach to predicting the com-
positionality of multiword expressions. In NAACL-
HTL. Denver, Colorado, pages 977–983.

Hinrich Sch¨utze. 1998. Automatic word sense discrim-
ination. Computational linguistics 24(1):97–123.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher Manning, Andrew Ng, and
Christopher Potts. 2013. Parsing with compositional
vector grammars. In Proceedings of EMNLP. Soﬁa,
Bulgaria, pages 455–465.

Simon ˇSuster, Ivan Titov, and Gertjan van Noord. 2016.
Bilingual learning of multi-sense embeddings with
In Proceedings of NAACL-
discrete autoencoders.
HLT. San Diego, California, pages 1346–1356.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
sentiment classiﬁcation. In Porceedings of EMNLP.
Lisbon, Portugal, pages 1422–1432.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints abs/1605.02688.

Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In COLING. pages 151–160.

Rocco Tripodi and Marcello Pelillo. 2017. A game-
theoretic approach to word sense disambiguation.
Computational Linguistics 43(1):31–70.

Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
laume Lample, and Chris Dyer. 2015. Evaluation
of word vector representations by subspace align-
ment. In Proceedings of EMNLP (2). Lisbon, Por-
tugal, pages 2049–2054.

Yulia Tsvetkov and Shuly Wintner. 2014.

Identiﬁca-
tion of multiword expressions by combining multi-
ple linguistic information sources. Computational
Linguistics 40(2):449–468.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural network
In Proceedings of ACL.
transition-based parsing.
Beijing, China, pages 323–333.

Yijun Xiao and Kyunghyun Cho. 2016.

Efﬁcient
character-level document classiﬁcation by com-
bining convolution and recurrent layers. CoRR
abs/1602.00367.

Shweta Yadav, Asif Ekbal, Sriparna Saha, and Pushpak
Bhattacharyya. 2017. Entity extraction in biomedi-
cal corpora: An approach to evaluate word embed-
ding features with pso based feature selection.
In
Proceedings of EACL. Valencia, Spain, pages 1159–
1170.

Yiming Yang. 1999. An evaluation of statistical ap-
Information re-

proaches to text categorization.
trieval 1(1-2):69–90.

Jay Young, Valerio Basile, Lars Kunze, Elena Cabrio,
and Nick Hawes. 2016. Towards lifelong object
learning by integrating situated robot perception and
In Proceedings of the Eu-
semantic web mining.
ropean Conference on Artiﬁcial Intelligence confer-
ence. The Hague, Netherland, pages 1458–1466.

Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage Word Sense Disambiguation sys-
tem for free text. In Proceedings of the ACL System
Demonstrations. Uppsala, Sweden, pages 78–83.

Will Y. Zou, Richard Socher, Daniel M. Cer, and
Christopher D. Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation.
In
Proceedings of EMNLP. Seattle, USA, pages 1393–
1398.

Towards a Seamless Integration of Word Senses
into Downstream NLP Applications

Mohammad Taher Pilehvar2, Jose Camacho-Collados1,
Roberto Navigli1 and Nigel Collier2

1Department of Computer Science, Sapienza University of Rome
2Department of Theoretical and Applied Linguistics, University of Cambridge
1{collados,navigli}@di.uniroma1.it
2{mp792,nhc30}@cam.ac.uk

Abstract

Lexical ambiguity can impede NLP sys-
tems from accurate understanding of se-
mantics. Despite its potential beneﬁts, the
integration of sense-level information into
NLP systems has remained understudied.
By incorporating a novel disambiguation
algorithm into a state-of-the-art classiﬁca-
tion model, we create a pipeline to inte-
grate sense-level information into down-
stream NLP applications. We show that
a simple disambiguation of the input text
can lead to consistent performance im-
provement on multiple topic categoriza-
tion and polarity detection datasets, par-
ticularly when the ﬁne granularity of the
underlying sense inventory is reduced and
the document is sufﬁciently large. Our re-
sults also point to the need for sense rep-
resentation research to focus more on in
vivo evaluations which target the perfor-
mance in downstream NLP applications
rather than artiﬁcial benchmarks.

1

Introduction

As a general trend, most current Natural Language
Processing (NLP) systems function at the word
level, i.e.
individual words constitute the most
ﬁne-grained meaning-bearing elements of their in-
put. The word level functionality can affect the
performance of these systems in two ways: (1)
it can hamper their efﬁciency in handling words
that are not encountered frequently during train-
ing, such as multiwords, inﬂections and deriva-
tions, and (2) it can restrict their semantic under-
standing to the level of words, with all their am-
biguities, and thereby prevent accurate capture of
the intended meanings.

The ﬁrst issue has recently been alleviated by

techniques that aim to boost the generalisation
power of NLP systems by resorting to sub-word
or character-level information (Ballesteros et al.,
2015; Kim et al., 2016). The second limitation,
however, has not yet been studied sufﬁciently. A
reasonable way to handle word ambiguity, and
hence to tackle the second issue, is to semantify
the input text: transform it from its surface-level
semantics to the deeper level of word senses, i.e.
their intended meanings. We take a step in this di-
rection by designing a pipeline that enables seam-
less integration of word senses into downstream
NLP applications, while beneﬁting from knowl-
edge extracted from semantic networks. To this
end, we propose a quick graph-based Word Sense
Disambiguation (WSD) algorithm which allows
high conﬁdence disambiguation of words without
much computation overload on the system. We
evaluate the pipeline in two downstream NLP ap-
plications: polarity detection and topic categoriza-
tion. Speciﬁcally, we use a classiﬁcation model
based on Convolutional Neural Networks which
has been shown to be very effective in various
text classiﬁcation tasks (Kalchbrenner et al., 2014;
Kim, 2014; Johnson and Zhang, 2015; Tang et al.,
2015; Xiao and Cho, 2016). We show that a simple
disambiguation of input can lead to performance
improvement of a state-of-the-art text classiﬁca-
tion system on multiple datasets, particularly for
long inputs and when the granularity of the sense
inventory is reduced. Our pipeline is quite ﬂexible
and modular, as it permits the integration of differ-
ent WSD and sense representation techniques.

2 Motivation

With the help of an example news article from the
BBC, shown in Figure 1, we highlight some of the
potential deﬁciencies of word-based models.

7
1
0
2
 
t
c
O
 
8
1
 
 
]
L
C
.
s
c
[
 
 
1
v
2
3
6
6
0
.
0
1
7
1
:
v
i
X
r
a

Algorithm 1 Disambiguation algorithm
Input: Input text T and semantic network N
Output: Set of disambiguated senses ˆS
1: Graph representation of T : (S, E) ← getGraph(T, N )
2: ˆS ← ∅
3: for each iteration i ∈ {1, ..., len(T )}
4:
5:
6:
7:
8:
9:
10:
11: return Disambiguation output ˆS

ˆs = argmaxs∈S |{(s, s(cid:48)) ∈ E : s(cid:48) ∈ S}|
maxDeg = |{(ˆs, s(cid:48)) ∈ E : s(cid:48) ∈ S}|
if maxDeg < θ|S| / 100 then

ˆS ← ˆS ∪ {ˆs}
E ← E \ {(s, s(cid:48)) : s ∨ s(cid:48) ∈ getLex(ˆs)}

break

else

towards resolving ambiguities, but it brings about
other advantages mentioned in the previous sec-
tion. The aim is to provide the system with an
input of reduced ambiguity which can facilitate its
decision making.

To this end, we developed a simple graph-based
joint disambiguation and entity linking algorithm
which can take any arbitrary semantic network
as input. The gist of our disambiguation tech-
nique lies in its speed and scalability. Conven-
tional knowledge-based disambiguation systems
(Hoffart et al., 2012; Agirre et al., 2014; Moro
et al., 2014; Ling et al., 2015; Pilehvar and Nav-
igli, 2014) often rely on computationally expen-
sive graph algorithms, which limits their applica-
tion to on-the-ﬂy processing of large number of
text documents, as is the case in our experiments.
Moreover, unlike supervised WSD and entity link-
ing techniques (Zhong and Ng, 2010; Cheng and
Roth, 2013; Melamud et al., 2016; Limsopatham
and Collier, 2016), our algorithm relies only on
semantic networks and does not require any sense-
annotated data, which is limited to English and al-
most non-existent for other languages.

Algorithm 1 shows our procedure for disam-
biguating an input document T . First, we retrieve
from our semantic network the list of candidate
senses1 for each content word, as well as seman-
tic relationships among them. As a result, we ob-
tain a graph representation (S, E) of the input text,
where S is the set of candidate senses and E is
the set of edges among different senses in S. The
graph is, in fact, a small sub-graph of the input se-
mantic network, N . Our algorithm then selects the
best candidates iteratively. In each iteration, the

1As deﬁned in the underlying sense inventory, up to tri-
grams. We used Stanford CoreNLP (Manning et al., 2014)
for tokenization, Part-of-Speech (PoS) tagging and lemmati-
zation.

Figure 1: Excerpt of a news article from the BBC.

Ambiguity. Language is inherently ambiguous.
For instance, Mercedes, race, Hamilton and For-
mula can refer to several different entities or mean-
ings. Current neural models have managed to
successfully represent complex semantic associ-
ations by effectively analyzing large amounts of
data. However, the word-level functionality of
these systems is still a barrier to the depth of their
natural language understanding. Our proposal is
particularly tailored towards addressing this issue.

Multiword expressions (MWE). MWE are lex-
ical units made up of two or more words which
are idiosyncratic in nature (Sag et al., 2002), e.g,
Lewis Hamilton, Nico Rosberg and Formula 1.
Most existing word-based models ignore the in-
terdependency between MWE’s subunits and treat
them as individual units. Handling MWE has
been a long-standing problem in NLP and has re-
cently received a considerable amount of interest
(Tsvetkov and Wintner, 2014; Salehi et al., 2015).
Our pipeline facilitates this goal.

Co-reference. Co-reference resolution of con-
cepts and entities is not explicitly tackled by our
approach. However, thanks to the fact that words
that refer to the same meaning in context, e.g., For-
mula 1-F1 or German Grand Prix-German GP-
Hockenheim, are all disambiguated to the same
concept, the co-reference issue is also partly ad-
dressed by our pipeline.

3 Disambiguation Algorithm

Our proposal relies on a seamless integration of
word senses in word-based systems. The goal is
to semantify the text prior to its being fed into the
system by transforming its individual units from
word surface form to the deeper level of word
senses. The semantiﬁcation step is mainly tailored

Figure 2: Simpliﬁed graph-based representation of
a sample sentence.

candidate sense that has the highest graph degree
maxDeg is chosen as the winning sense:

maxDeg = max
s∈S

|{(s, s(cid:48)) ∈ E : s(cid:48) ∈ S}|

(1)

After each iteration, when a candidate sense ˆs
is selected, all the possible candidate senses of the
corresponding word (i.e. getLex(ˆs)) are removed
from E (line 10 in the algorithm).

Figure 2 shows a simpliﬁed version of the graph
for a sample sentence. The algorithm would dis-
ambiguate the content words in this sentence as
follows. It ﬁrst associates Oasis with its rock band
sense, since its corresponding node has the high-
est degree, i.e. 3. On the basis of this, the desert
sense of Oasis and its link to the stone sense of
rock are removed from the graph. In the second it-
eration, rock band is disambiguated as music band
given that its degree is 2.2 Finally, Manchester is
associated with its city sense (with a degree of 1).
In order to enable disambiguating at differ-
ent conﬁdence levels, we introduce a threshold θ
which determines the stopping criterion of the al-
gorithm.
Iteration continues until the following
condition is fulﬁlled: maxDeg < θ|S| / 100. This
ensures that the system will only disambiguate
those words for which it has a high conﬁdence and
backs off to the word form otherwise, avoiding the
introduction of unwanted noise in the data for un-
certain cases or for word senses that are not de-
ﬁned in the inventory.

2For bigrams and trigrams whose individual words might
also be disambiguated (such as rock and band in rock band),
the longest unit has the highest priority (i.e. rock band).

Figure 3: Text classiﬁcation model architecture.

4 Classiﬁcation Model

In our experiments, we use a standard neural net-
work based classiﬁcation approach which is simi-
lar to the Convolution Neural Network classiﬁer of
Kim (2014) and the pioneering model of Collobert
et al. (2011). Figure 3 depicts the architecture of
the model. The network receives the concatenated
vector representations of the input words, v1:n =
v1⊕v2⊕· · ·⊕vn, and applies (convolves) ﬁlters F
on windows of h words, mi = f (F.vi:i+h−1 + b),
where b is a bias term and f () is a non-linear func-
tion, for which we use ReLU (Nair and Hinton,
2010). The convolution transforms the input text
to a feature map m = [m1, m2, . . . , mn−h+1].
A max pooling operation then selects the most
salient feature ˆm = max{m} for each ﬁlter.

In the network of Kim (2014), the pooled fea-
tures are directly passed to a fully connected soft-
max layer whose outputs are class probabilities.
However, we add a recurrent layer before soft-
max in order to enable better capturing of long-
distance dependencies. It has been shown by Xiao
and Cho (2016) that a recurrent layer can replace
multiple layers of convolution and be beneﬁcial,
particularly when the length of input text grows.
Speciﬁcally, we use a Long Short-Term Memory
(Hochreiter and Schmidhuber, 1997, LSTM) as
our recurrent layer which was originally proposed
to avoid the vanishing gradient problem and has
proven its abilities in capturing distant dependen-
cies. The LSTM unit computes three gate vectors

(forget, input, and output) as follows:

ft = σ(Wf gt + Uf ht−1 + bf ),
it = σ(Wi gt + Ui ht−1 + bi),
ot = σ(Wo gt + Uo ht−1 + bo),

(2)

where W, U, and b are model parameters and
g and h are input and output sequences, respec-
tively. The cell state vector ct is then computed as
ct = ft ct−1 + it tanh(˜ct) where ˜ct = Wc gt +
Uc ht−1. Finally, the output sequence is computed
as ht = ot tanh(ct). As for regularization, we
used dropout (Hinton et al., 2012) after the em-
bedding layer.

We perform experiments with two conﬁgura-
tions of the embedding layer: (1) Random, initial-
ized randomly and updated during training, and
(2) Pre-trained, initialized by pre-trained repre-
sentations and updated during training. In the fol-
lowing section we describe the pre-trained word
and sense representation used for the initialization
of the second conﬁguration.

4.1 Pre-trained Word and Sense Embeddings

One of the main advantages of neural models
is that they usually represent the input words as
dense vectors. This can signiﬁcantly boost a
system’s generalisation power and results in im-
proved performance (Zou et al., 2013; Bordes
et al., 2014; Kim, 2014; Weiss et al., 2015, inter-
alia). This feature also enables us to directly plug
in pre-trained sense representations and check
them in a downstream application.

In our experiments we generate a set of sense
embeddings by extending DeConf, a recent tech-
nique with state-of-the-art performance on multi-
ple semantic similarity benchmarks (Pilehvar and
Collier, 2016). We leave the evaluation of other
representations to future work. DeConf gets a
pre-trained set of word embeddings and computes
sense embeddings in the same semantic space. To
this end, the approach exploits the semantic net-
work of WordNet (Miller, 1995), using the Person-
alized PageRank (Haveliwala, 2002) algorithm,
and obtains a set of sense biasing words Bs for
a word sense s. The sense representation of s is
then obtained using the following formula:

ˆv(s) =

|Bs|
(cid:88)

1
|Bs|

−i

e

δ v(wi),

(3)

i=1
where δ is a decay parameter and v(wi) is the em-
the ith word in the sense bi-
bedding of wi, i.e.

asing list of s, i.e. Bs. We follow Pilehvar and
Collier (2016) and set δ = 5. Finally, the vector
for sense s is calculated as the average of ˆv(s) and
the embedding of its corresponding word.

Owing to its reliance on WordNet’s semantic
network, DeConf is limited to generating only
those word senses that are covered by this lexical
resource. We propose to use Wikipedia in order
to expand the vocabulary of the computed word
senses. Wikipedia provides a high coverage of
named entities and domain-speciﬁc terms in many
languages, while at the same time also beneﬁting
from a continuous update by collaborators. More-
over, it can easily be viewed as a sense inventory
where individual articles are word senses arranged
through hyperlinks and redirections.

Camacho-Collados et al.

(2016b) proposed
NASARI3, a technique to compute the most salient
words for each Wikipedia page. These salient
words were computed by exploiting the struc-
ture and content of Wikipedia and proved effec-
tive in tasks such as Word Sense Disambiguation
(Tripodi and Pelillo, 2017; Camacho-Collados
et al., 2016a), knowledge-base construction (Li-
eto et al., 2016), domain-adapted hypernym dis-
covery (Espinosa-Anke et al., 2016; Camacho-
Collados and Navigli, 2017) or object recogni-
tion (Young et al., 2016). We view these lists
as biasing words for individual Wikipedia pages,
and then leverage the exponential decay function
(Equation 3) to compute new sense embeddings
in the same semantic space.
In order to repre-
sent both WordNet and Wikipedia sense represen-
tations in the same space, we rely on the WordNet-
Wikipedia mapping provided by BabelNet4 (Nav-
igli and Ponzetto, 2012). For the WordNet synsets
which are mapped to Wikipedia pages in Babel-
Net, we average the corresponding Wikipedia-
based and WordNet-based sense embeddings.

4.2 Pre-trained Supersense Embeddings

It has been argued that WordNet sense distinctions
are too ﬁne-grained for many NLP applications
(Hovy et al., 2013). The issue can be tackled by
grouping together similar senses of the same word,
either using automatic clustering techniques (Nav-
igli, 2006; Agirre and Lopez, 2003; Snow et al.,
2007) or with the help of WordNet’s lexicographer

3We downloaded the salient words for Wikipedia pages
(NASARI English lexical vectors, version 3.0) from http://lcl.
uniroma1.it/nasari/

4We used the Java API from http://babelnet.org

ﬁles5. Various applications have been shown to
improve upon moving from senses to supersenses
(R¨ud et al., 2011; Severyn et al., 2013; Flekova
and Gurevych, 2016). In WordNet’s lexicographer
ﬁles there are a total of 44 sense clusters, referred
to as supersenses, for categories such as event, ani-
mal, and quantity. In our experiments we use these
supersenses in order to reduce granularity of our
WordNet and Wikipedia senses. To generate su-
persense embeddings, we simply average the em-
beddings of senses in the corresponding cluster.

5 Evaluation

We evaluated our model on two classiﬁcation
tasks: topic categorization (Section 5.2) and po-
larity detection (Section 5.3). In the following sec-
tion we present the common experimental setup.

5.1 Experimental setup

Classiﬁcation model. Throughout all the exper-
iments we used the classiﬁcation model described
in Section 4. The general architecture of the model
was the same for both tasks, with slight variations
in hyperparameters given the different natures of
the tasks, following the values suggested by Kim
(2014) and Xiao and Cho (2016) for the two tasks.
Hyperparameters were ﬁxed across all conﬁgura-
tions in the corresponding tasks. The embedding
layer was ﬁxed to 300 dimensions, irrespective of
the conﬁguration, i.e. Random and Pre-trained.
For both tasks the evaluation was carried out by
10-fold cross-validation unless standard training-
testing splits were available. The disambiguation
threshold θ (cf. Section 3) was tuned on the train-
ing portion of the corresponding data, over seven
values in [0,3] in steps of 0.5.6 We used Keras
(Chollet, 2015) and Theano (Team, 2016) for our
model implementations.

Semantic network. The integration of senses
was carried out as described in Section 3. For
disambiguating with both WordNet and Wikipedia
senses we relied on the joint semantic network of
Wikipedia hyperlinks and WordNet via the map-
ping provided by BabelNet.7

5https://wordnet.princeton.edu/man/lexnames.5WN.html
6We observed that values higher than 3 led to very few dis-
ambiguations. While the best results were generally achieved
in the [1.5,2.5] range, performance differences across thresh-
old values were not statistically signiﬁcant in most cases.

7For simplicity we refer to this joint sense inventory as

all

the

experiments we

Pre-trained word and sense
embeddings.
used
Throughout
Word2vec (Mikolov et al., 2013) embeddings,
trained on the Google News corpus.8 We trun-
cated this set to its 250K most frequent words.
We also used WordNet 3.0 (Fellbaum, 1998)
and the Wikipedia dump of November 2014 to
compute the sense embeddings (see Section 4.1).
As a result, we obtained a set of 757,262 sense
embeddings in the same space as the pre-trained
Word2vec word embeddings. We used DeConf
(Pilehvar and Collier, 2016) as our pre-trained
WordNet sense embeddings. All vectors had a
ﬁxed dimensionality of 300.

Supersenses.
In addition to WordNet senses, we
experimented with supersenses (see Section 4.2)
to check how reducing granularity would affect
system performance. For obtaining supersenses
in a given text we relied on our disambiguation
pipeline and simply clustered together senses be-
longing to the same WordNet supersense.

Evaluation measures. We report the results in
terms of standard accuracy and F1 measures.9

5.2 Topic Categorization

The task of topic categorization consists of assign-
ing a label (i.e. topic) to a given document from a
pre-deﬁned set of labels.

5.2.1 Datasets

For this task we used two newswire and one med-
ical topic categorization datasets. Table 1 sum-
marizes the statistics of each dataset.10 The BBC
news dataset11 (Greene and Cunningham, 2006)
comprises news articles taken from BBC, divided
into ﬁve topics: business, entertainment, politics,
sport and tech. Newsgroups (Lang, 1995) is a col-
lection of 11,314 documents for training and 7532
for testing12 divided into six topics: computing,
sport and motor vehicles, science, politics, reli-

8https://code.google.com/archive/p/word2vec/
9Since all models in our experiments provide full cover-
age, accuracy and F1 denote micro- and macro-averaged F1,
respectively (Yang, 1999).

10The coverage of the datasets was computed using the
250K top words in the Google News Word2vec embeddings.

11http://mlg.ucd.ie/datasets/bbc.html
12We used the train-test partition available at http://qwone.

Wikipedia, but note that WordNet senses are also covered.

com/∼jason/20Newsgroups/

Dataset

Domain No. of classes No. of docs Avg. doc. size Size of vocab. Coverage

Evaluation

BBC
News
Newsgroups News
Ohsumed

Medical

5
6
23

2,225
18,846
23,166

439.5
394.0
201.2

35,628
225,046
65,323

87.4%
83.4%
79.3%

10 cross valid.
Train-Test
Train-Test

Table 1: Statistics of the topic categorization datasets.

Initialization

Input type

BBC News
Acc
93.0

F1
92.8

Newsgroups
F1
Acc
85.6
87.7

Ohsumed

Random

Pre-trained

Word

Sense

WordNet
Wikipedia
Supersense WordNet
Wikipedia

Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

93.5
92.7
93.6
94.6∗

97.6
97.3

96.3
96.8
96.9

93.3
92.5
93.4
94.4

97.5
97.1

96.2
96.7
96.9

88.1
86.7
90.1∗
88.5

91.1
90.2
89.6†
89.6
88.6

86.9
84.9
89.0
85.8

90.6
88.6

88.9
88.9
87.4

Acc
30.1
27.2†
29.7
31.8∗
31.1

29.4
30.2

32.4
29.5
30.6∗

F1
20.7

18.3
20.9
22.0
21.3

20.1
20.4

22.3
19.9
20.3

Table 2: Classiﬁcation performance at the word, sense, and supersense levels with random and pre-
trained embedding initialization. We show in bold those settings that improve the word-based model.

gion and sales.13 Finally, Ohsumed14 is a col-
lection of medical abstracts from MEDLINE, an
online medical information database, categorized
according to 23 cardiovascular diseases. For our
experiments we used the partition split of 10,433
documents for training and 12,733 for testing.15

5.2.2 Results

Table 2 shows the results of our classiﬁcation
model and its variants on the three datasets.16
When the embedding layer is initialized randomly,
the model integrated with word senses consistently
improves over the word-based model, particularly
when the ﬁne-granularity of the underlying sense
inventory is reduced using supersenses (with sta-
tistically signiﬁcant gains on the three datasets).
This highlights the fact that a simple disambigua-
tion of the input can bring about performance gain
for a state-of-the-art classiﬁcation system. Also,

13The dataset has 20 ﬁne-grained categories clustered into
six general topics. We used the coarse-grained labels for their
clearer distinction and consistency with BBC topics.

14ftp://medir.ohsu.edu/pub/ohsumed
15http://disi.unitn.it/moschitti/corpora.htm
16Symbols ∗ and † indicate the sense-based model with the
smallest margin to the word-based model whose accuracy is
statistically signiﬁcant at 0.95 conﬁdence level according to
unpaired t-test (∗ for positive and † for negative change).

the better performance of supersenses suggests
that the sense distinctions of WordNet are too ﬁne-
grained for the topic categorization task. How-
ever, when pre-trained representations are used to
initialize the embedding layer, no improvement is
observed over the word-based model. This can
be attributed to the quality of the representations,
as the model utilizing them was unable to beneﬁt
from the advantage offered by sense distinctions.
Our results suggest that research in sense represen-
tation should put special emphasis on real-world
evaluations on benchmarks for downstream appli-
cations, rather than on artiﬁcial tasks such as word
similarity. In fact, research has previously shown
that word similarity might not constitute a reliable
proxy to measure the performance of word embed-
dings in downstream applications (Tsvetkov et al.,
2015; Chiu et al., 2016).

Among the three datasets, Ohsumed proves to
be the most challenging one, mainly for its larger
number of classes (i.e. 23) and its domain-speciﬁc
nature (i.e. medicine). Interestingly, unlike for the
other two datasets, the introduction of pre-trained
word embeddings to the system results in reduced
performance on Ohsumed. This suggests that gen-
eral domain embeddings might not be beneﬁcial

in specialized domains, which corroborates previ-
ous ﬁndings by Yadav et al. (2017) on a different
task, i.e. entity extraction. This performance drop
may also be due to diachronic issues (Ohsumed
the
dates back to the 1980s) and low coverage:
pre-trained Word2vec embeddings cover 79.3% of
the words in Ohsumed (see Table 1), in contrast
to the higher coverage on the newswire datasets,
i.e. Newsgroups (83.4%) and BBC (87.4%). How-
ever, also note that the best overall performance
is attained when our pre-trained Wikipedia sense
embeddings are used. This highlights the effec-
tiveness of Wikipedia in handling domain-speciﬁc
entities, thanks to its broad sense inventory.

5.3 Polarity Detection

Polarity detection is the most popular evaluation
framework for sentiment analysis (Dong et al.,
2015). The task is essentially a binary classiﬁca-
tion which determines if the sentiment of a given
sentence or document is negative or positive.

5.3.1 Datasets

For the polarity detection task we used ﬁve stan-
dard evaluation datasets. Table 1 summarizes
statistics. PL04 (Pang and Lee, 2004) is a polar-
ity detection dataset composed of full movie re-
views. PL0518 (Pang and Lee, 2005), instead, is
composed of short snippets from movie reviews.
RTC contains critic reviews from Rotten Toma-
toes19, divided into 436,000 training and 2,000
test instances. IMDB (Maas et al., 2011) includes
50,000 movie reviews, split evenly between train-
ing and test. Finally, we used the Stanford Sen-
timent dataset (Socher et al., 2013), which asso-
ciates each review with a value that denotes its
sentiment. To be consistent with the binary classi-
ﬁcation of the other datasets, we removed the neu-
tral phrases according to the dataset’s scale (be-
tween 0.4 and 0.6) and considered the reviews
whose values were below 0.4 as negative and
above 0.6 as positive. This resulted in a binary po-
larity dataset of 119,783 phrases. Unlike the previ-
ous four datasets, this dataset does not contain an
even distribution of positive and negative labels.

5.3.2 Results

Table 4 lists accuracy performance of our classi-
ﬁcation model and all its variants on ﬁve polar-

18Both PL04 and PL05 were downloaded from http://

www.cs.cornell.edu/people/pabo/movie-review-data/

19http://www.rottentomatoes.com

BBC

Ohsumed

PL04

IMDB

Newsgroups

n
i
a
g

y
c
a
r
u
c
c
A

1.5

0.5

1

0

−0.5

−1

−1.5

RTC
Stanford

PL05

0

100

200

300

400

500

600

700

800

Average document size

Figure 4: Relation between average document size
and performance improvement using Wikipedia
supersenses with random initialization.

ity detection datasets. Results are generally better
than those of Kim (2014), showing that the addi-
tion of the recurrent layer to the model (cf. Section
4) was beneﬁcial. However, interestingly, no con-
sistent performance gain is observed in the polar-
ity detection task, when the model is provided with
disambiguated input, particularly for datasets with
relatively short reviews. We attribute this to the
nature of the task. Firstly, given that words rarely
happen to be ambiguous with respect to their senti-
ment, the semantic sense distinctions provided by
the disambiguation stage do not assist the classiﬁer
in better decision making, and instead introduce
data sparsity. Secondly, since the datasets mostly
contain short texts, e.g., sentences or snippets, the
disambiguation algorithm does not have sufﬁcient
context to make high-conﬁdence judgements, re-
sulting in fewer disambiguations or less reliable
ones. In the following section we perform a more
in-depth analysis of the impact of document size
on the performance of our sense-based models.

5.4 Analysis

Document size. A detailed analysis revealed a
relation between document size (the number of
tokens) and performance gain of our sense-level
model. We show in Figure 4 how these two
vary for our most consistent conﬁguration, i.e.
Wikipedia supersenses, with random initialization.
Interestingly, as a general trend, the performance
gain increases with average document size, irre-

19Stanford is the only unbalanced dataset, but F1 results

were almost identical to accuracy.

Dataset

Type

No. of docs

Avg. doc. size

Vocabulary size

Coverage

Evaluation

RTC
IMDB
PL05
PL04
Stanford

Snippets
Reviews
Snippets
Reviews
Phrases

438,000
50,000
10,662
2,000
119,783

23.4
268.8
21.5
762.1
10.0

128,056
140,172
19,825
45,077
19,400

81.3%
82.5%
81.3%
82.4%
81.6%

Train-Test
Train-Test
10 cross valid.
10 cross valid.
10 cross valid.

Table 3: Statistics of the polarity detection datasets.

Initialization

Random

Pre-trained

Input type
Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

Word

Sense

WordNet

Wikipedia
Supersense WordNet
Wikipedia

RTC
83.6
83.2

83.1

84.4
83.1

85.5

83.4

83.8

85.2

84.2

IMDB
87.7
87.4

88.0

88.0
88.4∗

88.3

88.3
87.0†
88.8

87.9

PL05
77.3
76.6
75.9†
75.9
75.8

80.2

79.2

79.2

79.5
78.3†

PL04
67.9
67.4

67.1

66.2
69.3∗

72.5
69.7†
73.1

73.8

72.6

Stanford
91.8
91.3

91.0
91.4†
91.0

93.1

92.6

92.3
92.7†
92.2

Table 4: Accuracy performance on ﬁve polarity detection datasets. Given that polarity datasets are
balanced17, we do not report F1 which would have been identical to accuracy.

spective of the classiﬁcation task. We attribute this
to two main factors:

1. Sparsity: Splitting a word into multiple word
senses can have the negative side effect that
the corresponding training data for that word
is distributed among multiple independent
senses. This reduces the training instances
per word sense, which might affect the classi-
ﬁer’s performance, particularly when senses
are semantically related (in comparison to
ﬁne-grained senses, supersenses address this
issue to some extent).

2. Disambiguation quality: As also mentioned
previously, our disambiguation algorithm re-
quires the input text to be sufﬁciently large so
as to create a graph with an adequate num-
ber of coherent connections to function ef-
fectively. In fact, for topic categorization, in
which the documents are relatively long, our
algorithm manages to disambiguate a larger
proportion of words in documents with high
conﬁdence. The lower performance of graph-
based disambiguation algorithms on short

texts is a known issue (Moro et al., 2014; Ra-
ganato et al., 2017), the tackling of which re-
mains an area of exploration.

Senses granularity. Our results showed that re-
ducing ﬁne-granularity of sense distinctions can
irrespective of the
be beneﬁcial to both tasks,
i.e. WordNet or
underlying sense inventory,
Wikipedia, which corroborates previous ﬁndings
(Hovy et al., 2013; Flekova and Gurevych, 2016).
This suggests that text classiﬁcation does not re-
In this
quire ﬁne-grained semantic distinctions.
work we used a simple technique based on Word-
Net’s lexicographer ﬁles for coarsening senses in
this sense inventory as well as in Wikipedia. We
leave the exploration of this promising area as well
as the evaluation of other granularity reduction
techniques for WordNet (Snow et al., 2007; Bhag-
wani et al., 2013) and Wikipedia (Dandala et al.,
2013) sense inventories to future work.

6 Related Work

The past few years have witnessed a growing re-
search interest in semantic representation, mainly
as a consequence of the word embedding tsunami

(Mikolov et al., 2013; Pennington et al., 2014).
Soon after their introduction, word embeddings
were integrated into different NLP applications,
thanks to the migration of the ﬁeld to deep learning
and the fact that most deep learning models view
words as dense vectors. The waves of the word
embedding tsunami have also lapped on the shores
of sense representation. Several techniques have
been proposed that either extend word embedding
models to cluster contexts and induce senses, usu-
ally referred to as unsupervised sense represen-
tations (Sch¨utze, 1998; Reisinger and Mooney,
2010; Huang et al., 2012; Neelakantan et al., 2014;
Guo et al., 2014; Tian et al., 2014; ˇSuster et al.,
2016; Ettinger et al., 2016; Qiu et al., 2016) or
exploit external sense inventories and lexical re-
sources for generating sense representations for
individual meanings of words (Chen et al., 2014;
Johansson and Pina, 2015; Jauhar et al., 2015; Ia-
cobacci et al., 2015; Rothe and Sch¨utze, 2015;
Camacho-Collados et al., 2016b; Mancini et al.,
2016; Pilehvar and Collier, 2016).

However, the integration of sense representa-
tions into deep learning models has not been so
straightforward, and research in this ﬁeld has of-
ten opted for alternative evaluation benchmarks
such as WSD, or artiﬁcial tasks, such as word
similarity. Consequently, the problem of integrat-
ing sense representations into downstream NLP
applications has remained understudied, despite
the potential beneﬁts it can have. Li and Juraf-
sky (2015) proposed a “multi-sense embedding”
pipeline to check the beneﬁt that can be gained
by replacing word embeddings with sense embed-
dings in multiple tasks. With the help of two
simple disambiguation algorithms, unsupervised
sense embeddings were integrated into various
downstream applications, with varying degrees of
success. Given the interdependency of sense rep-
resentation and disambiguation in this model, it is
very difﬁcult to introduce alternative algorithms
into its pipeline, either to beneﬁt from the state
of the art, or to carry out an evaluation. Instead,
our pipeline provides the advantage of being mod-
thanks to its use of disambiguation in the
ular:
pre-processing stage and use of sense representa-
tions that are linked to external sense inventories,
different WSD techniques and sense representa-
tions can be easily plugged in and checked. Along
the same lines, Flekova and Gurevych (2016) pro-
posed a technique for learning supersense rep-

resentations, using automatically-annotated cor-
pora. Coupled with a supersense tagger, the rep-
resentations were fed into a neural network clas-
siﬁer as additional features to the word-based in-
put. Through a set of experiments, Flekova and
Gurevych (2016) showed that the supersense en-
richment can be beneﬁcial to a range of binary
classiﬁcation tasks. Our proposal is different in
that it focuses directly on the beneﬁts that can
be gained by semantifying the input,
re-
ducing lexical ambiguity in the input text, rather
than assisting the model with additional sources
of knowledge.

i.e.

7 Conclusion and Future Work

We proposed a pipeline for the integration of sense
level knowledge into a state-of-the-art text clas-
siﬁer. We showed that a simple disambiguation
of the input can lead to consistent performance
gain, particularly for longer documents and when
the granularity of the underlying sense inventory
is reduced. Our pipeline is modular and can be
used as an in vivo evaluation framework for WSD
and sense representation techniques. We release
our code and data to reproduce our experiments
(including pre-trained sense and supersense em-
beddings) at https://github.com/pilehvar/sensecnn
to allow further checking of the choice of hyperpa-
rameters and to allow further analysis and compar-
ison. We hope that our work will foster future re-
search on the integration of sense-level knowledge
into downstream applications. As future work, we
plan to investigate the extension of the approach
to other languages and applications. Also, given
the promising results observed for supersenses, we
will investigate task-speciﬁc coarsening of sense
inventories, particularly Wikipedia, or the use of
SentiWordNet (Baccianella et al., 2010), which
could be more suitable for polarity detection.

Acknowledgments

The authors gratefully acknowledge the sup-
port of the MRC grant No. MR/M025160/1
for PheneBank and ERC Consolidator Grant
MOUSSE No. 726487. Jose Camacho-Collados
is supported by a Google Doctoral Fellowship in
Natural Language Processing. Nigel Collier is
supported by EPSRC Grant No. EP/M005089/1.
We thank Jim McManus for his suggestions on the
manuscript and the anonymous reviewers for their
helpful comments.

References

Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics
40(1):57–84.

Eneko Agirre and Oier Lopez. 2003. Clustering Word-
In Proceedings of Recent Ad-
Net word senses.
vances in Natural Language Processing. Borovets,
Bulgaria, pages 121–130.

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC. volume 10, pages 2200–2204.

Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res. 12:2493–2537.

Bharath Dandala, Chris Hokamp, Rada Mihalcea, and
Razvan C. Bunescu. 2013. Sense clustering us-
ing Wikipedia. In Proceedings of Recent Advances
in Natural Language Processing. Hissar, Bulgaria,
pages 164–171.

Li Dong, Furu Wei, Shujie Liu, Ming Zhou, and
Ke Xu. 2015. A statistical parsing framework for
sentiment classiﬁcation. Computational Linguistics
41(2):293–336.

Miguel Ballesteros, Chris Dyer, and Noah A Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Pro-
ceedings of EMNLP.

Luis Espinosa-Anke, Jose Camacho-Collados, Claudio
Delli Bovi, and Horacio Saggion. 2016. Supervised
distributional hypernym discovery via domain adap-
tation. In Proceedings of EMNLP. pages 424–435.

Sumit Bhagwani, Shrutiranjan Satapathy, and Harish
In Pro-
Karnick. 2013. Merging word senses.
ceedings of TextGraphs-8 Graph-based Methods for
Natural Language Processing. Seattle, Washington,
USA, pages 11–19.

Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
dings. In EMNLP.

Jos´e Camacho-Collados, Claudio Delli Bovi, Alessan-
dro Raganato, and Roberto Navigli. 2016a. A
of
Large-Scale Multilingual Disambiguation
In Proceedings of LREC. Portoroz,
Glosses.
Slovenia, pages 1701–1708.

Jose Camacho-Collados and Roberto Navigli. 2017.
BabelDomains: Large-Scale Domain Labeling of
Lexical Resources. In Proceedings of EACL (2). Va-
lencia, Spain.

Jos´e Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2016b. Nasari: Integrating ex-
plicit knowledge and corpus statistics for a multilin-
gual representation of concepts and entities. Artiﬁ-
cial Intelligence 240:36–64.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A uniﬁed model for word sense representation and
disambiguation. In Proceedings of EMNLP. Doha,
Qatar, pages 1025–1035.

Xiao Cheng and Dan Roth. 2013. Relational inference
for wikiﬁcation. In Proceedings of EMNLP. Seattle,
Washington, pages 1787–1796.

Billy Chiu, Anna Korhonen, and Sampo Pyysalo. 2016.
Intrinsic evaluation of word vectors fails to predict
extrinsic performance. In Proceedings of the Work-
shop on Evaluating Vector Space Representations
for NLP, ACL.

Franc¸ois Chollet. 2015. Keras.

https://github.com/

fchollet/keras.

Allyson Ettinger, Philip Resnik, and Marine Carpuat.
2016. Retroﬁtting sense-speciﬁc word vectors using
In Proceedings of NAACL-HLT. San
parallel text.
Diego, California, pages 1378–1383.

Christiane Fellbaum, editor. 1998. WordNet: An Elec-

tronic Database. MIT Press, Cambridge, MA.

Lucie Flekova and Iryna Gurevych. 2016. Supersense
embeddings: A uniﬁed model for supersense inter-
pretation, prediction, and utilization. In Proceedings
of ACL.

Derek Greene and P´adraig Cunningham. 2006. Practi-
cal solutions to the problem of diagonal dominance
in kernel document clustering. In Proceedings of the
23rd International conference on Machine learning.
ACM, pages 377–384.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-speciﬁc word embed-
In COL-
dings by exploiting bilingual resources.
ING. pages 497–507.

Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
In Proceedings of the 11th International Conference
on World Wide Web. Hawaii, USA, pages 517–526.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
Improving neural networks by
dinov. 2012.
preventing co-adaptation of feature detectors. CoRR
abs/1207.0580.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation

Long short-term memory.
9(8):1735–1780.

Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen,
Martin Theobald, and Gerhard Weikum. 2012.
Kore: keyphrase overlap relatedness for entity dis-
ambiguation. In Proceedings of CIKM. pages 545–
554.

Eduard H. Hovy, Roberto Navigli, and Simone Paolo
semi-
Ponzetto. 2013.
structured content and Artiﬁcial Intelligence: The
story so far. Artiﬁcial Intelligence 194:2–27.

Collaboratively built

Xiao Ling, Sameer Singh, and Daniel S Weld. 2015.
Design challenges for entity linking. Transactions
of the Association for Computational Linguistics
3:315–328.

Eric H. Huang, Richard Socher, Christopher D. Man-
Improving word
ning, and Andrew Y. Ng. 2012.
representations via global context and multiple word
prototypes. In Proceedings of ACL. Jeju Island, Ko-
rea, pages 873–882.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of ACL-HLT. Portland, Oregon,
USA, pages 142–150.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning sense
embeddings for word and relational similarity.
In
Proceedings of ACL. Beijing, China, pages 95–105.

Sujay Kumar Jauhar, Chris Dyer, and Eduard Hovy.
2015. Ontologically grounded multi-sense repre-
sentation learning for semantic vector space models.
In Proceedings of NAACL. Denver, Colorado, pages
683–693.

Richard Johansson and Luis Nieto Pina. 2015. Embed-
ding a semantic network in a word space. In Pro-
ceedings of NAACL. Denver, Colorado, pages 1428–
1433.

Rie Johnson and Tong Zhang. 2015. Effective use
of word order for text categorization with convolu-
tional neural networks. In Proceedings of NAACL.
Denver, Colorado, pages 103–112.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL. Bal-
timore, USA, pages 655–665.

Yoon Kim. 2014. Convolutional neural networks for
sentence classiﬁcation. In Proceedings of EMNLP.
Doha, Qatar, pages 1746–1751.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proceedings of AAAI. Phoenix, Arizona,
pages 2741–2749.

Ken Lang. 1995. Newsweeder: Learning to ﬁlter net-
news. In Proceedings of the 12th International Con-
ference on Machine Learning. Tahoe City, Califor-
nia, pages 331–339.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
In Proceedings of EMNLP. Lisbon, Portugal, pages
683–693.

Antonio Lieto, Enrico Mensa, and Daniele P Radicioni.
2016. A resource-driven approach for anchoring lin-
In AI* IA
guistic resources to conceptual spaces.
2016 Advances in Artiﬁcial Intelligence, Springer,
pages 435–449.

Nut Limsopatham and Nigel Collier. 2016. Normalis-
ing medical concepts in social media texts by learn-
ing semantic representation. In Proceedings of ACL.
Berlin, Germany, pages 1014–1023.

Massimiliano Mancini,

Jose Camacho-Collados,
Ignacio Iacobacci, and Roberto Navigli. 2016.
via
Embedding words
CoRR
joint
abs/1612.02703. http://arxiv.org/abs/1612.02703.

knowledge-enhanced

training.

together

senses

and

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations.
pages 55–60.

Oren Melamud, Jacob Goldberger, and Ido Dagan.
context2vec: Learning generic context
2016.
In Proceed-
embedding with bidirectional lstm.
ings of The 20th SIGNLL Conference on Compu-
tational Natural Language Learning. Berlin, Ger-
many, pages 51–61.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efﬁcient estimation of word represen-
tations in vector space. CoRR abs/1301.3781.

George A Miller. 1995.

a lexical
database for english. Communications of the ACM
38(11):39–41.

WordNet:

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Disam-
biguation: a Uniﬁed Approach. Transactions of the
Association for Computational Linguistics (TACL)
2:231–244.

Vinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning. pages 807–814.

Roberto Navigli. 2006. Meaningful clustering of
senses helps boost Word Sense Disambiguation per-
In Proceedings of COLING-ACL. Syd-
formance.
ney, Australia, pages 105–112.

Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artiﬁcial Intelligence 193:217–
250.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efﬁcient non-
parametric estimation of multiple embeddings per
In Proceedings of EMNLP.
word in vector space.
Doha, Qatar, pages 1059–1069.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
ACL. Barcelona, Spain, pages 51–61.

Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. Learning semantic textual similar-
In Proceedings
ity with structural representations.
of ACL (2). Soﬁa, Bulgaria, pages 714–718.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL.
Ann Arbor, Michigan, pages 115–124.

Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-
drew Y. Ng. 2007. Learning to merge word senses.
In Proceedings of EMNLP. Prague, Czech Republic,
pages 1005–1014.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
In Proceedings of EMNLP. pages
representation.
1532–1543.

Mohammad Taher Pilehvar and Nigel Collier. 2016.
De-conﬂated semantic representations. In Proceed-
ings of EMNLP. Austin, TX, pages 1680–1690.

Mohammad Taher Pilehvar and Roberto Navigli. 2014.
A large-scale pseudoword-based evaluation frame-
work for state-of-the-art Word Sense Disambigua-
tion. Computational Linguistics 40(4).

Lin Qiu, Kewei Tu, and Yong Yu. 2016. Context-
In Proceedings of

dependent sense embedding.
EMNLP. Austin, Texas, pages 183–191.

Alessandro Raganato, Jose Camacho-Collados, and
Roberto Navigli. 2017. Word sense disambiguation:
A uniﬁed evaluation framework and empirical com-
parison. In Proceedings of EACL. Valencia, Spain,
pages 99–110.

Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceedings of ACL. pages 109–117.

Sascha Rothe and Hinrich Sch¨utze. 2015. Autoex-
tend: Extending word embeddings to embeddings
In Proceedings of ACL.
for synsets and lexemes.
Beijing, China, pages 1793–1803.

Stefan R¨ud, Massimiliano Ciaramita, Jens M¨uller, and
Hinrich Sch¨utze. 2011. Piggyback: Using search
engines for robust cross-domain named entity recog-
nition. In Proceedings of ACL-HLT. Portland, Ore-
gon, USA, pages 965–975.

Ivan A Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
In Inter-
expressions: A pain in the neck for nlp.
national Conference on Intelligent Text Processing
and Computational Linguistics. Mexico City, Mex-
ico, pages 1–15.

Bahar Salehi, Paul Cook, and Timothy Baldwin. 2015.
A word embedding approach to predicting the com-
positionality of multiword expressions. In NAACL-
HTL. Denver, Colorado, pages 977–983.

Hinrich Sch¨utze. 1998. Automatic word sense discrim-
ination. Computational linguistics 24(1):97–123.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher Manning, Andrew Ng, and
Christopher Potts. 2013. Parsing with compositional
vector grammars. In Proceedings of EMNLP. Soﬁa,
Bulgaria, pages 455–465.

Simon ˇSuster, Ivan Titov, and Gertjan van Noord. 2016.
Bilingual learning of multi-sense embeddings with
In Proceedings of NAACL-
discrete autoencoders.
HLT. San Diego, California, pages 1346–1356.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
sentiment classiﬁcation. In Porceedings of EMNLP.
Lisbon, Portugal, pages 1422–1432.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints abs/1605.02688.

Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In COLING. pages 151–160.

Rocco Tripodi and Marcello Pelillo. 2017. A game-
theoretic approach to word sense disambiguation.
Computational Linguistics 43(1):31–70.

Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
laume Lample, and Chris Dyer. 2015. Evaluation
of word vector representations by subspace align-
ment. In Proceedings of EMNLP (2). Lisbon, Por-
tugal, pages 2049–2054.

Yulia Tsvetkov and Shuly Wintner. 2014.

Identiﬁca-
tion of multiword expressions by combining multi-
ple linguistic information sources. Computational
Linguistics 40(2):449–468.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural network
In Proceedings of ACL.
transition-based parsing.
Beijing, China, pages 323–333.

Yijun Xiao and Kyunghyun Cho. 2016.

Efﬁcient
character-level document classiﬁcation by com-
bining convolution and recurrent layers. CoRR
abs/1602.00367.

Shweta Yadav, Asif Ekbal, Sriparna Saha, and Pushpak
Bhattacharyya. 2017. Entity extraction in biomedi-
cal corpora: An approach to evaluate word embed-
ding features with pso based feature selection.
In
Proceedings of EACL. Valencia, Spain, pages 1159–
1170.

Yiming Yang. 1999. An evaluation of statistical ap-
Information re-

proaches to text categorization.
trieval 1(1-2):69–90.

Jay Young, Valerio Basile, Lars Kunze, Elena Cabrio,
and Nick Hawes. 2016. Towards lifelong object
learning by integrating situated robot perception and
In Proceedings of the Eu-
semantic web mining.
ropean Conference on Artiﬁcial Intelligence confer-
ence. The Hague, Netherland, pages 1458–1466.

Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage Word Sense Disambiguation sys-
tem for free text. In Proceedings of the ACL System
Demonstrations. Uppsala, Sweden, pages 78–83.

Will Y. Zou, Richard Socher, Daniel M. Cer, and
Christopher D. Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation.
In
Proceedings of EMNLP. Seattle, USA, pages 1393–
1398.

