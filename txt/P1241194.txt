Table-to-text Generation by Structure-aware Seq2seq Learning

Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang and Zhifang Sui
Key Laboratory of Computational Linguistics, Ministry of Education,
School of Electronics Engineering and Computer Science, Peking University, Beijing, China
{tianyu0421, wkx, shalei, chbb, szf}@pku.edu.cn

7
1
0
2
 
v
o
N
 
7
2
 
 
]
L
C
.
s
c
[
 
 
1
v
4
2
7
9
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Table-to-text generation aims to generate a description for
a factual table which can be viewed as a set of ﬁeld-value
records. To encode both the content and the structure of a
table, we propose a novel structure-aware seq2seq architec-
ture which consists of ﬁeld-gating encoder and description
generator with dual attention. In the encoding phase, we up-
date the cell memory of the LSTM unit by a ﬁeld gate and
its corresponding ﬁeld value in order to incorporate ﬁeld in-
formation into table representation. In the decoding phase,
dual attention mechanism which contains word level atten-
tion and ﬁeld level attention is proposed to model the seman-
tic relevance between the generated description and the ta-
ble. We conduct experiments on the WIKIBIO dataset which
contains over 700k biographies and corresponding infoboxes
from Wikipedia. The attention visualizations and case stud-
ies show that our model is capable of generating coherent
and informative descriptions based on the comprehensive un-
derstanding of both the content and the structure of a table.
Automatic evaluations also show our model outperforms the
baselines by a great margin. Code for this work is available
on https://github.com/tyliupku/wiki2bio.

Introduction
Generating natural language description for a structured ta-
ble is an important task for text generation from struc-
tured data. Previous researches include weather forecast
based on a set of weather records (Liang, Jordan, and Klein
2009) and sportscasting based on temporally ordered events
(Chen and Mooney 2008). However, previous work models
the structured data in the limited pre-deﬁned schemas. For
example, a weather record rainChance(time:06:00-21:00,
mode:SSE, value:20) is represented by a ﬁxed-length one-
hot vector by its record type, record time, record value
and record value. To this end, we focus on table-to-text
generation which involves comprehensive representation for
the complex structure of a table rather than pre-deﬁned
schemas. In contrast to previous work experimented on
small datasets which contain only a few tens of thousands
of records such as WEATHERGOV (Liang, Jordan, and Klein
2009) and ROBOCUP (Chen and Mooney 2008), we fo-
cus on a more challenging task to generate biographies

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: The Wikipedia infobox of Charles Winstead, the
corresponding introduction on his wiki page reads “Charles
Winstead (1891 - 1973) was an FBI agent in the 1930s - 40s,
famous for being one of the agents who shot and killed John
Dillinger.”.

based on the Wikipedia infoboxes. As shown in Fig 1, a
biographic infobox is a ﬁxed-format table that describes a
person with many ﬁeld-value records like (Name, Charles
B. Winstead), (Nationality, American), (Occupation, FBI
Agent), etc. We utilize WIKIBIO dataset proposed by Le-
bret, Grangier, and Auli (2016) which contains 700k biogra-
phies from Wikipedia, with 400k words in total as the bench-
mark dataset.

Previous work has made signiﬁcant progress on this
task. Lebret, Grangier, and Auli (2016) proposed a statis-
tical n-gram model with local and global conditioning on a
Wikipedia infobox. However the ﬁeld content of a record
is likely to be a sequence of words, the statistical language
model is not good at capturing long-range dependencies be-
tween words. Mei, Bansal, and Walter (2015) proposed a
selective generation method based on an encoder-aligner-
decoder framework. The model utilizes a sparse one-hot vec-
tor to represent a weather record. However it’s inefﬁcient to
represent the complex structure of a table by one-hot vec-
tors.

We propose a structure-aware sequence to sequence
(seq2seq) generation framework to model both content and
structure of the table by local and global addressing. When a
human writes a biography for a person based on the related

Wikipedia infobox, he will ﬁrstly determine which records
in the table should be included in the introduction and how
to arrange the order of these records before wording. After
that, the writer will further consider which words or phrases
in the table should be more focused on to paraphrase. We
summarize the two phases of generation as two scopes of
addressing: local and global addressing. Local addressing
determines which particular word in the table should be fo-
cused on while generating a piece of description at certain
time step. However, the word level addressing can not fully
address the table-to-text generation problem as the factual
tables usually have complex structures which might confuse
the generator. Global addressing is proposed to determine
which records of the table should be more focused on while
generating corresponding description. Global addressing is
necessary as the description of a table may not cover all the
records. For example, the ‘cause of death’ ﬁeld in Fig 1 is
not mentioned in the description. Furthermore, the order of
records in the tables may not always be homogeneous. For
example, we can introduce a person as an order of his (Birth-
Death-Nationality-Occupation) according to his Wikipedia
infobox. However the other infoboxes may be arranged as
(Occupation-Nationality-Birth-Death). Local addressing is
realized by content encoding of the LSTM encoder and word
level attention while global addressing is realized by ﬁeld
encoding of the ﬁeld-gating LSTM variation and ﬁeld level
attention in our model.

The structure-aware seq2seq architecture we proposed
exploits encoder-decoder framework using long short-term
memory (LSTM) (Hochreiter and Schmidhuber 1997) units
with local and global addressing on the structured table. In
the encoding phase, our model ﬁrst encodes the sets of ﬁeld-
value records in the table by integrating ﬁeld information
and content representation. To make better use of ﬁeld in-
formation, we add a ﬁeld gate to the cell state of the encoder
LSTM unit to incorporate the ﬁeld embedding into the struc-
tural representation of the table. The model next employs
a LSTM decoder to generate natural language description
by the structural representation of the table. In the decoding
phase, we also propose a novel dual attention mechanism
which consists of two parts: word-level attention for local
addressing and ﬁeld-level attention for global addressing.

Our contributions are three-fold: (1) We propose an end-
to-end structure-aware encoder-decoder architecture to en-
code ﬁeld information into the representation of a structured
table. (2) Field-gating encoder and dual attention mecha-
nism are proposed to operate local and global addressing
between the content and the ﬁeld information of a structured
table. (3) Experiments on WIKIBIO dataset show that our
model achieves substantial improvement over baselines.

Related Work
Most generation systems can be divided into two indepen-
dent modules: (1)content selection involves choosing a sub-
set of relevant records in a table to talk about. (2)surface re-
alization is concerned with generating natural language de-
scriptions for this subset.

Many approaches have been proposed to learn the indi-
vidual modules. For content selection module, one approach

builds a content selection model by aligning records and
sentences (Barzilay and Lapata 2005; Duboue and McKe-
own 2002). A hierarchical semi-Markov method is proposed
by (Liang, Jordan, and Klein 2009) which ﬁrst associates
the text sequences to corresponding records and then gener-
ates corresponding descriptions from these records. Surface
realization is often treated as a concept-to-text generation
task from a given representation. Reiter and Dale (2000),
Walker, Rambow, and Rogati (2001) and Stent, Prasad, and
Walker (2001) utilize various linguistic features to train sen-
tence planners for sentence generation. Context-free gram-
mars are also used to generate natural language sentences
from formal meaning representations (Lu and Ng 2011; Belz
2008). Other effective approaches include hybrid alignment
tree (Kim and Mooney 2010), tree conditional random ﬁelds
(Lu, Ng, and Lee 2009), tree adjoining grammar (Gyawali
2016) and template extraction in a log-linear framework
(Angeli, Liang, and Klein 2010). Recent work combines
content selection and surface realization in a uniﬁed frame-
work (Ratnaparkhi 2002; Konstas and Lapata 2012; 2013;
Sha et al. 2017)

Our model borrowed the idea of representing a struc-
tured table by its ﬁeld and content information from (Le-
bret, Grangier, and Auli 2016). However, their n-gram model
is inefﬁcient to model long-range dependencies while gen-
erating descriptions. Mei, Bansal, and Walter (2015) also
proposed a seq2seq model with an aligner between weather
records and weather broadcast. The model used one-hot en-
coding to represent the weather records as they are relatively
simple and highly structured. However, the model is not
capable to represent the tables with complex structure like
Wikipedia infoboxes.

Task Deﬁnition
We model the table-to-text generation in an end-to-end
structure-aware seq2seq framework. The given table T can
be viewed as a combination of n ﬁeld-value records {
R1, R2, · · · , Rn}. Each record Ri consists of a sequence of
words { d1, d2, · · · , dm} and their corresponding ﬁeld rep-
resent { Zd1 , Zd2, · · · , Zdm}.

The output of the model is the generated description S
for table T which contains p tokens {w1, w2, · · · , wp} with
wt being the word at time t. We formulate the table-to-text
generation as the inference over a probabilistic model. The
goal of the inference is to generate a sequence w∗
1:p which
maximizes P (w1:p|R1:n).

w∗

1:p = arg max
w1:p

p
(cid:89)

t=1

P (wt|w0:t−1, R1:n)

(1)

Structure-aware Seq2seq

Field representation
A Wikipedia infobox can be viewed as a set of ﬁeld-value
records, in which values are sequences or segments of words
corresponding to certain ﬁelds. The structural representation
of an infobox consists of context embedding and ﬁeld em-
bedding. The context embedding is formulated as an em-

ht = ot (cid:12) tanh(ct)
(5)
where it, ft, ot ∈ [0, 1]n are input, forget and output gates
respectively, and ˆct and ct are proposed cell value and true
cell value in time t. n is the hidden size.

To make better understanding of the structure of a table,
the ﬁeld information should also be encoded into the en-
coder. One simple way is to take the concatenation of word
embedding and corresponding ﬁeld embedding as the input
for the vanilla LSTM unit. Actually, the method is indeed
proved to be useful in our experiments and serves as a base-
line for comparison. However, the concatenation of word
embedding and ﬁeld embedding only treats the ﬁeld infor-
mation as an additional label of certain token which loses
the structural information of the table.

To better encode the structural information of a table, we
propose a ﬁeld-gating variation on the vanilla LSTM unit to
update the cell memory by a ﬁeld gate and its corresponding
ﬁeld value. The ﬁeld-gating cell state is described as follows:

(cid:19)

(cid:18)lt
ˆzt

(cid:19)

(cid:18)sigmoid
tanh

=

W d

2n,2n (zt)

(6)

(cid:48)

c

t = ft (cid:12) ct−1 + it (cid:12) ˆct + lt (cid:12) ˆzt
where zt is the ﬁeld embedding described before, lt ∈
[0, 1]n is the ﬁeld gate to determine how much ﬁeld infor-
mation should be kept in the cell memory, ˆzt is the proposed
ﬁeld value corresponding to ﬁeld gate. The cell state c
t is up-
dated from the original ct by incorporating ﬁeld information
of the table.

(7)

(cid:48)

Description Decoder with Dual Attention
To conduct local and global addressing towards the struc-
tured table, we use LSTM architecture with dual attention
mechanism as our description generator. As deﬁned in the
equation 1, the generated token wt at time t in the decoder
is predicated based on all the previously generated tokens
w<t before wt, the hidden states H = {ht}L
t=1 of the table
encoder and the ﬁeld embeddings Z = {zt}L
t=1. To be more
speciﬁc:

P (wt|H, Z, w<t) = sof tmax(Ws (cid:12) gt)

gt = tanh(Wt[st, at])

st = LST M (wt−1, st−1)

(8)

(9)

(10)

where st is the t-th hidden state of the decoder calculated
by the LSTM unit. The computational details can be referred
in Equation 3, 4 and 5. at is the attention vector which is
widely used in many applications (Xu et al. 2015; Luong
et al. 2014; Ma et al. 2017). Vanilla attention mechanism
is proposed to encode the semantic relevance between the
encoder states {ht}L
t=1 and and the decoder state st at time
t. The attention vector is usually represented by the weighted
sum of encoder hidden states.

αti =

eg(st,hi)
j=1 eg(st,hj )

(cid:80)N

L
(cid:88)

i=1

; at =

αtihi

(11)

Figure 2: The wiki infobox of George Mikell (left) and the
table of its ﬁeld representation (right).

bedding for a segment of words in the ﬁeld content. The
ﬁeld embedding is a key point to label each word in the ﬁeld
content by its corresponding ﬁeld name and occurrence in
the table. Lebret, Grangier, and Auli (2016) represented the
ﬁeld embeddding Zw = {fw; pw} for a word w in the table
with corresponding ﬁeld name fw and position information
pw. The position information can be further represented as a
tuple (p+
w) which includes the positions of the token w
counted from the begining and the end of the ﬁeld respec-
tively. So the ﬁeld embedding of token w is extended to a
triple:

w, p−

Zw = {fw; p+

w; p−
w}

(2)

As shown in Fig 2, the infobox of George Mikell contains
several ﬁeld-value records, the ﬁeld content for the record
(birthname, Jurgis Mikelatitis) is ‘Jurgis Mikelatitis’. The
word ‘Jurgis’ is the ﬁrst token counted from the beginning
of the ﬁeld ‘birthname’ and also the second token counted
from the end. So the ﬁeld embedding for the word ‘Jurgis’
is described as {birthname; 1; 2}. Each token in the table
has an unique ﬁeld embedding even if there exists two same
words in the same ﬁeld due to the unique (ﬁeld, position)
pair.

Field-gating Table Encoder
The table encoder aims to encode each word dj in the table
together with its ﬁeld embedding Zdj into the hidden state
hj using LSTM encoder. We present a novel ﬁeld-gating
LSTM unit to incorporate ﬁeld information into table encod-
ing. LSTM is a recurrent neural network (RNN) architecture
which uses a vector of cell state ct and a set of element-wise
multiplication gates to control how information is stored,
forgotten and exploited inside the network. Following the
design for an LSTM cell in (Graves, Mohamed, and Hinton
2013) , the architecture used in the table encoder is deﬁned
by following equations:






it
ft
ot
ˆct






 =





 W c


sigmoid
sigmoid
sigmoid
tanh

(cid:19)

(cid:18) dt
ht−1

4n,2n

ct = ft (cid:12) ct−1 + it (cid:12) ˆct

(3)

(4)

Figure 3: The overall diagram of structure-aware seq2seq architecture for generating description for George Mikell in Fig 2.

where g(st, hi) is a relevant score between decoder hidden
state st and encoder hidden state hi. There are many differ-
ent ways to calculate the relevant scores. In our paper, we
use the following dot product to measure the similarity be-
tween st and hi. Ws, Wt, Wp, Wq are all model parameters.
g(st, hi) = tanh(Wphi) (cid:12) tanh(Wqst)
(12)
However, the word level attention described above can
only capture the semantic relevance between generated to-
kens and the content information in the table, ignoring the
structure information of the table. To fully utilize the struc-
ture information, we propose a higher level attention over
generated tokens and the ﬁeld embedding of the table. Field
level attention can locate the particular ﬁeld-value record
which should be focused on while generating next token in
the description by modeling the relevance between all ﬁeld
embeddings {zt}L
t=1 and the decoder state st at t-th time.
Field level attention weight βti is presented as Equation 13.
We use the same relevant score function g(st, zi) as equation
12. Dual attention weight γt is the element-wise production
between ﬁeld level attention weight βt and word level atten-
tion weight αt. The dual attention vector a
t is updated as
the weighted sum of encoder states {ht}t=1 by γt (Equation
15):

(cid:48)

βti =

eg(st,zi)
j=1 eg(st,zj )

(cid:80)N

g(st, zi) = tanh(Wxzi) (cid:12) tanh(Wyst)

γti =

αti · βti
j=1 αtj · βtj

(cid:80)N

(cid:48)
; a

t =

γtihi

L
(cid:88)

i=1

(13)

(14)

(15)

Furthermore, we utilize a post-process operation for the
generated unknown (UNK) tokens to alleviate the out-of-
vocabulary (OOV) problem. We replace a speciﬁc generated
UNK token with the most relevant token in the correspond-
ing table according to the related dual attention matrix.

Local and Global Addressing

Local and global addressing determine which part of the ta-
ble should be more focused on in different steps of descrip-
tion generation. The two scopes of addressings play a very
important role in understanding and representing the inner-
structure of a table. Next we will introduce how our model
conducts local and global addressing on table-to-text gener-
ation with the help of Fig 3.

Local addressing: A table can be treated as a set of ﬁeld-
value records. Local addressing tends to encode the table
content inside each record. The value in each ﬁeld-value
record is a sequence of words which contains 2.7 tokens
on average. Some records in the Wikipedia infoboxes even
contain several phrases or sentences. Previous models which
used one-hot encoding or statistical language model to en-
code ﬁeld content are inefﬁcient to capture the semantic rel-
evance between words inside a ﬁeld. The seq2seq structure
itself has a strong ability to model the context of a piece of
words. For one thing, the LSTM encoder can capture long-
range dependencies between words in the table. For another,
the word level attention of the proposed dual attention mech-
anism can also build a connection between the words in the
description and the tokens in the table. The generated word
‘actor’ in Fig 3 refers to the word ‘actor’ in the ‘Occupa-
tion’ ﬁeld.

Global addressing: The goal of local addressing is to
represent inner-record information while global addressing
aims to model inter-record relevance within the table. For
example, it’s noteworthy that the generated token ‘actor’ in
Fig 3 is mapped to the ‘occupation’ ﬁeld in Table 2.

Field-gating table representation and ﬁeld level attention
mechanism are proposed for global addressing. For table
representation, we encode the structure of a table by incor-
porating ﬁeld and position embedding into table representa-
tion apart from word embedding. The position of a token in

Mean

# tokens per sentence
26.1

# table token per sent.
9.5

# tokens per table
53.1

# ﬁelds per table
19.7

Table 1: Statistics of WIKIBIO dataset.

Word dimension
400

Field dimension
50

Position dimension Hidden size Batch size

Learning rate Optimizer

5

500

32

0.0005

Adam

Table 2: Parameter settings of our experiments.

the ﬁeld content of a table is determined only by its ﬁeld and
position information. Even two same words in the table can
be distinguished by their ﬁeld and position. We propose a
novel ﬁeld-gating LSTM to incorporate the ﬁeld embedding
into the cell memory of LSTM unit.

Furthermore, the information in a table is likely to be re-
dundant. Some records in the table are unimportant or even
useless for generating description. We should make appro-
priate choices on selecting useful information from all the ta-
ble records. The order of records may also inﬂuence the per-
formance of generation (Vinyals, Bengio, and Kudlur 2015).
We should make it clear which records the token to be gen-
erated is focused on by global addressing between the ﬁeld
information of a table and its description. The ﬁeld level at-
tention of dual attention mechanism is introduced to deter-
mine which ﬁeld the generator focused on in certain time
step. Experiments show that our dual attention mechanism
is of great help to generate description from certain table
and insensible to different orders of table records.

Experiments
We ﬁrst introduce the dataset, evaluation metrics and exper-
imental setups in our experiments. Then we compare our
model with several baselines. After that, we assess the per-
formance of our model on table-to-text generation. Further-
more, we also conduct experiments on the disordered tables
to show the efﬁciency of global addressing mechanism.

Dataset and Evaluation Metrics
We use WIKBIO dataset proposed by Lebret, Grangier, and
Auli (2016) as the benchmark dataset. WIKBIO contains
728,321 articles from English Wikipedia (Sep 2015). The
dataset uses the ﬁrst sentence of each article as the descrip-
tion of the corresponding infobox. Table 1 summarizes the
dataset statistics: on average, the tokens in the table (53.1)
are twice as long as those in the ﬁrst sentence (26.1). 9.5
tokens in the description text also occur in the table. The
corpus has been divided in to training (80%), testing (10%)
and validation (10%) sets.

We assess the generation quality automatically with

BLEU-4 and ROUGE-4 (F measure)1 .

Baselines
We compare the proposed structure-aware seq2seq model
with several statistical language models and the vanilla
encoder-decoder model. The baselines are listed as follows:

1We use standard scripts NIST mteval-v13a.pl (for BLEU), and

rouge-1.5.5 (for ROUGE).

• KN: The Kneser-Ney (KN) model is a widely used lan-
guage model proposed by Heaﬁeld et al. (2013). We use
the KenLM toolkit to train 5-gram models without prun-
ing.

• Template KN: Template KN is a KN model over
templates which also serves as a baseline in (Lebret,
Grangier, and Auli 2016). The model
replaces the
words occurring in both the table and the training
sentences with a special token reﬂecting its ﬁeld. The
introduction section of the table in Fig 2 looks as
follows under this scheme: “ name 1 name 2 (born
birthname 1 ... birthdate 3) is a Lithuanian-
Australian occupation 1 and occupation 3
best known for his performances in known for 1
... known for 4 (1961) and known for 5 ...
known for 7 (1963) ”. During inference, the decoder
is constrained to emit words from the regular vocabulary
or special tokens occurring in the input table.

• NLM: A naive statistical language model proposed by
(Lebret, Grangier, and Auli 2016) for comparison. The
model uses only the ﬁeld content as input without ﬁeld
and position information.

• Table NLM: The most competitive statistical language
model proposed by (Lebret, Grangier, and Auli 2016),
which includes local and global conditioning over the ta-
ble by integrating related ﬁeld and position embedding
into the table representation.

• Vanilla Seq2seq: The vanilla seq2seq neural architecture
is also provided as a strong baseline which uses the con-
catenation of word embedding, ﬁeld embedding and po-
sition embedding as the model input. The model can op-
erate local addressing over the table by the natural advan-
tages of LSTM units and word level attention mechanism.

Experiment Setup
In the table encoding phase, we use a sequence of word em-
beddings and their corresponding ﬁeld and position embed-
ding as input. We select the most frequent 20,000 words in
the training set as the word vocabulary. For ﬁeld embedding,
we select 1480 ﬁelds occurring more than 100 times from
the training set as ﬁeld vocabulary. Additionally, we ﬁlter all
empty ﬁelds whose values are (cid:104)none(cid:105) while feeding ﬁeld in-
formation to the network. We also limit the largest position
number as 30. Any position number over 30 will be counted
as 30.

While generating description for the table, a special start
token (cid:104)sos(cid:105) is feed into the generator in the beginning of the

Figure 4: An example of word level, ﬁeld level and aggregated dual attention on generating the biography of Fr´ed´eric Fonteyne.
Note there are two adjacent ‘belgium’s in ‘birthplace-3’ and ‘nationality-1’ ﬁeld, respectively. The word level attention focuses
improperly on the ﬁrst ‘belgium’ while generating ‘a belgian ﬁlm director’. In contrast, the ﬁeld level attention and dual
attention can locate the second ‘belgium’ properly by word-ﬁeld modeling (marked in the black boxes).

Model
KN
Template KN
NLM
Table NLM
Seq2seq
+ ﬁeld (concate)
+ pos (concate)
Field-gating Seq2seq
+ dual attention
+ beam search (k=5)

BLEU
2.21
19.80
4.17 ± 0.54
34.70 ± 0.36
42.06 ± 0.32
43.34 ± 0.37
43.65 ± 0.44
43.74 ± 0.23
44.89 ± 0.33
44.71

ROUGE
0.38
10.70
1.48 ± 0.23
25.80 ± 0.36
38.06 ± 0.36
39.84 ± 0.32
40.32 ± 0.23
40.53 ± 0.31
41.21 ± 0.25
41.65

Table 3: BLEU-4 and ROUGE-4 for structure-aware
seq2seq model (last three rows), statistical language model
(ﬁrst four rows) and vanilla seq2seq model with ﬁeld and
position input (three rows in the middle).

decoding phase. Then we use the last generated token as the
input at the next time step. A special end token (cid:104)eos(cid:105) is used
to mark the end of decoding. We also restrict the generated
text by a pre-deﬁned max length to avoid redundant or irrel-
evant generation. We also try beam search with beam size
2-10 to enhance the performance. We use grid search to de-
termine the parameters of our model. The detail of model
parameters is listed in Table 2.

Generation Assessment
The assessment for description generation is listed in Ta-
ble 3. We have following observations: (1) Neural network
models perform much better than statistical language mod-
els. Even vanilla seq2seq architecture with word level atten-

tion outperform the most competitive statistical model by a
great margin. (2)The proposed structure-aware seq2seq ar-
chitecture can further improve the table-to-text generation
compared with the competitive vanilla seq2seq. Dual atten-
tion mechanism is able to boost the model performance by
over 1 BLEU compared to vanilla attention mechanism.

Research on Disordered Tables

We view a structured table as a set of ﬁeld-value records and
then feed the records into the generator sequentially as the
order they are presented in the table. The order of records
can guide the description generator to produce an introduc-
tion in the pre-deﬁned schemas (Vinyals, Bengio, and Kud-
lur 2015). However, not all the tables are arranged in the
proper order. So global addressing between the generated
descriptions and the records of the table is necessary for
table-to-text generation.

Furthermore, the schemas of various types of tables dif-
fer greatly from each other. A biography about a politician
may emphasize his or her social activities and working ex-
perience while a biography of a soccer player is likely to
highlight which team he or she used to serve in or the per-
formance in his or her career. To cope with various schemas
of different tables, it’s essential to model inter-record infor-
mation within the tables by global addressing.

For these reasons, we propose a pair of disordered train-
ing and testing set based on WIKIBIO by randomly shuf-
ﬂing the records of a infobox. For example, the order of
several records in a speciﬁc infobox is ‘name-birthdate-
occupation-spouse’, we randomly shufﬂe the table records
as ‘occupation-name-spouse-birthdate’, without changing
the ﬁeld content inside the ‘occupation’, ‘name’, ‘spouse’
and ‘birthdate’ records.

Figure 5: The generated descriptions for Binky Jones and the corresponding reference in the Wikipedia. Our proposed struct-
aware seq2seq model can generate more informative and accurate description compared to vanilla seq2seq model.

Table 4 shows that all three neural network models per-
form not as good as before, which means the order of ta-
ble records is an essential aspect for table-to-text generation.
However, the BLEU and ROUGE decreases on the structure-
aware seq2seq model are much smaller than the other two
models, which proves the efﬁciency of global addressing
mechanism.

Model
Seq2seq
+ ﬁeld & pos
Structure-aware

BLEU
40.04 (-2.02)
42.10 (-1.55)
44.28 (-0.61)

ROUGE
36.85 (-1.21)
38.97 (-1.35)
40.79 (-0.42)

Table 4: Experiments on the disordered tables to show the
efﬁciency of global addressing.

Qualitative Analysis

Analysis on Dual Attention
Dual attention mechanism models the relationship between
the generated tokens and table content inside each record by
word level attention while encoding the relevance of gener-
ated description and inter-record information within the ta-
ble by ﬁeld level attention. The aggregation of word level
attention and ﬁeld level attention can model more precise
connection between the table and its generated description.
Fig 4 shows an example of the three attention mecha-
nisms while generating a piece of description for Fr´ed´eric
Fonteyne based on his Wikipedia infobox. We can ﬁnd out
that the name, birthdate, nationality and occupation informa-
tion contained in the generated sentence can properly refer
to the related table content by the aggregated dual attention.

Case Study
Fig 5 shows the generated descriptions for different variants
of our model based on the related Wikipedia infobox. All

three neural network generators can produce coherent and
understandable sentences with the help of local addressing
mechanism. All of them contain the word ‘baseball’ which
is not directly mentioned in the infobox. It means the genera-
tors deduce from table content that Binky Jones is a baseball
player.

However, the two vanilla seq2seq models also generate
‘major league baseball’ or ‘major leagues’ which are not
mentioned in the table and probably not correct. Vanilla
seq2seq model without global addressing on the table just
generates the most possible league in Wikipedia for a base-
ball player to play in.

Furthermore, the two biographies generated by vanilla
seq2seq model fail to contain the information from the in-
fobox which team he served in, as well as the time period
of his playing in that team. The biography generated by
our proposed structure-aware seq2seq model is able to cover
nearly all the information mentioned in the table. The gen-
erated segment ‘who played shortstop from april 15 to april
27 for the brooklyn robins in 1924’ (15 words) includes in-
formation in ﬁve ﬁelds of the table: ‘position’, ‘debutdate’,
‘ﬁnaldate’, ‘debutteam’ and ‘ﬁnalteam’, which is achieved
by the global addressing between the ﬁelds and the gener-
ated tokens.

Conclusions

We propose a structure-aware seq2seq architecture to encode
both the content and the structure of a table for table-to-text
generation. The model consists of ﬁeld-gating encoder and
description generator with dual attention. We add a ﬁeld gate
to the encoder LSTM unit to incorporate the ﬁeld informa-
tion. Furthermore, dual attention mechanism which contains
word level attention and ﬁeld level attention can operate lo-
cal and global addressing to the content and the structure of a
table. A series of visualizations, case studies and generation
assessments show that our model outperforms the competi-
tive baselines by a large margin.

Acknowledgments
Our work is
supported by the National Key Re-
search and Development Program of China under Grant
No.2017YFB1002101 and project 61772040 supported by
NSFC. The corresponding authors of this paper are Baobao
Chang and Zhifang Sui.

References
Angeli, G.; Liang, P.; and Klein, D. 2010. A simple domain-
In Pro-
independent probabilistic approach to generation.
ceedings of the 2010 Conference on Empirical Methods in
Natural Language Processing, 502–512. Association for
Computational Linguistics.
Barzilay, R., and Lapata, M. 2005. Collective content se-
lection for concept-to-text generation. In Proceedings of the
conference on Human Language Technology and Empirical
Methods in Natural Language Processing, 331–338. Asso-
ciation for Computational Linguistics.
Belz, A. 2008. Automatic generation of weather forecast
texts using comprehensive probabilistic generation-space
models. Natural Language Engineering 14(4):431–455.
Chen, D. L., and Mooney, R. J. 2008. Learning to sportscast:
In Proceedings
a test of grounded language acquisition.
of the 25th international conference on Machine learning,
128–135. ACM.
Duboue, P. A., and McKeown, K. R. 2002. Content planner
construction via evolutionary algorithms and a corpus-based
ﬁtness function. In Proceedings of INLG 2002, 89–96.
Graves, A.; Mohamed, A.-r.; and Hinton, G. 2013. Speech
recognition with deep recurrent neural networks. In Acous-
tics, speech and signal processing (icassp), 2013 ieee inter-
national conference on, 6645–6649. IEEE.
Gyawali, B. 2016. Surface Realisation from Knowledge
Bases. Ph.D. Dissertation, Universite de Lorraine.
Heaﬁeld, K.; Pouzyrevsky, I.; Clark, J. H.; and Koehn, P.
2013. Scalable modiﬁed kneser-ney language model esti-
mation. In ACL (2), 690–696.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term
memory. Neural computation 9(8):1735–1780.
Kim, J., and Mooney, R. J. 2010. Generative alignment
and semantic parsing for learning from ambiguous supervi-
sion. In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, 543–551. Associa-
tion for Computational Linguistics.
Konstas, I., and Lapata, M. 2012. Unsupervised concept-
to-text generation with hypergraphs. In Proceedings of the
2012 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, 752–761. Association for Computational Lin-
guistics.
Konstas, I., and Lapata, M. 2013. A global model for
concept-to-text generation. Journal of Artiﬁcial Intelligence
Research 48:305–346.
Lebret, R.; Grangier, D.; and Auli, M. 2016. Neural text
generation from structured data with application to the biog-
raphy domain. arXiv preprint arXiv:1603.07771.

Liang, P.; Jordan, M. I.; and Klein, D. 2009. Learning se-
mantic correspondences with less supervision. In Proceed-
ings of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Volume 1-Volume
1, 91–99. Association for Computational Linguistics.
Lu, W., and Ng, H. T. 2011. A probabilistic forest-to-string
model for language generation from typed lambda calculus
expressions. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, 1611–1622.
Association for Computational Linguistics.
Lu, W.; Ng, H. T.; and Lee, W. S. 2009. Natural language
generation with tree conditional random ﬁelds. In Proceed-
ings of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 1-Volume 1, 400–409.
Association for Computational Linguistics.
Luong, M.-T.; Sutskever, I.; Le, Q. V.; Vinyals, O.; and
Zaremba, W. 2014. Addressing the rare word problem in
neural machine translation. arXiv preprint arXiv:1410.8206.
Ma, S.; Sun, X.; Xu, J.; Wang, H.; Li, W.; and Su, Q.
2017.
Improving semantic relevance for sequence-to-
sequence learning of chinese social media text summariza-
tion. In Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL 2017, Vancou-
ver, Canada, July 30 - August 4, Volume 2: Short Papers,
635–640.
Mei, H.; Bansal, M.; and Walter, M. R. 2015. What to
talk about and how? selective generation using lstms with
coarse-to-ﬁne alignment. arXiv preprint arXiv:1509.00838.
Ratnaparkhi, A. 2002. Trainable approaches to surface
natural language generation and their application to con-
versational dialog systems. Computer Speech & Language
16(3):435–455.
Reiter, E., and Dale, R. 2000. Building natural language
generation systems. Cambridge university press.
Sha, L.; Mou, L.; Liu, T.; Poupart, P.; Li, S.; Chang, B.; and
Sui, Z. 2017. Order-planning neural text generation from
structured data. CoRR abs/1709.00155.
Stent, A.; Prasad, R.; and Walker, M. 2004. Trainable sen-
tence planning for complex information presentation in spo-
ken dialog systems. In Proceedings of the 42nd annual meet-
ing on association for computational linguistics, 79. Asso-
ciation for Computational Linguistics.
Vinyals, O.; Bengio, S.; and Kudlur, M.
matters: Sequence to sequence for sets.
arXiv:1511.06391.
Walker, M. A.; Rambow, O.; and Rogati, M. 2001. Spot:
A trainable sentence planner. In Proceedings of the second
meeting of the North American Chapter of the Association
for Computational Linguistics on Language technologies,
1–8. Association for Computational Linguistics.
Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi-
nov, R.; Zemel, R.; and Bengio, Y. 2015. Show, attend and
tell: Neural image caption generation with visual attention.
In International Conference on Machine Learning, 2048–
2057.

2015. Order
arXiv preprint

Table-to-text Generation by Structure-aware Seq2seq Learning

Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang and Zhifang Sui
Key Laboratory of Computational Linguistics, Ministry of Education,
School of Electronics Engineering and Computer Science, Peking University, Beijing, China
{tianyu0421, wkx, shalei, chbb, szf}@pku.edu.cn

7
1
0
2
 
v
o
N
 
7
2
 
 
]
L
C
.
s
c
[
 
 
1
v
4
2
7
9
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Table-to-text generation aims to generate a description for
a factual table which can be viewed as a set of ﬁeld-value
records. To encode both the content and the structure of a
table, we propose a novel structure-aware seq2seq architec-
ture which consists of ﬁeld-gating encoder and description
generator with dual attention. In the encoding phase, we up-
date the cell memory of the LSTM unit by a ﬁeld gate and
its corresponding ﬁeld value in order to incorporate ﬁeld in-
formation into table representation. In the decoding phase,
dual attention mechanism which contains word level atten-
tion and ﬁeld level attention is proposed to model the seman-
tic relevance between the generated description and the ta-
ble. We conduct experiments on the WIKIBIO dataset which
contains over 700k biographies and corresponding infoboxes
from Wikipedia. The attention visualizations and case stud-
ies show that our model is capable of generating coherent
and informative descriptions based on the comprehensive un-
derstanding of both the content and the structure of a table.
Automatic evaluations also show our model outperforms the
baselines by a great margin. Code for this work is available
on https://github.com/tyliupku/wiki2bio.

Introduction
Generating natural language description for a structured ta-
ble is an important task for text generation from struc-
tured data. Previous researches include weather forecast
based on a set of weather records (Liang, Jordan, and Klein
2009) and sportscasting based on temporally ordered events
(Chen and Mooney 2008). However, previous work models
the structured data in the limited pre-deﬁned schemas. For
example, a weather record rainChance(time:06:00-21:00,
mode:SSE, value:20) is represented by a ﬁxed-length one-
hot vector by its record type, record time, record value
and record value. To this end, we focus on table-to-text
generation which involves comprehensive representation for
the complex structure of a table rather than pre-deﬁned
schemas. In contrast to previous work experimented on
small datasets which contain only a few tens of thousands
of records such as WEATHERGOV (Liang, Jordan, and Klein
2009) and ROBOCUP (Chen and Mooney 2008), we fo-
cus on a more challenging task to generate biographies

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: The Wikipedia infobox of Charles Winstead, the
corresponding introduction on his wiki page reads “Charles
Winstead (1891 - 1973) was an FBI agent in the 1930s - 40s,
famous for being one of the agents who shot and killed John
Dillinger.”.

based on the Wikipedia infoboxes. As shown in Fig 1, a
biographic infobox is a ﬁxed-format table that describes a
person with many ﬁeld-value records like (Name, Charles
B. Winstead), (Nationality, American), (Occupation, FBI
Agent), etc. We utilize WIKIBIO dataset proposed by Le-
bret, Grangier, and Auli (2016) which contains 700k biogra-
phies from Wikipedia, with 400k words in total as the bench-
mark dataset.

Previous work has made signiﬁcant progress on this
task. Lebret, Grangier, and Auli (2016) proposed a statis-
tical n-gram model with local and global conditioning on a
Wikipedia infobox. However the ﬁeld content of a record
is likely to be a sequence of words, the statistical language
model is not good at capturing long-range dependencies be-
tween words. Mei, Bansal, and Walter (2015) proposed a
selective generation method based on an encoder-aligner-
decoder framework. The model utilizes a sparse one-hot vec-
tor to represent a weather record. However it’s inefﬁcient to
represent the complex structure of a table by one-hot vec-
tors.

We propose a structure-aware sequence to sequence
(seq2seq) generation framework to model both content and
structure of the table by local and global addressing. When a
human writes a biography for a person based on the related

Wikipedia infobox, he will ﬁrstly determine which records
in the table should be included in the introduction and how
to arrange the order of these records before wording. After
that, the writer will further consider which words or phrases
in the table should be more focused on to paraphrase. We
summarize the two phases of generation as two scopes of
addressing: local and global addressing. Local addressing
determines which particular word in the table should be fo-
cused on while generating a piece of description at certain
time step. However, the word level addressing can not fully
address the table-to-text generation problem as the factual
tables usually have complex structures which might confuse
the generator. Global addressing is proposed to determine
which records of the table should be more focused on while
generating corresponding description. Global addressing is
necessary as the description of a table may not cover all the
records. For example, the ‘cause of death’ ﬁeld in Fig 1 is
not mentioned in the description. Furthermore, the order of
records in the tables may not always be homogeneous. For
example, we can introduce a person as an order of his (Birth-
Death-Nationality-Occupation) according to his Wikipedia
infobox. However the other infoboxes may be arranged as
(Occupation-Nationality-Birth-Death). Local addressing is
realized by content encoding of the LSTM encoder and word
level attention while global addressing is realized by ﬁeld
encoding of the ﬁeld-gating LSTM variation and ﬁeld level
attention in our model.

The structure-aware seq2seq architecture we proposed
exploits encoder-decoder framework using long short-term
memory (LSTM) (Hochreiter and Schmidhuber 1997) units
with local and global addressing on the structured table. In
the encoding phase, our model ﬁrst encodes the sets of ﬁeld-
value records in the table by integrating ﬁeld information
and content representation. To make better use of ﬁeld in-
formation, we add a ﬁeld gate to the cell state of the encoder
LSTM unit to incorporate the ﬁeld embedding into the struc-
tural representation of the table. The model next employs
a LSTM decoder to generate natural language description
by the structural representation of the table. In the decoding
phase, we also propose a novel dual attention mechanism
which consists of two parts: word-level attention for local
addressing and ﬁeld-level attention for global addressing.

Our contributions are three-fold: (1) We propose an end-
to-end structure-aware encoder-decoder architecture to en-
code ﬁeld information into the representation of a structured
table. (2) Field-gating encoder and dual attention mecha-
nism are proposed to operate local and global addressing
between the content and the ﬁeld information of a structured
table. (3) Experiments on WIKIBIO dataset show that our
model achieves substantial improvement over baselines.

Related Work
Most generation systems can be divided into two indepen-
dent modules: (1)content selection involves choosing a sub-
set of relevant records in a table to talk about. (2)surface re-
alization is concerned with generating natural language de-
scriptions for this subset.

Many approaches have been proposed to learn the indi-
vidual modules. For content selection module, one approach

builds a content selection model by aligning records and
sentences (Barzilay and Lapata 2005; Duboue and McKe-
own 2002). A hierarchical semi-Markov method is proposed
by (Liang, Jordan, and Klein 2009) which ﬁrst associates
the text sequences to corresponding records and then gener-
ates corresponding descriptions from these records. Surface
realization is often treated as a concept-to-text generation
task from a given representation. Reiter and Dale (2000),
Walker, Rambow, and Rogati (2001) and Stent, Prasad, and
Walker (2001) utilize various linguistic features to train sen-
tence planners for sentence generation. Context-free gram-
mars are also used to generate natural language sentences
from formal meaning representations (Lu and Ng 2011; Belz
2008). Other effective approaches include hybrid alignment
tree (Kim and Mooney 2010), tree conditional random ﬁelds
(Lu, Ng, and Lee 2009), tree adjoining grammar (Gyawali
2016) and template extraction in a log-linear framework
(Angeli, Liang, and Klein 2010). Recent work combines
content selection and surface realization in a uniﬁed frame-
work (Ratnaparkhi 2002; Konstas and Lapata 2012; 2013;
Sha et al. 2017)

Our model borrowed the idea of representing a struc-
tured table by its ﬁeld and content information from (Le-
bret, Grangier, and Auli 2016). However, their n-gram model
is inefﬁcient to model long-range dependencies while gen-
erating descriptions. Mei, Bansal, and Walter (2015) also
proposed a seq2seq model with an aligner between weather
records and weather broadcast. The model used one-hot en-
coding to represent the weather records as they are relatively
simple and highly structured. However, the model is not
capable to represent the tables with complex structure like
Wikipedia infoboxes.

Task Deﬁnition
We model the table-to-text generation in an end-to-end
structure-aware seq2seq framework. The given table T can
be viewed as a combination of n ﬁeld-value records {
R1, R2, · · · , Rn}. Each record Ri consists of a sequence of
words { d1, d2, · · · , dm} and their corresponding ﬁeld rep-
resent { Zd1 , Zd2, · · · , Zdm}.

The output of the model is the generated description S
for table T which contains p tokens {w1, w2, · · · , wp} with
wt being the word at time t. We formulate the table-to-text
generation as the inference over a probabilistic model. The
goal of the inference is to generate a sequence w∗
1:p which
maximizes P (w1:p|R1:n).

w∗

1:p = arg max
w1:p

p
(cid:89)

t=1

P (wt|w0:t−1, R1:n)

(1)

Structure-aware Seq2seq

Field representation
A Wikipedia infobox can be viewed as a set of ﬁeld-value
records, in which values are sequences or segments of words
corresponding to certain ﬁelds. The structural representation
of an infobox consists of context embedding and ﬁeld em-
bedding. The context embedding is formulated as an em-

ht = ot (cid:12) tanh(ct)
(5)
where it, ft, ot ∈ [0, 1]n are input, forget and output gates
respectively, and ˆct and ct are proposed cell value and true
cell value in time t. n is the hidden size.

To make better understanding of the structure of a table,
the ﬁeld information should also be encoded into the en-
coder. One simple way is to take the concatenation of word
embedding and corresponding ﬁeld embedding as the input
for the vanilla LSTM unit. Actually, the method is indeed
proved to be useful in our experiments and serves as a base-
line for comparison. However, the concatenation of word
embedding and ﬁeld embedding only treats the ﬁeld infor-
mation as an additional label of certain token which loses
the structural information of the table.

To better encode the structural information of a table, we
propose a ﬁeld-gating variation on the vanilla LSTM unit to
update the cell memory by a ﬁeld gate and its corresponding
ﬁeld value. The ﬁeld-gating cell state is described as follows:

(cid:19)

(cid:18)lt
ˆzt

(cid:19)

(cid:18)sigmoid
tanh

=

W d

2n,2n (zt)

(6)

(cid:48)

c

t = ft (cid:12) ct−1 + it (cid:12) ˆct + lt (cid:12) ˆzt
where zt is the ﬁeld embedding described before, lt ∈
[0, 1]n is the ﬁeld gate to determine how much ﬁeld infor-
mation should be kept in the cell memory, ˆzt is the proposed
ﬁeld value corresponding to ﬁeld gate. The cell state c
t is up-
dated from the original ct by incorporating ﬁeld information
of the table.

(7)

(cid:48)

Description Decoder with Dual Attention
To conduct local and global addressing towards the struc-
tured table, we use LSTM architecture with dual attention
mechanism as our description generator. As deﬁned in the
equation 1, the generated token wt at time t in the decoder
is predicated based on all the previously generated tokens
w<t before wt, the hidden states H = {ht}L
t=1 of the table
encoder and the ﬁeld embeddings Z = {zt}L
t=1. To be more
speciﬁc:

P (wt|H, Z, w<t) = sof tmax(Ws (cid:12) gt)

gt = tanh(Wt[st, at])

st = LST M (wt−1, st−1)

(8)

(9)

(10)

where st is the t-th hidden state of the decoder calculated
by the LSTM unit. The computational details can be referred
in Equation 3, 4 and 5. at is the attention vector which is
widely used in many applications (Xu et al. 2015; Luong
et al. 2014; Ma et al. 2017). Vanilla attention mechanism
is proposed to encode the semantic relevance between the
encoder states {ht}L
t=1 and and the decoder state st at time
t. The attention vector is usually represented by the weighted
sum of encoder hidden states.

αti =

eg(st,hi)
j=1 eg(st,hj )

(cid:80)N

L
(cid:88)

i=1

; at =

αtihi

(11)

Figure 2: The wiki infobox of George Mikell (left) and the
table of its ﬁeld representation (right).

bedding for a segment of words in the ﬁeld content. The
ﬁeld embedding is a key point to label each word in the ﬁeld
content by its corresponding ﬁeld name and occurrence in
the table. Lebret, Grangier, and Auli (2016) represented the
ﬁeld embeddding Zw = {fw; pw} for a word w in the table
with corresponding ﬁeld name fw and position information
pw. The position information can be further represented as a
tuple (p+
w) which includes the positions of the token w
counted from the begining and the end of the ﬁeld respec-
tively. So the ﬁeld embedding of token w is extended to a
triple:

w, p−

Zw = {fw; p+

w; p−
w}

(2)

As shown in Fig 2, the infobox of George Mikell contains
several ﬁeld-value records, the ﬁeld content for the record
(birthname, Jurgis Mikelatitis) is ‘Jurgis Mikelatitis’. The
word ‘Jurgis’ is the ﬁrst token counted from the beginning
of the ﬁeld ‘birthname’ and also the second token counted
from the end. So the ﬁeld embedding for the word ‘Jurgis’
is described as {birthname; 1; 2}. Each token in the table
has an unique ﬁeld embedding even if there exists two same
words in the same ﬁeld due to the unique (ﬁeld, position)
pair.

Field-gating Table Encoder
The table encoder aims to encode each word dj in the table
together with its ﬁeld embedding Zdj into the hidden state
hj using LSTM encoder. We present a novel ﬁeld-gating
LSTM unit to incorporate ﬁeld information into table encod-
ing. LSTM is a recurrent neural network (RNN) architecture
which uses a vector of cell state ct and a set of element-wise
multiplication gates to control how information is stored,
forgotten and exploited inside the network. Following the
design for an LSTM cell in (Graves, Mohamed, and Hinton
2013) , the architecture used in the table encoder is deﬁned
by following equations:






it
ft
ot
ˆct






 =





 W c


sigmoid
sigmoid
sigmoid
tanh

(cid:19)

(cid:18) dt
ht−1

4n,2n

ct = ft (cid:12) ct−1 + it (cid:12) ˆct

(3)

(4)

Figure 3: The overall diagram of structure-aware seq2seq architecture for generating description for George Mikell in Fig 2.

where g(st, hi) is a relevant score between decoder hidden
state st and encoder hidden state hi. There are many differ-
ent ways to calculate the relevant scores. In our paper, we
use the following dot product to measure the similarity be-
tween st and hi. Ws, Wt, Wp, Wq are all model parameters.
g(st, hi) = tanh(Wphi) (cid:12) tanh(Wqst)
(12)
However, the word level attention described above can
only capture the semantic relevance between generated to-
kens and the content information in the table, ignoring the
structure information of the table. To fully utilize the struc-
ture information, we propose a higher level attention over
generated tokens and the ﬁeld embedding of the table. Field
level attention can locate the particular ﬁeld-value record
which should be focused on while generating next token in
the description by modeling the relevance between all ﬁeld
embeddings {zt}L
t=1 and the decoder state st at t-th time.
Field level attention weight βti is presented as Equation 13.
We use the same relevant score function g(st, zi) as equation
12. Dual attention weight γt is the element-wise production
between ﬁeld level attention weight βt and word level atten-
tion weight αt. The dual attention vector a
t is updated as
the weighted sum of encoder states {ht}t=1 by γt (Equation
15):

(cid:48)

βti =

eg(st,zi)
j=1 eg(st,zj )

(cid:80)N

g(st, zi) = tanh(Wxzi) (cid:12) tanh(Wyst)

γti =

αti · βti
j=1 αtj · βtj

(cid:80)N

(cid:48)
; a

t =

γtihi

L
(cid:88)

i=1

(13)

(14)

(15)

Furthermore, we utilize a post-process operation for the
generated unknown (UNK) tokens to alleviate the out-of-
vocabulary (OOV) problem. We replace a speciﬁc generated
UNK token with the most relevant token in the correspond-
ing table according to the related dual attention matrix.

Local and Global Addressing

Local and global addressing determine which part of the ta-
ble should be more focused on in different steps of descrip-
tion generation. The two scopes of addressings play a very
important role in understanding and representing the inner-
structure of a table. Next we will introduce how our model
conducts local and global addressing on table-to-text gener-
ation with the help of Fig 3.

Local addressing: A table can be treated as a set of ﬁeld-
value records. Local addressing tends to encode the table
content inside each record. The value in each ﬁeld-value
record is a sequence of words which contains 2.7 tokens
on average. Some records in the Wikipedia infoboxes even
contain several phrases or sentences. Previous models which
used one-hot encoding or statistical language model to en-
code ﬁeld content are inefﬁcient to capture the semantic rel-
evance between words inside a ﬁeld. The seq2seq structure
itself has a strong ability to model the context of a piece of
words. For one thing, the LSTM encoder can capture long-
range dependencies between words in the table. For another,
the word level attention of the proposed dual attention mech-
anism can also build a connection between the words in the
description and the tokens in the table. The generated word
‘actor’ in Fig 3 refers to the word ‘actor’ in the ‘Occupa-
tion’ ﬁeld.

Global addressing: The goal of local addressing is to
represent inner-record information while global addressing
aims to model inter-record relevance within the table. For
example, it’s noteworthy that the generated token ‘actor’ in
Fig 3 is mapped to the ‘occupation’ ﬁeld in Table 2.

Field-gating table representation and ﬁeld level attention
mechanism are proposed for global addressing. For table
representation, we encode the structure of a table by incor-
porating ﬁeld and position embedding into table representa-
tion apart from word embedding. The position of a token in

Mean

# tokens per sentence
26.1

# table token per sent.
9.5

# tokens per table
53.1

# ﬁelds per table
19.7

Table 1: Statistics of WIKIBIO dataset.

Word dimension
400

Field dimension
50

Position dimension Hidden size Batch size

Learning rate Optimizer

5

500

32

0.0005

Adam

Table 2: Parameter settings of our experiments.

the ﬁeld content of a table is determined only by its ﬁeld and
position information. Even two same words in the table can
be distinguished by their ﬁeld and position. We propose a
novel ﬁeld-gating LSTM to incorporate the ﬁeld embedding
into the cell memory of LSTM unit.

Furthermore, the information in a table is likely to be re-
dundant. Some records in the table are unimportant or even
useless for generating description. We should make appro-
priate choices on selecting useful information from all the ta-
ble records. The order of records may also inﬂuence the per-
formance of generation (Vinyals, Bengio, and Kudlur 2015).
We should make it clear which records the token to be gen-
erated is focused on by global addressing between the ﬁeld
information of a table and its description. The ﬁeld level at-
tention of dual attention mechanism is introduced to deter-
mine which ﬁeld the generator focused on in certain time
step. Experiments show that our dual attention mechanism
is of great help to generate description from certain table
and insensible to different orders of table records.

Experiments
We ﬁrst introduce the dataset, evaluation metrics and exper-
imental setups in our experiments. Then we compare our
model with several baselines. After that, we assess the per-
formance of our model on table-to-text generation. Further-
more, we also conduct experiments on the disordered tables
to show the efﬁciency of global addressing mechanism.

Dataset and Evaluation Metrics
We use WIKBIO dataset proposed by Lebret, Grangier, and
Auli (2016) as the benchmark dataset. WIKBIO contains
728,321 articles from English Wikipedia (Sep 2015). The
dataset uses the ﬁrst sentence of each article as the descrip-
tion of the corresponding infobox. Table 1 summarizes the
dataset statistics: on average, the tokens in the table (53.1)
are twice as long as those in the ﬁrst sentence (26.1). 9.5
tokens in the description text also occur in the table. The
corpus has been divided in to training (80%), testing (10%)
and validation (10%) sets.

We assess the generation quality automatically with

BLEU-4 and ROUGE-4 (F measure)1 .

Baselines
We compare the proposed structure-aware seq2seq model
with several statistical language models and the vanilla
encoder-decoder model. The baselines are listed as follows:

1We use standard scripts NIST mteval-v13a.pl (for BLEU), and

rouge-1.5.5 (for ROUGE).

• KN: The Kneser-Ney (KN) model is a widely used lan-
guage model proposed by Heaﬁeld et al. (2013). We use
the KenLM toolkit to train 5-gram models without prun-
ing.

• Template KN: Template KN is a KN model over
templates which also serves as a baseline in (Lebret,
Grangier, and Auli 2016). The model
replaces the
words occurring in both the table and the training
sentences with a special token reﬂecting its ﬁeld. The
introduction section of the table in Fig 2 looks as
follows under this scheme: “ name 1 name 2 (born
birthname 1 ... birthdate 3) is a Lithuanian-
Australian occupation 1 and occupation 3
best known for his performances in known for 1
... known for 4 (1961) and known for 5 ...
known for 7 (1963) ”. During inference, the decoder
is constrained to emit words from the regular vocabulary
or special tokens occurring in the input table.

• NLM: A naive statistical language model proposed by
(Lebret, Grangier, and Auli 2016) for comparison. The
model uses only the ﬁeld content as input without ﬁeld
and position information.

• Table NLM: The most competitive statistical language
model proposed by (Lebret, Grangier, and Auli 2016),
which includes local and global conditioning over the ta-
ble by integrating related ﬁeld and position embedding
into the table representation.

• Vanilla Seq2seq: The vanilla seq2seq neural architecture
is also provided as a strong baseline which uses the con-
catenation of word embedding, ﬁeld embedding and po-
sition embedding as the model input. The model can op-
erate local addressing over the table by the natural advan-
tages of LSTM units and word level attention mechanism.

Experiment Setup
In the table encoding phase, we use a sequence of word em-
beddings and their corresponding ﬁeld and position embed-
ding as input. We select the most frequent 20,000 words in
the training set as the word vocabulary. For ﬁeld embedding,
we select 1480 ﬁelds occurring more than 100 times from
the training set as ﬁeld vocabulary. Additionally, we ﬁlter all
empty ﬁelds whose values are (cid:104)none(cid:105) while feeding ﬁeld in-
formation to the network. We also limit the largest position
number as 30. Any position number over 30 will be counted
as 30.

While generating description for the table, a special start
token (cid:104)sos(cid:105) is feed into the generator in the beginning of the

Figure 4: An example of word level, ﬁeld level and aggregated dual attention on generating the biography of Fr´ed´eric Fonteyne.
Note there are two adjacent ‘belgium’s in ‘birthplace-3’ and ‘nationality-1’ ﬁeld, respectively. The word level attention focuses
improperly on the ﬁrst ‘belgium’ while generating ‘a belgian ﬁlm director’. In contrast, the ﬁeld level attention and dual
attention can locate the second ‘belgium’ properly by word-ﬁeld modeling (marked in the black boxes).

Model
KN
Template KN
NLM
Table NLM
Seq2seq
+ ﬁeld (concate)
+ pos (concate)
Field-gating Seq2seq
+ dual attention
+ beam search (k=5)

BLEU
2.21
19.80
4.17 ± 0.54
34.70 ± 0.36
42.06 ± 0.32
43.34 ± 0.37
43.65 ± 0.44
43.74 ± 0.23
44.89 ± 0.33
44.71

ROUGE
0.38
10.70
1.48 ± 0.23
25.80 ± 0.36
38.06 ± 0.36
39.84 ± 0.32
40.32 ± 0.23
40.53 ± 0.31
41.21 ± 0.25
41.65

Table 3: BLEU-4 and ROUGE-4 for structure-aware
seq2seq model (last three rows), statistical language model
(ﬁrst four rows) and vanilla seq2seq model with ﬁeld and
position input (three rows in the middle).

decoding phase. Then we use the last generated token as the
input at the next time step. A special end token (cid:104)eos(cid:105) is used
to mark the end of decoding. We also restrict the generated
text by a pre-deﬁned max length to avoid redundant or irrel-
evant generation. We also try beam search with beam size
2-10 to enhance the performance. We use grid search to de-
termine the parameters of our model. The detail of model
parameters is listed in Table 2.

Generation Assessment
The assessment for description generation is listed in Ta-
ble 3. We have following observations: (1) Neural network
models perform much better than statistical language mod-
els. Even vanilla seq2seq architecture with word level atten-

tion outperform the most competitive statistical model by a
great margin. (2)The proposed structure-aware seq2seq ar-
chitecture can further improve the table-to-text generation
compared with the competitive vanilla seq2seq. Dual atten-
tion mechanism is able to boost the model performance by
over 1 BLEU compared to vanilla attention mechanism.

Research on Disordered Tables

We view a structured table as a set of ﬁeld-value records and
then feed the records into the generator sequentially as the
order they are presented in the table. The order of records
can guide the description generator to produce an introduc-
tion in the pre-deﬁned schemas (Vinyals, Bengio, and Kud-
lur 2015). However, not all the tables are arranged in the
proper order. So global addressing between the generated
descriptions and the records of the table is necessary for
table-to-text generation.

Furthermore, the schemas of various types of tables dif-
fer greatly from each other. A biography about a politician
may emphasize his or her social activities and working ex-
perience while a biography of a soccer player is likely to
highlight which team he or she used to serve in or the per-
formance in his or her career. To cope with various schemas
of different tables, it’s essential to model inter-record infor-
mation within the tables by global addressing.

For these reasons, we propose a pair of disordered train-
ing and testing set based on WIKIBIO by randomly shuf-
ﬂing the records of a infobox. For example, the order of
several records in a speciﬁc infobox is ‘name-birthdate-
occupation-spouse’, we randomly shufﬂe the table records
as ‘occupation-name-spouse-birthdate’, without changing
the ﬁeld content inside the ‘occupation’, ‘name’, ‘spouse’
and ‘birthdate’ records.

Figure 5: The generated descriptions for Binky Jones and the corresponding reference in the Wikipedia. Our proposed struct-
aware seq2seq model can generate more informative and accurate description compared to vanilla seq2seq model.

Table 4 shows that all three neural network models per-
form not as good as before, which means the order of ta-
ble records is an essential aspect for table-to-text generation.
However, the BLEU and ROUGE decreases on the structure-
aware seq2seq model are much smaller than the other two
models, which proves the efﬁciency of global addressing
mechanism.

Model
Seq2seq
+ ﬁeld & pos
Structure-aware

BLEU
40.04 (-2.02)
42.10 (-1.55)
44.28 (-0.61)

ROUGE
36.85 (-1.21)
38.97 (-1.35)
40.79 (-0.42)

Table 4: Experiments on the disordered tables to show the
efﬁciency of global addressing.

Qualitative Analysis

Analysis on Dual Attention
Dual attention mechanism models the relationship between
the generated tokens and table content inside each record by
word level attention while encoding the relevance of gener-
ated description and inter-record information within the ta-
ble by ﬁeld level attention. The aggregation of word level
attention and ﬁeld level attention can model more precise
connection between the table and its generated description.
Fig 4 shows an example of the three attention mecha-
nisms while generating a piece of description for Fr´ed´eric
Fonteyne based on his Wikipedia infobox. We can ﬁnd out
that the name, birthdate, nationality and occupation informa-
tion contained in the generated sentence can properly refer
to the related table content by the aggregated dual attention.

Case Study
Fig 5 shows the generated descriptions for different variants
of our model based on the related Wikipedia infobox. All

three neural network generators can produce coherent and
understandable sentences with the help of local addressing
mechanism. All of them contain the word ‘baseball’ which
is not directly mentioned in the infobox. It means the genera-
tors deduce from table content that Binky Jones is a baseball
player.

However, the two vanilla seq2seq models also generate
‘major league baseball’ or ‘major leagues’ which are not
mentioned in the table and probably not correct. Vanilla
seq2seq model without global addressing on the table just
generates the most possible league in Wikipedia for a base-
ball player to play in.

Furthermore, the two biographies generated by vanilla
seq2seq model fail to contain the information from the in-
fobox which team he served in, as well as the time period
of his playing in that team. The biography generated by
our proposed structure-aware seq2seq model is able to cover
nearly all the information mentioned in the table. The gen-
erated segment ‘who played shortstop from april 15 to april
27 for the brooklyn robins in 1924’ (15 words) includes in-
formation in ﬁve ﬁelds of the table: ‘position’, ‘debutdate’,
‘ﬁnaldate’, ‘debutteam’ and ‘ﬁnalteam’, which is achieved
by the global addressing between the ﬁelds and the gener-
ated tokens.

Conclusions

We propose a structure-aware seq2seq architecture to encode
both the content and the structure of a table for table-to-text
generation. The model consists of ﬁeld-gating encoder and
description generator with dual attention. We add a ﬁeld gate
to the encoder LSTM unit to incorporate the ﬁeld informa-
tion. Furthermore, dual attention mechanism which contains
word level attention and ﬁeld level attention can operate lo-
cal and global addressing to the content and the structure of a
table. A series of visualizations, case studies and generation
assessments show that our model outperforms the competi-
tive baselines by a large margin.

Acknowledgments
Our work is
supported by the National Key Re-
search and Development Program of China under Grant
No.2017YFB1002101 and project 61772040 supported by
NSFC. The corresponding authors of this paper are Baobao
Chang and Zhifang Sui.

References
Angeli, G.; Liang, P.; and Klein, D. 2010. A simple domain-
In Pro-
independent probabilistic approach to generation.
ceedings of the 2010 Conference on Empirical Methods in
Natural Language Processing, 502–512. Association for
Computational Linguistics.
Barzilay, R., and Lapata, M. 2005. Collective content se-
lection for concept-to-text generation. In Proceedings of the
conference on Human Language Technology and Empirical
Methods in Natural Language Processing, 331–338. Asso-
ciation for Computational Linguistics.
Belz, A. 2008. Automatic generation of weather forecast
texts using comprehensive probabilistic generation-space
models. Natural Language Engineering 14(4):431–455.
Chen, D. L., and Mooney, R. J. 2008. Learning to sportscast:
In Proceedings
a test of grounded language acquisition.
of the 25th international conference on Machine learning,
128–135. ACM.
Duboue, P. A., and McKeown, K. R. 2002. Content planner
construction via evolutionary algorithms and a corpus-based
ﬁtness function. In Proceedings of INLG 2002, 89–96.
Graves, A.; Mohamed, A.-r.; and Hinton, G. 2013. Speech
recognition with deep recurrent neural networks. In Acous-
tics, speech and signal processing (icassp), 2013 ieee inter-
national conference on, 6645–6649. IEEE.
Gyawali, B. 2016. Surface Realisation from Knowledge
Bases. Ph.D. Dissertation, Universite de Lorraine.
Heaﬁeld, K.; Pouzyrevsky, I.; Clark, J. H.; and Koehn, P.
2013. Scalable modiﬁed kneser-ney language model esti-
mation. In ACL (2), 690–696.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term
memory. Neural computation 9(8):1735–1780.
Kim, J., and Mooney, R. J. 2010. Generative alignment
and semantic parsing for learning from ambiguous supervi-
sion. In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, 543–551. Associa-
tion for Computational Linguistics.
Konstas, I., and Lapata, M. 2012. Unsupervised concept-
to-text generation with hypergraphs. In Proceedings of the
2012 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, 752–761. Association for Computational Lin-
guistics.
Konstas, I., and Lapata, M. 2013. A global model for
concept-to-text generation. Journal of Artiﬁcial Intelligence
Research 48:305–346.
Lebret, R.; Grangier, D.; and Auli, M. 2016. Neural text
generation from structured data with application to the biog-
raphy domain. arXiv preprint arXiv:1603.07771.

Liang, P.; Jordan, M. I.; and Klein, D. 2009. Learning se-
mantic correspondences with less supervision. In Proceed-
ings of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Volume 1-Volume
1, 91–99. Association for Computational Linguistics.
Lu, W., and Ng, H. T. 2011. A probabilistic forest-to-string
model for language generation from typed lambda calculus
expressions. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, 1611–1622.
Association for Computational Linguistics.
Lu, W.; Ng, H. T.; and Lee, W. S. 2009. Natural language
generation with tree conditional random ﬁelds. In Proceed-
ings of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 1-Volume 1, 400–409.
Association for Computational Linguistics.
Luong, M.-T.; Sutskever, I.; Le, Q. V.; Vinyals, O.; and
Zaremba, W. 2014. Addressing the rare word problem in
neural machine translation. arXiv preprint arXiv:1410.8206.
Ma, S.; Sun, X.; Xu, J.; Wang, H.; Li, W.; and Su, Q.
2017.
Improving semantic relevance for sequence-to-
sequence learning of chinese social media text summariza-
tion. In Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL 2017, Vancou-
ver, Canada, July 30 - August 4, Volume 2: Short Papers,
635–640.
Mei, H.; Bansal, M.; and Walter, M. R. 2015. What to
talk about and how? selective generation using lstms with
coarse-to-ﬁne alignment. arXiv preprint arXiv:1509.00838.
Ratnaparkhi, A. 2002. Trainable approaches to surface
natural language generation and their application to con-
versational dialog systems. Computer Speech & Language
16(3):435–455.
Reiter, E., and Dale, R. 2000. Building natural language
generation systems. Cambridge university press.
Sha, L.; Mou, L.; Liu, T.; Poupart, P.; Li, S.; Chang, B.; and
Sui, Z. 2017. Order-planning neural text generation from
structured data. CoRR abs/1709.00155.
Stent, A.; Prasad, R.; and Walker, M. 2004. Trainable sen-
tence planning for complex information presentation in spo-
ken dialog systems. In Proceedings of the 42nd annual meet-
ing on association for computational linguistics, 79. Asso-
ciation for Computational Linguistics.
Vinyals, O.; Bengio, S.; and Kudlur, M.
matters: Sequence to sequence for sets.
arXiv:1511.06391.
Walker, M. A.; Rambow, O.; and Rogati, M. 2001. Spot:
A trainable sentence planner. In Proceedings of the second
meeting of the North American Chapter of the Association
for Computational Linguistics on Language technologies,
1–8. Association for Computational Linguistics.
Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi-
nov, R.; Zemel, R.; and Bengio, Y. 2015. Show, attend and
tell: Neural image caption generation with visual attention.
In International Conference on Machine Learning, 2048–
2057.

2015. Order
arXiv preprint

Table-to-text Generation by Structure-aware Seq2seq Learning

Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang and Zhifang Sui
Key Laboratory of Computational Linguistics, Ministry of Education,
School of Electronics Engineering and Computer Science, Peking University, Beijing, China
{tianyu0421, wkx, shalei, chbb, szf}@pku.edu.cn

7
1
0
2
 
v
o
N
 
7
2
 
 
]
L
C
.
s
c
[
 
 
1
v
4
2
7
9
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Table-to-text generation aims to generate a description for
a factual table which can be viewed as a set of ﬁeld-value
records. To encode both the content and the structure of a
table, we propose a novel structure-aware seq2seq architec-
ture which consists of ﬁeld-gating encoder and description
generator with dual attention. In the encoding phase, we up-
date the cell memory of the LSTM unit by a ﬁeld gate and
its corresponding ﬁeld value in order to incorporate ﬁeld in-
formation into table representation. In the decoding phase,
dual attention mechanism which contains word level atten-
tion and ﬁeld level attention is proposed to model the seman-
tic relevance between the generated description and the ta-
ble. We conduct experiments on the WIKIBIO dataset which
contains over 700k biographies and corresponding infoboxes
from Wikipedia. The attention visualizations and case stud-
ies show that our model is capable of generating coherent
and informative descriptions based on the comprehensive un-
derstanding of both the content and the structure of a table.
Automatic evaluations also show our model outperforms the
baselines by a great margin. Code for this work is available
on https://github.com/tyliupku/wiki2bio.

Introduction
Generating natural language description for a structured ta-
ble is an important task for text generation from struc-
tured data. Previous researches include weather forecast
based on a set of weather records (Liang, Jordan, and Klein
2009) and sportscasting based on temporally ordered events
(Chen and Mooney 2008). However, previous work models
the structured data in the limited pre-deﬁned schemas. For
example, a weather record rainChance(time:06:00-21:00,
mode:SSE, value:20) is represented by a ﬁxed-length one-
hot vector by its record type, record time, record value
and record value. To this end, we focus on table-to-text
generation which involves comprehensive representation for
the complex structure of a table rather than pre-deﬁned
schemas. In contrast to previous work experimented on
small datasets which contain only a few tens of thousands
of records such as WEATHERGOV (Liang, Jordan, and Klein
2009) and ROBOCUP (Chen and Mooney 2008), we fo-
cus on a more challenging task to generate biographies

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: The Wikipedia infobox of Charles Winstead, the
corresponding introduction on his wiki page reads “Charles
Winstead (1891 - 1973) was an FBI agent in the 1930s - 40s,
famous for being one of the agents who shot and killed John
Dillinger.”.

based on the Wikipedia infoboxes. As shown in Fig 1, a
biographic infobox is a ﬁxed-format table that describes a
person with many ﬁeld-value records like (Name, Charles
B. Winstead), (Nationality, American), (Occupation, FBI
Agent), etc. We utilize WIKIBIO dataset proposed by Le-
bret, Grangier, and Auli (2016) which contains 700k biogra-
phies from Wikipedia, with 400k words in total as the bench-
mark dataset.

Previous work has made signiﬁcant progress on this
task. Lebret, Grangier, and Auli (2016) proposed a statis-
tical n-gram model with local and global conditioning on a
Wikipedia infobox. However the ﬁeld content of a record
is likely to be a sequence of words, the statistical language
model is not good at capturing long-range dependencies be-
tween words. Mei, Bansal, and Walter (2015) proposed a
selective generation method based on an encoder-aligner-
decoder framework. The model utilizes a sparse one-hot vec-
tor to represent a weather record. However it’s inefﬁcient to
represent the complex structure of a table by one-hot vec-
tors.

We propose a structure-aware sequence to sequence
(seq2seq) generation framework to model both content and
structure of the table by local and global addressing. When a
human writes a biography for a person based on the related

Wikipedia infobox, he will ﬁrstly determine which records
in the table should be included in the introduction and how
to arrange the order of these records before wording. After
that, the writer will further consider which words or phrases
in the table should be more focused on to paraphrase. We
summarize the two phases of generation as two scopes of
addressing: local and global addressing. Local addressing
determines which particular word in the table should be fo-
cused on while generating a piece of description at certain
time step. However, the word level addressing can not fully
address the table-to-text generation problem as the factual
tables usually have complex structures which might confuse
the generator. Global addressing is proposed to determine
which records of the table should be more focused on while
generating corresponding description. Global addressing is
necessary as the description of a table may not cover all the
records. For example, the ‘cause of death’ ﬁeld in Fig 1 is
not mentioned in the description. Furthermore, the order of
records in the tables may not always be homogeneous. For
example, we can introduce a person as an order of his (Birth-
Death-Nationality-Occupation) according to his Wikipedia
infobox. However the other infoboxes may be arranged as
(Occupation-Nationality-Birth-Death). Local addressing is
realized by content encoding of the LSTM encoder and word
level attention while global addressing is realized by ﬁeld
encoding of the ﬁeld-gating LSTM variation and ﬁeld level
attention in our model.

The structure-aware seq2seq architecture we proposed
exploits encoder-decoder framework using long short-term
memory (LSTM) (Hochreiter and Schmidhuber 1997) units
with local and global addressing on the structured table. In
the encoding phase, our model ﬁrst encodes the sets of ﬁeld-
value records in the table by integrating ﬁeld information
and content representation. To make better use of ﬁeld in-
formation, we add a ﬁeld gate to the cell state of the encoder
LSTM unit to incorporate the ﬁeld embedding into the struc-
tural representation of the table. The model next employs
a LSTM decoder to generate natural language description
by the structural representation of the table. In the decoding
phase, we also propose a novel dual attention mechanism
which consists of two parts: word-level attention for local
addressing and ﬁeld-level attention for global addressing.

Our contributions are three-fold: (1) We propose an end-
to-end structure-aware encoder-decoder architecture to en-
code ﬁeld information into the representation of a structured
table. (2) Field-gating encoder and dual attention mecha-
nism are proposed to operate local and global addressing
between the content and the ﬁeld information of a structured
table. (3) Experiments on WIKIBIO dataset show that our
model achieves substantial improvement over baselines.

Related Work
Most generation systems can be divided into two indepen-
dent modules: (1)content selection involves choosing a sub-
set of relevant records in a table to talk about. (2)surface re-
alization is concerned with generating natural language de-
scriptions for this subset.

Many approaches have been proposed to learn the indi-
vidual modules. For content selection module, one approach

builds a content selection model by aligning records and
sentences (Barzilay and Lapata 2005; Duboue and McKe-
own 2002). A hierarchical semi-Markov method is proposed
by (Liang, Jordan, and Klein 2009) which ﬁrst associates
the text sequences to corresponding records and then gener-
ates corresponding descriptions from these records. Surface
realization is often treated as a concept-to-text generation
task from a given representation. Reiter and Dale (2000),
Walker, Rambow, and Rogati (2001) and Stent, Prasad, and
Walker (2001) utilize various linguistic features to train sen-
tence planners for sentence generation. Context-free gram-
mars are also used to generate natural language sentences
from formal meaning representations (Lu and Ng 2011; Belz
2008). Other effective approaches include hybrid alignment
tree (Kim and Mooney 2010), tree conditional random ﬁelds
(Lu, Ng, and Lee 2009), tree adjoining grammar (Gyawali
2016) and template extraction in a log-linear framework
(Angeli, Liang, and Klein 2010). Recent work combines
content selection and surface realization in a uniﬁed frame-
work (Ratnaparkhi 2002; Konstas and Lapata 2012; 2013;
Sha et al. 2017)

Our model borrowed the idea of representing a struc-
tured table by its ﬁeld and content information from (Le-
bret, Grangier, and Auli 2016). However, their n-gram model
is inefﬁcient to model long-range dependencies while gen-
erating descriptions. Mei, Bansal, and Walter (2015) also
proposed a seq2seq model with an aligner between weather
records and weather broadcast. The model used one-hot en-
coding to represent the weather records as they are relatively
simple and highly structured. However, the model is not
capable to represent the tables with complex structure like
Wikipedia infoboxes.

Task Deﬁnition
We model the table-to-text generation in an end-to-end
structure-aware seq2seq framework. The given table T can
be viewed as a combination of n ﬁeld-value records {
R1, R2, · · · , Rn}. Each record Ri consists of a sequence of
words { d1, d2, · · · , dm} and their corresponding ﬁeld rep-
resent { Zd1 , Zd2, · · · , Zdm}.

The output of the model is the generated description S
for table T which contains p tokens {w1, w2, · · · , wp} with
wt being the word at time t. We formulate the table-to-text
generation as the inference over a probabilistic model. The
goal of the inference is to generate a sequence w∗
1:p which
maximizes P (w1:p|R1:n).

w∗

1:p = arg max
w1:p

p
(cid:89)

t=1

P (wt|w0:t−1, R1:n)

(1)

Structure-aware Seq2seq

Field representation
A Wikipedia infobox can be viewed as a set of ﬁeld-value
records, in which values are sequences or segments of words
corresponding to certain ﬁelds. The structural representation
of an infobox consists of context embedding and ﬁeld em-
bedding. The context embedding is formulated as an em-

ht = ot (cid:12) tanh(ct)
(5)
where it, ft, ot ∈ [0, 1]n are input, forget and output gates
respectively, and ˆct and ct are proposed cell value and true
cell value in time t. n is the hidden size.

To make better understanding of the structure of a table,
the ﬁeld information should also be encoded into the en-
coder. One simple way is to take the concatenation of word
embedding and corresponding ﬁeld embedding as the input
for the vanilla LSTM unit. Actually, the method is indeed
proved to be useful in our experiments and serves as a base-
line for comparison. However, the concatenation of word
embedding and ﬁeld embedding only treats the ﬁeld infor-
mation as an additional label of certain token which loses
the structural information of the table.

To better encode the structural information of a table, we
propose a ﬁeld-gating variation on the vanilla LSTM unit to
update the cell memory by a ﬁeld gate and its corresponding
ﬁeld value. The ﬁeld-gating cell state is described as follows:

(cid:19)

(cid:18)lt
ˆzt

(cid:19)

(cid:18)sigmoid
tanh

=

W d

2n,2n (zt)

(6)

(cid:48)

c

t = ft (cid:12) ct−1 + it (cid:12) ˆct + lt (cid:12) ˆzt
where zt is the ﬁeld embedding described before, lt ∈
[0, 1]n is the ﬁeld gate to determine how much ﬁeld infor-
mation should be kept in the cell memory, ˆzt is the proposed
ﬁeld value corresponding to ﬁeld gate. The cell state c
t is up-
dated from the original ct by incorporating ﬁeld information
of the table.

(7)

(cid:48)

Description Decoder with Dual Attention
To conduct local and global addressing towards the struc-
tured table, we use LSTM architecture with dual attention
mechanism as our description generator. As deﬁned in the
equation 1, the generated token wt at time t in the decoder
is predicated based on all the previously generated tokens
w<t before wt, the hidden states H = {ht}L
t=1 of the table
encoder and the ﬁeld embeddings Z = {zt}L
t=1. To be more
speciﬁc:

P (wt|H, Z, w<t) = sof tmax(Ws (cid:12) gt)

gt = tanh(Wt[st, at])

st = LST M (wt−1, st−1)

(8)

(9)

(10)

where st is the t-th hidden state of the decoder calculated
by the LSTM unit. The computational details can be referred
in Equation 3, 4 and 5. at is the attention vector which is
widely used in many applications (Xu et al. 2015; Luong
et al. 2014; Ma et al. 2017). Vanilla attention mechanism
is proposed to encode the semantic relevance between the
encoder states {ht}L
t=1 and and the decoder state st at time
t. The attention vector is usually represented by the weighted
sum of encoder hidden states.

αti =

eg(st,hi)
j=1 eg(st,hj )

(cid:80)N

L
(cid:88)

i=1

; at =

αtihi

(11)

Figure 2: The wiki infobox of George Mikell (left) and the
table of its ﬁeld representation (right).

bedding for a segment of words in the ﬁeld content. The
ﬁeld embedding is a key point to label each word in the ﬁeld
content by its corresponding ﬁeld name and occurrence in
the table. Lebret, Grangier, and Auli (2016) represented the
ﬁeld embeddding Zw = {fw; pw} for a word w in the table
with corresponding ﬁeld name fw and position information
pw. The position information can be further represented as a
tuple (p+
w) which includes the positions of the token w
counted from the begining and the end of the ﬁeld respec-
tively. So the ﬁeld embedding of token w is extended to a
triple:

w, p−

Zw = {fw; p+

w; p−
w}

(2)

As shown in Fig 2, the infobox of George Mikell contains
several ﬁeld-value records, the ﬁeld content for the record
(birthname, Jurgis Mikelatitis) is ‘Jurgis Mikelatitis’. The
word ‘Jurgis’ is the ﬁrst token counted from the beginning
of the ﬁeld ‘birthname’ and also the second token counted
from the end. So the ﬁeld embedding for the word ‘Jurgis’
is described as {birthname; 1; 2}. Each token in the table
has an unique ﬁeld embedding even if there exists two same
words in the same ﬁeld due to the unique (ﬁeld, position)
pair.

Field-gating Table Encoder
The table encoder aims to encode each word dj in the table
together with its ﬁeld embedding Zdj into the hidden state
hj using LSTM encoder. We present a novel ﬁeld-gating
LSTM unit to incorporate ﬁeld information into table encod-
ing. LSTM is a recurrent neural network (RNN) architecture
which uses a vector of cell state ct and a set of element-wise
multiplication gates to control how information is stored,
forgotten and exploited inside the network. Following the
design for an LSTM cell in (Graves, Mohamed, and Hinton
2013) , the architecture used in the table encoder is deﬁned
by following equations:






it
ft
ot
ˆct






 =





 W c


sigmoid
sigmoid
sigmoid
tanh

(cid:19)

(cid:18) dt
ht−1

4n,2n

ct = ft (cid:12) ct−1 + it (cid:12) ˆct

(3)

(4)

Figure 3: The overall diagram of structure-aware seq2seq architecture for generating description for George Mikell in Fig 2.

where g(st, hi) is a relevant score between decoder hidden
state st and encoder hidden state hi. There are many differ-
ent ways to calculate the relevant scores. In our paper, we
use the following dot product to measure the similarity be-
tween st and hi. Ws, Wt, Wp, Wq are all model parameters.
g(st, hi) = tanh(Wphi) (cid:12) tanh(Wqst)
(12)
However, the word level attention described above can
only capture the semantic relevance between generated to-
kens and the content information in the table, ignoring the
structure information of the table. To fully utilize the struc-
ture information, we propose a higher level attention over
generated tokens and the ﬁeld embedding of the table. Field
level attention can locate the particular ﬁeld-value record
which should be focused on while generating next token in
the description by modeling the relevance between all ﬁeld
embeddings {zt}L
t=1 and the decoder state st at t-th time.
Field level attention weight βti is presented as Equation 13.
We use the same relevant score function g(st, zi) as equation
12. Dual attention weight γt is the element-wise production
between ﬁeld level attention weight βt and word level atten-
tion weight αt. The dual attention vector a
t is updated as
the weighted sum of encoder states {ht}t=1 by γt (Equation
15):

(cid:48)

βti =

eg(st,zi)
j=1 eg(st,zj )

(cid:80)N

g(st, zi) = tanh(Wxzi) (cid:12) tanh(Wyst)

γti =

αti · βti
j=1 αtj · βtj

(cid:80)N

(cid:48)
; a

t =

γtihi

L
(cid:88)

i=1

(13)

(14)

(15)

Furthermore, we utilize a post-process operation for the
generated unknown (UNK) tokens to alleviate the out-of-
vocabulary (OOV) problem. We replace a speciﬁc generated
UNK token with the most relevant token in the correspond-
ing table according to the related dual attention matrix.

Local and Global Addressing

Local and global addressing determine which part of the ta-
ble should be more focused on in different steps of descrip-
tion generation. The two scopes of addressings play a very
important role in understanding and representing the inner-
structure of a table. Next we will introduce how our model
conducts local and global addressing on table-to-text gener-
ation with the help of Fig 3.

Local addressing: A table can be treated as a set of ﬁeld-
value records. Local addressing tends to encode the table
content inside each record. The value in each ﬁeld-value
record is a sequence of words which contains 2.7 tokens
on average. Some records in the Wikipedia infoboxes even
contain several phrases or sentences. Previous models which
used one-hot encoding or statistical language model to en-
code ﬁeld content are inefﬁcient to capture the semantic rel-
evance between words inside a ﬁeld. The seq2seq structure
itself has a strong ability to model the context of a piece of
words. For one thing, the LSTM encoder can capture long-
range dependencies between words in the table. For another,
the word level attention of the proposed dual attention mech-
anism can also build a connection between the words in the
description and the tokens in the table. The generated word
‘actor’ in Fig 3 refers to the word ‘actor’ in the ‘Occupa-
tion’ ﬁeld.

Global addressing: The goal of local addressing is to
represent inner-record information while global addressing
aims to model inter-record relevance within the table. For
example, it’s noteworthy that the generated token ‘actor’ in
Fig 3 is mapped to the ‘occupation’ ﬁeld in Table 2.

Field-gating table representation and ﬁeld level attention
mechanism are proposed for global addressing. For table
representation, we encode the structure of a table by incor-
porating ﬁeld and position embedding into table representa-
tion apart from word embedding. The position of a token in

Mean

# tokens per sentence
26.1

# table token per sent.
9.5

# tokens per table
53.1

# ﬁelds per table
19.7

Table 1: Statistics of WIKIBIO dataset.

Word dimension
400

Field dimension
50

Position dimension Hidden size Batch size

Learning rate Optimizer

5

500

32

0.0005

Adam

Table 2: Parameter settings of our experiments.

the ﬁeld content of a table is determined only by its ﬁeld and
position information. Even two same words in the table can
be distinguished by their ﬁeld and position. We propose a
novel ﬁeld-gating LSTM to incorporate the ﬁeld embedding
into the cell memory of LSTM unit.

Furthermore, the information in a table is likely to be re-
dundant. Some records in the table are unimportant or even
useless for generating description. We should make appro-
priate choices on selecting useful information from all the ta-
ble records. The order of records may also inﬂuence the per-
formance of generation (Vinyals, Bengio, and Kudlur 2015).
We should make it clear which records the token to be gen-
erated is focused on by global addressing between the ﬁeld
information of a table and its description. The ﬁeld level at-
tention of dual attention mechanism is introduced to deter-
mine which ﬁeld the generator focused on in certain time
step. Experiments show that our dual attention mechanism
is of great help to generate description from certain table
and insensible to different orders of table records.

Experiments
We ﬁrst introduce the dataset, evaluation metrics and exper-
imental setups in our experiments. Then we compare our
model with several baselines. After that, we assess the per-
formance of our model on table-to-text generation. Further-
more, we also conduct experiments on the disordered tables
to show the efﬁciency of global addressing mechanism.

Dataset and Evaluation Metrics
We use WIKBIO dataset proposed by Lebret, Grangier, and
Auli (2016) as the benchmark dataset. WIKBIO contains
728,321 articles from English Wikipedia (Sep 2015). The
dataset uses the ﬁrst sentence of each article as the descrip-
tion of the corresponding infobox. Table 1 summarizes the
dataset statistics: on average, the tokens in the table (53.1)
are twice as long as those in the ﬁrst sentence (26.1). 9.5
tokens in the description text also occur in the table. The
corpus has been divided in to training (80%), testing (10%)
and validation (10%) sets.

We assess the generation quality automatically with

BLEU-4 and ROUGE-4 (F measure)1 .

Baselines
We compare the proposed structure-aware seq2seq model
with several statistical language models and the vanilla
encoder-decoder model. The baselines are listed as follows:

1We use standard scripts NIST mteval-v13a.pl (for BLEU), and

rouge-1.5.5 (for ROUGE).

• KN: The Kneser-Ney (KN) model is a widely used lan-
guage model proposed by Heaﬁeld et al. (2013). We use
the KenLM toolkit to train 5-gram models without prun-
ing.

• Template KN: Template KN is a KN model over
templates which also serves as a baseline in (Lebret,
Grangier, and Auli 2016). The model
replaces the
words occurring in both the table and the training
sentences with a special token reﬂecting its ﬁeld. The
introduction section of the table in Fig 2 looks as
follows under this scheme: “ name 1 name 2 (born
birthname 1 ... birthdate 3) is a Lithuanian-
Australian occupation 1 and occupation 3
best known for his performances in known for 1
... known for 4 (1961) and known for 5 ...
known for 7 (1963) ”. During inference, the decoder
is constrained to emit words from the regular vocabulary
or special tokens occurring in the input table.

• NLM: A naive statistical language model proposed by
(Lebret, Grangier, and Auli 2016) for comparison. The
model uses only the ﬁeld content as input without ﬁeld
and position information.

• Table NLM: The most competitive statistical language
model proposed by (Lebret, Grangier, and Auli 2016),
which includes local and global conditioning over the ta-
ble by integrating related ﬁeld and position embedding
into the table representation.

• Vanilla Seq2seq: The vanilla seq2seq neural architecture
is also provided as a strong baseline which uses the con-
catenation of word embedding, ﬁeld embedding and po-
sition embedding as the model input. The model can op-
erate local addressing over the table by the natural advan-
tages of LSTM units and word level attention mechanism.

Experiment Setup
In the table encoding phase, we use a sequence of word em-
beddings and their corresponding ﬁeld and position embed-
ding as input. We select the most frequent 20,000 words in
the training set as the word vocabulary. For ﬁeld embedding,
we select 1480 ﬁelds occurring more than 100 times from
the training set as ﬁeld vocabulary. Additionally, we ﬁlter all
empty ﬁelds whose values are (cid:104)none(cid:105) while feeding ﬁeld in-
formation to the network. We also limit the largest position
number as 30. Any position number over 30 will be counted
as 30.

While generating description for the table, a special start
token (cid:104)sos(cid:105) is feed into the generator in the beginning of the

Figure 4: An example of word level, ﬁeld level and aggregated dual attention on generating the biography of Fr´ed´eric Fonteyne.
Note there are two adjacent ‘belgium’s in ‘birthplace-3’ and ‘nationality-1’ ﬁeld, respectively. The word level attention focuses
improperly on the ﬁrst ‘belgium’ while generating ‘a belgian ﬁlm director’. In contrast, the ﬁeld level attention and dual
attention can locate the second ‘belgium’ properly by word-ﬁeld modeling (marked in the black boxes).

Model
KN
Template KN
NLM
Table NLM
Seq2seq
+ ﬁeld (concate)
+ pos (concate)
Field-gating Seq2seq
+ dual attention
+ beam search (k=5)

BLEU
2.21
19.80
4.17 ± 0.54
34.70 ± 0.36
42.06 ± 0.32
43.34 ± 0.37
43.65 ± 0.44
43.74 ± 0.23
44.89 ± 0.33
44.71

ROUGE
0.38
10.70
1.48 ± 0.23
25.80 ± 0.36
38.06 ± 0.36
39.84 ± 0.32
40.32 ± 0.23
40.53 ± 0.31
41.21 ± 0.25
41.65

Table 3: BLEU-4 and ROUGE-4 for structure-aware
seq2seq model (last three rows), statistical language model
(ﬁrst four rows) and vanilla seq2seq model with ﬁeld and
position input (three rows in the middle).

decoding phase. Then we use the last generated token as the
input at the next time step. A special end token (cid:104)eos(cid:105) is used
to mark the end of decoding. We also restrict the generated
text by a pre-deﬁned max length to avoid redundant or irrel-
evant generation. We also try beam search with beam size
2-10 to enhance the performance. We use grid search to de-
termine the parameters of our model. The detail of model
parameters is listed in Table 2.

Generation Assessment
The assessment for description generation is listed in Ta-
ble 3. We have following observations: (1) Neural network
models perform much better than statistical language mod-
els. Even vanilla seq2seq architecture with word level atten-

tion outperform the most competitive statistical model by a
great margin. (2)The proposed structure-aware seq2seq ar-
chitecture can further improve the table-to-text generation
compared with the competitive vanilla seq2seq. Dual atten-
tion mechanism is able to boost the model performance by
over 1 BLEU compared to vanilla attention mechanism.

Research on Disordered Tables

We view a structured table as a set of ﬁeld-value records and
then feed the records into the generator sequentially as the
order they are presented in the table. The order of records
can guide the description generator to produce an introduc-
tion in the pre-deﬁned schemas (Vinyals, Bengio, and Kud-
lur 2015). However, not all the tables are arranged in the
proper order. So global addressing between the generated
descriptions and the records of the table is necessary for
table-to-text generation.

Furthermore, the schemas of various types of tables dif-
fer greatly from each other. A biography about a politician
may emphasize his or her social activities and working ex-
perience while a biography of a soccer player is likely to
highlight which team he or she used to serve in or the per-
formance in his or her career. To cope with various schemas
of different tables, it’s essential to model inter-record infor-
mation within the tables by global addressing.

For these reasons, we propose a pair of disordered train-
ing and testing set based on WIKIBIO by randomly shuf-
ﬂing the records of a infobox. For example, the order of
several records in a speciﬁc infobox is ‘name-birthdate-
occupation-spouse’, we randomly shufﬂe the table records
as ‘occupation-name-spouse-birthdate’, without changing
the ﬁeld content inside the ‘occupation’, ‘name’, ‘spouse’
and ‘birthdate’ records.

Figure 5: The generated descriptions for Binky Jones and the corresponding reference in the Wikipedia. Our proposed struct-
aware seq2seq model can generate more informative and accurate description compared to vanilla seq2seq model.

Table 4 shows that all three neural network models per-
form not as good as before, which means the order of ta-
ble records is an essential aspect for table-to-text generation.
However, the BLEU and ROUGE decreases on the structure-
aware seq2seq model are much smaller than the other two
models, which proves the efﬁciency of global addressing
mechanism.

Model
Seq2seq
+ ﬁeld & pos
Structure-aware

BLEU
40.04 (-2.02)
42.10 (-1.55)
44.28 (-0.61)

ROUGE
36.85 (-1.21)
38.97 (-1.35)
40.79 (-0.42)

Table 4: Experiments on the disordered tables to show the
efﬁciency of global addressing.

Qualitative Analysis

Analysis on Dual Attention
Dual attention mechanism models the relationship between
the generated tokens and table content inside each record by
word level attention while encoding the relevance of gener-
ated description and inter-record information within the ta-
ble by ﬁeld level attention. The aggregation of word level
attention and ﬁeld level attention can model more precise
connection between the table and its generated description.
Fig 4 shows an example of the three attention mecha-
nisms while generating a piece of description for Fr´ed´eric
Fonteyne based on his Wikipedia infobox. We can ﬁnd out
that the name, birthdate, nationality and occupation informa-
tion contained in the generated sentence can properly refer
to the related table content by the aggregated dual attention.

Case Study
Fig 5 shows the generated descriptions for different variants
of our model based on the related Wikipedia infobox. All

three neural network generators can produce coherent and
understandable sentences with the help of local addressing
mechanism. All of them contain the word ‘baseball’ which
is not directly mentioned in the infobox. It means the genera-
tors deduce from table content that Binky Jones is a baseball
player.

However, the two vanilla seq2seq models also generate
‘major league baseball’ or ‘major leagues’ which are not
mentioned in the table and probably not correct. Vanilla
seq2seq model without global addressing on the table just
generates the most possible league in Wikipedia for a base-
ball player to play in.

Furthermore, the two biographies generated by vanilla
seq2seq model fail to contain the information from the in-
fobox which team he served in, as well as the time period
of his playing in that team. The biography generated by
our proposed structure-aware seq2seq model is able to cover
nearly all the information mentioned in the table. The gen-
erated segment ‘who played shortstop from april 15 to april
27 for the brooklyn robins in 1924’ (15 words) includes in-
formation in ﬁve ﬁelds of the table: ‘position’, ‘debutdate’,
‘ﬁnaldate’, ‘debutteam’ and ‘ﬁnalteam’, which is achieved
by the global addressing between the ﬁelds and the gener-
ated tokens.

Conclusions

We propose a structure-aware seq2seq architecture to encode
both the content and the structure of a table for table-to-text
generation. The model consists of ﬁeld-gating encoder and
description generator with dual attention. We add a ﬁeld gate
to the encoder LSTM unit to incorporate the ﬁeld informa-
tion. Furthermore, dual attention mechanism which contains
word level attention and ﬁeld level attention can operate lo-
cal and global addressing to the content and the structure of a
table. A series of visualizations, case studies and generation
assessments show that our model outperforms the competi-
tive baselines by a large margin.

Acknowledgments
Our work is
supported by the National Key Re-
search and Development Program of China under Grant
No.2017YFB1002101 and project 61772040 supported by
NSFC. The corresponding authors of this paper are Baobao
Chang and Zhifang Sui.

References
Angeli, G.; Liang, P.; and Klein, D. 2010. A simple domain-
In Pro-
independent probabilistic approach to generation.
ceedings of the 2010 Conference on Empirical Methods in
Natural Language Processing, 502–512. Association for
Computational Linguistics.
Barzilay, R., and Lapata, M. 2005. Collective content se-
lection for concept-to-text generation. In Proceedings of the
conference on Human Language Technology and Empirical
Methods in Natural Language Processing, 331–338. Asso-
ciation for Computational Linguistics.
Belz, A. 2008. Automatic generation of weather forecast
texts using comprehensive probabilistic generation-space
models. Natural Language Engineering 14(4):431–455.
Chen, D. L., and Mooney, R. J. 2008. Learning to sportscast:
In Proceedings
a test of grounded language acquisition.
of the 25th international conference on Machine learning,
128–135. ACM.
Duboue, P. A., and McKeown, K. R. 2002. Content planner
construction via evolutionary algorithms and a corpus-based
ﬁtness function. In Proceedings of INLG 2002, 89–96.
Graves, A.; Mohamed, A.-r.; and Hinton, G. 2013. Speech
recognition with deep recurrent neural networks. In Acous-
tics, speech and signal processing (icassp), 2013 ieee inter-
national conference on, 6645–6649. IEEE.
Gyawali, B. 2016. Surface Realisation from Knowledge
Bases. Ph.D. Dissertation, Universite de Lorraine.
Heaﬁeld, K.; Pouzyrevsky, I.; Clark, J. H.; and Koehn, P.
2013. Scalable modiﬁed kneser-ney language model esti-
mation. In ACL (2), 690–696.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term
memory. Neural computation 9(8):1735–1780.
Kim, J., and Mooney, R. J. 2010. Generative alignment
and semantic parsing for learning from ambiguous supervi-
sion. In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, 543–551. Associa-
tion for Computational Linguistics.
Konstas, I., and Lapata, M. 2012. Unsupervised concept-
to-text generation with hypergraphs. In Proceedings of the
2012 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, 752–761. Association for Computational Lin-
guistics.
Konstas, I., and Lapata, M. 2013. A global model for
concept-to-text generation. Journal of Artiﬁcial Intelligence
Research 48:305–346.
Lebret, R.; Grangier, D.; and Auli, M. 2016. Neural text
generation from structured data with application to the biog-
raphy domain. arXiv preprint arXiv:1603.07771.

Liang, P.; Jordan, M. I.; and Klein, D. 2009. Learning se-
mantic correspondences with less supervision. In Proceed-
ings of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Volume 1-Volume
1, 91–99. Association for Computational Linguistics.
Lu, W., and Ng, H. T. 2011. A probabilistic forest-to-string
model for language generation from typed lambda calculus
expressions. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, 1611–1622.
Association for Computational Linguistics.
Lu, W.; Ng, H. T.; and Lee, W. S. 2009. Natural language
generation with tree conditional random ﬁelds. In Proceed-
ings of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 1-Volume 1, 400–409.
Association for Computational Linguistics.
Luong, M.-T.; Sutskever, I.; Le, Q. V.; Vinyals, O.; and
Zaremba, W. 2014. Addressing the rare word problem in
neural machine translation. arXiv preprint arXiv:1410.8206.
Ma, S.; Sun, X.; Xu, J.; Wang, H.; Li, W.; and Su, Q.
2017.
Improving semantic relevance for sequence-to-
sequence learning of chinese social media text summariza-
tion. In Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL 2017, Vancou-
ver, Canada, July 30 - August 4, Volume 2: Short Papers,
635–640.
Mei, H.; Bansal, M.; and Walter, M. R. 2015. What to
talk about and how? selective generation using lstms with
coarse-to-ﬁne alignment. arXiv preprint arXiv:1509.00838.
Ratnaparkhi, A. 2002. Trainable approaches to surface
natural language generation and their application to con-
versational dialog systems. Computer Speech & Language
16(3):435–455.
Reiter, E., and Dale, R. 2000. Building natural language
generation systems. Cambridge university press.
Sha, L.; Mou, L.; Liu, T.; Poupart, P.; Li, S.; Chang, B.; and
Sui, Z. 2017. Order-planning neural text generation from
structured data. CoRR abs/1709.00155.
Stent, A.; Prasad, R.; and Walker, M. 2004. Trainable sen-
tence planning for complex information presentation in spo-
ken dialog systems. In Proceedings of the 42nd annual meet-
ing on association for computational linguistics, 79. Asso-
ciation for Computational Linguistics.
Vinyals, O.; Bengio, S.; and Kudlur, M.
matters: Sequence to sequence for sets.
arXiv:1511.06391.
Walker, M. A.; Rambow, O.; and Rogati, M. 2001. Spot:
A trainable sentence planner. In Proceedings of the second
meeting of the North American Chapter of the Association
for Computational Linguistics on Language technologies,
1–8. Association for Computational Linguistics.
Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi-
nov, R.; Zemel, R.; and Bengio, Y. 2015. Show, attend and
tell: Neural image caption generation with visual attention.
In International Conference on Machine Learning, 2048–
2057.

2015. Order
arXiv preprint

Table-to-text Generation by Structure-aware Seq2seq Learning

Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang and Zhifang Sui
Key Laboratory of Computational Linguistics, Ministry of Education,
School of Electronics Engineering and Computer Science, Peking University, Beijing, China
{tianyu0421, wkx, shalei, chbb, szf}@pku.edu.cn

7
1
0
2
 
v
o
N
 
7
2
 
 
]
L
C
.
s
c
[
 
 
1
v
4
2
7
9
0
.
1
1
7
1
:
v
i
X
r
a

Abstract

Table-to-text generation aims to generate a description for
a factual table which can be viewed as a set of ﬁeld-value
records. To encode both the content and the structure of a
table, we propose a novel structure-aware seq2seq architec-
ture which consists of ﬁeld-gating encoder and description
generator with dual attention. In the encoding phase, we up-
date the cell memory of the LSTM unit by a ﬁeld gate and
its corresponding ﬁeld value in order to incorporate ﬁeld in-
formation into table representation. In the decoding phase,
dual attention mechanism which contains word level atten-
tion and ﬁeld level attention is proposed to model the seman-
tic relevance between the generated description and the ta-
ble. We conduct experiments on the WIKIBIO dataset which
contains over 700k biographies and corresponding infoboxes
from Wikipedia. The attention visualizations and case stud-
ies show that our model is capable of generating coherent
and informative descriptions based on the comprehensive un-
derstanding of both the content and the structure of a table.
Automatic evaluations also show our model outperforms the
baselines by a great margin. Code for this work is available
on https://github.com/tyliupku/wiki2bio.

Introduction
Generating natural language description for a structured ta-
ble is an important task for text generation from struc-
tured data. Previous researches include weather forecast
based on a set of weather records (Liang, Jordan, and Klein
2009) and sportscasting based on temporally ordered events
(Chen and Mooney 2008). However, previous work models
the structured data in the limited pre-deﬁned schemas. For
example, a weather record rainChance(time:06:00-21:00,
mode:SSE, value:20) is represented by a ﬁxed-length one-
hot vector by its record type, record time, record value
and record value. To this end, we focus on table-to-text
generation which involves comprehensive representation for
the complex structure of a table rather than pre-deﬁned
schemas. In contrast to previous work experimented on
small datasets which contain only a few tens of thousands
of records such as WEATHERGOV (Liang, Jordan, and Klein
2009) and ROBOCUP (Chen and Mooney 2008), we fo-
cus on a more challenging task to generate biographies

Copyright c(cid:13) 2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Figure 1: The Wikipedia infobox of Charles Winstead, the
corresponding introduction on his wiki page reads “Charles
Winstead (1891 - 1973) was an FBI agent in the 1930s - 40s,
famous for being one of the agents who shot and killed John
Dillinger.”.

based on the Wikipedia infoboxes. As shown in Fig 1, a
biographic infobox is a ﬁxed-format table that describes a
person with many ﬁeld-value records like (Name, Charles
B. Winstead), (Nationality, American), (Occupation, FBI
Agent), etc. We utilize WIKIBIO dataset proposed by Le-
bret, Grangier, and Auli (2016) which contains 700k biogra-
phies from Wikipedia, with 400k words in total as the bench-
mark dataset.

Previous work has made signiﬁcant progress on this
task. Lebret, Grangier, and Auli (2016) proposed a statis-
tical n-gram model with local and global conditioning on a
Wikipedia infobox. However the ﬁeld content of a record
is likely to be a sequence of words, the statistical language
model is not good at capturing long-range dependencies be-
tween words. Mei, Bansal, and Walter (2015) proposed a
selective generation method based on an encoder-aligner-
decoder framework. The model utilizes a sparse one-hot vec-
tor to represent a weather record. However it’s inefﬁcient to
represent the complex structure of a table by one-hot vec-
tors.

We propose a structure-aware sequence to sequence
(seq2seq) generation framework to model both content and
structure of the table by local and global addressing. When a
human writes a biography for a person based on the related

Wikipedia infobox, he will ﬁrstly determine which records
in the table should be included in the introduction and how
to arrange the order of these records before wording. After
that, the writer will further consider which words or phrases
in the table should be more focused on to paraphrase. We
summarize the two phases of generation as two scopes of
addressing: local and global addressing. Local addressing
determines which particular word in the table should be fo-
cused on while generating a piece of description at certain
time step. However, the word level addressing can not fully
address the table-to-text generation problem as the factual
tables usually have complex structures which might confuse
the generator. Global addressing is proposed to determine
which records of the table should be more focused on while
generating corresponding description. Global addressing is
necessary as the description of a table may not cover all the
records. For example, the ‘cause of death’ ﬁeld in Fig 1 is
not mentioned in the description. Furthermore, the order of
records in the tables may not always be homogeneous. For
example, we can introduce a person as an order of his (Birth-
Death-Nationality-Occupation) according to his Wikipedia
infobox. However the other infoboxes may be arranged as
(Occupation-Nationality-Birth-Death). Local addressing is
realized by content encoding of the LSTM encoder and word
level attention while global addressing is realized by ﬁeld
encoding of the ﬁeld-gating LSTM variation and ﬁeld level
attention in our model.

The structure-aware seq2seq architecture we proposed
exploits encoder-decoder framework using long short-term
memory (LSTM) (Hochreiter and Schmidhuber 1997) units
with local and global addressing on the structured table. In
the encoding phase, our model ﬁrst encodes the sets of ﬁeld-
value records in the table by integrating ﬁeld information
and content representation. To make better use of ﬁeld in-
formation, we add a ﬁeld gate to the cell state of the encoder
LSTM unit to incorporate the ﬁeld embedding into the struc-
tural representation of the table. The model next employs
a LSTM decoder to generate natural language description
by the structural representation of the table. In the decoding
phase, we also propose a novel dual attention mechanism
which consists of two parts: word-level attention for local
addressing and ﬁeld-level attention for global addressing.

Our contributions are three-fold: (1) We propose an end-
to-end structure-aware encoder-decoder architecture to en-
code ﬁeld information into the representation of a structured
table. (2) Field-gating encoder and dual attention mecha-
nism are proposed to operate local and global addressing
between the content and the ﬁeld information of a structured
table. (3) Experiments on WIKIBIO dataset show that our
model achieves substantial improvement over baselines.

Related Work
Most generation systems can be divided into two indepen-
dent modules: (1)content selection involves choosing a sub-
set of relevant records in a table to talk about. (2)surface re-
alization is concerned with generating natural language de-
scriptions for this subset.

Many approaches have been proposed to learn the indi-
vidual modules. For content selection module, one approach

builds a content selection model by aligning records and
sentences (Barzilay and Lapata 2005; Duboue and McKe-
own 2002). A hierarchical semi-Markov method is proposed
by (Liang, Jordan, and Klein 2009) which ﬁrst associates
the text sequences to corresponding records and then gener-
ates corresponding descriptions from these records. Surface
realization is often treated as a concept-to-text generation
task from a given representation. Reiter and Dale (2000),
Walker, Rambow, and Rogati (2001) and Stent, Prasad, and
Walker (2001) utilize various linguistic features to train sen-
tence planners for sentence generation. Context-free gram-
mars are also used to generate natural language sentences
from formal meaning representations (Lu and Ng 2011; Belz
2008). Other effective approaches include hybrid alignment
tree (Kim and Mooney 2010), tree conditional random ﬁelds
(Lu, Ng, and Lee 2009), tree adjoining grammar (Gyawali
2016) and template extraction in a log-linear framework
(Angeli, Liang, and Klein 2010). Recent work combines
content selection and surface realization in a uniﬁed frame-
work (Ratnaparkhi 2002; Konstas and Lapata 2012; 2013;
Sha et al. 2017)

Our model borrowed the idea of representing a struc-
tured table by its ﬁeld and content information from (Le-
bret, Grangier, and Auli 2016). However, their n-gram model
is inefﬁcient to model long-range dependencies while gen-
erating descriptions. Mei, Bansal, and Walter (2015) also
proposed a seq2seq model with an aligner between weather
records and weather broadcast. The model used one-hot en-
coding to represent the weather records as they are relatively
simple and highly structured. However, the model is not
capable to represent the tables with complex structure like
Wikipedia infoboxes.

Task Deﬁnition
We model the table-to-text generation in an end-to-end
structure-aware seq2seq framework. The given table T can
be viewed as a combination of n ﬁeld-value records {
R1, R2, · · · , Rn}. Each record Ri consists of a sequence of
words { d1, d2, · · · , dm} and their corresponding ﬁeld rep-
resent { Zd1 , Zd2, · · · , Zdm}.

The output of the model is the generated description S
for table T which contains p tokens {w1, w2, · · · , wp} with
wt being the word at time t. We formulate the table-to-text
generation as the inference over a probabilistic model. The
goal of the inference is to generate a sequence w∗
1:p which
maximizes P (w1:p|R1:n).

w∗

1:p = arg max
w1:p

p
(cid:89)

t=1

P (wt|w0:t−1, R1:n)

(1)

Structure-aware Seq2seq

Field representation
A Wikipedia infobox can be viewed as a set of ﬁeld-value
records, in which values are sequences or segments of words
corresponding to certain ﬁelds. The structural representation
of an infobox consists of context embedding and ﬁeld em-
bedding. The context embedding is formulated as an em-

ht = ot (cid:12) tanh(ct)
(5)
where it, ft, ot ∈ [0, 1]n are input, forget and output gates
respectively, and ˆct and ct are proposed cell value and true
cell value in time t. n is the hidden size.

To make better understanding of the structure of a table,
the ﬁeld information should also be encoded into the en-
coder. One simple way is to take the concatenation of word
embedding and corresponding ﬁeld embedding as the input
for the vanilla LSTM unit. Actually, the method is indeed
proved to be useful in our experiments and serves as a base-
line for comparison. However, the concatenation of word
embedding and ﬁeld embedding only treats the ﬁeld infor-
mation as an additional label of certain token which loses
the structural information of the table.

To better encode the structural information of a table, we
propose a ﬁeld-gating variation on the vanilla LSTM unit to
update the cell memory by a ﬁeld gate and its corresponding
ﬁeld value. The ﬁeld-gating cell state is described as follows:

(cid:19)

(cid:18)lt
ˆzt

(cid:19)

(cid:18)sigmoid
tanh

=

W d

2n,2n (zt)

(6)

(cid:48)

c

t = ft (cid:12) ct−1 + it (cid:12) ˆct + lt (cid:12) ˆzt
where zt is the ﬁeld embedding described before, lt ∈
[0, 1]n is the ﬁeld gate to determine how much ﬁeld infor-
mation should be kept in the cell memory, ˆzt is the proposed
ﬁeld value corresponding to ﬁeld gate. The cell state c
t is up-
dated from the original ct by incorporating ﬁeld information
of the table.

(7)

(cid:48)

Description Decoder with Dual Attention
To conduct local and global addressing towards the struc-
tured table, we use LSTM architecture with dual attention
mechanism as our description generator. As deﬁned in the
equation 1, the generated token wt at time t in the decoder
is predicated based on all the previously generated tokens
w<t before wt, the hidden states H = {ht}L
t=1 of the table
encoder and the ﬁeld embeddings Z = {zt}L
t=1. To be more
speciﬁc:

P (wt|H, Z, w<t) = sof tmax(Ws (cid:12) gt)

gt = tanh(Wt[st, at])

st = LST M (wt−1, st−1)

(8)

(9)

(10)

where st is the t-th hidden state of the decoder calculated
by the LSTM unit. The computational details can be referred
in Equation 3, 4 and 5. at is the attention vector which is
widely used in many applications (Xu et al. 2015; Luong
et al. 2014; Ma et al. 2017). Vanilla attention mechanism
is proposed to encode the semantic relevance between the
encoder states {ht}L
t=1 and and the decoder state st at time
t. The attention vector is usually represented by the weighted
sum of encoder hidden states.

αti =

eg(st,hi)
j=1 eg(st,hj )

(cid:80)N

L
(cid:88)

i=1

; at =

αtihi

(11)

Figure 2: The wiki infobox of George Mikell (left) and the
table of its ﬁeld representation (right).

bedding for a segment of words in the ﬁeld content. The
ﬁeld embedding is a key point to label each word in the ﬁeld
content by its corresponding ﬁeld name and occurrence in
the table. Lebret, Grangier, and Auli (2016) represented the
ﬁeld embeddding Zw = {fw; pw} for a word w in the table
with corresponding ﬁeld name fw and position information
pw. The position information can be further represented as a
tuple (p+
w) which includes the positions of the token w
counted from the begining and the end of the ﬁeld respec-
tively. So the ﬁeld embedding of token w is extended to a
triple:

w, p−

Zw = {fw; p+

w; p−
w}

(2)

As shown in Fig 2, the infobox of George Mikell contains
several ﬁeld-value records, the ﬁeld content for the record
(birthname, Jurgis Mikelatitis) is ‘Jurgis Mikelatitis’. The
word ‘Jurgis’ is the ﬁrst token counted from the beginning
of the ﬁeld ‘birthname’ and also the second token counted
from the end. So the ﬁeld embedding for the word ‘Jurgis’
is described as {birthname; 1; 2}. Each token in the table
has an unique ﬁeld embedding even if there exists two same
words in the same ﬁeld due to the unique (ﬁeld, position)
pair.

Field-gating Table Encoder
The table encoder aims to encode each word dj in the table
together with its ﬁeld embedding Zdj into the hidden state
hj using LSTM encoder. We present a novel ﬁeld-gating
LSTM unit to incorporate ﬁeld information into table encod-
ing. LSTM is a recurrent neural network (RNN) architecture
which uses a vector of cell state ct and a set of element-wise
multiplication gates to control how information is stored,
forgotten and exploited inside the network. Following the
design for an LSTM cell in (Graves, Mohamed, and Hinton
2013) , the architecture used in the table encoder is deﬁned
by following equations:






it
ft
ot
ˆct






 =





 W c


sigmoid
sigmoid
sigmoid
tanh

(cid:19)

(cid:18) dt
ht−1

4n,2n

ct = ft (cid:12) ct−1 + it (cid:12) ˆct

(3)

(4)

Figure 3: The overall diagram of structure-aware seq2seq architecture for generating description for George Mikell in Fig 2.

where g(st, hi) is a relevant score between decoder hidden
state st and encoder hidden state hi. There are many differ-
ent ways to calculate the relevant scores. In our paper, we
use the following dot product to measure the similarity be-
tween st and hi. Ws, Wt, Wp, Wq are all model parameters.
g(st, hi) = tanh(Wphi) (cid:12) tanh(Wqst)
(12)
However, the word level attention described above can
only capture the semantic relevance between generated to-
kens and the content information in the table, ignoring the
structure information of the table. To fully utilize the struc-
ture information, we propose a higher level attention over
generated tokens and the ﬁeld embedding of the table. Field
level attention can locate the particular ﬁeld-value record
which should be focused on while generating next token in
the description by modeling the relevance between all ﬁeld
embeddings {zt}L
t=1 and the decoder state st at t-th time.
Field level attention weight βti is presented as Equation 13.
We use the same relevant score function g(st, zi) as equation
12. Dual attention weight γt is the element-wise production
between ﬁeld level attention weight βt and word level atten-
tion weight αt. The dual attention vector a
t is updated as
the weighted sum of encoder states {ht}t=1 by γt (Equation
15):

(cid:48)

βti =

eg(st,zi)
j=1 eg(st,zj )

(cid:80)N

g(st, zi) = tanh(Wxzi) (cid:12) tanh(Wyst)

γti =

αti · βti
j=1 αtj · βtj

(cid:80)N

(cid:48)
; a

t =

γtihi

L
(cid:88)

i=1

(13)

(14)

(15)

Furthermore, we utilize a post-process operation for the
generated unknown (UNK) tokens to alleviate the out-of-
vocabulary (OOV) problem. We replace a speciﬁc generated
UNK token with the most relevant token in the correspond-
ing table according to the related dual attention matrix.

Local and Global Addressing

Local and global addressing determine which part of the ta-
ble should be more focused on in different steps of descrip-
tion generation. The two scopes of addressings play a very
important role in understanding and representing the inner-
structure of a table. Next we will introduce how our model
conducts local and global addressing on table-to-text gener-
ation with the help of Fig 3.

Local addressing: A table can be treated as a set of ﬁeld-
value records. Local addressing tends to encode the table
content inside each record. The value in each ﬁeld-value
record is a sequence of words which contains 2.7 tokens
on average. Some records in the Wikipedia infoboxes even
contain several phrases or sentences. Previous models which
used one-hot encoding or statistical language model to en-
code ﬁeld content are inefﬁcient to capture the semantic rel-
evance between words inside a ﬁeld. The seq2seq structure
itself has a strong ability to model the context of a piece of
words. For one thing, the LSTM encoder can capture long-
range dependencies between words in the table. For another,
the word level attention of the proposed dual attention mech-
anism can also build a connection between the words in the
description and the tokens in the table. The generated word
‘actor’ in Fig 3 refers to the word ‘actor’ in the ‘Occupa-
tion’ ﬁeld.

Global addressing: The goal of local addressing is to
represent inner-record information while global addressing
aims to model inter-record relevance within the table. For
example, it’s noteworthy that the generated token ‘actor’ in
Fig 3 is mapped to the ‘occupation’ ﬁeld in Table 2.

Field-gating table representation and ﬁeld level attention
mechanism are proposed for global addressing. For table
representation, we encode the structure of a table by incor-
porating ﬁeld and position embedding into table representa-
tion apart from word embedding. The position of a token in

Mean

# tokens per sentence
26.1

# table token per sent.
9.5

# tokens per table
53.1

# ﬁelds per table
19.7

Table 1: Statistics of WIKIBIO dataset.

Word dimension
400

Field dimension
50

Position dimension Hidden size Batch size

Learning rate Optimizer

5

500

32

0.0005

Adam

Table 2: Parameter settings of our experiments.

the ﬁeld content of a table is determined only by its ﬁeld and
position information. Even two same words in the table can
be distinguished by their ﬁeld and position. We propose a
novel ﬁeld-gating LSTM to incorporate the ﬁeld embedding
into the cell memory of LSTM unit.

Furthermore, the information in a table is likely to be re-
dundant. Some records in the table are unimportant or even
useless for generating description. We should make appro-
priate choices on selecting useful information from all the ta-
ble records. The order of records may also inﬂuence the per-
formance of generation (Vinyals, Bengio, and Kudlur 2015).
We should make it clear which records the token to be gen-
erated is focused on by global addressing between the ﬁeld
information of a table and its description. The ﬁeld level at-
tention of dual attention mechanism is introduced to deter-
mine which ﬁeld the generator focused on in certain time
step. Experiments show that our dual attention mechanism
is of great help to generate description from certain table
and insensible to different orders of table records.

Experiments
We ﬁrst introduce the dataset, evaluation metrics and exper-
imental setups in our experiments. Then we compare our
model with several baselines. After that, we assess the per-
formance of our model on table-to-text generation. Further-
more, we also conduct experiments on the disordered tables
to show the efﬁciency of global addressing mechanism.

Dataset and Evaluation Metrics
We use WIKBIO dataset proposed by Lebret, Grangier, and
Auli (2016) as the benchmark dataset. WIKBIO contains
728,321 articles from English Wikipedia (Sep 2015). The
dataset uses the ﬁrst sentence of each article as the descrip-
tion of the corresponding infobox. Table 1 summarizes the
dataset statistics: on average, the tokens in the table (53.1)
are twice as long as those in the ﬁrst sentence (26.1). 9.5
tokens in the description text also occur in the table. The
corpus has been divided in to training (80%), testing (10%)
and validation (10%) sets.

We assess the generation quality automatically with

BLEU-4 and ROUGE-4 (F measure)1 .

Baselines
We compare the proposed structure-aware seq2seq model
with several statistical language models and the vanilla
encoder-decoder model. The baselines are listed as follows:

1We use standard scripts NIST mteval-v13a.pl (for BLEU), and

rouge-1.5.5 (for ROUGE).

• KN: The Kneser-Ney (KN) model is a widely used lan-
guage model proposed by Heaﬁeld et al. (2013). We use
the KenLM toolkit to train 5-gram models without prun-
ing.

• Template KN: Template KN is a KN model over
templates which also serves as a baseline in (Lebret,
Grangier, and Auli 2016). The model
replaces the
words occurring in both the table and the training
sentences with a special token reﬂecting its ﬁeld. The
introduction section of the table in Fig 2 looks as
follows under this scheme: “ name 1 name 2 (born
birthname 1 ... birthdate 3) is a Lithuanian-
Australian occupation 1 and occupation 3
best known for his performances in known for 1
... known for 4 (1961) and known for 5 ...
known for 7 (1963) ”. During inference, the decoder
is constrained to emit words from the regular vocabulary
or special tokens occurring in the input table.

• NLM: A naive statistical language model proposed by
(Lebret, Grangier, and Auli 2016) for comparison. The
model uses only the ﬁeld content as input without ﬁeld
and position information.

• Table NLM: The most competitive statistical language
model proposed by (Lebret, Grangier, and Auli 2016),
which includes local and global conditioning over the ta-
ble by integrating related ﬁeld and position embedding
into the table representation.

• Vanilla Seq2seq: The vanilla seq2seq neural architecture
is also provided as a strong baseline which uses the con-
catenation of word embedding, ﬁeld embedding and po-
sition embedding as the model input. The model can op-
erate local addressing over the table by the natural advan-
tages of LSTM units and word level attention mechanism.

Experiment Setup
In the table encoding phase, we use a sequence of word em-
beddings and their corresponding ﬁeld and position embed-
ding as input. We select the most frequent 20,000 words in
the training set as the word vocabulary. For ﬁeld embedding,
we select 1480 ﬁelds occurring more than 100 times from
the training set as ﬁeld vocabulary. Additionally, we ﬁlter all
empty ﬁelds whose values are (cid:104)none(cid:105) while feeding ﬁeld in-
formation to the network. We also limit the largest position
number as 30. Any position number over 30 will be counted
as 30.

While generating description for the table, a special start
token (cid:104)sos(cid:105) is feed into the generator in the beginning of the

Figure 4: An example of word level, ﬁeld level and aggregated dual attention on generating the biography of Fr´ed´eric Fonteyne.
Note there are two adjacent ‘belgium’s in ‘birthplace-3’ and ‘nationality-1’ ﬁeld, respectively. The word level attention focuses
improperly on the ﬁrst ‘belgium’ while generating ‘a belgian ﬁlm director’. In contrast, the ﬁeld level attention and dual
attention can locate the second ‘belgium’ properly by word-ﬁeld modeling (marked in the black boxes).

Model
KN
Template KN
NLM
Table NLM
Seq2seq
+ ﬁeld (concate)
+ pos (concate)
Field-gating Seq2seq
+ dual attention
+ beam search (k=5)

BLEU
2.21
19.80
4.17 ± 0.54
34.70 ± 0.36
42.06 ± 0.32
43.34 ± 0.37
43.65 ± 0.44
43.74 ± 0.23
44.89 ± 0.33
44.71

ROUGE
0.38
10.70
1.48 ± 0.23
25.80 ± 0.36
38.06 ± 0.36
39.84 ± 0.32
40.32 ± 0.23
40.53 ± 0.31
41.21 ± 0.25
41.65

Table 3: BLEU-4 and ROUGE-4 for structure-aware
seq2seq model (last three rows), statistical language model
(ﬁrst four rows) and vanilla seq2seq model with ﬁeld and
position input (three rows in the middle).

decoding phase. Then we use the last generated token as the
input at the next time step. A special end token (cid:104)eos(cid:105) is used
to mark the end of decoding. We also restrict the generated
text by a pre-deﬁned max length to avoid redundant or irrel-
evant generation. We also try beam search with beam size
2-10 to enhance the performance. We use grid search to de-
termine the parameters of our model. The detail of model
parameters is listed in Table 2.

Generation Assessment
The assessment for description generation is listed in Ta-
ble 3. We have following observations: (1) Neural network
models perform much better than statistical language mod-
els. Even vanilla seq2seq architecture with word level atten-

tion outperform the most competitive statistical model by a
great margin. (2)The proposed structure-aware seq2seq ar-
chitecture can further improve the table-to-text generation
compared with the competitive vanilla seq2seq. Dual atten-
tion mechanism is able to boost the model performance by
over 1 BLEU compared to vanilla attention mechanism.

Research on Disordered Tables

We view a structured table as a set of ﬁeld-value records and
then feed the records into the generator sequentially as the
order they are presented in the table. The order of records
can guide the description generator to produce an introduc-
tion in the pre-deﬁned schemas (Vinyals, Bengio, and Kud-
lur 2015). However, not all the tables are arranged in the
proper order. So global addressing between the generated
descriptions and the records of the table is necessary for
table-to-text generation.

Furthermore, the schemas of various types of tables dif-
fer greatly from each other. A biography about a politician
may emphasize his or her social activities and working ex-
perience while a biography of a soccer player is likely to
highlight which team he or she used to serve in or the per-
formance in his or her career. To cope with various schemas
of different tables, it’s essential to model inter-record infor-
mation within the tables by global addressing.

For these reasons, we propose a pair of disordered train-
ing and testing set based on WIKIBIO by randomly shuf-
ﬂing the records of a infobox. For example, the order of
several records in a speciﬁc infobox is ‘name-birthdate-
occupation-spouse’, we randomly shufﬂe the table records
as ‘occupation-name-spouse-birthdate’, without changing
the ﬁeld content inside the ‘occupation’, ‘name’, ‘spouse’
and ‘birthdate’ records.

Figure 5: The generated descriptions for Binky Jones and the corresponding reference in the Wikipedia. Our proposed struct-
aware seq2seq model can generate more informative and accurate description compared to vanilla seq2seq model.

Table 4 shows that all three neural network models per-
form not as good as before, which means the order of ta-
ble records is an essential aspect for table-to-text generation.
However, the BLEU and ROUGE decreases on the structure-
aware seq2seq model are much smaller than the other two
models, which proves the efﬁciency of global addressing
mechanism.

Model
Seq2seq
+ ﬁeld & pos
Structure-aware

BLEU
40.04 (-2.02)
42.10 (-1.55)
44.28 (-0.61)

ROUGE
36.85 (-1.21)
38.97 (-1.35)
40.79 (-0.42)

Table 4: Experiments on the disordered tables to show the
efﬁciency of global addressing.

Qualitative Analysis

Analysis on Dual Attention
Dual attention mechanism models the relationship between
the generated tokens and table content inside each record by
word level attention while encoding the relevance of gener-
ated description and inter-record information within the ta-
ble by ﬁeld level attention. The aggregation of word level
attention and ﬁeld level attention can model more precise
connection between the table and its generated description.
Fig 4 shows an example of the three attention mecha-
nisms while generating a piece of description for Fr´ed´eric
Fonteyne based on his Wikipedia infobox. We can ﬁnd out
that the name, birthdate, nationality and occupation informa-
tion contained in the generated sentence can properly refer
to the related table content by the aggregated dual attention.

Case Study
Fig 5 shows the generated descriptions for different variants
of our model based on the related Wikipedia infobox. All

three neural network generators can produce coherent and
understandable sentences with the help of local addressing
mechanism. All of them contain the word ‘baseball’ which
is not directly mentioned in the infobox. It means the genera-
tors deduce from table content that Binky Jones is a baseball
player.

However, the two vanilla seq2seq models also generate
‘major league baseball’ or ‘major leagues’ which are not
mentioned in the table and probably not correct. Vanilla
seq2seq model without global addressing on the table just
generates the most possible league in Wikipedia for a base-
ball player to play in.

Furthermore, the two biographies generated by vanilla
seq2seq model fail to contain the information from the in-
fobox which team he served in, as well as the time period
of his playing in that team. The biography generated by
our proposed structure-aware seq2seq model is able to cover
nearly all the information mentioned in the table. The gen-
erated segment ‘who played shortstop from april 15 to april
27 for the brooklyn robins in 1924’ (15 words) includes in-
formation in ﬁve ﬁelds of the table: ‘position’, ‘debutdate’,
‘ﬁnaldate’, ‘debutteam’ and ‘ﬁnalteam’, which is achieved
by the global addressing between the ﬁelds and the gener-
ated tokens.

Conclusions

We propose a structure-aware seq2seq architecture to encode
both the content and the structure of a table for table-to-text
generation. The model consists of ﬁeld-gating encoder and
description generator with dual attention. We add a ﬁeld gate
to the encoder LSTM unit to incorporate the ﬁeld informa-
tion. Furthermore, dual attention mechanism which contains
word level attention and ﬁeld level attention can operate lo-
cal and global addressing to the content and the structure of a
table. A series of visualizations, case studies and generation
assessments show that our model outperforms the competi-
tive baselines by a large margin.

Acknowledgments
Our work is
supported by the National Key Re-
search and Development Program of China under Grant
No.2017YFB1002101 and project 61772040 supported by
NSFC. The corresponding authors of this paper are Baobao
Chang and Zhifang Sui.

References
Angeli, G.; Liang, P.; and Klein, D. 2010. A simple domain-
In Pro-
independent probabilistic approach to generation.
ceedings of the 2010 Conference on Empirical Methods in
Natural Language Processing, 502–512. Association for
Computational Linguistics.
Barzilay, R., and Lapata, M. 2005. Collective content se-
lection for concept-to-text generation. In Proceedings of the
conference on Human Language Technology and Empirical
Methods in Natural Language Processing, 331–338. Asso-
ciation for Computational Linguistics.
Belz, A. 2008. Automatic generation of weather forecast
texts using comprehensive probabilistic generation-space
models. Natural Language Engineering 14(4):431–455.
Chen, D. L., and Mooney, R. J. 2008. Learning to sportscast:
In Proceedings
a test of grounded language acquisition.
of the 25th international conference on Machine learning,
128–135. ACM.
Duboue, P. A., and McKeown, K. R. 2002. Content planner
construction via evolutionary algorithms and a corpus-based
ﬁtness function. In Proceedings of INLG 2002, 89–96.
Graves, A.; Mohamed, A.-r.; and Hinton, G. 2013. Speech
recognition with deep recurrent neural networks. In Acous-
tics, speech and signal processing (icassp), 2013 ieee inter-
national conference on, 6645–6649. IEEE.
Gyawali, B. 2016. Surface Realisation from Knowledge
Bases. Ph.D. Dissertation, Universite de Lorraine.
Heaﬁeld, K.; Pouzyrevsky, I.; Clark, J. H.; and Koehn, P.
2013. Scalable modiﬁed kneser-ney language model esti-
mation. In ACL (2), 690–696.
Hochreiter, S., and Schmidhuber, J. 1997. Long short-term
memory. Neural computation 9(8):1735–1780.
Kim, J., and Mooney, R. J. 2010. Generative alignment
and semantic parsing for learning from ambiguous supervi-
sion. In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, 543–551. Associa-
tion for Computational Linguistics.
Konstas, I., and Lapata, M. 2012. Unsupervised concept-
to-text generation with hypergraphs. In Proceedings of the
2012 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, 752–761. Association for Computational Lin-
guistics.
Konstas, I., and Lapata, M. 2013. A global model for
concept-to-text generation. Journal of Artiﬁcial Intelligence
Research 48:305–346.
Lebret, R.; Grangier, D.; and Auli, M. 2016. Neural text
generation from structured data with application to the biog-
raphy domain. arXiv preprint arXiv:1603.07771.

Liang, P.; Jordan, M. I.; and Klein, D. 2009. Learning se-
mantic correspondences with less supervision. In Proceed-
ings of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP: Volume 1-Volume
1, 91–99. Association for Computational Linguistics.
Lu, W., and Ng, H. T. 2011. A probabilistic forest-to-string
model for language generation from typed lambda calculus
expressions. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, 1611–1622.
Association for Computational Linguistics.
Lu, W.; Ng, H. T.; and Lee, W. S. 2009. Natural language
generation with tree conditional random ﬁelds. In Proceed-
ings of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 1-Volume 1, 400–409.
Association for Computational Linguistics.
Luong, M.-T.; Sutskever, I.; Le, Q. V.; Vinyals, O.; and
Zaremba, W. 2014. Addressing the rare word problem in
neural machine translation. arXiv preprint arXiv:1410.8206.
Ma, S.; Sun, X.; Xu, J.; Wang, H.; Li, W.; and Su, Q.
2017.
Improving semantic relevance for sequence-to-
sequence learning of chinese social media text summariza-
tion. In Proceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL 2017, Vancou-
ver, Canada, July 30 - August 4, Volume 2: Short Papers,
635–640.
Mei, H.; Bansal, M.; and Walter, M. R. 2015. What to
talk about and how? selective generation using lstms with
coarse-to-ﬁne alignment. arXiv preprint arXiv:1509.00838.
Ratnaparkhi, A. 2002. Trainable approaches to surface
natural language generation and their application to con-
versational dialog systems. Computer Speech & Language
16(3):435–455.
Reiter, E., and Dale, R. 2000. Building natural language
generation systems. Cambridge university press.
Sha, L.; Mou, L.; Liu, T.; Poupart, P.; Li, S.; Chang, B.; and
Sui, Z. 2017. Order-planning neural text generation from
structured data. CoRR abs/1709.00155.
Stent, A.; Prasad, R.; and Walker, M. 2004. Trainable sen-
tence planning for complex information presentation in spo-
ken dialog systems. In Proceedings of the 42nd annual meet-
ing on association for computational linguistics, 79. Asso-
ciation for Computational Linguistics.
Vinyals, O.; Bengio, S.; and Kudlur, M.
matters: Sequence to sequence for sets.
arXiv:1511.06391.
Walker, M. A.; Rambow, O.; and Rogati, M. 2001. Spot:
A trainable sentence planner. In Proceedings of the second
meeting of the North American Chapter of the Association
for Computational Linguistics on Language technologies,
1–8. Association for Computational Linguistics.
Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi-
nov, R.; Zemel, R.; and Bengio, Y. 2015. Show, attend and
tell: Neural image caption generation with visual attention.
In International Conference on Machine Learning, 2048–
2057.

2015. Order
arXiv preprint

