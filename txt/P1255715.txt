Linear Maximum Margin Classiﬁer for Learning
from Uncertain Data

Christos Tzelepis, Student Member, IEEE, Vasileios Mezaris, Senior Member, IEEE,
and Ioannis Patras, Senior Member, IEEE

1

(cid:70)

Abstract—In this paper, we propose a maximum margin classiﬁer that deals with uncertainty in data input. More speciﬁcally, we reformulate the
SVM framework such that each training example can be modeled by a multi-dimensional Gaussian distribution described by its mean vector and
its covariance matrix – the latter modeling the uncertainty. We address the classiﬁcation problem and deﬁne a cost function that is the expected
value of the classical SVM cost when data samples are drawn from the multi-dimensional Gaussian distributions that form the set of the training
examples. Our formulation approximates the classical SVM formulation when the training examples are isotropic Gaussians with variance tending
to zero. We arrive at a convex optimization problem, which we solve efﬁciently in the primal form using a stochastic gradient descent approach. The
resulting classiﬁer, which we name SVM with Gaussian Sample Uncertainty (SVM-GSU), is tested on synthetic data and ﬁve publicly available and
popular datasets; namely, the MNIST, WDBC, DEAP, TV News Channel Commercial Detection, and TRECVID MED datasets. Experimental results
verify the effectiveness of the proposed method.

Index Terms—Classiﬁcation, convex optimization, Gaussian anisotropic uncertainty, large margin methods, learning with uncertainty, statistical
learning theory

7
1
0
2
 
v
o
N
 
9
1
 
 
]

G
L
.
s
c
[
 
 
2
v
2
9
8
3
0
.
4
0
5
1
:
v
i
X
r
a

1 INTRODUCTION

S UPPORT Vector Machine (SVM) has been shown to be a

powerful paradigm for pattern classiﬁcation. Its origins
can be traced back to [1]. Vapnik established the standard
regularized SVM algorithm for computing a linear discrim-
inative function that optimizes the margin between the
so called support vectors and the separating hyperplane.
Despite the fact that the standard SVM algorithm is a
well-studied and general framework for statistical learning
analysis, it is still an active research ﬁeld (e.g., [2], [3]).

However, the classical SVM formulation, as well as the
majority of classiﬁcation methods, do not explicitly model
input uncertainty. In standard SVM, each training datum is
a vector, whose position in the feature space is considered
certain. This does not model the fact that measurement
inaccuracies or artifacts of the feature extraction process
contaminate the training examples with noise. In several
cases the noise distribution is known or can be modeled;
e.g., there are cases where each training example represents
the average of several measurements or of several samples
whose distribution around the mean can be modeled or
estimated. Finally, in some cases it is possible to model
the process by which the data is generated, for example by
modeling the process by which new data is generated from
transforms applied on an already given training dataset.

C. Tzelepis is with the School of Electronic Engineering and Computer Science,
Queen Mary University of London, London E1 4NS, U.K. and also with the
Information Technologies Institute/Centre for Research and Technology Hellas
(CERTH), Thermi 57001, Greece, (email: c.tzelepis@qmul.ac.uk).
V. Mezaris is with the Information Technologies Institute/Centre for Re-
search and Technology Hellas (CERTH), Thermi 57001, Greece (email:
bmezaris@iti.gr).
I. Patras is with the School of Electronic Engineering and Computer Sci-
ence, Queen Mary University of London, London E1 4NS, U.K. (e-mail:
i.patras@qmul.ac.uk).

Fig. 1: Linear SVM with Gaussian Sample Uncertainty
(LSVM-GSU). The solid line depicts the decision boundary
of the proposed algorithm, and the dashed line depicts the
decision boundary of the standard linear SVM (LSVM).

In this work, we consider that our training examples
are multivariate Gaussian distributions with known means
and covariance matrices – each example having a different
covariance matrix expressing the uncertainty around its
mean. An illustration is given in Fig. 1, where the shaded
regions are bounded by iso-density loci of the Gaussians,
and the means of the Gaussians for examples of the positive
and negative classes are located at × and ◦ respectively. A
classical SVM formulation would consider only the means
of the Gaussians as training examples and, by optimizing
the soft margin using the hinge loss and a regularization
term, would arrive at the separating hyperplane depicted by
the dashed line. In our formulation, we optimize for the soft

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

margin using the same regularization but the expected value
of the hinge loss, where the expectation is taken under the
given Gaussians. By doing so, we take into consideration
the various uncertainties and arrive at a drastically different
decision border, depicted by the solid line in Fig. 1. It is
worth noting that one would arrive at the same decision
border with the classical SVM trained on a dataset contain-
ing samples drawn from the Gaussians in question, as the
number of samples tend to inﬁnity. In addition, our method
degenerates to a classical SVM in the case that all of the
Gaussians are isotropic with a variance that tends to zero.

Our work differs from previous works that model uncer-
tainty in the SVM framework either by considering isotropic
noise or by using expensive sampling schemes to approx-
imate their loss functions. By contrast, our formulation
allows for full covariance matrices that can be different
for each example. This allows dealing, among others, with
cases where the uncertainty of only a few examples, and/or
the uncertainty along only a few of their dimensions, is
known or modeled. In the experimental results section we
show several real-world problems in which such modeling
is beneﬁcial. More speciﬁcally, we show cases, in which the
variances along (some) of the dimensions are part of the
dataset – this includes medical data where both the means
and the variances of several measurements are reported,
and large scale video datasets, where the means and the
variances of some of the features that are extracted at several
time instances in the video in question are reported. We
then show a case in which means and variances are a by-
product of the feature extraction method, namely the Welch
method for extracting periodograms from temporal EEG
data. And ﬁnally, we show a case in which, for an image
dataset (MNIST) we model the distribution of images under
small geometric transforms as Gaussians, using a ﬁrst-order
Taylor approximation to arrive in an analytic form.

In general, modeling of the uncertainty is a domain-
and/or dataset-speciﬁc problem, and in this respect, sim-
ilarly to all of the other methods in the literature that
model/use uncertainties, we do not offer a deﬁnitive answer
on how this can or should be done on any existing dataset.
We note, however, that means and (co)-variances are the
most commonly reported statistics and that the modeling
used in Sect. 4.2, 4.4 could be used also in other similar
datasets. In particular, the Taylor expansion method (Ap-
pendix B) that is behind the modeling used in Sect. 4.2,
has been used to model the propagation of uncertainties
due to a feature extraction process in other domains; for
instance, in [4] (Sect. II.B) this is used to model as Gaussian
the uncertainty in the estimation of illumination invariant
image derivatives. Finally, in our work the cost function,
which is based on the expectation of the hinge loss, and its
derivatives, can be calculated in closed forms. This allows an
efﬁcient implementation using a stochastic gradient descent
(SGD) algorithm.

The remainder of this paper is organized as follows.
In Section 2, we review related work, focusing on SVM-
based formulations that explicitly model data uncertainty.
In Section 3, we present the proposed algorithm which we
call SVM with Gaussian Sample Uncertainty (SVM-GSU).
In Section 4, we provide the experimental results of the
application of SVM-GSU to synthetic data and to ﬁve pub-

licly available and popular datasets. In the same section, we
provide comparisons with the standard SVM and other state
of the art methods. In Section 5, we draw some conclusions
and give directions for future work.

2 RELATED WORK
Uncertainty is ubiquitous in almost all ﬁelds of scientiﬁc
studies [5]. Exploiting uncertainty in supervised learning
has been studied in many different aspects [6], [7], [8]. More
speciﬁcally, the research community has studied learning
problems where uncertainty is present either in the labels or
in the representation of the training data.

In [9], Liu and Tao studied a classiﬁcation problem
in which sample labels are randomly corrupted. In this
scenario, there is an unobservable sample with noise-free
labels. However, before being observed, the true labels are
independently ﬂipped with a probability p ∈ [0, 0.5), and
the random label noise can be class-conditional. Tzelepis et
al. [10], [11] proposed an SVM extension where each training
example is assigned a relevance degree in (0, 1] expressing
the conﬁdence that the respective example belongs to the
given class. Li and Sethi [12] proposed an active learning
approach based on identifying and annotating uncertain
samples. Their approach estimates the uncertainty value
for each input sample according to its output score from
a classiﬁer and selects only samples with uncertainty value
above a user-deﬁned threshold. In [13], the authors used
weights to quantify the conﬁdence of automatic training
label assignment to images from clicks and showed that
using these weights with Fuzzy SVM and Power SVM [14]
can lead to signiﬁcant improvements in retrieval effective-
ness compared to the standard SVM. Finally, the problem of
conﬁdence-weighted learning is addressed in [15], [16], [17],
where uncertainty in the weights of a linear classiﬁer (under
online learning conditions) is taken into consideration.

Assuming uncertainty in data representation has also
drawn the attention of the research community in recent
years. Different types of robust SVMs have been proposed
in several recent works. Bi and Zhang [18] considered a
statistical formulation where the input noise is modeled
as a hidden mixture component, but in this way the “iid”
assumption for the training data is violated. In that work,
the uncertainty is modeled isotropically. Second order cone
programming (SOCP) [19] methods have also been em-
ployed in numerous works to handle missing and uncertain
data. In addition, Robust Optimization techniques [20], [21]
have been proposed for optimization problems where the
data is not speciﬁed exactly, but it is known to belong to
a given uncertainty set U , yet the optimization constraints
must hold for all possible values of the data from U .

Lanckriet et al. [22] considered a binary classiﬁcation
problem where the mean and covariance matrix of each
class are assumed to be known. Then, a minimax problem is
formulated such that the worst-case (maximum) probability
of misclassiﬁcation of future data points is minimized. That
is, under all possible choices of class-conditional densities
with a given mean and covariance matrix, the worst-case
probability of misclassiﬁcation of new data is minimized.

Shivaswamy et al. [23], who extended Bhattacharyya et
al. [24], also adopted a SOCP formulation and used gen-
eralized Chebyshev inequalities to design robust classiﬁers

2

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

dealing with uncertain observations. In their work uncer-
tainty arises in ellipsoidal form, as follows from the mul-
tivariate Chebyshev inequality. This formulation achieves
robustness by requiring that the ellipsoid of every uncertain
data point should lie in the correct halfspace. The expected
error of misclassifying a sample is obtained by computing
the volume of the ellipsoid that lies on the wrong side of
the hyperplane. However, this quantity is not computed an-
alytically; instead, a large number of uniformly distributed
points are generated in the ellipsoid, and the ratio of the
number of points on the wrong side of the hyperplane to
the total number of generated points is computed.

Several works [22], [23], [24] robustiﬁed regularized
classiﬁcation using box-type uncertainty. By contrast, Xu
et al. [25], [26] considered the robust classiﬁcation prob-
lem for a class of non-box-typed uncertainty sets; that
is, they considered a setup where the joint uncertainty is
the Cartesian product of uncertainty in each input. This
leads to penalty terms on each constraint of the resulting
formulation. Furthermore, Xu et al. gave evidence on the
equivalence between the standard regularized SVM and this
robust optimization formulation, establishing robustness as
the reason why regularized SVMs generalize well.

In [27], motivated by GEPSVM [28], Qi et al. robusti-
ﬁed a twin support vector machine (TWSVM) [29]. Robust
TWSVM [27] deals with data affected by measurement noise
using a SOCP formulation. In their work, the input data
is contaminated with isotropic noise (i.e., spherical distur-
bances centred at the training examples), and thus cannot
model real-world uncertainty, which is typically described
by more complex noise patterns. Power SVM [14] uses a
spherical uncertainty measure for each training example. In
this formulation, each example is represented by a spherical
region in the feature space, rather than a point. If any point
of this region is classiﬁed correctly, then the corresponding
loss introduced is zero.

Our proposed classiﬁer does not violate the “iid” as-
sumption for the training input data (in contrast to [18]),
and can model the uncertainty of each input training ex-
ample using an arbitrary covariance matrix; that is, it al-
lows anisotropic modeling of the uncertainty analytically
in contrast to [14], [23], [27]. Moreover, we deﬁne a cost
function that is convex and whose derivatives with respect
to the parameters of the unknown separating hyperplane
can be expressed in closed form. Therefore, we can ﬁnd their
global optimal using an iterative gradient descent algorithm
whose complexity is linear with respect to the number of
training data. Finally, we apply a linear subspace learning
approach in order to address the situation where most of the
mass of the Gaussians lies in a low dimensional manifold
that can be different for each Gaussian, and subsequently
solve the problem in lower-dimensional spaces. Learning
in subspaces is widely used in various statistical learning
problems [30], [31], [32].

3 PROPOSED APPROACH

Gaussian distributions; that is, each training example con-
sists of a mean vector xi ∈ D and a covariance matrix
Σi ∈ Sn
++; the latter expresses the uncertainty around the
corresponding mean*. In Sect. 3.1, we ﬁrst brieﬂy review the
linear SVM and then describe in detail the proposed linear
SVM with Gaussian Sample Uncertainty (SVM-GSU). In
Sect. 3.2 we motivate and describe a formulation that allows
learning in linear subspaces. In the general case we arrive
at different subspaces for the different Gaussians – this
allows, for example, dealing with covariance matrices that
are of low rank. In Sect. 3.3 we discuss how the proposed
algorithm relates to standard SVM when the latter is fed
with samples drawn from the input Gaussians. Finally, in
Sect. 3.4 we describe a SGD algorithm for efﬁciently solving
the SVM-GSU optimization problem.

3.1 SVM with Gaussian Sample Uncertainty

We begin by brieﬂy describing the standard SVM algorithm.
Let us consider the supervised learning framework and
denote the training set with X = (cid:8)(xi, yi) : xi ∈ Rn, yi ∈
{±1}, i = 1, . . . , (cid:96)(cid:9)
, where xi is a training example and yi
is the corresponding class label. Then, the standard linear
SVM learns a hyperplane H : w(cid:62)x + b = 0 that minimizes
with respect to w, b the following objective function:

(cid:107)w(cid:107)2 +

max

0, 1 − yi(w(cid:62)xi + b)

(1)

(cid:17)

,

λ
2

1
(cid:96)

(cid:96)
(cid:88)

i=1

(cid:16)

where h(t) = max(0, 1 − t) is the “hinge” loss function [33].
An illustrative example of the hinge loss calculation is given
in Fig. 2, where in Fig. 2a the red dashed line indicates the
loss introduced by the misclassiﬁed example (xi, yi) and in
Fig. 2c the hinge loss is shown in the black bold line.

In this work we assume that, instead of the i-th training
example in the form of a vector, we are given a multivariate
Gaussian distribution with mean vector xi and covariance
matrix Σi. One could think of this as that the covariance
matrix, Σi, models the uncertainty about the position of
training samples around xi. Formally, our training set is
i.e., X (cid:48) =
a set of (cid:96) annotated Gaussian distributions,
++, yi ∈ {±1}, i = 1, . . . , (cid:96)(cid:9)
(cid:8)(xi, Σi, yi) : xi ∈ Rn, Σi ∈ Sn
,
where xi ∈ Rn and Σi ∈ Sn
++ are respectively the mean
vector and the covariance matrix of the i-th example, and
yi is the corresponding label. Then, we deﬁne (cid:96) random
variables, Xi, each of which we assume that follows the cor-
responding n-dimensional Gaussian distribution N (xi, Σi)
and deﬁne an optimization problem where the misclassi-
ﬁcation cost for the i-th example is the expected value of
the hinge loss for the corresponding Gaussian. Formally, the
optimization problem, in its unconstrained primal form, is
the minimization with respect to w, b of

λ
2

(cid:107)w(cid:107)2 +

(cid:90)

(cid:96)
(cid:88)

1
(cid:96)

i=1

Rn

max (cid:0)0, 1 − yi(w(cid:62)x + b)(cid:1)fXi(x) dx, (2)

(x − xi)(cid:1)
where fXi(x) =
is the probability density function (PDF) of the i-th Gaussian

2 (x − xi)(cid:62)Σ−1

exp (cid:0)− 1

1
n
2 |Σi|

(2π)

1
2

i

In this section we develop a new classiﬁcation algorithm
whose training set is not just a set of vectors xi in some
multi-dimensional space, but rather a set of multivariate

*D is typically a subset of the n-dimensional Euclidean space of
++ denotes the convex cone of all symmetric

column vectors, while Sn
positive deﬁnite n × n matrices with entries in D ⊆ Rn.

3

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

(a)

(b)

(c)

Fig. 2: Illustrative example of calculating (a) the standard linear SVM’s hinge loss, and (b) the proposed linear SVM-GSU’s
loss. In (c), the hinge loss is compared with the proposed linear SVM-GSU’s loss for various quantities of uncertainty.

distribution. The above objective function J : Rn × R → R
can be written as

J (w, b) =

(cid:107)w(cid:107)2 +

L(cid:0)w, b; (xi, Σi, yi)(cid:1),

(3)

λ
2

1
(cid:96)

(cid:96)
(cid:88)

i=1

where, as stated above, the loss function L for the i-th
example (i.e. the i-th Gaussian) is deﬁned as the expected
value of the hinge loss for the Gaussian in question. That is,

(cid:90)
L(w, b)=

Rn

max (cid:0)0, 1− yi(w(cid:62)x + b)(cid:1)fXi(x) dx.

(4)

We proceed to express the objective function (3) and its
derivatives in closed form. This will allow us to solve the
corresponding optimization problem using an efﬁcient SGD
approach. More speciﬁcally, the loss can be expressed as

L(w, b)=

(cid:104)

(cid:105)
1 − yi(w(cid:62)x + b)

fXi(x) dx,

(5)

(cid:90)

Ωi

where Ωi denotes the halfspace of Rn that is deﬁned by
the hyperplane H(cid:48) : yi(w(cid:62)x + b) = 1 as Ωi = {x ∈
Rn : yi(w(cid:62)x + b) ≤ 1}, and is the halfspace to which
misclassiﬁed samples lie. This is illustrated in Fig. 2b,
where a misclassiﬁed example (xi, Σi, yi) introduces a loss
indicated by the shaded region. For the calculation of this
loss, all points that belong to the halfspace Ωi = {x ∈
Rn : yi(w(cid:62)x + b) ≤ 1}, i.e., the points x(cid:48) ∈ Ωi, contribute
to it by a quantity of [1 − yi(w(cid:62)x(cid:48) + b)]fXi(x(cid:48)). For one
such x(cid:48) denoted by a red circle in Fig. 2b, the ﬁrst part of
the above product, 1 − yi(w(cid:62)x(cid:48) + b), corresponds to the
typical hinge loss of SVM, shown as a red dashed line in
this example. The total loss introduced by the misclassiﬁed
example (xi, Σi, yi) is obtained by integrating all these
quantities over the halfspace Ωi.

Using Theorem 1 proved in Appendix A, for the half-
, the above inte-

(cid:0)w(cid:62)x + b(cid:1) ≤ 1(cid:9)

i = (cid:8)x ∈ Rn : yi

space Ω+
gral is evaluated in terms of w and b as follows
(cid:32)
(cid:18) dxi
dΣi
(cid:0)w(cid:62)xi + b(cid:1)

dΣi
√
π
2

L(w, b) =

dxi
2

exp

+ 1

erf

+

(cid:19)

(cid:21)

(cid:20)

, dΣi = (cid:112)2w(cid:62)Σiw, and
where dxi = 1 − yi
erf : R → (−1, 1) is the error function, deﬁned as erf(x) =

d2
xi
d2
Σi

, (6)

−

(cid:33)

2√
π

(cid:82) x
0 e−t2

dt. For a training example (x, Σ, y), Fig. 2c shows
the proposed loss in dashed green lines for constant values
of dΣ (constant amounts of uncertainty). We note that as
dΣ → 0, SVM-GSU’s loss virtually coincides with the SVM’s
hinge loss, while it can be easily veriﬁed that, regardless
of dΣ, as dx → ∞ the SVM-GSU’s loss will eventually
converge to zero (as the hinge loss does).

Let us note that the covariance matrix of each training ex-
ample describes the uncertainty around the corresponding
mean; that is, as the covariance matrix approaches the zero
matrix, the certainty increases. At the extreme†, as Σ → 0,
the proposed loss converges to the hinge loss function used
in the standard SVM formulation [33]. This implies that the
proposed formulation is a generalization of the standard
SVM; the two classiﬁers are equivalent when the covariance
matrices tend to the zero matrix.

It is easy to show that the objective function (3) is convex
with respect to w and b; therefore, we propose a SGD
algorithm in Sect. 3.4 for solving the corresponding opti-
mization problem. Since the objective function is convex,
we can obtain the global optimal solution. Moreover, it can
be shown that the proposed loss function (4) enjoys the
consistency property [34], [35], i.e., it leads to consistent
results with the 0 − 1 loss given the presence of inﬁnite data.
By differentiating J with respect to w and b, we obtain,
respectively,

∂J
∂w

= λw +

1
(cid:96)

(cid:96)
(cid:88)

i=1

(cid:34) exp (cid:0)−d2
√

(cid:1)

xi/d2
Σi

Σiw

πdΣi
(cid:18)

−

1
2

erf

(cid:18) dxi
dΣi

(cid:19)

(cid:19)

(cid:35)

+ 1

xi

,

(7)

∂J
∂b

= −

1
(cid:96)

(cid:96)
(cid:88)

(cid:20)

erf

i=1

(cid:18) dxi
dΣi

(cid:19)

(cid:21)

+ 1

.

(8)

Despite the complex appearance of the loss function and
its derivatives, their computation essentially requires the
calculation of the inner product w(cid:62)xi (which is the same as

†A zero covariance matrix exists due to the well known property
that the set of symmetric positive deﬁnite matrices is a convex cone
with vertex at zero.

4

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

n(n+1)

in standard SVM), plus that of the quadratic form w(cid:62)Σiw,
2 multiplications, since Σi is symmet-
which requires
ric. The latter, in the case of diagonal covariance matrices,
is equivalent to the computation of an inner product, i.e., of
complexity O(n). Moreover, each one of w(cid:62)xi and w(cid:62)Σiw
needs to be computed just once for calculating the loss
function and its derivatives for a given w. It is worth noting
that, in practice, as shown in Sect. 4, in real-world problems
uncertainty usually rises in diagonal form. In such cases,
the proposed algorithm is quite efﬁcient and exhibits very
similar complexity to the standard linear SVM.

Once the optimal values of the parameters w and b are
learned, an unseen testing datum, xt, can be classiﬁed to
one of the two classes according to the sign of the (signed)
distance between xt and the separating hyperplane. That is,
the predicted label of xt is computed as yt = sgn(dt), where
dt = (w(cid:62)xt + b)/(cid:107)w(cid:107). The posterior class probability, i.e,
a probabilistic degree of conﬁdence that the testing sample
belongs to the class to which it has been classiﬁed, can be
calculated using the well-known Platt scaling approach [36]
for ﬁtting a sigmoid function, S(t) = 1/(1 + eσAt+σB ). This
is the same approach that is used in the standard linear
SVM formulation (e.g., see [37]) for evaluating a sample’s
class membership at the testing phase.

3.2 Solving the SVM-GSU in linear subspaces

The derivations in Sect. 3.1 were made for the general case of
full rank covariance matrices that can be different for each
of the examples. Clearly, one can introduce constraints on
the covariance matrices, such as them being diagonal, block
diagonal, or multiples of the identity matrix. In this way one
can model different types of uncertainty – examples will
be given in the section of experimental results. However,
in some cases, especially when the dimensionality of the
data is high, most of the mass of the Gaussian distributions
will lie in a few directions in the feature space that may be
different for each example and may not be aligned with the
feature axes. To address this issue we alter the formulation
and work directly in the subspaces that preserve most of the
variance. More speciﬁcally, we propose a methodology for
approximating the loss function of SVM-GSU, by projecting
the vectors x in (5) into a linear subspace and integrating the
hinge loss function in that subspace instead of the original
feature space. A separate subspace is used for each of the
training examples, that is, for each of the input Gaussians.
For a given Gaussian distribution, the projection matrix is
found by performing eigenanalysis on the covariance matrix
and the dimensionality of each subspace is deﬁned so as to
preserve a certain fraction of the total variance.

i , . . . , λn

i , where Λi

i ), so that λ1

More speciﬁcally, by performing eigenanalysis on the
covariance matrix of the random vector Xi, the latter is
decomposed as Σi = UiΛiU (cid:62)
is an n × n
diagonal matrix consisting of the eigenvalues of Σi, i.e.
Λi = diag(λ1
i > 0, while
Ui is an n × n orthonormal matrix, whose j-th column, uj
i ,
is the eigenvector corresponding to the j-th eigenvalue, λj
i .
Let us keep the ﬁrst di ≤ n eigenvectors, so that a certain
fraction p ∈ (0, 1] of the total variance is preserved, i.e.,
(cid:80)di
(cid:80)n
keeping the ﬁrst di columns of Ui, i.e., U (cid:48)

> p. Then, we construct the n × di matrix U (cid:48)

i ≥, . . . , ≥ λn

t=1 λt
i
t=1 λt
i

. . . udi

i by
i ].

i = [u1

i u2
i

(cid:62)

i = Λz

Now, by using the projection matrix Pi = U (cid:48)
, we deﬁne
i
a new random vector Zi, such that Zi = PiXi. Then,
Zi ∈ Rdi follows a multivariate Gaussian distribution (since
Xi ∼ N (xi, Σi)), i.e. Zi ∼ N (zi, Σz
i ), with mean vector
zi = E(cid:2)PiXi
(cid:3) = PiE(cid:2)Xi
(cid:3) = Pixi and (diagonal) covariance
matrix Σz

i . Let fZi denote the PDF of Zi.
We proceed to approximate the expected value of the
hinge loss in the original space (5), by considering the
integral in the new, lower-dimensional space where most of
the variance is preserved. More speciﬁcally, x ≈ P (cid:62)
i z =⇒
w(cid:62)x ≈ w(cid:62)(P (cid:62)
(cid:62)z, where wz = Piw. Con-
sequently, the loss function for the i-th example, that is
the integral in the RHS of (5) can be approximated by
(cid:0)w(cid:62)
i de-
the quantity
i = (cid:8)z ∈
notes the projected halfspace on Rdi , that is, Ωz
Rdi : yi
. Using Theorem 1 (Appendix A),
we can then give this approximation of the loss function
L(cid:48) : Rdi × R → R, in closed form as follows:
(cid:32)

z z + b(cid:1)(cid:3) fZi(z) dz, where Ωz

z z + b(cid:1) ≤ 1(cid:9)

i z) = wz

(cid:2)1 − yi

(cid:0)w(cid:62)

Ωz
i

(cid:33)

(cid:32)

(cid:33)

(cid:82)

(cid:34)

(cid:35)

+ 1

+

erf

dzi
dΣz
z zi + b(cid:1)

i

(cid:0)w(cid:62)

i

exp

dΣz
√
2
π
= (cid:112)2w(cid:62)

−

d2
zi
d2
Σz
i

(9)

where dzi = 1 − yi
i wz. There-
fore, the objective function J (cid:48) : Rn × R → R, given by (3)
can be approximated as follows

, dΣz

i

z Σz

L(cid:48)(w, b) =

dzi
2

J (cid:48)(w, b) =

(cid:107)w(cid:107)2 +

L(cid:48) (Piw, b; (zi, Σz

i , yi)) .

(10)

λ
2

1
(cid:96)

(cid:96)
(cid:88)

i=1

Similarly to J , we can show that J (cid:48) is also convex with
respect to the unknown parameters w and b of the sepa-
rating hyperplane. Moreover, using the chain rule, we can
obtain the partial derivatives of J (cid:48) with respect to w and
b in closed form, and therefore use a stochastic gradient
method to arrive at the global optimum. More speciﬁcally,

∂J (cid:48)
∂w

= λw +

1
(cid:96)

(cid:96)
(cid:88)

i=1

∂
∂wz

L(cid:48)(cid:0)wz, b; (zi, Σz

i , yi)(cid:1) ∂wz
∂w

,

∂w wz = ∂

where ∂
respect to wz, and replacing in the above, we arrive at
(cid:16)

∂w Piw = Pi. By differentiating L(cid:48) with

(cid:17)

∂J (cid:48)
∂w

= λw +

(cid:34) exp

1
(cid:96)

(cid:96)
(cid:88)

i=1

−d2

zi/d2
Σz
i

√

πdΣz
i
(cid:32)

(cid:32)

erf

dzi
dΣz

i

−

1
2

i (Σz
P (cid:62)

i wz)

(cid:33)

(cid:33)

+ 1

P (cid:62)

i zi

(11)

(cid:35)
,

that is a closed form equation that gives the partial deriva-
tives of the cost with respect to w. Similarly, the ﬁrst partial
derivative of J (cid:48) with respect to b can be obtained as follows
(cid:34)

(cid:32)

(cid:33)

(cid:35)

∂J (cid:48)
∂b

(cid:96)
(cid:88)

1
(cid:96)

dzi
dΣz

= −

erf

+ 1

.

(12)

i

where wz = Piw, Σz

i=1
i = PiΣiP (cid:62)
i .
To summarize, in the low-dimensional spaces Rdi , the
loss function is computed as shown in (9). The objective
function is computed as shown in (10) and its ﬁrst deriva-
tives are computed as in (11) and (12). Finally, let us note that
in the above equations, the only matrix operations involve
the projection matrix Pi. Since the covariance matrices Σz
i
are diagonal, all operations that involve them boil down to
efﬁcient vector rescaling and vector norm calculations.

5

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

3.3 To sample or not to sample?

The data term in our formulation (see (4)) is the expected
value of the classical SVM cost when data samples are
drawn from the multi-dimensional Gaussian distributions.
It therefore follows that a standard linear SVM would arrive
at the same hyperplane when sufﬁciently many samples
are drawn from them. How many samples are needed to
arrive at the same hyperplane is something that cannot
be computed analytically. Nevertheless, our analysis and
results indicate that this number can be prohibitively high,
especially in the case of high-dimensional spaces.

More speciﬁcally, in what follows, we show that the dif-
ference between the analytically calculated expected value
of the hinge loss (4) and its sample mean is bounded by
a quantity that is inversely related to the dimensionality
of the feature space. Let L be the expected loss given
analytically as in (4), and ˜LN its approximation when N
samples are drawn from the Gaussians. Since the hinge loss
is (cid:107)w(cid:107)-Lipschitz‡ with respect to the Euclidean norm, we
can use a result due to Tsirelson et al. [38] that provides a
concentration inequality for Lipschitz functions of Gaussian
variables. By doing so, for all r ≥ 0, we arrive at the
following concentration inequality

P

(cid:16)(cid:12)
(cid:12)L − ˜LN
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≥ r

(cid:17)

≤ 2 exp

−

(cid:18)

r2
2(cid:107)w(cid:107)2

(cid:19)

.

(13)

That is, the tails of the error probability decay exponentially
with r2. More interestingly, they increase with the squared
norm of (cid:107)w(cid:107), and therefore with the dimensionality of the
input space, n. Consequently, as n increases, one needs
to generate more samples from the Gaussians in order to
preserve a desired approximation of the loss.

This means that for spaces of high dimensionality the
number of samples needed to approximate (4) sufﬁciently
well, can be prohibitively high. We experimentally demon-
strated this with a toy example in Sect. 4.1 (see Fig. 4), where
we show that in 2 dimensions we need approximately 3
orders of magnitude more samples to arrive at the same
hyperplane, while for 3 dimensions we need 4 orders of
magnitude more samples. Our experimental results on the
large-scale MED dataset (Sect. 4.6) also show the limitations
of a sampling approach.

3.4 A stochastic gradient descent solver for SVM-GSU

Motivated by the Pegasos algorithm (Primal Estimated
sub-GrAdient SOlver for SVM), ﬁrst proposed by Shalev-
Shwartz et al. in [39], we present a stochastic sub-gradient
descent algorithm for solving SVM-GSU in order to efﬁ-
ciently address scalability requirements§.

Pegasos is a well-studied algorithm [39], [40] providing
both state of the art classiﬁcation performance and great
scalability. It requires ˜O(1/(cid:15)) number of iterations in order
to obtain a solution of accuracy (cid:15), in contrast to previous
analyses of SGD methods that require ˜O(d/(λ(cid:15))) iterations,

‡A function h : Rn → R is L -Lipschitz with respect to the Eu-
clidean norm if |h(x) − h(y)| ≤ L (cid:107)x − y(cid:107), L > 0. Indeed, the
hinge loss h(x) = max(0, 1 − y(w(cid:62)x + b)) is (cid:107)w(cid:107)-Lipschitz since
(cid:12)1 − y(w(cid:62)x + b) − 1 + y(w(cid:62)y + b)(cid:12)
|h(x) − h(y)| ≤ (cid:12)
(cid:12) ≤(cid:107)w(cid:107)(cid:107)x − y(cid:107).
§A C++ implementation of the proposed method can be found at

https://github.com/chi0tzp/svm-gsu.

where d is a bound on the number of non-zero features in
each example¶. Since the run-time does not depend directly
on the size of the training set, the resulting algorithm is
especially suited for learning from large datasets.

Given a training set X = (cid:8)(xi, Σi, yi) : xi ∈ Rn, Σi ∈
, the proposed algorithm

++, yi ∈ {±1}, i = 1, . . . , (cid:96)(cid:9)
Sn
solves the following optimization problem

min
w,b

λ
2

(cid:107)w(cid:107)2 +

1
(cid:96)

(cid:96)
(cid:88)

i=1

L(cid:0)w, b; (xi, Σi, yi)(cid:1).

(14)

The algorithm receives as input two parameters: (i) the
number of iterations, T , and (ii) the number of examples
to use for calculating sub-gradients, k. Initially, we set w(1)
λ and b(1) = 0.
to any vector whose norm is at most 1/
On the t-th iteration, we randomly choose a subset of X ,
of cardinality k, i.e., Xt ⊆ X , where |Xt| = k, and set the
learning rate to ηt = 1
λt . Then, we approximate the objective
function of the above optimization problem with

√

λ
2

(cid:107)w(cid:107)2 +

1
k

(xi,Σi,yi)∈Xt

(cid:88)

L(cid:0)w, b; (xi, Σi, yi)(cid:1).

Then, we perform the update steps

w(t+1) ← w(t) −

b(t+1) ← b(t) −

ηt
k

∂J
∂w

,

ηt
k

∂J
∂b

,

where the ﬁrst-order derivatives are given in (7), (8), if the
training is conducted in the original space (Sect. 3.1), or in
(11), (12), if the learning is conducted in linear subspaces
(Sect. 3.2). Last, we project w(t+1) onto the ball of radius
λ}. The output
1/
of the algorithm is the pair of w(T +1), b(T +1). Algorithm 1
describes the proposed method in pseudocode.

λ, i.e., the set B = {w : (cid:107)w(cid:107) ≤ 1/

√

√

Algorithm 1 A stochastic sub-gradient descent algorithm
for solving SVM-GSU.

1: Inputs:

X , λ, T , k

2: Initialize:

b(1) = 0, w(1) such that (cid:107)w(1)(cid:107) ≤ 1√
λ

3: for t = 1, 2, . . . , T do
4:
5:

Choose Xt ⊆ X , where |Xt| = k
Set ηt = 1
λt
w(t+1) ← w(t) − ηt
k
w(t+1) ← min
1,
b(t+1) ← b(t) − ηt
k

(cid:107)w(t+1)(cid:107)
∂J
∂b

∂J
∂w
√
1/

(cid:16)

(cid:17)

λ

6:

7:

w(t+1)

8:
9: end for

4 EXPERIMENTS
In this section we ﬁrst illustrate the workings of the pro-
posed linear SVM-GSU classiﬁer on a synthetic 2D toy
example (Sect. 4.1) and then apply the algorithm on ﬁve
different classiﬁcation problems using publicly available
and popular datasets. Here, we summarize how the uncer-
tainty is modeled in each case, so as to illustrate how our
framework can be applied in practice.

¶We use the ˜O notation (soft-O) as a shorthand for the variant of O
(big-O) that ignores logarithmic factors; that is, f (n) ∈ ˜O(g(n)) ⇐⇒
∃k ∈ N : f (n) ∈ O(g(n) logk(g(n))).

6

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

(a)

(b) N = 10

(c) N = 102

(d) N = 103

Fig. 3: Toy example illustrating on 2D data, (a) the proposed LSVM-GSU (red solid line) in comparison with the standard
LSVM (blue dashed line), and (b)-(d) with the standard SVM that learns by sampling from the input Gaussians (LSVM-
sampling), where N is the sampling size.

First, we address the problem of image classiﬁcation
of handwritten digits (Sect. 4.2) using the MNIST dataset.
As we show in Appendix B, by using a ﬁrst-order Taylor
approximation around a certain image with respect to some
common image transformations (small translations in our
case), we show that the images that would be produced
by those translations would follow a Gaussian distribution
with mean the image in question and a covariance matrix
whose elements are functions of the derivatives of the image
intensities/color with respect to those transformations. In
the simple case of spatial translations, the covariance ele-
ments are functions of the spatial gradients. This is a case
where the uncertainty is modeled. We show that our method
outperforms the linear SVM and other SVM variants that
handle uncertainty isotropically.

Second, we address the binary classiﬁcation problem us-
ing the Wisconsin Diagnostic Breast Cancer (WDBC) dataset
(Sect. 4.3). This is a case in which each data example summa-
rizes a collection of samples by their second order statistics.
More speciﬁcally, each data example contains as features the
mean and the variance of measurements on several cancer
cells – mean and variances over the different cells. With our
formulation we obtain state of the art results on this dataset.
Third, we address the problem of emotional analysis
using electroencephalogram (EEG) signals (Sect. 4.4). In this
case, we exploit a very popular method for estimating the
power spectrum of time signals; namely the Welch method,
which allows for estimating not only the mean values of the
features (periodograms), but also their variances, making it
suitable for using the proposed SVM-GSU.

Fourth, we address the problem of detection of adver-
tisements in TV news videos (Sect. 4.5). This is an interesting
case where uncertainty information is given only for a few
dimensions of the input space, rendering inapplicable the
methods that treat uncertainty isotropically. In contrast, the
proposed method can model such uncertainty types using
low-rank covariance matrices.

Finally, we address the challenging problem of complex
event detection in video (Sect. 4.6). We used the ∼5K out-
puts of a pre-trained DCNN in order to extract a representa-
tion for each frame in a video and calculated the mean and
covariances over the frames of a video in order to classify
it. This is a second example in which the mean and the

covariance matrices are calculated from data. We show that
our formulation outperforms the linear SVM and other SVM
variants that handle uncertainty isotropically.

4.1 Toy example using synthetic data
In this subsection, we present a toy example on 2D data that
provides insights to the way the proposed algorithm works.
As shown in Fig. 3a, negative examples are denoted by red
× marks, while positive ones by green crosses. We assume
that the uncertainty of each training example is given via
a covariance matrix. For illustration purposes, we draw the
iso-density loci of points at which the value of the PDF of
the Gaussian is the 0.03% of its maximum value.

First, a baseline linear SVM (LSVM) is trained using
solely the centres of the distributions; i.e., ignoring the un-
certainty of each example. The resulting separating bound-
ary is the dashed blue line in Fig. 3a. The proposed linear
SVM-GSU (LSVM-GSU) is trained using both the centres
of the above distributions and the covariance matrices.
The resulting separating boundary is the solid red line in
Fig. 3a. It is clear that the separating boundaries can be very
different and that the solid red line is a better one given the
assumed uncertainty modeling.

Next, we investigate on how many samples are needed
in order to obtain LSVM-GSU’s separating line by sampling
N samples from each Gaussian and using the standard
LSVM (LSVM-sampling). The results for various values of
N are depicted in Fig. 3, where it is clear that ones needs
almost 3 orders of magnitude more examples. In order to
investigate how this number changes with the dimensional-
ity of the feature space we performed the same experiment
in a similar 3D dataset. In Fig. 4 we plot the angle between
the hyperplanes obtained by the LSVM-GSU and the LSVM-
sampling for both the 2D and the 3D datasets. We observe
that, in the 3D case, we need at least one order of magnitude
more samples from each Gaussian, compared to the 2D
case; that is, in the 2D case, we obtain θ ≈ 1.7◦ using
N = 103 samples from each Gaussian, while in the 3D case,
the sampling size for obtaining the same approximation
(θ ≈ 1.7◦) is N = 5×104. This is indicative of the difﬁculties
of using the sampling approach when dealing with high-
dimensional data, where the number of dimensions is in the
hundreds or thousands.

7

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

4.2.2 Uncertainty modeling

In Appendix B, we propose a methodology that, given an
image, models the distribution of the images that result by
small random translations of it. We show that under a ﬁrst-
order Taylor approximation of the image intensities/color
with respect to those translations, and the assumption that
the translations are small and follow a Gaussian distribu-
tion, the resulting distribution of the images is also a Gaus-
sian with mean the original image and a covariance matrix
whose elements are functions of the image derivatives with
respect to the transforms – in this case functions of the image
spatial gradients. The derivation could be straightforwardly
extended to other transforms (e.g. rotations, scaling).

In our experiments in this dataset we set the variances
of the horizontal and the vertical components of the transla-
(cid:1)2
tion, denoted by σ2
,
so that the translation falls in the square [−pt, pt] × [−pt, pt]
with probability 99.7%. The pt is measured in pixels and for
the experiments described below, it is set to pt = 5 pixels.

v respectively, to σ2

h and σ2

v = (cid:0) pt

h = σ2

3

4.2.3 Experimental results

Table 1 shows the performance of the proposed classiﬁer
(LSVM-GSU) and the compared techniques in terms of
testing accuracy for each dataset deﬁned above (D0-D5).
The optimization of the training parameter for the various
SVM variants was performed using a line search on a 3-
fold cross-validation procedure. The performance of LSVM-
GSU when the training of each classiﬁer is carried out in
the original feature space is shown in row 5, and in linear
subspaces in row 6. In row 6 we report both the classiﬁcation
performance, and in parentheses the fraction of variance
that resulted in the best classiﬁcation result.

The performance of the baseline linear SVM (LSVM) is
shown in the second row, the performance of Power SVM
(PSVM) [14] is shown in the third row, and the performance
of the linear SVM extension, based on the proposed formula-
tion, handling the noise isotropically, as in [18], [27], (LSVM-
iso) is shown in the fourth row. Moreover, Fig. 5 shows
the results of the above experimental scenarios for datasets
D0-D5. The horizontal axis of each subﬁgure describes the
fraction of the total variance preserved for each covariance
matrix, while the vertical axis shows the respective per-
formance of LSVM-GSU with learning in linear subspaces
(LSVM-GSU-SLp). Furthermore, in each subﬁgure, for p = 1
we draw the result of LSVM-GSU in the original feature
space (denoted with a rhombus), the result of PSVM [14]
(denoted with a circle), as well as the result of LSVM-
iso [18], [27] (denoted with a star).

We report the mean, and with an error-bar show the
variance of the 100 iterations. The performance of the base-
line LSVM is shown with a solid line, while two dashed
lines show the corresponding variance of the 100 runs. From
the obtained results, we observe that the proposed LSVM-
GSU with learning in linear subspaces outperforms LSVM,
PSVM, and LSVM-iso for all datasets D0-D5. Moreover,
LSVM-GSU achieves better classiﬁcation results than PSVM
in all datasets, and than LSVM-iso in 5 out of 6 datasets,
when learning is carried out in the original feature space.
Finally, all the reported results are shown to be statistically
signiﬁcant using the t-test [43]; signiﬁcance values (p-values)

Fig. 4: Difference between the separating hyperplanes of
LSVM-GSU and the standard LSVM with sampling (angle
θ), when varying the number of samples used in the stan-
dard SVM, for the 2D and 3D toy datasets.

4.2 Hand-written digit classiﬁcation

4.2.1 Dataset and experimental setup

The proposed algorithm is also evaluated in the problem of
image classiﬁcation using the MNIST dataset of handwritten
digits [41]. The MNIST dataset provides a training set of 60K
examples (approx. 6000 examples per digit), and a test set
of 10K examples (approx. 1000 examples per digit). Each
sample is represented by a 28 × 28 8-bit image.

In order to make the dataset more challenging, as well as
to model a realistic distortion that may happen to this kind
of images, the original MNIST dataset was “polluted” with
noise. More speciﬁcally, each image example was rotated by
a random angle uniformly drawn from [−θ, +θ], where θ is
measured in degrees. Moreover, each image was translated
by a random vector t uniformly drawn from [−tp, +tp]2,
where tp is a positive integer expressing distance that is
measured in pixels. We created ﬁve different noisy datasets
by setting θ = 15° and tp ∈ {3, 5, 7, 9, 11}, resulting in the
polluted datasets D1 to D5, respectively. D0 denotes the
original MNIST dataset.

We created six different experimental scenarios using
the above datasets (D0-D5). First, we deﬁned the problem
of discriminating the digit one (“1”) from the digit seven
(“7”) similarly to [42]. Each class in the training procedure
consists of 25 samples, randomly chosen from the pool of
digits one (6k totally) and seven (6k totally), while the
evaluation of the trained classiﬁer is carried out on the full
testing set (2k examples). In each experimental scenario we
report the average of 100 runs and we compare the pro-
posed linear SVM-GSU (LSVM-GSU) to the baseline linear
SVM (LSVM), Power SVM [14], and LSVM-iso (a variation
of SVM formulation that handles only isotropic uncertainty,
similarly to [18], [27]). We report the testing accuracy and the
mean testing accuracy across 100 runs. Finally, we repeat the
above experiments for various sizes of the training set; i.e.,
using 25, 50, 100, 500, 1000, 3000, 6000 positive examples
per digit, in order to investigate how this affects the results.

8

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

TABLE 1: MNIST “1” versus “7” experimental results in terms of testing accuracy. The proposed LSVM-GSU is compared
to the baseline linear SVM (LSVM), Power SVM (PSVM) [14], and a linear SVM extension which handles the uncertainty
isotropically (LSVM-iso), as in [18], [27].

Dataset
LSVM
PSVM [14]
LSVM-iso (as in [18], [27])

LSVM-GSU

Learning in original space
Learning in linear subspaces

D0
0.9952
0.9963
0.9968
0.9971
0.9972 (0.99)

D1
0.9362
0.9315
0.9327
0.9452
0.9480 (0.97)

D2
0.8240
0.8157
0.8133
0.8310
0.8562 (0.89)

D3
0.6830
0.7017
0.7222
0.7216
0.7543 (0.85)

D4
0.6558
0.6650
0.6675
0.6708
0.6974 (0.95)

D5
0.6027
0.6259
0.6328
0.6353
0.6640 (0.25)

(a)

(b)

(c)

(d)

(e)

(f)

Fig. 5: Comparisons between the proposed LSVM-GSU, the
baseline LSVM, and the LSVM with isotropic noise in (a) the
original MNIST dataset (D0), and (b)-(f) the noisy generated
datasets D1-D5.

were much lower than the signiﬁcance level of 1%. Finally,
in Fig. 6, we show the experimental results using various
training set sizes and we observe that this does not qualita-
tively affect the behavior of the various compared methods.

4.3 Wisconsin Diagnostic Breast Cancer dataset

[44] consists of

(WDBC)
The Wisconsin Diagnostic Breast Cancer
features computed from 569
dataset
images, each belonging to one of the following two classes:
malignant (212 instances) and benign (357 instances). The
digitized images depict breast mass obtained by Fine

9

(a)

(b)

Fig. 6: MNIST “1” versus “7” experimental results using 25,
50, 100, 500, 1000, 3000, 6000 positive examples per digit.
The proposed LSVM-GSU using learning linear subspaces
(LSVM-GSU-Sp) is compared to the baseline linear SVM
(LSVM), Power SVM (PSVM) [14], and a linear SVM ex-
tension which handles the uncertainty isotropically (LSVM-
iso), as in [18], [27]. The fraction of variance preserved
for the proposed method is (a) p = 0.85 (dataset D3), (b)
p = 0.95 (dataset D4). Very similar results are observed for
all other datasets.

Needle Aspirate (FNA) and they describe characteristics of
the cell nuclei present in the image. Each feature vector is of
the form x = (x1, . . . , x10, s1, . . . , s10, w1, . . . , w10)(cid:62) ∈ R30,
where xj is the mean value, sj the standard error, and
wj the largest value of the j-th feature, j = 1, . . . , 10. Ten
real-valued features are computed for each cell nucleus.

i

Since the standard error si and variance σ2

i are connected
via the relation si = σ2
N , where N is the (unknown) size
of the sample where standard deviation was computed,
we assign to each input example a diagonal covariance
matrix given by Σi = diag(σ2
1, . . . , σ2
++,
where σ2
0 is set to a small positive constant (e.g., 10−6)
indicating very low uncertainty for the respective features,
and σ2
j is computed using the standard error by scaling the
standard error values into the range of mean values; that
is, the maximum variance is set to 80% of the range of the
corresponding mean value.

0, . . . , σ2

0) ∈ S30

10, σ2

The proposed algorithm is compared in terms of testing
accuracy both to the baseline linear SVM (LSVM), Power
SVM [14] (PSVM), and to LSVM-iso, similarly to Sect. 4.2.
Since the original dataset does not provide a division in
training and evaluation subsets, we divided the dataset
randomly into a training subset (90%) and an evaluation
subset (10%). The optimization of the λ parameter for all
classiﬁers was performed using a line search on a 10-fold
cross-validation procedure. We repeated the experiment 10

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

times and report the average results in Table 2. The re-
sults are statistically signiﬁcant and show the superiority
of LSVM-GSU. More speciﬁcally, we used the t-test [43] and
obtained signiﬁcance values (p-values) lower than 0.05.

TABLE 2: Comparison between the proposed LSVM-GSU,
the baseline LSVM, Power SVM, and LSVM-iso.

Classiﬁer
LSVM
PSVM [14]
LSVM-iso (as in [18], [27])
LSVM-GSU (proposed)

Testing Accuracy
95.15%
96.37%
96.53%
97.14%

4.4 Emotion analysis using physiological signals

4.4.1 Dataset and experimental setup

For evaluating the proposed method in the domain of
emotional analysis using physiological signals, we used
the publicly available DEAP [45] dataset, which provides
EEG features of 32 participants who were recorded while
watching 40 one-minute long excerpts of music videos.
Three different binary classiﬁcation problems were deﬁned:
the classiﬁcation of low/high arousal, low/high valence and
low/high liking videos.

From the EEG signals, power spectral features were ex-
tracted using the Welch method [46]. The logarithms of the
spectral power from theta (4 − 8 Hz), slow alpha (8 − 10 Hz),
alpha (8−12 Hz), beta (12-30Hz), and gamma (30+ Hz) bands
were extracted from all 32 electrodes as features, similarly
to [45]. In addition to power spectral features, the difference
between the spectral power of all the symmetrical pairs of
electrodes on the right and left hemisphere was extracted to
measure the possible asymmetry in the brain activities due
to emotional stimuli. The total number of EEG features of a
video for 32 electrodes is 216. For feature selection, we used
Fisher’s linear discriminant similarly to [45].

4.4.2 Uncertainty modeling

For modeling the uncertainty of each training example, we
used a well-known property of the Welch method [46] for es-
timating the power spectrum of a time signal. First, the time
signal was divided into (overlapping or non-overlapping)
windows, where the periodogram was computed for each
window. Then the resulting frequency-domain values were
averaged over all windows. Besides these mean values, that
are the desired outcomes of the Welch method, we also
computed the variances, and, thus, each 216-element vector
was assigned with a diagonal covariance matrix.

4.4.3 Experimental results

Table 3 shows the performance of the proposed linear SVM-
GSU (LSVM-GSU) in terms of accuracy and F1 score for
each target class in comparison to LSVM, PSVM [14], and
LSVM-iso, similarly to Sect. 4.2 and 4.3, as well as the Naive
Bayesian (NB) classiﬁer used in [45]. For each participant,
the F1 measure was used to evaluate the performance of
emotion classiﬁcation in a leave-one-out cross validation
scheme. At each step of the cross validation, one video was
used as the test-set and the rest were used for training. For

TABLE 3: Comparisons between the proposed LSVM-GSU,
the baseline NB, LSVM, Power SVM, and the LSVM with
isotropic noise.

Classiﬁer
NB [45]
LSVM
PSVM [14]
LSVM-iso [18], [27]
LSVM-GSU

Arousal
ACC F1
0.620
0.626
0.625
0.645
0.659

0.583
0.451
0.521
0.531
0.551

Valence
ACC F1
0.576
0.616
0.633
0.645
0.650

0.563
0.538
0.561
0.603
0.609

Liking
ACC F1
0.554
0.655
0.651
0.658
0.666

0.502
0.470
0.522
0.530
0.539

optimizing the λ parameter of the various SVM classiﬁers,
we used a line search on a 3-fold cross-validation procedure.
From the obtained results, we observe that the proposed
algorithm achieved better classiﬁcation performance than
LSVM, PSVM, LSVM-iso, as well as the NB classiﬁer used
in [45] for all three classes, in terms of testing accuracy, and
for the two out of three classes in terms of F1 score.

4.5 TV News Channel Commercial Detection

4.5.1 Dataset and experimental setup
The proposed algorithm is evaluated in the problem of
detection of advertisements in TV news videos using the
publicly available and very large dataset of [47]. This dataset
comprises 120 hours of TV news broadcasts from CNN,
CNNIBN, NDTV, and TIMES NOW (approximately 22k,
33k, 17k, and 39k videos, respectively). The authors of [47]
used various low-level audio and static-, motion-, and
text-based visual features, to extract and provide a 4125-
dimensional representation for each video, that includes the
variance values for 24 of the above features. For a detailed
description of the dataset, see [47].

4.5.2 Uncertainty modeling
This dataset represents a real-world case where uncertainty
information is given only for a few dimensions of the
feature space. In this case we model the covariance ma-
trix of each input example as a low-rank diagonal matrix,
whose non-zero variance values correspond to the dimen-
sions for which uncertainty is provided. Each such matrix
corresponds to a Gaussian with non-zero variance along
the few speciﬁc given dimensions. Since the information
about the input variance is provided just for the 24 of the
4125 features, there is no natural way of estimating a single
variance value, i.e., an isotropic covariance matrix, for each
training example.

4.5.3 Experimental results
Table 4 shows the performance of the proposed linear SVM-
GSU (LSVM-GSU) in terms of F1 score in comparison to
LSVM, similarly to [47]. As discussed above, since methods
that model the uncertainty isotropically (such as [14], [18],
[27]), are not applicable in this dataset, we experimented
on this dataset using only the proposed algorithm and the
standard linear SVM. Following the protocol of [47], we
did cross-dataset training and testing. For optimizing the
λ parameter of both LSVM and LSVM-GSU we used a
line search on a 3-fold cross-validation procedure. From the
obtained results, we observe that the proposed algorithm
achieved considerably better classiﬁcation than LSVM in
almost all cases (more than 10% relative boost on average).

10

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

TABLE 4: Comparisons between the proposed LSVM-GSU and the baseline LSVM, similarly to [47].

CNN
LSVM LSVM-GSU
0.7799
0.7915
CNNIBN
NDTV
0.8484
TIMES NOW 0.7809

0.9589
0.8836
0.9248
0.9461

n CNN
o
g
n
i
t
s
e
T

CNNIBN
LSVM LSVM-GSU
0.7799
0.7915
0.8484
0.7809

0.8050
0.9215
0.8565
0.7863

NDTV
LSVM LSVM-GSU
0.7799
0.7915
0.8484
0.7809

0.8113
0.8978
0.9709
0.7493

TIMES NOW
LSVM LSVM-GSU
0.7799
0.7915
0.8484
0.7809

0.9226
0.8611
0.8823
0.9421

Training on

4.6 Video Event Detection

4.6.1 Dataset and experimental setup

In our experiments on video event detection we used
datasets from the challenging TRECVID Multimedia Event
Detection (MED)
task [48]. For training, we used the
MED 2015 training dataset consisting of the “pre-speciﬁed”
(PS) video subset (2000 videos, 80 hours) and the “event
background” (Event-BG) video subset (5000 videos, 200
hours). For testing, we used the large-scale “MED14Test”
dataset [48], [49] (∼ 24K videos, 850 hours). Each video in
the above datasets belongs to, either one of 20 target event
classes, or to the “rest of the world” (background) class.
More speciﬁcally, in the training set, 100 positive and 5000
negative samples are available for each event class, while
the evaluation set includes only a small number of positive
(e.g., only 16 positives for event E021, and 28 for E031) and
approximately 24K negative videos.

For video representation, approximately 2 keyframes per
second were extracted from each video. Each keyframe was
represented using the last hidden layer of a pre-trained deep
convolutional neural network (DCNN). More speciﬁcally, a
22-layer inception style network, trained according to the
GoogLeNet architecture [50], was used. This network had
been trained on various selections of the ImageNet “Fall
2011” dataset and provides scores for 5055 concepts [51].

4.6.2 Uncertainty modeling
Let us now deﬁne a set X of (cid:96) annotated random vec-
tors representing the aforementioned video-level feature
vectors. Each random vector is assumed to be distributed
normally; i.e., for the random vector representing the i-
th video, Xi, we have Xi ∼ N (xi, Σi). That is, X =
{(xi, Σi, yi) : xi ∈ Rn, Σi ∈ Sn
++, yi ∈ {±1}, i = 1, . . . , (cid:96)}.
For each random vector Xi, a number, Ni, of observations,
i ∈ Rn : t = 1, . . . , Ni} are available (these are the
{xt
keyframe-level vectors that have been computed). Then, the
sample mean vector and the sample covariance matrix of
Xi are computed. However, the number of observations per
each video that are available for our dataset is in most cases
much lower than the dimensionality of the input space.
Consequently, the covariance matrices that arise are typi-
cally low-rank; i.e. rank(Σi) ≤ Ni ≤ n. To overcome this
issue, we assumed that the desired covariance matrices are
diagonal. That is, we require that the covariance matrix of
the i-th training example is given by (cid:99)Σi = diag (cid:0)ˆσ1
(cid:1)
,
such that the squared Frobenious norm of the difference
Σi − (cid:99)Σi is minimum. That is, the estimator covariance matrix
(cid:99)Σi must be equal to the diagonal part of the sample covari-
ance matrix Σi, i.e. (cid:99)Σi = diag (cid:0)σ1
. We note that,
using this approximation approach, the covariance matrices
are diagonal but anisotropic and different for each training

i , . . . , ˆσn
i

i , . . . , σn
i

(cid:1)

input example. This is in contrast with other methods (e.g.
[14], [18], [27]) that assume more restrictive modeling for the
uncertainty; e.g., isotropic noise for each training sample.

4.6.3 Experimental results

We experimented using two different feature conﬁgurations.
First, we used the mean vectors and covariance matrices as
computed using the method discussed above (Sect. 4.6.2).
Furthermore, in order to investigate the role of variances in
learning with baseline LSVM, we constructed mean vectors
and covariance matrices as shown in Table 6, where σ0
is typically set to a small positive constant (e.g., 10−6)
indicating very low uncertainty for the respective features.
For both feature conﬁgurations, Table 5 shows the per-
formance of the proposed linear SVM-GSU (LSVM-GSU) in
terms of average precision (AP) [10], [48] for each target
event in comparison with LSVM, PSVM [14], and LSVM-iso
approaches. Moreover, for each dataset, the mean average
precision (MAP) across all target events is reported. The
optimization of the λ parameter for the various SVMs was
performed using a line search on a 10-fold cross-validation
procedure. The bold-faced numbers indicate the best result
achieved for each event class. We also report the results
of the McNemar [52], [53], statistical signiﬁcance tests. A
∗ denotes statistically signiﬁcant differences between the
proposed LSVM-GSU and baseline LSVM, a (cid:5) denotes
statistically signiﬁcant differences between LSVM-GSU and
PSVM, and a ∼ denotes statistically signiﬁcant differences
between LSVM-GSU and LSVM-iso.

From the obtained results, we observe that the pro-
posed algorithm achieved better detection performance than
LSVM, PSVM, and LSVM-iso, in both feature conﬁgura-
tions. For feature conﬁguration 1, the proposed LSVM-GSU
achieved a relative boost of 22.2% compared to the baseline
standard LSVM and 19.4% compared to Power SVM, while
for feature conﬁguration 2 respective relative boosts of
12.7% and 11.7%, respectively, in terms of MAP. We also
experimented using directly the samples from which the
covariance matrix of each example was estimated and ob-
tained inferior results; that is, a MAP of 10.15%, compared
to LSVM’s 14.78% and 18.06% of the proposed SVM-GSU.

5 CONCLUSION

In this paper we proposed a novel classiﬁer that efﬁ-
ciently exploits uncertainty in its input under the SVM
paradigm. The proposed SVM-GSU was evaluated on syn-
thetic data and on ﬁve publicly available datasets; namely,
the MNIST dataset of handwritten digits, the WDBC, the
DEAP for emotion analysis, the TV News Commercial
Detection dataset and TRECVID MED for the problem of
video event detection. For each of the above datasets and

11

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

TABLE 5: Event detection performance (AP and MAP) of the linear SVM-GSU compared to the baseline linear SVM, Power
SVM [14], and a LSVM extension for handling isotropic uncertainty (as in [18], [27]) using the MED15 (for training) and
MED14Test (for testing) datasets.

Event
Class

E021
E022
E023
E024
E025
E026
E027
E028
E029
E030
E031
E032
E033
E034
E035
E036
E037
E038
E039
E040
MAP

LSVM

0.0483
0.0227
0.4159
0.0071
0.0052
0.0457
0.1319
0.4242
0.0812
0.0516
0.4416
0.0280
0.3483
0.0583
0.3330
0.0894
0.0884
0.0261
0.2677
0.0421
0.1478

Feature Conﬁguration 1
(5055-D)

Feature Conﬁguration 2
(10110-D)

PSVM
[14]
0.0510
0.0310
0.4515
0.0081
0.0052
0.0459
0.1424
0.4125
0.0914
0.0551
0.4425
0.0400
0.3614
0.0588
0.3419
0.0748
0.0880
0.0241
0.2698
0.0315
0.1513

LSVM-iso
[18], [27]
0.0500
0.0350
0.6059
0.0097
0.0074
0.0606
0.1174
0.3819
0.1793
0.0877
0.4480
0.0870
0.3901
0.0599
0.3500
0.0695
0.1981
0.0212
0.2959
0.0375
0.1746

LSVM-GSU
(proposed)
0.0515
0.0277
0.6057
0.0105
0.0068
0.0608
0.1219
0.4335
0.1791
0.0884
0.4796
0.1196
0.4187
0.0614
0.3369
0.0704
0.1968
0.0291
0.2757
0.0377
0.1806

McNemar
Tests
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5)
(cid:5)

(cid:5)
∗, (cid:5), ∼
∗, (cid:5), ∼
(cid:5)

∗, (cid:5), ∼
∗, (cid:5), ∼
∗, ∼
(cid:5)
∗, (cid:5), ∼
(cid:5)
∗, (cid:5), ∼
(cid:5)
∗, (cid:5), ∼
∗, (cid:5)

–

LSVM

0.0829
0.0674
0.7050
0.0187
0.0219
0.0731
0.1152
0.1863
0.2046
0.1001
0.7595
0.0989
0.4571
0.3207
0.3516
0.1156
0.1169
0.0558
0.4188
0.0837
0.2177

PSVM
[14]
0.0834
0.0773
0.7236
0.0223
0.0245
0.0745
0.0133
0.2214
0.1987
0.1276
0.7599
0.1011
0.4789
0.3214
0.3419
0.1186
0.1257
0.0498
0.4219
0.0889
0.2187

LSVM-iso
[18], [27]
0.1074
0.1023
0.7802
0.0394
0.0161
0.0976
0.1254
0.2700
0.2149
0.1596
0.7422
0.1290
0.5091
0.3200
0.3252
0.1064
0.1598
0.0557
0.4349
0.0856
0.2390

LSVM-GSU
(proposed)
0.0778
0.1429
0.7943
0.0367
0.0135
0.1109
0.1812
0.2278
0.1999
0.1774
0.7697
0.1292
0.5164
0.3380
0.3059
0.1288
0.1629
0.0539
0.4271
0.0902
0.2442

McNemar
Tests
(cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5), ∼
∗
(cid:5)
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5)
∗
∗, (cid:5), ∼
∗, (cid:5)
∗, (cid:5), ∼
∗, (cid:5), ∼
(cid:5)
∗, (cid:5), ∼
∗, (cid:5)

–

TABLE 6: Mean vector and covariance matrix of the i-th
example for feature conﬁgurations 1 and 2 of the video event
detection experiments.

and Ω+ ∩Ω− = ∅. Then, the integrals I± : Rn ×R → R, deﬁned
as

I±(a, b) (cid:44)

(a(cid:62)x + b)fX(x) dx,

(cid:90)

Ω±

Conﬁguration 1

Conﬁguration 2

xi = (xi,1, . . . , xi,n)(cid:62) ∈ Rn
(cid:1) ∈ Sn
Σi, = diag (cid:0)σ1
xi = (xi,1, . . . , xi,n, σ1
Σi, = diag (cid:0)σ1
i , . . . , σn

i , . . . , σn
i
i , . . . , σn
i , σ0, . . . , σ0

++
i )(cid:62) ∈ R2n
(cid:1) ∈ S2n
++

are given by

problems, either uncertainty information (e.g., variance for
each example and for all or some of the input space di-
mensions) was part of the original dataset, or a method for
modeling and estimating the uncertainty of each training
example was proposed. As shown in the experiments, SVM-
GSU efﬁciently takes input uncertainty into consideration
and achieves better detection or classiﬁcation performance
than standard SVM, previous SVM extensions that model
uncertainty isotropically, and other state of the art methods.
Finally, we plan to investigate the kernalization of the pro-
posed algorithm and the extensions of it for the problem of
regression under Gaussian input uncertainty.

APPENDIX A
ON GAUSSIAN-LIKE INTEGRALS OVER HALFSPACES

Theorem 1. Let X ∈ Rn be a random vector that follows the
multivariate Gaussian distribution with mean vector µ ∈ Rn and
++, where Sn
covariance matrix Σ ∈ Sn
++ denotes the space of n×n
symmetric positive deﬁnite matrices with real entries. The proba-
bility density function of this distribution is given by fX : Rn →
2 (x − µ)(cid:62)Σ−1(x − µ)(cid:1). More-
R, fX(x) =
let H be the hyperplane given by a(cid:62)x + b = 0. H
over,
divides the Euclidean n-dimensional space into two halfspaces,
i.e., Ω± = {x ∈ Rn : a(cid:62)x + b ≷ 0}, so that Ω+ ∪ Ω− = Rn

exp (cid:0)− 1

1
n
2 |Σ|

(2π)

1
2

12

I±(a, b) =

1 ± erf

(cid:20)

dµ
2

(cid:19)(cid:21)

(cid:18) dµ
dΣ

±

dΣ
√
π
2

exp

−

,

(15)

(cid:32)

(cid:33)

d2
µ
d2
Σ

where dµ = a(cid:62)µ + b and dΣ =

2a(cid:62)Σa.

√

Proof. We begin with the integral I+. In our approach we
will need several coordinate transforms. First, we start with
a translation in order to get rid of the mean, x = y+µ. Then

I+(a, b) =
(cid:19)

(cid:18)

(cid:90)

2

−

Ω+
1

1
2

dy,

y(cid:62)Σ−1y

(a(cid:62)y + a(cid:62)µ + b) exp

1
(2π) n
2 |Σ| 1
where Ω+
1 = {y ∈ Rn : a(cid:62)y + a(cid:62)µ + b ≥ 0}. Next, since
Σ ∈ Sn
++, there exist an orthonormal matrix U and a diag-
onal matrix D with positive elements, i.e. the eigenvalues
of Σ, such that Σ = U (cid:62)DU . Thus, it holds that Σ−1 =
(U (cid:62)DU )−1 = U −1D−1(U (cid:62))−1 = U (cid:62)D−1U . Then, by let-
ting z = U y and a1 = U a, we have a(cid:62)y = a(cid:62)(U −1U )y =
1 z, and y(cid:62)Σ−1y = y(cid:62)(U (cid:62)DU )−1y =
a(cid:62)U (cid:62)U z = a(cid:62)
(y(cid:62)U (cid:62))D−1(U y) = (U y)(cid:62)D−1(U y) = z(cid:62)D−1z. Then

I+(a, b) =
(cid:19)

(cid:18)

(cid:90)

2

Ω+
2

(a(cid:62)

1 z + a(cid:62)µ + b) exp

1
(2π) n
2 |Σ| 1
where Ω+
1 z + a(cid:62)µ + b ≥ 0}, since for the
Jacobian J = |U |, it holds that |J| = 1. Now, in order to do
rescaling, we set z = D 1

2 = {z ∈ Rn : a(cid:62)

2 a1. Thus,

z(cid:62)D−1z

dz,

1
2

−

z(cid:62)D−1z = (D

2 v)(cid:62)D−1(D

1

1

2 D−1D

2 )v = v(cid:62)v.

1

2 v and a2 = D 1
2 v) = v(cid:62)(D

1

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

1 z = a(cid:62)

1 (D 1
2 = |Σ| 1

2 v) = (D 1
2 and dz = |D 1

2 a1)(cid:62)v = a(cid:62)

2 |dv = |Σ| 1

2 v. Also,
2 dv.

REFERENCES

Moreover, a(cid:62)
it holds that |D| 1
Consequently,
1
(2π) n

I+(a, b) =

(cid:90)

(a(cid:62)

2 v + a(cid:62)µ + b) exp

−

v(cid:62)v

dv,

(cid:18)

(cid:19)

1
2

2

Ω+
3
3 = {v ∈ Rn : a(cid:62)

where Ω+
2 v + a(cid:62)µ + b ≥ 0}. Let B
be an orthogonal matrix such that Ba2 = (cid:107)a2(cid:107)en, which
also means that a2 = B(cid:62)(cid:107)a2(cid:107)en. Moreover, let m = Bv.
Then, a(cid:62)
2 v = (B(cid:62)(cid:107)a2(cid:107)en)(cid:62)v = (cid:107)a2(cid:107)e(cid:62)
n m.
Moreover, v(cid:62)v = v(cid:62)(B−1B)v = m(cid:62)m. Then
(cid:18)

n (Bv) = (cid:107)a2(cid:107)e(cid:62)

(cid:90) +∞

(cid:19)

I+(a, b) =

(cid:0)(cid:107)a2(cid:107)t + a(cid:62)µ + b(cid:1) exp

1
√
2π
(cid:107)a2(cid:107) . Since (cid:107)a2(cid:107)2 = a(cid:62)Σa,

c

where c = − a(cid:62)µ+b

−

t2

dt,

1
2

I+(a, b) =

(cid:90) +∞

√

1
√
2π

c

(cid:0)

a(cid:62)Σat+a(cid:62)µ+b(cid:1) exp

−

t2

dt,

(cid:18)

(cid:19)

1
2

which is easily evaluated as (15). Following similar argu-
ments as above, we arrive at I−.

APPENDIX B
MODELING THE UNCERTAINTY OF AN IMAGE
Let f (0) = (f1(0), . . . , fj(0), . . . , fn(0))(cid:62) ∈ Rn be an
image with n pixels in row-wise form, and let f (t) =
(f1(t), . . . , fj(t), . . . , fn(t))(cid:62) ∈ Rn be a translated version
of it by t = (h, v)(cid:62) pixels. Clearly, fj : R2 → R denotes the
intensity function of the j-th pixel, after a translation by t.

We will use the multivariate Taylor’s theorem in order
to approximate the intensity function of the j-th pixel of
the given image; i.e., function fj. That is, the intensity is
approximated as fj(t) = fj(0) + ∇(cid:62)fj(0)t. Then,

f (t) = f (0) +

(16)






∇(cid:62)f1(0)
...
∇(cid:62)fn(0)




 t.

Let us now assume that t is a random vector distributed
normally with mean µt and covariance matrix Σt,
i.e.
t ∼ N (µt, Σt). Then, X = f (t) is also distributed normally
with mean vector and covariance matrix that are given,
respectively, by

µX = f (0) +

E [t] ,

(17)






∇(cid:62)f1(0)
...
∇(cid:62)fn(0)






and

ΣX =






∇(cid:62)f1(0)
...
∇(cid:62)fn(0)






 Σt




(cid:62)






.

∇(cid:62)f1(0)
...
∇(cid:62)fn(0)

(18)

Thus, if t ∼ N (µt, Σt), then X ∼ N (µX , ΣX ), where the
mean vector µX and the covariance matrix ΣX are given by
(17) and (18), respectively.

ACKNOWLEDGMENT
This work was supported by the EU’s Horizon 2020 pro-
gramme H2020-693092 MOVING. We would also like to
thank the authors of [13] for providing an implementation
of Power SVM.

[1] V. Vapnik, The nature of statistical learning theory. Springer Heidel-

berg, 1995.

[2] F. Solera, S. Calderara, and R. Cucchiara, “Socially constrained
structural learning for groups detection in crowd,” Pattern Analysis
and Machine Intelligence, IEEE Trans. on, vol. 38, no. 5, pp. 995–1008,
2016.

[3] C. Sentelle, G. C. Anagnostopoulos, and M. Georgiopoulos, “A
simple method for solving the SVM regularization path for
semideﬁnite kernels,” Neural Networks and Learning Systems, IEEE
Trans. on, vol. 27, no. 4, pp. 709–722, 2016.

[4] A. Diplaros, T. Gevers, and I. Patras, “Combining color and shape
information for illumination-viewpoint invariant object recogni-
tion,” IEEE Transactions on Image Processing, vol. 15, no. 1, pp. 1–11,
2006.

[5] Y. Li, J. Chen, and L. Feng, “Dealing with uncertainty: a survey
of theories and practices,” Knowledge and Data Engineering, IEEE
Trans. on, vol. 25, no. 11, pp. 2463–2482, 2013.

[6] M. P. Deisenroth, D. Fox, and C. E. Rasmussen, “Gaussian pro-
cesses for data-efﬁcient learning in robotics and control,” Pattern
Analysis and Machine Intelligence, IEEE Trans. on, vol. 37, no. 2, pp.
408–423, 2015.

[7] Y. Bengio, A. Courville, and P. Vincent, “Representation learning:
A review and new perspectives,” Pattern Analysis and Machine
Intelligence, IEEE Trans. on, vol. 35, no. 8, pp. 1798–1828, 2013.
[8] A. J. Joshi, F. Porikli, and N. Papanikolopoulos, “Multi-class active
learning for image classiﬁcation,” in Computer Vision and Pattern
Recognition, Conf. on.

IEEE, 2009, pp. 2372–2379.

[9] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” Pattern Analysis and Machine Intelligence, IEEE Trans.
on, vol. 38, no. 3, pp. 447–461, 2016.

[10] C. Tzelepis, N. Gkalelis, V. Mezaris, and I. Kompatsiaris, “Improv-
ing event detection using related videos and relevance degree
support vector machines,” in Proc. of the 21st ACM Int. Conf. on
Multimedia. ACM, 2013, pp. 673–676.

[11] C. Tzelepis, D. Galanopoulos, V. Mezaris, and I. Patras, “Learning
to detect video events from zero or very few video examples,”
Image and vision Computing, vol. 53, pp. 35–44, 2016.

[12] M. Li and I. K. Sethi, “Conﬁdence-based active learning,” Pattern
Analysis and Machine Intelligence, IEEE Trans. on, vol. 28, no. 8, pp.
1251–1261, 2006.

[13] I. Saraﬁs, C. Diou, and A. Delopoulos, “Building effective svm
concept detectors from clickthrough data for large-scale image
retrieval,” Int. Journal of Multimedia Information Retrieval, vol. 4,
no. 2, pp. 129–142, 2015.

[14] W. Zhang, X. Y. Stella, and S.-H. Teng, “Power SVM: Generaliza-
tion with exemplar classiﬁcation uncertainty,” in Computer Vision
and Pattern Recognition (CVPR), 2012 IEEE Conf. on.
IEEE, 2012,
pp. 2144–2151.

[15] K. Crammer, A. Kulesza, and M. Dredze, “Adaptive regularization
of weight vectors,” in Advances in neural information processing
systems, 2009, pp. 414–422.

[16] M. Dredze, K. Crammer, and F. Pereira, “Conﬁdence-weighted lin-
ear classiﬁcation,” in Proceedings of the 25th international conference
on Machine learning. ACM, 2008, pp. 264–271.

[17] S. C. H. Hoi, J. Wang, and P. Zhao, “Exact soft conﬁdence-weighted
learning,” in Proceedings of the 29th International Conference on
Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 -
July 1, 2012, 2012.

[18] J. Bi and T. Zhang, “Support vector classiﬁcation with input data
uncertainty.” in Advances in Neural Information Processing Systems,
2004.

[19] F. Alizadeh and D. Goldfarb, “Second-order cone programming,”

Mathematical programming, vol. 95, no. 1, pp. 3–51, 2003.

[20] A. Ben-Tal and A. Nemirovski, “Robust convex optimization,”
Mathematics of Operations Research, vol. 23, no. 4, pp. 769–805, 1998.
[21] D. Bertsimas, D. B. Brown, and C. Caramanis, “Theory and appli-
cations of robust optimization,” SIAM review, vol. 53, no. 3, pp.
464–501, 2011.

[22] G. R. Lanckriet, L. E. Ghaoui, C. Bhattacharyya, and M. I. Jordan,
“A robust minimax approach to classiﬁcation,” The Journal of
Machine Learning Research, vol. 3, pp. 555–582, 2003.

[23] P. K. Shivaswamy, C. Bhattacharyya, and A. J. Smola, “Second
order cone programming approaches for handling missing and
uncertain data,” The Journal of Machine Learning Research, vol. 7,
pp. 1283–1314, 2006.

13

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

[24] C. Bhattacharyya, P. K. Shivaswamy, and A. J. Smola, “A sec-
ond order cone programming formulation for classifying missing
data.” in Advances in Neural Information Processing Systems, 2004.

[25] H. Xu, C. Caramanis, and S. Mannor, “Robustness and regulariza-
tion of support vector machines,” The Journal of Machine Learning
Research, vol. 10, pp. 1485–1510, 2009.

[26] H. Xu and S. Mannor, “Robustness and generalization,” Machine

learning, vol. 86, no. 3, pp. 391–423, 2012.

[27] Z. Qi, Y. Tian, and Y. Shi, “Robust twin support vector machine
for pattern classiﬁcation,” Pattern Recognition, vol. 46, no. 1, pp.
305–316, 2013.

[28] O. L. Mangasarian and E. W. Wild, “Multisurface proximal sup-
port vector machine classiﬁcation via generalized eigenvalues,”
Pattern Analysis and Machine Intelligence, IEEE Trans. on, vol. 28,
no. 1, pp. 69–74, 2006.

[29] R. Khemchandani, S. Chandra et al., “Twin support vector ma-
chines for pattern classiﬁcation,” Pattern Analysis and Machine
Intelligence, IEEE Trans. on, vol. 29, no. 5, pp. 905–910, 2007.
[30] F. De La Torre and M. J. Black, “A framework for robust subspace
learning,” Int. Journal of Computer Vision, vol. 54, no. 1-3, pp. 117–
142, 2003.

[31] S. Liwicki, S. Zafeiriou, G. Tzimiropoulos, and M. Pantic, “Efﬁcient
online subspace learning with an indeﬁnite kernel for visual
tracking and recognition,” Neural Networks and Learning Systems,
IEEE Trans. on, vol. 23, no. 10, pp. 1624–1636, 2012.

[32] H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, “Uncorre-
lated multilinear principal component analysis for unsupervised
multilinear subspace learning,” Neural Networks, IEEE Trans. on,
vol. 20, no. 11, pp. 1820–1836, 2009.

[33] T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu, “The entire reg-
ularization path for the support vector machine,” The Journal of
Machine Learning Research, vol. 5, pp. 1391–1415, 2004.

[34] L. Rosasco, E. D. Vito, A. Caponnetto, M. Piana, and A. Verri, “Are
loss functions all the same?” Neural Computation, vol. 16, no. 5, pp.
1063–1076, 2004.

[35] T. Zhang, “Statistical behavior and consistency of classiﬁcation
methods based on convex risk minimization,” Annals of Statistics,
pp. 56–85, 2004.

[36] J. Platt et al., “Probabilistic outputs for support vector machines
and comparisons to regularized likelihood methods,” Advances in
large margin classiﬁers, vol. 10, no. 3, pp. 61–74, 1999.

[37] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector
machines,” ACM Trans. on Intelligent Systems and Technology, vol. 2,
pp. 27:1–27:27, 2011, software available at http://www.csie.ntu.
edu.tw/∼cjlin/libsvm.

[38] B. S. Tsirelson, I. A. Ibragimov, and V. N. Sudakov, Norms
of Gaussian sample
Berlin, Heidelberg: Springer
Berlin Heidelberg, 1976, pp. 20–41. [Online]. Available: http:
//dx.doi.org/10.1007/BFb0077482

functions.

[39] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter, “Pegasos:
Primal estimated sub-gradient solver for SVM,” Mathematical pro-
gramming, vol. 127, no. 1, pp. 3–30, 2011.

[40] S. M. Kakade and A. Tewari, “On the generalization ability of
online strongly convex programming algorithms,” in Advances in
Neural Information Processing Systems, 2009, pp. 801–808.

[41] L. Bottou, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, L. D.
Jackel, Y. LeCun, U. A. Muller, E. Sackinger, P. Simard et al.,
“Comparison of classiﬁer methods: a case study in handwritten
digit recognition,” in Pattern Recognition, 1994. Vol. 2-Conference B:
Computer Vision & Image Processing., Proceedings of the 12th IAPR
International. Conference on, vol. 2.

IEEE, 1994, pp. 77–82.

[42] A. Ghio, D. Anguita, L. Oneto, S. Ridella, and C. Schatten, “Nested
sequential minimal optimization for support vector machines,”
in Artiﬁcial Neural Networks and Machine Learning–ICANN 2012.
Springer, 2012, pp. 156–163.

[43] W. W. Hines, D. C. Montgomery, and D. M. G. C. M. Borror,

Probability and statistics in engineering.

John Wiley & Sons, 2008.

[44] M. Lichman, “UCI machine learning repository,” 2013. [Online].

Available: http://archive.ics.uci.edu/ml
[45] S. Koelstra, C. M ¨uhl, M. Soleymani,

J.-S. Lee, A. Yazdani,
T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras, “DEAP: A database
for emotion analysis; using physiological signals,” Affective Com-
puting, IEEE Trans. on, vol. 3, no. 1, pp. 18–31, 2012.

[46] P. D. Welch, “The use of fast fourier transform for the estimation
of power spectra: A method based on time averaging over short,
modiﬁed periodograms,” IEEE Trans. on Audio and Electroacoustics,
vol. 15, no. 2, pp. 70–73, 1967.

14

[47] A. Vyas, R. Kannao, V. Bhargava, and P. Guha, “Commercial block
detection in broadcast news videos,” in Proceedings of the 2014
Indian Conference on Computer Vision Graphics and Image Processing.
ACM, 2014, p. 63.

[48] P. Over, G. Awad, J. Fiscus, M. Michel, D. Joy, A. F. Smeaton,
W. Kraaij, G. Quenot, and R. Ordelman, “TRECVID 2015 – an
overview of the goals, tasks, data, evaluation mechanisms and
metrics,” in Proc. of TRECVID 2015. NIST, USA, 2015.

[49] L. Jiang, S.-I. Yu, D. Meng, T. Mitamura, and A. G. Hauptmann,
“Bridging the ultimate semantic gap: A semantic search engine for
internet videos,” in ACM Int. Conf. on Multimedia Retrieval, 2015.

[50] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in IEEE Conf. on Computer Vision and Pattern Recog-
nition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, 2015, pp. 1–9.
[51] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
Int. Journal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211–252,
2015.

[52] Q. McNemar, “Note on the sampling error of the difference
between correlated proportions or percentages,” Psychometrika,
vol. 12, no. 2, pp. 153–157, 1947.

[53] N. Gkalelis, V. Mezaris, I. Kompatsiaris, and T. Stathaki, “Mixture
subclass discriminant analysis link to restricted gaussian model
and other generalizations,” Neural Networks and Learning Systems,
IEEE Trans. on, vol. 24, no. 1, pp. 8–21, 2013.

Christos Tzelepis received the Diploma degree
in electrical engineering from Aristotle Univer-
sity of Thessaloniki, Greece, in 2011. During his
diploma thesis, he focused on machine learning
techniques with training data of variable relia-
bility. Currently, he is a PhD student in Elec-
tronic Engineering and Computer Science at
Queen Mary, University of London, within the
ﬁeld of discriminative machine learning, and with
ITI/CERTH.

Vasileios Mezaris received the BSc and PhD
in Electrical and Computer Engineering from the
Aristotle University of Thessaloniki in 2001 and
2005, respectively. He is a Senior Researcher
(Researcher B) at the Information Technologies
Institute (ITI) of the Centre for Research of Tech-
nology Hellas (CERTH). His research interests
include image and video analysis, event detec-
tion in multimedia, machine learning for multime-
dia, and image and video retrieval. He has co-
authored more than 35 journal papers, 10 book
chapters, 140 conference papers and 3 patents. He is/was an Associate
Editor for the IEEE Signal Processing Letters (2016-present) and IEEE
Tran. on Multimedia (2012-2015), and is a Senior Member of the IEEE.

Ioannis Patras received the BSc and MSc de-
grees in computer science from the Computer
Science Department, University of Crete, Herak-
lion, Greece, in 1994 and 1997, respectively, and
the PhD degree from the Department of Electri-
cal Engineering, Delft University of Technology,
Delft (TU Delft), The Netherlands, in 2001. He is
a Reader (Associate Professor) in the School of
Electronic Engineering and Computer Science
Queen Mary University of London, London, U.K.
His current research interests are in the area
of Computer Vision, Machine Learning and Affective Computing, with
emphasis on the analysis of visual data depicting humans and their
activities. He is an Associate Editor of the Image and Vision Computing
Journal, Pattern Recognition, and Computer Vision and Image Under-
standing.

Linear Maximum Margin Classiﬁer for Learning
from Uncertain Data

Christos Tzelepis, Student Member, IEEE, Vasileios Mezaris, Senior Member, IEEE,
and Ioannis Patras, Senior Member, IEEE

1

(cid:70)

Abstract—In this paper, we propose a maximum margin classiﬁer that deals with uncertainty in data input. More speciﬁcally, we reformulate the
SVM framework such that each training example can be modeled by a multi-dimensional Gaussian distribution described by its mean vector and
its covariance matrix – the latter modeling the uncertainty. We address the classiﬁcation problem and deﬁne a cost function that is the expected
value of the classical SVM cost when data samples are drawn from the multi-dimensional Gaussian distributions that form the set of the training
examples. Our formulation approximates the classical SVM formulation when the training examples are isotropic Gaussians with variance tending
to zero. We arrive at a convex optimization problem, which we solve efﬁciently in the primal form using a stochastic gradient descent approach. The
resulting classiﬁer, which we name SVM with Gaussian Sample Uncertainty (SVM-GSU), is tested on synthetic data and ﬁve publicly available and
popular datasets; namely, the MNIST, WDBC, DEAP, TV News Channel Commercial Detection, and TRECVID MED datasets. Experimental results
verify the effectiveness of the proposed method.

Index Terms—Classiﬁcation, convex optimization, Gaussian anisotropic uncertainty, large margin methods, learning with uncertainty, statistical
learning theory

7
1
0
2
 
v
o
N
 
9
1
 
 
]

G
L
.
s
c
[
 
 
2
v
2
9
8
3
0
.
4
0
5
1
:
v
i
X
r
a

1 INTRODUCTION

S UPPORT Vector Machine (SVM) has been shown to be a

powerful paradigm for pattern classiﬁcation. Its origins
can be traced back to [1]. Vapnik established the standard
regularized SVM algorithm for computing a linear discrim-
inative function that optimizes the margin between the
so called support vectors and the separating hyperplane.
Despite the fact that the standard SVM algorithm is a
well-studied and general framework for statistical learning
analysis, it is still an active research ﬁeld (e.g., [2], [3]).

However, the classical SVM formulation, as well as the
majority of classiﬁcation methods, do not explicitly model
input uncertainty. In standard SVM, each training datum is
a vector, whose position in the feature space is considered
certain. This does not model the fact that measurement
inaccuracies or artifacts of the feature extraction process
contaminate the training examples with noise. In several
cases the noise distribution is known or can be modeled;
e.g., there are cases where each training example represents
the average of several measurements or of several samples
whose distribution around the mean can be modeled or
estimated. Finally, in some cases it is possible to model
the process by which the data is generated, for example by
modeling the process by which new data is generated from
transforms applied on an already given training dataset.

C. Tzelepis is with the School of Electronic Engineering and Computer Science,
Queen Mary University of London, London E1 4NS, U.K. and also with the
Information Technologies Institute/Centre for Research and Technology Hellas
(CERTH), Thermi 57001, Greece, (email: c.tzelepis@qmul.ac.uk).
V. Mezaris is with the Information Technologies Institute/Centre for Re-
search and Technology Hellas (CERTH), Thermi 57001, Greece (email:
bmezaris@iti.gr).
I. Patras is with the School of Electronic Engineering and Computer Sci-
ence, Queen Mary University of London, London E1 4NS, U.K. (e-mail:
i.patras@qmul.ac.uk).

Fig. 1: Linear SVM with Gaussian Sample Uncertainty
(LSVM-GSU). The solid line depicts the decision boundary
of the proposed algorithm, and the dashed line depicts the
decision boundary of the standard linear SVM (LSVM).

In this work, we consider that our training examples
are multivariate Gaussian distributions with known means
and covariance matrices – each example having a different
covariance matrix expressing the uncertainty around its
mean. An illustration is given in Fig. 1, where the shaded
regions are bounded by iso-density loci of the Gaussians,
and the means of the Gaussians for examples of the positive
and negative classes are located at × and ◦ respectively. A
classical SVM formulation would consider only the means
of the Gaussians as training examples and, by optimizing
the soft margin using the hinge loss and a regularization
term, would arrive at the separating hyperplane depicted by
the dashed line. In our formulation, we optimize for the soft

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

margin using the same regularization but the expected value
of the hinge loss, where the expectation is taken under the
given Gaussians. By doing so, we take into consideration
the various uncertainties and arrive at a drastically different
decision border, depicted by the solid line in Fig. 1. It is
worth noting that one would arrive at the same decision
border with the classical SVM trained on a dataset contain-
ing samples drawn from the Gaussians in question, as the
number of samples tend to inﬁnity. In addition, our method
degenerates to a classical SVM in the case that all of the
Gaussians are isotropic with a variance that tends to zero.

Our work differs from previous works that model uncer-
tainty in the SVM framework either by considering isotropic
noise or by using expensive sampling schemes to approx-
imate their loss functions. By contrast, our formulation
allows for full covariance matrices that can be different
for each example. This allows dealing, among others, with
cases where the uncertainty of only a few examples, and/or
the uncertainty along only a few of their dimensions, is
known or modeled. In the experimental results section we
show several real-world problems in which such modeling
is beneﬁcial. More speciﬁcally, we show cases, in which the
variances along (some) of the dimensions are part of the
dataset – this includes medical data where both the means
and the variances of several measurements are reported,
and large scale video datasets, where the means and the
variances of some of the features that are extracted at several
time instances in the video in question are reported. We
then show a case in which means and variances are a by-
product of the feature extraction method, namely the Welch
method for extracting periodograms from temporal EEG
data. And ﬁnally, we show a case in which, for an image
dataset (MNIST) we model the distribution of images under
small geometric transforms as Gaussians, using a ﬁrst-order
Taylor approximation to arrive in an analytic form.

In general, modeling of the uncertainty is a domain-
and/or dataset-speciﬁc problem, and in this respect, sim-
ilarly to all of the other methods in the literature that
model/use uncertainties, we do not offer a deﬁnitive answer
on how this can or should be done on any existing dataset.
We note, however, that means and (co)-variances are the
most commonly reported statistics and that the modeling
used in Sect. 4.2, 4.4 could be used also in other similar
datasets. In particular, the Taylor expansion method (Ap-
pendix B) that is behind the modeling used in Sect. 4.2,
has been used to model the propagation of uncertainties
due to a feature extraction process in other domains; for
instance, in [4] (Sect. II.B) this is used to model as Gaussian
the uncertainty in the estimation of illumination invariant
image derivatives. Finally, in our work the cost function,
which is based on the expectation of the hinge loss, and its
derivatives, can be calculated in closed forms. This allows an
efﬁcient implementation using a stochastic gradient descent
(SGD) algorithm.

The remainder of this paper is organized as follows.
In Section 2, we review related work, focusing on SVM-
based formulations that explicitly model data uncertainty.
In Section 3, we present the proposed algorithm which we
call SVM with Gaussian Sample Uncertainty (SVM-GSU).
In Section 4, we provide the experimental results of the
application of SVM-GSU to synthetic data and to ﬁve pub-

licly available and popular datasets. In the same section, we
provide comparisons with the standard SVM and other state
of the art methods. In Section 5, we draw some conclusions
and give directions for future work.

2 RELATED WORK
Uncertainty is ubiquitous in almost all ﬁelds of scientiﬁc
studies [5]. Exploiting uncertainty in supervised learning
has been studied in many different aspects [6], [7], [8]. More
speciﬁcally, the research community has studied learning
problems where uncertainty is present either in the labels or
in the representation of the training data.

In [9], Liu and Tao studied a classiﬁcation problem
in which sample labels are randomly corrupted. In this
scenario, there is an unobservable sample with noise-free
labels. However, before being observed, the true labels are
independently ﬂipped with a probability p ∈ [0, 0.5), and
the random label noise can be class-conditional. Tzelepis et
al. [10], [11] proposed an SVM extension where each training
example is assigned a relevance degree in (0, 1] expressing
the conﬁdence that the respective example belongs to the
given class. Li and Sethi [12] proposed an active learning
approach based on identifying and annotating uncertain
samples. Their approach estimates the uncertainty value
for each input sample according to its output score from
a classiﬁer and selects only samples with uncertainty value
above a user-deﬁned threshold. In [13], the authors used
weights to quantify the conﬁdence of automatic training
label assignment to images from clicks and showed that
using these weights with Fuzzy SVM and Power SVM [14]
can lead to signiﬁcant improvements in retrieval effective-
ness compared to the standard SVM. Finally, the problem of
conﬁdence-weighted learning is addressed in [15], [16], [17],
where uncertainty in the weights of a linear classiﬁer (under
online learning conditions) is taken into consideration.

Assuming uncertainty in data representation has also
drawn the attention of the research community in recent
years. Different types of robust SVMs have been proposed
in several recent works. Bi and Zhang [18] considered a
statistical formulation where the input noise is modeled
as a hidden mixture component, but in this way the “iid”
assumption for the training data is violated. In that work,
the uncertainty is modeled isotropically. Second order cone
programming (SOCP) [19] methods have also been em-
ployed in numerous works to handle missing and uncertain
data. In addition, Robust Optimization techniques [20], [21]
have been proposed for optimization problems where the
data is not speciﬁed exactly, but it is known to belong to
a given uncertainty set U , yet the optimization constraints
must hold for all possible values of the data from U .

Lanckriet et al. [22] considered a binary classiﬁcation
problem where the mean and covariance matrix of each
class are assumed to be known. Then, a minimax problem is
formulated such that the worst-case (maximum) probability
of misclassiﬁcation of future data points is minimized. That
is, under all possible choices of class-conditional densities
with a given mean and covariance matrix, the worst-case
probability of misclassiﬁcation of new data is minimized.

Shivaswamy et al. [23], who extended Bhattacharyya et
al. [24], also adopted a SOCP formulation and used gen-
eralized Chebyshev inequalities to design robust classiﬁers

2

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

dealing with uncertain observations. In their work uncer-
tainty arises in ellipsoidal form, as follows from the mul-
tivariate Chebyshev inequality. This formulation achieves
robustness by requiring that the ellipsoid of every uncertain
data point should lie in the correct halfspace. The expected
error of misclassifying a sample is obtained by computing
the volume of the ellipsoid that lies on the wrong side of
the hyperplane. However, this quantity is not computed an-
alytically; instead, a large number of uniformly distributed
points are generated in the ellipsoid, and the ratio of the
number of points on the wrong side of the hyperplane to
the total number of generated points is computed.

Several works [22], [23], [24] robustiﬁed regularized
classiﬁcation using box-type uncertainty. By contrast, Xu
et al. [25], [26] considered the robust classiﬁcation prob-
lem for a class of non-box-typed uncertainty sets; that
is, they considered a setup where the joint uncertainty is
the Cartesian product of uncertainty in each input. This
leads to penalty terms on each constraint of the resulting
formulation. Furthermore, Xu et al. gave evidence on the
equivalence between the standard regularized SVM and this
robust optimization formulation, establishing robustness as
the reason why regularized SVMs generalize well.

In [27], motivated by GEPSVM [28], Qi et al. robusti-
ﬁed a twin support vector machine (TWSVM) [29]. Robust
TWSVM [27] deals with data affected by measurement noise
using a SOCP formulation. In their work, the input data
is contaminated with isotropic noise (i.e., spherical distur-
bances centred at the training examples), and thus cannot
model real-world uncertainty, which is typically described
by more complex noise patterns. Power SVM [14] uses a
spherical uncertainty measure for each training example. In
this formulation, each example is represented by a spherical
region in the feature space, rather than a point. If any point
of this region is classiﬁed correctly, then the corresponding
loss introduced is zero.

Our proposed classiﬁer does not violate the “iid” as-
sumption for the training input data (in contrast to [18]),
and can model the uncertainty of each input training ex-
ample using an arbitrary covariance matrix; that is, it al-
lows anisotropic modeling of the uncertainty analytically
in contrast to [14], [23], [27]. Moreover, we deﬁne a cost
function that is convex and whose derivatives with respect
to the parameters of the unknown separating hyperplane
can be expressed in closed form. Therefore, we can ﬁnd their
global optimal using an iterative gradient descent algorithm
whose complexity is linear with respect to the number of
training data. Finally, we apply a linear subspace learning
approach in order to address the situation where most of the
mass of the Gaussians lies in a low dimensional manifold
that can be different for each Gaussian, and subsequently
solve the problem in lower-dimensional spaces. Learning
in subspaces is widely used in various statistical learning
problems [30], [31], [32].

3 PROPOSED APPROACH

Gaussian distributions; that is, each training example con-
sists of a mean vector xi ∈ D and a covariance matrix
Σi ∈ Sn
++; the latter expresses the uncertainty around the
corresponding mean*. In Sect. 3.1, we ﬁrst brieﬂy review the
linear SVM and then describe in detail the proposed linear
SVM with Gaussian Sample Uncertainty (SVM-GSU). In
Sect. 3.2 we motivate and describe a formulation that allows
learning in linear subspaces. In the general case we arrive
at different subspaces for the different Gaussians – this
allows, for example, dealing with covariance matrices that
are of low rank. In Sect. 3.3 we discuss how the proposed
algorithm relates to standard SVM when the latter is fed
with samples drawn from the input Gaussians. Finally, in
Sect. 3.4 we describe a SGD algorithm for efﬁciently solving
the SVM-GSU optimization problem.

3.1 SVM with Gaussian Sample Uncertainty

We begin by brieﬂy describing the standard SVM algorithm.
Let us consider the supervised learning framework and
denote the training set with X = (cid:8)(xi, yi) : xi ∈ Rn, yi ∈
{±1}, i = 1, . . . , (cid:96)(cid:9)
, where xi is a training example and yi
is the corresponding class label. Then, the standard linear
SVM learns a hyperplane H : w(cid:62)x + b = 0 that minimizes
with respect to w, b the following objective function:

(cid:107)w(cid:107)2 +

max

0, 1 − yi(w(cid:62)xi + b)

(1)

(cid:17)

,

λ
2

1
(cid:96)

(cid:96)
(cid:88)

i=1

(cid:16)

where h(t) = max(0, 1 − t) is the “hinge” loss function [33].
An illustrative example of the hinge loss calculation is given
in Fig. 2, where in Fig. 2a the red dashed line indicates the
loss introduced by the misclassiﬁed example (xi, yi) and in
Fig. 2c the hinge loss is shown in the black bold line.

In this work we assume that, instead of the i-th training
example in the form of a vector, we are given a multivariate
Gaussian distribution with mean vector xi and covariance
matrix Σi. One could think of this as that the covariance
matrix, Σi, models the uncertainty about the position of
training samples around xi. Formally, our training set is
i.e., X (cid:48) =
a set of (cid:96) annotated Gaussian distributions,
++, yi ∈ {±1}, i = 1, . . . , (cid:96)(cid:9)
(cid:8)(xi, Σi, yi) : xi ∈ Rn, Σi ∈ Sn
,
where xi ∈ Rn and Σi ∈ Sn
++ are respectively the mean
vector and the covariance matrix of the i-th example, and
yi is the corresponding label. Then, we deﬁne (cid:96) random
variables, Xi, each of which we assume that follows the cor-
responding n-dimensional Gaussian distribution N (xi, Σi)
and deﬁne an optimization problem where the misclassi-
ﬁcation cost for the i-th example is the expected value of
the hinge loss for the corresponding Gaussian. Formally, the
optimization problem, in its unconstrained primal form, is
the minimization with respect to w, b of

λ
2

(cid:107)w(cid:107)2 +

(cid:90)

(cid:96)
(cid:88)

1
(cid:96)

i=1

Rn

max (cid:0)0, 1 − yi(w(cid:62)x + b)(cid:1)fXi(x) dx, (2)

(x − xi)(cid:1)
where fXi(x) =
is the probability density function (PDF) of the i-th Gaussian

2 (x − xi)(cid:62)Σ−1

exp (cid:0)− 1

1
n
2 |Σi|

(2π)

1
2

i

In this section we develop a new classiﬁcation algorithm
whose training set is not just a set of vectors xi in some
multi-dimensional space, but rather a set of multivariate

*D is typically a subset of the n-dimensional Euclidean space of
++ denotes the convex cone of all symmetric

column vectors, while Sn
positive deﬁnite n × n matrices with entries in D ⊆ Rn.

3

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

(a)

(b)

(c)

Fig. 2: Illustrative example of calculating (a) the standard linear SVM’s hinge loss, and (b) the proposed linear SVM-GSU’s
loss. In (c), the hinge loss is compared with the proposed linear SVM-GSU’s loss for various quantities of uncertainty.

distribution. The above objective function J : Rn × R → R
can be written as

J (w, b) =

(cid:107)w(cid:107)2 +

L(cid:0)w, b; (xi, Σi, yi)(cid:1),

(3)

λ
2

1
(cid:96)

(cid:96)
(cid:88)

i=1

where, as stated above, the loss function L for the i-th
example (i.e. the i-th Gaussian) is deﬁned as the expected
value of the hinge loss for the Gaussian in question. That is,

(cid:90)
L(w, b)=

Rn

max (cid:0)0, 1− yi(w(cid:62)x + b)(cid:1)fXi(x) dx.

(4)

We proceed to express the objective function (3) and its
derivatives in closed form. This will allow us to solve the
corresponding optimization problem using an efﬁcient SGD
approach. More speciﬁcally, the loss can be expressed as

L(w, b)=

(cid:104)

(cid:105)
1 − yi(w(cid:62)x + b)

fXi(x) dx,

(5)

(cid:90)

Ωi

where Ωi denotes the halfspace of Rn that is deﬁned by
the hyperplane H(cid:48) : yi(w(cid:62)x + b) = 1 as Ωi = {x ∈
Rn : yi(w(cid:62)x + b) ≤ 1}, and is the halfspace to which
misclassiﬁed samples lie. This is illustrated in Fig. 2b,
where a misclassiﬁed example (xi, Σi, yi) introduces a loss
indicated by the shaded region. For the calculation of this
loss, all points that belong to the halfspace Ωi = {x ∈
Rn : yi(w(cid:62)x + b) ≤ 1}, i.e., the points x(cid:48) ∈ Ωi, contribute
to it by a quantity of [1 − yi(w(cid:62)x(cid:48) + b)]fXi(x(cid:48)). For one
such x(cid:48) denoted by a red circle in Fig. 2b, the ﬁrst part of
the above product, 1 − yi(w(cid:62)x(cid:48) + b), corresponds to the
typical hinge loss of SVM, shown as a red dashed line in
this example. The total loss introduced by the misclassiﬁed
example (xi, Σi, yi) is obtained by integrating all these
quantities over the halfspace Ωi.

Using Theorem 1 proved in Appendix A, for the half-
, the above inte-

(cid:0)w(cid:62)x + b(cid:1) ≤ 1(cid:9)

i = (cid:8)x ∈ Rn : yi

space Ω+
gral is evaluated in terms of w and b as follows
(cid:32)
(cid:18) dxi
dΣi
(cid:0)w(cid:62)xi + b(cid:1)

dΣi
√
π
2

L(w, b) =

dxi
2

exp

+ 1

erf

+

(cid:19)

(cid:21)

(cid:20)

, dΣi = (cid:112)2w(cid:62)Σiw, and
where dxi = 1 − yi
erf : R → (−1, 1) is the error function, deﬁned as erf(x) =

d2
xi
d2
Σi

, (6)

−

(cid:33)

2√
π

(cid:82) x
0 e−t2

dt. For a training example (x, Σ, y), Fig. 2c shows
the proposed loss in dashed green lines for constant values
of dΣ (constant amounts of uncertainty). We note that as
dΣ → 0, SVM-GSU’s loss virtually coincides with the SVM’s
hinge loss, while it can be easily veriﬁed that, regardless
of dΣ, as dx → ∞ the SVM-GSU’s loss will eventually
converge to zero (as the hinge loss does).

Let us note that the covariance matrix of each training ex-
ample describes the uncertainty around the corresponding
mean; that is, as the covariance matrix approaches the zero
matrix, the certainty increases. At the extreme†, as Σ → 0,
the proposed loss converges to the hinge loss function used
in the standard SVM formulation [33]. This implies that the
proposed formulation is a generalization of the standard
SVM; the two classiﬁers are equivalent when the covariance
matrices tend to the zero matrix.

It is easy to show that the objective function (3) is convex
with respect to w and b; therefore, we propose a SGD
algorithm in Sect. 3.4 for solving the corresponding opti-
mization problem. Since the objective function is convex,
we can obtain the global optimal solution. Moreover, it can
be shown that the proposed loss function (4) enjoys the
consistency property [34], [35], i.e., it leads to consistent
results with the 0 − 1 loss given the presence of inﬁnite data.
By differentiating J with respect to w and b, we obtain,
respectively,

∂J
∂w

= λw +

1
(cid:96)

(cid:96)
(cid:88)

i=1

(cid:34) exp (cid:0)−d2
√

(cid:1)

xi/d2
Σi

Σiw

πdΣi
(cid:18)

−

1
2

erf

(cid:18) dxi
dΣi

(cid:19)

(cid:19)

(cid:35)

+ 1

xi

,

(7)

∂J
∂b

= −

1
(cid:96)

(cid:96)
(cid:88)

(cid:20)

erf

i=1

(cid:18) dxi
dΣi

(cid:19)

(cid:21)

+ 1

.

(8)

Despite the complex appearance of the loss function and
its derivatives, their computation essentially requires the
calculation of the inner product w(cid:62)xi (which is the same as

†A zero covariance matrix exists due to the well known property
that the set of symmetric positive deﬁnite matrices is a convex cone
with vertex at zero.

4

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

n(n+1)

in standard SVM), plus that of the quadratic form w(cid:62)Σiw,
2 multiplications, since Σi is symmet-
which requires
ric. The latter, in the case of diagonal covariance matrices,
is equivalent to the computation of an inner product, i.e., of
complexity O(n). Moreover, each one of w(cid:62)xi and w(cid:62)Σiw
needs to be computed just once for calculating the loss
function and its derivatives for a given w. It is worth noting
that, in practice, as shown in Sect. 4, in real-world problems
uncertainty usually rises in diagonal form. In such cases,
the proposed algorithm is quite efﬁcient and exhibits very
similar complexity to the standard linear SVM.

Once the optimal values of the parameters w and b are
learned, an unseen testing datum, xt, can be classiﬁed to
one of the two classes according to the sign of the (signed)
distance between xt and the separating hyperplane. That is,
the predicted label of xt is computed as yt = sgn(dt), where
dt = (w(cid:62)xt + b)/(cid:107)w(cid:107). The posterior class probability, i.e,
a probabilistic degree of conﬁdence that the testing sample
belongs to the class to which it has been classiﬁed, can be
calculated using the well-known Platt scaling approach [36]
for ﬁtting a sigmoid function, S(t) = 1/(1 + eσAt+σB ). This
is the same approach that is used in the standard linear
SVM formulation (e.g., see [37]) for evaluating a sample’s
class membership at the testing phase.

3.2 Solving the SVM-GSU in linear subspaces

The derivations in Sect. 3.1 were made for the general case of
full rank covariance matrices that can be different for each
of the examples. Clearly, one can introduce constraints on
the covariance matrices, such as them being diagonal, block
diagonal, or multiples of the identity matrix. In this way one
can model different types of uncertainty – examples will
be given in the section of experimental results. However,
in some cases, especially when the dimensionality of the
data is high, most of the mass of the Gaussian distributions
will lie in a few directions in the feature space that may be
different for each example and may not be aligned with the
feature axes. To address this issue we alter the formulation
and work directly in the subspaces that preserve most of the
variance. More speciﬁcally, we propose a methodology for
approximating the loss function of SVM-GSU, by projecting
the vectors x in (5) into a linear subspace and integrating the
hinge loss function in that subspace instead of the original
feature space. A separate subspace is used for each of the
training examples, that is, for each of the input Gaussians.
For a given Gaussian distribution, the projection matrix is
found by performing eigenanalysis on the covariance matrix
and the dimensionality of each subspace is deﬁned so as to
preserve a certain fraction of the total variance.

i , . . . , λn

i , where Λi

i ), so that λ1

More speciﬁcally, by performing eigenanalysis on the
covariance matrix of the random vector Xi, the latter is
decomposed as Σi = UiΛiU (cid:62)
is an n × n
diagonal matrix consisting of the eigenvalues of Σi, i.e.
Λi = diag(λ1
i > 0, while
Ui is an n × n orthonormal matrix, whose j-th column, uj
i ,
is the eigenvector corresponding to the j-th eigenvalue, λj
i .
Let us keep the ﬁrst di ≤ n eigenvectors, so that a certain
fraction p ∈ (0, 1] of the total variance is preserved, i.e.,
(cid:80)di
(cid:80)n
keeping the ﬁrst di columns of Ui, i.e., U (cid:48)

> p. Then, we construct the n × di matrix U (cid:48)

i ≥, . . . , ≥ λn

t=1 λt
i
t=1 λt
i

. . . udi

i by
i ].

i = [u1

i u2
i

(cid:62)

i = Λz

Now, by using the projection matrix Pi = U (cid:48)
, we deﬁne
i
a new random vector Zi, such that Zi = PiXi. Then,
Zi ∈ Rdi follows a multivariate Gaussian distribution (since
Xi ∼ N (xi, Σi)), i.e. Zi ∼ N (zi, Σz
i ), with mean vector
zi = E(cid:2)PiXi
(cid:3) = PiE(cid:2)Xi
(cid:3) = Pixi and (diagonal) covariance
matrix Σz

i . Let fZi denote the PDF of Zi.
We proceed to approximate the expected value of the
hinge loss in the original space (5), by considering the
integral in the new, lower-dimensional space where most of
the variance is preserved. More speciﬁcally, x ≈ P (cid:62)
i z =⇒
w(cid:62)x ≈ w(cid:62)(P (cid:62)
(cid:62)z, where wz = Piw. Con-
sequently, the loss function for the i-th example, that is
the integral in the RHS of (5) can be approximated by
(cid:0)w(cid:62)
i de-
the quantity
i = (cid:8)z ∈
notes the projected halfspace on Rdi , that is, Ωz
Rdi : yi
. Using Theorem 1 (Appendix A),
we can then give this approximation of the loss function
L(cid:48) : Rdi × R → R, in closed form as follows:
(cid:32)

z z + b(cid:1)(cid:3) fZi(z) dz, where Ωz

z z + b(cid:1) ≤ 1(cid:9)

i z) = wz

(cid:2)1 − yi

(cid:0)w(cid:62)

Ωz
i

(cid:33)

(cid:32)

(cid:33)

(cid:82)

(cid:34)

(cid:35)

+ 1

+

erf

dzi
dΣz
z zi + b(cid:1)

i

(cid:0)w(cid:62)

i

exp

dΣz
√
2
π
= (cid:112)2w(cid:62)

−

d2
zi
d2
Σz
i

(9)

where dzi = 1 − yi
i wz. There-
fore, the objective function J (cid:48) : Rn × R → R, given by (3)
can be approximated as follows

, dΣz

i

z Σz

L(cid:48)(w, b) =

dzi
2

J (cid:48)(w, b) =

(cid:107)w(cid:107)2 +

L(cid:48) (Piw, b; (zi, Σz

i , yi)) .

(10)

λ
2

1
(cid:96)

(cid:96)
(cid:88)

i=1

Similarly to J , we can show that J (cid:48) is also convex with
respect to the unknown parameters w and b of the sepa-
rating hyperplane. Moreover, using the chain rule, we can
obtain the partial derivatives of J (cid:48) with respect to w and
b in closed form, and therefore use a stochastic gradient
method to arrive at the global optimum. More speciﬁcally,

∂J (cid:48)
∂w

= λw +

1
(cid:96)

(cid:96)
(cid:88)

i=1

∂
∂wz

L(cid:48)(cid:0)wz, b; (zi, Σz

i , yi)(cid:1) ∂wz
∂w

,

∂w wz = ∂

where ∂
respect to wz, and replacing in the above, we arrive at
(cid:16)

∂w Piw = Pi. By differentiating L(cid:48) with

(cid:17)

∂J (cid:48)
∂w

= λw +

(cid:34) exp

1
(cid:96)

(cid:96)
(cid:88)

i=1

−d2

zi/d2
Σz
i

√

πdΣz
i
(cid:32)

(cid:32)

erf

dzi
dΣz

i

−

1
2

i (Σz
P (cid:62)

i wz)

(cid:33)

(cid:33)

+ 1

P (cid:62)

i zi

(11)

(cid:35)
,

that is a closed form equation that gives the partial deriva-
tives of the cost with respect to w. Similarly, the ﬁrst partial
derivative of J (cid:48) with respect to b can be obtained as follows
(cid:34)

(cid:33)

(cid:32)

(cid:35)

∂J (cid:48)
∂b

(cid:96)
(cid:88)

1
(cid:96)

dzi
dΣz

= −

erf

+ 1

.

(12)

i

where wz = Piw, Σz

i=1
i = PiΣiP (cid:62)
i .
To summarize, in the low-dimensional spaces Rdi , the
loss function is computed as shown in (9). The objective
function is computed as shown in (10) and its ﬁrst deriva-
tives are computed as in (11) and (12). Finally, let us note that
in the above equations, the only matrix operations involve
the projection matrix Pi. Since the covariance matrices Σz
i
are diagonal, all operations that involve them boil down to
efﬁcient vector rescaling and vector norm calculations.

5

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

3.3 To sample or not to sample?

The data term in our formulation (see (4)) is the expected
value of the classical SVM cost when data samples are
drawn from the multi-dimensional Gaussian distributions.
It therefore follows that a standard linear SVM would arrive
at the same hyperplane when sufﬁciently many samples
are drawn from them. How many samples are needed to
arrive at the same hyperplane is something that cannot
be computed analytically. Nevertheless, our analysis and
results indicate that this number can be prohibitively high,
especially in the case of high-dimensional spaces.

More speciﬁcally, in what follows, we show that the dif-
ference between the analytically calculated expected value
of the hinge loss (4) and its sample mean is bounded by
a quantity that is inversely related to the dimensionality
of the feature space. Let L be the expected loss given
analytically as in (4), and ˜LN its approximation when N
samples are drawn from the Gaussians. Since the hinge loss
is (cid:107)w(cid:107)-Lipschitz‡ with respect to the Euclidean norm, we
can use a result due to Tsirelson et al. [38] that provides a
concentration inequality for Lipschitz functions of Gaussian
variables. By doing so, for all r ≥ 0, we arrive at the
following concentration inequality

P

(cid:16)(cid:12)
(cid:12)L − ˜LN
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≥ r

(cid:17)

≤ 2 exp

−

(cid:18)

r2
2(cid:107)w(cid:107)2

(cid:19)

.

(13)

That is, the tails of the error probability decay exponentially
with r2. More interestingly, they increase with the squared
norm of (cid:107)w(cid:107), and therefore with the dimensionality of the
input space, n. Consequently, as n increases, one needs
to generate more samples from the Gaussians in order to
preserve a desired approximation of the loss.

This means that for spaces of high dimensionality the
number of samples needed to approximate (4) sufﬁciently
well, can be prohibitively high. We experimentally demon-
strated this with a toy example in Sect. 4.1 (see Fig. 4), where
we show that in 2 dimensions we need approximately 3
orders of magnitude more samples to arrive at the same
hyperplane, while for 3 dimensions we need 4 orders of
magnitude more samples. Our experimental results on the
large-scale MED dataset (Sect. 4.6) also show the limitations
of a sampling approach.

3.4 A stochastic gradient descent solver for SVM-GSU

Motivated by the Pegasos algorithm (Primal Estimated
sub-GrAdient SOlver for SVM), ﬁrst proposed by Shalev-
Shwartz et al. in [39], we present a stochastic sub-gradient
descent algorithm for solving SVM-GSU in order to efﬁ-
ciently address scalability requirements§.

Pegasos is a well-studied algorithm [39], [40] providing
both state of the art classiﬁcation performance and great
scalability. It requires ˜O(1/(cid:15)) number of iterations in order
to obtain a solution of accuracy (cid:15), in contrast to previous
analyses of SGD methods that require ˜O(d/(λ(cid:15))) iterations,

‡A function h : Rn → R is L -Lipschitz with respect to the Eu-
clidean norm if |h(x) − h(y)| ≤ L (cid:107)x − y(cid:107), L > 0. Indeed, the
hinge loss h(x) = max(0, 1 − y(w(cid:62)x + b)) is (cid:107)w(cid:107)-Lipschitz since
(cid:12)1 − y(w(cid:62)x + b) − 1 + y(w(cid:62)y + b)(cid:12)
|h(x) − h(y)| ≤ (cid:12)
(cid:12) ≤(cid:107)w(cid:107)(cid:107)x − y(cid:107).
§A C++ implementation of the proposed method can be found at

https://github.com/chi0tzp/svm-gsu.

where d is a bound on the number of non-zero features in
each example¶. Since the run-time does not depend directly
on the size of the training set, the resulting algorithm is
especially suited for learning from large datasets.

Given a training set X = (cid:8)(xi, Σi, yi) : xi ∈ Rn, Σi ∈
, the proposed algorithm

++, yi ∈ {±1}, i = 1, . . . , (cid:96)(cid:9)
Sn
solves the following optimization problem

min
w,b

λ
2

(cid:107)w(cid:107)2 +

1
(cid:96)

(cid:96)
(cid:88)

i=1

L(cid:0)w, b; (xi, Σi, yi)(cid:1).

(14)

The algorithm receives as input two parameters: (i) the
number of iterations, T , and (ii) the number of examples
to use for calculating sub-gradients, k. Initially, we set w(1)
λ and b(1) = 0.
to any vector whose norm is at most 1/
On the t-th iteration, we randomly choose a subset of X ,
of cardinality k, i.e., Xt ⊆ X , where |Xt| = k, and set the
learning rate to ηt = 1
λt . Then, we approximate the objective
function of the above optimization problem with

√

λ
2

(cid:107)w(cid:107)2 +

1
k

(xi,Σi,yi)∈Xt

(cid:88)

L(cid:0)w, b; (xi, Σi, yi)(cid:1).

Then, we perform the update steps

w(t+1) ← w(t) −

b(t+1) ← b(t) −

ηt
k

∂J
∂w

,

ηt
k

∂J
∂b

,

where the ﬁrst-order derivatives are given in (7), (8), if the
training is conducted in the original space (Sect. 3.1), or in
(11), (12), if the learning is conducted in linear subspaces
(Sect. 3.2). Last, we project w(t+1) onto the ball of radius
λ}. The output
1/
of the algorithm is the pair of w(T +1), b(T +1). Algorithm 1
describes the proposed method in pseudocode.

λ, i.e., the set B = {w : (cid:107)w(cid:107) ≤ 1/

√

√

Algorithm 1 A stochastic sub-gradient descent algorithm
for solving SVM-GSU.

1: Inputs:

X , λ, T , k

2: Initialize:

b(1) = 0, w(1) such that (cid:107)w(1)(cid:107) ≤ 1√
λ

3: for t = 1, 2, . . . , T do
4:
5:

Choose Xt ⊆ X , where |Xt| = k
Set ηt = 1
λt
w(t+1) ← w(t) − ηt
k
w(t+1) ← min
1,
b(t+1) ← b(t) − ηt
k

(cid:107)w(t+1)(cid:107)
∂J
∂b

∂J
∂w
√
1/

(cid:16)

(cid:17)

λ

6:

7:

w(t+1)

8:
9: end for

4 EXPERIMENTS
In this section we ﬁrst illustrate the workings of the pro-
posed linear SVM-GSU classiﬁer on a synthetic 2D toy
example (Sect. 4.1) and then apply the algorithm on ﬁve
different classiﬁcation problems using publicly available
and popular datasets. Here, we summarize how the uncer-
tainty is modeled in each case, so as to illustrate how our
framework can be applied in practice.

¶We use the ˜O notation (soft-O) as a shorthand for the variant of O
(big-O) that ignores logarithmic factors; that is, f (n) ∈ ˜O(g(n)) ⇐⇒
∃k ∈ N : f (n) ∈ O(g(n) logk(g(n))).

6

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

(a)

(b) N = 10

(c) N = 102

(d) N = 103

Fig. 3: Toy example illustrating on 2D data, (a) the proposed LSVM-GSU (red solid line) in comparison with the standard
LSVM (blue dashed line), and (b)-(d) with the standard SVM that learns by sampling from the input Gaussians (LSVM-
sampling), where N is the sampling size.

First, we address the problem of image classiﬁcation
of handwritten digits (Sect. 4.2) using the MNIST dataset.
As we show in Appendix B, by using a ﬁrst-order Taylor
approximation around a certain image with respect to some
common image transformations (small translations in our
case), we show that the images that would be produced
by those translations would follow a Gaussian distribution
with mean the image in question and a covariance matrix
whose elements are functions of the derivatives of the image
intensities/color with respect to those transformations. In
the simple case of spatial translations, the covariance ele-
ments are functions of the spatial gradients. This is a case
where the uncertainty is modeled. We show that our method
outperforms the linear SVM and other SVM variants that
handle uncertainty isotropically.

Second, we address the binary classiﬁcation problem us-
ing the Wisconsin Diagnostic Breast Cancer (WDBC) dataset
(Sect. 4.3). This is a case in which each data example summa-
rizes a collection of samples by their second order statistics.
More speciﬁcally, each data example contains as features the
mean and the variance of measurements on several cancer
cells – mean and variances over the different cells. With our
formulation we obtain state of the art results on this dataset.
Third, we address the problem of emotional analysis
using electroencephalogram (EEG) signals (Sect. 4.4). In this
case, we exploit a very popular method for estimating the
power spectrum of time signals; namely the Welch method,
which allows for estimating not only the mean values of the
features (periodograms), but also their variances, making it
suitable for using the proposed SVM-GSU.

Fourth, we address the problem of detection of adver-
tisements in TV news videos (Sect. 4.5). This is an interesting
case where uncertainty information is given only for a few
dimensions of the input space, rendering inapplicable the
methods that treat uncertainty isotropically. In contrast, the
proposed method can model such uncertainty types using
low-rank covariance matrices.

Finally, we address the challenging problem of complex
event detection in video (Sect. 4.6). We used the ∼5K out-
puts of a pre-trained DCNN in order to extract a representa-
tion for each frame in a video and calculated the mean and
covariances over the frames of a video in order to classify
it. This is a second example in which the mean and the

covariance matrices are calculated from data. We show that
our formulation outperforms the linear SVM and other SVM
variants that handle uncertainty isotropically.

4.1 Toy example using synthetic data
In this subsection, we present a toy example on 2D data that
provides insights to the way the proposed algorithm works.
As shown in Fig. 3a, negative examples are denoted by red
× marks, while positive ones by green crosses. We assume
that the uncertainty of each training example is given via
a covariance matrix. For illustration purposes, we draw the
iso-density loci of points at which the value of the PDF of
the Gaussian is the 0.03% of its maximum value.

First, a baseline linear SVM (LSVM) is trained using
solely the centres of the distributions; i.e., ignoring the un-
certainty of each example. The resulting separating bound-
ary is the dashed blue line in Fig. 3a. The proposed linear
SVM-GSU (LSVM-GSU) is trained using both the centres
of the above distributions and the covariance matrices.
The resulting separating boundary is the solid red line in
Fig. 3a. It is clear that the separating boundaries can be very
different and that the solid red line is a better one given the
assumed uncertainty modeling.

Next, we investigate on how many samples are needed
in order to obtain LSVM-GSU’s separating line by sampling
N samples from each Gaussian and using the standard
LSVM (LSVM-sampling). The results for various values of
N are depicted in Fig. 3, where it is clear that ones needs
almost 3 orders of magnitude more examples. In order to
investigate how this number changes with the dimensional-
ity of the feature space we performed the same experiment
in a similar 3D dataset. In Fig. 4 we plot the angle between
the hyperplanes obtained by the LSVM-GSU and the LSVM-
sampling for both the 2D and the 3D datasets. We observe
that, in the 3D case, we need at least one order of magnitude
more samples from each Gaussian, compared to the 2D
case; that is, in the 2D case, we obtain θ ≈ 1.7◦ using
N = 103 samples from each Gaussian, while in the 3D case,
the sampling size for obtaining the same approximation
(θ ≈ 1.7◦) is N = 5×104. This is indicative of the difﬁculties
of using the sampling approach when dealing with high-
dimensional data, where the number of dimensions is in the
hundreds or thousands.

7

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

4.2.2 Uncertainty modeling

In Appendix B, we propose a methodology that, given an
image, models the distribution of the images that result by
small random translations of it. We show that under a ﬁrst-
order Taylor approximation of the image intensities/color
with respect to those translations, and the assumption that
the translations are small and follow a Gaussian distribu-
tion, the resulting distribution of the images is also a Gaus-
sian with mean the original image and a covariance matrix
whose elements are functions of the image derivatives with
respect to the transforms – in this case functions of the image
spatial gradients. The derivation could be straightforwardly
extended to other transforms (e.g. rotations, scaling).

In our experiments in this dataset we set the variances
of the horizontal and the vertical components of the transla-
(cid:1)2
tion, denoted by σ2
,
so that the translation falls in the square [−pt, pt] × [−pt, pt]
with probability 99.7%. The pt is measured in pixels and for
the experiments described below, it is set to pt = 5 pixels.

v respectively, to σ2

h and σ2

v = (cid:0) pt

h = σ2

3

4.2.3 Experimental results

Table 1 shows the performance of the proposed classiﬁer
(LSVM-GSU) and the compared techniques in terms of
testing accuracy for each dataset deﬁned above (D0-D5).
The optimization of the training parameter for the various
SVM variants was performed using a line search on a 3-
fold cross-validation procedure. The performance of LSVM-
GSU when the training of each classiﬁer is carried out in
the original feature space is shown in row 5, and in linear
subspaces in row 6. In row 6 we report both the classiﬁcation
performance, and in parentheses the fraction of variance
that resulted in the best classiﬁcation result.

The performance of the baseline linear SVM (LSVM) is
shown in the second row, the performance of Power SVM
(PSVM) [14] is shown in the third row, and the performance
of the linear SVM extension, based on the proposed formula-
tion, handling the noise isotropically, as in [18], [27], (LSVM-
iso) is shown in the fourth row. Moreover, Fig. 5 shows
the results of the above experimental scenarios for datasets
D0-D5. The horizontal axis of each subﬁgure describes the
fraction of the total variance preserved for each covariance
matrix, while the vertical axis shows the respective per-
formance of LSVM-GSU with learning in linear subspaces
(LSVM-GSU-SLp). Furthermore, in each subﬁgure, for p = 1
we draw the result of LSVM-GSU in the original feature
space (denoted with a rhombus), the result of PSVM [14]
(denoted with a circle), as well as the result of LSVM-
iso [18], [27] (denoted with a star).

We report the mean, and with an error-bar show the
variance of the 100 iterations. The performance of the base-
line LSVM is shown with a solid line, while two dashed
lines show the corresponding variance of the 100 runs. From
the obtained results, we observe that the proposed LSVM-
GSU with learning in linear subspaces outperforms LSVM,
PSVM, and LSVM-iso for all datasets D0-D5. Moreover,
LSVM-GSU achieves better classiﬁcation results than PSVM
in all datasets, and than LSVM-iso in 5 out of 6 datasets,
when learning is carried out in the original feature space.
Finally, all the reported results are shown to be statistically
signiﬁcant using the t-test [43]; signiﬁcance values (p-values)

Fig. 4: Difference between the separating hyperplanes of
LSVM-GSU and the standard LSVM with sampling (angle
θ), when varying the number of samples used in the stan-
dard SVM, for the 2D and 3D toy datasets.

4.2 Hand-written digit classiﬁcation

4.2.1 Dataset and experimental setup

The proposed algorithm is also evaluated in the problem of
image classiﬁcation using the MNIST dataset of handwritten
digits [41]. The MNIST dataset provides a training set of 60K
examples (approx. 6000 examples per digit), and a test set
of 10K examples (approx. 1000 examples per digit). Each
sample is represented by a 28 × 28 8-bit image.

In order to make the dataset more challenging, as well as
to model a realistic distortion that may happen to this kind
of images, the original MNIST dataset was “polluted” with
noise. More speciﬁcally, each image example was rotated by
a random angle uniformly drawn from [−θ, +θ], where θ is
measured in degrees. Moreover, each image was translated
by a random vector t uniformly drawn from [−tp, +tp]2,
where tp is a positive integer expressing distance that is
measured in pixels. We created ﬁve different noisy datasets
by setting θ = 15° and tp ∈ {3, 5, 7, 9, 11}, resulting in the
polluted datasets D1 to D5, respectively. D0 denotes the
original MNIST dataset.

We created six different experimental scenarios using
the above datasets (D0-D5). First, we deﬁned the problem
of discriminating the digit one (“1”) from the digit seven
(“7”) similarly to [42]. Each class in the training procedure
consists of 25 samples, randomly chosen from the pool of
digits one (6k totally) and seven (6k totally), while the
evaluation of the trained classiﬁer is carried out on the full
testing set (2k examples). In each experimental scenario we
report the average of 100 runs and we compare the pro-
posed linear SVM-GSU (LSVM-GSU) to the baseline linear
SVM (LSVM), Power SVM [14], and LSVM-iso (a variation
of SVM formulation that handles only isotropic uncertainty,
similarly to [18], [27]). We report the testing accuracy and the
mean testing accuracy across 100 runs. Finally, we repeat the
above experiments for various sizes of the training set; i.e.,
using 25, 50, 100, 500, 1000, 3000, 6000 positive examples
per digit, in order to investigate how this affects the results.

8

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

TABLE 1: MNIST “1” versus “7” experimental results in terms of testing accuracy. The proposed LSVM-GSU is compared
to the baseline linear SVM (LSVM), Power SVM (PSVM) [14], and a linear SVM extension which handles the uncertainty
isotropically (LSVM-iso), as in [18], [27].

Dataset
LSVM
PSVM [14]
LSVM-iso (as in [18], [27])

LSVM-GSU

Learning in original space
Learning in linear subspaces

D0
0.9952
0.9963
0.9968
0.9971
0.9972 (0.99)

D1
0.9362
0.9315
0.9327
0.9452
0.9480 (0.97)

D2
0.8240
0.8157
0.8133
0.8310
0.8562 (0.89)

D3
0.6830
0.7017
0.7222
0.7216
0.7543 (0.85)

D4
0.6558
0.6650
0.6675
0.6708
0.6974 (0.95)

D5
0.6027
0.6259
0.6328
0.6353
0.6640 (0.25)

(a)

(b)

(c)

(d)

(e)

(f)

Fig. 5: Comparisons between the proposed LSVM-GSU, the
baseline LSVM, and the LSVM with isotropic noise in (a) the
original MNIST dataset (D0), and (b)-(f) the noisy generated
datasets D1-D5.

were much lower than the signiﬁcance level of 1%. Finally,
in Fig. 6, we show the experimental results using various
training set sizes and we observe that this does not qualita-
tively affect the behavior of the various compared methods.

4.3 Wisconsin Diagnostic Breast Cancer dataset

[44] consists of

(WDBC)
The Wisconsin Diagnostic Breast Cancer
features computed from 569
dataset
images, each belonging to one of the following two classes:
malignant (212 instances) and benign (357 instances). The
digitized images depict breast mass obtained by Fine

9

(a)

(b)

Fig. 6: MNIST “1” versus “7” experimental results using 25,
50, 100, 500, 1000, 3000, 6000 positive examples per digit.
The proposed LSVM-GSU using learning linear subspaces
(LSVM-GSU-Sp) is compared to the baseline linear SVM
(LSVM), Power SVM (PSVM) [14], and a linear SVM ex-
tension which handles the uncertainty isotropically (LSVM-
iso), as in [18], [27]. The fraction of variance preserved
for the proposed method is (a) p = 0.85 (dataset D3), (b)
p = 0.95 (dataset D4). Very similar results are observed for
all other datasets.

Needle Aspirate (FNA) and they describe characteristics of
the cell nuclei present in the image. Each feature vector is of
the form x = (x1, . . . , x10, s1, . . . , s10, w1, . . . , w10)(cid:62) ∈ R30,
where xj is the mean value, sj the standard error, and
wj the largest value of the j-th feature, j = 1, . . . , 10. Ten
real-valued features are computed for each cell nucleus.

i

Since the standard error si and variance σ2

i are connected
via the relation si = σ2
N , where N is the (unknown) size
of the sample where standard deviation was computed,
we assign to each input example a diagonal covariance
matrix given by Σi = diag(σ2
1, . . . , σ2
++,
where σ2
0 is set to a small positive constant (e.g., 10−6)
indicating very low uncertainty for the respective features,
and σ2
j is computed using the standard error by scaling the
standard error values into the range of mean values; that
is, the maximum variance is set to 80% of the range of the
corresponding mean value.

0, . . . , σ2

0) ∈ S30

10, σ2

The proposed algorithm is compared in terms of testing
accuracy both to the baseline linear SVM (LSVM), Power
SVM [14] (PSVM), and to LSVM-iso, similarly to Sect. 4.2.
Since the original dataset does not provide a division in
training and evaluation subsets, we divided the dataset
randomly into a training subset (90%) and an evaluation
subset (10%). The optimization of the λ parameter for all
classiﬁers was performed using a line search on a 10-fold
cross-validation procedure. We repeated the experiment 10

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

times and report the average results in Table 2. The re-
sults are statistically signiﬁcant and show the superiority
of LSVM-GSU. More speciﬁcally, we used the t-test [43] and
obtained signiﬁcance values (p-values) lower than 0.05.

TABLE 2: Comparison between the proposed LSVM-GSU,
the baseline LSVM, Power SVM, and LSVM-iso.

Classiﬁer
LSVM
PSVM [14]
LSVM-iso (as in [18], [27])
LSVM-GSU (proposed)

Testing Accuracy
95.15%
96.37%
96.53%
97.14%

4.4 Emotion analysis using physiological signals

4.4.1 Dataset and experimental setup

For evaluating the proposed method in the domain of
emotional analysis using physiological signals, we used
the publicly available DEAP [45] dataset, which provides
EEG features of 32 participants who were recorded while
watching 40 one-minute long excerpts of music videos.
Three different binary classiﬁcation problems were deﬁned:
the classiﬁcation of low/high arousal, low/high valence and
low/high liking videos.

From the EEG signals, power spectral features were ex-
tracted using the Welch method [46]. The logarithms of the
spectral power from theta (4 − 8 Hz), slow alpha (8 − 10 Hz),
alpha (8−12 Hz), beta (12-30Hz), and gamma (30+ Hz) bands
were extracted from all 32 electrodes as features, similarly
to [45]. In addition to power spectral features, the difference
between the spectral power of all the symmetrical pairs of
electrodes on the right and left hemisphere was extracted to
measure the possible asymmetry in the brain activities due
to emotional stimuli. The total number of EEG features of a
video for 32 electrodes is 216. For feature selection, we used
Fisher’s linear discriminant similarly to [45].

4.4.2 Uncertainty modeling

For modeling the uncertainty of each training example, we
used a well-known property of the Welch method [46] for es-
timating the power spectrum of a time signal. First, the time
signal was divided into (overlapping or non-overlapping)
windows, where the periodogram was computed for each
window. Then the resulting frequency-domain values were
averaged over all windows. Besides these mean values, that
are the desired outcomes of the Welch method, we also
computed the variances, and, thus, each 216-element vector
was assigned with a diagonal covariance matrix.

4.4.3 Experimental results

Table 3 shows the performance of the proposed linear SVM-
GSU (LSVM-GSU) in terms of accuracy and F1 score for
each target class in comparison to LSVM, PSVM [14], and
LSVM-iso, similarly to Sect. 4.2 and 4.3, as well as the Naive
Bayesian (NB) classiﬁer used in [45]. For each participant,
the F1 measure was used to evaluate the performance of
emotion classiﬁcation in a leave-one-out cross validation
scheme. At each step of the cross validation, one video was
used as the test-set and the rest were used for training. For

TABLE 3: Comparisons between the proposed LSVM-GSU,
the baseline NB, LSVM, Power SVM, and the LSVM with
isotropic noise.

Classiﬁer
NB [45]
LSVM
PSVM [14]
LSVM-iso [18], [27]
LSVM-GSU

Arousal
ACC F1
0.620
0.626
0.625
0.645
0.659

0.583
0.451
0.521
0.531
0.551

Valence
ACC F1
0.576
0.616
0.633
0.645
0.650

0.563
0.538
0.561
0.603
0.609

Liking
ACC F1
0.554
0.655
0.651
0.658
0.666

0.502
0.470
0.522
0.530
0.539

optimizing the λ parameter of the various SVM classiﬁers,
we used a line search on a 3-fold cross-validation procedure.
From the obtained results, we observe that the proposed
algorithm achieved better classiﬁcation performance than
LSVM, PSVM, LSVM-iso, as well as the NB classiﬁer used
in [45] for all three classes, in terms of testing accuracy, and
for the two out of three classes in terms of F1 score.

4.5 TV News Channel Commercial Detection

4.5.1 Dataset and experimental setup
The proposed algorithm is evaluated in the problem of
detection of advertisements in TV news videos using the
publicly available and very large dataset of [47]. This dataset
comprises 120 hours of TV news broadcasts from CNN,
CNNIBN, NDTV, and TIMES NOW (approximately 22k,
33k, 17k, and 39k videos, respectively). The authors of [47]
used various low-level audio and static-, motion-, and
text-based visual features, to extract and provide a 4125-
dimensional representation for each video, that includes the
variance values for 24 of the above features. For a detailed
description of the dataset, see [47].

4.5.2 Uncertainty modeling
This dataset represents a real-world case where uncertainty
information is given only for a few dimensions of the
feature space. In this case we model the covariance ma-
trix of each input example as a low-rank diagonal matrix,
whose non-zero variance values correspond to the dimen-
sions for which uncertainty is provided. Each such matrix
corresponds to a Gaussian with non-zero variance along
the few speciﬁc given dimensions. Since the information
about the input variance is provided just for the 24 of the
4125 features, there is no natural way of estimating a single
variance value, i.e., an isotropic covariance matrix, for each
training example.

4.5.3 Experimental results
Table 4 shows the performance of the proposed linear SVM-
GSU (LSVM-GSU) in terms of F1 score in comparison to
LSVM, similarly to [47]. As discussed above, since methods
that model the uncertainty isotropically (such as [14], [18],
[27]), are not applicable in this dataset, we experimented
on this dataset using only the proposed algorithm and the
standard linear SVM. Following the protocol of [47], we
did cross-dataset training and testing. For optimizing the
λ parameter of both LSVM and LSVM-GSU we used a
line search on a 3-fold cross-validation procedure. From the
obtained results, we observe that the proposed algorithm
achieved considerably better classiﬁcation than LSVM in
almost all cases (more than 10% relative boost on average).

10

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

TABLE 4: Comparisons between the proposed LSVM-GSU and the baseline LSVM, similarly to [47].

CNN
LSVM LSVM-GSU
0.7799
0.7915
CNNIBN
NDTV
0.8484
TIMES NOW 0.7809

0.9589
0.8836
0.9248
0.9461

n CNN
o
g
n
i
t
s
e
T

CNNIBN
LSVM LSVM-GSU
0.7799
0.7915
0.8484
0.7809

0.8050
0.9215
0.8565
0.7863

NDTV
LSVM LSVM-GSU
0.7799
0.7915
0.8484
0.7809

0.8113
0.8978
0.9709
0.7493

TIMES NOW
LSVM LSVM-GSU
0.7799
0.7915
0.8484
0.7809

0.9226
0.8611
0.8823
0.9421

Training on

4.6 Video Event Detection

4.6.1 Dataset and experimental setup

In our experiments on video event detection we used
datasets from the challenging TRECVID Multimedia Event
Detection (MED)
task [48]. For training, we used the
MED 2015 training dataset consisting of the “pre-speciﬁed”
(PS) video subset (2000 videos, 80 hours) and the “event
background” (Event-BG) video subset (5000 videos, 200
hours). For testing, we used the large-scale “MED14Test”
dataset [48], [49] (∼ 24K videos, 850 hours). Each video in
the above datasets belongs to, either one of 20 target event
classes, or to the “rest of the world” (background) class.
More speciﬁcally, in the training set, 100 positive and 5000
negative samples are available for each event class, while
the evaluation set includes only a small number of positive
(e.g., only 16 positives for event E021, and 28 for E031) and
approximately 24K negative videos.

For video representation, approximately 2 keyframes per
second were extracted from each video. Each keyframe was
represented using the last hidden layer of a pre-trained deep
convolutional neural network (DCNN). More speciﬁcally, a
22-layer inception style network, trained according to the
GoogLeNet architecture [50], was used. This network had
been trained on various selections of the ImageNet “Fall
2011” dataset and provides scores for 5055 concepts [51].

4.6.2 Uncertainty modeling
Let us now deﬁne a set X of (cid:96) annotated random vec-
tors representing the aforementioned video-level feature
vectors. Each random vector is assumed to be distributed
normally; i.e., for the random vector representing the i-
th video, Xi, we have Xi ∼ N (xi, Σi). That is, X =
{(xi, Σi, yi) : xi ∈ Rn, Σi ∈ Sn
++, yi ∈ {±1}, i = 1, . . . , (cid:96)}.
For each random vector Xi, a number, Ni, of observations,
i ∈ Rn : t = 1, . . . , Ni} are available (these are the
{xt
keyframe-level vectors that have been computed). Then, the
sample mean vector and the sample covariance matrix of
Xi are computed. However, the number of observations per
each video that are available for our dataset is in most cases
much lower than the dimensionality of the input space.
Consequently, the covariance matrices that arise are typi-
cally low-rank; i.e. rank(Σi) ≤ Ni ≤ n. To overcome this
issue, we assumed that the desired covariance matrices are
diagonal. That is, we require that the covariance matrix of
the i-th training example is given by (cid:99)Σi = diag (cid:0)ˆσ1
(cid:1)
,
such that the squared Frobenious norm of the difference
Σi − (cid:99)Σi is minimum. That is, the estimator covariance matrix
(cid:99)Σi must be equal to the diagonal part of the sample covari-
ance matrix Σi, i.e. (cid:99)Σi = diag (cid:0)σ1
. We note that,
using this approximation approach, the covariance matrices
are diagonal but anisotropic and different for each training

i , . . . , ˆσn
i

i , . . . , σn
i

(cid:1)

input example. This is in contrast with other methods (e.g.
[14], [18], [27]) that assume more restrictive modeling for the
uncertainty; e.g., isotropic noise for each training sample.

4.6.3 Experimental results

We experimented using two different feature conﬁgurations.
First, we used the mean vectors and covariance matrices as
computed using the method discussed above (Sect. 4.6.2).
Furthermore, in order to investigate the role of variances in
learning with baseline LSVM, we constructed mean vectors
and covariance matrices as shown in Table 6, where σ0
is typically set to a small positive constant (e.g., 10−6)
indicating very low uncertainty for the respective features.
For both feature conﬁgurations, Table 5 shows the per-
formance of the proposed linear SVM-GSU (LSVM-GSU) in
terms of average precision (AP) [10], [48] for each target
event in comparison with LSVM, PSVM [14], and LSVM-iso
approaches. Moreover, for each dataset, the mean average
precision (MAP) across all target events is reported. The
optimization of the λ parameter for the various SVMs was
performed using a line search on a 10-fold cross-validation
procedure. The bold-faced numbers indicate the best result
achieved for each event class. We also report the results
of the McNemar [52], [53], statistical signiﬁcance tests. A
∗ denotes statistically signiﬁcant differences between the
proposed LSVM-GSU and baseline LSVM, a (cid:5) denotes
statistically signiﬁcant differences between LSVM-GSU and
PSVM, and a ∼ denotes statistically signiﬁcant differences
between LSVM-GSU and LSVM-iso.

From the obtained results, we observe that the pro-
posed algorithm achieved better detection performance than
LSVM, PSVM, and LSVM-iso, in both feature conﬁgura-
tions. For feature conﬁguration 1, the proposed LSVM-GSU
achieved a relative boost of 22.2% compared to the baseline
standard LSVM and 19.4% compared to Power SVM, while
for feature conﬁguration 2 respective relative boosts of
12.7% and 11.7%, respectively, in terms of MAP. We also
experimented using directly the samples from which the
covariance matrix of each example was estimated and ob-
tained inferior results; that is, a MAP of 10.15%, compared
to LSVM’s 14.78% and 18.06% of the proposed SVM-GSU.

5 CONCLUSION

In this paper we proposed a novel classiﬁer that efﬁ-
ciently exploits uncertainty in its input under the SVM
paradigm. The proposed SVM-GSU was evaluated on syn-
thetic data and on ﬁve publicly available datasets; namely,
the MNIST dataset of handwritten digits, the WDBC, the
DEAP for emotion analysis, the TV News Commercial
Detection dataset and TRECVID MED for the problem of
video event detection. For each of the above datasets and

11

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

TABLE 5: Event detection performance (AP and MAP) of the linear SVM-GSU compared to the baseline linear SVM, Power
SVM [14], and a LSVM extension for handling isotropic uncertainty (as in [18], [27]) using the MED15 (for training) and
MED14Test (for testing) datasets.

Event
Class

E021
E022
E023
E024
E025
E026
E027
E028
E029
E030
E031
E032
E033
E034
E035
E036
E037
E038
E039
E040
MAP

LSVM

0.0483
0.0227
0.4159
0.0071
0.0052
0.0457
0.1319
0.4242
0.0812
0.0516
0.4416
0.0280
0.3483
0.0583
0.3330
0.0894
0.0884
0.0261
0.2677
0.0421
0.1478

Feature Conﬁguration 1
(5055-D)

Feature Conﬁguration 2
(10110-D)

PSVM
[14]
0.0510
0.0310
0.4515
0.0081
0.0052
0.0459
0.1424
0.4125
0.0914
0.0551
0.4425
0.0400
0.3614
0.0588
0.3419
0.0748
0.0880
0.0241
0.2698
0.0315
0.1513

LSVM-iso
[18], [27]
0.0500
0.0350
0.6059
0.0097
0.0074
0.0606
0.1174
0.3819
0.1793
0.0877
0.4480
0.0870
0.3901
0.0599
0.3500
0.0695
0.1981
0.0212
0.2959
0.0375
0.1746

LSVM-GSU
(proposed)
0.0515
0.0277
0.6057
0.0105
0.0068
0.0608
0.1219
0.4335
0.1791
0.0884
0.4796
0.1196
0.4187
0.0614
0.3369
0.0704
0.1968
0.0291
0.2757
0.0377
0.1806

McNemar
Tests
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5)
(cid:5)

(cid:5)
∗, (cid:5), ∼
∗, (cid:5), ∼
(cid:5)

∗, (cid:5), ∼
∗, (cid:5), ∼
∗, ∼
(cid:5)
∗, (cid:5), ∼
(cid:5)
∗, (cid:5), ∼
(cid:5)
∗, (cid:5), ∼
∗, (cid:5)

–

LSVM

0.0829
0.0674
0.7050
0.0187
0.0219
0.0731
0.1152
0.1863
0.2046
0.1001
0.7595
0.0989
0.4571
0.3207
0.3516
0.1156
0.1169
0.0558
0.4188
0.0837
0.2177

PSVM
[14]
0.0834
0.0773
0.7236
0.0223
0.0245
0.0745
0.0133
0.2214
0.1987
0.1276
0.7599
0.1011
0.4789
0.3214
0.3419
0.1186
0.1257
0.0498
0.4219
0.0889
0.2187

LSVM-iso
[18], [27]
0.1074
0.1023
0.7802
0.0394
0.0161
0.0976
0.1254
0.2700
0.2149
0.1596
0.7422
0.1290
0.5091
0.3200
0.3252
0.1064
0.1598
0.0557
0.4349
0.0856
0.2390

LSVM-GSU
(proposed)
0.0778
0.1429
0.7943
0.0367
0.0135
0.1109
0.1812
0.2278
0.1999
0.1774
0.7697
0.1292
0.5164
0.3380
0.3059
0.1288
0.1629
0.0539
0.4271
0.0902
0.2442

McNemar
Tests
(cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5), ∼
∗
(cid:5)
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5), ∼
∗, (cid:5)
∗
∗, (cid:5), ∼
∗, (cid:5)
∗, (cid:5), ∼
∗, (cid:5), ∼
(cid:5)
∗, (cid:5), ∼
∗, (cid:5)

–

TABLE 6: Mean vector and covariance matrix of the i-th
example for feature conﬁgurations 1 and 2 of the video event
detection experiments.

and Ω+ ∩Ω− = ∅. Then, the integrals I± : Rn ×R → R, deﬁned
as

I±(a, b) (cid:44)

(a(cid:62)x + b)fX(x) dx,

(cid:90)

Ω±

Conﬁguration 1

Conﬁguration 2

xi = (xi,1, . . . , xi,n)(cid:62) ∈ Rn
(cid:1) ∈ Sn
Σi, = diag (cid:0)σ1
xi = (xi,1, . . . , xi,n, σ1
Σi, = diag (cid:0)σ1
i , . . . , σn

i , . . . , σn
i
i , . . . , σn
i , σ0, . . . , σ0

++
i )(cid:62) ∈ R2n
(cid:1) ∈ S2n
++

are given by

problems, either uncertainty information (e.g., variance for
each example and for all or some of the input space di-
mensions) was part of the original dataset, or a method for
modeling and estimating the uncertainty of each training
example was proposed. As shown in the experiments, SVM-
GSU efﬁciently takes input uncertainty into consideration
and achieves better detection or classiﬁcation performance
than standard SVM, previous SVM extensions that model
uncertainty isotropically, and other state of the art methods.
Finally, we plan to investigate the kernalization of the pro-
posed algorithm and the extensions of it for the problem of
regression under Gaussian input uncertainty.

APPENDIX A
ON GAUSSIAN-LIKE INTEGRALS OVER HALFSPACES

Theorem 1. Let X ∈ Rn be a random vector that follows the
multivariate Gaussian distribution with mean vector µ ∈ Rn and
++, where Sn
covariance matrix Σ ∈ Sn
++ denotes the space of n×n
symmetric positive deﬁnite matrices with real entries. The proba-
bility density function of this distribution is given by fX : Rn →
2 (x − µ)(cid:62)Σ−1(x − µ)(cid:1). More-
R, fX(x) =
let H be the hyperplane given by a(cid:62)x + b = 0. H
over,
divides the Euclidean n-dimensional space into two halfspaces,
i.e., Ω± = {x ∈ Rn : a(cid:62)x + b ≷ 0}, so that Ω+ ∪ Ω− = Rn

exp (cid:0)− 1

1
n
2 |Σ|

(2π)

1
2

12

I±(a, b) =

1 ± erf

(cid:20)

dµ
2

(cid:19)(cid:21)

(cid:18) dµ
dΣ

±

dΣ
√
π
2

exp

−

,

(15)

(cid:32)

(cid:33)

d2
µ
d2
Σ

where dµ = a(cid:62)µ + b and dΣ =

2a(cid:62)Σa.

√

Proof. We begin with the integral I+. In our approach we
will need several coordinate transforms. First, we start with
a translation in order to get rid of the mean, x = y+µ. Then

I+(a, b) =
(cid:19)

(cid:18)

(cid:90)

2

−

Ω+
1

1
2

dy,

y(cid:62)Σ−1y

(a(cid:62)y + a(cid:62)µ + b) exp

1
(2π) n
2 |Σ| 1
where Ω+
1 = {y ∈ Rn : a(cid:62)y + a(cid:62)µ + b ≥ 0}. Next, since
Σ ∈ Sn
++, there exist an orthonormal matrix U and a diag-
onal matrix D with positive elements, i.e. the eigenvalues
of Σ, such that Σ = U (cid:62)DU . Thus, it holds that Σ−1 =
(U (cid:62)DU )−1 = U −1D−1(U (cid:62))−1 = U (cid:62)D−1U . Then, by let-
ting z = U y and a1 = U a, we have a(cid:62)y = a(cid:62)(U −1U )y =
1 z, and y(cid:62)Σ−1y = y(cid:62)(U (cid:62)DU )−1y =
a(cid:62)U (cid:62)U z = a(cid:62)
(y(cid:62)U (cid:62))D−1(U y) = (U y)(cid:62)D−1(U y) = z(cid:62)D−1z. Then

I+(a, b) =
(cid:19)

(cid:18)

(cid:90)

2

Ω+
2

(a(cid:62)

1 z + a(cid:62)µ + b) exp

1
(2π) n
2 |Σ| 1
where Ω+
1 z + a(cid:62)µ + b ≥ 0}, since for the
Jacobian J = |U |, it holds that |J| = 1. Now, in order to do
rescaling, we set z = D 1

2 = {z ∈ Rn : a(cid:62)

2 a1. Thus,

z(cid:62)D−1z

dz,

1
2

−

z(cid:62)D−1z = (D

2 v)(cid:62)D−1(D

1

1

2 D−1D

2 )v = v(cid:62)v.

1

2 v and a2 = D 1
2 v) = v(cid:62)(D

1

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

1 z = a(cid:62)

1 (D 1
2 = |Σ| 1

2 v) = (D 1
2 and dz = |D 1

2 a1)(cid:62)v = a(cid:62)

2 |dv = |Σ| 1

2 v. Also,
2 dv.

REFERENCES

Moreover, a(cid:62)
it holds that |D| 1
Consequently,
1
(2π) n

I+(a, b) =

(cid:90)

(a(cid:62)

2 v + a(cid:62)µ + b) exp

−

v(cid:62)v

dv,

(cid:18)

(cid:19)

1
2

2

Ω+
3
3 = {v ∈ Rn : a(cid:62)

where Ω+
2 v + a(cid:62)µ + b ≥ 0}. Let B
be an orthogonal matrix such that Ba2 = (cid:107)a2(cid:107)en, which
also means that a2 = B(cid:62)(cid:107)a2(cid:107)en. Moreover, let m = Bv.
Then, a(cid:62)
2 v = (B(cid:62)(cid:107)a2(cid:107)en)(cid:62)v = (cid:107)a2(cid:107)e(cid:62)
n m.
Moreover, v(cid:62)v = v(cid:62)(B−1B)v = m(cid:62)m. Then
(cid:18)

n (Bv) = (cid:107)a2(cid:107)e(cid:62)

(cid:90) +∞

(cid:19)

I+(a, b) =

(cid:0)(cid:107)a2(cid:107)t + a(cid:62)µ + b(cid:1) exp

1
√
2π
(cid:107)a2(cid:107) . Since (cid:107)a2(cid:107)2 = a(cid:62)Σa,

c

where c = − a(cid:62)µ+b

−

t2

dt,

1
2

I+(a, b) =

(cid:90) +∞

√

1
√
2π

c

(cid:0)

a(cid:62)Σat+a(cid:62)µ+b(cid:1) exp

−

t2

dt,

(cid:18)

(cid:19)

1
2

which is easily evaluated as (15). Following similar argu-
ments as above, we arrive at I−.

APPENDIX B
MODELING THE UNCERTAINTY OF AN IMAGE
Let f (0) = (f1(0), . . . , fj(0), . . . , fn(0))(cid:62) ∈ Rn be an
image with n pixels in row-wise form, and let f (t) =
(f1(t), . . . , fj(t), . . . , fn(t))(cid:62) ∈ Rn be a translated version
of it by t = (h, v)(cid:62) pixels. Clearly, fj : R2 → R denotes the
intensity function of the j-th pixel, after a translation by t.

We will use the multivariate Taylor’s theorem in order
to approximate the intensity function of the j-th pixel of
the given image; i.e., function fj. That is, the intensity is
approximated as fj(t) = fj(0) + ∇(cid:62)fj(0)t. Then,

f (t) = f (0) +

(16)






∇(cid:62)f1(0)
...
∇(cid:62)fn(0)




 t.

Let us now assume that t is a random vector distributed
normally with mean µt and covariance matrix Σt,
i.e.
t ∼ N (µt, Σt). Then, X = f (t) is also distributed normally
with mean vector and covariance matrix that are given,
respectively, by

µX = f (0) +

E [t] ,

(17)






∇(cid:62)f1(0)
...
∇(cid:62)fn(0)






and

ΣX =






∇(cid:62)f1(0)
...
∇(cid:62)fn(0)






 Σt




(cid:62)






.

∇(cid:62)f1(0)
...
∇(cid:62)fn(0)

(18)

Thus, if t ∼ N (µt, Σt), then X ∼ N (µX , ΣX ), where the
mean vector µX and the covariance matrix ΣX are given by
(17) and (18), respectively.

ACKNOWLEDGMENT
This work was supported by the EU’s Horizon 2020 pro-
gramme H2020-693092 MOVING. We would also like to
thank the authors of [13] for providing an implementation
of Power SVM.

[1] V. Vapnik, The nature of statistical learning theory. Springer Heidel-

berg, 1995.

[2] F. Solera, S. Calderara, and R. Cucchiara, “Socially constrained
structural learning for groups detection in crowd,” Pattern Analysis
and Machine Intelligence, IEEE Trans. on, vol. 38, no. 5, pp. 995–1008,
2016.

[3] C. Sentelle, G. C. Anagnostopoulos, and M. Georgiopoulos, “A
simple method for solving the SVM regularization path for
semideﬁnite kernels,” Neural Networks and Learning Systems, IEEE
Trans. on, vol. 27, no. 4, pp. 709–722, 2016.

[4] A. Diplaros, T. Gevers, and I. Patras, “Combining color and shape
information for illumination-viewpoint invariant object recogni-
tion,” IEEE Transactions on Image Processing, vol. 15, no. 1, pp. 1–11,
2006.

[5] Y. Li, J. Chen, and L. Feng, “Dealing with uncertainty: a survey
of theories and practices,” Knowledge and Data Engineering, IEEE
Trans. on, vol. 25, no. 11, pp. 2463–2482, 2013.

[6] M. P. Deisenroth, D. Fox, and C. E. Rasmussen, “Gaussian pro-
cesses for data-efﬁcient learning in robotics and control,” Pattern
Analysis and Machine Intelligence, IEEE Trans. on, vol. 37, no. 2, pp.
408–423, 2015.

[7] Y. Bengio, A. Courville, and P. Vincent, “Representation learning:
A review and new perspectives,” Pattern Analysis and Machine
Intelligence, IEEE Trans. on, vol. 35, no. 8, pp. 1798–1828, 2013.
[8] A. J. Joshi, F. Porikli, and N. Papanikolopoulos, “Multi-class active
learning for image classiﬁcation,” in Computer Vision and Pattern
Recognition, Conf. on.

IEEE, 2009, pp. 2372–2379.

[9] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” Pattern Analysis and Machine Intelligence, IEEE Trans.
on, vol. 38, no. 3, pp. 447–461, 2016.

[10] C. Tzelepis, N. Gkalelis, V. Mezaris, and I. Kompatsiaris, “Improv-
ing event detection using related videos and relevance degree
support vector machines,” in Proc. of the 21st ACM Int. Conf. on
Multimedia. ACM, 2013, pp. 673–676.

[11] C. Tzelepis, D. Galanopoulos, V. Mezaris, and I. Patras, “Learning
to detect video events from zero or very few video examples,”
Image and vision Computing, vol. 53, pp. 35–44, 2016.

[12] M. Li and I. K. Sethi, “Conﬁdence-based active learning,” Pattern
Analysis and Machine Intelligence, IEEE Trans. on, vol. 28, no. 8, pp.
1251–1261, 2006.

[13] I. Saraﬁs, C. Diou, and A. Delopoulos, “Building effective svm
concept detectors from clickthrough data for large-scale image
retrieval,” Int. Journal of Multimedia Information Retrieval, vol. 4,
no. 2, pp. 129–142, 2015.

[14] W. Zhang, X. Y. Stella, and S.-H. Teng, “Power SVM: Generaliza-
tion with exemplar classiﬁcation uncertainty,” in Computer Vision
and Pattern Recognition (CVPR), 2012 IEEE Conf. on.
IEEE, 2012,
pp. 2144–2151.

[15] K. Crammer, A. Kulesza, and M. Dredze, “Adaptive regularization
of weight vectors,” in Advances in neural information processing
systems, 2009, pp. 414–422.

[16] M. Dredze, K. Crammer, and F. Pereira, “Conﬁdence-weighted lin-
ear classiﬁcation,” in Proceedings of the 25th international conference
on Machine learning. ACM, 2008, pp. 264–271.

[17] S. C. H. Hoi, J. Wang, and P. Zhao, “Exact soft conﬁdence-weighted
learning,” in Proceedings of the 29th International Conference on
Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 -
July 1, 2012, 2012.

[18] J. Bi and T. Zhang, “Support vector classiﬁcation with input data
uncertainty.” in Advances in Neural Information Processing Systems,
2004.

[19] F. Alizadeh and D. Goldfarb, “Second-order cone programming,”

Mathematical programming, vol. 95, no. 1, pp. 3–51, 2003.

[20] A. Ben-Tal and A. Nemirovski, “Robust convex optimization,”
Mathematics of Operations Research, vol. 23, no. 4, pp. 769–805, 1998.
[21] D. Bertsimas, D. B. Brown, and C. Caramanis, “Theory and appli-
cations of robust optimization,” SIAM review, vol. 53, no. 3, pp.
464–501, 2011.

[22] G. R. Lanckriet, L. E. Ghaoui, C. Bhattacharyya, and M. I. Jordan,
“A robust minimax approach to classiﬁcation,” The Journal of
Machine Learning Research, vol. 3, pp. 555–582, 2003.

[23] P. K. Shivaswamy, C. Bhattacharyya, and A. J. Smola, “Second
order cone programming approaches for handling missing and
uncertain data,” The Journal of Machine Learning Research, vol. 7,
pp. 1283–1314, 2006.

13

IEEE Transactions on Pattern Analysis and Machine Intelligence. (c) 2017 IEEE. DOI: 10.1109/TPAMI.2017.2772235.
Author’s accepted version. The ﬁnal publication is available at http://ieeexplore.ieee.org/document/8103808/

[24] C. Bhattacharyya, P. K. Shivaswamy, and A. J. Smola, “A sec-
ond order cone programming formulation for classifying missing
data.” in Advances in Neural Information Processing Systems, 2004.

[25] H. Xu, C. Caramanis, and S. Mannor, “Robustness and regulariza-
tion of support vector machines,” The Journal of Machine Learning
Research, vol. 10, pp. 1485–1510, 2009.

[26] H. Xu and S. Mannor, “Robustness and generalization,” Machine

learning, vol. 86, no. 3, pp. 391–423, 2012.

[27] Z. Qi, Y. Tian, and Y. Shi, “Robust twin support vector machine
for pattern classiﬁcation,” Pattern Recognition, vol. 46, no. 1, pp.
305–316, 2013.

[28] O. L. Mangasarian and E. W. Wild, “Multisurface proximal sup-
port vector machine classiﬁcation via generalized eigenvalues,”
Pattern Analysis and Machine Intelligence, IEEE Trans. on, vol. 28,
no. 1, pp. 69–74, 2006.

[29] R. Khemchandani, S. Chandra et al., “Twin support vector ma-
chines for pattern classiﬁcation,” Pattern Analysis and Machine
Intelligence, IEEE Trans. on, vol. 29, no. 5, pp. 905–910, 2007.
[30] F. De La Torre and M. J. Black, “A framework for robust subspace
learning,” Int. Journal of Computer Vision, vol. 54, no. 1-3, pp. 117–
142, 2003.

[31] S. Liwicki, S. Zafeiriou, G. Tzimiropoulos, and M. Pantic, “Efﬁcient
online subspace learning with an indeﬁnite kernel for visual
tracking and recognition,” Neural Networks and Learning Systems,
IEEE Trans. on, vol. 23, no. 10, pp. 1624–1636, 2012.

[32] H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, “Uncorre-
lated multilinear principal component analysis for unsupervised
multilinear subspace learning,” Neural Networks, IEEE Trans. on,
vol. 20, no. 11, pp. 1820–1836, 2009.

[33] T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu, “The entire reg-
ularization path for the support vector machine,” The Journal of
Machine Learning Research, vol. 5, pp. 1391–1415, 2004.

[34] L. Rosasco, E. D. Vito, A. Caponnetto, M. Piana, and A. Verri, “Are
loss functions all the same?” Neural Computation, vol. 16, no. 5, pp.
1063–1076, 2004.

[35] T. Zhang, “Statistical behavior and consistency of classiﬁcation
methods based on convex risk minimization,” Annals of Statistics,
pp. 56–85, 2004.

[36] J. Platt et al., “Probabilistic outputs for support vector machines
and comparisons to regularized likelihood methods,” Advances in
large margin classiﬁers, vol. 10, no. 3, pp. 61–74, 1999.

[37] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector
machines,” ACM Trans. on Intelligent Systems and Technology, vol. 2,
pp. 27:1–27:27, 2011, software available at http://www.csie.ntu.
edu.tw/∼cjlin/libsvm.

[38] B. S. Tsirelson, I. A. Ibragimov, and V. N. Sudakov, Norms
of Gaussian sample
Berlin, Heidelberg: Springer
Berlin Heidelberg, 1976, pp. 20–41. [Online]. Available: http:
//dx.doi.org/10.1007/BFb0077482

functions.

[39] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter, “Pegasos:
Primal estimated sub-gradient solver for SVM,” Mathematical pro-
gramming, vol. 127, no. 1, pp. 3–30, 2011.

[40] S. M. Kakade and A. Tewari, “On the generalization ability of
online strongly convex programming algorithms,” in Advances in
Neural Information Processing Systems, 2009, pp. 801–808.

[41] L. Bottou, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, L. D.
Jackel, Y. LeCun, U. A. Muller, E. Sackinger, P. Simard et al.,
“Comparison of classiﬁer methods: a case study in handwritten
digit recognition,” in Pattern Recognition, 1994. Vol. 2-Conference B:
Computer Vision & Image Processing., Proceedings of the 12th IAPR
International. Conference on, vol. 2.

IEEE, 1994, pp. 77–82.

[42] A. Ghio, D. Anguita, L. Oneto, S. Ridella, and C. Schatten, “Nested
sequential minimal optimization for support vector machines,”
in Artiﬁcial Neural Networks and Machine Learning–ICANN 2012.
Springer, 2012, pp. 156–163.

[43] W. W. Hines, D. C. Montgomery, and D. M. G. C. M. Borror,

Probability and statistics in engineering.

John Wiley & Sons, 2008.

[44] M. Lichman, “UCI machine learning repository,” 2013. [Online].

Available: http://archive.ics.uci.edu/ml
[45] S. Koelstra, C. M ¨uhl, M. Soleymani,

J.-S. Lee, A. Yazdani,
T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras, “DEAP: A database
for emotion analysis; using physiological signals,” Affective Com-
puting, IEEE Trans. on, vol. 3, no. 1, pp. 18–31, 2012.

[46] P. D. Welch, “The use of fast fourier transform for the estimation
of power spectra: A method based on time averaging over short,
modiﬁed periodograms,” IEEE Trans. on Audio and Electroacoustics,
vol. 15, no. 2, pp. 70–73, 1967.

14

[47] A. Vyas, R. Kannao, V. Bhargava, and P. Guha, “Commercial block
detection in broadcast news videos,” in Proceedings of the 2014
Indian Conference on Computer Vision Graphics and Image Processing.
ACM, 2014, p. 63.

[48] P. Over, G. Awad, J. Fiscus, M. Michel, D. Joy, A. F. Smeaton,
W. Kraaij, G. Quenot, and R. Ordelman, “TRECVID 2015 – an
overview of the goals, tasks, data, evaluation mechanisms and
metrics,” in Proc. of TRECVID 2015. NIST, USA, 2015.

[49] L. Jiang, S.-I. Yu, D. Meng, T. Mitamura, and A. G. Hauptmann,
“Bridging the ultimate semantic gap: A semantic search engine for
internet videos,” in ACM Int. Conf. on Multimedia Retrieval, 2015.

[50] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in IEEE Conf. on Computer Vision and Pattern Recog-
nition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, 2015, pp. 1–9.
[51] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and
L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”
Int. Journal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211–252,
2015.

[52] Q. McNemar, “Note on the sampling error of the difference
between correlated proportions or percentages,” Psychometrika,
vol. 12, no. 2, pp. 153–157, 1947.

[53] N. Gkalelis, V. Mezaris, I. Kompatsiaris, and T. Stathaki, “Mixture
subclass discriminant analysis link to restricted gaussian model
and other generalizations,” Neural Networks and Learning Systems,
IEEE Trans. on, vol. 24, no. 1, pp. 8–21, 2013.

Christos Tzelepis received the Diploma degree
in electrical engineering from Aristotle Univer-
sity of Thessaloniki, Greece, in 2011. During his
diploma thesis, he focused on machine learning
techniques with training data of variable relia-
bility. Currently, he is a PhD student in Elec-
tronic Engineering and Computer Science at
Queen Mary, University of London, within the
ﬁeld of discriminative machine learning, and with
ITI/CERTH.

Vasileios Mezaris received the BSc and PhD
in Electrical and Computer Engineering from the
Aristotle University of Thessaloniki in 2001 and
2005, respectively. He is a Senior Researcher
(Researcher B) at the Information Technologies
Institute (ITI) of the Centre for Research of Tech-
nology Hellas (CERTH). His research interests
include image and video analysis, event detec-
tion in multimedia, machine learning for multime-
dia, and image and video retrieval. He has co-
authored more than 35 journal papers, 10 book
chapters, 140 conference papers and 3 patents. He is/was an Associate
Editor for the IEEE Signal Processing Letters (2016-present) and IEEE
Tran. on Multimedia (2012-2015), and is a Senior Member of the IEEE.

Ioannis Patras received the BSc and MSc de-
grees in computer science from the Computer
Science Department, University of Crete, Herak-
lion, Greece, in 1994 and 1997, respectively, and
the PhD degree from the Department of Electri-
cal Engineering, Delft University of Technology,
Delft (TU Delft), The Netherlands, in 2001. He is
a Reader (Associate Professor) in the School of
Electronic Engineering and Computer Science
Queen Mary University of London, London, U.K.
His current research interests are in the area
of Computer Vision, Machine Learning and Affective Computing, with
emphasis on the analysis of visual data depicting humans and their
activities. He is an Associate Editor of the Image and Vision Computing
Journal, Pattern Recognition, and Computer Vision and Image Under-
standing.

