Top-Down Tree Structured Text Generation

Qipeng Guo ∗

Xipeng Qiu ∗

Xiangyang Xue ∗

Zheng Zhang †

∗ School of Computer Science, Fudan University
† New York University Shanghai
{qpguo16,xpqiu,xyxue} @fudan.edu.cn

zz@nyu.edu

8
1
0
2
 
g
u
A
 
4
1
 
 
]
L
C
.
s
c
[
 
 
1
v
5
6
8
4
0
.
8
0
8
1
:
v
i
X
r
a

Abstract

Text generation is a fundamental building
block in natural language processing tasks.
Existing sequential models performs autore-
gression directly over the text sequence and
have difﬁculty generating long sentences of
complex structures. This paper advocates a
simple approach that treats sentence gener-
ation as a tree-generation task. By explic-
itly modelling syntactic structures in a con-
stituent syntactic tree and performing top-
down, breadth-ﬁrst tree generation, our model
ﬁxes dependencies appropriately and performs
implicit global planning. This is in contrast
to transition-based depth-ﬁrst generation pro-
cess, which has difﬁculty dealing with incom-
plete texts when parsing and also does not
incorporate future contexts in planning. Our
preliminary results on two generation tasks
and one parsing task demonstrate that this is
an effective strategy.

1

Introduction

Generating coherent and semantically rich text
sequences is crucial to a wide range of important
natural language processing applications. Rapid
progress is made using deep neural networks. In
image captioning task, for example, a recurrent
neural network (RNN) generates a sequence of
words conditioned on features extracted from the
image (Vinyals et al., 2015b; Xu et al., 2015).
The same is true for machine translation, except
the RNN is conditioned on the encoding of the
source sentence (Bahdanau et al., 2014). The same
framework can be extended to parsing to output a
linearised parse tree (Vinyals et al., 2015a).

Despite these encouraging results, generating
long sentences with complex structures remains
an unsolved problem. The main issue is that the
most popular models are sequential in nature,
letting the RNN preform autoregression directly

on text sequence. RNN has difﬁculty to remember
and distinguish the complex and long dependen-
cies embedded in a ﬂattened sequence. This is
true even with advanced variants such as LSTM
and GRU (Hochreiter and Schmidhuber, 1997;
Chung et al., 2014). Error signals either can
not back-propagate to the right sources or worse
yet do so indiscriminately, leading to overﬁtting.
Attentional mechanism (Bahdanau et al., 2014)
sidesteps this issue but is applicable only when
alignment is possible (e.g. machine translation).

Sentences are inherently hierarchical. Properly
leveraged, hierarchical structures in the form of
constituency parse tree or dependency tree can
guide and scale text generation. Transition-based
models (Titov and Henderson, 2010; Buys and
Blunsom, 2015; Dyer et al., 2016) is a step towards
this direction. A parser works from bottom-up,
consuming a sentence while building the hierar-
chy. This working mechanism can be converted
to a generative model, by replacing the SHIFT
operator with an action that generates a word.
We observe two difﬁculties here. One is that
the bottom-up parsing actions can have difﬁculty
dealing with partially complete texts. Second, the
depth-ﬁrst nature ignores the opportunity to have
a global planning with future contexts.

This paper advocates a simpler approach. Our
model, called Top-Down Tree Decoder (TDTD),
treats sentence generation as a tree-generation
problem. TDTD generates a constituent syntactic
tree layer by layer until reaching the leaf nodes,
at which points words are predicted. By mod-
eling structures explicitly with a set of internal
RNNs that summarize prediction histories among
siblings and ancestors, error signals modify the
model appropriately. Importantly, the breadth-ﬁrst
nature pushes the model to consider (a summary)
of future contexts during the generation process.
This form of regularization implicitly leads to a

more global planning, instead of the purely local
decision as in a sequential model. We also give
an extended version TDTD-P as a parser using an
encoder-decoder framework.

These models are easy to train, only involving
curriculum learning (Bengio et al., 2009) and
scheduled sampling (Bengio et al., 2015). Experi-
ment results on two language generation tasks and
one parsing task demonstrate the promise of this
approach.

2 Probabilistic Text Generation

We review two related approaches of probability
text generation. They share the goal of generating
a text sequence X1:L = x1, x2, · · · , xL where
xi belongs to some vocabulary V. A parametric
model is learned from a dataset {X (n)}N
n=1, typi-
cally with maximum likelihood method.

For each sentence X i we can always ﬁnd a
constituency tree, or a parse tree T i, whose leaf
nodes are the words in the sentence X i itself.

2.1 Sequential Text Generation

Observing that the joint probability of the se-
quence X(1:L) can be decomposed by chain rule:

pθ(X1:L) =

pθ(xi|x0:i−1),

(1)

L
(cid:89)

i=1

We can have a sequential generator which outputs
a word xi at time step i, conditioning on all the
historical words x0:i−1, where x0 is a special
beginning-of-sentence symbol and θ is the model.
With a recurrent network, we can summarize the
history x0:i−1 into a hidden state hi:

hi = f (hi−1, xi−1),

(2)

and predict the distribution of xi by projecting hi
through a softmax operator.

This model is conceptually simple and works
very well for short sentences. It, however, ignores
syntactic structures embedded in a sentence. To
ﬁx this, the parse tree can be deterministically
linearized (e.g via depth-ﬁrst traversal) into a mod-
iﬁed sequence with words and constituencies. The
challenge with this approach is dealing with long
dependency spans, especially for long and com-
plex sentences. Attentional mechanisms sidestep
the issue in machine translation task by aligning
hidden states of the decoder to hidden states of the
encoder (Bahdanau et al., 2014; Vaswani et al.,

2017). While effective, it is clearly task-speciﬁc,
applicable only in an encoder-decoder framework
and when source sentences are available.

2.2 Transition-based Tree Structure Text

Generation

Recent works to generate text by incorporating
syntactic information include
(Titov and Hen-
derson, 2010; Buys and Blunsom, 2015; Dyer
et al., 2016). They leverage transition-based pars-
ing, where a syntactic tree is translated to an
oracle sequence of actions, involving SHIFT, RE-
DUCE and NT (open nonterminal) operations.
These operations perform on an input buffer and
a stack. Replacing the input buffer with an output
buffer and a SHIFT operation with GEN(x), these
models can be modiﬁed as a generator (Sagae and
Lavie, 2005; Nivre, 2008).

These methods explicitly model syntactic and
relationships among words. One
hierarchical
problem is their complexity, including both the
data structures to be maintained and the operations
to be performed. For the latter, there are two kinds:
the bottom-up ones to build a partial tree based on
parsing actions, and the top-down ones to generate
the next word. Parsing actions are hard to predict
when the generated text is partial and incomplete.
Also, the depth-ﬁrst order of actions limits the
opportunity to perform global planning.

3 Proposed Model

Our core idea is straightforward. Given that the
syntactic structure is a tree and is available in
training set, our generation model simply focuses
on generating a tree. This is done in a breadth-
ﬁrst fashion, ensuring the access to a summary of
future context when needed. Recall that the leaf
nodes of a tree T is simply the sequence X1:L
itself, therefore p(X1:L, T ) = p(T ).

Let V d be the sequence of nodes with length ld

at layer d, i.e.:

V d = vd

1, vd

2, · · · , vd
ld

.

(3)

We can rewrite the joint probability p(T ) via chain
rule, but across depth:

p(T ) =

p(V d+1|V d, · · · , V 1),

(4)

D−1
(cid:89)

d=0

where D is the maximum depth of tree T .

vd+1
i

. We have:

ld+1
(cid:89)

i=1
ld+1
(cid:89)

i=1

p(V d+1|Hd) =

p(vd+1
i

|ud+1
i

, Hd)

(9)

=

p(vd+1
i

|ud+1
i

, hd

π(i)),

(10)

where hd

π(i) encodes the parent of vd+1

i

.

Generating node vd+1

depends on the predic-
tion history on its siblings and ancestors in the
tree, as we describe next.

i

Gen-RNN We ﬁrst integrate the prediction his-
tory of the siblings or cousins into ui by a forward
RNN,

ud+1
i = RNN(ud+1

i−1 , vd+1
i−1 ),

(11)

Then ud+1
ancestor sd−1
tion.

is concatenated with history of the
i
π(i) to form a representation for predic-

p(yV |ud+1
p(yN |ud+1

i

i

, sd
, sd

π(i)) = SM(WV [ud+1
π(i)) = SM(WN [ud+1

, sd
, sd

i

i

π(i)] + bV ),

(12)

π(i)] + bN ). (13)

Terminal (with subscript V) and nonterminal
(with subscript N ) words are predicted separately
because the later is much smaller set. Finally, we
also predict a special STOP symbol, identifying
the point when the model switches to a cousin or
next layer after the last symbol of the current layer.
The process continues layer by layer until all leaf
nodes are terminal symbols.

3.1 Analysis

Our proposed model has the following advantages:

• Compared to the sequential models,

the
depending paths are usually signiﬁcantly
shorter in the tree structure. By explicitly
encoding the path, the model also has less
unrelated connections, reducing overﬁtting.
• The top-down process enforces future plan-
ning as a form of regularization. Node split-
ting in higher level pushes the model to plan
the production of the whole sentence rather
than focus on local features, as typically is
the case in sequential models.

• Compared to the transition-based models, our
model does not need to incorporate bottom-
up parsing operations, which are usually
difﬁcult for a partially generated text. Like

Figure 1: Tree-Stacked RNN.

We now proceed to explain how to generate the
tree layer-by-layer; RNN and SM denote the uni-
directional RNN and softmax operator, respec-
tively, each with their own learnable parameters.

To generate the nodes at (d + 1)-th layer, we
ﬁrst encode the history information from the 1st
layer to the d-th layer, V 1, · · · , V d with a tree-
stacked RNN to encode the history information
(see Figure 1). Here, each layer is encoded by a
bidirectional RNN to compute the context repre-
sentation. For each node vd
i , let π(i) denote the
index of the parent node and vd−1
π(i) be its parent
i of node vd
node. The hidden state hd
i is then:

−→
h d
←−
h d

i = RNN(

i = RNN(
−→
h d
i ;

i = [

hd

−→
h d
←−
h d
←−
h d
i ],

π(i)),

i−1, vd
i+1, vd

i , hd−1
i , hd−1

π(i)),

(5)

(6)

(7)

i , hd−1
i is the embeddings of node vd
where vd
the hidden state of the parent of node vd
i .

π(i) is

Unlike a vanilla stacked RNN, the input of RNN
i in tree-stacked RNN is the output of the

at node vd
RNN at node vd−1

π(i) , the parent of vd
i .

Thus, the hidden state hd
i

is a contextual rep-
resentation of node vd
i by integrating its sib-
ling and ancestral information. Therefore, Hd =
[hd
] of all the nodes in (d)-th layer
captures all the information of V d, · · · , V 1. The
conditional probability of the nodes in d+1-th
layer is

2, · · · , hd
ld

1, hd

p(V d+1|Hd) =

p(vd+1
i

|vd+1

i−1 , · · · , vd+1

1

, Hd).

(8)

ld+1
(cid:89)

i=1

Let ud+1
i
i−1 , · · · , vd+1
vd+1

1

encode all
the information of
, the left siblings or cousins of

Figure 2: An overall view of our parsing model, TDTD-P. The input of the root node is a vector output by
a Bi-directional RNN encoder which takes the texts (the sentences in the bottom of this ﬁgure) as inputs.
The red-blue bar represents the attention over the input sequence. The content in the black dash box
shows the pipeline of generating the red dash node (NN), the model merges three kinds of information to
get the vector representation and its label. We use Gen-RNN to collect the contextual information from
its left siblings and ancestors, and the attention results over the input sequence. The whole tree is growing
in a Top-Down Breadth-First fashion, such as {S,NP,VP,NP,PP,VBP,PP,.. }.

sequential models, a transition-based model
also lacks the overall planning due to its
depth-ﬁrst nature.

• We employ constituent syntactic tree to gen-
erate text in this paper; adapting dependency
tree is not difﬁcult.

4 TDTD-P: an Extension to Generative

Parsing Model

A parsing model receives a ﬂat word sequence
X1:T = x1, x2, · · · , xL as input and outputs its
constituency parse tree T to represent the compo-
sitional relationships among these words.

To extend our proposed model to a generative
parsing model, we formulate parsing as a condi-
tional tree generation, where the leaf nodes must
exactly match the input X1:L.

p(T |X1:L) =

p(V d+1|V d, · · · , V 1|X1:L),

(14)

D−1
(cid:89)

d=0

Therefore, the generative parsing can be re-
garded as an encoder-decoder problem. We use
bidirectional RNN to encode the input sentence,
and then apply our model as a decoder to generate
the parse tree.

Attention Mechanism Since the input sequence
can be very long (> 40 tokens), the bidirectional
RNN encoder can miss details. A popular solution
is using attention mechanism to access the origin

sequence and perform alignment dynamically at
each time-step. We also apply this idea here.

1

, henc
2

Assuming that the encoding of input sentence
is Henc = henc
..., henc
L , to generate the i-
th node of d-th layer, we use the current state
] to obtain an aligned context vector
[u
h(att) from the input sequence:

(d−1)
π(i)

(d)
i

; h

h(att) = attention([ud+1

i

; hd

π(i)], Henc),

(15)

where attention(·) is an dot-production attention
function (Vaswani et al., 2017).

Then the context vector h(att) is used as an

additional feature to generate the next node,

z = [ud+1

i

; hd

π(i); h(att)],

(16)

(17)

p(vd+1
i

|ud+1
i

, hd

π(i)) = SM(W z + b).

Since the generation process of TDTD is top-
down breadth-ﬁrst, it is hard to make the leaf
nodes exactly match the input X1:L. In this pa-
per, we treat our TDTD-P model as a scorer for
measuring the matching of a candidate tree and
its text. We use scores from TDTD-P to re-rank
candidate trees and select the tree which has the
highest score as the parsing result (see Sec-6.3).

5 Training

Given a training set {T (n)}N
n=1, we use mini-batch
stochastic gradient descent to look for a set of

parameters θ that maximizes the log likelihood of
producing the correct tree.

θ = arg max

log pθ(T (n)),

(18)

1
N

N
(cid:88)

n=1

where θ denotes all the parameters in our model.

We employ two techniques to improve train-
ing for the parsing task. The ﬁrst is curriculum
learning (Bengio et al., 2009): we constrain the
depth and width of the tree to limit the solution
space at the beginning of training and gradually
relax it. The second is schedule sampling which
has been shown to be beneﬁcial dealing with dis-
tribution shift in sequence learning (Bengio et al.,
2015): the labels are gradually replaced by model
predictions with a slowly annealing probability,
following a greedy strategy.

6 Experiments

To evaluate our model, we conduct experiments
on two generation tasks. The ﬁrst generates trees,
using a synthetic tree dataset produced by an ora-
cle of probabilistic context-free grammar (PCFG)
model. The second generates text sentences, learn-
ing from a large movie review corpus. Addition-
ally, we test the parsing task on PTB.

System compared We compare against a sim-
ilarly conﬁgured vanilla LSTM, SeqGAN and
LeakGAN (Yu et al., 2017; Guo et al., 2017);
all of them are sequential generator based on
the recurrent neural network. For these models,
we ﬁrst linearise the tree structure to brackets
expression form. SeqGAN and LeakGAN sidestep
exposure bias by adopting an adversarial training
paradigm, and LeakGAN optimizes further for
long sequences. The GAN approach, however,
suffers from mode collapsing, as our results show.

Comparison methodology It is difﬁcult to mea-
sure the quality of a generated text with syntactic
structure. First, if we evaluate our model as joint
generative model p(X, T ), there is no oracle to
judge the quality of tree T . Second, if we evaluate
our model as a language model, the marginal
probability p(X) = (cid:80)
T p(X, T ) is intractable.
Therefore, we conduct two experiments. The ﬁrst
experiment uses an oracle to score the generated
tree, and the second uses BLEU score to judge the
generated text regardless of its syntactic tree.

X
S18

→
→
ADJP21 →

Y
VP30
JJ37

NP13
PP7
PP14
S16

@CONJP0 → RB24 RB11
→ DT1 NN42
→ IN28 NP21
→ TO0 NP42
→ NP42 VP30
ADVP13 → NP13 RB43
→ CD7 NN16

NP17

prob
0.977
0.959
0.954
0.907
0.845
0.629
0.540
0.386
0.351

Table 1: PCFG production rule examples from
Berkeley parser.

Implementation Details
In TDTD, both the
Depth-RNN and the Gen-RNN are single-layer
GRU, and the Layer-RNN is a single-layer bidi-
rectional GRU. To keep the setting comparable to
previous works, we set the hidden size to 32 and
the size of input embedding of tags to 32. In the
TDTD-P variant, we set the hidden and embedding
size to 128.

6.1 Exp-I: Synthetic Data

We conduct a simulated experiment with synthetic
data, using an oracle PCFG model to play two
roles: generating training samples and evaluat-
ing the syntactic trees generated by our model.
Berkeley Parser (Petrov et al., 2006) has both
capabilities. Its PCFG model contains around
1.9M production rules and can generate various
constituency trees, available on-line1. These rules
and their associated probability can also evaluate
the likelihood of newly generated samples (see
Table 1).

To ensure stability, we remove production rules
with a probability less than 1e−6. Likewise, in the
evaluation, a 1e−6 penalty is imposed for unseen
rules in our samples. Since we wish the generated
text to be a complete sentence instead of a phrase
or substructures, we generate samples by limiting
the starting nodes to be a subset consisting of
the non-terminals “S∗” family. In addition, The
maximum depth of generated trees is set to 7.

Under the above conﬁgurations, we prepare
three datasets, varying number of nodes as 10, 15
and 20. For each dataset, we sampled 10,000 trees
as the training set. This way, we can inspect how
our model scales with sentence length. Note that
linearisation increases sequence length: samples

1https://github.com/slavpetrov/berkeleyparser

Model

Len

NLL Fail (%) Dup (%)

LSTM SeqGAN LeakGAN TDTD

2.43
10(30)
Oracle
3.85
10(30)
LSTM
SeqGAN 10(30)
0.67
LeakGAN 10(30) 8.25†
10(30) 3.58
TDTD

15(45)
15(45)

2.63
Oracle
6.39
LSTM
SeqGAN 15(45) 7.41†
LeakGAN 15(45) 6.42†
15(45) 3.86
TDTD

20(60)
20(60)

2.85
Oracle
7.55
LSTM
SeqGAN 20(60) 7.88†
LeakGAN 20(60) 6.79†
20(60) 4.32
TDTD

−
54.1
2.6
52.0
0.0

−
66.2
93.7
78.2
0.0

−
67.8
94.2
71.1
0.0

11.2
8.6
93.0
0.0
25.7

1.3
0.0
0.0
0.0
16.7

0.3
0.0
0.0
0.0
11.8

Table 2: Results on synthetic datasets. “Len”
means number of nodes in the tree, and the number
in bracket is the length of brackets sequence.
“Fail” denotes the percentage of ill-formed gen-
erated samples, which have unmatched brackets
and cannot be converted into a tree. “Dup” is
the percentage of duplicate samples in all the
generated samples. Failed samples are not counted
in the NLL score. † means that the performance
falls after a few iterations during the training, in
which case we perform early-stop.

in brackets form are always three times of the
number of nodes.

Evaluation We use the negative log-likelihood
(NLL) to evaluate the generated samples by the or-
acle PCFG model. As mention before, we limit the
starting nodes to be the “S∗” subset. Therefore, for
a tree T with root v1, we can use the conditional
probability P (T |v1) instead of P (T ).

Since the oracle is a PCFG model, the probabil-
ity can be decomposed by a chain of productions.
Thus, NLL of a given tree T is:

N LL(V |v1) = − log Poracle(T |v1)

(19)

= −

log Poracle(V C
i

|vi), (20)

(cid:88)

vi∈T

where v1 is the root node, V C
i
of the non-leaf node vi, and Poracle(V C
i
probability of production vi → V C
i

are the children
|vi) is the

in oracle.

BLEU-2 0.652
BLEU-3 0.405
BLEU-4 0.304
BLEU-5 0.202

0.683
0.418
0.315
0.221

0.809
0.554
0.358
0.252

0.718
0.568
0.375
0.263

Table 3: BLEU results on IMDB.

Generation results Table 2 shows that, except
SeqGAN with 10 nodes, our model outperforms
all others on NLL. However, SeqGAN suffers
from mode collapsing, as seen by its high dupli-
cation rate in its generated samples (93%, the 3-
rd row). The performance of all sequential models
drop uniformly with longer sentences and be-
come increasingly difﬁcult to recover coherent
tree structures. These results indicate the funda-
mental defect of the sequential models.

TDTD works consistently well on small and
large trees, suggesting that the hierarchical struc-
ture has a high potential for dealing with long
sequence because the dependency path of tree
structures grows much slower.

6.2 Exp-II: Real-world Text Generation

To evaluate the ability to generate real-world texts,
we experiment our method on an unconditional
text generation task similar to (Yu et al., 2017;
Guo et al., 2017) but use a large IMDB text
corpus (Diao et al., 2014) to train our model. This
dataset is a collection of 350K movie reviews and
contains various kinds of compound sentences.
We select sentences with the length between 17
and 25, set threshold at 180 for high-frequency
words and only select sentences with words above
that threshold. Finally, we randomly choose 80000
sentences for training and 3000 for testing, with
vocabulary size at 4979 and the average sentence
length at 19.6. We use Stanford Parser to obtain
the syntactic tree for training our model.

We use Stanford Parser2 to obtain the syntactic
tree, and BLEU score (Papineni et al., 2002) to
measure similarity degree between the generated
texts and the texts in test set.

The results in Table 3 show that the BLEU
scores of TDTD are consistently higher than the
other models except BLUE-2. The results indicate
that the generated sentences of TDTD are of high
quality to mimic the real text.

2https://nlp.stanford.edu/software/

Method

LSTM

SeqGAN

LeakGAN

TDTD

Cases
(1) His monster is modeled after the old classic
“B” science ﬁction movies we hate to love, but
it was known with the ﬁrst humor .
(2) But I was completely bored the movie the
way I saw this movie at the same time .
(1) This does not star Kurt Russell, but rather
allows him what amounts to an extended cameo
.
(2) Don’t all of them because it will not be a very
boring love stories and so many people judge at
some people .
(1) The ﬁlm is a simple, the premise that an
ordinary guy can become a hero, only if he can
be easy to see .
(2) It is a non-stop adrenaline rush from begin-
ning to end, and as for how it was the main
villain .
(1) Need for Speed is a great movie with a very
enjoyable storyline and a very talented cast .
(2) The story is modeled after the old classic “B”
science ﬁction movies

Table 4: Samples from different models

Case Study Table 4 shows some generated sam-
ples of our and other methods, which also shows
that
the generated sentences of TDTD are of
higher global consistency and better readability
than others.

We also give an error analysis in Figure 3,
which illustrates a real generated text with its
syntactic tree, including some errors. We make a
few observations:

• Our model successfully applies global plan-
ning early on, as expected. Such as “S → NP
VP .” at the top-level.

• This example highlights some of the error
patterns, some of them were contributed by
bad trees that were automatically parsed on
the noisy IMDB texts. In this example, the
ﬁrst error (VP → VP, “,” , S) occurs in the
earlier phase, which persists till the end. The
second error is the redundant double “NP”s,
and the third error is that the role of “all” is
closer to pronoun rather than a determiner in
this sentence.

6.3 Exp-III: Parsing results

Although our model is not intended for parsing,
it can be converted into a generative parser by
adopting an encoder-decoder structure (TDTD-P;
see Section 4). The experiment is conducted on
Penn Treebank, §2-21 are used for training, §24
used for development, and §23 used for evaluation.

S

NP

VP

.

DT

NNS

VP

,

S

VP

,

CC

VP

NP

VP

VBP

NP

VBP

ADVP

DT

VBG

NP

NN

RB

The

effects

are

nothing

,

but

are

still

,

all

considering

.

Figure 3: An error case generated by TDTD, in-
volving three main errors.

Algorithm

Petrov et al. (2006)
Zhu et al. (2013)
Zhu et al. (2013) †
Vinyals et al. (2015a) PTB only
Vinyals et al. (2015a) †

RNNG-D (Dyer et al., 2016)
RNNG-G (Dyer et al., 2016)
GA-RNNG (Smith et al., 2017)
TDTD-P

F1

90.4
90.4
91.8
88.3
92.1

91.2
93.3
93.5
91.9

Table 5: Parsing results on PTB. † means semi-
supervised method.

Since this parser is a generative model, we adopt
the same evaluation paradigm of RNNG (Dyer
et al., 2016). We ﬁrstly obtained 100 independent
candidates3 for each test case with a discriminative
parser. Then we re-rank the candidates according
to their probabilities computed by our model.

We use the F1-score, a standard measurement
in parsing tree evaluations as our metric. Table 5
gives the performance of our model, along with
several representative models. RNNG-D (Dis-
criminative) is the result before re-ranking, and
RNNG-G is the re-ranked results by RNNG. GA-
RNNG incorporates gated attention into RNNG.
The result shows that TDTD-P can work as a
decent parser, but does not work as well as RNNG
methods. One reason is that TDTD-P only has
top-down actions, unlike RNNG that also include
bottom-up parsing actions. Therefore TDTD-P is
less suited for parsing task.

3The

samples

are
https://github.com/clab/rnng,
2016).

publicly

on
released by (Dyer et al.,

available

S

CC

NP 1

VP 1

.

PRP

MD

RB

VP 2

VB

NP 2

DT

But

he

could

n’t

sell

any

.

(a) A parsing result.

(b) Visualization of attention.

Figure 4: A case study of TDTD-P. (a) The red token indicates an error in the prediction, our model
predicted a wrong label “NN” and the correct label is “DT”. Circled numbers are used to distinguish tags
with the same name. (b) Visualization of attention, and horizontal lines split the nodes with its depth.

Case Study In the parsing experiment, we are
interested in how our model converts a ﬂatten se-
quence into a tree, a meaningful viewpoint is prob-
ing the attention of each node. Speciﬁcally, we
look for the attention difference between sibling
nodes and the difference between the parent and
the children nodes. Figure 4 gives an illustration
of how attention working in TDTD-P.

7 Related Work

Although deep neural networks have made a great
progress in text generation, most of them em-
ployed sequential models, performing autoregres-
sion directly on the text sequence. Generating text
by incorporating its syntactic tree structure has not
been a popular approach.

Vinyals et al. (2015a) linearises parsing trees to
brackets expression form and use the attention-
enhanced sequence-to-sequence model to parse
sentences. Although their method can generate a
tree-structured text with slight modiﬁcation, the
linearization process only aggravates the issue of
long-distance dependency for sequential models.
Also, the attentional mechanism is available in an
encoder-decoder framework.

Dyer et al. (2016) proposes recurrent neural net-
work grammars, a generative probabilistic model
of phrase-structure trees. Their model operates via
a recursive syntactic process reminiscent of prob-
abilistic context-free grammar generation. How-
ever, decisions parametrized with RNNs are con-

ditioned on the entire syntactic derivation history.
This greatly relaxes context-free independence
assumptions.

All the above methods generate text in depth-
ﬁrst traversal order. Different from them, ours is
the ﬁrst work to use syntactically structured neural
models to generate language in top-down breadth-
ﬁrst fashion.

Besides, there are also some works that explore
the syntactically structured neural architectures in
a number of other applications, including discrim-
inative parsing (Socher et al., 2011), sentiment
analysis (Socher et al., 2013; Tai et al., 2015).
However, these model focus to learn sentence
representation and they typically utilize the syn-
tactical structure in a bottom-up fashion.

8 Conclusion

In this paper, we propose a new method that treats
text generation as a tree-generation problem. The
approach explicitly utilizes syntactic information
and considers a more global planning, two issues
existing models fail to deal with efﬁciently. The re-
sults are promising, and future extensions include
incorporating work from reinforcement learning
and graph generation.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly

learning to align and translate.
arXiv:1409.0473.

arXiv preprint

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015.
Scheduled sampling for
sequence prediction with recurrent neural networks.
In NIPS, pages 1171–1179.

Yoshua Bengio, J´erˆome Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
ICML, volume 382 of ACM International Confer-
ence Proceeding Series, pages 41–48. ACM.

Jan Buys and Phil Blunsom. 2015. A bayesian model
for generative transition-based dependency parsing.
In DepLing, pages 58–67. Uppsala University, De-
partment of Linguistics and Philology.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence mod-
eling. Advances in Neural Information Processing
Systems Deep Learning Workshop.

Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexan-
der J Smola, Jing Jiang, and Chong Wang. 2014.
Jointly modeling aspects, ratings and sentiments for
movie recommendation (jmars). In SIGKDD, pages
193–202. ACM.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural network
In HLT-NAACL, pages 199–209. The
grammars.
Association for Computational Linguistics.

Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong
Yu, and Jun Wang. 2017. Long text generation via
adversarial training with leaked information. arXiv.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Joakim Nivre. 2008. Algorithms for deterministic
incremental dependency parsing. Computational
Linguistics, 34(4):513–553.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
In ACL. The Association
pretable tree annotation.
for Computer Linguistics.

Kenji Sagae and Alon Lavie. 2005. A classiﬁer-based
In IWPT,
parser with linear run-time complexity.
pages 125–132. Association for Computational Lin-
guistics.

Noah A. Smith, Chris Dyer, Miguel Ballesteros, Gra-
ham Neubig, Lingpeng Kong, and Adhiguna Kun-
coro. 2017. What do recurrent neural network gram-
mars learn about syntax? In EACL (1), pages 1249–
1258. Association for Computational Linguistics.

Richard Socher, Cliff C Lin, Chris Manning, and
Andrew Y Ng. 2011. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of ICML.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
In ACL (1), pages 1556–1566. The Asso-
works.
ciation for Computer Linguistics.

Ivan Titov and James Henderson. 2010. A latent
variable model for generative dependency parsing.
In Trends in Parsing Technology, pages 35–55.
Springer.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. arXiv preprint arXiv:1706.03762.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey E. Hinton. 2015a.
In NIPS, pages
Grammar as a foreign language.
2773–2781.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015b. Show and tell: A neural
In CVPR, pages 3156–
image caption generator.
3164. IEEE Computer Society.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. 2015. Show,
attend and tell: Neural image caption generation
In ICML, volume 37
with visual attention.
of JMLR Workshop and Conference Proceedings,
pages 2048–2057. JMLR.org.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In AAAI, pages 2852–2858.

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013.
Fast and accurate shift-
reduce constituent parsing. In ACL (1), pages 434–
443.

