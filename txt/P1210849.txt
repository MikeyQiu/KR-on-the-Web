Modeling cognitive deﬁcits following neurodegenerative diseases and traumatic brain
injuries with deep convolutional neural networks

Bethany Lusch∗, Jake Weholt, Pedro D. Maia, J. Nathan Kutz

Department of Applied Mathematics, University of Washington, United States

6
1
0
2
 
c
e
D
 
3
1
 
 
]

C
N
.
o
i
b
-
q
[
 
 
1
v
3
2
4
4
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

The accurate diagnosis and assessment of neurodegenerative disease and traumatic brain injuries (TBI) remain open
challenges to doctors and medical practitioners. Both cause cognitive and functional deﬁcits due to the diﬀuse presence
of focal axonal swellings (FAS), but it is diﬃcult to deliver a prognosis due to our limited ability to assess damaged
neurons at a cellular level in vivo. In this paper, we simulate the eﬀects of neurodegenerative disease and TBI using
convolutional neural networks (CNNs) as our model of cognition. CNNs were originally inspired by neuroscience and the
hierarchical layers of neurons used for processing input stimulus to the brain. We start with CNNs pre-trained to perform
classiﬁcation, then utilize biophysically relevant statistical data on FAS to damage the connections in a functionally
relevant way. In order to improve the model, we incorporate the idea that brains operate under energy constraints by
pruning the CNNs to be less over-engineered. Qualitatively, we demonstrate that damage to the connections leads to
human-like mistakes. Our experiments also provide quantitative assessments of how accuracy is aﬀected by various types
and levels of damage. The deﬁcit resulting from a ﬁxed amount of damage greatly depends on which connections are
randomly injured, providing intuition for why it is diﬃcult to predict the impairments resulting from an injury. There is
a large degree of subjectivity when it comes to interpreting cognitive deﬁcits from dynamically evolving complex systems
such as the human brain. However, we provide important insight and a quantitative framework for several disorders in
which FAS are implicated, such as TBI, Alzheimer’s, Parkinson’s, and Multiple Sclerosis.

Keywords: Traumatic brain injury, neurodegenerative disease, focal axonal swellings, convolutional neural networks

1. Introduction

The mathematical architecture of convolutional neu-
ral networks (CNNs) was originally inspired by the No-
bel prize-winning work of Hubel and Wiesel on the pri-
mary visual cortex of cats [41]. Their seminal experiments
were the ﬁrst to suggest that neurons in the visual sys-
tem organize themselves in hierarchical layers of cells for
processing visual stimulus. The ﬁrst quantitative model
of the CNN, termed the Neocognitron by Fukushima in
1980 [27], already displayed many of the characteristic fea-
tures of today’s deep CNNs, including a multi-layer struc-
ture, convolution, max pooling and nonlinear dynamical
nodes. The connection between neuroscience and CNN
theory, although clearly a conceptional abstraction [74],
has since been instrumental to improving quantitative mod-
els of how the brain integrates neuro-sensory information
for stimulus classiﬁcation and decision making. Given that
CNNs mimic many of the important cognitive features of
the brain, we use it as a model for understanding how neu-
rodegenerative diseases and traumatic brain injuries (TBI)

∗Corresponding author at: University of Washington, Lewis Hall

#202, Box 353925, Seattle, WA 98195-3925 USA

Email address: herwaldt@uw.edu (Bethany Lusch)

can compromise an array of recognition tasks. Specif-
ically, by using well-established biophysical data on the
statistics (distribution and size) of focal axonal swellings
(FAS), which area among the primary symptoms of neu-
rodegeneration and TBI, we evaluate the progress of im-
pairments on a CNN-based model of cognition. Our model
provides quantitative metrics for understanding how cog-
nitive deﬁcits are accumulated as a function of FAS devel-
opment, allowing for potentially new diagnostics for the
evaluation of brain disorders due to neurodegenerative dis-
eases and/or TBI.

Understanding how neurodegenerative diseases and TBI
aﬀect cognitive function remains a critically important chal-
lenge for societal mental health. TBI alone is one of the
major causes of disability and mortality worldwide, which
in turn, dramatically jeopardizes society in several socioe-
conomic ways [67]. Not only is it the signature injury of
the wars in Afghanistan and Iraq [46], it is also the leading
cause of death among young people [24]. While many sur-
vive the events that induce TBI, persistent cognitive, psy-
chiatric, and physiological dysfunction often follows from
the mechanical impact (see Sec. 2). Likewise, neurode-
generative diseases are responsible for an overwhelming
variety of functional deﬁcits, with common symptoms in-
cluding memory loss or behavioral/cognitive impairments

December 15, 2016

Figure 1: Damaging a Convolutional Neural Network (CNN). (a) We start with a “healthy” CNN that accepts an image of a handwritten
digit as an input and outputs scores for each possible digit, 0-9. We classify the image as the digit with the highest score. (b) We then
damage the weights on the network in a biophysically-relevant way. In this ﬁgure, the healthy network correctly classiﬁes the image as a 2,
but the damaged network classiﬁes it as a 1.

which are related to an inability to correctly process multi-
modal information for decision-making tasks. The major-
ity of brain disorders have a complex cascade of patholog-
ical eﬀects spanning multiple spatial scales: from cellular
or network levels to tissues or entire brain areas. Unfor-
tunately, our limited ability to diagnose cerebral malfunc-
tions in vivo cannot detect several anomalies that occur on
smaller scales. FAS, however, are ubiquitous to TBI and
most leading and incurable disorders that dramatically af-
fect signaling properties of neurons, such as Multiple Scle-
rosis, Alzheimer’s and Parkinson’s diseases.

Given the currently available wealth of data on FAS
morphology from TBI studies and from almost every lead-
ing neurodegenerative disease, signiﬁcant progress can be
made towards understanding qualitatively how FAS im-
pacts cognitive function. In this work, we consider a set of
deep CNN models as an abstraction for functioning brains.
Our goal is to understand how the processing of input data
(classiﬁcation) is compromised as a function of increasing
injury and/or disease progression. Of course, it is obvi-
ous that the system’s performance will be compromised as
the CNN is injured, but the manner in which the cogni-
tive impairments arise is quite illustrative and informative,
providing intuitively appealing results about how cognitive
deﬁcits can develop and evolve as a neurodegenerative dis-
ease progresses.

Figure 1 illustrates our approach. We begin with the
original (healthy) CNN, which is trained to perform a clas-
siﬁcation task. In Figure 1, the speciﬁc task is to label a
handwritten digit. We then expose the CNN to diﬀerent

injury protocols based upon biophysical observations of
FAS statistics and morphological parameters. In particu-
lar, we use statistical distributions of FAS from a recent
experiment consisting of TBI-induced damage in the vi-
sual cortex of rats [93]. To impose these injury statistics
on the original CNN, we assume that each neuronal con-
nection has a biophysically plausible probability to mal-
function; while mild axonal injury may simply weaken a
connection, severe cases may break it permanently (i.e., an
axomtotomy occurs so that the connection strength goes
to zero). Ultimately, the severity of the injury and re-
weighting of connections is also determined by biophysical
data and the statistical distribution of the size of the FAS.
We can then progressively monitor the deleterious eﬀects
of the injury on the functionality of the CNN, providing
metrics for cognitive deﬁcits that arise.

The paper is outlined as follows: In Sec. 2 we provide
key background material on the two primary ﬁelds inte-
grated into this work: convolutional neural networks and
neural disorders in which FAS are implicated. We describe
our methodology in Sec. 3 and present results in Sec. 4.
We summarize our conclusions in Sec. 5. For full details,
all MATLAB and Python codes used for this paper are
available online at github.com/BethanyL/damaged_cnns.

2. Background

2.1. Convolutional Neural Networks

Deep convolutional neural networks (DCNNs) are trans-
forming almost every ﬁeld of science involving big data.

2

The success of the method has been enabled by two criti-
cal components: (i) the continued growth of computational
power (e.g. GPU and networked computing), and (ii) ex-
ceptionally large labeled data sets capable of taking advan-
tage of the full power of a multi-layer architecture. Indeed,
although the theoretical inception of CNNs has an almost
four-decade history, the analysis [52] of the ImageNet data
set [19] in 2012 provided a watershed moment for CNNs
and Deep Learning [57]. Prior to this data set, there were
a number of data sets available with approximately tens of
thousands of labeled images. ImageNet provided over 15
million labeled, high-resolution images with over 22,000
categories. DCNNs have since transformed the ﬁeld of
computer vision by dominating the performance metrics
in almost every meaningful computer vision task intended
for classiﬁcation and identiﬁcation (see, for example, the
International Conference on Computer Vision 2015).

ImageNet has been a critically enabling data set for
the evolution of the ﬁeld. However, CNNs were a topic of
intensive research long before. Indeed, they were highly
successful in a wide range of applications and machine
learning architectures. By the early 1990s, neural net-
works were studied as standard textbook material [7], with
the focus typically on a small number of layers. Critical
machine learning tasks such as principal component anal-
ysis (PCA) were shown to be intimately connected with
networks which included back propagation [4, 79]. Impor-
tantly, there were a number of critical innovations which
established multilayer feedforward networks as a class of
universal approximators. Speciﬁcally, Hornik et al. [39]
rigorously established that standard multilayer feedforward
networks with as few as one hidden layer using arbitrary
squashing functions were capable of approximating any
Borel measurable function from one ﬁnite dimensional space
to another to any desired degree of accuracy, provided suf-
ﬁciently many hidden units were available. Thus, multi-
layer feedforward networks could be thought of as a class
of universal approximators [39].

The past ﬁve years have seen tremendous advances in
the DCNN architecture. Innovations have come from al-
gorithmic tricks and modiﬁcations that have led to sig-
niﬁcant performance gains in a variety of ﬁelds. These
innovations include pretraining [38, 6, 22], dropout [84],
max pooling [52], inception modules [85], data augmen-
tation (virtual examples) [71], batch normalization [42]
and/or residual learning [31]. This is only a partial list
of potential algorithmic innovations available for improv-
ing the performance of classiﬁcation and labeling. Our
goal is not to provide a complete review of the DCNN
ﬁeld, but rather to highlight the continuing and rapid pace
Integrating the state-of-the-art
of progress in the ﬁeld.
in DCNNs is the open source software called TensorFlow
(tensorﬂow.org). TensorFlow was originally developed by
researchers and engineers working on the Google Brain
Team within Google’s Machine Intelligence research orga-
nization. The system is designed to facilitate research in
machine learning and to make it quick and easy to transi-

Figure 2: Four Types of Damaged Axons. A spike train passes
through a swollen axon. Depending on the way that the axon is
swollen, there are four ways that the information can be transmit-
ted. In transmission, the spike train is propagated correctly despite
the damage.
In ﬁltering, the spike train goes through a low-pass
ﬁlter. Regions of the spike train with high frequency are especially
likely to lose spikes. In reﬂection, pairs of spikes combine and only
half of the spikes are transmitted. In blockage, none of the spikes
are transmitted.

tion from research prototype to production system. Ten-
sorFlow has allowed for the test-bedding of new algorith-
mic structures in a reproducible and veriﬁable manner,
which is a signiﬁcant and important advancement in the
ﬁeld. Indeed, the DCNN architecture used here relies on
the TensorFlow architecture, helping us understand how
state-of-the-art DCNNs relate to cognitive abilities.

2.2. Focal Axonal Swellings

Concussions and Traumatic Brain Injuries (TBI) are more
than ever a concern for contact sport practitioners [23], for
veteran soldiers exposed to blast injuries [18, 46], and for
society as a whole [24, 67, 77]. In fact, TBI contributes
to one-third of all injury-related deaths and is one of the
major sources of functional impairments. TBI pathologies
aﬀect several spatial scales [80], but a ubiquitous develop-
ment at the neuronal microenvironment level is the pres-
ence of axonal injury [37, 45, 81]. As reviewed in [37], rapid
axonal stretch injury triggers secondary axonal changes
that can vary in extent and severity [21, 30, 35], but most
often culminate in Focal Axonal Swellings (FAS).

FAS are monitored whenever possible in in-vitro stud-
ies [12, 25, 32, 33, 34, 61, 69, 82], in in-vivo experiments

3

[9, 20, 66, 93], and in human patients [3, 8, 15, 29, 46, 51,
75]. In many cases, FAS critically aﬀect the axonal mor-
phology [87, 88] and consequently, the information content
encoded in spike trains propagating throughout them.

Recent computational studies distinguished geometri-
cal axonal enlargements that lead to minor changes in
propagation from those that result in critical phenomena
such as reﬂection or blockage of the original traveling pulse
[65], or ﬁltering of action potentials [64]. This led to a di-
agnostic toolbox that extracts meaningful geometrical pa-
rameters from sequential images of injured axon segments
[63]. These algorithms provide a principled approach to
deal with imaging distortions caused by experimental ar-
tifacts in order to extract the cross-section of an axon by
detecting local symmetries, turning points and turning re-
gions. More importantly, they provide the ﬁrst description
of biologically plausible injurious eﬀects due to FAS that
can be incorporated into neuronal network simulations.
Figure 2 reviews these diﬀerent eﬀects. In the transmis-
sion regime, the spike train propagates through the FAS
without signiﬁcant modiﬁcations. In the ﬁltering regime,
pulses that are too close to each other get deleted by a
mechanism resembling a pile-up collision [64]. As the FAS
geometrical parameters worsen, a single spike will split into
two components, one propagating forward and the other
propagating backward. The reﬂected, back-propagating
pulse will collide with the next spike in the train and they
will mutually annihilate each other. Thus, the reﬂection
regime eﬀectively halves the ﬁring rate of the neuron. Fi-
nally, in the worst-case scenario, the FAS will block all
spikes and transmit no information whatsoever.

In what follows, we will introduce FAS in a biologically
plausible way to a few examples of deep-learning convolu-
tional neural networks and evaluate the extent to which
cognitive deﬁcits develop.

3. Materials and Methods

3.1. CNN training, calibration, and performance

We simulate the development of FAS damage in three
diﬀerent convolutional neural networks. Each network has
its own properties and was trained with diﬀerent data sets
for separate tasks (see Table 1).

First, we consider a network trained on the MNIST
dataset [58], which is composed of images of handwritten
digits. We train the network to classify each image as a
digit from 0 to 9.
It could be used, for example, by a
post oﬃce machine to read zip codes from envelopes. The
training data set consists of a series of black and white
images that are 28 by 28 pixels. We use the TensorFlow
framework [1] to train a CNN with two convolutional layers
and a fully connected layer, as advised by a TensorFlow
tutorial. We use a subset of the standard MNIST test
set for our testing purposes so that our set contains the
In particular,
same number of examples for each digit.
we choose the ﬁrst 852 images for each digit. Our trained
network has an accuracy of 98.74% on this test set.

Next, we use a network trained on the ImageNet data
set to classify images from the ILSVRC 2012 challenge as
one of one thousand objects [78]. The CNN-F network
was pre-trained by the Visual Geometry Group at Oxford
[11] and made available through the MatConvNet Matlab
Toolbox [92], where it is referred to as imagenet-vgg-f. The
network contains ﬁve convolutional layers and three fully
connected layers. For our experiments, we use a subset of
the data with two examples randomly chosen from each
class. The network is 54.6% accurate on this test set.

The third network that we use was trained to classify
faces as one of one thousand people. However, if you re-
move the last classiﬁcation layer and normalize the output
vector, the network can instead be used to create feature
vectors for face veriﬁcation. If the Euclidean norm of the
diﬀerence between the feature vectors for two images is un-
der a threshold τ , the pair of images is classiﬁed as being
the same person. This network was also trained by the Vi-
sual Geometry Group [72] and made available through the
MatConvNet toolbox [92], where it is called vgg-face. For
our experiments, we randomly choose ﬁve pictures each of
ﬁfty randomly chosen celebrities from the Labeled Faces
in the Wild (LFW) data set [40]. We also need to choose a
threshold τ . We choose τ = 1.2 based on Linear Discrimi-
nant Analysis on a training set of 5700 examples from the
LFW data set. Each of the 250 images in our test set is
then compared to the four other images of the same per-
son and four images of other people. We thus test one
thousand pairs of images, half of which are of the same
person and half of which are not. The network is 81.6%
accurate with τ = 1.2 on our test set of one thousand pairs
of images.

3.2. Network impairments following FAS injuries

To simulate the eﬀects of traumatic brain injury on a
CNN, we randomly “damage” p percent of the weights in
the convolutional and fully connected layers. For consis-
tency with the TBI analogy, we only target the connections
between neurons and not bias weights. Note that these
CNNs are designed to use the same weights for multiple
connections. Thus, damaging p percent of the weights is
not equivalent to damaging p percent of the connections.
For simplicity, we ﬁrst assume that all axonal injuries lead
to the total blockage of spikes, which eﬀectively sets p per-
cent of weights to zero. We consider damage examples for
each one of the previously described networks to develop
intuition about possible functional impairments.

In Figure 3, we choose a handwritten “2” as the in-
put to the MNIST network. The network assigns a score
to each of the ten possible digits and then classiﬁes the
image as the digit with the highest score. The original
network gives scores of .999987 to 2, .000013 to 1, and 0
to the rest of the digits and thus correctly classiﬁes the im-
age as a 2. We then randomly damage the network in 100
separate experiments, setting p = .01, .02, . . . , 1. Since we
are simulating TBI, the damage happens all at once and
is not accumulated across experiments. Thus, the set of

4

Table 1: Summary of CNNs Used

Task

Training Set

# conv. # fully connected # weights

layers

layers

to damage

handwritten digit classiﬁcation

MNIST: 55K

object classiﬁcation

ImageNet, ILSVRC

2012: 1.2M

2

5

face veriﬁcation

VGG-Face: 2.6M

13

1

3

2

83K

61M

134M

mistakes seem less understandable, such as “hair slide.”
Note that this network was trained on about 1.2 million
images of the one thousand classes, encompassing a wide
range of examples for each class. For illustration purposes
in this ﬁgure, we show an example image from the test set
for each class. However, the input image is downloaded
from Flickr [48].

In Figure 5, we give a more diﬃcult input image to
the ImageNet network—a group of vegetables composed
predominately of peppers with a variety of colors but also
containing garlic. This image accompanies the Image Pro-
cessing Toolbox for Matlab as peppers.png and is used as
a demo for this network in the MatConvNet Toolbox. The
network successfully chooses the bell pepper class among
one thousand possibilities, but it is not as robust to injury
as the one in the previous, easier example. Misclassiﬁca-
tions begin at 4% injury. Again, some errors are reason-
able, such as a “cucumber” or “orange” (which is not that
diﬀerent from an orange-colored pepper). Others are quite
surprising, such as “socks” or “teddy bear”.

In Figure 6, we show analogous deﬁcits for the facial
recognition network (VGG-Face). We input an image of
George W. Bush to the network and have it compare the
image with three other pictures—one of his father George
H. W. Bush, one of Bill Clinton, and another one of him-
self. All images come from the LFW data set. The healthy
network correctly identiﬁes that both pictures of George
W. Bush are of the same person and that the pictures of
his father and Bill Clinton are of diﬀerent people. We also
see that George W. Bush is closer to his father than to Bill
Clinton. As the damage increases, the network occasion-
ally classiﬁes George H. W. Bush as being the same as his
son and, eventually, cannot even distinguish Bill Clinton.
After about 70% damage, all images start looking alike,
and the network continuously exchanges the ordering of
the distances as the damage increases. This pattern con-
tinues in the broader experiments and, with enough dam-
age, all pairs of images are labeled as being of the same
person. Note that adjusting the threshold as damage in-
creases would not improve the accuracy since the second
picture of George W. Bush does not remain closer than
the pictures of other people.

Figure 3: Change in Class Scores on Damaged Networks. This CNN
accepts an image of a handwritten digit as an input and outputs
scores for each possible digit, 0-9. In these two examples, the original
network correctly and conﬁdently classiﬁes the digit. As we increase
the damage level, conﬁdence drops and the classes eventually become
confused. For high levels of damage, all classes have similar scores.

damaged neurons when p = .01 may have little overlap
with the targeted neurons in the p = .02 case. At around
12% damage, the network becomes noticeably less conﬁ-
dent, but still correct. The network makes its ﬁrst mistake
at 30% damage by labeling the image as a 1. At higher
levels of damage, it frequently confuses classes 1 and 2.
After 90% damage, the ordering of the class scores con-
tinues to change, but their values become quite similar.
We see an analogous pattern in the second part of Fig-
ure 3 where we input a “5” as an example, although the
damaged networks make fewer mistakes.

Next, we consider two examples from the image classi-
ﬁcation problem. In Figure 4, we use an image of a green
bell pepper as the input to the ImageNet network. Here
the network assigns a score to each of one thousand possi-
ble classes before classifying the image as the class with the
highest score. We visualize how the classiﬁcation changes
as the damage increases (p = 0, 0.01, 0.02, . . . , 0.3). The
network makes its ﬁrst mistake at 12% damage but some-
times returns to classifying the image as a bell pepper.
Some of the mistakes are relatively sensible, such as “granny
smith” or “tennis ball,” and share similar colors, textures
and/or shapes with the original image. Some of the later

5

Figure 4: Classiﬁcation Mistakes as Damage Increases, Example 1. We start with a healthy network trained to classify images. The original
network correctly classiﬁes this image as a green pepper, but with enough damage, the network makes mistakes. For moderate amounts of
damage, the wrong classiﬁcations make some intuitive sense.

Figure 5: Classiﬁcation Mistakes as Damage Increases, Example 2. We increase the diﬃculty by using an image of a group of vegetables,
primarily bell peppers. The network does not maintain the “bell pepper” classiﬁcation as long, but the early mistakes are also produce or
also round items.

6

Figure 6: Change in Distance Between Images. This network outputs
a feature vector for each image and can be used to ﬁnd the distance
between two images. If the distance is below our threshold τ , the
pair is labeled as being of the same person. The network originally
correctly identiﬁes the second image of George W. Bush as being
the same person while labeling the images of George H. W. Bush
and Bill Clinton as being diﬀerent people. After suﬃcient damage,
the distances between the images all shrink and it is not possible to
determine whether or not a pair of images are of the same person.

4. Results

In this section, we move from qualitative descriptions of
single network errors to a more broad, statistical account
of mistakes within the test sets. We also consider a few
variations of FAS injury protocols, network settings, and
their dynamics to model biologically relevant phenomena
such as aging and the development of neurodegenerative
eﬀects across CNNs.

4.1. Overall network impairments

In Figure 7, we return to the MNIST handwritten digit
classiﬁcation task and plot confusion matrices Mi,j for
the ten digits as the damage percentage p increases. At
p = 0%, the network has 98.75% accuracy, so the matrix
is concentrated on the diagonal (i = j). At p = 20%, we
begin to see substantial errors (i (cid:54)= j), especially by over-
classifying digits 0, 4, and 9. As the damage increases, the
confusion matrices become even more distributed, but the
types of errors change. For example, at 40% damage, some
especially common labels are 1 and 6, while at 60% dam-
age, the disproportionately common labels become 0, 2, 4,
and 7. However, recall that in our TBI analogy, the dam-
age is not accumulated—in each experiment, we return to
the original network and randomly choose a new set of
weights to damage. At p = 100%, there is no randomness;
all weights are set to 0 and all images are labeled as a 1.
Overall, confusion matrices provide a straightforward visu-
alization for misclassiﬁcation within the CNN data set that
could be advantageous for diagnosing cognitive deﬁcits.

In Figure 8, we summarize our TBI experiments for (i)
the MNIST network, (ii) the ImageNet network, and (iii)
the Facial Recognition network. All targeted neurons are
assumed to malfunction the same way, fully blocking the
signal transmission to their neighbors. For each damage
level p (%), we average the accuracy across all random

Figure 7: Confusion matrices as damage increases. We depict the
classiﬁcation results of the handwritten digit classiﬁcation network
for varying amounts of damage. If the images are perfectly classi-
ﬁed, only the diagonal is non-zero. As the damage increases, most
images are mapped to the same few digits. Eventually, all images
are classiﬁed as a “1.”

Table 2: Sigmoid Parameters for Accuracy Drop-oﬀ Curves

Figure 8, MNIST

Figure 8, ImageNet

Figure 8, Face Veriﬁcation

Figure 10, combination

Figure 10, blockage

a

0.94

0.64

0.31

1.2

0.98

b

c

-7.9

-0.38

-13

-30

-2.2

-6.3

-0.12

-0.58

-0.42

-0.25

trials on that network. All three curves have a sigmoidal
shape of the form

(cid:18)

y = a

1 −

1
1 + exp(b(x + c))

(cid:19)

+

1
n

,

where n is the number of classes (see Table 2). Again, there
is no randomness when p = 0% or 100%. At p = 100%,
all weights are set to 0 and all examples are placed in the
same class. Therefore, the accuracy of the network decays
asymptotically to 1/n. Note that the MNIST network and
the ImageNet network have qualitatively similar trends,
and display some accuracy deﬁcit even at low injury levels.
On the other hand, the Facial Recognition network is able
to maintain its original accuracy level past p = 50% before
decaying abruptly.

4.2. Relevance of connections and biological constraints

In all three examples of network dysfunction, there is a
considerable amount of variability across trials even for the
same injury levels. We found that the deﬁcits greatly de-
pend on which weights were randomly selected. In other

7

ing principles in physiology. In fact, nervous systems are
a major drain on an animal’s energy budget and many
aspects of the brain’s anatomy seem to limit wiring costs
[13, 49, 83]. Brain networks can therefore be said to ne-
gotiate an economical trade-oﬀ between minimizing inter-
neuron connection cost & maximizing topological value
and capacity for information processing. See Bullmore and
Sporns [10] for a recent review on the topic.

The MNIST network could be more biophysically plau-
sible if it was not as over-engineered. With its original
topology and settings, the CNN becomes artiﬁcially resis-
tant to damage. In what follows, we will ﬁrst sparsify the
CNN by picking a point on the accuracy-eﬃciency trade
oﬀ curve (see Figure 9). There are multiple ways to choose
the best trade-oﬀ point. A reasonable choice is to remove
the weakest 69.4% of the links, which decreases the accu-
racy from 98.74% to 91.47%.

4.3. Diﬀerent types of FAS dysfunctions

As described in Section 2.2, Focal Axonal Swellings
(FAS) aﬀect spike trains in four qualitatively diﬀerent regimes:
transmission, ﬁltering, reﬂection, and blockage. So far
we have only considered the worst case, blockage, which
we model by setting a weight to zero. Now we also con-
sider the other types of neuronal malfunctions. We model
transmission as not damaging a weight, reﬂection as halv-
ing a weight, and ﬁltering as applying a low-pass ﬁlter
on each weight. We choose an example ﬁltering function
of f (x) = −.2774x2 + .9094x − .0192 plus Gaussian noise
∼ N (0, 0.05) by ﬁtting a confusion matrix from experimen-
tal results [64]. We believe that these additions to CNNs
contain, in a tractable way, the key features of the jeopar-
dizing eﬀects caused by FAS described in [65, 64, 63].

Recent experimental results provide detailed morpho-
logical descriptions of the FAS developing after traumatic
brain injuries. Wang et al [93] damaged the optic nerve of
adult rats with a central ﬂuid percussions injury. The optic
nerve is a relatively organized bundle of axons and allowed
for monitoring of FAS development 12h, 24h and 48h after
the impact. They divided the nerve in 12 serial grids and
reported for each of them the number of axonal swellings
per unit area, the total area of axonal swellings, and the
individual sizes of swellings. It is possible to infer the frac-
tion of FAS in each dysfunctional regime from these sta-
tistical distributions [62]. Based on these results, we con-
duct numerical experiments with 30% blockage, 45% re-
ﬂection, 20% ﬁltering, and 5% transmission. In Figure 10,
we show results for the sparsiﬁed MNIST network, com-
paring its average accuracy for heterogeneous and homoge-
neous FAS distributions. As expected, the worst deﬁcits
occurred when all of the swellings were in the blockage
regime. The best case is when all of the FAS are in the
ﬁltering regime, closely followed by the related reﬂection
case. When we combine these regimes (30% blockage, 45%
reﬂection, 20% ﬁltering, and 5% transmission), the accu-
racy is between these more extreme cases. Although the

Figure 8: Accuracy decay as damage increases. We randomly dam-
age edges of the network by setting their weights to zero. We plot the
percentage of edges that are damaged against the average accuracy
of the network for three problems. We see that damage initially has
little eﬀect, but then there’s a steep drop oﬀ until the accuracy levels
oﬀ around the level of random guessing.

words, neuronal connections in CNNs do not contribute
equally to a task, and damaging weights with large mag-
nitude typically impacts the accuracy more than targeting
weaker links—although magnitude alone cannot explain
all cases.

We illustrate some of these issues in Figure 9 for the
the MNIST network. We repeat the average decay in ac-
curacy in blue but add error bars. We found that roughly
the worst case is to damage the weights in decreasing or-
der of magnitude instead of randomly. The resulting steep
accuracy drop oﬀ is plotted in teal. Conversely, the ap-
proximate best case is to damage the weights in increasing
order of magnitude (plotted in gold). These three dam-
age strategies are visualized in terms of their eﬀect on
the distribution of weights. The purple histogram dis-
plays the distribution of the original weights. In general,
we randomly choose weights to damage, so the eﬀect is
distributed across the distribution of weights. However,
damaging the weights in order of decreasing magnitude is
equivalent to progressively removing the tails of the his-
togram, and choosing the weights in increasing order of
magnitude is equivalent to removing the middle of the
histogram. These experiments may provide intuition into
why the outcomes of TBI are so diﬃcult to predict. We
hypothesize that randomness in the location of FAS could
explain, for instance, why two soldiers near the same explo-
sion site may develop signiﬁcantly diﬀerent post-traumatic
outcomes.

One of the most striking diﬀerences between artiﬁcial
CNNs and biological neuronal networks is that the lat-
ter must operate under geometric, biophysical and energy
constraints. As reviewed in [54], brains have evolved to
operate eﬃciently since economy and proﬁciency are guid-

8

Figure 10: Comparing types of damage. In these experiments, we
begin with a “sparsiﬁed” network with the smallest 69.4% of the
weights removed. Then we compare the types of FAS (blockage,
reﬂection, and ﬁltering) and a combination of all types based on ex-
perimental evidence. As expected, blockage causes the most damage,
and reﬂection is a strong form of ﬁltering.

accuracy decreases more moderately, it can still be ﬁt with
a sigmoid function (see Table 2).

4.4. Aging and Neurodegenerative Diseases

Alzheimer’s Disease (AD) is the most commonly found
type of dementia, which is an umbrella term for a vari-
ety of brain disorders and pathologies [47]. Aging is the
single greatest risk factor for AD [73], and most public
health systems across the developed world are expected
to face huge challenges due to the growing elderly popu-
lation [76]. Recent estimates suggest that more than 5.2
million people have AD in the United States alone and
that a new case occurs every 68 seconds [89]. The most
typical symptom of the disease is an increasing diﬃculty
in recalling new information, although it sometimes oc-
curs in conjunction with challenges in completing familiar
tasks, confusion with time or place, and trouble under-
standing visual images and spatial relationships. W. Thies
and L. Bleiler [89] report that in many cases, AD diagnos-
tics are accompanied by cognitive tests, since individuals
with mild cognitive impairments have changes in think-
ing abilities that are noticeable to family members and
friends. We believe, however, that there is still a large de-
gree of subjectivity when it comes to interpreting cognitive
deﬁcits from dynamically evolving complex systems such
as the human brain. Thus, simulations with convolutional
neural networks that incorporate biophysically plausible
neural malfunctions may provide a window of opportunity
to better diagnose, for instance, confusion in visual image
classiﬁcation.

Figure 9: Range of possible outcomes. The change in accuracy as
weights are damaged varies depending on which weights were ran-
domly chosen. In blue, we plot the average accuracy plus error bars
for each level of damage. We also add curves in teal and yellow for
approximations of best and worst-case accuracies, respectively. The
approximate worst-case was found by damaging the weights in de-
creasing order of their absolute value. Similarly, the approximate
best-case was found by damaging the weights in increasing order.
We give a visualization in terms of a histogram of what it means
to damage the weights in a random order (“average case”), in de-
creasing order (“worst case”), and in increasing order (“best case”).
The yellow “best case” provides an accuracy-eﬃciency trade oﬀ. We
choose a turning point in the curve: if we remove the smallest 69.4%
of the weights, the accuracy only decreases from 98.74% to 91.47%.

9

Both CNNs and brains operate somewhere on an accuracy-

with memory. There are some tools available, including
the Mini-Mental State Examination (MMSE). The MMSE
assigns a score after testing performance on a brief se-
ries of tasks such as identifying objects and following writ-
ten instructions. This score can be used to quantitatively
track changes in a person’s cognitive function. Similarly, in
this paper, we calculate the change in accuracy on related
tasks, such as reading handwritten numbers and labeling
objects. Since we can conduct extensive experiments with
any level of injury, we believe that simulating FAS on our
model of cognition can lead to insight into the complex
processes underlying TBI and neurodegeneration.

Non-invasive diagnostic tools cannot detect anomalies
in vivo such as FAS that occur at the cellular level.
In
fact, this has motivated a large body of in vitro exper-
iments to replicate these injuries in a controlled setting
[12, 25, 32, 33, 34, 61, 69, 82]. However, in the latter
case, the cognitive eﬀects of these injuries cannot be as-
sessed. Simulations provide an opportunity to connect un-
derstanding of FAS to measures of cognitive performance.

eﬃciency trade-oﬀ curve. However, it can be argued that
brains are more highly constrained than CNNs due to the
high energy costs of maintaining the nervous systems [54].
In contrast, many CNNs are trained with a high focus on
small gains in accuracy, especially those trained for com-
petitions such as ImageNet. In addition, all three of the
CNNs studied here utilized dropout, which encourages re-
dundancy in the weights [84]. A key step in our method-
ology was to prune the CNNs to be less over-engineered
and thus more biologically plausible. Remarkably, the net-
works performed very well even if many weak connections
were removed.

Simulations of FAS damage to our CNN model of cogni-
tion result in interpretable and human-like mistakes, such
as confusing a handwritten “5” with a “6” (Figure 3), la-
beling peppers as an apple or a cucumber (Figures 4 and
5) and confusing George W. Bush with his father (Fig-
ure 6). We are able to quantify how accuracy changes as
damage increases (Figures 8, 9, 10, and 11) as well exactly
which kinds of mistakes are being made (Figure 7). We
demonstrate that the eﬀect on accuracy is highly variable
and depends on which connections are randomly selected
(Figure 9), providing intuition for why impairments are
diﬃcult to predict.

As with any model, using CNNs as an abstraction for
the brain comes with limitations. Biological neural net-
works have many complex features and constraints that
are not reﬂected in CNNs, such as transmitting informa-
tion through spike trains and utilizing feedback. One im-
portant diﬀerence between convolutional neural networks
and human subjects is the latter’s ability to infer signif-
icantly more information from the context of an image.
For instance, a patient classifying all objects depicted in
Fig. 5 might, due to some form of meta-analysis, read-
ily interpret them as a collection of many-colored peppers.
Consequently, he could discard extraneous objects from

Figure 11: Accumulating damage over time. In aging or neurode-
generative disease, damage to axons is accumulated over time, in
contrast to a one-time injury. We compare the accuracy curves for a
constant number of connections damaged for each time step to the
case where the number of connections damaged increases with time.
When damage increases over time, the initial loss in accuracy is slow
and the later loss is faster.

Focal axonal swelling pathologies are present in AD [2,
17, 53, 91] and in other neurodegenerative diseases such
as Parkinson’s disease [86, 60, 28], Multiple Sclerosis [26,
70, 90], and others [36, 50, 55, 56]. In many cases, FAS
arise by the agglomeration of speciﬁc proteins over time
[16, 68], and again, the computational modeling of focal
axonal swellings and their eﬀects to spike propagation from
[63] provide a platform to investigate network dysfunction.
In all of the previous experiments, we simulated TBI
by abruptly applying axonal injuries. Here we instead
simulate aging and its neurodegenerative eﬀects by grad-
ually accumulating random damage. We continue to use
the sparsiﬁed network and the heterogeneous FAS distri-
bution. Figure 11 shows that if damage is applied at a
constant rate (targeting 1% of the connections at each
step), the results will look similar to a sequence of TBI
experiments with p = .01, .02, . . . (Figure 10, in dark blue)
except that each trial will have a smoother trajectory.
This is translationally relevant since traumatic brain in-
juries dramatically increase the risk of dementia later in
life [5, 43, 44, 59]. Perhaps a more plausible and biophysi-
cally relevant case occurs when the FAS accumulation rate
increases linearly with time (in cyan). There, the young
brain accumulates very little damage, but the older brain
rapidly acquires new swellings.

5. Conclusions

Assessing levels of cognitive deﬁcits in patients is largely
a subjective task, with indicators such as whether or not
the patient and those close to them have noticed diﬃculties

10

a list of candidates (like ping-pong/tennis balls) even if
their shape and color alone do not provide suﬃcient evi-
dence for such dismissal. We would encourage the usage of
images with non-sensical pairings of objects to circumvent
this diﬃculty in diagnostic tests for cognitive deﬁcits.

An interesting avenue for future work would be retrain-
ing a CNN after damaging it. To be biologically relevant,
damaged weights would need to remain damaged, which is
non-trivial to implement. However, the greater challenge
is choosing an appropriate update rule. A key aspect of
CNNs is taking advantage of some form of back propa-
gation with gradient descent to solve the training opti-
mization problem in a reasonable amount of time. This
optimization problem is not convex, but for practical pur-
poses, we generally do not worry about choosing a local
optimum [14]. However, to retrain a CNN after removing
important weights, we may need to compensate by signif-
icantly changing other weights, escaping a local optimum.
Even if we are already in the correct local optimum, it
could be diﬃcult to choose a step size. Moving in the
direction of the gradient should move towards a local op-
timum unless the step size is too big. On the other hand,
it will not converge quickly if the step size is too small. In
the case of retraining a pre-trained but damaged network,
some weights may be already optimal but others may need
signiﬁcant change, creating a diﬃcult balance problem for
choosing a step size.

In summary, we provide a platform for quantitatively
and qualitatively studying the progression of focal axonal
swellings in a neural network. We can provide insight into
disorders which feature FAS, such as TBI, Alzheimer’s,
Parkinson’s, and Multiple Sclerosis, linking damage at the
cellular level to changes in cognitive behavior.

Acknowledgments

B. Lusch would like to acknowledge fellowship support
from the National Physical Science Consortium and Na-
tional Security Agency.

References

[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghe-
mawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia,
Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man´e,
D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M.,
Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
Vanhoucke, V., Vasudevan, V., Vi´egas, F., Vinyals, O., Warden,
P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X., 2015. Tensor-
Flow: Large-scale machine learning on heterogeneous systems.
Software available from tensorﬂow.org.
URL http://tensorflow.org/

[2] Adalbert, R., Nogradi, A., Babetto, E., Janeckova, L., Walker,
S. A., Kerschensteiner, M., Misgeld, T., Coleman, M. P., 2009.
Severely dystrophic axons at amyloid plaques remain continuous
and connected to viable cell bodies. BRAIN 132, 402–416.
[3] Adams, J. H., Jennett, B., Murray, L. S., Teasdale, G. M.,
Gennarelli, T. A., Graham, D. I., 2011. Neuropathological ﬁnd-
ings in disabled survivors of a head injury. Journal of Neuro-
trauma 28, 701–709.

11

[4] Baldi, P., Hornik, K., 1989. Neural networks and principal com-
ponent analysis: Learning from examples without local minima.
Neural networks 2, 53–58.

[5] Barnes, D. E., Kaup, A., Kirby, K., Byers, A. L., R.Diaz-
Arrastia, Yaﬀe, K., 2014. Traumatic brain injury and risk of
dementia in older veterans. Neurology 83, 312–319.

[6] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., et al.,
2007. Greedy layer-wise training of deep networks. Advances in
neural information processing systems 19, 153.

[7] Bishop, C. M., 1995. Neural networks for pattern recognition.

Oxford university press.

[8] Blumbergs, P., Scott, G., Manavis, J., Wainwright, H., Simp-
son, D., McLean, A., 1995. Topography of axonal injury as
deﬁned by amyloid precursor protein and the sector scoring
method in mild and severe closed head injury. Journal of Neu-
rotrauma 12, 565–572.

[9] Browne, K. D., Chen, X. H., Meaney, D. F., Smith, D. H., 2011.
Mild traumatic brain injury and diﬀuse axonal injury in swine.
Journal of Neurotrauma 28 (9), 1747–1755.

[10] Bullmore, E., Sporns, O., 2012. The economy of brain network
organization. Nature Reviews Neuroscience 13, 336–349.
[11] Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A., 2014.
Return of the devil in the details: Delving deep into convolu-
tional nets. In: British Machine Vision Conference.

[12] Chen, Y. C., Smith, D. H., Meaney, D., 2009. In-vitro ap-
proaches for studying blast-induced traumatic brain injury.
Journal of Neurotrauma 26 (6), 861–876.

[13] Chklovskii, D. B., Koulakov, A. A., 2004. Maps in the brain:
what can we learn from them? Annual Reviews in Neuroscience
27, 369–392.

[14] Choromanska, A., Henaﬀ, M., Mathieu, M., Arous, G. B., Le-
Cun, Y., 2015. The loss surfaces of multilayer networks. In:
AISTATS.

[15] Christman, C., Grady, M., Walker, S., Hol-Loway, K.,
Povlishock, J., 1994. Ultra-structural studies of diﬀuse axonal
injury in humans. Journal of Neurotrauma 11, 173–186.

[16] Coleman, M., 2005. Axon degeneration mechanisms: common-
ality amid diversity. Nature Reviews Neuroscience 6 (11), 889–
898.

[17] Daianu, M., Jacobs, R. E., Town, T., Thompson, P. M., 2016.
Axonal diameter and density estimated with 7-tesla hybrid dif-
fusion imaging in transgenic alzheimer rats. SPIE Proceedings
9784, 1–6.

[18] del Razo, M. J., Morofuji, Y., Meabon, J. S., Huber, B. R.,
Peskind, E. R., Banks, W. A., Mourad, P. D., LeVeque, R. J.,
Cook, D. G., 2016. Computational and in vitro studies of blast-
induced blood-brain barrier disruption. SIAM Journal on Sci-
entiﬁc Computing 38 (3), 347–374.

[19] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.,
2009. Imagenet: A large-scale hierarchical image database. In:
Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on. IEEE, pp. 248–255.

[20] Dikranian, K., Cohen, R., Donald, C. M., Pan, Y., Brakeﬁeld,
D., Bayly, P., Parsadanian, A., 2008. Mild traumatic brain in-
jury to the infant mouse causes robust white matter axonal de-
generation which precedes apoptotic death of cortical and tha-
lamic neurons. Experimental Neurology 211, 551–560.

[21] Edlow, B. L., Copen, W. A., Izzy, S., van der Kouwe, A., Glenn,
M. B., Greenberg, S. M., Greer, D. M., Wu, O., 2016. Longi-
tudinal diﬀusion tensor imaging detects recovery of fractional
anisotropy within traumatic axonal injury lesions. Neurocriti-
cal Care 24 (3), 342–352.

[22] Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent,
P., Bengio, S., 2010. Why does unsupervised pre-training help
deep learning? Journal of Machine Learning Research 11 (Feb),
625–660.

[23] Fainaru-Wada, M., Fainaru, S., 2013. League of denial: The nﬂ,
concussions, and the battle for truth. Crown Archetype.
[24] Faul, M., Xu, L., Wald, M. M., Coronado, V. G., 2010. Trau-
matic brain injury in the united states: emergency department
visits, hospitalizations, and deaths. Atlanta (GA): Centers for

Disease Control and Prevention, National Center for Injury Pre-
vention and Control.

[25] Fayanz, I., Tator, C. H., 2000. Modeling axonal injury in vitro:
injury and regeneration following acute neuritic trauma. Journal
of Neuroscience Methods 102, 69–79.

[26] Friese, M. A., Schattling, B., Fugger, L., 2014. Mechanisms of
neurodegeneration and axonal dysfunction in multiple sclerosis.
Nature Reviews Neurology 10, 225–238.

[27] Fukushima, F., 1980. A self-organizing neural network model
for a mechanism of pattern recognition unaﬀected by shift in
position. Biological Cybernetic 36, 193–202.

[28] Galvin, J. E., Uryu, K., Lee, V. M., Trojanowski, J. Q., 1999.
Axon pathology in parkinson’s disease and lewy body dementia
hippocampus contains α-, β-, and γ -synuclein. Proceedings of
National Academy of Science 96, 13450–13455.

[29] Grady, M., Mclaughlin, M., Christman, C., Valadaka, A.,
Flinger, C., Povlishock, J., 1993. The use of antibodies against
neuroﬁlament subunits for the detection of diﬀuse axonal in-
jury in humans. Journal of Neuropathology and Experimental
Neurology 52, 143–152.

[30] Hanell, A., Greer, J. E., McGinn, M. J., Povlishock, J. T., 2015.
Traumatic brain injury induced axonal phenotypes react diﬀer-
ently to treatment. Acta Neuropathologica 129, 317–332.
[31] He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep residual learning

for image recognition. arXiv preprint arXiv:1512.03385.

[32] Hellman, A. N., Vahidi, B., Kim, H. J., Mismar, W., Stew-
ard, O., Jeonde, N. L., Venugopalan, V., 2010. Examination of
axonal injury and regeneration in micropatterned neuronal cul-
ture using pulsed laser microbeam dissection. Lab on a Chip 16,
20832092.

[33] Hemphill, M., Dabiri, B., Gabriele, S., Kerscher, L., Franck, C.,
Goss, J., Alford, P., Parker, K., 2011. A possible role for integrin
signaling in diﬀuse axonal injury. PLos ONE 6 (7), e22899.
[34] Hemphill, M., Dauth, S., Yu, C. J., Dabiri, B., Parker, K., 2015.
Traumatic brain injury and the neuronal microenvironment: A
potential role for neuropathological mechanotransduction. Neu-
ron 86 (6), 1177–1192.

[35] Henninger, N., Bouley, J., Sikoglu, E. M., An, J., Moore, C. M.,
King, J. A., Bowser, R., Freeman, M. R., Jr, R. H. B., 2016.
Attenuated traumatic axonal injury and improved functional
outcome after traumatic brain injury in mice lacking sarm1.
BRAIN, 1–12.

[36] Herwerth, M., Kalluri, S. R., Srivastava, R., Kleele, T., Kenet,
S., Illes, Z., Merkler, D., Bennett, J. L., Misgeld, T., Hemmer,
B., 2016. In vivo imaging reveals rapid astrocyte depletion and
axon damage in a model of neuromyelitis optica-related pathol-
ogy. Annals of Neurology 79, 794–805.

[37] Hill, C. S., Coleman, M. P., Menon, D. K., 2016. Traumatic
injury: mechanisms and translational opportunities.

axonal
Trends in Neuroscience 39 (5), 311–324.

[38] Hinton, G. E., Osindero, S., Teh, Y.-W., 2006. A fast learning
algorithm for deep belief nets. Neural computation 18 (7), 1527–
1554.

[39] Hornik, K., Stinchcombe, M., White, H., 1989. Multilayer feed-
forward networks are universal approximators. Neural networks
2 (5), 359–366.

[40] Huang, G. B., Ramesh, M., Berg, T., Learned-Miller, E., 2007.
Labeled faces in the wild: A database for studying face recog-
nition in unconstrained environments. Tech. rep., Technical Re-
port 07-49, University of Massachusetts, Amherst.

[41] Hubel, D. H., Wiesel, T. N., 1962. Receptive ﬁelds, binocular
interaction and functional architecture in the cat’s visual cortex.
Journal of Physiology 160, 106–154.

[42] Ioﬀe, S., Szegedy, C., 2015. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. arXiv
preprint arXiv:1502.03167.

[43] Johnson, V. E., Stewart, W., Smith, D. H., 2010. Traumatic
brain injury and amyloid-β pathology: a link to alzheimer’s
disease? Nature Reviews Neuroscience 11, 361–370.

[44] Johnson, V. E., Stewart, W., Smith, D. H., 2012. Widespread
tau and amyloid-beta pathology many years after a single trau-

matic brain injury in humans. Brain Pathology 22, 142–149.

[45] Johnson, V. E., Stewart, W., Smith, D. H., 2013. Axonal pathol-
ogy in traumatic brain injury. Experimental Neurology 246, 35–
43.

[46] Jorge, R. E., Acion, L., White, T., Tordesillas-Gutierrez, D.,
Pierson, R., Crespo-Facorro, B., Magnotta, V., 2012. White
matter abnormalities in veterans with mild traumatic brain in-
jury. American Journal of Psychiatry 169 (12), 1284–1291.
[47] Jorm, A. F., Jolley, D., 1998. The incidence of dementia: a meta

analysis. Neurology 51, 728–733.

[48] Jurevic, S., 2011. Green pepper.

[Online; accessed June 1,

2016].
URL
5572058462

https://www.flickr.com/photos/stuffedpeppers/

[49] Kaiser, M., Hilgetag, C. C., 2006. Nonoptimal component place-
ment, but short processing paths, due to long- distance projec-
tions in neural systems. PLoS Computational Biology 2 (e95).
[50] Karlsson, P., Haroutounian, S., Polydefkis, M., Nyengaard,
J. R., Jensen, T. S., 2016. Structural and functional character-
ization of nerve ﬁbres in polyneuropathy and healthy subjects.
Scandinavian Journal of Pain 10, 28–35.

[51] Kinnunen, K. M., Greenwood, R., Powell, J. H., Leech, R.,
Hawkins, P. C., Bonnelle, V., Patel, M. C., Counsell, S. J.,
Sharp, D. J., 2010. White matter damage and cognitive impair-
ment after traumatic brain injury. Brain, 1–15.

[52] Krizhevsky, A., Sutskever, I., Hinton, G. E., 2012. Imagenet
classiﬁcation with deep convolutional neural networks. In: Ad-
vances in neural information processing systems. pp. 1097–1105.
[53] Krstic, D., Knuesel, I., 2012. Deciphering the mechanism under-
lying late-onset alzheimer disease. Nature Reviews Neuroscience
9 (1), 25–34.

[54] Laughlin, S. B., Sejnowski, T., 2003. Communication in neu-

ronal networks. Science 301, 1870–1874.

[55] Laukka, J. J., Kamholz, J., Bessert, D., 2016. Novel pathologic
ﬁndings in patients with pelizaeus-merzbacher disease. Neuro-
science Letters.

[56] Lauria, G., Morbin, M., Lombardi, R., Borgna, M., Mazzoleni,
G., Sghirlanzoni, A., Pareyson, D., 2003. Axonal swellings pre-
dict the degeneration of epidermal nerve ﬁbers in painful neu-
ropathies. Neurology 61, 631–636.

[57] LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature

[58] LeCun, Y., Cortes, C., Burges, C. J., 1998. The mnist database

521 (7553), 436–444.

of handwritten digits.

[59] LoBue, C., Denney, D., Hynan, L. S., Rossetti, H. C., Lacritz,
L. H., Jr., J. H., Womack, K. B., Woon, F. L., Cullum, C. M.,
2016. Self-reported traumatic brain injury and mild cognitive
impairment: increased risk and earlier age of diagnosis. Journal
of Alzheimer’s Disease 51, 727–736.

[60] Louis, E. D., Faust, P. L., Vonsattel, J., Honig, L. S., Rajput,
A., Rajput, A., Pahwa, R., Lyons, K. E., Ross, G. W., Elble,
R. J., Erickson-Davis, C., Moskowitz, C. B., Lawton, A., 2009.
Torpedoes in parkinson’s disease, alzheimer’s disease, essential
tremor, and control brains. Movement Disorders 24 (11), 1600–
1605.

[61] Magdesian, M. H., Sanchez, F., Lopez, M., Thostrup, P.,
Durisic, N., Belkaid, W., Liazoghli, D., Gr¨utter, P., Colman,
R., 2012. Atomic force microscopy reveals important diﬀerences
in axonal resistance to injury. Biophysical Journal 103 (3), 405–
414.

[62] Maia, P. D., 2014. Mathematical modeling of focal axonal
swellings arising in traumatic brain injuries and neurodegen-
erative diseases. Ph.D. thesis, University of Washington.
[63] Maia, P. D., Hemphill, M. A., Zehnder, B., Zhang, C., Parker,
K. K., Kutz, J. N., 2015. Diagnostic tools for evaluating the
impact of focal axonal swellings arising in neurodegenerative
diseases and/or traumatic brain injury. Journal of Neuroscience
Methods 253, 233–243.

[64] Maia, P. D., Kutz, J. N., 2014. Compromised axonal functional-
ity after neurodegeneration, concussion and/or traumatic brain
injury. Journal of Computational Neuroscience 27, 317–332.

12

[85] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov,
D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going
deeper with convolutions. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition. pp. 1–9.

[86] Tagliaferro, P., Burke, R. E., 2016. Retrograde axonal degen-
eration in parkinson disease. Journal of Parkinson’s Disease 6,
1–15.

[87] Tang-Schomer, M. D., Johnson, V. E., Baas, P. W., Stewart,
W., Smith, D. H., 2012. Partial interruption of axonal trans-
port due to microtubule breakage accounts for the formation of
periodic varicosities after traumatic axonal injury. Experimental
Neurology 233, 364–372.

[88] Tang-Schomer, M. D., Patel, A., Bass, P. W., Smith, D. H.,
2010. Mechanical breaking of microtubules in axons during dy-
namic stretch injury underlies delayed elasticity, microtubule
disassembly, and axon degeneration. The FASEB Journal 24 (5),
1401–1410.

[89] Thies, W., Bleiler, L., 2013. Alzheimer’s disease facts and ﬁg-

ures. Alzheimer’s & Dementia 9 (2), 208–245.

[90] Trapp, B. D., Nave, K.-A., 2008. Multiple sclerosis: An immune
or neurodegenerative disorder? Annual Review Neuroscience
31 (1), 247–269.

[91] Tsai, J., Grutzendler, J., Duﬀ, K., Gan, W. B., 2004. Fibrillar
amyloid deposition leads to local synaptic abnormalities and
breakage of neuronal branches. Nature Neuroscience 7, 1181–
1183.

[92] Vedaldi, A., Lenc, K., 2015. Matconvnet – convolutional neural
networks for matlab. In: Proceeding of the ACM Int. Conf. on
Multimedia.

[93] Wang, J., Hamm, R. J., Povlishock, J. T., 2011. Traumatic
axonal injury in the optic nerve: evidence for axonal swelling,
disconnection, dieback and reorganization. Journal of Neuro-
trauma, 28 (7), 1185–1198.

[65] Maia, P. D., Kutz, J. N., 2014. Identifying critical regions for
spike propagation in axon segments. Journal of Computational
Neuroscience 36 (2), 141–155.

[66] Maxwell, W. L., Povlishock, J. T., Graham, D. L., 1997. A
mechanistic analysis of nondisruptive axonal injury: A review.
Journal of Neurotrauma 17 (7), 419–440.

[67] Menon, D. K., Maas, A. I. R., 2015. Progress, failures and new
approaches for tbi research. Nature Reviews Neuroloy 11, 71–72.
[68] Millecamps, S., Julien, J., 2013. Axonal transport deﬁcits
and neurodegenerative diseases. Nature Reviews Neuroscience
14 (161), 161–176.

[69] Morrison, B., Elkin, B. S., Dolle, J. P., Yarmush, M. L., 2011.
In vitro models of traumatic brain injury. Annual Reviews in
Biomedical Engineering 13 (1), 91–126.

[70] Nikic, I., Merkler, D., Sorbara, C., Brinkoetter, M., Kreutzfeld,
M., Bareyre, F., Bruck, W., Bishop, D., Misgeld, T., Kerschen-
steiner, M., 2011. A reversible form of axon damage in exper-
imental autoimmune encephalomyelitis and multiple sclerosis.
Nature Medicine 17 (4), 495–499.

[71] Niyogi, P., Girosi, F., Poggio, T., 1998. Incorporating prior
information in machine learning by creating virtual examples.
Proceedings of the IEEE 86 (11), 2196–2209.

[72] Parkhi, O. M., Vedaldi, A., Zisserman, A., September 2015.
Deep face recognition. In: Xianghua Xie, M. W. J., Tam, G.
K. L. (Eds.), Proceedings of the British Machine Vision Con-
ference (BMVC). BMVA Press, pp. 41.1–41.12.
URL https://dx.doi.org/10.5244/C.29.41

[73] Patterson, B. W., Elbert, D. L., Mawuenyega, K. G., Kasten,
T., Ovod, V., Ma, S., Xiong, C., Chott, R., Yarasheski, K.,
Sigurdson, W., Zhang, L., Goate, A., Benzinger, T., Morris,
J. C., Holtzman, D., Bateman, R. J., 2015. Age and amyloid
eﬀects on human central nervous system amyloid-beta kinetics.
American Neurological Association 78 (3), 439–453.

[74] Poggio, T., 2016. Deep learning: mathematics and neuroscience.
Views & Reviews, McGovern Center for Brains, Minds and Ma-
chines, 1–7.

[75] Povlishock, J. T., Katz, D. I., 2005. Update of neuropathology
and neurological recovery after traumatic brain injury. Journal
of Head Trauma Rehabilitation 20 (1), 76–94.

[76] Qiu, C., Kivipelto, M., von Strauss, E., 2009. Epidemiology of
alzheimer’s disease: occurrence, determinants, and strategies
toward intervention. Dialogues in Clinical Neuroscience 11 (2),
111–128.

[77] Roozenbeek, B., Maas, A. I. R., Menon, D. K., 2013. Changing
patterns in the epidemiology of traumatic brain injury. Nature
Reviews Neurology 9, 231–236.

[78] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M.,
Berg, A. C., Fei-Fei, L., 2015. ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer Vi-
sion (IJCV) 115 (3), 211–252.

[79] Sanger, T. D., 1989. Optimal unsupervised learning in a single-
layer linear feedforward neural network. Neural networks 2 (6),
459–473.

[80] Sharp, D. J., Scott, G., Leech, R., 2014. Network dysfunction
after traumatic brain injury. Nature Reviews Neurology 10, 156–
166.

[81] Skandsen, T., Kvistad, K. A., Solheim, O., Strand, I. H., Folvik,
M., Vik, A., 2010. Prevalence and impact of diﬀuse axonal in-
jury in patients with moderate and severe head injury: a cohort
study of early magnetic resonance imaging ﬁndings and 1-year
outcome. Journal of Neurosurgery 113 (3), 556–563.

[82] Smith, D., Wolf, J., Lusardi, T., Lee, V., Meaney, D., 1999.
High tolerance and delayed elastic response of cultured axons
to dynamic stretch injury. The Journal of Neuroscience 19 (11),
4263–4269.

[83] Sporn, O., 2011. Networks of the brain. MIT Press.
[84] Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I.,
Salakhutdinov, R., 2014. Dropout: a simple way to prevent
neural networks from overﬁtting. Journal of Machine Learning
Research 15 (1), 1929–1958.

13

Modeling cognitive deﬁcits following neurodegenerative diseases and traumatic brain
injuries with deep convolutional neural networks

Bethany Lusch∗, Jake Weholt, Pedro D. Maia, J. Nathan Kutz

Department of Applied Mathematics, University of Washington, United States

6
1
0
2
 
c
e
D
 
3
1
 
 
]

C
N
.
o
i
b
-
q
[
 
 
1
v
3
2
4
4
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

The accurate diagnosis and assessment of neurodegenerative disease and traumatic brain injuries (TBI) remain open
challenges to doctors and medical practitioners. Both cause cognitive and functional deﬁcits due to the diﬀuse presence
of focal axonal swellings (FAS), but it is diﬃcult to deliver a prognosis due to our limited ability to assess damaged
neurons at a cellular level in vivo. In this paper, we simulate the eﬀects of neurodegenerative disease and TBI using
convolutional neural networks (CNNs) as our model of cognition. CNNs were originally inspired by neuroscience and the
hierarchical layers of neurons used for processing input stimulus to the brain. We start with CNNs pre-trained to perform
classiﬁcation, then utilize biophysically relevant statistical data on FAS to damage the connections in a functionally
relevant way. In order to improve the model, we incorporate the idea that brains operate under energy constraints by
pruning the CNNs to be less over-engineered. Qualitatively, we demonstrate that damage to the connections leads to
human-like mistakes. Our experiments also provide quantitative assessments of how accuracy is aﬀected by various types
and levels of damage. The deﬁcit resulting from a ﬁxed amount of damage greatly depends on which connections are
randomly injured, providing intuition for why it is diﬃcult to predict the impairments resulting from an injury. There is
a large degree of subjectivity when it comes to interpreting cognitive deﬁcits from dynamically evolving complex systems
such as the human brain. However, we provide important insight and a quantitative framework for several disorders in
which FAS are implicated, such as TBI, Alzheimer’s, Parkinson’s, and Multiple Sclerosis.

Keywords: Traumatic brain injury, neurodegenerative disease, focal axonal swellings, convolutional neural networks

1. Introduction

The mathematical architecture of convolutional neu-
ral networks (CNNs) was originally inspired by the No-
bel prize-winning work of Hubel and Wiesel on the pri-
mary visual cortex of cats [41]. Their seminal experiments
were the ﬁrst to suggest that neurons in the visual sys-
tem organize themselves in hierarchical layers of cells for
processing visual stimulus. The ﬁrst quantitative model
of the CNN, termed the Neocognitron by Fukushima in
1980 [27], already displayed many of the characteristic fea-
tures of today’s deep CNNs, including a multi-layer struc-
ture, convolution, max pooling and nonlinear dynamical
nodes. The connection between neuroscience and CNN
theory, although clearly a conceptional abstraction [74],
has since been instrumental to improving quantitative mod-
els of how the brain integrates neuro-sensory information
for stimulus classiﬁcation and decision making. Given that
CNNs mimic many of the important cognitive features of
the brain, we use it as a model for understanding how neu-
rodegenerative diseases and traumatic brain injuries (TBI)

∗Corresponding author at: University of Washington, Lewis Hall

#202, Box 353925, Seattle, WA 98195-3925 USA

Email address: herwaldt@uw.edu (Bethany Lusch)

can compromise an array of recognition tasks. Specif-
ically, by using well-established biophysical data on the
statistics (distribution and size) of focal axonal swellings
(FAS), which area among the primary symptoms of neu-
rodegeneration and TBI, we evaluate the progress of im-
pairments on a CNN-based model of cognition. Our model
provides quantitative metrics for understanding how cog-
nitive deﬁcits are accumulated as a function of FAS devel-
opment, allowing for potentially new diagnostics for the
evaluation of brain disorders due to neurodegenerative dis-
eases and/or TBI.

Understanding how neurodegenerative diseases and TBI
aﬀect cognitive function remains a critically important chal-
lenge for societal mental health. TBI alone is one of the
major causes of disability and mortality worldwide, which
in turn, dramatically jeopardizes society in several socioe-
conomic ways [67]. Not only is it the signature injury of
the wars in Afghanistan and Iraq [46], it is also the leading
cause of death among young people [24]. While many sur-
vive the events that induce TBI, persistent cognitive, psy-
chiatric, and physiological dysfunction often follows from
the mechanical impact (see Sec. 2). Likewise, neurode-
generative diseases are responsible for an overwhelming
variety of functional deﬁcits, with common symptoms in-
cluding memory loss or behavioral/cognitive impairments

December 15, 2016

Figure 1: Damaging a Convolutional Neural Network (CNN). (a) We start with a “healthy” CNN that accepts an image of a handwritten
digit as an input and outputs scores for each possible digit, 0-9. We classify the image as the digit with the highest score. (b) We then
damage the weights on the network in a biophysically-relevant way. In this ﬁgure, the healthy network correctly classiﬁes the image as a 2,
but the damaged network classiﬁes it as a 1.

which are related to an inability to correctly process multi-
modal information for decision-making tasks. The major-
ity of brain disorders have a complex cascade of patholog-
ical eﬀects spanning multiple spatial scales: from cellular
or network levels to tissues or entire brain areas. Unfor-
tunately, our limited ability to diagnose cerebral malfunc-
tions in vivo cannot detect several anomalies that occur on
smaller scales. FAS, however, are ubiquitous to TBI and
most leading and incurable disorders that dramatically af-
fect signaling properties of neurons, such as Multiple Scle-
rosis, Alzheimer’s and Parkinson’s diseases.

Given the currently available wealth of data on FAS
morphology from TBI studies and from almost every lead-
ing neurodegenerative disease, signiﬁcant progress can be
made towards understanding qualitatively how FAS im-
pacts cognitive function. In this work, we consider a set of
deep CNN models as an abstraction for functioning brains.
Our goal is to understand how the processing of input data
(classiﬁcation) is compromised as a function of increasing
injury and/or disease progression. Of course, it is obvi-
ous that the system’s performance will be compromised as
the CNN is injured, but the manner in which the cogni-
tive impairments arise is quite illustrative and informative,
providing intuitively appealing results about how cognitive
deﬁcits can develop and evolve as a neurodegenerative dis-
ease progresses.

Figure 1 illustrates our approach. We begin with the
original (healthy) CNN, which is trained to perform a clas-
siﬁcation task. In Figure 1, the speciﬁc task is to label a
handwritten digit. We then expose the CNN to diﬀerent

injury protocols based upon biophysical observations of
FAS statistics and morphological parameters. In particu-
lar, we use statistical distributions of FAS from a recent
experiment consisting of TBI-induced damage in the vi-
sual cortex of rats [93]. To impose these injury statistics
on the original CNN, we assume that each neuronal con-
nection has a biophysically plausible probability to mal-
function; while mild axonal injury may simply weaken a
connection, severe cases may break it permanently (i.e., an
axomtotomy occurs so that the connection strength goes
to zero). Ultimately, the severity of the injury and re-
weighting of connections is also determined by biophysical
data and the statistical distribution of the size of the FAS.
We can then progressively monitor the deleterious eﬀects
of the injury on the functionality of the CNN, providing
metrics for cognitive deﬁcits that arise.

The paper is outlined as follows: In Sec. 2 we provide
key background material on the two primary ﬁelds inte-
grated into this work: convolutional neural networks and
neural disorders in which FAS are implicated. We describe
our methodology in Sec. 3 and present results in Sec. 4.
We summarize our conclusions in Sec. 5. For full details,
all MATLAB and Python codes used for this paper are
available online at github.com/BethanyL/damaged_cnns.

2. Background

2.1. Convolutional Neural Networks

Deep convolutional neural networks (DCNNs) are trans-
forming almost every ﬁeld of science involving big data.

2

The success of the method has been enabled by two criti-
cal components: (i) the continued growth of computational
power (e.g. GPU and networked computing), and (ii) ex-
ceptionally large labeled data sets capable of taking advan-
tage of the full power of a multi-layer architecture. Indeed,
although the theoretical inception of CNNs has an almost
four-decade history, the analysis [52] of the ImageNet data
set [19] in 2012 provided a watershed moment for CNNs
and Deep Learning [57]. Prior to this data set, there were
a number of data sets available with approximately tens of
thousands of labeled images. ImageNet provided over 15
million labeled, high-resolution images with over 22,000
categories. DCNNs have since transformed the ﬁeld of
computer vision by dominating the performance metrics
in almost every meaningful computer vision task intended
for classiﬁcation and identiﬁcation (see, for example, the
International Conference on Computer Vision 2015).

ImageNet has been a critically enabling data set for
the evolution of the ﬁeld. However, CNNs were a topic of
intensive research long before. Indeed, they were highly
successful in a wide range of applications and machine
learning architectures. By the early 1990s, neural net-
works were studied as standard textbook material [7], with
the focus typically on a small number of layers. Critical
machine learning tasks such as principal component anal-
ysis (PCA) were shown to be intimately connected with
networks which included back propagation [4, 79]. Impor-
tantly, there were a number of critical innovations which
established multilayer feedforward networks as a class of
universal approximators. Speciﬁcally, Hornik et al. [39]
rigorously established that standard multilayer feedforward
networks with as few as one hidden layer using arbitrary
squashing functions were capable of approximating any
Borel measurable function from one ﬁnite dimensional space
to another to any desired degree of accuracy, provided suf-
ﬁciently many hidden units were available. Thus, multi-
layer feedforward networks could be thought of as a class
of universal approximators [39].

The past ﬁve years have seen tremendous advances in
the DCNN architecture. Innovations have come from al-
gorithmic tricks and modiﬁcations that have led to sig-
niﬁcant performance gains in a variety of ﬁelds. These
innovations include pretraining [38, 6, 22], dropout [84],
max pooling [52], inception modules [85], data augmen-
tation (virtual examples) [71], batch normalization [42]
and/or residual learning [31]. This is only a partial list
of potential algorithmic innovations available for improv-
ing the performance of classiﬁcation and labeling. Our
goal is not to provide a complete review of the DCNN
ﬁeld, but rather to highlight the continuing and rapid pace
Integrating the state-of-the-art
of progress in the ﬁeld.
in DCNNs is the open source software called TensorFlow
(tensorﬂow.org). TensorFlow was originally developed by
researchers and engineers working on the Google Brain
Team within Google’s Machine Intelligence research orga-
nization. The system is designed to facilitate research in
machine learning and to make it quick and easy to transi-

Figure 2: Four Types of Damaged Axons. A spike train passes
through a swollen axon. Depending on the way that the axon is
swollen, there are four ways that the information can be transmit-
ted. In transmission, the spike train is propagated correctly despite
the damage.
In ﬁltering, the spike train goes through a low-pass
ﬁlter. Regions of the spike train with high frequency are especially
likely to lose spikes. In reﬂection, pairs of spikes combine and only
half of the spikes are transmitted. In blockage, none of the spikes
are transmitted.

tion from research prototype to production system. Ten-
sorFlow has allowed for the test-bedding of new algorith-
mic structures in a reproducible and veriﬁable manner,
which is a signiﬁcant and important advancement in the
ﬁeld. Indeed, the DCNN architecture used here relies on
the TensorFlow architecture, helping us understand how
state-of-the-art DCNNs relate to cognitive abilities.

2.2. Focal Axonal Swellings

Concussions and Traumatic Brain Injuries (TBI) are more
than ever a concern for contact sport practitioners [23], for
veteran soldiers exposed to blast injuries [18, 46], and for
society as a whole [24, 67, 77]. In fact, TBI contributes
to one-third of all injury-related deaths and is one of the
major sources of functional impairments. TBI pathologies
aﬀect several spatial scales [80], but a ubiquitous develop-
ment at the neuronal microenvironment level is the pres-
ence of axonal injury [37, 45, 81]. As reviewed in [37], rapid
axonal stretch injury triggers secondary axonal changes
that can vary in extent and severity [21, 30, 35], but most
often culminate in Focal Axonal Swellings (FAS).

FAS are monitored whenever possible in in-vitro stud-
ies [12, 25, 32, 33, 34, 61, 69, 82], in in-vivo experiments

3

[9, 20, 66, 93], and in human patients [3, 8, 15, 29, 46, 51,
75]. In many cases, FAS critically aﬀect the axonal mor-
phology [87, 88] and consequently, the information content
encoded in spike trains propagating throughout them.

Recent computational studies distinguished geometri-
cal axonal enlargements that lead to minor changes in
propagation from those that result in critical phenomena
such as reﬂection or blockage of the original traveling pulse
[65], or ﬁltering of action potentials [64]. This led to a di-
agnostic toolbox that extracts meaningful geometrical pa-
rameters from sequential images of injured axon segments
[63]. These algorithms provide a principled approach to
deal with imaging distortions caused by experimental ar-
tifacts in order to extract the cross-section of an axon by
detecting local symmetries, turning points and turning re-
gions. More importantly, they provide the ﬁrst description
of biologically plausible injurious eﬀects due to FAS that
can be incorporated into neuronal network simulations.
Figure 2 reviews these diﬀerent eﬀects. In the transmis-
sion regime, the spike train propagates through the FAS
without signiﬁcant modiﬁcations. In the ﬁltering regime,
pulses that are too close to each other get deleted by a
mechanism resembling a pile-up collision [64]. As the FAS
geometrical parameters worsen, a single spike will split into
two components, one propagating forward and the other
propagating backward. The reﬂected, back-propagating
pulse will collide with the next spike in the train and they
will mutually annihilate each other. Thus, the reﬂection
regime eﬀectively halves the ﬁring rate of the neuron. Fi-
nally, in the worst-case scenario, the FAS will block all
spikes and transmit no information whatsoever.

In what follows, we will introduce FAS in a biologically
plausible way to a few examples of deep-learning convolu-
tional neural networks and evaluate the extent to which
cognitive deﬁcits develop.

3. Materials and Methods

3.1. CNN training, calibration, and performance

We simulate the development of FAS damage in three
diﬀerent convolutional neural networks. Each network has
its own properties and was trained with diﬀerent data sets
for separate tasks (see Table 1).

First, we consider a network trained on the MNIST
dataset [58], which is composed of images of handwritten
digits. We train the network to classify each image as a
digit from 0 to 9.
It could be used, for example, by a
post oﬃce machine to read zip codes from envelopes. The
training data set consists of a series of black and white
images that are 28 by 28 pixels. We use the TensorFlow
framework [1] to train a CNN with two convolutional layers
and a fully connected layer, as advised by a TensorFlow
tutorial. We use a subset of the standard MNIST test
set for our testing purposes so that our set contains the
In particular,
same number of examples for each digit.
we choose the ﬁrst 852 images for each digit. Our trained
network has an accuracy of 98.74% on this test set.

Next, we use a network trained on the ImageNet data
set to classify images from the ILSVRC 2012 challenge as
one of one thousand objects [78]. The CNN-F network
was pre-trained by the Visual Geometry Group at Oxford
[11] and made available through the MatConvNet Matlab
Toolbox [92], where it is referred to as imagenet-vgg-f. The
network contains ﬁve convolutional layers and three fully
connected layers. For our experiments, we use a subset of
the data with two examples randomly chosen from each
class. The network is 54.6% accurate on this test set.

The third network that we use was trained to classify
faces as one of one thousand people. However, if you re-
move the last classiﬁcation layer and normalize the output
vector, the network can instead be used to create feature
vectors for face veriﬁcation. If the Euclidean norm of the
diﬀerence between the feature vectors for two images is un-
der a threshold τ , the pair of images is classiﬁed as being
the same person. This network was also trained by the Vi-
sual Geometry Group [72] and made available through the
MatConvNet toolbox [92], where it is called vgg-face. For
our experiments, we randomly choose ﬁve pictures each of
ﬁfty randomly chosen celebrities from the Labeled Faces
in the Wild (LFW) data set [40]. We also need to choose a
threshold τ . We choose τ = 1.2 based on Linear Discrimi-
nant Analysis on a training set of 5700 examples from the
LFW data set. Each of the 250 images in our test set is
then compared to the four other images of the same per-
son and four images of other people. We thus test one
thousand pairs of images, half of which are of the same
person and half of which are not. The network is 81.6%
accurate with τ = 1.2 on our test set of one thousand pairs
of images.

3.2. Network impairments following FAS injuries

To simulate the eﬀects of traumatic brain injury on a
CNN, we randomly “damage” p percent of the weights in
the convolutional and fully connected layers. For consis-
tency with the TBI analogy, we only target the connections
between neurons and not bias weights. Note that these
CNNs are designed to use the same weights for multiple
connections. Thus, damaging p percent of the weights is
not equivalent to damaging p percent of the connections.
For simplicity, we ﬁrst assume that all axonal injuries lead
to the total blockage of spikes, which eﬀectively sets p per-
cent of weights to zero. We consider damage examples for
each one of the previously described networks to develop
intuition about possible functional impairments.

In Figure 3, we choose a handwritten “2” as the in-
put to the MNIST network. The network assigns a score
to each of the ten possible digits and then classiﬁes the
image as the digit with the highest score. The original
network gives scores of .999987 to 2, .000013 to 1, and 0
to the rest of the digits and thus correctly classiﬁes the im-
age as a 2. We then randomly damage the network in 100
separate experiments, setting p = .01, .02, . . . , 1. Since we
are simulating TBI, the damage happens all at once and
is not accumulated across experiments. Thus, the set of

4

Table 1: Summary of CNNs Used

Task

Training Set

# conv. # fully connected # weights

layers

layers

to damage

handwritten digit classiﬁcation

MNIST: 55K

object classiﬁcation

ImageNet, ILSVRC

2012: 1.2M

2

5

face veriﬁcation

VGG-Face: 2.6M

13

1

3

2

83K

61M

134M

mistakes seem less understandable, such as “hair slide.”
Note that this network was trained on about 1.2 million
images of the one thousand classes, encompassing a wide
range of examples for each class. For illustration purposes
in this ﬁgure, we show an example image from the test set
for each class. However, the input image is downloaded
from Flickr [48].

In Figure 5, we give a more diﬃcult input image to
the ImageNet network—a group of vegetables composed
predominately of peppers with a variety of colors but also
containing garlic. This image accompanies the Image Pro-
cessing Toolbox for Matlab as peppers.png and is used as
a demo for this network in the MatConvNet Toolbox. The
network successfully chooses the bell pepper class among
one thousand possibilities, but it is not as robust to injury
as the one in the previous, easier example. Misclassiﬁca-
tions begin at 4% injury. Again, some errors are reason-
able, such as a “cucumber” or “orange” (which is not that
diﬀerent from an orange-colored pepper). Others are quite
surprising, such as “socks” or “teddy bear”.

In Figure 6, we show analogous deﬁcits for the facial
recognition network (VGG-Face). We input an image of
George W. Bush to the network and have it compare the
image with three other pictures—one of his father George
H. W. Bush, one of Bill Clinton, and another one of him-
self. All images come from the LFW data set. The healthy
network correctly identiﬁes that both pictures of George
W. Bush are of the same person and that the pictures of
his father and Bill Clinton are of diﬀerent people. We also
see that George W. Bush is closer to his father than to Bill
Clinton. As the damage increases, the network occasion-
ally classiﬁes George H. W. Bush as being the same as his
son and, eventually, cannot even distinguish Bill Clinton.
After about 70% damage, all images start looking alike,
and the network continuously exchanges the ordering of
the distances as the damage increases. This pattern con-
tinues in the broader experiments and, with enough dam-
age, all pairs of images are labeled as being of the same
person. Note that adjusting the threshold as damage in-
creases would not improve the accuracy since the second
picture of George W. Bush does not remain closer than
the pictures of other people.

Figure 3: Change in Class Scores on Damaged Networks. This CNN
accepts an image of a handwritten digit as an input and outputs
scores for each possible digit, 0-9. In these two examples, the original
network correctly and conﬁdently classiﬁes the digit. As we increase
the damage level, conﬁdence drops and the classes eventually become
confused. For high levels of damage, all classes have similar scores.

damaged neurons when p = .01 may have little overlap
with the targeted neurons in the p = .02 case. At around
12% damage, the network becomes noticeably less conﬁ-
dent, but still correct. The network makes its ﬁrst mistake
at 30% damage by labeling the image as a 1. At higher
levels of damage, it frequently confuses classes 1 and 2.
After 90% damage, the ordering of the class scores con-
tinues to change, but their values become quite similar.
We see an analogous pattern in the second part of Fig-
ure 3 where we input a “5” as an example, although the
damaged networks make fewer mistakes.

Next, we consider two examples from the image classi-
ﬁcation problem. In Figure 4, we use an image of a green
bell pepper as the input to the ImageNet network. Here
the network assigns a score to each of one thousand possi-
ble classes before classifying the image as the class with the
highest score. We visualize how the classiﬁcation changes
as the damage increases (p = 0, 0.01, 0.02, . . . , 0.3). The
network makes its ﬁrst mistake at 12% damage but some-
times returns to classifying the image as a bell pepper.
Some of the mistakes are relatively sensible, such as “granny
smith” or “tennis ball,” and share similar colors, textures
and/or shapes with the original image. Some of the later

5

Figure 4: Classiﬁcation Mistakes as Damage Increases, Example 1. We start with a healthy network trained to classify images. The original
network correctly classiﬁes this image as a green pepper, but with enough damage, the network makes mistakes. For moderate amounts of
damage, the wrong classiﬁcations make some intuitive sense.

Figure 5: Classiﬁcation Mistakes as Damage Increases, Example 2. We increase the diﬃculty by using an image of a group of vegetables,
primarily bell peppers. The network does not maintain the “bell pepper” classiﬁcation as long, but the early mistakes are also produce or
also round items.

6

Figure 6: Change in Distance Between Images. This network outputs
a feature vector for each image and can be used to ﬁnd the distance
between two images. If the distance is below our threshold τ , the
pair is labeled as being of the same person. The network originally
correctly identiﬁes the second image of George W. Bush as being
the same person while labeling the images of George H. W. Bush
and Bill Clinton as being diﬀerent people. After suﬃcient damage,
the distances between the images all shrink and it is not possible to
determine whether or not a pair of images are of the same person.

4. Results

In this section, we move from qualitative descriptions of
single network errors to a more broad, statistical account
of mistakes within the test sets. We also consider a few
variations of FAS injury protocols, network settings, and
their dynamics to model biologically relevant phenomena
such as aging and the development of neurodegenerative
eﬀects across CNNs.

4.1. Overall network impairments

In Figure 7, we return to the MNIST handwritten digit
classiﬁcation task and plot confusion matrices Mi,j for
the ten digits as the damage percentage p increases. At
p = 0%, the network has 98.75% accuracy, so the matrix
is concentrated on the diagonal (i = j). At p = 20%, we
begin to see substantial errors (i (cid:54)= j), especially by over-
classifying digits 0, 4, and 9. As the damage increases, the
confusion matrices become even more distributed, but the
types of errors change. For example, at 40% damage, some
especially common labels are 1 and 6, while at 60% dam-
age, the disproportionately common labels become 0, 2, 4,
and 7. However, recall that in our TBI analogy, the dam-
age is not accumulated—in each experiment, we return to
the original network and randomly choose a new set of
weights to damage. At p = 100%, there is no randomness;
all weights are set to 0 and all images are labeled as a 1.
Overall, confusion matrices provide a straightforward visu-
alization for misclassiﬁcation within the CNN data set that
could be advantageous for diagnosing cognitive deﬁcits.

In Figure 8, we summarize our TBI experiments for (i)
the MNIST network, (ii) the ImageNet network, and (iii)
the Facial Recognition network. All targeted neurons are
assumed to malfunction the same way, fully blocking the
signal transmission to their neighbors. For each damage
level p (%), we average the accuracy across all random

Figure 7: Confusion matrices as damage increases. We depict the
classiﬁcation results of the handwritten digit classiﬁcation network
for varying amounts of damage. If the images are perfectly classi-
ﬁed, only the diagonal is non-zero. As the damage increases, most
images are mapped to the same few digits. Eventually, all images
are classiﬁed as a “1.”

Table 2: Sigmoid Parameters for Accuracy Drop-oﬀ Curves

Figure 8, MNIST

Figure 8, ImageNet

Figure 8, Face Veriﬁcation

Figure 10, combination

Figure 10, blockage

a

0.94

0.64

0.31

1.2

0.98

b

c

-7.9

-0.38

-13

-30

-2.2

-6.3

-0.12

-0.58

-0.42

-0.25

trials on that network. All three curves have a sigmoidal
shape of the form

(cid:18)

y = a

1 −

1
1 + exp(b(x + c))

(cid:19)

+

1
n

,

where n is the number of classes (see Table 2). Again, there
is no randomness when p = 0% or 100%. At p = 100%,
all weights are set to 0 and all examples are placed in the
same class. Therefore, the accuracy of the network decays
asymptotically to 1/n. Note that the MNIST network and
the ImageNet network have qualitatively similar trends,
and display some accuracy deﬁcit even at low injury levels.
On the other hand, the Facial Recognition network is able
to maintain its original accuracy level past p = 50% before
decaying abruptly.

4.2. Relevance of connections and biological constraints

In all three examples of network dysfunction, there is a
considerable amount of variability across trials even for the
same injury levels. We found that the deﬁcits greatly de-
pend on which weights were randomly selected. In other

7

ing principles in physiology. In fact, nervous systems are
a major drain on an animal’s energy budget and many
aspects of the brain’s anatomy seem to limit wiring costs
[13, 49, 83]. Brain networks can therefore be said to ne-
gotiate an economical trade-oﬀ between minimizing inter-
neuron connection cost & maximizing topological value
and capacity for information processing. See Bullmore and
Sporns [10] for a recent review on the topic.

The MNIST network could be more biophysically plau-
sible if it was not as over-engineered. With its original
topology and settings, the CNN becomes artiﬁcially resis-
tant to damage. In what follows, we will ﬁrst sparsify the
CNN by picking a point on the accuracy-eﬃciency trade
oﬀ curve (see Figure 9). There are multiple ways to choose
the best trade-oﬀ point. A reasonable choice is to remove
the weakest 69.4% of the links, which decreases the accu-
racy from 98.74% to 91.47%.

4.3. Diﬀerent types of FAS dysfunctions

As described in Section 2.2, Focal Axonal Swellings
(FAS) aﬀect spike trains in four qualitatively diﬀerent regimes:
transmission, ﬁltering, reﬂection, and blockage. So far
we have only considered the worst case, blockage, which
we model by setting a weight to zero. Now we also con-
sider the other types of neuronal malfunctions. We model
transmission as not damaging a weight, reﬂection as halv-
ing a weight, and ﬁltering as applying a low-pass ﬁlter
on each weight. We choose an example ﬁltering function
of f (x) = −.2774x2 + .9094x − .0192 plus Gaussian noise
∼ N (0, 0.05) by ﬁtting a confusion matrix from experimen-
tal results [64]. We believe that these additions to CNNs
contain, in a tractable way, the key features of the jeopar-
dizing eﬀects caused by FAS described in [65, 64, 63].

Recent experimental results provide detailed morpho-
logical descriptions of the FAS developing after traumatic
brain injuries. Wang et al [93] damaged the optic nerve of
adult rats with a central ﬂuid percussions injury. The optic
nerve is a relatively organized bundle of axons and allowed
for monitoring of FAS development 12h, 24h and 48h after
the impact. They divided the nerve in 12 serial grids and
reported for each of them the number of axonal swellings
per unit area, the total area of axonal swellings, and the
individual sizes of swellings. It is possible to infer the frac-
tion of FAS in each dysfunctional regime from these sta-
tistical distributions [62]. Based on these results, we con-
duct numerical experiments with 30% blockage, 45% re-
ﬂection, 20% ﬁltering, and 5% transmission. In Figure 10,
we show results for the sparsiﬁed MNIST network, com-
paring its average accuracy for heterogeneous and homoge-
neous FAS distributions. As expected, the worst deﬁcits
occurred when all of the swellings were in the blockage
regime. The best case is when all of the FAS are in the
ﬁltering regime, closely followed by the related reﬂection
case. When we combine these regimes (30% blockage, 45%
reﬂection, 20% ﬁltering, and 5% transmission), the accu-
racy is between these more extreme cases. Although the

Figure 8: Accuracy decay as damage increases. We randomly dam-
age edges of the network by setting their weights to zero. We plot the
percentage of edges that are damaged against the average accuracy
of the network for three problems. We see that damage initially has
little eﬀect, but then there’s a steep drop oﬀ until the accuracy levels
oﬀ around the level of random guessing.

words, neuronal connections in CNNs do not contribute
equally to a task, and damaging weights with large mag-
nitude typically impacts the accuracy more than targeting
weaker links—although magnitude alone cannot explain
all cases.

We illustrate some of these issues in Figure 9 for the
the MNIST network. We repeat the average decay in ac-
curacy in blue but add error bars. We found that roughly
the worst case is to damage the weights in decreasing or-
der of magnitude instead of randomly. The resulting steep
accuracy drop oﬀ is plotted in teal. Conversely, the ap-
proximate best case is to damage the weights in increasing
order of magnitude (plotted in gold). These three dam-
age strategies are visualized in terms of their eﬀect on
the distribution of weights. The purple histogram dis-
plays the distribution of the original weights. In general,
we randomly choose weights to damage, so the eﬀect is
distributed across the distribution of weights. However,
damaging the weights in order of decreasing magnitude is
equivalent to progressively removing the tails of the his-
togram, and choosing the weights in increasing order of
magnitude is equivalent to removing the middle of the
histogram. These experiments may provide intuition into
why the outcomes of TBI are so diﬃcult to predict. We
hypothesize that randomness in the location of FAS could
explain, for instance, why two soldiers near the same explo-
sion site may develop signiﬁcantly diﬀerent post-traumatic
outcomes.

One of the most striking diﬀerences between artiﬁcial
CNNs and biological neuronal networks is that the lat-
ter must operate under geometric, biophysical and energy
constraints. As reviewed in [54], brains have evolved to
operate eﬃciently since economy and proﬁciency are guid-

8

Figure 10: Comparing types of damage. In these experiments, we
begin with a “sparsiﬁed” network with the smallest 69.4% of the
weights removed. Then we compare the types of FAS (blockage,
reﬂection, and ﬁltering) and a combination of all types based on ex-
perimental evidence. As expected, blockage causes the most damage,
and reﬂection is a strong form of ﬁltering.

accuracy decreases more moderately, it can still be ﬁt with
a sigmoid function (see Table 2).

4.4. Aging and Neurodegenerative Diseases

Alzheimer’s Disease (AD) is the most commonly found
type of dementia, which is an umbrella term for a vari-
ety of brain disorders and pathologies [47]. Aging is the
single greatest risk factor for AD [73], and most public
health systems across the developed world are expected
to face huge challenges due to the growing elderly popu-
lation [76]. Recent estimates suggest that more than 5.2
million people have AD in the United States alone and
that a new case occurs every 68 seconds [89]. The most
typical symptom of the disease is an increasing diﬃculty
in recalling new information, although it sometimes oc-
curs in conjunction with challenges in completing familiar
tasks, confusion with time or place, and trouble under-
standing visual images and spatial relationships. W. Thies
and L. Bleiler [89] report that in many cases, AD diagnos-
tics are accompanied by cognitive tests, since individuals
with mild cognitive impairments have changes in think-
ing abilities that are noticeable to family members and
friends. We believe, however, that there is still a large de-
gree of subjectivity when it comes to interpreting cognitive
deﬁcits from dynamically evolving complex systems such
as the human brain. Thus, simulations with convolutional
neural networks that incorporate biophysically plausible
neural malfunctions may provide a window of opportunity
to better diagnose, for instance, confusion in visual image
classiﬁcation.

Figure 9: Range of possible outcomes. The change in accuracy as
weights are damaged varies depending on which weights were ran-
domly chosen. In blue, we plot the average accuracy plus error bars
for each level of damage. We also add curves in teal and yellow for
approximations of best and worst-case accuracies, respectively. The
approximate worst-case was found by damaging the weights in de-
creasing order of their absolute value. Similarly, the approximate
best-case was found by damaging the weights in increasing order.
We give a visualization in terms of a histogram of what it means
to damage the weights in a random order (“average case”), in de-
creasing order (“worst case”), and in increasing order (“best case”).
The yellow “best case” provides an accuracy-eﬃciency trade oﬀ. We
choose a turning point in the curve: if we remove the smallest 69.4%
of the weights, the accuracy only decreases from 98.74% to 91.47%.

9

Both CNNs and brains operate somewhere on an accuracy-

with memory. There are some tools available, including
the Mini-Mental State Examination (MMSE). The MMSE
assigns a score after testing performance on a brief se-
ries of tasks such as identifying objects and following writ-
ten instructions. This score can be used to quantitatively
track changes in a person’s cognitive function. Similarly, in
this paper, we calculate the change in accuracy on related
tasks, such as reading handwritten numbers and labeling
objects. Since we can conduct extensive experiments with
any level of injury, we believe that simulating FAS on our
model of cognition can lead to insight into the complex
processes underlying TBI and neurodegeneration.

Non-invasive diagnostic tools cannot detect anomalies
in vivo such as FAS that occur at the cellular level.
In
fact, this has motivated a large body of in vitro exper-
iments to replicate these injuries in a controlled setting
[12, 25, 32, 33, 34, 61, 69, 82]. However, in the latter
case, the cognitive eﬀects of these injuries cannot be as-
sessed. Simulations provide an opportunity to connect un-
derstanding of FAS to measures of cognitive performance.

eﬃciency trade-oﬀ curve. However, it can be argued that
brains are more highly constrained than CNNs due to the
high energy costs of maintaining the nervous systems [54].
In contrast, many CNNs are trained with a high focus on
small gains in accuracy, especially those trained for com-
petitions such as ImageNet. In addition, all three of the
CNNs studied here utilized dropout, which encourages re-
dundancy in the weights [84]. A key step in our method-
ology was to prune the CNNs to be less over-engineered
and thus more biologically plausible. Remarkably, the net-
works performed very well even if many weak connections
were removed.

Simulations of FAS damage to our CNN model of cogni-
tion result in interpretable and human-like mistakes, such
as confusing a handwritten “5” with a “6” (Figure 3), la-
beling peppers as an apple or a cucumber (Figures 4 and
5) and confusing George W. Bush with his father (Fig-
ure 6). We are able to quantify how accuracy changes as
damage increases (Figures 8, 9, 10, and 11) as well exactly
which kinds of mistakes are being made (Figure 7). We
demonstrate that the eﬀect on accuracy is highly variable
and depends on which connections are randomly selected
(Figure 9), providing intuition for why impairments are
diﬃcult to predict.

As with any model, using CNNs as an abstraction for
the brain comes with limitations. Biological neural net-
works have many complex features and constraints that
are not reﬂected in CNNs, such as transmitting informa-
tion through spike trains and utilizing feedback. One im-
portant diﬀerence between convolutional neural networks
and human subjects is the latter’s ability to infer signif-
icantly more information from the context of an image.
For instance, a patient classifying all objects depicted in
Fig. 5 might, due to some form of meta-analysis, read-
ily interpret them as a collection of many-colored peppers.
Consequently, he could discard extraneous objects from

Figure 11: Accumulating damage over time. In aging or neurode-
generative disease, damage to axons is accumulated over time, in
contrast to a one-time injury. We compare the accuracy curves for a
constant number of connections damaged for each time step to the
case where the number of connections damaged increases with time.
When damage increases over time, the initial loss in accuracy is slow
and the later loss is faster.

Focal axonal swelling pathologies are present in AD [2,
17, 53, 91] and in other neurodegenerative diseases such
as Parkinson’s disease [86, 60, 28], Multiple Sclerosis [26,
70, 90], and others [36, 50, 55, 56]. In many cases, FAS
arise by the agglomeration of speciﬁc proteins over time
[16, 68], and again, the computational modeling of focal
axonal swellings and their eﬀects to spike propagation from
[63] provide a platform to investigate network dysfunction.
In all of the previous experiments, we simulated TBI
by abruptly applying axonal injuries. Here we instead
simulate aging and its neurodegenerative eﬀects by grad-
ually accumulating random damage. We continue to use
the sparsiﬁed network and the heterogeneous FAS distri-
bution. Figure 11 shows that if damage is applied at a
constant rate (targeting 1% of the connections at each
step), the results will look similar to a sequence of TBI
experiments with p = .01, .02, . . . (Figure 10, in dark blue)
except that each trial will have a smoother trajectory.
This is translationally relevant since traumatic brain in-
juries dramatically increase the risk of dementia later in
life [5, 43, 44, 59]. Perhaps a more plausible and biophysi-
cally relevant case occurs when the FAS accumulation rate
increases linearly with time (in cyan). There, the young
brain accumulates very little damage, but the older brain
rapidly acquires new swellings.

5. Conclusions

Assessing levels of cognitive deﬁcits in patients is largely
a subjective task, with indicators such as whether or not
the patient and those close to them have noticed diﬃculties

10

a list of candidates (like ping-pong/tennis balls) even if
their shape and color alone do not provide suﬃcient evi-
dence for such dismissal. We would encourage the usage of
images with non-sensical pairings of objects to circumvent
this diﬃculty in diagnostic tests for cognitive deﬁcits.

An interesting avenue for future work would be retrain-
ing a CNN after damaging it. To be biologically relevant,
damaged weights would need to remain damaged, which is
non-trivial to implement. However, the greater challenge
is choosing an appropriate update rule. A key aspect of
CNNs is taking advantage of some form of back propa-
gation with gradient descent to solve the training opti-
mization problem in a reasonable amount of time. This
optimization problem is not convex, but for practical pur-
poses, we generally do not worry about choosing a local
optimum [14]. However, to retrain a CNN after removing
important weights, we may need to compensate by signif-
icantly changing other weights, escaping a local optimum.
Even if we are already in the correct local optimum, it
could be diﬃcult to choose a step size. Moving in the
direction of the gradient should move towards a local op-
timum unless the step size is too big. On the other hand,
it will not converge quickly if the step size is too small. In
the case of retraining a pre-trained but damaged network,
some weights may be already optimal but others may need
signiﬁcant change, creating a diﬃcult balance problem for
choosing a step size.

In summary, we provide a platform for quantitatively
and qualitatively studying the progression of focal axonal
swellings in a neural network. We can provide insight into
disorders which feature FAS, such as TBI, Alzheimer’s,
Parkinson’s, and Multiple Sclerosis, linking damage at the
cellular level to changes in cognitive behavior.

Acknowledgments

B. Lusch would like to acknowledge fellowship support
from the National Physical Science Consortium and Na-
tional Security Agency.

References

[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghe-
mawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia,
Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man´e,
D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M.,
Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
Vanhoucke, V., Vasudevan, V., Vi´egas, F., Vinyals, O., Warden,
P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X., 2015. Tensor-
Flow: Large-scale machine learning on heterogeneous systems.
Software available from tensorﬂow.org.
URL http://tensorflow.org/

[2] Adalbert, R., Nogradi, A., Babetto, E., Janeckova, L., Walker,
S. A., Kerschensteiner, M., Misgeld, T., Coleman, M. P., 2009.
Severely dystrophic axons at amyloid plaques remain continuous
and connected to viable cell bodies. BRAIN 132, 402–416.
[3] Adams, J. H., Jennett, B., Murray, L. S., Teasdale, G. M.,
Gennarelli, T. A., Graham, D. I., 2011. Neuropathological ﬁnd-
ings in disabled survivors of a head injury. Journal of Neuro-
trauma 28, 701–709.

11

[4] Baldi, P., Hornik, K., 1989. Neural networks and principal com-
ponent analysis: Learning from examples without local minima.
Neural networks 2, 53–58.

[5] Barnes, D. E., Kaup, A., Kirby, K., Byers, A. L., R.Diaz-
Arrastia, Yaﬀe, K., 2014. Traumatic brain injury and risk of
dementia in older veterans. Neurology 83, 312–319.

[6] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., et al.,
2007. Greedy layer-wise training of deep networks. Advances in
neural information processing systems 19, 153.

[7] Bishop, C. M., 1995. Neural networks for pattern recognition.

Oxford university press.

[8] Blumbergs, P., Scott, G., Manavis, J., Wainwright, H., Simp-
son, D., McLean, A., 1995. Topography of axonal injury as
deﬁned by amyloid precursor protein and the sector scoring
method in mild and severe closed head injury. Journal of Neu-
rotrauma 12, 565–572.

[9] Browne, K. D., Chen, X. H., Meaney, D. F., Smith, D. H., 2011.
Mild traumatic brain injury and diﬀuse axonal injury in swine.
Journal of Neurotrauma 28 (9), 1747–1755.

[10] Bullmore, E., Sporns, O., 2012. The economy of brain network
organization. Nature Reviews Neuroscience 13, 336–349.
[11] Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A., 2014.
Return of the devil in the details: Delving deep into convolu-
tional nets. In: British Machine Vision Conference.

[12] Chen, Y. C., Smith, D. H., Meaney, D., 2009. In-vitro ap-
proaches for studying blast-induced traumatic brain injury.
Journal of Neurotrauma 26 (6), 861–876.

[13] Chklovskii, D. B., Koulakov, A. A., 2004. Maps in the brain:
what can we learn from them? Annual Reviews in Neuroscience
27, 369–392.

[14] Choromanska, A., Henaﬀ, M., Mathieu, M., Arous, G. B., Le-
Cun, Y., 2015. The loss surfaces of multilayer networks. In:
AISTATS.

[15] Christman, C., Grady, M., Walker, S., Hol-Loway, K.,
Povlishock, J., 1994. Ultra-structural studies of diﬀuse axonal
injury in humans. Journal of Neurotrauma 11, 173–186.

[16] Coleman, M., 2005. Axon degeneration mechanisms: common-
ality amid diversity. Nature Reviews Neuroscience 6 (11), 889–
898.

[17] Daianu, M., Jacobs, R. E., Town, T., Thompson, P. M., 2016.
Axonal diameter and density estimated with 7-tesla hybrid dif-
fusion imaging in transgenic alzheimer rats. SPIE Proceedings
9784, 1–6.

[18] del Razo, M. J., Morofuji, Y., Meabon, J. S., Huber, B. R.,
Peskind, E. R., Banks, W. A., Mourad, P. D., LeVeque, R. J.,
Cook, D. G., 2016. Computational and in vitro studies of blast-
induced blood-brain barrier disruption. SIAM Journal on Sci-
entiﬁc Computing 38 (3), 347–374.

[19] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.,
2009. Imagenet: A large-scale hierarchical image database. In:
Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on. IEEE, pp. 248–255.

[20] Dikranian, K., Cohen, R., Donald, C. M., Pan, Y., Brakeﬁeld,
D., Bayly, P., Parsadanian, A., 2008. Mild traumatic brain in-
jury to the infant mouse causes robust white matter axonal de-
generation which precedes apoptotic death of cortical and tha-
lamic neurons. Experimental Neurology 211, 551–560.

[21] Edlow, B. L., Copen, W. A., Izzy, S., van der Kouwe, A., Glenn,
M. B., Greenberg, S. M., Greer, D. M., Wu, O., 2016. Longi-
tudinal diﬀusion tensor imaging detects recovery of fractional
anisotropy within traumatic axonal injury lesions. Neurocriti-
cal Care 24 (3), 342–352.

[22] Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent,
P., Bengio, S., 2010. Why does unsupervised pre-training help
deep learning? Journal of Machine Learning Research 11 (Feb),
625–660.

[23] Fainaru-Wada, M., Fainaru, S., 2013. League of denial: The nﬂ,
concussions, and the battle for truth. Crown Archetype.
[24] Faul, M., Xu, L., Wald, M. M., Coronado, V. G., 2010. Trau-
matic brain injury in the united states: emergency department
visits, hospitalizations, and deaths. Atlanta (GA): Centers for

Disease Control and Prevention, National Center for Injury Pre-
vention and Control.

[25] Fayanz, I., Tator, C. H., 2000. Modeling axonal injury in vitro:
injury and regeneration following acute neuritic trauma. Journal
of Neuroscience Methods 102, 69–79.

[26] Friese, M. A., Schattling, B., Fugger, L., 2014. Mechanisms of
neurodegeneration and axonal dysfunction in multiple sclerosis.
Nature Reviews Neurology 10, 225–238.

[27] Fukushima, F., 1980. A self-organizing neural network model
for a mechanism of pattern recognition unaﬀected by shift in
position. Biological Cybernetic 36, 193–202.

[28] Galvin, J. E., Uryu, K., Lee, V. M., Trojanowski, J. Q., 1999.
Axon pathology in parkinson’s disease and lewy body dementia
hippocampus contains α-, β-, and γ -synuclein. Proceedings of
National Academy of Science 96, 13450–13455.

[29] Grady, M., Mclaughlin, M., Christman, C., Valadaka, A.,
Flinger, C., Povlishock, J., 1993. The use of antibodies against
neuroﬁlament subunits for the detection of diﬀuse axonal in-
jury in humans. Journal of Neuropathology and Experimental
Neurology 52, 143–152.

[30] Hanell, A., Greer, J. E., McGinn, M. J., Povlishock, J. T., 2015.
Traumatic brain injury induced axonal phenotypes react diﬀer-
ently to treatment. Acta Neuropathologica 129, 317–332.
[31] He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep residual learning

for image recognition. arXiv preprint arXiv:1512.03385.

[32] Hellman, A. N., Vahidi, B., Kim, H. J., Mismar, W., Stew-
ard, O., Jeonde, N. L., Venugopalan, V., 2010. Examination of
axonal injury and regeneration in micropatterned neuronal cul-
ture using pulsed laser microbeam dissection. Lab on a Chip 16,
20832092.

[33] Hemphill, M., Dabiri, B., Gabriele, S., Kerscher, L., Franck, C.,
Goss, J., Alford, P., Parker, K., 2011. A possible role for integrin
signaling in diﬀuse axonal injury. PLos ONE 6 (7), e22899.
[34] Hemphill, M., Dauth, S., Yu, C. J., Dabiri, B., Parker, K., 2015.
Traumatic brain injury and the neuronal microenvironment: A
potential role for neuropathological mechanotransduction. Neu-
ron 86 (6), 1177–1192.

[35] Henninger, N., Bouley, J., Sikoglu, E. M., An, J., Moore, C. M.,
King, J. A., Bowser, R., Freeman, M. R., Jr, R. H. B., 2016.
Attenuated traumatic axonal injury and improved functional
outcome after traumatic brain injury in mice lacking sarm1.
BRAIN, 1–12.

[36] Herwerth, M., Kalluri, S. R., Srivastava, R., Kleele, T., Kenet,
S., Illes, Z., Merkler, D., Bennett, J. L., Misgeld, T., Hemmer,
B., 2016. In vivo imaging reveals rapid astrocyte depletion and
axon damage in a model of neuromyelitis optica-related pathol-
ogy. Annals of Neurology 79, 794–805.

[37] Hill, C. S., Coleman, M. P., Menon, D. K., 2016. Traumatic
injury: mechanisms and translational opportunities.

axonal
Trends in Neuroscience 39 (5), 311–324.

[38] Hinton, G. E., Osindero, S., Teh, Y.-W., 2006. A fast learning
algorithm for deep belief nets. Neural computation 18 (7), 1527–
1554.

[39] Hornik, K., Stinchcombe, M., White, H., 1989. Multilayer feed-
forward networks are universal approximators. Neural networks
2 (5), 359–366.

[40] Huang, G. B., Ramesh, M., Berg, T., Learned-Miller, E., 2007.
Labeled faces in the wild: A database for studying face recog-
nition in unconstrained environments. Tech. rep., Technical Re-
port 07-49, University of Massachusetts, Amherst.

[41] Hubel, D. H., Wiesel, T. N., 1962. Receptive ﬁelds, binocular
interaction and functional architecture in the cat’s visual cortex.
Journal of Physiology 160, 106–154.

[42] Ioﬀe, S., Szegedy, C., 2015. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. arXiv
preprint arXiv:1502.03167.

[43] Johnson, V. E., Stewart, W., Smith, D. H., 2010. Traumatic
brain injury and amyloid-β pathology: a link to alzheimer’s
disease? Nature Reviews Neuroscience 11, 361–370.

[44] Johnson, V. E., Stewart, W., Smith, D. H., 2012. Widespread
tau and amyloid-beta pathology many years after a single trau-

matic brain injury in humans. Brain Pathology 22, 142–149.

[45] Johnson, V. E., Stewart, W., Smith, D. H., 2013. Axonal pathol-
ogy in traumatic brain injury. Experimental Neurology 246, 35–
43.

[46] Jorge, R. E., Acion, L., White, T., Tordesillas-Gutierrez, D.,
Pierson, R., Crespo-Facorro, B., Magnotta, V., 2012. White
matter abnormalities in veterans with mild traumatic brain in-
jury. American Journal of Psychiatry 169 (12), 1284–1291.
[47] Jorm, A. F., Jolley, D., 1998. The incidence of dementia: a meta

analysis. Neurology 51, 728–733.

[48] Jurevic, S., 2011. Green pepper.

[Online; accessed June 1,

2016].
URL
5572058462

https://www.flickr.com/photos/stuffedpeppers/

[49] Kaiser, M., Hilgetag, C. C., 2006. Nonoptimal component place-
ment, but short processing paths, due to long- distance projec-
tions in neural systems. PLoS Computational Biology 2 (e95).
[50] Karlsson, P., Haroutounian, S., Polydefkis, M., Nyengaard,
J. R., Jensen, T. S., 2016. Structural and functional character-
ization of nerve ﬁbres in polyneuropathy and healthy subjects.
Scandinavian Journal of Pain 10, 28–35.

[51] Kinnunen, K. M., Greenwood, R., Powell, J. H., Leech, R.,
Hawkins, P. C., Bonnelle, V., Patel, M. C., Counsell, S. J.,
Sharp, D. J., 2010. White matter damage and cognitive impair-
ment after traumatic brain injury. Brain, 1–15.

[52] Krizhevsky, A., Sutskever, I., Hinton, G. E., 2012. Imagenet
classiﬁcation with deep convolutional neural networks. In: Ad-
vances in neural information processing systems. pp. 1097–1105.
[53] Krstic, D., Knuesel, I., 2012. Deciphering the mechanism under-
lying late-onset alzheimer disease. Nature Reviews Neuroscience
9 (1), 25–34.

[54] Laughlin, S. B., Sejnowski, T., 2003. Communication in neu-

ronal networks. Science 301, 1870–1874.

[55] Laukka, J. J., Kamholz, J., Bessert, D., 2016. Novel pathologic
ﬁndings in patients with pelizaeus-merzbacher disease. Neuro-
science Letters.

[56] Lauria, G., Morbin, M., Lombardi, R., Borgna, M., Mazzoleni,
G., Sghirlanzoni, A., Pareyson, D., 2003. Axonal swellings pre-
dict the degeneration of epidermal nerve ﬁbers in painful neu-
ropathies. Neurology 61, 631–636.

[57] LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature

[58] LeCun, Y., Cortes, C., Burges, C. J., 1998. The mnist database

521 (7553), 436–444.

of handwritten digits.

[59] LoBue, C., Denney, D., Hynan, L. S., Rossetti, H. C., Lacritz,
L. H., Jr., J. H., Womack, K. B., Woon, F. L., Cullum, C. M.,
2016. Self-reported traumatic brain injury and mild cognitive
impairment: increased risk and earlier age of diagnosis. Journal
of Alzheimer’s Disease 51, 727–736.

[60] Louis, E. D., Faust, P. L., Vonsattel, J., Honig, L. S., Rajput,
A., Rajput, A., Pahwa, R., Lyons, K. E., Ross, G. W., Elble,
R. J., Erickson-Davis, C., Moskowitz, C. B., Lawton, A., 2009.
Torpedoes in parkinson’s disease, alzheimer’s disease, essential
tremor, and control brains. Movement Disorders 24 (11), 1600–
1605.

[61] Magdesian, M. H., Sanchez, F., Lopez, M., Thostrup, P.,
Durisic, N., Belkaid, W., Liazoghli, D., Gr¨utter, P., Colman,
R., 2012. Atomic force microscopy reveals important diﬀerences
in axonal resistance to injury. Biophysical Journal 103 (3), 405–
414.

[62] Maia, P. D., 2014. Mathematical modeling of focal axonal
swellings arising in traumatic brain injuries and neurodegen-
erative diseases. Ph.D. thesis, University of Washington.
[63] Maia, P. D., Hemphill, M. A., Zehnder, B., Zhang, C., Parker,
K. K., Kutz, J. N., 2015. Diagnostic tools for evaluating the
impact of focal axonal swellings arising in neurodegenerative
diseases and/or traumatic brain injury. Journal of Neuroscience
Methods 253, 233–243.

[64] Maia, P. D., Kutz, J. N., 2014. Compromised axonal functional-
ity after neurodegeneration, concussion and/or traumatic brain
injury. Journal of Computational Neuroscience 27, 317–332.

12

[85] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov,
D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going
deeper with convolutions. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition. pp. 1–9.

[86] Tagliaferro, P., Burke, R. E., 2016. Retrograde axonal degen-
eration in parkinson disease. Journal of Parkinson’s Disease 6,
1–15.

[87] Tang-Schomer, M. D., Johnson, V. E., Baas, P. W., Stewart,
W., Smith, D. H., 2012. Partial interruption of axonal trans-
port due to microtubule breakage accounts for the formation of
periodic varicosities after traumatic axonal injury. Experimental
Neurology 233, 364–372.

[88] Tang-Schomer, M. D., Patel, A., Bass, P. W., Smith, D. H.,
2010. Mechanical breaking of microtubules in axons during dy-
namic stretch injury underlies delayed elasticity, microtubule
disassembly, and axon degeneration. The FASEB Journal 24 (5),
1401–1410.

[89] Thies, W., Bleiler, L., 2013. Alzheimer’s disease facts and ﬁg-

ures. Alzheimer’s & Dementia 9 (2), 208–245.

[90] Trapp, B. D., Nave, K.-A., 2008. Multiple sclerosis: An immune
or neurodegenerative disorder? Annual Review Neuroscience
31 (1), 247–269.

[91] Tsai, J., Grutzendler, J., Duﬀ, K., Gan, W. B., 2004. Fibrillar
amyloid deposition leads to local synaptic abnormalities and
breakage of neuronal branches. Nature Neuroscience 7, 1181–
1183.

[92] Vedaldi, A., Lenc, K., 2015. Matconvnet – convolutional neural
networks for matlab. In: Proceeding of the ACM Int. Conf. on
Multimedia.

[93] Wang, J., Hamm, R. J., Povlishock, J. T., 2011. Traumatic
axonal injury in the optic nerve: evidence for axonal swelling,
disconnection, dieback and reorganization. Journal of Neuro-
trauma, 28 (7), 1185–1198.

[65] Maia, P. D., Kutz, J. N., 2014. Identifying critical regions for
spike propagation in axon segments. Journal of Computational
Neuroscience 36 (2), 141–155.

[66] Maxwell, W. L., Povlishock, J. T., Graham, D. L., 1997. A
mechanistic analysis of nondisruptive axonal injury: A review.
Journal of Neurotrauma 17 (7), 419–440.

[67] Menon, D. K., Maas, A. I. R., 2015. Progress, failures and new
approaches for tbi research. Nature Reviews Neuroloy 11, 71–72.
[68] Millecamps, S., Julien, J., 2013. Axonal transport deﬁcits
and neurodegenerative diseases. Nature Reviews Neuroscience
14 (161), 161–176.

[69] Morrison, B., Elkin, B. S., Dolle, J. P., Yarmush, M. L., 2011.
In vitro models of traumatic brain injury. Annual Reviews in
Biomedical Engineering 13 (1), 91–126.

[70] Nikic, I., Merkler, D., Sorbara, C., Brinkoetter, M., Kreutzfeld,
M., Bareyre, F., Bruck, W., Bishop, D., Misgeld, T., Kerschen-
steiner, M., 2011. A reversible form of axon damage in exper-
imental autoimmune encephalomyelitis and multiple sclerosis.
Nature Medicine 17 (4), 495–499.

[71] Niyogi, P., Girosi, F., Poggio, T., 1998. Incorporating prior
information in machine learning by creating virtual examples.
Proceedings of the IEEE 86 (11), 2196–2209.

[72] Parkhi, O. M., Vedaldi, A., Zisserman, A., September 2015.
Deep face recognition. In: Xianghua Xie, M. W. J., Tam, G.
K. L. (Eds.), Proceedings of the British Machine Vision Con-
ference (BMVC). BMVA Press, pp. 41.1–41.12.
URL https://dx.doi.org/10.5244/C.29.41

[73] Patterson, B. W., Elbert, D. L., Mawuenyega, K. G., Kasten,
T., Ovod, V., Ma, S., Xiong, C., Chott, R., Yarasheski, K.,
Sigurdson, W., Zhang, L., Goate, A., Benzinger, T., Morris,
J. C., Holtzman, D., Bateman, R. J., 2015. Age and amyloid
eﬀects on human central nervous system amyloid-beta kinetics.
American Neurological Association 78 (3), 439–453.

[74] Poggio, T., 2016. Deep learning: mathematics and neuroscience.
Views & Reviews, McGovern Center for Brains, Minds and Ma-
chines, 1–7.

[75] Povlishock, J. T., Katz, D. I., 2005. Update of neuropathology
and neurological recovery after traumatic brain injury. Journal
of Head Trauma Rehabilitation 20 (1), 76–94.

[76] Qiu, C., Kivipelto, M., von Strauss, E., 2009. Epidemiology of
alzheimer’s disease: occurrence, determinants, and strategies
toward intervention. Dialogues in Clinical Neuroscience 11 (2),
111–128.

[77] Roozenbeek, B., Maas, A. I. R., Menon, D. K., 2013. Changing
patterns in the epidemiology of traumatic brain injury. Nature
Reviews Neurology 9, 231–236.

[78] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M.,
Berg, A. C., Fei-Fei, L., 2015. ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer Vi-
sion (IJCV) 115 (3), 211–252.

[79] Sanger, T. D., 1989. Optimal unsupervised learning in a single-
layer linear feedforward neural network. Neural networks 2 (6),
459–473.

[80] Sharp, D. J., Scott, G., Leech, R., 2014. Network dysfunction
after traumatic brain injury. Nature Reviews Neurology 10, 156–
166.

[81] Skandsen, T., Kvistad, K. A., Solheim, O., Strand, I. H., Folvik,
M., Vik, A., 2010. Prevalence and impact of diﬀuse axonal in-
jury in patients with moderate and severe head injury: a cohort
study of early magnetic resonance imaging ﬁndings and 1-year
outcome. Journal of Neurosurgery 113 (3), 556–563.

[82] Smith, D., Wolf, J., Lusardi, T., Lee, V., Meaney, D., 1999.
High tolerance and delayed elastic response of cultured axons
to dynamic stretch injury. The Journal of Neuroscience 19 (11),
4263–4269.

[83] Sporn, O., 2011. Networks of the brain. MIT Press.
[84] Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I.,
Salakhutdinov, R., 2014. Dropout: a simple way to prevent
neural networks from overﬁtting. Journal of Machine Learning
Research 15 (1), 1929–1958.

13

Modeling cognitive deﬁcits following neurodegenerative diseases and traumatic brain
injuries with deep convolutional neural networks

Bethany Lusch∗, Jake Weholt, Pedro D. Maia, J. Nathan Kutz

Department of Applied Mathematics, University of Washington, United States

6
1
0
2
 
c
e
D
 
3
1
 
 
]

C
N
.
o
i
b
-
q
[
 
 
1
v
3
2
4
4
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

The accurate diagnosis and assessment of neurodegenerative disease and traumatic brain injuries (TBI) remain open
challenges to doctors and medical practitioners. Both cause cognitive and functional deﬁcits due to the diﬀuse presence
of focal axonal swellings (FAS), but it is diﬃcult to deliver a prognosis due to our limited ability to assess damaged
neurons at a cellular level in vivo. In this paper, we simulate the eﬀects of neurodegenerative disease and TBI using
convolutional neural networks (CNNs) as our model of cognition. CNNs were originally inspired by neuroscience and the
hierarchical layers of neurons used for processing input stimulus to the brain. We start with CNNs pre-trained to perform
classiﬁcation, then utilize biophysically relevant statistical data on FAS to damage the connections in a functionally
relevant way. In order to improve the model, we incorporate the idea that brains operate under energy constraints by
pruning the CNNs to be less over-engineered. Qualitatively, we demonstrate that damage to the connections leads to
human-like mistakes. Our experiments also provide quantitative assessments of how accuracy is aﬀected by various types
and levels of damage. The deﬁcit resulting from a ﬁxed amount of damage greatly depends on which connections are
randomly injured, providing intuition for why it is diﬃcult to predict the impairments resulting from an injury. There is
a large degree of subjectivity when it comes to interpreting cognitive deﬁcits from dynamically evolving complex systems
such as the human brain. However, we provide important insight and a quantitative framework for several disorders in
which FAS are implicated, such as TBI, Alzheimer’s, Parkinson’s, and Multiple Sclerosis.

Keywords: Traumatic brain injury, neurodegenerative disease, focal axonal swellings, convolutional neural networks

1. Introduction

The mathematical architecture of convolutional neu-
ral networks (CNNs) was originally inspired by the No-
bel prize-winning work of Hubel and Wiesel on the pri-
mary visual cortex of cats [41]. Their seminal experiments
were the ﬁrst to suggest that neurons in the visual sys-
tem organize themselves in hierarchical layers of cells for
processing visual stimulus. The ﬁrst quantitative model
of the CNN, termed the Neocognitron by Fukushima in
1980 [27], already displayed many of the characteristic fea-
tures of today’s deep CNNs, including a multi-layer struc-
ture, convolution, max pooling and nonlinear dynamical
nodes. The connection between neuroscience and CNN
theory, although clearly a conceptional abstraction [74],
has since been instrumental to improving quantitative mod-
els of how the brain integrates neuro-sensory information
for stimulus classiﬁcation and decision making. Given that
CNNs mimic many of the important cognitive features of
the brain, we use it as a model for understanding how neu-
rodegenerative diseases and traumatic brain injuries (TBI)

∗Corresponding author at: University of Washington, Lewis Hall

#202, Box 353925, Seattle, WA 98195-3925 USA

Email address: herwaldt@uw.edu (Bethany Lusch)

can compromise an array of recognition tasks. Specif-
ically, by using well-established biophysical data on the
statistics (distribution and size) of focal axonal swellings
(FAS), which area among the primary symptoms of neu-
rodegeneration and TBI, we evaluate the progress of im-
pairments on a CNN-based model of cognition. Our model
provides quantitative metrics for understanding how cog-
nitive deﬁcits are accumulated as a function of FAS devel-
opment, allowing for potentially new diagnostics for the
evaluation of brain disorders due to neurodegenerative dis-
eases and/or TBI.

Understanding how neurodegenerative diseases and TBI
aﬀect cognitive function remains a critically important chal-
lenge for societal mental health. TBI alone is one of the
major causes of disability and mortality worldwide, which
in turn, dramatically jeopardizes society in several socioe-
conomic ways [67]. Not only is it the signature injury of
the wars in Afghanistan and Iraq [46], it is also the leading
cause of death among young people [24]. While many sur-
vive the events that induce TBI, persistent cognitive, psy-
chiatric, and physiological dysfunction often follows from
the mechanical impact (see Sec. 2). Likewise, neurode-
generative diseases are responsible for an overwhelming
variety of functional deﬁcits, with common symptoms in-
cluding memory loss or behavioral/cognitive impairments

December 15, 2016

Figure 1: Damaging a Convolutional Neural Network (CNN). (a) We start with a “healthy” CNN that accepts an image of a handwritten
digit as an input and outputs scores for each possible digit, 0-9. We classify the image as the digit with the highest score. (b) We then
damage the weights on the network in a biophysically-relevant way. In this ﬁgure, the healthy network correctly classiﬁes the image as a 2,
but the damaged network classiﬁes it as a 1.

which are related to an inability to correctly process multi-
modal information for decision-making tasks. The major-
ity of brain disorders have a complex cascade of patholog-
ical eﬀects spanning multiple spatial scales: from cellular
or network levels to tissues or entire brain areas. Unfor-
tunately, our limited ability to diagnose cerebral malfunc-
tions in vivo cannot detect several anomalies that occur on
smaller scales. FAS, however, are ubiquitous to TBI and
most leading and incurable disorders that dramatically af-
fect signaling properties of neurons, such as Multiple Scle-
rosis, Alzheimer’s and Parkinson’s diseases.

Given the currently available wealth of data on FAS
morphology from TBI studies and from almost every lead-
ing neurodegenerative disease, signiﬁcant progress can be
made towards understanding qualitatively how FAS im-
pacts cognitive function. In this work, we consider a set of
deep CNN models as an abstraction for functioning brains.
Our goal is to understand how the processing of input data
(classiﬁcation) is compromised as a function of increasing
injury and/or disease progression. Of course, it is obvi-
ous that the system’s performance will be compromised as
the CNN is injured, but the manner in which the cogni-
tive impairments arise is quite illustrative and informative,
providing intuitively appealing results about how cognitive
deﬁcits can develop and evolve as a neurodegenerative dis-
ease progresses.

Figure 1 illustrates our approach. We begin with the
original (healthy) CNN, which is trained to perform a clas-
siﬁcation task. In Figure 1, the speciﬁc task is to label a
handwritten digit. We then expose the CNN to diﬀerent

injury protocols based upon biophysical observations of
FAS statistics and morphological parameters. In particu-
lar, we use statistical distributions of FAS from a recent
experiment consisting of TBI-induced damage in the vi-
sual cortex of rats [93]. To impose these injury statistics
on the original CNN, we assume that each neuronal con-
nection has a biophysically plausible probability to mal-
function; while mild axonal injury may simply weaken a
connection, severe cases may break it permanently (i.e., an
axomtotomy occurs so that the connection strength goes
to zero). Ultimately, the severity of the injury and re-
weighting of connections is also determined by biophysical
data and the statistical distribution of the size of the FAS.
We can then progressively monitor the deleterious eﬀects
of the injury on the functionality of the CNN, providing
metrics for cognitive deﬁcits that arise.

The paper is outlined as follows: In Sec. 2 we provide
key background material on the two primary ﬁelds inte-
grated into this work: convolutional neural networks and
neural disorders in which FAS are implicated. We describe
our methodology in Sec. 3 and present results in Sec. 4.
We summarize our conclusions in Sec. 5. For full details,
all MATLAB and Python codes used for this paper are
available online at github.com/BethanyL/damaged_cnns.

2. Background

2.1. Convolutional Neural Networks

Deep convolutional neural networks (DCNNs) are trans-
forming almost every ﬁeld of science involving big data.

2

The success of the method has been enabled by two criti-
cal components: (i) the continued growth of computational
power (e.g. GPU and networked computing), and (ii) ex-
ceptionally large labeled data sets capable of taking advan-
tage of the full power of a multi-layer architecture. Indeed,
although the theoretical inception of CNNs has an almost
four-decade history, the analysis [52] of the ImageNet data
set [19] in 2012 provided a watershed moment for CNNs
and Deep Learning [57]. Prior to this data set, there were
a number of data sets available with approximately tens of
thousands of labeled images. ImageNet provided over 15
million labeled, high-resolution images with over 22,000
categories. DCNNs have since transformed the ﬁeld of
computer vision by dominating the performance metrics
in almost every meaningful computer vision task intended
for classiﬁcation and identiﬁcation (see, for example, the
International Conference on Computer Vision 2015).

ImageNet has been a critically enabling data set for
the evolution of the ﬁeld. However, CNNs were a topic of
intensive research long before. Indeed, they were highly
successful in a wide range of applications and machine
learning architectures. By the early 1990s, neural net-
works were studied as standard textbook material [7], with
the focus typically on a small number of layers. Critical
machine learning tasks such as principal component anal-
ysis (PCA) were shown to be intimately connected with
networks which included back propagation [4, 79]. Impor-
tantly, there were a number of critical innovations which
established multilayer feedforward networks as a class of
universal approximators. Speciﬁcally, Hornik et al. [39]
rigorously established that standard multilayer feedforward
networks with as few as one hidden layer using arbitrary
squashing functions were capable of approximating any
Borel measurable function from one ﬁnite dimensional space
to another to any desired degree of accuracy, provided suf-
ﬁciently many hidden units were available. Thus, multi-
layer feedforward networks could be thought of as a class
of universal approximators [39].

The past ﬁve years have seen tremendous advances in
the DCNN architecture. Innovations have come from al-
gorithmic tricks and modiﬁcations that have led to sig-
niﬁcant performance gains in a variety of ﬁelds. These
innovations include pretraining [38, 6, 22], dropout [84],
max pooling [52], inception modules [85], data augmen-
tation (virtual examples) [71], batch normalization [42]
and/or residual learning [31]. This is only a partial list
of potential algorithmic innovations available for improv-
ing the performance of classiﬁcation and labeling. Our
goal is not to provide a complete review of the DCNN
ﬁeld, but rather to highlight the continuing and rapid pace
Integrating the state-of-the-art
of progress in the ﬁeld.
in DCNNs is the open source software called TensorFlow
(tensorﬂow.org). TensorFlow was originally developed by
researchers and engineers working on the Google Brain
Team within Google’s Machine Intelligence research orga-
nization. The system is designed to facilitate research in
machine learning and to make it quick and easy to transi-

Figure 2: Four Types of Damaged Axons. A spike train passes
through a swollen axon. Depending on the way that the axon is
swollen, there are four ways that the information can be transmit-
ted. In transmission, the spike train is propagated correctly despite
the damage.
In ﬁltering, the spike train goes through a low-pass
ﬁlter. Regions of the spike train with high frequency are especially
likely to lose spikes. In reﬂection, pairs of spikes combine and only
half of the spikes are transmitted. In blockage, none of the spikes
are transmitted.

tion from research prototype to production system. Ten-
sorFlow has allowed for the test-bedding of new algorith-
mic structures in a reproducible and veriﬁable manner,
which is a signiﬁcant and important advancement in the
ﬁeld. Indeed, the DCNN architecture used here relies on
the TensorFlow architecture, helping us understand how
state-of-the-art DCNNs relate to cognitive abilities.

2.2. Focal Axonal Swellings

Concussions and Traumatic Brain Injuries (TBI) are more
than ever a concern for contact sport practitioners [23], for
veteran soldiers exposed to blast injuries [18, 46], and for
society as a whole [24, 67, 77]. In fact, TBI contributes
to one-third of all injury-related deaths and is one of the
major sources of functional impairments. TBI pathologies
aﬀect several spatial scales [80], but a ubiquitous develop-
ment at the neuronal microenvironment level is the pres-
ence of axonal injury [37, 45, 81]. As reviewed in [37], rapid
axonal stretch injury triggers secondary axonal changes
that can vary in extent and severity [21, 30, 35], but most
often culminate in Focal Axonal Swellings (FAS).

FAS are monitored whenever possible in in-vitro stud-
ies [12, 25, 32, 33, 34, 61, 69, 82], in in-vivo experiments

3

[9, 20, 66, 93], and in human patients [3, 8, 15, 29, 46, 51,
75]. In many cases, FAS critically aﬀect the axonal mor-
phology [87, 88] and consequently, the information content
encoded in spike trains propagating throughout them.

Recent computational studies distinguished geometri-
cal axonal enlargements that lead to minor changes in
propagation from those that result in critical phenomena
such as reﬂection or blockage of the original traveling pulse
[65], or ﬁltering of action potentials [64]. This led to a di-
agnostic toolbox that extracts meaningful geometrical pa-
rameters from sequential images of injured axon segments
[63]. These algorithms provide a principled approach to
deal with imaging distortions caused by experimental ar-
tifacts in order to extract the cross-section of an axon by
detecting local symmetries, turning points and turning re-
gions. More importantly, they provide the ﬁrst description
of biologically plausible injurious eﬀects due to FAS that
can be incorporated into neuronal network simulations.
Figure 2 reviews these diﬀerent eﬀects. In the transmis-
sion regime, the spike train propagates through the FAS
without signiﬁcant modiﬁcations. In the ﬁltering regime,
pulses that are too close to each other get deleted by a
mechanism resembling a pile-up collision [64]. As the FAS
geometrical parameters worsen, a single spike will split into
two components, one propagating forward and the other
propagating backward. The reﬂected, back-propagating
pulse will collide with the next spike in the train and they
will mutually annihilate each other. Thus, the reﬂection
regime eﬀectively halves the ﬁring rate of the neuron. Fi-
nally, in the worst-case scenario, the FAS will block all
spikes and transmit no information whatsoever.

In what follows, we will introduce FAS in a biologically
plausible way to a few examples of deep-learning convolu-
tional neural networks and evaluate the extent to which
cognitive deﬁcits develop.

3. Materials and Methods

3.1. CNN training, calibration, and performance

We simulate the development of FAS damage in three
diﬀerent convolutional neural networks. Each network has
its own properties and was trained with diﬀerent data sets
for separate tasks (see Table 1).

First, we consider a network trained on the MNIST
dataset [58], which is composed of images of handwritten
digits. We train the network to classify each image as a
digit from 0 to 9.
It could be used, for example, by a
post oﬃce machine to read zip codes from envelopes. The
training data set consists of a series of black and white
images that are 28 by 28 pixels. We use the TensorFlow
framework [1] to train a CNN with two convolutional layers
and a fully connected layer, as advised by a TensorFlow
tutorial. We use a subset of the standard MNIST test
set for our testing purposes so that our set contains the
In particular,
same number of examples for each digit.
we choose the ﬁrst 852 images for each digit. Our trained
network has an accuracy of 98.74% on this test set.

Next, we use a network trained on the ImageNet data
set to classify images from the ILSVRC 2012 challenge as
one of one thousand objects [78]. The CNN-F network
was pre-trained by the Visual Geometry Group at Oxford
[11] and made available through the MatConvNet Matlab
Toolbox [92], where it is referred to as imagenet-vgg-f. The
network contains ﬁve convolutional layers and three fully
connected layers. For our experiments, we use a subset of
the data with two examples randomly chosen from each
class. The network is 54.6% accurate on this test set.

The third network that we use was trained to classify
faces as one of one thousand people. However, if you re-
move the last classiﬁcation layer and normalize the output
vector, the network can instead be used to create feature
vectors for face veriﬁcation. If the Euclidean norm of the
diﬀerence between the feature vectors for two images is un-
der a threshold τ , the pair of images is classiﬁed as being
the same person. This network was also trained by the Vi-
sual Geometry Group [72] and made available through the
MatConvNet toolbox [92], where it is called vgg-face. For
our experiments, we randomly choose ﬁve pictures each of
ﬁfty randomly chosen celebrities from the Labeled Faces
in the Wild (LFW) data set [40]. We also need to choose a
threshold τ . We choose τ = 1.2 based on Linear Discrimi-
nant Analysis on a training set of 5700 examples from the
LFW data set. Each of the 250 images in our test set is
then compared to the four other images of the same per-
son and four images of other people. We thus test one
thousand pairs of images, half of which are of the same
person and half of which are not. The network is 81.6%
accurate with τ = 1.2 on our test set of one thousand pairs
of images.

3.2. Network impairments following FAS injuries

To simulate the eﬀects of traumatic brain injury on a
CNN, we randomly “damage” p percent of the weights in
the convolutional and fully connected layers. For consis-
tency with the TBI analogy, we only target the connections
between neurons and not bias weights. Note that these
CNNs are designed to use the same weights for multiple
connections. Thus, damaging p percent of the weights is
not equivalent to damaging p percent of the connections.
For simplicity, we ﬁrst assume that all axonal injuries lead
to the total blockage of spikes, which eﬀectively sets p per-
cent of weights to zero. We consider damage examples for
each one of the previously described networks to develop
intuition about possible functional impairments.

In Figure 3, we choose a handwritten “2” as the in-
put to the MNIST network. The network assigns a score
to each of the ten possible digits and then classiﬁes the
image as the digit with the highest score. The original
network gives scores of .999987 to 2, .000013 to 1, and 0
to the rest of the digits and thus correctly classiﬁes the im-
age as a 2. We then randomly damage the network in 100
separate experiments, setting p = .01, .02, . . . , 1. Since we
are simulating TBI, the damage happens all at once and
is not accumulated across experiments. Thus, the set of

4

Table 1: Summary of CNNs Used

Task

Training Set

# conv. # fully connected # weights

layers

layers

to damage

handwritten digit classiﬁcation

MNIST: 55K

object classiﬁcation

ImageNet, ILSVRC

2012: 1.2M

2

5

face veriﬁcation

VGG-Face: 2.6M

13

1

3

2

83K

61M

134M

mistakes seem less understandable, such as “hair slide.”
Note that this network was trained on about 1.2 million
images of the one thousand classes, encompassing a wide
range of examples for each class. For illustration purposes
in this ﬁgure, we show an example image from the test set
for each class. However, the input image is downloaded
from Flickr [48].

In Figure 5, we give a more diﬃcult input image to
the ImageNet network—a group of vegetables composed
predominately of peppers with a variety of colors but also
containing garlic. This image accompanies the Image Pro-
cessing Toolbox for Matlab as peppers.png and is used as
a demo for this network in the MatConvNet Toolbox. The
network successfully chooses the bell pepper class among
one thousand possibilities, but it is not as robust to injury
as the one in the previous, easier example. Misclassiﬁca-
tions begin at 4% injury. Again, some errors are reason-
able, such as a “cucumber” or “orange” (which is not that
diﬀerent from an orange-colored pepper). Others are quite
surprising, such as “socks” or “teddy bear”.

In Figure 6, we show analogous deﬁcits for the facial
recognition network (VGG-Face). We input an image of
George W. Bush to the network and have it compare the
image with three other pictures—one of his father George
H. W. Bush, one of Bill Clinton, and another one of him-
self. All images come from the LFW data set. The healthy
network correctly identiﬁes that both pictures of George
W. Bush are of the same person and that the pictures of
his father and Bill Clinton are of diﬀerent people. We also
see that George W. Bush is closer to his father than to Bill
Clinton. As the damage increases, the network occasion-
ally classiﬁes George H. W. Bush as being the same as his
son and, eventually, cannot even distinguish Bill Clinton.
After about 70% damage, all images start looking alike,
and the network continuously exchanges the ordering of
the distances as the damage increases. This pattern con-
tinues in the broader experiments and, with enough dam-
age, all pairs of images are labeled as being of the same
person. Note that adjusting the threshold as damage in-
creases would not improve the accuracy since the second
picture of George W. Bush does not remain closer than
the pictures of other people.

Figure 3: Change in Class Scores on Damaged Networks. This CNN
accepts an image of a handwritten digit as an input and outputs
scores for each possible digit, 0-9. In these two examples, the original
network correctly and conﬁdently classiﬁes the digit. As we increase
the damage level, conﬁdence drops and the classes eventually become
confused. For high levels of damage, all classes have similar scores.

damaged neurons when p = .01 may have little overlap
with the targeted neurons in the p = .02 case. At around
12% damage, the network becomes noticeably less conﬁ-
dent, but still correct. The network makes its ﬁrst mistake
at 30% damage by labeling the image as a 1. At higher
levels of damage, it frequently confuses classes 1 and 2.
After 90% damage, the ordering of the class scores con-
tinues to change, but their values become quite similar.
We see an analogous pattern in the second part of Fig-
ure 3 where we input a “5” as an example, although the
damaged networks make fewer mistakes.

Next, we consider two examples from the image classi-
ﬁcation problem. In Figure 4, we use an image of a green
bell pepper as the input to the ImageNet network. Here
the network assigns a score to each of one thousand possi-
ble classes before classifying the image as the class with the
highest score. We visualize how the classiﬁcation changes
as the damage increases (p = 0, 0.01, 0.02, . . . , 0.3). The
network makes its ﬁrst mistake at 12% damage but some-
times returns to classifying the image as a bell pepper.
Some of the mistakes are relatively sensible, such as “granny
smith” or “tennis ball,” and share similar colors, textures
and/or shapes with the original image. Some of the later

5

Figure 4: Classiﬁcation Mistakes as Damage Increases, Example 1. We start with a healthy network trained to classify images. The original
network correctly classiﬁes this image as a green pepper, but with enough damage, the network makes mistakes. For moderate amounts of
damage, the wrong classiﬁcations make some intuitive sense.

Figure 5: Classiﬁcation Mistakes as Damage Increases, Example 2. We increase the diﬃculty by using an image of a group of vegetables,
primarily bell peppers. The network does not maintain the “bell pepper” classiﬁcation as long, but the early mistakes are also produce or
also round items.

6

Figure 6: Change in Distance Between Images. This network outputs
a feature vector for each image and can be used to ﬁnd the distance
between two images. If the distance is below our threshold τ , the
pair is labeled as being of the same person. The network originally
correctly identiﬁes the second image of George W. Bush as being
the same person while labeling the images of George H. W. Bush
and Bill Clinton as being diﬀerent people. After suﬃcient damage,
the distances between the images all shrink and it is not possible to
determine whether or not a pair of images are of the same person.

4. Results

In this section, we move from qualitative descriptions of
single network errors to a more broad, statistical account
of mistakes within the test sets. We also consider a few
variations of FAS injury protocols, network settings, and
their dynamics to model biologically relevant phenomena
such as aging and the development of neurodegenerative
eﬀects across CNNs.

4.1. Overall network impairments

In Figure 7, we return to the MNIST handwritten digit
classiﬁcation task and plot confusion matrices Mi,j for
the ten digits as the damage percentage p increases. At
p = 0%, the network has 98.75% accuracy, so the matrix
is concentrated on the diagonal (i = j). At p = 20%, we
begin to see substantial errors (i (cid:54)= j), especially by over-
classifying digits 0, 4, and 9. As the damage increases, the
confusion matrices become even more distributed, but the
types of errors change. For example, at 40% damage, some
especially common labels are 1 and 6, while at 60% dam-
age, the disproportionately common labels become 0, 2, 4,
and 7. However, recall that in our TBI analogy, the dam-
age is not accumulated—in each experiment, we return to
the original network and randomly choose a new set of
weights to damage. At p = 100%, there is no randomness;
all weights are set to 0 and all images are labeled as a 1.
Overall, confusion matrices provide a straightforward visu-
alization for misclassiﬁcation within the CNN data set that
could be advantageous for diagnosing cognitive deﬁcits.

In Figure 8, we summarize our TBI experiments for (i)
the MNIST network, (ii) the ImageNet network, and (iii)
the Facial Recognition network. All targeted neurons are
assumed to malfunction the same way, fully blocking the
signal transmission to their neighbors. For each damage
level p (%), we average the accuracy across all random

Figure 7: Confusion matrices as damage increases. We depict the
classiﬁcation results of the handwritten digit classiﬁcation network
for varying amounts of damage. If the images are perfectly classi-
ﬁed, only the diagonal is non-zero. As the damage increases, most
images are mapped to the same few digits. Eventually, all images
are classiﬁed as a “1.”

Table 2: Sigmoid Parameters for Accuracy Drop-oﬀ Curves

Figure 8, MNIST

Figure 8, ImageNet

Figure 8, Face Veriﬁcation

Figure 10, combination

Figure 10, blockage

a

0.94

0.64

0.31

1.2

0.98

b

c

-7.9

-0.38

-13

-30

-2.2

-6.3

-0.12

-0.58

-0.42

-0.25

trials on that network. All three curves have a sigmoidal
shape of the form

(cid:18)

y = a

1 −

1
1 + exp(b(x + c))

(cid:19)

+

1
n

,

where n is the number of classes (see Table 2). Again, there
is no randomness when p = 0% or 100%. At p = 100%,
all weights are set to 0 and all examples are placed in the
same class. Therefore, the accuracy of the network decays
asymptotically to 1/n. Note that the MNIST network and
the ImageNet network have qualitatively similar trends,
and display some accuracy deﬁcit even at low injury levels.
On the other hand, the Facial Recognition network is able
to maintain its original accuracy level past p = 50% before
decaying abruptly.

4.2. Relevance of connections and biological constraints

In all three examples of network dysfunction, there is a
considerable amount of variability across trials even for the
same injury levels. We found that the deﬁcits greatly de-
pend on which weights were randomly selected. In other

7

ing principles in physiology. In fact, nervous systems are
a major drain on an animal’s energy budget and many
aspects of the brain’s anatomy seem to limit wiring costs
[13, 49, 83]. Brain networks can therefore be said to ne-
gotiate an economical trade-oﬀ between minimizing inter-
neuron connection cost & maximizing topological value
and capacity for information processing. See Bullmore and
Sporns [10] for a recent review on the topic.

The MNIST network could be more biophysically plau-
sible if it was not as over-engineered. With its original
topology and settings, the CNN becomes artiﬁcially resis-
tant to damage. In what follows, we will ﬁrst sparsify the
CNN by picking a point on the accuracy-eﬃciency trade
oﬀ curve (see Figure 9). There are multiple ways to choose
the best trade-oﬀ point. A reasonable choice is to remove
the weakest 69.4% of the links, which decreases the accu-
racy from 98.74% to 91.47%.

4.3. Diﬀerent types of FAS dysfunctions

As described in Section 2.2, Focal Axonal Swellings
(FAS) aﬀect spike trains in four qualitatively diﬀerent regimes:
transmission, ﬁltering, reﬂection, and blockage. So far
we have only considered the worst case, blockage, which
we model by setting a weight to zero. Now we also con-
sider the other types of neuronal malfunctions. We model
transmission as not damaging a weight, reﬂection as halv-
ing a weight, and ﬁltering as applying a low-pass ﬁlter
on each weight. We choose an example ﬁltering function
of f (x) = −.2774x2 + .9094x − .0192 plus Gaussian noise
∼ N (0, 0.05) by ﬁtting a confusion matrix from experimen-
tal results [64]. We believe that these additions to CNNs
contain, in a tractable way, the key features of the jeopar-
dizing eﬀects caused by FAS described in [65, 64, 63].

Recent experimental results provide detailed morpho-
logical descriptions of the FAS developing after traumatic
brain injuries. Wang et al [93] damaged the optic nerve of
adult rats with a central ﬂuid percussions injury. The optic
nerve is a relatively organized bundle of axons and allowed
for monitoring of FAS development 12h, 24h and 48h after
the impact. They divided the nerve in 12 serial grids and
reported for each of them the number of axonal swellings
per unit area, the total area of axonal swellings, and the
individual sizes of swellings. It is possible to infer the frac-
tion of FAS in each dysfunctional regime from these sta-
tistical distributions [62]. Based on these results, we con-
duct numerical experiments with 30% blockage, 45% re-
ﬂection, 20% ﬁltering, and 5% transmission. In Figure 10,
we show results for the sparsiﬁed MNIST network, com-
paring its average accuracy for heterogeneous and homoge-
neous FAS distributions. As expected, the worst deﬁcits
occurred when all of the swellings were in the blockage
regime. The best case is when all of the FAS are in the
ﬁltering regime, closely followed by the related reﬂection
case. When we combine these regimes (30% blockage, 45%
reﬂection, 20% ﬁltering, and 5% transmission), the accu-
racy is between these more extreme cases. Although the

Figure 8: Accuracy decay as damage increases. We randomly dam-
age edges of the network by setting their weights to zero. We plot the
percentage of edges that are damaged against the average accuracy
of the network for three problems. We see that damage initially has
little eﬀect, but then there’s a steep drop oﬀ until the accuracy levels
oﬀ around the level of random guessing.

words, neuronal connections in CNNs do not contribute
equally to a task, and damaging weights with large mag-
nitude typically impacts the accuracy more than targeting
weaker links—although magnitude alone cannot explain
all cases.

We illustrate some of these issues in Figure 9 for the
the MNIST network. We repeat the average decay in ac-
curacy in blue but add error bars. We found that roughly
the worst case is to damage the weights in decreasing or-
der of magnitude instead of randomly. The resulting steep
accuracy drop oﬀ is plotted in teal. Conversely, the ap-
proximate best case is to damage the weights in increasing
order of magnitude (plotted in gold). These three dam-
age strategies are visualized in terms of their eﬀect on
the distribution of weights. The purple histogram dis-
plays the distribution of the original weights. In general,
we randomly choose weights to damage, so the eﬀect is
distributed across the distribution of weights. However,
damaging the weights in order of decreasing magnitude is
equivalent to progressively removing the tails of the his-
togram, and choosing the weights in increasing order of
magnitude is equivalent to removing the middle of the
histogram. These experiments may provide intuition into
why the outcomes of TBI are so diﬃcult to predict. We
hypothesize that randomness in the location of FAS could
explain, for instance, why two soldiers near the same explo-
sion site may develop signiﬁcantly diﬀerent post-traumatic
outcomes.

One of the most striking diﬀerences between artiﬁcial
CNNs and biological neuronal networks is that the lat-
ter must operate under geometric, biophysical and energy
constraints. As reviewed in [54], brains have evolved to
operate eﬃciently since economy and proﬁciency are guid-

8

Figure 10: Comparing types of damage. In these experiments, we
begin with a “sparsiﬁed” network with the smallest 69.4% of the
weights removed. Then we compare the types of FAS (blockage,
reﬂection, and ﬁltering) and a combination of all types based on ex-
perimental evidence. As expected, blockage causes the most damage,
and reﬂection is a strong form of ﬁltering.

accuracy decreases more moderately, it can still be ﬁt with
a sigmoid function (see Table 2).

4.4. Aging and Neurodegenerative Diseases

Alzheimer’s Disease (AD) is the most commonly found
type of dementia, which is an umbrella term for a vari-
ety of brain disorders and pathologies [47]. Aging is the
single greatest risk factor for AD [73], and most public
health systems across the developed world are expected
to face huge challenges due to the growing elderly popu-
lation [76]. Recent estimates suggest that more than 5.2
million people have AD in the United States alone and
that a new case occurs every 68 seconds [89]. The most
typical symptom of the disease is an increasing diﬃculty
in recalling new information, although it sometimes oc-
curs in conjunction with challenges in completing familiar
tasks, confusion with time or place, and trouble under-
standing visual images and spatial relationships. W. Thies
and L. Bleiler [89] report that in many cases, AD diagnos-
tics are accompanied by cognitive tests, since individuals
with mild cognitive impairments have changes in think-
ing abilities that are noticeable to family members and
friends. We believe, however, that there is still a large de-
gree of subjectivity when it comes to interpreting cognitive
deﬁcits from dynamically evolving complex systems such
as the human brain. Thus, simulations with convolutional
neural networks that incorporate biophysically plausible
neural malfunctions may provide a window of opportunity
to better diagnose, for instance, confusion in visual image
classiﬁcation.

Figure 9: Range of possible outcomes. The change in accuracy as
weights are damaged varies depending on which weights were ran-
domly chosen. In blue, we plot the average accuracy plus error bars
for each level of damage. We also add curves in teal and yellow for
approximations of best and worst-case accuracies, respectively. The
approximate worst-case was found by damaging the weights in de-
creasing order of their absolute value. Similarly, the approximate
best-case was found by damaging the weights in increasing order.
We give a visualization in terms of a histogram of what it means
to damage the weights in a random order (“average case”), in de-
creasing order (“worst case”), and in increasing order (“best case”).
The yellow “best case” provides an accuracy-eﬃciency trade oﬀ. We
choose a turning point in the curve: if we remove the smallest 69.4%
of the weights, the accuracy only decreases from 98.74% to 91.47%.

9

Both CNNs and brains operate somewhere on an accuracy-

with memory. There are some tools available, including
the Mini-Mental State Examination (MMSE). The MMSE
assigns a score after testing performance on a brief se-
ries of tasks such as identifying objects and following writ-
ten instructions. This score can be used to quantitatively
track changes in a person’s cognitive function. Similarly, in
this paper, we calculate the change in accuracy on related
tasks, such as reading handwritten numbers and labeling
objects. Since we can conduct extensive experiments with
any level of injury, we believe that simulating FAS on our
model of cognition can lead to insight into the complex
processes underlying TBI and neurodegeneration.

Non-invasive diagnostic tools cannot detect anomalies
in vivo such as FAS that occur at the cellular level.
In
fact, this has motivated a large body of in vitro exper-
iments to replicate these injuries in a controlled setting
[12, 25, 32, 33, 34, 61, 69, 82]. However, in the latter
case, the cognitive eﬀects of these injuries cannot be as-
sessed. Simulations provide an opportunity to connect un-
derstanding of FAS to measures of cognitive performance.

eﬃciency trade-oﬀ curve. However, it can be argued that
brains are more highly constrained than CNNs due to the
high energy costs of maintaining the nervous systems [54].
In contrast, many CNNs are trained with a high focus on
small gains in accuracy, especially those trained for com-
petitions such as ImageNet. In addition, all three of the
CNNs studied here utilized dropout, which encourages re-
dundancy in the weights [84]. A key step in our method-
ology was to prune the CNNs to be less over-engineered
and thus more biologically plausible. Remarkably, the net-
works performed very well even if many weak connections
were removed.

Simulations of FAS damage to our CNN model of cogni-
tion result in interpretable and human-like mistakes, such
as confusing a handwritten “5” with a “6” (Figure 3), la-
beling peppers as an apple or a cucumber (Figures 4 and
5) and confusing George W. Bush with his father (Fig-
ure 6). We are able to quantify how accuracy changes as
damage increases (Figures 8, 9, 10, and 11) as well exactly
which kinds of mistakes are being made (Figure 7). We
demonstrate that the eﬀect on accuracy is highly variable
and depends on which connections are randomly selected
(Figure 9), providing intuition for why impairments are
diﬃcult to predict.

As with any model, using CNNs as an abstraction for
the brain comes with limitations. Biological neural net-
works have many complex features and constraints that
are not reﬂected in CNNs, such as transmitting informa-
tion through spike trains and utilizing feedback. One im-
portant diﬀerence between convolutional neural networks
and human subjects is the latter’s ability to infer signif-
icantly more information from the context of an image.
For instance, a patient classifying all objects depicted in
Fig. 5 might, due to some form of meta-analysis, read-
ily interpret them as a collection of many-colored peppers.
Consequently, he could discard extraneous objects from

Figure 11: Accumulating damage over time. In aging or neurode-
generative disease, damage to axons is accumulated over time, in
contrast to a one-time injury. We compare the accuracy curves for a
constant number of connections damaged for each time step to the
case where the number of connections damaged increases with time.
When damage increases over time, the initial loss in accuracy is slow
and the later loss is faster.

Focal axonal swelling pathologies are present in AD [2,
17, 53, 91] and in other neurodegenerative diseases such
as Parkinson’s disease [86, 60, 28], Multiple Sclerosis [26,
70, 90], and others [36, 50, 55, 56]. In many cases, FAS
arise by the agglomeration of speciﬁc proteins over time
[16, 68], and again, the computational modeling of focal
axonal swellings and their eﬀects to spike propagation from
[63] provide a platform to investigate network dysfunction.
In all of the previous experiments, we simulated TBI
by abruptly applying axonal injuries. Here we instead
simulate aging and its neurodegenerative eﬀects by grad-
ually accumulating random damage. We continue to use
the sparsiﬁed network and the heterogeneous FAS distri-
bution. Figure 11 shows that if damage is applied at a
constant rate (targeting 1% of the connections at each
step), the results will look similar to a sequence of TBI
experiments with p = .01, .02, . . . (Figure 10, in dark blue)
except that each trial will have a smoother trajectory.
This is translationally relevant since traumatic brain in-
juries dramatically increase the risk of dementia later in
life [5, 43, 44, 59]. Perhaps a more plausible and biophysi-
cally relevant case occurs when the FAS accumulation rate
increases linearly with time (in cyan). There, the young
brain accumulates very little damage, but the older brain
rapidly acquires new swellings.

5. Conclusions

Assessing levels of cognitive deﬁcits in patients is largely
a subjective task, with indicators such as whether or not
the patient and those close to them have noticed diﬃculties

10

a list of candidates (like ping-pong/tennis balls) even if
their shape and color alone do not provide suﬃcient evi-
dence for such dismissal. We would encourage the usage of
images with non-sensical pairings of objects to circumvent
this diﬃculty in diagnostic tests for cognitive deﬁcits.

An interesting avenue for future work would be retrain-
ing a CNN after damaging it. To be biologically relevant,
damaged weights would need to remain damaged, which is
non-trivial to implement. However, the greater challenge
is choosing an appropriate update rule. A key aspect of
CNNs is taking advantage of some form of back propa-
gation with gradient descent to solve the training opti-
mization problem in a reasonable amount of time. This
optimization problem is not convex, but for practical pur-
poses, we generally do not worry about choosing a local
optimum [14]. However, to retrain a CNN after removing
important weights, we may need to compensate by signif-
icantly changing other weights, escaping a local optimum.
Even if we are already in the correct local optimum, it
could be diﬃcult to choose a step size. Moving in the
direction of the gradient should move towards a local op-
timum unless the step size is too big. On the other hand,
it will not converge quickly if the step size is too small. In
the case of retraining a pre-trained but damaged network,
some weights may be already optimal but others may need
signiﬁcant change, creating a diﬃcult balance problem for
choosing a step size.

In summary, we provide a platform for quantitatively
and qualitatively studying the progression of focal axonal
swellings in a neural network. We can provide insight into
disorders which feature FAS, such as TBI, Alzheimer’s,
Parkinson’s, and Multiple Sclerosis, linking damage at the
cellular level to changes in cognitive behavior.

Acknowledgments

B. Lusch would like to acknowledge fellowship support
from the National Physical Science Consortium and Na-
tional Security Agency.

References

[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghe-
mawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia,
Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man´e,
D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M.,
Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
Vanhoucke, V., Vasudevan, V., Vi´egas, F., Vinyals, O., Warden,
P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X., 2015. Tensor-
Flow: Large-scale machine learning on heterogeneous systems.
Software available from tensorﬂow.org.
URL http://tensorflow.org/

[2] Adalbert, R., Nogradi, A., Babetto, E., Janeckova, L., Walker,
S. A., Kerschensteiner, M., Misgeld, T., Coleman, M. P., 2009.
Severely dystrophic axons at amyloid plaques remain continuous
and connected to viable cell bodies. BRAIN 132, 402–416.
[3] Adams, J. H., Jennett, B., Murray, L. S., Teasdale, G. M.,
Gennarelli, T. A., Graham, D. I., 2011. Neuropathological ﬁnd-
ings in disabled survivors of a head injury. Journal of Neuro-
trauma 28, 701–709.

11

[4] Baldi, P., Hornik, K., 1989. Neural networks and principal com-
ponent analysis: Learning from examples without local minima.
Neural networks 2, 53–58.

[5] Barnes, D. E., Kaup, A., Kirby, K., Byers, A. L., R.Diaz-
Arrastia, Yaﬀe, K., 2014. Traumatic brain injury and risk of
dementia in older veterans. Neurology 83, 312–319.

[6] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., et al.,
2007. Greedy layer-wise training of deep networks. Advances in
neural information processing systems 19, 153.

[7] Bishop, C. M., 1995. Neural networks for pattern recognition.

Oxford university press.

[8] Blumbergs, P., Scott, G., Manavis, J., Wainwright, H., Simp-
son, D., McLean, A., 1995. Topography of axonal injury as
deﬁned by amyloid precursor protein and the sector scoring
method in mild and severe closed head injury. Journal of Neu-
rotrauma 12, 565–572.

[9] Browne, K. D., Chen, X. H., Meaney, D. F., Smith, D. H., 2011.
Mild traumatic brain injury and diﬀuse axonal injury in swine.
Journal of Neurotrauma 28 (9), 1747–1755.

[10] Bullmore, E., Sporns, O., 2012. The economy of brain network
organization. Nature Reviews Neuroscience 13, 336–349.
[11] Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A., 2014.
Return of the devil in the details: Delving deep into convolu-
tional nets. In: British Machine Vision Conference.

[12] Chen, Y. C., Smith, D. H., Meaney, D., 2009. In-vitro ap-
proaches for studying blast-induced traumatic brain injury.
Journal of Neurotrauma 26 (6), 861–876.

[13] Chklovskii, D. B., Koulakov, A. A., 2004. Maps in the brain:
what can we learn from them? Annual Reviews in Neuroscience
27, 369–392.

[14] Choromanska, A., Henaﬀ, M., Mathieu, M., Arous, G. B., Le-
Cun, Y., 2015. The loss surfaces of multilayer networks. In:
AISTATS.

[15] Christman, C., Grady, M., Walker, S., Hol-Loway, K.,
Povlishock, J., 1994. Ultra-structural studies of diﬀuse axonal
injury in humans. Journal of Neurotrauma 11, 173–186.

[16] Coleman, M., 2005. Axon degeneration mechanisms: common-
ality amid diversity. Nature Reviews Neuroscience 6 (11), 889–
898.

[17] Daianu, M., Jacobs, R. E., Town, T., Thompson, P. M., 2016.
Axonal diameter and density estimated with 7-tesla hybrid dif-
fusion imaging in transgenic alzheimer rats. SPIE Proceedings
9784, 1–6.

[18] del Razo, M. J., Morofuji, Y., Meabon, J. S., Huber, B. R.,
Peskind, E. R., Banks, W. A., Mourad, P. D., LeVeque, R. J.,
Cook, D. G., 2016. Computational and in vitro studies of blast-
induced blood-brain barrier disruption. SIAM Journal on Sci-
entiﬁc Computing 38 (3), 347–374.

[19] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.,
2009. Imagenet: A large-scale hierarchical image database. In:
Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on. IEEE, pp. 248–255.

[20] Dikranian, K., Cohen, R., Donald, C. M., Pan, Y., Brakeﬁeld,
D., Bayly, P., Parsadanian, A., 2008. Mild traumatic brain in-
jury to the infant mouse causes robust white matter axonal de-
generation which precedes apoptotic death of cortical and tha-
lamic neurons. Experimental Neurology 211, 551–560.

[21] Edlow, B. L., Copen, W. A., Izzy, S., van der Kouwe, A., Glenn,
M. B., Greenberg, S. M., Greer, D. M., Wu, O., 2016. Longi-
tudinal diﬀusion tensor imaging detects recovery of fractional
anisotropy within traumatic axonal injury lesions. Neurocriti-
cal Care 24 (3), 342–352.

[22] Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent,
P., Bengio, S., 2010. Why does unsupervised pre-training help
deep learning? Journal of Machine Learning Research 11 (Feb),
625–660.

[23] Fainaru-Wada, M., Fainaru, S., 2013. League of denial: The nﬂ,
concussions, and the battle for truth. Crown Archetype.
[24] Faul, M., Xu, L., Wald, M. M., Coronado, V. G., 2010. Trau-
matic brain injury in the united states: emergency department
visits, hospitalizations, and deaths. Atlanta (GA): Centers for

Disease Control and Prevention, National Center for Injury Pre-
vention and Control.

[25] Fayanz, I., Tator, C. H., 2000. Modeling axonal injury in vitro:
injury and regeneration following acute neuritic trauma. Journal
of Neuroscience Methods 102, 69–79.

[26] Friese, M. A., Schattling, B., Fugger, L., 2014. Mechanisms of
neurodegeneration and axonal dysfunction in multiple sclerosis.
Nature Reviews Neurology 10, 225–238.

[27] Fukushima, F., 1980. A self-organizing neural network model
for a mechanism of pattern recognition unaﬀected by shift in
position. Biological Cybernetic 36, 193–202.

[28] Galvin, J. E., Uryu, K., Lee, V. M., Trojanowski, J. Q., 1999.
Axon pathology in parkinson’s disease and lewy body dementia
hippocampus contains α-, β-, and γ -synuclein. Proceedings of
National Academy of Science 96, 13450–13455.

[29] Grady, M., Mclaughlin, M., Christman, C., Valadaka, A.,
Flinger, C., Povlishock, J., 1993. The use of antibodies against
neuroﬁlament subunits for the detection of diﬀuse axonal in-
jury in humans. Journal of Neuropathology and Experimental
Neurology 52, 143–152.

[30] Hanell, A., Greer, J. E., McGinn, M. J., Povlishock, J. T., 2015.
Traumatic brain injury induced axonal phenotypes react diﬀer-
ently to treatment. Acta Neuropathologica 129, 317–332.
[31] He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep residual learning

for image recognition. arXiv preprint arXiv:1512.03385.

[32] Hellman, A. N., Vahidi, B., Kim, H. J., Mismar, W., Stew-
ard, O., Jeonde, N. L., Venugopalan, V., 2010. Examination of
axonal injury and regeneration in micropatterned neuronal cul-
ture using pulsed laser microbeam dissection. Lab on a Chip 16,
20832092.

[33] Hemphill, M., Dabiri, B., Gabriele, S., Kerscher, L., Franck, C.,
Goss, J., Alford, P., Parker, K., 2011. A possible role for integrin
signaling in diﬀuse axonal injury. PLos ONE 6 (7), e22899.
[34] Hemphill, M., Dauth, S., Yu, C. J., Dabiri, B., Parker, K., 2015.
Traumatic brain injury and the neuronal microenvironment: A
potential role for neuropathological mechanotransduction. Neu-
ron 86 (6), 1177–1192.

[35] Henninger, N., Bouley, J., Sikoglu, E. M., An, J., Moore, C. M.,
King, J. A., Bowser, R., Freeman, M. R., Jr, R. H. B., 2016.
Attenuated traumatic axonal injury and improved functional
outcome after traumatic brain injury in mice lacking sarm1.
BRAIN, 1–12.

[36] Herwerth, M., Kalluri, S. R., Srivastava, R., Kleele, T., Kenet,
S., Illes, Z., Merkler, D., Bennett, J. L., Misgeld, T., Hemmer,
B., 2016. In vivo imaging reveals rapid astrocyte depletion and
axon damage in a model of neuromyelitis optica-related pathol-
ogy. Annals of Neurology 79, 794–805.

[37] Hill, C. S., Coleman, M. P., Menon, D. K., 2016. Traumatic
injury: mechanisms and translational opportunities.

axonal
Trends in Neuroscience 39 (5), 311–324.

[38] Hinton, G. E., Osindero, S., Teh, Y.-W., 2006. A fast learning
algorithm for deep belief nets. Neural computation 18 (7), 1527–
1554.

[39] Hornik, K., Stinchcombe, M., White, H., 1989. Multilayer feed-
forward networks are universal approximators. Neural networks
2 (5), 359–366.

[40] Huang, G. B., Ramesh, M., Berg, T., Learned-Miller, E., 2007.
Labeled faces in the wild: A database for studying face recog-
nition in unconstrained environments. Tech. rep., Technical Re-
port 07-49, University of Massachusetts, Amherst.

[41] Hubel, D. H., Wiesel, T. N., 1962. Receptive ﬁelds, binocular
interaction and functional architecture in the cat’s visual cortex.
Journal of Physiology 160, 106–154.

[42] Ioﬀe, S., Szegedy, C., 2015. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. arXiv
preprint arXiv:1502.03167.

[43] Johnson, V. E., Stewart, W., Smith, D. H., 2010. Traumatic
brain injury and amyloid-β pathology: a link to alzheimer’s
disease? Nature Reviews Neuroscience 11, 361–370.

[44] Johnson, V. E., Stewart, W., Smith, D. H., 2012. Widespread
tau and amyloid-beta pathology many years after a single trau-

matic brain injury in humans. Brain Pathology 22, 142–149.

[45] Johnson, V. E., Stewart, W., Smith, D. H., 2013. Axonal pathol-
ogy in traumatic brain injury. Experimental Neurology 246, 35–
43.

[46] Jorge, R. E., Acion, L., White, T., Tordesillas-Gutierrez, D.,
Pierson, R., Crespo-Facorro, B., Magnotta, V., 2012. White
matter abnormalities in veterans with mild traumatic brain in-
jury. American Journal of Psychiatry 169 (12), 1284–1291.
[47] Jorm, A. F., Jolley, D., 1998. The incidence of dementia: a meta

analysis. Neurology 51, 728–733.

[48] Jurevic, S., 2011. Green pepper.

[Online; accessed June 1,

2016].
URL
5572058462

https://www.flickr.com/photos/stuffedpeppers/

[49] Kaiser, M., Hilgetag, C. C., 2006. Nonoptimal component place-
ment, but short processing paths, due to long- distance projec-
tions in neural systems. PLoS Computational Biology 2 (e95).
[50] Karlsson, P., Haroutounian, S., Polydefkis, M., Nyengaard,
J. R., Jensen, T. S., 2016. Structural and functional character-
ization of nerve ﬁbres in polyneuropathy and healthy subjects.
Scandinavian Journal of Pain 10, 28–35.

[51] Kinnunen, K. M., Greenwood, R., Powell, J. H., Leech, R.,
Hawkins, P. C., Bonnelle, V., Patel, M. C., Counsell, S. J.,
Sharp, D. J., 2010. White matter damage and cognitive impair-
ment after traumatic brain injury. Brain, 1–15.

[52] Krizhevsky, A., Sutskever, I., Hinton, G. E., 2012. Imagenet
classiﬁcation with deep convolutional neural networks. In: Ad-
vances in neural information processing systems. pp. 1097–1105.
[53] Krstic, D., Knuesel, I., 2012. Deciphering the mechanism under-
lying late-onset alzheimer disease. Nature Reviews Neuroscience
9 (1), 25–34.

[54] Laughlin, S. B., Sejnowski, T., 2003. Communication in neu-

ronal networks. Science 301, 1870–1874.

[55] Laukka, J. J., Kamholz, J., Bessert, D., 2016. Novel pathologic
ﬁndings in patients with pelizaeus-merzbacher disease. Neuro-
science Letters.

[56] Lauria, G., Morbin, M., Lombardi, R., Borgna, M., Mazzoleni,
G., Sghirlanzoni, A., Pareyson, D., 2003. Axonal swellings pre-
dict the degeneration of epidermal nerve ﬁbers in painful neu-
ropathies. Neurology 61, 631–636.

[57] LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature

[58] LeCun, Y., Cortes, C., Burges, C. J., 1998. The mnist database

521 (7553), 436–444.

of handwritten digits.

[59] LoBue, C., Denney, D., Hynan, L. S., Rossetti, H. C., Lacritz,
L. H., Jr., J. H., Womack, K. B., Woon, F. L., Cullum, C. M.,
2016. Self-reported traumatic brain injury and mild cognitive
impairment: increased risk and earlier age of diagnosis. Journal
of Alzheimer’s Disease 51, 727–736.

[60] Louis, E. D., Faust, P. L., Vonsattel, J., Honig, L. S., Rajput,
A., Rajput, A., Pahwa, R., Lyons, K. E., Ross, G. W., Elble,
R. J., Erickson-Davis, C., Moskowitz, C. B., Lawton, A., 2009.
Torpedoes in parkinson’s disease, alzheimer’s disease, essential
tremor, and control brains. Movement Disorders 24 (11), 1600–
1605.

[61] Magdesian, M. H., Sanchez, F., Lopez, M., Thostrup, P.,
Durisic, N., Belkaid, W., Liazoghli, D., Gr¨utter, P., Colman,
R., 2012. Atomic force microscopy reveals important diﬀerences
in axonal resistance to injury. Biophysical Journal 103 (3), 405–
414.

[62] Maia, P. D., 2014. Mathematical modeling of focal axonal
swellings arising in traumatic brain injuries and neurodegen-
erative diseases. Ph.D. thesis, University of Washington.
[63] Maia, P. D., Hemphill, M. A., Zehnder, B., Zhang, C., Parker,
K. K., Kutz, J. N., 2015. Diagnostic tools for evaluating the
impact of focal axonal swellings arising in neurodegenerative
diseases and/or traumatic brain injury. Journal of Neuroscience
Methods 253, 233–243.

[64] Maia, P. D., Kutz, J. N., 2014. Compromised axonal functional-
ity after neurodegeneration, concussion and/or traumatic brain
injury. Journal of Computational Neuroscience 27, 317–332.

12

[85] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov,
D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going
deeper with convolutions. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition. pp. 1–9.

[86] Tagliaferro, P., Burke, R. E., 2016. Retrograde axonal degen-
eration in parkinson disease. Journal of Parkinson’s Disease 6,
1–15.

[87] Tang-Schomer, M. D., Johnson, V. E., Baas, P. W., Stewart,
W., Smith, D. H., 2012. Partial interruption of axonal trans-
port due to microtubule breakage accounts for the formation of
periodic varicosities after traumatic axonal injury. Experimental
Neurology 233, 364–372.

[88] Tang-Schomer, M. D., Patel, A., Bass, P. W., Smith, D. H.,
2010. Mechanical breaking of microtubules in axons during dy-
namic stretch injury underlies delayed elasticity, microtubule
disassembly, and axon degeneration. The FASEB Journal 24 (5),
1401–1410.

[89] Thies, W., Bleiler, L., 2013. Alzheimer’s disease facts and ﬁg-

ures. Alzheimer’s & Dementia 9 (2), 208–245.

[90] Trapp, B. D., Nave, K.-A., 2008. Multiple sclerosis: An immune
or neurodegenerative disorder? Annual Review Neuroscience
31 (1), 247–269.

[91] Tsai, J., Grutzendler, J., Duﬀ, K., Gan, W. B., 2004. Fibrillar
amyloid deposition leads to local synaptic abnormalities and
breakage of neuronal branches. Nature Neuroscience 7, 1181–
1183.

[92] Vedaldi, A., Lenc, K., 2015. Matconvnet – convolutional neural
networks for matlab. In: Proceeding of the ACM Int. Conf. on
Multimedia.

[93] Wang, J., Hamm, R. J., Povlishock, J. T., 2011. Traumatic
axonal injury in the optic nerve: evidence for axonal swelling,
disconnection, dieback and reorganization. Journal of Neuro-
trauma, 28 (7), 1185–1198.

[65] Maia, P. D., Kutz, J. N., 2014. Identifying critical regions for
spike propagation in axon segments. Journal of Computational
Neuroscience 36 (2), 141–155.

[66] Maxwell, W. L., Povlishock, J. T., Graham, D. L., 1997. A
mechanistic analysis of nondisruptive axonal injury: A review.
Journal of Neurotrauma 17 (7), 419–440.

[67] Menon, D. K., Maas, A. I. R., 2015. Progress, failures and new
approaches for tbi research. Nature Reviews Neuroloy 11, 71–72.
[68] Millecamps, S., Julien, J., 2013. Axonal transport deﬁcits
and neurodegenerative diseases. Nature Reviews Neuroscience
14 (161), 161–176.

[69] Morrison, B., Elkin, B. S., Dolle, J. P., Yarmush, M. L., 2011.
In vitro models of traumatic brain injury. Annual Reviews in
Biomedical Engineering 13 (1), 91–126.

[70] Nikic, I., Merkler, D., Sorbara, C., Brinkoetter, M., Kreutzfeld,
M., Bareyre, F., Bruck, W., Bishop, D., Misgeld, T., Kerschen-
steiner, M., 2011. A reversible form of axon damage in exper-
imental autoimmune encephalomyelitis and multiple sclerosis.
Nature Medicine 17 (4), 495–499.

[71] Niyogi, P., Girosi, F., Poggio, T., 1998. Incorporating prior
information in machine learning by creating virtual examples.
Proceedings of the IEEE 86 (11), 2196–2209.

[72] Parkhi, O. M., Vedaldi, A., Zisserman, A., September 2015.
Deep face recognition. In: Xianghua Xie, M. W. J., Tam, G.
K. L. (Eds.), Proceedings of the British Machine Vision Con-
ference (BMVC). BMVA Press, pp. 41.1–41.12.
URL https://dx.doi.org/10.5244/C.29.41

[73] Patterson, B. W., Elbert, D. L., Mawuenyega, K. G., Kasten,
T., Ovod, V., Ma, S., Xiong, C., Chott, R., Yarasheski, K.,
Sigurdson, W., Zhang, L., Goate, A., Benzinger, T., Morris,
J. C., Holtzman, D., Bateman, R. J., 2015. Age and amyloid
eﬀects on human central nervous system amyloid-beta kinetics.
American Neurological Association 78 (3), 439–453.

[74] Poggio, T., 2016. Deep learning: mathematics and neuroscience.
Views & Reviews, McGovern Center for Brains, Minds and Ma-
chines, 1–7.

[75] Povlishock, J. T., Katz, D. I., 2005. Update of neuropathology
and neurological recovery after traumatic brain injury. Journal
of Head Trauma Rehabilitation 20 (1), 76–94.

[76] Qiu, C., Kivipelto, M., von Strauss, E., 2009. Epidemiology of
alzheimer’s disease: occurrence, determinants, and strategies
toward intervention. Dialogues in Clinical Neuroscience 11 (2),
111–128.

[77] Roozenbeek, B., Maas, A. I. R., Menon, D. K., 2013. Changing
patterns in the epidemiology of traumatic brain injury. Nature
Reviews Neurology 9, 231–236.

[78] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M.,
Berg, A. C., Fei-Fei, L., 2015. ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer Vi-
sion (IJCV) 115 (3), 211–252.

[79] Sanger, T. D., 1989. Optimal unsupervised learning in a single-
layer linear feedforward neural network. Neural networks 2 (6),
459–473.

[80] Sharp, D. J., Scott, G., Leech, R., 2014. Network dysfunction
after traumatic brain injury. Nature Reviews Neurology 10, 156–
166.

[81] Skandsen, T., Kvistad, K. A., Solheim, O., Strand, I. H., Folvik,
M., Vik, A., 2010. Prevalence and impact of diﬀuse axonal in-
jury in patients with moderate and severe head injury: a cohort
study of early magnetic resonance imaging ﬁndings and 1-year
outcome. Journal of Neurosurgery 113 (3), 556–563.

[82] Smith, D., Wolf, J., Lusardi, T., Lee, V., Meaney, D., 1999.
High tolerance and delayed elastic response of cultured axons
to dynamic stretch injury. The Journal of Neuroscience 19 (11),
4263–4269.

[83] Sporn, O., 2011. Networks of the brain. MIT Press.
[84] Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I.,
Salakhutdinov, R., 2014. Dropout: a simple way to prevent
neural networks from overﬁtting. Journal of Machine Learning
Research 15 (1), 1929–1958.

13

Modeling cognitive deﬁcits following neurodegenerative diseases and traumatic brain
injuries with deep convolutional neural networks

Bethany Lusch∗, Jake Weholt, Pedro D. Maia, J. Nathan Kutz

Department of Applied Mathematics, University of Washington, United States

6
1
0
2
 
c
e
D
 
3
1
 
 
]

C
N
.
o
i
b
-
q
[
 
 
1
v
3
2
4
4
0
.
2
1
6
1
:
v
i
X
r
a

Abstract

The accurate diagnosis and assessment of neurodegenerative disease and traumatic brain injuries (TBI) remain open
challenges to doctors and medical practitioners. Both cause cognitive and functional deﬁcits due to the diﬀuse presence
of focal axonal swellings (FAS), but it is diﬃcult to deliver a prognosis due to our limited ability to assess damaged
neurons at a cellular level in vivo. In this paper, we simulate the eﬀects of neurodegenerative disease and TBI using
convolutional neural networks (CNNs) as our model of cognition. CNNs were originally inspired by neuroscience and the
hierarchical layers of neurons used for processing input stimulus to the brain. We start with CNNs pre-trained to perform
classiﬁcation, then utilize biophysically relevant statistical data on FAS to damage the connections in a functionally
relevant way. In order to improve the model, we incorporate the idea that brains operate under energy constraints by
pruning the CNNs to be less over-engineered. Qualitatively, we demonstrate that damage to the connections leads to
human-like mistakes. Our experiments also provide quantitative assessments of how accuracy is aﬀected by various types
and levels of damage. The deﬁcit resulting from a ﬁxed amount of damage greatly depends on which connections are
randomly injured, providing intuition for why it is diﬃcult to predict the impairments resulting from an injury. There is
a large degree of subjectivity when it comes to interpreting cognitive deﬁcits from dynamically evolving complex systems
such as the human brain. However, we provide important insight and a quantitative framework for several disorders in
which FAS are implicated, such as TBI, Alzheimer’s, Parkinson’s, and Multiple Sclerosis.

Keywords: Traumatic brain injury, neurodegenerative disease, focal axonal swellings, convolutional neural networks

1. Introduction

The mathematical architecture of convolutional neu-
ral networks (CNNs) was originally inspired by the No-
bel prize-winning work of Hubel and Wiesel on the pri-
mary visual cortex of cats [41]. Their seminal experiments
were the ﬁrst to suggest that neurons in the visual sys-
tem organize themselves in hierarchical layers of cells for
processing visual stimulus. The ﬁrst quantitative model
of the CNN, termed the Neocognitron by Fukushima in
1980 [27], already displayed many of the characteristic fea-
tures of today’s deep CNNs, including a multi-layer struc-
ture, convolution, max pooling and nonlinear dynamical
nodes. The connection between neuroscience and CNN
theory, although clearly a conceptional abstraction [74],
has since been instrumental to improving quantitative mod-
els of how the brain integrates neuro-sensory information
for stimulus classiﬁcation and decision making. Given that
CNNs mimic many of the important cognitive features of
the brain, we use it as a model for understanding how neu-
rodegenerative diseases and traumatic brain injuries (TBI)

∗Corresponding author at: University of Washington, Lewis Hall

#202, Box 353925, Seattle, WA 98195-3925 USA

Email address: herwaldt@uw.edu (Bethany Lusch)

can compromise an array of recognition tasks. Specif-
ically, by using well-established biophysical data on the
statistics (distribution and size) of focal axonal swellings
(FAS), which area among the primary symptoms of neu-
rodegeneration and TBI, we evaluate the progress of im-
pairments on a CNN-based model of cognition. Our model
provides quantitative metrics for understanding how cog-
nitive deﬁcits are accumulated as a function of FAS devel-
opment, allowing for potentially new diagnostics for the
evaluation of brain disorders due to neurodegenerative dis-
eases and/or TBI.

Understanding how neurodegenerative diseases and TBI
aﬀect cognitive function remains a critically important chal-
lenge for societal mental health. TBI alone is one of the
major causes of disability and mortality worldwide, which
in turn, dramatically jeopardizes society in several socioe-
conomic ways [67]. Not only is it the signature injury of
the wars in Afghanistan and Iraq [46], it is also the leading
cause of death among young people [24]. While many sur-
vive the events that induce TBI, persistent cognitive, psy-
chiatric, and physiological dysfunction often follows from
the mechanical impact (see Sec. 2). Likewise, neurode-
generative diseases are responsible for an overwhelming
variety of functional deﬁcits, with common symptoms in-
cluding memory loss or behavioral/cognitive impairments

December 15, 2016

Figure 1: Damaging a Convolutional Neural Network (CNN). (a) We start with a “healthy” CNN that accepts an image of a handwritten
digit as an input and outputs scores for each possible digit, 0-9. We classify the image as the digit with the highest score. (b) We then
damage the weights on the network in a biophysically-relevant way. In this ﬁgure, the healthy network correctly classiﬁes the image as a 2,
but the damaged network classiﬁes it as a 1.

which are related to an inability to correctly process multi-
modal information for decision-making tasks. The major-
ity of brain disorders have a complex cascade of patholog-
ical eﬀects spanning multiple spatial scales: from cellular
or network levels to tissues or entire brain areas. Unfor-
tunately, our limited ability to diagnose cerebral malfunc-
tions in vivo cannot detect several anomalies that occur on
smaller scales. FAS, however, are ubiquitous to TBI and
most leading and incurable disorders that dramatically af-
fect signaling properties of neurons, such as Multiple Scle-
rosis, Alzheimer’s and Parkinson’s diseases.

Given the currently available wealth of data on FAS
morphology from TBI studies and from almost every lead-
ing neurodegenerative disease, signiﬁcant progress can be
made towards understanding qualitatively how FAS im-
pacts cognitive function. In this work, we consider a set of
deep CNN models as an abstraction for functioning brains.
Our goal is to understand how the processing of input data
(classiﬁcation) is compromised as a function of increasing
injury and/or disease progression. Of course, it is obvi-
ous that the system’s performance will be compromised as
the CNN is injured, but the manner in which the cogni-
tive impairments arise is quite illustrative and informative,
providing intuitively appealing results about how cognitive
deﬁcits can develop and evolve as a neurodegenerative dis-
ease progresses.

Figure 1 illustrates our approach. We begin with the
original (healthy) CNN, which is trained to perform a clas-
siﬁcation task. In Figure 1, the speciﬁc task is to label a
handwritten digit. We then expose the CNN to diﬀerent

injury protocols based upon biophysical observations of
FAS statistics and morphological parameters. In particu-
lar, we use statistical distributions of FAS from a recent
experiment consisting of TBI-induced damage in the vi-
sual cortex of rats [93]. To impose these injury statistics
on the original CNN, we assume that each neuronal con-
nection has a biophysically plausible probability to mal-
function; while mild axonal injury may simply weaken a
connection, severe cases may break it permanently (i.e., an
axomtotomy occurs so that the connection strength goes
to zero). Ultimately, the severity of the injury and re-
weighting of connections is also determined by biophysical
data and the statistical distribution of the size of the FAS.
We can then progressively monitor the deleterious eﬀects
of the injury on the functionality of the CNN, providing
metrics for cognitive deﬁcits that arise.

The paper is outlined as follows: In Sec. 2 we provide
key background material on the two primary ﬁelds inte-
grated into this work: convolutional neural networks and
neural disorders in which FAS are implicated. We describe
our methodology in Sec. 3 and present results in Sec. 4.
We summarize our conclusions in Sec. 5. For full details,
all MATLAB and Python codes used for this paper are
available online at github.com/BethanyL/damaged_cnns.

2. Background

2.1. Convolutional Neural Networks

Deep convolutional neural networks (DCNNs) are trans-
forming almost every ﬁeld of science involving big data.

2

The success of the method has been enabled by two criti-
cal components: (i) the continued growth of computational
power (e.g. GPU and networked computing), and (ii) ex-
ceptionally large labeled data sets capable of taking advan-
tage of the full power of a multi-layer architecture. Indeed,
although the theoretical inception of CNNs has an almost
four-decade history, the analysis [52] of the ImageNet data
set [19] in 2012 provided a watershed moment for CNNs
and Deep Learning [57]. Prior to this data set, there were
a number of data sets available with approximately tens of
thousands of labeled images. ImageNet provided over 15
million labeled, high-resolution images with over 22,000
categories. DCNNs have since transformed the ﬁeld of
computer vision by dominating the performance metrics
in almost every meaningful computer vision task intended
for classiﬁcation and identiﬁcation (see, for example, the
International Conference on Computer Vision 2015).

ImageNet has been a critically enabling data set for
the evolution of the ﬁeld. However, CNNs were a topic of
intensive research long before. Indeed, they were highly
successful in a wide range of applications and machine
learning architectures. By the early 1990s, neural net-
works were studied as standard textbook material [7], with
the focus typically on a small number of layers. Critical
machine learning tasks such as principal component anal-
ysis (PCA) were shown to be intimately connected with
networks which included back propagation [4, 79]. Impor-
tantly, there were a number of critical innovations which
established multilayer feedforward networks as a class of
universal approximators. Speciﬁcally, Hornik et al. [39]
rigorously established that standard multilayer feedforward
networks with as few as one hidden layer using arbitrary
squashing functions were capable of approximating any
Borel measurable function from one ﬁnite dimensional space
to another to any desired degree of accuracy, provided suf-
ﬁciently many hidden units were available. Thus, multi-
layer feedforward networks could be thought of as a class
of universal approximators [39].

The past ﬁve years have seen tremendous advances in
the DCNN architecture. Innovations have come from al-
gorithmic tricks and modiﬁcations that have led to sig-
niﬁcant performance gains in a variety of ﬁelds. These
innovations include pretraining [38, 6, 22], dropout [84],
max pooling [52], inception modules [85], data augmen-
tation (virtual examples) [71], batch normalization [42]
and/or residual learning [31]. This is only a partial list
of potential algorithmic innovations available for improv-
ing the performance of classiﬁcation and labeling. Our
goal is not to provide a complete review of the DCNN
ﬁeld, but rather to highlight the continuing and rapid pace
Integrating the state-of-the-art
of progress in the ﬁeld.
in DCNNs is the open source software called TensorFlow
(tensorﬂow.org). TensorFlow was originally developed by
researchers and engineers working on the Google Brain
Team within Google’s Machine Intelligence research orga-
nization. The system is designed to facilitate research in
machine learning and to make it quick and easy to transi-

Figure 2: Four Types of Damaged Axons. A spike train passes
through a swollen axon. Depending on the way that the axon is
swollen, there are four ways that the information can be transmit-
ted. In transmission, the spike train is propagated correctly despite
the damage.
In ﬁltering, the spike train goes through a low-pass
ﬁlter. Regions of the spike train with high frequency are especially
likely to lose spikes. In reﬂection, pairs of spikes combine and only
half of the spikes are transmitted. In blockage, none of the spikes
are transmitted.

tion from research prototype to production system. Ten-
sorFlow has allowed for the test-bedding of new algorith-
mic structures in a reproducible and veriﬁable manner,
which is a signiﬁcant and important advancement in the
ﬁeld. Indeed, the DCNN architecture used here relies on
the TensorFlow architecture, helping us understand how
state-of-the-art DCNNs relate to cognitive abilities.

2.2. Focal Axonal Swellings

Concussions and Traumatic Brain Injuries (TBI) are more
than ever a concern for contact sport practitioners [23], for
veteran soldiers exposed to blast injuries [18, 46], and for
society as a whole [24, 67, 77]. In fact, TBI contributes
to one-third of all injury-related deaths and is one of the
major sources of functional impairments. TBI pathologies
aﬀect several spatial scales [80], but a ubiquitous develop-
ment at the neuronal microenvironment level is the pres-
ence of axonal injury [37, 45, 81]. As reviewed in [37], rapid
axonal stretch injury triggers secondary axonal changes
that can vary in extent and severity [21, 30, 35], but most
often culminate in Focal Axonal Swellings (FAS).

FAS are monitored whenever possible in in-vitro stud-
ies [12, 25, 32, 33, 34, 61, 69, 82], in in-vivo experiments

3

[9, 20, 66, 93], and in human patients [3, 8, 15, 29, 46, 51,
75]. In many cases, FAS critically aﬀect the axonal mor-
phology [87, 88] and consequently, the information content
encoded in spike trains propagating throughout them.

Recent computational studies distinguished geometri-
cal axonal enlargements that lead to minor changes in
propagation from those that result in critical phenomena
such as reﬂection or blockage of the original traveling pulse
[65], or ﬁltering of action potentials [64]. This led to a di-
agnostic toolbox that extracts meaningful geometrical pa-
rameters from sequential images of injured axon segments
[63]. These algorithms provide a principled approach to
deal with imaging distortions caused by experimental ar-
tifacts in order to extract the cross-section of an axon by
detecting local symmetries, turning points and turning re-
gions. More importantly, they provide the ﬁrst description
of biologically plausible injurious eﬀects due to FAS that
can be incorporated into neuronal network simulations.
Figure 2 reviews these diﬀerent eﬀects. In the transmis-
sion regime, the spike train propagates through the FAS
without signiﬁcant modiﬁcations. In the ﬁltering regime,
pulses that are too close to each other get deleted by a
mechanism resembling a pile-up collision [64]. As the FAS
geometrical parameters worsen, a single spike will split into
two components, one propagating forward and the other
propagating backward. The reﬂected, back-propagating
pulse will collide with the next spike in the train and they
will mutually annihilate each other. Thus, the reﬂection
regime eﬀectively halves the ﬁring rate of the neuron. Fi-
nally, in the worst-case scenario, the FAS will block all
spikes and transmit no information whatsoever.

In what follows, we will introduce FAS in a biologically
plausible way to a few examples of deep-learning convolu-
tional neural networks and evaluate the extent to which
cognitive deﬁcits develop.

3. Materials and Methods

3.1. CNN training, calibration, and performance

We simulate the development of FAS damage in three
diﬀerent convolutional neural networks. Each network has
its own properties and was trained with diﬀerent data sets
for separate tasks (see Table 1).

First, we consider a network trained on the MNIST
dataset [58], which is composed of images of handwritten
digits. We train the network to classify each image as a
digit from 0 to 9.
It could be used, for example, by a
post oﬃce machine to read zip codes from envelopes. The
training data set consists of a series of black and white
images that are 28 by 28 pixels. We use the TensorFlow
framework [1] to train a CNN with two convolutional layers
and a fully connected layer, as advised by a TensorFlow
tutorial. We use a subset of the standard MNIST test
set for our testing purposes so that our set contains the
In particular,
same number of examples for each digit.
we choose the ﬁrst 852 images for each digit. Our trained
network has an accuracy of 98.74% on this test set.

Next, we use a network trained on the ImageNet data
set to classify images from the ILSVRC 2012 challenge as
one of one thousand objects [78]. The CNN-F network
was pre-trained by the Visual Geometry Group at Oxford
[11] and made available through the MatConvNet Matlab
Toolbox [92], where it is referred to as imagenet-vgg-f. The
network contains ﬁve convolutional layers and three fully
connected layers. For our experiments, we use a subset of
the data with two examples randomly chosen from each
class. The network is 54.6% accurate on this test set.

The third network that we use was trained to classify
faces as one of one thousand people. However, if you re-
move the last classiﬁcation layer and normalize the output
vector, the network can instead be used to create feature
vectors for face veriﬁcation. If the Euclidean norm of the
diﬀerence between the feature vectors for two images is un-
der a threshold τ , the pair of images is classiﬁed as being
the same person. This network was also trained by the Vi-
sual Geometry Group [72] and made available through the
MatConvNet toolbox [92], where it is called vgg-face. For
our experiments, we randomly choose ﬁve pictures each of
ﬁfty randomly chosen celebrities from the Labeled Faces
in the Wild (LFW) data set [40]. We also need to choose a
threshold τ . We choose τ = 1.2 based on Linear Discrimi-
nant Analysis on a training set of 5700 examples from the
LFW data set. Each of the 250 images in our test set is
then compared to the four other images of the same per-
son and four images of other people. We thus test one
thousand pairs of images, half of which are of the same
person and half of which are not. The network is 81.6%
accurate with τ = 1.2 on our test set of one thousand pairs
of images.

3.2. Network impairments following FAS injuries

To simulate the eﬀects of traumatic brain injury on a
CNN, we randomly “damage” p percent of the weights in
the convolutional and fully connected layers. For consis-
tency with the TBI analogy, we only target the connections
between neurons and not bias weights. Note that these
CNNs are designed to use the same weights for multiple
connections. Thus, damaging p percent of the weights is
not equivalent to damaging p percent of the connections.
For simplicity, we ﬁrst assume that all axonal injuries lead
to the total blockage of spikes, which eﬀectively sets p per-
cent of weights to zero. We consider damage examples for
each one of the previously described networks to develop
intuition about possible functional impairments.

In Figure 3, we choose a handwritten “2” as the in-
put to the MNIST network. The network assigns a score
to each of the ten possible digits and then classiﬁes the
image as the digit with the highest score. The original
network gives scores of .999987 to 2, .000013 to 1, and 0
to the rest of the digits and thus correctly classiﬁes the im-
age as a 2. We then randomly damage the network in 100
separate experiments, setting p = .01, .02, . . . , 1. Since we
are simulating TBI, the damage happens all at once and
is not accumulated across experiments. Thus, the set of

4

Table 1: Summary of CNNs Used

Task

Training Set

# conv. # fully connected # weights

layers

layers

to damage

handwritten digit classiﬁcation

MNIST: 55K

object classiﬁcation

ImageNet, ILSVRC

2012: 1.2M

2

5

face veriﬁcation

VGG-Face: 2.6M

13

1

3

2

83K

61M

134M

mistakes seem less understandable, such as “hair slide.”
Note that this network was trained on about 1.2 million
images of the one thousand classes, encompassing a wide
range of examples for each class. For illustration purposes
in this ﬁgure, we show an example image from the test set
for each class. However, the input image is downloaded
from Flickr [48].

In Figure 5, we give a more diﬃcult input image to
the ImageNet network—a group of vegetables composed
predominately of peppers with a variety of colors but also
containing garlic. This image accompanies the Image Pro-
cessing Toolbox for Matlab as peppers.png and is used as
a demo for this network in the MatConvNet Toolbox. The
network successfully chooses the bell pepper class among
one thousand possibilities, but it is not as robust to injury
as the one in the previous, easier example. Misclassiﬁca-
tions begin at 4% injury. Again, some errors are reason-
able, such as a “cucumber” or “orange” (which is not that
diﬀerent from an orange-colored pepper). Others are quite
surprising, such as “socks” or “teddy bear”.

In Figure 6, we show analogous deﬁcits for the facial
recognition network (VGG-Face). We input an image of
George W. Bush to the network and have it compare the
image with three other pictures—one of his father George
H. W. Bush, one of Bill Clinton, and another one of him-
self. All images come from the LFW data set. The healthy
network correctly identiﬁes that both pictures of George
W. Bush are of the same person and that the pictures of
his father and Bill Clinton are of diﬀerent people. We also
see that George W. Bush is closer to his father than to Bill
Clinton. As the damage increases, the network occasion-
ally classiﬁes George H. W. Bush as being the same as his
son and, eventually, cannot even distinguish Bill Clinton.
After about 70% damage, all images start looking alike,
and the network continuously exchanges the ordering of
the distances as the damage increases. This pattern con-
tinues in the broader experiments and, with enough dam-
age, all pairs of images are labeled as being of the same
person. Note that adjusting the threshold as damage in-
creases would not improve the accuracy since the second
picture of George W. Bush does not remain closer than
the pictures of other people.

Figure 3: Change in Class Scores on Damaged Networks. This CNN
accepts an image of a handwritten digit as an input and outputs
scores for each possible digit, 0-9. In these two examples, the original
network correctly and conﬁdently classiﬁes the digit. As we increase
the damage level, conﬁdence drops and the classes eventually become
confused. For high levels of damage, all classes have similar scores.

damaged neurons when p = .01 may have little overlap
with the targeted neurons in the p = .02 case. At around
12% damage, the network becomes noticeably less conﬁ-
dent, but still correct. The network makes its ﬁrst mistake
at 30% damage by labeling the image as a 1. At higher
levels of damage, it frequently confuses classes 1 and 2.
After 90% damage, the ordering of the class scores con-
tinues to change, but their values become quite similar.
We see an analogous pattern in the second part of Fig-
ure 3 where we input a “5” as an example, although the
damaged networks make fewer mistakes.

Next, we consider two examples from the image classi-
ﬁcation problem. In Figure 4, we use an image of a green
bell pepper as the input to the ImageNet network. Here
the network assigns a score to each of one thousand possi-
ble classes before classifying the image as the class with the
highest score. We visualize how the classiﬁcation changes
as the damage increases (p = 0, 0.01, 0.02, . . . , 0.3). The
network makes its ﬁrst mistake at 12% damage but some-
times returns to classifying the image as a bell pepper.
Some of the mistakes are relatively sensible, such as “granny
smith” or “tennis ball,” and share similar colors, textures
and/or shapes with the original image. Some of the later

5

Figure 4: Classiﬁcation Mistakes as Damage Increases, Example 1. We start with a healthy network trained to classify images. The original
network correctly classiﬁes this image as a green pepper, but with enough damage, the network makes mistakes. For moderate amounts of
damage, the wrong classiﬁcations make some intuitive sense.

Figure 5: Classiﬁcation Mistakes as Damage Increases, Example 2. We increase the diﬃculty by using an image of a group of vegetables,
primarily bell peppers. The network does not maintain the “bell pepper” classiﬁcation as long, but the early mistakes are also produce or
also round items.

6

Figure 6: Change in Distance Between Images. This network outputs
a feature vector for each image and can be used to ﬁnd the distance
between two images. If the distance is below our threshold τ , the
pair is labeled as being of the same person. The network originally
correctly identiﬁes the second image of George W. Bush as being
the same person while labeling the images of George H. W. Bush
and Bill Clinton as being diﬀerent people. After suﬃcient damage,
the distances between the images all shrink and it is not possible to
determine whether or not a pair of images are of the same person.

4. Results

In this section, we move from qualitative descriptions of
single network errors to a more broad, statistical account
of mistakes within the test sets. We also consider a few
variations of FAS injury protocols, network settings, and
their dynamics to model biologically relevant phenomena
such as aging and the development of neurodegenerative
eﬀects across CNNs.

4.1. Overall network impairments

In Figure 7, we return to the MNIST handwritten digit
classiﬁcation task and plot confusion matrices Mi,j for
the ten digits as the damage percentage p increases. At
p = 0%, the network has 98.75% accuracy, so the matrix
is concentrated on the diagonal (i = j). At p = 20%, we
begin to see substantial errors (i (cid:54)= j), especially by over-
classifying digits 0, 4, and 9. As the damage increases, the
confusion matrices become even more distributed, but the
types of errors change. For example, at 40% damage, some
especially common labels are 1 and 6, while at 60% dam-
age, the disproportionately common labels become 0, 2, 4,
and 7. However, recall that in our TBI analogy, the dam-
age is not accumulated—in each experiment, we return to
the original network and randomly choose a new set of
weights to damage. At p = 100%, there is no randomness;
all weights are set to 0 and all images are labeled as a 1.
Overall, confusion matrices provide a straightforward visu-
alization for misclassiﬁcation within the CNN data set that
could be advantageous for diagnosing cognitive deﬁcits.

In Figure 8, we summarize our TBI experiments for (i)
the MNIST network, (ii) the ImageNet network, and (iii)
the Facial Recognition network. All targeted neurons are
assumed to malfunction the same way, fully blocking the
signal transmission to their neighbors. For each damage
level p (%), we average the accuracy across all random

Figure 7: Confusion matrices as damage increases. We depict the
classiﬁcation results of the handwritten digit classiﬁcation network
for varying amounts of damage. If the images are perfectly classi-
ﬁed, only the diagonal is non-zero. As the damage increases, most
images are mapped to the same few digits. Eventually, all images
are classiﬁed as a “1.”

Table 2: Sigmoid Parameters for Accuracy Drop-oﬀ Curves

Figure 8, MNIST

Figure 8, ImageNet

Figure 8, Face Veriﬁcation

Figure 10, combination

Figure 10, blockage

a

0.94

0.64

0.31

1.2

0.98

b

c

-7.9

-0.38

-13

-30

-2.2

-6.3

-0.12

-0.58

-0.42

-0.25

trials on that network. All three curves have a sigmoidal
shape of the form

(cid:18)

y = a

1 −

1
1 + exp(b(x + c))

(cid:19)

+

1
n

,

where n is the number of classes (see Table 2). Again, there
is no randomness when p = 0% or 100%. At p = 100%,
all weights are set to 0 and all examples are placed in the
same class. Therefore, the accuracy of the network decays
asymptotically to 1/n. Note that the MNIST network and
the ImageNet network have qualitatively similar trends,
and display some accuracy deﬁcit even at low injury levels.
On the other hand, the Facial Recognition network is able
to maintain its original accuracy level past p = 50% before
decaying abruptly.

4.2. Relevance of connections and biological constraints

In all three examples of network dysfunction, there is a
considerable amount of variability across trials even for the
same injury levels. We found that the deﬁcits greatly de-
pend on which weights were randomly selected. In other

7

ing principles in physiology. In fact, nervous systems are
a major drain on an animal’s energy budget and many
aspects of the brain’s anatomy seem to limit wiring costs
[13, 49, 83]. Brain networks can therefore be said to ne-
gotiate an economical trade-oﬀ between minimizing inter-
neuron connection cost & maximizing topological value
and capacity for information processing. See Bullmore and
Sporns [10] for a recent review on the topic.

The MNIST network could be more biophysically plau-
sible if it was not as over-engineered. With its original
topology and settings, the CNN becomes artiﬁcially resis-
tant to damage. In what follows, we will ﬁrst sparsify the
CNN by picking a point on the accuracy-eﬃciency trade
oﬀ curve (see Figure 9). There are multiple ways to choose
the best trade-oﬀ point. A reasonable choice is to remove
the weakest 69.4% of the links, which decreases the accu-
racy from 98.74% to 91.47%.

4.3. Diﬀerent types of FAS dysfunctions

As described in Section 2.2, Focal Axonal Swellings
(FAS) aﬀect spike trains in four qualitatively diﬀerent regimes:
transmission, ﬁltering, reﬂection, and blockage. So far
we have only considered the worst case, blockage, which
we model by setting a weight to zero. Now we also con-
sider the other types of neuronal malfunctions. We model
transmission as not damaging a weight, reﬂection as halv-
ing a weight, and ﬁltering as applying a low-pass ﬁlter
on each weight. We choose an example ﬁltering function
of f (x) = −.2774x2 + .9094x − .0192 plus Gaussian noise
∼ N (0, 0.05) by ﬁtting a confusion matrix from experimen-
tal results [64]. We believe that these additions to CNNs
contain, in a tractable way, the key features of the jeopar-
dizing eﬀects caused by FAS described in [65, 64, 63].

Recent experimental results provide detailed morpho-
logical descriptions of the FAS developing after traumatic
brain injuries. Wang et al [93] damaged the optic nerve of
adult rats with a central ﬂuid percussions injury. The optic
nerve is a relatively organized bundle of axons and allowed
for monitoring of FAS development 12h, 24h and 48h after
the impact. They divided the nerve in 12 serial grids and
reported for each of them the number of axonal swellings
per unit area, the total area of axonal swellings, and the
individual sizes of swellings. It is possible to infer the frac-
tion of FAS in each dysfunctional regime from these sta-
tistical distributions [62]. Based on these results, we con-
duct numerical experiments with 30% blockage, 45% re-
ﬂection, 20% ﬁltering, and 5% transmission. In Figure 10,
we show results for the sparsiﬁed MNIST network, com-
paring its average accuracy for heterogeneous and homoge-
neous FAS distributions. As expected, the worst deﬁcits
occurred when all of the swellings were in the blockage
regime. The best case is when all of the FAS are in the
ﬁltering regime, closely followed by the related reﬂection
case. When we combine these regimes (30% blockage, 45%
reﬂection, 20% ﬁltering, and 5% transmission), the accu-
racy is between these more extreme cases. Although the

Figure 8: Accuracy decay as damage increases. We randomly dam-
age edges of the network by setting their weights to zero. We plot the
percentage of edges that are damaged against the average accuracy
of the network for three problems. We see that damage initially has
little eﬀect, but then there’s a steep drop oﬀ until the accuracy levels
oﬀ around the level of random guessing.

words, neuronal connections in CNNs do not contribute
equally to a task, and damaging weights with large mag-
nitude typically impacts the accuracy more than targeting
weaker links—although magnitude alone cannot explain
all cases.

We illustrate some of these issues in Figure 9 for the
the MNIST network. We repeat the average decay in ac-
curacy in blue but add error bars. We found that roughly
the worst case is to damage the weights in decreasing or-
der of magnitude instead of randomly. The resulting steep
accuracy drop oﬀ is plotted in teal. Conversely, the ap-
proximate best case is to damage the weights in increasing
order of magnitude (plotted in gold). These three dam-
age strategies are visualized in terms of their eﬀect on
the distribution of weights. The purple histogram dis-
plays the distribution of the original weights. In general,
we randomly choose weights to damage, so the eﬀect is
distributed across the distribution of weights. However,
damaging the weights in order of decreasing magnitude is
equivalent to progressively removing the tails of the his-
togram, and choosing the weights in increasing order of
magnitude is equivalent to removing the middle of the
histogram. These experiments may provide intuition into
why the outcomes of TBI are so diﬃcult to predict. We
hypothesize that randomness in the location of FAS could
explain, for instance, why two soldiers near the same explo-
sion site may develop signiﬁcantly diﬀerent post-traumatic
outcomes.

One of the most striking diﬀerences between artiﬁcial
CNNs and biological neuronal networks is that the lat-
ter must operate under geometric, biophysical and energy
constraints. As reviewed in [54], brains have evolved to
operate eﬃciently since economy and proﬁciency are guid-

8

Figure 10: Comparing types of damage. In these experiments, we
begin with a “sparsiﬁed” network with the smallest 69.4% of the
weights removed. Then we compare the types of FAS (blockage,
reﬂection, and ﬁltering) and a combination of all types based on ex-
perimental evidence. As expected, blockage causes the most damage,
and reﬂection is a strong form of ﬁltering.

accuracy decreases more moderately, it can still be ﬁt with
a sigmoid function (see Table 2).

4.4. Aging and Neurodegenerative Diseases

Alzheimer’s Disease (AD) is the most commonly found
type of dementia, which is an umbrella term for a vari-
ety of brain disorders and pathologies [47]. Aging is the
single greatest risk factor for AD [73], and most public
health systems across the developed world are expected
to face huge challenges due to the growing elderly popu-
lation [76]. Recent estimates suggest that more than 5.2
million people have AD in the United States alone and
that a new case occurs every 68 seconds [89]. The most
typical symptom of the disease is an increasing diﬃculty
in recalling new information, although it sometimes oc-
curs in conjunction with challenges in completing familiar
tasks, confusion with time or place, and trouble under-
standing visual images and spatial relationships. W. Thies
and L. Bleiler [89] report that in many cases, AD diagnos-
tics are accompanied by cognitive tests, since individuals
with mild cognitive impairments have changes in think-
ing abilities that are noticeable to family members and
friends. We believe, however, that there is still a large de-
gree of subjectivity when it comes to interpreting cognitive
deﬁcits from dynamically evolving complex systems such
as the human brain. Thus, simulations with convolutional
neural networks that incorporate biophysically plausible
neural malfunctions may provide a window of opportunity
to better diagnose, for instance, confusion in visual image
classiﬁcation.

Figure 9: Range of possible outcomes. The change in accuracy as
weights are damaged varies depending on which weights were ran-
domly chosen. In blue, we plot the average accuracy plus error bars
for each level of damage. We also add curves in teal and yellow for
approximations of best and worst-case accuracies, respectively. The
approximate worst-case was found by damaging the weights in de-
creasing order of their absolute value. Similarly, the approximate
best-case was found by damaging the weights in increasing order.
We give a visualization in terms of a histogram of what it means
to damage the weights in a random order (“average case”), in de-
creasing order (“worst case”), and in increasing order (“best case”).
The yellow “best case” provides an accuracy-eﬃciency trade oﬀ. We
choose a turning point in the curve: if we remove the smallest 69.4%
of the weights, the accuracy only decreases from 98.74% to 91.47%.

9

Both CNNs and brains operate somewhere on an accuracy-

with memory. There are some tools available, including
the Mini-Mental State Examination (MMSE). The MMSE
assigns a score after testing performance on a brief se-
ries of tasks such as identifying objects and following writ-
ten instructions. This score can be used to quantitatively
track changes in a person’s cognitive function. Similarly, in
this paper, we calculate the change in accuracy on related
tasks, such as reading handwritten numbers and labeling
objects. Since we can conduct extensive experiments with
any level of injury, we believe that simulating FAS on our
model of cognition can lead to insight into the complex
processes underlying TBI and neurodegeneration.

Non-invasive diagnostic tools cannot detect anomalies
in vivo such as FAS that occur at the cellular level.
In
fact, this has motivated a large body of in vitro exper-
iments to replicate these injuries in a controlled setting
[12, 25, 32, 33, 34, 61, 69, 82]. However, in the latter
case, the cognitive eﬀects of these injuries cannot be as-
sessed. Simulations provide an opportunity to connect un-
derstanding of FAS to measures of cognitive performance.

eﬃciency trade-oﬀ curve. However, it can be argued that
brains are more highly constrained than CNNs due to the
high energy costs of maintaining the nervous systems [54].
In contrast, many CNNs are trained with a high focus on
small gains in accuracy, especially those trained for com-
petitions such as ImageNet. In addition, all three of the
CNNs studied here utilized dropout, which encourages re-
dundancy in the weights [84]. A key step in our method-
ology was to prune the CNNs to be less over-engineered
and thus more biologically plausible. Remarkably, the net-
works performed very well even if many weak connections
were removed.

Simulations of FAS damage to our CNN model of cogni-
tion result in interpretable and human-like mistakes, such
as confusing a handwritten “5” with a “6” (Figure 3), la-
beling peppers as an apple or a cucumber (Figures 4 and
5) and confusing George W. Bush with his father (Fig-
ure 6). We are able to quantify how accuracy changes as
damage increases (Figures 8, 9, 10, and 11) as well exactly
which kinds of mistakes are being made (Figure 7). We
demonstrate that the eﬀect on accuracy is highly variable
and depends on which connections are randomly selected
(Figure 9), providing intuition for why impairments are
diﬃcult to predict.

As with any model, using CNNs as an abstraction for
the brain comes with limitations. Biological neural net-
works have many complex features and constraints that
are not reﬂected in CNNs, such as transmitting informa-
tion through spike trains and utilizing feedback. One im-
portant diﬀerence between convolutional neural networks
and human subjects is the latter’s ability to infer signif-
icantly more information from the context of an image.
For instance, a patient classifying all objects depicted in
Fig. 5 might, due to some form of meta-analysis, read-
ily interpret them as a collection of many-colored peppers.
Consequently, he could discard extraneous objects from

Figure 11: Accumulating damage over time. In aging or neurode-
generative disease, damage to axons is accumulated over time, in
contrast to a one-time injury. We compare the accuracy curves for a
constant number of connections damaged for each time step to the
case where the number of connections damaged increases with time.
When damage increases over time, the initial loss in accuracy is slow
and the later loss is faster.

Focal axonal swelling pathologies are present in AD [2,
17, 53, 91] and in other neurodegenerative diseases such
as Parkinson’s disease [86, 60, 28], Multiple Sclerosis [26,
70, 90], and others [36, 50, 55, 56]. In many cases, FAS
arise by the agglomeration of speciﬁc proteins over time
[16, 68], and again, the computational modeling of focal
axonal swellings and their eﬀects to spike propagation from
[63] provide a platform to investigate network dysfunction.
In all of the previous experiments, we simulated TBI
by abruptly applying axonal injuries. Here we instead
simulate aging and its neurodegenerative eﬀects by grad-
ually accumulating random damage. We continue to use
the sparsiﬁed network and the heterogeneous FAS distri-
bution. Figure 11 shows that if damage is applied at a
constant rate (targeting 1% of the connections at each
step), the results will look similar to a sequence of TBI
experiments with p = .01, .02, . . . (Figure 10, in dark blue)
except that each trial will have a smoother trajectory.
This is translationally relevant since traumatic brain in-
juries dramatically increase the risk of dementia later in
life [5, 43, 44, 59]. Perhaps a more plausible and biophysi-
cally relevant case occurs when the FAS accumulation rate
increases linearly with time (in cyan). There, the young
brain accumulates very little damage, but the older brain
rapidly acquires new swellings.

5. Conclusions

Assessing levels of cognitive deﬁcits in patients is largely
a subjective task, with indicators such as whether or not
the patient and those close to them have noticed diﬃculties

10

a list of candidates (like ping-pong/tennis balls) even if
their shape and color alone do not provide suﬃcient evi-
dence for such dismissal. We would encourage the usage of
images with non-sensical pairings of objects to circumvent
this diﬃculty in diagnostic tests for cognitive deﬁcits.

An interesting avenue for future work would be retrain-
ing a CNN after damaging it. To be biologically relevant,
damaged weights would need to remain damaged, which is
non-trivial to implement. However, the greater challenge
is choosing an appropriate update rule. A key aspect of
CNNs is taking advantage of some form of back propa-
gation with gradient descent to solve the training opti-
mization problem in a reasonable amount of time. This
optimization problem is not convex, but for practical pur-
poses, we generally do not worry about choosing a local
optimum [14]. However, to retrain a CNN after removing
important weights, we may need to compensate by signif-
icantly changing other weights, escaping a local optimum.
Even if we are already in the correct local optimum, it
could be diﬃcult to choose a step size. Moving in the
direction of the gradient should move towards a local op-
timum unless the step size is too big. On the other hand,
it will not converge quickly if the step size is too small. In
the case of retraining a pre-trained but damaged network,
some weights may be already optimal but others may need
signiﬁcant change, creating a diﬃcult balance problem for
choosing a step size.

In summary, we provide a platform for quantitatively
and qualitatively studying the progression of focal axonal
swellings in a neural network. We can provide insight into
disorders which feature FAS, such as TBI, Alzheimer’s,
Parkinson’s, and Multiple Sclerosis, linking damage at the
cellular level to changes in cognitive behavior.

Acknowledgments

B. Lusch would like to acknowledge fellowship support
from the National Physical Science Consortium and Na-
tional Security Agency.

References

[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghe-
mawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia,
Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man´e,
D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M.,
Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
Vanhoucke, V., Vasudevan, V., Vi´egas, F., Vinyals, O., Warden,
P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X., 2015. Tensor-
Flow: Large-scale machine learning on heterogeneous systems.
Software available from tensorﬂow.org.
URL http://tensorflow.org/

[2] Adalbert, R., Nogradi, A., Babetto, E., Janeckova, L., Walker,
S. A., Kerschensteiner, M., Misgeld, T., Coleman, M. P., 2009.
Severely dystrophic axons at amyloid plaques remain continuous
and connected to viable cell bodies. BRAIN 132, 402–416.
[3] Adams, J. H., Jennett, B., Murray, L. S., Teasdale, G. M.,
Gennarelli, T. A., Graham, D. I., 2011. Neuropathological ﬁnd-
ings in disabled survivors of a head injury. Journal of Neuro-
trauma 28, 701–709.

11

[4] Baldi, P., Hornik, K., 1989. Neural networks and principal com-
ponent analysis: Learning from examples without local minima.
Neural networks 2, 53–58.

[5] Barnes, D. E., Kaup, A., Kirby, K., Byers, A. L., R.Diaz-
Arrastia, Yaﬀe, K., 2014. Traumatic brain injury and risk of
dementia in older veterans. Neurology 83, 312–319.

[6] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., et al.,
2007. Greedy layer-wise training of deep networks. Advances in
neural information processing systems 19, 153.

[7] Bishop, C. M., 1995. Neural networks for pattern recognition.

Oxford university press.

[8] Blumbergs, P., Scott, G., Manavis, J., Wainwright, H., Simp-
son, D., McLean, A., 1995. Topography of axonal injury as
deﬁned by amyloid precursor protein and the sector scoring
method in mild and severe closed head injury. Journal of Neu-
rotrauma 12, 565–572.

[9] Browne, K. D., Chen, X. H., Meaney, D. F., Smith, D. H., 2011.
Mild traumatic brain injury and diﬀuse axonal injury in swine.
Journal of Neurotrauma 28 (9), 1747–1755.

[10] Bullmore, E., Sporns, O., 2012. The economy of brain network
organization. Nature Reviews Neuroscience 13, 336–349.
[11] Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A., 2014.
Return of the devil in the details: Delving deep into convolu-
tional nets. In: British Machine Vision Conference.

[12] Chen, Y. C., Smith, D. H., Meaney, D., 2009. In-vitro ap-
proaches for studying blast-induced traumatic brain injury.
Journal of Neurotrauma 26 (6), 861–876.

[13] Chklovskii, D. B., Koulakov, A. A., 2004. Maps in the brain:
what can we learn from them? Annual Reviews in Neuroscience
27, 369–392.

[14] Choromanska, A., Henaﬀ, M., Mathieu, M., Arous, G. B., Le-
Cun, Y., 2015. The loss surfaces of multilayer networks. In:
AISTATS.

[15] Christman, C., Grady, M., Walker, S., Hol-Loway, K.,
Povlishock, J., 1994. Ultra-structural studies of diﬀuse axonal
injury in humans. Journal of Neurotrauma 11, 173–186.

[16] Coleman, M., 2005. Axon degeneration mechanisms: common-
ality amid diversity. Nature Reviews Neuroscience 6 (11), 889–
898.

[17] Daianu, M., Jacobs, R. E., Town, T., Thompson, P. M., 2016.
Axonal diameter and density estimated with 7-tesla hybrid dif-
fusion imaging in transgenic alzheimer rats. SPIE Proceedings
9784, 1–6.

[18] del Razo, M. J., Morofuji, Y., Meabon, J. S., Huber, B. R.,
Peskind, E. R., Banks, W. A., Mourad, P. D., LeVeque, R. J.,
Cook, D. G., 2016. Computational and in vitro studies of blast-
induced blood-brain barrier disruption. SIAM Journal on Sci-
entiﬁc Computing 38 (3), 347–374.

[19] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.,
2009. Imagenet: A large-scale hierarchical image database. In:
Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on. IEEE, pp. 248–255.

[20] Dikranian, K., Cohen, R., Donald, C. M., Pan, Y., Brakeﬁeld,
D., Bayly, P., Parsadanian, A., 2008. Mild traumatic brain in-
jury to the infant mouse causes robust white matter axonal de-
generation which precedes apoptotic death of cortical and tha-
lamic neurons. Experimental Neurology 211, 551–560.

[21] Edlow, B. L., Copen, W. A., Izzy, S., van der Kouwe, A., Glenn,
M. B., Greenberg, S. M., Greer, D. M., Wu, O., 2016. Longi-
tudinal diﬀusion tensor imaging detects recovery of fractional
anisotropy within traumatic axonal injury lesions. Neurocriti-
cal Care 24 (3), 342–352.

[22] Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent,
P., Bengio, S., 2010. Why does unsupervised pre-training help
deep learning? Journal of Machine Learning Research 11 (Feb),
625–660.

[23] Fainaru-Wada, M., Fainaru, S., 2013. League of denial: The nﬂ,
concussions, and the battle for truth. Crown Archetype.
[24] Faul, M., Xu, L., Wald, M. M., Coronado, V. G., 2010. Trau-
matic brain injury in the united states: emergency department
visits, hospitalizations, and deaths. Atlanta (GA): Centers for

Disease Control and Prevention, National Center for Injury Pre-
vention and Control.

[25] Fayanz, I., Tator, C. H., 2000. Modeling axonal injury in vitro:
injury and regeneration following acute neuritic trauma. Journal
of Neuroscience Methods 102, 69–79.

[26] Friese, M. A., Schattling, B., Fugger, L., 2014. Mechanisms of
neurodegeneration and axonal dysfunction in multiple sclerosis.
Nature Reviews Neurology 10, 225–238.

[27] Fukushima, F., 1980. A self-organizing neural network model
for a mechanism of pattern recognition unaﬀected by shift in
position. Biological Cybernetic 36, 193–202.

[28] Galvin, J. E., Uryu, K., Lee, V. M., Trojanowski, J. Q., 1999.
Axon pathology in parkinson’s disease and lewy body dementia
hippocampus contains α-, β-, and γ -synuclein. Proceedings of
National Academy of Science 96, 13450–13455.

[29] Grady, M., Mclaughlin, M., Christman, C., Valadaka, A.,
Flinger, C., Povlishock, J., 1993. The use of antibodies against
neuroﬁlament subunits for the detection of diﬀuse axonal in-
jury in humans. Journal of Neuropathology and Experimental
Neurology 52, 143–152.

[30] Hanell, A., Greer, J. E., McGinn, M. J., Povlishock, J. T., 2015.
Traumatic brain injury induced axonal phenotypes react diﬀer-
ently to treatment. Acta Neuropathologica 129, 317–332.
[31] He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep residual learning

for image recognition. arXiv preprint arXiv:1512.03385.

[32] Hellman, A. N., Vahidi, B., Kim, H. J., Mismar, W., Stew-
ard, O., Jeonde, N. L., Venugopalan, V., 2010. Examination of
axonal injury and regeneration in micropatterned neuronal cul-
ture using pulsed laser microbeam dissection. Lab on a Chip 16,
20832092.

[33] Hemphill, M., Dabiri, B., Gabriele, S., Kerscher, L., Franck, C.,
Goss, J., Alford, P., Parker, K., 2011. A possible role for integrin
signaling in diﬀuse axonal injury. PLos ONE 6 (7), e22899.
[34] Hemphill, M., Dauth, S., Yu, C. J., Dabiri, B., Parker, K., 2015.
Traumatic brain injury and the neuronal microenvironment: A
potential role for neuropathological mechanotransduction. Neu-
ron 86 (6), 1177–1192.

[35] Henninger, N., Bouley, J., Sikoglu, E. M., An, J., Moore, C. M.,
King, J. A., Bowser, R., Freeman, M. R., Jr, R. H. B., 2016.
Attenuated traumatic axonal injury and improved functional
outcome after traumatic brain injury in mice lacking sarm1.
BRAIN, 1–12.

[36] Herwerth, M., Kalluri, S. R., Srivastava, R., Kleele, T., Kenet,
S., Illes, Z., Merkler, D., Bennett, J. L., Misgeld, T., Hemmer,
B., 2016. In vivo imaging reveals rapid astrocyte depletion and
axon damage in a model of neuromyelitis optica-related pathol-
ogy. Annals of Neurology 79, 794–805.

[37] Hill, C. S., Coleman, M. P., Menon, D. K., 2016. Traumatic
injury: mechanisms and translational opportunities.

axonal
Trends in Neuroscience 39 (5), 311–324.

[38] Hinton, G. E., Osindero, S., Teh, Y.-W., 2006. A fast learning
algorithm for deep belief nets. Neural computation 18 (7), 1527–
1554.

[39] Hornik, K., Stinchcombe, M., White, H., 1989. Multilayer feed-
forward networks are universal approximators. Neural networks
2 (5), 359–366.

[40] Huang, G. B., Ramesh, M., Berg, T., Learned-Miller, E., 2007.
Labeled faces in the wild: A database for studying face recog-
nition in unconstrained environments. Tech. rep., Technical Re-
port 07-49, University of Massachusetts, Amherst.

[41] Hubel, D. H., Wiesel, T. N., 1962. Receptive ﬁelds, binocular
interaction and functional architecture in the cat’s visual cortex.
Journal of Physiology 160, 106–154.

[42] Ioﬀe, S., Szegedy, C., 2015. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. arXiv
preprint arXiv:1502.03167.

[43] Johnson, V. E., Stewart, W., Smith, D. H., 2010. Traumatic
brain injury and amyloid-β pathology: a link to alzheimer’s
disease? Nature Reviews Neuroscience 11, 361–370.

[44] Johnson, V. E., Stewart, W., Smith, D. H., 2012. Widespread
tau and amyloid-beta pathology many years after a single trau-

matic brain injury in humans. Brain Pathology 22, 142–149.

[45] Johnson, V. E., Stewart, W., Smith, D. H., 2013. Axonal pathol-
ogy in traumatic brain injury. Experimental Neurology 246, 35–
43.

[46] Jorge, R. E., Acion, L., White, T., Tordesillas-Gutierrez, D.,
Pierson, R., Crespo-Facorro, B., Magnotta, V., 2012. White
matter abnormalities in veterans with mild traumatic brain in-
jury. American Journal of Psychiatry 169 (12), 1284–1291.
[47] Jorm, A. F., Jolley, D., 1998. The incidence of dementia: a meta

analysis. Neurology 51, 728–733.

[48] Jurevic, S., 2011. Green pepper.

[Online; accessed June 1,

2016].
URL
5572058462

https://www.flickr.com/photos/stuffedpeppers/

[49] Kaiser, M., Hilgetag, C. C., 2006. Nonoptimal component place-
ment, but short processing paths, due to long- distance projec-
tions in neural systems. PLoS Computational Biology 2 (e95).
[50] Karlsson, P., Haroutounian, S., Polydefkis, M., Nyengaard,
J. R., Jensen, T. S., 2016. Structural and functional character-
ization of nerve ﬁbres in polyneuropathy and healthy subjects.
Scandinavian Journal of Pain 10, 28–35.

[51] Kinnunen, K. M., Greenwood, R., Powell, J. H., Leech, R.,
Hawkins, P. C., Bonnelle, V., Patel, M. C., Counsell, S. J.,
Sharp, D. J., 2010. White matter damage and cognitive impair-
ment after traumatic brain injury. Brain, 1–15.

[52] Krizhevsky, A., Sutskever, I., Hinton, G. E., 2012. Imagenet
classiﬁcation with deep convolutional neural networks. In: Ad-
vances in neural information processing systems. pp. 1097–1105.
[53] Krstic, D., Knuesel, I., 2012. Deciphering the mechanism under-
lying late-onset alzheimer disease. Nature Reviews Neuroscience
9 (1), 25–34.

[54] Laughlin, S. B., Sejnowski, T., 2003. Communication in neu-

ronal networks. Science 301, 1870–1874.

[55] Laukka, J. J., Kamholz, J., Bessert, D., 2016. Novel pathologic
ﬁndings in patients with pelizaeus-merzbacher disease. Neuro-
science Letters.

[56] Lauria, G., Morbin, M., Lombardi, R., Borgna, M., Mazzoleni,
G., Sghirlanzoni, A., Pareyson, D., 2003. Axonal swellings pre-
dict the degeneration of epidermal nerve ﬁbers in painful neu-
ropathies. Neurology 61, 631–636.

[57] LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature

[58] LeCun, Y., Cortes, C., Burges, C. J., 1998. The mnist database

521 (7553), 436–444.

of handwritten digits.

[59] LoBue, C., Denney, D., Hynan, L. S., Rossetti, H. C., Lacritz,
L. H., Jr., J. H., Womack, K. B., Woon, F. L., Cullum, C. M.,
2016. Self-reported traumatic brain injury and mild cognitive
impairment: increased risk and earlier age of diagnosis. Journal
of Alzheimer’s Disease 51, 727–736.

[60] Louis, E. D., Faust, P. L., Vonsattel, J., Honig, L. S., Rajput,
A., Rajput, A., Pahwa, R., Lyons, K. E., Ross, G. W., Elble,
R. J., Erickson-Davis, C., Moskowitz, C. B., Lawton, A., 2009.
Torpedoes in parkinson’s disease, alzheimer’s disease, essential
tremor, and control brains. Movement Disorders 24 (11), 1600–
1605.

[61] Magdesian, M. H., Sanchez, F., Lopez, M., Thostrup, P.,
Durisic, N., Belkaid, W., Liazoghli, D., Gr¨utter, P., Colman,
R., 2012. Atomic force microscopy reveals important diﬀerences
in axonal resistance to injury. Biophysical Journal 103 (3), 405–
414.

[62] Maia, P. D., 2014. Mathematical modeling of focal axonal
swellings arising in traumatic brain injuries and neurodegen-
erative diseases. Ph.D. thesis, University of Washington.
[63] Maia, P. D., Hemphill, M. A., Zehnder, B., Zhang, C., Parker,
K. K., Kutz, J. N., 2015. Diagnostic tools for evaluating the
impact of focal axonal swellings arising in neurodegenerative
diseases and/or traumatic brain injury. Journal of Neuroscience
Methods 253, 233–243.

[64] Maia, P. D., Kutz, J. N., 2014. Compromised axonal functional-
ity after neurodegeneration, concussion and/or traumatic brain
injury. Journal of Computational Neuroscience 27, 317–332.

12

[85] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov,
D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going
deeper with convolutions. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition. pp. 1–9.

[86] Tagliaferro, P., Burke, R. E., 2016. Retrograde axonal degen-
eration in parkinson disease. Journal of Parkinson’s Disease 6,
1–15.

[87] Tang-Schomer, M. D., Johnson, V. E., Baas, P. W., Stewart,
W., Smith, D. H., 2012. Partial interruption of axonal trans-
port due to microtubule breakage accounts for the formation of
periodic varicosities after traumatic axonal injury. Experimental
Neurology 233, 364–372.

[88] Tang-Schomer, M. D., Patel, A., Bass, P. W., Smith, D. H.,
2010. Mechanical breaking of microtubules in axons during dy-
namic stretch injury underlies delayed elasticity, microtubule
disassembly, and axon degeneration. The FASEB Journal 24 (5),
1401–1410.

[89] Thies, W., Bleiler, L., 2013. Alzheimer’s disease facts and ﬁg-

ures. Alzheimer’s & Dementia 9 (2), 208–245.

[90] Trapp, B. D., Nave, K.-A., 2008. Multiple sclerosis: An immune
or neurodegenerative disorder? Annual Review Neuroscience
31 (1), 247–269.

[91] Tsai, J., Grutzendler, J., Duﬀ, K., Gan, W. B., 2004. Fibrillar
amyloid deposition leads to local synaptic abnormalities and
breakage of neuronal branches. Nature Neuroscience 7, 1181–
1183.

[92] Vedaldi, A., Lenc, K., 2015. Matconvnet – convolutional neural
networks for matlab. In: Proceeding of the ACM Int. Conf. on
Multimedia.

[93] Wang, J., Hamm, R. J., Povlishock, J. T., 2011. Traumatic
axonal injury in the optic nerve: evidence for axonal swelling,
disconnection, dieback and reorganization. Journal of Neuro-
trauma, 28 (7), 1185–1198.

[65] Maia, P. D., Kutz, J. N., 2014. Identifying critical regions for
spike propagation in axon segments. Journal of Computational
Neuroscience 36 (2), 141–155.

[66] Maxwell, W. L., Povlishock, J. T., Graham, D. L., 1997. A
mechanistic analysis of nondisruptive axonal injury: A review.
Journal of Neurotrauma 17 (7), 419–440.

[67] Menon, D. K., Maas, A. I. R., 2015. Progress, failures and new
approaches for tbi research. Nature Reviews Neuroloy 11, 71–72.
[68] Millecamps, S., Julien, J., 2013. Axonal transport deﬁcits
and neurodegenerative diseases. Nature Reviews Neuroscience
14 (161), 161–176.

[69] Morrison, B., Elkin, B. S., Dolle, J. P., Yarmush, M. L., 2011.
In vitro models of traumatic brain injury. Annual Reviews in
Biomedical Engineering 13 (1), 91–126.

[70] Nikic, I., Merkler, D., Sorbara, C., Brinkoetter, M., Kreutzfeld,
M., Bareyre, F., Bruck, W., Bishop, D., Misgeld, T., Kerschen-
steiner, M., 2011. A reversible form of axon damage in exper-
imental autoimmune encephalomyelitis and multiple sclerosis.
Nature Medicine 17 (4), 495–499.

[71] Niyogi, P., Girosi, F., Poggio, T., 1998. Incorporating prior
information in machine learning by creating virtual examples.
Proceedings of the IEEE 86 (11), 2196–2209.

[72] Parkhi, O. M., Vedaldi, A., Zisserman, A., September 2015.
Deep face recognition. In: Xianghua Xie, M. W. J., Tam, G.
K. L. (Eds.), Proceedings of the British Machine Vision Con-
ference (BMVC). BMVA Press, pp. 41.1–41.12.
URL https://dx.doi.org/10.5244/C.29.41

[73] Patterson, B. W., Elbert, D. L., Mawuenyega, K. G., Kasten,
T., Ovod, V., Ma, S., Xiong, C., Chott, R., Yarasheski, K.,
Sigurdson, W., Zhang, L., Goate, A., Benzinger, T., Morris,
J. C., Holtzman, D., Bateman, R. J., 2015. Age and amyloid
eﬀects on human central nervous system amyloid-beta kinetics.
American Neurological Association 78 (3), 439–453.

[74] Poggio, T., 2016. Deep learning: mathematics and neuroscience.
Views & Reviews, McGovern Center for Brains, Minds and Ma-
chines, 1–7.

[75] Povlishock, J. T., Katz, D. I., 2005. Update of neuropathology
and neurological recovery after traumatic brain injury. Journal
of Head Trauma Rehabilitation 20 (1), 76–94.

[76] Qiu, C., Kivipelto, M., von Strauss, E., 2009. Epidemiology of
alzheimer’s disease: occurrence, determinants, and strategies
toward intervention. Dialogues in Clinical Neuroscience 11 (2),
111–128.

[77] Roozenbeek, B., Maas, A. I. R., Menon, D. K., 2013. Changing
patterns in the epidemiology of traumatic brain injury. Nature
Reviews Neurology 9, 231–236.

[78] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M.,
Berg, A. C., Fei-Fei, L., 2015. ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer Vi-
sion (IJCV) 115 (3), 211–252.

[79] Sanger, T. D., 1989. Optimal unsupervised learning in a single-
layer linear feedforward neural network. Neural networks 2 (6),
459–473.

[80] Sharp, D. J., Scott, G., Leech, R., 2014. Network dysfunction
after traumatic brain injury. Nature Reviews Neurology 10, 156–
166.

[81] Skandsen, T., Kvistad, K. A., Solheim, O., Strand, I. H., Folvik,
M., Vik, A., 2010. Prevalence and impact of diﬀuse axonal in-
jury in patients with moderate and severe head injury: a cohort
study of early magnetic resonance imaging ﬁndings and 1-year
outcome. Journal of Neurosurgery 113 (3), 556–563.

[82] Smith, D., Wolf, J., Lusardi, T., Lee, V., Meaney, D., 1999.
High tolerance and delayed elastic response of cultured axons
to dynamic stretch injury. The Journal of Neuroscience 19 (11),
4263–4269.

[83] Sporn, O., 2011. Networks of the brain. MIT Press.
[84] Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I.,
Salakhutdinov, R., 2014. Dropout: a simple way to prevent
neural networks from overﬁtting. Journal of Machine Learning
Research 15 (1), 1929–1958.

13

