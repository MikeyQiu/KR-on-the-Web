A Deep Multimodal Approach for Cold-start
Music Recommendation

Sergio Oramas
Music Technology Group
Universitat Pompeu Fabra
sergio.oramas@upf.edu

Mohamed Sordo
Pandora Media, Inc.
Oakland, CA
msordo@pandora.com

Oriol Nieto
Pandora Media, Inc.
Oakland, CA
onieto@pandora.com

Xavier Serra
Music Technology Group
Universitat Pompeu Fabra
xavier.serra@upf.edu

7
1
0
2
 
l
u
J
 
4
2
 
 
]

R

I
.
s
c
[
 
 
2
v
9
3
7
9
0
.
6
0
7
1
:
v
i
X
r
a

ABSTRACT
An increasing amount of digital music is being published daily.
Music streaming services often ingest all available music, but this
poses a challenge: how to recommend new artists for which prior
knowledge is scarce? In this work we aim to address this so-called
cold-start problem by combining text and audio information with
user feedback data using deep network architectures. Our method
is divided into three steps. First, artist embeddings are learned from
biographies by combining semantics, text features, and aggregated
usage data. Second, track embeddings are learned from the audio
signal and available feedback data. Finally, artist and track em-
beddings are combined in a multimodal network. Results suggest
that both splitting the recommendation problem between feature
levels (i.e., artist metadata and audio track), and merging feature
embeddings in a multimodal approach improve the accuracy of the
recommendations.

CCS CONCEPTS
•Information systems → Recommender systems; •Computing
methodologies → Neural networks;

KEYWORDS
recommender systems, deep learning, multimodal, music, semantics

ACM Reference format:
Sergio Oramas, Oriol Nieto, Mohamed Sordo, and Xavier Serra. 2017. A
Deep Multimodal Approach for Cold-start
Music Recommendation. In Proceedings of DLRS 2017, Como, Italy, August
27, 2017, 6 pages.
DOI: 10.1145/3125486.3125492

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
DLRS 2017, Como, Italy
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5353-3/17/08. . . $15.00
DOI: 10.1145/3125486.3125492

1 INTRODUCTION
It is common for online music streaming services nowadays to
offer ever-growing catalogs with dozens of millions of music tracks.
Since manually managing these large libraries is not feasible due
to size constraints, automatic exploration and exploitation of large-
scale music collections has been an active area of research in recent
years [31]. While several existing algorithmic techniques are able
to produce successful recommendations for popular content [17],
the exploration of new or undiscovered artists (i.e., the long tail [6])
remains a major challenge that we aim to address.

Recommender systems can be broadly classified into collabora-
tive filtering (CF), content-based, and hybrid methods. CF methods
[17] use the item-user feedback matrix and predictions are based
on the similarity of user or item profiles. Matrix factorization tech-
niques are currently CF state-of-the-art [17]. CF methods suffer
from the cold-start problem, as new items do not have feedback
information [30]. Content-based methods [22] rely only on item
features, and recommendations are based on similarity between
such features. Finally, hybrid methods [5] try to combine both item
content and item-user feedback.

Social tags have been extensively used as a source of artist con-
tent features to recommend music [16]. However, these tags are
usually collectively annotated, which often introduce an artist pop-
ularity bias [35]. Artist biographies and press releases, on the other
hand, do not necessarily require a collaborative effort, as they may
be produced by artists themselves. However, they have seldom been
exploited for music recommendation [28]. Part of this work focuses
on learning artist features from these biographies. Furthermore,
we also make use of audio signals, since these are generally always
available and have shown to be helpful when recommending music
in the long tail [36].

According to [11], composing simpler tasks is more likely to
yield effective local minima for neural networks. In addition, as
stated in [18], directly training all the layers of a deep network
together make it difficult to exploit all the extra modeling power
of a deeper architecture. Therefore, we decided to separate the
problem of music recommendation into artist and song levels. Artist
feature embeddings are learned from artist metadata in an artist
recommendation scenario. Track feature embeddings are learned
from audio signals in a song recommendation scenario. In both
cases, a hybrid recommendation approach is used based on learning
attribute-to-feature mappings [10]. This method addresses the lack

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

of feedback for uncommon items in two steps: (1) factorizing the
collaborative matrix, and (2) learning a mapping between item
content features and item latent factors [1, 36]. Lastly, both feature
embeddings are combined in a multimodal network to predict song
recommendations of cold-start artists. We show how dividing the
problem into artists and songs, and combining text and audio in a
multimodal approach yields improved recommendations.

For the sake of reproducibility, source code and data splits used
in the experiments have been released1. Our main contributions in
this work are summarized as follows:

• Method to enrich artist biographies with semantic infor-

mation leveraging an external Knowledge Base.

• Dividing the problem of music recommendation into artist

and song levels to obtain better performance.

• A multimodal deep learning pipeline for music recommen-
dation that combines audio with text and yields improved
results.

• The release of an extended version of the Million Song

Dataset with artist biographies and tags.

2 RECOMMENDATION APPROACH
To produce cold-start music recommendations, we propose the
following framework. Given the set of artist features As of a song
s, and the set of track features Ts of s, the complete feature set
of s is defined as the aggregation of its artist and track features
Fs = As ∪ Ts .

Given the heterogeneity of these two feature sets (audio and
text), a learning process involving them may under-explore one of
the modalities, as the stronger modality may dominate quickly. To
ensure that the variability of the input data is fully represented, we
divide the problem into three phases (see Figure 1). First, we aggre-
gate the collaborative information of all songs of the same artist,
and learn an artist feature embedding A(cid:48)
from As in an artist rec-
s
ommendation scenario. Second, we learn a track feature embedding
T (cid:48)
from Ts in a pure audio-based recommendation scenario. Third,
s
we combine both feature embeddings A(cid:48)
in a multimodal
s
network and compute song recommendations.

and T (cid:48)
s

Since songs from the same artist share the same set As , if different
songs from the same artist appear in multiple sets (e.g., train and
test), a problem of overfitting may arise [9]. To approach this issue,
we use non-overlapping artists across the train, validation, and test
sets.

Let M be the matrix of implicit feedback, where mus is the num-
ber of play counts for user u on song s. M is split into Mt r ain ,
and Mt est , for train, validation and test, respectively, where
Mval
no artist is shared across sets. Factorizing Mt r ain using weighted
matrix factorization (WMF) [12] yields Ik
, the k dimensional
sets of song and user latent factors, respectively. We set k = 200,
and apply the alternating least squares (ALS) optimization method.
To learn the artist embeddings, we obtain the matrix of artist
implicit feedback R from M, being Rua = (cid:205)
s mus for all songs s
from the same artist a. This matrix is split into train, validation,
and test sets following the same partition of artists made for M,
and thus keeping the mutual exclusion restriction. Latent factors
of artists and users are later obtained via WMF. Lastly, a deep

and Uk

Figure 1: Model architecture.

neural network is trained on the prediction of artist latent factors
from artist content features A. On the other hand, the song latent
factors are predicted with a deep convolutional network, using Ik
as training data and the track features T as input (similar to [36]).
Once the artist and track models are trained and optimized, we
gather the activations from the penultimate layer of each network
for all the sets. These activations constitute what we call the artist
and track feature embeddings A(cid:48)
, which are in turn used as
s
input to a third network. This final multimodal network is trained
on the prediction of song latent factors Ik
. Finally,
the list of item recommendations for user u is obtained by ranking
the results of computing the dot product between the user latent
factor fu ∈ Uk

and the set of item factors.

from S (cid:48)
s

and T (cid:48)
s

The different architectures used in each one of the three neu-
ral networks involved in the approach are described in Sections
3 ,4, and 5, respectively. Nevertheless, all networks have a final
fully connected layer of 200 units2 with linear activation and l2-
normalization. In addition, mini batches of 32 items are randomly
sampled from the training data to compute the gradient in all net-
works, and Adam [15] is the optimizer used to train the models,
with the default suggested learning parameters. Given that the out-
put of the architectures are l2-normalized, we use cosine proximity
as the loss function, as in [7].

s ∪T (cid:48)
s

= A(cid:48)

3 ARTIST TEXT EMBEDDINGS
In this section we describe two different, competing approaches to
exploit artist texts in a deep learning process.

3.1 Semantic enrichment
We propose a method for enriching artist biographies by associat-
ing text fragments with relevant entities defined in online semantic
datasets, and then gathering relevant information about these en-
tities from a Knowledge Base (KB). For this purpose, we adopted
Babelfy, a state-of-the-art tool for Entity Linking (EL) [23], a task
to associate, for a given textual fragment candidate, the most suit-
able entry in a reference KB. Babelfy maps words from a given

1https://github.com/sergiooramas/tartarus

2to match the dimensions of the factors to be predicted.

A Deep Multimodal Approach for Cold-start
Music Recommendation

text to entities in the BabelNet Knowledge Base [24], which has a
direct mapping to DBpeadia3, a structured version of Wikipedia4.
We use semantic information about the identified entities coming
from DBpedia to enrich the biographies. DBpedia resources are
generally classified using the DBpedia Ontology, which is a shallow,
cross-domain ontology based on the most common infoboxes of
Wikipedia.

EL systems are useful for music recommendation [28]. However,
they are not optimized for the music domain, and are prone to
errors [27]. The application of a filtering process over the set of
identified entities based on their classification within the DBpedia
Ontology, has demonstrated its utility to improve music retrieval
tasks, such as artist similarity [29]. Therefore, we only keep entities
of classes related to the music domain such as MusicalArtist, Band,
MusicGenre, MusicalWork, RecordLabel, Instrument, Engineer, and
Place. Then, we query DBpedia to get all the available information
about the filtered entities. From the information gathered, we
keep some specific properties for every kind of entity, such as
homeTown, instrument, genre or associatedBand for MusicalArtists,
writer, producer or recordedIn for MusicalWorks, stylisticOrigin or
instrument for MusicGenres, and so on5. In addition, we also kept
all the Wikipedia categories associated to each entity. In Wikipedia,
categories are used to organize resources, and they help users to
group articles of the same subject.

To build the enriched biographies we proceed as follows: First,
Babelfy is applied over the biography texts. Second, information
is gathered from DBpedia for the entities of the selected classes.
Finally, the collected data are added at the end of the biography text
separated by spaces. A vector space model (VSM) is then applied
to the set of enriched biographies, and tf-idf weighting [37] is used.
We limited the vocabulary size to 10,000 terms for the VSM, as
this number provides a good trade-off between performance and
number of parameters required for training. Note that either words,
entities, dates, or categories may be part of this vocabulary. From
this data representation, a feedforward network with two dense
layers of 2048 neurons each is trained to predict the artist latent
factors.

3.2 Word embeddings
Much of the work with deep learning in Natural Language Pro-
cessing has involved the learning of word vector representations
[3, 21], and their further composition [8]. Word embeddings aim
to represent words as low-dimensional dense vectors. They have
demonstrated to greatly benefit NLP tasks, such as word similarity,
sentiment analysis, or parsing [26].

The use of convolutional neural networks (CNN) over pre-trained
word vectors has become state-of-the-art in sentence classification
[14]. We re-adapt the architecture proposed in [14] for sentence
classification to learn artist latent factors from artist biographies.
This consists in an embedding layer, followed by a one dimensional
convolutional layer with multiple filter widths, a max-over-time
pooling layer, a dense hidden layer and the output layer. We employ
the same architecture and parameters, changing only the output

3http://dbpedia.org
4http://wikipedia.org
5The
list
complete
http://mtg.upf.edu/download/datasets/msd-a

classes

of

and

properties

is

available

at

DLRS 2017, August 27, 2017, Como, Italy

layer and the loss function. We initialize the input embedding layer
of the network with word2vec word embeddings pre-trained on
the Google News dataset, and also with word embeddings trained
in our own corpus of biographies.

4 TRACK AUDIO EMBEDDINGS
It is common in the field of music informatics to make use of CNNs
to learn higher-level features from spectrograms. These representa-
tions are typically contained in RF×N matrices with F frequency
bins and N time frames. In this work we compute 96 frequency
bin, log-compressed constant-Q transforms (CQT) [32] for all the
tracks in our dataset using librosa [20] with the following param-
eters: audio sampling rate at 22050 Hz, hop length of 1024 samples,
Hann analysis window, and 12 bins per octave. Following a sim-
ilar approach to [36], we address the variability of the length N
across songs by sampling one 15-seconds long patch from each
track, resulting in the fixed-size input to the CNN.

The deep model trained with these data is defined as follows:
the CQT patches are fed to four convolutional layers with rectified
linear units (ReLU) as activations. The four convolutions have the
following number of filters, from first to last: 256, 512, 1024, and
1024. The convolutions are only applied to the time axis, leaving
the frequencies fixed since the absolute and relative bin placement
is important when aiming to capture particular sounds (as opposed
to the irrelevance of where in time a certain sonic event occurs).
Maxpooling of 4 units across the time axis is applied after each of
the first three ReLUs, and 50% dropout is applied to all layers. The
flattened output of the last layer has 4096 units, which becomes the
vector embedding to later use in the multimodal approach described
next.

5 MULTIMODAL FUSION
There are several approaches in the literature for multimodal fea-
ture learning [25, 34], and late fusion of multimodal feature vectors
[2, 33]. In this work, audio and text feature vectors are learned sep-
arately and then combined via late fusion in a multimodal network
(see Figure 1).

Given the different nature of the artist and track embeddings,
a normalization step is necessary. Normalized feature vectors are
then fed to a feed forward neural network (a simple Multi Layer
Perceptron, MLP). Two different architectures were explored: (i)
each embedding vector is connected to an isolated dense layer of
512 hidden units with ReLU activations after a process of batch
normalization [13]. Then, both dense layers are connected to the
output layer. The rationale behind this is that the isolated dense
layers help the network learn non-linearities from each modality
separately. (ii) each embedding vector is l2-normed and then con-
catenated into a single feature vector which is directly connected
to the output layer, resulting in a linear model. Regularization is
obtained by applying dropout with an empirically selected factor
of 70% after the input layer for both architectures.

6 EXPERIMENTS
6.1 Dataset
The Million Song Dataset (MSD) [19] is a collection of metadata
and precomputed audio features for 1 million songs. Along with

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

this dataset, the Echo Nest Taste Profile Subset [4] provides play
counts of 1 million users on more than 380,000 songs from the MSD.
Starting from this subset, we gather biographies and social tags
from Last.fm for all the artists that have at least one song in the
dataset. When there are several artists with the same name, they
are stored in the same page of Last.fm, which makes the biography
and social tags ambiguous. We automatically removed all ambigu-
ous artists by applying text processing on the biographies. The
song features provided with the MSD are not generally suitable
for deep learning, so we instead use audio previews between 7 and
30 seconds retrieved from 7digital.com. After removing ambigu-
ous artists and missing tracks, the final dataset consists of 328,821
tracks from 24,043 artists. Each track has at least 15 seconds of
audio, each biography is at least 50 characters long, and each artist
has at least 1 tag associated with it. All artist metadata, implicit
feedback matrices, and splits are released as a new dataset called
the MSD-A6.

6.2 Artist Recommendation
To investigate to what extent the different feature sets, data models
and architectures influence the quality of the deep artist features,
we evaluate the different approaches in an artist recommendation
scenario. Given the matrix of implicit feedback R, and the set of
artist and user factors obtained through matrix factorization (see
Section 2), we predict the artist factors for the test set, and use
them to compute a ranked list of recommended artists for every
user. We use mean average precision (MAP) with a cut-off at 500
recommendations per user.

We compare four different approaches using the biography texts
as input. (1) a pure text-based approach using a VSM and a feed-
forward network a-TEXT. (2) similar to (1) but with a semantically
enriched version of the texts a-sem (cf. Section 3.1). (3) An ap-
proach based on word embeddings initialized with vectors trained
on Google News and a CNN a-w2v-goo (cf. Section 3.2). (4) Similar
to (3) but inizializing the embeddings with word vectors previously
trained on the corpus of biographies a-w2v. To properly frame the
results, we compute two baselines and one competitor approach.
The tags baseline approach uses artist social tags as input features,
and text-rf uses biography texts as input, but Random Forest
Regression for the learning instead of a deep neural network. The
former baseline is added to compare the potential of biography
texts with respect to curated metadata, whilst the latter was added
to study to which extent the deep network improves the results
over other learning methods typically used in natural language
processing. There are few recommendation approaches able to
deal with an extreme cold-start scenario like ours. Therefore, we
select ItemAttributeKnn [10] as the competitor approach (tags-
itemKnn), using artist social tags as attribute data and computed
using the MyMediaLite library7. We also show the scores achieved
when the latent factor vectors are randomized (random), and when
they are learned from feedback data using matrix factorization
(upper-bound).

6https://doi.org/10.5281/zenodo.831348
7http://www.mymedialite.net/

Table 1: Artist Recommendation Results

Aproach

Input

Data model

Arch MAP

a-text
a-sem
a-w2v-goo
a-w2v

a-tags
tags-itemKnn
text-rf

random
upper-bound

Bio
Sem Bio
Bio
Bio

VSM
VSM
w2v-pretrain
w2v-trained

FF
FF
CNN
CNN

Tags
Tags
Bio

-
-

VSM
-
VSM

-
-

FF
itemKnn
RF

-
-

0.0161
0.0201
0.0119
0.0145

0.0314
0.0161
0.0089

0.0014
0.5528

Mean average precision (MAP) at 500 for the predictions of artist recommen-
dations in 1M users. VSM refers to Vector Space Model, FF to Feedforward,
RF to Random Forest, CNN to Convolutional Neural Network, and itemKnn
to itemAttributeKnn approach. Bio refers to biography texts and Sem Bio to
semantically enriched texts.

Results reported in Table 1 show that the semantic enrichment
of the biographies a-sem outperforms the pure text approach a-
text. As expected, the use of tags improves the results over the
use of text. However, the addition of semantic features reduces the
gap in performance between the use of tags and unstructured text.
Moreover, the difference between a-text and text-rf shows that
the use of deep learning with respect to random forest improves
the results. We also note that a VSM model with a feedforward
network outperforms the use of word embeddings with convolu-
tions. Although, according to the literature, this latter approach
has demonstrated its utility for simple tasks like binary classifica-
tion with short texts, our task puts forward two challenges for this
architecture: the greater length of the input texts, and the higher
dimensionality of the output. Although we have shown that initial-
izing the embedding layer with word vectors trained on the corpus
itself (a-w2v) outperforms the use of Google News pre-trained
vectors (a-w2v-goo), further work is necessary to properly opti-
mize a convolutional architecture for this task. Finally, we observe
that our approach a-tags outperforms the competitor approach
tags-itemKnn using the same item attributes.

Once the network is trained, we predict the activations of the
penultimate layer for the entire dataset of artists. Thus, we obtain a
vector embedding of 2048 dimensions, which represents the artist
deep features A(cid:48). From the evaluated approaches, we compute the
artist embedding from the a-sem and a-tags approaches.

6.3 Song Recommendation
In this experiment, audio embeddings are obtained after training
the convolutional network (see Section 4) with 260k patches of
15 seconds, corresponding to the 80% of the tracks described in
Section 6.1. Patches are divided into training (80%), validation (10%)
and test (10%) sets. Results reported in Table 2 are computed over
the remaining 20% of tracks. As opposed to [36], no artist appears
in more than one subset to avoid overfitting. Finally, multimodal
approaches are computed on the same sets.

In our experiments, we want to measure the impact of the artist
embeddings in the song recommendation problem, and also the

A Deep Multimodal Approach for Cold-start
Music Recommendation

Table 2: Song Recommendation Results

Approach

Artist Input Track Input Arch MAP

audio
sem-vsm
sem-emb

mm-lf-lin
mm-lf-h1
mm

tags-vsm
tags-emb

random
upper-bound

-
Sem Bio
a-sem

a-sem
a-sem
Sem Bio

Tags
a-tags

rnd emb
-

audio spec
-
-

CNN 0.0015
0.0032
0.0034

FF
FF

audio emb MLP 0.0036
0.0035
audio emb
MLP
CNN 0.0014
audio spec

-
-

-
-

FF
FF

FF
-

0.0043
0.0049

0.0002
0.1649

Mean average precision (MAP) at 500 for the predictions of song recom-
mendations in 1M users. audio emb refers to the track embedding of audio
approach, sem to artist embedding of sem approach, tags to artist embedding
of tags approach, spec to spectrogram, mm to multimodal, lf to late fusion,
lin to linear, and h1 to one hidden layer.

potential of the multimodal approach. We experimented with two
approaches, sem-emb and tags-emb, that exploit the feature em-
beddings learned from the artists features (see Section 6.2), either
based on biography texts (a-sem) or artists tags (a-tags). To mea-
sure the potential of the artist embeddings, we also computed two
approaches based on the original artist features (sem-vsm for se-
mantic text features and tags-vsm for tag features). Results on
Table 2 show that sem-emb and tags-emb outperform sem-vsm
and tags-vsm, suggesting that artist embeddings outperform artist
features.

An approach based on the audio spectrograms was computed
(audio). From this latter approach, audio embeddings where ob-
tained (audio emb) and combined with a-sem in a multimodal late
fusion approach mm-lf-lin (without hidden layers and l2-norm)
and mm-lf-h1 (with one hidden layer after each feature vector and
batch normalization) (cf. Section 5). We also tried with different
combinations of hidden layers and normalization steps in the mul-
timodal network but all of them yielded lower results than the ones
reported for mm-lf-lin and mm-lf-h1. We compared this network
with a multimodal approach trained directly on the original features
(semantically enriched text and audio spectrograms). Results on the
combination of artist and track features show that the late fusion of
artist and track embeddings mm-lf-lin outperforms the simultane-
ous training of artist and track features mm. In addition, we observe
that we achieve better results when no hidden layer is added to
the multimodal network mm-lf-lin. Finally, we observe that the
multimodal approach that combines text and audio features with
late fusion mm-lf-lin improves the results of pure text sem-emb
or pure audio audio approaches. All the differences between the
approaches are statistically significant (p < 0.01) according to the
paired t-test.

We also compared the results with an upper-bound approach
obtained from the feedback data and an approach trained with
random vector embeddings. Although results are in general far
from the upper-bound, the comparative analysis of the proposed
approaches gives some insights of the behavior of different feature

DLRS 2017, August 27, 2017, Como, Italy

representations and modalities in the cold-start recommendation
problem.

7 CONCLUSIONS
In this work, a multimodal approach for song recommendation has
been presented. The approach is divided into three steps. (1) Artist
feature embeddings are learned from text and semantic features
in an artist recommendation scenario using a deep network archi-
tecture. (2) Track feature embeddings are learned from the audio
spectrograms using convolutional neural networks. (3) Embeddings
are combined in a multimodal network.

Results show that splitting the problem of song recommendation
at artist and song levels improves the quality of recommendations.
Learning artist feature embeddings separately benefits from the
aggregation of the information about the different songs of the
same artist, yielding more robust artist features. Related to this,
an approach for the semantic enrichment of artist metadata has
been proposed, leading to a significant improvement in the results.
In addition, we have shown the potential of exploiting artist bi-
ographies in music recommendation. Moreover, the deep learning
architectures of this work have demonstrated their capacity to im-
prove upon other learning models under the music recommendation
framework. Finally, we have shown how a multimodal approach,
based on the late fusion of track and artist feature embeddings
that are learned separately, outperforms end-to-end multimodal
approaches where the different modalities are learned simultane-
ously. Moreover, results have shown that our multimodal approach
achieves better results than pure text or audio approaches. As fu-
ture work, we plan to do fine-tunning of the combined model after
pre-training the network on each modality separately.

ACKNOWLEDGMENTS
This work was partially funded by the Spanish Ministry of Economy
and Competitiveness under the Maria de Maeztu Units of Excellence
Programme (MDM-2015-0502). The Tesla K40 used for this research
was donated by the NVIDIA Corporation.

REFERENCES
[1] Trapit Bansal, David Belanger, and Andrew McCallum. 2016. Ask the GRU:
Multi-task Learning for Deep Text Recommendations. RecSys (2016), 107–114.
https://doi.org/10.1145/2959100.2959180 arXiv:1609.02116

[2] M. Rouvier Bechet, S. Delecraz, B. Favre, M. Bendris, and F. 2015. Multimodal
embedding fusion for robust speaker role recognition in video broadcast. IEEE
Workshop on Automatic Speech Recognition and Understanding (ASRU) (2015).
[3] Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003.
A Neural Probabilistic Language Model. The Journal of Machine Learn-
ing Research 3 (2003), 1137–1155. https://doi.org/10.1162/153244303322533223
arXiv:arXiv:1301.3781v3

[4] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere.
2011. The Million Song Dataset. In Proceedings of the 12th International Conference
on Music Information Retrieval (ISMIR 2011).

[5] Robin Burke. 2002. Hybrid Recommender Systems: Survey and Experiments.

[6]

User Modeling and User-Adapted Interaction 12, 4 (2002), 331–370.
`Oscar Celma. 2010. The Long Tail in Recommender Systems. Springer Berlin Hei-
delberg, Berlin, Heidelberg, 87–107. https://doi.org/10.1007/978-3-642-13287-2 4
[7] Franc¸ois Chollet. 2016. Information-theoretical label embeddings for large-scale
image classification. CoRR (2016), 1–10. arXiv:1607.05691 http://arxiv.org/abs/
1607.05691

[8] Ronan Collobert,

Jason Weston, L´eon Bottou, Michael Karlen, Koray
Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost)
Journal of Machine Learning Research 12 (2011), 2493–2537.
from Scratch.
https://doi.org/10.1.1.231.4614 arXiv:1103.0398

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

[32] Christian Sch¨orkhuber and Anssi Klapuri. 2010. Constant-Q transform toolbox
for music processing. 7th Sound and Music Computing Conference JANUARY
(2010), 3–64.

[33] Olga Slizovskaia, Emilia G´omez, and Gloria Haro. 2017. Musical instrument
recognition in user-generated videos using a multimodal convolutional neural
network architecture. In ICMR ’17: Proceedings of the 2017 ACM on International
Conference on Multimedia Retrieval. Bucharest, Romania.

[34] Nitish Srivastava and Ruslan Salakhutdinov. 2012. Learning representations for
multimodal data with deep belief nets. In International conference on machine
learning workshop.

[35] Douglas Turnbull, Luke Barrington, and Gert Lanckriet. 2008. Five approaches
to collecting tags for music. Proceedings of 9th the International Society for
Music Information Retrieval Conference (2008), 225–230. http://books.google.
com/books?hl=en

[36] Aaron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep
content-based music recommendation. Electronics and Information Systems
department (ELIS) (2013), 9. https://doi.org/10.1109/MMUL.2011.34.van
Justin Zobel and Alistair Moffat. 1998. Exploring the similarity space. ACM
SIGIR Forum 32, 1 (1998), 18–34. https://doi.org/10.1145/281250.281256

[37]

[9] Arthur Flexer. 2007. A Closer Look on Artist Filters for Musical Genre Classifica-
tion. In Proceedings of the 8th International Society for Music Information Retrieval
Conference.

[10] Zeno Gantner, Lucas Drumond, Christoph Freudenthaler, Steffen Rendle, and
Lars Schmidt-Thieme. 2010. Learning Attribute-to-Feature Mappings for Cold-
Start Recommendations. In Proceedings of the 2010 IEEE International Conference
on Data Mining (ICDM ’10). IEEE Computer Society, Washington, DC, USA,
176–185.

[11] C¸a˘glar G¨ulc¸ehre and Yoshua Bengio. 2016. Knowledge matters: Importance of
prior information for optimization. Journal of Machine Learning Research 17, 8
(2016), 1–32.

[12] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative Filtering for
Implicit Feedback Datasets. In Proceedings of the 2008 Eighth IEEE International
Conference on Data Mining (ICDM ’08). 263–272.

[13] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Acceler-
ating Deep Network Training by Reducing Internal Covariate Shift. CoRR
abs/1502.03167 (2015). http://arxiv.org/abs/1502.03167

[14] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP 2014) (2014), 1746–1751. https://doi.org/10.1109/LSP.2014.
2325781 arXiv:arXiv:1408.5882v1

[15] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic
Optimization. CoRR abs/1412.6980 (2014). http://arxiv.org/abs/1412.6980
[16] Peter Knees and Markus Schedl. 2013. A survey of music similarity and
recommendation from music context data. ACM Transactions on Multime-
dia Computing, Communications, and Applications 10, 1 (2013), 1–21. https:
//doi.org/10.1145/2542205.2542206

[17] Y. Koren, R. Bell, and C. Volinsky. 2009. Matrix Factorization Techniques for
Recommender Systems. Computer 42, 8 (2009), 42–49. https://doi.org/10.1109/
MC.2009.263 arXiv:ISSN 0018-9162

[18] Hugo Larochelle, Yoshua Bengio, J´erˆome Louradour, and Pascal Lamblin. 2009.
Exploring strategies for training deep neural networks. Journal of Machine
Learning Research 10, Jan (2009), 1–40.

[19] Brian McFee, Thierry Bertin-Mahieux, Daniel P.W. Ellis, and Gert R.G. Lanckriet.
2012. The million song dataset challenge. Proceedings of the 21st international
conference companion on World Wide Web - WWW ’12 Companion (2012), 909.
https://doi.org/10.1145/2187980.2188222

[20] Brian Mcfee, Colin Raffel, Dawen Liang, Daniel P W Ellis, Matt Mcvicar, Eric
Battenberg, and Oriol Nieto. 2015. librosa: Audio and Music Signal Analysis in
Python. PROC. OF THE 14th PYTHON IN SCIENCE CONF Scipy (2015), 1–7.
[21] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed
Representations of Words and Phrases and their Compositionality. Nips (2013),
1–9. https://doi.org/10.1162/jmlr.2003.3.4-5.951 arXiv:1310.4546

[22] Raymond J Mooney and Loriene Roy. 1999. Content-Based Book Recommending.
Proceedings of the SIGIR-99 Workshop on Recommender Systems: Algorithms and
Evaluation August (1999).

[23] Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking
meets Word Sense Disambiguation: A Unified Approach. Transactions of the
Association for Computational Linguistics 2 (2014), 231–244. https://tacl2013.cs.
columbia.edu/ojs/index.php/tacl/article/view/291

[24] Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic con-
struction, evaluation and application of a wide-coverage multilingual semantic
network. Artificial Intelligence 193 (2012), 217–250. https://doi.org/10.1016/j.
artint.2012.07.001
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and
Andrew Y Ng. 2011. Multimodal deep learning. In Proceedings of the 28th inter-
national conference on machine learning (ICML-11). 689–696.

[25]

[26] Kim Anh Nguyen, Sabine Schulte im Walde, and Ngoc Thang Vu. 2016. Neural-
based Noise Filtering from Word Embeddings. Coling-2016 (2016), 2699–2707.
arXiv:1610.01874 http://arxiv.org/abs/1610.01874

[27] Sergio Oramas, Luis Espinosa-Anke, Mohamed Sordo, Horacio Saggion, and
Xavier Serra. 2016. Information extraction for knowledge base construction in
the music domain. Data and Knowledge Engineering 106 (2016), 70–83. https:
//doi.org/10.1016/j.datak.2016.06.001

[28] Sergio Oramas, Vito Claudio Ostuni, Tommaso Di Noia, Xavier Serra, and Euge-
nio Di Sciascio. 2015. Sound and Music Recommendation with Knowledge Graphs.
ACM Trans. Intell. Syst. Technol. 9, 4 (2015), 1–21. https://doi.org/10.1145/2926718
[29] Sergio Oramas, Mohamed Sordo, Luis Espinosa-Anke, and Xavier Serra. 2015.
A Semantic-based Approach for Artist Similarity. Proceedings of 16th the In-
ternational Society for Music Information Retrieval Conference October (2015),
100–106.

[30] Martin Saveski and a Mantrach. 2014. Item cold-start recommendations: learning
local collective embeddings. RecSys ’14 Proceedings of the 8th ACM Conference on
Recommender systems (2014), 89–96. https://doi.org/10.1145/2645710.2645751

[31] Markus Schedl, Emilia G´omez, Juli´an Urbano, et al. 2014. Music information
retrieval: Recent developments and applications. Foundations and Trends® in
Information Retrieval 8, 2-3 (2014), 127–261.

A Deep Multimodal Approach for Cold-start
Music Recommendation

Sergio Oramas
Music Technology Group
Universitat Pompeu Fabra
sergio.oramas@upf.edu

Mohamed Sordo
Pandora Media, Inc.
Oakland, CA
msordo@pandora.com

Oriol Nieto
Pandora Media, Inc.
Oakland, CA
onieto@pandora.com

Xavier Serra
Music Technology Group
Universitat Pompeu Fabra
xavier.serra@upf.edu

7
1
0
2
 
l
u
J
 
4
2
 
 
]

R

I
.
s
c
[
 
 
2
v
9
3
7
9
0
.
6
0
7
1
:
v
i
X
r
a

ABSTRACT
An increasing amount of digital music is being published daily.
Music streaming services often ingest all available music, but this
poses a challenge: how to recommend new artists for which prior
knowledge is scarce? In this work we aim to address this so-called
cold-start problem by combining text and audio information with
user feedback data using deep network architectures. Our method
is divided into three steps. First, artist embeddings are learned from
biographies by combining semantics, text features, and aggregated
usage data. Second, track embeddings are learned from the audio
signal and available feedback data. Finally, artist and track em-
beddings are combined in a multimodal network. Results suggest
that both splitting the recommendation problem between feature
levels (i.e., artist metadata and audio track), and merging feature
embeddings in a multimodal approach improve the accuracy of the
recommendations.

CCS CONCEPTS
•Information systems → Recommender systems; •Computing
methodologies → Neural networks;

KEYWORDS
recommender systems, deep learning, multimodal, music, semantics

ACM Reference format:
Sergio Oramas, Oriol Nieto, Mohamed Sordo, and Xavier Serra. 2017. A
Deep Multimodal Approach for Cold-start
Music Recommendation. In Proceedings of DLRS 2017, Como, Italy, August
27, 2017, 6 pages.
DOI: 10.1145/3125486.3125492

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
DLRS 2017, Como, Italy
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5353-3/17/08. . . $15.00
DOI: 10.1145/3125486.3125492

1 INTRODUCTION
It is common for online music streaming services nowadays to
offer ever-growing catalogs with dozens of millions of music tracks.
Since manually managing these large libraries is not feasible due
to size constraints, automatic exploration and exploitation of large-
scale music collections has been an active area of research in recent
years [31]. While several existing algorithmic techniques are able
to produce successful recommendations for popular content [17],
the exploration of new or undiscovered artists (i.e., the long tail [6])
remains a major challenge that we aim to address.

Recommender systems can be broadly classified into collabora-
tive filtering (CF), content-based, and hybrid methods. CF methods
[17] use the item-user feedback matrix and predictions are based
on the similarity of user or item profiles. Matrix factorization tech-
niques are currently CF state-of-the-art [17]. CF methods suffer
from the cold-start problem, as new items do not have feedback
information [30]. Content-based methods [22] rely only on item
features, and recommendations are based on similarity between
such features. Finally, hybrid methods [5] try to combine both item
content and item-user feedback.

Social tags have been extensively used as a source of artist con-
tent features to recommend music [16]. However, these tags are
usually collectively annotated, which often introduce an artist pop-
ularity bias [35]. Artist biographies and press releases, on the other
hand, do not necessarily require a collaborative effort, as they may
be produced by artists themselves. However, they have seldom been
exploited for music recommendation [28]. Part of this work focuses
on learning artist features from these biographies. Furthermore,
we also make use of audio signals, since these are generally always
available and have shown to be helpful when recommending music
in the long tail [36].

According to [11], composing simpler tasks is more likely to
yield effective local minima for neural networks. In addition, as
stated in [18], directly training all the layers of a deep network
together make it difficult to exploit all the extra modeling power
of a deeper architecture. Therefore, we decided to separate the
problem of music recommendation into artist and song levels. Artist
feature embeddings are learned from artist metadata in an artist
recommendation scenario. Track feature embeddings are learned
from audio signals in a song recommendation scenario. In both
cases, a hybrid recommendation approach is used based on learning
attribute-to-feature mappings [10]. This method addresses the lack

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

of feedback for uncommon items in two steps: (1) factorizing the
collaborative matrix, and (2) learning a mapping between item
content features and item latent factors [1, 36]. Lastly, both feature
embeddings are combined in a multimodal network to predict song
recommendations of cold-start artists. We show how dividing the
problem into artists and songs, and combining text and audio in a
multimodal approach yields improved recommendations.

For the sake of reproducibility, source code and data splits used
in the experiments have been released1. Our main contributions in
this work are summarized as follows:

• Method to enrich artist biographies with semantic infor-

mation leveraging an external Knowledge Base.

• Dividing the problem of music recommendation into artist

and song levels to obtain better performance.

• A multimodal deep learning pipeline for music recommen-
dation that combines audio with text and yields improved
results.

• The release of an extended version of the Million Song

Dataset with artist biographies and tags.

2 RECOMMENDATION APPROACH
To produce cold-start music recommendations, we propose the
following framework. Given the set of artist features As of a song
s, and the set of track features Ts of s, the complete feature set
of s is defined as the aggregation of its artist and track features
Fs = As ∪ Ts .

Given the heterogeneity of these two feature sets (audio and
text), a learning process involving them may under-explore one of
the modalities, as the stronger modality may dominate quickly. To
ensure that the variability of the input data is fully represented, we
divide the problem into three phases (see Figure 1). First, we aggre-
gate the collaborative information of all songs of the same artist,
and learn an artist feature embedding A(cid:48)
from As in an artist rec-
s
ommendation scenario. Second, we learn a track feature embedding
T (cid:48)
from Ts in a pure audio-based recommendation scenario. Third,
s
we combine both feature embeddings A(cid:48)
in a multimodal
s
network and compute song recommendations.

and T (cid:48)
s

Since songs from the same artist share the same set As , if different
songs from the same artist appear in multiple sets (e.g., train and
test), a problem of overfitting may arise [9]. To approach this issue,
we use non-overlapping artists across the train, validation, and test
sets.

Let M be the matrix of implicit feedback, where mus is the num-
ber of play counts for user u on song s. M is split into Mt r ain ,
and Mt est , for train, validation and test, respectively, where
Mval
no artist is shared across sets. Factorizing Mt r ain using weighted
matrix factorization (WMF) [12] yields Ik
, the k dimensional
sets of song and user latent factors, respectively. We set k = 200,
and apply the alternating least squares (ALS) optimization method.
To learn the artist embeddings, we obtain the matrix of artist
implicit feedback R from M, being Rua = (cid:205)
s mus for all songs s
from the same artist a. This matrix is split into train, validation,
and test sets following the same partition of artists made for M,
and thus keeping the mutual exclusion restriction. Latent factors
of artists and users are later obtained via WMF. Lastly, a deep

and Uk

Figure 1: Model architecture.

neural network is trained on the prediction of artist latent factors
from artist content features A. On the other hand, the song latent
factors are predicted with a deep convolutional network, using Ik
as training data and the track features T as input (similar to [36]).
Once the artist and track models are trained and optimized, we
gather the activations from the penultimate layer of each network
for all the sets. These activations constitute what we call the artist
and track feature embeddings A(cid:48)
, which are in turn used as
s
input to a third network. This final multimodal network is trained
on the prediction of song latent factors Ik
. Finally,
the list of item recommendations for user u is obtained by ranking
the results of computing the dot product between the user latent
factor fu ∈ Uk

and the set of item factors.

from S (cid:48)
s

and T (cid:48)
s

The different architectures used in each one of the three neu-
ral networks involved in the approach are described in Sections
3 ,4, and 5, respectively. Nevertheless, all networks have a final
fully connected layer of 200 units2 with linear activation and l2-
normalization. In addition, mini batches of 32 items are randomly
sampled from the training data to compute the gradient in all net-
works, and Adam [15] is the optimizer used to train the models,
with the default suggested learning parameters. Given that the out-
put of the architectures are l2-normalized, we use cosine proximity
as the loss function, as in [7].

s ∪T (cid:48)
s

= A(cid:48)

3 ARTIST TEXT EMBEDDINGS
In this section we describe two different, competing approaches to
exploit artist texts in a deep learning process.

3.1 Semantic enrichment
We propose a method for enriching artist biographies by associat-
ing text fragments with relevant entities defined in online semantic
datasets, and then gathering relevant information about these en-
tities from a Knowledge Base (KB). For this purpose, we adopted
Babelfy, a state-of-the-art tool for Entity Linking (EL) [23], a task
to associate, for a given textual fragment candidate, the most suit-
able entry in a reference KB. Babelfy maps words from a given

1https://github.com/sergiooramas/tartarus

2to match the dimensions of the factors to be predicted.

A Deep Multimodal Approach for Cold-start
Music Recommendation

text to entities in the BabelNet Knowledge Base [24], which has a
direct mapping to DBpeadia3, a structured version of Wikipedia4.
We use semantic information about the identified entities coming
from DBpedia to enrich the biographies. DBpedia resources are
generally classified using the DBpedia Ontology, which is a shallow,
cross-domain ontology based on the most common infoboxes of
Wikipedia.

EL systems are useful for music recommendation [28]. However,
they are not optimized for the music domain, and are prone to
errors [27]. The application of a filtering process over the set of
identified entities based on their classification within the DBpedia
Ontology, has demonstrated its utility to improve music retrieval
tasks, such as artist similarity [29]. Therefore, we only keep entities
of classes related to the music domain such as MusicalArtist, Band,
MusicGenre, MusicalWork, RecordLabel, Instrument, Engineer, and
Place. Then, we query DBpedia to get all the available information
about the filtered entities. From the information gathered, we
keep some specific properties for every kind of entity, such as
homeTown, instrument, genre or associatedBand for MusicalArtists,
writer, producer or recordedIn for MusicalWorks, stylisticOrigin or
instrument for MusicGenres, and so on5. In addition, we also kept
all the Wikipedia categories associated to each entity. In Wikipedia,
categories are used to organize resources, and they help users to
group articles of the same subject.

To build the enriched biographies we proceed as follows: First,
Babelfy is applied over the biography texts. Second, information
is gathered from DBpedia for the entities of the selected classes.
Finally, the collected data are added at the end of the biography text
separated by spaces. A vector space model (VSM) is then applied
to the set of enriched biographies, and tf-idf weighting [37] is used.
We limited the vocabulary size to 10,000 terms for the VSM, as
this number provides a good trade-off between performance and
number of parameters required for training. Note that either words,
entities, dates, or categories may be part of this vocabulary. From
this data representation, a feedforward network with two dense
layers of 2048 neurons each is trained to predict the artist latent
factors.

3.2 Word embeddings
Much of the work with deep learning in Natural Language Pro-
cessing has involved the learning of word vector representations
[3, 21], and their further composition [8]. Word embeddings aim
to represent words as low-dimensional dense vectors. They have
demonstrated to greatly benefit NLP tasks, such as word similarity,
sentiment analysis, or parsing [26].

The use of convolutional neural networks (CNN) over pre-trained
word vectors has become state-of-the-art in sentence classification
[14]. We re-adapt the architecture proposed in [14] for sentence
classification to learn artist latent factors from artist biographies.
This consists in an embedding layer, followed by a one dimensional
convolutional layer with multiple filter widths, a max-over-time
pooling layer, a dense hidden layer and the output layer. We employ
the same architecture and parameters, changing only the output

3http://dbpedia.org
4http://wikipedia.org
5The
list
complete
http://mtg.upf.edu/download/datasets/msd-a

classes

of

and

properties

is

available

at

DLRS 2017, August 27, 2017, Como, Italy

layer and the loss function. We initialize the input embedding layer
of the network with word2vec word embeddings pre-trained on
the Google News dataset, and also with word embeddings trained
in our own corpus of biographies.

4 TRACK AUDIO EMBEDDINGS
It is common in the field of music informatics to make use of CNNs
to learn higher-level features from spectrograms. These representa-
tions are typically contained in RF×N matrices with F frequency
bins and N time frames. In this work we compute 96 frequency
bin, log-compressed constant-Q transforms (CQT) [32] for all the
tracks in our dataset using librosa [20] with the following param-
eters: audio sampling rate at 22050 Hz, hop length of 1024 samples,
Hann analysis window, and 12 bins per octave. Following a sim-
ilar approach to [36], we address the variability of the length N
across songs by sampling one 15-seconds long patch from each
track, resulting in the fixed-size input to the CNN.

The deep model trained with these data is defined as follows:
the CQT patches are fed to four convolutional layers with rectified
linear units (ReLU) as activations. The four convolutions have the
following number of filters, from first to last: 256, 512, 1024, and
1024. The convolutions are only applied to the time axis, leaving
the frequencies fixed since the absolute and relative bin placement
is important when aiming to capture particular sounds (as opposed
to the irrelevance of where in time a certain sonic event occurs).
Maxpooling of 4 units across the time axis is applied after each of
the first three ReLUs, and 50% dropout is applied to all layers. The
flattened output of the last layer has 4096 units, which becomes the
vector embedding to later use in the multimodal approach described
next.

5 MULTIMODAL FUSION
There are several approaches in the literature for multimodal fea-
ture learning [25, 34], and late fusion of multimodal feature vectors
[2, 33]. In this work, audio and text feature vectors are learned sep-
arately and then combined via late fusion in a multimodal network
(see Figure 1).

Given the different nature of the artist and track embeddings,
a normalization step is necessary. Normalized feature vectors are
then fed to a feed forward neural network (a simple Multi Layer
Perceptron, MLP). Two different architectures were explored: (i)
each embedding vector is connected to an isolated dense layer of
512 hidden units with ReLU activations after a process of batch
normalization [13]. Then, both dense layers are connected to the
output layer. The rationale behind this is that the isolated dense
layers help the network learn non-linearities from each modality
separately. (ii) each embedding vector is l2-normed and then con-
catenated into a single feature vector which is directly connected
to the output layer, resulting in a linear model. Regularization is
obtained by applying dropout with an empirically selected factor
of 70% after the input layer for both architectures.

6 EXPERIMENTS
6.1 Dataset
The Million Song Dataset (MSD) [19] is a collection of metadata
and precomputed audio features for 1 million songs. Along with

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

this dataset, the Echo Nest Taste Profile Subset [4] provides play
counts of 1 million users on more than 380,000 songs from the MSD.
Starting from this subset, we gather biographies and social tags
from Last.fm for all the artists that have at least one song in the
dataset. When there are several artists with the same name, they
are stored in the same page of Last.fm, which makes the biography
and social tags ambiguous. We automatically removed all ambigu-
ous artists by applying text processing on the biographies. The
song features provided with the MSD are not generally suitable
for deep learning, so we instead use audio previews between 7 and
30 seconds retrieved from 7digital.com. After removing ambigu-
ous artists and missing tracks, the final dataset consists of 328,821
tracks from 24,043 artists. Each track has at least 15 seconds of
audio, each biography is at least 50 characters long, and each artist
has at least 1 tag associated with it. All artist metadata, implicit
feedback matrices, and splits are released as a new dataset called
the MSD-A6.

6.2 Artist Recommendation
To investigate to what extent the different feature sets, data models
and architectures influence the quality of the deep artist features,
we evaluate the different approaches in an artist recommendation
scenario. Given the matrix of implicit feedback R, and the set of
artist and user factors obtained through matrix factorization (see
Section 2), we predict the artist factors for the test set, and use
them to compute a ranked list of recommended artists for every
user. We use mean average precision (MAP) with a cut-off at 500
recommendations per user.

We compare four different approaches using the biography texts
as input. (1) a pure text-based approach using a VSM and a feed-
forward network a-TEXT. (2) similar to (1) but with a semantically
enriched version of the texts a-sem (cf. Section 3.1). (3) An ap-
proach based on word embeddings initialized with vectors trained
on Google News and a CNN a-w2v-goo (cf. Section 3.2). (4) Similar
to (3) but inizializing the embeddings with word vectors previously
trained on the corpus of biographies a-w2v. To properly frame the
results, we compute two baselines and one competitor approach.
The tags baseline approach uses artist social tags as input features,
and text-rf uses biography texts as input, but Random Forest
Regression for the learning instead of a deep neural network. The
former baseline is added to compare the potential of biography
texts with respect to curated metadata, whilst the latter was added
to study to which extent the deep network improves the results
over other learning methods typically used in natural language
processing. There are few recommendation approaches able to
deal with an extreme cold-start scenario like ours. Therefore, we
select ItemAttributeKnn [10] as the competitor approach (tags-
itemKnn), using artist social tags as attribute data and computed
using the MyMediaLite library7. We also show the scores achieved
when the latent factor vectors are randomized (random), and when
they are learned from feedback data using matrix factorization
(upper-bound).

6https://doi.org/10.5281/zenodo.831348
7http://www.mymedialite.net/

Table 1: Artist Recommendation Results

Aproach

Input

Data model

Arch MAP

a-text
a-sem
a-w2v-goo
a-w2v

a-tags
tags-itemKnn
text-rf

random
upper-bound

Bio
Sem Bio
Bio
Bio

VSM
VSM
w2v-pretrain
w2v-trained

FF
FF
CNN
CNN

Tags
Tags
Bio

-
-

VSM
-
VSM

-
-

FF
itemKnn
RF

-
-

0.0161
0.0201
0.0119
0.0145

0.0314
0.0161
0.0089

0.0014
0.5528

Mean average precision (MAP) at 500 for the predictions of artist recommen-
dations in 1M users. VSM refers to Vector Space Model, FF to Feedforward,
RF to Random Forest, CNN to Convolutional Neural Network, and itemKnn
to itemAttributeKnn approach. Bio refers to biography texts and Sem Bio to
semantically enriched texts.

Results reported in Table 1 show that the semantic enrichment
of the biographies a-sem outperforms the pure text approach a-
text. As expected, the use of tags improves the results over the
use of text. However, the addition of semantic features reduces the
gap in performance between the use of tags and unstructured text.
Moreover, the difference between a-text and text-rf shows that
the use of deep learning with respect to random forest improves
the results. We also note that a VSM model with a feedforward
network outperforms the use of word embeddings with convolu-
tions. Although, according to the literature, this latter approach
has demonstrated its utility for simple tasks like binary classifica-
tion with short texts, our task puts forward two challenges for this
architecture: the greater length of the input texts, and the higher
dimensionality of the output. Although we have shown that initial-
izing the embedding layer with word vectors trained on the corpus
itself (a-w2v) outperforms the use of Google News pre-trained
vectors (a-w2v-goo), further work is necessary to properly opti-
mize a convolutional architecture for this task. Finally, we observe
that our approach a-tags outperforms the competitor approach
tags-itemKnn using the same item attributes.

Once the network is trained, we predict the activations of the
penultimate layer for the entire dataset of artists. Thus, we obtain a
vector embedding of 2048 dimensions, which represents the artist
deep features A(cid:48). From the evaluated approaches, we compute the
artist embedding from the a-sem and a-tags approaches.

6.3 Song Recommendation
In this experiment, audio embeddings are obtained after training
the convolutional network (see Section 4) with 260k patches of
15 seconds, corresponding to the 80% of the tracks described in
Section 6.1. Patches are divided into training (80%), validation (10%)
and test (10%) sets. Results reported in Table 2 are computed over
the remaining 20% of tracks. As opposed to [36], no artist appears
in more than one subset to avoid overfitting. Finally, multimodal
approaches are computed on the same sets.

In our experiments, we want to measure the impact of the artist
embeddings in the song recommendation problem, and also the

A Deep Multimodal Approach for Cold-start
Music Recommendation

Table 2: Song Recommendation Results

Approach

Artist Input Track Input Arch MAP

audio
sem-vsm
sem-emb

mm-lf-lin
mm-lf-h1
mm

tags-vsm
tags-emb

random
upper-bound

-
Sem Bio
a-sem

a-sem
a-sem
Sem Bio

Tags
a-tags

rnd emb
-

audio spec
-
-

CNN 0.0015
0.0032
0.0034

FF
FF

audio emb MLP 0.0036
0.0035
audio emb
MLP
CNN 0.0014
audio spec

-
-

-
-

FF
FF

FF
-

0.0043
0.0049

0.0002
0.1649

Mean average precision (MAP) at 500 for the predictions of song recom-
mendations in 1M users. audio emb refers to the track embedding of audio
approach, sem to artist embedding of sem approach, tags to artist embedding
of tags approach, spec to spectrogram, mm to multimodal, lf to late fusion,
lin to linear, and h1 to one hidden layer.

potential of the multimodal approach. We experimented with two
approaches, sem-emb and tags-emb, that exploit the feature em-
beddings learned from the artists features (see Section 6.2), either
based on biography texts (a-sem) or artists tags (a-tags). To mea-
sure the potential of the artist embeddings, we also computed two
approaches based on the original artist features (sem-vsm for se-
mantic text features and tags-vsm for tag features). Results on
Table 2 show that sem-emb and tags-emb outperform sem-vsm
and tags-vsm, suggesting that artist embeddings outperform artist
features.

An approach based on the audio spectrograms was computed
(audio). From this latter approach, audio embeddings where ob-
tained (audio emb) and combined with a-sem in a multimodal late
fusion approach mm-lf-lin (without hidden layers and l2-norm)
and mm-lf-h1 (with one hidden layer after each feature vector and
batch normalization) (cf. Section 5). We also tried with different
combinations of hidden layers and normalization steps in the mul-
timodal network but all of them yielded lower results than the ones
reported for mm-lf-lin and mm-lf-h1. We compared this network
with a multimodal approach trained directly on the original features
(semantically enriched text and audio spectrograms). Results on the
combination of artist and track features show that the late fusion of
artist and track embeddings mm-lf-lin outperforms the simultane-
ous training of artist and track features mm. In addition, we observe
that we achieve better results when no hidden layer is added to
the multimodal network mm-lf-lin. Finally, we observe that the
multimodal approach that combines text and audio features with
late fusion mm-lf-lin improves the results of pure text sem-emb
or pure audio audio approaches. All the differences between the
approaches are statistically significant (p < 0.01) according to the
paired t-test.

We also compared the results with an upper-bound approach
obtained from the feedback data and an approach trained with
random vector embeddings. Although results are in general far
from the upper-bound, the comparative analysis of the proposed
approaches gives some insights of the behavior of different feature

DLRS 2017, August 27, 2017, Como, Italy

representations and modalities in the cold-start recommendation
problem.

7 CONCLUSIONS
In this work, a multimodal approach for song recommendation has
been presented. The approach is divided into three steps. (1) Artist
feature embeddings are learned from text and semantic features
in an artist recommendation scenario using a deep network archi-
tecture. (2) Track feature embeddings are learned from the audio
spectrograms using convolutional neural networks. (3) Embeddings
are combined in a multimodal network.

Results show that splitting the problem of song recommendation
at artist and song levels improves the quality of recommendations.
Learning artist feature embeddings separately benefits from the
aggregation of the information about the different songs of the
same artist, yielding more robust artist features. Related to this,
an approach for the semantic enrichment of artist metadata has
been proposed, leading to a significant improvement in the results.
In addition, we have shown the potential of exploiting artist bi-
ographies in music recommendation. Moreover, the deep learning
architectures of this work have demonstrated their capacity to im-
prove upon other learning models under the music recommendation
framework. Finally, we have shown how a multimodal approach,
based on the late fusion of track and artist feature embeddings
that are learned separately, outperforms end-to-end multimodal
approaches where the different modalities are learned simultane-
ously. Moreover, results have shown that our multimodal approach
achieves better results than pure text or audio approaches. As fu-
ture work, we plan to do fine-tunning of the combined model after
pre-training the network on each modality separately.

ACKNOWLEDGMENTS
This work was partially funded by the Spanish Ministry of Economy
and Competitiveness under the Maria de Maeztu Units of Excellence
Programme (MDM-2015-0502). The Tesla K40 used for this research
was donated by the NVIDIA Corporation.

REFERENCES
[1] Trapit Bansal, David Belanger, and Andrew McCallum. 2016. Ask the GRU:
Multi-task Learning for Deep Text Recommendations. RecSys (2016), 107–114.
https://doi.org/10.1145/2959100.2959180 arXiv:1609.02116

[2] M. Rouvier Bechet, S. Delecraz, B. Favre, M. Bendris, and F. 2015. Multimodal
embedding fusion for robust speaker role recognition in video broadcast. IEEE
Workshop on Automatic Speech Recognition and Understanding (ASRU) (2015).
[3] Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003.
A Neural Probabilistic Language Model. The Journal of Machine Learn-
ing Research 3 (2003), 1137–1155. https://doi.org/10.1162/153244303322533223
arXiv:arXiv:1301.3781v3

[4] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere.
2011. The Million Song Dataset. In Proceedings of the 12th International Conference
on Music Information Retrieval (ISMIR 2011).

[5] Robin Burke. 2002. Hybrid Recommender Systems: Survey and Experiments.

[6]

User Modeling and User-Adapted Interaction 12, 4 (2002), 331–370.
`Oscar Celma. 2010. The Long Tail in Recommender Systems. Springer Berlin Hei-
delberg, Berlin, Heidelberg, 87–107. https://doi.org/10.1007/978-3-642-13287-2 4
[7] Franc¸ois Chollet. 2016. Information-theoretical label embeddings for large-scale
image classification. CoRR (2016), 1–10. arXiv:1607.05691 http://arxiv.org/abs/
1607.05691

[8] Ronan Collobert,

Jason Weston, L´eon Bottou, Michael Karlen, Koray
Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost)
Journal of Machine Learning Research 12 (2011), 2493–2537.
from Scratch.
https://doi.org/10.1.1.231.4614 arXiv:1103.0398

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

[32] Christian Sch¨orkhuber and Anssi Klapuri. 2010. Constant-Q transform toolbox
for music processing. 7th Sound and Music Computing Conference JANUARY
(2010), 3–64.

[33] Olga Slizovskaia, Emilia G´omez, and Gloria Haro. 2017. Musical instrument
recognition in user-generated videos using a multimodal convolutional neural
network architecture. In ICMR ’17: Proceedings of the 2017 ACM on International
Conference on Multimedia Retrieval. Bucharest, Romania.

[34] Nitish Srivastava and Ruslan Salakhutdinov. 2012. Learning representations for
multimodal data with deep belief nets. In International conference on machine
learning workshop.

[35] Douglas Turnbull, Luke Barrington, and Gert Lanckriet. 2008. Five approaches
to collecting tags for music. Proceedings of 9th the International Society for
Music Information Retrieval Conference (2008), 225–230. http://books.google.
com/books?hl=en

[36] Aaron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep
content-based music recommendation. Electronics and Information Systems
department (ELIS) (2013), 9. https://doi.org/10.1109/MMUL.2011.34.van
Justin Zobel and Alistair Moffat. 1998. Exploring the similarity space. ACM
SIGIR Forum 32, 1 (1998), 18–34. https://doi.org/10.1145/281250.281256

[37]

[9] Arthur Flexer. 2007. A Closer Look on Artist Filters for Musical Genre Classifica-
tion. In Proceedings of the 8th International Society for Music Information Retrieval
Conference.

[10] Zeno Gantner, Lucas Drumond, Christoph Freudenthaler, Steffen Rendle, and
Lars Schmidt-Thieme. 2010. Learning Attribute-to-Feature Mappings for Cold-
Start Recommendations. In Proceedings of the 2010 IEEE International Conference
on Data Mining (ICDM ’10). IEEE Computer Society, Washington, DC, USA,
176–185.

[11] C¸a˘glar G¨ulc¸ehre and Yoshua Bengio. 2016. Knowledge matters: Importance of
prior information for optimization. Journal of Machine Learning Research 17, 8
(2016), 1–32.

[12] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative Filtering for
Implicit Feedback Datasets. In Proceedings of the 2008 Eighth IEEE International
Conference on Data Mining (ICDM ’08). 263–272.

[13] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Acceler-
ating Deep Network Training by Reducing Internal Covariate Shift. CoRR
abs/1502.03167 (2015). http://arxiv.org/abs/1502.03167

[14] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP 2014) (2014), 1746–1751. https://doi.org/10.1109/LSP.2014.
2325781 arXiv:arXiv:1408.5882v1

[15] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic
Optimization. CoRR abs/1412.6980 (2014). http://arxiv.org/abs/1412.6980
[16] Peter Knees and Markus Schedl. 2013. A survey of music similarity and
recommendation from music context data. ACM Transactions on Multime-
dia Computing, Communications, and Applications 10, 1 (2013), 1–21. https:
//doi.org/10.1145/2542205.2542206

[17] Y. Koren, R. Bell, and C. Volinsky. 2009. Matrix Factorization Techniques for
Recommender Systems. Computer 42, 8 (2009), 42–49. https://doi.org/10.1109/
MC.2009.263 arXiv:ISSN 0018-9162

[18] Hugo Larochelle, Yoshua Bengio, J´erˆome Louradour, and Pascal Lamblin. 2009.
Exploring strategies for training deep neural networks. Journal of Machine
Learning Research 10, Jan (2009), 1–40.

[19] Brian McFee, Thierry Bertin-Mahieux, Daniel P.W. Ellis, and Gert R.G. Lanckriet.
2012. The million song dataset challenge. Proceedings of the 21st international
conference companion on World Wide Web - WWW ’12 Companion (2012), 909.
https://doi.org/10.1145/2187980.2188222

[20] Brian Mcfee, Colin Raffel, Dawen Liang, Daniel P W Ellis, Matt Mcvicar, Eric
Battenberg, and Oriol Nieto. 2015. librosa: Audio and Music Signal Analysis in
Python. PROC. OF THE 14th PYTHON IN SCIENCE CONF Scipy (2015), 1–7.
[21] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed
Representations of Words and Phrases and their Compositionality. Nips (2013),
1–9. https://doi.org/10.1162/jmlr.2003.3.4-5.951 arXiv:1310.4546

[22] Raymond J Mooney and Loriene Roy. 1999. Content-Based Book Recommending.
Proceedings of the SIGIR-99 Workshop on Recommender Systems: Algorithms and
Evaluation August (1999).

[23] Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking
meets Word Sense Disambiguation: A Unified Approach. Transactions of the
Association for Computational Linguistics 2 (2014), 231–244. https://tacl2013.cs.
columbia.edu/ojs/index.php/tacl/article/view/291

[24] Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic con-
struction, evaluation and application of a wide-coverage multilingual semantic
network. Artificial Intelligence 193 (2012), 217–250. https://doi.org/10.1016/j.
artint.2012.07.001
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and
Andrew Y Ng. 2011. Multimodal deep learning. In Proceedings of the 28th inter-
national conference on machine learning (ICML-11). 689–696.

[25]

[26] Kim Anh Nguyen, Sabine Schulte im Walde, and Ngoc Thang Vu. 2016. Neural-
based Noise Filtering from Word Embeddings. Coling-2016 (2016), 2699–2707.
arXiv:1610.01874 http://arxiv.org/abs/1610.01874

[27] Sergio Oramas, Luis Espinosa-Anke, Mohamed Sordo, Horacio Saggion, and
Xavier Serra. 2016. Information extraction for knowledge base construction in
the music domain. Data and Knowledge Engineering 106 (2016), 70–83. https:
//doi.org/10.1016/j.datak.2016.06.001

[28] Sergio Oramas, Vito Claudio Ostuni, Tommaso Di Noia, Xavier Serra, and Euge-
nio Di Sciascio. 2015. Sound and Music Recommendation with Knowledge Graphs.
ACM Trans. Intell. Syst. Technol. 9, 4 (2015), 1–21. https://doi.org/10.1145/2926718
[29] Sergio Oramas, Mohamed Sordo, Luis Espinosa-Anke, and Xavier Serra. 2015.
A Semantic-based Approach for Artist Similarity. Proceedings of 16th the In-
ternational Society for Music Information Retrieval Conference October (2015),
100–106.

[30] Martin Saveski and a Mantrach. 2014. Item cold-start recommendations: learning
local collective embeddings. RecSys ’14 Proceedings of the 8th ACM Conference on
Recommender systems (2014), 89–96. https://doi.org/10.1145/2645710.2645751

[31] Markus Schedl, Emilia G´omez, Juli´an Urbano, et al. 2014. Music information
retrieval: Recent developments and applications. Foundations and Trends® in
Information Retrieval 8, 2-3 (2014), 127–261.

A Deep Multimodal Approach for Cold-start
Music Recommendation

Sergio Oramas
Music Technology Group
Universitat Pompeu Fabra
sergio.oramas@upf.edu

Mohamed Sordo
Pandora Media, Inc.
Oakland, CA
msordo@pandora.com

Oriol Nieto
Pandora Media, Inc.
Oakland, CA
onieto@pandora.com

Xavier Serra
Music Technology Group
Universitat Pompeu Fabra
xavier.serra@upf.edu

7
1
0
2
 
l
u
J
 
4
2
 
 
]

R

I
.
s
c
[
 
 
2
v
9
3
7
9
0
.
6
0
7
1
:
v
i
X
r
a

ABSTRACT
An increasing amount of digital music is being published daily.
Music streaming services often ingest all available music, but this
poses a challenge: how to recommend new artists for which prior
knowledge is scarce? In this work we aim to address this so-called
cold-start problem by combining text and audio information with
user feedback data using deep network architectures. Our method
is divided into three steps. First, artist embeddings are learned from
biographies by combining semantics, text features, and aggregated
usage data. Second, track embeddings are learned from the audio
signal and available feedback data. Finally, artist and track em-
beddings are combined in a multimodal network. Results suggest
that both splitting the recommendation problem between feature
levels (i.e., artist metadata and audio track), and merging feature
embeddings in a multimodal approach improve the accuracy of the
recommendations.

CCS CONCEPTS
•Information systems → Recommender systems; •Computing
methodologies → Neural networks;

KEYWORDS
recommender systems, deep learning, multimodal, music, semantics

ACM Reference format:
Sergio Oramas, Oriol Nieto, Mohamed Sordo, and Xavier Serra. 2017. A
Deep Multimodal Approach for Cold-start
Music Recommendation. In Proceedings of DLRS 2017, Como, Italy, August
27, 2017, 6 pages.
DOI: 10.1145/3125486.3125492

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
DLRS 2017, Como, Italy
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5353-3/17/08. . . $15.00
DOI: 10.1145/3125486.3125492

1 INTRODUCTION
It is common for online music streaming services nowadays to
offer ever-growing catalogs with dozens of millions of music tracks.
Since manually managing these large libraries is not feasible due
to size constraints, automatic exploration and exploitation of large-
scale music collections has been an active area of research in recent
years [31]. While several existing algorithmic techniques are able
to produce successful recommendations for popular content [17],
the exploration of new or undiscovered artists (i.e., the long tail [6])
remains a major challenge that we aim to address.

Recommender systems can be broadly classified into collabora-
tive filtering (CF), content-based, and hybrid methods. CF methods
[17] use the item-user feedback matrix and predictions are based
on the similarity of user or item profiles. Matrix factorization tech-
niques are currently CF state-of-the-art [17]. CF methods suffer
from the cold-start problem, as new items do not have feedback
information [30]. Content-based methods [22] rely only on item
features, and recommendations are based on similarity between
such features. Finally, hybrid methods [5] try to combine both item
content and item-user feedback.

Social tags have been extensively used as a source of artist con-
tent features to recommend music [16]. However, these tags are
usually collectively annotated, which often introduce an artist pop-
ularity bias [35]. Artist biographies and press releases, on the other
hand, do not necessarily require a collaborative effort, as they may
be produced by artists themselves. However, they have seldom been
exploited for music recommendation [28]. Part of this work focuses
on learning artist features from these biographies. Furthermore,
we also make use of audio signals, since these are generally always
available and have shown to be helpful when recommending music
in the long tail [36].

According to [11], composing simpler tasks is more likely to
yield effective local minima for neural networks. In addition, as
stated in [18], directly training all the layers of a deep network
together make it difficult to exploit all the extra modeling power
of a deeper architecture. Therefore, we decided to separate the
problem of music recommendation into artist and song levels. Artist
feature embeddings are learned from artist metadata in an artist
recommendation scenario. Track feature embeddings are learned
from audio signals in a song recommendation scenario. In both
cases, a hybrid recommendation approach is used based on learning
attribute-to-feature mappings [10]. This method addresses the lack

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

of feedback for uncommon items in two steps: (1) factorizing the
collaborative matrix, and (2) learning a mapping between item
content features and item latent factors [1, 36]. Lastly, both feature
embeddings are combined in a multimodal network to predict song
recommendations of cold-start artists. We show how dividing the
problem into artists and songs, and combining text and audio in a
multimodal approach yields improved recommendations.

For the sake of reproducibility, source code and data splits used
in the experiments have been released1. Our main contributions in
this work are summarized as follows:

• Method to enrich artist biographies with semantic infor-

mation leveraging an external Knowledge Base.

• Dividing the problem of music recommendation into artist

and song levels to obtain better performance.

• A multimodal deep learning pipeline for music recommen-
dation that combines audio with text and yields improved
results.

• The release of an extended version of the Million Song

Dataset with artist biographies and tags.

2 RECOMMENDATION APPROACH
To produce cold-start music recommendations, we propose the
following framework. Given the set of artist features As of a song
s, and the set of track features Ts of s, the complete feature set
of s is defined as the aggregation of its artist and track features
Fs = As ∪ Ts .

Given the heterogeneity of these two feature sets (audio and
text), a learning process involving them may under-explore one of
the modalities, as the stronger modality may dominate quickly. To
ensure that the variability of the input data is fully represented, we
divide the problem into three phases (see Figure 1). First, we aggre-
gate the collaborative information of all songs of the same artist,
and learn an artist feature embedding A(cid:48)
from As in an artist rec-
s
ommendation scenario. Second, we learn a track feature embedding
T (cid:48)
from Ts in a pure audio-based recommendation scenario. Third,
s
we combine both feature embeddings A(cid:48)
in a multimodal
s
network and compute song recommendations.

and T (cid:48)
s

Since songs from the same artist share the same set As , if different
songs from the same artist appear in multiple sets (e.g., train and
test), a problem of overfitting may arise [9]. To approach this issue,
we use non-overlapping artists across the train, validation, and test
sets.

Let M be the matrix of implicit feedback, where mus is the num-
ber of play counts for user u on song s. M is split into Mt r ain ,
and Mt est , for train, validation and test, respectively, where
Mval
no artist is shared across sets. Factorizing Mt r ain using weighted
matrix factorization (WMF) [12] yields Ik
, the k dimensional
sets of song and user latent factors, respectively. We set k = 200,
and apply the alternating least squares (ALS) optimization method.
To learn the artist embeddings, we obtain the matrix of artist
implicit feedback R from M, being Rua = (cid:205)
s mus for all songs s
from the same artist a. This matrix is split into train, validation,
and test sets following the same partition of artists made for M,
and thus keeping the mutual exclusion restriction. Latent factors
of artists and users are later obtained via WMF. Lastly, a deep

and Uk

Figure 1: Model architecture.

neural network is trained on the prediction of artist latent factors
from artist content features A. On the other hand, the song latent
factors are predicted with a deep convolutional network, using Ik
as training data and the track features T as input (similar to [36]).
Once the artist and track models are trained and optimized, we
gather the activations from the penultimate layer of each network
for all the sets. These activations constitute what we call the artist
and track feature embeddings A(cid:48)
, which are in turn used as
s
input to a third network. This final multimodal network is trained
on the prediction of song latent factors Ik
. Finally,
the list of item recommendations for user u is obtained by ranking
the results of computing the dot product between the user latent
factor fu ∈ Uk

and the set of item factors.

from S (cid:48)
s

and T (cid:48)
s

The different architectures used in each one of the three neu-
ral networks involved in the approach are described in Sections
3 ,4, and 5, respectively. Nevertheless, all networks have a final
fully connected layer of 200 units2 with linear activation and l2-
normalization. In addition, mini batches of 32 items are randomly
sampled from the training data to compute the gradient in all net-
works, and Adam [15] is the optimizer used to train the models,
with the default suggested learning parameters. Given that the out-
put of the architectures are l2-normalized, we use cosine proximity
as the loss function, as in [7].

s ∪T (cid:48)
s

= A(cid:48)

3 ARTIST TEXT EMBEDDINGS
In this section we describe two different, competing approaches to
exploit artist texts in a deep learning process.

3.1 Semantic enrichment
We propose a method for enriching artist biographies by associat-
ing text fragments with relevant entities defined in online semantic
datasets, and then gathering relevant information about these en-
tities from a Knowledge Base (KB). For this purpose, we adopted
Babelfy, a state-of-the-art tool for Entity Linking (EL) [23], a task
to associate, for a given textual fragment candidate, the most suit-
able entry in a reference KB. Babelfy maps words from a given

1https://github.com/sergiooramas/tartarus

2to match the dimensions of the factors to be predicted.

A Deep Multimodal Approach for Cold-start
Music Recommendation

text to entities in the BabelNet Knowledge Base [24], which has a
direct mapping to DBpeadia3, a structured version of Wikipedia4.
We use semantic information about the identified entities coming
from DBpedia to enrich the biographies. DBpedia resources are
generally classified using the DBpedia Ontology, which is a shallow,
cross-domain ontology based on the most common infoboxes of
Wikipedia.

EL systems are useful for music recommendation [28]. However,
they are not optimized for the music domain, and are prone to
errors [27]. The application of a filtering process over the set of
identified entities based on their classification within the DBpedia
Ontology, has demonstrated its utility to improve music retrieval
tasks, such as artist similarity [29]. Therefore, we only keep entities
of classes related to the music domain such as MusicalArtist, Band,
MusicGenre, MusicalWork, RecordLabel, Instrument, Engineer, and
Place. Then, we query DBpedia to get all the available information
about the filtered entities. From the information gathered, we
keep some specific properties for every kind of entity, such as
homeTown, instrument, genre or associatedBand for MusicalArtists,
writer, producer or recordedIn for MusicalWorks, stylisticOrigin or
instrument for MusicGenres, and so on5. In addition, we also kept
all the Wikipedia categories associated to each entity. In Wikipedia,
categories are used to organize resources, and they help users to
group articles of the same subject.

To build the enriched biographies we proceed as follows: First,
Babelfy is applied over the biography texts. Second, information
is gathered from DBpedia for the entities of the selected classes.
Finally, the collected data are added at the end of the biography text
separated by spaces. A vector space model (VSM) is then applied
to the set of enriched biographies, and tf-idf weighting [37] is used.
We limited the vocabulary size to 10,000 terms for the VSM, as
this number provides a good trade-off between performance and
number of parameters required for training. Note that either words,
entities, dates, or categories may be part of this vocabulary. From
this data representation, a feedforward network with two dense
layers of 2048 neurons each is trained to predict the artist latent
factors.

3.2 Word embeddings
Much of the work with deep learning in Natural Language Pro-
cessing has involved the learning of word vector representations
[3, 21], and their further composition [8]. Word embeddings aim
to represent words as low-dimensional dense vectors. They have
demonstrated to greatly benefit NLP tasks, such as word similarity,
sentiment analysis, or parsing [26].

The use of convolutional neural networks (CNN) over pre-trained
word vectors has become state-of-the-art in sentence classification
[14]. We re-adapt the architecture proposed in [14] for sentence
classification to learn artist latent factors from artist biographies.
This consists in an embedding layer, followed by a one dimensional
convolutional layer with multiple filter widths, a max-over-time
pooling layer, a dense hidden layer and the output layer. We employ
the same architecture and parameters, changing only the output

3http://dbpedia.org
4http://wikipedia.org
5The
list
complete
http://mtg.upf.edu/download/datasets/msd-a

classes

of

and

properties

is

available

at

DLRS 2017, August 27, 2017, Como, Italy

layer and the loss function. We initialize the input embedding layer
of the network with word2vec word embeddings pre-trained on
the Google News dataset, and also with word embeddings trained
in our own corpus of biographies.

4 TRACK AUDIO EMBEDDINGS
It is common in the field of music informatics to make use of CNNs
to learn higher-level features from spectrograms. These representa-
tions are typically contained in RF×N matrices with F frequency
bins and N time frames. In this work we compute 96 frequency
bin, log-compressed constant-Q transforms (CQT) [32] for all the
tracks in our dataset using librosa [20] with the following param-
eters: audio sampling rate at 22050 Hz, hop length of 1024 samples,
Hann analysis window, and 12 bins per octave. Following a sim-
ilar approach to [36], we address the variability of the length N
across songs by sampling one 15-seconds long patch from each
track, resulting in the fixed-size input to the CNN.

The deep model trained with these data is defined as follows:
the CQT patches are fed to four convolutional layers with rectified
linear units (ReLU) as activations. The four convolutions have the
following number of filters, from first to last: 256, 512, 1024, and
1024. The convolutions are only applied to the time axis, leaving
the frequencies fixed since the absolute and relative bin placement
is important when aiming to capture particular sounds (as opposed
to the irrelevance of where in time a certain sonic event occurs).
Maxpooling of 4 units across the time axis is applied after each of
the first three ReLUs, and 50% dropout is applied to all layers. The
flattened output of the last layer has 4096 units, which becomes the
vector embedding to later use in the multimodal approach described
next.

5 MULTIMODAL FUSION
There are several approaches in the literature for multimodal fea-
ture learning [25, 34], and late fusion of multimodal feature vectors
[2, 33]. In this work, audio and text feature vectors are learned sep-
arately and then combined via late fusion in a multimodal network
(see Figure 1).

Given the different nature of the artist and track embeddings,
a normalization step is necessary. Normalized feature vectors are
then fed to a feed forward neural network (a simple Multi Layer
Perceptron, MLP). Two different architectures were explored: (i)
each embedding vector is connected to an isolated dense layer of
512 hidden units with ReLU activations after a process of batch
normalization [13]. Then, both dense layers are connected to the
output layer. The rationale behind this is that the isolated dense
layers help the network learn non-linearities from each modality
separately. (ii) each embedding vector is l2-normed and then con-
catenated into a single feature vector which is directly connected
to the output layer, resulting in a linear model. Regularization is
obtained by applying dropout with an empirically selected factor
of 70% after the input layer for both architectures.

6 EXPERIMENTS
6.1 Dataset
The Million Song Dataset (MSD) [19] is a collection of metadata
and precomputed audio features for 1 million songs. Along with

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

this dataset, the Echo Nest Taste Profile Subset [4] provides play
counts of 1 million users on more than 380,000 songs from the MSD.
Starting from this subset, we gather biographies and social tags
from Last.fm for all the artists that have at least one song in the
dataset. When there are several artists with the same name, they
are stored in the same page of Last.fm, which makes the biography
and social tags ambiguous. We automatically removed all ambigu-
ous artists by applying text processing on the biographies. The
song features provided with the MSD are not generally suitable
for deep learning, so we instead use audio previews between 7 and
30 seconds retrieved from 7digital.com. After removing ambigu-
ous artists and missing tracks, the final dataset consists of 328,821
tracks from 24,043 artists. Each track has at least 15 seconds of
audio, each biography is at least 50 characters long, and each artist
has at least 1 tag associated with it. All artist metadata, implicit
feedback matrices, and splits are released as a new dataset called
the MSD-A6.

6.2 Artist Recommendation
To investigate to what extent the different feature sets, data models
and architectures influence the quality of the deep artist features,
we evaluate the different approaches in an artist recommendation
scenario. Given the matrix of implicit feedback R, and the set of
artist and user factors obtained through matrix factorization (see
Section 2), we predict the artist factors for the test set, and use
them to compute a ranked list of recommended artists for every
user. We use mean average precision (MAP) with a cut-off at 500
recommendations per user.

We compare four different approaches using the biography texts
as input. (1) a pure text-based approach using a VSM and a feed-
forward network a-TEXT. (2) similar to (1) but with a semantically
enriched version of the texts a-sem (cf. Section 3.1). (3) An ap-
proach based on word embeddings initialized with vectors trained
on Google News and a CNN a-w2v-goo (cf. Section 3.2). (4) Similar
to (3) but inizializing the embeddings with word vectors previously
trained on the corpus of biographies a-w2v. To properly frame the
results, we compute two baselines and one competitor approach.
The tags baseline approach uses artist social tags as input features,
and text-rf uses biography texts as input, but Random Forest
Regression for the learning instead of a deep neural network. The
former baseline is added to compare the potential of biography
texts with respect to curated metadata, whilst the latter was added
to study to which extent the deep network improves the results
over other learning methods typically used in natural language
processing. There are few recommendation approaches able to
deal with an extreme cold-start scenario like ours. Therefore, we
select ItemAttributeKnn [10] as the competitor approach (tags-
itemKnn), using artist social tags as attribute data and computed
using the MyMediaLite library7. We also show the scores achieved
when the latent factor vectors are randomized (random), and when
they are learned from feedback data using matrix factorization
(upper-bound).

6https://doi.org/10.5281/zenodo.831348
7http://www.mymedialite.net/

Table 1: Artist Recommendation Results

Aproach

Input

Data model

Arch MAP

a-text
a-sem
a-w2v-goo
a-w2v

a-tags
tags-itemKnn
text-rf

random
upper-bound

Bio
Sem Bio
Bio
Bio

VSM
VSM
w2v-pretrain
w2v-trained

FF
FF
CNN
CNN

Tags
Tags
Bio

-
-

VSM
-
VSM

-
-

FF
itemKnn
RF

-
-

0.0161
0.0201
0.0119
0.0145

0.0314
0.0161
0.0089

0.0014
0.5528

Mean average precision (MAP) at 500 for the predictions of artist recommen-
dations in 1M users. VSM refers to Vector Space Model, FF to Feedforward,
RF to Random Forest, CNN to Convolutional Neural Network, and itemKnn
to itemAttributeKnn approach. Bio refers to biography texts and Sem Bio to
semantically enriched texts.

Results reported in Table 1 show that the semantic enrichment
of the biographies a-sem outperforms the pure text approach a-
text. As expected, the use of tags improves the results over the
use of text. However, the addition of semantic features reduces the
gap in performance between the use of tags and unstructured text.
Moreover, the difference between a-text and text-rf shows that
the use of deep learning with respect to random forest improves
the results. We also note that a VSM model with a feedforward
network outperforms the use of word embeddings with convolu-
tions. Although, according to the literature, this latter approach
has demonstrated its utility for simple tasks like binary classifica-
tion with short texts, our task puts forward two challenges for this
architecture: the greater length of the input texts, and the higher
dimensionality of the output. Although we have shown that initial-
izing the embedding layer with word vectors trained on the corpus
itself (a-w2v) outperforms the use of Google News pre-trained
vectors (a-w2v-goo), further work is necessary to properly opti-
mize a convolutional architecture for this task. Finally, we observe
that our approach a-tags outperforms the competitor approach
tags-itemKnn using the same item attributes.

Once the network is trained, we predict the activations of the
penultimate layer for the entire dataset of artists. Thus, we obtain a
vector embedding of 2048 dimensions, which represents the artist
deep features A(cid:48). From the evaluated approaches, we compute the
artist embedding from the a-sem and a-tags approaches.

6.3 Song Recommendation
In this experiment, audio embeddings are obtained after training
the convolutional network (see Section 4) with 260k patches of
15 seconds, corresponding to the 80% of the tracks described in
Section 6.1. Patches are divided into training (80%), validation (10%)
and test (10%) sets. Results reported in Table 2 are computed over
the remaining 20% of tracks. As opposed to [36], no artist appears
in more than one subset to avoid overfitting. Finally, multimodal
approaches are computed on the same sets.

In our experiments, we want to measure the impact of the artist
embeddings in the song recommendation problem, and also the

A Deep Multimodal Approach for Cold-start
Music Recommendation

Table 2: Song Recommendation Results

Approach

Artist Input Track Input Arch MAP

audio
sem-vsm
sem-emb

mm-lf-lin
mm-lf-h1
mm

tags-vsm
tags-emb

random
upper-bound

-
Sem Bio
a-sem

a-sem
a-sem
Sem Bio

Tags
a-tags

rnd emb
-

audio spec
-
-

CNN 0.0015
0.0032
0.0034

FF
FF

audio emb MLP 0.0036
0.0035
audio emb
MLP
CNN 0.0014
audio spec

-
-

-
-

FF
FF

FF
-

0.0043
0.0049

0.0002
0.1649

Mean average precision (MAP) at 500 for the predictions of song recom-
mendations in 1M users. audio emb refers to the track embedding of audio
approach, sem to artist embedding of sem approach, tags to artist embedding
of tags approach, spec to spectrogram, mm to multimodal, lf to late fusion,
lin to linear, and h1 to one hidden layer.

potential of the multimodal approach. We experimented with two
approaches, sem-emb and tags-emb, that exploit the feature em-
beddings learned from the artists features (see Section 6.2), either
based on biography texts (a-sem) or artists tags (a-tags). To mea-
sure the potential of the artist embeddings, we also computed two
approaches based on the original artist features (sem-vsm for se-
mantic text features and tags-vsm for tag features). Results on
Table 2 show that sem-emb and tags-emb outperform sem-vsm
and tags-vsm, suggesting that artist embeddings outperform artist
features.

An approach based on the audio spectrograms was computed
(audio). From this latter approach, audio embeddings where ob-
tained (audio emb) and combined with a-sem in a multimodal late
fusion approach mm-lf-lin (without hidden layers and l2-norm)
and mm-lf-h1 (with one hidden layer after each feature vector and
batch normalization) (cf. Section 5). We also tried with different
combinations of hidden layers and normalization steps in the mul-
timodal network but all of them yielded lower results than the ones
reported for mm-lf-lin and mm-lf-h1. We compared this network
with a multimodal approach trained directly on the original features
(semantically enriched text and audio spectrograms). Results on the
combination of artist and track features show that the late fusion of
artist and track embeddings mm-lf-lin outperforms the simultane-
ous training of artist and track features mm. In addition, we observe
that we achieve better results when no hidden layer is added to
the multimodal network mm-lf-lin. Finally, we observe that the
multimodal approach that combines text and audio features with
late fusion mm-lf-lin improves the results of pure text sem-emb
or pure audio audio approaches. All the differences between the
approaches are statistically significant (p < 0.01) according to the
paired t-test.

We also compared the results with an upper-bound approach
obtained from the feedback data and an approach trained with
random vector embeddings. Although results are in general far
from the upper-bound, the comparative analysis of the proposed
approaches gives some insights of the behavior of different feature

DLRS 2017, August 27, 2017, Como, Italy

representations and modalities in the cold-start recommendation
problem.

7 CONCLUSIONS
In this work, a multimodal approach for song recommendation has
been presented. The approach is divided into three steps. (1) Artist
feature embeddings are learned from text and semantic features
in an artist recommendation scenario using a deep network archi-
tecture. (2) Track feature embeddings are learned from the audio
spectrograms using convolutional neural networks. (3) Embeddings
are combined in a multimodal network.

Results show that splitting the problem of song recommendation
at artist and song levels improves the quality of recommendations.
Learning artist feature embeddings separately benefits from the
aggregation of the information about the different songs of the
same artist, yielding more robust artist features. Related to this,
an approach for the semantic enrichment of artist metadata has
been proposed, leading to a significant improvement in the results.
In addition, we have shown the potential of exploiting artist bi-
ographies in music recommendation. Moreover, the deep learning
architectures of this work have demonstrated their capacity to im-
prove upon other learning models under the music recommendation
framework. Finally, we have shown how a multimodal approach,
based on the late fusion of track and artist feature embeddings
that are learned separately, outperforms end-to-end multimodal
approaches where the different modalities are learned simultane-
ously. Moreover, results have shown that our multimodal approach
achieves better results than pure text or audio approaches. As fu-
ture work, we plan to do fine-tunning of the combined model after
pre-training the network on each modality separately.

ACKNOWLEDGMENTS
This work was partially funded by the Spanish Ministry of Economy
and Competitiveness under the Maria de Maeztu Units of Excellence
Programme (MDM-2015-0502). The Tesla K40 used for this research
was donated by the NVIDIA Corporation.

REFERENCES
[1] Trapit Bansal, David Belanger, and Andrew McCallum. 2016. Ask the GRU:
Multi-task Learning for Deep Text Recommendations. RecSys (2016), 107–114.
https://doi.org/10.1145/2959100.2959180 arXiv:1609.02116

[2] M. Rouvier Bechet, S. Delecraz, B. Favre, M. Bendris, and F. 2015. Multimodal
embedding fusion for robust speaker role recognition in video broadcast. IEEE
Workshop on Automatic Speech Recognition and Understanding (ASRU) (2015).
[3] Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003.
A Neural Probabilistic Language Model. The Journal of Machine Learn-
ing Research 3 (2003), 1137–1155. https://doi.org/10.1162/153244303322533223
arXiv:arXiv:1301.3781v3

[4] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere.
2011. The Million Song Dataset. In Proceedings of the 12th International Conference
on Music Information Retrieval (ISMIR 2011).

[5] Robin Burke. 2002. Hybrid Recommender Systems: Survey and Experiments.

[6]

User Modeling and User-Adapted Interaction 12, 4 (2002), 331–370.
`Oscar Celma. 2010. The Long Tail in Recommender Systems. Springer Berlin Hei-
delberg, Berlin, Heidelberg, 87–107. https://doi.org/10.1007/978-3-642-13287-2 4
[7] Franc¸ois Chollet. 2016. Information-theoretical label embeddings for large-scale
image classification. CoRR (2016), 1–10. arXiv:1607.05691 http://arxiv.org/abs/
1607.05691

[8] Ronan Collobert,

Jason Weston, L´eon Bottou, Michael Karlen, Koray
Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost)
Journal of Machine Learning Research 12 (2011), 2493–2537.
from Scratch.
https://doi.org/10.1.1.231.4614 arXiv:1103.0398

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

[32] Christian Sch¨orkhuber and Anssi Klapuri. 2010. Constant-Q transform toolbox
for music processing. 7th Sound and Music Computing Conference JANUARY
(2010), 3–64.

[33] Olga Slizovskaia, Emilia G´omez, and Gloria Haro. 2017. Musical instrument
recognition in user-generated videos using a multimodal convolutional neural
network architecture. In ICMR ’17: Proceedings of the 2017 ACM on International
Conference on Multimedia Retrieval. Bucharest, Romania.

[34] Nitish Srivastava and Ruslan Salakhutdinov. 2012. Learning representations for
multimodal data with deep belief nets. In International conference on machine
learning workshop.

[35] Douglas Turnbull, Luke Barrington, and Gert Lanckriet. 2008. Five approaches
to collecting tags for music. Proceedings of 9th the International Society for
Music Information Retrieval Conference (2008), 225–230. http://books.google.
com/books?hl=en

[36] Aaron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep
content-based music recommendation. Electronics and Information Systems
department (ELIS) (2013), 9. https://doi.org/10.1109/MMUL.2011.34.van
Justin Zobel and Alistair Moffat. 1998. Exploring the similarity space. ACM
SIGIR Forum 32, 1 (1998), 18–34. https://doi.org/10.1145/281250.281256

[37]

[9] Arthur Flexer. 2007. A Closer Look on Artist Filters for Musical Genre Classifica-
tion. In Proceedings of the 8th International Society for Music Information Retrieval
Conference.

[10] Zeno Gantner, Lucas Drumond, Christoph Freudenthaler, Steffen Rendle, and
Lars Schmidt-Thieme. 2010. Learning Attribute-to-Feature Mappings for Cold-
Start Recommendations. In Proceedings of the 2010 IEEE International Conference
on Data Mining (ICDM ’10). IEEE Computer Society, Washington, DC, USA,
176–185.

[11] C¸a˘glar G¨ulc¸ehre and Yoshua Bengio. 2016. Knowledge matters: Importance of
prior information for optimization. Journal of Machine Learning Research 17, 8
(2016), 1–32.

[12] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative Filtering for
Implicit Feedback Datasets. In Proceedings of the 2008 Eighth IEEE International
Conference on Data Mining (ICDM ’08). 263–272.

[13] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Acceler-
ating Deep Network Training by Reducing Internal Covariate Shift. CoRR
abs/1502.03167 (2015). http://arxiv.org/abs/1502.03167

[14] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP 2014) (2014), 1746–1751. https://doi.org/10.1109/LSP.2014.
2325781 arXiv:arXiv:1408.5882v1

[15] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic
Optimization. CoRR abs/1412.6980 (2014). http://arxiv.org/abs/1412.6980
[16] Peter Knees and Markus Schedl. 2013. A survey of music similarity and
recommendation from music context data. ACM Transactions on Multime-
dia Computing, Communications, and Applications 10, 1 (2013), 1–21. https:
//doi.org/10.1145/2542205.2542206

[17] Y. Koren, R. Bell, and C. Volinsky. 2009. Matrix Factorization Techniques for
Recommender Systems. Computer 42, 8 (2009), 42–49. https://doi.org/10.1109/
MC.2009.263 arXiv:ISSN 0018-9162

[18] Hugo Larochelle, Yoshua Bengio, J´erˆome Louradour, and Pascal Lamblin. 2009.
Exploring strategies for training deep neural networks. Journal of Machine
Learning Research 10, Jan (2009), 1–40.

[19] Brian McFee, Thierry Bertin-Mahieux, Daniel P.W. Ellis, and Gert R.G. Lanckriet.
2012. The million song dataset challenge. Proceedings of the 21st international
conference companion on World Wide Web - WWW ’12 Companion (2012), 909.
https://doi.org/10.1145/2187980.2188222

[20] Brian Mcfee, Colin Raffel, Dawen Liang, Daniel P W Ellis, Matt Mcvicar, Eric
Battenberg, and Oriol Nieto. 2015. librosa: Audio and Music Signal Analysis in
Python. PROC. OF THE 14th PYTHON IN SCIENCE CONF Scipy (2015), 1–7.
[21] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed
Representations of Words and Phrases and their Compositionality. Nips (2013),
1–9. https://doi.org/10.1162/jmlr.2003.3.4-5.951 arXiv:1310.4546

[22] Raymond J Mooney and Loriene Roy. 1999. Content-Based Book Recommending.
Proceedings of the SIGIR-99 Workshop on Recommender Systems: Algorithms and
Evaluation August (1999).

[23] Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking
meets Word Sense Disambiguation: A Unified Approach. Transactions of the
Association for Computational Linguistics 2 (2014), 231–244. https://tacl2013.cs.
columbia.edu/ojs/index.php/tacl/article/view/291

[24] Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic con-
struction, evaluation and application of a wide-coverage multilingual semantic
network. Artificial Intelligence 193 (2012), 217–250. https://doi.org/10.1016/j.
artint.2012.07.001
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and
Andrew Y Ng. 2011. Multimodal deep learning. In Proceedings of the 28th inter-
national conference on machine learning (ICML-11). 689–696.

[25]

[26] Kim Anh Nguyen, Sabine Schulte im Walde, and Ngoc Thang Vu. 2016. Neural-
based Noise Filtering from Word Embeddings. Coling-2016 (2016), 2699–2707.
arXiv:1610.01874 http://arxiv.org/abs/1610.01874

[27] Sergio Oramas, Luis Espinosa-Anke, Mohamed Sordo, Horacio Saggion, and
Xavier Serra. 2016. Information extraction for knowledge base construction in
the music domain. Data and Knowledge Engineering 106 (2016), 70–83. https:
//doi.org/10.1016/j.datak.2016.06.001

[28] Sergio Oramas, Vito Claudio Ostuni, Tommaso Di Noia, Xavier Serra, and Euge-
nio Di Sciascio. 2015. Sound and Music Recommendation with Knowledge Graphs.
ACM Trans. Intell. Syst. Technol. 9, 4 (2015), 1–21. https://doi.org/10.1145/2926718
[29] Sergio Oramas, Mohamed Sordo, Luis Espinosa-Anke, and Xavier Serra. 2015.
A Semantic-based Approach for Artist Similarity. Proceedings of 16th the In-
ternational Society for Music Information Retrieval Conference October (2015),
100–106.

[30] Martin Saveski and a Mantrach. 2014. Item cold-start recommendations: learning
local collective embeddings. RecSys ’14 Proceedings of the 8th ACM Conference on
Recommender systems (2014), 89–96. https://doi.org/10.1145/2645710.2645751

[31] Markus Schedl, Emilia G´omez, Juli´an Urbano, et al. 2014. Music information
retrieval: Recent developments and applications. Foundations and Trends® in
Information Retrieval 8, 2-3 (2014), 127–261.

A Deep Multimodal Approach for Cold-start
Music Recommendation

Sergio Oramas
Music Technology Group
Universitat Pompeu Fabra
sergio.oramas@upf.edu

Mohamed Sordo
Pandora Media, Inc.
Oakland, CA
msordo@pandora.com

Oriol Nieto
Pandora Media, Inc.
Oakland, CA
onieto@pandora.com

Xavier Serra
Music Technology Group
Universitat Pompeu Fabra
xavier.serra@upf.edu

7
1
0
2
 
l
u
J
 
4
2
 
 
]

R

I
.
s
c
[
 
 
2
v
9
3
7
9
0
.
6
0
7
1
:
v
i
X
r
a

ABSTRACT
An increasing amount of digital music is being published daily.
Music streaming services often ingest all available music, but this
poses a challenge: how to recommend new artists for which prior
knowledge is scarce? In this work we aim to address this so-called
cold-start problem by combining text and audio information with
user feedback data using deep network architectures. Our method
is divided into three steps. First, artist embeddings are learned from
biographies by combining semantics, text features, and aggregated
usage data. Second, track embeddings are learned from the audio
signal and available feedback data. Finally, artist and track em-
beddings are combined in a multimodal network. Results suggest
that both splitting the recommendation problem between feature
levels (i.e., artist metadata and audio track), and merging feature
embeddings in a multimodal approach improve the accuracy of the
recommendations.

CCS CONCEPTS
•Information systems → Recommender systems; •Computing
methodologies → Neural networks;

KEYWORDS
recommender systems, deep learning, multimodal, music, semantics

ACM Reference format:
Sergio Oramas, Oriol Nieto, Mohamed Sordo, and Xavier Serra. 2017. A
Deep Multimodal Approach for Cold-start
Music Recommendation. In Proceedings of DLRS 2017, Como, Italy, August
27, 2017, 6 pages.
DOI: 10.1145/3125486.3125492

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
DLRS 2017, Como, Italy
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5353-3/17/08. . . $15.00
DOI: 10.1145/3125486.3125492

1 INTRODUCTION
It is common for online music streaming services nowadays to
offer ever-growing catalogs with dozens of millions of music tracks.
Since manually managing these large libraries is not feasible due
to size constraints, automatic exploration and exploitation of large-
scale music collections has been an active area of research in recent
years [31]. While several existing algorithmic techniques are able
to produce successful recommendations for popular content [17],
the exploration of new or undiscovered artists (i.e., the long tail [6])
remains a major challenge that we aim to address.

Recommender systems can be broadly classified into collabora-
tive filtering (CF), content-based, and hybrid methods. CF methods
[17] use the item-user feedback matrix and predictions are based
on the similarity of user or item profiles. Matrix factorization tech-
niques are currently CF state-of-the-art [17]. CF methods suffer
from the cold-start problem, as new items do not have feedback
information [30]. Content-based methods [22] rely only on item
features, and recommendations are based on similarity between
such features. Finally, hybrid methods [5] try to combine both item
content and item-user feedback.

Social tags have been extensively used as a source of artist con-
tent features to recommend music [16]. However, these tags are
usually collectively annotated, which often introduce an artist pop-
ularity bias [35]. Artist biographies and press releases, on the other
hand, do not necessarily require a collaborative effort, as they may
be produced by artists themselves. However, they have seldom been
exploited for music recommendation [28]. Part of this work focuses
on learning artist features from these biographies. Furthermore,
we also make use of audio signals, since these are generally always
available and have shown to be helpful when recommending music
in the long tail [36].

According to [11], composing simpler tasks is more likely to
yield effective local minima for neural networks. In addition, as
stated in [18], directly training all the layers of a deep network
together make it difficult to exploit all the extra modeling power
of a deeper architecture. Therefore, we decided to separate the
problem of music recommendation into artist and song levels. Artist
feature embeddings are learned from artist metadata in an artist
recommendation scenario. Track feature embeddings are learned
from audio signals in a song recommendation scenario. In both
cases, a hybrid recommendation approach is used based on learning
attribute-to-feature mappings [10]. This method addresses the lack

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

of feedback for uncommon items in two steps: (1) factorizing the
collaborative matrix, and (2) learning a mapping between item
content features and item latent factors [1, 36]. Lastly, both feature
embeddings are combined in a multimodal network to predict song
recommendations of cold-start artists. We show how dividing the
problem into artists and songs, and combining text and audio in a
multimodal approach yields improved recommendations.

For the sake of reproducibility, source code and data splits used
in the experiments have been released1. Our main contributions in
this work are summarized as follows:

• Method to enrich artist biographies with semantic infor-

mation leveraging an external Knowledge Base.

• Dividing the problem of music recommendation into artist

and song levels to obtain better performance.

• A multimodal deep learning pipeline for music recommen-
dation that combines audio with text and yields improved
results.

• The release of an extended version of the Million Song

Dataset with artist biographies and tags.

2 RECOMMENDATION APPROACH
To produce cold-start music recommendations, we propose the
following framework. Given the set of artist features As of a song
s, and the set of track features Ts of s, the complete feature set
of s is defined as the aggregation of its artist and track features
Fs = As ∪ Ts .

Given the heterogeneity of these two feature sets (audio and
text), a learning process involving them may under-explore one of
the modalities, as the stronger modality may dominate quickly. To
ensure that the variability of the input data is fully represented, we
divide the problem into three phases (see Figure 1). First, we aggre-
gate the collaborative information of all songs of the same artist,
and learn an artist feature embedding A(cid:48)
from As in an artist rec-
s
ommendation scenario. Second, we learn a track feature embedding
T (cid:48)
from Ts in a pure audio-based recommendation scenario. Third,
s
we combine both feature embeddings A(cid:48)
in a multimodal
s
network and compute song recommendations.

and T (cid:48)
s

Since songs from the same artist share the same set As , if different
songs from the same artist appear in multiple sets (e.g., train and
test), a problem of overfitting may arise [9]. To approach this issue,
we use non-overlapping artists across the train, validation, and test
sets.

Let M be the matrix of implicit feedback, where mus is the num-
ber of play counts for user u on song s. M is split into Mt r ain ,
and Mt est , for train, validation and test, respectively, where
Mval
no artist is shared across sets. Factorizing Mt r ain using weighted
matrix factorization (WMF) [12] yields Ik
, the k dimensional
sets of song and user latent factors, respectively. We set k = 200,
and apply the alternating least squares (ALS) optimization method.
To learn the artist embeddings, we obtain the matrix of artist
implicit feedback R from M, being Rua = (cid:205)
s mus for all songs s
from the same artist a. This matrix is split into train, validation,
and test sets following the same partition of artists made for M,
and thus keeping the mutual exclusion restriction. Latent factors
of artists and users are later obtained via WMF. Lastly, a deep

and Uk

Figure 1: Model architecture.

neural network is trained on the prediction of artist latent factors
from artist content features A. On the other hand, the song latent
factors are predicted with a deep convolutional network, using Ik
as training data and the track features T as input (similar to [36]).
Once the artist and track models are trained and optimized, we
gather the activations from the penultimate layer of each network
for all the sets. These activations constitute what we call the artist
and track feature embeddings A(cid:48)
, which are in turn used as
s
input to a third network. This final multimodal network is trained
on the prediction of song latent factors Ik
. Finally,
the list of item recommendations for user u is obtained by ranking
the results of computing the dot product between the user latent
factor fu ∈ Uk

and the set of item factors.

from S (cid:48)
s

and T (cid:48)
s

The different architectures used in each one of the three neu-
ral networks involved in the approach are described in Sections
3 ,4, and 5, respectively. Nevertheless, all networks have a final
fully connected layer of 200 units2 with linear activation and l2-
normalization. In addition, mini batches of 32 items are randomly
sampled from the training data to compute the gradient in all net-
works, and Adam [15] is the optimizer used to train the models,
with the default suggested learning parameters. Given that the out-
put of the architectures are l2-normalized, we use cosine proximity
as the loss function, as in [7].

s ∪T (cid:48)
s

= A(cid:48)

3 ARTIST TEXT EMBEDDINGS
In this section we describe two different, competing approaches to
exploit artist texts in a deep learning process.

3.1 Semantic enrichment
We propose a method for enriching artist biographies by associat-
ing text fragments with relevant entities defined in online semantic
datasets, and then gathering relevant information about these en-
tities from a Knowledge Base (KB). For this purpose, we adopted
Babelfy, a state-of-the-art tool for Entity Linking (EL) [23], a task
to associate, for a given textual fragment candidate, the most suit-
able entry in a reference KB. Babelfy maps words from a given

1https://github.com/sergiooramas/tartarus

2to match the dimensions of the factors to be predicted.

A Deep Multimodal Approach for Cold-start
Music Recommendation

text to entities in the BabelNet Knowledge Base [24], which has a
direct mapping to DBpeadia3, a structured version of Wikipedia4.
We use semantic information about the identified entities coming
from DBpedia to enrich the biographies. DBpedia resources are
generally classified using the DBpedia Ontology, which is a shallow,
cross-domain ontology based on the most common infoboxes of
Wikipedia.

EL systems are useful for music recommendation [28]. However,
they are not optimized for the music domain, and are prone to
errors [27]. The application of a filtering process over the set of
identified entities based on their classification within the DBpedia
Ontology, has demonstrated its utility to improve music retrieval
tasks, such as artist similarity [29]. Therefore, we only keep entities
of classes related to the music domain such as MusicalArtist, Band,
MusicGenre, MusicalWork, RecordLabel, Instrument, Engineer, and
Place. Then, we query DBpedia to get all the available information
about the filtered entities. From the information gathered, we
keep some specific properties for every kind of entity, such as
homeTown, instrument, genre or associatedBand for MusicalArtists,
writer, producer or recordedIn for MusicalWorks, stylisticOrigin or
instrument for MusicGenres, and so on5. In addition, we also kept
all the Wikipedia categories associated to each entity. In Wikipedia,
categories are used to organize resources, and they help users to
group articles of the same subject.

To build the enriched biographies we proceed as follows: First,
Babelfy is applied over the biography texts. Second, information
is gathered from DBpedia for the entities of the selected classes.
Finally, the collected data are added at the end of the biography text
separated by spaces. A vector space model (VSM) is then applied
to the set of enriched biographies, and tf-idf weighting [37] is used.
We limited the vocabulary size to 10,000 terms for the VSM, as
this number provides a good trade-off between performance and
number of parameters required for training. Note that either words,
entities, dates, or categories may be part of this vocabulary. From
this data representation, a feedforward network with two dense
layers of 2048 neurons each is trained to predict the artist latent
factors.

3.2 Word embeddings
Much of the work with deep learning in Natural Language Pro-
cessing has involved the learning of word vector representations
[3, 21], and their further composition [8]. Word embeddings aim
to represent words as low-dimensional dense vectors. They have
demonstrated to greatly benefit NLP tasks, such as word similarity,
sentiment analysis, or parsing [26].

The use of convolutional neural networks (CNN) over pre-trained
word vectors has become state-of-the-art in sentence classification
[14]. We re-adapt the architecture proposed in [14] for sentence
classification to learn artist latent factors from artist biographies.
This consists in an embedding layer, followed by a one dimensional
convolutional layer with multiple filter widths, a max-over-time
pooling layer, a dense hidden layer and the output layer. We employ
the same architecture and parameters, changing only the output

3http://dbpedia.org
4http://wikipedia.org
5The
list
complete
http://mtg.upf.edu/download/datasets/msd-a

classes

of

and

properties

is

available

at

DLRS 2017, August 27, 2017, Como, Italy

layer and the loss function. We initialize the input embedding layer
of the network with word2vec word embeddings pre-trained on
the Google News dataset, and also with word embeddings trained
in our own corpus of biographies.

4 TRACK AUDIO EMBEDDINGS
It is common in the field of music informatics to make use of CNNs
to learn higher-level features from spectrograms. These representa-
tions are typically contained in RF×N matrices with F frequency
bins and N time frames. In this work we compute 96 frequency
bin, log-compressed constant-Q transforms (CQT) [32] for all the
tracks in our dataset using librosa [20] with the following param-
eters: audio sampling rate at 22050 Hz, hop length of 1024 samples,
Hann analysis window, and 12 bins per octave. Following a sim-
ilar approach to [36], we address the variability of the length N
across songs by sampling one 15-seconds long patch from each
track, resulting in the fixed-size input to the CNN.

The deep model trained with these data is defined as follows:
the CQT patches are fed to four convolutional layers with rectified
linear units (ReLU) as activations. The four convolutions have the
following number of filters, from first to last: 256, 512, 1024, and
1024. The convolutions are only applied to the time axis, leaving
the frequencies fixed since the absolute and relative bin placement
is important when aiming to capture particular sounds (as opposed
to the irrelevance of where in time a certain sonic event occurs).
Maxpooling of 4 units across the time axis is applied after each of
the first three ReLUs, and 50% dropout is applied to all layers. The
flattened output of the last layer has 4096 units, which becomes the
vector embedding to later use in the multimodal approach described
next.

5 MULTIMODAL FUSION
There are several approaches in the literature for multimodal fea-
ture learning [25, 34], and late fusion of multimodal feature vectors
[2, 33]. In this work, audio and text feature vectors are learned sep-
arately and then combined via late fusion in a multimodal network
(see Figure 1).

Given the different nature of the artist and track embeddings,
a normalization step is necessary. Normalized feature vectors are
then fed to a feed forward neural network (a simple Multi Layer
Perceptron, MLP). Two different architectures were explored: (i)
each embedding vector is connected to an isolated dense layer of
512 hidden units with ReLU activations after a process of batch
normalization [13]. Then, both dense layers are connected to the
output layer. The rationale behind this is that the isolated dense
layers help the network learn non-linearities from each modality
separately. (ii) each embedding vector is l2-normed and then con-
catenated into a single feature vector which is directly connected
to the output layer, resulting in a linear model. Regularization is
obtained by applying dropout with an empirically selected factor
of 70% after the input layer for both architectures.

6 EXPERIMENTS
6.1 Dataset
The Million Song Dataset (MSD) [19] is a collection of metadata
and precomputed audio features for 1 million songs. Along with

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

this dataset, the Echo Nest Taste Profile Subset [4] provides play
counts of 1 million users on more than 380,000 songs from the MSD.
Starting from this subset, we gather biographies and social tags
from Last.fm for all the artists that have at least one song in the
dataset. When there are several artists with the same name, they
are stored in the same page of Last.fm, which makes the biography
and social tags ambiguous. We automatically removed all ambigu-
ous artists by applying text processing on the biographies. The
song features provided with the MSD are not generally suitable
for deep learning, so we instead use audio previews between 7 and
30 seconds retrieved from 7digital.com. After removing ambigu-
ous artists and missing tracks, the final dataset consists of 328,821
tracks from 24,043 artists. Each track has at least 15 seconds of
audio, each biography is at least 50 characters long, and each artist
has at least 1 tag associated with it. All artist metadata, implicit
feedback matrices, and splits are released as a new dataset called
the MSD-A6.

6.2 Artist Recommendation
To investigate to what extent the different feature sets, data models
and architectures influence the quality of the deep artist features,
we evaluate the different approaches in an artist recommendation
scenario. Given the matrix of implicit feedback R, and the set of
artist and user factors obtained through matrix factorization (see
Section 2), we predict the artist factors for the test set, and use
them to compute a ranked list of recommended artists for every
user. We use mean average precision (MAP) with a cut-off at 500
recommendations per user.

We compare four different approaches using the biography texts
as input. (1) a pure text-based approach using a VSM and a feed-
forward network a-TEXT. (2) similar to (1) but with a semantically
enriched version of the texts a-sem (cf. Section 3.1). (3) An ap-
proach based on word embeddings initialized with vectors trained
on Google News and a CNN a-w2v-goo (cf. Section 3.2). (4) Similar
to (3) but inizializing the embeddings with word vectors previously
trained on the corpus of biographies a-w2v. To properly frame the
results, we compute two baselines and one competitor approach.
The tags baseline approach uses artist social tags as input features,
and text-rf uses biography texts as input, but Random Forest
Regression for the learning instead of a deep neural network. The
former baseline is added to compare the potential of biography
texts with respect to curated metadata, whilst the latter was added
to study to which extent the deep network improves the results
over other learning methods typically used in natural language
processing. There are few recommendation approaches able to
deal with an extreme cold-start scenario like ours. Therefore, we
select ItemAttributeKnn [10] as the competitor approach (tags-
itemKnn), using artist social tags as attribute data and computed
using the MyMediaLite library7. We also show the scores achieved
when the latent factor vectors are randomized (random), and when
they are learned from feedback data using matrix factorization
(upper-bound).

6https://doi.org/10.5281/zenodo.831348
7http://www.mymedialite.net/

Table 1: Artist Recommendation Results

Aproach

Input

Data model

Arch MAP

a-text
a-sem
a-w2v-goo
a-w2v

a-tags
tags-itemKnn
text-rf

random
upper-bound

Bio
Sem Bio
Bio
Bio

VSM
VSM
w2v-pretrain
w2v-trained

FF
FF
CNN
CNN

Tags
Tags
Bio

-
-

VSM
-
VSM

-
-

FF
itemKnn
RF

-
-

0.0161
0.0201
0.0119
0.0145

0.0314
0.0161
0.0089

0.0014
0.5528

Mean average precision (MAP) at 500 for the predictions of artist recommen-
dations in 1M users. VSM refers to Vector Space Model, FF to Feedforward,
RF to Random Forest, CNN to Convolutional Neural Network, and itemKnn
to itemAttributeKnn approach. Bio refers to biography texts and Sem Bio to
semantically enriched texts.

Results reported in Table 1 show that the semantic enrichment
of the biographies a-sem outperforms the pure text approach a-
text. As expected, the use of tags improves the results over the
use of text. However, the addition of semantic features reduces the
gap in performance between the use of tags and unstructured text.
Moreover, the difference between a-text and text-rf shows that
the use of deep learning with respect to random forest improves
the results. We also note that a VSM model with a feedforward
network outperforms the use of word embeddings with convolu-
tions. Although, according to the literature, this latter approach
has demonstrated its utility for simple tasks like binary classifica-
tion with short texts, our task puts forward two challenges for this
architecture: the greater length of the input texts, and the higher
dimensionality of the output. Although we have shown that initial-
izing the embedding layer with word vectors trained on the corpus
itself (a-w2v) outperforms the use of Google News pre-trained
vectors (a-w2v-goo), further work is necessary to properly opti-
mize a convolutional architecture for this task. Finally, we observe
that our approach a-tags outperforms the competitor approach
tags-itemKnn using the same item attributes.

Once the network is trained, we predict the activations of the
penultimate layer for the entire dataset of artists. Thus, we obtain a
vector embedding of 2048 dimensions, which represents the artist
deep features A(cid:48). From the evaluated approaches, we compute the
artist embedding from the a-sem and a-tags approaches.

6.3 Song Recommendation
In this experiment, audio embeddings are obtained after training
the convolutional network (see Section 4) with 260k patches of
15 seconds, corresponding to the 80% of the tracks described in
Section 6.1. Patches are divided into training (80%), validation (10%)
and test (10%) sets. Results reported in Table 2 are computed over
the remaining 20% of tracks. As opposed to [36], no artist appears
in more than one subset to avoid overfitting. Finally, multimodal
approaches are computed on the same sets.

In our experiments, we want to measure the impact of the artist
embeddings in the song recommendation problem, and also the

A Deep Multimodal Approach for Cold-start
Music Recommendation

Table 2: Song Recommendation Results

Approach

Artist Input Track Input Arch MAP

audio
sem-vsm
sem-emb

mm-lf-lin
mm-lf-h1
mm

tags-vsm
tags-emb

random
upper-bound

-
Sem Bio
a-sem

a-sem
a-sem
Sem Bio

Tags
a-tags

rnd emb
-

audio spec
-
-

CNN 0.0015
0.0032
0.0034

FF
FF

audio emb MLP 0.0036
0.0035
audio emb
MLP
CNN 0.0014
audio spec

-
-

-
-

FF
FF

FF
-

0.0043
0.0049

0.0002
0.1649

Mean average precision (MAP) at 500 for the predictions of song recom-
mendations in 1M users. audio emb refers to the track embedding of audio
approach, sem to artist embedding of sem approach, tags to artist embedding
of tags approach, spec to spectrogram, mm to multimodal, lf to late fusion,
lin to linear, and h1 to one hidden layer.

potential of the multimodal approach. We experimented with two
approaches, sem-emb and tags-emb, that exploit the feature em-
beddings learned from the artists features (see Section 6.2), either
based on biography texts (a-sem) or artists tags (a-tags). To mea-
sure the potential of the artist embeddings, we also computed two
approaches based on the original artist features (sem-vsm for se-
mantic text features and tags-vsm for tag features). Results on
Table 2 show that sem-emb and tags-emb outperform sem-vsm
and tags-vsm, suggesting that artist embeddings outperform artist
features.

An approach based on the audio spectrograms was computed
(audio). From this latter approach, audio embeddings where ob-
tained (audio emb) and combined with a-sem in a multimodal late
fusion approach mm-lf-lin (without hidden layers and l2-norm)
and mm-lf-h1 (with one hidden layer after each feature vector and
batch normalization) (cf. Section 5). We also tried with different
combinations of hidden layers and normalization steps in the mul-
timodal network but all of them yielded lower results than the ones
reported for mm-lf-lin and mm-lf-h1. We compared this network
with a multimodal approach trained directly on the original features
(semantically enriched text and audio spectrograms). Results on the
combination of artist and track features show that the late fusion of
artist and track embeddings mm-lf-lin outperforms the simultane-
ous training of artist and track features mm. In addition, we observe
that we achieve better results when no hidden layer is added to
the multimodal network mm-lf-lin. Finally, we observe that the
multimodal approach that combines text and audio features with
late fusion mm-lf-lin improves the results of pure text sem-emb
or pure audio audio approaches. All the differences between the
approaches are statistically significant (p < 0.01) according to the
paired t-test.

We also compared the results with an upper-bound approach
obtained from the feedback data and an approach trained with
random vector embeddings. Although results are in general far
from the upper-bound, the comparative analysis of the proposed
approaches gives some insights of the behavior of different feature

DLRS 2017, August 27, 2017, Como, Italy

representations and modalities in the cold-start recommendation
problem.

7 CONCLUSIONS
In this work, a multimodal approach for song recommendation has
been presented. The approach is divided into three steps. (1) Artist
feature embeddings are learned from text and semantic features
in an artist recommendation scenario using a deep network archi-
tecture. (2) Track feature embeddings are learned from the audio
spectrograms using convolutional neural networks. (3) Embeddings
are combined in a multimodal network.

Results show that splitting the problem of song recommendation
at artist and song levels improves the quality of recommendations.
Learning artist feature embeddings separately benefits from the
aggregation of the information about the different songs of the
same artist, yielding more robust artist features. Related to this,
an approach for the semantic enrichment of artist metadata has
been proposed, leading to a significant improvement in the results.
In addition, we have shown the potential of exploiting artist bi-
ographies in music recommendation. Moreover, the deep learning
architectures of this work have demonstrated their capacity to im-
prove upon other learning models under the music recommendation
framework. Finally, we have shown how a multimodal approach,
based on the late fusion of track and artist feature embeddings
that are learned separately, outperforms end-to-end multimodal
approaches where the different modalities are learned simultane-
ously. Moreover, results have shown that our multimodal approach
achieves better results than pure text or audio approaches. As fu-
ture work, we plan to do fine-tunning of the combined model after
pre-training the network on each modality separately.

ACKNOWLEDGMENTS
This work was partially funded by the Spanish Ministry of Economy
and Competitiveness under the Maria de Maeztu Units of Excellence
Programme (MDM-2015-0502). The Tesla K40 used for this research
was donated by the NVIDIA Corporation.

REFERENCES
[1] Trapit Bansal, David Belanger, and Andrew McCallum. 2016. Ask the GRU:
Multi-task Learning for Deep Text Recommendations. RecSys (2016), 107–114.
https://doi.org/10.1145/2959100.2959180 arXiv:1609.02116

[2] M. Rouvier Bechet, S. Delecraz, B. Favre, M. Bendris, and F. 2015. Multimodal
embedding fusion for robust speaker role recognition in video broadcast. IEEE
Workshop on Automatic Speech Recognition and Understanding (ASRU) (2015).
[3] Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003.
A Neural Probabilistic Language Model. The Journal of Machine Learn-
ing Research 3 (2003), 1137–1155. https://doi.org/10.1162/153244303322533223
arXiv:arXiv:1301.3781v3

[4] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere.
2011. The Million Song Dataset. In Proceedings of the 12th International Conference
on Music Information Retrieval (ISMIR 2011).

[5] Robin Burke. 2002. Hybrid Recommender Systems: Survey and Experiments.

[6]

User Modeling and User-Adapted Interaction 12, 4 (2002), 331–370.
`Oscar Celma. 2010. The Long Tail in Recommender Systems. Springer Berlin Hei-
delberg, Berlin, Heidelberg, 87–107. https://doi.org/10.1007/978-3-642-13287-2 4
[7] Franc¸ois Chollet. 2016. Information-theoretical label embeddings for large-scale
image classification. CoRR (2016), 1–10. arXiv:1607.05691 http://arxiv.org/abs/
1607.05691

[8] Ronan Collobert,

Jason Weston, L´eon Bottou, Michael Karlen, Koray
Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost)
Journal of Machine Learning Research 12 (2011), 2493–2537.
from Scratch.
https://doi.org/10.1.1.231.4614 arXiv:1103.0398

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

[32] Christian Sch¨orkhuber and Anssi Klapuri. 2010. Constant-Q transform toolbox
for music processing. 7th Sound and Music Computing Conference JANUARY
(2010), 3–64.

[33] Olga Slizovskaia, Emilia G´omez, and Gloria Haro. 2017. Musical instrument
recognition in user-generated videos using a multimodal convolutional neural
network architecture. In ICMR ’17: Proceedings of the 2017 ACM on International
Conference on Multimedia Retrieval. Bucharest, Romania.

[34] Nitish Srivastava and Ruslan Salakhutdinov. 2012. Learning representations for
multimodal data with deep belief nets. In International conference on machine
learning workshop.

[35] Douglas Turnbull, Luke Barrington, and Gert Lanckriet. 2008. Five approaches
to collecting tags for music. Proceedings of 9th the International Society for
Music Information Retrieval Conference (2008), 225–230. http://books.google.
com/books?hl=en

[36] Aaron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep
content-based music recommendation. Electronics and Information Systems
department (ELIS) (2013), 9. https://doi.org/10.1109/MMUL.2011.34.van
Justin Zobel and Alistair Moffat. 1998. Exploring the similarity space. ACM
SIGIR Forum 32, 1 (1998), 18–34. https://doi.org/10.1145/281250.281256

[37]

[9] Arthur Flexer. 2007. A Closer Look on Artist Filters for Musical Genre Classifica-
tion. In Proceedings of the 8th International Society for Music Information Retrieval
Conference.

[10] Zeno Gantner, Lucas Drumond, Christoph Freudenthaler, Steffen Rendle, and
Lars Schmidt-Thieme. 2010. Learning Attribute-to-Feature Mappings for Cold-
Start Recommendations. In Proceedings of the 2010 IEEE International Conference
on Data Mining (ICDM ’10). IEEE Computer Society, Washington, DC, USA,
176–185.

[11] C¸a˘glar G¨ulc¸ehre and Yoshua Bengio. 2016. Knowledge matters: Importance of
prior information for optimization. Journal of Machine Learning Research 17, 8
(2016), 1–32.

[12] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative Filtering for
Implicit Feedback Datasets. In Proceedings of the 2008 Eighth IEEE International
Conference on Data Mining (ICDM ’08). 263–272.

[13] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Acceler-
ating Deep Network Training by Reducing Internal Covariate Shift. CoRR
abs/1502.03167 (2015). http://arxiv.org/abs/1502.03167

[14] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP 2014) (2014), 1746–1751. https://doi.org/10.1109/LSP.2014.
2325781 arXiv:arXiv:1408.5882v1

[15] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic
Optimization. CoRR abs/1412.6980 (2014). http://arxiv.org/abs/1412.6980
[16] Peter Knees and Markus Schedl. 2013. A survey of music similarity and
recommendation from music context data. ACM Transactions on Multime-
dia Computing, Communications, and Applications 10, 1 (2013), 1–21. https:
//doi.org/10.1145/2542205.2542206

[17] Y. Koren, R. Bell, and C. Volinsky. 2009. Matrix Factorization Techniques for
Recommender Systems. Computer 42, 8 (2009), 42–49. https://doi.org/10.1109/
MC.2009.263 arXiv:ISSN 0018-9162

[18] Hugo Larochelle, Yoshua Bengio, J´erˆome Louradour, and Pascal Lamblin. 2009.
Exploring strategies for training deep neural networks. Journal of Machine
Learning Research 10, Jan (2009), 1–40.

[19] Brian McFee, Thierry Bertin-Mahieux, Daniel P.W. Ellis, and Gert R.G. Lanckriet.
2012. The million song dataset challenge. Proceedings of the 21st international
conference companion on World Wide Web - WWW ’12 Companion (2012), 909.
https://doi.org/10.1145/2187980.2188222

[20] Brian Mcfee, Colin Raffel, Dawen Liang, Daniel P W Ellis, Matt Mcvicar, Eric
Battenberg, and Oriol Nieto. 2015. librosa: Audio and Music Signal Analysis in
Python. PROC. OF THE 14th PYTHON IN SCIENCE CONF Scipy (2015), 1–7.
[21] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed
Representations of Words and Phrases and their Compositionality. Nips (2013),
1–9. https://doi.org/10.1162/jmlr.2003.3.4-5.951 arXiv:1310.4546

[22] Raymond J Mooney and Loriene Roy. 1999. Content-Based Book Recommending.
Proceedings of the SIGIR-99 Workshop on Recommender Systems: Algorithms and
Evaluation August (1999).

[23] Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking
meets Word Sense Disambiguation: A Unified Approach. Transactions of the
Association for Computational Linguistics 2 (2014), 231–244. https://tacl2013.cs.
columbia.edu/ojs/index.php/tacl/article/view/291

[24] Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic con-
struction, evaluation and application of a wide-coverage multilingual semantic
network. Artificial Intelligence 193 (2012), 217–250. https://doi.org/10.1016/j.
artint.2012.07.001
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and
Andrew Y Ng. 2011. Multimodal deep learning. In Proceedings of the 28th inter-
national conference on machine learning (ICML-11). 689–696.

[25]

[26] Kim Anh Nguyen, Sabine Schulte im Walde, and Ngoc Thang Vu. 2016. Neural-
based Noise Filtering from Word Embeddings. Coling-2016 (2016), 2699–2707.
arXiv:1610.01874 http://arxiv.org/abs/1610.01874

[27] Sergio Oramas, Luis Espinosa-Anke, Mohamed Sordo, Horacio Saggion, and
Xavier Serra. 2016. Information extraction for knowledge base construction in
the music domain. Data and Knowledge Engineering 106 (2016), 70–83. https:
//doi.org/10.1016/j.datak.2016.06.001

[28] Sergio Oramas, Vito Claudio Ostuni, Tommaso Di Noia, Xavier Serra, and Euge-
nio Di Sciascio. 2015. Sound and Music Recommendation with Knowledge Graphs.
ACM Trans. Intell. Syst. Technol. 9, 4 (2015), 1–21. https://doi.org/10.1145/2926718
[29] Sergio Oramas, Mohamed Sordo, Luis Espinosa-Anke, and Xavier Serra. 2015.
A Semantic-based Approach for Artist Similarity. Proceedings of 16th the In-
ternational Society for Music Information Retrieval Conference October (2015),
100–106.

[30] Martin Saveski and a Mantrach. 2014. Item cold-start recommendations: learning
local collective embeddings. RecSys ’14 Proceedings of the 8th ACM Conference on
Recommender systems (2014), 89–96. https://doi.org/10.1145/2645710.2645751

[31] Markus Schedl, Emilia G´omez, Juli´an Urbano, et al. 2014. Music information
retrieval: Recent developments and applications. Foundations and Trends® in
Information Retrieval 8, 2-3 (2014), 127–261.

A Deep Multimodal Approach for Cold-start
Music Recommendation

Sergio Oramas
Music Technology Group
Universitat Pompeu Fabra
sergio.oramas@upf.edu

Mohamed Sordo
Pandora Media, Inc.
Oakland, CA
msordo@pandora.com

Oriol Nieto
Pandora Media, Inc.
Oakland, CA
onieto@pandora.com

Xavier Serra
Music Technology Group
Universitat Pompeu Fabra
xavier.serra@upf.edu

7
1
0
2
 
l
u
J
 
4
2
 
 
]

R

I
.
s
c
[
 
 
2
v
9
3
7
9
0
.
6
0
7
1
:
v
i
X
r
a

ABSTRACT
An increasing amount of digital music is being published daily.
Music streaming services often ingest all available music, but this
poses a challenge: how to recommend new artists for which prior
knowledge is scarce? In this work we aim to address this so-called
cold-start problem by combining text and audio information with
user feedback data using deep network architectures. Our method
is divided into three steps. First, artist embeddings are learned from
biographies by combining semantics, text features, and aggregated
usage data. Second, track embeddings are learned from the audio
signal and available feedback data. Finally, artist and track em-
beddings are combined in a multimodal network. Results suggest
that both splitting the recommendation problem between feature
levels (i.e., artist metadata and audio track), and merging feature
embeddings in a multimodal approach improve the accuracy of the
recommendations.

CCS CONCEPTS
•Information systems → Recommender systems; •Computing
methodologies → Neural networks;

KEYWORDS
recommender systems, deep learning, multimodal, music, semantics

ACM Reference format:
Sergio Oramas, Oriol Nieto, Mohamed Sordo, and Xavier Serra. 2017. A
Deep Multimodal Approach for Cold-start
Music Recommendation. In Proceedings of DLRS 2017, Como, Italy, August
27, 2017, 6 pages.
DOI: 10.1145/3125486.3125492

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
DLRS 2017, Como, Italy
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5353-3/17/08. . . $15.00
DOI: 10.1145/3125486.3125492

1 INTRODUCTION
It is common for online music streaming services nowadays to
offer ever-growing catalogs with dozens of millions of music tracks.
Since manually managing these large libraries is not feasible due
to size constraints, automatic exploration and exploitation of large-
scale music collections has been an active area of research in recent
years [31]. While several existing algorithmic techniques are able
to produce successful recommendations for popular content [17],
the exploration of new or undiscovered artists (i.e., the long tail [6])
remains a major challenge that we aim to address.

Recommender systems can be broadly classified into collabora-
tive filtering (CF), content-based, and hybrid methods. CF methods
[17] use the item-user feedback matrix and predictions are based
on the similarity of user or item profiles. Matrix factorization tech-
niques are currently CF state-of-the-art [17]. CF methods suffer
from the cold-start problem, as new items do not have feedback
information [30]. Content-based methods [22] rely only on item
features, and recommendations are based on similarity between
such features. Finally, hybrid methods [5] try to combine both item
content and item-user feedback.

Social tags have been extensively used as a source of artist con-
tent features to recommend music [16]. However, these tags are
usually collectively annotated, which often introduce an artist pop-
ularity bias [35]. Artist biographies and press releases, on the other
hand, do not necessarily require a collaborative effort, as they may
be produced by artists themselves. However, they have seldom been
exploited for music recommendation [28]. Part of this work focuses
on learning artist features from these biographies. Furthermore,
we also make use of audio signals, since these are generally always
available and have shown to be helpful when recommending music
in the long tail [36].

According to [11], composing simpler tasks is more likely to
yield effective local minima for neural networks. In addition, as
stated in [18], directly training all the layers of a deep network
together make it difficult to exploit all the extra modeling power
of a deeper architecture. Therefore, we decided to separate the
problem of music recommendation into artist and song levels. Artist
feature embeddings are learned from artist metadata in an artist
recommendation scenario. Track feature embeddings are learned
from audio signals in a song recommendation scenario. In both
cases, a hybrid recommendation approach is used based on learning
attribute-to-feature mappings [10]. This method addresses the lack

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

of feedback for uncommon items in two steps: (1) factorizing the
collaborative matrix, and (2) learning a mapping between item
content features and item latent factors [1, 36]. Lastly, both feature
embeddings are combined in a multimodal network to predict song
recommendations of cold-start artists. We show how dividing the
problem into artists and songs, and combining text and audio in a
multimodal approach yields improved recommendations.

For the sake of reproducibility, source code and data splits used
in the experiments have been released1. Our main contributions in
this work are summarized as follows:

• Method to enrich artist biographies with semantic infor-

mation leveraging an external Knowledge Base.

• Dividing the problem of music recommendation into artist

and song levels to obtain better performance.

• A multimodal deep learning pipeline for music recommen-
dation that combines audio with text and yields improved
results.

• The release of an extended version of the Million Song

Dataset with artist biographies and tags.

2 RECOMMENDATION APPROACH
To produce cold-start music recommendations, we propose the
following framework. Given the set of artist features As of a song
s, and the set of track features Ts of s, the complete feature set
of s is defined as the aggregation of its artist and track features
Fs = As ∪ Ts .

Given the heterogeneity of these two feature sets (audio and
text), a learning process involving them may under-explore one of
the modalities, as the stronger modality may dominate quickly. To
ensure that the variability of the input data is fully represented, we
divide the problem into three phases (see Figure 1). First, we aggre-
gate the collaborative information of all songs of the same artist,
and learn an artist feature embedding A(cid:48)
from As in an artist rec-
s
ommendation scenario. Second, we learn a track feature embedding
T (cid:48)
from Ts in a pure audio-based recommendation scenario. Third,
s
we combine both feature embeddings A(cid:48)
in a multimodal
s
network and compute song recommendations.

and T (cid:48)
s

Since songs from the same artist share the same set As , if different
songs from the same artist appear in multiple sets (e.g., train and
test), a problem of overfitting may arise [9]. To approach this issue,
we use non-overlapping artists across the train, validation, and test
sets.

Let M be the matrix of implicit feedback, where mus is the num-
ber of play counts for user u on song s. M is split into Mt r ain ,
and Mt est , for train, validation and test, respectively, where
Mval
no artist is shared across sets. Factorizing Mt r ain using weighted
matrix factorization (WMF) [12] yields Ik
, the k dimensional
sets of song and user latent factors, respectively. We set k = 200,
and apply the alternating least squares (ALS) optimization method.
To learn the artist embeddings, we obtain the matrix of artist
implicit feedback R from M, being Rua = (cid:205)
s mus for all songs s
from the same artist a. This matrix is split into train, validation,
and test sets following the same partition of artists made for M,
and thus keeping the mutual exclusion restriction. Latent factors
of artists and users are later obtained via WMF. Lastly, a deep

and Uk

Figure 1: Model architecture.

neural network is trained on the prediction of artist latent factors
from artist content features A. On the other hand, the song latent
factors are predicted with a deep convolutional network, using Ik
as training data and the track features T as input (similar to [36]).
Once the artist and track models are trained and optimized, we
gather the activations from the penultimate layer of each network
for all the sets. These activations constitute what we call the artist
and track feature embeddings A(cid:48)
, which are in turn used as
s
input to a third network. This final multimodal network is trained
on the prediction of song latent factors Ik
. Finally,
the list of item recommendations for user u is obtained by ranking
the results of computing the dot product between the user latent
factor fu ∈ Uk

and the set of item factors.

from S (cid:48)
s

and T (cid:48)
s

The different architectures used in each one of the three neu-
ral networks involved in the approach are described in Sections
3 ,4, and 5, respectively. Nevertheless, all networks have a final
fully connected layer of 200 units2 with linear activation and l2-
normalization. In addition, mini batches of 32 items are randomly
sampled from the training data to compute the gradient in all net-
works, and Adam [15] is the optimizer used to train the models,
with the default suggested learning parameters. Given that the out-
put of the architectures are l2-normalized, we use cosine proximity
as the loss function, as in [7].

s ∪T (cid:48)
s

= A(cid:48)

3 ARTIST TEXT EMBEDDINGS
In this section we describe two different, competing approaches to
exploit artist texts in a deep learning process.

3.1 Semantic enrichment
We propose a method for enriching artist biographies by associat-
ing text fragments with relevant entities defined in online semantic
datasets, and then gathering relevant information about these en-
tities from a Knowledge Base (KB). For this purpose, we adopted
Babelfy, a state-of-the-art tool for Entity Linking (EL) [23], a task
to associate, for a given textual fragment candidate, the most suit-
able entry in a reference KB. Babelfy maps words from a given

1https://github.com/sergiooramas/tartarus

2to match the dimensions of the factors to be predicted.

A Deep Multimodal Approach for Cold-start
Music Recommendation

text to entities in the BabelNet Knowledge Base [24], which has a
direct mapping to DBpeadia3, a structured version of Wikipedia4.
We use semantic information about the identified entities coming
from DBpedia to enrich the biographies. DBpedia resources are
generally classified using the DBpedia Ontology, which is a shallow,
cross-domain ontology based on the most common infoboxes of
Wikipedia.

EL systems are useful for music recommendation [28]. However,
they are not optimized for the music domain, and are prone to
errors [27]. The application of a filtering process over the set of
identified entities based on their classification within the DBpedia
Ontology, has demonstrated its utility to improve music retrieval
tasks, such as artist similarity [29]. Therefore, we only keep entities
of classes related to the music domain such as MusicalArtist, Band,
MusicGenre, MusicalWork, RecordLabel, Instrument, Engineer, and
Place. Then, we query DBpedia to get all the available information
about the filtered entities. From the information gathered, we
keep some specific properties for every kind of entity, such as
homeTown, instrument, genre or associatedBand for MusicalArtists,
writer, producer or recordedIn for MusicalWorks, stylisticOrigin or
instrument for MusicGenres, and so on5. In addition, we also kept
all the Wikipedia categories associated to each entity. In Wikipedia,
categories are used to organize resources, and they help users to
group articles of the same subject.

To build the enriched biographies we proceed as follows: First,
Babelfy is applied over the biography texts. Second, information
is gathered from DBpedia for the entities of the selected classes.
Finally, the collected data are added at the end of the biography text
separated by spaces. A vector space model (VSM) is then applied
to the set of enriched biographies, and tf-idf weighting [37] is used.
We limited the vocabulary size to 10,000 terms for the VSM, as
this number provides a good trade-off between performance and
number of parameters required for training. Note that either words,
entities, dates, or categories may be part of this vocabulary. From
this data representation, a feedforward network with two dense
layers of 2048 neurons each is trained to predict the artist latent
factors.

3.2 Word embeddings
Much of the work with deep learning in Natural Language Pro-
cessing has involved the learning of word vector representations
[3, 21], and their further composition [8]. Word embeddings aim
to represent words as low-dimensional dense vectors. They have
demonstrated to greatly benefit NLP tasks, such as word similarity,
sentiment analysis, or parsing [26].

The use of convolutional neural networks (CNN) over pre-trained
word vectors has become state-of-the-art in sentence classification
[14]. We re-adapt the architecture proposed in [14] for sentence
classification to learn artist latent factors from artist biographies.
This consists in an embedding layer, followed by a one dimensional
convolutional layer with multiple filter widths, a max-over-time
pooling layer, a dense hidden layer and the output layer. We employ
the same architecture and parameters, changing only the output

3http://dbpedia.org
4http://wikipedia.org
5The
list
complete
http://mtg.upf.edu/download/datasets/msd-a

classes

of

and

properties

is

available

at

DLRS 2017, August 27, 2017, Como, Italy

layer and the loss function. We initialize the input embedding layer
of the network with word2vec word embeddings pre-trained on
the Google News dataset, and also with word embeddings trained
in our own corpus of biographies.

4 TRACK AUDIO EMBEDDINGS
It is common in the field of music informatics to make use of CNNs
to learn higher-level features from spectrograms. These representa-
tions are typically contained in RF×N matrices with F frequency
bins and N time frames. In this work we compute 96 frequency
bin, log-compressed constant-Q transforms (CQT) [32] for all the
tracks in our dataset using librosa [20] with the following param-
eters: audio sampling rate at 22050 Hz, hop length of 1024 samples,
Hann analysis window, and 12 bins per octave. Following a sim-
ilar approach to [36], we address the variability of the length N
across songs by sampling one 15-seconds long patch from each
track, resulting in the fixed-size input to the CNN.

The deep model trained with these data is defined as follows:
the CQT patches are fed to four convolutional layers with rectified
linear units (ReLU) as activations. The four convolutions have the
following number of filters, from first to last: 256, 512, 1024, and
1024. The convolutions are only applied to the time axis, leaving
the frequencies fixed since the absolute and relative bin placement
is important when aiming to capture particular sounds (as opposed
to the irrelevance of where in time a certain sonic event occurs).
Maxpooling of 4 units across the time axis is applied after each of
the first three ReLUs, and 50% dropout is applied to all layers. The
flattened output of the last layer has 4096 units, which becomes the
vector embedding to later use in the multimodal approach described
next.

5 MULTIMODAL FUSION
There are several approaches in the literature for multimodal fea-
ture learning [25, 34], and late fusion of multimodal feature vectors
[2, 33]. In this work, audio and text feature vectors are learned sep-
arately and then combined via late fusion in a multimodal network
(see Figure 1).

Given the different nature of the artist and track embeddings,
a normalization step is necessary. Normalized feature vectors are
then fed to a feed forward neural network (a simple Multi Layer
Perceptron, MLP). Two different architectures were explored: (i)
each embedding vector is connected to an isolated dense layer of
512 hidden units with ReLU activations after a process of batch
normalization [13]. Then, both dense layers are connected to the
output layer. The rationale behind this is that the isolated dense
layers help the network learn non-linearities from each modality
separately. (ii) each embedding vector is l2-normed and then con-
catenated into a single feature vector which is directly connected
to the output layer, resulting in a linear model. Regularization is
obtained by applying dropout with an empirically selected factor
of 70% after the input layer for both architectures.

6 EXPERIMENTS
6.1 Dataset
The Million Song Dataset (MSD) [19] is a collection of metadata
and precomputed audio features for 1 million songs. Along with

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

this dataset, the Echo Nest Taste Profile Subset [4] provides play
counts of 1 million users on more than 380,000 songs from the MSD.
Starting from this subset, we gather biographies and social tags
from Last.fm for all the artists that have at least one song in the
dataset. When there are several artists with the same name, they
are stored in the same page of Last.fm, which makes the biography
and social tags ambiguous. We automatically removed all ambigu-
ous artists by applying text processing on the biographies. The
song features provided with the MSD are not generally suitable
for deep learning, so we instead use audio previews between 7 and
30 seconds retrieved from 7digital.com. After removing ambigu-
ous artists and missing tracks, the final dataset consists of 328,821
tracks from 24,043 artists. Each track has at least 15 seconds of
audio, each biography is at least 50 characters long, and each artist
has at least 1 tag associated with it. All artist metadata, implicit
feedback matrices, and splits are released as a new dataset called
the MSD-A6.

6.2 Artist Recommendation
To investigate to what extent the different feature sets, data models
and architectures influence the quality of the deep artist features,
we evaluate the different approaches in an artist recommendation
scenario. Given the matrix of implicit feedback R, and the set of
artist and user factors obtained through matrix factorization (see
Section 2), we predict the artist factors for the test set, and use
them to compute a ranked list of recommended artists for every
user. We use mean average precision (MAP) with a cut-off at 500
recommendations per user.

We compare four different approaches using the biography texts
as input. (1) a pure text-based approach using a VSM and a feed-
forward network a-TEXT. (2) similar to (1) but with a semantically
enriched version of the texts a-sem (cf. Section 3.1). (3) An ap-
proach based on word embeddings initialized with vectors trained
on Google News and a CNN a-w2v-goo (cf. Section 3.2). (4) Similar
to (3) but inizializing the embeddings with word vectors previously
trained on the corpus of biographies a-w2v. To properly frame the
results, we compute two baselines and one competitor approach.
The tags baseline approach uses artist social tags as input features,
and text-rf uses biography texts as input, but Random Forest
Regression for the learning instead of a deep neural network. The
former baseline is added to compare the potential of biography
texts with respect to curated metadata, whilst the latter was added
to study to which extent the deep network improves the results
over other learning methods typically used in natural language
processing. There are few recommendation approaches able to
deal with an extreme cold-start scenario like ours. Therefore, we
select ItemAttributeKnn [10] as the competitor approach (tags-
itemKnn), using artist social tags as attribute data and computed
using the MyMediaLite library7. We also show the scores achieved
when the latent factor vectors are randomized (random), and when
they are learned from feedback data using matrix factorization
(upper-bound).

6https://doi.org/10.5281/zenodo.831348
7http://www.mymedialite.net/

Table 1: Artist Recommendation Results

Aproach

Input

Data model

Arch MAP

a-text
a-sem
a-w2v-goo
a-w2v

a-tags
tags-itemKnn
text-rf

random
upper-bound

Bio
Sem Bio
Bio
Bio

VSM
VSM
w2v-pretrain
w2v-trained

FF
FF
CNN
CNN

Tags
Tags
Bio

-
-

VSM
-
VSM

-
-

FF
itemKnn
RF

-
-

0.0161
0.0201
0.0119
0.0145

0.0314
0.0161
0.0089

0.0014
0.5528

Mean average precision (MAP) at 500 for the predictions of artist recommen-
dations in 1M users. VSM refers to Vector Space Model, FF to Feedforward,
RF to Random Forest, CNN to Convolutional Neural Network, and itemKnn
to itemAttributeKnn approach. Bio refers to biography texts and Sem Bio to
semantically enriched texts.

Results reported in Table 1 show that the semantic enrichment
of the biographies a-sem outperforms the pure text approach a-
text. As expected, the use of tags improves the results over the
use of text. However, the addition of semantic features reduces the
gap in performance between the use of tags and unstructured text.
Moreover, the difference between a-text and text-rf shows that
the use of deep learning with respect to random forest improves
the results. We also note that a VSM model with a feedforward
network outperforms the use of word embeddings with convolu-
tions. Although, according to the literature, this latter approach
has demonstrated its utility for simple tasks like binary classifica-
tion with short texts, our task puts forward two challenges for this
architecture: the greater length of the input texts, and the higher
dimensionality of the output. Although we have shown that initial-
izing the embedding layer with word vectors trained on the corpus
itself (a-w2v) outperforms the use of Google News pre-trained
vectors (a-w2v-goo), further work is necessary to properly opti-
mize a convolutional architecture for this task. Finally, we observe
that our approach a-tags outperforms the competitor approach
tags-itemKnn using the same item attributes.

Once the network is trained, we predict the activations of the
penultimate layer for the entire dataset of artists. Thus, we obtain a
vector embedding of 2048 dimensions, which represents the artist
deep features A(cid:48). From the evaluated approaches, we compute the
artist embedding from the a-sem and a-tags approaches.

6.3 Song Recommendation
In this experiment, audio embeddings are obtained after training
the convolutional network (see Section 4) with 260k patches of
15 seconds, corresponding to the 80% of the tracks described in
Section 6.1. Patches are divided into training (80%), validation (10%)
and test (10%) sets. Results reported in Table 2 are computed over
the remaining 20% of tracks. As opposed to [36], no artist appears
in more than one subset to avoid overfitting. Finally, multimodal
approaches are computed on the same sets.

In our experiments, we want to measure the impact of the artist
embeddings in the song recommendation problem, and also the

A Deep Multimodal Approach for Cold-start
Music Recommendation

Table 2: Song Recommendation Results

Approach

Artist Input Track Input Arch MAP

audio
sem-vsm
sem-emb

mm-lf-lin
mm-lf-h1
mm

tags-vsm
tags-emb

random
upper-bound

-
Sem Bio
a-sem

a-sem
a-sem
Sem Bio

Tags
a-tags

rnd emb
-

audio spec
-
-

CNN 0.0015
0.0032
0.0034

FF
FF

audio emb MLP 0.0036
0.0035
audio emb
MLP
CNN 0.0014
audio spec

-
-

-
-

FF
FF

FF
-

0.0043
0.0049

0.0002
0.1649

Mean average precision (MAP) at 500 for the predictions of song recom-
mendations in 1M users. audio emb refers to the track embedding of audio
approach, sem to artist embedding of sem approach, tags to artist embedding
of tags approach, spec to spectrogram, mm to multimodal, lf to late fusion,
lin to linear, and h1 to one hidden layer.

potential of the multimodal approach. We experimented with two
approaches, sem-emb and tags-emb, that exploit the feature em-
beddings learned from the artists features (see Section 6.2), either
based on biography texts (a-sem) or artists tags (a-tags). To mea-
sure the potential of the artist embeddings, we also computed two
approaches based on the original artist features (sem-vsm for se-
mantic text features and tags-vsm for tag features). Results on
Table 2 show that sem-emb and tags-emb outperform sem-vsm
and tags-vsm, suggesting that artist embeddings outperform artist
features.

An approach based on the audio spectrograms was computed
(audio). From this latter approach, audio embeddings where ob-
tained (audio emb) and combined with a-sem in a multimodal late
fusion approach mm-lf-lin (without hidden layers and l2-norm)
and mm-lf-h1 (with one hidden layer after each feature vector and
batch normalization) (cf. Section 5). We also tried with different
combinations of hidden layers and normalization steps in the mul-
timodal network but all of them yielded lower results than the ones
reported for mm-lf-lin and mm-lf-h1. We compared this network
with a multimodal approach trained directly on the original features
(semantically enriched text and audio spectrograms). Results on the
combination of artist and track features show that the late fusion of
artist and track embeddings mm-lf-lin outperforms the simultane-
ous training of artist and track features mm. In addition, we observe
that we achieve better results when no hidden layer is added to
the multimodal network mm-lf-lin. Finally, we observe that the
multimodal approach that combines text and audio features with
late fusion mm-lf-lin improves the results of pure text sem-emb
or pure audio audio approaches. All the differences between the
approaches are statistically significant (p < 0.01) according to the
paired t-test.

We also compared the results with an upper-bound approach
obtained from the feedback data and an approach trained with
random vector embeddings. Although results are in general far
from the upper-bound, the comparative analysis of the proposed
approaches gives some insights of the behavior of different feature

DLRS 2017, August 27, 2017, Como, Italy

representations and modalities in the cold-start recommendation
problem.

7 CONCLUSIONS
In this work, a multimodal approach for song recommendation has
been presented. The approach is divided into three steps. (1) Artist
feature embeddings are learned from text and semantic features
in an artist recommendation scenario using a deep network archi-
tecture. (2) Track feature embeddings are learned from the audio
spectrograms using convolutional neural networks. (3) Embeddings
are combined in a multimodal network.

Results show that splitting the problem of song recommendation
at artist and song levels improves the quality of recommendations.
Learning artist feature embeddings separately benefits from the
aggregation of the information about the different songs of the
same artist, yielding more robust artist features. Related to this,
an approach for the semantic enrichment of artist metadata has
been proposed, leading to a significant improvement in the results.
In addition, we have shown the potential of exploiting artist bi-
ographies in music recommendation. Moreover, the deep learning
architectures of this work have demonstrated their capacity to im-
prove upon other learning models under the music recommendation
framework. Finally, we have shown how a multimodal approach,
based on the late fusion of track and artist feature embeddings
that are learned separately, outperforms end-to-end multimodal
approaches where the different modalities are learned simultane-
ously. Moreover, results have shown that our multimodal approach
achieves better results than pure text or audio approaches. As fu-
ture work, we plan to do fine-tunning of the combined model after
pre-training the network on each modality separately.

ACKNOWLEDGMENTS
This work was partially funded by the Spanish Ministry of Economy
and Competitiveness under the Maria de Maeztu Units of Excellence
Programme (MDM-2015-0502). The Tesla K40 used for this research
was donated by the NVIDIA Corporation.

REFERENCES
[1] Trapit Bansal, David Belanger, and Andrew McCallum. 2016. Ask the GRU:
Multi-task Learning for Deep Text Recommendations. RecSys (2016), 107–114.
https://doi.org/10.1145/2959100.2959180 arXiv:1609.02116

[2] M. Rouvier Bechet, S. Delecraz, B. Favre, M. Bendris, and F. 2015. Multimodal
embedding fusion for robust speaker role recognition in video broadcast. IEEE
Workshop on Automatic Speech Recognition and Understanding (ASRU) (2015).
[3] Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003.
A Neural Probabilistic Language Model. The Journal of Machine Learn-
ing Research 3 (2003), 1137–1155. https://doi.org/10.1162/153244303322533223
arXiv:arXiv:1301.3781v3

[4] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere.
2011. The Million Song Dataset. In Proceedings of the 12th International Conference
on Music Information Retrieval (ISMIR 2011).

[5] Robin Burke. 2002. Hybrid Recommender Systems: Survey and Experiments.

[6]

User Modeling and User-Adapted Interaction 12, 4 (2002), 331–370.
`Oscar Celma. 2010. The Long Tail in Recommender Systems. Springer Berlin Hei-
delberg, Berlin, Heidelberg, 87–107. https://doi.org/10.1007/978-3-642-13287-2 4
[7] Franc¸ois Chollet. 2016. Information-theoretical label embeddings for large-scale
image classification. CoRR (2016), 1–10. arXiv:1607.05691 http://arxiv.org/abs/
1607.05691

[8] Ronan Collobert,

Jason Weston, L´eon Bottou, Michael Karlen, Koray
Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost)
Journal of Machine Learning Research 12 (2011), 2493–2537.
from Scratch.
https://doi.org/10.1.1.231.4614 arXiv:1103.0398

DLRS 2017, August 27, 2017, Como, Italy

Oramas et al.

[32] Christian Sch¨orkhuber and Anssi Klapuri. 2010. Constant-Q transform toolbox
for music processing. 7th Sound and Music Computing Conference JANUARY
(2010), 3–64.

[33] Olga Slizovskaia, Emilia G´omez, and Gloria Haro. 2017. Musical instrument
recognition in user-generated videos using a multimodal convolutional neural
network architecture. In ICMR ’17: Proceedings of the 2017 ACM on International
Conference on Multimedia Retrieval. Bucharest, Romania.

[34] Nitish Srivastava and Ruslan Salakhutdinov. 2012. Learning representations for
multimodal data with deep belief nets. In International conference on machine
learning workshop.

[35] Douglas Turnbull, Luke Barrington, and Gert Lanckriet. 2008. Five approaches
to collecting tags for music. Proceedings of 9th the International Society for
Music Information Retrieval Conference (2008), 225–230. http://books.google.
com/books?hl=en

[36] Aaron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep
content-based music recommendation. Electronics and Information Systems
department (ELIS) (2013), 9. https://doi.org/10.1109/MMUL.2011.34.van
Justin Zobel and Alistair Moffat. 1998. Exploring the similarity space. ACM
SIGIR Forum 32, 1 (1998), 18–34. https://doi.org/10.1145/281250.281256

[37]

[9] Arthur Flexer. 2007. A Closer Look on Artist Filters for Musical Genre Classifica-
tion. In Proceedings of the 8th International Society for Music Information Retrieval
Conference.

[10] Zeno Gantner, Lucas Drumond, Christoph Freudenthaler, Steffen Rendle, and
Lars Schmidt-Thieme. 2010. Learning Attribute-to-Feature Mappings for Cold-
Start Recommendations. In Proceedings of the 2010 IEEE International Conference
on Data Mining (ICDM ’10). IEEE Computer Society, Washington, DC, USA,
176–185.

[11] C¸a˘glar G¨ulc¸ehre and Yoshua Bengio. 2016. Knowledge matters: Importance of
prior information for optimization. Journal of Machine Learning Research 17, 8
(2016), 1–32.

[12] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative Filtering for
Implicit Feedback Datasets. In Proceedings of the 2008 Eighth IEEE International
Conference on Data Mining (ICDM ’08). 263–272.

[13] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Acceler-
ating Deep Network Training by Reducing Internal Covariate Shift. CoRR
abs/1502.03167 (2015). http://arxiv.org/abs/1502.03167

[14] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP 2014) (2014), 1746–1751. https://doi.org/10.1109/LSP.2014.
2325781 arXiv:arXiv:1408.5882v1

[15] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic
Optimization. CoRR abs/1412.6980 (2014). http://arxiv.org/abs/1412.6980
[16] Peter Knees and Markus Schedl. 2013. A survey of music similarity and
recommendation from music context data. ACM Transactions on Multime-
dia Computing, Communications, and Applications 10, 1 (2013), 1–21. https:
//doi.org/10.1145/2542205.2542206

[17] Y. Koren, R. Bell, and C. Volinsky. 2009. Matrix Factorization Techniques for
Recommender Systems. Computer 42, 8 (2009), 42–49. https://doi.org/10.1109/
MC.2009.263 arXiv:ISSN 0018-9162

[18] Hugo Larochelle, Yoshua Bengio, J´erˆome Louradour, and Pascal Lamblin. 2009.
Exploring strategies for training deep neural networks. Journal of Machine
Learning Research 10, Jan (2009), 1–40.

[19] Brian McFee, Thierry Bertin-Mahieux, Daniel P.W. Ellis, and Gert R.G. Lanckriet.
2012. The million song dataset challenge. Proceedings of the 21st international
conference companion on World Wide Web - WWW ’12 Companion (2012), 909.
https://doi.org/10.1145/2187980.2188222

[20] Brian Mcfee, Colin Raffel, Dawen Liang, Daniel P W Ellis, Matt Mcvicar, Eric
Battenberg, and Oriol Nieto. 2015. librosa: Audio and Music Signal Analysis in
Python. PROC. OF THE 14th PYTHON IN SCIENCE CONF Scipy (2015), 1–7.
[21] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed
Representations of Words and Phrases and their Compositionality. Nips (2013),
1–9. https://doi.org/10.1162/jmlr.2003.3.4-5.951 arXiv:1310.4546

[22] Raymond J Mooney and Loriene Roy. 1999. Content-Based Book Recommending.
Proceedings of the SIGIR-99 Workshop on Recommender Systems: Algorithms and
Evaluation August (1999).

[23] Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking
meets Word Sense Disambiguation: A Unified Approach. Transactions of the
Association for Computational Linguistics 2 (2014), 231–244. https://tacl2013.cs.
columbia.edu/ojs/index.php/tacl/article/view/291

[24] Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic con-
struction, evaluation and application of a wide-coverage multilingual semantic
network. Artificial Intelligence 193 (2012), 217–250. https://doi.org/10.1016/j.
artint.2012.07.001
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and
Andrew Y Ng. 2011. Multimodal deep learning. In Proceedings of the 28th inter-
national conference on machine learning (ICML-11). 689–696.

[25]

[26] Kim Anh Nguyen, Sabine Schulte im Walde, and Ngoc Thang Vu. 2016. Neural-
based Noise Filtering from Word Embeddings. Coling-2016 (2016), 2699–2707.
arXiv:1610.01874 http://arxiv.org/abs/1610.01874

[27] Sergio Oramas, Luis Espinosa-Anke, Mohamed Sordo, Horacio Saggion, and
Xavier Serra. 2016. Information extraction for knowledge base construction in
the music domain. Data and Knowledge Engineering 106 (2016), 70–83. https:
//doi.org/10.1016/j.datak.2016.06.001

[28] Sergio Oramas, Vito Claudio Ostuni, Tommaso Di Noia, Xavier Serra, and Euge-
nio Di Sciascio. 2015. Sound and Music Recommendation with Knowledge Graphs.
ACM Trans. Intell. Syst. Technol. 9, 4 (2015), 1–21. https://doi.org/10.1145/2926718
[29] Sergio Oramas, Mohamed Sordo, Luis Espinosa-Anke, and Xavier Serra. 2015.
A Semantic-based Approach for Artist Similarity. Proceedings of 16th the In-
ternational Society for Music Information Retrieval Conference October (2015),
100–106.

[30] Martin Saveski and a Mantrach. 2014. Item cold-start recommendations: learning
local collective embeddings. RecSys ’14 Proceedings of the 8th ACM Conference on
Recommender systems (2014), 89–96. https://doi.org/10.1145/2645710.2645751

[31] Markus Schedl, Emilia G´omez, Juli´an Urbano, et al. 2014. Music information
retrieval: Recent developments and applications. Foundations and Trends® in
Information Retrieval 8, 2-3 (2014), 127–261.

