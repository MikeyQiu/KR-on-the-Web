IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

1

Hyperspectral Image Classiﬁcation in the Presence
of Noisy Labels

Junjun Jiang, Member, IEEE, Jiayi Ma, Member, IEEE, Zheng Wang, Chen Chen, Member, IEEE, Xianming
Liu, Member, IEEE

9
1
0
2
 
r
p
A
 
2
 
 
]

V
C
.
s
c
[
 
 
2
v
2
1
2
4
0
.
9
0
8
1
:
v
i
X
r
a

Abstract—Label information plays an important role in su-
pervised hyperspectral image classiﬁcation problem. However,
current classiﬁcation methods all
ignore an important and
inevitable problem—labels may be corrupted and collecting clean
labels for training samples is difﬁcult, and often impractical.
Therefore, how to learn from the database with noisy labels is a
problem of great practical importance. In this paper, we study the
inﬂuence of label noise on hyperspectral image classiﬁcation, and
develop a random label propagation algorithm (RLPA) to cleanse
the label noise. The key idea of RLPA is to exploit knowledge
(e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of
label propagation. Speciﬁcally, RLPA ﬁrst constructs a spectral-
spatial probability transfer matrix (SSPTM) that simultaneously
considers the spectral similarity and superpixel based spatial
information. It then randomly chooses some training samples
as “clean” samples and sets the rest as unlabeled samples, and
propagates the label information from the “clean” samples to
the rest unlabeled samples with the SSPTM. By repeating the
random assignment (of “clean” labeled samples and unlabeled
samples) and propagation, we can obtain multiple labels for
each training sample. Therefore,
the ﬁnal propagated label
can be calculated by a majority vote algorithm. Experimental
studies show that RLPA can reduce the level of noisy label and
demonstrates the advantages of our proposed method over four
major classiﬁers with a signiﬁcant margin—the gains in terms
of the average OA, AA, Kappa are impressive, e.g., 9.18%,
9.58%, and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA.

Index Terms—Hyperspectral image classiﬁcation, noisy label,

label propagation, superpixel segmentation.

I. INTRODUCTION

D Ue to the rapid development and proliferation of hyper-

spectral remote sensing technology, hundreds of narrow
spectral wavelengths for each image pixel can be easily

The research was supported by the National Natural Science Foundation of
China (61501413, 61503288, 61773295, 61672193). (Corresponding author:
Jiayi Ma).

J. Jiang and X. Liu are with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Harbin 150001, China, and are also with
Peng Cheng Laboratory, Shenzhen, China. E-mail: junjun0595@163.com;
csxm@hit.edu.cn.

J. Ma is with the Electronic Information School, Wuhan University, Wuhan

430072, China. E-mail: jyma2010@gmail.com.

Z. Wang is with the Digital Content and Media Sciences Research Di-
vision, National Institute of Informatics, Tokyo 101-8430, Japan. E-mail:
wangz@nii.ac.jp.

C. Chen is with the Center for Research in Computer Vision, Uni-
versity of Central Florida (UCF), Orlando, FL 32816-2365 USA. E-Mail:
chenchen870713@gmail.com.

Copyright (c) 2016 IEEE. Personal use of this material

is permitted.
However, permission to use this material for any other purposes must be
obtained from the IEEE by sending an email to pubs-permissions@ieee.org.

acquired by space borne or airborne sensors, such as AVIRIS,
HyMap, HYDICE, and Hyperion. This detailed spectral re-
ﬂectance signature makes accurately discriminating materials
of interest possible [1], [2], [3]. Because of the numerous de-
mands in ecological science, ecology management, precision
agriculture, and military applications, a large number of hyper-
spectral image classiﬁcation algorithms have appeared on the
scene [4], [5], [6], [7] by exploiting the spectral similarity and
spectral-spatial feature [8], [9], [10], [11]. These methods can
be divided into two categories: supervised and unsupervised.
The former is generally based on clustering ﬁrst and then
manually determining the classes. Through incorporating the
label information, these supervised methods leverage powerful
machine learning algorithms to train a decision rule to predict
the labels of the testing pixels. In this paper, we mainly
focus on the supervised hyperspectral
image classiﬁcation
techniques.

In the past decade, the remote sensing community has intro-
duced intensive works to establish an accurate hyperspectral
image classiﬁer. A number of supervised hyperspectral image
classiﬁcation methods have been proposed, such as Bayesian
models [12], neural networks [13], random forest [14], [15],
support vector machine (SVM) [16], sparse representation
classiﬁcation [17], [18], extreme learning machine (ELM)
[19], [20], and their variants [21]. Beneﬁting from elaborately
established hyperspectral image databases, these well-trained
classiﬁers have achieved remarkably good results in terms of
classiﬁcation accuracy.

However, actual hyperspectral image data inevitably contain
considerable noise [22]: feature noise and label noise. To deal
with the feature noise, which is caused by limited light in
individual bands, and atmospheric and instrumental factors,
many spectral feature noise robust approaches have been
proposed [23], [24], [25], [26]. Label noise has received less
attention than feature noise, however, it is pervasive due to
the following reasons: (i) When the information provided to an
expert is very limited or the land cover is highly complex, e.g.,
low inter-class and high intra-class variabilities, it is very easy
to cause mislabeling. (ii) The low-cost, easy-to-get automatic
labeling systems or inexperienced personnel assessments are
less reliable [27]. (iii) If multiple experts label the same image
at the same time, the labeling results may be inconsistent
between different experts [28]. (iv) Information loss (due to
data encoding and decoding and data dissemination) will also
cause label noise.

Recently, the classiﬁcation problem in the presence of label
noise is becoming increasingly important and many label

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

2

Fig. 1. Schematic diagram of the proposed random label propagation algorithm based label noise cleansing process. The up dashed block demonstrates the
procedure of SSPTM generation, while the bottom dashed block demonstrates the main steps of the random label propagation algorithm.

noise robust classiﬁcation algorithms have been proposed [29],
[30], [31], [32]. These methods be divided into two major
categories: label noise-tolerant classiﬁcation and label noise
cleansing.The former adopts the strategies of bagging and
boosting, or decision tree based ensemble techniques, while
the latter aims to ﬁlter the label noise by exploiting the prior
knowledge of the training samples. For more details about
the general classiﬁcation problem with label noise, interested
reader is referred to [33] and the references therein. Generally
speaking, the label noise-tolerant classiﬁcation model is often
designed for a speciﬁc classiﬁer, so that the algorithm lacks
universality. In contrast, as a pre-processing method, the label
noise cleansing method is more general and can be used
for any classiﬁer, including the above-mentioned noisy label
robust classiﬁcation model. Therefore, this study will focus on
the more universal noisy label cleansing approach.

Although considerable literature deals with the general
image classiﬁcation, there is very little research work on the
classiﬁcation of hyperspectral images under noisy labels [22],
[34]. However, in the actual classiﬁcation of hyperspectral
images, this is a more urgent and unavoidable problem. As
reported by Pelletier et al.’s study [22], the noisy labels will
mislead the training procedure of the hyperspectral image
classiﬁcation algorithm and severely decrease the classiﬁcation
accuracy of land cover. Nevertheless, there is still relatively
little work speciﬁcally developed for hyperspectral
image
classiﬁcation when encountered with label noise. Therefore,
hyperspectral image classiﬁcation in the presence of noisy
labels is a problem that requires a solution.

In this paper, we propose to exploit the spectral-spatial
constraints based knowledge to guide the cleansing of noisy
labels under the label propagation framework. In particular,
we develop a random label propagation algorithm (RLPA). As
shown in Fig. 1, it includes two steps: (i) spectral-spatial prob-

ability transfer matrix (SSPTM) generation and (ii) random
label propagation. At the ﬁrst step, considering that spatial
information is very important for the similarity measurement
of different pixels [9], [35], [10], [36], we propose a novel
afﬁnity graph construction method which simultaneously con-
siders the spectral similarity and the superpixel segmentation
based spatial constraint. The SSPTM can be generated through
the constructed afﬁnity graph. In the second step, we randomly
divide the training database to a labeled subset (with “clean”
labels) and an unlabeled subset (without labels), and then
perform the label propagation procedure on the afﬁnity graph
to propagate the label information from labeled subset to the
unlabeled subset. Since the process of random assignment (of
clean labeled samples and unlabeled samples) and propagation
can be executed multiple times, the unlabeled subset will
receive the multiple propagated labels. Through fusing the
multiple labels of many label propagation steps with a majority
vote algorithm (MVA), it can be expected to cleanse the label
information. The philosophy behind this is that the samples
with real labels dominate all training classes, and we can grad-
ually propagate the clean label information to the entire dataset
by random splitting and propagation. The proposed method
is tested on three real hyperspectral image databases, namely
the Indian Pines, University of Pavia, and Salinas Scene, and
compared to some existing approaches using overall accuracy
(OA), average accuracy (AA), and the kappa metrics. It is
shown that the proposed method outperforms these methods
in terms of objective metrics and visual classiﬁcation map.

The main contributions of this article can be summarized

as follows:

• We provide an effective solution for hyperspectral image
classiﬁcation in the presence of noisy labels. It is very
general and can be seamlessly applied to the current
classiﬁers.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

3

image prior,

• By exploiting the hyperspectral

the
superpixel based spectral-spatial constraints, we propose
a novel probability transfer matrix generation method,
which can ensure label information of the same class
propagate to each other, and prevent the label propagation
of samples from different classes.

i.e.,

• The proposed RLPA method is very effective in cleansing
the label noise. Through the preprocess of RLPA,
it
can greatly improve the performance of the original
classiﬁers, especially when the label noise level is very
large.

This paper is organized as follows: In Section II, we present
the problem setup. Section III shows the inﬂuence of label
noise on the hyperspectral image classiﬁcation performance.
In Section IV, the details of the proposed RLPA method are
given. Simulations and experiments are presented in Section
V, and Section VI concludes this paper.

Algorithm 1 Noisy label generation.

1: Input: The clean label matrix Y and the level of label

noise ρ.

2: Output: The noisy label matrix ˜Y.
3: [N, C] = size(Y);
4: ˜Y = Y;
5: k = rand(N, 1);
6: for i = 1 to N do
if k(i) ≤ ρ then
7:

p = find(Yi,: = 1);
r = randperm(C);
r(p) = [ ];
˜Yr(1),: = 1;

8:
9:
10:
11:
end if
12:
13: end for

\\ [ ] is the null set.

II. PROBLEM FORMULATION

the labels of unseen pixels. Speciﬁcally,

In this section, we formalize the foundational deﬁnitions
and setup of the noisy label hyperspectral image classiﬁcation
problem. A hyperspectral image cube consists of hundreds
of nearly contiguous spectral bands, with high spectral res-
olution (5-10 nm), from the visible to infrared spectrum for
each image pixel. Given some labeled pixels in a hyper-
spectral image, the task of hyperspectral image classiﬁcation
is to predict
let
X = {x1, x2 · · · ,xN } ∈ RD denote a database of pixels in
a D dimensional input spectral space, and Y = {1, 2, · · · ,C}
denote a label set. The class labels of {x1, x2, · · · ,xN } are
denoted as {y1, y2, · · · ,yN }. Mathematically, we use a matrix
Y ∈ RN ×C to represent the label, where Yij = 1 if xi is
labeled as j. In order to model the label noise process, we
additionally introduce another variable ˜Y ∈ RN ×C that is
used to denote the noise observed label. Let ρ denotes the label
noise level (also called error rate or noise rate [37]) specifying
the probability of one label being ﬂipped to another, and thus
ρjk can be mathematically formalized as:

ρjk = P ( ˜Yik = 1|Yij = 1), ∀j (cid:54)= k, and j, k ∈ {1, 2, · · · , C}.

(1)
For example, when ρ = 0.3, it means that for a pixel xi,
whose label is j, there is a 30% probability to be labeled as
the other class k (k (cid:54)= j). To help make sense of this, we
give the pseudo-codes of the noisy label generation process in
Algorithm 1. size(X) is a function that returns the sizes of
each dimension of array X, rand(N ) is a function that returns
a random scalar drawn from the standard uniform distribution
on the open interval (0, 1), find(X) is a function that locates
all nonzero elements of an array X, and randperm(N ) is
a function that returns a row vector containing a random
permutation of the integers from 1 to N inclusive.

In this paper, our main task it to predict the label of an
unseen pixel xt, with the training data X = [x1, x2, · · · ,xN ]
and the noisy label matrix ˜Y.

III. INFLUENCE OF LABEL NOISE ON HYPERSPECTRAL
IMAGE CLASSIFICATION

In this section, we examine the inﬂuence of label noise
on the hyperspectral image classiﬁcation problem. As shown
in Fig. 2, we demonstrate the impact of label noise on four
different classiﬁers: neighbor nearest (NN), support vector
machines (SVM), random forest (RF), and extreme learning
machine (ELM). The noise level changes from 0 to 0.9 at
an interval of 0.1. In Fig. 2, we report the average OA over
ten runs (more details about the experimental settings can be
found in Section V) as a function of the noise level. Noisy
label based algorithm (NLA) represents the classiﬁcation with
the noisy labels without cleansing. From these results, we can
draw the following four conclusions:

1) With the increase of the label noise level, the perfor-
mance of all classiﬁcation methods is gradually declining.
Meanwhile, we also notice that the impact of label noise is
not identical for all classiﬁers. Among these four classiﬁers,
RF and ELM are relatively robust to label noise. When the
label noise level is not large, these two classiﬁers can obtain
better performance. In contrast, NN and SVM are much more
sensitive to the label noise level. The poor results of NN and
SVM can be attributed to their reliance on nearest samples
and support vectors.

2) The University of Pavia and Salinas Scene databases have
the same number of training samples (e.g., 50)1, but the decline
rate of OA on the University of Pavia is signiﬁcantly faster
than that of Salinas Scene database. This is mainly because
that the number of classes in the Salinas database is larger than
that of the University of Pavia database (C = 16 vs. C = 9).
With the same label noise level and same number of training
samples, the more the classes are, the greater the probability

1In this analysis, we only paid attention to these two databases in order to
avoid the impact of different numbers of training sample. For the Indian Pines
database, we select 10% samples for each class and the number of training
samples is not the same as that in the two other databases.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

4

Fig. 2.
databases.

Inﬂuence of the label noise on the performance (in term of OA of different classiﬁers) on the Indian Pines, University of Pavia, and Salinas Scene

Fig. 3. The distribution of a correct class at different levels of label noise ρ. First row: the distribution of samples with true label 7 under different label
noise ρ for the University of Pavia database which has nine classes. Second row: the distribution of samples with true label 5 under different levels of label
noise ρ for the Salinas Scene database which has 16 classes.

of choosing the correct samples is2. This point is illustrated
by Fig. 3. When the noise is not very large, e.g., ρ ≤ 0.7, the
samples with true labels can often dominate. In this case, a
good classiﬁer can also get satisfactory performance.

3) We also show the ideal case that we know the noisy
label samples and remove these training samples to obtain a
noiseless training subset. From the comparisons (please refer
to the same colors in each subﬁgure), we observe that there
is considerable room of improvement for the strategy of label
noise cleansing-based algorithms. This also demonstrates the
importance of preprocessing based on label noise cleansing.

ρ , this will reduce to

2In this situation, for each class (after adding label noise), although the ratio
of samples with corrected labels to samples with incorrectly labeled sample
is 1−ρ
ρ/(C−1) when we consider the ratio of samples
with corrected labels to samples labeled another class. For example, when
ρ = 0.5, C = 16, the ratio of samples with corrected labels to samples
labeled another class is 15:1.

1−ρ

IV. PROPOSED METHOD

A. Overview of the Framework

To handle the label noise, there are two main kinds of
methods. The ﬁrst class is to design a speciﬁc classiﬁer that is
robust to the presence of label noise, while the other obvious
and tempting method is to improve the label quality of training
samples. Since the latter is intuitive and can be applied to any
of the subsequent classiﬁers, in this paper we mainly focus
on how to improve and cleanse the labels. The main steps
are illustrated by Fig. 4. Firstly, the prior knowledge (e.g.,
neighborhood relationship or topology) is extracted from the
training set and used to regularize the ﬁlter of label noise.
Based on the cleaned labels, we can expect an intermediate
classiﬁcation result.

The core idea of the proposed label cleansing approach is
to randomly remove the labels of some selected samples, and
then apply the label propagation algorithm to predict the labels
of these selected (unlabeled) samples according to a predeﬁned

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

5

Fig. 4. The typical procedure of labels cleansing based mthod for hyperspec-
tral image classiﬁcation in the presence of label noise.

SSPTM. The philosophy behind this method is that the sam-
ples with correct labels account for the majority, therefore,
we can gradually propagate the clean label information to
the entire samples by random splitting and propagation. This
is reasonable because when the samples with wrong labels
account for the majority, we cannot obtain the clean label for
the samples anyway. As we know, traditional label propagation
methods are sensitive to noise. This is mainly because when
the label contains noise and there is no extra prior information,
it is very hard for these traditional methods to construct a
reasonable probability transfer matrix. The label noise can not
only be removed, but is likely to be spread. Though our method
is also label propagation based, we can take full advantage
of the priori knowledge of hyperspectral
the
superpixel based spectral-spatial constraint, to construct the
SSPTM, which is the key to this label propagation based
algorithm. Based on the constructed SSPTM, we can ensure
that samples with same classes can be propagated to each other
with a high probability, and samples with different classes
cannot be propagated.

images,

i.e.,

Fig. 1 illustrates the schematic diagram of the proposed
method. In the following, we will ﬁrst
introduce how to
generate the probability transfer matrix with both the spectral
and spatial constraints. Then we present the random label
propagation approach.

B. Construction of Spectral-Spatial Afﬁnity Graph

The deﬁnition of the edge weights between neighbors is
the key problem in constructing an afﬁnity graph. To measure
the similarity between pixels in a hyperspectral image, the
simplest way is to calculate the spectral difference through
Euclidean distance, spectral angle mapper (SAM), spectral
correlation mapper (SCM), or spectral information measure
(SIM). However, these measurements all ignore the rich spa-
tial information contained in a hyperspectral image, and the
spectral similarity is often inaccurate due to low inter-class
and high intra-class variabilities.

Our goal is to propagate label information only among
samples with the same category. However, the spectral sim-
ilarity based afﬁnity graph cannot prevent label propagation
of similar samples with different classes. In this paper, we
propose a spectral-spatial similarity measurement approach.
The basic assumption of our method is that the hyperspectral
image has many homogeneous regions and pixels from one
homogeneous region are more likely to be the same class.
Therefore, when deﬁning the edge weights of the afﬁnity
graph, the spectral similarity as well as the spatial constraint
is taken into account at the same time.

1) Generation of Homogeneous Regions: As in many su-
perpixel segmentation based hyperspectral image classiﬁcation
and restoration methods [38], [39], [40], [41], we adopt
entropy rate superpixel segmentation (ESR) [42] due to its
promising performance in both efﬁciency and efﬁcacy. Other
state-of-the-art methods such as simple linear iterative cluster-
ing (SLIC) [43] can also be used to replace the ERS. Specially,
we ﬁrst obtain the ﬁrst principal component (through principal
component analysis (PCA) [44]) of hyperspectral
images,
If , capturing the major information of hyperspectral images.
This further reduces the computational cost for superpixel
segmentation. It should be noted that other state-of-the-art
methods such as [45] can also be equally used to replace the
PCA. Then, we perform ESR on If to obtain the superpixel
segmentation,

If =

Xk, s.t. Xk ∩ Xg = ∅, (k (cid:54)= g),

(2)

T
(cid:91)

k

where T denotes the number of superpixels, and Xk is the
k-th superpixel. The setting of T is an open and challenging
problem, and is usually set experimentally. Following [46],
we also introduce an adaptive parameter setting scheme to
determine the value of T by exploiting the texture information.
Speciﬁcally, the Laplacian of Gaussian (LoG) operator [47]
is applied to detect the image structure of the ﬁrst principal
component of hyperspectral images. Then we can measure
images based on
the texture complexity of hyperspectral
the detected edge image. The more complex the texture of
hyperspectral images, the larger the number of superpixels,
and vice versa. Therefore, we deﬁne the number of superpixel
as follows:

T = Tbase

Nf
NI

,

(3)

where Nf denotes the number of nonzero elements in the
detected edge image, NI is the size of If , i.e., the total
number of pixels in If , and Tbase is a ﬁxed number for all
hyperspectral images. In this way, the number of superpixels
T is set adaptively, based on the spatial characteristics of
different hyperspectral images. In all our experiments, we set
Tbase = 2000.

2) Construction of Spectral-Spatial Regularized Probabilis-
tic Transition Matrix: Based on the segmentation result, we
can construct the afﬁnity graph by putting an edge between
pixels within a homogeneous region and letting the edge
weights between pixels from different homogeneous region
be zero:

Wij =

(cid:40)

exp
0,

(cid:16)

− sim(xi,xj )2
2σ2

(cid:17)

,

xi, xj ∈ Xk,
xi ∈ Xk and xj ∈ Xg.

(4)
Here, sim(xi, xj) denotes the spectral similarity of xi and xj.
In this paper, we use the Euclidean distance to measure their
similarity,

sim(xi, xj) = ||xi − xj||2,

(5)

where (cid:107)·(cid:107)2 is the l2 norm of a vector. In Eq. (4), the variance
σ is calculated region adaptively through the mean variance

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

6

Algorithm 2 Random label propagation algorithm (RLPA)
based label noise cleansing.

1: Input: Training samples {x1, x2, · · · ,xN }, and the corre-
sponding labels {y1, y2, · · · ,yN }, parameters η and α.

1, y∗

2, · · · ,y∗

N }.

7:

(s)

2: Output: The cleaned labels {y∗
3: for s = 1 to S do
rand(‘seed‘, s);
4:
k = randperm(N );
5:
l = round(N ∗ η);
6:
˜Y
˜Y
˜Y
∗(s)
˜F
for i = 1 to N do
y(s)
i = arg max

L = ˜Y(:, k(1 : l)) ∈ Rl×C;
(s)
U = 0;
(s)
LU = [ ˜Y

(s)
U ];
= (1 − α)(I − T)−1 ˜Y

L ; ˜Y

F∗(s)
ij

(s)

8:

9:

10:
11:

12:

(s)
LU ;

j

end for

13:
14: end for
15: for i = 1 to N do
16:
17: end for

i = M V A({y(1)
y∗

i

, y(2)
i

, · · · , y(s)

i })

Fig. 5. Plots of the number of noisy label samples (N.N.L.S.) according
to the iterations of the RLPA (blue line) under three different noise levels
(ρ = 0.1, 0.2, 0.5). We also show the initial number (red dashed line) of
noisy label samples for comparison.

of all pixels in each homogeneous region:





1
|Xk|

σ =

(cid:88)

xi,xj ∈Xk



0.5

(cid:107)xi − xj(cid:107)2
2



,

(6)

where |·| is the cardinality operator.

Upon acquiring the spectral-spatial

regularized afﬁnity
graph, the label information can be propagated between nodes
through the connected edges. The larger the weight between
two nodes, the easier it becomes to travel. Therefore, we can
deﬁne a probability transition matrix T:

Tij = P (j → i) =

(7)

Wij
k=1 Wkj

,

(cid:80)N

where Tij can be seen as the probability to jump from node
j to node i.

C. Random Label Propagation through Spectral-Spatial
Neighborhoods

the availableinformation about

It is a very challenging problem to cleanse the label noise
from the original label space. However, as a hyperspectral
the
image, we can exploit
spectral-spatial knowledge to guide the labeling of adjacent
pixels. Speciﬁcally, to cleanse the noise of labels, we propose a
RLPA based method. We randomly select some noisy training
samples as “clean’ labeled samples and set the remaining
samples as unlabeled samples. The label propagation algorithm
is then used to propagate the information from the “clean”
labeled samples to the unlabeled samples.

Concretely, we divide the training database X to a labeled
subset XL = {x1, x2, · · · ,xl}, whose label matrix is denoted

as ˜YL = ˜Y(:, 1 :
l) ∈ Rl×C, and an unlabeled subset
XU = {xl+1, xl+2, · · · ,xN }, whose labels are discarded. l is
the number of training samples that are selected for building up
the “clean” labeled subset, l = round(N ∗η). Here, η denotes
the “clean” sample proportion in the total training samples, and
round(a) is a function that rounds the elements of a to the
nearest integers. It should be noted that we set the ﬁrst l pixels
as the labeled subset, and the rest as the unlabeled subset for
the convenience of expression. In our experiments, these two
subsets are randomly selected from the training database X .
Now, our task is to predict the labels ˜YU of unlabeled pixels
XU , based on the graph constructed by the superpixel based
spectral-spatial afﬁnity graph.

In the same manner as the label propagation algorithm
(LPA) [48], in this paper we present to iteratively propagate
the labels of the labeled subset ˜YL to the remaining unlabeled
subset XU based on the spectral-spatial afﬁnity graph. Let
F =[f1, f2 · · · ,fN ] ∈ RN ×C be the predicted label. At each
propagation step, we expect that each pixel absorbs a fraction
of label
information from its neighbors within the homo-
geneous region on the spectral-spatial constraint graph, and
retains some label information of its initial label. Therefore,
the label of xi at time t + 1 becomes,

ft+1
i = α

(cid:88)

Tijft

j + (1 − α)˜yLU

i

,

(8)

xi,xj ∈Xk

where 0 < α < 1 is a parameter that balancing the con-
tribution between the current label information and the label
information received from its neighbors, and ˜yLU
is the i-th
column of ˜YLU = [ ˜YL; ˜YU ]. It is worth noting that we set the
initial labels of these unlabeled samples as ˜YU = 0.

i

Mathematically, Eq. (8) can be also rewritten as follows,

Ft+1 = αTFt + (1 − α) ˜YLU .

(9)

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

7

Fig. 6. The quantitative classiﬁcation results in term of OA of four different methods (NLA, Bagging, iForest, and RLPA) with four different classiﬁers (NN,
SVM, RF, and ELM) on the Indian Pines (the ﬁrst row), University of Pavia (the second row), and Salinas Scene (the third row). The average OAs of four
different methods on three databases with four different classiﬁers are: NLA (OA = 73.93%), Bagging (OA = 73.20%), iForest (OA = 77.09%), and RLPA
(OA = 83.11%).

Following [49], we learn that Eq. (9) can be converged to an
optimal solution:

F∗ = lim
t→∞

Ft = (1 − α)(I − T)−1 ˜YLU .

(10)

F∗ can be seen as a function that assigns labels for each pixel,

yi = arg max

F∗
ij

j

(11)

Since the initial label and unlabeled samples are generated
randomly, We can repeat the above process of random as-
signment (of “clean” labeled samples and unlabeled samples)
and propagation, and obtain multiple labels for each training
(s)
sample. In particular, we can get different label matrices ˜Y
LU
at the s th round, s = 1, 2, ..., S. Here, S is the total number in
iterations. We can then calculate the label assignment matrix
F∗(1), F∗(2), · · · , F∗(S) according to Eq. (10). Thus, we obtain
S labels for xi, y(1), y(2), · · · , y(S). The ﬁnal propagated label
can be calculated by MVA [50].

Because we fully considered the spatial

information of
hyperspectral images in the process of propagation, we can
these propagated label results are better than
expect

that

the original noisy labels in the sense of the proportion that
noisy label samples is decreasing (as the number of iterations
increase). We illustrate this point in Fig. 5, which plots the
number of noisy label samples according to the iterations of
the proposed RLPA under three different noise levels. With
the increase of iteration, the number of noisy label samples
becomes less and less. The red dashed line shows the initial
number of noisy label samples. Obviously, after a certain
number of iterations, the number of noisy label samples is
signiﬁcantly reduced. In our experiments, we ﬁx the value of
S to 100.

Algorithm 2 shows the entire process of our proposed RLPA
based label cleansing method. M V A represents the majority
vote algorithm that returns the majority of a sequence of
elements.

V. EXPERIMENTS

In this section, we describe how we set up the experiments.
Firstly, we introduce the three hyperspectral image databases
used in our experiments. Then, we show the comparison
of our results with four other methods. Subsequently, we

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

8

TABLE I
NUMBER OF SAMPLES IN THE INDIAN PINES, UNIVERSITY OF PAVIA, AND SALINAS SCENE IMAGES. THE BACKGROUND COLOR IS USED TO
DISTINGUISH DIFFERENT CLASSES.

Indian Pines

University of Pavia

Salinas Scene

Class Names

Numbers

Class Names

Numbers

Class Names

Numbers

Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Buildings-Grass-Trees-Drives
Stone-Steel-Towers
Total Number

46
1428
830
237
483
730
28
478
20
972
2455
593
205
1265
386
93
10249

Asphalt
Bare soil
Bitumen
Bricks
Gravel
Meadows
Metal sheets
Shadows
Trees

6631
18649
2099
3064
1345
5029
1330
3682
947

Total Number

42776

Brocoli green weeds 1
Brocoli green weeds 2
Fallow
Fallow rough plow
Fallow smooth
Stubble
Celery
Grapes untrained
Soil vinyard develop
Corn senesced green weeds
Lettuce romaine 4wk
Lettuce romaine 5wk
Lettuce romaine 6wk
Lettuce romaine 7wk
Vinyard untrained
Vinyard vertical trellis
Total Number

2009
3726
1976
1394
2678
3959
3579
11271
6203
3278
1068
1927
916
1070
7268
1807
54129

paper, 20 low SNR bands are removed and a total of 200
bands are used for classiﬁcation. This database contains
16 different land-cover types, and approximately 10,249
labeled pixels are from the ground-truth map. Fig. 7 (a)
shows an infrared color composite image and Fig. 7 (d)
is the ground reference data.

2) The second hyperspectral image database is the Uni-
versity of Pavia, covering an urban area with some
buildings and large meadows, which contains a spatial
coverage of 610×340 pixels and is collected by the
ROSIS sensor under the HySens project managed by
DLR (the German Aerospace Agency). It generates 115
spectral bands, of which 12 noisy and water-bands are
removed. It has a spectral coverage from 0.43-0.86 µm
and a spatial resolution of 1.3 m. Approximately 42,776
labeled pixels with nine classes are from the ground truth
map, details of which are provided in Table I. Fig. 7 (b)
shows an infrared color composite image and Fig. 7 (e)
is the ground reference data.

3) The third hyperspectral image database is the Salinas
Scene, capturing an area over Salinas Valley, CA, USA,
was collected by the 224-band AVIRIS sensor over
Salinas Valley, California. It generates 512×217 pixels
and 204 bands over 0.4-2.5 µm with spatial resolution
of 3.7 m, of which 20 water absorption bands are
removed before classiﬁcation. In this image, there are
approximately 54,129 labeled pixels with 16 classes
sampled from the ground truth map, details of which are
provided in Table I. Fig. 7 (c) shows an infrared color
composite image and Fig. 7 (f) is the ground reference
data.

For the three databases, the training and testing samples
are randomly selected from the available ground truth maps.
The class-speciﬁc numbers of labeled samples are shown in
Table I. For the Indian Pines database, 10% of the samples are
randomly selected for training, and the rest is used testing. As
for the other databases, i.e., University of Pavia and Salinas
Scene, we randomly choose 50 samples from each class to
build the training set, leaving the remaining samples form the

Fig. 7. The RGB composite images and ground reference information of three
hyperspectral image databases: (a) Indian Pines, (b) University of Pavia, and
(c) Salinas Scene.

demonstrate the effectiveness of our proposed SSPTM. Finally,
we assess the inﬂuence of parameter settings. We intend to
release our codes to the research community to reproduce our
experimental results and learn more details of our proposed
method from them.

A. Database

In order to evaluate the proposed RLPA method, we use

three publicly available hyperspectral image databases3.

1) The ﬁrst hyperspectral image database is the Indian
Pine, covering the agricultural ﬁelds with regular ge-
ometry, was acquired by the AVIRIS sensor in June
1992. The scene is 145×145 pixels with 20 m spatial
resolution and 220 bands in the 0.4-2.45 m region. In this

3http://www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing

Scenes

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

9

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 8. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Indian Pines database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM. Please zoom in on the electronic version to see a more obvious contrast.

testing set.

As discussed previously, we add random noise to the labels
of training samples with the level of ρ. In other words, each
label in the training set will ﬂip to another with the probability
of ρ. In our experiments, we only show the comparison
results of different methods with ρ ≤ 0.5. That is, given
a labeled training database, we assume that more than half
of the labels are correct, because that the label information
is provided by an expert and the labels are not random.
Therefore, there are reasons to make such an assumption.
Speciﬁcally, in our experiments we test typical cases where
ρ = 0.1, 0.2, 0.3, 0.4, 0.5.

B. Result Comparison

To demonstrate the effectiveness of the proposed method,
we test our proposed framework with four widely used clas-
siﬁers in the ﬁeld of hyperspectral image calciﬁcation, which
are nearest neighborhood (NN) [51], support vector machine
(SVM) [16], random forest [14], [15], and extreme learning
machine (ELM) [19], [20]. Since there is no speciﬁc noisy
label classiﬁcation algorithm for hyperspectral
images, we
carefully design and adjust some label noise robust general
classiﬁcation methods to adapt our framework. In particular,
the four comparison methods used in our experiments are the
following:

• Noisy label based algorithm (NLA): we directly use the
training samples and their corresponding noisy labels to
train the classiﬁcation models using the above-mentioned
four classiﬁers.

• Bagging-based classiﬁcation (Bagging)

the ap-
proach of [52] ﬁrst produces different training subsets
by resampling (70% of training samples are selected each

[52]:

time), and then fuses the classiﬁcation results of different
training subsets.

• isolation Forest (iForest) [53]: this is an anomaly detec-
tion algorithm, and we apply it to detect the noisy label
samples. In particular, in the training phase, it constructs
many isolation trees using sub-samples of the given
training samples. In the evaluation phase, the isolation
trees can be used to calculate the score for each sample
to determine the anomaly points. Finally, these samples
will be removed when their anomaly scores exceeds the
predeﬁned threshold.

• RLPA: the proposed random label propagation based label
noise cleansing method operates by repeating the random
assignment and label propagation, and fusing the label
information by different iterations.

NLA can be seen as a baseline, Bagging-based method [52]
is a classiﬁcation ensemble strategy that has been proven to
be robust to label noise [54]. iForest [53] can be regarded
as a label cleansing processing as our proposed method in
the sense that the goals of these methods are to remove the
samples with noisy labels. In our experiments, we carefully
tuned the parameters of the four classiﬁers to achieve the best
performance under different comparison methods. Speciﬁcally,
set all parameters to a larger range, and the reported results
of different comparison methods with different classiﬁers are
the best when setting appropriate values for the parameters.

Generally speaking, the OA, AA, and the Kappa coefﬁcient
can be used to measure the performance of different classiﬁ-
cation results. In Table II, Table III, and Table IV, we report
the OA, AA, and Kappa scores of four different methods with
four different classiﬁers on the Indian Pines, University of
Pavia, and Salinas Scene databases, resepctively. The average
OA, AA, and Kappa of LNA, Bagging, iForest, and RLPA for

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

10

TABLE II
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE INDIAN PINES DATABASE.

TABLE III
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE UNIVERSITY OF PAVIA DATABASE.

TABLE IV
OA, AA, AND KAPPA PERFORMANCE OF FOUR DIFFERENT METHODS WITH FOUR DIFFERENT CLASSIFIERS ON THE SALINAS SCENE DATABASE.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

11

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 9. The classiﬁcation maps of four different method (each row represents different methods) with four different classiﬁers (each column represents different
classiﬁers) on the University of Pavia database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

TABLE V
THE AVERAGE PERFORMANCE IN TERMS OF OA, AA, AND KAPPA OF
NLA, BAGGING, IFOREST, AND RLPA.

Methods
NLA
Bagging
iForest
RLPA

OA [%]
73.93
73.20
77.09
83.11

AA [%]
73.26
71.94
78.04
82.84

Kappa
0.6951
0.6865
0.7293
0.7994

all cases are reported at Table V. To make the comparison
more intuitive, we plot their OA performance in Fig. 64. In
the legend of each subﬁgure, we also give the average OA of
all ﬁve noise levels of different methods. From these results,
we can draw the following conclusions:

• When compared with using the original training samples

4Since these three measurements of OA, AA, and Kappa are consistent with

each other, we only plot the results in terms of OA in all our experiments

with label noise (i.e., the NLA method), the Bagging
method cannot boost
the performance. This indicates
that re-sampling the training samples cannot improve the
performance of the algorithm in the presence of noisy
labels. Moreover, the strategy of re-sampling will result
in decreasing the total amount of training samples, so that
the classiﬁcation performance may also be degraded, e.g.,
the performance of Bagging is even worse than NLA.
• The performance of iForest (the cyan lines) is classiﬁer
and database dependent. Speciﬁcally, it performs well on
the NN for all three databases, but it may be even worse
than the NLA and Bagging methods. From the average
result, iForest can gain more than three percentages when
compare to NLA. It should be noted that as an anomaly
detection algorithm, iForest has a bottleneck that it can
only detect the noise samples but cannot cleanse its label.
• The proposed RLPA method (the red lines) can obtain

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

12

(a) ρ = 0.1

(b) ρ = 0.5

Fig. 10. The classiﬁcation maps of four different methods (each row represents different methods) with four different classiﬁers (each column represents
different classiﬁers) on the Salinas Scene database when (a) ρ = 0.1 and (b) ρ = 0.5. From the ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and ELM.

better performance (especially when the noise level is
large) than all comparison methods in almost all situa-
tions. The improvement also depends on the classiﬁer,
e.g., the gain of RLPA over NLA can reach 10% for
the NN and SVM classiﬁers and will reduce to 3% for
the RF and ELM classiﬁers. Nevertheless, the gains in
term of the average OA, AA, Kappa of our proposed

RLPA method over the NLA are still very impressive,
e.g., 9.18%, 9.58%, and 0.1043.

To further demonstrate the classiﬁcation results of different
methods, in Fig. 8, Fig. 9, and Fig. 10, we show the visual
results in term of the classiﬁcation map on two noise levels
(ρ = 0.1 and ρ = 0.5) for the three databases. For each
subﬁgure, each row represents different methods and each

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

13

(a) Indian Pines

(b) University of Pavia

(c) Salinas

Fig. 11. Classiﬁcation accuracy statistics using RLPA with/without spatial
constraint on the three databases. The horizontal axis represents the OA scores,
while the vertical axis marks the percentage of larger than the score marked
on the horizontal axis.

column represents different classiﬁers. Speciﬁcally, from the
ﬁrst row to the last row: LNA, Bagging, iForest, and RLPA,
from the ﬁrst column to the last column: NN, SVM, RF, and
ELM. When compared with LNA, Bagging, and iForest, the
proposed RLPA with ELM classiﬁer achieves the best perfor-
mance. However, the classiﬁcation maps of RLPA may result
in a salt-and-pepper effect especially in the smooth regions,
whose pixels should be the same class. This is mainly because
that the RLPA is essentially a pixel-wise method, and the
neighbor pixels may produce inconsistent classiﬁcation results.
To alleviate this problem, the approach of incorporating spatial
constraint to fuse the classiﬁcation result of RLPA can be
expected to obtain satisfying results.

C. Effectiveness of SSPTM

To verify the effectiveness of the proposed spectral-spatial
probability transform matrix generation method, we compare
it to the baseline that the similarity between two pixels in
only calculated by their spectral difference. To compare the
results of spectral-spatial probability transform matrix (SS-
PTM) based method and spectral probability transform matrix
based method (S-PTM), in Fig. 11, we report the statistical
curves of OA scores of four comparison methods with four
classiﬁers, i.e., a vector containing 20 elements, whose values
are the OA of different situations. It shows a considerable
quantitative advantage of SS-PTM compared to S-PTM.

To further analysis the effectiveness of introducing the
spatial constraint, in Fig. 12 we show the probability transform
matrices with/without a spatial constraint. The two matrices
are generated on the University of Pavia database, in which
includes 9 classes and 50 training samples per class. From
the results, we observe that SS-PTM is a sparse and highly
diagonalization matrix, and S-PTM is a dense and non-
diagonal matrix. That is to say, SS-PTM does make sense for
recovering the hidden structure of data and guarantees the label
propagation only within the same class. In contrast to S-PTM,
which has many edges between samples with different labels
(please refer to the non-diagonal blocks), it may wrongly
propagate the label information.

D. Parameter Analysis

From the framework of RLPA, we learn that

there are
two parameters determining the performance of the proposed
method: (i) the parameter η denoting the “clean” sample

(a) S-PTM

(b) SS-PTM

Fig. 12. Visualizations of the probability transfer matrices of (a) S-PTM
and (b) SS-PTM. Note that we rescale the intensity values of the matrix for
observation.

proportion in the total training samples, and (ii) the param-
eter α used to balance the contribution between the current
label information and the label information received from its
neighbors. In our study, we empirically set their values by grid
search. Fig. 13 shows the inﬂuence of these two parameters
on the classiﬁcation performance in term of OA. It should
be noted that we only give the average results of RLPA on
the three databases with ELM classiﬁer under ρ = 0.3. In
fact, we can obtain similar conclusions under other situations.
From the results, we observe that too small values of η or
α may be inappropriate. This indicates that “clean” labeled
samples play an important role in label propagation. If too
few “clean” labeled samples are selected (η is small), the label
information will be insufﬁcient for the subsequent effective
label propagation process. At the same time, as the value of
η becomes larger, the performance also starts to deteriorate.
This is mainly because that too large value of η will make
the label propagation meaningless in the sense that very few
samples need to absorb label information from its neighbors.
In our experiments, we ﬁx η to 0.7. Similarly, the value of
α cannot be set too large or too small. A too small value
of α implies that the ﬁnal labels completely determined by
the selected “clean” labeled samples. At the same time, a too
large value of α will make it very difﬁcult to absorb label
information from the labeled samples. In our experiments, we
ﬁx α to 0.9.

VI. CONCLUSION AND FUTURE WORK

In this paper we study a very important but pervasive
problem in practice—hyperspectral image classiﬁcation in the
presence of noisy labels. The existing classiﬁers assume,
without exception, that the label of a sample is completely
clean. However, due to the lack of information, the subjectivity
of human judgment or human mistakes, label noise inevitably
exists in the generated hyperspectral image data. Such noisy
labels will mislead the classiﬁer training and severely decrease
the classiﬁcation performance. Therefore, in this paper we
develop a label noise cleansing algorithm based on the random
label propagation algorithm (RLPA). RLPA can incorporate
the spectral-spatial prior to guide the propagation process
of label information. Extensive experiments on three public
databases are presented to verify the effectiveness of our
proposed approach, and the experimental results demonstrate

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

14

[12] D. A. Landgrebe, Signal theory methods in multispectral remote sensing.

John Wiley & Sons, 2005, vol. 29.

[13] F. Ratle, G. Camps-Valls, and J. Weston, “Semisupervised neural
networks for efﬁcient hyperspectral image classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 5, pp. 2271–2282, 2010.

[14] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the
random forest framework for classiﬁcation of hyperspectral data,” IEEE
Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492–501, 2005.
[15] J. Xia, P. Du, X. He, and J. Chanussot, “Hyperspectral remote sensing
image classiﬁcation based on rotation forest,” IEEE Geoscience and
Remote Sensing Letters, vol. 11, no. 1, pp. 239–243, 2014.

[16] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote
sensing images with support vector machines,” IEEE Trans. Geosci.
Remote Sens., vol. 42, no. 8, pp. 1778–1790, 2004.

[17] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral

image
classiﬁcation using dictionary-based sparse representation,” IEEE Trans.
Geosci. Remote Sens., vol. 49, no. 10, pp. 3973–3985, 2011.

[18] Y. Gao, J. Ma, and A. L. Yuille, “Semi-supervised sparse representa-
tion based classiﬁcation for face recognition with insufﬁcient labeled
samples,” IEEE Transactions on Image Processing, vol. 26, no. 5, pp.
2545–2560, 2017.

[19] W. Li, C. Chen, H. Su, and Q. Du, “Local binary patterns and extreme
learning machine for hyperspectral imagery classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 53, no. 7, pp. 3681–3693, 2015.

[20] A. Samat, P. Du, S. Liu, J. Li, and L. Cheng, “E2LMs: Ensemble extreme
learning machines for hyperspectral image classiﬁcation,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens., vol. 7, no. 4, pp. 1060–1069,
2014.

[21] B. Waske, S. van der Linden, J. A. Benediktsson, A. Rabe, and
P. Hostert, “Sensitivity of support vector machines to random feature
selection in classiﬁcation of hyperspectral data,” IEEE Trans. Geosci.
Remote Sens., vol. 48, no. 7, pp. 2880–2889, 2010.

[22] C. Pelletier, S. Valero, J. Inglada, N. Champion, C. Marais Sicre,
and G. Dedieu, “Effect of training class label noise on classiﬁcation
performances for land cover mapping with satellite image time series,”
Remote Sensing, vol. 9, no. 2, p. 173, 2017.

[23] H. Othman and S.-E. Qian, “Noise reduction of hyperspectral imagery
using hybrid spatial-spectral derivative-domain wavelet shrinkage,” IEEE
Trans. Geosci. Remote Sens., vol. 44, no. 2, pp. 397–408, 2006.
[24] S. Prasad, W. Li, J. E. Fowler, and L. M. Bruce, “Information fusion in
the redundant-wavelet-transform domain for noise-robust hyperspectral
classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 9, pp.
3474–3486, 2012.

[25] Q. Yuan, L. Zhang, and H. Shen, “Hyperspectral

image denoising
employing a spectral–spatial adaptive total variation model,” IEEE
Trans. Geosci. Remote Sens., vol. 50, no. 10, pp. 3660–3677, 2012.
[26] C. Li, Y. Ma, J. Huang, X. Mei, and J. Ma, “Hyperspectral image
denoising using the robust low-rank tensor recovery,” JOSA A, vol. 32,
no. 9, pp. 1604–1612, 2015.

[27] R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng, “Cheap and fast—
but is it good?: evaluating non-expert annotations for natural language
tasks,” in Proceedings of the conference on empirical methods in natural
language processing. Association for Computational Linguistics, 2008,
pp. 254–263.

[28] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni,
and L. Moy, “Learning from crowds,” Journal of Machine Learning
Research, vol. 00, no. Apr, pp. 1297–1322, 2010.

[29] D. Angluin and P. Laird, “Learning from noisy examples,” Machine

Learning, vol. 2, no. 4, pp. 343–370, 1988.

[30] N. D. Lawrence and B. Sch¨olkopf, “Estimating a kernel ﬁsher discrim-
inant in the presence of label noise,” in ICML, vol. 1. Citeseer, 2001,
pp. 306–313.

[31] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learn-
ing with noisy labels,” in Advances in neural information processing
systems, 2013, pp. 1196–1204.

[32] T. Liu and D. Tao, “Classiﬁcation with noisy labels by importance
reweighting,” IEEE Transactions on pattern analysis and machine
intelligence, vol. 38, no. 3, pp. 447–461, 2016.

[33] B. Fr´enay and M. Verleysen, “Classiﬁcation in the presence of label
noise: a survey,” IEEE transactions on neural networks and learning
systems, vol. 25, no. 5, pp. 845–869, 2014.

[34] F. Condessa, J. Bioucas-Dias, and J. Kovaˇcevi´c, “Supervised hyperspec-
tral image classiﬁcation with rejection,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens., vol. 9, no. 6, pp. 2321–2332, 2016.

[35] H. Pu, Z. Chen, B. Wang, and G. M. Jiang, “A novel spatial-spectral
similarity measure for dimensionality reduction and classiﬁcation of

Fig. 13. The classiﬁcation result in term of average OA on the three databases
with ELM classiﬁer under ρ = 0.3 according to different α and η, whose
values vary from 0.1 to 0.975.

much improvement over the approach of directly using the
noisy samples.

In this paper, we simply use random noise to generate
noisy labels. For all classes, they have the same percentage
of samples with label noise. However, in real conditions label
noise may be sample-dependent, class-dependent, or even
adversarial. For example, when the mislabeled pixels come
from the edge of the region or are similar to one another,
such noise will be more difﬁcult to handle. Therefore, how to
deal with real label noise will be our future work.

REFERENCES

[1] A. J. Brown, “Spectral curve ﬁtting for automatic hyperspectral data
analysis,” IEEE Trans. Geosci. Remote Sens., vol. 44, no. 6, pp. 1601–
1608, 2006.

[2] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
Tilton, “Advances in spectral-spatial classiﬁcation of hyperspectral im-
ages,” Proceedings of the IEEE, vol. 101, no. 3, pp. 652–675, 2013.
[3] J. Ma, J. Jiang, H. Zhou, J. Zhao, and X. Guo, “Guided locality
preserving feature matching for remote sensing image registration,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 8, pp. 4435–4447, 2018.
[4] X. Jia, B.-C. Kuo, and M. M. Crawford, “Feature mining for hyperspec-
tral image classiﬁcation,” Proceedings of the IEEE, vol. 101, no. 3, pp.
676–697, 2013.

[5] A. J. Brown, B. Sutter, and S. Dunagan, “The marte vnir imaging
spectrometer experiment: Design and analysis,” Astrobiology, vol. 8,
no. 5, pp. 1001–1011, 2008.

[6] A. J. Brown, S. J. Hook, A. M. Baldridge, J. K. Crowley, N. T. Bridges,
B. J. Thomson, G. M. Marion, C. R. de Souza Filho, and J. L. Bishop,
“Hydrothermal formation of clay-carbonate alteration assemblages in the
nili fossae region of mars,” Earth and Planetary Science Letters, vol.
297, no. 1-2, pp. 174–182, 2010.

[7] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial
hyperspectral image classiﬁcation: An overview and new guidelines,”
IEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, 2018.
[8] J. Jiang, C. Chen, Y. Yu, X. Jiang, and J. Ma, “Spatial-aware collab-
orative representation for hyperspectral remote sensing image classiﬁ-
cation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 3, pp. 404–408,
2017.

[9] R. Ji, Y. Gao, R. Hong, Q. Liu, D. Tao, and X. Li, “Spectral-spatial con-
straint hyperspectral image classiﬁcation,” IEEE Trans. Geosci. Remote
Sens., vol. 52, no. 3, pp. 1811–1824, 2014.

[10] X. Kang, S. Li, and J. A. Benediktsson, “Spectral–spatial hyperspectral
image classiﬁcation with edge-preserving ﬁltering,” IEEE Trans. Geosci.
Remote Sens., vol. 52, no. 5, pp. 2666–2677, 2014.

[11] F. Tong, H. Tong, J. Jiang, and Y. Zhang, “Multiscale union regions
adaptive sparse representation for hyperspectral image classiﬁcation,”
Remote Sens., vol. 9, no. 9, 2017.

IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING

15

hyperspectral imagery,” IEEE Trans. Geosci. Remote Sens., vol. 52,
no. 11, pp. 7008–7022, Nov 2014.

Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp.
37–52, 1987.

[36] X. Zheng, Y. Yuan, and X. Lu, “Dimensionality reduction by spatial–
spectral preservation in selected bands,” IEEE Trans. Geosci. Remote
Sens., vol. 55, no. 9, pp. 5185–5197, 2017.

[37] A. T. Kalai and R. A. Servedio, “Boosting in the presence of noise,”
Journal of Computer and System Sciences, vol. 71, no. 3, pp. 266–290,
2005.

[38] J. Li, H. Zhang, and L. Zhang, “Efﬁcient superpixel-level multitask
joint sparse representation for hyperspectral image classiﬁcation,” IEEE
Trans. Geosci. Remote Sens., vol. 53, no. 10, pp. 5338–5351, 2015.
[39] J. Jiang, J. Ma, C. Chen, Z. Wang, Z. Cai, and L. Wang, “SuperPCA:
A superpixelwise principal component analysis approach for unsuper-
vised feature extraction of hyperspectral imagery,” IEEE Trans. Geosci.
Remote Sens., vol. 56, no. 8, pp. 4581–4593, 2018.

[40] S. Zhang, S. Li, W. Fu, and L. Fang, “Multiscale superpixel-based sparse
representation for hyperspectral image classiﬁcation,” Remote Sensing,
vol. 9, no. 2, p. 139, 2017.

[41] F. Fan, Y. Ma, C. Li, X. Mei, J. Huang, and J. Ma, “Hyperspectral image
denoising with superpixel segmentation and low-rank representation,”
Information Sciences, vol. 397, pp. 48–68, 2017.

[42] M.-Y. Liu, O. Tuzel, S. Ramalingam, and R. Chellappa, “Entropy rate

superpixel segmentation,” in CVPR.

IEEE, 2011, pp. 2097–2104.

[43] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, p. 2274, 2012.
[44] S. Wold, K. Esbensen, and P. Geladi, “Principal component analysis,”

[45] Q. Wang, J. Lin, and Y. Yuan, “Salient band selection for hyperspectral
image classiﬁcation via manifold ranking,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 27, no. 6, pp. 1279–1289, June 2016.

[46] L. Fang, N. He, S. Li, P. Ghamisi, and J. A. Benediktsson, “Extinction
proﬁles fusion for hyperspectral images classiﬁcation,” IEEE Trans.
Geosci. Remote Sens., vol. 56, no. 3, pp. 1803–1815, 2018.

[47] J. Canny, “A computational approach to edge detection,” in Readings in

Computer Vision. Elsevier, 1987, pp. 184–203.

[48] R. Kothari and V. Jain, “Learning from labeled and unlabeled data,” in
International Joint Conference on Neural Networks, 2002, pp. 2803–
2808.

[49] X. Zhu and Z. Ghahramani, “Learning from labeled and unlabeled data

with label propagation,” 2002.

[50] Y. Freund, “Boosting a weak learning algorithm by majority,” Informa-

tion and computation, vol. 121, no. 2, pp. 256–285, 1995.

[51] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE

transactions on information theory, vol. 13, no. 1, pp. 21–27, 1967.

[52] J. Abell´an and A. R. Masegosa, “Bagging schemes on the presence of
class noise in classiﬁcation,” Expert Systems with Applications, vol. 39,
no. 8, pp. 6827–6837, 2012.

[53] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation-based anomaly
detection,” ACM Transactions on Knowledge Discovery from Data
(TKDD), vol. 6, no. 1, p. 3, 2012.

[54] A. Krieger, C. Long, and A. Wyner, “Boosting noisy data,” in ICML,
2001, pp. 274–281.

