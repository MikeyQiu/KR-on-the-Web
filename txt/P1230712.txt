Grounding Referring Expressions in Images by Variational Context

Hanwang Zhang1∗

Yulei Niu2†
1Nanyang Technological University, 2Renmin University of China, 3Columbia University,
hanwangzhang@ntu.edu.sg; niu@ruc.edu.cn; shih.fu.chang@columbia.edu

Shih-Fu Chang3

8
1
0
2
 
r
a

M
 
1
3
 
 
]

V
C
.
s
c
[
 
 
2
v
2
9
8
1
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

We focus on grounding (i.e., localizing or linking) refer-
ring expressions in images, e.g., “largest elephant stand-
ing behind baby elephant”. This is a general yet chal-
lenging vision-language task since it does not only require
the localization of objects, but also the multimodal com-
prehension of context — visual attributes (e.g., “largest”,
“baby”) and relationships (e.g., “behind”) that help to dis-
tinguish the referent from other objects, especially those
of the same category. Due to the exponential complex-
ity involved in modeling the context associated with mul-
tiple image regions, existing work oversimpliﬁes this task
to pairwise region modeling by multiple instance learning.
In this paper, we propose a variational Bayesian method,
called Variational Context, to solve the problem of com-
plex context modeling in referring expression grounding.
Our model exploits the reciprocal relation between the ref-
erent and context, i.e., either of them inﬂuences estima-
tion of the posterior distribution of the other, and thereby
the search space of context can be greatly reduced. We
also extend the model to unsupervised setting where no
annotation for the referent is available. Extensive exper-
iments on various benchmarks show consistent improve-
ment over state-of-the-art methods in both supervised and
unsupervised settings. The code is available at https:
//github.com/yuleiniu/vc/.

1. Introduction

Grounding natural language in visual data is a hallmark
of AI, since it establishes a communication channel be-
tween humans, machines, and the physical world, under-
pinning a variety of multimodal AI tasks such as robotic
navigation [38], visual Q&A [1, 16, 49], and visual chat-
bot [6]. Thanks to the rapid development in deep learning-
based CV and NLP, we have witnessed promising results
not only in grounding nouns (e.g., object detection [30]),

∗Hanwang was a research scientist at Columbia University.
†Yulei was a visiting student at Columbia University.

Figure 1. The proposed Variational Context model. Given an input
referring expression and an image with region proposals, we local-
ize the referent as output. We develop a grounding score function,
with the variational lower-bound composed by three cue-speciﬁc
multimodal modules, indicated by the description in the dashed
color boxes.

but also short phrases (e.g., noun phrases [28] and rela-
tions [47, 36]). However, the more general task: grounding
referring expressions [25], is still far from resolved due to
the challenges in understanding of both language and scene
compositions [10]. As illustrated in Figure 1, given an in-
put referring expression “largest elephant standing behind
baby elephant” and an image with region proposals, a model
that can only localize “elephant” is not satisfactory as there
are multiple elephants. Therefore, the key for referring ex-
pression grounding is to comprehend and model the con-
text. Here, we refer to context as the visual objects (e.g.,
“elephant”), attributes (e.g., “largest” and “baby”), and re-
lationships (e.g., “behind”) mentioned in the expression that
help to distinguish the referent from other objects.

One straightforward way of modeling the relations be-
tween the referent and context is to: 1) use external syn-
tactic parsers to parse the expression into entities, mod-
iﬁers, and relations [34], and then 2) apply visual rela-
tion detectors to localize them [47]. However, this two-
stage approach is not practical due to the limited general-
ization ability of the detectors applied in the highly unre-
stricted language and scene compositions. To this end, re-

1

cent approaches use multimodal embedding networks that
jointly comprehend language and model the visual rela-
tions [26, 11]. Due to the prohibitively high cost of an-
notating both referent and context of referring expressions
in images, multiple instance learning (MIL) [7] is usually
adopted in them to handle the weak supervision of the unan-
notated context objects, by maximizing the joint likelihood
of every region pair. However, for a referent, the MIL
framework essentially oversimpliﬁes the number of context
conﬁgurations of N regions from O(2N ) to O(N ). For ex-
ample, to localize the “elephant” in Figure 1, we may need
to consider the other three elephants all together as a multi-
nomial subset for modeling the context such as “largest”,
“behind” and “baby elephant”.

In this paper, we propose a novel model called Varia-
tional Context for grounding referring expressions in im-
ages. Compared to the previous MIL-based approaches [26,
11], our model approximates the combinatorial context
conﬁgurations with weak supervision using a variational
Bayesian framework [15]. Intuitively, it exploits the reci-
procity between referent and context, given either of which
can help to localize the other. As shown in Figure 1, for
each region x, we ﬁrst estimate a coarse context z, which
will help to reﬁne the true localizations of the referent. This
reciprocity is formulated into the variational lower-bound
of the grounding likelihood p(x|L), where L is the text
expression and the context is considered as a hidden vari-
able z (cf. Section 3). Speciﬁcally, the model consists of
three multimodal modules: context posterior q(z|x, L), ref-
erent posterior p(x|z, L), and context prior pz(z|L), each
of which performs a grounding task (cf. Section 4.3) that
aligns image regions with a cue-speciﬁc language feature;
each cue dynamically encodes different subsets of words in
the expression L that help the corresponding localization
(cf. Section 4.2).

Thanks to the reciprocity between referent and context,
our model can not only be used in the conventional super-
vised setting, where there is annotation for referent , but
also in the challenging unsupervised setting, where there
is no instance-level annotation (e.g., bounding boxes) of
both referent and context. We perform extensive experi-
ments on four benchmark referring expression datasets: Re-
fCLEF [14], RefCOCO [45], RefCOCO+ [45], and Ref-
COCOg [25]. Our model consistently outperforms previous
methods in both supervised and unsupervised settings. We
also qualitatively show that our model can ground the con-
text in the expression to the corresponding image regions
(cf. Section 5).

2. Related Work

Grounding Referring Expressions. Grounding refer-
ring expression is also known as referring expression com-
prehension, whose inverse task is called referring expres-

sion generation [25]. Different from grounding phrases [29,
28] and descriptive sentences [12, 32], the key for ground-
ing referring expression is to use the context (or pragmatics
in linguistics [37]) to distinguish the referent from other ob-
jects, usually of the same category [10]. However, most pre-
vious works resort to use holistic context such as the entire
image [25, 12, 32] or visual feature difference between re-
gions [45, 46]. Our model is similar to the works on explic-
itly modeling the referent and context region pairs [11, 26],
however, due to the lack of context annotation, they re-
duce the grounding task into a multiple instance learning
framework [7]. As we will discuss later, this framework
is not a proper approximation to the original task. There
are also studies on visual relation detection that detect ob-
jects and their relationships [21, 5, 47, 17, 48]. However,
they are limited to a ﬁxed-vocabulary set of relation triplets
and hence are difﬁcult to be applied in natural language
grounding. Our cue-speciﬁc language feature is similar to
the language modular network [11] that learns to decom-
pose a sentence into referent/context-related words, which
are different from other approaches that treat the expression
as a whole [25, 23, 46, 19].

Variational Bayesian Model vs. Multiple Instance
Learning. Our proposed variational context model is in a
similar vein of the deep neural network based variational
autoencoder (VAE) [15], which uses neural networks to
approximate the posterior distribution of the hidden value
q(z|x), i.e., encoder, and the conditional distribution of the
observation p(x|z), i.e., decoder. VAE shows efﬁcient and
effective end-to-end optimization for the intractable log-
sum likelihood log (cid:80)
z p(x, z) that is widely used in genera-
tive processes such as image synthesis [44] and video frame
prediction [43]. Considering the unannotated context as the
hidden variable z, the referring expression grounding task
can also be formulated into the above log-sum marginaliza-
tion (cf. Eq. (2)). The MIL framework [7] is essentially a
sum-log approximation of the log-sum, i.e., (cid:80)
z log p(x, z).
To see this, the max-pooling function log maxz p(x, z) used
in [11] can be viewed as the sum-log (cid:80)
z log p(x|z)p(z),
where p(z) = 1 if z is the correct context and 0 otherwise,
indicating there is only one positive instance; maximizing
the noisy-or function log(1 − (cid:81)
z(1 − p(x, z))) used in [26]
is equivalent to maximize (cid:80)
z log p(x, z), assuming there is
at least one positive instance. However, due to the numer-
ical property of the log function, this sum-log approxima-
tion will unnecessarily force every (x, z) pair to explain the
data [8]. Instead, we use the variational Bayesian upper-
bound to obtain a better sum-log approximation. Note
that visual attention models [2, 42] simplify the variational
lower bound by assuming p(z) = q(z|x); however, we
explicitly use the KL divergence KL(q(z|x)||p(z)) in the
lower bound to regularize the approximate posterior q(z|x)
not being too far from the prior p(z).

2

3. Variational Context

In this section, we derive the variational Bayesian for-
mulation of the proposed variational context model and the
objective function for training and test.

3.1. Problem Formulation

The task of grounding a referring expression L in an im-
age I, represented by a set of regions x ∈ X , can be viewed
as a region retrieval task with the natural language query
L. Formally, we maximize the log-likelihood of the condi-
tional distribution to localize the referent region x∗ ∈ X :

x∗ = arg max

log p(x|L),

(1)

x∈X

where we omit the image I in p(x|I, L).

As there is usually no annotation for the context, we
consider it as a hidden variable z. Therefore, Eq. (1)
can be rewritten as the following maximization of the log-
likelihood of the conditional marginal distribution:

x∗ = arg max

log

p(x, z|L).

(2)

x∈X

(cid:88)

z

Note that z is NOT necessary to be one region as assumed
in recent MIL approaches [11, 26], i.e., z ∈ X . For ex-
ample, the contextual objects “surrounding elephants” in
“a bigger elephant than the surrounding elephants” should
be composed by a multinomial subset of X , resulting in an
extremely large sample space that requires O(2|X |) search
complexity. Therefore, the marginalization in Eq (2) is in-
tractable in general.

To this end, we use the variational lower-bound [15] to

approximate the marginal distribution in Eq. (2) as:

log p(x|L) = log

p(x, z|L) ≥ Q(x, L) =

(cid:88)

z

Ez∼qφ(z|x,L) log pθ(x|z, L)
(cid:125)
(cid:123)(cid:122)
(cid:124)
Localization

− KL (qφ(z|x, L)||pω(z|L))
(cid:123)(cid:122)
(cid:125)
Regularization

(cid:124)

,

(3)

where KL(·) is the Kullback-Leibler divergence, φ, θ, and
ω are independent parameter sets for the respective distri-
butions. As shown in Figure 1, the lower bound Q(x, L)
offers a new perspective for exploiting the reciprocal nature
of referent and context in referring expression grounding:
Localization. This term calculates the localization score
for x given an estimated context z, using the referent-cue
of L parameterized by θ.
In particular, we design a new
posterior qφ(z|x, L) that approximates the true context prior
p(z|x, L), which models the context z using the context-cue
of L parameterized by φ. In the view of variational auto-
encoder [15, 35], this term works in an encoding-decoding
fashion: qφ is the encoder from x to z, and pθ is the decoder

from z to x.
Regularization. As KL is non-negative, maximizing
Q(x, L) would encourage that the posterior qφ is similar
to the prior pω, i.e., the estimated context z sampled from
qφ(z|x, L) should not be too far from the referring expres-
sion, which is modeled by pω(z|L) with the generic-cue of
L parameterized by ω. This term is necessary as the esti-
mated z could be overﬁtted to region features that are incon-
sistent with the visual context described in the expression.

3.2. Training and Test

Deterministic Context.

The lower-bound Q(x, L)
transforms the intractable log-sum in Eq. (2) into the efﬁ-
cient sum-log in Eq. (3), which can be optimized by us-
ing Monte Carlo unbiased gradient estimator such as RE-
INFORCE [40]. However, due to that φ is dependent on
the sampling of z over O(2|X |) conﬁgurations, its gradient
variance is large. To this end, we implement qφ(z|x, L) as
a differentiable but biased encoder:

z = f (x, L) =

x(cid:48) · qφ(x(cid:48)|x, L),

(4)

(cid:88)

x(cid:48)∈X

where we slightly abuse qφ as a score function such that
(cid:80)
x(cid:48) qφ(x(cid:48)|x, L) = 1. Note that this deterministic context
can be viewed as applying the “re-parameterization” trick as
in Variational Auto-Encoder [15]: rewriting z ∼ qφ(z|x, L)
to z = f (x, L; (cid:15)), (cid:15) ∼ p((cid:15)), where the stochasticity of the
auxiliary random variable (cid:15) comes from training samples
x ∈ X ((cid:15)). A clear example is Adversarial Autoencoder [24]
which shows that such stochasticity achieves similar test-
likelihood compared to other distributions such as Gaus-
sian.

Objective Function. Applying Eq. (4) to Eq. (3), we can
rewrite Q(x, L) into a function of only one sample estima-
tion, which is a common practice in SGD:

Q(x,L) = log pθ(x|z,L)−log qφ(z|x,L)+log pω(z|L).

(5)

is known,

In supervised setting where the ground truth of the ref-
erent
to distinguish the referent from other
objects, we need to train a model that outputs a high
p(x|L) (i.e., Q(x, L)), while maintaining a low p(x(cid:48)|L)
(i.e., Q(x(cid:48), L)), whenever x(cid:48)
(cid:54)= x. Therefore, we use
the so-called Maximum Mutual Information loss as in [25]
− log{Q(x, L)/ (cid:80)
x(cid:48) Q(x(cid:48), L)}, where we do not need to
explicitly model the distributions with normalizations; we
use the following score function:

Q(x, L) ∝ S(x, L) = sθ(x, L)−sφ(x, L)+sω(x, L), (6)

where z is omitted as it is a function of x in Eq. (4). sθ,
sφ, and sω are the score functions (e.g., pθ ∝ sθ) for pθ,
qφ, and pω, respectively. These functions will be detailed in

3

Figure 2. The architecture of the proposed Variational Context model. It consists of a region feature extraction module (Section 4.1, and
a language feature extraction module (Section 4.2), and three grounding modules (Section 4.3). It can be trained in an end-to-end fashion
with the input of a set of image regions and a referring expression, using the supervised loss ( Eq. (7)) or the unsupervised loss (Eq. (8)).
fc: fully-connected layer. concat: vector concatenation. L2Norm: L2 normalization layer. (cid:12): element-wise vector multiplication. ⊕: add.

Section 4.3. In this way, maximizing Eq. (5) is equivalent
to minimizing the following softmax loss:

Ls = − log softmax S(xgt, L),

(7)

where the softmax is over x ∈ X and xgt is the ground truth
referent region.

Note that the reciprocity between referent and context
can be extended to unsupervised learning, where neither of
the referent and context has annotation. In this setting, we
adopt the image-level max-pooled MIL loss functions for
unsupervised referring expression grounding:

Lu = − max
x∈X

log softmax S(x, L),

(8)

where the softmax is over x ∈ X . Note that the max-pooled
MIL function is reasonable since there is only one ground
truth referent given an expression and image training pair.

At test stage, in both supervised and unsupervised set-
tings, we predict the referent region x∗ by selecting the re-
gion x ∈ X with the highest score:

x∗ = arg max

S(x, L),

x∈X

(9)

4. Model Architecture

The overall architecture of the proposed variational con-
text model is illustrated in Figure 2. Thanks to the deter-
ministic context in Eq. (4), the ﬁve modules in our model
can be integrated into an end-to-end differentiable fashion.
Next, we will detail the implementation of each module.

4.1. RoI Features

Given an image with a set of Region of Interests (RoIs)
X , obtained by any off-the-shelf proposal generator [50] or
object detectors [20], this module extracts the feature vec-
tor xi for every RoI. In particular, xi is the concatenation
of visual feature vi and spatial feature pi. For vi, we can
use the output of a pre-trained convolutional network (cf.
Section 5). If the object category of each RoI is available,
we can further utilize the comparison between the referent

(cid:80)

and other objects to capture the visual difference such as
“the largest/baby elephant”. Speciﬁcally, we append the vi-
vi−vj
sual difference feature [45] δvi = 1
||vi−vj || to the
n
original vi visual feature, where n is the number of objects
chosen for comparison (e.g., the number of RoI in the same
object category). For spatial feature, we use the 5-d spatial
W , ybr
attributes pi = [ xtl
W ·H ], where x and y are
the coordinates the top left (tl) and bottom right (br) RoI of
the size w × h, and the image is of the size W × H.

H , w·h

H , xbr

W , ytl

j(cid:54)=i

4.2. Cue-Speciﬁc Language Features

The cue-speciﬁc language feature representation for a re-
ferring expression is inspired by the attention weighted sum
of word vectors [11, 22, 3], where the weights are param-
eterized by context-cue φ, referent-cue θ, and generic-cue
ω. The context-cue language feature yc = [yc1, yc2] is
a concatenation of yc1 for language-vision association be-
tween single RoI and the expression, and yc2 for the as-
sociation between pairwise RoIs; the referent-cue language
feature yr can be represented in a similar way to yc; the
generic-cue language feature yg is only for single RoI asso-
ciation as it is an independent prior. The weights of each cue
are calculated from the hidden state vectors of a 2-layer bi-
directional LSTM (BLSTM) [33], scanning through the ex-
pression. The hidden states encode forward and backward
compositional semantic meanings of the sentences, beneﬁ-
cial for selecting words that are useful for single and pair-
wise associations. Speciﬁcally, suppose hj as the 4,000-d
concatenation of forward and backward hidden vectors of
the j-th word, without loss of generality, the word attention
weight αj and the language feature y for single/pairwise
association of any cue can be calculated as:

mj = fc(hj), αj = softmaxj(mj), y =

αjwj, (10)

(cid:88)

j

where wj is a 300-d vector. Note that the BLSTM module
can be jointly trained with the entire model.

Figure 3 shows that the cue-speciﬁc language features
dynamically weight words in different expressions. We can
have two interesting observations. First, c1 is almost uni-

4

Figure 3. Two qualitative examples of the cue-speciﬁc language
feature word weights. Darker color indicates higher weights.
c/r+1/2: context/referent-cue + single/pairwise.

form while c2 is highly skewed; although r2 is more skewed
than c1, it is still less skewed than r1. This is reasonable
since: 1) without ground-truth, individual score (c1) does
not help much for context estimation from scratch; context
is more easily found by the pairwise score (c2) induced by
relationships or other objects (e.g., “left” or “frisbee”); 2) in
referent grounding with ground truth, individual score (r1)
is sufﬁcient (e.g., “dog lying” and “black white dog”) and
pairwise score (r2) is helpful; 3) g is adaptive to the num-
ber of object categories in the expression, i.e., if the context
object is of the same category as the referent, g weighs de-
scriptive or relationship words higher (e.g., “lying, stand-
ing, left”), and nouns higher (e.g., “frisbee”), otherwise;
moreover, it demonstrates that the deterministic guess of z
in Eq. (4) is meaningful.

4.3. Score Functions

For any image and expression pair, given the RoI feature
xi, and the cue-speciﬁc language feature yc, yr, and yg, we
implement the ﬁnal grounding score in Eq. (6) as:

zi =

(cid:88)
j

softmaxj (sφ(xi, xj, yc)) xj,

sθ(x, L) ← sθ(xi, zi, yr),
sφ(x, L) ← sφ(xi, zi, yc),
sω(x, L) ← sω(zi, yg),

(11)

where the right-hand side functions are deﬁned as below.

Context Estimation Score: sφ(xi, xj, yc). It is a score
function for modeling the context posterior qφ(z|x, L), i.e.,
given an RoI xi as the candidate referent, we calculate the
likelihood of any RoI xj to be the context. We can also
use this function to estimate the ﬁnal context posterior score
sφ(xi, zi, yc). Speciﬁcally, the context estimation score is a
sum of the single and pairwise vision-language association
scores: xj and yc1, [xi, xj] and yc2. Each associate score
is an fc output from the input of a normalized feature:

j = yc1 (cid:12) fc(xj), m2
m1
(cid:101)m1
j ), (cid:101)m2
j = L2Norm(m1
sφ(xi, xj, yc) = fc( (cid:101)m1

j = yc2 (cid:12) fc([xi, xj]),
j = L2Norm(m2
j ) + fc( (cid:101)m2
j ),

j ),

(12)

obtain the estimated context z as zi = (cid:80)
βj = softmaxj(sφ(xi, xj, yc)).

j βjxj, where

Referent Grounding Score: sθ(xi, zi, yr). After ob-
taining the context feature zi, we can use this score function
to calculate how likely a candidate RoI xi is the referent
given the context zi. This function is similar to Eq. (12).

Score:

Context Regularization

sω(zi, yg) −
sφ(xi, zi, yc). As discussed in Eq. (6),
this function
scores how likely the estimated context feature zi
is
consistent with the content mentioned in the expression. In
particular, sω(zi, yg) is only dependent on single RoI:
i (cid:12) fc(zi), (cid:101)mi=L2Norm(mi), sω(zi, yg
mi=yg

i )=fc(mi).
(13)

5. Experiment

5.1. Datasets

We used four popular benchmarks for the referring ex-

pression grounding task.

RefCOCO [45]. It has 142,210 referring expressions for
50,000 referents (e.g., object instances) in 19,994 images
from MSCOCO [18]. The expressions are collected in an
interactive way [14]. The dataset is split into train, valida-
tion, Test A, and Test B, which has 120,624, 10,834, 5,657
and 5,095 expression-referent pairs, respectively. An image
contains multiple people in Test A and multiple objects in
Test B.

RefCOCO+ [45]. It has 141,564 expressions for 49,856
referents in 19,992 images from MSCOCO. The difference
from RefCOCO is that it only allows appearances but no
locations to describe the referents. The split is 120,191,
10,758, 5,726 and 4,889 expression-referent pairs for train,
validation, Test A, and Test B respectively.

RefCOCOg [25]. It has 95,010 referring expressions for
49,822 objects in 25,799 images from MSCOCO. Different
from RefCOCO and RefCOCO+, this dataset not collected
in an interactive way and contains longer sentences contain-
ing both appearance and location expressions. The split is
85,474 and 9,536 expression-referent pairs for training and
validation. Note that there is no open test split for Ref-
COCOg, so we used the hyper-parameters cross-validated
on RefCOCO and RefCOCO+.

RefCLEF [14]. It contains 20,000 images with anno-
tated image regions. It has some ambiguous (e.g. anywhere)
phrases and mistakenly annotated image regions that are not
described in the expressions. For fair comparison, we used
the split released by [12, 32], i.e., 58,838, 6,333 and 65,193
expression-referent pairs for training, validation and test, re-
spectively.

5.2. Settings and Metrics

where the element-wise multiplication (cid:12) is an effective way
for multimodal features [2]. According to Eq. (4), we can

We used an English vocabulary of 72,704 words con-
tained in the GloVe pre-trained word vectors [27], which

5

Table 1. Supervised grounding performances (Acc%) of comparing methods on RefCOCO, RefCOCO+, and RefCOCOg. Note that [46]
reports slightly higher accuracies using ensemble models of Listener and Speaker. For fair comparison, we only report their single models.

Dataset

Split MMI [25] NegBag [26] Attr [19] CMN [11]

Speaker [46]

Listener [46] VC w/o reg VC w/o α

VC

State-of-The-Arts

Our Baselines

RefCOCO

RefCOCO+

RefCOCOg

Val

RefCOCO(det)

RefCOCO+(det)

RefCOCOg(det)

Val

Test A

Test B

Test A

Test B

Test A

Test B

Test A

Test B

71.72

71.09

58.42

51.23

62.14

64.90

54.51

54.03

42.81

45.85

75.6

78.0

—

—

68.4

58.6

56.4

—

—

39.5

78.85

78.07

61.47

57.22

69.83

72.08

57.29

57.97

46.20

52.35

75.94

79.57

59.29

59.34

69.30

71.03

65.77

54.32

47.76

57.47

78.95

80.22

64.60

59.62

72.63

72.95

63.43

60.43

48.74

59.51

78.45

80.10

63.34

58.91

72.25

72.95

62.98

59.61

48.44

58.32

75.59

79.69

60.76

60.14

71.05

70.78

65.10

56.82

51.30

60.95

74.03

78.27

57.61

54.37

65.13

70.73

64.63

53.33

46.88

55.72

78.98

82.39

62.56

62.90

73.98

73.33

67.44

58.40

53.18

62.30

Figure 4. Qualitative results on RefCOCOg (det) showing comparisons between correct (green tick) and wrong referent grounds (red cross)
by VC and CMN. The denotations of the bounding box colors are as follows. Solid red: referent ground; solid green: ground truth; dashed
yellow: context ground. We only display top 3 context objects with the context ground probability > 0.1. We can observe that VC has
more reasonable context localizations than CMN, even in cases when the referent ground of VC fails.

was also used for the initialization of our word vectors. We
used a “unk” symbol for the input word of the BLSTM if the
word is out of the vocabulary; we set the sentence length to
20 and used “pad” symbol to pad expression sentence < 20.
For RoI visual features on RefCOCO, RefCOCO+, and Re-
fCOCOg which have MSCOCO annotated regions with ob-
ject categories, we used the concatenation of the 4,096-d
fc7 output of a VGG-16 based Faster-RCNN network [31]
trained on MSCOCO and its corresponding 4,096-d visd-
iff feature [45]; although RefCLEF regions also have object

categories, for fair comparison with [32], we did not use the
visdiff feature.

The model training is single-image based, with all re-
ferring expressions annotated. We applied SGD of 0.95-
momentum with initial learning rate of 0.01, multiplied by
0.1 after every 120,000 iterations, up to 160,000 iterations.
Parameters in BILSTM and fc-layers were initialized by
Xavier [9] with 0.0005 weight decay. Other settings were
default in TensorFlow. Note that our model is trained with-
out bells and whistles, therefore, other optimization tricks

6

such as batch normalization [13] and GRU [4] are expected
to further improve the results reported here. Besides the
ground truth annotations, grounding to automatically de-
tected objects is a more practical setting. Therefore, we also
evaluated with the SSD-detected bounding boxes [20] on
the four datasets provided by [46]. A grounding is consid-
ered as correct if the intersection-over-union (IoU) of the
top-1 scored region and the ground-truth object is larger
than 0.5. The grounding accuracy (a.k.a, P@1) is the frac-
tion of correctly grounded test expressions.

5.3. Evaluations of Supervised Grounding

We compared our variational context model (VC) with
state-of-the-art referring expression methods published in
recent years, which can be categorized into: 1) generation-
comprehension based such as MMI
[19],
Speaker [46], Listener [46], and SCRC [12]; 2) localiza-
tion based such as GroundR [32], NegBag [26], CMN [11].
Note that NegBag and CMN are MIL-based models. In par-
ticular, we used the author-released code to obtain the re-
sults of CMN on RefCLEF, RefCOCO, and RefCOCO+.

[25], Attr

From the results on RefCOCO, RefCOCO+, and Re-
fCOCOg in Table 1 and that on RefCLEF in Table 2,
we can see that VC achieves the state-of-the-art perfor-
mance. We believe that the improvement is attributed to
the variational Bayesian modeling of context. First, on all
datasets, except for the most recent reinforcement learn-
ing based [46], VC outperforms all the other sentence
generation-comprehension methods that do not model con-
text. Second, compared to VC without the regularization
term in Eq. (3) (VC w/o reg), VC can boost the performance
by around 2% on all datasets. This demonstrates the ef-
fectiveness of the KL divergence for the prevention of the
overﬁtted context estimation.

In particular, we further demonstrate the superiority of
VC over the most recent MIL-based method CMN. As il-
lustrated in Figure 4, VC has better context comprehension
in both of the language and image regions than CMN. For
example, in the top two rows where VC is correct and CMN
is wrong, for the grounding in the second column, CMN
unnecessarily considers the “girl” as context but the expres-
sion only describes using “elephant”; in the last column,
CMN misses the key context “frisbee”. Even in the fail-
ure cases where VC is wrong and CMN is correct, VC still
localizes reasonable context. For example, in the fourth col-
umn, although CMN grounds the correct TV, but it is based
on incorrect context of other TVs; while VC can predict
the correct context “children”.
In addition, we observed
that most of the cases that CMN is better than VC involves
multiple humans. This demonstrates that VC is better at
grounding objects of different categories.

VC is also effective in images with more objects. Fig-
ure 5 shows the performances of VC and CMN with various

Table 2. Performances (Acc%) of supervised and unsupervised
methods on RefCLEF.

Sup.

Sup. (det) Unsup. (det)

SCRC [12]

72.74

GroundR [32] —

CMN [11]

VC

VC w/o α

81.52

82.43

79.60

17.93

26.93

28.33

31.13

27.40

10.70

—

—

14.11

14.50

Table 3. Unsupervised grounding performances (Acc%) of com-
paring methods on RefCOCO, RefCOCO+, and RefCOCOg.
VC w/o α

VC w/o reg

Dataset

Split

VC

RefCOCO

RefCOCO+

RefCOCOg

Val

RefCOCO(det)

RefCOCO+(det)

RefCOCOg(det)

Val

Test A

Test B

Test A

Test B

Test A

Test B

Test A

Test B

13.59

21.65

18.79

24.14

25.14

17.14

22.30

19.74

24.05

28.14

17.34

20.98

23.24

24.91

33.79

20.91

21.77

25.79

25.54

33.66

33.29

30.13

34.60

31.58

30.26

32.68

27.22

34.68

28.10

29.65

number of bounding boxes. We can observe that VC con-
siderably outperforms CMN over all bounding boxes num-
bers. Recall that context is the key to distinguish objects of
the same category. In particular, on the Test A sets of Ref-
COCO and RefCOCO+ where the grounding is only about
people, i.e., the same object category, the gap between VC
and CMN is becoming larger as the box number increases.
This demonstrates that MIL is ineffective in modeling con-
text, especially when the number of image regions is large.

5.4. Evaluations of Unsupervised Grounding

We follow the unsupervised setting in GroundR [32]. To
our best knowledge, it is the only work on unsupervised re-
ferring expression grounding. Note that it is also known as
“weakly supervised” detection [48] as there is still image-
level ground truth (i.e., the referring expression). Table 2 re-
ports the unsupervised results on the RefCLEF. We can see
that VC outperforms the state-of-the-art GroundR, which is
a generation-comprehension based method. This demon-
strates that using context also helps unsupervised ground-
ing. As there is no published unsupervised results on Re-
fCOCO, RefCOCO+, and RefCOCOg, we only compared
our baselines on them in Table 3. We can have the follow-
ing three key observations which highlight the challenges
of unsupervised grounding:

Context Prior. VC w/o reg is the baseline without the
KL divergence as a context regularization in Eq. (3). We can
see that in most of the cases, VC considerably outperforms
VC w/o reg by over 2%, even over 5% on RefCOCO+ (det)

7

Figure 5. Performances of VC and CMN with different number of object bounding boxes on RefCOCO Test A &B, RefCOCO+ Test A &
B, and RefCOCOg Val. Compared to CMN, we can see that VC is more effective in context modeling when the number of objects is large.

Figure 6. Common failure cases in unsupervised grounding with
detected bounding boxes. From left to right: RefCOCO, Ref-
COCO+, and RefCOCOg. The failure is mainly to the challenging
unsupervised relation modeling between referent and context.

and RefCOCOg (det). Note that this improvement is signif-
icantly higher than that in supervised setting (e.g., < 3% as
reported in Table 1). The reason is that the context estima-
tion in Eq. (4) would be easier to be stuck in image regions
that are irrelevant to the expression in unsupervised setting,
therefore, context prior is necessary.

Language Feature. Except on RefCOCOg, we consis-
tently observed the ineffectiveness of the cue-speciﬁc lan-
guage feature in unsupervised setting, i.e., VC w/o α out-
performs VC in Table 2 and 3. Here α represents the cue-
speciﬁc word attention. This is contrary to the observation
in the supervised setting as listed in Table 1, where VC w/o
α is consistently lower than VC. Note that without the cue-
speciﬁc word attention α in Eq. (10), the language feature is
merely the average value of the word embedding vectors in
the expression. In this way, VC w/o α does not encode any
structural language composition as illustrated in Figure 3,
thus, it is better for short expressions. However, when the
expression is long in RefCOCOg, discarding the language
structure still degrades the performance on RefCOCOg.

Unsupervised Relation Discovery.

Although we
demonstrated that VC improves the unsupervised ground-
ing by modeling context, we believe that there is still a large
space for improving the quality of modeling the context.
As the failure examples shown in Figure (6), 1) many con-
text estimations are still out of the scope of the expression,
e.g., we may localize the “cup” and “table” as context even
though the expression is “woman with green t-shirt”; 2) we

8

Figure 7. Word cloud visualizations of cue-speciﬁc word attention
α in Eq. 10 of context-cue (c2), referent-cue (r1), and generic-cue
(g) using supervised (top row) and unsupervised training (bottom
row) on RefCOCOg. Without supervision, it is difﬁcult to discover
meaningful language compositions.

may mistake due to the wrong comprehension of the rela-
tions, e.g., “right” as “left”, even if the objects belong to
the same category, e.g., “elephant”. For further investiga-
tion, Figure 7 visualizes the cue-speciﬁc word attentions in
supervised and unsupervised settings. The almost identi-
cal word attentions in unsupervised setting reﬂect the fact
that the relation modeling between referent and context is
not as successful as in supervised setting. This inspires us
to exploit stronger prior knowledge such as language struc-
ture [41] and spatial conﬁgurations [48, 39].

6. Conclusions

We focused on the task of grounding referring expres-
sions in images and discussed that the key problem is how
to model the complex context, which is not effectively re-
solved by the multiple instance learning framework used
in prior works. Towards this challenge, we introduced
the Variational Context model, where the variational lower-
bound can be interpreted by the reciprocity between the ref-
erent and context: given any of which can help to localize
the other, and hence is expected to signiﬁcantly reduce the
context complexity in a principled way. We implemented
the model using cue-speciﬁc language-vision embedding
network that can be efﬁciently trained end-to-end. We val-
idated the effectiveness of this reciprocity by promising
supervised and unsupervised experiments on four bench-
marks. Moving forward, we are going to 1) incorporate ex-
pression language generation in the variational framework,
2) use more structural features of language rather than word
attentions, and 3) further investigate the potential of our
model in the unsupervised referring expression grounding.

the visual grounding task [11]. Therefore, we adopt the
parser jointly trained on the referring expression ground-
ing task [11]. As illustrated in Figure 8, this parser assigns
word-level attention weights of subject, relation, and object.
In particular, we consider the language features of c1, r2 as
the average word embeddings, c2 as the relation weights,
r1 as the subject weights, g as the object weights. Table 4
shows the performances on unsupervised grounding. We
can see that there is no signiﬁcant improvement of VC w/
parser over VC w/o α.

7.5. More Qualitative Results

Figure 10 shows more qualitative results on supervised
and unsupervised grounding results on RefCOCO, Ref-
COCO+, and RefCOCOg.

References

[1] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zit-
nick, and D. Parikh. Vqa: Visual question answering. In ICCV, 2015.
1

[2] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition

with visual attention. In ICLR, 2015. 2, 5

[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by

jointly learning to align and translate. 2015. 4

[4] K. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio. On
the properties of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259, 2014. 7

[5] B. Dai, Y. Zhang, and D. Lin. Detecting visual relationships with

deep relational networks. In CVPR, 2017. 2

[6] A. Das, S. Kottur, J. M. Moura, S. Lee, and D. Batra. Learning
cooperative visual dialog agents with deep reinforcement learning.
In ICCV, 2017. 1

[7] T. G. Dietterich, R. H. Lathrop, and T. Lozano-P´erez. Solving the
multiple instance problem with axis-parallel rectangles. Artiﬁcial
intelligence, 1997. 2

[8] C. W. Fox and S. J. Roberts. A tutorial on variational bayesian infer-

[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of training

deep feedforward neural networks. In ICAIS, 2010. 6

[10] D. Golland, P. Liang, and D. Klein. A game-theoretic approach to

generating spatial descriptions. In EMNLP, 2010. 1, 2

[11] R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko. Model-
ing relationships in referential expressions with compositional mod-
ular networks. In CVPR, 2017. 2, 3, 4, 6, 7

[12] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell.

Natural language object retrieval. In CVPR, 2016. 2, 5, 7

[13] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In ICML, 2015.
7

[14] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg. Refer-
itgame: Referring to objects in photographs of natural scenes.
In
EMNLP, 2014. 2, 5

[15] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In

ICLR, 2014. 2, 3

[16] Y. Li, N. Duan, B. Zhou, X. Chu, W. Ouyang, X. Wang, and M. Zhou.
Visual question generation as dual task of visual question answering.
In CVPR, 2018. 1

[17] Y. Li, W. Ouyang, and X. Wang. Vip-cnn: Visual phrase guided

convolutional neural network. In CVPR, 2017. 2

Figure 8. Some parsing examples from [11].

7. Supplementary Material

7.1. Derivation of Eq. (3)

Recall that we are going to transform the log-sum ob-
jective function in Eq. (2) to sum-log for tractable training.
Without loss of generality, we omit the conditional L in the
derivation. By using the concavity of the log function:

log((1 − α)x + αy) ≥ (1 − α) log(x) + α log(y),

(14)

to the following rewriting of Eq. (2):

(cid:88)

log

p(x, z) = log

(cid:32)

(cid:88)

z

(cid:33)

q(z|x)p(x, z)
q(z|x)

q(z|x) log

p(x, z)
q(z|x)

q(z|x) log p(x|z)p(z) −

q(z|x) log q(z|x)

q(z|x) log p(x|z) +

q(z|x) log p(z)

(cid:88)

=

q(z|x) log p(x|z) −

q(z|x)

(cid:88)

q(z|x)
p(z)

= Ez∼q(z|x) log p(x|z) − KL (q(z|x)||p(z)) = Q(x)

(15)

7.2. More Examples on Language Features

Figure 9 shows more cue-speciﬁc language features on

RefCOCOg.

7.3. More Results on Unsupervised Grounding

7.4. External Parsers

As discussed in Section 5.4 that the language feature in
unsupervised VC is not as good as that in the supervised
setting. An alternative is to use external NLP parsers to ob-
tain the compositions. However, conventional parsers (e.g.,
Standford Dependency) are observed to be suboptimal to

(cid:88)

z

(cid:88)

(cid:88)

−

z

z

z

z
(cid:88)

z
(cid:88)

z
(cid:88)

≥

=

=

z

z

9

q(z|x) log q(z|x)

ence. Artiﬁcial intelligence review, 2012. 2

Table 4. Unsupervised grounding performances (%) of VC w/ or w/o parsers on the four datasets.
RefCLEF RefCOCO TestA RefCOCO TestB

RefCOCO+TestA

RefCOCO+TestB

RefCOCOg

VC w/ parser

GT

VC

VC w/o α

VC w/ parser

VC

VC w/o α

22.57

21.06

20.72

14.90

14.11

14.50

23.00

17.34

33.29

24.07

20.91

32.68

27.50

20.98

30.13

25.12

21.77

27.22

24.69

23.24

34.60

26.18

25.79

34.68

28.96

24.91

31.58

26.92

25.54

28.10

30.73

33.79

30.26

30.64

33.66

29.65

DET

RefCLEF RefCOCO TestA RefCOCO TestB RefCOCO+ TestA RefCOCO+ TestB RefCOCOg

Figure 9. Qualitative results of the cue-speciﬁc language features on RefCOCOg.

[18] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick. Microsoft coco: Common objects in
context. In ECCV, 2014. 5

[19] J. Liu, L. Wang, and M.-H. Yang. Referring expression generation

and comprehension via attributes. In ICCV, 2017. 2, 6, 7

[20] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and
A. C. Berg. Ssd: Single shot multibox detector. In ECCV, 2016. 4, 7
[21] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual relationship

10

Figure 10. Qualitative results on RefCOCO, RefCOCO, RefCOCO+, and RefCOCOg. All of them are from det bounding boxes. The ﬁrst
line denotes the supervised results and the second line denotes the unsupervised results. Solid red: referent grounding; solid green: ground
truth; dashed yellow: context grounding. We only display top 3 context objects with the context grounding probability > 0.1. Please zoom
in.

detection with language priors. In ECCV, 2016. 2

[22] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical question-image
co-attention for visual question answering. In NIPS, 2016. 4
[23] R. Luo and G. Shakhnarovich. Comprehension-guided referring ex-

pressions. In CVPR, 2017. 2

[24] A. Makhzani, J. Shlens, N. Jaitly, and I. J. Goodfellow. Adversarial

autoencoders. In ICLR Workshop, 2016. 3

[25] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Mur-
phy. Generation and comprehension of unambiguous object descrip-
tions. In CVPR, 2016. 1, 2, 3, 5, 6, 7

[26] V. K. Nagaraja, V. I. Morariu, and L. S. Davis. Modeling context
between objects for referring expression understanding. In ECCV,
2016. 2, 3, 6, 7

[29] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hock-
enmaier, and S. Lazebnik. Flickr30k entities: Collecting region-
to-phrase correspondences for richer image-to-sentence models. In
ICCV, 2015. 2

[30] J. Redmon and A. Farhadi. Yolo9000: better, faster, stronger.

In

CVPR, 2017. 1

[31] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-
time object detection with region proposal networks. In NIPS, 2015.
6

[32] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and B. Schiele.
Grounding of textual phrases in images by reconstruction. In ECCV,
2016. 2, 5, 6, 7

[33] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural net-

[27] J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for

works. TSP, 1997. 4

word representation. In EMNLP, 2014. 5

[28] B. A. Plummer, A. Mallya, C. M. Cervantes, J. Hockenmaier, and
S. Lazebnik. Phrase localization and visual relationship detection
with comprehensive linguistic cues. In ICCV, 2017. 1, 2

[34] S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D. Manning.
Generating semantically precise scene graphs from textual descrip-
tions for improved image retrieval. In Workshop on Vision and Lan-
guage, 2015. 1

11

[35] K. Sohn, H. Lee, and X. Yan. Learning structured output representa-
tion using deep conditional generative models. In NIPS, 2015. 3
[36] Q. Sun, B. Schiele, and M. Fritz. A domain based approach to social

relation recognition. In CVPR, 2017. 1

[37] J. A. Thomas. Meaning in interaction: An introduction to pragmat-

ics. Routledge, 2014. 2

[38] J. Thomason, J. Sinapov, and R. Mooney. Guiding interaction be-
haviors for multi-modal grounded language learning. In Proceedings
of the First Workshop on Language Grounding for Robotics, pages
20–24, 2017. 1

[39] Y. Wei, J. Feng, X. Liang, C. Ming-Ming, Y. Zhao, and S. Yan. Ob-
ject region mining with adversarial erasing: A simple classiﬁcation
to semantic segmentation approach. In CVPR, 2017. 8

[40] R. J. Williams. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. In Machine Learning. 1992. 3
[41] F. Xiao, L. Sigal, and Y.-J. Lee. Weakly-supervised visual grounding

of phrases with linguistic structures. In CVPR, 2017. 8

[42] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,
R. Zemel, and Y. Bengio. Show, attend and tell: Neural image cap-
tion generation with visual attention. In ICML, 2015. 2

[43] T. Xue, J. Wu, K. Bouman, and B. Freeman. Visual dynamics: Prob-
abilistic future frame synthesis via cross convolutional networks. In
NIPS, 2016. 2

[44] X. Yan, J. Yang, K. Sohn, and H. Lee. Attribute2image: Conditional

image generation from visual attributes. In ECCV, 2016. 2

[45] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling

context in referring expressions. In ECCV, 2016. 2, 4, 5, 6

[46] L. Yu, H. Tan, M. Bansal, and T. L. Berg. A joint speaker-listener-

reinforcer model for referring expressions. In ICCV, 2017. 2, 6, 7

[47] H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua. Visual translation
embedding network for visual relation detection. In CVPR, 2017. 1,
2

[48] H. Zhang, Z. Kyaw, J. Yu, and S.-F. Chang. Ppr-fcn: Weakly super-
vised visual relation detection via parallel pairwise r-fcn. In ICCV,
2017. 2, 7, 8

[49] Z. Zhao, Q. Yang, D. Cai, X. He, and Y. Zhuang. Video question
answering via hierarchical spatio-temporal attention networks. In In-
ternational Joint Conference on Artiﬁcial Intelligence (IJCAI), vol-
ume 2, 2017. 1

[50] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object proposals

from edges. In ECCV, 2014. 4

12

Grounding Referring Expressions in Images by Variational Context

Hanwang Zhang1∗

Yulei Niu2†
1Nanyang Technological University, 2Renmin University of China, 3Columbia University,
hanwangzhang@ntu.edu.sg; niu@ruc.edu.cn; shih.fu.chang@columbia.edu

Shih-Fu Chang3

8
1
0
2
 
r
a

M
 
1
3
 
 
]

V
C
.
s
c
[
 
 
2
v
2
9
8
1
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

We focus on grounding (i.e., localizing or linking) refer-
ring expressions in images, e.g., “largest elephant stand-
ing behind baby elephant”. This is a general yet chal-
lenging vision-language task since it does not only require
the localization of objects, but also the multimodal com-
prehension of context — visual attributes (e.g., “largest”,
“baby”) and relationships (e.g., “behind”) that help to dis-
tinguish the referent from other objects, especially those
of the same category. Due to the exponential complex-
ity involved in modeling the context associated with mul-
tiple image regions, existing work oversimpliﬁes this task
to pairwise region modeling by multiple instance learning.
In this paper, we propose a variational Bayesian method,
called Variational Context, to solve the problem of com-
plex context modeling in referring expression grounding.
Our model exploits the reciprocal relation between the ref-
erent and context, i.e., either of them inﬂuences estima-
tion of the posterior distribution of the other, and thereby
the search space of context can be greatly reduced. We
also extend the model to unsupervised setting where no
annotation for the referent is available. Extensive exper-
iments on various benchmarks show consistent improve-
ment over state-of-the-art methods in both supervised and
unsupervised settings. The code is available at https:
//github.com/yuleiniu/vc/.

1. Introduction

Grounding natural language in visual data is a hallmark
of AI, since it establishes a communication channel be-
tween humans, machines, and the physical world, under-
pinning a variety of multimodal AI tasks such as robotic
navigation [38], visual Q&A [1, 16, 49], and visual chat-
bot [6]. Thanks to the rapid development in deep learning-
based CV and NLP, we have witnessed promising results
not only in grounding nouns (e.g., object detection [30]),

∗Hanwang was a research scientist at Columbia University.
†Yulei was a visiting student at Columbia University.

Figure 1. The proposed Variational Context model. Given an input
referring expression and an image with region proposals, we local-
ize the referent as output. We develop a grounding score function,
with the variational lower-bound composed by three cue-speciﬁc
multimodal modules, indicated by the description in the dashed
color boxes.

but also short phrases (e.g., noun phrases [28] and rela-
tions [47, 36]). However, the more general task: grounding
referring expressions [25], is still far from resolved due to
the challenges in understanding of both language and scene
compositions [10]. As illustrated in Figure 1, given an in-
put referring expression “largest elephant standing behind
baby elephant” and an image with region proposals, a model
that can only localize “elephant” is not satisfactory as there
are multiple elephants. Therefore, the key for referring ex-
pression grounding is to comprehend and model the con-
text. Here, we refer to context as the visual objects (e.g.,
“elephant”), attributes (e.g., “largest” and “baby”), and re-
lationships (e.g., “behind”) mentioned in the expression that
help to distinguish the referent from other objects.

One straightforward way of modeling the relations be-
tween the referent and context is to: 1) use external syn-
tactic parsers to parse the expression into entities, mod-
iﬁers, and relations [34], and then 2) apply visual rela-
tion detectors to localize them [47]. However, this two-
stage approach is not practical due to the limited general-
ization ability of the detectors applied in the highly unre-
stricted language and scene compositions. To this end, re-

1

cent approaches use multimodal embedding networks that
jointly comprehend language and model the visual rela-
tions [26, 11]. Due to the prohibitively high cost of an-
notating both referent and context of referring expressions
in images, multiple instance learning (MIL) [7] is usually
adopted in them to handle the weak supervision of the unan-
notated context objects, by maximizing the joint likelihood
of every region pair. However, for a referent, the MIL
framework essentially oversimpliﬁes the number of context
conﬁgurations of N regions from O(2N ) to O(N ). For ex-
ample, to localize the “elephant” in Figure 1, we may need
to consider the other three elephants all together as a multi-
nomial subset for modeling the context such as “largest”,
“behind” and “baby elephant”.

In this paper, we propose a novel model called Varia-
tional Context for grounding referring expressions in im-
ages. Compared to the previous MIL-based approaches [26,
11], our model approximates the combinatorial context
conﬁgurations with weak supervision using a variational
Bayesian framework [15]. Intuitively, it exploits the reci-
procity between referent and context, given either of which
can help to localize the other. As shown in Figure 1, for
each region x, we ﬁrst estimate a coarse context z, which
will help to reﬁne the true localizations of the referent. This
reciprocity is formulated into the variational lower-bound
of the grounding likelihood p(x|L), where L is the text
expression and the context is considered as a hidden vari-
able z (cf. Section 3). Speciﬁcally, the model consists of
three multimodal modules: context posterior q(z|x, L), ref-
erent posterior p(x|z, L), and context prior pz(z|L), each
of which performs a grounding task (cf. Section 4.3) that
aligns image regions with a cue-speciﬁc language feature;
each cue dynamically encodes different subsets of words in
the expression L that help the corresponding localization
(cf. Section 4.2).

Thanks to the reciprocity between referent and context,
our model can not only be used in the conventional super-
vised setting, where there is annotation for referent , but
also in the challenging unsupervised setting, where there
is no instance-level annotation (e.g., bounding boxes) of
both referent and context. We perform extensive experi-
ments on four benchmark referring expression datasets: Re-
fCLEF [14], RefCOCO [45], RefCOCO+ [45], and Ref-
COCOg [25]. Our model consistently outperforms previous
methods in both supervised and unsupervised settings. We
also qualitatively show that our model can ground the con-
text in the expression to the corresponding image regions
(cf. Section 5).

2. Related Work

Grounding Referring Expressions. Grounding refer-
ring expression is also known as referring expression com-
prehension, whose inverse task is called referring expres-

sion generation [25]. Different from grounding phrases [29,
28] and descriptive sentences [12, 32], the key for ground-
ing referring expression is to use the context (or pragmatics
in linguistics [37]) to distinguish the referent from other ob-
jects, usually of the same category [10]. However, most pre-
vious works resort to use holistic context such as the entire
image [25, 12, 32] or visual feature difference between re-
gions [45, 46]. Our model is similar to the works on explic-
itly modeling the referent and context region pairs [11, 26],
however, due to the lack of context annotation, they re-
duce the grounding task into a multiple instance learning
framework [7]. As we will discuss later, this framework
is not a proper approximation to the original task. There
are also studies on visual relation detection that detect ob-
jects and their relationships [21, 5, 47, 17, 48]. However,
they are limited to a ﬁxed-vocabulary set of relation triplets
and hence are difﬁcult to be applied in natural language
grounding. Our cue-speciﬁc language feature is similar to
the language modular network [11] that learns to decom-
pose a sentence into referent/context-related words, which
are different from other approaches that treat the expression
as a whole [25, 23, 46, 19].

Variational Bayesian Model vs. Multiple Instance
Learning. Our proposed variational context model is in a
similar vein of the deep neural network based variational
autoencoder (VAE) [15], which uses neural networks to
approximate the posterior distribution of the hidden value
q(z|x), i.e., encoder, and the conditional distribution of the
observation p(x|z), i.e., decoder. VAE shows efﬁcient and
effective end-to-end optimization for the intractable log-
sum likelihood log (cid:80)
z p(x, z) that is widely used in genera-
tive processes such as image synthesis [44] and video frame
prediction [43]. Considering the unannotated context as the
hidden variable z, the referring expression grounding task
can also be formulated into the above log-sum marginaliza-
tion (cf. Eq. (2)). The MIL framework [7] is essentially a
sum-log approximation of the log-sum, i.e., (cid:80)
z log p(x, z).
To see this, the max-pooling function log maxz p(x, z) used
in [11] can be viewed as the sum-log (cid:80)
z log p(x|z)p(z),
where p(z) = 1 if z is the correct context and 0 otherwise,
indicating there is only one positive instance; maximizing
the noisy-or function log(1 − (cid:81)
z(1 − p(x, z))) used in [26]
is equivalent to maximize (cid:80)
z log p(x, z), assuming there is
at least one positive instance. However, due to the numer-
ical property of the log function, this sum-log approxima-
tion will unnecessarily force every (x, z) pair to explain the
data [8]. Instead, we use the variational Bayesian upper-
bound to obtain a better sum-log approximation. Note
that visual attention models [2, 42] simplify the variational
lower bound by assuming p(z) = q(z|x); however, we
explicitly use the KL divergence KL(q(z|x)||p(z)) in the
lower bound to regularize the approximate posterior q(z|x)
not being too far from the prior p(z).

2

3. Variational Context

In this section, we derive the variational Bayesian for-
mulation of the proposed variational context model and the
objective function for training and test.

3.1. Problem Formulation

The task of grounding a referring expression L in an im-
age I, represented by a set of regions x ∈ X , can be viewed
as a region retrieval task with the natural language query
L. Formally, we maximize the log-likelihood of the condi-
tional distribution to localize the referent region x∗ ∈ X :

x∗ = arg max

log p(x|L),

(1)

x∈X

where we omit the image I in p(x|I, L).

As there is usually no annotation for the context, we
consider it as a hidden variable z. Therefore, Eq. (1)
can be rewritten as the following maximization of the log-
likelihood of the conditional marginal distribution:

x∗ = arg max

log

p(x, z|L).

(2)

x∈X

(cid:88)

z

Note that z is NOT necessary to be one region as assumed
in recent MIL approaches [11, 26], i.e., z ∈ X . For ex-
ample, the contextual objects “surrounding elephants” in
“a bigger elephant than the surrounding elephants” should
be composed by a multinomial subset of X , resulting in an
extremely large sample space that requires O(2|X |) search
complexity. Therefore, the marginalization in Eq (2) is in-
tractable in general.

To this end, we use the variational lower-bound [15] to

approximate the marginal distribution in Eq. (2) as:

log p(x|L) = log

p(x, z|L) ≥ Q(x, L) =

(cid:88)

z

Ez∼qφ(z|x,L) log pθ(x|z, L)
(cid:125)
(cid:123)(cid:122)
(cid:124)
Localization

− KL (qφ(z|x, L)||pω(z|L))
(cid:123)(cid:122)
(cid:125)
Regularization

(cid:124)

,

(3)

where KL(·) is the Kullback-Leibler divergence, φ, θ, and
ω are independent parameter sets for the respective distri-
butions. As shown in Figure 1, the lower bound Q(x, L)
offers a new perspective for exploiting the reciprocal nature
of referent and context in referring expression grounding:
Localization. This term calculates the localization score
for x given an estimated context z, using the referent-cue
of L parameterized by θ.
In particular, we design a new
posterior qφ(z|x, L) that approximates the true context prior
p(z|x, L), which models the context z using the context-cue
of L parameterized by φ. In the view of variational auto-
encoder [15, 35], this term works in an encoding-decoding
fashion: qφ is the encoder from x to z, and pθ is the decoder

from z to x.
Regularization. As KL is non-negative, maximizing
Q(x, L) would encourage that the posterior qφ is similar
to the prior pω, i.e., the estimated context z sampled from
qφ(z|x, L) should not be too far from the referring expres-
sion, which is modeled by pω(z|L) with the generic-cue of
L parameterized by ω. This term is necessary as the esti-
mated z could be overﬁtted to region features that are incon-
sistent with the visual context described in the expression.

3.2. Training and Test

Deterministic Context.

The lower-bound Q(x, L)
transforms the intractable log-sum in Eq. (2) into the efﬁ-
cient sum-log in Eq. (3), which can be optimized by us-
ing Monte Carlo unbiased gradient estimator such as RE-
INFORCE [40]. However, due to that φ is dependent on
the sampling of z over O(2|X |) conﬁgurations, its gradient
variance is large. To this end, we implement qφ(z|x, L) as
a differentiable but biased encoder:

z = f (x, L) =

x(cid:48) · qφ(x(cid:48)|x, L),

(4)

(cid:88)

x(cid:48)∈X

where we slightly abuse qφ as a score function such that
(cid:80)
x(cid:48) qφ(x(cid:48)|x, L) = 1. Note that this deterministic context
can be viewed as applying the “re-parameterization” trick as
in Variational Auto-Encoder [15]: rewriting z ∼ qφ(z|x, L)
to z = f (x, L; (cid:15)), (cid:15) ∼ p((cid:15)), where the stochasticity of the
auxiliary random variable (cid:15) comes from training samples
x ∈ X ((cid:15)). A clear example is Adversarial Autoencoder [24]
which shows that such stochasticity achieves similar test-
likelihood compared to other distributions such as Gaus-
sian.

Objective Function. Applying Eq. (4) to Eq. (3), we can
rewrite Q(x, L) into a function of only one sample estima-
tion, which is a common practice in SGD:

Q(x,L) = log pθ(x|z,L)−log qφ(z|x,L)+log pω(z|L).

(5)

is known,

In supervised setting where the ground truth of the ref-
erent
to distinguish the referent from other
objects, we need to train a model that outputs a high
p(x|L) (i.e., Q(x, L)), while maintaining a low p(x(cid:48)|L)
(i.e., Q(x(cid:48), L)), whenever x(cid:48)
(cid:54)= x. Therefore, we use
the so-called Maximum Mutual Information loss as in [25]
− log{Q(x, L)/ (cid:80)
x(cid:48) Q(x(cid:48), L)}, where we do not need to
explicitly model the distributions with normalizations; we
use the following score function:

Q(x, L) ∝ S(x, L) = sθ(x, L)−sφ(x, L)+sω(x, L), (6)

where z is omitted as it is a function of x in Eq. (4). sθ,
sφ, and sω are the score functions (e.g., pθ ∝ sθ) for pθ,
qφ, and pω, respectively. These functions will be detailed in

3

Figure 2. The architecture of the proposed Variational Context model. It consists of a region feature extraction module (Section 4.1, and
a language feature extraction module (Section 4.2), and three grounding modules (Section 4.3). It can be trained in an end-to-end fashion
with the input of a set of image regions and a referring expression, using the supervised loss ( Eq. (7)) or the unsupervised loss (Eq. (8)).
fc: fully-connected layer. concat: vector concatenation. L2Norm: L2 normalization layer. (cid:12): element-wise vector multiplication. ⊕: add.

Section 4.3. In this way, maximizing Eq. (5) is equivalent
to minimizing the following softmax loss:

Ls = − log softmax S(xgt, L),

(7)

where the softmax is over x ∈ X and xgt is the ground truth
referent region.

Note that the reciprocity between referent and context
can be extended to unsupervised learning, where neither of
the referent and context has annotation. In this setting, we
adopt the image-level max-pooled MIL loss functions for
unsupervised referring expression grounding:

Lu = − max
x∈X

log softmax S(x, L),

(8)

where the softmax is over x ∈ X . Note that the max-pooled
MIL function is reasonable since there is only one ground
truth referent given an expression and image training pair.

At test stage, in both supervised and unsupervised set-
tings, we predict the referent region x∗ by selecting the re-
gion x ∈ X with the highest score:

x∗ = arg max

S(x, L),

x∈X

(9)

4. Model Architecture

The overall architecture of the proposed variational con-
text model is illustrated in Figure 2. Thanks to the deter-
ministic context in Eq. (4), the ﬁve modules in our model
can be integrated into an end-to-end differentiable fashion.
Next, we will detail the implementation of each module.

4.1. RoI Features

Given an image with a set of Region of Interests (RoIs)
X , obtained by any off-the-shelf proposal generator [50] or
object detectors [20], this module extracts the feature vec-
tor xi for every RoI. In particular, xi is the concatenation
of visual feature vi and spatial feature pi. For vi, we can
use the output of a pre-trained convolutional network (cf.
Section 5). If the object category of each RoI is available,
we can further utilize the comparison between the referent

(cid:80)

and other objects to capture the visual difference such as
“the largest/baby elephant”. Speciﬁcally, we append the vi-
vi−vj
sual difference feature [45] δvi = 1
||vi−vj || to the
n
original vi visual feature, where n is the number of objects
chosen for comparison (e.g., the number of RoI in the same
object category). For spatial feature, we use the 5-d spatial
W , ybr
attributes pi = [ xtl
W ·H ], where x and y are
the coordinates the top left (tl) and bottom right (br) RoI of
the size w × h, and the image is of the size W × H.

H , w·h

H , xbr

W , ytl

j(cid:54)=i

4.2. Cue-Speciﬁc Language Features

The cue-speciﬁc language feature representation for a re-
ferring expression is inspired by the attention weighted sum
of word vectors [11, 22, 3], where the weights are param-
eterized by context-cue φ, referent-cue θ, and generic-cue
ω. The context-cue language feature yc = [yc1, yc2] is
a concatenation of yc1 for language-vision association be-
tween single RoI and the expression, and yc2 for the as-
sociation between pairwise RoIs; the referent-cue language
feature yr can be represented in a similar way to yc; the
generic-cue language feature yg is only for single RoI asso-
ciation as it is an independent prior. The weights of each cue
are calculated from the hidden state vectors of a 2-layer bi-
directional LSTM (BLSTM) [33], scanning through the ex-
pression. The hidden states encode forward and backward
compositional semantic meanings of the sentences, beneﬁ-
cial for selecting words that are useful for single and pair-
wise associations. Speciﬁcally, suppose hj as the 4,000-d
concatenation of forward and backward hidden vectors of
the j-th word, without loss of generality, the word attention
weight αj and the language feature y for single/pairwise
association of any cue can be calculated as:

mj = fc(hj), αj = softmaxj(mj), y =

αjwj, (10)

(cid:88)

j

where wj is a 300-d vector. Note that the BLSTM module
can be jointly trained with the entire model.

Figure 3 shows that the cue-speciﬁc language features
dynamically weight words in different expressions. We can
have two interesting observations. First, c1 is almost uni-

4

Figure 3. Two qualitative examples of the cue-speciﬁc language
feature word weights. Darker color indicates higher weights.
c/r+1/2: context/referent-cue + single/pairwise.

form while c2 is highly skewed; although r2 is more skewed
than c1, it is still less skewed than r1. This is reasonable
since: 1) without ground-truth, individual score (c1) does
not help much for context estimation from scratch; context
is more easily found by the pairwise score (c2) induced by
relationships or other objects (e.g., “left” or “frisbee”); 2) in
referent grounding with ground truth, individual score (r1)
is sufﬁcient (e.g., “dog lying” and “black white dog”) and
pairwise score (r2) is helpful; 3) g is adaptive to the num-
ber of object categories in the expression, i.e., if the context
object is of the same category as the referent, g weighs de-
scriptive or relationship words higher (e.g., “lying, stand-
ing, left”), and nouns higher (e.g., “frisbee”), otherwise;
moreover, it demonstrates that the deterministic guess of z
in Eq. (4) is meaningful.

4.3. Score Functions

For any image and expression pair, given the RoI feature
xi, and the cue-speciﬁc language feature yc, yr, and yg, we
implement the ﬁnal grounding score in Eq. (6) as:

zi =

(cid:88)
j

softmaxj (sφ(xi, xj, yc)) xj,

sθ(x, L) ← sθ(xi, zi, yr),
sφ(x, L) ← sφ(xi, zi, yc),
sω(x, L) ← sω(zi, yg),

(11)

where the right-hand side functions are deﬁned as below.

Context Estimation Score: sφ(xi, xj, yc). It is a score
function for modeling the context posterior qφ(z|x, L), i.e.,
given an RoI xi as the candidate referent, we calculate the
likelihood of any RoI xj to be the context. We can also
use this function to estimate the ﬁnal context posterior score
sφ(xi, zi, yc). Speciﬁcally, the context estimation score is a
sum of the single and pairwise vision-language association
scores: xj and yc1, [xi, xj] and yc2. Each associate score
is an fc output from the input of a normalized feature:

j = yc1 (cid:12) fc(xj), m2
m1
(cid:101)m1
j ), (cid:101)m2
j = L2Norm(m1
sφ(xi, xj, yc) = fc( (cid:101)m1

j = yc2 (cid:12) fc([xi, xj]),
j = L2Norm(m2
j ) + fc( (cid:101)m2
j ),

j ),

(12)

obtain the estimated context z as zi = (cid:80)
βj = softmaxj(sφ(xi, xj, yc)).

j βjxj, where

Referent Grounding Score: sθ(xi, zi, yr). After ob-
taining the context feature zi, we can use this score function
to calculate how likely a candidate RoI xi is the referent
given the context zi. This function is similar to Eq. (12).

Score:

Context Regularization

sω(zi, yg) −
sφ(xi, zi, yc). As discussed in Eq. (6),
this function
scores how likely the estimated context feature zi
is
consistent with the content mentioned in the expression. In
particular, sω(zi, yg) is only dependent on single RoI:
i (cid:12) fc(zi), (cid:101)mi=L2Norm(mi), sω(zi, yg
mi=yg

i )=fc(mi).
(13)

5. Experiment

5.1. Datasets

We used four popular benchmarks for the referring ex-

pression grounding task.

RefCOCO [45]. It has 142,210 referring expressions for
50,000 referents (e.g., object instances) in 19,994 images
from MSCOCO [18]. The expressions are collected in an
interactive way [14]. The dataset is split into train, valida-
tion, Test A, and Test B, which has 120,624, 10,834, 5,657
and 5,095 expression-referent pairs, respectively. An image
contains multiple people in Test A and multiple objects in
Test B.

RefCOCO+ [45]. It has 141,564 expressions for 49,856
referents in 19,992 images from MSCOCO. The difference
from RefCOCO is that it only allows appearances but no
locations to describe the referents. The split is 120,191,
10,758, 5,726 and 4,889 expression-referent pairs for train,
validation, Test A, and Test B respectively.

RefCOCOg [25]. It has 95,010 referring expressions for
49,822 objects in 25,799 images from MSCOCO. Different
from RefCOCO and RefCOCO+, this dataset not collected
in an interactive way and contains longer sentences contain-
ing both appearance and location expressions. The split is
85,474 and 9,536 expression-referent pairs for training and
validation. Note that there is no open test split for Ref-
COCOg, so we used the hyper-parameters cross-validated
on RefCOCO and RefCOCO+.

RefCLEF [14]. It contains 20,000 images with anno-
tated image regions. It has some ambiguous (e.g. anywhere)
phrases and mistakenly annotated image regions that are not
described in the expressions. For fair comparison, we used
the split released by [12, 32], i.e., 58,838, 6,333 and 65,193
expression-referent pairs for training, validation and test, re-
spectively.

5.2. Settings and Metrics

where the element-wise multiplication (cid:12) is an effective way
for multimodal features [2]. According to Eq. (4), we can

We used an English vocabulary of 72,704 words con-
tained in the GloVe pre-trained word vectors [27], which

5

Table 1. Supervised grounding performances (Acc%) of comparing methods on RefCOCO, RefCOCO+, and RefCOCOg. Note that [46]
reports slightly higher accuracies using ensemble models of Listener and Speaker. For fair comparison, we only report their single models.

Dataset

Split MMI [25] NegBag [26] Attr [19] CMN [11]

Speaker [46]

Listener [46] VC w/o reg VC w/o α

VC

State-of-The-Arts

Our Baselines

RefCOCO

RefCOCO+

RefCOCOg

Val

RefCOCO(det)

RefCOCO+(det)

RefCOCOg(det)

Val

Test A

Test B

Test A

Test B

Test A

Test B

Test A

Test B

71.72

71.09

58.42

51.23

62.14

64.90

54.51

54.03

42.81

45.85

75.6

78.0

—

—

68.4

58.6

56.4

—

—

39.5

78.85

78.07

61.47

57.22

69.83

72.08

57.29

57.97

46.20

52.35

75.94

79.57

59.29

59.34

69.30

71.03

65.77

54.32

47.76

57.47

78.95

80.22

64.60

59.62

72.63

72.95

63.43

60.43

48.74

59.51

78.45

80.10

63.34

58.91

72.25

72.95

62.98

59.61

48.44

58.32

75.59

79.69

60.76

60.14

71.05

70.78

65.10

56.82

51.30

60.95

74.03

78.27

57.61

54.37

65.13

70.73

64.63

53.33

46.88

55.72

78.98

82.39

62.56

62.90

73.98

73.33

67.44

58.40

53.18

62.30

Figure 4. Qualitative results on RefCOCOg (det) showing comparisons between correct (green tick) and wrong referent grounds (red cross)
by VC and CMN. The denotations of the bounding box colors are as follows. Solid red: referent ground; solid green: ground truth; dashed
yellow: context ground. We only display top 3 context objects with the context ground probability > 0.1. We can observe that VC has
more reasonable context localizations than CMN, even in cases when the referent ground of VC fails.

was also used for the initialization of our word vectors. We
used a “unk” symbol for the input word of the BLSTM if the
word is out of the vocabulary; we set the sentence length to
20 and used “pad” symbol to pad expression sentence < 20.
For RoI visual features on RefCOCO, RefCOCO+, and Re-
fCOCOg which have MSCOCO annotated regions with ob-
ject categories, we used the concatenation of the 4,096-d
fc7 output of a VGG-16 based Faster-RCNN network [31]
trained on MSCOCO and its corresponding 4,096-d visd-
iff feature [45]; although RefCLEF regions also have object

categories, for fair comparison with [32], we did not use the
visdiff feature.

The model training is single-image based, with all re-
ferring expressions annotated. We applied SGD of 0.95-
momentum with initial learning rate of 0.01, multiplied by
0.1 after every 120,000 iterations, up to 160,000 iterations.
Parameters in BILSTM and fc-layers were initialized by
Xavier [9] with 0.0005 weight decay. Other settings were
default in TensorFlow. Note that our model is trained with-
out bells and whistles, therefore, other optimization tricks

6

such as batch normalization [13] and GRU [4] are expected
to further improve the results reported here. Besides the
ground truth annotations, grounding to automatically de-
tected objects is a more practical setting. Therefore, we also
evaluated with the SSD-detected bounding boxes [20] on
the four datasets provided by [46]. A grounding is consid-
ered as correct if the intersection-over-union (IoU) of the
top-1 scored region and the ground-truth object is larger
than 0.5. The grounding accuracy (a.k.a, P@1) is the frac-
tion of correctly grounded test expressions.

5.3. Evaluations of Supervised Grounding

We compared our variational context model (VC) with
state-of-the-art referring expression methods published in
recent years, which can be categorized into: 1) generation-
comprehension based such as MMI
[19],
Speaker [46], Listener [46], and SCRC [12]; 2) localiza-
tion based such as GroundR [32], NegBag [26], CMN [11].
Note that NegBag and CMN are MIL-based models. In par-
ticular, we used the author-released code to obtain the re-
sults of CMN on RefCLEF, RefCOCO, and RefCOCO+.

[25], Attr

From the results on RefCOCO, RefCOCO+, and Re-
fCOCOg in Table 1 and that on RefCLEF in Table 2,
we can see that VC achieves the state-of-the-art perfor-
mance. We believe that the improvement is attributed to
the variational Bayesian modeling of context. First, on all
datasets, except for the most recent reinforcement learn-
ing based [46], VC outperforms all the other sentence
generation-comprehension methods that do not model con-
text. Second, compared to VC without the regularization
term in Eq. (3) (VC w/o reg), VC can boost the performance
by around 2% on all datasets. This demonstrates the ef-
fectiveness of the KL divergence for the prevention of the
overﬁtted context estimation.

In particular, we further demonstrate the superiority of
VC over the most recent MIL-based method CMN. As il-
lustrated in Figure 4, VC has better context comprehension
in both of the language and image regions than CMN. For
example, in the top two rows where VC is correct and CMN
is wrong, for the grounding in the second column, CMN
unnecessarily considers the “girl” as context but the expres-
sion only describes using “elephant”; in the last column,
CMN misses the key context “frisbee”. Even in the fail-
ure cases where VC is wrong and CMN is correct, VC still
localizes reasonable context. For example, in the fourth col-
umn, although CMN grounds the correct TV, but it is based
on incorrect context of other TVs; while VC can predict
the correct context “children”.
In addition, we observed
that most of the cases that CMN is better than VC involves
multiple humans. This demonstrates that VC is better at
grounding objects of different categories.

VC is also effective in images with more objects. Fig-
ure 5 shows the performances of VC and CMN with various

Table 2. Performances (Acc%) of supervised and unsupervised
methods on RefCLEF.

Sup.

Sup. (det) Unsup. (det)

SCRC [12]

72.74

GroundR [32] —

CMN [11]

VC

VC w/o α

81.52

82.43

79.60

17.93

26.93

28.33

31.13

27.40

10.70

—

—

14.11

14.50

Table 3. Unsupervised grounding performances (Acc%) of com-
paring methods on RefCOCO, RefCOCO+, and RefCOCOg.
VC w/o α

VC w/o reg

Dataset

Split

VC

RefCOCO

RefCOCO+

RefCOCOg

Val

RefCOCO(det)

RefCOCO+(det)

RefCOCOg(det)

Val

Test A

Test B

Test A

Test B

Test A

Test B

Test A

Test B

13.59

21.65

18.79

24.14

25.14

17.14

22.30

19.74

24.05

28.14

17.34

20.98

23.24

24.91

33.79

20.91

21.77

25.79

25.54

33.66

33.29

30.13

34.60

31.58

30.26

32.68

27.22

34.68

28.10

29.65

number of bounding boxes. We can observe that VC con-
siderably outperforms CMN over all bounding boxes num-
bers. Recall that context is the key to distinguish objects of
the same category. In particular, on the Test A sets of Ref-
COCO and RefCOCO+ where the grounding is only about
people, i.e., the same object category, the gap between VC
and CMN is becoming larger as the box number increases.
This demonstrates that MIL is ineffective in modeling con-
text, especially when the number of image regions is large.

5.4. Evaluations of Unsupervised Grounding

We follow the unsupervised setting in GroundR [32]. To
our best knowledge, it is the only work on unsupervised re-
ferring expression grounding. Note that it is also known as
“weakly supervised” detection [48] as there is still image-
level ground truth (i.e., the referring expression). Table 2 re-
ports the unsupervised results on the RefCLEF. We can see
that VC outperforms the state-of-the-art GroundR, which is
a generation-comprehension based method. This demon-
strates that using context also helps unsupervised ground-
ing. As there is no published unsupervised results on Re-
fCOCO, RefCOCO+, and RefCOCOg, we only compared
our baselines on them in Table 3. We can have the follow-
ing three key observations which highlight the challenges
of unsupervised grounding:

Context Prior. VC w/o reg is the baseline without the
KL divergence as a context regularization in Eq. (3). We can
see that in most of the cases, VC considerably outperforms
VC w/o reg by over 2%, even over 5% on RefCOCO+ (det)

7

Figure 5. Performances of VC and CMN with different number of object bounding boxes on RefCOCO Test A &B, RefCOCO+ Test A &
B, and RefCOCOg Val. Compared to CMN, we can see that VC is more effective in context modeling when the number of objects is large.

Figure 6. Common failure cases in unsupervised grounding with
detected bounding boxes. From left to right: RefCOCO, Ref-
COCO+, and RefCOCOg. The failure is mainly to the challenging
unsupervised relation modeling between referent and context.

and RefCOCOg (det). Note that this improvement is signif-
icantly higher than that in supervised setting (e.g., < 3% as
reported in Table 1). The reason is that the context estima-
tion in Eq. (4) would be easier to be stuck in image regions
that are irrelevant to the expression in unsupervised setting,
therefore, context prior is necessary.

Language Feature. Except on RefCOCOg, we consis-
tently observed the ineffectiveness of the cue-speciﬁc lan-
guage feature in unsupervised setting, i.e., VC w/o α out-
performs VC in Table 2 and 3. Here α represents the cue-
speciﬁc word attention. This is contrary to the observation
in the supervised setting as listed in Table 1, where VC w/o
α is consistently lower than VC. Note that without the cue-
speciﬁc word attention α in Eq. (10), the language feature is
merely the average value of the word embedding vectors in
the expression. In this way, VC w/o α does not encode any
structural language composition as illustrated in Figure 3,
thus, it is better for short expressions. However, when the
expression is long in RefCOCOg, discarding the language
structure still degrades the performance on RefCOCOg.

Unsupervised Relation Discovery.

Although we
demonstrated that VC improves the unsupervised ground-
ing by modeling context, we believe that there is still a large
space for improving the quality of modeling the context.
As the failure examples shown in Figure (6), 1) many con-
text estimations are still out of the scope of the expression,
e.g., we may localize the “cup” and “table” as context even
though the expression is “woman with green t-shirt”; 2) we

8

Figure 7. Word cloud visualizations of cue-speciﬁc word attention
α in Eq. 10 of context-cue (c2), referent-cue (r1), and generic-cue
(g) using supervised (top row) and unsupervised training (bottom
row) on RefCOCOg. Without supervision, it is difﬁcult to discover
meaningful language compositions.

may mistake due to the wrong comprehension of the rela-
tions, e.g., “right” as “left”, even if the objects belong to
the same category, e.g., “elephant”. For further investiga-
tion, Figure 7 visualizes the cue-speciﬁc word attentions in
supervised and unsupervised settings. The almost identi-
cal word attentions in unsupervised setting reﬂect the fact
that the relation modeling between referent and context is
not as successful as in supervised setting. This inspires us
to exploit stronger prior knowledge such as language struc-
ture [41] and spatial conﬁgurations [48, 39].

6. Conclusions

We focused on the task of grounding referring expres-
sions in images and discussed that the key problem is how
to model the complex context, which is not effectively re-
solved by the multiple instance learning framework used
in prior works. Towards this challenge, we introduced
the Variational Context model, where the variational lower-
bound can be interpreted by the reciprocity between the ref-
erent and context: given any of which can help to localize
the other, and hence is expected to signiﬁcantly reduce the
context complexity in a principled way. We implemented
the model using cue-speciﬁc language-vision embedding
network that can be efﬁciently trained end-to-end. We val-
idated the effectiveness of this reciprocity by promising
supervised and unsupervised experiments on four bench-
marks. Moving forward, we are going to 1) incorporate ex-
pression language generation in the variational framework,
2) use more structural features of language rather than word
attentions, and 3) further investigate the potential of our
model in the unsupervised referring expression grounding.

the visual grounding task [11]. Therefore, we adopt the
parser jointly trained on the referring expression ground-
ing task [11]. As illustrated in Figure 8, this parser assigns
word-level attention weights of subject, relation, and object.
In particular, we consider the language features of c1, r2 as
the average word embeddings, c2 as the relation weights,
r1 as the subject weights, g as the object weights. Table 4
shows the performances on unsupervised grounding. We
can see that there is no signiﬁcant improvement of VC w/
parser over VC w/o α.

7.5. More Qualitative Results

Figure 10 shows more qualitative results on supervised
and unsupervised grounding results on RefCOCO, Ref-
COCO+, and RefCOCOg.

References

[1] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zit-
nick, and D. Parikh. Vqa: Visual question answering. In ICCV, 2015.
1

[2] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition

with visual attention. In ICLR, 2015. 2, 5

[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by

jointly learning to align and translate. 2015. 4

[4] K. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio. On
the properties of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259, 2014. 7

[5] B. Dai, Y. Zhang, and D. Lin. Detecting visual relationships with

deep relational networks. In CVPR, 2017. 2

[6] A. Das, S. Kottur, J. M. Moura, S. Lee, and D. Batra. Learning
cooperative visual dialog agents with deep reinforcement learning.
In ICCV, 2017. 1

[7] T. G. Dietterich, R. H. Lathrop, and T. Lozano-P´erez. Solving the
multiple instance problem with axis-parallel rectangles. Artiﬁcial
intelligence, 1997. 2

[8] C. W. Fox and S. J. Roberts. A tutorial on variational bayesian infer-

[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of training

deep feedforward neural networks. In ICAIS, 2010. 6

[10] D. Golland, P. Liang, and D. Klein. A game-theoretic approach to

generating spatial descriptions. In EMNLP, 2010. 1, 2

[11] R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko. Model-
ing relationships in referential expressions with compositional mod-
ular networks. In CVPR, 2017. 2, 3, 4, 6, 7

[12] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell.

Natural language object retrieval. In CVPR, 2016. 2, 5, 7

[13] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In ICML, 2015.
7

[14] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg. Refer-
itgame: Referring to objects in photographs of natural scenes.
In
EMNLP, 2014. 2, 5

[15] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In

ICLR, 2014. 2, 3

[16] Y. Li, N. Duan, B. Zhou, X. Chu, W. Ouyang, X. Wang, and M. Zhou.
Visual question generation as dual task of visual question answering.
In CVPR, 2018. 1

[17] Y. Li, W. Ouyang, and X. Wang. Vip-cnn: Visual phrase guided

convolutional neural network. In CVPR, 2017. 2

Figure 8. Some parsing examples from [11].

7. Supplementary Material

7.1. Derivation of Eq. (3)

Recall that we are going to transform the log-sum ob-
jective function in Eq. (2) to sum-log for tractable training.
Without loss of generality, we omit the conditional L in the
derivation. By using the concavity of the log function:

log((1 − α)x + αy) ≥ (1 − α) log(x) + α log(y),

(14)

to the following rewriting of Eq. (2):

(cid:88)

log

p(x, z) = log

(cid:32)

(cid:88)

z

(cid:33)

q(z|x)p(x, z)
q(z|x)

q(z|x) log

p(x, z)
q(z|x)

q(z|x) log p(x|z)p(z) −

q(z|x) log q(z|x)

q(z|x) log p(x|z) +

q(z|x) log p(z)

(cid:88)

=

q(z|x) log p(x|z) −

q(z|x)

(cid:88)

q(z|x)
p(z)

= Ez∼q(z|x) log p(x|z) − KL (q(z|x)||p(z)) = Q(x)

(15)

7.2. More Examples on Language Features

Figure 9 shows more cue-speciﬁc language features on

RefCOCOg.

7.3. More Results on Unsupervised Grounding

7.4. External Parsers

As discussed in Section 5.4 that the language feature in
unsupervised VC is not as good as that in the supervised
setting. An alternative is to use external NLP parsers to ob-
tain the compositions. However, conventional parsers (e.g.,
Standford Dependency) are observed to be suboptimal to

(cid:88)

z

(cid:88)

(cid:88)

−

z

z

z

z
(cid:88)

z
(cid:88)

z
(cid:88)

≥

=

=

z

z

9

q(z|x) log q(z|x)

ence. Artiﬁcial intelligence review, 2012. 2

Table 4. Unsupervised grounding performances (%) of VC w/ or w/o parsers on the four datasets.
RefCLEF RefCOCO TestA RefCOCO TestB

RefCOCO+TestA

RefCOCO+TestB

RefCOCOg

VC w/ parser

GT

VC

VC w/o α

VC w/ parser

VC

VC w/o α

22.57

21.06

20.72

14.90

14.11

14.50

23.00

17.34

33.29

24.07

20.91

32.68

27.50

20.98

30.13

25.12

21.77

27.22

24.69

23.24

34.60

26.18

25.79

34.68

28.96

24.91

31.58

26.92

25.54

28.10

30.73

33.79

30.26

30.64

33.66

29.65

DET

RefCLEF RefCOCO TestA RefCOCO TestB RefCOCO+ TestA RefCOCO+ TestB RefCOCOg

Figure 9. Qualitative results of the cue-speciﬁc language features on RefCOCOg.

[18] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick. Microsoft coco: Common objects in
context. In ECCV, 2014. 5

[19] J. Liu, L. Wang, and M.-H. Yang. Referring expression generation

and comprehension via attributes. In ICCV, 2017. 2, 6, 7

[20] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and
A. C. Berg. Ssd: Single shot multibox detector. In ECCV, 2016. 4, 7
[21] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual relationship

10

Figure 10. Qualitative results on RefCOCO, RefCOCO, RefCOCO+, and RefCOCOg. All of them are from det bounding boxes. The ﬁrst
line denotes the supervised results and the second line denotes the unsupervised results. Solid red: referent grounding; solid green: ground
truth; dashed yellow: context grounding. We only display top 3 context objects with the context grounding probability > 0.1. Please zoom
in.

detection with language priors. In ECCV, 2016. 2

[22] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical question-image
co-attention for visual question answering. In NIPS, 2016. 4
[23] R. Luo and G. Shakhnarovich. Comprehension-guided referring ex-

pressions. In CVPR, 2017. 2

[24] A. Makhzani, J. Shlens, N. Jaitly, and I. J. Goodfellow. Adversarial

autoencoders. In ICLR Workshop, 2016. 3

[25] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Mur-
phy. Generation and comprehension of unambiguous object descrip-
tions. In CVPR, 2016. 1, 2, 3, 5, 6, 7

[26] V. K. Nagaraja, V. I. Morariu, and L. S. Davis. Modeling context
between objects for referring expression understanding. In ECCV,
2016. 2, 3, 6, 7

[29] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hock-
enmaier, and S. Lazebnik. Flickr30k entities: Collecting region-
to-phrase correspondences for richer image-to-sentence models. In
ICCV, 2015. 2

[30] J. Redmon and A. Farhadi. Yolo9000: better, faster, stronger.

In

CVPR, 2017. 1

[31] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-
time object detection with region proposal networks. In NIPS, 2015.
6

[32] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and B. Schiele.
Grounding of textual phrases in images by reconstruction. In ECCV,
2016. 2, 5, 6, 7

[33] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural net-

[27] J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for

works. TSP, 1997. 4

word representation. In EMNLP, 2014. 5

[28] B. A. Plummer, A. Mallya, C. M. Cervantes, J. Hockenmaier, and
S. Lazebnik. Phrase localization and visual relationship detection
with comprehensive linguistic cues. In ICCV, 2017. 1, 2

[34] S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D. Manning.
Generating semantically precise scene graphs from textual descrip-
tions for improved image retrieval. In Workshop on Vision and Lan-
guage, 2015. 1

11

[35] K. Sohn, H. Lee, and X. Yan. Learning structured output representa-
tion using deep conditional generative models. In NIPS, 2015. 3
[36] Q. Sun, B. Schiele, and M. Fritz. A domain based approach to social

relation recognition. In CVPR, 2017. 1

[37] J. A. Thomas. Meaning in interaction: An introduction to pragmat-

ics. Routledge, 2014. 2

[38] J. Thomason, J. Sinapov, and R. Mooney. Guiding interaction be-
haviors for multi-modal grounded language learning. In Proceedings
of the First Workshop on Language Grounding for Robotics, pages
20–24, 2017. 1

[39] Y. Wei, J. Feng, X. Liang, C. Ming-Ming, Y. Zhao, and S. Yan. Ob-
ject region mining with adversarial erasing: A simple classiﬁcation
to semantic segmentation approach. In CVPR, 2017. 8

[40] R. J. Williams. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. In Machine Learning. 1992. 3
[41] F. Xiao, L. Sigal, and Y.-J. Lee. Weakly-supervised visual grounding

of phrases with linguistic structures. In CVPR, 2017. 8

[42] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,
R. Zemel, and Y. Bengio. Show, attend and tell: Neural image cap-
tion generation with visual attention. In ICML, 2015. 2

[43] T. Xue, J. Wu, K. Bouman, and B. Freeman. Visual dynamics: Prob-
abilistic future frame synthesis via cross convolutional networks. In
NIPS, 2016. 2

[44] X. Yan, J. Yang, K. Sohn, and H. Lee. Attribute2image: Conditional

image generation from visual attributes. In ECCV, 2016. 2

[45] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling

context in referring expressions. In ECCV, 2016. 2, 4, 5, 6

[46] L. Yu, H. Tan, M. Bansal, and T. L. Berg. A joint speaker-listener-

reinforcer model for referring expressions. In ICCV, 2017. 2, 6, 7

[47] H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua. Visual translation
embedding network for visual relation detection. In CVPR, 2017. 1,
2

[48] H. Zhang, Z. Kyaw, J. Yu, and S.-F. Chang. Ppr-fcn: Weakly super-
vised visual relation detection via parallel pairwise r-fcn. In ICCV,
2017. 2, 7, 8

[49] Z. Zhao, Q. Yang, D. Cai, X. He, and Y. Zhuang. Video question
answering via hierarchical spatio-temporal attention networks. In In-
ternational Joint Conference on Artiﬁcial Intelligence (IJCAI), vol-
ume 2, 2017. 1

[50] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object proposals

from edges. In ECCV, 2014. 4

12

Grounding Referring Expressions in Images by Variational Context

Hanwang Zhang1∗

Yulei Niu2†
1Nanyang Technological University, 2Renmin University of China, 3Columbia University,
hanwangzhang@ntu.edu.sg; niu@ruc.edu.cn; shih.fu.chang@columbia.edu

Shih-Fu Chang3

8
1
0
2
 
r
a

M
 
1
3
 
 
]

V
C
.
s
c
[
 
 
2
v
2
9
8
1
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

We focus on grounding (i.e., localizing or linking) refer-
ring expressions in images, e.g., “largest elephant stand-
ing behind baby elephant”. This is a general yet chal-
lenging vision-language task since it does not only require
the localization of objects, but also the multimodal com-
prehension of context — visual attributes (e.g., “largest”,
“baby”) and relationships (e.g., “behind”) that help to dis-
tinguish the referent from other objects, especially those
of the same category. Due to the exponential complex-
ity involved in modeling the context associated with mul-
tiple image regions, existing work oversimpliﬁes this task
to pairwise region modeling by multiple instance learning.
In this paper, we propose a variational Bayesian method,
called Variational Context, to solve the problem of com-
plex context modeling in referring expression grounding.
Our model exploits the reciprocal relation between the ref-
erent and context, i.e., either of them inﬂuences estima-
tion of the posterior distribution of the other, and thereby
the search space of context can be greatly reduced. We
also extend the model to unsupervised setting where no
annotation for the referent is available. Extensive exper-
iments on various benchmarks show consistent improve-
ment over state-of-the-art methods in both supervised and
unsupervised settings. The code is available at https:
//github.com/yuleiniu/vc/.

1. Introduction

Grounding natural language in visual data is a hallmark
of AI, since it establishes a communication channel be-
tween humans, machines, and the physical world, under-
pinning a variety of multimodal AI tasks such as robotic
navigation [38], visual Q&A [1, 16, 49], and visual chat-
bot [6]. Thanks to the rapid development in deep learning-
based CV and NLP, we have witnessed promising results
not only in grounding nouns (e.g., object detection [30]),

∗Hanwang was a research scientist at Columbia University.
†Yulei was a visiting student at Columbia University.

Figure 1. The proposed Variational Context model. Given an input
referring expression and an image with region proposals, we local-
ize the referent as output. We develop a grounding score function,
with the variational lower-bound composed by three cue-speciﬁc
multimodal modules, indicated by the description in the dashed
color boxes.

but also short phrases (e.g., noun phrases [28] and rela-
tions [47, 36]). However, the more general task: grounding
referring expressions [25], is still far from resolved due to
the challenges in understanding of both language and scene
compositions [10]. As illustrated in Figure 1, given an in-
put referring expression “largest elephant standing behind
baby elephant” and an image with region proposals, a model
that can only localize “elephant” is not satisfactory as there
are multiple elephants. Therefore, the key for referring ex-
pression grounding is to comprehend and model the con-
text. Here, we refer to context as the visual objects (e.g.,
“elephant”), attributes (e.g., “largest” and “baby”), and re-
lationships (e.g., “behind”) mentioned in the expression that
help to distinguish the referent from other objects.

One straightforward way of modeling the relations be-
tween the referent and context is to: 1) use external syn-
tactic parsers to parse the expression into entities, mod-
iﬁers, and relations [34], and then 2) apply visual rela-
tion detectors to localize them [47]. However, this two-
stage approach is not practical due to the limited general-
ization ability of the detectors applied in the highly unre-
stricted language and scene compositions. To this end, re-

1

cent approaches use multimodal embedding networks that
jointly comprehend language and model the visual rela-
tions [26, 11]. Due to the prohibitively high cost of an-
notating both referent and context of referring expressions
in images, multiple instance learning (MIL) [7] is usually
adopted in them to handle the weak supervision of the unan-
notated context objects, by maximizing the joint likelihood
of every region pair. However, for a referent, the MIL
framework essentially oversimpliﬁes the number of context
conﬁgurations of N regions from O(2N ) to O(N ). For ex-
ample, to localize the “elephant” in Figure 1, we may need
to consider the other three elephants all together as a multi-
nomial subset for modeling the context such as “largest”,
“behind” and “baby elephant”.

In this paper, we propose a novel model called Varia-
tional Context for grounding referring expressions in im-
ages. Compared to the previous MIL-based approaches [26,
11], our model approximates the combinatorial context
conﬁgurations with weak supervision using a variational
Bayesian framework [15]. Intuitively, it exploits the reci-
procity between referent and context, given either of which
can help to localize the other. As shown in Figure 1, for
each region x, we ﬁrst estimate a coarse context z, which
will help to reﬁne the true localizations of the referent. This
reciprocity is formulated into the variational lower-bound
of the grounding likelihood p(x|L), where L is the text
expression and the context is considered as a hidden vari-
able z (cf. Section 3). Speciﬁcally, the model consists of
three multimodal modules: context posterior q(z|x, L), ref-
erent posterior p(x|z, L), and context prior pz(z|L), each
of which performs a grounding task (cf. Section 4.3) that
aligns image regions with a cue-speciﬁc language feature;
each cue dynamically encodes different subsets of words in
the expression L that help the corresponding localization
(cf. Section 4.2).

Thanks to the reciprocity between referent and context,
our model can not only be used in the conventional super-
vised setting, where there is annotation for referent , but
also in the challenging unsupervised setting, where there
is no instance-level annotation (e.g., bounding boxes) of
both referent and context. We perform extensive experi-
ments on four benchmark referring expression datasets: Re-
fCLEF [14], RefCOCO [45], RefCOCO+ [45], and Ref-
COCOg [25]. Our model consistently outperforms previous
methods in both supervised and unsupervised settings. We
also qualitatively show that our model can ground the con-
text in the expression to the corresponding image regions
(cf. Section 5).

2. Related Work

Grounding Referring Expressions. Grounding refer-
ring expression is also known as referring expression com-
prehension, whose inverse task is called referring expres-

sion generation [25]. Different from grounding phrases [29,
28] and descriptive sentences [12, 32], the key for ground-
ing referring expression is to use the context (or pragmatics
in linguistics [37]) to distinguish the referent from other ob-
jects, usually of the same category [10]. However, most pre-
vious works resort to use holistic context such as the entire
image [25, 12, 32] or visual feature difference between re-
gions [45, 46]. Our model is similar to the works on explic-
itly modeling the referent and context region pairs [11, 26],
however, due to the lack of context annotation, they re-
duce the grounding task into a multiple instance learning
framework [7]. As we will discuss later, this framework
is not a proper approximation to the original task. There
are also studies on visual relation detection that detect ob-
jects and their relationships [21, 5, 47, 17, 48]. However,
they are limited to a ﬁxed-vocabulary set of relation triplets
and hence are difﬁcult to be applied in natural language
grounding. Our cue-speciﬁc language feature is similar to
the language modular network [11] that learns to decom-
pose a sentence into referent/context-related words, which
are different from other approaches that treat the expression
as a whole [25, 23, 46, 19].

Variational Bayesian Model vs. Multiple Instance
Learning. Our proposed variational context model is in a
similar vein of the deep neural network based variational
autoencoder (VAE) [15], which uses neural networks to
approximate the posterior distribution of the hidden value
q(z|x), i.e., encoder, and the conditional distribution of the
observation p(x|z), i.e., decoder. VAE shows efﬁcient and
effective end-to-end optimization for the intractable log-
sum likelihood log (cid:80)
z p(x, z) that is widely used in genera-
tive processes such as image synthesis [44] and video frame
prediction [43]. Considering the unannotated context as the
hidden variable z, the referring expression grounding task
can also be formulated into the above log-sum marginaliza-
tion (cf. Eq. (2)). The MIL framework [7] is essentially a
sum-log approximation of the log-sum, i.e., (cid:80)
z log p(x, z).
To see this, the max-pooling function log maxz p(x, z) used
in [11] can be viewed as the sum-log (cid:80)
z log p(x|z)p(z),
where p(z) = 1 if z is the correct context and 0 otherwise,
indicating there is only one positive instance; maximizing
the noisy-or function log(1 − (cid:81)
z(1 − p(x, z))) used in [26]
is equivalent to maximize (cid:80)
z log p(x, z), assuming there is
at least one positive instance. However, due to the numer-
ical property of the log function, this sum-log approxima-
tion will unnecessarily force every (x, z) pair to explain the
data [8]. Instead, we use the variational Bayesian upper-
bound to obtain a better sum-log approximation. Note
that visual attention models [2, 42] simplify the variational
lower bound by assuming p(z) = q(z|x); however, we
explicitly use the KL divergence KL(q(z|x)||p(z)) in the
lower bound to regularize the approximate posterior q(z|x)
not being too far from the prior p(z).

2

3. Variational Context

In this section, we derive the variational Bayesian for-
mulation of the proposed variational context model and the
objective function for training and test.

3.1. Problem Formulation

The task of grounding a referring expression L in an im-
age I, represented by a set of regions x ∈ X , can be viewed
as a region retrieval task with the natural language query
L. Formally, we maximize the log-likelihood of the condi-
tional distribution to localize the referent region x∗ ∈ X :

x∗ = arg max

log p(x|L),

(1)

x∈X

where we omit the image I in p(x|I, L).

As there is usually no annotation for the context, we
consider it as a hidden variable z. Therefore, Eq. (1)
can be rewritten as the following maximization of the log-
likelihood of the conditional marginal distribution:

x∗ = arg max

log

p(x, z|L).

(2)

x∈X

(cid:88)

z

Note that z is NOT necessary to be one region as assumed
in recent MIL approaches [11, 26], i.e., z ∈ X . For ex-
ample, the contextual objects “surrounding elephants” in
“a bigger elephant than the surrounding elephants” should
be composed by a multinomial subset of X , resulting in an
extremely large sample space that requires O(2|X |) search
complexity. Therefore, the marginalization in Eq (2) is in-
tractable in general.

To this end, we use the variational lower-bound [15] to

approximate the marginal distribution in Eq. (2) as:

log p(x|L) = log

p(x, z|L) ≥ Q(x, L) =

(cid:88)

z

Ez∼qφ(z|x,L) log pθ(x|z, L)
(cid:125)
(cid:123)(cid:122)
(cid:124)
Localization

− KL (qφ(z|x, L)||pω(z|L))
(cid:123)(cid:122)
(cid:125)
Regularization

(cid:124)

,

(3)

where KL(·) is the Kullback-Leibler divergence, φ, θ, and
ω are independent parameter sets for the respective distri-
butions. As shown in Figure 1, the lower bound Q(x, L)
offers a new perspective for exploiting the reciprocal nature
of referent and context in referring expression grounding:
Localization. This term calculates the localization score
for x given an estimated context z, using the referent-cue
of L parameterized by θ.
In particular, we design a new
posterior qφ(z|x, L) that approximates the true context prior
p(z|x, L), which models the context z using the context-cue
of L parameterized by φ. In the view of variational auto-
encoder [15, 35], this term works in an encoding-decoding
fashion: qφ is the encoder from x to z, and pθ is the decoder

from z to x.
Regularization. As KL is non-negative, maximizing
Q(x, L) would encourage that the posterior qφ is similar
to the prior pω, i.e., the estimated context z sampled from
qφ(z|x, L) should not be too far from the referring expres-
sion, which is modeled by pω(z|L) with the generic-cue of
L parameterized by ω. This term is necessary as the esti-
mated z could be overﬁtted to region features that are incon-
sistent with the visual context described in the expression.

3.2. Training and Test

Deterministic Context.

The lower-bound Q(x, L)
transforms the intractable log-sum in Eq. (2) into the efﬁ-
cient sum-log in Eq. (3), which can be optimized by us-
ing Monte Carlo unbiased gradient estimator such as RE-
INFORCE [40]. However, due to that φ is dependent on
the sampling of z over O(2|X |) conﬁgurations, its gradient
variance is large. To this end, we implement qφ(z|x, L) as
a differentiable but biased encoder:

z = f (x, L) =

x(cid:48) · qφ(x(cid:48)|x, L),

(4)

(cid:88)

x(cid:48)∈X

where we slightly abuse qφ as a score function such that
(cid:80)
x(cid:48) qφ(x(cid:48)|x, L) = 1. Note that this deterministic context
can be viewed as applying the “re-parameterization” trick as
in Variational Auto-Encoder [15]: rewriting z ∼ qφ(z|x, L)
to z = f (x, L; (cid:15)), (cid:15) ∼ p((cid:15)), where the stochasticity of the
auxiliary random variable (cid:15) comes from training samples
x ∈ X ((cid:15)). A clear example is Adversarial Autoencoder [24]
which shows that such stochasticity achieves similar test-
likelihood compared to other distributions such as Gaus-
sian.

Objective Function. Applying Eq. (4) to Eq. (3), we can
rewrite Q(x, L) into a function of only one sample estima-
tion, which is a common practice in SGD:

Q(x,L) = log pθ(x|z,L)−log qφ(z|x,L)+log pω(z|L).

(5)

is known,

In supervised setting where the ground truth of the ref-
erent
to distinguish the referent from other
objects, we need to train a model that outputs a high
p(x|L) (i.e., Q(x, L)), while maintaining a low p(x(cid:48)|L)
(i.e., Q(x(cid:48), L)), whenever x(cid:48)
(cid:54)= x. Therefore, we use
the so-called Maximum Mutual Information loss as in [25]
− log{Q(x, L)/ (cid:80)
x(cid:48) Q(x(cid:48), L)}, where we do not need to
explicitly model the distributions with normalizations; we
use the following score function:

Q(x, L) ∝ S(x, L) = sθ(x, L)−sφ(x, L)+sω(x, L), (6)

where z is omitted as it is a function of x in Eq. (4). sθ,
sφ, and sω are the score functions (e.g., pθ ∝ sθ) for pθ,
qφ, and pω, respectively. These functions will be detailed in

3

Figure 2. The architecture of the proposed Variational Context model. It consists of a region feature extraction module (Section 4.1, and
a language feature extraction module (Section 4.2), and three grounding modules (Section 4.3). It can be trained in an end-to-end fashion
with the input of a set of image regions and a referring expression, using the supervised loss ( Eq. (7)) or the unsupervised loss (Eq. (8)).
fc: fully-connected layer. concat: vector concatenation. L2Norm: L2 normalization layer. (cid:12): element-wise vector multiplication. ⊕: add.

Section 4.3. In this way, maximizing Eq. (5) is equivalent
to minimizing the following softmax loss:

Ls = − log softmax S(xgt, L),

(7)

where the softmax is over x ∈ X and xgt is the ground truth
referent region.

Note that the reciprocity between referent and context
can be extended to unsupervised learning, where neither of
the referent and context has annotation. In this setting, we
adopt the image-level max-pooled MIL loss functions for
unsupervised referring expression grounding:

Lu = − max
x∈X

log softmax S(x, L),

(8)

where the softmax is over x ∈ X . Note that the max-pooled
MIL function is reasonable since there is only one ground
truth referent given an expression and image training pair.

At test stage, in both supervised and unsupervised set-
tings, we predict the referent region x∗ by selecting the re-
gion x ∈ X with the highest score:

x∗ = arg max

S(x, L),

x∈X

(9)

4. Model Architecture

The overall architecture of the proposed variational con-
text model is illustrated in Figure 2. Thanks to the deter-
ministic context in Eq. (4), the ﬁve modules in our model
can be integrated into an end-to-end differentiable fashion.
Next, we will detail the implementation of each module.

4.1. RoI Features

Given an image with a set of Region of Interests (RoIs)
X , obtained by any off-the-shelf proposal generator [50] or
object detectors [20], this module extracts the feature vec-
tor xi for every RoI. In particular, xi is the concatenation
of visual feature vi and spatial feature pi. For vi, we can
use the output of a pre-trained convolutional network (cf.
Section 5). If the object category of each RoI is available,
we can further utilize the comparison between the referent

(cid:80)

and other objects to capture the visual difference such as
“the largest/baby elephant”. Speciﬁcally, we append the vi-
vi−vj
sual difference feature [45] δvi = 1
||vi−vj || to the
n
original vi visual feature, where n is the number of objects
chosen for comparison (e.g., the number of RoI in the same
object category). For spatial feature, we use the 5-d spatial
W , ybr
attributes pi = [ xtl
W ·H ], where x and y are
the coordinates the top left (tl) and bottom right (br) RoI of
the size w × h, and the image is of the size W × H.

H , w·h

H , xbr

W , ytl

j(cid:54)=i

4.2. Cue-Speciﬁc Language Features

The cue-speciﬁc language feature representation for a re-
ferring expression is inspired by the attention weighted sum
of word vectors [11, 22, 3], where the weights are param-
eterized by context-cue φ, referent-cue θ, and generic-cue
ω. The context-cue language feature yc = [yc1, yc2] is
a concatenation of yc1 for language-vision association be-
tween single RoI and the expression, and yc2 for the as-
sociation between pairwise RoIs; the referent-cue language
feature yr can be represented in a similar way to yc; the
generic-cue language feature yg is only for single RoI asso-
ciation as it is an independent prior. The weights of each cue
are calculated from the hidden state vectors of a 2-layer bi-
directional LSTM (BLSTM) [33], scanning through the ex-
pression. The hidden states encode forward and backward
compositional semantic meanings of the sentences, beneﬁ-
cial for selecting words that are useful for single and pair-
wise associations. Speciﬁcally, suppose hj as the 4,000-d
concatenation of forward and backward hidden vectors of
the j-th word, without loss of generality, the word attention
weight αj and the language feature y for single/pairwise
association of any cue can be calculated as:

mj = fc(hj), αj = softmaxj(mj), y =

αjwj, (10)

(cid:88)

j

where wj is a 300-d vector. Note that the BLSTM module
can be jointly trained with the entire model.

Figure 3 shows that the cue-speciﬁc language features
dynamically weight words in different expressions. We can
have two interesting observations. First, c1 is almost uni-

4

Figure 3. Two qualitative examples of the cue-speciﬁc language
feature word weights. Darker color indicates higher weights.
c/r+1/2: context/referent-cue + single/pairwise.

form while c2 is highly skewed; although r2 is more skewed
than c1, it is still less skewed than r1. This is reasonable
since: 1) without ground-truth, individual score (c1) does
not help much for context estimation from scratch; context
is more easily found by the pairwise score (c2) induced by
relationships or other objects (e.g., “left” or “frisbee”); 2) in
referent grounding with ground truth, individual score (r1)
is sufﬁcient (e.g., “dog lying” and “black white dog”) and
pairwise score (r2) is helpful; 3) g is adaptive to the num-
ber of object categories in the expression, i.e., if the context
object is of the same category as the referent, g weighs de-
scriptive or relationship words higher (e.g., “lying, stand-
ing, left”), and nouns higher (e.g., “frisbee”), otherwise;
moreover, it demonstrates that the deterministic guess of z
in Eq. (4) is meaningful.

4.3. Score Functions

For any image and expression pair, given the RoI feature
xi, and the cue-speciﬁc language feature yc, yr, and yg, we
implement the ﬁnal grounding score in Eq. (6) as:

zi =

(cid:88)
j

softmaxj (sφ(xi, xj, yc)) xj,

sθ(x, L) ← sθ(xi, zi, yr),
sφ(x, L) ← sφ(xi, zi, yc),
sω(x, L) ← sω(zi, yg),

(11)

where the right-hand side functions are deﬁned as below.

Context Estimation Score: sφ(xi, xj, yc). It is a score
function for modeling the context posterior qφ(z|x, L), i.e.,
given an RoI xi as the candidate referent, we calculate the
likelihood of any RoI xj to be the context. We can also
use this function to estimate the ﬁnal context posterior score
sφ(xi, zi, yc). Speciﬁcally, the context estimation score is a
sum of the single and pairwise vision-language association
scores: xj and yc1, [xi, xj] and yc2. Each associate score
is an fc output from the input of a normalized feature:

j = yc1 (cid:12) fc(xj), m2
m1
(cid:101)m1
j ), (cid:101)m2
j = L2Norm(m1
sφ(xi, xj, yc) = fc( (cid:101)m1

j = yc2 (cid:12) fc([xi, xj]),
j = L2Norm(m2
j ) + fc( (cid:101)m2
j ),

j ),

(12)

obtain the estimated context z as zi = (cid:80)
βj = softmaxj(sφ(xi, xj, yc)).

j βjxj, where

Referent Grounding Score: sθ(xi, zi, yr). After ob-
taining the context feature zi, we can use this score function
to calculate how likely a candidate RoI xi is the referent
given the context zi. This function is similar to Eq. (12).

Score:

Context Regularization

sω(zi, yg) −
sφ(xi, zi, yc). As discussed in Eq. (6),
this function
scores how likely the estimated context feature zi
is
consistent with the content mentioned in the expression. In
particular, sω(zi, yg) is only dependent on single RoI:
i (cid:12) fc(zi), (cid:101)mi=L2Norm(mi), sω(zi, yg
mi=yg

i )=fc(mi).
(13)

5. Experiment

5.1. Datasets

We used four popular benchmarks for the referring ex-

pression grounding task.

RefCOCO [45]. It has 142,210 referring expressions for
50,000 referents (e.g., object instances) in 19,994 images
from MSCOCO [18]. The expressions are collected in an
interactive way [14]. The dataset is split into train, valida-
tion, Test A, and Test B, which has 120,624, 10,834, 5,657
and 5,095 expression-referent pairs, respectively. An image
contains multiple people in Test A and multiple objects in
Test B.

RefCOCO+ [45]. It has 141,564 expressions for 49,856
referents in 19,992 images from MSCOCO. The difference
from RefCOCO is that it only allows appearances but no
locations to describe the referents. The split is 120,191,
10,758, 5,726 and 4,889 expression-referent pairs for train,
validation, Test A, and Test B respectively.

RefCOCOg [25]. It has 95,010 referring expressions for
49,822 objects in 25,799 images from MSCOCO. Different
from RefCOCO and RefCOCO+, this dataset not collected
in an interactive way and contains longer sentences contain-
ing both appearance and location expressions. The split is
85,474 and 9,536 expression-referent pairs for training and
validation. Note that there is no open test split for Ref-
COCOg, so we used the hyper-parameters cross-validated
on RefCOCO and RefCOCO+.

RefCLEF [14]. It contains 20,000 images with anno-
tated image regions. It has some ambiguous (e.g. anywhere)
phrases and mistakenly annotated image regions that are not
described in the expressions. For fair comparison, we used
the split released by [12, 32], i.e., 58,838, 6,333 and 65,193
expression-referent pairs for training, validation and test, re-
spectively.

5.2. Settings and Metrics

where the element-wise multiplication (cid:12) is an effective way
for multimodal features [2]. According to Eq. (4), we can

We used an English vocabulary of 72,704 words con-
tained in the GloVe pre-trained word vectors [27], which

5

Table 1. Supervised grounding performances (Acc%) of comparing methods on RefCOCO, RefCOCO+, and RefCOCOg. Note that [46]
reports slightly higher accuracies using ensemble models of Listener and Speaker. For fair comparison, we only report their single models.

Dataset

Split MMI [25] NegBag [26] Attr [19] CMN [11]

Speaker [46]

Listener [46] VC w/o reg VC w/o α

VC

State-of-The-Arts

Our Baselines

RefCOCO

RefCOCO+

RefCOCOg

Val

RefCOCO(det)

RefCOCO+(det)

RefCOCOg(det)

Val

Test A

Test B

Test A

Test B

Test A

Test B

Test A

Test B

71.72

71.09

58.42

51.23

62.14

64.90

54.51

54.03

42.81

45.85

75.6

78.0

—

—

68.4

58.6

56.4

—

—

39.5

78.85

78.07

61.47

57.22

69.83

72.08

57.29

57.97

46.20

52.35

75.94

79.57

59.29

59.34

69.30

71.03

65.77

54.32

47.76

57.47

78.95

80.22

64.60

59.62

72.63

72.95

63.43

60.43

48.74

59.51

78.45

80.10

63.34

58.91

72.25

72.95

62.98

59.61

48.44

58.32

75.59

79.69

60.76

60.14

71.05

70.78

65.10

56.82

51.30

60.95

74.03

78.27

57.61

54.37

65.13

70.73

64.63

53.33

46.88

55.72

78.98

82.39

62.56

62.90

73.98

73.33

67.44

58.40

53.18

62.30

Figure 4. Qualitative results on RefCOCOg (det) showing comparisons between correct (green tick) and wrong referent grounds (red cross)
by VC and CMN. The denotations of the bounding box colors are as follows. Solid red: referent ground; solid green: ground truth; dashed
yellow: context ground. We only display top 3 context objects with the context ground probability > 0.1. We can observe that VC has
more reasonable context localizations than CMN, even in cases when the referent ground of VC fails.

was also used for the initialization of our word vectors. We
used a “unk” symbol for the input word of the BLSTM if the
word is out of the vocabulary; we set the sentence length to
20 and used “pad” symbol to pad expression sentence < 20.
For RoI visual features on RefCOCO, RefCOCO+, and Re-
fCOCOg which have MSCOCO annotated regions with ob-
ject categories, we used the concatenation of the 4,096-d
fc7 output of a VGG-16 based Faster-RCNN network [31]
trained on MSCOCO and its corresponding 4,096-d visd-
iff feature [45]; although RefCLEF regions also have object

categories, for fair comparison with [32], we did not use the
visdiff feature.

The model training is single-image based, with all re-
ferring expressions annotated. We applied SGD of 0.95-
momentum with initial learning rate of 0.01, multiplied by
0.1 after every 120,000 iterations, up to 160,000 iterations.
Parameters in BILSTM and fc-layers were initialized by
Xavier [9] with 0.0005 weight decay. Other settings were
default in TensorFlow. Note that our model is trained with-
out bells and whistles, therefore, other optimization tricks

6

such as batch normalization [13] and GRU [4] are expected
to further improve the results reported here. Besides the
ground truth annotations, grounding to automatically de-
tected objects is a more practical setting. Therefore, we also
evaluated with the SSD-detected bounding boxes [20] on
the four datasets provided by [46]. A grounding is consid-
ered as correct if the intersection-over-union (IoU) of the
top-1 scored region and the ground-truth object is larger
than 0.5. The grounding accuracy (a.k.a, P@1) is the frac-
tion of correctly grounded test expressions.

5.3. Evaluations of Supervised Grounding

We compared our variational context model (VC) with
state-of-the-art referring expression methods published in
recent years, which can be categorized into: 1) generation-
comprehension based such as MMI
[19],
Speaker [46], Listener [46], and SCRC [12]; 2) localiza-
tion based such as GroundR [32], NegBag [26], CMN [11].
Note that NegBag and CMN are MIL-based models. In par-
ticular, we used the author-released code to obtain the re-
sults of CMN on RefCLEF, RefCOCO, and RefCOCO+.

[25], Attr

From the results on RefCOCO, RefCOCO+, and Re-
fCOCOg in Table 1 and that on RefCLEF in Table 2,
we can see that VC achieves the state-of-the-art perfor-
mance. We believe that the improvement is attributed to
the variational Bayesian modeling of context. First, on all
datasets, except for the most recent reinforcement learn-
ing based [46], VC outperforms all the other sentence
generation-comprehension methods that do not model con-
text. Second, compared to VC without the regularization
term in Eq. (3) (VC w/o reg), VC can boost the performance
by around 2% on all datasets. This demonstrates the ef-
fectiveness of the KL divergence for the prevention of the
overﬁtted context estimation.

In particular, we further demonstrate the superiority of
VC over the most recent MIL-based method CMN. As il-
lustrated in Figure 4, VC has better context comprehension
in both of the language and image regions than CMN. For
example, in the top two rows where VC is correct and CMN
is wrong, for the grounding in the second column, CMN
unnecessarily considers the “girl” as context but the expres-
sion only describes using “elephant”; in the last column,
CMN misses the key context “frisbee”. Even in the fail-
ure cases where VC is wrong and CMN is correct, VC still
localizes reasonable context. For example, in the fourth col-
umn, although CMN grounds the correct TV, but it is based
on incorrect context of other TVs; while VC can predict
the correct context “children”.
In addition, we observed
that most of the cases that CMN is better than VC involves
multiple humans. This demonstrates that VC is better at
grounding objects of different categories.

VC is also effective in images with more objects. Fig-
ure 5 shows the performances of VC and CMN with various

Table 2. Performances (Acc%) of supervised and unsupervised
methods on RefCLEF.

Sup.

Sup. (det) Unsup. (det)

SCRC [12]

72.74

GroundR [32] —

CMN [11]

VC

VC w/o α

81.52

82.43

79.60

17.93

26.93

28.33

31.13

27.40

10.70

—

—

14.11

14.50

Table 3. Unsupervised grounding performances (Acc%) of com-
paring methods on RefCOCO, RefCOCO+, and RefCOCOg.
VC w/o α

VC w/o reg

Dataset

Split

VC

RefCOCO

RefCOCO+

RefCOCOg

Val

RefCOCO(det)

RefCOCO+(det)

RefCOCOg(det)

Val

Test A

Test B

Test A

Test B

Test A

Test B

Test A

Test B

13.59

21.65

18.79

24.14

25.14

17.14

22.30

19.74

24.05

28.14

17.34

20.98

23.24

24.91

33.79

20.91

21.77

25.79

25.54

33.66

33.29

30.13

34.60

31.58

30.26

32.68

27.22

34.68

28.10

29.65

number of bounding boxes. We can observe that VC con-
siderably outperforms CMN over all bounding boxes num-
bers. Recall that context is the key to distinguish objects of
the same category. In particular, on the Test A sets of Ref-
COCO and RefCOCO+ where the grounding is only about
people, i.e., the same object category, the gap between VC
and CMN is becoming larger as the box number increases.
This demonstrates that MIL is ineffective in modeling con-
text, especially when the number of image regions is large.

5.4. Evaluations of Unsupervised Grounding

We follow the unsupervised setting in GroundR [32]. To
our best knowledge, it is the only work on unsupervised re-
ferring expression grounding. Note that it is also known as
“weakly supervised” detection [48] as there is still image-
level ground truth (i.e., the referring expression). Table 2 re-
ports the unsupervised results on the RefCLEF. We can see
that VC outperforms the state-of-the-art GroundR, which is
a generation-comprehension based method. This demon-
strates that using context also helps unsupervised ground-
ing. As there is no published unsupervised results on Re-
fCOCO, RefCOCO+, and RefCOCOg, we only compared
our baselines on them in Table 3. We can have the follow-
ing three key observations which highlight the challenges
of unsupervised grounding:

Context Prior. VC w/o reg is the baseline without the
KL divergence as a context regularization in Eq. (3). We can
see that in most of the cases, VC considerably outperforms
VC w/o reg by over 2%, even over 5% on RefCOCO+ (det)

7

Figure 5. Performances of VC and CMN with different number of object bounding boxes on RefCOCO Test A &B, RefCOCO+ Test A &
B, and RefCOCOg Val. Compared to CMN, we can see that VC is more effective in context modeling when the number of objects is large.

Figure 6. Common failure cases in unsupervised grounding with
detected bounding boxes. From left to right: RefCOCO, Ref-
COCO+, and RefCOCOg. The failure is mainly to the challenging
unsupervised relation modeling between referent and context.

and RefCOCOg (det). Note that this improvement is signif-
icantly higher than that in supervised setting (e.g., < 3% as
reported in Table 1). The reason is that the context estima-
tion in Eq. (4) would be easier to be stuck in image regions
that are irrelevant to the expression in unsupervised setting,
therefore, context prior is necessary.

Language Feature. Except on RefCOCOg, we consis-
tently observed the ineffectiveness of the cue-speciﬁc lan-
guage feature in unsupervised setting, i.e., VC w/o α out-
performs VC in Table 2 and 3. Here α represents the cue-
speciﬁc word attention. This is contrary to the observation
in the supervised setting as listed in Table 1, where VC w/o
α is consistently lower than VC. Note that without the cue-
speciﬁc word attention α in Eq. (10), the language feature is
merely the average value of the word embedding vectors in
the expression. In this way, VC w/o α does not encode any
structural language composition as illustrated in Figure 3,
thus, it is better for short expressions. However, when the
expression is long in RefCOCOg, discarding the language
structure still degrades the performance on RefCOCOg.

Unsupervised Relation Discovery.

Although we
demonstrated that VC improves the unsupervised ground-
ing by modeling context, we believe that there is still a large
space for improving the quality of modeling the context.
As the failure examples shown in Figure (6), 1) many con-
text estimations are still out of the scope of the expression,
e.g., we may localize the “cup” and “table” as context even
though the expression is “woman with green t-shirt”; 2) we

8

Figure 7. Word cloud visualizations of cue-speciﬁc word attention
α in Eq. 10 of context-cue (c2), referent-cue (r1), and generic-cue
(g) using supervised (top row) and unsupervised training (bottom
row) on RefCOCOg. Without supervision, it is difﬁcult to discover
meaningful language compositions.

may mistake due to the wrong comprehension of the rela-
tions, e.g., “right” as “left”, even if the objects belong to
the same category, e.g., “elephant”. For further investiga-
tion, Figure 7 visualizes the cue-speciﬁc word attentions in
supervised and unsupervised settings. The almost identi-
cal word attentions in unsupervised setting reﬂect the fact
that the relation modeling between referent and context is
not as successful as in supervised setting. This inspires us
to exploit stronger prior knowledge such as language struc-
ture [41] and spatial conﬁgurations [48, 39].

6. Conclusions

We focused on the task of grounding referring expres-
sions in images and discussed that the key problem is how
to model the complex context, which is not effectively re-
solved by the multiple instance learning framework used
in prior works. Towards this challenge, we introduced
the Variational Context model, where the variational lower-
bound can be interpreted by the reciprocity between the ref-
erent and context: given any of which can help to localize
the other, and hence is expected to signiﬁcantly reduce the
context complexity in a principled way. We implemented
the model using cue-speciﬁc language-vision embedding
network that can be efﬁciently trained end-to-end. We val-
idated the effectiveness of this reciprocity by promising
supervised and unsupervised experiments on four bench-
marks. Moving forward, we are going to 1) incorporate ex-
pression language generation in the variational framework,
2) use more structural features of language rather than word
attentions, and 3) further investigate the potential of our
model in the unsupervised referring expression grounding.

the visual grounding task [11]. Therefore, we adopt the
parser jointly trained on the referring expression ground-
ing task [11]. As illustrated in Figure 8, this parser assigns
word-level attention weights of subject, relation, and object.
In particular, we consider the language features of c1, r2 as
the average word embeddings, c2 as the relation weights,
r1 as the subject weights, g as the object weights. Table 4
shows the performances on unsupervised grounding. We
can see that there is no signiﬁcant improvement of VC w/
parser over VC w/o α.

7.5. More Qualitative Results

Figure 10 shows more qualitative results on supervised
and unsupervised grounding results on RefCOCO, Ref-
COCO+, and RefCOCOg.

References

[1] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zit-
nick, and D. Parikh. Vqa: Visual question answering. In ICCV, 2015.
1

[2] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition

with visual attention. In ICLR, 2015. 2, 5

[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by

jointly learning to align and translate. 2015. 4

[4] K. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio. On
the properties of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259, 2014. 7

[5] B. Dai, Y. Zhang, and D. Lin. Detecting visual relationships with

deep relational networks. In CVPR, 2017. 2

[6] A. Das, S. Kottur, J. M. Moura, S. Lee, and D. Batra. Learning
cooperative visual dialog agents with deep reinforcement learning.
In ICCV, 2017. 1

[7] T. G. Dietterich, R. H. Lathrop, and T. Lozano-P´erez. Solving the
multiple instance problem with axis-parallel rectangles. Artiﬁcial
intelligence, 1997. 2

[8] C. W. Fox and S. J. Roberts. A tutorial on variational bayesian infer-

[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of training

deep feedforward neural networks. In ICAIS, 2010. 6

[10] D. Golland, P. Liang, and D. Klein. A game-theoretic approach to

generating spatial descriptions. In EMNLP, 2010. 1, 2

[11] R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko. Model-
ing relationships in referential expressions with compositional mod-
ular networks. In CVPR, 2017. 2, 3, 4, 6, 7

[12] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell.

Natural language object retrieval. In CVPR, 2016. 2, 5, 7

[13] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In ICML, 2015.
7

[14] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg. Refer-
itgame: Referring to objects in photographs of natural scenes.
In
EMNLP, 2014. 2, 5

[15] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In

ICLR, 2014. 2, 3

[16] Y. Li, N. Duan, B. Zhou, X. Chu, W. Ouyang, X. Wang, and M. Zhou.
Visual question generation as dual task of visual question answering.
In CVPR, 2018. 1

[17] Y. Li, W. Ouyang, and X. Wang. Vip-cnn: Visual phrase guided

convolutional neural network. In CVPR, 2017. 2

Figure 8. Some parsing examples from [11].

7. Supplementary Material

7.1. Derivation of Eq. (3)

Recall that we are going to transform the log-sum ob-
jective function in Eq. (2) to sum-log for tractable training.
Without loss of generality, we omit the conditional L in the
derivation. By using the concavity of the log function:

log((1 − α)x + αy) ≥ (1 − α) log(x) + α log(y),

(14)

to the following rewriting of Eq. (2):

(cid:88)

log

p(x, z) = log

(cid:32)

(cid:88)

z

(cid:33)

q(z|x)p(x, z)
q(z|x)

q(z|x) log

p(x, z)
q(z|x)

q(z|x) log p(x|z)p(z) −

q(z|x) log q(z|x)

q(z|x) log p(x|z) +

q(z|x) log p(z)

(cid:88)

=

q(z|x) log p(x|z) −

q(z|x)

(cid:88)

q(z|x)
p(z)

= Ez∼q(z|x) log p(x|z) − KL (q(z|x)||p(z)) = Q(x)

(15)

7.2. More Examples on Language Features

Figure 9 shows more cue-speciﬁc language features on

RefCOCOg.

7.3. More Results on Unsupervised Grounding

7.4. External Parsers

As discussed in Section 5.4 that the language feature in
unsupervised VC is not as good as that in the supervised
setting. An alternative is to use external NLP parsers to ob-
tain the compositions. However, conventional parsers (e.g.,
Standford Dependency) are observed to be suboptimal to

(cid:88)

z

(cid:88)

(cid:88)

−

z

z

z

z
(cid:88)

z
(cid:88)

z
(cid:88)

≥

=

=

z

z

9

q(z|x) log q(z|x)

ence. Artiﬁcial intelligence review, 2012. 2

Table 4. Unsupervised grounding performances (%) of VC w/ or w/o parsers on the four datasets.
RefCLEF RefCOCO TestA RefCOCO TestB

RefCOCO+TestA

RefCOCO+TestB

RefCOCOg

VC w/ parser

GT

VC

VC w/o α

VC w/ parser

VC

VC w/o α

22.57

21.06

20.72

14.90

14.11

14.50

23.00

17.34

33.29

24.07

20.91

32.68

27.50

20.98

30.13

25.12

21.77

27.22

24.69

23.24

34.60

26.18

25.79

34.68

28.96

24.91

31.58

26.92

25.54

28.10

30.73

33.79

30.26

30.64

33.66

29.65

DET

RefCLEF RefCOCO TestA RefCOCO TestB RefCOCO+ TestA RefCOCO+ TestB RefCOCOg

Figure 9. Qualitative results of the cue-speciﬁc language features on RefCOCOg.

[18] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick. Microsoft coco: Common objects in
context. In ECCV, 2014. 5

[19] J. Liu, L. Wang, and M.-H. Yang. Referring expression generation

and comprehension via attributes. In ICCV, 2017. 2, 6, 7

[20] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and
A. C. Berg. Ssd: Single shot multibox detector. In ECCV, 2016. 4, 7
[21] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual relationship

10

Figure 10. Qualitative results on RefCOCO, RefCOCO, RefCOCO+, and RefCOCOg. All of them are from det bounding boxes. The ﬁrst
line denotes the supervised results and the second line denotes the unsupervised results. Solid red: referent grounding; solid green: ground
truth; dashed yellow: context grounding. We only display top 3 context objects with the context grounding probability > 0.1. Please zoom
in.

detection with language priors. In ECCV, 2016. 2

[22] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical question-image
co-attention for visual question answering. In NIPS, 2016. 4
[23] R. Luo and G. Shakhnarovich. Comprehension-guided referring ex-

pressions. In CVPR, 2017. 2

[24] A. Makhzani, J. Shlens, N. Jaitly, and I. J. Goodfellow. Adversarial

autoencoders. In ICLR Workshop, 2016. 3

[25] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Mur-
phy. Generation and comprehension of unambiguous object descrip-
tions. In CVPR, 2016. 1, 2, 3, 5, 6, 7

[26] V. K. Nagaraja, V. I. Morariu, and L. S. Davis. Modeling context
between objects for referring expression understanding. In ECCV,
2016. 2, 3, 6, 7

[29] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hock-
enmaier, and S. Lazebnik. Flickr30k entities: Collecting region-
to-phrase correspondences for richer image-to-sentence models. In
ICCV, 2015. 2

[30] J. Redmon and A. Farhadi. Yolo9000: better, faster, stronger.

In

CVPR, 2017. 1

[31] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-
time object detection with region proposal networks. In NIPS, 2015.
6

[32] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and B. Schiele.
Grounding of textual phrases in images by reconstruction. In ECCV,
2016. 2, 5, 6, 7

[33] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural net-

[27] J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for

works. TSP, 1997. 4

word representation. In EMNLP, 2014. 5

[28] B. A. Plummer, A. Mallya, C. M. Cervantes, J. Hockenmaier, and
S. Lazebnik. Phrase localization and visual relationship detection
with comprehensive linguistic cues. In ICCV, 2017. 1, 2

[34] S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D. Manning.
Generating semantically precise scene graphs from textual descrip-
tions for improved image retrieval. In Workshop on Vision and Lan-
guage, 2015. 1

11

[35] K. Sohn, H. Lee, and X. Yan. Learning structured output representa-
tion using deep conditional generative models. In NIPS, 2015. 3
[36] Q. Sun, B. Schiele, and M. Fritz. A domain based approach to social

relation recognition. In CVPR, 2017. 1

[37] J. A. Thomas. Meaning in interaction: An introduction to pragmat-

ics. Routledge, 2014. 2

[38] J. Thomason, J. Sinapov, and R. Mooney. Guiding interaction be-
haviors for multi-modal grounded language learning. In Proceedings
of the First Workshop on Language Grounding for Robotics, pages
20–24, 2017. 1

[39] Y. Wei, J. Feng, X. Liang, C. Ming-Ming, Y. Zhao, and S. Yan. Ob-
ject region mining with adversarial erasing: A simple classiﬁcation
to semantic segmentation approach. In CVPR, 2017. 8

[40] R. J. Williams. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. In Machine Learning. 1992. 3
[41] F. Xiao, L. Sigal, and Y.-J. Lee. Weakly-supervised visual grounding

of phrases with linguistic structures. In CVPR, 2017. 8

[42] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,
R. Zemel, and Y. Bengio. Show, attend and tell: Neural image cap-
tion generation with visual attention. In ICML, 2015. 2

[43] T. Xue, J. Wu, K. Bouman, and B. Freeman. Visual dynamics: Prob-
abilistic future frame synthesis via cross convolutional networks. In
NIPS, 2016. 2

[44] X. Yan, J. Yang, K. Sohn, and H. Lee. Attribute2image: Conditional

image generation from visual attributes. In ECCV, 2016. 2

[45] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling

context in referring expressions. In ECCV, 2016. 2, 4, 5, 6

[46] L. Yu, H. Tan, M. Bansal, and T. L. Berg. A joint speaker-listener-

reinforcer model for referring expressions. In ICCV, 2017. 2, 6, 7

[47] H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua. Visual translation
embedding network for visual relation detection. In CVPR, 2017. 1,
2

[48] H. Zhang, Z. Kyaw, J. Yu, and S.-F. Chang. Ppr-fcn: Weakly super-
vised visual relation detection via parallel pairwise r-fcn. In ICCV,
2017. 2, 7, 8

[49] Z. Zhao, Q. Yang, D. Cai, X. He, and Y. Zhuang. Video question
answering via hierarchical spatio-temporal attention networks. In In-
ternational Joint Conference on Artiﬁcial Intelligence (IJCAI), vol-
ume 2, 2017. 1

[50] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object proposals

from edges. In ECCV, 2014. 4

12

Grounding Referring Expressions in Images by Variational Context

Hanwang Zhang1∗

Yulei Niu2†
1Nanyang Technological University, 2Renmin University of China, 3Columbia University,
hanwangzhang@ntu.edu.sg; niu@ruc.edu.cn; shih.fu.chang@columbia.edu

Shih-Fu Chang3

8
1
0
2
 
r
a

M
 
1
3
 
 
]

V
C
.
s
c
[
 
 
2
v
2
9
8
1
0
.
2
1
7
1
:
v
i
X
r
a

Abstract

We focus on grounding (i.e., localizing or linking) refer-
ring expressions in images, e.g., “largest elephant stand-
ing behind baby elephant”. This is a general yet chal-
lenging vision-language task since it does not only require
the localization of objects, but also the multimodal com-
prehension of context — visual attributes (e.g., “largest”,
“baby”) and relationships (e.g., “behind”) that help to dis-
tinguish the referent from other objects, especially those
of the same category. Due to the exponential complex-
ity involved in modeling the context associated with mul-
tiple image regions, existing work oversimpliﬁes this task
to pairwise region modeling by multiple instance learning.
In this paper, we propose a variational Bayesian method,
called Variational Context, to solve the problem of com-
plex context modeling in referring expression grounding.
Our model exploits the reciprocal relation between the ref-
erent and context, i.e., either of them inﬂuences estima-
tion of the posterior distribution of the other, and thereby
the search space of context can be greatly reduced. We
also extend the model to unsupervised setting where no
annotation for the referent is available. Extensive exper-
iments on various benchmarks show consistent improve-
ment over state-of-the-art methods in both supervised and
unsupervised settings. The code is available at https:
//github.com/yuleiniu/vc/.

1. Introduction

Grounding natural language in visual data is a hallmark
of AI, since it establishes a communication channel be-
tween humans, machines, and the physical world, under-
pinning a variety of multimodal AI tasks such as robotic
navigation [38], visual Q&A [1, 16, 49], and visual chat-
bot [6]. Thanks to the rapid development in deep learning-
based CV and NLP, we have witnessed promising results
not only in grounding nouns (e.g., object detection [30]),

∗Hanwang was a research scientist at Columbia University.
†Yulei was a visiting student at Columbia University.

Figure 1. The proposed Variational Context model. Given an input
referring expression and an image with region proposals, we local-
ize the referent as output. We develop a grounding score function,
with the variational lower-bound composed by three cue-speciﬁc
multimodal modules, indicated by the description in the dashed
color boxes.

but also short phrases (e.g., noun phrases [28] and rela-
tions [47, 36]). However, the more general task: grounding
referring expressions [25], is still far from resolved due to
the challenges in understanding of both language and scene
compositions [10]. As illustrated in Figure 1, given an in-
put referring expression “largest elephant standing behind
baby elephant” and an image with region proposals, a model
that can only localize “elephant” is not satisfactory as there
are multiple elephants. Therefore, the key for referring ex-
pression grounding is to comprehend and model the con-
text. Here, we refer to context as the visual objects (e.g.,
“elephant”), attributes (e.g., “largest” and “baby”), and re-
lationships (e.g., “behind”) mentioned in the expression that
help to distinguish the referent from other objects.

One straightforward way of modeling the relations be-
tween the referent and context is to: 1) use external syn-
tactic parsers to parse the expression into entities, mod-
iﬁers, and relations [34], and then 2) apply visual rela-
tion detectors to localize them [47]. However, this two-
stage approach is not practical due to the limited general-
ization ability of the detectors applied in the highly unre-
stricted language and scene compositions. To this end, re-

1

cent approaches use multimodal embedding networks that
jointly comprehend language and model the visual rela-
tions [26, 11]. Due to the prohibitively high cost of an-
notating both referent and context of referring expressions
in images, multiple instance learning (MIL) [7] is usually
adopted in them to handle the weak supervision of the unan-
notated context objects, by maximizing the joint likelihood
of every region pair. However, for a referent, the MIL
framework essentially oversimpliﬁes the number of context
conﬁgurations of N regions from O(2N ) to O(N ). For ex-
ample, to localize the “elephant” in Figure 1, we may need
to consider the other three elephants all together as a multi-
nomial subset for modeling the context such as “largest”,
“behind” and “baby elephant”.

In this paper, we propose a novel model called Varia-
tional Context for grounding referring expressions in im-
ages. Compared to the previous MIL-based approaches [26,
11], our model approximates the combinatorial context
conﬁgurations with weak supervision using a variational
Bayesian framework [15]. Intuitively, it exploits the reci-
procity between referent and context, given either of which
can help to localize the other. As shown in Figure 1, for
each region x, we ﬁrst estimate a coarse context z, which
will help to reﬁne the true localizations of the referent. This
reciprocity is formulated into the variational lower-bound
of the grounding likelihood p(x|L), where L is the text
expression and the context is considered as a hidden vari-
able z (cf. Section 3). Speciﬁcally, the model consists of
three multimodal modules: context posterior q(z|x, L), ref-
erent posterior p(x|z, L), and context prior pz(z|L), each
of which performs a grounding task (cf. Section 4.3) that
aligns image regions with a cue-speciﬁc language feature;
each cue dynamically encodes different subsets of words in
the expression L that help the corresponding localization
(cf. Section 4.2).

Thanks to the reciprocity between referent and context,
our model can not only be used in the conventional super-
vised setting, where there is annotation for referent , but
also in the challenging unsupervised setting, where there
is no instance-level annotation (e.g., bounding boxes) of
both referent and context. We perform extensive experi-
ments on four benchmark referring expression datasets: Re-
fCLEF [14], RefCOCO [45], RefCOCO+ [45], and Ref-
COCOg [25]. Our model consistently outperforms previous
methods in both supervised and unsupervised settings. We
also qualitatively show that our model can ground the con-
text in the expression to the corresponding image regions
(cf. Section 5).

2. Related Work

Grounding Referring Expressions. Grounding refer-
ring expression is also known as referring expression com-
prehension, whose inverse task is called referring expres-

sion generation [25]. Different from grounding phrases [29,
28] and descriptive sentences [12, 32], the key for ground-
ing referring expression is to use the context (or pragmatics
in linguistics [37]) to distinguish the referent from other ob-
jects, usually of the same category [10]. However, most pre-
vious works resort to use holistic context such as the entire
image [25, 12, 32] or visual feature difference between re-
gions [45, 46]. Our model is similar to the works on explic-
itly modeling the referent and context region pairs [11, 26],
however, due to the lack of context annotation, they re-
duce the grounding task into a multiple instance learning
framework [7]. As we will discuss later, this framework
is not a proper approximation to the original task. There
are also studies on visual relation detection that detect ob-
jects and their relationships [21, 5, 47, 17, 48]. However,
they are limited to a ﬁxed-vocabulary set of relation triplets
and hence are difﬁcult to be applied in natural language
grounding. Our cue-speciﬁc language feature is similar to
the language modular network [11] that learns to decom-
pose a sentence into referent/context-related words, which
are different from other approaches that treat the expression
as a whole [25, 23, 46, 19].

Variational Bayesian Model vs. Multiple Instance
Learning. Our proposed variational context model is in a
similar vein of the deep neural network based variational
autoencoder (VAE) [15], which uses neural networks to
approximate the posterior distribution of the hidden value
q(z|x), i.e., encoder, and the conditional distribution of the
observation p(x|z), i.e., decoder. VAE shows efﬁcient and
effective end-to-end optimization for the intractable log-
sum likelihood log (cid:80)
z p(x, z) that is widely used in genera-
tive processes such as image synthesis [44] and video frame
prediction [43]. Considering the unannotated context as the
hidden variable z, the referring expression grounding task
can also be formulated into the above log-sum marginaliza-
tion (cf. Eq. (2)). The MIL framework [7] is essentially a
sum-log approximation of the log-sum, i.e., (cid:80)
z log p(x, z).
To see this, the max-pooling function log maxz p(x, z) used
in [11] can be viewed as the sum-log (cid:80)
z log p(x|z)p(z),
where p(z) = 1 if z is the correct context and 0 otherwise,
indicating there is only one positive instance; maximizing
the noisy-or function log(1 − (cid:81)
z(1 − p(x, z))) used in [26]
is equivalent to maximize (cid:80)
z log p(x, z), assuming there is
at least one positive instance. However, due to the numer-
ical property of the log function, this sum-log approxima-
tion will unnecessarily force every (x, z) pair to explain the
data [8]. Instead, we use the variational Bayesian upper-
bound to obtain a better sum-log approximation. Note
that visual attention models [2, 42] simplify the variational
lower bound by assuming p(z) = q(z|x); however, we
explicitly use the KL divergence KL(q(z|x)||p(z)) in the
lower bound to regularize the approximate posterior q(z|x)
not being too far from the prior p(z).

2

3. Variational Context

In this section, we derive the variational Bayesian for-
mulation of the proposed variational context model and the
objective function for training and test.

3.1. Problem Formulation

The task of grounding a referring expression L in an im-
age I, represented by a set of regions x ∈ X , can be viewed
as a region retrieval task with the natural language query
L. Formally, we maximize the log-likelihood of the condi-
tional distribution to localize the referent region x∗ ∈ X :

x∗ = arg max

log p(x|L),

(1)

x∈X

where we omit the image I in p(x|I, L).

As there is usually no annotation for the context, we
consider it as a hidden variable z. Therefore, Eq. (1)
can be rewritten as the following maximization of the log-
likelihood of the conditional marginal distribution:

x∗ = arg max

log

p(x, z|L).

(2)

x∈X

(cid:88)

z

Note that z is NOT necessary to be one region as assumed
in recent MIL approaches [11, 26], i.e., z ∈ X . For ex-
ample, the contextual objects “surrounding elephants” in
“a bigger elephant than the surrounding elephants” should
be composed by a multinomial subset of X , resulting in an
extremely large sample space that requires O(2|X |) search
complexity. Therefore, the marginalization in Eq (2) is in-
tractable in general.

To this end, we use the variational lower-bound [15] to

approximate the marginal distribution in Eq. (2) as:

log p(x|L) = log

p(x, z|L) ≥ Q(x, L) =

(cid:88)

z

Ez∼qφ(z|x,L) log pθ(x|z, L)
(cid:125)
(cid:123)(cid:122)
(cid:124)
Localization

− KL (qφ(z|x, L)||pω(z|L))
(cid:123)(cid:122)
(cid:125)
Regularization

(cid:124)

,

(3)

where KL(·) is the Kullback-Leibler divergence, φ, θ, and
ω are independent parameter sets for the respective distri-
butions. As shown in Figure 1, the lower bound Q(x, L)
offers a new perspective for exploiting the reciprocal nature
of referent and context in referring expression grounding:
Localization. This term calculates the localization score
for x given an estimated context z, using the referent-cue
of L parameterized by θ.
In particular, we design a new
posterior qφ(z|x, L) that approximates the true context prior
p(z|x, L), which models the context z using the context-cue
of L parameterized by φ. In the view of variational auto-
encoder [15, 35], this term works in an encoding-decoding
fashion: qφ is the encoder from x to z, and pθ is the decoder

from z to x.
Regularization. As KL is non-negative, maximizing
Q(x, L) would encourage that the posterior qφ is similar
to the prior pω, i.e., the estimated context z sampled from
qφ(z|x, L) should not be too far from the referring expres-
sion, which is modeled by pω(z|L) with the generic-cue of
L parameterized by ω. This term is necessary as the esti-
mated z could be overﬁtted to region features that are incon-
sistent with the visual context described in the expression.

3.2. Training and Test

Deterministic Context.

The lower-bound Q(x, L)
transforms the intractable log-sum in Eq. (2) into the efﬁ-
cient sum-log in Eq. (3), which can be optimized by us-
ing Monte Carlo unbiased gradient estimator such as RE-
INFORCE [40]. However, due to that φ is dependent on
the sampling of z over O(2|X |) conﬁgurations, its gradient
variance is large. To this end, we implement qφ(z|x, L) as
a differentiable but biased encoder:

z = f (x, L) =

x(cid:48) · qφ(x(cid:48)|x, L),

(4)

(cid:88)

x(cid:48)∈X

where we slightly abuse qφ as a score function such that
(cid:80)
x(cid:48) qφ(x(cid:48)|x, L) = 1. Note that this deterministic context
can be viewed as applying the “re-parameterization” trick as
in Variational Auto-Encoder [15]: rewriting z ∼ qφ(z|x, L)
to z = f (x, L; (cid:15)), (cid:15) ∼ p((cid:15)), where the stochasticity of the
auxiliary random variable (cid:15) comes from training samples
x ∈ X ((cid:15)). A clear example is Adversarial Autoencoder [24]
which shows that such stochasticity achieves similar test-
likelihood compared to other distributions such as Gaus-
sian.

Objective Function. Applying Eq. (4) to Eq. (3), we can
rewrite Q(x, L) into a function of only one sample estima-
tion, which is a common practice in SGD:

Q(x,L) = log pθ(x|z,L)−log qφ(z|x,L)+log pω(z|L).

(5)

is known,

In supervised setting where the ground truth of the ref-
erent
to distinguish the referent from other
objects, we need to train a model that outputs a high
p(x|L) (i.e., Q(x, L)), while maintaining a low p(x(cid:48)|L)
(i.e., Q(x(cid:48), L)), whenever x(cid:48)
(cid:54)= x. Therefore, we use
the so-called Maximum Mutual Information loss as in [25]
− log{Q(x, L)/ (cid:80)
x(cid:48) Q(x(cid:48), L)}, where we do not need to
explicitly model the distributions with normalizations; we
use the following score function:

Q(x, L) ∝ S(x, L) = sθ(x, L)−sφ(x, L)+sω(x, L), (6)

where z is omitted as it is a function of x in Eq. (4). sθ,
sφ, and sω are the score functions (e.g., pθ ∝ sθ) for pθ,
qφ, and pω, respectively. These functions will be detailed in

3

Figure 2. The architecture of the proposed Variational Context model. It consists of a region feature extraction module (Section 4.1, and
a language feature extraction module (Section 4.2), and three grounding modules (Section 4.3). It can be trained in an end-to-end fashion
with the input of a set of image regions and a referring expression, using the supervised loss ( Eq. (7)) or the unsupervised loss (Eq. (8)).
fc: fully-connected layer. concat: vector concatenation. L2Norm: L2 normalization layer. (cid:12): element-wise vector multiplication. ⊕: add.

Section 4.3. In this way, maximizing Eq. (5) is equivalent
to minimizing the following softmax loss:

Ls = − log softmax S(xgt, L),

(7)

where the softmax is over x ∈ X and xgt is the ground truth
referent region.

Note that the reciprocity between referent and context
can be extended to unsupervised learning, where neither of
the referent and context has annotation. In this setting, we
adopt the image-level max-pooled MIL loss functions for
unsupervised referring expression grounding:

Lu = − max
x∈X

log softmax S(x, L),

(8)

where the softmax is over x ∈ X . Note that the max-pooled
MIL function is reasonable since there is only one ground
truth referent given an expression and image training pair.

At test stage, in both supervised and unsupervised set-
tings, we predict the referent region x∗ by selecting the re-
gion x ∈ X with the highest score:

x∗ = arg max

S(x, L),

x∈X

(9)

4. Model Architecture

The overall architecture of the proposed variational con-
text model is illustrated in Figure 2. Thanks to the deter-
ministic context in Eq. (4), the ﬁve modules in our model
can be integrated into an end-to-end differentiable fashion.
Next, we will detail the implementation of each module.

4.1. RoI Features

Given an image with a set of Region of Interests (RoIs)
X , obtained by any off-the-shelf proposal generator [50] or
object detectors [20], this module extracts the feature vec-
tor xi for every RoI. In particular, xi is the concatenation
of visual feature vi and spatial feature pi. For vi, we can
use the output of a pre-trained convolutional network (cf.
Section 5). If the object category of each RoI is available,
we can further utilize the comparison between the referent

(cid:80)

and other objects to capture the visual difference such as
“the largest/baby elephant”. Speciﬁcally, we append the vi-
vi−vj
sual difference feature [45] δvi = 1
||vi−vj || to the
n
original vi visual feature, where n is the number of objects
chosen for comparison (e.g., the number of RoI in the same
object category). For spatial feature, we use the 5-d spatial
W , ybr
attributes pi = [ xtl
W ·H ], where x and y are
the coordinates the top left (tl) and bottom right (br) RoI of
the size w × h, and the image is of the size W × H.

H , w·h

H , xbr

W , ytl

j(cid:54)=i

4.2. Cue-Speciﬁc Language Features

The cue-speciﬁc language feature representation for a re-
ferring expression is inspired by the attention weighted sum
of word vectors [11, 22, 3], where the weights are param-
eterized by context-cue φ, referent-cue θ, and generic-cue
ω. The context-cue language feature yc = [yc1, yc2] is
a concatenation of yc1 for language-vision association be-
tween single RoI and the expression, and yc2 for the as-
sociation between pairwise RoIs; the referent-cue language
feature yr can be represented in a similar way to yc; the
generic-cue language feature yg is only for single RoI asso-
ciation as it is an independent prior. The weights of each cue
are calculated from the hidden state vectors of a 2-layer bi-
directional LSTM (BLSTM) [33], scanning through the ex-
pression. The hidden states encode forward and backward
compositional semantic meanings of the sentences, beneﬁ-
cial for selecting words that are useful for single and pair-
wise associations. Speciﬁcally, suppose hj as the 4,000-d
concatenation of forward and backward hidden vectors of
the j-th word, without loss of generality, the word attention
weight αj and the language feature y for single/pairwise
association of any cue can be calculated as:

mj = fc(hj), αj = softmaxj(mj), y =

αjwj, (10)

(cid:88)

j

where wj is a 300-d vector. Note that the BLSTM module
can be jointly trained with the entire model.

Figure 3 shows that the cue-speciﬁc language features
dynamically weight words in different expressions. We can
have two interesting observations. First, c1 is almost uni-

4

Figure 3. Two qualitative examples of the cue-speciﬁc language
feature word weights. Darker color indicates higher weights.
c/r+1/2: context/referent-cue + single/pairwise.

form while c2 is highly skewed; although r2 is more skewed
than c1, it is still less skewed than r1. This is reasonable
since: 1) without ground-truth, individual score (c1) does
not help much for context estimation from scratch; context
is more easily found by the pairwise score (c2) induced by
relationships or other objects (e.g., “left” or “frisbee”); 2) in
referent grounding with ground truth, individual score (r1)
is sufﬁcient (e.g., “dog lying” and “black white dog”) and
pairwise score (r2) is helpful; 3) g is adaptive to the num-
ber of object categories in the expression, i.e., if the context
object is of the same category as the referent, g weighs de-
scriptive or relationship words higher (e.g., “lying, stand-
ing, left”), and nouns higher (e.g., “frisbee”), otherwise;
moreover, it demonstrates that the deterministic guess of z
in Eq. (4) is meaningful.

4.3. Score Functions

For any image and expression pair, given the RoI feature
xi, and the cue-speciﬁc language feature yc, yr, and yg, we
implement the ﬁnal grounding score in Eq. (6) as:

zi =

(cid:88)
j

softmaxj (sφ(xi, xj, yc)) xj,

sθ(x, L) ← sθ(xi, zi, yr),
sφ(x, L) ← sφ(xi, zi, yc),
sω(x, L) ← sω(zi, yg),

(11)

where the right-hand side functions are deﬁned as below.

Context Estimation Score: sφ(xi, xj, yc). It is a score
function for modeling the context posterior qφ(z|x, L), i.e.,
given an RoI xi as the candidate referent, we calculate the
likelihood of any RoI xj to be the context. We can also
use this function to estimate the ﬁnal context posterior score
sφ(xi, zi, yc). Speciﬁcally, the context estimation score is a
sum of the single and pairwise vision-language association
scores: xj and yc1, [xi, xj] and yc2. Each associate score
is an fc output from the input of a normalized feature:

j = yc1 (cid:12) fc(xj), m2
m1
(cid:101)m1
j ), (cid:101)m2
j = L2Norm(m1
sφ(xi, xj, yc) = fc( (cid:101)m1

j = yc2 (cid:12) fc([xi, xj]),
j = L2Norm(m2
j ) + fc( (cid:101)m2
j ),

j ),

(12)

obtain the estimated context z as zi = (cid:80)
βj = softmaxj(sφ(xi, xj, yc)).

j βjxj, where

Referent Grounding Score: sθ(xi, zi, yr). After ob-
taining the context feature zi, we can use this score function
to calculate how likely a candidate RoI xi is the referent
given the context zi. This function is similar to Eq. (12).

Score:

Context Regularization

sω(zi, yg) −
sφ(xi, zi, yc). As discussed in Eq. (6),
this function
scores how likely the estimated context feature zi
is
consistent with the content mentioned in the expression. In
particular, sω(zi, yg) is only dependent on single RoI:
i (cid:12) fc(zi), (cid:101)mi=L2Norm(mi), sω(zi, yg
mi=yg

i )=fc(mi).
(13)

5. Experiment

5.1. Datasets

We used four popular benchmarks for the referring ex-

pression grounding task.

RefCOCO [45]. It has 142,210 referring expressions for
50,000 referents (e.g., object instances) in 19,994 images
from MSCOCO [18]. The expressions are collected in an
interactive way [14]. The dataset is split into train, valida-
tion, Test A, and Test B, which has 120,624, 10,834, 5,657
and 5,095 expression-referent pairs, respectively. An image
contains multiple people in Test A and multiple objects in
Test B.

RefCOCO+ [45]. It has 141,564 expressions for 49,856
referents in 19,992 images from MSCOCO. The difference
from RefCOCO is that it only allows appearances but no
locations to describe the referents. The split is 120,191,
10,758, 5,726 and 4,889 expression-referent pairs for train,
validation, Test A, and Test B respectively.

RefCOCOg [25]. It has 95,010 referring expressions for
49,822 objects in 25,799 images from MSCOCO. Different
from RefCOCO and RefCOCO+, this dataset not collected
in an interactive way and contains longer sentences contain-
ing both appearance and location expressions. The split is
85,474 and 9,536 expression-referent pairs for training and
validation. Note that there is no open test split for Ref-
COCOg, so we used the hyper-parameters cross-validated
on RefCOCO and RefCOCO+.

RefCLEF [14]. It contains 20,000 images with anno-
tated image regions. It has some ambiguous (e.g. anywhere)
phrases and mistakenly annotated image regions that are not
described in the expressions. For fair comparison, we used
the split released by [12, 32], i.e., 58,838, 6,333 and 65,193
expression-referent pairs for training, validation and test, re-
spectively.

5.2. Settings and Metrics

where the element-wise multiplication (cid:12) is an effective way
for multimodal features [2]. According to Eq. (4), we can

We used an English vocabulary of 72,704 words con-
tained in the GloVe pre-trained word vectors [27], which

5

Table 1. Supervised grounding performances (Acc%) of comparing methods on RefCOCO, RefCOCO+, and RefCOCOg. Note that [46]
reports slightly higher accuracies using ensemble models of Listener and Speaker. For fair comparison, we only report their single models.

Dataset

Split MMI [25] NegBag [26] Attr [19] CMN [11]

Speaker [46]

Listener [46] VC w/o reg VC w/o α

VC

State-of-The-Arts

Our Baselines

RefCOCO

RefCOCO+

RefCOCOg

Val

RefCOCO(det)

RefCOCO+(det)

RefCOCOg(det)

Val

Test A

Test B

Test A

Test B

Test A

Test B

Test A

Test B

71.72

71.09

58.42

51.23

62.14

64.90

54.51

54.03

42.81

45.85

75.6

78.0

—

—

68.4

58.6

56.4

—

—

39.5

78.85

78.07

61.47

57.22

69.83

72.08

57.29

57.97

46.20

52.35

75.94

79.57

59.29

59.34

69.30

71.03

65.77

54.32

47.76

57.47

78.95

80.22

64.60

59.62

72.63

72.95

63.43

60.43

48.74

59.51

78.45

80.10

63.34

58.91

72.25

72.95

62.98

59.61

48.44

58.32

75.59

79.69

60.76

60.14

71.05

70.78

65.10

56.82

51.30

60.95

74.03

78.27

57.61

54.37

65.13

70.73

64.63

53.33

46.88

55.72

78.98

82.39

62.56

62.90

73.98

73.33

67.44

58.40

53.18

62.30

Figure 4. Qualitative results on RefCOCOg (det) showing comparisons between correct (green tick) and wrong referent grounds (red cross)
by VC and CMN. The denotations of the bounding box colors are as follows. Solid red: referent ground; solid green: ground truth; dashed
yellow: context ground. We only display top 3 context objects with the context ground probability > 0.1. We can observe that VC has
more reasonable context localizations than CMN, even in cases when the referent ground of VC fails.

was also used for the initialization of our word vectors. We
used a “unk” symbol for the input word of the BLSTM if the
word is out of the vocabulary; we set the sentence length to
20 and used “pad” symbol to pad expression sentence < 20.
For RoI visual features on RefCOCO, RefCOCO+, and Re-
fCOCOg which have MSCOCO annotated regions with ob-
ject categories, we used the concatenation of the 4,096-d
fc7 output of a VGG-16 based Faster-RCNN network [31]
trained on MSCOCO and its corresponding 4,096-d visd-
iff feature [45]; although RefCLEF regions also have object

categories, for fair comparison with [32], we did not use the
visdiff feature.

The model training is single-image based, with all re-
ferring expressions annotated. We applied SGD of 0.95-
momentum with initial learning rate of 0.01, multiplied by
0.1 after every 120,000 iterations, up to 160,000 iterations.
Parameters in BILSTM and fc-layers were initialized by
Xavier [9] with 0.0005 weight decay. Other settings were
default in TensorFlow. Note that our model is trained with-
out bells and whistles, therefore, other optimization tricks

6

such as batch normalization [13] and GRU [4] are expected
to further improve the results reported here. Besides the
ground truth annotations, grounding to automatically de-
tected objects is a more practical setting. Therefore, we also
evaluated with the SSD-detected bounding boxes [20] on
the four datasets provided by [46]. A grounding is consid-
ered as correct if the intersection-over-union (IoU) of the
top-1 scored region and the ground-truth object is larger
than 0.5. The grounding accuracy (a.k.a, P@1) is the frac-
tion of correctly grounded test expressions.

5.3. Evaluations of Supervised Grounding

We compared our variational context model (VC) with
state-of-the-art referring expression methods published in
recent years, which can be categorized into: 1) generation-
comprehension based such as MMI
[19],
Speaker [46], Listener [46], and SCRC [12]; 2) localiza-
tion based such as GroundR [32], NegBag [26], CMN [11].
Note that NegBag and CMN are MIL-based models. In par-
ticular, we used the author-released code to obtain the re-
sults of CMN on RefCLEF, RefCOCO, and RefCOCO+.

[25], Attr

From the results on RefCOCO, RefCOCO+, and Re-
fCOCOg in Table 1 and that on RefCLEF in Table 2,
we can see that VC achieves the state-of-the-art perfor-
mance. We believe that the improvement is attributed to
the variational Bayesian modeling of context. First, on all
datasets, except for the most recent reinforcement learn-
ing based [46], VC outperforms all the other sentence
generation-comprehension methods that do not model con-
text. Second, compared to VC without the regularization
term in Eq. (3) (VC w/o reg), VC can boost the performance
by around 2% on all datasets. This demonstrates the ef-
fectiveness of the KL divergence for the prevention of the
overﬁtted context estimation.

In particular, we further demonstrate the superiority of
VC over the most recent MIL-based method CMN. As il-
lustrated in Figure 4, VC has better context comprehension
in both of the language and image regions than CMN. For
example, in the top two rows where VC is correct and CMN
is wrong, for the grounding in the second column, CMN
unnecessarily considers the “girl” as context but the expres-
sion only describes using “elephant”; in the last column,
CMN misses the key context “frisbee”. Even in the fail-
ure cases where VC is wrong and CMN is correct, VC still
localizes reasonable context. For example, in the fourth col-
umn, although CMN grounds the correct TV, but it is based
on incorrect context of other TVs; while VC can predict
the correct context “children”.
In addition, we observed
that most of the cases that CMN is better than VC involves
multiple humans. This demonstrates that VC is better at
grounding objects of different categories.

VC is also effective in images with more objects. Fig-
ure 5 shows the performances of VC and CMN with various

Table 2. Performances (Acc%) of supervised and unsupervised
methods on RefCLEF.

Sup.

Sup. (det) Unsup. (det)

SCRC [12]

72.74

GroundR [32] —

CMN [11]

VC

VC w/o α

81.52

82.43

79.60

17.93

26.93

28.33

31.13

27.40

10.70

—

—

14.11

14.50

Table 3. Unsupervised grounding performances (Acc%) of com-
paring methods on RefCOCO, RefCOCO+, and RefCOCOg.
VC w/o α

VC w/o reg

Dataset

Split

VC

RefCOCO

RefCOCO+

RefCOCOg

Val

RefCOCO(det)

RefCOCO+(det)

RefCOCOg(det)

Val

Test A

Test B

Test A

Test B

Test A

Test B

Test A

Test B

13.59

21.65

18.79

24.14

25.14

17.14

22.30

19.74

24.05

28.14

17.34

20.98

23.24

24.91

33.79

20.91

21.77

25.79

25.54

33.66

33.29

30.13

34.60

31.58

30.26

32.68

27.22

34.68

28.10

29.65

number of bounding boxes. We can observe that VC con-
siderably outperforms CMN over all bounding boxes num-
bers. Recall that context is the key to distinguish objects of
the same category. In particular, on the Test A sets of Ref-
COCO and RefCOCO+ where the grounding is only about
people, i.e., the same object category, the gap between VC
and CMN is becoming larger as the box number increases.
This demonstrates that MIL is ineffective in modeling con-
text, especially when the number of image regions is large.

5.4. Evaluations of Unsupervised Grounding

We follow the unsupervised setting in GroundR [32]. To
our best knowledge, it is the only work on unsupervised re-
ferring expression grounding. Note that it is also known as
“weakly supervised” detection [48] as there is still image-
level ground truth (i.e., the referring expression). Table 2 re-
ports the unsupervised results on the RefCLEF. We can see
that VC outperforms the state-of-the-art GroundR, which is
a generation-comprehension based method. This demon-
strates that using context also helps unsupervised ground-
ing. As there is no published unsupervised results on Re-
fCOCO, RefCOCO+, and RefCOCOg, we only compared
our baselines on them in Table 3. We can have the follow-
ing three key observations which highlight the challenges
of unsupervised grounding:

Context Prior. VC w/o reg is the baseline without the
KL divergence as a context regularization in Eq. (3). We can
see that in most of the cases, VC considerably outperforms
VC w/o reg by over 2%, even over 5% on RefCOCO+ (det)

7

Figure 5. Performances of VC and CMN with different number of object bounding boxes on RefCOCO Test A &B, RefCOCO+ Test A &
B, and RefCOCOg Val. Compared to CMN, we can see that VC is more effective in context modeling when the number of objects is large.

Figure 6. Common failure cases in unsupervised grounding with
detected bounding boxes. From left to right: RefCOCO, Ref-
COCO+, and RefCOCOg. The failure is mainly to the challenging
unsupervised relation modeling between referent and context.

and RefCOCOg (det). Note that this improvement is signif-
icantly higher than that in supervised setting (e.g., < 3% as
reported in Table 1). The reason is that the context estima-
tion in Eq. (4) would be easier to be stuck in image regions
that are irrelevant to the expression in unsupervised setting,
therefore, context prior is necessary.

Language Feature. Except on RefCOCOg, we consis-
tently observed the ineffectiveness of the cue-speciﬁc lan-
guage feature in unsupervised setting, i.e., VC w/o α out-
performs VC in Table 2 and 3. Here α represents the cue-
speciﬁc word attention. This is contrary to the observation
in the supervised setting as listed in Table 1, where VC w/o
α is consistently lower than VC. Note that without the cue-
speciﬁc word attention α in Eq. (10), the language feature is
merely the average value of the word embedding vectors in
the expression. In this way, VC w/o α does not encode any
structural language composition as illustrated in Figure 3,
thus, it is better for short expressions. However, when the
expression is long in RefCOCOg, discarding the language
structure still degrades the performance on RefCOCOg.

Unsupervised Relation Discovery.

Although we
demonstrated that VC improves the unsupervised ground-
ing by modeling context, we believe that there is still a large
space for improving the quality of modeling the context.
As the failure examples shown in Figure (6), 1) many con-
text estimations are still out of the scope of the expression,
e.g., we may localize the “cup” and “table” as context even
though the expression is “woman with green t-shirt”; 2) we

8

Figure 7. Word cloud visualizations of cue-speciﬁc word attention
α in Eq. 10 of context-cue (c2), referent-cue (r1), and generic-cue
(g) using supervised (top row) and unsupervised training (bottom
row) on RefCOCOg. Without supervision, it is difﬁcult to discover
meaningful language compositions.

may mistake due to the wrong comprehension of the rela-
tions, e.g., “right” as “left”, even if the objects belong to
the same category, e.g., “elephant”. For further investiga-
tion, Figure 7 visualizes the cue-speciﬁc word attentions in
supervised and unsupervised settings. The almost identi-
cal word attentions in unsupervised setting reﬂect the fact
that the relation modeling between referent and context is
not as successful as in supervised setting. This inspires us
to exploit stronger prior knowledge such as language struc-
ture [41] and spatial conﬁgurations [48, 39].

6. Conclusions

We focused on the task of grounding referring expres-
sions in images and discussed that the key problem is how
to model the complex context, which is not effectively re-
solved by the multiple instance learning framework used
in prior works. Towards this challenge, we introduced
the Variational Context model, where the variational lower-
bound can be interpreted by the reciprocity between the ref-
erent and context: given any of which can help to localize
the other, and hence is expected to signiﬁcantly reduce the
context complexity in a principled way. We implemented
the model using cue-speciﬁc language-vision embedding
network that can be efﬁciently trained end-to-end. We val-
idated the effectiveness of this reciprocity by promising
supervised and unsupervised experiments on four bench-
marks. Moving forward, we are going to 1) incorporate ex-
pression language generation in the variational framework,
2) use more structural features of language rather than word
attentions, and 3) further investigate the potential of our
model in the unsupervised referring expression grounding.

the visual grounding task [11]. Therefore, we adopt the
parser jointly trained on the referring expression ground-
ing task [11]. As illustrated in Figure 8, this parser assigns
word-level attention weights of subject, relation, and object.
In particular, we consider the language features of c1, r2 as
the average word embeddings, c2 as the relation weights,
r1 as the subject weights, g as the object weights. Table 4
shows the performances on unsupervised grounding. We
can see that there is no signiﬁcant improvement of VC w/
parser over VC w/o α.

7.5. More Qualitative Results

Figure 10 shows more qualitative results on supervised
and unsupervised grounding results on RefCOCO, Ref-
COCO+, and RefCOCOg.

References

[1] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zit-
nick, and D. Parikh. Vqa: Visual question answering. In ICCV, 2015.
1

[2] J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition

with visual attention. In ICLR, 2015. 2, 5

[3] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by

jointly learning to align and translate. 2015. 4

[4] K. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio. On
the properties of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259, 2014. 7

[5] B. Dai, Y. Zhang, and D. Lin. Detecting visual relationships with

deep relational networks. In CVPR, 2017. 2

[6] A. Das, S. Kottur, J. M. Moura, S. Lee, and D. Batra. Learning
cooperative visual dialog agents with deep reinforcement learning.
In ICCV, 2017. 1

[7] T. G. Dietterich, R. H. Lathrop, and T. Lozano-P´erez. Solving the
multiple instance problem with axis-parallel rectangles. Artiﬁcial
intelligence, 1997. 2

[8] C. W. Fox and S. J. Roberts. A tutorial on variational bayesian infer-

[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of training

deep feedforward neural networks. In ICAIS, 2010. 6

[10] D. Golland, P. Liang, and D. Klein. A game-theoretic approach to

generating spatial descriptions. In EMNLP, 2010. 1, 2

[11] R. Hu, M. Rohrbach, J. Andreas, T. Darrell, and K. Saenko. Model-
ing relationships in referential expressions with compositional mod-
ular networks. In CVPR, 2017. 2, 3, 4, 6, 7

[12] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell.

Natural language object retrieval. In CVPR, 2016. 2, 5, 7

[13] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep
network training by reducing internal covariate shift. In ICML, 2015.
7

[14] S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg. Refer-
itgame: Referring to objects in photographs of natural scenes.
In
EMNLP, 2014. 2, 5

[15] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In

ICLR, 2014. 2, 3

[16] Y. Li, N. Duan, B. Zhou, X. Chu, W. Ouyang, X. Wang, and M. Zhou.
Visual question generation as dual task of visual question answering.
In CVPR, 2018. 1

[17] Y. Li, W. Ouyang, and X. Wang. Vip-cnn: Visual phrase guided

convolutional neural network. In CVPR, 2017. 2

Figure 8. Some parsing examples from [11].

7. Supplementary Material

7.1. Derivation of Eq. (3)

Recall that we are going to transform the log-sum ob-
jective function in Eq. (2) to sum-log for tractable training.
Without loss of generality, we omit the conditional L in the
derivation. By using the concavity of the log function:

log((1 − α)x + αy) ≥ (1 − α) log(x) + α log(y),

(14)

to the following rewriting of Eq. (2):

(cid:88)

log

p(x, z) = log

(cid:32)

(cid:88)

z

(cid:33)

q(z|x)p(x, z)
q(z|x)

q(z|x) log

p(x, z)
q(z|x)

q(z|x) log p(x|z)p(z) −

q(z|x) log q(z|x)

q(z|x) log p(x|z) +

q(z|x) log p(z)

(cid:88)

=

q(z|x) log p(x|z) −

q(z|x)

(cid:88)

q(z|x)
p(z)

= Ez∼q(z|x) log p(x|z) − KL (q(z|x)||p(z)) = Q(x)

(15)

7.2. More Examples on Language Features

Figure 9 shows more cue-speciﬁc language features on

RefCOCOg.

7.3. More Results on Unsupervised Grounding

7.4. External Parsers

As discussed in Section 5.4 that the language feature in
unsupervised VC is not as good as that in the supervised
setting. An alternative is to use external NLP parsers to ob-
tain the compositions. However, conventional parsers (e.g.,
Standford Dependency) are observed to be suboptimal to

(cid:88)

z

(cid:88)

(cid:88)

−

z

z

z

z
(cid:88)

z
(cid:88)

z
(cid:88)

≥

=

=

z

z

9

q(z|x) log q(z|x)

ence. Artiﬁcial intelligence review, 2012. 2

Table 4. Unsupervised grounding performances (%) of VC w/ or w/o parsers on the four datasets.
RefCLEF RefCOCO TestA RefCOCO TestB

RefCOCO+TestA

RefCOCO+TestB

RefCOCOg

VC w/ parser

GT

VC

VC w/o α

VC w/ parser

VC

VC w/o α

22.57

21.06

20.72

14.90

14.11

14.50

23.00

17.34

33.29

24.07

20.91

32.68

27.50

20.98

30.13

25.12

21.77

27.22

24.69

23.24

34.60

26.18

25.79

34.68

28.96

24.91

31.58

26.92

25.54

28.10

30.73

33.79

30.26

30.64

33.66

29.65

DET

RefCLEF RefCOCO TestA RefCOCO TestB RefCOCO+ TestA RefCOCO+ TestB RefCOCOg

Figure 9. Qualitative results of the cue-speciﬁc language features on RefCOCOg.

[18] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick. Microsoft coco: Common objects in
context. In ECCV, 2014. 5

[19] J. Liu, L. Wang, and M.-H. Yang. Referring expression generation

and comprehension via attributes. In ICCV, 2017. 2, 6, 7

[20] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and
A. C. Berg. Ssd: Single shot multibox detector. In ECCV, 2016. 4, 7
[21] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual relationship

10

Figure 10. Qualitative results on RefCOCO, RefCOCO, RefCOCO+, and RefCOCOg. All of them are from det bounding boxes. The ﬁrst
line denotes the supervised results and the second line denotes the unsupervised results. Solid red: referent grounding; solid green: ground
truth; dashed yellow: context grounding. We only display top 3 context objects with the context grounding probability > 0.1. Please zoom
in.

detection with language priors. In ECCV, 2016. 2

[22] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical question-image
co-attention for visual question answering. In NIPS, 2016. 4
[23] R. Luo and G. Shakhnarovich. Comprehension-guided referring ex-

pressions. In CVPR, 2017. 2

[24] A. Makhzani, J. Shlens, N. Jaitly, and I. J. Goodfellow. Adversarial

autoencoders. In ICLR Workshop, 2016. 3

[25] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Mur-
phy. Generation and comprehension of unambiguous object descrip-
tions. In CVPR, 2016. 1, 2, 3, 5, 6, 7

[26] V. K. Nagaraja, V. I. Morariu, and L. S. Davis. Modeling context
between objects for referring expression understanding. In ECCV,
2016. 2, 3, 6, 7

[29] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hock-
enmaier, and S. Lazebnik. Flickr30k entities: Collecting region-
to-phrase correspondences for richer image-to-sentence models. In
ICCV, 2015. 2

[30] J. Redmon and A. Farhadi. Yolo9000: better, faster, stronger.

In

CVPR, 2017. 1

[31] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-
time object detection with region proposal networks. In NIPS, 2015.
6

[32] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and B. Schiele.
Grounding of textual phrases in images by reconstruction. In ECCV,
2016. 2, 5, 6, 7

[33] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural net-

[27] J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for

works. TSP, 1997. 4

word representation. In EMNLP, 2014. 5

[28] B. A. Plummer, A. Mallya, C. M. Cervantes, J. Hockenmaier, and
S. Lazebnik. Phrase localization and visual relationship detection
with comprehensive linguistic cues. In ICCV, 2017. 1, 2

[34] S. Schuster, R. Krishna, A. Chang, L. Fei-Fei, and C. D. Manning.
Generating semantically precise scene graphs from textual descrip-
tions for improved image retrieval. In Workshop on Vision and Lan-
guage, 2015. 1

11

[35] K. Sohn, H. Lee, and X. Yan. Learning structured output representa-
tion using deep conditional generative models. In NIPS, 2015. 3
[36] Q. Sun, B. Schiele, and M. Fritz. A domain based approach to social

relation recognition. In CVPR, 2017. 1

[37] J. A. Thomas. Meaning in interaction: An introduction to pragmat-

ics. Routledge, 2014. 2

[38] J. Thomason, J. Sinapov, and R. Mooney. Guiding interaction be-
haviors for multi-modal grounded language learning. In Proceedings
of the First Workshop on Language Grounding for Robotics, pages
20–24, 2017. 1

[39] Y. Wei, J. Feng, X. Liang, C. Ming-Ming, Y. Zhao, and S. Yan. Ob-
ject region mining with adversarial erasing: A simple classiﬁcation
to semantic segmentation approach. In CVPR, 2017. 8

[40] R. J. Williams. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. In Machine Learning. 1992. 3
[41] F. Xiao, L. Sigal, and Y.-J. Lee. Weakly-supervised visual grounding

of phrases with linguistic structures. In CVPR, 2017. 8

[42] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,
R. Zemel, and Y. Bengio. Show, attend and tell: Neural image cap-
tion generation with visual attention. In ICML, 2015. 2

[43] T. Xue, J. Wu, K. Bouman, and B. Freeman. Visual dynamics: Prob-
abilistic future frame synthesis via cross convolutional networks. In
NIPS, 2016. 2

[44] X. Yan, J. Yang, K. Sohn, and H. Lee. Attribute2image: Conditional

image generation from visual attributes. In ECCV, 2016. 2

[45] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling

context in referring expressions. In ECCV, 2016. 2, 4, 5, 6

[46] L. Yu, H. Tan, M. Bansal, and T. L. Berg. A joint speaker-listener-

reinforcer model for referring expressions. In ICCV, 2017. 2, 6, 7

[47] H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua. Visual translation
embedding network for visual relation detection. In CVPR, 2017. 1,
2

[48] H. Zhang, Z. Kyaw, J. Yu, and S.-F. Chang. Ppr-fcn: Weakly super-
vised visual relation detection via parallel pairwise r-fcn. In ICCV,
2017. 2, 7, 8

[49] Z. Zhao, Q. Yang, D. Cai, X. He, and Y. Zhuang. Video question
answering via hierarchical spatio-temporal attention networks. In In-
ternational Joint Conference on Artiﬁcial Intelligence (IJCAI), vol-
ume 2, 2017. 1

[50] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object proposals

from edges. In ECCV, 2014. 4

12

