The End of Optimism?
An Asymptotic Analysis of Finite-Armed Linear Bandits

6
1
0
2
 
t
c
O
 
4
1

 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
1
9
4
4
0
.
0
1
6
1
:
v
i
X
r
a

Tor Lattimore
Indiana University, Bloomington

Csaba Szepesv´ari
University of Alberta, Edmonton

Abstract

regret), which is given by

Stochastic linear bandits are a natural and
simple generalisation of ﬁnite-armed bandits
with numerous practical applications. Cur-
rent approaches focus on generalising existing
techniques for ﬁnite-armed bandits, notably
the optimism principle and Thompson sam-
pling. While prior work has mostly been in the
worst-case setting, we analyse the asymptotic
instance-dependent regret and show matching
upper and lower bounds on what is achiev-
able. Surprisingly, our results show that no al-
gorithm based on optimism or Thompson sam-
pling will ever achieve the optimal rate, and
indeed, can be arbitrarily far from optimal,
even in very simple cases. This is a disturb-
ing result because these techniques are stan-
dard tools that are widely used for sequential
optimisation. For example, for generalised lin-
ear bandits and reinforcement learning.

1 INTRODUCTION

|A|

∈ A

= k and

The linear bandit is the simplest generalisation of the
Rd be a ﬁnite set that spans
ﬁnite-armed bandit. Let
A ⊂
Rd with
. A learner
1 for all x
x
k2 ≤
k
interacts with the bandit over n rounds. In each round
and ob-
t the learner chooses an action (arm) At ∈ A
serves a payoff Yt =
+ ηt where ηt ∼ N
(0, 1)
At, θ
i
h
Rd is an unknown param-
is Gaussian noise and θ
∈
eter. The optimal action is x∗ = arg maxx∈A h
,
i
which is not known since it depends on θ. The assump-
tion that
)
A
has rank r < d, then one can simply use a different basis
for which all but r coordinates are always zero and then
drop them from the analysis. The Gaussian assumption
can be relaxed to 1-subgaussian for our upper bound, but
is needed for the lower bound. Our performance mea-
sure is the expected pseudo-regret (from now on just the

spans Rd is non-restrictive, since if span(

x, θ

A

Rπ

θ (n) = E

x∗
h

−

At, θ

,

i#

n

"

t=1
X

where the expectation is taken with respect to the ac-
tions of the strategy and the noise. There are a num-
ber of algorithms designed for minimising the regret,
all of which use one of two algorithmic designs. The
ﬁrst is the principle of optimism in the face of uncer-
tainty, which was originally applied to ﬁnite-armed ban-
dits by Agrawal [1995], Katehakis and Robbins [1995],
Auer et al. [2002] and many others, and more re-
cently to linear bandits [Auer, 2002, Dani et al., 2008,
Abbasi-Yadkori et al., 2011, 2012]. The second algo-
rithm design is Thompson sampling, which is an old al-
gorithm [Thompson, 1933] that has experienced a resur-
gence in popularity because of its impressive practi-
cal performance and theoretical guarantees for ﬁnite-
armed bandits [Kaufmann et al., 2012, Korda et al.,
2013]. Thompson sampling has also recently been ap-
plied to linear bandits with good empirical performance
[Chapelle and Li, 2011] and near-minimax theoretical
guarantees [Agrawal and Goyal, 2013].

While both approaches lead to practical algorithms
(especially Thompson sampling), we will show they
are fundamentally ﬂawed in that algorithms based on
these ideas cannot be close to asymptotically optimal.
Along the way we characterise the optimal achievable
asymptotic regret and design a strategy achieving it.
This is an important message because optimism and
Thompson sampling are widely used beyond the ﬁnite-
armed case. Examples include generalised linear ban-
dits [Filippi et al., 2010], spectral bandits [Valko et al.,
2014], and even learning in Markov decision processes
[Auer et al., 2010, Gopalan and Mannor, 2015].

The disadvantages of these approaches is obscured in the
worst-case regime, where both are quite close to opti-
mal. One might question whether or not the asymp-
totic analysis is relevant in practice. The gold stan-
dard would be instance-dependent ﬁnite-time guarantees

like what is available for ﬁnite-armed bandits, but his-
torically the asymptotic analysis has served as a use-
ful guide towards understanding the trade-offs in ﬁnite-
time. Besides hiding the structure of speciﬁc problems,
pushing for optimality in the worst-case regime can also
lead to sub-optimal instance-dependent guarantees. For
example, the MOSS algorithm for ﬁnite-armed bandits
is minimax optimal, but far from ﬁnite-time optimal
[Audibert and Bubeck, 2009]. For these reasons we be-
lieve that understanding the asymptotics of a problem is
a useful ﬁrst step towards optimal ﬁnite-time instance-
dependent guarantees that are most desirable.

It is worth mentioning that partial monitoring (a more
complicated online learning setting) is a well known ex-
ample of the failure of optimism [Bart´ok et al., 2014].
Although related, the partial monitoring framework is
more general than the bandit setting because the learner
may not observe the reward even for the action they
take, which means that additional exploration is usually
necessary in order to gain information. Basic results
in partial monitoring are concerned with characterizing
whether an instance is easier or harder than bandit in-
stances. More recently, the question of asymptotic in-
stance optimality was studied in ﬁnite stochastic partial
monitoring [Komiyama et al., 2015], and the special set-
ting of learning with side information [Wu et al., 2015].
While the algorithms derived in these works served as
inspiration, the analysis and the algorithms do not gener-
alise in a simple direct fashion to the linear setting, which
requires a careful study of how information is transferred
between actions in a linear setting.

2 NOTATION

−

(cid:23)

∈ A

x
k
k

is µx =

0) and
For positive semideﬁnite G (written as G
2
G = x⊤Gx. The Euclidean
vector x we write
x
k
k
Rd is
and the spectral
norm of a vector x
∈
norm of a matrix A is
. The pseudo-inverse of a
A
k
k
matrix A is denoted by A†. The mean of arm x

µx and ∆min = min

∈
and the optimal mean is µ∗ =
x, θ
A
i
h
maxx∈A µx. Let x∗
be any optimal action such
that µx∗ = µ∗. The sub-optimality gap of arm x is ∆x =
µ∗
and
∆x : ∆x > 0, x
{
. The number of times arm
∆max = max
∈ A}
x has been chosen after round t is denoted by Tx(t) =
. A
policy π is consistent if for all θ and p > 0 it holds that
P
Rπ
θ (n) =
O(np) and also to lim supn→∞ log(Rπ
≤
0. When more appropriate, we will use the more pre-
cise Landau notation an ∈
O(bn) (also with Ω, o and
ω). Vectors in Rk will often be indexed by the action set,
which we assume has an arbitrary ﬁxed order. For ex-

θ (n) = o(np). Note that this is equivalent to Rπ

θ (n))/ log(n)

At = x
}
{

∆x : x
{

and T∗(t) =

µAt = µ∗

∈ A}

t
s=1

t
s=1

P

1

1

{

}

Rk and refer to αx ∈

∈

R for

ample, we might write α
.
some x
∈ A

3 LOWER BOUND

We note ﬁrst that the ﬁnite-armed UCB algorithm of
Agrawal [1995], Katehakis and Robbins [1995] can be
used on this problem by disregarding the structure on the
arms to achieve an asymptotic regret of

lim sup
n→∞

RUCB
(n)
θ
log(n)

=

2
∆x

.

Xx∈A:∆x>0

This quantity depends linearly on the number of subop-
timal arms, which may be very large (much larger than
the dimension) and is very undesirable. Nevertheless we
immediately observe that the asymptotic regret should
be logarithmic. The following theorem and its corollary
characterises the optimal asymptotic regret.

Theorem 1. Fix θ
mal arm. Let π be a consistent policy and let

Rd such that there is a unique opti-

∈

¯Gn = E

n

"

AtA⊤
t

,

#

t=1
X
which we assume is invertible for sufﬁciently large n.
Then for all suboptimal x

it holds that

∈ A

lim sup
n→∞

log(n)

x

k

−

x∗

2
¯G−1
k

n ≤

∆2
x
2

.

x∗

k

−

¯G−1
n

x
k

The astute reader may recognize
as the
leading factor in the width of the conﬁdence interval for
estimating the gap ∆x using a linear least squares esti-
mator. The result says that this width has to shrink at
least logarithmically with a speciﬁc constant. Before the
proof of Theorem 1 we present a trivial corollary and
some consequences. The assumption that ¯Gn is eventu-
ally invertible can be relaxed. In fact, if ¯Gn is not even-
tually invertible, then the algorithm must suffer linear re-
gret on some problem. This is quite natural because a
singular ¯Gn implies the algorithm has not explored at all
in some direction. The proof of this fact may be found in
Appendix C.

Corollary 2. Let π be a consistent policy, θ
. Then
that there is a unique optimal arm in

∈

Rd such

lim sup
n→∞

log(n)

x
k

k

2
¯G−1

n ≤

(1)

A

∆2
x
2

and also lim sup
n→∞
, θ) is deﬁned as the solution to the following

where c(

, θ) ,

c(

A

Rπ
θ (n)
log(n) ≥

2

A

optimisation problem:

α(x)∆x subject to

inf
α∈[0,∞)A

Xx∈A−

x
k
k

2
H−1(α) ≤

∆2
x
2

,

x

∀

∈ A

− ,

(2)

where H(α) =

x∈A α(x)xx⊤.

P

As with the previous result, in (1) the reader may rec-
ognize the leading term of the conﬁdence width for esti-
mating the mean reward of x. Unsurprisingly, the width
of this conﬁdence interval has to shrink at least as fast
as the width of the conﬁdence interval for estimating the
gap ∆x. The intuition underlying the optimisation prob-
lem (2) is that no consistent strategy can escape allocat-
ing samples so that the gaps of all suboptimal actions
are identiﬁed with high conﬁdence, while a good strategy
will also minimise the regret subject to the identiﬁability
condition. The proof of Corollary 2 is given in Appendix
B.

Example 3 (Finite armed bandits). Suppose k = d and

=

e1, . . . , ek}
{

A

be the standard basis vectors. Then

c(

, θ) =

A

Xx∈A:∆x>0

2
∆x

,

which recovers the lower bound by Lai and Robbins
[1985].

{

=

A

c(

A − {

A
, θ) = 2ε−1
x2}

Example 4. Let α > 1 and d = 2 and
x1, x2, x3}
with x1 = (1, 0) and x2 = (0, 1) and x3 = (1
ε, αε)
−
, θ) = 2α2 for all sufﬁciently
and θ = (1, 0). Then c(
small ε. The example serves to illustrate the interest-
, θ), which
ing fact that c(
means that the problem becomes signiﬁcantly harder if
x2 is removed from the action-set. The reason is that
x1 and x3 are pointing in nearly the same direction, so
learning the difference is very challenging. But deter-
mining which of x1 and x3 is optimal is easy by playing
x2. So we see that in linear bandits there is a complicated
trade-off between information and regret that makes the
structure of the optimal strategy more interesting than in
the ﬁnite setting.

≫

A

The closest prior work to our lower bound is by
Komiyama et al. [2015] and Agrawal et al. [1989]. The
latter consider stochastic partial monitoring when the re-
ward is part of the observation. In this setting in each
round, the learner selects one of ﬁnitely many actions
and receives an observation from a distribution that de-
pends on the action chosen and an unknown parameter,
but is otherwise known. While this model could cover
our setting, the results in the paper are developed only
for the case when the unknown parameter belongs to a

ﬁnite set, an assumption that all the results of the paper
heavily depend on. Komiyama et al. [2015] on the other
hand restricts partial monitoring to the case when the ob-
servations belong to a ﬁnite set, while the parameter be-
longs to the unit simplex. While this problem also has
a linear structure, their results do not generalize beyond
the discrete observation setting.

4 PROOF OF THEOREM 1

We make use of two standard results from information
theory. The ﬁrst is a high probability version of Pinsker’s
inequality.
Lemma 5. Let P and P′ be measures on the same mea-
surable space (Ω,

). Then for any event A

,

F

∈ F

P (A) + P′ (Ac)

exp (

KL(P, P′)) ,

(3)

1
2

≥

−

where Ac is the complementer event of A (Ac = Ω
A)
and KL(P, P′) is the relative entropy between P and P′,
, if P is not absolutely continuous
which is deﬁned as +
∞
with respect to P′, and is
dP′ (ω) otherwise.

\

Ω dP(ω) log dP
R

This result follows easily from Lemma 2.6 of Tsybakov
[2008].

The second lemma is sometimes called the information
processing lemma and shows that the relative entropy be-
tween measures on sequences of outcomes for the same
algorithm interacting with different bandits can be de-
composed in terms of the expected number of times each
arm is chosen and the relative entropies of the distribu-
tions of the arms. There are many versions of this result
(e.g., Auer et al. [1995] and Gerchinovitz and Lattimore
[2016]). To state the result, assume without the loss
of generality that the measure space underlying the
.
action-reward sequence (A1, Y1, . . . , An, Yn) is Ωn
=
R)n and At and Yt are the respective coor-
(
A ×
dinate projections: At(a1, y1, . . . , an, yn) = at and
Yt(a1, y1, . . . , an, yn) = yt, 1
Lemma 6. Let P and P′ be the probability measures on
Ωn for a ﬁxed ban-
the sequence (A1, Y1, . . . , An, Yn)
dit policy π interacting with a linear bandit with stan-
dard Gaussian noise and parameters θ and θ′ respec-
tively. Under these conditions the KL divergence of P
and P′ can be computed exactly and is given by

n.

≤

≤

∈

t

KL(P, P′) =

1
2

E[Tx(n)]

x, θ
h

−

θ′

2 ,
i

(4)

Xx∈A
where E is the expectation operator induced by P.

Proof of Theorem 1. Recall that x∗ is the optimal arm,
be
which we assumed to be unique. Let x

∈ A

3

Ωn be
a suboptimal arm (so ∆x > 0) and A
an event to be chosen later. Rearranging (3) gives
KL(P, P′)
1
2P(A)+2P′(Ac) ) and recalling that
¯Gn = E
, together with Lemma 6 we get
that

log(
≥
n
t=1 AtA⊤
t

⊂

(cid:2)P
2
θ′
¯Gn

k

1
2 k

θ

−

(cid:3)

= KL(P, P′)

log

≥

1
2P (A) + 2P′ (Ac)
(cid:19)
(5)

.

(cid:18)

i

}

−

x∗, θ′

Tx∗(n)

Now we choose θ′ “close” to θ, but in a such a way that
> 0, meaning in the bandit determined by θ′
x
h
the optimal action is not x∗. Selecting A =
≤
ensures that P (A) + P′ (Ac) is small, because π
n/2
is consistent. Intuitively, this holds because if P (A) is
large then x∗ is not used much in θ, hence Rn
θ (n)
must be large. If P′ (Ac) is large, then x∗ is used often
in θ′, hence R′
θ′(n) must be large. But from the
n
consistency of π we know that both Rn and R′
n are sub-
Rd×d) to be
polynomial. Let ε > 0 and H
chosen later and deﬁne θ′ by

.
= Rπ

.
= Rπ

0 (H

(cid:23)

∈

{

θ′ = θ +

(∆x + ε) ,

(6)

H(x

x
k

−

−
x∗

x∗)
2
H
k

where we also restrict H so that

x

x∗

2
H > 0. Then,

k

−

k
+ ∆x + ε = ε > 0 .

x
h

x∗, θ′

=

x
h

x∗, θ

i

−

i
Hence the mean reward of x is higher than that of x∗ in
θ′.

−

(7)

Rn =

∆xE [Tx(n)]

∆minE [(n

T∗(n))]

≥

−

x
X
∆min E

∆minn
2

≥

=

1

T∗(n)
{
h
P (T∗(n)

n/2

≤

n
2

}

i

n/2) .

≤

On the other hand, introducing ∆′
i −
and E′ to denote the expectation operator induced
y, θ′
h
by P′ and using that by (7), x∗ is suboptimal in θ′, we
also have

y = maxzh

z, θ′

i

R′

n =

∆′
x

E′ [Tx(n)]

∆′

x∗E′ [T∗(n)]

≥

x
X
εE′ [1
ε n
2

≥

≥

T∗(n) > n/2
}
{
P′ (T∗(n) > n/2) .

T∗(n)]

Adding up the two inequalities and lower bounding ε +
∆min by 2ε, which holds when ε
∆min (which we
assume from now on), we get

≤

Rn + R′
n
εn

≥

n
2

≤

P

T∗(n)

+ P′

T∗(n) >

,

(cid:16)

(cid:17)

(cid:16)

n
2

(cid:17)

(8)

4

which completes the proof that P (T∗(n)
n/2) +
P′ (T∗(n) > n/2) is indeed small. Now we calculate the
term on the left-hand side of (5). Using the deﬁnition of
θ′, we get

≤

1
2 k

θ

θ′

2
¯Gn
k

−

=

(∆x + ε)2
2
(∆x + ε)2
2
s
2
¯G−1
k
k
n

=

x∗

2
H ¯GnH
4
H

k
x∗

k

x

k

−
x

k
−
ρn(H)

where in the last line we introduced

ρn(H)

s
.
= k

k

2
H ¯GnH

2
¯G−1

s
k
n k
4
s
H
k

, s = x

x∗ .

−

k
Combining this with (8), (5) and some algebra gives

(∆x + ε)2ρn(H)
2 log(n)

s

2
¯G−1
k
n

k

1

≥

−

log( ε

2 ) + log(Rn + R′
n)
log(n)

.

(9)

(10)

Since π is consistent, lim supn→∞
Hence, for all H

0 such that

log(Rn+R′
log(n)

n)

0.

≤

(cid:23)

1

≤

lim inf
n→∞

s
k

kH > 0,
(∆x + ε)2ρn(H)
2 log(n)

2
¯G−1
n

s
k

k

.

¯Gnk }

∞
k=1 such that

Now take a subsequence

c

.
= lim sup
n→∞

log(n)

s

k

= lim
k→∞

log(nk)

s

2
¯G−1
k
nk

k

.

{
2
¯G−1
k
n

Hence,

lim inf
n→∞

ρn(H)
2
s
¯G−1
k
n

k

log(n)

lim inf
k→∞

≤

ρnk (H)
s
k

k

log(nk)

2
¯G−1
nk

= lim inf
k→∞

ρnk (H)
limj→∞ log(nj)

=

lim inf k→∞ ρnk (H)
c

.

2
¯G−1
nj

s
k

k

(11)

¯G−1
Let ˜Hn = ¯G−1
. A simple calculation gives
n /
n
−4
2
2
H and hence if H
s
ρn(H) =
s
˜Hn k
H ˜H−1
(cid:13)
(cid:13)
n H k
k
k
k
(cid:13)
(cid:13)
˜Hnk }k, say, the subsequence
is any cluster point of
{
˜Hn′
˜Hnk }k converges to H,
{
and

k

{

s

k }k of the subsequence
kH > 0 then
s
k
s
lim inf
k→∞ k

k
2
˜Hn′
k

2
s
˜Hnk k
k
s
lim
k→∞ k
2
H k
k

k

s

s

2
H ˜H−1

−4
H
k

s
nk H k
−4
2
s
H ˜H−1
H
H k
k
k k
n′
k
−4
H = 1 ,

k

s

2
s
HH−1H k
k

k

≤

=

showing that

1

≤

lim inf
n→∞

(∆x + ε)2ρn(H)
2 log(n)

s

2
¯G−1
k
n

(∆x + ε)2
2c

.

≤

k
Since ε > 0 was arbitrary small, the result will follow
kH > 0. To show this, assume
once we establish that
s
k
on the contrary that
kH = 0. This implies that Hs = 0
s
and through ker(H) = ker(H −1) it also implies that
H −1s = 0. Let Hγ = H + γI, where I is the d
d
identity matrix. Then, Hγs = γs, so
>
s
k
0 and thus

2
Hγ
k

×
k

= γ

k

k

s

= lim

s

lim inf
k→∞

ρnk (Hγ)

lim
k→∞ k

s

≤

s

−4
Hγ
k

2
˜Hn′
k
2
˜Hn′
k

s
k k

s
k k

2
Hγ ˜H−1
Hγ k
k
n′
k
−4
2
s
˜H−1
k
k
k k
n′
−4 = 0 .
k

=

k→∞ k
2
H k
k
Chaining (10), (11) and the last display gives 1
contradiction. Thus,
s
proof.

0, a
kH > 0 must hold, ﬁnishing the

2
H−1 k
k

≤

k

k

s

s

s

:

i

∈ A

∗(θ) =

= maxy∈A

Remark 7. The uniqueness assumption of the theorem
can be lifted at the price of more work and by slightly
changing the theorem statement. In particular, the the-
orem statement must be restricted to those suboptimal
− that can be made optimal by changing θ
actions x
to θ′, while none of the optimal actions

y, θ
i}
h
∈ A
Rd such that

{
∈
are optimal. That is, the
x, θ
A
h
∗(θ) but
such that x
statement only concerns x
∗(θ′)
there exists θ′
and
A
∗(θ′). The choice of θ′ would still be as before,
x
except that x∗ is selected as the optimal action under θ
iH .
that maximizes c(H, θ) = inf x′∈A∗(θ)h
x
Then, in the proof, T∗(n) has to be redeﬁned to be
x∈A∗(θ) Tx(n) (the total number of times an optimal
action is chosen), and at the end one also needs to show
P
that the chosen H satisﬁes c(H, θ) > 0.

6∈ A
∗(θ) =

x′, x

∈ A

∩ A

x∗

A

−

−

∈

x

∅

5 CONCENTRATION

Before introducing the new algorithm we analyse the
concentration properties of the least squares estima-
tor. Our results reﬁne the existing guarantees by
Abbasi-Yadkori et al. [2011], and are necessary in order
to obtain asymptotic optimality. Let Gt be the Gram
s≤t AsA⊤
matrix after round t deﬁned by Gt =
s
t
and ˆθ(t) = G−1
s=1 AsYs be the empirical (least
squares) estimate, where As is selected based on
A1, Y1, . . . , As−1, Ys−1 and Ys =
+ ηs, ηs ∼
N (0, 1). We will only use ˆθ(t) for rounds t when
Gt is invertible. The empirical estimate of the sub-
optimal gaps is ˆ∆x(t) = maxy∈A ˆµy(t)
ˆµx(t), where
x, ˆθ(t)
. We will also use the notation ˆµ(t)
ˆµx(t) =
i
h

As, θ
h

P

P

−

i

t

5

and ˆ∆(t)
optimality gaps (indexed by the arms).

Rk for vectors of empirical means and sub-

∈

[1/n, 1), n sufﬁciently large
N such that Gt0 is almost surely non-singular,

∈

Theorem 8. For any δ
and t0 ∈
P

t0, x :

ˆµx(t)

t
∃

≥

(cid:18)

|

µx| ≥

−

x
k
k
q

2
G−1
t

fn,δ

δ ,

≤

(cid:19)

where for some c > 0 universal constant

fn,δ = 2

1 +

log(1/δ) + cd log(d log(n)) .

1
log(n)

(cid:19)

(cid:18)

The result improves on the elegant concentration guar-
antee of Abbasi-Yadkori et al. [2011] because asymptot-
2 log(n), while there it was
ically we have fn,1/n ∼
2d log(n). Note that the restriction on δ may be relaxed
with a small additional argument. The proof of Theorem
8 relies on a peeling argument and is given in Appendix
A. For the remainder we abbreviate fn = fn,1/n and
gn = fn,1/ log(n), which are chosen so that

P

P

t
∃

≥

(cid:18)

t
∃

≥

(cid:18)

|

|

t0, x :

ˆµx(t)

µx| ≥

−

x
k
k
q

2
G−1
t

fn

≤

(cid:19)

t0, x :

ˆµx(t)

µx| ≥

−

x
k
k
q

2
G−1
t

gn

≤

(cid:19)

,

1
n
(12)

1
log(n)

.

6 OPTIMAL STRATEGY

∈

A

1, 1]d with x =

A barycentric spanner of the action space is a set B =
there exists
A such that for any x
x1, . . . , xd} ⊆
{
∈ A
d
i=1 αixi. The existence of
an α
[
−
is ﬁnite
a barycentric spanner is guaranteed because
P
and spans Rd [Awerbuch and Kleinberg, 2004]. We pro-
pose a simple strategy that operates in three phases called
the warm-up phase, the success phase and the recovery
In the warm-up the algorithm deterministically
phase.
chooses its actions from a barycentric spanner to obtain
a rough estimate of the sub-optimality gaps. The algo-
rithm then uses the estimated gaps as a substitute for the
true gaps to determine the optimal pull counts for each
action, and starts implementing this strategy. Finally, if
an anomaly is detected that indicates the inaccuracy of
the estimated gaps then the algorithm switches to the re-
covery phase where it simply plays UCB.

Deﬁnition 9. For any ∆
[0,

∈
]k to be a solution to the optimisation problem

)k deﬁne Tn(∆)

[0,

∞

∈

Tx∆x subject to

∞

min
T ∈[0,∞]k

Xx∈A
2
H†

T ≤

∆2
x
fn

x
k
k

for all x

, where HT =

Txxx⊤ .

∈ A

Xx∈A

Algorithm 1 Optimal Algorithm

x1, . . . , xd}
{
log1/2(n)
⌉

times

⌈

A

1: Input:
and n
2: // Warmup phase
3: Find a barycentric spanner: B =
4: Choose each arm in B exactly
5: // Success phase
6: εn ←
7: ˆ∆
←
8: while t
9:
10: end while
11: // Recovery phase
12: Discard all data and play UCB until t = n.

n + 1
←
Tn( ˆ∆) and ˆµ
1)
∞
k
≤
ˆTx, t
≤

−
k
Play actions x with Tx(t)

x
kG−1
1) and ˆT
ˆµ

max
x∈A k
ˆ∆(t

g1/2
n , t

−
n and

←
ˆµ(t

←

≤

−

n

ˆµ(t
←
2εn do

−

1)

t + 1

A

and the parameter θ). Since F ′

factor that depends only on the problem (determined by
the action set
n occurs
with probability at most 1/ log(n), the contribution of
the latter component is negligible asymptotically.
Lemma 11. If Fn does not occur then Algorithm 1 never
enters the recovery phase. Furthermore,
not Fn}

∆At

t∈Tsucc.

E

1

{

, θ) .

c(

log(n)
P

≤

A

(cid:21)

lim sup
n→∞

(cid:20)

Before proving Lemma 11 we need a naive bound on the
solution to the optimisation problem, the proof of which
is given in Appendix D.
Lemma 12. Let T = Tn(∆) for any n. Then

Theorem 10. Assuming that x∗ is unique, the strategy
given in Algorithm 1 satisﬁes

lim sup
n→∞

Rπ
θ (n)
log(n) ≤

c(

, θ) for all θ

A

Rd .

∈

7 PROOF OF THEOREM 10

log1/2(n)
⌉

We analyse the regret in each of the three phases. The
warm-up phase has length d
, so its contribu-
⌈
tion to the asymptotic regret is negligible. There are two
challenges. The ﬁrst is to show that the recovery phase
happens with probability at most 1/ log(n). Then, since
the regret in the recovery phase is logarithmic by known
results for UCB, this ensures that the expected regret in-
curred in the recovery phase is also negligible. The sec-
ond challenge is to show that the expected regret incurred
during the success phase is asymptotically matching the
lower bound in Theorem 1.

The set of rounds when the algorithm is in the warm-
up/success/recovery phases are denoted by Twarm., Tsucc.
and Trec. respectively. We introduce two failure events
that occur when the errors in the empirical estimates of
the arms are excessively large. Let Fn be the event that
there exists an arm x and round t

d such that

|

ˆµx(t)

−

gn .

µx| ≥

x
k
k
q
n be the event that there exists an arm x
d such that

Similarly, let F ′
and round t

≥

2
G−1
t

|

−

fn .

x
k

ˆµx(t)

k
q

µx| ≥
Theorem 8 with t0 = d and (12) imply that P (Fn)
≤
1/ log(n) and P (F ′
1/n. The failure events deter-
n)
mine the quality of the estimates throughout time. The
following two lemmas show that if Fn does not occur
then the regret is asymptotically optimal, while if F ′
n oc-
curs then the regret is logarithmic with some constant

≤

≥
2
G−1
t

6

Tx ≤

2d3fn∆max
∆3

min

.

Xx:∆x>0

log1/2(n)
⌉

Proof of Lemma 11. First, if t = d
is the
⌈
round at the end of the warm-up period then by the
deﬁnition of the algorithm there is a barycentric span-
for
ner B =
be arbitrary. Then, by
1
∈ A
the deﬁnition of the barycentric spanner, we can write
1, 1] for all i. Therefore,
[
x =

x1, . . . , xd}
{
d. Let x
≤
i=1 αixi where αi ∈

log1/2(n)
⌉

and Txi(t) =

≤

−

⌈

i

d

P

x
kG−1

t ≤

k

xikG−1

t ≤

k

d
log1/4(n)

.

d

i=1
X

Recalling the deﬁnition of εn in the algorithm we have

εn = max

x∈A k

x
kG−1

n √gn = O

d log1/2(log(n))

 

log1/4(n) !

.

Consider the case when Fn does not hold. Then, for all
arms x and rounds t after the warm-up period we have

ˆµx(t)

|

µx| ≤ k

x
kG−1

t

√gn ≤

−

εn ,

−

| ≤

ˆµx(s)

Therefore for all s, t after the warm-up period we have
2εn, which means the success phase
ˆµx(t)
|
never ends and so the ﬁrst part of the lemma is proven. It
remains to bound the regret. Since we are only concerned
with the asymptotics we may take n to be large enough
∆min/2, which implies that ˆ∆x∗ = 0.
so that 2εn ≤
For Tn(∆), the solution to the optimisation problem in
Deﬁnition 9 with the true gaps, it holds that

lim sup
n→∞ P

x6=x∗ Tn,x(∆)∆x
log(n)

= c(

, θ) .

A

(13)

Letting T ∗ = Tn(∆) and 1 + δn = maxx: ˆ∆x>0 ∆2
we have

x/ ˆ∆2
x,

2
H−1

x
k
k

(1+δn )T ∗

x
k
= k

2
H−1
T ∗
1 + δn ≤

∆2
x
(1 + δn)fn ≤

ˆ∆2
x
fn

.

Therefore,
.
where T
= (Tx)x
P

x6=x∗ Tx ˆ∆x ≤

.
= Tx(n). Also,

P

(1 + δn)

x6=x∗ T ∗

x ∆x,

1 + δn = max
x: ˆ∆x>0

= max

1 +

x: ˆ∆x>0  

∆2
x
ˆ∆2
x ≤
4(∆x −
(∆x −

max
x: ˆ∆x>0

εn)εn
2εn)2

∆2
x
2εn)2
(∆x −
1 +

16εn
∆min

! ≤

, (14)

where in the last inequality we used the fact that 0
2εn ≤

∆min/2. Then the regret in the success phase is

≤

Tx∆x

∆At ≤

Xt∈Tsucc.
=

Xx6=x∗

Tx ˆ∆x +

Tx(∆x −

ˆ∆x)

Xx6=x∗
(1 + δn)

Xx6=x∗
T ∗
x

ˆ∆x + 2εn

Tx

≤

≤

Xx6=x∗

Xx6=x∗

Xx6=x∗

Xx6=x∗

(1 + δn)

T ∗
x ∆x + 2εn

((1 + δn)T ∗

x + Tx) .

The result follows by taking the limit as n tends to inﬁn-
ity and from Lemma 12 and (13) and (14), together with
the reverse Fatou lemma.

Our second lemma shows that provided F ′
gret in the success phase is at most logarithmic:

n fails, the re-

Lemma 13. It holds that:

lim sup
n→∞

E

1

(cid:2)

Fn and not F ′
{

n}
log(n)

P

t∈Tsucc. ∆At

= 0 .

(cid:3)

The proof follows by showing the existence of a constant
and θ, but not n such that the regret
m that depends on
A
suffered in the success phase whenever F ′
n does not hold
is almost surely at most m log(n). The result follows
from this because P (Fn)
1/ log(n). See Appendix E
for details.

≤

Proof of Theorem 10. We decompose the regret into the
regret suffered in each of the phases:

Rπ

θ (n) = E

∆At +

∆At +

∆At

.

"

t∈Twarm.
X

t∈Tsucc.
X

Xt∈Trec.

#
(15)

The warm-up phase has length d
⌈
tributes asymptotically negligibly to the regret:

log1/2(n)
⌉

, which con-

lim sup
n→∞

E

t∈Twarm.
log(n)

(cid:2)P

∆At

(cid:3)

= 0 .

(16)

By Lemma 11, the recovery phase only occurs if Fn oc-
curs and P (Fn)
1/ log(n). Therefore by well-known

≤

7

guarantees for UCB [Bubeck and Cesa-Bianchi, 2012]
there exists a universal constant c > 0 such that

E

"
Xt∈Trec.

∆At

= E

#

∆At

Trec.
(cid:12)
(cid:12)
(cid:12)
(cid:12)
P (Trec.
(cid:12)

"

Xt∈Trec.
ck log(n)
∆min

≤

=

∅#

P (Trec.

=

)

∅

=

)

∅

≤

ck
∆min

.

Therefore

lim sup
n→∞

E

t∈Trec.
log(n)

(cid:2)P

∆t

(cid:3)

= 0 .

(17)

Finally we use the previous lemmas to analyse the regret
in the success phase:

E

"

Xt∈Tsucc.

∆At

= E

1

#

"

not Fn}
{

∆At

#

Xt∈Tsucc.
Fn and not F ′

+ E

1

{

"

+ E

1

F ′

n}

{

"

∆At

.

#

Xt∈Tsucc.

By (12), the last term satisﬁes

n}

Xt∈Tsucc.

∆At

#

(18)

lim sup
n→∞

(cid:2)

E

1

F ′

n}

{

t∈Tsucc.

∆At

log(n)
P
n∆maxP (F ′
n)
log(n)

(cid:3)

= 0 .

lim sup
n→∞

≤

The ﬁrst two terms in (18) are bounded using Lemmas
11 and 13, leading to

lim sup
n→∞

E

t∈Tsucc.
log(n)

(cid:2)P

∆At

c(

, θ) .

A

≤

(cid:3)

Substituting the above display together with (16) and
(17) into (15) completes the result.

8 SUB-OPTIMALITY OF OPTIMISM
AND THOMPSON SAMPLING

We now argue that algorithms based on optimism or
Thompson sampling cannot be close to asymptotically
optimal. In each round t an optimistic algorithm con-
Rd and chooses At ac-
structs a conﬁdence set
Ct ⊆
x, ˜θ
. In order
cording to At = arg maxx∈A max˜θ∈Cth
i
to proceed we need to make some assumptions on
Ct,
otherwise one can deﬁne a “conﬁdence set” to ensure
any behaviour at all. First of all, we will assume that
P (
∈ Ct) = O(1/n). That is, that the prob-
t
∃
ability that the true parameter is ever outside the con-
ﬁdence set is not too large. Second, we assume that

n : θ /

≤

Ct ⊆ Et where
estimator given by

Et is the ellipsoid about the least squares

,

k

o

n

−

˜θ :

ˆθ(t)

Et =

α log(n)

2
Gt ≤

˜θ
k
where α is some constant and ˆθ(t) is the empirical es-
timate of θ based on the observations so far. Existing
algorithms based on conﬁdence all use such conﬁdence
sets. Standard wisdom when designing optimistic algo-
rithms is to use the smallest conﬁdence set possible, so an
alternative algorithm that used a different form of conﬁ-
dence set would normally be advised to use the intersec-
Ct ∩Et, which remains valid with high probability by
tion
a union bound. If the optimistic algorithm is not consis-
tent, then its regret is not logarithmic on some problem
and so diverges relative to the optimal strategy. Suppose
now that the algorithm is consistent. Then we design a
bandit on which its asymptotic regret is worse than opti-
mal by an arbitrarily large constant factor.

Let d = 2 and e1 = (1, 0) and e2 = (0, 1) be the stan-
dard basis vectors. The counter-example (illustrated in
Figure 1) is very simple with
where
ε, 8αε). The true parameter is given by
x = (1
θ = e1, which means that x∗ = e1 and ∆e2 = 1 and
∆x = ε. Suppose a consistent optimistic algorithm has
chosen Te2 (t

4α log(n) and that θ

e1, e2, x
}
{

Ct. Then,

1)

A

−

=

−

≥
e2, ˆθ(t

e2, ˜θ

max
˜θ∈Cth

i ≤ h

∈

2
G−1
t

e2k

1 .

+

1)
i

−

k
q
α log(n)

2
G−1
t

e2k

< 2

≤

k
q
Ct, the optimistic value of the optimal
But because θ
∈
action is at least
= e2.
= 1, which means that At 6
e1, θ
i
h
Ct for all rounds, then the op-
We conclude that if θ
∈
1 + 4α log(n).
timistic algorithm satisﬁes Te2 (t
1)
Ct with probability at least
By the assumption that θ
2 + 4α log(n). By con-
1
sistency of the optimistic algorithm and our lower bound
(Theorem 1) we have

∈
1/n we bound E[Te2 (n)]

−

≤

−

≤

α log(n)

lim sup
n→∞

log(n)

x
k

e1k

−

2
¯G−1

n ≤

ε2
2

,

Therefore by choosing ε sufﬁciently small we conclude
E[Tx(n)]/ log(n) = Ω(1/ε2) and so
that lim supn→∞
the asymptotic regret of the optimistic algorithm is at
least

lim sup
n→∞

(n)

θ

ROPTIMISTIC
log(n)

= Ω

.

1
ε

(cid:18)

(cid:19)

, θ) = 128α2 and so by choosing ε

However, for small ε the optimal regret for this problem
is c(
α we can
see that the optimistic approach is sub-optimal by an ar-
bitrarily large constant factor. The intuition is that the

≪

A

8

optimistic algorithms very quickly learn that e2 is a sub-
optimal arm and stop playing it. But as it turns out, the
information gained by choosing e2 is sufﬁciently valu-
able that an optimal algorithm should use it for explo-
ration.

(0, 1)

ε

(1, 0)

(1 − ε, 2ε)

sampling
proposed

has
Thompson
also
for
been
the linear bandit problem
[Agrawal and Goyal, 2013].
The standard approach uses
a nearly ﬂat Gaussian prior
(and so posterior), which
Figure 1: Counter-example
means that essentially the
(ˆµ(t), αG−1
algorithm operates by sampling θt from
)
t
N
. Why
and choosing the arm At = arg maxx∈Ah
x, θti
does this approach fail? By the assumption of consis-
tency we expect that the optimal arm will be played all
but logarithmically often, which means that the posterior
will concentrate quickly about the value of the optimal
µ∗. Then using the same
action so that
counter-example as for the optimistic algorithm we see
0 is vanishingly
that the likelihood that
e2 −
h
1) = Ω(α log(n)) and so Thompson
small once Te2(t
sampling will also fail to sample action e2 sufﬁciently
often.

x∗, θti ≈
h

e1, θti ≥

−

9 SUMMARY

We characterised the optimal asymptotic regret for lin-
ear bandits with Gaussian noise and ﬁnitely many ac-
tions in the sense of Lai and Robbins [1985]. The results
highlight a surprising fact that all reasonable algorithms
based on optimism can be arbitrarily worse than opti-
mal. While this behaviour has been observed before in
more complicated settings (notably, partial monitoring),
our results are the ﬁrst to illustrate this issue in a setting
only barely more complicated than ﬁnite-armed bandits.
Besides this we improve the self-normalised concentra-
tion guarantees by Abbasi-Yadkori et al. [2011] by a fac-
tor of d asymptotically.

As usual, we open more questions than we answer.
While the proposed strategy is asymptotically optimal, it
is also extraordinarily naive and the analysis is far from
showing ﬁnite-time optimality. For this reason we think
the most pressing task is to develop efﬁcient and practical
algorithms that exploit the available information in a way
that Thompson sampling and optimism do not. There are
two natural research directions towards this goal. The
ﬁrst is to push the optimisation approach used here and
also by Wu et al. [2015], but applied more “smoothly”
without discarding data or long phases. The second is to
generalise information-theoretic ideas used (for instance)
by Russo and Van Roy [2014] or Reddy et al. [2016].

References

Yasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari.
Improved algorithms for linear stochastic bandits. In
Advances in Neural Information Processing Systems
(NIPS), pages 2312–2320, 2011.

Yasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari.
Online-to-conﬁdence-set conversions and application
to sparse stochastic bandits. In AISTATS, pages 1–9,
2012.

Rajeev Agrawal. Sample mean based index policies with
O(log n) regret for the multi-armed bandit problem.
Advances in Applied Probability, pages 1054–1078,
1995.

Rajeev Agrawal, Demosthenis Teneketzis, and Venkat-
achalam Anantharam. Asymptotically efﬁcient adap-
tive allocation schemes for controlled i.i.d. processes:
Finite parameter space.
IEEE Transaction on Auto-
matic Control, 34:258–267, 1989.

Shipra Agrawal and Navin Goyal. Thompson sampling
for contextual bandits with linear payoffs. In ICML,
pages 127–135, 2013.

Jean-Yves Audibert and S´ebastien Bubeck. Minimax
policies for adversarial and stochastic bandits. In Pro-
ceedings of Conference on Learning Theory (COLT),
pages 217–226, 2009.

Peter Auer. Using conﬁdence bounds for exploitation-
exploration trade-offs. Journal of Machine Learning
Research, 3(Nov):397–422, 2002.

Peter Auer, Nicol´o Cesa-Bianchi, Yoav Freund, and
Robert E. Schapire. Gambling in a rigged casino: The
adversarial multi-armed bandit problem. In Proceed-
ings of the 36th Annual Symposium on Foundations of
Computer Science, pages 322–331, 1995.

Peter Auer, Nicol´o Cesa-Bianchi, and Paul Fischer.
Finite-time analysis of the multiarmed bandit problem.
Machine Learning, 47:235–256, 2002.

Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-
learning.
optimal regret bounds for reinforcement
Journal of Machine Learning Research, 99:1563–
1600, August 2010. ISSN 1532-4435.

Baruch Awerbuch and Robert D Kleinberg. Adaptive
routing with end-to-end feedback: Distributed learn-
ing and geometric approaches. In Proceedings of the
36th Annual ACM Symposium on the Theory of Com-
puting, pages 45–53, 2004.

S´ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret
Analysis of Stochastic and Nonstochastic Multi-armed
Bandit Problems. Foundations and Trends in Machine
Learning. Now Publishers Incorporated, 2012. ISBN
9781601986269.

Olivier Chapelle and Lihong Li. An empirical evaluation
of Thompson sampling. In Advances in Neural Infor-
mation Processing Systems (NIPS), pages 2249–2257,
2011.

Varsha Dani, Thomas P Hayes, and Sham M Kakade.
Stochastic linear optimization under bandit feedback.
In Proceedings of Conference on Learning Theory
(COLT), pages 355–366, 2008.

Sarah Filippi, Olivier Capp´e, Aur´elien Garivier, and
Csaba Szepesv´ari. Parametric bandits: The general-
ized linear case. In NIPS, pages 586–594, December
2010.

S´ebastien Gerchinovitz and Tor Lattimore. Reﬁned
lower bounds for adversarial bandits. arXiv preprint
arXiv:1605.07416, 2016.

Aditya Gopalan and Shie Mannor. Thompson sam-
pling for learning parameterized Markov decision pro-
cesses.
In Proceedings of the 28th Conference on
Learning Theory (COLT), pages 861–898, 2015.

Michael N Katehakis and Herbert Robbins. Sequential
choice from several populations. Proceedings of the
National Academy of Sciences of the United States of
America, 92(19):8584, 1995.

Emilie Kaufmann, Nathaniel Korda, and R´emi Munos.
Thompson sampling: An asymptotically optimal
ﬁnite-time analysis.
In Nader H. Bshouty, Gilles
Stoltz, Nicolas Vayatis, and Thomas Zeugmann, ed-
itors, Algorithmic Learning Theory, pages 199–213,
2012.

Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa.
Regret lower bound and optimal algorithm in ﬁnite
stochastic partial monitoring. In Advances in Neural
Information Processing Systems (NIPS), pages 1792–
1800, 2015.

Nathaniel Korda, Emilie Kaufmann, and R´emi Munos.
Thompson sampling for 1-dimensional exponential
family bandits.
In Advances in Neural Information
Processing Systems (NIPS), pages 1448–1456, 2013.

Tze Leung Lai and Herbert Robbins. Asymptotically ef-
ﬁcient adaptive allocation rules. Advances in Applied
Mathematics, 6:4–22, 1985.

G´abor Bart´ok, Dean P Foster, D´avid P´al, Alexander
Rakhlin, and Csaba Szepesv´ari. Partial monitoring-
classiﬁcation, regret bounds, and algorithms. Mathe-
matics of Operations Research, 39(4):967–997, 2014.

Gautam Reddy, Antonio Celani, and Massimo Vergas-
sola.
Infomax strategies for an optimal balance be-
tween exploration and exploitation. Journal of Statis-
tical Physics, 163(6):1454–1476, 2016.

9

Daniel Russo and Benjamin Van Roy. Learning to opti-
mize via posterior sampling. Mathematics of Opera-
tions Research, 39(4):1221–1243, 2014.

William Thompson. On the likelihood that one unknown
probability exceeds another in view of the evidence of
two samples. Biometrika, 25(3/4):285–294, 1933.

Alexandre B Tsybakov. Introduction to nonparametric
estimation. Springer Science & Business Media, 2008.

Michal Valko, R´emi Munos, Branislav Kveton, and
Tomas Kocak. Spectral bandits for smooth graph func-
tions. In ICML, pages 46–54, 2014.

Yifan Wu, Andr´as Gy¨orgy, and Csaba Szepesv´ari. On-
line Learning with Gaussian Payoffs and Side Obser-
vations. In Advances in Neural Information Process-
ing Systems (NIPS), pages 1360–1368, 2015.

A PROOF OF THEOREM 8

At, θ

1 for all t

Recall that At is the action chosen in round t and that
is the noise term, which we assumed
ηt = Yt − h
i
t
to be a standard Gaussian. Let St =
s=1 Asηs. By
assumption,
Atk ≤
k
P
N and ε > 0 and σ2 > 0.
Lemma 14. Let n
Let X1, X2, . . . , Xn be a sequence of Gaussian ran-
dom variables adapted to ﬁltration
F2, . . . such that
E[Xt|Ft−1] = 0. Deﬁne σ2
t = Var[Xt|Ft−1] and as-
sume that σ2
t ≤

F1,
σ2 almost surely. Then

1.

≥

∈

P

t
 ∃

≤

n :

Xs ≥ s

2γnVt log

N
δ

(cid:18)

(cid:19)! ≤

δ ,

t

s=1
X

where Vt = max

ε,

γn = 1 +

n

1
log(n)

t
s=1 σ2
t

and

o

P
and N = 1 +

Proof. For ψ

R deﬁne

∈

log(nσ2/ε)
log(γn)

.

(cid:25)

(cid:24)

Mt,ψ = exp

t

 

ψXt −

ψ2σ2
t
2 !

.

s=1
X
n is a stopping time with respect to

If τ
, then as
≤
in the proof [Abbasi-Yadkori et al., 2011, Lemma 8] we
have E[Mτ,ψ]
1. Therefore, by Markov’s inequality
we have

≤

F

For k

1, 2, . . . , N

∈ {

1/δ)

δ .

≤

P (Mτ,ψ ≥
deﬁne

}

ψk =

2
εγk−1
n

s

log

N
δ

(cid:18)

(cid:19)

(19)

10

Then rearranging (19) leads to

τ

t=1
X

P

k
 ∃

∈

[N ] :

Therefore letting

Xt ≥

1
ψk

log

N
δ

(cid:18)

(cid:19)

ψkVτ

+

δ .

2 ! ≤

k∗ = min

k

[N ] : ψk ≥

∈

2 log(N/δ)/Vτ

p

o

leads to

n

τ

P

δ

≥

P

≥

 

t=1
X
τ

 

t=1
X

Xt ≥

1
ψk∗

log

N
δ

(cid:18)

(cid:19)

ψk∗ Vτ

2 !

Xt ≥ s

2γnVτ log

.

(cid:18)

(cid:19)!

+

N
δ

The result is completed by choosing stopping time τ by
τ = min(n, τn), where

τn = min

t

n :

≤

(

Xs ≥ s

2γnVt log

N
δ

(cid:18)

(cid:19))

.

t

s=1
X

Lemma 15. Let δ
Then

∈

[1/n, 1) and λ

Rd with

∈

λ
k

k ≤

1.

P

t
 ∃

≤

n :

λ, Sti ≥ r

h

1
n2 ∨ k

λ
k

2
Gt

hn,δ

δ ,

! ≤

where

hn,δ = 2

1 +

(cid:18)

1
log(n)

log

(cid:19)

(cid:18)

c log(n)
δ

(cid:19)

with some universal constant c

1.

≥

Proof. We prepare to use the previous lemma. First note
that

t

λ, Sti
h

=

ηs h

λ, Ati

.

s=1
X
Since ηs is a standard Gaussian, the predictable vari-
ance of the term inside the sum is σ2
2
λ
k

1. Therefore

λ, Ati
h

Atk
k

t =

≤

≤

k

2

2

t

s=1
X

t

s=1
X

s = λ⊤
σ2

AsA⊤

s λ =

2
Gt

.

λ
k

k

Therefore the result follows by the previous lemma with
and ε = 1/(n2 log(n)3) and σ2 = 1.
Xt = ηt h

λ, Ati

The following lemma can be extracted from the proof of
Theorem 1 in Abbasi-Yadkori et al. [2011].

where

k·kF is the Frobenius norm. Then

Lemma 16. Assume that
is such that for some t0 >
As}
0, Gt0 is non-singular almost surely. Then, for some c >
0 universal constant,

{

P

t
∃

≥

t0 :

Stk
k

2
G−1

t ≥

cd log(n/δ)

(cid:16)

δ .

≤

(cid:17)

C ⊂

)G−1

t x, where

Proof of Theorem 8. Let ε > 0 be some small real num-
Rd to
ber to be tuned subsequently and choose
be a ﬁnite covering set such that for all x
and
∈ A
t with Gt non-singular there exists a λ
such that
∈ C
is some diagonal matrix
λ = (I +
(possibly depending on x and G−1
) with entries bound
in [0, ε]. Of course Gt is a random variable, so we insist
the existence of λ is almost sure (that is, no matter how
the actions are taken). We defer calculating the necessary
size N =
until later. Let δ1 = δ/(N + 1) and Fλ be
|C|
the event that

E

E

t

Fλ =

t :

λ, Sti ≥ r

h

(∃

1
n2 ∨ k

2
Gt

λ
k

hn,δ1

.

)

Then a union bound and Lemma 15 leads to

P (

∪λ∈CFλ)
=

N δ1 .

≤

t
{∃

≥

t0 :

2
G−1

Stk
k

t ≥

By Lemma 16, for
cd log(n/δ1)
}

G
, we have

P (
G

)

≤

δ1 .

Another union bound shows that the P (
≤
(N + 1)δ1 = δ. From now on we assume that neither
be arbitrary
)G−1
t x

∪λ∈CFλ, nor
be such that λ = (I +
t0 let λ
≥
is diagonal with entries in [0, ε]. Then

.
=
F
and for t
where

∪λ∈CFλ ∪ G

occurs and let x

G
∈ C

∈ A

E

)

(21)

(22)

ˆµx(t)

µx =

−
G−1
t x

−

G−1

t x, St
+

λ, St
(cid:10)

G−1

t x

λ

−

(cid:11)
Gt k

λ, Sti
(cid:11)
h
StkG−1
+

t

E

=

≤

(cid:10)

(cid:13)
(cid:13)

1
n2 ∨ k

λ
k

2
Gt hn,δ1 .

r

(23)

We bound each term separately using matrix algebra and
the assumption that the failure events
do not
occur:

and

F

G

G−1

t x

λ

−

Gt

(cid:13)
(cid:13)

=

=

t x

G−1
E
G1/2
(cid:13)
t E
(cid:13)
k
G1/2
t E

Gt
G−1/2
(cid:13)
t
(cid:13)
G−1/2
t

≤ k

G−1/2
t

x
k
x
kG−1
kF k

t

,

(cid:13)
(cid:13)

(cid:13)
(cid:13)

G1/2
t E

k

G−1/2
t

kF =

)

tr(GtE
kEk∞ ≤

G−1
t E
ε√d .

q
√d

≤
Therefore if ε = 1/(d3/2 log(n)), then the ﬁrst term in
(23) is bounded by

G−1

t x

λ

−

StkG−1

t

Gt k

= O(1)

x
kG−1

t

· k

.

(24)

For the second term we proceed similarly:

(cid:13)
(cid:13)

(cid:13)
(cid:13)

2
Gt

λ
k
k

2
Gt
2

=

G−1

t x +

G−1

t x

E
(cid:13)
(cid:13)
1 + ε√d
x
(cid:13)
(cid:13)
≤ k
k
(cid:16)
(cid:17)
= (1 + o(1))
.

2
G−1
t

2
G−1
t

x
k
k

/n

Therefore, assuming n is large enough so that 1/n2
x
k

≤
= 0 we
k
simply note that the following equality holds trivially),
we have

(in the unique case that

2
x
G−1
k
t

x
k

≤ k

k

1
n2 ∨ k

λ
k

2
Gt

r

hn,δ1 = (1 + o(1))

2
G−1
t

x
k
k

hn,δ1 .

q

(20)

Substituting the above expression along with (24) into
(23) leads to

ˆµx(t)

µx = (1 + o(1))

−

2
G−1
t

x
k

hn,δ1 .

k
q

C

Finally we note that
can be chosen in such a way
that for suitably large universal constant c > 0 its
size is log N = O(d log d log(n)). This follows by
separately and noting that
treating each arm x
G−1
Then letting J =
.
x
t x
x
k
≤
k
k
≤ k
= O(d3/2 log2(n)), the covering
log(n)/ log(1 + ε)
(cid:13)
(cid:13)
⌈
⌉
(cid:13)
(cid:13)
set is given by
Cx is a product cov-
x∈A Cx where
=
ering space with a geometrical grid.
S

∈ A

/n

C

Cx =

d

×i=1 (cid:26)

x
k
k

(1 + ε)j
n

: 0

j

≤

≤

J

.

(cid:27)

The theorem is completed by using the deﬁnition of hn,δ1
in Lemma 15.

B PROOF OF COROLLARY 2

x∗

− =

A

Let
be the set of suboptimal actions. To
see (1), it sufﬁces to show that for every consistent policy
π and vector y

}
Rd,

A \ {

∈

lim
n→∞

log(n)y⊤ ¯G−1

n x∗ = 0 .

(25)

11

∈

∋

∈ A

The proof hinges on the fact that E [T∗(n)]
Ω(n)
∈
−, E [Tx(n)]
∈ ∩p>0O(np).
and for x
Indeed,
these follow from the assumption that π is consistent
Rπ
and as such for any p > 0, O(np)
θ (n) =
x∈A− ∆xE [Tx(n)], so E [Tx(n)]
∈ ∩p>0O(np) in-
Ω(n).

deed, and thus also E [T∗(n)]
P
Let us return to proving (25). Clearly, it is enough
to see this in the two cases: when y = x∗ and
when y and x∗ are perpendicular. Consider ﬁrst when
y = x∗. Then, from ¯Gn (cid:23)
E [T∗(n)] x∗(x∗)⊤ it
follows that ¯G−1
(E [T∗(n)])−1x∗(x∗)⊤ and hence
n (cid:22)
log(n)(x∗)⊤ ¯G−1
x∗
n x∗
.
→ ∞

log(n)
E[T∗(n)] k
Now consider the case when y and x∗ are perpen-
dicular.
it must hold
n y.
Using the deﬁnition of ¯Gn,
that ¯Gnv = y.
x∈A− E [Tx(n)] xx⊤v.
y = E [T∗(n)] x∗(x∗)⊤v +
Since by assumption, y and x∗ are perpendicu-
2 (x∗)⊤v +
lar, 0 = (x∗)⊤y = E [T∗(n)]
x∗
k
x∈A− E [Tx(n)] (x∗)⊤xx⊤v. Hence,

Let v = ¯G−1

0 as n

Then,

2
k

P

→

≤

k

P
log(n)(x∗)⊤v =

log(n)

−

Xx∈A−

E [Tx(n)]
E [T∗(n)]

(x∗)⊤xx⊤v
x∗
k

2
k

. This ﬁnishes the proof of

converges to zero as n
(25) and thus of (1).

→ ∞

For the second part we start with

Rπ
θ (n)
log(n)

=

E [Tx(n)]
log(n)

∆x .

Xx∈A−

Then αn(x) = E [Tx(n)] / log(n) is asymptotically fea-
sible for n large. Indeed, ¯Gn = log(n)H(αn), hence
¯G−1

n = H −1(αn)/ log(n) and so
∆2
x
2 ≥

lim sup
n→∞

log(n)

2
¯G−1
n

x
k

k

= lim sup

n→∞ k

x
k

2
H−1(αn) .

Thus for any ε > 0 and n large enough,
∆2

2
x
H−1(αn) ≤
k

k

Therefore

x/2 + ε and also
Rπ
θ (n)
log(n)

=

Xx∈A−

E [Tx(n)]
log(n)

∆x ≥

cε(

, θ) ,

A

A

x/2 is replaced by ∆2

where cε(
lem (2) where ∆2
Rπ
θ (n)
lim inf n→∞
log(n) ≥
trary and inf ε>0 cε(
A
result.

, θ) is the solution to the optimisation prob-
x/2 + ε. Hence,
, θ). Since ε > 0 was arbi-
, θ), we get the desired
(cid:3)

cε(
A
, θ) = c(

A

C PROOF THAT THE GRAM MATRIX
IS EVENTUALLY NON-SINGULAR

Let π be a consistent strategy and
set and parameter for a linear bandit. Deﬁne

and θ be the action-
′ =

A

A

12

{

}

1

n
t=1

P

span

At = x
}

∈ A
′. Decompose x = y + z where y
A
span

x : E[
to be the set of arms that
] > 0
{
are played at least once with non-zero probability. We
proceed by contradiction. Suppose that ¯Gn is singu-
such that
lar for all n. Then there exists an x
′
span
x /
A
∈
′⊥ is non-zero and in the orthogonal
and z
′. Therefore
complement of the subspace spanned by
′. Deﬁne an alternative bandit
w, z
h
with the same action-set and parameter θ′ = θ+2∆maxz.
′. Therefore the ban-
Then
−
dits determined by θ and θ′ appear identical to the algo-
rithm, and in particular, E′[
] = 0, and
}
yet by construction we have

= 0 for all w

= 0 for all w

∈ A
1

At /
{

′
∈ A

w, θ
h

n
t=1

∈ A

θ′

A

A

∈

∈

i

i

P

n

"

t=1
X

Rπ

θ′(n)

∆maxE′

≥

1

′

At ∈ A
{

}#

= n∆max .

Therefore the regret is linear for θ′, which implies that
π is not consistent. Therefore for sufﬁciently large n we
have ¯Gn is non-singular.

D PROOF OF LEMMA 12

Let B
be an alternative to T given by

⊆ A

be a barycentric spanner and let S

[0,

]k

∞

∈

,
∞
2d2fn
∆2
0 ,

min

,

if x = x∗;
if x
B;

∈
otherwise .

Sx = 


Then

x∗

k

kH†

s


= 0 and for x∗

= y

we have

∈ A

2

y

k

2
HS† ≤  
k

x
kHS† !
k

2

Xx∈B
∆min
√2fn (cid:19)

≤

(cid:18)

∆2
y
2fn

.

≤

Tx ≤

1
∆min

Xx:∆x>0

≤

Xx:∆x>0

Tx∆x

Xx:∆x>0
Sx∆x ≤

2d3∆maxfn
∆3

min

.

(cid:3)

E PROOF OF LEMMA 13

The proof of Lemma 13 requires one more technical re-
sult.
Lemma 17. Let ε > 0 and recall the deﬁnition of Tn( ˆ∆)
given in Deﬁnition 9. For m

N deﬁne

Sn,m( ˆ∆) = min

∈
mfn, Tn( ˆ∆)

n

.

o

Then there exists an m such that for all n
[0,

)k and x

∈

∞

∈ A

N and ˆ∆

∈

2
H−1

x
k

k

Sn,m( ˆ∆) ≤

max

ε2
fn

,

ˆ∆2
x
fn )

.

(

Proof of Lemma 13. Assume that F ′
consider three cases.

n does not hold. We

Case 1. ˆ∆x∗ > 0.
Case 2. ˆ∆x∗ = 0 and ˆ∆min > ∆min/4.
Case 3. ˆ∆x∗ = 0 and ˆ∆min ≤
The idea is to show that in each case the regret is at most
logarithmic, with a leading constant that depends on θ
and
, but not on the observed samples. Treating each
case separately.

∆min/4.

A

∈

Case 1 Recall that ˆ∆
Rk (indexed by the actions)
is the empirical estimate of the sub-optimality gaps after
the warm-up phase. Let x be the sub-optimal arm for
which ˆ∆x = 0. By the deﬁnition of the optimisation
problem this arm will be played in every while loop. Let
t be the ﬁrst round when for all x it holds that

x
k

k

2
G−1

t ≤

max

ˆ∆2
x
fn

,

∆2
min
16fn )

.

(
By Lemma 17 there exists a constant m1 depending only
on

and θ such that

A

m1fn .
≤
By the assumption that F ′
n does not hold (and its deﬁni-
tion) we have

t

ˆµx∗ (t)

µx∗

max

ˆ∆x∗, ∆min/4

≥

≥

≥

n
ˆ∆x∗

−
µx + ∆x −
ˆµx(t) + ∆x −
∆min
ˆµx(t) +
2

o

∆min
4

−
∆min

ˆ∆x∗

2 −

≥

+ ˆµx∗(t0)

ˆµx(t0) ,

≥
is the round at the end of the
where t0 = d
⌈
warm-up phase. Therefore if n is sufﬁciently large that
∆min/2

log1/2(n)
⌉

4εn, then

−

∆min

ˆµx∗ (t)

ˆµx∗(t0) + ˆµx(t0)

ˆµx(t)

−

≥
−
(a + b)/2 for all
which by the fact that max
a, b
{
R implies that the success phase of the algorithm
a, b
ends. Therefore if n is sufﬁciently large, then in case 1
the regret in the success phase is at most

2 ≥

} ≥

∈

4εn ,

∆At ≤

∆maxm1fn .

Xt∈Tsucc.

(26)

13

Case 2 Recall that ˆT is the strategy used in the success
phase based on samples collected in the warm-up phase.
Since ˆ∆x∗ = 0 and ˆ∆min ≥
∆min/4, by Lemma 12 it
holds that

2

·

ˆTx ≤

43d3fn∆max

.

∆3

min

Xx6=x∗

And again we have that for sufﬁciently large n that the
regret in the success phase is at most

∆At ≤

2

·

43d3fn∆2
∆3

min

max

.

(27)

Xt∈Tsucc.

Case 3 For the ﬁnal case we assume that ˆ∆x∗ = 0 and
there exists an x for which ˆ∆x ≤
∆min/4. Let t be the
it holds that
ﬁrst time-step when for all x
∈ A
∆2
min
64fn

x
k
k

2
G−1

t ≤

max

ˆ∆2
x
fn )

(

,

Then by Lemma 17 there exists a constant m2 that is
independent of ˆ∆ and n such that t
m2fn. Then since
F ′

n does not hold we have

≤

ˆµx∗ (t)

ˆµx∗ (t0) + ˆµx(t0)

−

ˆµx∗(t)

ˆµx(t)

−

∆min

2 ≥

2εn .

≥

≥

−

ˆµx(t)
ˆ∆x ≥

−

∆min

4 −

ˆ∆x

Therefore provided that n is sufﬁciently large, the suc-
cess phase ends and by the same reasoning as in Case 1
the regret in the success phase is bounded by

∆At ≤

∆maxm2fn .

(28)

Xt∈Tsucc.

The proof of the lemma is completed by combining (26),
(27) and (28), which imply the existence of a constant
m3 that is independent of n and ˆ∆ such that

1

not F ′
{

n}

∆At ≤

m3fn .

Xt∈Tsucc.

Therefore by (12) and the deﬁnition of fn ∼
have

2 log(n) we

t∈Tsucc.

∆At

P

(cid:3)

lim sup
n→∞

E

1

Fn and not F ′
{

n}
log(n)

m3fn]

(cid:2)
E[1

Fn}
{
log(n)
P (Fn) m3fn
log(n)

lim sup
n→∞

≤

= lim sup
n→∞

lim sup
n→∞

≤

m3fn
log2(n)

= 0 .

