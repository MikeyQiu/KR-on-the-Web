7
1
0
2
 
n
u
J
 
2
 
 
]
L
M

.
t
a
t
s
[
 
 
1
v
8
3
3
1
0
.
6
0
7
1
:
v
i
X
r
a

Understanding LISTA

Understanding the Learned Iterative Soft Thresholding
Algorithm with matrix factorization

thomas.moreau@cmla.ens-cachan.fr

bruna@cims.nyu.edu

Thomas Moreau
CMLA, ENS Cachan, CNRS,
Universit´e Paris-Saclay,
94235 Cachan, France

Joan Bruna
Courant Institute of Mathematical Sciences,∗
New York University ,
New York, NY 10012, USA

Editor:

Abstract

Sparse coding is a core building block in many data analysis and machine learning pipelines.
Typically it is solved by relying on generic optimization techniques, such as the Iterative
Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). These methods
are optimal in the class of ﬁrst-order methods for non-smooth, convex functions. How-
ever, they do not exploit the particular structure of the problem at hand nor the input
data distribution. An acceleration using neural networks, coined LISTA, was proposed in
Gregor and Le Cun (2010), which showed empirically that one could achieve high qual-
ity estimates with few iterations by modifying the parameters of the proximal splitting
appropriately.

In this paper we study the reasons for such acceleration. Our mathematical analysis reveals
that it is related to a speciﬁc matrix factorization of the Gram kernel of the dictionary, which
attempts to nearly diagonalise the kernel with a basis that produces a small perturbation of
the (cid:96)1 ball. When this factorization succeeds, we prove that the resulting splitting algorithm
enjoys an improved convergence bound with respect to the non-adaptive version. Moreover,
our analysis also shows that conditions for acceleration occur mostly at the beginning of the
iterative process, consistent with numerical experiments. We further validate our analysis
by showing that on dictionaries where this factorization does not exist, adaptive acceleration
fails.

1. Introduction

Feature selection is a crucial point in high dimensional data analysis. Diﬀerent techniques
have been developed to tackle this problem eﬃciently, and amongst them sparsity has
emerged as a leading paradigm. In statistics, the LASSO estimator (Tibshirani, 1996) pro-

∗. Work done while appointed at UC Berkeley, Statistics Department (currently on leave)

Ongoing work - This document is not ﬁnished or considered for publication yet.

1

Moreau and Bruna

vides a reliable way to select features and has been extensively studied in the last two
decades (Hastie et al. (2015) and references therein). In machine learning and signal pro-
cessing, sparse coding has made its way into several modern architectures, including large
scale computer vision (Coates and Ng, 2011) and biologically inspired models (Cadieu and
Olshausen, 2012). Also, Dictionary learning is a generic unsupervised learning method to
perform nonlinear dimensionality reduction with eﬃcient computational complexity (Mairal
et al., 2009). All these techniques heavily rely on the resolution of (cid:96)1-regularized least
squares.

The (cid:96)1-sparse coding problem is deﬁned as solving, for a given input x ∈ Rn and dictionary
D ∈ Rn×m, the following problem:

z∗(x) = arg min

Fx(z) ∆=

(cid:107)x − Dz(cid:107)2 + λ(cid:107)z(cid:107)1 .

z

1
2

(1)

This problem is convex and can therefore be solved using convex optimization machinery.
Proximal splitting methods (Beck and Teboulle, 2009) alternate between the minimization
of the smooth and diﬀerentiable part using the gradient information and the minimization
of the non-diﬀerentiable part using a proximal operator (Combettes and Bauschke, 2011).
These methods can also be accelerated by considering a momentum term, as it is done in
FISTA (Beck and Teboulle, 2009; Nesterov, 2005). Coordinate descent (Friedman et al.,
2007; Osher and Li, 2009) leverages the closed formula that can be derived for optimizing
the problem (1) for one coordinate zi given that all the other are ﬁxed. At each step of
the algorithm, one coordinate is updated to its optimal value, which yields an inexpensive
scheme to perform each step. The choice of the coordinate to update at each step is
critical for the performance of the optimization procedure. Least Angle Regression (LARS)
(Hesterberg et al., 2008) is another method that computes the whole LASSO regularization
path. These algorithms all provide an optimization procedure that leverages the local
properties of the cost function iteratively. They can be shown to be optimal among the
class of ﬁrst-order methods for generic convex, non-smooth functions (Bubeck, 2014).

But all these results are given in the worst case and do not use the distribution of the
considered problem. One can thus wonder whether a more eﬃcient algorithm to solve
(1) exists for a ﬁxed dictionary D and generic input x drawn from a certain input data
distribution. In Gregor and Le Cun (2010), the authors introduced LISTA, a trained version
of ISTA that adapts the parameters of the proximal splitting algorithm to approximate the
solution of the LASSO using a ﬁnite number of steps. This method exploits the common
structure of the problem to learn a better transform than the generic ISTA step. As ISTA
is composed of a succession of linear operations and piecewise non linearities, the authors
use the neural network framework and the backpropagation to derive an eﬃcient procedure
solving the LASSO problem. In Sprechmann et al. (2012), the authors extended LISTA
to more generic sparse coding scenarios and showed that adaptive acceleration is possible
under general input distributions and sparsity conditions.

In this paper, we are interested in the following question: Given a ﬁnite computational bud-
get, what is the optimum estimator of the sparse coding? This question belongs to the gen-
eral topic of computational tradeoﬀs in statistical inference. Randomized sketches (Alaoui
and Mahoney, 2015; Yang et al., 2015) reduce the size of convex problems by projecting

Ongoing work - This document is not ﬁnished or considered for publication yet.

2

Understanding LISTA

expensive kernel operators into random subspaces, and reveal a tradeoﬀ between computa-
tional eﬃciency and statistical accuracy. Agarwal (2012) provides several theoretical results
on perfoming inference under various computational constraints, and Chandrasekaran and
Jordan (2013) considers a hierarchy of convex relaxations that provide practical tradeoﬀs
between accuracy and computational cost. More recently, Oymak et al. (2015) provides
sharp time-data tradeoﬀs in the context of linear inverse problems, showing the existence
of a phase transition between the number of measurements and the convergence rate of
the resulting recovery optimization algorithm. Giryes et al. (2016) builds on this result
to produce an analysis of LISTA that describes acceleration in conditions where the it-
erative procedure has linear convergence rate. Finally, Xin et al. (2016) also studies the
capabilities of Deep Neural networks at approximating sparse inference. The authors show
that unrolled iterations lead to better approximation if one allows the weights to vary at
each layer, contrary to standard splitting algorithms. Whereas their focus is on relaxing
the convergence hypothesis of iterative thresholding algorithms, we study a complementary
question, namely when is speedup possible, without assuming strongly convex optimization.
Their results are consistent with ours, since our analysis also shows that learning shared
layer weights is less eﬀective.

Inspired by the LISTA architecture, our mathematical analysis reveals that adaptive accel-
eration is related to a speciﬁc matrix factorization of the Gram matrix of the dictionary
B = DTD as B = ATSA − R ,where A is unitary, S is diagonal and the residual is positive
semideﬁnite: R (cid:23) 0. Our factorization balances between near diagonalization by asking
that (cid:107)R(cid:107) is small and small perturbation of the (cid:96)1 norm, i.e. (cid:107)Az(cid:107)1 − (cid:107)z(cid:107)1 is small. When
this factorization succeeds, we prove that the resulting splitting algorithm enjoys a conver-
gence rate with improved constants with respect to the non-adaptive version. Moreover,
our analysis also shows that acceleration is mostly possible at the beginning of the iterative
process, when the current estimate is far from the optimal solution, which is consistent with
numerical experiments. We also show that the existence of this factorization is not only
suﬃcient for acceleration, but also necessary. This is shown by constructing dictionaries
whose Gram matrix diagonalizes in a basis that is incoherent with the canonical basis, and
verifying that LISTA fails in that case to accelerate with respect to ISTA.

In our numerical experiments, we design a specialized version of LISTA called FacNet, with
more constrained parameters, which is then used as a tool to show that our theoretical anal-
ysis captures the acceleration mechanism of LISTA. Our theoretical results can be applied
to FacNet and as LISTA is a generalization of this model, it always performs at least as
well, showing that the existence of the factorization is a suﬃcient certiﬁcate for accelera-
tion by LISTA. Reciprocally, we show that for cases where no acceleration is possible with
FacNet, the LISTA model also fail to provide acceleration, linking the two speedup mecha-
nisms. This numerical evidence suggest that the existence of our proposed factorization is
suﬃcient and somewhat necessary for LISTA to show good results.

The rest of the paper is structured as follows. Section 2 presents our mathematical analysis
and proves the convergence of the adaptive algorithm as a function of the quality of the
matrix factorization. In Section 3, we prove that for generic dictionaries, drawn uniformly
on the (cid:96)2 unit sphere, it is possible to accelerate ISTA in our framework. Finally, Sec-

Ongoing work - This document is not ﬁnished or considered for publication yet.

3

Moreau and Bruna

tion 4 presents the generic architectures that will enable the usage of such schemes and the
numerical experiments, which validate our analysis over a range of diﬀerent scenarios.

2. Accelerating Sparse Coding with Sparse Matrix Factorizations

2.1 Unitary Proximal Splitting

In this section we describe our setup for accelerating sparse coding based on the Proximal
Splitting method. Let Ω ⊂ Rn be the set describing our input data, and D ∈ Rn×m be a
dictionary, with m > n. We wish to ﬁnd fast and accurate approximations of the sparse
coding z∗(x) of any x ∈ Ω, deﬁned in (1) For simplicity, we denote B = DTD and y = D†x
to rewrite (1) as

z∗(x) = arg min

Fx(z) =

z

1
2
(cid:124)

(y − z)TB(y − z)
(cid:123)(cid:122)
(cid:125)
E(z)

+ λ(cid:107)z(cid:107)1
(cid:124) (cid:123)(cid:122) (cid:125)
G(z)

.

(2)

For clarity, we will refer to Fx as F and to z∗(x) as z∗. The classic proximal splitting
technique ﬁnds z∗ as the limit of sequence (zk)k, obtained by successively constructing a
surrogate loss Fk(z) of the form

Fk(z) = E(zk) + (zk − y)TB(z − zk) + Lk(cid:107)z − zk(cid:107)2

2 + λ(cid:107)z(cid:107)1 ,

(3)

satisfying Fk(z) ≥ F (z) for all z ∈ Rm . Since Fk is separable in each coordinate of z,
zk+1 = arg minz Fk(z) can be computed eﬃciently. This scheme is based on a majoration
of the quadratic form (y − z)TB(y − z) with an isotropic quadratic form Lk(cid:107)zk − z(cid:107)2
2.
The convergence rate of the splitting algorithm is optimized by choosing Lk as the smallest
constant satisfying Fk(z) ≥ F (z), which corresponds to the largest singular value of B.

The computation of zk+1 remains separable by replacing the quadratic form LkI by any
diagonal form. However, the Gram matrix B = DTD might be poorly approximated via
diagonal forms for general dictionaries. Our objective is to accelerate the convergence of
this algorithm by ﬁnding appropriate factorizations of the matrix B such that

B ≈ ATSA , and (cid:107)Az(cid:107)1 ≈ (cid:107)z(cid:107)1 ,

where A is unitary and S is diagonal positive deﬁnite. Given a point zk at iteration k, we
can rewrite F (z) as

F (z) = E(zk) + (zk − y)TB(z − zk) + QB(z, zk) ,

(4)

1
2

(v − w)TB(v − w) + λ(cid:107)v(cid:107)1 . For any diagonal positive deﬁnite matrix S

with QB(v, w) :=
and unitary matrix A, the surrogate loss (cid:101)F (z, zk) := E(zk) + (zk − y)TB(z − zk) + QS(Az, Azk)
can be explicitly minimized, since

arg min

z

(cid:101)F (z, zk) = AT arg min
u

(cid:16)

(cid:17)
(zk − y)TBAT(u − Azk) + QS(u, Azk)

= AT arg min

QS

u

(cid:16)

(cid:17)
u, Azk − S−1AB(zk − y)

(5)

Ongoing work - This document is not ﬁnished or considered for publication yet.

4

Understanding LISTA

where we use the variable change u = Az. As S is diagonal positive deﬁnite, (5) is separable
and can be computed easily, using a linear operation followed by a point-wise non linear
soft-thresholding. Thus, any couple (A, S) ensures an computationally cheap scheme. The
question is then how to factorize B using S and A in an optimal manner, that is, such that
the resulting proximal splitting sequence converges as fast as possible to the sparse coding
solution.

2.2 Non-asymptotic Analysis

We will now establish convergence results based on the previous factorization. These bounds
will inform us on how to best choose the factors Ak and Sk in each iteration.

For that purpose, let us deﬁne

δA(z) = λ

(cid:107)Az(cid:107)1 − (cid:107)z(cid:107)1

, and R = ATSA − B .

(cid:16)

(cid:17)

The quantity δA(z) thus measures how invariant the (cid:96)1 norm is to the unitary operator A,
whereas R corresponds to the residual of approximating the original Gram matrix B by our
factorization ATSA . Given a current estimate zk, we can rewrite

(cid:101)F (z, zk) = F (z) +

(z − zk)TR(z − zk) + δA(z) .

1
2

By imposing that R is a positive semideﬁnite residual one immediately obtains the following
bound.

Proposition 1 Suppose that R = ATSA − B is positive deﬁnite, and deﬁne

Then

F (zk+1) − F (z∗) ≤

(cid:107)R(cid:107)(cid:107)zk − z∗(cid:107)2

2+δA(z∗) − δA(zk+1) .

zk+1 = arg min

(cid:101)F (z, zk) .

z

1
2

Proof By deﬁnition of zk+1 and using the fact that R (cid:31) 0 we have

F (zk+1) − F (z∗) ≤ F (zk+1) − (cid:101)F (zk+1, zk) + (cid:101)F (z∗, zk) − F (z∗)
(zk+1 − zk)TR(zk+1 − zk) − δA(zk+1) +

= −

1
2
(z∗ − zk)TR(z∗ − zk) +

1
2
δA(z∗) − δA(zk+1)

(cid:16)

≤

.

1
2

(z∗ − zk)TR(z∗ − zk) + δA(z∗)
(cid:17)

where the ﬁrst line results from the deﬁnition of zk+1 and the third line makes use of R
positiveness.

This simple bound reveals that to obtain fast approximations to the sparse coding it is
suﬃcient to ﬁnd S and A such that (cid:107)R(cid:107) is small and that the (cid:96)1 commutation term δA
is small. These two conditions will be often in tension: one can always obtain R ≡ 0 by

Ongoing work - This document is not ﬁnished or considered for publication yet.

5

(6)

(7)

(8)

(9)

Moreau and Bruna

using the Singular Value Decomposition of B = AT
0 S0A0 and setting A = A0 and S = S0.
However, the resulting A0 might introduce large commutation error δA0. Similarly, as the
(cid:12)
(cid:12) ≤ (cid:12)
(cid:12)
absolute value is non-expansive, i.e.

(cid:12)
(cid:12)
(cid:12)|a| − |b|

(cid:12), we have that

(cid:12)a − b(cid:12)

|δA(z)| = λ

(cid:12)
(cid:12)
(cid:12)(cid:107)Az(cid:107)1 − (cid:107)z(cid:107)1

(cid:12)
(cid:12)
(cid:12) ≤ λ(cid:107)(A − I)z(cid:107)1

(10)

≤ λ(cid:112)2 max((cid:107)Az(cid:107)0, (cid:107)z(cid:107)0) · (cid:107)A − I(cid:107) · (cid:107)z(cid:107)2 ,

where we have used the Cauchy-Schwartz inequality (cid:107)x(cid:107)1 ≤ (cid:112)(cid:107)x(cid:107)0(cid:107)x(cid:107)2 in the last equation.
In particular, (10) shows that unitary matrices in the neighborhood of I with (cid:107)A − I(cid:107) small
have small (cid:96)1 commutation error δA but can be inappropriate to approximate general B
matrix.

The commutation error also depends upon the sparsity of z and Az . If both z and Az are
sparse then the commutation error is reduced, which can be achieved if A is itself a sparse
unitary matrix. Moreover, since

|δA(z) − δA(z(cid:48))| ≤ λ|(cid:107)z(cid:107)1 − (cid:107)z(cid:48)(cid:107)1| + λ|(cid:107)Az(cid:107)1 − (cid:107)Az(cid:48)(cid:107)1|
and |(cid:107)z(cid:107)1 − (cid:107)z(cid:48)(cid:107)1| ≤ (cid:107)z − z(cid:48)(cid:107)1 ≤ (cid:112)(cid:107)z − z(cid:48)(cid:107)0(cid:107)z − z(cid:48)(cid:107)2
it results that δA is Lipschitz with respect to the Euclidean norm; let us denote by LA(z)
its local Lipschitz constant in z, which can be computed using the norm of the subgradient
in z1. An uniform upper bound for this constant is (1 + (cid:107)A(cid:107)1)λ
m, but it is typically much
smaller when z and Az are both sparse.
Equation (8) deﬁnes an iterative procedure determined by the pairs {(Ak, Sk)}k. The fol-
lowing theorem uses the previous results to compute an upper bound of the resulting sparse
coding estimator.

√

Theorem 2 Let Ak, Sk be the pair of unitary and diagonal matrices corresponding to iter-
ation k, chosen such that Rk = AT

k SkAk − B (cid:31) 0. It results that

F (zk) − F (z∗) ≤

(z∗ − z0)TR0(z∗ − z0) + 2LA0(z1)(cid:107)z∗ − z1(cid:107)2
2k

+

α − β
2k

,

(11)

with

α =

(cid:16)

(cid:17)
2LAi (zi+1)(cid:107)z∗ − zi+1(cid:107)2 + (z∗ − zi)T(Ri−1 − Ri)(z∗ − zi)

,

β =

(i + 1)

(zi+1 − zi)TRi(zi+1 − zi) + 2δAi(zi+1) − 2δAi(zi)

(cid:16)

(cid:17)

,

k−1
(cid:88)

i=1

k−1
(cid:88)

i=0

where LA(z) denote the local lipschitz constant of δA at z.

1. This quantity exists as δA is a diﬀerence of convex. See proof of Proposition B.1 in appendices for

details.

Ongoing work - This document is not ﬁnished or considered for publication yet.

6

Understanding LISTA

(12)

(13)

(14)

Remark If one sets Ak = I and Sk = (cid:107)B(cid:107)I for all k ≥ 0, (11) corresponds to the bound
of the ISTA algorithm (Beck and Teboulle, 2009).

The proof is deferred to Section B. We can specialize the theorem in the case when A0, S0
are chosen to minimize the bound (9) and Ak = I, Sk = (cid:107)B(cid:107)I for k ≥ 1.

Corollary 3 If Ak = I, Sk = (cid:107)B(cid:107)I for k ≥ 1 then

F (zk)−F (z∗) ≤

(z∗ − z0)TR0(z∗ − z0) + 2LA0 (z1)((cid:107)z∗ − z1(cid:107) + (cid:107)z1 − z0(cid:107)) + (z∗ − z1)TR0(z∗ − z1)T
2k

.

This corollary shows that by simply replacing the ﬁrst step of ISTA by the modiﬁed proximal
step detailed in (5), one can obtain an improved bound at ﬁxed k as soon as

2(cid:107)R0(cid:107) max((cid:107)z∗ − z0(cid:107)2

2, (cid:107)z∗ − z1(cid:107)2

2) + 4LA0(z1) max((cid:107)z∗ − z0(cid:107)2, (cid:107)z∗ − z1(cid:107)2) ≤ (cid:107)B(cid:107)(cid:107)z∗ − z0(cid:107)2
2 ,

which, assuming (cid:107)z∗ − z0(cid:107)2 ≥ (cid:107)z∗ − z1(cid:107)2, translates into

More generally, given a current estimate zk, searching for a factorization (Ak, Sk) will im-
prove the upper bound when

(cid:107)R0(cid:107) + 2

LA0(z1)
(cid:107)z∗ − z0(cid:107)2

≤

(cid:107)B(cid:107)
2

.

(cid:107)Rk(cid:107) + 2

LAk (zk+1)
(cid:107)z∗ − zk(cid:107)2

≤

(cid:107)B(cid:107)
2

.

We emphasize that this is not a guarantee of acceleration, since it is based on improving
an upper bound. However, it provides a simple picture on the mechanism that makes
non-asymptotic acceleration possible.

2.3 Interpretation

In this section we analyze the consequences of Theorem 2 in the design of fast sparse
coding approximations, and provide a possible explanation for the behavior observed nu-
merically.

2.3.1 ‘Phase Transition” and Law of Diminishing Returns

(14) reveals that the optimum matrix factorization in terms of minimizing the upper bound
depends upon the current scale of the problem, that is, of the distance (cid:107)z∗ − zk(cid:107). At the
beginning of the optimization, when (cid:107)z∗ − zk(cid:107) is large, the bound (14) makes it easier to
explore the space of factorizations (A, S) with A further away from the identity. Indeed,
the bound tolerates larger increases in LA(zk+1), which is dominated by

LA(zk+1) ≤ λ((cid:112)(cid:107)zk+1(cid:107)0 + (cid:112)(cid:107)Azk+1(cid:107)0) ,

i.e. the sparsity of both z1 and A0(z1). On the other hand, when we reach intermediate
solutions zk such that (cid:107)z∗ − zk(cid:107) is small with respect to LA(zk+1), the upper bound is

Ongoing work - This document is not ﬁnished or considered for publication yet.

7

Moreau and Bruna

minimized by choosing factorizations where A is closer and closer to the identity, leading
to the non-adaptive regime of standard ISTA (A = Id).

This is consistent with the numerical experiments, which show that the gains provided by
learned sparse coding methods are mostly concentrated in the ﬁrst iterations. Once the
estimates reach a certain energy level, section 4 shows that LISTA enters a steady state in
which the convergence rate matches that of standard ISTA.

The natural follow-up question is to determine how many layers of adaptive splitting are
suﬃcient before entering the steady regime of convergence. A conservative estimate of this
quantity would require an upper bound of (cid:107)z∗ − zk(cid:107) from the energy bound F (zk) − F (z∗).
Since in general F is convex but not strongly convex, such bound does not exist unless one
can assume that F is locally strongly convex (for instance for suﬃciently small values of
F ).

2.3.2 Improving the factorization to particular input distributions

Given an input dataset D = (xi, z(0)
i
z(0)
and sparse coding solutions z∗
i , the factorization adapted to D is deﬁned as
i

i )i≤N , containing examples xi ∈ Rn, initial estimates

, z∗

min
A,S; ATA=I,ATSA−B(cid:31)0

1
N

(cid:88)

i≤N

1
2

(z(0)

i − z∗

i )T(ATSA − B)(z(0)

i − z∗

i ) + δA(z∗

i ) − δA(z1,i) . (15)

Therefore, adapting the factorization to a particular dataset, as opposed to enforcing it
uniformly over a given ball B(z∗; R) (where the radius R ensures that the initial value
z0 ∈ B(z∗; R)), will always improve the upper bound (9). Studying the gains resulting from
the adaptation to the input distribution will be let for future work.

3. Generic gap control

In this section, we consider the problem of accelerating the resolution of (1) in the case
where D is a generic dictionary, i.e. its elements Di are draw uniformly over the (cid:96)2 unit-
sphere.

Deﬁnition 4 (Generic dictionary) A dictionary D ∈ Rp×K is a generic dictionary when
its columns Di are drawn uniformly over the (cid:96)2 unit sphere S p−1.

The results by Song and Gupta (1997) show that such dictionaries emerge when the atoms
are drawn independently from normal distributions N (0, Ip) and then normalized on the unit
with di ∼ N (0, Ip) for all i ∈ (cid:8)1..K(cid:9). In this context, we consider
sphere. Thus, Di = di
(cid:107)di(cid:107)2
the matrices A which are perturbation of the identity and highlight the conditions under
which it is possible to ﬁnd a perturbation of the identity A which is more advantageous than
the identity to resolve (1). For a ﬁxed integer i ∈ [K], ei denotes the canonical direction
and we introduce Eδ,i , the ensemble such that

(cid:26)

Eδ,i =

u ∈ RK : ∃µ < δ, ∃hi ∈ Span(ei)⊥ ∩ S K−1 s.t u =

1 − µ2ei + µhi

,

(cid:112)

(cid:27)

Ongoing work - This document is not ﬁnished or considered for publication yet.

8

Understanding LISTA

This ensemble contains the vectors which are mainly supported by one of the canonical
u ∈ RK : (cid:107)u(cid:107)2 = 1, (cid:107)u(cid:107)∞ >
directions. Indeed, ∪K
We will denote A ⊂
Eδ when a matrix A is such that each of its columns Ai are in Eδ,i. These matrices are
diagonally dominant and are close to the identity when δ is close to 0, as (cid:107)A − I(cid:107)F =
Kδ .

1 − δ2(cid:111)

i=1Eδ,i =

√

(cid:110)

3.1 Control the deviation of the space rotation for B

First, we analyze the possible gain of replacing B by an approximate diagonalization A−1SA
for a diagonally dominant matrix A ⊂ Eδ . We choose to study the case where S is chosen
deterministicaly when A is ﬁxed. For A, B ﬁxed, we choose the matrix S which minimizes
the frobenius norm of the diagonalization error, i.e.

S = argmax
S(cid:48)diagonal

(cid:107)B − ATS(cid:48)A(cid:107)F

(16)

This matrix S can easily be computed as Si,i = AT

i BAi .

Lemma 5 For a generic dictionary D and a diagonally dominant matrix A ⊂ Eδ,

(cid:34)

ED

min
Ai∈Eδ,i

(cid:13)
(cid:13)A−1SA − B
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

≤

K(K − 1)
p

(cid:35)

− 4δ(K − 1)

(cid:115)

K
p

(cid:32)

(cid:33)

+ δ2

8ED

(cid:107)B(cid:107)4
F

− 6

(cid:104)

(cid:105)

K(K − 1)
p

(cid:16)

δ3(cid:17)

.

+ O
δ→0

Proof sketch for Lemma 5. (The full proof can be found Subsection C.2)
Using the properties of the matrix A ⊂ Eδ we can show that

(cid:13)
(cid:13)A−1SA − B
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

≤ (cid:13)

(cid:13)B(cid:13)
2
F (1 + 8δ2K) −
(cid:13)

(cid:107)DAi(cid:107)4

2 + O
δ→0

(cid:16)

δ3(cid:17)

.

(17)

K
(cid:88)

i=1

The ﬁrst term is the squared Frobenius norm of a Wishart matrix and we can show

(cid:104)

ED

(cid:107)B(cid:107)2
F

(cid:105)

=

K(K − 1)
p

+ K .

The columns Ai are chosen in Eδ,i, we can thus show that

(cid:34)

ED

max
u∈Eδ,i

(cid:35)

(cid:107)Du(cid:107)4
2

≥ 1 + 4δED

(cid:107)DTdi(cid:107)2

2 − 1

+ 6δ2ED

(cid:107)DTdi(cid:107)2

2 − 1

(cid:20)(cid:113)

(cid:21)

(cid:104)

(cid:105)

+ O
δ→0

(cid:16)

δ3(cid:17)

. (18)

Denoting Yi the random variable such that pY 2
lower bounds

i = p((cid:107)DTdi(cid:107)2

2 − 1), we can compute the

ED

(cid:2)Yi

(cid:3) =

(cid:114) 2
p

(cid:17)

(cid:16) K
Γ
2
(cid:16) K−1
2

Γ

(cid:17) ≥

K − 1
√
pK

and ED

(cid:105)

(cid:104)

Y 2
i

=

K − 1
p

Ongoing work - This document is not ﬁnished or considered for publication yet.

9

Moreau and Bruna

Combining these results with (18) yields the following lower bound when δ → 0 ,

(cid:34)

ED

max
u∈Eδ,i

(cid:35)

(cid:107)DTu(cid:107)4
2

(cid:38) 1 + 4δ

K − 1
√
pK

+ 6δ2 K − 1

+ O
δ→0

p

(cid:16)

δ3(cid:17)

The ﬁnal bound is obtained using these results with (17).

3.2 Controling Ez∼Z

(cid:104)

(cid:105)
δA(z)

In this subsection, we analyze the deformation of the (cid:96)1-norm due to a rotation of the code
space with a diagonally dominant matrix A ⊂ Eδ .

Lemma 6 Let A ⊂ Eδ be a diagonally dominant matrix and let z be a random variable in
RK with iid coordinates zi. Then

Ez,D

(cid:104)
δA(z)

(cid:105)

≤

(cid:32)
δ

√

K − 1 −

δ2
2

+ O
δ→0

(cid:33)

(cid:16)

δ4(cid:17)

(cid:104)
(cid:107)z(cid:107)1

(cid:105)

Ez

Proof sketch for Lemma 6. (The full proof can be found Subsection C.3)
First, we show that if z is a random variable in RK with iid coordinates zi, then

This permits to decouple the expectations and we obtain the following upper bound

Ez,D

(cid:34)

(cid:107)Az(cid:107)1
(cid:107)z(cid:107)1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

(cid:104)

ED

(cid:107)A(cid:107)1,1

(cid:105)

(cid:107)z(cid:107)1

≤

.

K

(cid:104)
δA(z)

(cid:105)

Ez

≤

(cid:104)
(cid:107)A(cid:107)1,1

(cid:105)

ED

− (cid:107)I(cid:107)1,1

K

(cid:104)
(cid:107)z(cid:107)1

(cid:105)

.

Ez

Then, for A ⊂ Eδ, the (cid:96)1-norm of the columns Ai is

(cid:104)

ED

(cid:105)

(cid:112)

(cid:107)Ai(cid:107)1

≤

√

1 − δ2 + δ

K − 1 .

Basic computations permit to show that

(cid:104)

ED

(cid:107)A(cid:107)1,1

− IK

√

(cid:105)

K

≤ δ

K − 1 −

δ2
2

+ O
δ→0

(cid:16)

δ4(cid:17)

.

δ→0

Ongoing work - This document is not ﬁnished or considered for publication yet.

10

Understanding LISTA

3.3 Accelerating sparse coding resolution

The two previous results permits to control the upper bound of the cost update deﬁned in
Proposition 1 for generic dictionaries. It si interesting to see when this upper bound become
smaller than the upper bound obtained using the identity I.

Theorem 7 (Acceleration certiﬁcate) Given a generic dictionary D, it is possible ﬁnd
a diagonally dominant matrix A ⊂ Eδ, which provide better performance than identity to
solve (1) when

(cid:16)

λ

(cid:107)z(cid:107)1 + (cid:107)z∗(cid:107)

≤

(cid:17)

(cid:115)

K(K − 1)
p

(cid:107)zk − z∗(cid:107)2
2

Proof sketch for Theorem 7. (The full proof can be found Subsection C.4)
For A ⊂ Eδ with columns chosen greedily in Eδ,i, using results from Lemma 5 and Lemma 6,
(cid:35)
2 + λδA(z)

(cid:13)
(cid:13)A−1SA − B
(cid:13)

(cid:107)v(cid:107)2

ED

≤

(cid:34)

(cid:13)
2
(cid:13)
(cid:13)
F

min
A⊂Eδ

(K − 1)K
p

√

(cid:107)v(cid:107)2

2 + δ

K − 1

λ(cid:107)z(cid:107)1 −



(cid:115)

K(K − 1)
p

(cid:107)v(cid:107)2
2

 + O
δ→0

(cid:16)

δ2(cid:17)



(19)

Starting from Proposition 1, and using the results from as

ED

(cid:104)
F (zk+1) − F (z∗)

(cid:105)

≤

(K − 1)K
p

(cid:107)zk − z∗(cid:107)2
2

√

+ δ

K − 1

λ



(cid:16)

(cid:124)

(cid:107)z(cid:107)1 + (cid:107)z∗(cid:107)

−

(cid:17)

K(K − 1)
p

(cid:107)zk − z∗(cid:107)2
2



+ O
δ→0

(cid:16)

δ2(cid:17)

(cid:115)

(cid:123)(cid:122)
≤0



(cid:125)

Gap between I and A∗
trying to minimize. If

Let f (A) ∆= 1

2 vT(ABAT − S)v + λδA(z) be the function we are

(cid:115)

λ(cid:107)z(cid:107)1 ≤

(cid:105)
(cid:104)
f (A)

(cid:107)v(cid:107)2
2

K(K − 1)
p
(cid:104)
f (I)

≤ ED

(cid:105)

(20)

= 0 and there is a gap to improve

Then for δ close to 0, we have ED
the factorization.

Remark 1 This does not depend on D as it is a result in expectation. But this means
that there is a gap for most of the matrices D.

Remark 2 This gap seems to be consistent with our adversarial dictionary. See the dif-
ferences in Figure 1 between a Gaussian dictionary and a worst case, adversarial one.

Ongoing work - This document is not ﬁnished or considered for publication yet.

11

Moreau and Bruna

Figure 1: Evolution of the gap condition with the iteration of ISTA on very small problems

for an adversarial dictionary and a gaussian dictionary.

X

We

Z

X

W (0)
e

W (1)
e

W (2)
e

Wg

W (1)
g

W (2)
g

Z

(a) ISTA - Recurrent Neural Network

(b) LISTA - Unfolded network

Figure 2: Network architecture for ISTA/LISTA. The unfolded version (b) is trainable
through backpropagation and permits to approximate the sparse coding solution
eﬃciently.

4. Numerical Experiments

This section provides numerical arguments to analyse adaptive optimization algorithms
and their performances, and relates them to the theoretical properties developed in the
previous section. All the experiments were run using Python and Tensorﬂow. For all the
experiments, the training is performed using Adagrad (Duchi et al., 2011). The code to
reproduce the ﬁgures is available online2.

4.1 Adaptive Optimization Networks Architectures

LISTA/LFISTA In Gregor and Le Cun (2010), the authors introduced LISTA, a neural
network constructed by considering ISTA as a recurrent neural net. At each step, ISTA

2. The code can be found at https://github.com/tomMoral/AdaptiveOptim

Ongoing work - This document is not ﬁnished or considered for publication yet.

12

Understanding LISTA

performs the following 2-step procedure :

1. uk+1 = zk −

DT(Dzk − x) = (I −

zk +

x ,

1
DTD)
L
(cid:123)(cid:122)
(cid:125)
Wg

(cid:124)

1
DT
L
(cid:124) (cid:123)(cid:122) (cid:125)
We

1
L

2.

zk+1 = h λ

(uk+1) where hθ(u) = sign(u)(|u| − θ)+ ,

L






step k of ISTA (21)

This procedure combines a linear operation to compute uk+1 with an element-wise non
linearity.
It can be summarized as a recurrent neural network, presented in Figure 2a.,
with tied weights. The autors in Gregor and Le Cun (2010) considered the architecture ΦK
Θ
with parameters Θ = (W (k)
, θ(k))k=1,...K obtained by unfolding K times the recurrent
g
network, as presented in Figure 2b. The layers φk

, W (k)
e

Θ are deﬁned as

zk+1 = φk

Θ(zk) := hθ(Wgzk + Wex) .

(22)

e = DT

L , W (k)

g = I − DTD

If W (k)
L are ﬁxed for all the K layers, the out-
put of this neural net is exactly the vector zK resulting from K steps of ISTA. With
LISTA, the parameters Θ are learned using back propagation to minimize the cost func-
tion: f (Θ) = Ex

L and θ(k) = λ

(cid:104)
Fx(ΦK

.

(cid:105)
Θ (x))

A similar algorithm can be derived from FISTA, the accelerated version of ISTA to obtain
LFISTA (see Figure 6 in Section A ). The architecture is very similar to LISTA, now with
two memory tapes:

zk+1 = hθ(Wgzk + Wmzk−1 + Wex) .

Factorization network Our analysis in Section 2 suggests a refactorization of LISTA in
more a structured class of parameters. Following the same basic architecture, and using
(5), the network FacNet, ΨK
Θ is formed using layers such that:

zk+1 = ψk

Θ(zk) := AThλS−1(Azk − S−1A(DTDzk − DTx)) ,

(23)

with S diagonal and A unitary, the parameters of the k-th layer. The parameters obtained
after training such a network with back-propagation can be used with the theory devel-
oped in Section 2. Up to the last linear operation AT of the network, this network is a
re-parametrization of LISTA in a more constrained parameter space. Thus, LISTA is a
generalization of this proposed network and should have performances at least as good as
FacNet, for a ﬁxed number of layers.

The optimization can also be performed using backpropagation. To enforce the unitary
constraints on A(k), the cost function is modiﬁed with a penalty:

f (Θ) = Ex

(cid:104)

Fx(ΨK

(cid:105)
Θ (x))

+

(cid:16)

A(k)(cid:17)T

I −

A(k)

(24)

µ
K

K
(cid:88)

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

,

with Θ = (A(k), S(k))k=1...K the parameters of the K layers and µ a scaling factor for the
regularization. The resulting matrix A(k) is then projected on the Stiefel Manifold using a
SVD to obtain ﬁnal parameters, coherent with the network structure.

Ongoing work - This document is not ﬁnished or considered for publication yet.

13

Moreau and Bruna

Figure 3: Evolution of the cost function F (zk) − F (z∗) with the number of layers or the
number of iteration k for diﬀerent sparsity level. (left) ρ = 1/20 and (right)ρ = 1/4 .

Linear model Finally, it is important to distinguish the performance gain resulting from
choosing a suitable starting point and the acceleration from our model. To highlights the
gain obtain by changing the starting point, we considered a linear model with one layer
such that zout = A(0)x. This model is learned using SGD with the convex cost function
2 + λ(cid:107)A(0)x(cid:107)1 . It computes a tradeoﬀ between starting from the
f (A(0)) = (cid:107)(I − DA(0))x(cid:107)2
sparsest point 0 and a point with minimal reconstruction error y . Then, we observe
the performance of the classical iteration of ISTA using zout as a stating point instead of
0 .

4.2 Synthetic problems with known distributions

(cid:17)

(cid:16)

di/(cid:107)di(cid:107)2

Gaussian dictionary In order to disentangle the role of dictionary structure from the
role of data distribution structure, the minimization problem is tested using a synthetic
generative model with no structure in the weights distribution. First, m atoms di ∈ Rn are
drawn iid from a multivariate Gaussian with mean 0 and covariance In and the dictionary
D is deﬁned as
. The data points are generated from its sparse codes
following a Bernoulli-Gaussian model. The coeﬃcients z = (z1, . . . , zm) are constructed
with zi = biai, where bi ∼ B(ρ) and ai ∼ N (0, σIm) , where ρ controls the sparsity of the
data. The values are set to m=100, n=64 for the dictionary dimension, ρ = 5/m for the
sparsity level and σ=10 for the activation coeﬃcient generation parameters. The sparsity
regularization is set to λ=0.01. The batches used for the training are generated with the
model at each step and the cost function is evaluated over a ﬁxed test set, not used in the
training.

i=1...m

Figure 3 displays the cost performance for methods ISTA/FISTA/Linear relatively to their
iterations and for methods LISTA/LFISTA/FacNet relatively to the number of layers used
to solve our generated problem. Linear has performances comparable to learned methods
with the ﬁrst iteration but a gap appears as the number of layers increases, until a point
where it achieves the same performances as non adaptive methods. This highlights that the
adaptation is possible in the subsequent layers of the networks, going farther than choosing
a suitable starting point for iterative methods. The ﬁrst layers permit to achieve a large
gain over the classical optimization strategy, by leveraging the structure of the problem.

Ongoing work - This document is not ﬁnished or considered for publication yet.

14

Understanding LISTA

Figure 4: Evolution of the cost function F (zk) − F (z∗) with the number of layers or the

number of iteration k for a problem generated with an adversarial dictionary.

This appears even with no structure in the sparsity patterns of input data, in accordance
with the results in the previous section. We also observe diminishing returns as the number
of layers increases. This results from the phase transition described in Subsubsection 2.3.1,
as the last layers behave as ISTA steps and do not speed up the convergence. The 3 learned
algorithms are always performing at least as well as their classical counterpart, as it was
stated in Theorem 2. We also explored the eﬀect of the sparsity level in the training and
learning of adaptive networks. In the denser setting, the arbitrage between the (cid:96)1-norm and
the squared error is easier as the solution has a lot of non zero coeﬃcients. Thus in this
setting, the approximate method is more precise than in the very sparse setting where the
approximation must perform a ﬁne selection of the coeﬃcients. But it also yield lower gain
at the beggining as the sparser solution can move faster.

There is a small gap between LISTA and FacNet in this setup. This can be explained
from the extra constraints on the weights that we impose in the FacNet, which eﬀectively
reduce the parameter space by half. Also, we implement the unitary constraints on the
matrix A by a soft regularization (see (24)), involving an extra hyper-parameter µ that also
contributes to the small performance gap. In any case, these experiments show that our
analysis accounts for most of the acceleration provided by LISTA, as the performance of
both methods are similar, up to optimization errors.

Adversarial dictionary The results from Section 2 show that problems with a gram matrix
composed of large eigenvalues associated to non sparse eigenvectors are harder to accelerate.
Indeed, it is not possible in this case to ﬁnd a quasi diagonalization of the matrix B that
does not distort the (cid:96)1 norm. It is possible to generate such a dictionary using Harmonic
Analysis. The Discrete Fourier Transform (DFT) distorts a lot the (cid:96)1 ball, since a very
sparse vector in the temporal space is transformed in widely spread spectrum in the Fourier
domain. We can thus design a dictionary for which LISTA and FacNet performances should
be degraded. D =

is constructed such that dj,k = e−2πijζk , with (cid:0)ζk

(cid:16)

(cid:17)

(cid:1)

k≤n

randomly selected from

without replacement.

di/(cid:107)di(cid:107)2
(cid:110)

i=1...m
1/m, . . . , m/2/m

(cid:111)

The resulting performances are reported in Figure 4. The ﬁrst layer provides a big gain by
changing the starting point of the iterative methods. It realizes an arbitrage of the tradeoﬀ

Ongoing work - This document is not ﬁnished or considered for publication yet.

15

Moreau and Bruna

(a) Pascal VOC 2008

(b) MNIST

Figure 5: Evolution of the cost function F (zk) − F (z∗) with the number of layers or the

number of iteration k for two image datasets.

between starting from 0 and starting from y . But the next layers do not yield any extra
gain compared to the original ISTA algorithm. After 4 layers, the cost performance of both
adaptive methods and ISTA are equivalent. It is clear that in this case, FacNet does not
accelerate eﬃciently the sparse coding, in accordance with our result from Section 2. LISTA
also displays poor performances in this setting. This provides further evidence that FacNet
and LISTA share the same acceleration mechanism as adversarial dictionaries for FacNet
are also adversarial for LISTA.

4.3 Sparse coding with over complete dictionary on images

Wavelet encoding for natural images A highly structured dictionary composed of trans-
lation invariant Haar wavelets is used to encode 8x8 patches of images from the PASCAL
VOC 2008 dataset. The network is used to learn an eﬃcient sparse coder for natural im-
ages over this family. 500 images are sampled from dataset to train the encoder. Training
batches are obtained by uniformly sampling patches from the training image set to feed
the stochastic optimization of the network. The encoder is then tested with 10000 patches
sampled from 100 new images from the same dataset.

Learned dictionary for MNIST To evaluate the performance of LISTA for dictionary
learning, LISTA was used to encode MNIST images over an unconstrained dictionary,
learned a priori using classical dictionary learning techniques. The dictionary of 100 atoms
was learned from 10000 MNIST images in grayscale rescaled to 17x17 using the implemen-
tation of Mairal et al. (2009) proposed in scikit-learn, with λ = 0.05. Then, the networks
were trained through backpropagation using all the 60000 images from the training set of
MNIST. Finally, the perfornance of these encoders were evaluated with the 10000 images
of the training set of MNIST.

The Figure 5 displays the cost performance of the adaptive procedures compared to non-
adaptive algorithms. In both scenario, FacNet has performances comparable to the one of
LISTA and their behavior are in accordance with the theory developed in Section 2. The
gains become smaller for each added layer and the initial gain is achieved for dictionary
either structured or unstructured. The MNIST case presents a much larger gain compare

Ongoing work - This document is not ﬁnished or considered for publication yet.

16

Understanding LISTA

to the experiment with natural images. This results from the diﬀerence of structure of
the input distribution, as the MNIST digits are much more constrained than patches from
natural images and the network is able to leverage it to ﬁnd a better encoder. In the MNIST
case, a network composed of 12 layers is suﬃcient to achieve performance comparable to
ISTA with more than 1000 iterations.

5. Conclusions

In this paper we studied the problem of ﬁnite computational budget approximation of sparse
coding. Inspired by the ability of neural networks to accelerate over splitting methods on the
ﬁrst few iterations, we have studied which properties of the dictionary matrix and the data
distribution lead to such acceleration. Our analysis reveals that one can obtain acceleration
by ﬁnding approximate matrix factorizations of the dictionary which nearly diagonalize its
Gram matrix, but whose orthogonal transformations leave approximately invariant the (cid:96)1
ball. By appropriately balancing these two conditions, we show that the resulting rotated
proximal splitting scheme has an upper bound which improves over the ISTA upper bound
under appropriate sparsity.

In order to relate this speciﬁc factorization property to the actual LISTA algorithm, we
have introduced a reparametrization of the neural network that speciﬁcally computes the
factorization, and incidentally provides reduced learning complexity (less parameters) from
the original LISTA. Numerical experiments of Section 4 show that such reparametrization
recovers the same gains as the original neural network, providing evidence that our the-
oretical analysis is partially explaining the behavior of the LISTA neural network. Our
acceleration scheme is inherently transient, in the sense that once the iterates are suﬃ-
ciently close to the optimum, the factorization is not eﬀective anymore. This transient
eﬀect is also consistent with the performance observed numerically, although the possibility
remains open to ﬁnd alternative models that further exploit the particular structure of the
sparse coding. Finally, we provide evidence that successful matrix factorization is not only
suﬃcient but also necessary for acceleration, by showing that Fourier dictionaries are not
accelerated.

Despite these initial results, a lot remains to be understood on the general question of op-
timal tradeoﬀs between computational budget and statistical accuracy. Our analysis so far
did not take into account any probabilistic consideration (e.g. obtain approximations that
hold with high probability or in expectation). Another area of further study is the exten-
sion of our analysis to the FISTA case, and more generally to other inference tasks that are
currently solved via iterative procedures compatible with neural network parametrizations,
such as inference in Graphical Models using Belief Propagation or other ill-posed inverse
problems.

References

Alekh Agarwal. Computational Trade-oﬀs in Statistical Learning. PhD thesis, University

of California, Berkeley, 2012.

Ongoing work - This document is not ﬁnished or considered for publication yet.

17

Moreau and Bruna

Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with
statistical guarantees. In Advances in Neural Information Processing Systems (NIPS),
pages 775–783, 2015.

Amir Beck and Marc Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for

Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

S´ebastien Bubeck. Theory of convex optimization for machine learning. preprint, arXiv:1405

(4980), 2014.

Charles F Cadieu and Bruno A Olshausen. Learning intermediate-level representations of

form and motion from natural movies. Neural computation, 24(4):827–866, 2012.

Venkat Chandrasekaran and Michael I Jordan. Computational and statistical tradeoﬀs via
convex relaxation. Proceedings of the National Academy of Sciences, 110(13):E1181–
E1190, 2013.

Adam Coates and Andrew Y Ng. The importance of encoding versus training with sparse
coding and vector quantization. In Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 921–928, 2011.

Patrick L Combettes and Heinz H. Bauschke. Convex Analysis and Monotone Operator

Theory in Hilbert Spaces, volume 1. 2011.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online
learning and stochastic optimization. The Journal of Machine Learning Research, 12:
2121–2159, 2011.

Jerome Friedman, Trevor Hastie, Holger H¨oﬂing, and Robert Tibshirani. Pathwise coordi-

nate optimization. The Annals of Applied Statistics, 1(2):302–332, 2007.

Raja Giryes, Yonina C Eldar, Alex M Bronstein, and Guillermo Sapiro. Tradeoﬀs between
convergence speed and reconstruction accuracy in inverse problems. preprint, arXiv:1605
(09232), 2016.

Karol Gregor and Yann Le Cun. Learning Fast Approximations of Sparse Coding.

In

International Conference on Machine Learning (ICML), pages 399–406, 2010.

Trevor Hastie, Robert Tibshirani, and Martin J. Wainwright. Statistical Learning with

Sparsity. CRC Press, 2015.

Tim Hesterberg, Nam Hee Choi, Lukas Meier, and Chris Fraley. Least angle and 1 penalized

regression: A review. Statistics Surveys, 2:61–93, 2008.

J. B. Hiriart-Urruty. How to regularize a diﬀerence of convex functions. Journal of Mathe-

matical Analysis and Applications, 162(1):196–209, 1991.

Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online Learning for Matrix
Factorization and Sparse Coding. Journal of Machine Learning Research, 11(1):19–60,
2009.

Ongoing work - This document is not ﬁnished or considered for publication yet.

18

Understanding LISTA

Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming,

103(1):127–152, 2005.

Stanley Osher and Yingying Li. Coordinate descent optimization for l1 minimization with
application to compressed sensing; a greedy algorithm. Inverse Problems and Imaging, 3
(3):487–503, 2009.

Samet Oymak, Benjamin Recht, and Mahdi Soltanolkotabi. Sharp time–data tradeoﬀs for

linear inverse problems. preprint, arXiv:1507(04793), 2015.

D Song and Arjun K. Gupta. $L p$-norm Uniform Distribution. The American Mathemat-

ical Society, 125(2):595–601, 1997.

Pablo Sprechmann, Alex Bronstein, and Guillermo Sapiro. Learning Eﬃcient Structured
Sparse Models. In International Conference on Machine Learning (ICML), pages 615–
622, 2012.

Robert Tibshirani. Regression Shrinkage and Selection via the Lasso. Journal of the royal

statistical society. Series B (methodological), 58(1):267–288, 1996.

Bo Xin, Yizhou Wang, Wen Gao, and David Wipf. Maximal sparsity with deep networks?

preprint, arXiv:1605(01636), 2016.

Yun Yang, Mert Pilanci, and Martin J Wainwright. Randomized sketches for kernels: Fast

and optimal non-parametric regression. preprint, arXiv:1501(06195), 2015.

Appendix A. Learned Fista

A similar algorithm can be derived from FISTA, the accelerated version of ISTA to ob-
tain LFISTA (see Figure 6 ). The architecture is very similar to LISTA, now with two
memory taps: It introduces a momentum term to improve the convergence rate of ISTA as
follows:

1. yk = zk +

(zk − zk−1) ,

2. zk+1 = h λ

yk −

∇E(yk)

(cid:33)

(cid:32)

= h λ
L

(I −

B)yk +

1
L

(cid:33)

DTx

,

1
L

tk−1 − 1
tk

(cid:32)

L

1
L

3. tk+1 =

(cid:113)

1 +

1 + 4t2
k

.

2

By substituting the expression for yk into the ﬁrst equation, we obtain a generic recur-
rent architecture very similar to LISTA, now with two memory taps, that we denote by
LFISTA:

zk+1 = hθ(W (k)

g zk + W (k)

m zk−1 + W (k)

e x) .

Ongoing work - This document is not ﬁnished or considered for publication yet.

19

Moreau and Bruna

X

W (0)
e

W (1)
e

+

W (1)
g

W (1)
m

W (2)
m

W (2)
e

W (3)
e

W (2)
g

+

W (3)
g

+

Z

Figure 6: Network architecture for LFISTA. This network is trainable through backpropa-
gation and permits to approximate the sparse coding solution eﬃciently.

This model is equivalent to running K-steps of FISTA when its parameters are initialized
with

W (k)
g

=

1 +

tk−1 − 1
tk

(cid:33) (cid:32)

(cid:33) (cid:32)

(cid:33)

I −

B

,

1
L
(cid:33)

W (k)

m =

1 − tk−1
tk

I −

B

,

1
L

W (k)
e

=

DT .

(cid:32)

(cid:32)

1
L

The parameters of this new architecture, presented in Figure 6 , are trained analogously as
in the LISTA case.

Appendix B. Proofs for the convergence rate using LISTA

Proposition B.1 Suppose that R = ATSA − B is positive deﬁnite, and deﬁne

δA(z) = (cid:107)Az(cid:107)1 − (cid:107)z(cid:107)1. Then we have

zk+1 = arg min

(cid:101)F (z, zk) , and

z

F (zk+1)−F (z∗) ≤

(cid:16)

(cid:17)
(z∗ − zk)TR(z∗ − zk) − (z∗ − zk+1)TR(z∗ − zk+1)

+(cid:104)∂δA(zk+1), zk+1−z∗(cid:105) .

1
2

(25)

(26)

Proof We deﬁne

tzk+1 + (1 − t)z∗(cid:17)
Since F is convex, f is also convex in [0, 1]. Since f (0) = F (z∗) is the global minimum, it
results that f (cid:48)(t) is increasing in (0, 1], and hence

, t ∈ [0, 1] .

f (t) = F

(cid:16)

F (zk+1) − F (z∗) = f (1) − f (0) =

f (cid:48)(t)dt ≤ f (cid:48)(1) ,

(cid:90)

where f (cid:48)(1) is any element of ∂f (1). Since δA(z) is a diﬀerence of convex functions, its
subgradient can be deﬁned as a limit of inﬁmal convolutions Hiriart-Urruty (1991). We
have

∂f (1) = (cid:104)∂F (zk+1), zk+1 − z∗(cid:105) ,

Ongoing work - This document is not ﬁnished or considered for publication yet.

20

Understanding LISTA

and since

it results that

and thus

∂F (z) = ∂ (cid:101)F (z, zk) − R(z − zk) − ∂δA(z) and 0 ∈ ∂ (cid:101)F (zk+1, zk)

∂F (zk+1) = −R(zk+1 − zk) − ∂δA(zk+1) ,

F (zk+1) − F (z∗) ≤ (z∗ − zk+1)TR(zk+1 − zk) + (cid:104)∂δA(zk+1), (z∗ − zk+1)(cid:105) .

(27)

(10) is obtained by observing that

(z∗ − zk+1)TR(zk+1 − zk) ≤

(z∗ − zk)TR(z∗ − zk) − (z∗ − zk+1)TR(z∗ − zk+1)

, (28)

(cid:17)

(cid:16)

1
2

thanks to the fact that R (cid:31) 0.

Theorem 2 Let Ak, Sk be the pair of unitary and diagonal matrices corresponding to iter-
ation k, chosen such that Rk = AT

k SkAk − B (cid:31) 0. It results that
(z∗ − z0)TR0(z∗ − z0) + 2LA0(z1)(cid:107)z∗ − z1(cid:107)2
2k

+

α − β
2k

,

(11)

F (zk) − F (z∗) ≤

with

α =

(cid:16)

(cid:17)
2LAi (zi+1)(cid:107)z∗ − zi+1(cid:107)2 + (z∗ − zi)T(Ri−1 − Ri)(z∗ − zi)

,

β =

(i + 1)

(zi+1 − zi)TRi(zi+1 − zi) + 2δAi(zi+1) − 2δAi(zi)

(cid:16)

(cid:17)

,

k−1
(cid:88)

i=1

k−1
(cid:88)

i=0

where LA(z) denote the local lipschitz constant of δA at z.

Proof The proof is adapted from (Beck and Teboulle, 2009, Theorem 3.1).
From Proposition B.1, we start by using (26) to bound terms of the form F (zn)−F (z∗):
(cid:16)

(cid:17)
(z∗ − zn)TRn(z∗ − zn) − (z∗ − zn+1)TRn(z∗ − zn+1)

.

F (zn)−F (z∗) ≤ (cid:104)∂δAn (zn+1), (z∗−zn+1)(cid:105)+

1
2

Adding these inequalities for n = 0 . . . k − 1 we obtain


 − kF (z∗) ≤

F (zn)





k−1
(cid:88)

n=0

k−1
(cid:88)

n=0
1
2

+

+

1
2

k−1
(cid:88)

n=1

(cid:104)∂δAn(zn+1), (z∗ − zn+1)(cid:105) +

(29)

(cid:16)

(cid:17)
(z∗ − z0)TR0(z∗ − z0) − (z∗ − zk)TRk−1(z∗ − zk)

+

(z∗ − zn)T(Rn−1 − Rn)(z∗ − zn) .

On the other hand, we also have

F (zn) − F (zn+1) ≥ F (zn) − ˜F (zn, zn) + ˜F (zn+1, zn) − F (zn+1)

= −δAn(zn) + δAn(zn+1) +

(zn+1 − zn)TRn(zn+1 − zn) ,

1
2

Ongoing work - This document is not ﬁnished or considered for publication yet.

21

(n + 1)(F (zn) − F (zn+1)) ≥

(n + 1)(zn+1 − zn)TRn(zn+1 − zn) +

Moreau and Bruna

+

(n + 1)

δAn(zn+1) − δAn(zn)

(cid:17)

1
2

k−1
(cid:88)

n=0
k−1
(cid:88)

n=0

k−1
(cid:88)

n=0

(cid:16)

1
2

(cid:32)

(30)

(cid:33)

F (zn)

 − kF (zk) ≥

(n + 1)

(zn+1 − zn)TRn(zn+1 − zn) + δAn(zn+1) − δAn(zn)

.

Combining (29) and (30) we obtain

F (zk) − F (z∗) ≤

(z∗ − z0)TR0(z∗ − z0) + 2(cid:104)∇δA0(z1), (z∗ − z1)(cid:105)
2k

+

α − β
2k

(31)

which results in

k−1
(cid:88)

n=0







k−1
(cid:88)

n=0

with

α =

k−1
(cid:88)

n=1

k−1
(cid:88)

n=0

(cid:16)

(cid:17)
2(cid:104)∇δAn(zn+1), (z∗ − zn+1)(cid:105) + (z∗ − zn)T(Rn−1 − Rn)(z∗ − zn)

,

β =

(n + 1)

(cid:16)

(cid:17)
(zn+1 − zn)TRn(zn+1 − zn) + 2δAn(zn+1) − 2δAn(zn)

.

Corollary 3 If Ak = I, Sk = (cid:107)B(cid:107)I for k ≥ 1 then

F (zk)−F (z∗) ≤

(z∗ − z0)TR0(z∗ − z0) + 2LA0 (z1)((cid:107)z∗ − z1(cid:107) + (cid:107)z1 − z0(cid:107)) + (z∗ − z1)TR0(z∗ − z1)T
2k

.

(12)

Proof We verify that in that case, Rn−1 − Rn ≡ 0 and for n > 1 and δAn ≡ 0 for n > 0 .

Appendix C. Existence of a gap for generic dictionary.

C.1 Properties of Eδ

Proposition C.1 If a matrix A has its columns in Eδ,i, then it is almost unitary for small
value of δ. More precisely, denoting ν = ATA − IK, when δ → 0

(cid:107)ν(cid:107)F = O (cid:0)δ(cid:1)

Ongoing work - This document is not ﬁnished or considered for publication yet.

22

Understanding LISTA

Proof Let ν = ATA − IK. As Ai are in Eδ,i,

νi,i = AT

i Ai − 1 = 0

We can verify that for i (cid:54)= j

νi,j = AT

i Aj = δ

(cid:112)

1 − δ2(eT

j hi) + δ2hT
i hj + eT
δ2(cid:17)
(cid:16)

j hi) + O

i hj

= δ(eT

i hj + eT

This permits to bound the Frobenius norm of ν i.e.

(cid:107)ν(cid:107)2

F =

(cid:88)

i,j = δ2 (cid:88)
ν2

(eT

i hj + eT

j hi)2 + O

(cid:16)

δ3(cid:17)

.

1≤i,j≤K

1≤i,j≤K
i(cid:54)=j

(cid:107)ν(cid:107)2

F =

(cid:88)

i,j = δ2 (cid:88)
ν2

(eT

i hj + eT

j hi)2 + O

(cid:16)

δ3(cid:17)

,

1≤i,j≤K

= δ2 (cid:88)

(hi,j + hj,i)2 + O

(cid:16)

δ3(cid:17)

,

1≤i,j≤K
i(cid:54)=j

1≤i,j≤K

= δ2(cid:107)H + H T(cid:107)2

= 4δ2(cid:107)H(cid:107)2

F + O

F + O
δ3(cid:17)
(cid:16)

(cid:16)

δ3(cid:17)

,

= 4δ2K + O

(cid:16)

δ3(cid:17)

.

as (cid:107)hi(cid:107)2

2 = 1

Proposition C.2 For A ⊂ Eδ, and for any symmetric matrix U ∈ RK×K, when δ → 0,

(cid:13)
(cid:13)A−1U A
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

≤ (cid:107)U (cid:107)2

F + O

(cid:16)

δ3(cid:17)

Proof For U ∈ RK×K symmetric, as A is quasi unitary,

(cid:13)
(cid:13)A−1U A
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

= Tr

(cid:104)
ATU (A−1)TA−1U A

(cid:105)

= Tr

(cid:104)

U (ATA)−1U AAT(cid:105)

= Tr

(cid:105)
(cid:104)
U (IK + ν)−1U (IK + ν)

= Tr

(cid:20)
U (IK − ν + ν2 + O

(cid:16)

ν3(cid:17)

(cid:21)
)U (IK + ν)

(cid:20)
U U + U ν2U − U νU ν + O

(cid:16)

= Tr

(cid:21)

)

ν3(cid:17)
(cid:16)

= (cid:107)U (cid:107)2

F + (cid:107)νU (cid:107)2

F − (cid:107)ν

1
2 U ν

1

2 (cid:107)2

F + O

(cid:107)ν3/2(cid:107)2
F

(cid:17)

Notice that

(cid:107)ν

1

1
2 U ν

2 (cid:107)F = (cid:107)(U ν)

1

2 T(U ν)

1

2 (cid:107)F = (cid:107)(U ν)

1

2 (cid:107)2

F ≥ (cid:107)U ν(cid:107)F

Ongoing work - This document is not ﬁnished or considered for publication yet.

23

Moreau and Bruna

Thus (cid:107)νU (cid:107)2

F − (cid:107)ν

1

1
2 U ν

2 (cid:107)2

F ≤ 0 and by submultiplicativity of (cid:107) · (cid:107)2
F ,

(cid:107)ν3/2(cid:107)2

F ≤ (cid:107)ν(cid:107)3

F = O

(cid:16)

δ3(cid:17)

⇒ O

(cid:16)

(cid:107)ν3/2(cid:107)2
F

(cid:17)

(cid:16)

δ3(cid:17)

.

= O

By combining all these results, we get:

(cid:13)
(cid:13)A−1U A
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

≤ (cid:107)U (cid:107)2

F + O

(cid:16)

δ3(cid:17)

Proposition C.3 For a matrix A ⊂ Eδ and any matrices X, Y ∈ RK×K, when δ → 0 ,
(cid:13)X − AY AT(cid:13)

(cid:13)
(cid:13)A−1XA − Y
(cid:13)

(cid:13)Y (cid:13)
2
F (cid:107)ν(cid:107)2
(cid:13)

F + O

δ3(cid:17)

+ (cid:13)

(cid:13)
(cid:13)

≤

(cid:16)

.

(cid:13)
2
(cid:13)
(cid:13)
F

2
(cid:13)
(cid:13)
F

Proof First, we split the error of replacing
in two
terms. Both are linked to the quasi unitarity of A. The ﬁrst term arises as we replace A−1
by AT,

by

(cid:13)
(cid:13)A−1XA − Y
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

(cid:13)
(cid:13)

(cid:13)X − AY AT(cid:13)

2
(cid:13)
(cid:13)
F

(cid:13)
(cid:13)A−1XA − Y
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

=

(cid:13)
A−1 (cid:16)
(cid:13)
(cid:13)
(cid:13)

X − AY (ATA − ν)
(cid:125)
(cid:124)

(cid:123)(cid:122)
IK

A−1

A







(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F

X − AY A−1(cid:17)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
A−1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
X − AY AT + AY νA−1(cid:17)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
F

=

A







A

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
F
(cid:13)
A−1 (cid:16)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
F

=

(cid:13)
A−1 (cid:16)
(cid:13)
(cid:13)
(cid:13)

≤ 2

(cid:13)
A−1 (cid:16)
(cid:13)
(cid:13)
(cid:13)

X − AY AT(cid:17)

A

+ 2

AY νA−1(cid:17)

A

≤ 2

≤ 2

≤ 2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)X − AY AT(cid:13)
(cid:13)X − AY AT(cid:13)
(cid:13)X − AY AT(cid:13)

2
(cid:13)
(cid:13)
F
2
(cid:13)
(cid:13)
F
2
(cid:13)
(cid:13)
F

(cid:13)
(cid:13)

(cid:16)

δ3(cid:17)
(cid:16)

+ 2 (cid:13)

+ 2 (cid:13)

(cid:13)Y ν(cid:13)
2
F + O
(cid:13)
(cid:13)Y (cid:13)
2
F (cid:107)ν(cid:107)2
(cid:13)
(cid:13)Y (cid:13)
+ 8δ2K (cid:13)
2
F + O
(cid:13)

F + O
(cid:16)

δ3(cid:17)
δ3(cid:17)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
F
(triangular inequality)

(Proposition C.2)

(submultiplicativity (cid:107) · (cid:107)F )

(Proposition C.1)

Proposition C.4 For A ⊂ Eδ, and for any matrix U ∈ RK×K, when δ → 0,

(cid:13)
(cid:13)
2
(cid:13)ATU A
(cid:13)
(cid:13)
(cid:13)
F

(cid:16)

= (cid:107)U (cid:107)2
F

1 + (cid:107)ν(cid:107)2
F

(cid:17)

(cid:16)

δ3(cid:17)

+ O

Proof

Ongoing work - This document is not ﬁnished or considered for publication yet.

24

Understanding LISTA

(cid:107)AU AT(cid:107)F = (cid:107)AXATAA−1(cid:107)2

F = (cid:107)AU (IK + ν)A−1(cid:107)2
F
(cid:16)

= (cid:107)U + U ν(cid:107)2

F + O

δ3(cid:17)

= 2(cid:107)U (cid:107)2

F + 2(cid:107)U ν(cid:107)2

F + O

= 2(cid:107)U (cid:107)2

F + 2(cid:107)U (cid:107)2

F (cid:107)ν(cid:107)2

(cid:16)

δ3(cid:17)
(cid:16)

F + O
(cid:16)

δ3(cid:17)
δ3(cid:17)

(Proposition C.2)

(triangular inequality)

((cid:107) · (cid:107)F is sub multiplicative)

≤ 2(cid:107)U (cid:107)2

F + 8δ2K(cid:107)U (cid:107)2

F + O

(Proposition C.1)

C.2 Control the deviation of (cid:107) · (cid:107)B

Lemma 5 For a generic dictionary D and a diagonally dominant matrix A ⊂ Eδ,

(cid:34)

ED

min
Ai∈Eδ,i

(cid:13)
(cid:13)A−1SA − B
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

≤

K(K − 1)
p

(cid:35)

− 4δ(K − 1)

(cid:115)

K
p

(cid:32)

(cid:33)

+ δ2

8ED

(cid:107)B(cid:107)4
F

− 6

(cid:104)

(cid:105)

K(K − 1)
p

(cid:16)

δ3(cid:17)

.

+ O
δ→0

Proof First, we use the results from Proposition C.3 to remove the inverse matrix A−1
(cid:13)S − ABAT(cid:13)

(cid:13)
(cid:13)A−1SA − B
(cid:13)

(cid:13)B(cid:13)
2
F (cid:107)ν(cid:107)2
(cid:13)

F + O

δ3(cid:17)

+ (cid:13)

(cid:13)
(cid:13)

≤

(cid:16)

.

(cid:13)
2
(cid:13)
(cid:13)
F

2
(cid:13)
(cid:13)
F

Using Proposition C.1 with A ⊂ Eδ,

(cid:107)ν(cid:107)2

F = 4δ2K + O

(cid:16)

δ3(cid:17)

and

(cid:13)
(cid:13)A−1SA − B
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

≤

(cid:13)
(cid:13)

(cid:13)S − ABAT(cid:13)

2
(cid:13)
(cid:13)
F

+ 4δ2K(cid:107)B(cid:107)2

F + O

(cid:16)

δ3(cid:17)

.

Then, we only need to control
terms

(cid:13)
(cid:13)

(cid:13)S − ABAT(cid:13)

2
(cid:13)
(cid:13)
F

. First we note that this can be split into 2

(cid:13)
(cid:13)

(cid:13)S − ABAT(cid:13)

2
(cid:13)
(cid:13)
F

=

K
(cid:88)

K
(cid:88)

i=1

j=1
j(cid:54)=i

(AT

i BAj)2 =

(AT

i BAj)2 −

(AT

i BAi)2

K
(cid:88)

K
(cid:88)

i=1

j=1

K
(cid:88)

i=1

=

(cid:13)
(cid:13)

(cid:13)ABAT(cid:13)

2
(cid:13)
(cid:13)
F

−

K
(cid:88)

i=1

(cid:107)DAi(cid:107)4
2

= (cid:13)

(cid:13)B(cid:13)
2
F (1 + 4δ2K) −
(cid:13)

(cid:107)DAi(cid:107)4

2 + O

(cid:16)

δ3(cid:17)

(Proposition C.4)

Ongoing work - This document is not ﬁnished or considered for publication yet.

K
(cid:88)

i=1

25

Moreau and Bruna

The ﬁrst term is the squared Frobenius norm of a Wishart matrix and can be controlled
by

(cid:104)

(cid:105)

ED

(cid:107)B(cid:107)2
F

= ED






K
(cid:88)

K
(cid:88)



B2
i,j


 =

K
(cid:88)

K
(cid:88)

ED

i=1

j=1

i=1

j=1











p
(cid:88)

l=1


2







di,kdj,k













p
(cid:88)

l=1


2







di,ldj,l



+

ED

(cid:104)
(cid:107)di(cid:107)4
2

(cid:105)

K
(cid:88)

i=1

(cid:104)

ED

j,ld2
d2
i,l

(cid:105)

+

p
(cid:88)

p
(cid:88)

l=1

m=1
m(cid:54)=l

(cid:104)

ED
(cid:124)

(cid:123)(cid:122)
=0

di,ldi,mdj,ldj,m

+ K




(cid:105)



(cid:125)

Edj

(cid:105)

(cid:104)
d2
j,l

Edi

(cid:104)

d2
i,l

(cid:105)

+ K

(di are independent)

1
p2 + K =

K(K − 1)
p

+ K .

(cid:20)

(cid:21)

(Ed

d2
i,j

= 1

p , (Song and Gupta, 1997))

K
(cid:88)

K
(cid:88)

=

ED

i=1

j=1
i(cid:54)=j








K
(cid:88)

K
(cid:88)

p
(cid:88)

i=1

j=1
i(cid:54)=j

l=1

K
(cid:88)

K
(cid:88)

p
(cid:88)

i=1

l=1

j=1
i(cid:54)=j

K
(cid:88)

K
(cid:88)

p
(cid:88)

i=1

l=1

j=1
i(cid:54)=j

=

=

=

For the second term, consider u ∈ Eδ,i , such that u = (cid:112)1 − µ2ei + µh for 0 < µ < δ,
h ∈ Span(ei)⊥. Given i ∈ [K], Bei can be decomposed as z1ei +z2hi, with hi ∈ Span(ei)⊥ ∩
S K−1. Using basic algebra, z1 and z2 are:

z1 = eT

i Bei = (cid:107)Dei(cid:107)2

2 = (cid:107)di(cid:107)2

2 = 1 .

2 = (cid:107)Bei(cid:107)2
z2

1 = (cid:107)DTdi(cid:107)2
2 − z2

0


2 − 1

dT
i dj
(cid:107)DTdi(cid:107)2



2−1

if (cid:107)DTdi(cid:107)2
elsewhere

2 = 1

Then

(32)

(33)

Also, for all i, j ∈ [K], if i (cid:54)= j then hT

i ej =

(cid:107)Du(cid:107)2

2 = uTDTDu = uTBu
= (1 − µ2) (cid:107)Dei(cid:107)2
2
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:107)di(cid:107)2=1

+µ2(cid:107)Dh(cid:107)2

2 + 2µ

(cid:112)

1 − µ2 hTBei
(cid:124) (cid:123)(cid:122) (cid:125)
z2hThi

Thus, with the notation from (33), hTBei = z2hThi and

(cid:107)Du(cid:107)2

2 = (1 − δ2) + δ2(cid:107)Dh(cid:107)2

2 + 2δ

(cid:112)

1 − δ2

(cid:113)

(cid:107)DTdi(cid:107)2

2 − 1hThi

(34)

Ongoing work - This document is not ﬁnished or considered for publication yet.

26

Understanding LISTA

Now we can use this to derive a lower bound on maxu∈Eδ,i (cid:107)DTu(cid:107)2
(cid:113)

2 when δ → 0 ,

(cid:107)DTdi(cid:107)2

2 − 1 + δ2 (cid:16)

(cid:107)DTdi(cid:107)2

2 − 1

(cid:17)

(cid:16)

δ3(cid:17)

+ O

max
u∈Eδ,i

(cid:107)Du(cid:107)2

2 ≥ 1 + 2δ

(u= ˆAi)

Taking the square of this relation yields

max
u∈Eδ,i

(cid:107)Du(cid:107)4

2 ≥ 1 + 4δ

(cid:113)

(cid:107)DTdi(cid:107)2

2 − 1 + 6δ2 (cid:16)

(cid:107)DTdi(cid:107)2

2 − 1

(cid:17)

(cid:16)

δ3(cid:17)

+ O

Taking the expectation yields

(cid:34)

ED

max
u∈Eδ,i

(cid:35)

(cid:107)Du(cid:107)4
2

≥ 1 + 4δED

(cid:20)(cid:113)

(cid:107)DTdi(cid:107)2

(cid:21)
2 − 1

(cid:104)

+ 6δ2ED

(cid:107)DTdi(cid:107)2

2 − 1

(cid:105)

(cid:16)

δ3(cid:17)

.

+ O

i = p((cid:107)DTdi(cid:107)2

The random variable pY 2
K−1. Indeed, the atoms di
are uniformly distributed over S K−1. As this distribution is rotational invariant, without
loss of generality, we can take di = e1. Then Yi is simply sum of K − 1 squared normal
gaussians rv with variance 1
p ,

2 − 1) are distributed as χ2

i = (cid:107)DTe1(cid:107)2
Y 2

2 − 1 =

d2
j,1 ,

K
(cid:88)

j=2

√

and

pYi is distributed as χK−1. A lower bound for its expectation is

ED

(cid:2)Yi

(cid:3) =

(cid:114) 2
p

(cid:17)

(cid:16) K
Γ
2
(cid:16) K−1
2

Γ

(cid:17) ≥

K − 1
√
pK

and ED

(cid:105)

(cid:104)

Y 2
i

=

K − 1
p

We derive a lower bound for the second term when δ → 0 ,

(cid:34)

ED

max
u∈Eδ,i

(cid:35)

(cid:107)DTu(cid:107)4
2

(cid:38) 1 + 4δ

K − 1
√
pK

+ 6δ2 K − 1

(cid:16)

δ3(cid:17)

+ O

p

Using these results, we derive an upper bound for the expected distortion of B with A with
columns in Eδ,i,

(cid:34)

ED

min
Ai∈Eδ,i

(cid:13)
(cid:13)

(cid:13)S − ABAT(cid:13)

2
(cid:13)
(cid:13)
F

(cid:35)

≤ ED

(cid:20)

(cid:13)B(cid:13)
(cid:13)
2
(cid:13)
F

(cid:21)

−

(cid:34)

max
Ai∈Eδ

(cid:35)

(cid:107)DAi(cid:107)4
2

+ C1δ2 + O

(cid:16)

δ3(cid:17)

K
(cid:88)

i=1

ED

K
(cid:88)

i=1

≤ K +

K(K − 1)
p

−

1 + 4δ

K − 1
√
pK

+ C2δ2 + O

(cid:16)

δ3(cid:17)

≤

K(K − 1)
p

− 4δ(K − 1)

+ C(cid:48)δ2 + O

(cid:16)

δ3(cid:17)

(cid:115)

K
p

Ongoing work - This document is not ﬁnished or considered for publication yet.

27

Moreau and Bruna

(cid:32)

C(cid:48) = δ2

4KED

(cid:104)
(cid:107)B(cid:107)2
F

(cid:105)

− 6

K(K − 1)
p

(cid:33)

This concludes our proof as

(cid:34)

ED

min
Ai∈Eδ,i

(cid:13)
(cid:13)A−1SA − B
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

(cid:35)

(cid:34)

= ED

min
Ai∈Eδ,i

(cid:13)
(cid:13)

(cid:13)S − ABAT(cid:13)

2
(cid:13)
(cid:13)
F

(cid:35)

+ 4δ2KED

(cid:107)B(cid:107)2
F

(cid:104)

(cid:105)

(Proposition C.3)

(cid:16)

δ3(cid:17)

+ O

≤

K(K − 1)
p

− 4δ(K − 1)

+ Cδ2 + O

(cid:16)

δ3(cid:17)

.

(Lemma 5)

(cid:115)

K
p

C = 8KED

(cid:107)B(cid:107)2
F

− 6

(cid:104)

(cid:105)

K(K − 1)
p

And

And

C.3 Controling Ez∼Z

(cid:104)

(cid:105)
δA(z)

Lemma 6 Let A ⊂ Eδ be a diagonally dominant matrix and let z be a random variable in
RK with iid coordinates zi. Then

Ez,D

(cid:104)
δA(z)

(cid:105)

≤

(cid:32)
δ

√

K − 1 −

δ2
2

+ O
δ→0

(cid:33)

(cid:16)

δ4(cid:17)

(cid:104)
(cid:107)z(cid:107)1

(cid:105)

Ez

Proof For any random variable z = (z1, . . . , zK) ∼ Z ∈ RK s.t.
invariant, then

the zi are rotational

1 = Ez∼Z

(cid:2)1(cid:3) = Ez∼Z

(cid:34)

(cid:107)z(cid:107)1
(cid:107)z(cid:107)1

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:107)z(cid:107)1
(cid:12)
(cid:12)

=

K
(cid:88)

i=1

Ez∼Z

(cid:34)

|zi|
(cid:107)z(cid:107)1

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:107)z(cid:107)1
(cid:12)
(cid:12)

= KEz∼Z

(cid:34)

|z1|
(cid:107)z(cid:107)1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

(cid:107)z(cid:107)1

Thus we get:

Ez∼Z

(cid:34)

|z1|
(cid:107)z(cid:107)1

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:107)z(cid:107)1
(cid:12)
(cid:12)

=

1
K

.

(35)

Let z = (cid:0)z1, . . . zK

(cid:1) be a vector of RK. Then

(cid:107)Az(cid:107)1 =

Ai,jzj

≤

K
(cid:88)

K
(cid:88)

i=1

j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

K
(cid:88)

K
(cid:88)

i=1

j=1

(cid:12)
(cid:12)
(cid:12)Ai,j

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)zj

(cid:12)
(cid:12)
(cid:12) ≤

(cid:13)
(cid:13)Ai

(cid:13)
(cid:13)1

(cid:12)
(cid:12)
(cid:12)zj

(cid:12)
(cid:12)
(cid:12)

K
(cid:88)

j=1

Ongoing work - This document is not ﬁnished or considered for publication yet.

28

Understanding LISTA

Using (35), we can compute an upper bound for Ez∼Z

(cid:34)

(cid:35)

:

|Az|1
(cid:107)z(cid:107)1

Ez∼Z

(cid:34)

(cid:107)Az(cid:107)1
(cid:107)z(cid:107)1

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:107)z(cid:107)1
(cid:12)
(cid:12)

≤

K
(cid:88)

i=1

(cid:107)Ai(cid:107)1Ez∼Z

(cid:34)

|zi|
(cid:107)z(cid:107)1

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:107)z(cid:107)1
(cid:12)
(cid:12)

=

(cid:107)A(cid:107)1,1
K

Finally, we can get an upper bound on Ez∼Z

(cid:104)
(cid:107)Az(cid:107)1

(cid:105)
:

(cid:104)

(cid:105)

Ez∼Z

(cid:107)Az(cid:107)1

= Ez∼Z

(cid:34)


Ez∼Z

(cid:107)A

z
(cid:107)z(cid:107)1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)



(cid:107)1

(cid:107)z(cid:107)1

(cid:107)z(cid:107)1



≤ Ez∼Z

(cid:34)

(cid:35)

(cid:107)A(cid:107)1,1
K

(cid:107)z(cid:107)1

≤

(cid:107)A(cid:107)1,1
K

Ez∼Z

(cid:104)
(cid:107)z(cid:107)1

(cid:105)

This permits to control Ez∼Z

(cid:104)

(cid:105)
δA(z)

with

Ez∼Z

(cid:104)

(cid:105)
δA(z)

= Ez∼Z

(cid:104)

(cid:107)Az(cid:107)1 − (cid:107)z(cid:107)1

≤

(cid:105)

(cid:107)A(cid:107)1,1 − (cid:107)I(cid:107)1,1
K

(cid:104)

(cid:105)

Ez∼Z

(cid:107)z(cid:107)1

Then, for A ∈ Eδ, the (cid:96)1-norm of the columns Ai is

(cid:107)Ai(cid:107)1 ≤

1 − δ2 + δ

K − 1

(cid:112)

√

We can derive an expression of (cid:107)A(cid:107)1,1−(cid:107)I(cid:107)1,1

for δ → 0,

K

(cid:107)A(cid:107)1,1
K

1
K

K
(cid:88)

i=1

− 1 =

(cid:107)Ai(cid:107)1 − 1 ≤

1 − δ2 + δ

K − 1 − 1

(cid:112)

√

√

δ2
2

(cid:16)

δ4(cid:17)

δ→0

≤ δ

K − 1 −

+ O

(36)

C.4 Evalutation of the gap

Proposition C.5 For A with columns chosen greedily in Eδ,i, using results from Lemma 5
and Lemma 6,

(cid:34)

ED

(cid:13)
(cid:13)A−1SA − B
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

min
A⊂Eδ

(cid:35)

(cid:107)v(cid:107)2

2 + λδA(z)

≤

(K − 1)K
p

√

(cid:107)v(cid:107)2

2 + δ

K − 1

λ(cid:107)z(cid:107)1 −



(cid:115)



K(K − 1)
p

(cid:107)v(cid:107)2
2

 + O

(cid:16)

δ2(cid:17)

Ongoing work - This document is not ﬁnished or considered for publication yet.

29

Moreau and Bruna

Proof We denote v = z − zk. For A with columns chosen greedily in Eδ,i, using results from
Lemma 5 and Lemma 6,

(cid:34)
(cid:13)
(cid:13)A−1SA − B
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

ED

(cid:35)
2 + λδA(z)

(cid:107)v(cid:107)2

≤(cid:107)v(cid:107)2
2





K − 1
√
p

(cid:32)

K
√
p

(cid:33)

√

− 4δ

K

+ O





(cid:16)

δ2(cid:17)

+ λ(cid:107)z(cid:107)1

K − 1 + O

√

(cid:18)
δ

(cid:16)

δ2(cid:17)(cid:19)

≤

(K − 1)K
p

(cid:107)v(cid:107)2
2



(cid:115)

√

+ δ

K − 1

λ(cid:107)z(cid:107)1 −



K(K − 1)
p

(cid:107)v(cid:107)2
2

 + O

(cid:16)

δ2(cid:17)

(37)

Theorem 7 (Acceleration certiﬁcate) Given a generic dictionary D, it is possible ﬁnd
a diagonally dominant matrix A ⊂ Eδ, which provide better performance than identity to
solve (1) when

(cid:16)

λ

(cid:107)z(cid:107)1 + (cid:107)z∗(cid:107)

≤

(cid:17)

(cid:115)

K(K − 1)
p

(cid:107)zk − z∗(cid:107)2
2

Proof For A ⊂ Eδ with columns chosen greedily in Eδ,i, using results from Lemma 5 and
Lemma 6,

(cid:34)

ED

min
A⊂Eδ

(cid:13)
(cid:13)A−1SA − B
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
F

(cid:35)
2 + λδA(z)

(cid:107)v(cid:107)2

≤

(K − 1)K
p

√

(cid:107)v(cid:107)2

2 + δ

K − 1

λ(cid:107)z(cid:107)1 −



(cid:115)

K(K − 1)
p

(cid:107)v(cid:107)2
2

 + O
δ→0

(cid:16)

δ2(cid:17)



(38)

Starting from Proposition 1, and using the results from as

ED

(cid:104)
F (zk+1) − F (z∗)

(cid:105)

≤

(K − 1)K
p

(cid:107)zk − z∗(cid:107)2
2

√

+ δ

K − 1

λ



(cid:16)

(cid:124)

(cid:107)z(cid:107)1 + (cid:107)z∗(cid:107)

−

(cid:17)

K(K − 1)
p

(cid:107)zk − z∗(cid:107)2
2



+ O
δ→0

(cid:16)

δ2(cid:17)

(cid:115)

(cid:123)(cid:122)
≤0



(cid:125)

Ongoing work - This document is not ﬁnished or considered for publication yet.

30

