8
1
0
2
 
l
u
J
 
0
3
 
 
]

V
C
.
s
c
[
 
 
1
v
6
4
5
1
1
.
7
0
8
1
:
v
i
X
r
a

Textual Explanations for Self-Driving Vehicles

Jinkyu Kim1, Anna Rohrbach1,2, Trevor Darrell1, John Canny1, and Zeynep Akata2,3

1 EECS, University of California, Berkeley CA 94720, USA
{jinkyu.kim,anna.rohrbach,trevordarrell,canny}@berkeley.edu
2 MPI for Informatics, Saarland Informatics Campus, 66123 Saarbr¨ucken, Germany
3 AMLab, University of Amsterdam, 1098 XH Amsterdam, Netherlands
z.akata@uva.nl

Abstract. Deep neural perception and control networks have become key com-
ponents of self-driving vehicles. User acceptance is likely to beneﬁt from easy-
to-interpret textual explanations which allow end-users to understand what trig-
gered a particular behavior. Explanations may be triggered by the neural con-
troller, namely introspective explanations, or informed by the neural controller’s
output, namely rationalizations. We propose a new approach to introspective ex-
planations which consists of two parts. First, we use a visual (spatial) attention
model to train a convolutional network end-to-end from images to the vehicle
control commands, i.e., acceleration and change of course. The controller’s at-
tention identiﬁes image regions that potentially inﬂuence the network’s output.
Second, we use an attention-based video-to-text model to produce textual ex-
planations of model actions. The attention maps of controller and explanation
model are aligned so that explanations are grounded in the parts of the scene that
mattered to the controller. We explore two approaches to attention alignment,
strong- and weak-alignment. Finally, we explore a version of our model that
generates rationalizations, and compare with introspective explanations on the
same video segments. We evaluate these models on a novel driving dataset with
ground-truth human explanations, the Berkeley DeepDrive eXplanation (BDD-
X) dataset. Code is available at https://github.com/JinkyuKimUCB/
explainable-deep-driving

Keywords: Explainable Deep Driving · BDD-X dataset

1

Introduction

Deep neural networks are an effective tool [3,26] to learn vehicle controllers for self-
driving cars in an end-to-end manner. Despite their effectiveness as function estimators,
DNNs are typically cryptic black-boxes. There are no explainable states or labels in
such a network, and representations are fully distributed as sets of activations. Explain-
able models that make deep models more transparent are important for a number of
reasons: (i) user acceptance – self-driving vehicles are a radical technology for users
to accept, and require a very high level of trust, (ii) understanding and extrapolation of
vehicle behavior – users ideally should be able to anticipate what the vehicle will do
in most situations, (iii) effective communication – they help user communicate prefer-
ences to the vehicle and vice versa.

2

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Fig. 1: Our model predicts vehicles control commands, i.e., an acceleration and a change
of course, at each timestep, while an explanation model generates a natural language
explanation of the rationales, e.g., “The car is driving forward because there are no
other cars in its lane”, and a visual explanation in the form of attention – attended
regions directly inﬂuence the textual explanation generation process.

Explanations can be either rationalizations – explanations that justify the system’s
behavior in a post-hoc manner, or introspective explanations – explanations that are
based on the system’s internal state. Introspective explanations represent causal rela-
tionships between the system’s input and its behavior, and address all the goals above.
Rationalizations can address acceptance, (i) above, but are less helpful with (ii) under-
standing the causal behavior of the model or (iii) communication which is grounded in
the vehicle’s internal state (known as theory of mind in human communication).

One way of generating introspective explanations is via visual attention [27,11]. Vi-
sual attention ﬁlters out non-salient image regions, and image areas inside the attended
region have potential causal effect on the output (those outside cannot). As shown in
[11], additional salience ﬁltering can be applied so that the attention map shows only re-
gions that causally affect the output. Visual attention constrains the reasons for the con-
trollers actions but does not e.g., tie speciﬁc actions to speciﬁc input regions e.g., “the
vehicle slowed down because the light controlling the intersection is red”. It is also
likely to be less convenient for passengers to replay the attention map vs. a (typically
on-demand) speech presentation of a textual explanation.

In this work, we focus on generating textual descriptions and explanations, such as
the pair: “vehicle slows down” and “because it is approaching an intersection and the
light is red” as in Figure 1. Natural language has an advantage of being inherently un-
derstandable and does not require familiarity with the design of an intelligent system
in order to provide useful information. In order to train such a model, we collect ex-
planations from human annotators. Our explanation dataset is built on top of another
large-scale driving dataset [26] collected from dashboard cameras in human driven ve-

Textual Explanations for Self-Driving Vehicles

3

hicles. Annotators view the video dataset, compose descriptions of the vehicle’s activity
and explanations for the actions that the vehicle driver performed.

Obtaining training data for vehicle explanations is by itself a signiﬁcant challenge.
The ground truth explanations are in fact often rationalizations (generated by an ob-
server rather than the driver), and there are additional challenges with acquiring driver
data. But even more than that, it is currently impossible to obtain human explanations of
what the vehicle controller was thinking, i.e., a real ground truth. Nevertheless our ex-
periments show that using attention alignment between controller and explanation mod-
els generally improves the quality of explanations, i.e., generates explanations which
better match the human rationalizations of the driving videos.

Our contributions are as follows. (1) We propose an introspective textual explana-
tion model for self-driving cars to provide easy-to-interpret explanations for the behav-
ior of a deep vehicle control network. (2) We integrate our explanation generator with
the vehicle controller by aligning their attentions to ground the explanation, and com-
pare two approaches: attention-aligned explanations and non-aligned rationalizations.
(3) We generated a large-scale Berkeley DeepDrive eXplanation (BDD-X) dataset with
over 6,984 video clips annotated with driving descriptions, e.g., “The car slows down”
and explanations, e.g., “because it is about to merge with the busy highway”. Our
dataset provides a new test-bed for measuring progress towards developing explainable
models for self-driving cars.

2 Related Work

In this section, we review existing work on end-to-end learning for self-driving cars as
well as work on visual explanation and justiﬁcation.

End-to-End Learning for Self-Driving Cars: Most of vehicle controllers for self-
driving cars can be divided in two types of approaches [5]: (1) a mediated perception-
based approach and (2) an end-to-end learning approach. The mediated perception-
based approach depends on recognizing human-designated features, such as lane mark-
ings, trafﬁc lights, pedestrians or cars, which generally require demanding parameter
tuning for a balanced performance [19]. Notable examples include [23], [4], and [16].
As for the end-to-end approaches, recent works [3,26] suggest that neural networks
can be successfully applied to self-driving cars in an end-to-end manner. Most of these
approaches use behavioral cloning that learns a driving policy as a supervised learn-
ing problem over observation-action pairs from human driving demonstrations. Among
these, [3] present a deep neural vehicle controller network that directly maps a stream
of dashcam images to steering controls, while [26] use a deep neural network that takes
input raw pixels and prior vehicle states and predict vehicle’s future motion. Despite
their potential, the effectiveness of these approaches is limited by their inability to ex-
plain the rationale for the system’s decisions, which makes their behavior opaque and
uninterpretable. In this work, we propose an end-to-end trainable system for self driving
cars that is able to justify its predictions visually via attention maps and textually via
natural language.

4

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Fig. 2: Vehicle controller generates spatial attention maps αc for each frame, predicts
acceleration and change of course (ˆct, ˆat) that condition the explanation. Explanation
generator predicts temporal attention across frames (β) and a spatial attention in each
frame (αj). SAA uses αc, WAA enforces a loss between αj and αc.

Visual and Textual Explanations: The importance of explanations for an end-user has
been studied from the psychological perspective [17,18], showing that humans use ex-
planations as a guide for learning and understanding by building inferences and seeking
propositions or judgments that enrich their prior knowledge. They usually seek for ex-
planations to ﬁll the requested gap depending on prior knowledge and goal in question.

In support of this trend, recently explainability has been growing as a ﬁeld in com-
puter vision and machine learning. Especially, there is a growing interest in introspec-
tive deep neural networks. [28] use deconvolution to visualize inner-layer activations of
convolutional networks. [14] propose automatically-generated captions for textual ex-
planations of images. [2] develop a richer notion of contribution of a pixel to the output.
However, a difﬁculty with deconvolution-style approaches is the lack of formal mea-
sures of how the network output is affected by spatially-extended features (rather than
pixels). Exceptions to this rule are attention-based approaches. [11] propose attention-
based approach with causal ﬁltering that removes spurious attention blobs. However,
it is also important to be able to justify the decisions that were made and explain why
they are reasonable in a human understandable manner, i.e., a natural language. For an
image classiﬁcation problem, [7,8] used an LSTM [9] caption generation model that
generates textual justiﬁcations for a CNN model. [21] combine attention-based model
and a textual justiﬁcation system to produce an interpretable model. To our knowledge,
ours is the ﬁrst attempt to justify the decisions of a real-time deep controller through a
combination of attention and natural language explanations on a stream of images.

Textual Explanations for Self-Driving Vehicles

5

3 Explainable Driving Model

In this paper, we propose a driving model that explains how a driving decision was
made both (i) by visualizing image regions where the decision maker attends to and (ii)
by generating a textual description and explanation of what has triggered a particular
driving decision, e.g., “the car continues (description) because trafﬁc ﬂows freely (ex-
planation)”. As we summarize in Figure 2, our model involves two parts: (1) a Vehicle
controller, which is trained to learn human-demonstrated vehicle control commands,
e.g., an acceleration and a change of course; our controller uses a visual (spatial) atten-
tion mechanism that identiﬁes potentially inﬂuential image regions for the network’s
output; (2) a Textual explanation generator, which generates textual descriptions and
explanations controller behavior. The key to the approach is to align the attention maps.

Preprocessing. Our model is trained to predict two vehicle control commands, i.e., an
acceleration and a change of course. At each time t, an acceleration, at, is measured by
taking the derivative of speed measurements, and a change of course, ct, is computed by
taking a difference between a current vehicle’s course and a smoothed value by using
simple exponential smoothing method [10]. We provide details in supplemental mate-
rial. To reduce computational burden, we down-sample to 10Hz and reduce the input
dimensionality by resizing raw images to a 90×160×3 image with nearest-neighbor
scaling algorithm. Each image is then normalized by subtracting the mean from the raw
input pixels and dividing by its standard deviation. This preprocessing is applied to the
latest 4 frames, which are then stacked to produce the ﬁnal input to the neural network.

Convolutional Feature Encoder. We use a convolutional neural network to encode
the visual information into a set of visual feature vectors at time t, i.e., convolutional
feature cube Xt = {xt,1, xt,2, . . . , xt,l} where xt,i ∈ Rd for i ∈ {1, 2, . . . , l} and l is
the number of different spatial regions of the given input. Each feature vector contains a
high-level description of objects present in a certain input region. This allows us to focus
selectively on different regions of the given image by choosing a subset of these feature
vectors. We use a ﬁve-layered convolutional network as in [3,11] and omit max-pooling
layers to prevent spatial information loss [15]. The output is a three-dimensional feature
cube Xt and the feature block has the size w×h×d at each time t.

3.1 Vehicle Controller

Our vehicle controller is trained in an end-to-end manner. Given a stream of dashcam
images and the vehicle’s (current) sensor measurements, e.g., speed, the controller pre-
dicts the acceleration and the change of course at each timestep. We utilize a determin-
istic soft attention mechanism that is trainable by standard back-propagation methods.
The soft attention mechanism applies attention weights multiplicatively to the features
and additively pools the results through the maps π. Our model feeds the context vectors
t produced by the controller map πc to the controller LSTM:
yc

t = πc({αc
yc

t,i}, {xt,i}) =

αc

t,ixt,i

(1)

l
(cid:88)

i=1

6

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

where i = {1, 2, . . . , l}. αc
t,i is an attention weight map output by a spatial softmax and
satisﬁes (cid:80)
i αc
t,i = 1. These attention weights can be interpreted as the probability over
l convolutional feature vectors. A location with a high attention weight is salient for
the task (driving). The attention model f c
t−1) is conditioned on the previous
LSTM state hc
t−1, and the current feature vectors Xt. It comprises a fully-connected
layer and a spatial softmax to yield normalized {αc

attn(Xt, hc

t,i}.

The outputs of the vehicle controller are the vehicle’s acceleration ˆat and the change
of course ˆct. To this end, we use additional multi-layer fully-connected blocks with
ReLU non-linearities, denoted by fa(yc
t ). We also add the entropy
H of the attention weight to the objective function:

t ) and fc(yc

t , hc

t , hc

Lc =

(cid:88)

t

(cid:0)(at − ˆat)2 + (ct − ˆct)2 + λcH(αc

t )(cid:1)

(2)

The entropy is computed on the attention map as though it were a probability distribu-
tion. Minimizing loss corresponds to minimizing entropy. Low entropy attention maps
are sparse and emphasize relatively few regions. We use a hyperparameter λc to control
the strength of the entropy regularization term.

3.2 Attention Alignments

The controller attention map provides input regions that the network attends to, and
these regions have a direct inﬂuence on the network’s output. Thus, to yield “introspec-
tive” explanation, we argue that the agent must attend to those areas. For example, if
a vehicle controller predicts “acceleration” by detecting a green trafﬁc light, the tex-
tual justiﬁcation must mention this evidence, e.g., “because the light has turned green”.
Here, we explain two approaches to align the vehicle controller and the textual justiﬁer
such that they look at the same input regions.

Strongly Aligned Attention (SAA): A consecutive set of spatially attended input re-
gions, each of which is encoded as a context vector yc
t by the vehicle controller, can be
directly used to generate a textual explanation (see Figure 2, right-top). Thus, models
share a single layer of an attention. As we detail in Section 3.3, our explanation module
uses temporal attention with weights β to the controller context vectors {yj
t , t = 1, . . .}
directly, and thus allows ﬂexibility in output tokens relative to input samples.

Weakly Aligned Attention (WAA): Instead of directly using vehicle controller’s atten-
tion, an explanation generator can have its own spatial attention network (see Figure 2,
right-bottom). A loss, i.e., the Kullback-Leibler divergence (DKL), between the two at-
tention maps makes the explanation generator refer to the salient objects:

La = λa

DKL(αc

t ||αj

t ) = λa

t,i(log αc
αc

t,i − log αj

t,i)

(3)

(cid:88)

t

(cid:88)

l
(cid:88)

t

i=1

where αc and αj are the attention maps generated by the vehicle controller and the
explanation generator model, respectively. We use a hyperparameter λa to control the
strength of the regularization term.

Textual Explanations for Self-Driving Vehicles

7

3.3 Textual Explanation Generator

Our textual explanation generator takes sequence of video frames of variable length and
generates a variable-length description/explanation. Descriptions and explanations are
typically part of the same sentence in the training data but are annotated with a separator.
In training and testing we use a synthetic separator token <sep> between description
and explanation, but treat them as a single sequence. The explanation LSTM predicts
the description/explanation sequence and outputs per-word softmax probabilities.

The source of context vectors for the description generator depends on the type of
alignment between attention maps. For weakly aligned attention or rationalizations, the
explanation generator creates its own spatial attention map αj at each time step t. This
map includes a loss against the controller attention map for weakly-aligned attention,
but has no such loss when generating rationalizations. The attention map αj is applied
to the CNN output yielding context vectors yj
t .
Our textual explanation generator explains the rationale behind the driving model,
and thus we argue that a justiﬁer needs the outputs from the vehicle motion predictor as
an input. We concatenate a tuple (ˆat, ˆct) with a spatially-attended context vector yj
t and
yc
t respectively for weakly and strongly aligned attention approaches. This concatenated
vector is then used to update the LSTM for a textual explanation generation.

The explanation module applies temporal attention with weights β to either the con-
troller context vectors directly {yc
t , t = 1, . . .} (strong alignment), or to the explanation
vectors {yj
t , t = 1, . . .} (weak alignment or rationalization). Such input sequence atten-
tion is common in sequence-to-sequence models and allows ﬂexibility in output tokens
relative to input samples [1]. The result of temporal attention application is (dropping
the c or j superscripts on y):

zk = π({βk,t}, {yt}) =

βk,tyt

(4)

T
(cid:88)

t=1

where (cid:80)
attn({yt}, he
puted by an attention model f e
as we explained in previous section (see supplemental material for details).

t βk,t = 1. The weight βk,t at each time k (for sentence generation) is com-
k−1), which is similar to the spatial attention

To summarize, we minimize the following negative log-likelihood (for training our
justiﬁer) as well as vehicle control estimation loss Lc and attention alignment loss La:

L = Lc + La −

log p(ok|ok−1, he

k, zk)

(5)

(cid:88)

k

4 Berkeley DeepDrive eXplanation Dataset (BDD-X)

In order to effectively generate and evaluate textual driving rationales we have collected
textual justiﬁcations for a subset of the Berkeley Deep Drive (BDD) dataset [26]. This
dataset contains videos, approximately 40 seconds in length, captured by a dashcam
mounted behind the front mirror of the vehicle. Videos are mostly captured during ur-
ban driving in various weather conditions, featuring day and nighttime. The dataset
also includes driving on other road types, such as residential roads (with and without

8

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Fig. 3: (A) Examples of input frames and corresponding human-annotated action de-
scription and justiﬁcation of how a driving decision was made. For visualization, we
sample frames at every two seconds. (B) BDD-X dataset details. Over 77 hours of driv-
ing with time-stamped human annotations for action descriptions and justiﬁcations.

lane markings), and contains all the typical driver’s activities such as staying in a lane,
turning, switching lanes, etc. Alongside the video data, the dataset provides a set of
time-stamped sensor measurements, such as vehicle’s velocity, course, and GPS loca-
tion. For sensor logs unsynchronized with the time-stamps of video data, we use the
estimates of the interpolated measurements.

In order to increase trust and reliability, the machine learning system underlying self
driving cars should be able to explain why at a certain time they make certain decisions.
Moreover, a car that justiﬁes its decision through natural language would also be user
friendly. Hence, we populate a subset of the BDD dataset with action description and
justiﬁcation for all the driving events along with their timestamps. We provide examples
from our Berkeley Deep Drive eXplanation (BDD-X) dataset in Figure 3 (A).

Annotation. We provide a driving video and ask a human annotator in Amazon Me-
chanical Turk to imagine herself being a driving instructor. Note that we speciﬁcally
select human annotators who are familiar with US driving rules. The annotator has to
describe what the driver is doing (especially when the behavior changes) and why, from
a point of view of a driving instructor. Each described action has to be accompanied
with a start and end time-stamp. The annotator may stop the video, forward and back-
ward through it while searching for the activities that are interesting and justiﬁable.

To ensure that the annotators provide us the driving rationales as well as descrip-
tions, we require that they separately enter the action description and the action justiﬁ-
cation: e.g., “The car is moving into the left lane” and “because the school bus in front
of it is stopping.”. In our preliminary annotation studies, we found that giving separate
annotation boxes is helpful for the annotator to understand the task and perform better.

Textual Explanations for Self-Driving Vehicles

9

Dataset Statistics. Our dataset (see Figure 3 (B)) is composed of over 77 hours of driv-
ing within 6,984 videos. The videos are taken in diverse driving conditions, e.g., day/night,
highway/city/countryside, summer/winter etc. On an average of 40 seconds, each video
contains around 3-4 actions, e.g., speeding up, slowing down, turning right etc., all of
which are annotated with a description and an explanation. Our dataset contains over
26K activities in over 8.4M frames. We introduce a training, a validation and a test set,
containing 5,588, 698 and 698 videos, respectively.

Inter-human agreement. Although we cannot have access to the internal thought pro-
cess of the drivers, one can infer the reason behind their actions using the visual evi-
dence of the scene. Besides, it would be challenging to setup the data collection process
which enables drivers to report justiﬁcations for all their actions, if at all possible. We
ensure the high quality of the collected annotations by relying on a pool of qualiﬁed
workers (i.e., they pass a qualiﬁcation test) and selective manual inspection.

Further, we measure the inter-human agreement on a subset of 998 training videos,
each of which has been annotated by two different workers. Our analysis is as follows.
In 72% of videos the number of annotated intervals differs by less than 3. The average
temporal IoU across annotators is 0.63 (SD = 0.21). When IoU > 0.5 the CIDEr
score across action descriptions is 142.60, across action justiﬁcations it is 97.49 (ran-
dom choice: 39.40/28.39, respectively). When IoU > 0.5 and action descriptions from
two annotators are identical (165 clips1) the CIDEr score across justiﬁcations is 200.72,
while a strong baseline, selecting a justiﬁcation from a different video with the same
action description, results in CIDEr score 136.72. These results show an agreement
among annotators and relevance of collected action descriptions and justiﬁcations.

Coverage of justiﬁcations. BDD-X dataset has over 26k annotations (77 hours) col-
lected from a substantial random subset of large-scale crowd-sourced driving video
dataset, which consists of all the typical drivers activities during urban driving. The
vocabulary of training action descriptions and justiﬁcations is 906 and 1,668 words re-
spectively, suggesting that justiﬁcations are more diverse than descriptions. Some of
the common actions are (frequency decreasing): moving forward, stopping, accelerat-
ing, slowing, turning, merging, veering, pulling [in]. Justiﬁcations cover most of the
relevant concepts: trafﬁc signs/lights, cars, lanes, crosswalks, passing, parking, pedes-
trians, waiting, blocking, safety etc.

5 Results and Discussion

Here, we ﬁrst provide our training and evaluation details, then make a quantitative and
qualitative analysis of our vehicle controller and our textual justiﬁer.

Training and Evaluation Details. As the convolutional feature encoder, we use 5-layer
CNN [3] that produces a 12×20×64-dimensional convolutional feature cube from the
last layer. The controller following the CNN has 5 fully connected layers (i.e., #hidden
dims: 1164, 100, 50, 10, respectively), which predict the acceleration and the change of

1 The number of video intervals (not full videos), where the provided action descriptions (not

explanations) are identical (common actions e.g., “the car slows down”).

10

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Fig. 4: Vehicle controllers attention maps in terms of four different entropy regulariza-
tion coefﬁcient λc={0,10,100,1000}. Red parts indicate where the model pays more
attention. Higher value of λc makes the attention maps sparser. We observe that sparser
attention maps improves the performance of generating textual explanations, while con-
trol performance is slightly degraded.

course, and is trained end-to-end from scratch. Using other more expressive networks
may give a performance boost over our base CNN conﬁguration, but these explorations
are out of our scope. Given the obtained convolutional feature cube, we ﬁrst train our
vehicle controller, and then the explanation generator (single layer LSTM unless stated
otherwise) by freezing the control network. For training, we use Adam optimizer [12]
and dropout [22] of 0.5 at hidden state connections and Xavier initialization [6]. The
standard dataset is split as 80% (5,588 videos) as the training set, 10% (698 videos) as
the test, and 10% (698 videos) as the validation set. Our model takes less than a day to
train on a single NVIDIA Titan X GPU.

For evaluating the vehicle controller we use the mean absolute error (lower is better)
and the distance correlation (higher is better) and for the justiﬁer we use BLEU [20],
METEOR [13], and CIDEr-D [24], as well as human evaluation. The former metrics
are widely used for the evaluation of video and image captioning models automatically
against ground truth.

5.1 Evaluating Vehicle Controller

We start by quantitatively comparing variants of our vehicle controller and the state of
the art, which include variants of the work by Bojarski et al. [3] and Kim et al. [11]
in Table 1. Note that these works differ from ours in that their output is the curva-
ture of driving, while our model estimates continuous acceleration and the change of
course values. Thus, their models have a single output, while ours estimate both con-
trol commands. In this experiment, we replaced their output layer with ours. For a fair
comparison, we use an identical CNN for all models.

In this experiment, each model estimates vehicle’s acceleration and the change of
course. Our vehicle controller predicts acceleration and the change of course, which
generally requires prior knowledge of vehicle’s current state, i.e., speed and course, and
navigational inputs, especially in urban driving. We observe that the use of the latest

Textual Explanations for Self-Driving Vehicles

11

Model

CNN+FC [3]†
CNN+FC [3]+P
CNN+LSTM+Attention [11]†

CNN+LSTM+Attention+P (Ours) 1000

CNN+LSTM+Attention+P (Ours)

100

CNN+LSTM+Attention+P (Ours)

CNN+LSTM+Attention+P (Ours)

λc

-

-

-

10

0

Mean of absolute error (MAE)

Mean of distance correlation

Acceleration (m/s2) Course (degree) Acceleration (m/s2) Course (degree)

6.92 [7.50]

6.09 [7.73]

6.87 [7.44]

5.02 [6.32]

2.68 [3.73]

2.33 [3.38]

2.29 [3.33]

12.1 [19.7]

6.74 [14.9]

10.2 [18.4]

6.94 [15.4]

6.17 [14.7]

6.10 [14.7]

6.06 [14.7]

0.17 [0.15]

0.21 [0.18]

0.19 [0.16]

0.65 [0.25]

0.78 [0.28]

0.81 [0.27]

0.82 [0.26]

0.16 [0.14]

0.39 [0.33]

0.22 [0.18]

0.43 [0.33]

0.43 [0.34]

0.46 [0.35]

0.47 [0.35]

Table 1: Comparing variants of our vehicle controller with different values of the en-
tropy regularization coefﬁcient λc={0, 10, 100, 1000} and the state-of-the-art. High
value of λc produces low entropy attention maps that are sparse and emphasize rela-
tively few regions. †: Models use a single image frame as an input. The standard devia-
tion is in braces. Abbreviation: FC (fully connected layer), P (prior inputs)

four consecutive frames and prior inputs (i.e., vehicle’s motion measurement and nav-
igational information) improves the control prediction accuracy (see 3rd vs. 7th row),
while the use of visual attention also provides improvements (see 1st vs. 3rd row).
Speciﬁcally, our model without the entropy regularization term (last row) performs the
best compared to CNN based approaches [3] and [11]. The improvement is especially
pronounced for acceleration estimation.

In Figure 4 we compare input images (ﬁrst column) and corresponding attention
maps for different entropy regularization coefﬁcients λc={0, 10, 100, 1000}. Red is
high attention, blue is low. As we see, higher λc lead to sparser maps. For better vi-
sualization, an attention map is overlaid by its contour lines and an input image.

Quantitatively, the controller performance (error and correlation) slightly degrade as
λc increases and the attention maps become more sparse (see bottom four rows in Ta-
ble 1). So there is some tension between sparse maps (which are more interpretable),
and controller performance. An alternative to regularization, [11] use causal ﬁltering
over the controller’s attention maps and achieve about 60% reduction in “hot” attention
pixels. Causal ﬁltering is desirable for the present work not only to improve sparseness
but because after causal ﬁltering, “hot” regions necessarily do have a causal effect on
controller behavior, whereas unﬁltered attention regions may not. We will explore it in
future work.

5.2 Evaluating Textual Explanations

In this section, we evaluate textual explanations against the ground truth explanation
using automatic evaluation measures, and also provide human evaluation followed by a
qualitative analysis.

Automatic Evaluation. For state-of-the-art comparison, we implement the S2VT [25]
and its variants. Note that in our implementation S2VT uses our CNN and does not

12

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Type

Model

λa

λc

(e.g., “because the light is red”)

(e.g., “the car stops”)

Control
inputs

Explanations

Descriptions

BLEU-4 METEOR CIDEr-D BLEU-4 METEOR CIDEr-D

Rationalization Ours (no constraints)

6.515

12.04

61.99

31.01

28.64

205.0

S2VT [25]

S2VT [25]+SA

S2VT [25]+SA+TA

Ours (with SAA)

Ours (with SAA)

Ours (with SAA)

Ours (with WAA)

Ours (with WAA)

Ours (with WAA)

N

N

N

Y

Y

Y

Y

Y

Y

Y

-

-

-

0

-

-

10

10

-

-

-

0

0

10

0

10

- 100

10 100

6.332

5.668

5.847

6.998

6.760

7.074

6.967

6.951

7.281

11.19

10.96

10.91

12.08

12.23

12.23

12.14

12.34

12.24

53.35

51.37

52.74

62.24

63.36

66.09

64.19

68.56

69.52

30.21

28.94

27.11

32.44

29.99

31.84

32.24

30.40

32.34

27.53

26.91

26.41

29.13

28.26

29.11

29.00

28.57

29.22

179.8

171.3

157.0

213.6

203.6

214.8

219.7

206.6

215.8

Introspective
explanation

Table 2: Comparing generated and ground truth (columns 6-8) descriptions (e.g., “the
car stops”) and explanations (e.g., “because the light is red”). We implement S2VT [25]
and variants with spatial attention (SA) and temporal attention (TA) as a baseline. We
tested two different attention alignment approaches, i.e., WAA (weakly aligned atten-
tion) and SAA (strongly aligned attention), with different combinations of two regular-
ization coefﬁcients: λa={0, 10} for the attention alignment and λc={0, 10, 100} for the
vehicle controller. Rationalization baseline relies on our model (WAA approach) but
has no attention alignment. Note that we report all values as a percentage.

use optical ﬂow features. In Table 2, we report a summary of our experiment validating
the quantitative effectiveness of our approach. Rows 5-10 show that best explanation
results are generally obtained with weakly-aligned attention. Comparing with row 4,
the introspective models all gave higher scores than the rationalization model for ex-
planation generation. Description scores are more mixed, but most of the introspective
model scores are higher. As we will see in the next section, our rationalization model
focuses on visual saliencies, which is sometimes different from what controller actually
“looks at”. For example, in Figure 5 (5th example), our controller sees the front vehicle
and our introspective models generate explanations such as “because the car in front
is moving slowly”, while our rationalization model does not see the front vehicle and
generates “because it’s turning to the right”.

As our training data are human observer annotations of driving videos, and they are
not the explanations of drivers, they are post-hoc rationalizations. However, based on
the visual evidence, (e.g., the existence of a turn right sign explains why the driver has
turned right even if we do not have access to the exact thought process of the driver),
they reﬂect typical causes of human driver behavior. The data suggest that grounding the
explanations in controller internal state helps produce explanations that better align with
human third-party explanations. Biasing the explanations toward controller state (which
the WAA and SAA models do) improves their plausibility from a human perspective,
which is a good sign. We further analyze human preference in the evaluation below.

Textual Explanations for Self-Driving Vehicles

13

Type

Model

Control
inputs

λa

λc

Correctness rate

Explanations Descriptions

Rationalization Ours (no constraints)

0

0

64.0%

Introspective
explanation

Ours (with SAA)

Ours (with WAA)

- 100

10 100

62.4%

66.0%

Y

Y

Y

92.8%

90.8%

93.5%

Table 3: Human evaluation of the generated action descriptions and explanations for
randomly chosen 250 video intervals. We measure the success rate where at least 2
human judges rate the generated description or explanation with a score 1 (correct and
speciﬁc/detailed) or 2 (correct).

Human Evaluation. In our ﬁrst human evaluation experiment the human judges are
only shown the descriptions, while in the second experiment they only see the explana-
tions (e.g. “The car ... because < explanation >”), to exclude the effect of explana-
tions/descriptions on the ratings, respectively. We randomly select 250 video intervals
and compare the Rationalization, WAA (λa=10, λc=100) and SAA (λc=100) predic-
tions. The humans are asked to rate a description/explanation on the scale {1..4} (1:
correct and speciﬁc/detailed, 2: correct, 3: minor error, 4: major error). We collect rat-
ings from 3 human judges for each task. Finally, we compute the majority vote, i.e., at
least 2 out of 3 judges should rate the description/explanation with a score 1 or 2.

As shown in Table 3, our WAA model outperforms the other two, supporting the
results above. Interestingly, Rationalization does better than SAA on this subset, ac-
cording to humans. This is perhaps because the explanation in SAA relies on the exact
same visual evidence as the controller, which may include counterfactually important
regions (i.e., there could be a stop sign here), but may confuse the explanation module.

Qualitative Analysis of Textual Justiﬁer. As Figure 5 shows, our proposed textual
explanation model generates plausible descriptions and explanations, while our model
also provides attention visualization of their evidence. In the ﬁrst example of Figure 5,
controller sees neighboring vehicles and lane markings, while explanation model gen-
erates “the car is driving forward (description)” and “because trafﬁc is moving freely
(explanation)”. In Figure 5, we also provide other examples that cover common driving
situations, such as driving forward (1st example), slowing/stopping (2nd, 3rd, and 5th),
and turning (4th and 6th). We also observe that our explanations have signiﬁcant diver-
sity, e.g., they provide various reasons for stopping: red lights, stop signs, and trafﬁc.
We provide more diverse examples as supplemental materials.

6 Conclusion

We described an end-to-end explainable driving model for self-driving cars by incor-
porating a grounded introspective explanation model. We showed that (i) incorporation

14

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Fig. 5: Example descriptions and explanations generated by our model compared to
human annotations. We provide (top row) input raw images and attention maps by (from
the 2nd row) vehicle controller, textual explanation generator, and rationalization model
(Note: (λc, λa) = (100,10) and the synthetic separator token is replaced by ‘+’).

of an attention mechanism and prior inputs improves vehicle control prediction accu-
racy compared to baselines, (ii) our grounded (introspective) model generates accurate
human understandable textual descriptions and explanations for driving behaviors, (iii)
attention alignment is shown to be effective at combining the vehicle controller and the
justiﬁcation model, and (iv) our BDD-X dataset allows us to train and automatically
evaluate our interpretable justiﬁcation model by comparing with human annotations.

Recent work [11] suggests that causal ﬁltering over attention heat maps can achieve
a useful reduction in explanation complexity by removing spurious blobs, which do not
signiﬁcantly affect the output. Causal ﬁltering idea would be worth exploring to obtain
causal attention heat maps, which can provide the causal ground of reasoning. Fur-
thermore, it would be beneﬁcial to incorporate stronger perception pipeline, e.g. object
detectors, to introduce more “grounded” visual representations and further improve the
quality and diversity of the generated explanations. Besides, incorporating driver’s eye
gaze into our explanation model for mimicking driver’s behavior, would be an interest-
ing potential future direction.

Acknowledgements. This work was supported by DARPA XAI program and Berkeley
DeepDrive.

Textual Explanations for Self-Driving Vehicles

15

References

1. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align

and translate. Conference on Learning Representations (2014)

2. Bojarski, M., Choromanska, A., Choromanski, K., Firner, B., Jackel, L., Muller, U., Zieba,
K.: Visualbackprop: visualizing cnns for autonomous driving. CoRR, vol. abs/1611.05418
(2016)

3. Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L.D.,
Monfort, M., Muller, U., Zhang, J., et al.: End to end learning for self-driving cars. CoRR
abs/1604.07316 (2016)

4. Buehler, M., Iagnemma, K., Singh, S.: The DARPA urban challenge: autonomous vehicles

in city trafﬁc, vol. 56. springer (2009)

5. Chen, C., Seff, A., Kornhauser, A., Xiao, J.: Deepdriving: Learning affordance for direct
perception in autonomous driving. In: Computer Vision (ICCV), 2015 IEEE International
Conference on. pp. 2722–2730. IEEE (2015)

6. Glorot, X., Bengio, Y.: Understanding the difﬁculty of training deep feedforward neural net-

works. In: Aistats. vol. 9, pp. 249–256 (2010)

7. Hendricks, L.A., Akata, Z., Rohrbach, M., Donahue, J., Schiele, B., Darrell, T.: Generating

visual explanations. In: European Conference on Computer Vision (ECCV) (2016)

8. Hendricks, L.A., Hu, R., Darrell, T., Akata, Z.: Grounding visual explanations. In: European

Conference on Computer Vision (ECCV) (2018)

9. Hochreiter, S., Schmidhuber, J.: Lstm can solve hard long time lag problems. In: Advances

in neural information processing systems. pp. 473–479 (1997)

10. Hyndman, R., Koehler, A.B., Ord, J.K., Snyder, R.D.: Forecasting with exponential smooth-

ing: the state space approach. Springer Science & Business Media (2008)

11. Kim, J., Canny, J.: Interpretable learning for self-driving cars by visualizing causal attention.
Proceedings of the IEEE international conference on computer vision pp. 2942–2950 (2017)
12. Kinga, D., Adam, J.B.: A method for stochastic optimization. In: International Conference

on Learning Representations (ICLR) (2015)

13. Lavie, A., Agarwal, A.: Meteor: An automatic metric for mt evaluation with improved corre-
lation with human judgments. In: Proceedings of the EMNLP 2011 Workshop on Statistical
Machine Translation. pp. 65–72 (2005)

14. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444 (2015)
15. Lee, H., Grosse, R., Ranganath, R., Ng, A.Y.: Convolutional deep belief networks for scalable
unsupervised learning of hierarchical representations. In: ICML. pp. 609–616. ACM (2009)
16. Levinson, J., Askeland, J., Becker, J., Dolson, J., Held, D., Kammel, S., Kolter, J.Z., Langer,
D., Pink, O., Pratt, V., et al.: Towards fully autonomous driving: Systems and algorithms. In:
Intelligent Vehicles Symposium (IV). pp. 163–168. IEEE (2011)

17. Lombrozo, T.: Explanation and abductive inference. The Oxford handbook of thinking and

reasoning (2012)

10(10) (2006)

18. Lombrozo, T.: The structure and function of explanations. Trends in Cognitive Science

19. Paden, B., ˇC´ap, M., Yong, S.Z., Yershov, D., Frazzoli, E.: A survey of motion planning and
control techniques for self-driving urban vehicles. IEEE Transactions on Intelligent Vehicles
1(1), 33–55 (2016)

20. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of
machine translation. In: Proceedings of the 40th annual meeting on association for computa-
tional linguistics. pp. 311–318. Association for Computational Linguistics (2002)

21. Park, D.H., Hendricks, L.A., Akata, Z., Schiele, B., Darrell, T., Rohrbach, M.: Multimodal
explanations: Justifying decisions and pointing to the evidence. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2018)

16

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

22. Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a
simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Re-
search 15(1), 1929–1958 (2014)

23. Urmson, C., Anhalt, J., Bagnell, D., Baker, C., Bittner, R., Clark, M., Dolan, J., Duggins,
D., Galatali, T., Geyer, C., et al.: Autonomous driving in urban environments: Boss and the
urban challenge. Journal of Field Robotics 25(8), 425–466 (2008)

24. Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: Consensus-based image description
evaluation. In: Proceedings of the IEEE conference on computer vision and pattern recogni-
tion. pp. 4566–4575 (2015)

25. Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., Saenko, K.: Sequence
to sequence-video to text. In: Proceedings of the IEEE international conference on computer
vision. pp. 4534–4542 (2015)

26. Xu, H., Gao, Y., Yu, F., Darrell, T.: End-to-end learning of driving models from large-scale
video datasets. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. pp. 2174–2182 (2017)

27. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.:
Show, attend and tell: Neural image caption generation with visual attention. In: International
Conference on Machine Learning. pp. 2048–2057 (2015)

28. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Euro-

pean Conference on Computer Vision. pp. 818–833. Springer (2014)

Supplemental Material:
Textual Explanations for Self-Driving Vehicles

Jinkyu Kim1, Anna Rohrbach1,2, Trevor Darrell1, John Canny1, and Zeynep Akata2,3

1 EECS, University of California, Berkeley CA 94720, USA
{jinkyu.kim,anna.rohrbach,trevordarrell,canny}@berkeley.edu
2 MPI for Informatics, Saarland Informatics Campus, 66123 Saarbr¨ucken, Germany
3 AMLab, University of Amsterdam, 1098 XH Amsterdam, Netherlands
z.akata@uva.nl

This supplementary material provides more details on the presented BDD-X dataset
and approach as well as more qualitative results. The document as structured as follows.
Section S.1 provides details on our Amazon Mechanical Turk annotation interface

and data collection procedure.

tions in BDD-X.

Section S.2 includes additional details on the collected descriptions and explana-

Section S.3 provides implementation details.
Finally, Section S.4 shows more qualitative examples of the predicted visual atten-

tion maps as well as textual explanations.

18

Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata

S.1 Our Amazon Mechanical Turk annotation interface

Our annotation prompt is demonstrated on Figure S1. To ensure that the annotators
provide us the driving rationales as well as descriptions, we require that they separately
enter the action description and the action justiﬁcation e.g., :“The car is moving into
the left lane” and “because the school bus in front of it is stopping”. In our preliminary
annotation studies, we found that giving separate annotation boxes are helpful for the
annotator to understand the task and perform better.

Fig. S1: Our Amazon Mechanical Turk annotation interface. Shown is a “dummy” input
(The car is stopping + Because the light is red), not an actual annotation.

Supplemental Material: Textual Explanations for Self-Driving Vehicles

19

S.2 Berkeley DeepDrive eXplanation (BDD-X) Dataset Details

Table S1 includes word counts for the top-30 most frequent words (excluding stop-
words) used in the action descriptions and action explanations, respectively. Note, that
word counts are obtained but taking all word forms into account (slow, slows, slow-
ing, slowed, slowly, etc). Most common actions are related to changes in speed, driving
forward and turning. However many also include merging, pulling, changing lanes,
veering, and less frequent actions like reversing, parking or using wipers. Action ex-
planations cover a diverse list of concepts relevant to driving scenario, such as state of

Table S1: BDD-X dataset details. We provide counts for top-30 words, where words are
counted along with all their forms, such as slow, slows, slowing, slowed, slowly, etc.

BDD-X action descriptions

BDD-X action explanations

Count

Word

Count

Word

stop
slow
forward
drive
move
accelerate
right
left
turn
road
lane
street
speed
come
merge
pull
intersection
head
continue
slight
make
brake
travel
highway
maintain
steer
proceed
steady
complete
veer

6879
6122
4322
3994
3273
2882
2616
2574
1912
1907
1832
1704
1072
1033
923
723
717
629
625
554
539
517
513
450
390
371
359
329
323
214

trafﬁc
light
red
move
clear
ahead
road
stop
lane
turn
front
green
park
intersection
right
slow
cars
left
street
forward
sign
make
approach
speed
down
pedestrian
go
cross
get
pass

7486
6116
3979
3915
3660
3629
3528
3430
3407
3333
2715
1928
1523
1513
1464
1395
1390
1272
1225
984
984
718
702
681
658
649
614
588
536
490

20

Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata

trafﬁc/lanes, trafﬁc lights/signs, pedestrians crossing the street, passing other cars, etc.
Although less frequent, many explanations contain references to weather conditions
(rain, snow), different types of vehicles (buses, vans, trucks), road bumps and safety of
the performed action.

We provide some additional examples in Table S2, showing, in particular, that some
explanations require complex reasoning (1,2,3) and illustrate attention to detail (4,5,6).

Description

Explanation

1: The car is making its way carefully through the intersection because another car has cut it off and pedestrians are approaching the crosswalk.
2: The car stops behind the truck
3: The car slows to a stop next to a line of parked cars
4: The car slows down
5: The car pulls over to the left side of the street
6: The car is moving at a constant slow pace
Table S2: Example annotations where explanations require complex reasoning (1,2,3)
and demonstrate attention to detail (4,5,6).

because it’s waiting for a car in the opposite lane to pass by.
since there is no where available to park, but another car up ahead is also double parked.
because it’s entering a school crossing zone.
to avoid a large pothole on the right.
because it is a single lane road with snow and ice on the road.

Supplemental Material: Textual Explanations for Self-Driving Vehicles

21

S.3 Implementation Details

Preprocessing Our model is trained to predict two vehicle control commands, i.e., an
acceleration and a change of course. At each time t, a change of course, ct, is computed
by taking a difference between a current vehicles course rt and a smoothed value ¯rt by
using simple exponential smoothing method [10].

ct = rt − ¯rt = rt − (cid:0)αsrt + (1 − αs)¯rt+1

(cid:1)

(1)

where αS is a smoothing factor 0 ≤ αs ≤ 1. Note that they are same as the original
timeseries when s =1, while values of s closer to zero have a greater smoothing effect
and are less response to recent changes. In this paper, we use αs as 0.01.

Long Short-Term Memory (LSTM) network: We use a long short-term memory
(LSTM) network [9] in our vehicle controller and explanation generator. The LSTM
is deﬁned as follows:













it
ft
ot
gt













sigm
sigm
sigm
tanh

=

A

(cid:19)

(cid:18)ht−1
yt

where it, ft, ot, and ct ∈ RM are the M-dimensional input, forget, output, memory
state of the LSTM at time t, respectively. Internal states of the LSTM are computed
conditioned on the hidden state ht ∈ RM and an α-weighted context vector yt ∈ Rd.
We use an afﬁne transformation A : Rd+M → R4M. The logistic sigmoid activation
function and the hyperbolic tangent activation function are represented as sigm and
tanh, respectively. The hidden state ht and the cell state ct of the LSTM are deﬁned as:

ct = ft (cid:12) ct−1 + it (cid:12) gt
ht = ot (cid:12) tanh(ct)

where (cid:12) is element-wise multiplication.

To initialize memory state ct and hidden state ht of the LSTM, we use average of
the convolutional feature slices x0,i ∈ Rd for i ∈ {0, 1, . . . , l} and feed through two
additional hidden layers: finit,c and finit,h.

c0 = finit,c

x0,i

, h0 = finit,h

(cid:33)

(cid:32)

1
l

l
(cid:88)

i=1

(cid:32)

1
l

l
(cid:88)

i=1

(cid:33)

x0,i

(2)

(3)

(4)

22

Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata

S.4 Additional Examples

In this section, we provide more examples of explanations of the driving decisions by
exploiting both attention visualization and textual justiﬁcation.

Our model is also able to generate novel explanations not present in the training set.
We provide some examples as follows: (1) “because there are no obstructions in the
lane ahead”, (2) “because the car ahead is slowing to make a stop”, (3) “because the
car is turning onto a different street”, (4) “as it is now making a turn to enter another
street”, (5) “the car is stopped at an intersection at a red light”, and (6) “because there
are no other cars in the intersection”.

Supplemental Material: Textual Explanations for Self-Driving Vehicles

23

Fig. S2: Examples of descriptions and explanations. In this experiment, we use (λc,λa)
as (100,10). A synthetic separator token is replaced by ’+’.

24

Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata

Fig. S3: Additional examples of descriptions and explanations. In this experiment, we
use (λc,λa) as (100,10). A synthetic separator token is replaced by ’+’.

8
1
0
2
 
l
u
J
 
0
3
 
 
]

V
C
.
s
c
[
 
 
1
v
6
4
5
1
1
.
7
0
8
1
:
v
i
X
r
a

Textual Explanations for Self-Driving Vehicles

Jinkyu Kim1, Anna Rohrbach1,2, Trevor Darrell1, John Canny1, and Zeynep Akata2,3

1 EECS, University of California, Berkeley CA 94720, USA
{jinkyu.kim,anna.rohrbach,trevordarrell,canny}@berkeley.edu
2 MPI for Informatics, Saarland Informatics Campus, 66123 Saarbr¨ucken, Germany
3 AMLab, University of Amsterdam, 1098 XH Amsterdam, Netherlands
z.akata@uva.nl

Abstract. Deep neural perception and control networks have become key com-
ponents of self-driving vehicles. User acceptance is likely to beneﬁt from easy-
to-interpret textual explanations which allow end-users to understand what trig-
gered a particular behavior. Explanations may be triggered by the neural con-
troller, namely introspective explanations, or informed by the neural controller’s
output, namely rationalizations. We propose a new approach to introspective ex-
planations which consists of two parts. First, we use a visual (spatial) attention
model to train a convolutional network end-to-end from images to the vehicle
control commands, i.e., acceleration and change of course. The controller’s at-
tention identiﬁes image regions that potentially inﬂuence the network’s output.
Second, we use an attention-based video-to-text model to produce textual ex-
planations of model actions. The attention maps of controller and explanation
model are aligned so that explanations are grounded in the parts of the scene that
mattered to the controller. We explore two approaches to attention alignment,
strong- and weak-alignment. Finally, we explore a version of our model that
generates rationalizations, and compare with introspective explanations on the
same video segments. We evaluate these models on a novel driving dataset with
ground-truth human explanations, the Berkeley DeepDrive eXplanation (BDD-
X) dataset. Code is available at https://github.com/JinkyuKimUCB/
explainable-deep-driving

Keywords: Explainable Deep Driving · BDD-X dataset

1

Introduction

Deep neural networks are an effective tool [3,26] to learn vehicle controllers for self-
driving cars in an end-to-end manner. Despite their effectiveness as function estimators,
DNNs are typically cryptic black-boxes. There are no explainable states or labels in
such a network, and representations are fully distributed as sets of activations. Explain-
able models that make deep models more transparent are important for a number of
reasons: (i) user acceptance – self-driving vehicles are a radical technology for users
to accept, and require a very high level of trust, (ii) understanding and extrapolation of
vehicle behavior – users ideally should be able to anticipate what the vehicle will do
in most situations, (iii) effective communication – they help user communicate prefer-
ences to the vehicle and vice versa.

2

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Fig. 1: Our model predicts vehicles control commands, i.e., an acceleration and a change
of course, at each timestep, while an explanation model generates a natural language
explanation of the rationales, e.g., “The car is driving forward because there are no
other cars in its lane”, and a visual explanation in the form of attention – attended
regions directly inﬂuence the textual explanation generation process.

Explanations can be either rationalizations – explanations that justify the system’s
behavior in a post-hoc manner, or introspective explanations – explanations that are
based on the system’s internal state. Introspective explanations represent causal rela-
tionships between the system’s input and its behavior, and address all the goals above.
Rationalizations can address acceptance, (i) above, but are less helpful with (ii) under-
standing the causal behavior of the model or (iii) communication which is grounded in
the vehicle’s internal state (known as theory of mind in human communication).

One way of generating introspective explanations is via visual attention [27,11]. Vi-
sual attention ﬁlters out non-salient image regions, and image areas inside the attended
region have potential causal effect on the output (those outside cannot). As shown in
[11], additional salience ﬁltering can be applied so that the attention map shows only re-
gions that causally affect the output. Visual attention constrains the reasons for the con-
trollers actions but does not e.g., tie speciﬁc actions to speciﬁc input regions e.g., “the
vehicle slowed down because the light controlling the intersection is red”. It is also
likely to be less convenient for passengers to replay the attention map vs. a (typically
on-demand) speech presentation of a textual explanation.

In this work, we focus on generating textual descriptions and explanations, such as
the pair: “vehicle slows down” and “because it is approaching an intersection and the
light is red” as in Figure 1. Natural language has an advantage of being inherently un-
derstandable and does not require familiarity with the design of an intelligent system
in order to provide useful information. In order to train such a model, we collect ex-
planations from human annotators. Our explanation dataset is built on top of another
large-scale driving dataset [26] collected from dashboard cameras in human driven ve-

Textual Explanations for Self-Driving Vehicles

3

hicles. Annotators view the video dataset, compose descriptions of the vehicle’s activity
and explanations for the actions that the vehicle driver performed.

Obtaining training data for vehicle explanations is by itself a signiﬁcant challenge.
The ground truth explanations are in fact often rationalizations (generated by an ob-
server rather than the driver), and there are additional challenges with acquiring driver
data. But even more than that, it is currently impossible to obtain human explanations of
what the vehicle controller was thinking, i.e., a real ground truth. Nevertheless our ex-
periments show that using attention alignment between controller and explanation mod-
els generally improves the quality of explanations, i.e., generates explanations which
better match the human rationalizations of the driving videos.

Our contributions are as follows. (1) We propose an introspective textual explana-
tion model for self-driving cars to provide easy-to-interpret explanations for the behav-
ior of a deep vehicle control network. (2) We integrate our explanation generator with
the vehicle controller by aligning their attentions to ground the explanation, and com-
pare two approaches: attention-aligned explanations and non-aligned rationalizations.
(3) We generated a large-scale Berkeley DeepDrive eXplanation (BDD-X) dataset with
over 6,984 video clips annotated with driving descriptions, e.g., “The car slows down”
and explanations, e.g., “because it is about to merge with the busy highway”. Our
dataset provides a new test-bed for measuring progress towards developing explainable
models for self-driving cars.

2 Related Work

In this section, we review existing work on end-to-end learning for self-driving cars as
well as work on visual explanation and justiﬁcation.

End-to-End Learning for Self-Driving Cars: Most of vehicle controllers for self-
driving cars can be divided in two types of approaches [5]: (1) a mediated perception-
based approach and (2) an end-to-end learning approach. The mediated perception-
based approach depends on recognizing human-designated features, such as lane mark-
ings, trafﬁc lights, pedestrians or cars, which generally require demanding parameter
tuning for a balanced performance [19]. Notable examples include [23], [4], and [16].
As for the end-to-end approaches, recent works [3,26] suggest that neural networks
can be successfully applied to self-driving cars in an end-to-end manner. Most of these
approaches use behavioral cloning that learns a driving policy as a supervised learn-
ing problem over observation-action pairs from human driving demonstrations. Among
these, [3] present a deep neural vehicle controller network that directly maps a stream
of dashcam images to steering controls, while [26] use a deep neural network that takes
input raw pixels and prior vehicle states and predict vehicle’s future motion. Despite
their potential, the effectiveness of these approaches is limited by their inability to ex-
plain the rationale for the system’s decisions, which makes their behavior opaque and
uninterpretable. In this work, we propose an end-to-end trainable system for self driving
cars that is able to justify its predictions visually via attention maps and textually via
natural language.

4

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Fig. 2: Vehicle controller generates spatial attention maps αc for each frame, predicts
acceleration and change of course (ˆct, ˆat) that condition the explanation. Explanation
generator predicts temporal attention across frames (β) and a spatial attention in each
frame (αj). SAA uses αc, WAA enforces a loss between αj and αc.

Visual and Textual Explanations: The importance of explanations for an end-user has
been studied from the psychological perspective [17,18], showing that humans use ex-
planations as a guide for learning and understanding by building inferences and seeking
propositions or judgments that enrich their prior knowledge. They usually seek for ex-
planations to ﬁll the requested gap depending on prior knowledge and goal in question.

In support of this trend, recently explainability has been growing as a ﬁeld in com-
puter vision and machine learning. Especially, there is a growing interest in introspec-
tive deep neural networks. [28] use deconvolution to visualize inner-layer activations of
convolutional networks. [14] propose automatically-generated captions for textual ex-
planations of images. [2] develop a richer notion of contribution of a pixel to the output.
However, a difﬁculty with deconvolution-style approaches is the lack of formal mea-
sures of how the network output is affected by spatially-extended features (rather than
pixels). Exceptions to this rule are attention-based approaches. [11] propose attention-
based approach with causal ﬁltering that removes spurious attention blobs. However,
it is also important to be able to justify the decisions that were made and explain why
they are reasonable in a human understandable manner, i.e., a natural language. For an
image classiﬁcation problem, [7,8] used an LSTM [9] caption generation model that
generates textual justiﬁcations for a CNN model. [21] combine attention-based model
and a textual justiﬁcation system to produce an interpretable model. To our knowledge,
ours is the ﬁrst attempt to justify the decisions of a real-time deep controller through a
combination of attention and natural language explanations on a stream of images.

Textual Explanations for Self-Driving Vehicles

5

3 Explainable Driving Model

In this paper, we propose a driving model that explains how a driving decision was
made both (i) by visualizing image regions where the decision maker attends to and (ii)
by generating a textual description and explanation of what has triggered a particular
driving decision, e.g., “the car continues (description) because trafﬁc ﬂows freely (ex-
planation)”. As we summarize in Figure 2, our model involves two parts: (1) a Vehicle
controller, which is trained to learn human-demonstrated vehicle control commands,
e.g., an acceleration and a change of course; our controller uses a visual (spatial) atten-
tion mechanism that identiﬁes potentially inﬂuential image regions for the network’s
output; (2) a Textual explanation generator, which generates textual descriptions and
explanations controller behavior. The key to the approach is to align the attention maps.

Preprocessing. Our model is trained to predict two vehicle control commands, i.e., an
acceleration and a change of course. At each time t, an acceleration, at, is measured by
taking the derivative of speed measurements, and a change of course, ct, is computed by
taking a difference between a current vehicle’s course and a smoothed value by using
simple exponential smoothing method [10]. We provide details in supplemental mate-
rial. To reduce computational burden, we down-sample to 10Hz and reduce the input
dimensionality by resizing raw images to a 90×160×3 image with nearest-neighbor
scaling algorithm. Each image is then normalized by subtracting the mean from the raw
input pixels and dividing by its standard deviation. This preprocessing is applied to the
latest 4 frames, which are then stacked to produce the ﬁnal input to the neural network.

Convolutional Feature Encoder. We use a convolutional neural network to encode
the visual information into a set of visual feature vectors at time t, i.e., convolutional
feature cube Xt = {xt,1, xt,2, . . . , xt,l} where xt,i ∈ Rd for i ∈ {1, 2, . . . , l} and l is
the number of different spatial regions of the given input. Each feature vector contains a
high-level description of objects present in a certain input region. This allows us to focus
selectively on different regions of the given image by choosing a subset of these feature
vectors. We use a ﬁve-layered convolutional network as in [3,11] and omit max-pooling
layers to prevent spatial information loss [15]. The output is a three-dimensional feature
cube Xt and the feature block has the size w×h×d at each time t.

3.1 Vehicle Controller

Our vehicle controller is trained in an end-to-end manner. Given a stream of dashcam
images and the vehicle’s (current) sensor measurements, e.g., speed, the controller pre-
dicts the acceleration and the change of course at each timestep. We utilize a determin-
istic soft attention mechanism that is trainable by standard back-propagation methods.
The soft attention mechanism applies attention weights multiplicatively to the features
and additively pools the results through the maps π. Our model feeds the context vectors
t produced by the controller map πc to the controller LSTM:
yc

t = πc({αc
yc

t,i}, {xt,i}) =

αc

t,ixt,i

(1)

l
(cid:88)

i=1

6

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

where i = {1, 2, . . . , l}. αc
t,i is an attention weight map output by a spatial softmax and
satisﬁes (cid:80)
i αc
t,i = 1. These attention weights can be interpreted as the probability over
l convolutional feature vectors. A location with a high attention weight is salient for
the task (driving). The attention model f c
t−1) is conditioned on the previous
LSTM state hc
t−1, and the current feature vectors Xt. It comprises a fully-connected
layer and a spatial softmax to yield normalized {αc

attn(Xt, hc

t,i}.

The outputs of the vehicle controller are the vehicle’s acceleration ˆat and the change
of course ˆct. To this end, we use additional multi-layer fully-connected blocks with
ReLU non-linearities, denoted by fa(yc
t ). We also add the entropy
H of the attention weight to the objective function:

t ) and fc(yc

t , hc

t , hc

Lc =

(cid:88)

t

(cid:0)(at − ˆat)2 + (ct − ˆct)2 + λcH(αc

t )(cid:1)

(2)

The entropy is computed on the attention map as though it were a probability distribu-
tion. Minimizing loss corresponds to minimizing entropy. Low entropy attention maps
are sparse and emphasize relatively few regions. We use a hyperparameter λc to control
the strength of the entropy regularization term.

3.2 Attention Alignments

The controller attention map provides input regions that the network attends to, and
these regions have a direct inﬂuence on the network’s output. Thus, to yield “introspec-
tive” explanation, we argue that the agent must attend to those areas. For example, if
a vehicle controller predicts “acceleration” by detecting a green trafﬁc light, the tex-
tual justiﬁcation must mention this evidence, e.g., “because the light has turned green”.
Here, we explain two approaches to align the vehicle controller and the textual justiﬁer
such that they look at the same input regions.

Strongly Aligned Attention (SAA): A consecutive set of spatially attended input re-
gions, each of which is encoded as a context vector yc
t by the vehicle controller, can be
directly used to generate a textual explanation (see Figure 2, right-top). Thus, models
share a single layer of an attention. As we detail in Section 3.3, our explanation module
uses temporal attention with weights β to the controller context vectors {yj
t , t = 1, . . .}
directly, and thus allows ﬂexibility in output tokens relative to input samples.

Weakly Aligned Attention (WAA): Instead of directly using vehicle controller’s atten-
tion, an explanation generator can have its own spatial attention network (see Figure 2,
right-bottom). A loss, i.e., the Kullback-Leibler divergence (DKL), between the two at-
tention maps makes the explanation generator refer to the salient objects:

La = λa

DKL(αc

t ||αj

t ) = λa

t,i(log αc
αc

t,i − log αj

t,i)

(3)

(cid:88)

t

(cid:88)

l
(cid:88)

t

i=1

where αc and αj are the attention maps generated by the vehicle controller and the
explanation generator model, respectively. We use a hyperparameter λa to control the
strength of the regularization term.

Textual Explanations for Self-Driving Vehicles

7

3.3 Textual Explanation Generator

Our textual explanation generator takes sequence of video frames of variable length and
generates a variable-length description/explanation. Descriptions and explanations are
typically part of the same sentence in the training data but are annotated with a separator.
In training and testing we use a synthetic separator token <sep> between description
and explanation, but treat them as a single sequence. The explanation LSTM predicts
the description/explanation sequence and outputs per-word softmax probabilities.

The source of context vectors for the description generator depends on the type of
alignment between attention maps. For weakly aligned attention or rationalizations, the
explanation generator creates its own spatial attention map αj at each time step t. This
map includes a loss against the controller attention map for weakly-aligned attention,
but has no such loss when generating rationalizations. The attention map αj is applied
to the CNN output yielding context vectors yj
t .
Our textual explanation generator explains the rationale behind the driving model,
and thus we argue that a justiﬁer needs the outputs from the vehicle motion predictor as
an input. We concatenate a tuple (ˆat, ˆct) with a spatially-attended context vector yj
t and
yc
t respectively for weakly and strongly aligned attention approaches. This concatenated
vector is then used to update the LSTM for a textual explanation generation.

The explanation module applies temporal attention with weights β to either the con-
troller context vectors directly {yc
t , t = 1, . . .} (strong alignment), or to the explanation
vectors {yj
t , t = 1, . . .} (weak alignment or rationalization). Such input sequence atten-
tion is common in sequence-to-sequence models and allows ﬂexibility in output tokens
relative to input samples [1]. The result of temporal attention application is (dropping
the c or j superscripts on y):

zk = π({βk,t}, {yt}) =

βk,tyt

(4)

T
(cid:88)

t=1

where (cid:80)
attn({yt}, he
puted by an attention model f e
as we explained in previous section (see supplemental material for details).

t βk,t = 1. The weight βk,t at each time k (for sentence generation) is com-
k−1), which is similar to the spatial attention

To summarize, we minimize the following negative log-likelihood (for training our
justiﬁer) as well as vehicle control estimation loss Lc and attention alignment loss La:

L = Lc + La −

log p(ok|ok−1, he

k, zk)

(5)

(cid:88)

k

4 Berkeley DeepDrive eXplanation Dataset (BDD-X)

In order to effectively generate and evaluate textual driving rationales we have collected
textual justiﬁcations for a subset of the Berkeley Deep Drive (BDD) dataset [26]. This
dataset contains videos, approximately 40 seconds in length, captured by a dashcam
mounted behind the front mirror of the vehicle. Videos are mostly captured during ur-
ban driving in various weather conditions, featuring day and nighttime. The dataset
also includes driving on other road types, such as residential roads (with and without

8

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Fig. 3: (A) Examples of input frames and corresponding human-annotated action de-
scription and justiﬁcation of how a driving decision was made. For visualization, we
sample frames at every two seconds. (B) BDD-X dataset details. Over 77 hours of driv-
ing with time-stamped human annotations for action descriptions and justiﬁcations.

lane markings), and contains all the typical driver’s activities such as staying in a lane,
turning, switching lanes, etc. Alongside the video data, the dataset provides a set of
time-stamped sensor measurements, such as vehicle’s velocity, course, and GPS loca-
tion. For sensor logs unsynchronized with the time-stamps of video data, we use the
estimates of the interpolated measurements.

In order to increase trust and reliability, the machine learning system underlying self
driving cars should be able to explain why at a certain time they make certain decisions.
Moreover, a car that justiﬁes its decision through natural language would also be user
friendly. Hence, we populate a subset of the BDD dataset with action description and
justiﬁcation for all the driving events along with their timestamps. We provide examples
from our Berkeley Deep Drive eXplanation (BDD-X) dataset in Figure 3 (A).

Annotation. We provide a driving video and ask a human annotator in Amazon Me-
chanical Turk to imagine herself being a driving instructor. Note that we speciﬁcally
select human annotators who are familiar with US driving rules. The annotator has to
describe what the driver is doing (especially when the behavior changes) and why, from
a point of view of a driving instructor. Each described action has to be accompanied
with a start and end time-stamp. The annotator may stop the video, forward and back-
ward through it while searching for the activities that are interesting and justiﬁable.

To ensure that the annotators provide us the driving rationales as well as descrip-
tions, we require that they separately enter the action description and the action justiﬁ-
cation: e.g., “The car is moving into the left lane” and “because the school bus in front
of it is stopping.”. In our preliminary annotation studies, we found that giving separate
annotation boxes is helpful for the annotator to understand the task and perform better.

Textual Explanations for Self-Driving Vehicles

9

Dataset Statistics. Our dataset (see Figure 3 (B)) is composed of over 77 hours of driv-
ing within 6,984 videos. The videos are taken in diverse driving conditions, e.g., day/night,
highway/city/countryside, summer/winter etc. On an average of 40 seconds, each video
contains around 3-4 actions, e.g., speeding up, slowing down, turning right etc., all of
which are annotated with a description and an explanation. Our dataset contains over
26K activities in over 8.4M frames. We introduce a training, a validation and a test set,
containing 5,588, 698 and 698 videos, respectively.

Inter-human agreement. Although we cannot have access to the internal thought pro-
cess of the drivers, one can infer the reason behind their actions using the visual evi-
dence of the scene. Besides, it would be challenging to setup the data collection process
which enables drivers to report justiﬁcations for all their actions, if at all possible. We
ensure the high quality of the collected annotations by relying on a pool of qualiﬁed
workers (i.e., they pass a qualiﬁcation test) and selective manual inspection.

Further, we measure the inter-human agreement on a subset of 998 training videos,
each of which has been annotated by two different workers. Our analysis is as follows.
In 72% of videos the number of annotated intervals differs by less than 3. The average
temporal IoU across annotators is 0.63 (SD = 0.21). When IoU > 0.5 the CIDEr
score across action descriptions is 142.60, across action justiﬁcations it is 97.49 (ran-
dom choice: 39.40/28.39, respectively). When IoU > 0.5 and action descriptions from
two annotators are identical (165 clips1) the CIDEr score across justiﬁcations is 200.72,
while a strong baseline, selecting a justiﬁcation from a different video with the same
action description, results in CIDEr score 136.72. These results show an agreement
among annotators and relevance of collected action descriptions and justiﬁcations.

Coverage of justiﬁcations. BDD-X dataset has over 26k annotations (77 hours) col-
lected from a substantial random subset of large-scale crowd-sourced driving video
dataset, which consists of all the typical drivers activities during urban driving. The
vocabulary of training action descriptions and justiﬁcations is 906 and 1,668 words re-
spectively, suggesting that justiﬁcations are more diverse than descriptions. Some of
the common actions are (frequency decreasing): moving forward, stopping, accelerat-
ing, slowing, turning, merging, veering, pulling [in]. Justiﬁcations cover most of the
relevant concepts: trafﬁc signs/lights, cars, lanes, crosswalks, passing, parking, pedes-
trians, waiting, blocking, safety etc.

5 Results and Discussion

Here, we ﬁrst provide our training and evaluation details, then make a quantitative and
qualitative analysis of our vehicle controller and our textual justiﬁer.

Training and Evaluation Details. As the convolutional feature encoder, we use 5-layer
CNN [3] that produces a 12×20×64-dimensional convolutional feature cube from the
last layer. The controller following the CNN has 5 fully connected layers (i.e., #hidden
dims: 1164, 100, 50, 10, respectively), which predict the acceleration and the change of

1 The number of video intervals (not full videos), where the provided action descriptions (not

explanations) are identical (common actions e.g., “the car slows down”).

10

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Fig. 4: Vehicle controllers attention maps in terms of four different entropy regulariza-
tion coefﬁcient λc={0,10,100,1000}. Red parts indicate where the model pays more
attention. Higher value of λc makes the attention maps sparser. We observe that sparser
attention maps improves the performance of generating textual explanations, while con-
trol performance is slightly degraded.

course, and is trained end-to-end from scratch. Using other more expressive networks
may give a performance boost over our base CNN conﬁguration, but these explorations
are out of our scope. Given the obtained convolutional feature cube, we ﬁrst train our
vehicle controller, and then the explanation generator (single layer LSTM unless stated
otherwise) by freezing the control network. For training, we use Adam optimizer [12]
and dropout [22] of 0.5 at hidden state connections and Xavier initialization [6]. The
standard dataset is split as 80% (5,588 videos) as the training set, 10% (698 videos) as
the test, and 10% (698 videos) as the validation set. Our model takes less than a day to
train on a single NVIDIA Titan X GPU.

For evaluating the vehicle controller we use the mean absolute error (lower is better)
and the distance correlation (higher is better) and for the justiﬁer we use BLEU [20],
METEOR [13], and CIDEr-D [24], as well as human evaluation. The former metrics
are widely used for the evaluation of video and image captioning models automatically
against ground truth.

5.1 Evaluating Vehicle Controller

We start by quantitatively comparing variants of our vehicle controller and the state of
the art, which include variants of the work by Bojarski et al. [3] and Kim et al. [11]
in Table 1. Note that these works differ from ours in that their output is the curva-
ture of driving, while our model estimates continuous acceleration and the change of
course values. Thus, their models have a single output, while ours estimate both con-
trol commands. In this experiment, we replaced their output layer with ours. For a fair
comparison, we use an identical CNN for all models.

In this experiment, each model estimates vehicle’s acceleration and the change of
course. Our vehicle controller predicts acceleration and the change of course, which
generally requires prior knowledge of vehicle’s current state, i.e., speed and course, and
navigational inputs, especially in urban driving. We observe that the use of the latest

Textual Explanations for Self-Driving Vehicles

11

Model

CNN+FC [3]†
CNN+FC [3]+P
CNN+LSTM+Attention [11]†

CNN+LSTM+Attention+P (Ours) 1000

CNN+LSTM+Attention+P (Ours)

100

CNN+LSTM+Attention+P (Ours)

CNN+LSTM+Attention+P (Ours)

λc

-

-

-

10

0

Mean of absolute error (MAE)

Mean of distance correlation

Acceleration (m/s2) Course (degree) Acceleration (m/s2) Course (degree)

6.92 [7.50]

6.09 [7.73]

6.87 [7.44]

5.02 [6.32]

2.68 [3.73]

2.33 [3.38]

2.29 [3.33]

12.1 [19.7]

6.74 [14.9]

10.2 [18.4]

6.94 [15.4]

6.17 [14.7]

6.10 [14.7]

6.06 [14.7]

0.17 [0.15]

0.21 [0.18]

0.19 [0.16]

0.65 [0.25]

0.78 [0.28]

0.81 [0.27]

0.82 [0.26]

0.16 [0.14]

0.39 [0.33]

0.22 [0.18]

0.43 [0.33]

0.43 [0.34]

0.46 [0.35]

0.47 [0.35]

Table 1: Comparing variants of our vehicle controller with different values of the en-
tropy regularization coefﬁcient λc={0, 10, 100, 1000} and the state-of-the-art. High
value of λc produces low entropy attention maps that are sparse and emphasize rela-
tively few regions. †: Models use a single image frame as an input. The standard devia-
tion is in braces. Abbreviation: FC (fully connected layer), P (prior inputs)

four consecutive frames and prior inputs (i.e., vehicle’s motion measurement and nav-
igational information) improves the control prediction accuracy (see 3rd vs. 7th row),
while the use of visual attention also provides improvements (see 1st vs. 3rd row).
Speciﬁcally, our model without the entropy regularization term (last row) performs the
best compared to CNN based approaches [3] and [11]. The improvement is especially
pronounced for acceleration estimation.

In Figure 4 we compare input images (ﬁrst column) and corresponding attention
maps for different entropy regularization coefﬁcients λc={0, 10, 100, 1000}. Red is
high attention, blue is low. As we see, higher λc lead to sparser maps. For better vi-
sualization, an attention map is overlaid by its contour lines and an input image.

Quantitatively, the controller performance (error and correlation) slightly degrade as
λc increases and the attention maps become more sparse (see bottom four rows in Ta-
ble 1). So there is some tension between sparse maps (which are more interpretable),
and controller performance. An alternative to regularization, [11] use causal ﬁltering
over the controller’s attention maps and achieve about 60% reduction in “hot” attention
pixels. Causal ﬁltering is desirable for the present work not only to improve sparseness
but because after causal ﬁltering, “hot” regions necessarily do have a causal effect on
controller behavior, whereas unﬁltered attention regions may not. We will explore it in
future work.

5.2 Evaluating Textual Explanations

In this section, we evaluate textual explanations against the ground truth explanation
using automatic evaluation measures, and also provide human evaluation followed by a
qualitative analysis.

Automatic Evaluation. For state-of-the-art comparison, we implement the S2VT [25]
and its variants. Note that in our implementation S2VT uses our CNN and does not

12

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Type

Model

λa

λc

(e.g., “because the light is red”)

(e.g., “the car stops”)

Control
inputs

Explanations

Descriptions

BLEU-4 METEOR CIDEr-D BLEU-4 METEOR CIDEr-D

Rationalization Ours (no constraints)

6.515

12.04

61.99

31.01

28.64

205.0

S2VT [25]

S2VT [25]+SA

S2VT [25]+SA+TA

Ours (with SAA)

Ours (with SAA)

Ours (with SAA)

Ours (with WAA)

Ours (with WAA)

Ours (with WAA)

N

N

N

Y

Y

Y

Y

Y

Y

Y

-

-

-

0

-

-

10

10

-

-

-

0

0

10

0

10

- 100

10 100

6.332

5.668

5.847

6.998

6.760

7.074

6.967

6.951

7.281

11.19

10.96

10.91

12.08

12.23

12.23

12.14

12.34

12.24

53.35

51.37

52.74

62.24

63.36

66.09

64.19

68.56

69.52

30.21

28.94

27.11

32.44

29.99

31.84

32.24

30.40

32.34

27.53

26.91

26.41

29.13

28.26

29.11

29.00

28.57

29.22

179.8

171.3

157.0

213.6

203.6

214.8

219.7

206.6

215.8

Introspective
explanation

Table 2: Comparing generated and ground truth (columns 6-8) descriptions (e.g., “the
car stops”) and explanations (e.g., “because the light is red”). We implement S2VT [25]
and variants with spatial attention (SA) and temporal attention (TA) as a baseline. We
tested two different attention alignment approaches, i.e., WAA (weakly aligned atten-
tion) and SAA (strongly aligned attention), with different combinations of two regular-
ization coefﬁcients: λa={0, 10} for the attention alignment and λc={0, 10, 100} for the
vehicle controller. Rationalization baseline relies on our model (WAA approach) but
has no attention alignment. Note that we report all values as a percentage.

use optical ﬂow features. In Table 2, we report a summary of our experiment validating
the quantitative effectiveness of our approach. Rows 5-10 show that best explanation
results are generally obtained with weakly-aligned attention. Comparing with row 4,
the introspective models all gave higher scores than the rationalization model for ex-
planation generation. Description scores are more mixed, but most of the introspective
model scores are higher. As we will see in the next section, our rationalization model
focuses on visual saliencies, which is sometimes different from what controller actually
“looks at”. For example, in Figure 5 (5th example), our controller sees the front vehicle
and our introspective models generate explanations such as “because the car in front
is moving slowly”, while our rationalization model does not see the front vehicle and
generates “because it’s turning to the right”.

As our training data are human observer annotations of driving videos, and they are
not the explanations of drivers, they are post-hoc rationalizations. However, based on
the visual evidence, (e.g., the existence of a turn right sign explains why the driver has
turned right even if we do not have access to the exact thought process of the driver),
they reﬂect typical causes of human driver behavior. The data suggest that grounding the
explanations in controller internal state helps produce explanations that better align with
human third-party explanations. Biasing the explanations toward controller state (which
the WAA and SAA models do) improves their plausibility from a human perspective,
which is a good sign. We further analyze human preference in the evaluation below.

Textual Explanations for Self-Driving Vehicles

13

Type

Model

Control
inputs

λa

λc

Correctness rate

Explanations Descriptions

Rationalization Ours (no constraints)

0

0

64.0%

Introspective
explanation

Ours (with SAA)

Ours (with WAA)

- 100

10 100

62.4%

66.0%

Y

Y

Y

92.8%

90.8%

93.5%

Table 3: Human evaluation of the generated action descriptions and explanations for
randomly chosen 250 video intervals. We measure the success rate where at least 2
human judges rate the generated description or explanation with a score 1 (correct and
speciﬁc/detailed) or 2 (correct).

Human Evaluation. In our ﬁrst human evaluation experiment the human judges are
only shown the descriptions, while in the second experiment they only see the explana-
tions (e.g. “The car ... because < explanation >”), to exclude the effect of explana-
tions/descriptions on the ratings, respectively. We randomly select 250 video intervals
and compare the Rationalization, WAA (λa=10, λc=100) and SAA (λc=100) predic-
tions. The humans are asked to rate a description/explanation on the scale {1..4} (1:
correct and speciﬁc/detailed, 2: correct, 3: minor error, 4: major error). We collect rat-
ings from 3 human judges for each task. Finally, we compute the majority vote, i.e., at
least 2 out of 3 judges should rate the description/explanation with a score 1 or 2.

As shown in Table 3, our WAA model outperforms the other two, supporting the
results above. Interestingly, Rationalization does better than SAA on this subset, ac-
cording to humans. This is perhaps because the explanation in SAA relies on the exact
same visual evidence as the controller, which may include counterfactually important
regions (i.e., there could be a stop sign here), but may confuse the explanation module.

Qualitative Analysis of Textual Justiﬁer. As Figure 5 shows, our proposed textual
explanation model generates plausible descriptions and explanations, while our model
also provides attention visualization of their evidence. In the ﬁrst example of Figure 5,
controller sees neighboring vehicles and lane markings, while explanation model gen-
erates “the car is driving forward (description)” and “because trafﬁc is moving freely
(explanation)”. In Figure 5, we also provide other examples that cover common driving
situations, such as driving forward (1st example), slowing/stopping (2nd, 3rd, and 5th),
and turning (4th and 6th). We also observe that our explanations have signiﬁcant diver-
sity, e.g., they provide various reasons for stopping: red lights, stop signs, and trafﬁc.
We provide more diverse examples as supplemental materials.

6 Conclusion

We described an end-to-end explainable driving model for self-driving cars by incor-
porating a grounded introspective explanation model. We showed that (i) incorporation

14

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

Fig. 5: Example descriptions and explanations generated by our model compared to
human annotations. We provide (top row) input raw images and attention maps by (from
the 2nd row) vehicle controller, textual explanation generator, and rationalization model
(Note: (λc, λa) = (100,10) and the synthetic separator token is replaced by ‘+’).

of an attention mechanism and prior inputs improves vehicle control prediction accu-
racy compared to baselines, (ii) our grounded (introspective) model generates accurate
human understandable textual descriptions and explanations for driving behaviors, (iii)
attention alignment is shown to be effective at combining the vehicle controller and the
justiﬁcation model, and (iv) our BDD-X dataset allows us to train and automatically
evaluate our interpretable justiﬁcation model by comparing with human annotations.

Recent work [11] suggests that causal ﬁltering over attention heat maps can achieve
a useful reduction in explanation complexity by removing spurious blobs, which do not
signiﬁcantly affect the output. Causal ﬁltering idea would be worth exploring to obtain
causal attention heat maps, which can provide the causal ground of reasoning. Fur-
thermore, it would be beneﬁcial to incorporate stronger perception pipeline, e.g. object
detectors, to introduce more “grounded” visual representations and further improve the
quality and diversity of the generated explanations. Besides, incorporating driver’s eye
gaze into our explanation model for mimicking driver’s behavior, would be an interest-
ing potential future direction.

Acknowledgements. This work was supported by DARPA XAI program and Berkeley
DeepDrive.

Textual Explanations for Self-Driving Vehicles

15

References

1. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align

and translate. Conference on Learning Representations (2014)

2. Bojarski, M., Choromanska, A., Choromanski, K., Firner, B., Jackel, L., Muller, U., Zieba,
K.: Visualbackprop: visualizing cnns for autonomous driving. CoRR, vol. abs/1611.05418
(2016)

3. Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L.D.,
Monfort, M., Muller, U., Zhang, J., et al.: End to end learning for self-driving cars. CoRR
abs/1604.07316 (2016)

4. Buehler, M., Iagnemma, K., Singh, S.: The DARPA urban challenge: autonomous vehicles

in city trafﬁc, vol. 56. springer (2009)

5. Chen, C., Seff, A., Kornhauser, A., Xiao, J.: Deepdriving: Learning affordance for direct
perception in autonomous driving. In: Computer Vision (ICCV), 2015 IEEE International
Conference on. pp. 2722–2730. IEEE (2015)

6. Glorot, X., Bengio, Y.: Understanding the difﬁculty of training deep feedforward neural net-

works. In: Aistats. vol. 9, pp. 249–256 (2010)

7. Hendricks, L.A., Akata, Z., Rohrbach, M., Donahue, J., Schiele, B., Darrell, T.: Generating

visual explanations. In: European Conference on Computer Vision (ECCV) (2016)

8. Hendricks, L.A., Hu, R., Darrell, T., Akata, Z.: Grounding visual explanations. In: European

Conference on Computer Vision (ECCV) (2018)

9. Hochreiter, S., Schmidhuber, J.: Lstm can solve hard long time lag problems. In: Advances

in neural information processing systems. pp. 473–479 (1997)

10. Hyndman, R., Koehler, A.B., Ord, J.K., Snyder, R.D.: Forecasting with exponential smooth-

ing: the state space approach. Springer Science & Business Media (2008)

11. Kim, J., Canny, J.: Interpretable learning for self-driving cars by visualizing causal attention.
Proceedings of the IEEE international conference on computer vision pp. 2942–2950 (2017)
12. Kinga, D., Adam, J.B.: A method for stochastic optimization. In: International Conference

on Learning Representations (ICLR) (2015)

13. Lavie, A., Agarwal, A.: Meteor: An automatic metric for mt evaluation with improved corre-
lation with human judgments. In: Proceedings of the EMNLP 2011 Workshop on Statistical
Machine Translation. pp. 65–72 (2005)

14. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444 (2015)
15. Lee, H., Grosse, R., Ranganath, R., Ng, A.Y.: Convolutional deep belief networks for scalable
unsupervised learning of hierarchical representations. In: ICML. pp. 609–616. ACM (2009)
16. Levinson, J., Askeland, J., Becker, J., Dolson, J., Held, D., Kammel, S., Kolter, J.Z., Langer,
D., Pink, O., Pratt, V., et al.: Towards fully autonomous driving: Systems and algorithms. In:
Intelligent Vehicles Symposium (IV). pp. 163–168. IEEE (2011)

17. Lombrozo, T.: Explanation and abductive inference. The Oxford handbook of thinking and

reasoning (2012)

10(10) (2006)

18. Lombrozo, T.: The structure and function of explanations. Trends in Cognitive Science

19. Paden, B., ˇC´ap, M., Yong, S.Z., Yershov, D., Frazzoli, E.: A survey of motion planning and
control techniques for self-driving urban vehicles. IEEE Transactions on Intelligent Vehicles
1(1), 33–55 (2016)

20. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of
machine translation. In: Proceedings of the 40th annual meeting on association for computa-
tional linguistics. pp. 311–318. Association for Computational Linguistics (2002)

21. Park, D.H., Hendricks, L.A., Akata, Z., Schiele, B., Darrell, T., Rohrbach, M.: Multimodal
explanations: Justifying decisions and pointing to the evidence. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2018)

16

J. Kim, A. Rohrbach, T. Darrell, J. Canny, Z. Akata

22. Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a
simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Re-
search 15(1), 1929–1958 (2014)

23. Urmson, C., Anhalt, J., Bagnell, D., Baker, C., Bittner, R., Clark, M., Dolan, J., Duggins,
D., Galatali, T., Geyer, C., et al.: Autonomous driving in urban environments: Boss and the
urban challenge. Journal of Field Robotics 25(8), 425–466 (2008)

24. Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: Consensus-based image description
evaluation. In: Proceedings of the IEEE conference on computer vision and pattern recogni-
tion. pp. 4566–4575 (2015)

25. Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., Saenko, K.: Sequence
to sequence-video to text. In: Proceedings of the IEEE international conference on computer
vision. pp. 4534–4542 (2015)

26. Xu, H., Gao, Y., Yu, F., Darrell, T.: End-to-end learning of driving models from large-scale
video datasets. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. pp. 2174–2182 (2017)

27. Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y.:
Show, attend and tell: Neural image caption generation with visual attention. In: International
Conference on Machine Learning. pp. 2048–2057 (2015)

28. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In: Euro-

pean Conference on Computer Vision. pp. 818–833. Springer (2014)

Supplemental Material:
Textual Explanations for Self-Driving Vehicles

Jinkyu Kim1, Anna Rohrbach1,2, Trevor Darrell1, John Canny1, and Zeynep Akata2,3

1 EECS, University of California, Berkeley CA 94720, USA
{jinkyu.kim,anna.rohrbach,trevordarrell,canny}@berkeley.edu
2 MPI for Informatics, Saarland Informatics Campus, 66123 Saarbr¨ucken, Germany
3 AMLab, University of Amsterdam, 1098 XH Amsterdam, Netherlands
z.akata@uva.nl

This supplementary material provides more details on the presented BDD-X dataset
and approach as well as more qualitative results. The document as structured as follows.
Section S.1 provides details on our Amazon Mechanical Turk annotation interface

and data collection procedure.

tions in BDD-X.

Section S.2 includes additional details on the collected descriptions and explana-

Section S.3 provides implementation details.
Finally, Section S.4 shows more qualitative examples of the predicted visual atten-

tion maps as well as textual explanations.

18

Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata

S.1 Our Amazon Mechanical Turk annotation interface

Our annotation prompt is demonstrated on Figure S1. To ensure that the annotators
provide us the driving rationales as well as descriptions, we require that they separately
enter the action description and the action justiﬁcation e.g., :“The car is moving into
the left lane” and “because the school bus in front of it is stopping”. In our preliminary
annotation studies, we found that giving separate annotation boxes are helpful for the
annotator to understand the task and perform better.

Fig. S1: Our Amazon Mechanical Turk annotation interface. Shown is a “dummy” input
(The car is stopping + Because the light is red), not an actual annotation.

Supplemental Material: Textual Explanations for Self-Driving Vehicles

19

S.2 Berkeley DeepDrive eXplanation (BDD-X) Dataset Details

Table S1 includes word counts for the top-30 most frequent words (excluding stop-
words) used in the action descriptions and action explanations, respectively. Note, that
word counts are obtained but taking all word forms into account (slow, slows, slow-
ing, slowed, slowly, etc). Most common actions are related to changes in speed, driving
forward and turning. However many also include merging, pulling, changing lanes,
veering, and less frequent actions like reversing, parking or using wipers. Action ex-
planations cover a diverse list of concepts relevant to driving scenario, such as state of

Table S1: BDD-X dataset details. We provide counts for top-30 words, where words are
counted along with all their forms, such as slow, slows, slowing, slowed, slowly, etc.

BDD-X action descriptions

BDD-X action explanations

Count

Word

Count

Word

stop
slow
forward
drive
move
accelerate
right
left
turn
road
lane
street
speed
come
merge
pull
intersection
head
continue
slight
make
brake
travel
highway
maintain
steer
proceed
steady
complete
veer

6879
6122
4322
3994
3273
2882
2616
2574
1912
1907
1832
1704
1072
1033
923
723
717
629
625
554
539
517
513
450
390
371
359
329
323
214

trafﬁc
light
red
move
clear
ahead
road
stop
lane
turn
front
green
park
intersection
right
slow
cars
left
street
forward
sign
make
approach
speed
down
pedestrian
go
cross
get
pass

7486
6116
3979
3915
3660
3629
3528
3430
3407
3333
2715
1928
1523
1513
1464
1395
1390
1272
1225
984
984
718
702
681
658
649
614
588
536
490

20

Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata

trafﬁc/lanes, trafﬁc lights/signs, pedestrians crossing the street, passing other cars, etc.
Although less frequent, many explanations contain references to weather conditions
(rain, snow), different types of vehicles (buses, vans, trucks), road bumps and safety of
the performed action.

We provide some additional examples in Table S2, showing, in particular, that some
explanations require complex reasoning (1,2,3) and illustrate attention to detail (4,5,6).

Description

Explanation

1: The car is making its way carefully through the intersection because another car has cut it off and pedestrians are approaching the crosswalk.
2: The car stops behind the truck
3: The car slows to a stop next to a line of parked cars
4: The car slows down
5: The car pulls over to the left side of the street
6: The car is moving at a constant slow pace
Table S2: Example annotations where explanations require complex reasoning (1,2,3)
and demonstrate attention to detail (4,5,6).

because it’s waiting for a car in the opposite lane to pass by.
since there is no where available to park, but another car up ahead is also double parked.
because it’s entering a school crossing zone.
to avoid a large pothole on the right.
because it is a single lane road with snow and ice on the road.

Supplemental Material: Textual Explanations for Self-Driving Vehicles

21

S.3 Implementation Details

Preprocessing Our model is trained to predict two vehicle control commands, i.e., an
acceleration and a change of course. At each time t, a change of course, ct, is computed
by taking a difference between a current vehicles course rt and a smoothed value ¯rt by
using simple exponential smoothing method [10].

ct = rt − ¯rt = rt − (cid:0)αsrt + (1 − αs)¯rt+1

(cid:1)

(1)

where αS is a smoothing factor 0 ≤ αs ≤ 1. Note that they are same as the original
timeseries when s =1, while values of s closer to zero have a greater smoothing effect
and are less response to recent changes. In this paper, we use αs as 0.01.

Long Short-Term Memory (LSTM) network: We use a long short-term memory
(LSTM) network [9] in our vehicle controller and explanation generator. The LSTM
is deﬁned as follows:













it
ft
ot
gt













sigm
sigm
sigm
tanh

=

A

(cid:19)

(cid:18)ht−1
yt

where it, ft, ot, and ct ∈ RM are the M-dimensional input, forget, output, memory
state of the LSTM at time t, respectively. Internal states of the LSTM are computed
conditioned on the hidden state ht ∈ RM and an α-weighted context vector yt ∈ Rd.
We use an afﬁne transformation A : Rd+M → R4M. The logistic sigmoid activation
function and the hyperbolic tangent activation function are represented as sigm and
tanh, respectively. The hidden state ht and the cell state ct of the LSTM are deﬁned as:

ct = ft (cid:12) ct−1 + it (cid:12) gt
ht = ot (cid:12) tanh(ct)

where (cid:12) is element-wise multiplication.

To initialize memory state ct and hidden state ht of the LSTM, we use average of
the convolutional feature slices x0,i ∈ Rd for i ∈ {0, 1, . . . , l} and feed through two
additional hidden layers: finit,c and finit,h.

c0 = finit,c

x0,i

, h0 = finit,h

(cid:33)

(cid:32)

1
l

l
(cid:88)

i=1

(cid:32)

1
l

l
(cid:88)

i=1

(cid:33)

x0,i

(2)

(3)

(4)

22

Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata

S.4 Additional Examples

In this section, we provide more examples of explanations of the driving decisions by
exploiting both attention visualization and textual justiﬁcation.

Our model is also able to generate novel explanations not present in the training set.
We provide some examples as follows: (1) “because there are no obstructions in the
lane ahead”, (2) “because the car ahead is slowing to make a stop”, (3) “because the
car is turning onto a different street”, (4) “as it is now making a turn to enter another
street”, (5) “the car is stopped at an intersection at a red light”, and (6) “because there
are no other cars in the intersection”.

Supplemental Material: Textual Explanations for Self-Driving Vehicles

23

Fig. S2: Examples of descriptions and explanations. In this experiment, we use (λc,λa)
as (100,10). A synthetic separator token is replaced by ’+’.

24

Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata

Fig. S3: Additional examples of descriptions and explanations. In this experiment, we
use (λc,λa) as (100,10). A synthetic separator token is replaced by ’+’.

