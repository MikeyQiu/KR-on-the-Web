6
1
0
2
 
n
a
J
 
7
 
 
]

G
L
.
s
c
[
 
 
2
v
7
9
2
6
0
.
1
1
5
1
:
v
i
X
r
a

Under review as a conference paper at ICLR 2016

CONDITIONAL COMPUTATION IN NEURAL NETWORKS
FOR FASTER MODELS

Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau & Doina Precup
School of Computer Science
McGill University
Montreal, Canada
{ebengi,pbacon,jpineau,dprecup}@cs.mcgill.ca

ABSTRACT

Deep learning has become the state-of-art tool in many applications, but the eval-
uation and training of deep models can be time-consuming and computationally
expensive. The conditional computation approach has been proposed to tackle
this problem (Bengio et al., 2013; Davis & Arel, 2013). It operates by selectively
activating only parts of the network at a time. In this paper, we use reinforcement
learning as a tool to optimize conditional computation policies. More speciﬁ-
cally, we cast the problem of learning activation-dependent policies for dropping
out blocks of units as a reinforcement learning problem. We propose a learning
scheme motivated by computation speed, capturing the idea of wanting to have
parsimonious activations while maintaining prediction accuracy. We apply a pol-
icy gradient algorithm for learning policies that optimize this loss function and
propose a regularization mechanism that encourages diversiﬁcation of the dropout
policy. We present encouraging empirical results showing that this approach im-
proves the speed of computation without impacting the quality of the approxima-
tion.

Keywords Neural Networks, Conditional Computing, REINFORCE

1

INTRODUCTION

Large-scale neural networks, and in particular deep learning architectures, have seen a surge in
popularity in recent years, due to their impressive empirical performance in complex supervised
learning tasks, including state-of-the-art performance in image and speech recognition (He et al.,
2015). Yet the task of training such networks remains a challenging optimization problem. Several
related problems arise: very long training time (several weeks on modern computers, for some prob-
lems), potential for over-ﬁtting (whereby the learned function is too speciﬁc to the training data and
generalizes poorly to unseen data), and more technically, the vanishing gradient problem (Hochre-
iter, 1991; Bengio et al., 1994), whereby the gradient information gets increasingly diffuse as it
propagates from layer to layer.

Recent approaches (Bengio et al., 2013; Davis & Arel, 2013) have proposed the use of conditional
computation in order to address this problem. Conditional computation refers to activating only
some of the units in a network, in an input-dependent fashion. For example, if we think we’re
looking at a car, we only need to compute the activations of the vehicle detecting units, not of all
features that a network could possible compute. The immediate effect of activating fewer units is
that propagating information through the network will be faster, both at training as well as at test
time. However, one needs to be able to decide in an intelligent fashion which units to turn on and off,
depending on the input data. This is typically achieved with some form of gating structure, learned
in parallel with the original network.

A secondary effect of conditional computation is that during training, information will be propagated
along fewer links. Intuitively, this allows sharper gradients on the links that do get activated. More-
over, because only parts of the network are active, and fewer parameters are used in the computation,

1

Under review as a conference paper at ICLR 2016

the net effect can be viewed as a form of regularization of the main network, as the approximator
has to use only a small fraction of the possible parameters in order to produce an action.

In this paper, we explore the formulation of conditional computation using reinforcement learning.
We propose to learn input-dependent activation probabilities for every node (or blocks of nodes),
while trying to jointly minimize the prediction errors at the output and the number of participating
nodes at every layer, thus reducing the computational load. One can also think of our method as
being related to standard dropout, which has been used as a tool to both regularize and speed up the
computation. However, we emphasize that dropout is in fact a form of “unconditional” computation,
in which the computation paths are data-independent. Therefore, usual dropout is less likely to lead
to specialized computation paths within a network.

We present the problem formulation, and our solution to the proposed optimization problem, us-
ing policy search methods (Deisenroth et al., 2013). Preliminary results are included for standard
classiﬁcation benchmarks.

2 PROBLEM FORMULATION

Our model consists in a typical fully-connected neural network model, joined with stochastic per-
layer policies that activate or deactivate nodes of the neural network in an input-dependent manner,
both at train and test time. The exact algorithm is detailed in appendix A.

We cast the problem of learning the input-dependent activation probabilities at each layer in the
framework of Markov Decision Processes (MDP) (Puterman, 1994). We deﬁne a discrete time,
continuous state and discrete action MDP (cid:104)S, U, P (· | s, u) , C(cid:105). An action u ∈ {0, 1}k in this
model consists in the application of a mask over the units of a given layer. We deﬁne the state space
of the MDP over the vector-valued activations s ∈ Rk of all nodes at the previous layer. The cost C
is the loss of the neural network architecture (in our case the negative log-likelihood). This MDP is
single-step: an input is seen, an action is taken, a reward is observed and we are at the end state.

Similarly to the way dropout is described (Hinton et al., 2012), each node or block in a given layer
has an associated Bernoulli distribution which determines its probability of being activated. We train
a different policy for each layer l, and parameterize it (separately of the neural network) such that
it is input-dependent. For every layer l of k units, we deﬁne a policy as a k-dimensional Bernoulli
distribution:

π(l)(u | s) =

σui
i (1 − σi)(1−ui),

σi = [sigm(Z(l)s + d(l))]i,

(1)

k
(cid:89)

i=1

where the σi denotes the participation probability, to be computed from the activations s of the layer
below and the parameters θl = {Z(l), d(l)}. We denote the sigmoid function by sigm, the weight
matrix by Z, and the bias vector by d. The output of a typical hidden layer h(x) that uses this
policy is multiplied element-wise with the mask u sampled from the probabilities σ, and becomes
(h(x) ⊗ u). For clarity we did not superscript u, s and σi with l, but each layer has its own.

3 LEARNING SIGMOID-BERNOULLI POLICIES

We use REINFORCE (Williams, 1992) (detailed in appendix B) to learn the parameters Θπ =
{θ1, ..., θL} of the sigmoid-Bernoulli policies. Since the nature of the observation space changes
at each decision step, we learn L disjoint policies (one for each layer l of the deep network). As a
consequence, the summation in the policy gradient disappears and becomes:

∇θl L = E

C(x)∇θl log π(l)(u(l) | s(l))

(cid:110)

(cid:111)

(2)

since θl = {Z(l), d(l)} only appears in the l-th decision stage and the gradient is zero otherwise.

Estimating (2) from samples requires propagating through many instances at a time, which we
achieve through mini-batches of size mb . Under the mini-batch setting, s(l) becomes a matrix
and π(· | ·) a vector of dimension mb . Taking the gradient of the parameters with respect to the

2

Under review as a conference paper at ICLR 2016

log action probabilities can then be seen as forming a Jacobian. We can thus re-write the empirical
average in matrix form:

∇θl L ≈

C(xi)∇θl log π(l)(u(l)

i

| s(l)

i ) =

c(cid:62)∇θl log π(l)(U(l) | S(l))

(3)

1
mb

1
mb

mb(cid:88)

i=1

where C(xi) is the total cost for input xi and mb is the number of examples in the mini-batch. The
term c(cid:62) denotes the row vector containing the total costs for every example in the mini-batch.

3.1 FAST VECTOR-JACOBIAN MULTIPLICATION

While Eqn (3) suggests that the Jacobian might have to be formed explicitly, Pearlmutter (1994)
showed that computing a differential derivative sufﬁces to compute left or right vector-Jacobian
(or Hessian) multiplication. The same trick has also recently been revived with the class of so-
called “Hessian-free” (Martens, 2010) methods for artiﬁcial neural networks. Using the notation of
Pearlmutter (1994), we write Rθl {·} = c(cid:62)∇θl for the differential operator.

∇θl L ≈

Rθl

log π(U(l) | S(l))

(cid:110)

1
mb

(cid:111)

3.2 SPARSITY AND VARIANCE REGULARIZATIONS

In order to favour activation policies with sparse actions, we add two penalty terms Lb and Le that
depend on some target sparsity rate τ . The ﬁrst term pushes the policy distribution π to activate each
unit with probability τ in expectation over the data. The second term pushes the policy distribution
to have the desired sparsity of activations for each example. Thus, for a low τ , a valid conﬁguration
would be to learn a few high probability activations for some part of the data and low probability
activations for the rest of the data, which results in having activation probability τ in expectation.

Lb =

(cid:107)E{σj} − τ (cid:107)2

Le = E{(cid:107)(

σj) − τ (cid:107)2}

1
n

n
(cid:88)

j

n
(cid:88)

j

Since we are in a minibatch setting, these expectations can be approximated over the minibatch:

Lb ≈

n
(cid:88)

j

(cid:107)

1
mb

mb(cid:88)

i

(σij) − τ (cid:107)2

Le ≈

σij) − τ (cid:107)2

1
mb

mb(cid:88)

i

(cid:107)(

1
n

n
(cid:88)

j

We ﬁnally add a third term, Lv, in order to favour the aforementioned conﬁgurations, where units
only have a high probability of activation for certain examples, and low for the rest. We aim to max-
imize the variances of activations of each unit, across the data. This encourages units’ activations
to be varied, and while similar in spirit to the Lb term, this term explicitly discourages learning a
uniform distribution.

Lv = −

vari{σij} ≈ −

n
(cid:88)

j

(cid:32)

mb(cid:88)

n
(cid:88)

1
mb

j

i

σij −

(cid:33)(cid:33)2

(cid:32)

1
mb

mb(cid:88)

i

σij

(4)

(5)

(6)

(7)

3.3 ALGORITHM

We interleave the learning of the network parameters ΘN N and the learning of the policy parameters
Θπ. We ﬁrst update the network and policy parameters to minimize the following regularized loss
function via backpropagation (Rumelhart et al., 1988):

L = − log P (Y | X, ΘN N ) + λs(Lb + Le) + λv(Lv) + λL2(cid:107)ΘN N (cid:107)2 + λL2(cid:107)Θπ(cid:107)2
where λs can be understood as a trade-off parameter between prediction accuracy and parsimony of
computation (obtained through sparse node activation), and λv as a trade-off parameter between a
stochastic policy and a more input dependent saturated policy. We then minimize the cost function
C with a REINFORCE-style approach to update the policy parameters (Williams, 1992):

C = − log P (Y | X, ΘN N )
As previously mentioned, we use minibatch stochastic gradient descent as well as minibatch policy
gradient updates. A detailed algorithm is available in appendix A.

3

Under review as a conference paper at ICLR 2016

3.4 BLOCK ACTIVATION POLICY

To achieve computational gain, instead of activating single units in hidden layers, we activate con-
tiguous (equally-sized) groups of units together (independently for each example in the minibatch),
thus reducing the action space as well as the number of probabilities to compute and sample.
As such, there are two potential speedups. First, the policy is much smaller and faster to compute.
Second, it offers a computational advantage in the computation of the hidden layer themselves, since
we are now performing a matrix multiplication of the following form:

((H ⊗ MH )W ) ⊗ MO

where MH and MO are binary mask matrices. MO is obtained for each layer from the sampling
of the policy as described in eq. 1: each sampled action (0 or 1) is repeated so as to span the
corresponding block. MH is simply the mask of the previous layer. MH and MO resemble this
(here there are 3 blocks of size 2):






0
1

0

0
1

0

1
0
...
1

1
0

1

0
0

1






0
0

1

This allows us to quickly perform matrix multiplication by only considering the non-zero output
elements as well as the non-zero elements in H ⊗ MH .

4 EXPERIMENTS

4.1 MODEL IMPLEMENTATION

The proposed model was implemented within Theano (Bergstra et al., 2010), a standard library for
deep learning and neural networks. In addition to using optimizations offered by Theano, we also
implemented specialized matrix multiplication code for the operation exposed in section 3.4.
A straightforward and fairly naive CPU implementation of this operation yielded speedups of up to
5-10x, while an equally naive GPU implementation yielded speedups of up to 2-4x, both for sparsity
rates of under 20% and acceptable matrix and block sizes.1

We otherwise use fairly standard methods for our neural network. The weight matrices are initialized
using the heuristic of Glorot & Bengio (2010). We use a constant learning rate throughout minibatch
SGD. We also use early stopping (Bishop, 2006) to avoid overﬁtting. We only use fully-connected
layers with tanh activations (reLu activations offer similar performance).

4.2 MODEL EVALUATION

We ﬁrst evaluate the performance of our model on the MNIST digit dataset. We use a single hidden
layer of 16 blocks of 16 units (256 units total), with a target sparsity rate of τ = 6.25% = 1/16,
learning rates of 10−3 for the neural network and 5 × 10−5 for the policy, λv = λs = 200 and
λL2 = 0.005. Under these conditions, a test error of around 2.3% was achieved. A normal neural
network with the same number of hidden units achieves a test error of around 1.9%, while a normal
neural network with a similar amount of computation (multiply-adds) being made (32 hidden units)
achieves a test error of around 2.8%.

Looking at the activation of the policy (1c), we see that it tends towards what was hypothesized in
section 3.2, i.e. where examples activate most units with low probability and some units with high
probability. We can also observe that the policy is input-dependent in ﬁgures 1a and 1b, since we
see different activation patterns for inputs of class ’0’ and inputs of class ’1’.

Since the computation performed in our model is sparse, one could hope that it achieves this perfor-
mance with less computation time, yet we consistently observe that models that deal with MNIST
are too small to allow our specialized (3.4) sparse implementation to make a substantial difference.
We include this result to highlight conditions under which it is less desirable to use our model.

1Implementations used in this paper are available at http://github.com/bengioe/condnet/

4

Under review as a conference paper at ICLR 2016

(a)

(c)

(b)

(d)

Figure 1: MNIST, (a,b,c), probability distribution of the policy, each example’s probability (y axis)
of activating each unit (x axis) is plotted as a transparent red dot. Redder regions represent more
examples falling in the probability region. Plot (a) is for class ’0’, (b) for class ’1’, (c) for all classes.
(d), weight matrix of the policy.

model
condnet
condnet
condnet
bdNN
bdNN
NN
NN
NN

test error
0.511
0.514
0.497
0.629
0.590
0.560
0.546
0.497

τ
1/24
1/16
1/16
0.17
0.2
-
-
-

#blocks
24,24
16,32
10,10
10,10
10,10
64,64
128,128
480,480

block size
64
16
64
64
64
1
1
1

test time
6.8s(26.2s)
1.4s (8.2s)
2.0s(10.4s)
1.93s(10.3s)
2.8s(10.3s)
1.23s
2.31s
8.34s

speedup
3.8×
5.7×
5.3×
5.3×
3.5×
-
-
-

Figure 2: CIFAR-10, condnet: our approach, NN: Neural Network without the conditional activa-
tions, bdNN, block dropout Neural Network using a uniform policy. ’speedup’ is how many times
faster the forward pass is when using a specialized implementation (3.4). ’test time’ is the time
required to do a full pass over the test dataset using the implementation, on a CPU, running on a
single core; in parenthesis is the time without the optimization.

Next, we consider the performance of our model on the CIFAR-10 (Krizhevsky & Hinton, 2009)
image dataset. A brief hyperparameter search was made, and a few of the best models are shown in
ﬁgure 2. These results show that it is possible to achieve similar performance with our model (de-
noted condnet) as with a normal neural network (denoted NN), yet using sensibly reduced computa-
tion time. A few things are worth noting; we can set τ to be lower than 1 over the number of blocks,
since the model learns a policy that is actually not as sparse as τ , mostly because REINFORCE pulls
the policy towards higher probabilities on average. For example our best performing model has a
target of 1/16 but learns policies that average an 18% sparsity rate (we used λv = λs = 20, except
for the ﬁrst layer λv = 40, we used λL2 = 0.01, and the learning rates were 0.001 for the neural net,
10−5 and 5 × 10−4 for the ﬁrst and second policy layers respectively). The neural networks without
conditional activations are trained with L2 regularization as well as regular unit-wise dropout.
We also train networks with the same architecture as our models, using blocks, but with a uniform
policy (as in original dropout) instead of a learned conditional one. This model (denoted bdNN)
does not perform as well as our model, showing that the dropout noise by itself is not sufﬁcient, and
that learning a policy is required to fully take beneﬁt of this architecture.

5

Under review as a conference paper at ICLR 2016

Figure 3: SVHN, each point is an experiment. The x axis is the time required to do a full pass over
the valid dataset (log scale, lower is better). Note that we plot the full hyperparameter exploration
results, which is why condnet results are so varied.

model
condnet
condnet
condnet
NN
NN
NN

test error
0.183
0.139
0.073
0.116
0.100
0.091

τ
1/11
1/25,1/7
1/22
-
-
-

#blocks
13,8
27,7
25,22
288,928
800,736
1280,1056

block size
16
16
32
1
1
1

test time
1.5s(2.2s)
2.8s (4.3s)
10.2s(14.1s)
4.8s
10.7s
16.8s

speedup
1.4×
1.6×
1.4×
-
-
-

Figure 4: SVHN results (see ﬁg 2)

Finally we tested our model on the Street View House Numbers (SVHN) (Netzer et al., 2011)
dataset, which also yielded encouraging results (ﬁgure 3). As we restrain the capacity of the models
(by increasing sparsity or decreasing number of units), condnets retain acceptable performance with
low run times, while plain neural networks suffer highly (their performance dramatically decreases
with lower run times).
The best condnet model has a test error of 7.3%, and runs a validation epoch in 10s (14s without
speed optimization), while the best standard neural network model has a test error of 9.1%, and
runs in 16s. Note that the variance in the SVHN results (ﬁgure 3) is due to the mostly random
hyperparameter exploration, where block size, number of blocks, τ , λv, λs, as well of learning rates
are randomly picked. The normal neural network results were obtained by varying the number of
hidden units of a 2-hidden-layer model.

For all three datasets and all condnet models used, the required training time was higher, but still
reasonable. On average experiments took 1.5 to 3 times longer (wall time).

4.3 EFFECTS OF REGULARIZATION

The added regularization proposed in section 3.2 seems to play an important role in our ability to
train the conditional model. When using only the prediction score, we observed that the algorithm
tried to compensate by recruiting more units and saturating their participation probability, or even
failed by dismissing very early what were probably considered bad units.
In practice, the variance regularization term Lv only slightly affects the prediction accuracy and
learned policies of models, but we have observed that it signiﬁcantly speeds up the training process,
probably by encouraging policies to become less uniform earlier in the learning process. This can

6

Under review as a conference paper at ICLR 2016

(a)

(b)

Figure 5: CIFAR-10, (a) each pair of circle and triangle is an experiment made with a given lambda
(x axis), resulting in a model with a certain error and running time (y axes). As λs increases the
running time decreases, but so does performance. (b) The same model is being trained with different
values of λv. Redder means lower, greener means higher.

be seen in ﬁgure 5b, where we train a model with different values of λv. When λv is increased, the
ﬁrst few epochs have a much lower error rate.

It is possible to tune some hyperparameters to affect the point at which the trade-off between com-
putation speed and performance lies, thus one could push the error downwards at the expense of
also more computation time. This is suggested by ﬁgure 5a, which shows the effect of one such
hyperparameter (λs) on both running times and performance for the CIFAR dataset. Here it seems
that λ ∼ [300, 400] offers the best trade-off, yet other values could be selected, depending on the
speciﬁc requirements of an application.

5 RELATED WORK

Ba & Frey (2013) proposed a learning algorithm called standout for computing an input-dependent
dropout distribution at every node. As opposed to our layer-wise method, standout computes a one-
shot dropout mask over the entire network, conditioned on the input to the network. Additionally,
masks are unit-wise, while our approach uses masks that span blocks of units. Bengio et al. (2013)
introduced Stochastic Times Smooth neurons as gaters for conditional computation within a deep
neural network. STS neurons are highly non-linear and non-differentiable functions learned using
estimators of the gradient obtained through REINFORCE. They allow a sparse binary gater to be
computed as a function of the input, thus reducing computations in the then sparse activation of
hidden layers.

Stollenga et al. (2014) recently proposed to learn a sequential decision process over the ﬁlters of a
convolutional neural network (CNN). As in our work, a direct policy search method was chosen to
ﬁnd the parameters of a control policy. Their problem formulation differs from ours mainly in the
notion of decision “stage”. In their model, an input is ﬁrst fed through a network, the activations are
computed during forward propagation then they are served to the next decision stage. The goal of
the policy is to select relevant ﬁlters from the previous stage so as to improve the decision accuracy
on the current example. They also use a gradient-free evolutionary algorithm, in contrast to our
gradient-based method.

The Deep Sequential Neural Network (DSNN) model of Denoyer & Gallinari (2014) is possibly
closest to our approach. The control process is carried over the layers of the network and uses the
output of the previous layer to compute actions. The REINFORCE algorithm is used to train the pol-
icy with the reward/cost function being deﬁned as the loss at the output in the base network. DSNN
considers the general problem of choosing between between different type of mappings (weights) in

7

Under review as a conference paper at ICLR 2016

a composition of functions. However, they test their model on datasets in which different modes are
proeminent, making it easy for a policy to distinguish between them.

Another point of comparison for our work are attention models (Mnih et al., 2014; Gregor et al.,
2015; Xu et al., 2015). These models typically learn a policy, or a form of policy, that allows them
to selectively attend to parts of their input sequentially, in a visual 2D environnement. Both attention
and our approach aim to reduce computation times. While attention aims to perform dense compu-
tations on subsets of the inputs, our approach aims to be more general, since the policy focuses on
subsets of the whole computation (it is in a sense more distributed). It should also be possible to
combine these approaches, since one acts on the input space and the other acts on the representa-
tion space, altough the resulting policies would be much more complex, and not necessarily easily
trainable.

6 CONCLUSION

This paper presents a method for tackling the problem of conditional computation in deep networks
by using reinforcement learning. We propose a type of parameterized conditional computation pol-
icy that maps the activations of a layer to a Bernoulli mask. The reinforcement signal accounts for
the loss function of the network in its prediction task, while the policy network itself is regularized
to account for the desire to have sparse computations. The REINFORCE algorithm is used to train
policies to optimize this cost. Our experiments show that it is possible to train such models at the
same levels of accuracy as their standard counterparts. Additionally, it seems possible to execute
these similarly accurate models faster due to their sparsity. Furthermore, the model has a few simple
parameters that allow to control the trade-off between accuracy and running time.

The use of REINFORCE could be replaced by a more efﬁcient policy search algorithm, and also,
perhaps, one in which rewards (or costs) as described above are replaced by a more sequential
variant. The more direct use of computation time as a cost may prove beneﬁcial. In general, we
consider conditional computation to be an area in which reinforcement learning could be very useful,
and deserves further study.

All the running times reported in the Experiments section are for a CPU, running on a single core.
The motivation for this is to explore deployment of large neural networks on cheap, low-power,
single core CPUs such as phones, while retaining high model capacity and expressiveness. While
the results presented here show that our model for conditional computation can achieve speedups in
this context, it is worth also investigating adaptation of these sparse computation models in multi-
core/GPU architectures; this is the subject of ongoing work.

ACKNOWLEDGEMENTS

The authors gratefully acknowledge ﬁnancial support for this work by the Samsung Advanced In-
stitute of Technology (SAIT), the Natural Sciences and Engineering Research Council of Canada
(NSERC) and the Fonds de recherche du Qu´ebec - Nature et Technologies (FQRNT).

REFERENCES

Ba, Jimmy and Frey, Brendan. Adaptive dropout for training deep neural networks.

In
Burges, C.J.C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K.Q. (eds.), Ad-
vances in Neural Information Processing Systems 26, pp. 3084–3092. Curran Associates,
Inc., 2013. URL http://papers.nips.cc/paper/5032-adaptive-dropout-for-
training-deep-neural-networks.pdf.

Bengio, Y., Simard, P., and Frasconi, P. Learning long-term dependencies with gradient descent is

difﬁcult. IEEE Transactions on Neural Nets, pp. 157–166, 1994.

Bengio, Yoshua, L´eonard, Nicholas, and Courville, Aaron. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des-
jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU

8

Under review as a conference paper at ICLR 2016

and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con-
ference (SciPy), June 2010. Oral Presentation.

Bishop, Christopher M. Pattern Recognition and Machine Learning (Information Science and Statis-

tics). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006. ISBN 0387310738.

Davis, Andrew and Arel, Itamar. Low-rank approximations for conditional feedforward computation

in deep neural networks. arXiv preprint arXiv:1312.4461, 2013.

Deisenroth, Marc Peter, Neumann, Gerhard, and Peters, Jan. A survey on policy search for robotics.
Foundations and Trends in Robotics, 2(1-2):1–142, 2013. doi: 10.1561/2300000021. URL
http://dx.doi.org/10.1561/2300000021.

Denoyer, Ludovic and Gallinari, Patrick. Deep sequential neural network. CoRR, abs/1410.0510,

2014. URL http://arxiv.org/abs/1410.0510.

Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and
Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp. 249–256,
2010. URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.

Gregor, Karol, Danihelka, Ivo, Graves, Alex, and Wierstra, Daan. Draw: A recurrent neural network

for image generation. arXiv preprint arXiv:1502.04623, 2015.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers: Sur-
passing human-level performance on imagenet classiﬁcation. arXiv preprint arXiv:1502.01852,
2015.

Hinton, Geoffrey E., Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov,
Improving neural networks by preventing co-adaptation of feature detectors. CoRR,

Ruslan.
abs/1207.0580, 2012. URL http://arxiv.org/abs/1207.0580.

Hochreiter, S. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, T.U. M¨unich,

1991.

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images, 2009.

Martens, James. Deep learning via hessian-free optimization. In Proceedings of the 27th Interna-
tional Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 735–
742, 2010. URL http://www.icml2010.org/papers/458.pdf.

Mnih, Volodymyr, Heess, Nicolas, Graves, Alex, and kavukcuoglu, koray. Recurrent models of vi-
sual attention. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q.
(eds.), Advances in Neural Information Processing Systems 27, pp. 2204–2212. Curran Asso-
ciates, Inc., 2014. URL http://papers.nips.cc/paper/5542-recurrent-models-
of-visual-attention.pdf.

Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Read-
ing digits in natural images with unsupervised feature learning. In NIPS workshop on deep learn-
ing and unsupervised feature learning, volume 2011, pp. 5. Granada, Spain, 2011.

Pearlmutter, Barak A. Fast exact multiplication by the hessian. Neural Comput., 6(1):147–
doi: 10.1162/neco.1994.6.1.147. URL http:

160, January 1994.
//dx.doi.org/10.1162/neco.1994.6.1.147.

ISSN 0899-7667.

Puterman, Martin L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John

Wiley & Sons, Inc., New York, NY, USA, 1st edition, 1994. ISBN 0471619779.

Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning representations by

back-propagating errors. Cognitive modeling, 5, 1988.

Silver, David, Lever, Guy, Heess, Nicolas, Degris, Thomas, Wierstra, Daan, and Riedmiller, Martin.
Deterministic policy gradient algorithms. In Proceedings of the 31th International Conference
on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pp. 387–395, 2014. URL
http://jmlr.org/proceedings/papers/v32/silver14.html.

9

Under review as a conference paper at ICLR 2016

Stollenga, Marijn F, Masci, Jonathan, Gomez, Faustino, and Schmidhuber, J¨urgen. Deep
networks with internal selective attention through feedback connections.
In Ghahra-
mani, Z., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q. (eds.), Ad-
vances in Neural Information Processing Systems 27, pp. 3545–3553. Curran Associates,
Inc., 2014. URL http://papers.nips.cc/paper/5276-deep-networks-with-
internal-selective-attention-through-feedback-connections.pdf.

Williams, Ronald J.

forcement learning. Machine Learning, 8(3-4):229–256, 1992.
10.1007/BF00992696. URL http://dx.doi.org/10.1007/BF00992696.

Simple statistical gradient-following algorithms for connectionist rein-
doi:

ISSN 0885-6125.

Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Courville, Aaron, Salakhutdinov, Ruslan, Zemel, Richard, and
Bengio, Yoshua. Show, attend and tell: Neural image caption generation with visual attention.
arXiv preprint arXiv:1502.03044, 2015.

10

Under review as a conference paper at ICLR 2016

A ALGORITHM

The forward pass in our model is done as described in the algorithm below (1), both at train time
and test time.

input: x
1 h0 ← x
2 u0 ← 1 ;
3 for each hidden layer l ∈ 1, ..., L do
4

pl ← sigm(Z(l)hl−1 + d(l)) = πl(ul|sl = hl−1)
ul ∼ Ber(pl) ;
if blocksize > 1 then

extend ul by repeating each value blocksize times

// the input mask is ones

// sample Bernoulli from probablities pl

end
// this operation can be performed efﬁciently as described in section 3.4:
hl ← f (cid:0)W(l)(hl−1 ⊗ ul−1) + b(l)(cid:1) ⊗ ul

Algorithm 1: Single-input forward pass

5

6

7

8

9
10 end

This algorithm can easily be extended to the minibatch setting by replacing vector operations by
matrix operations. Note that in the case of classiﬁcation, the last layer is a softmax layer and is not
multiplied by a mask.

input: x

1 ˆy = forward(x) ;
2 c ← C(x) = − log P (ˆy|x)
3 L ← c + λs(Lb + Le) + λv(Lv) + λL2(cid:107)ΘN N (cid:107)2 + λL2(cid:107)Θπ(cid:107)2 ;

// given the output of the forward pass

// as in sections 3.2 and 3.3

// update the neural network weights:

4 ΘN N ← ΘN N − α∇ΘN N L
// update the policy weights:

5 for each hidden layer l ∈ 1, ..., L do
θl ← θl − απ c∇θl log pl
6
(cid:124)
(cid:125)
(cid:123)(cid:122)
REINFORCE

−α∇θl L ;

7 end

// where pl is computed as in algorithm 1

Algorithm 2: Single-input backward pass

Note that in line 4, some gradients are zeroes, for example the gradient of the L2 regularisation of
Θπ with respect to ΘN N is zero. Similarly in line 5, the gradient of c with respect to Θπ is zero,
which is why we have to use REINFORCE to approximate a gradient in the direction that minimizes
c.

This algorithm can be extended to the minibatch setting efﬁciently by replacing the gradient compu-
tations in line 7 with the use of the so called R-op, as described in section 3.1, and other computations
as is usually done in the minibatch setting with matrix operations.

B REINFORCE

REINFORCE (Williams, 1992), also known as the likelihood-ratio method, is a policy search algo-
rithm. It aims to use gradient methods to improve a given parameterized policy.

In reinforcement learning, a sequence of state-action-reward tuples is described as a trajectory τ .
The objective function of a parameterized policy πθ for the cumulative return of a trajectory τ is
described as:

J(θ) = Eπθ
τ

r(St, At|S0 = s0)

(cid:41)

(cid:40) T

(cid:88)

t=1

11

(8)

(9)

(10)

(11)

Under review as a conference paper at ICLR 2016

where s0 is the initial state of the trajectory. Let R(τ ) denote the return for trajectory τ . The gradient
of the objective with respect to the parameters of the policy is:

Note that the interchange in (8) is only valid under some assumptions (see Silver et al. (2014)).

∇θJ(θ) = ∇θEπθ

τ {R(τ )}
(cid:90)

= ∇θ

P{τ |θ}R(τ )dτ

τ

(cid:90)

τ

=

∇θ [P{τ |θ}R(τ )] dτ

∇θJ(θ) =

∇θ [P{τ |θ}R(τ )] dτ

(cid:90)

τ
(cid:90)

τ
(cid:90)

=

=

[R(τ )∇θP{τ |θ} + ∇θR(τ )P{τ |θ}] dτ

(cid:20) R(τ )
P{τ |θ}

∇θP{τ |θ} + ∇θR(τ )

P{τ |θ}dτ

(cid:21)

τ
τ {R(τ )∇θ log P{τ |θ} + ∇θR(τ )}
= Eπθ

The product rule of derivatives is used in (9), and the derivative of a log in (10). Since R(τ ) does
not depend on θ directly, the gradient ∇θR(τ ) is zero. We end up with this gradient:

∇θJ(θ) = Eπθ

τ {R(τ )∇θ log P{τ |θ}}

Without knowing the transition probabilities, we cannot compute the probability of our trajectories
P{τ |θ}, or their gradient. Fortunately we are in a MDP setting, and we can make use of the Markov
property of the trajectories to compute the gradient:

∇θ log P{τ |θ} = ∇θ log

p(s0)

(cid:34)

(cid:35)
P{st+1|st, at}πθ(at|st)

= ∇θ log p(s0) +

∇θ log P{st+1|st, at} + ∇θ log πθ(at|st)

(12)

T
(cid:89)

t=1

T
(cid:88)

t=1

=

∇θ log πθ(at|st)

T
(cid:88)

t=1

In (12), p(s0) does not depend on θ, so the gradient is zero. Similarly, P{st+1|st, at} does not
depend on θ (not directly at least), so the gradient is also zero. We end up with the gradient of the
log policy, which is easy to compute.

In our particular case, the trajectories only have a single step and the reward of the trajectory is the
neural network cost C(x), thus the summation dissapears and the gradient found in (2) is found by
taking the log of the probability of our Bernoulli sample:
∇θl C(x) = E {C(x)∇θl log πθl (u|s)}

(cid:40)

(cid:40)

= E

C(x)∇θl log

σui
i (1 − σi)(1−ui)

= E

C(x)∇θl

log [σiui + (1 − σi)(1 − ui)]

(cid:41)

(cid:41)

k
(cid:89)

i=1

k
(cid:88)

i=1

12

6
1
0
2
 
n
a
J
 
7
 
 
]

G
L
.
s
c
[
 
 
2
v
7
9
2
6
0
.
1
1
5
1
:
v
i
X
r
a

Under review as a conference paper at ICLR 2016

CONDITIONAL COMPUTATION IN NEURAL NETWORKS
FOR FASTER MODELS

Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau & Doina Precup
School of Computer Science
McGill University
Montreal, Canada
{ebengi,pbacon,jpineau,dprecup}@cs.mcgill.ca

ABSTRACT

Deep learning has become the state-of-art tool in many applications, but the eval-
uation and training of deep models can be time-consuming and computationally
expensive. The conditional computation approach has been proposed to tackle
this problem (Bengio et al., 2013; Davis & Arel, 2013). It operates by selectively
activating only parts of the network at a time. In this paper, we use reinforcement
learning as a tool to optimize conditional computation policies. More speciﬁ-
cally, we cast the problem of learning activation-dependent policies for dropping
out blocks of units as a reinforcement learning problem. We propose a learning
scheme motivated by computation speed, capturing the idea of wanting to have
parsimonious activations while maintaining prediction accuracy. We apply a pol-
icy gradient algorithm for learning policies that optimize this loss function and
propose a regularization mechanism that encourages diversiﬁcation of the dropout
policy. We present encouraging empirical results showing that this approach im-
proves the speed of computation without impacting the quality of the approxima-
tion.

Keywords Neural Networks, Conditional Computing, REINFORCE

1

INTRODUCTION

Large-scale neural networks, and in particular deep learning architectures, have seen a surge in
popularity in recent years, due to their impressive empirical performance in complex supervised
learning tasks, including state-of-the-art performance in image and speech recognition (He et al.,
2015). Yet the task of training such networks remains a challenging optimization problem. Several
related problems arise: very long training time (several weeks on modern computers, for some prob-
lems), potential for over-ﬁtting (whereby the learned function is too speciﬁc to the training data and
generalizes poorly to unseen data), and more technically, the vanishing gradient problem (Hochre-
iter, 1991; Bengio et al., 1994), whereby the gradient information gets increasingly diffuse as it
propagates from layer to layer.

Recent approaches (Bengio et al., 2013; Davis & Arel, 2013) have proposed the use of conditional
computation in order to address this problem. Conditional computation refers to activating only
some of the units in a network, in an input-dependent fashion. For example, if we think we’re
looking at a car, we only need to compute the activations of the vehicle detecting units, not of all
features that a network could possible compute. The immediate effect of activating fewer units is
that propagating information through the network will be faster, both at training as well as at test
time. However, one needs to be able to decide in an intelligent fashion which units to turn on and off,
depending on the input data. This is typically achieved with some form of gating structure, learned
in parallel with the original network.

A secondary effect of conditional computation is that during training, information will be propagated
along fewer links. Intuitively, this allows sharper gradients on the links that do get activated. More-
over, because only parts of the network are active, and fewer parameters are used in the computation,

1

Under review as a conference paper at ICLR 2016

the net effect can be viewed as a form of regularization of the main network, as the approximator
has to use only a small fraction of the possible parameters in order to produce an action.

In this paper, we explore the formulation of conditional computation using reinforcement learning.
We propose to learn input-dependent activation probabilities for every node (or blocks of nodes),
while trying to jointly minimize the prediction errors at the output and the number of participating
nodes at every layer, thus reducing the computational load. One can also think of our method as
being related to standard dropout, which has been used as a tool to both regularize and speed up the
computation. However, we emphasize that dropout is in fact a form of “unconditional” computation,
in which the computation paths are data-independent. Therefore, usual dropout is less likely to lead
to specialized computation paths within a network.

We present the problem formulation, and our solution to the proposed optimization problem, us-
ing policy search methods (Deisenroth et al., 2013). Preliminary results are included for standard
classiﬁcation benchmarks.

2 PROBLEM FORMULATION

Our model consists in a typical fully-connected neural network model, joined with stochastic per-
layer policies that activate or deactivate nodes of the neural network in an input-dependent manner,
both at train and test time. The exact algorithm is detailed in appendix A.

We cast the problem of learning the input-dependent activation probabilities at each layer in the
framework of Markov Decision Processes (MDP) (Puterman, 1994). We deﬁne a discrete time,
continuous state and discrete action MDP (cid:104)S, U, P (· | s, u) , C(cid:105). An action u ∈ {0, 1}k in this
model consists in the application of a mask over the units of a given layer. We deﬁne the state space
of the MDP over the vector-valued activations s ∈ Rk of all nodes at the previous layer. The cost C
is the loss of the neural network architecture (in our case the negative log-likelihood). This MDP is
single-step: an input is seen, an action is taken, a reward is observed and we are at the end state.

Similarly to the way dropout is described (Hinton et al., 2012), each node or block in a given layer
has an associated Bernoulli distribution which determines its probability of being activated. We train
a different policy for each layer l, and parameterize it (separately of the neural network) such that
it is input-dependent. For every layer l of k units, we deﬁne a policy as a k-dimensional Bernoulli
distribution:

π(l)(u | s) =

σui
i (1 − σi)(1−ui),

σi = [sigm(Z(l)s + d(l))]i,

(1)

k
(cid:89)

i=1

where the σi denotes the participation probability, to be computed from the activations s of the layer
below and the parameters θl = {Z(l), d(l)}. We denote the sigmoid function by sigm, the weight
matrix by Z, and the bias vector by d. The output of a typical hidden layer h(x) that uses this
policy is multiplied element-wise with the mask u sampled from the probabilities σ, and becomes
(h(x) ⊗ u). For clarity we did not superscript u, s and σi with l, but each layer has its own.

3 LEARNING SIGMOID-BERNOULLI POLICIES

We use REINFORCE (Williams, 1992) (detailed in appendix B) to learn the parameters Θπ =
{θ1, ..., θL} of the sigmoid-Bernoulli policies. Since the nature of the observation space changes
at each decision step, we learn L disjoint policies (one for each layer l of the deep network). As a
consequence, the summation in the policy gradient disappears and becomes:

∇θl L = E

C(x)∇θl log π(l)(u(l) | s(l))

(cid:110)

(cid:111)

(2)

since θl = {Z(l), d(l)} only appears in the l-th decision stage and the gradient is zero otherwise.

Estimating (2) from samples requires propagating through many instances at a time, which we
achieve through mini-batches of size mb . Under the mini-batch setting, s(l) becomes a matrix
and π(· | ·) a vector of dimension mb . Taking the gradient of the parameters with respect to the

2

Under review as a conference paper at ICLR 2016

log action probabilities can then be seen as forming a Jacobian. We can thus re-write the empirical
average in matrix form:

∇θl L ≈

C(xi)∇θl log π(l)(u(l)

i

| s(l)

i ) =

c(cid:62)∇θl log π(l)(U(l) | S(l))

(3)

1
mb

1
mb

mb(cid:88)

i=1

where C(xi) is the total cost for input xi and mb is the number of examples in the mini-batch. The
term c(cid:62) denotes the row vector containing the total costs for every example in the mini-batch.

3.1 FAST VECTOR-JACOBIAN MULTIPLICATION

While Eqn (3) suggests that the Jacobian might have to be formed explicitly, Pearlmutter (1994)
showed that computing a differential derivative sufﬁces to compute left or right vector-Jacobian
(or Hessian) multiplication. The same trick has also recently been revived with the class of so-
called “Hessian-free” (Martens, 2010) methods for artiﬁcial neural networks. Using the notation of
Pearlmutter (1994), we write Rθl {·} = c(cid:62)∇θl for the differential operator.

∇θl L ≈

Rθl

log π(U(l) | S(l))

(cid:110)

1
mb

(cid:111)

3.2 SPARSITY AND VARIANCE REGULARIZATIONS

In order to favour activation policies with sparse actions, we add two penalty terms Lb and Le that
depend on some target sparsity rate τ . The ﬁrst term pushes the policy distribution π to activate each
unit with probability τ in expectation over the data. The second term pushes the policy distribution
to have the desired sparsity of activations for each example. Thus, for a low τ , a valid conﬁguration
would be to learn a few high probability activations for some part of the data and low probability
activations for the rest of the data, which results in having activation probability τ in expectation.

Lb =

(cid:107)E{σj} − τ (cid:107)2

Le = E{(cid:107)(

σj) − τ (cid:107)2}

1
n

n
(cid:88)

j

n
(cid:88)

j

Since we are in a minibatch setting, these expectations can be approximated over the minibatch:

Lb ≈

n
(cid:88)

j

(cid:107)

1
mb

mb(cid:88)

i

(σij) − τ (cid:107)2

Le ≈

σij) − τ (cid:107)2

1
mb

mb(cid:88)

i

(cid:107)(

1
n

n
(cid:88)

j

We ﬁnally add a third term, Lv, in order to favour the aforementioned conﬁgurations, where units
only have a high probability of activation for certain examples, and low for the rest. We aim to max-
imize the variances of activations of each unit, across the data. This encourages units’ activations
to be varied, and while similar in spirit to the Lb term, this term explicitly discourages learning a
uniform distribution.

Lv = −

vari{σij} ≈ −

n
(cid:88)

j

(cid:32)

mb(cid:88)

n
(cid:88)

1
mb

j

i

σij −

(cid:33)(cid:33)2

(cid:32)

1
mb

mb(cid:88)

i

σij

(4)

(5)

(6)

(7)

3.3 ALGORITHM

We interleave the learning of the network parameters ΘN N and the learning of the policy parameters
Θπ. We ﬁrst update the network and policy parameters to minimize the following regularized loss
function via backpropagation (Rumelhart et al., 1988):

L = − log P (Y | X, ΘN N ) + λs(Lb + Le) + λv(Lv) + λL2(cid:107)ΘN N (cid:107)2 + λL2(cid:107)Θπ(cid:107)2
where λs can be understood as a trade-off parameter between prediction accuracy and parsimony of
computation (obtained through sparse node activation), and λv as a trade-off parameter between a
stochastic policy and a more input dependent saturated policy. We then minimize the cost function
C with a REINFORCE-style approach to update the policy parameters (Williams, 1992):

C = − log P (Y | X, ΘN N )
As previously mentioned, we use minibatch stochastic gradient descent as well as minibatch policy
gradient updates. A detailed algorithm is available in appendix A.

3

Under review as a conference paper at ICLR 2016

3.4 BLOCK ACTIVATION POLICY

To achieve computational gain, instead of activating single units in hidden layers, we activate con-
tiguous (equally-sized) groups of units together (independently for each example in the minibatch),
thus reducing the action space as well as the number of probabilities to compute and sample.
As such, there are two potential speedups. First, the policy is much smaller and faster to compute.
Second, it offers a computational advantage in the computation of the hidden layer themselves, since
we are now performing a matrix multiplication of the following form:

((H ⊗ MH )W ) ⊗ MO

where MH and MO are binary mask matrices. MO is obtained for each layer from the sampling
of the policy as described in eq. 1: each sampled action (0 or 1) is repeated so as to span the
corresponding block. MH is simply the mask of the previous layer. MH and MO resemble this
(here there are 3 blocks of size 2):






0
1

0

0
1

0

1
0
...
1

1
0

1

0
0

1






0
0

1

This allows us to quickly perform matrix multiplication by only considering the non-zero output
elements as well as the non-zero elements in H ⊗ MH .

4 EXPERIMENTS

4.1 MODEL IMPLEMENTATION

The proposed model was implemented within Theano (Bergstra et al., 2010), a standard library for
deep learning and neural networks. In addition to using optimizations offered by Theano, we also
implemented specialized matrix multiplication code for the operation exposed in section 3.4.
A straightforward and fairly naive CPU implementation of this operation yielded speedups of up to
5-10x, while an equally naive GPU implementation yielded speedups of up to 2-4x, both for sparsity
rates of under 20% and acceptable matrix and block sizes.1

We otherwise use fairly standard methods for our neural network. The weight matrices are initialized
using the heuristic of Glorot & Bengio (2010). We use a constant learning rate throughout minibatch
SGD. We also use early stopping (Bishop, 2006) to avoid overﬁtting. We only use fully-connected
layers with tanh activations (reLu activations offer similar performance).

4.2 MODEL EVALUATION

We ﬁrst evaluate the performance of our model on the MNIST digit dataset. We use a single hidden
layer of 16 blocks of 16 units (256 units total), with a target sparsity rate of τ = 6.25% = 1/16,
learning rates of 10−3 for the neural network and 5 × 10−5 for the policy, λv = λs = 200 and
λL2 = 0.005. Under these conditions, a test error of around 2.3% was achieved. A normal neural
network with the same number of hidden units achieves a test error of around 1.9%, while a normal
neural network with a similar amount of computation (multiply-adds) being made (32 hidden units)
achieves a test error of around 2.8%.

Looking at the activation of the policy (1c), we see that it tends towards what was hypothesized in
section 3.2, i.e. where examples activate most units with low probability and some units with high
probability. We can also observe that the policy is input-dependent in ﬁgures 1a and 1b, since we
see different activation patterns for inputs of class ’0’ and inputs of class ’1’.

Since the computation performed in our model is sparse, one could hope that it achieves this perfor-
mance with less computation time, yet we consistently observe that models that deal with MNIST
are too small to allow our specialized (3.4) sparse implementation to make a substantial difference.
We include this result to highlight conditions under which it is less desirable to use our model.

1Implementations used in this paper are available at http://github.com/bengioe/condnet/

4

Under review as a conference paper at ICLR 2016

(a)

(c)

(b)

(d)

Figure 1: MNIST, (a,b,c), probability distribution of the policy, each example’s probability (y axis)
of activating each unit (x axis) is plotted as a transparent red dot. Redder regions represent more
examples falling in the probability region. Plot (a) is for class ’0’, (b) for class ’1’, (c) for all classes.
(d), weight matrix of the policy.

model
condnet
condnet
condnet
bdNN
bdNN
NN
NN
NN

test error
0.511
0.514
0.497
0.629
0.590
0.560
0.546
0.497

τ
1/24
1/16
1/16
0.17
0.2
-
-
-

#blocks
24,24
16,32
10,10
10,10
10,10
64,64
128,128
480,480

block size
64
16
64
64
64
1
1
1

test time
6.8s(26.2s)
1.4s (8.2s)
2.0s(10.4s)
1.93s(10.3s)
2.8s(10.3s)
1.23s
2.31s
8.34s

speedup
3.8×
5.7×
5.3×
5.3×
3.5×
-
-
-

Figure 2: CIFAR-10, condnet: our approach, NN: Neural Network without the conditional activa-
tions, bdNN, block dropout Neural Network using a uniform policy. ’speedup’ is how many times
faster the forward pass is when using a specialized implementation (3.4). ’test time’ is the time
required to do a full pass over the test dataset using the implementation, on a CPU, running on a
single core; in parenthesis is the time without the optimization.

Next, we consider the performance of our model on the CIFAR-10 (Krizhevsky & Hinton, 2009)
image dataset. A brief hyperparameter search was made, and a few of the best models are shown in
ﬁgure 2. These results show that it is possible to achieve similar performance with our model (de-
noted condnet) as with a normal neural network (denoted NN), yet using sensibly reduced computa-
tion time. A few things are worth noting; we can set τ to be lower than 1 over the number of blocks,
since the model learns a policy that is actually not as sparse as τ , mostly because REINFORCE pulls
the policy towards higher probabilities on average. For example our best performing model has a
target of 1/16 but learns policies that average an 18% sparsity rate (we used λv = λs = 20, except
for the ﬁrst layer λv = 40, we used λL2 = 0.01, and the learning rates were 0.001 for the neural net,
10−5 and 5 × 10−4 for the ﬁrst and second policy layers respectively). The neural networks without
conditional activations are trained with L2 regularization as well as regular unit-wise dropout.
We also train networks with the same architecture as our models, using blocks, but with a uniform
policy (as in original dropout) instead of a learned conditional one. This model (denoted bdNN)
does not perform as well as our model, showing that the dropout noise by itself is not sufﬁcient, and
that learning a policy is required to fully take beneﬁt of this architecture.

5

Under review as a conference paper at ICLR 2016

Figure 3: SVHN, each point is an experiment. The x axis is the time required to do a full pass over
the valid dataset (log scale, lower is better). Note that we plot the full hyperparameter exploration
results, which is why condnet results are so varied.

model
condnet
condnet
condnet
NN
NN
NN

test error
0.183
0.139
0.073
0.116
0.100
0.091

τ
1/11
1/25,1/7
1/22
-
-
-

#blocks
13,8
27,7
25,22
288,928
800,736
1280,1056

block size
16
16
32
1
1
1

test time
1.5s(2.2s)
2.8s (4.3s)
10.2s(14.1s)
4.8s
10.7s
16.8s

speedup
1.4×
1.6×
1.4×
-
-
-

Figure 4: SVHN results (see ﬁg 2)

Finally we tested our model on the Street View House Numbers (SVHN) (Netzer et al., 2011)
dataset, which also yielded encouraging results (ﬁgure 3). As we restrain the capacity of the models
(by increasing sparsity or decreasing number of units), condnets retain acceptable performance with
low run times, while plain neural networks suffer highly (their performance dramatically decreases
with lower run times).
The best condnet model has a test error of 7.3%, and runs a validation epoch in 10s (14s without
speed optimization), while the best standard neural network model has a test error of 9.1%, and
runs in 16s. Note that the variance in the SVHN results (ﬁgure 3) is due to the mostly random
hyperparameter exploration, where block size, number of blocks, τ , λv, λs, as well of learning rates
are randomly picked. The normal neural network results were obtained by varying the number of
hidden units of a 2-hidden-layer model.

For all three datasets and all condnet models used, the required training time was higher, but still
reasonable. On average experiments took 1.5 to 3 times longer (wall time).

4.3 EFFECTS OF REGULARIZATION

The added regularization proposed in section 3.2 seems to play an important role in our ability to
train the conditional model. When using only the prediction score, we observed that the algorithm
tried to compensate by recruiting more units and saturating their participation probability, or even
failed by dismissing very early what were probably considered bad units.
In practice, the variance regularization term Lv only slightly affects the prediction accuracy and
learned policies of models, but we have observed that it signiﬁcantly speeds up the training process,
probably by encouraging policies to become less uniform earlier in the learning process. This can

6

Under review as a conference paper at ICLR 2016

(a)

(b)

Figure 5: CIFAR-10, (a) each pair of circle and triangle is an experiment made with a given lambda
(x axis), resulting in a model with a certain error and running time (y axes). As λs increases the
running time decreases, but so does performance. (b) The same model is being trained with different
values of λv. Redder means lower, greener means higher.

be seen in ﬁgure 5b, where we train a model with different values of λv. When λv is increased, the
ﬁrst few epochs have a much lower error rate.

It is possible to tune some hyperparameters to affect the point at which the trade-off between com-
putation speed and performance lies, thus one could push the error downwards at the expense of
also more computation time. This is suggested by ﬁgure 5a, which shows the effect of one such
hyperparameter (λs) on both running times and performance for the CIFAR dataset. Here it seems
that λ ∼ [300, 400] offers the best trade-off, yet other values could be selected, depending on the
speciﬁc requirements of an application.

5 RELATED WORK

Ba & Frey (2013) proposed a learning algorithm called standout for computing an input-dependent
dropout distribution at every node. As opposed to our layer-wise method, standout computes a one-
shot dropout mask over the entire network, conditioned on the input to the network. Additionally,
masks are unit-wise, while our approach uses masks that span blocks of units. Bengio et al. (2013)
introduced Stochastic Times Smooth neurons as gaters for conditional computation within a deep
neural network. STS neurons are highly non-linear and non-differentiable functions learned using
estimators of the gradient obtained through REINFORCE. They allow a sparse binary gater to be
computed as a function of the input, thus reducing computations in the then sparse activation of
hidden layers.

Stollenga et al. (2014) recently proposed to learn a sequential decision process over the ﬁlters of a
convolutional neural network (CNN). As in our work, a direct policy search method was chosen to
ﬁnd the parameters of a control policy. Their problem formulation differs from ours mainly in the
notion of decision “stage”. In their model, an input is ﬁrst fed through a network, the activations are
computed during forward propagation then they are served to the next decision stage. The goal of
the policy is to select relevant ﬁlters from the previous stage so as to improve the decision accuracy
on the current example. They also use a gradient-free evolutionary algorithm, in contrast to our
gradient-based method.

The Deep Sequential Neural Network (DSNN) model of Denoyer & Gallinari (2014) is possibly
closest to our approach. The control process is carried over the layers of the network and uses the
output of the previous layer to compute actions. The REINFORCE algorithm is used to train the pol-
icy with the reward/cost function being deﬁned as the loss at the output in the base network. DSNN
considers the general problem of choosing between between different type of mappings (weights) in

7

Under review as a conference paper at ICLR 2016

a composition of functions. However, they test their model on datasets in which different modes are
proeminent, making it easy for a policy to distinguish between them.

Another point of comparison for our work are attention models (Mnih et al., 2014; Gregor et al.,
2015; Xu et al., 2015). These models typically learn a policy, or a form of policy, that allows them
to selectively attend to parts of their input sequentially, in a visual 2D environnement. Both attention
and our approach aim to reduce computation times. While attention aims to perform dense compu-
tations on subsets of the inputs, our approach aims to be more general, since the policy focuses on
subsets of the whole computation (it is in a sense more distributed). It should also be possible to
combine these approaches, since one acts on the input space and the other acts on the representa-
tion space, altough the resulting policies would be much more complex, and not necessarily easily
trainable.

6 CONCLUSION

This paper presents a method for tackling the problem of conditional computation in deep networks
by using reinforcement learning. We propose a type of parameterized conditional computation pol-
icy that maps the activations of a layer to a Bernoulli mask. The reinforcement signal accounts for
the loss function of the network in its prediction task, while the policy network itself is regularized
to account for the desire to have sparse computations. The REINFORCE algorithm is used to train
policies to optimize this cost. Our experiments show that it is possible to train such models at the
same levels of accuracy as their standard counterparts. Additionally, it seems possible to execute
these similarly accurate models faster due to their sparsity. Furthermore, the model has a few simple
parameters that allow to control the trade-off between accuracy and running time.

The use of REINFORCE could be replaced by a more efﬁcient policy search algorithm, and also,
perhaps, one in which rewards (or costs) as described above are replaced by a more sequential
variant. The more direct use of computation time as a cost may prove beneﬁcial. In general, we
consider conditional computation to be an area in which reinforcement learning could be very useful,
and deserves further study.

All the running times reported in the Experiments section are for a CPU, running on a single core.
The motivation for this is to explore deployment of large neural networks on cheap, low-power,
single core CPUs such as phones, while retaining high model capacity and expressiveness. While
the results presented here show that our model for conditional computation can achieve speedups in
this context, it is worth also investigating adaptation of these sparse computation models in multi-
core/GPU architectures; this is the subject of ongoing work.

ACKNOWLEDGEMENTS

The authors gratefully acknowledge ﬁnancial support for this work by the Samsung Advanced In-
stitute of Technology (SAIT), the Natural Sciences and Engineering Research Council of Canada
(NSERC) and the Fonds de recherche du Qu´ebec - Nature et Technologies (FQRNT).

REFERENCES

Ba, Jimmy and Frey, Brendan. Adaptive dropout for training deep neural networks.

In
Burges, C.J.C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K.Q. (eds.), Ad-
vances in Neural Information Processing Systems 26, pp. 3084–3092. Curran Associates,
Inc., 2013. URL http://papers.nips.cc/paper/5032-adaptive-dropout-for-
training-deep-neural-networks.pdf.

Bengio, Y., Simard, P., and Frasconi, P. Learning long-term dependencies with gradient descent is

difﬁcult. IEEE Transactions on Neural Nets, pp. 157–166, 1994.

Bengio, Yoshua, L´eonard, Nicholas, and Courville, Aaron. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lamblin, Pascal, Pascanu, Razvan, Des-
jardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU

8

Under review as a conference paper at ICLR 2016

and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Con-
ference (SciPy), June 2010. Oral Presentation.

Bishop, Christopher M. Pattern Recognition and Machine Learning (Information Science and Statis-

tics). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006. ISBN 0387310738.

Davis, Andrew and Arel, Itamar. Low-rank approximations for conditional feedforward computation

in deep neural networks. arXiv preprint arXiv:1312.4461, 2013.

Deisenroth, Marc Peter, Neumann, Gerhard, and Peters, Jan. A survey on policy search for robotics.
Foundations and Trends in Robotics, 2(1-2):1–142, 2013. doi: 10.1561/2300000021. URL
http://dx.doi.org/10.1561/2300000021.

Denoyer, Ludovic and Gallinari, Patrick. Deep sequential neural network. CoRR, abs/1410.0510,

2014. URL http://arxiv.org/abs/1410.0510.

Glorot, Xavier and Bengio, Yoshua. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and
Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pp. 249–256,
2010. URL http://www.jmlr.org/proceedings/papers/v9/glorot10a.html.

Gregor, Karol, Danihelka, Ivo, Graves, Alex, and Wierstra, Daan. Draw: A recurrent neural network

for image generation. arXiv preprint arXiv:1502.04623, 2015.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Delving deep into rectiﬁers: Sur-
passing human-level performance on imagenet classiﬁcation. arXiv preprint arXiv:1502.01852,
2015.

Hinton, Geoffrey E., Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov,
Improving neural networks by preventing co-adaptation of feature detectors. CoRR,

Ruslan.
abs/1207.0580, 2012. URL http://arxiv.org/abs/1207.0580.

Hochreiter, S. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, T.U. M¨unich,

1991.

Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images, 2009.

Martens, James. Deep learning via hessian-free optimization. In Proceedings of the 27th Interna-
tional Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 735–
742, 2010. URL http://www.icml2010.org/papers/458.pdf.

Mnih, Volodymyr, Heess, Nicolas, Graves, Alex, and kavukcuoglu, koray. Recurrent models of vi-
sual attention. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q.
(eds.), Advances in Neural Information Processing Systems 27, pp. 2204–2212. Curran Asso-
ciates, Inc., 2014. URL http://papers.nips.cc/paper/5542-recurrent-models-
of-visual-attention.pdf.

Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Read-
ing digits in natural images with unsupervised feature learning. In NIPS workshop on deep learn-
ing and unsupervised feature learning, volume 2011, pp. 5. Granada, Spain, 2011.

Pearlmutter, Barak A. Fast exact multiplication by the hessian. Neural Comput., 6(1):147–
doi: 10.1162/neco.1994.6.1.147. URL http:

160, January 1994.
//dx.doi.org/10.1162/neco.1994.6.1.147.

ISSN 0899-7667.

Puterman, Martin L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John

Wiley & Sons, Inc., New York, NY, USA, 1st edition, 1994. ISBN 0471619779.

Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning representations by

back-propagating errors. Cognitive modeling, 5, 1988.

Silver, David, Lever, Guy, Heess, Nicolas, Degris, Thomas, Wierstra, Daan, and Riedmiller, Martin.
Deterministic policy gradient algorithms. In Proceedings of the 31th International Conference
on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pp. 387–395, 2014. URL
http://jmlr.org/proceedings/papers/v32/silver14.html.

9

Under review as a conference paper at ICLR 2016

Stollenga, Marijn F, Masci, Jonathan, Gomez, Faustino, and Schmidhuber, J¨urgen. Deep
networks with internal selective attention through feedback connections.
In Ghahra-
mani, Z., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q. (eds.), Ad-
vances in Neural Information Processing Systems 27, pp. 3545–3553. Curran Associates,
Inc., 2014. URL http://papers.nips.cc/paper/5276-deep-networks-with-
internal-selective-attention-through-feedback-connections.pdf.

Williams, Ronald J.

forcement learning. Machine Learning, 8(3-4):229–256, 1992.
10.1007/BF00992696. URL http://dx.doi.org/10.1007/BF00992696.

Simple statistical gradient-following algorithms for connectionist rein-
doi:

ISSN 0885-6125.

Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Courville, Aaron, Salakhutdinov, Ruslan, Zemel, Richard, and
Bengio, Yoshua. Show, attend and tell: Neural image caption generation with visual attention.
arXiv preprint arXiv:1502.03044, 2015.

10

Under review as a conference paper at ICLR 2016

A ALGORITHM

The forward pass in our model is done as described in the algorithm below (1), both at train time
and test time.

input: x
1 h0 ← x
2 u0 ← 1 ;
3 for each hidden layer l ∈ 1, ..., L do
4

pl ← sigm(Z(l)hl−1 + d(l)) = πl(ul|sl = hl−1)
ul ∼ Ber(pl) ;
if blocksize > 1 then

extend ul by repeating each value blocksize times

// the input mask is ones

// sample Bernoulli from probablities pl

end
// this operation can be performed efﬁciently as described in section 3.4:
hl ← f (cid:0)W(l)(hl−1 ⊗ ul−1) + b(l)(cid:1) ⊗ ul

Algorithm 1: Single-input forward pass

5

6

7

8

9
10 end

This algorithm can easily be extended to the minibatch setting by replacing vector operations by
matrix operations. Note that in the case of classiﬁcation, the last layer is a softmax layer and is not
multiplied by a mask.

input: x

1 ˆy = forward(x) ;
2 c ← C(x) = − log P (ˆy|x)
3 L ← c + λs(Lb + Le) + λv(Lv) + λL2(cid:107)ΘN N (cid:107)2 + λL2(cid:107)Θπ(cid:107)2 ;

// given the output of the forward pass

// as in sections 3.2 and 3.3

// update the neural network weights:

4 ΘN N ← ΘN N − α∇ΘN N L
// update the policy weights:

5 for each hidden layer l ∈ 1, ..., L do
θl ← θl − απ c∇θl log pl
6
(cid:124)
(cid:125)
(cid:123)(cid:122)
REINFORCE

−α∇θl L ;

7 end

// where pl is computed as in algorithm 1

Algorithm 2: Single-input backward pass

Note that in line 4, some gradients are zeroes, for example the gradient of the L2 regularisation of
Θπ with respect to ΘN N is zero. Similarly in line 5, the gradient of c with respect to Θπ is zero,
which is why we have to use REINFORCE to approximate a gradient in the direction that minimizes
c.

This algorithm can be extended to the minibatch setting efﬁciently by replacing the gradient compu-
tations in line 7 with the use of the so called R-op, as described in section 3.1, and other computations
as is usually done in the minibatch setting with matrix operations.

B REINFORCE

REINFORCE (Williams, 1992), also known as the likelihood-ratio method, is a policy search algo-
rithm. It aims to use gradient methods to improve a given parameterized policy.

In reinforcement learning, a sequence of state-action-reward tuples is described as a trajectory τ .
The objective function of a parameterized policy πθ for the cumulative return of a trajectory τ is
described as:

J(θ) = Eπθ
τ

r(St, At|S0 = s0)

(cid:41)

(cid:40) T

(cid:88)

t=1

11

(8)

(9)

(10)

(11)

Under review as a conference paper at ICLR 2016

where s0 is the initial state of the trajectory. Let R(τ ) denote the return for trajectory τ . The gradient
of the objective with respect to the parameters of the policy is:

Note that the interchange in (8) is only valid under some assumptions (see Silver et al. (2014)).

∇θJ(θ) = ∇θEπθ

τ {R(τ )}
(cid:90)

= ∇θ

P{τ |θ}R(τ )dτ

τ

(cid:90)

τ

=

∇θ [P{τ |θ}R(τ )] dτ

∇θJ(θ) =

∇θ [P{τ |θ}R(τ )] dτ

(cid:90)

τ
(cid:90)

τ
(cid:90)

=

=

[R(τ )∇θP{τ |θ} + ∇θR(τ )P{τ |θ}] dτ

(cid:20) R(τ )
P{τ |θ}

∇θP{τ |θ} + ∇θR(τ )

P{τ |θ}dτ

(cid:21)

τ
τ {R(τ )∇θ log P{τ |θ} + ∇θR(τ )}
= Eπθ

The product rule of derivatives is used in (9), and the derivative of a log in (10). Since R(τ ) does
not depend on θ directly, the gradient ∇θR(τ ) is zero. We end up with this gradient:

∇θJ(θ) = Eπθ

τ {R(τ )∇θ log P{τ |θ}}

Without knowing the transition probabilities, we cannot compute the probability of our trajectories
P{τ |θ}, or their gradient. Fortunately we are in a MDP setting, and we can make use of the Markov
property of the trajectories to compute the gradient:

∇θ log P{τ |θ} = ∇θ log

p(s0)

(cid:34)

(cid:35)
P{st+1|st, at}πθ(at|st)

= ∇θ log p(s0) +

∇θ log P{st+1|st, at} + ∇θ log πθ(at|st)

(12)

T
(cid:89)

t=1

T
(cid:88)

t=1

=

∇θ log πθ(at|st)

T
(cid:88)

t=1

In (12), p(s0) does not depend on θ, so the gradient is zero. Similarly, P{st+1|st, at} does not
depend on θ (not directly at least), so the gradient is also zero. We end up with the gradient of the
log policy, which is easy to compute.

In our particular case, the trajectories only have a single step and the reward of the trajectory is the
neural network cost C(x), thus the summation dissapears and the gradient found in (2) is found by
taking the log of the probability of our Bernoulli sample:
∇θl C(x) = E {C(x)∇θl log πθl (u|s)}

(cid:40)

(cid:40)

= E

C(x)∇θl log

σui
i (1 − σi)(1−ui)

= E

C(x)∇θl

log [σiui + (1 − σi)(1 − ui)]

(cid:41)

(cid:41)

k
(cid:89)

i=1

k
(cid:88)

i=1

12

