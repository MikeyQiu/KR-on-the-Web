Aligning Linguistic Words and Visual Semantic Units
for Image Captioning

Longteng Guo1,4, Jing Liu1∗, Jinhui Tang2, Jiangwei Li3, Wei Luo3, Hanqing Lu1
1National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences
2School of Computer Science and Engineering, Nanjing University of Science and Technology
3Multimedia Department, Huawei Devices 4University of Chinese Academy of Sciences
{longteng.guo,jliu,luhq}@nlpr.ia.ac.cn jinhuitang@njust.edu.cn {lijiangwei1,luo.luowei}@huawei.com

9
1
0
2
 
g
u
A
 
6
 
 
]

V
C
.
s
c
[
 
 
1
v
7
2
1
2
0
.
8
0
9
1
:
v
i
X
r
a

ABSTRACT
Image captioning attempts to generate a sentence composed of
several linguistic words, which are used to describe objects, at-
tributes, and interactions in an image, denoted as visual semantic
units in this paper. Based on this view, we propose to explicitly
model the object interactions in semantics and geometry based
on Graph Convolutional Networks (GCNs), and fully exploit the
alignment between linguistic words and visual semantic units for
image captioning. Particularly, we construct a semantic graph and a
geometry graph, where each node corresponds to a visual semantic
unit, i.e., an object, an attribute, or a semantic (geometrical) interac-
tion between two objects. Accordingly, the semantic (geometrical)
context-aware embeddings for each unit are obtained through the
corresponding GCN learning processers. At each time step, a con-
text gated attention module takes as inputs the embeddings of the
visual semantic units and hierarchically align the current word
with these units by first deciding which type of visual semantic
unit (object, attribute, or interaction) the current word is about,
and then finding the most correlated visual semantic units under
this type. Extensive experiments are conducted on the challenging
MS-COCO image captioning dataset, and superior results are re-
ported when comparing to state-of-the-art approaches. The code is
publicly available at https://github.com/ltguo19/VSUA-Captioning.

CCS CONCEPTS
• Computing methodologies → Natural language generation;
Computer vision; Image representations.

KEYWORDS
image captioning; graph convolutional networks; visual relation-
ship; visual-language

ACM Reference Format:
Longteng Guo1, 4, Jing Liu1∗, Jinhui Tang2, Jiangwei Li3, Wei Luo3, Hanqing
Lu1. 2019. Aligning Linguistic Words and Visual Semantic Units for Image
Captioning. In Proceedings of the 27th ACM International Conference on
Multimedia (MM ’19), October 21–25, 2019, Nice, France. ACM, New York,
NY, USA, 9 pages. https://doi.org/10.1145/3343031.3350943

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MM ’19, October 21–25, 2019, Nice, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6889-6/19/10. . . $15.00
https://doi.org/10.1145/3343031.3350943

Figure 1: Typically, image captioning models consider the
visual content of images as uniform grids (top left) or collec-
tions of object regions (top right). Differently, our approach
represents images as structured graphs where nodes are
VSUs: object, attribute, and relationship units (bottom). We
make use of the alignment nature between caption words
and VSUs.

1 INTRODUCTION
Computer vision and natural language processing are becoming
increasingly intertwined. At the intersection of the two subjects,
automatically generating lingual descriptions of images, namely
image captioning [5, 12, 25], has emerged as a prominent inter-
disciplinary research problem. Modern image captioning models
typically employ an encoder-decoder framework, where the en-
coder encodes an image into visual representations and then the
decoder decodes them into a sequence of words.

How to represent visual content and how to reason over them
are fundamental problems in image captioning. Starting from the
static, single-vector representations, the visual representations have
evolved into using dynamic, multi-vector representations, which
are often fed into an attention module for information aggregation.
In the early times, the image is treated as uniform grid representa-
tions, then more recently, state-of-the-art methods regard visual
content as collections of individual object regions (the top row
in Figure 1). However, the isolated objects only represent the cat-
egories and properties of individual instances, which are often
related to nouns or noun phrases in a caption, but fail to model
object-object relations, e.g. the interactions or relative positions.

* Corresponding Author

1

While the relationships between objects are the natural basis for
describing an image.

In fact, the image is a structured combination of objects (“man",
“helmet"), their attributes (“helmet is red"), and more importantly,
relationships (“man hold bat") involving these objects. We call these
visual components visual semantic units (VSUs) in this paper,
which include three categories: object units, attribute units, and
relationship units. At the same time, a sentence is also composed of
syntactic units describing objects (e.g. nouns phrase), their proper-
ties (e.g. adjectives) and relations (e.g. verb, prepositions). Because
captions are abstractions of images, it is intuitive that each word
in the caption can roughly be aligned with the VSUs of the image.
Exploiting such vision-language correlation could benefit image
understanding and captioning.

In this paper, we propose to represent visual content with VSU-
based structured graphs, and take advantage of the strong correla-
tions between linguistic syntactic units and VSUs for image caption-
ing. First, we detect a collection of VSUs from the image through
object, attribute, and relationship detectors, respectively. Then we
construct structured graphs as explicit and unified representation
that connects the detected objects, attributes, and relationships (e.g.
bottom-right in Figure 1), where each node corresponds to a VSU
and the edges are the connections between two VSUs. In particular,
we construct a semantic graph and a geometry graph, where the
former models the semantic relationships (e.g. “man holding bat")
and the latter models the geometry relationships (e.g. “the man the
and bat are adjacent and overlapped"). After that, Graph Convolu-
tional Networks (GCNs) are then explored to learn context-aware
embeddings for each VSU in the graph.

We design a context gated attention module (CGA) that attends
to the three types of VSUs in a hierarchical manner when generating
each caption word. The key insight behind CGA is that each word
in the caption could be aligned with a VSU, and if the word is about
objects, e.g. a noun, then the corresponding VSU should also be
an object unit, meaning that more attention should be paid on the
object units. Specifically, CGA first performs three independent
attention mechanism inside the object, attribute, and relationship
units, respectively. Then a gated fusion process is performed to
adaptively decide how much attention should be paid to each of the
three VSU categories by referring to the current linguistic context.
Knowledge learned from the semantic graph and the geometry
graph are naturally fused by extending CGA’s input components
to include the VSUs from both graphs.

The main contributions of this paper are three-fold.

2 RELATED WORK
Image Captioning. Filling the information gap between the vi-
sual content of the images and their corresponding descriptions is a
long-standing problem in image captioning. Based on the encoder-
decoder pipeline [24, 30, 33], much progress has been made on
image captioning. For example, [27] introduces the visual attention
that adaptively attends to the salient areas in the image, [18] pro-
poses an adaptive attention model that decides whether to attend
to the image or to the visual sentinel, [29] corporates learned lan-
guage bias as a language prior for more human-like captions, [19]
and [8] focus on the discriminability and style properties of image
captions respectively, and [22] adopts reinforcement learning (RL)
that directly optimize evaluation metric.

Recently, some works have been proposed to encode more dis-
criminative visual information into captioning models. For instances,
Up-Down [2] extracts region-level image features for training, [32]
incorporates image-level attributes into the encoder-decoder frame-
work by training a Multiple Instance Learning [5] based attribute
detectors. However, all these works focus on representing visual
content with either objects/grids or global attributes, but fail to
model object-object relationships. Differently, our method simulta-
neously models objects, the instance-level attributes, and relation-
ships with structured graph of VSUs.

Scene Graphs Generation and GCNs. Recently, inspired by rep-
resentations studied by the graphics community, [11] introduced
the problem of generating scene graphs from images, which re-
quires detecting objects, attributes, and relationships of objects.
Many approaches have been proposed for the detection of both
objects and their relationships [26, 28, 34]. Recently, some works
have been proposed that leverage scene graph for improving scene
understanding in various tasks, e.g. visual question answering [23],
referring expression understanding [20], image retrieval [11], and
visual reasoning [4]. In these works, GCNs [3, 6, 14] are often
adopted to learn node embeddings. A GCN is a multilayer neural
network that operates directly on a graph, in which information
flows along edges of the graph.

More similar to our work, GCN-LSTM [31] refines the region-
level features by leveraging object relationships and GCN. However,
GCN-LSTM treats relationships as edges in the graph, which are
implicitly encoded in the model parameters. While instead, our
method considers relationships as additional nodes in the graph
and thus can explicitly model relationships by learning instance-
specific representations for them.

• We introduce visual semantic units as comprehensive repre-
sentation of the visual content, and exploit structured graphs,
i.e. semantic graph and geometry graph, and GCNs to uni-
formly represent them.

• We explore the vision-language correlation and design a con-
text gated attention module to hierarchically align linguistic
words and visual semantic units.

• Extensive experiments on MS COCO validates the superi-
ority of our method. Particularly, in terms of the popular
CIDEr-D metric, we achieve an absolute 8.5 points improve-
ment over the strong baseline, i.e. Up-Down [2], on Karpathy
test split.

3 APPROACH
3.1 Problem Formulation
Image captioning models typically follow the encoder-decoder
framework. Given an image I , the image encoder E is used to
obtain the visual representation V = {v1, . . . , vk } , vi ∈ RD , where
each vi represents some features about the image content. Based
on V , the caption decoder D generates a sentence y by:

V = E(I ), y = D(V ).

(1)

The objective of image captioning is to minimize a cross entropy
loss.

2

Figure 2: Overview of our method. Given an image, we represent it as structured graphs of visual semantic units (objects,
attributes, and relationships in the image) and generate a caption based on them.

How to define and represent V is a fundamental problem in
this framework. In this work, we consider the visual content as
structured combinations of the following three kinds of VSUs:

• Object units (O): the individual object instances in the image.
• Attribute units (A): the properties following each object.
• Relationship units (R): the interactions between object pairs.
That is, we define V = Vo ∪Va ∪Vr , where Vo, Va , and Vr denote the
collections of visual representations for O, A, and R, respectively.
The overall framework of our method is shown in Figure 2. First,
we detect O, A, and R from the image with object, attribute, and
relationship detectors respectively, based on which, a semantic
graph and a geometry graph are constructed by regarding each
VSU as the nodes and the connections between two VSUs as the
edges. Then, GCNs are applied to learn context-aware embeddings
for each of the nodes/VSUs. Afterward, a context gated attention
fusion module is introduced to hierarchically align each word with
the embedded VSUs, and the resulting context vector is fed into a
language decoder for predicting words.

3.2 Graph Representations of Visual Semantic

Units

Visual Semantic Units Detection. We first detect the three types
of VSUs by an object detector, an attribute classifier, and a rela-
tionship detector respectively, following [29]. Specifically, Faster
R-CNN [21] is adopted as the object detector. Then, we train an at-
tribute classifier to predict the instance attributes for each detected
object, which is a simple multi-layer perceptron (MLP) network
followed by a softmax function. MOTIFNET [34] is adopted as
the semantic relationship detector to detect pairwise relationships
between object instances using the publicly available code1. Fi-
nally, we obtain a set of objects, attributes, and relationships, i.e.
as
the VSUs (O, A, and R). We denote oi as the i-th object, ai,k
the k-th attribute of oi , and ri j as the relationship between oi and
oj . <oi , ri j , oj > forms a triplet, which means <subject, predicate,
object>.

Graph Construction. It is intuitive to introduce graph as a struc-
tured representation of the VSUs, which contains nodes and edges

1https://github.com/rowanz/neural-motifs

3

connecting them. We can naturally regard both objects and at-
is
tributes as nodes in the graph, where each attribute node ai,k
connected with the object node oi with a directed edge (oi → ai,k
),
meaning “object oi owns attribute ai,k
". However, whether to repre-
sent relationships as edges or nodes in the graph remains uncertain.
A common and straightforward solution is to represent relation-
ships as edges connecting pairwise object nodes in the graph [23, 31]
(the left side of Figure 3.). However, such models only learn vector
representation for each node (objects) and implicitly encode edge
(relationships) information in the form of GCN parameters, while
the relationship representations are not directly modeled.

Ideally, relationships should have instance-specific representa-
tions, in the same way as objects, and they should also inform
decisions made in the decoder. Thus, we propose to explicitly mod-
eling relationship representations by turning the relationships as
additional nodes in the graph. Concretely, for each relationship ri j ,
we add a node in the graph and draw two directed edges: oi → ri j
and ri j → oj . The subgraph of all oi and ri j is now a bipartite. An
instance is shown in the right side of Figure 3.

Formally, given three sets of object nodes (units) O = {oi },
attribute nodes (units) A = {ai,k }, and relationship nodes (units)
R = {ri j }, we define the graph G = (N, E) as comprehensive
representation of the visual content for the image, where N =
O ∪ A ∪ R is the nodes and E is a set of directed/undirected edges.

Semantic Graph and Geometry Graph. We consider two types
of visual relationships between objects in the image: semantic rela-
tionship and geometry relationship, which result in two kinds of
graphs, i.e. semantic graph GS and geometry graph GG . The
semantic relationship RS unfolds the inherent action/interaction
between objects, e.g. “woman riding horse". Semantic relationships
are detected using MOTIFNET. Geometry relationship RG between
objects is an important factor in visual understanding that connects
isolated regions together, e.g. “woman on horse". Here we consider
the undirected relative spatial relationships <oi ′, ri ′j′, oj′ >, where
oi ′ is the one with larger size between oi and oj , while oj′ is the
smaller one. Instead of using a fully connected graph, we assign
geometry relationships between two objects if their Intersection
over Union (IoU) and relative distance are within given thresholds.

by the classification confidences) of oi into a single attribute unit ai
for each object. Denote the features corresponding to oi , ai,k
, and
ri j nodes as f oi
, respectively, and use superscripts
f G and f S to denote the features of GS and GG . The feature of oi ,
ai , ri j are given by:

, and f ri j

, f ai

]),

; f д
; f s
oi
oi
; . . . ; f s

]),

ai, Na

f oi
f ai
f S
ri j
f G
ri j

= ϕo ([f v
oi
= ϕa ([f s
= ϕr (f s
ri j
= ϕr (f д
ri j

ai, 1
),

),

(4)

(5)

(6)

(7)

where [∗; ∗] means concatenation operation, and ϕo, ϕa , and ϕr are
feature projection layers which are all implemented as FC-ReLU-
Dropout.

Node Embedding with GCNs. After obtaining the features for
the three kinds of nodes/VSUs, we then adopt GCNs: дo, дa , and дr ,
to learn semantic (geometrical) context-aware embeddings for the
oi , ai , and ri j nodes in G, respectively.
The object unit embedding uoi

is calculated as:

uoi

= дo (f oi

) + f v
oi
serves as a kind of “residual connection", which

(8)

,

where adding f v
oi
we empirically found helpful for model performance.

by:

Given the feature f ai

of an attribute unit ai , we integrate it with
its object context oi for calculating the attribute unit embedding
uai

= дa

(cid:16)
uoi , f ai
For each relationship ri j and its associated relationship triplet
<oi , ri j , oj >, we aggregate the information from its neighboring
nodes (oi and oj ) to calculate the relationship unit embedding uri j
by:

(cid:17) + f ai

uai

(9)

.

uri j

= дr

(cid:16)
uoi , f ri j

, uoj

(cid:17) + f ri j

.

(10)

In our implementation, дo, дa , and дr use the same structure
with independent parameters: a concatenation operation followed
by a FC-ReLU-Dropout layer. Note that for GS and GG , their uoi
and uai
are independently calculated
respectively. We will introduce in Sec.
and denoted as uS
ri j
3.4 about how to fuse GS and GG by leveraging all uoi , uai , uS
,
ri j
and uG
ri j

are the same, while their uri j
and uG
ri j

.

3.4 Aligning Textual Words with Visual

(3)

Semantic Units

We next discuss how to effectively integrate the learned embeddings
of various types of VSUs into sequence generation network for
image captioning.

Context Gated Attention for Word-Unit Alignment. We have
two observations about the correlation between linguistic words
and VSUs: 1) both a word in the caption and a VSU can be assigned
into one of the three categories: objects, attributes, and relation-
ships, 2) a word often could be aligned with one of the VSUs in the
image, which convey the same information in different modalities.

Figure 3: Comparison between regarding relationships as
edges and as nodes in the graph. The attribute nodes are
omitted for clarity.

3.3 Embedding Visual Semantic Units with

GCN

Node Features. We integrate three kinds of content cues for oi , ai,k
and ri j , i.e. visual appearance cue, semantic embedding cue, and
geometry cue.

,

.

Visual Appearance Cue. Visual appearance contains delicate de-
tails of the object and is a key factor for image understanding. We
use the region-level RoI feature from Faster R-CNN as the visual
appearance cue for each object, which is a 2048-dimensional vector,
denoted as f v
oi

Semantic Embedding Cue. The content of objects, attributes and
relationships can largely be described with their categories. Thus,
we introduce their category labels as the semantic information,
which are obtained from the outputs of object, attribute and re-
lationship detectors. Specifically, we use three independent and
trainable word embedding layers to maps the object categories
into feature embedding vectors (denoted as f s
) for
oi
oi , ai,k

, and ri j , respectively.

, and f s
ri j

Geometry Cue. Geometry information is complementary to vi-
sual appearance information, as it reflects the spatial patterns of
individual objects or the relation between two objects. Denote the
box of a localized object oi as (xi , yi , wi , hi ), where (xi , yi ) are the
coordinates of the center of the box, and (wi , hi ) are its width and
height, and denote the width and height of the image as (w, h). We
encode the relative geometric cue of oi with a 5-dimensional vector:
wi
w
We encode the geometric cue of each relationship ri j with a 8-
dimensional vector:

= (cid:2) xi
w

wihi
wh

f д
oi

hi
h

yi
h

, f s

ai, k

(2)

(cid:3) .

,

,

,

,

f д
ri j

=(cid:104) xj − xi
√
wihi
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

,

,

yj − yi
√
wihi
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)

,

hj
wj
,
hi
wi
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:123)(cid:122)
(cid:124)
r2

,

wjhj
wihi
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)

,

oi ∩ oj
oi ∪ oj
(cid:32)(cid:32)
(cid:32)(cid:32)
(cid:123)(cid:122)
(cid:125)
(cid:124)
r3

(cid:123)(cid:122)
r1

(cid:124)

(cid:113)

(xj − xi )2 + (yj − yi )2
√
w2 + h2
(cid:123)(cid:122)
r4

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:125)

(cid:124)

, atan2(
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:124)

(cid:105)

,

)

yj − yi
xj − xi
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:123)(cid:122)
r5

(cid:125)

where r1 represents the normalized translation between the two
boxes, r2 is the ratio of box sizes, r3 is the IoU between boxes, r4 is
the relative distance normalized by diagonal length of the image,
r5 is the relative angle θ ∈ (−π , π ] between the two boxes.

We fuse the visual appearance cues, semantic embedding cues,
and geometry cues to obtain the features for each node/VSU. We
aggregate the top-Na predicted attributes [ai,1, ..., ai, Na ] (sorted

4

Starting from the two observations, we design a context gated at-
tention module (CGA) to hierarchically align each caption word
with the VSUs by soft-attention mechanism.

Specifically, at each step, CGA first performs three independent
soft-attentions for VSUs in the three categories, i.e. object, attribute,
and relationship, respectively. Mathematically, given the attention
query h
from the decoder at the t-th step, we calculate:

1
t

attO
t
att A
t
att R
t

1
= ATT O (h
t , {uoi }),
1
= ATT A(h
t , {uai }),
1
= ATT R (h
t , {uri j }),

where ATT O , ATT A, and ATT R are soft-attention functions with
are the result-
independent parameters, while attO
t
ing context vectors for object, attribute, and relationship units,
respectively. We implement ATT O , ATT A, and ATT R with the
same structure as proposed in [27]. The attention function attt =
ATT (h

1
t , {uk }) is calculated by:

, and att R
t

, att A
t

tanh (cid:16)

= wT
ak,t
a
α t = softmax (at ) ,

W a [uk

(cid:17)

; h

1
t ]

,

attt =

αk,tuk ,

K
(cid:213)

k =1

(11)

(12)

(13)

(14)

(15)

(16)

where αk,t
embedding vectors uk
are learnable parameters.

is a normalized attention weight for each of the unit
, attt is the aggregated result, and Wa and wa

, att A
t

, and att R
t

Given attO
t

, a gated fusion process is then per-
formed at the higher category level of VSUs. Concretely, we gen-
erate a probability for each of the three VSU categories: object,
attribute, and relationship as follows:

= siдmoid

βt

(cid:16)
W β [h

1
t

; attO
t

; att A
t

; att R
t ]

(cid:17)

,

(17)

= [βo

where βt ∈ R3×1 is the gating weights for each category, Wβ
are learnable parameters. Denote the gating weights for object,
, respectively.
attribute, and relationship categories as βo
Then βt
t ] indicates which VSU category the current
word is about (object, attribute, or relationships), and decides which
VSU category should be paid more attention currently. Then we
, and
compute the current context vector by aggregating attO
t
att R
t

according to βt

, att A
t

t , βa

t , βa

t , βr
t

t , βr

:

(18)

t , βr

t attO

t att A

t att R
t ).

ct = concat(βo

t , βa
In order to utilize the unit embeddings learned from both GS and
GG , we extend the calculation of ct to include the attentional results
of both GS and GG . Specifically, denote the att R
of GS and GG
t , βr
t
, respectively. Then we calculate ct as:
as att RS
, βr д
t att A
(19)
The context vector ct is then ready to be used by the sentence

, βr д
and βr s
, att RG
t
t
t
t , βa
t attO
ct = concat(βo

t att RG
t

t att RS
t

t , βr s

).

t

decoder for predicting the t-th word.

Attention based Sentence Decoder. We now introduce our sen-
tence decoder which is a two-layer Long Short-Term Memory
(LSTM) [10] network with our context gated attention module
injected in the middle it, as is shown in the right part of Figure 2.

5

Following [2], the input vector to the first LSTM (called attention
LSTM) at each time step is the concatenation of the embedding of
the current word, the the mean-pooled image feature v = 1
(cid:205)
i f v
,
oi
k
2
as well as the previous hidden state of the second LSTM, h
t −1. Hence
the updating procedure for the first LSTM is given by:

,

1

h

1
t

(cid:3) (cid:17)

(20)

= LSTM1 (cid:16)
h

2
t −1; v;W e Πt

t −1; (cid:2)h
where W e ∈ RE×N is a word embedding matrix, and Πt is one-hot
is leveraged as the
encoding vector of the current input word. h
query for the context gated attention module (Eqn. 13) to obtain
the current context vector ct .

The second LSTM (called language LSTM) takes as input the
context vector ct and the hidden state of the first LSTM. Its updating
procedure is given by:

1
t

2
t

h

= LSTM2 (cid:16)
h

2

t −1, (cid:2)h

1
t

; ct

(cid:3) (cid:17)

.

(21)

2
t

h

is then used to predict the next word yt through a softmax layer.

4 EXPERIMENTS
4.1 Datasets and Evaluation Metrics
MS-COCO [16]. The dataset is the most popular benchmark for
image captioning. We use the ‘Karpathy’ splits [19] that have been
used extensively for reporting results in prior works. This split con-
tains 113,287 training images with five captions each, and 5k images
for validation and test splits, respectively. We follow standard prac-
tice and perform only minimal text pre-processing: converting all
sentences to lower case, tokenizing on white space, discarding rare
words which occur less than 5 times, and trimming each caption
to a maximum of 16 words, resulting in a final vocabulary of 9,487
words.
Evaluation Metrics. We use the standard automatic evaluation
metrics — BLEU-1,2,3,4, METEOR, ROUGE-L, CIDEr [16], and SPICE
[1] — to evaluate caption quality, denoted as B@N, M, R, C and S,
respectively.

4.2 Implementation Details
Visual Genome (VG) [15] dataset is exploited to train our object
detector, attribute classifier, and relationship detector. We use three
vocabularies of 305 objects, 103 attributes, and 64 relationships
respectively, following [29]. For the object detector, we use the pre-
trained Faster R-CNN model along with ResNet-101 [9] backbone
provided by [2]. We use the top-3 predicted attributes for attribute
features, i.e. Na = 3. For constructing the geometry graph, we
consider two objects to have interactions if their boxes satisfy two
conditions: r2 < 0.2 and r4 < 0.5, where r2 and r4 are the IoU and
relative distance in Eqn. 3.

The number of output units in the feature projection layers
(ϕo, ϕa, ϕr ) and the GCNs (дo, дa, дr ) are all set to 1000. For the
decoder, we set the number of hidden units in each LSTM and each
attention layer to 1,000 and 512, respectively. We use four indepen-
dent word embedding layers for embedding input words, objects,
attributes, and relationships categories, with the word embedding
sizes set to 1000, 128, 128, and 128, respectively. We first train our
model under a cross-entropy loss using Adam [13] optimizer and
the learning rate was initialized to 5 × 10−4 and was decayed by 0.8

Table 1: Ablations of our method, evaluated on MS-COCO
Karpathy split.

Model

Base

O
A
RS
RG
OA
ORS
ORG
ARS
OARS (GS )
OARG (GG )
GS + GG

GS +gate
GG +gate

baseline: GG +gate
w/o f v
oi
w/o f s
oi
w/o f д
oi
GG +shareAtt.
Base+multiAtt.

B@4 M

R

C

S

36.7

27.9

57.5

122.8

20.9

36.9
36.9
35.7
36.6
37.6
37.7
37.8
37.3
37.9
38.0
38.4

38.1
38.3

38.3
32.8
37.9

38.3

37.4
37.1

27.9
27.8
27.5
27.7
28.1
28.2
28.2
28.1
28.3
28.3
28.5

28.4
28.4

28.4
26.3
28.3

28.4

28.0
27.9

57.6
57.6
57.1
57.4
58.0
58.1
57.9
57.7
58.2
58.1
58.4

58.1
58.3

58.3
55.4
58.2

58.4

57.9
57.7

123.8
123.4
119.3
121.5
126.1
126.3
126.5
125.9
127.2
127.2
128.6

128.1
128.2

128.2
111.9
126.9
127.7

124.1
123.7

21.1
21.1
21.2
21.1
21.7
21.9
21.9
21.7
21.9
21.9
22.0

22.0
22.0

22.0
19.7
21.8

22.0

21.3
21.3

(a)

(b)

(c)

(d)

every 5 epochs. After that, we train the model using reinforcement
learning [22] by optimizing the CIDEr reward. When testing, beam
search with a beam size of 3 is used.

4.3 Ablation Studies
We conduct extensive ablative experiments to compare our model
against alternative architectures and settings, as shown in Table 1.
We use Base to denote our baseline model, which is our implemen-
tation of Up-Down [2].

(a) How much does each kind of VSUs contribute? The exper-
iments in Table 1(a) answer this question, where we compare the
model performance of using different combinations of the three
categories of VSUs. We first denote the object, attribute, and rela-
tionship units as O, A, RS (RG ), respectively. RS means the semantic
relationship and RG in GS is the geometry relationship in GG . Then
we denote the various combinations of the input components (attO
,
t
) to be used in Eqn. 18 as the combinations of the
att A
t
corresponding symbols: O, A, RS (RG ). We have the following ob-
servations from Table 1 (a) .

, and att R
t

First, we look at the O model, where we simply replace the orig-
inal visual features f v
in the baseline model with our embeddings
oi
of object units (uoi
in Eqn. 8). We can see that this simple modifica-
tion brings slight improvement on model performance, indicating
the effectiveness of fusing the visual appearance cues, semantic
embedding cues, and geometry cues for representing object units.
Second, results of the A, RS , RG models show that using attribute
or relationship units alone result in decreased model performance.
That is because although the computation of their embeddings

6

(f ri

have involved the embeddings of the object units (e.g. Eqn. 9), the
added residual connections make f ai
) the dominant factors
in the computation process. It is also noteworthy that all of A, RS ,
RG achieve comparable results as to the baseline, indicating the
effectiveness of their learned unit embeddings. Third, the results
of OA, ORS , ORG models represent significant outperform that of
the O model. The performance of ARS is better than RS , however
is inferior to ORS , which again shows the importance of object
units. Fourth, Combining object, attribute, and relationship units
altogether, i.e. OARS and OARG , brings the highest performance.
The results show that the three kinds of VSU are highly comple-
mentary to each other. Finally, we combine VSUs from both the
spatial graph GS and the geometry graph GG for training, denoted
as GS + GG , which is also equivalent to OARS RG . We see that
GS + GG further improves the performance over GS and GG , indi-
cating the compatibility between RS and RG .

(b) The effect of context gated attention. In the above experi-
ments, the gating weights (βt
) are set to 1. We now further apply our
gated fusion process (Eqn. 17) upon them, whose results are shown
in Table 1(b). We can see that the performance of both OARS and
OARG is further improved, showing that the hierarchical alignment
process is beneficial to take full advantage of the VSUs. Overall,
compared to the baseline, the CIDEr score is boosted up from 122.8
to 128.6.

, semantic embedding cue f s
oi

(c) The effects of different content cues. Using GG +дate as the
baseline, we discard (denoted as w/o) each of the visual appearance
from
cue f v
oi
, we
the computation process of foi
remove all visual appearance features (including f v
and v) from the
oi
sentence decoder. We can see that removing any of them results in
decreased performance. Particularly, w/o f v
only achieves a CIDEr
oi
score of 111.9, indicating visual appearance cues are still necessary
for image captioning.

, and geometry cue f д
oi
(Eqn. 5). Note that, in w/o f v
oi

(d) Does the improvement come from more parameters or
computation? First, in the GG +shareAtt. model, instead of using
three independent attention modules for O, A, R in the GG model,
we use a single attention module for aggregating their embeddings.
We see that GG +shareAtt. deteriorates the performance. Second,
in the Base+multiAtt. model, we replace the attention module in
the Base model with three attention modules, which have the same
structure and inputs but independent parameters. We can see that
the performance of Base+multiAtt. is far worse than our GS and
GG models, although their decoders have similar numbers of pa-
rameters. The comparisons indicate that effect of our method is
beyond increasing computation and network capacity.

(e) How many relationship units to use? We compare the effect
of using various numbers of geometry relationship units for train-
ing. We have introduced in Section 4.1 that we consider two objects
to have interactions if r2 < 0.2 and r4 < 0.5, where r2 means IoU.
Thus, we adjust the threshold value γ for r2 to change the num-
ber of geometry relationship units for each image. Specifically, we
set γ to 0.1, 0.2, 0.3, 0.4 respectively, which result in an average of
77.3, 43.8, 20.8, 11.1 relationship units per image respectively. We
then separately train the GG + дate model for each of them. The

Table 2: Leaderboard of the published state-of-the-art image captioning models on the online COCO test server, where c5 and
c40 denote using 5 and 40 references for testing, respectively.

Model

Metric

SCST [22]
LSTM-A [32]
StackCap [7]
Up-Down [2]
CAVP [17]
Ours: VSUA

BLEU-1

BLEU-2

BLEU-3

BLEU-4

METEOR

ROUGE-L

CIDEr

c5

78.1
78.7
77.8
80.2
80.1
79.9

c40

93.7
93.7
93.2
95.2
94.9
94.7

c5

61.9
62.7
61.6
64.1
64.7
64.3

c40

86.0
86.7
86.1
88.8
88.8
88.6

c5

47.0
47.6
46.8
49.1
50.0
49.5

c40

75.9
76.5
76.0
79.4
79.7
79.3

c5

35.2
35.6
34.9
36.9
37.9
37.4

c40

64.5
65.2
64.6
68.5
69.0
68.3

c5

27.0
27.0
27.0
27.6
28.1
28.2

c40

35.5
35.4
35.6
36.7
37.0
37.1

c5

56.3
56.4
56.2
57.1
58.2
57.9

c40

10.7
70.5
70.6
72.4
73.1
72.8

c5

114.7
116.0
114.8
117.9
121.6
123.1

c40

116.0
118.0
118.3
120.5
123.8
125.5

Table 3: Performance comparison with the existing methods
on MS-COCO Karpathy test split.

Approach

B@4 M

R

C

S

Google NICv2 [25]
Soft-Attention [27]
LSTM-A [32]
Adaptive [18]

SCST [22]
Up-Down [2]
Stack-Cap [7]
CAVP [17]
GCN-LSTM [31]

Ours: VSUA

32.1
25.0
32.5
33.2

33.3
36.3
36.1
38.6
38.2

38.4

25.7
23.0
25.1
26.6

26.3
27.7
27.4
28.3
28.5

28.5

–
–
53.8
–

55.3
56.9
56.9
58.5
58.3

58.4

99.8
–
98.6
108.5

111.4
120.1
120.4
126.3
127.6

–
–
–
19.4

–
21.4
20.9
21.6
22.0

128.6

22.0

changes of the CIDEr score are shown in Figure 4. As we can see,
basically, as the number of relationship units increases, the CIDEr
score gradually increases. However, the performance differences are
not very significant. Consider the trade-off between computation
and model performance, we set γ to 0.2.

4.4 Comparisons with State-of-The-Arts
We compare our methods with Google NICv2 [25], Soft-Attention
[27], LSTM-A [32], Adaptive [18], SCST [22], StackCap [7], Up-
Down [2], CAVP [17], and GCN-LSTM [31]. Among them, LSTM-A
incorporates attributes of the whole image for captioning, SCST
uses reinforcement learning for training, Up-Down is the baseline
with the same decoder as ours, Stack-Cap adopts a three-layer
LSTM and more complex reward, CAVP models the visual context
over time, and GCN-LSTM treats visual relationships as edges in
a graph to help refining the region-level features. We name our
GS + GG model as VSUA (Visual Semantic Units Alignment) for
convenience.

We show in Table 3 the comparison between our single model and
state-of-the-art single-model methods on the MS-COCO Karpathy
test split. We can see that our model achieves a new state-of-the-art
score on CIDEr (128.6), and comparable scores with GCN-LSTM
and CAVP on the other metrics. Particular, relative to the Up-Down
baseline, we push the CIDEr from 120.1 to 128.6. It is noteworthy
that GCN-LSTM uses a considerable large batch size of 1024 and
training epochs of 250, which are far beyond our computing re-
source and also larger than that of ours and the other methods

7

Figure 4: Results of training our model with different num-
bers of relationship units for images.

(typically both are within 100). Table 2 reports the performances of
our single model without any ensemble on the official MS-COCO
evaluation server (by the date of 08/04/2019). We can see that our
approach achieves very competitive performance, compared to the
state-of-the-art.

4.5 Qualitative Analysis
Visualization of Gating Weights. To better understand the ef-
fect of our context gated fusion attention, we visualize the gating
in Eqn. 17) of object, attribute, and relationship cat-
weights (βt
egories for each word in the generated captions in Figure 6. Our
model successfully learns to attend to the category of VSUs that are
consistent with the type of the current word, i.e. object, attribute,
or relationship. For example, for the verbs like “laying", the weights
for the relationship category are generally the highest. The same
observations could be found for the adjectives like “black" and “big",
and the nouns like “cat" and “clock".

Example Results. Figure 5 shows four examples of the generated
captions and semantic graphs for the images, where “ours", “base",
“GT" denotes captions from our GS + дate model, the Up-Down
baseline, and the ground-truth, respectively. Generally, our model
can generate more descriptive captions than Up-Down by enriching
the sentences with more precise recognition of objects, detailed
description of attributes, and comprehensive understanding of in-
teractions between objects. For instance, in the second image, our
model generates “standing before a large tree", depicting the image
content more comprehensively, while the base model fails to recog-
nize the tree and the interactions between the animals and the tree.

Figure 5: Example results of the generated captions (by our model, Up-Down baseline, and ground truth) and semantic graphs.
Objects, attributes, and relationships are colored with blue, green, and orange, respectively.

770–778.

[10] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural

computation 9, 8 (1997), 1735–1780.

[11] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael
Bernstein, and Li Fei-Fei. 2015. Image retrieval using scene graphs. In Proceedings
of the IEEE conference on computer vision and pattern recognition. 3668–3678.
[12] Andrej Karpathy and Li Feifei. 2015. Deep visual-semantic alignments for generat-
ing image descriptions. computer vision and pattern recognition (2015), 3128–3137.
[13] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimiza-

tion. arXiv preprint arXiv:1412.6980 (2014).

[14] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph

convolutional networks. arXiv preprint arXiv:1609.02907 (2016).

[15] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
2017. Visual genome: Connecting language and vision using crowdsourced dense
image annotations. International Journal of Computer Vision 123, 1 (2017), 32–73.
[16] Tsungyi Lin, Michael Maire, Serge J Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollar, and C Lawrence Zitnick. 2014. Microsoft COCO: Common
Objects in Context. european conference on computer vision (2014), 740–755.
[17] Daqing Liu, Zheng-Jun Zha, Hanwang Zhang, Yongdong Zhang, and Feng Wu.
2018. Context-Aware Visual Policy Network for Sequence-Level Image Cap-
tioning. In 2018 ACM Multimedia Conference on Multimedia Conference. ACM,
1416–1424.

[18] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. 2017. Knowing
when to look: Adaptive attention via a visual sentinel for image captioning. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), Vol. 6.

[19] Ruotian Luo, Brian Price, Scott Cohen, and Gregory Shakhnarovich. 2018. Dis-
criminability objective for training descriptive captions. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. 6964–6974.
[20] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. 2016. Modeling context
between objects for referring expression understanding. In European Conference
on Computer Vision. Springer, 792–807.

[21] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:
Towards real-time object detection with region proposal networks. In Advances
in neural information processing systems. 91–99.

[22] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava
Goel. 2017. Self-critical Sequence Training for Image Captioning. computer vision
and pattern recognition (2017).

[23] Damien Teney, Lingqiao Liu, and Anton van den Hengel. 2017. Graph-structured
representations for visual question answering. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition. 1–9.

[24] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show
and tell: A neural image caption generator. In Computer Vision and Pattern Recog-
nition (CVPR), 2015 IEEE Conference on. IEEE, 3156–3164.

[25] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2017. Show
and tell: Lessons learned from the 2015 mscoco image captioning challenge. IEEE
transactions on pattern analysis and machine intelligence 39, 4 (2017), 652–663.

[26] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei. 2017. Scene graph
generation by iterative message passing. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. 5410–5419.

[27] Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan
Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell:
Neural Image Caption Generation with Visual Attention. international conference
on machine learning (2015), 2048–2057.

[28] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi Parikh. 2018. Graph
r-cnn for scene graph generation. In Proceedings of the European Conference on
Computer Vision (ECCV). 670–685.

[29] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai. 2019. Auto-encoding
scene graphs for image captioning. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. 10685–10694.

Figure 6: Visualization of the generated captions, and the
per-word gating weights (βt ) of them belonging to each of
the three categories: object, attribute, and relationship.

5 CONCLUSION
We proposed to fill the information gap between visual content and
linguistic description with visual semantic units (VSUs), which are
visual components about objects, their attributes, and object-object
interactions. We leverage structured graph (both semantic graph
and geometry graph) to uniformly represent and GCNs to contex-
tually embed the VSUs. A novel context gated attention module is
introduced to hierarchically align words and VSUs. Extensive ex-
periments on MS COCO have shown the superiority of our method.

REFERENCES
[1] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016.
SPICE: Semantic Propositional Image Caption Evaluation. (2016), 382–398.
[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson,
Stephen Gould, and Lei Zhang. 2017. Bottom-up and top-down attention for
image captioning and vqa. arXiv preprint arXiv:1707.07998 (2017).

[3] Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima’an.
2017. Graph convolutional encoders for syntax-aware neural machine translation.
arXiv preprint arXiv:1704.04675 (2017).

[4] Xinlei Chen, Li-Jia Li, Li Fei-Fei, and Abhinav Gupta. 2018.

Iterative visual
reasoning beyond convolutions. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 7239–7248.

[5] Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K. Srivastava, Li Deng, Piotr
DollÂĺÂćr, Jianfeng Gao, Xiaodong He, Margaret Mitchell, and John C. Platt.
2014. From captions to visual concepts and back. (2014), 1473–1482.

[6] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl. 2017. Neural message passing for quantum chemistry. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70. JMLR. org,
1263–1272.

[7] Jiuxiang Gu, Jianfei Cai, Gang Wang, and Tsuhan Chen. 2017. Stack-captioning:
Coarse-to-fine learning for image captioning. arXiv preprint arXiv:1709.03376
(2017).

[8] Longteng Guo, Jing Liu, Peng Yao, Jiangwei Li, and Hanqing Lu. 2019. MSCap:
Multi-Style Image Captioning With Unpaired Stylized Text. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. 4204–4213.
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. computer vision and pattern recognition (2016),

8

[30] Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, and William W Cohen.

[33] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. 2016. Image

2016. Review Networks for Caption Generation. (2016).

Captioning with Semantic Attention. (2016), 4651–4659.

[31] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. 2018. Exploring visual relationship
for image captioning. In Proceedings of the European Conference on Computer
Vision (ECCV). 684–699.

[32] Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao Mei. 2016. Boosting

Image Captioning with Attributes. (2016).

[34] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. 2018. Neural motifs:
Scene graph parsing with global context. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. 5831–5840.

9

