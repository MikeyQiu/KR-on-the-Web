7
1
0
2
 
g
u
A
 
5
1
 
 
]
E
M

.
t
a
t
s
[
 
 
1
v
7
2
5
4
0
.
8
0
7
1
:
v
i
X
r
a

The Trimmed Lasso: Sparsity and Robustness

Dimitris Bertsimas, Martin S. Copenhaver, and Rahul Mazumder∗

August 15, 2017

Abstract

Nonconvex penalty methods for sparse modeling in linear regression have been a topic of
fervent interest in recent years. Herein, we study a family of nonconvex penalty functions that
we call the trimmed Lasso and that oﬀers exact control over the desired level of sparsity of
estimators. We analyze its structural properties and in doing so show the following:

1. Drawing parallels between robust statistics and robust optimization, we show that the
trimmed-Lasso-regularized least squares problem can be viewed as a generalized form of
total least squares under a speciﬁc model of uncertainty.
In contrast, this same model
of uncertainty, viewed instead through a robust optimization lens, leads to the convex
SLOPE (or OWL) penalty.

2. Further, in relating the trimmed Lasso to commonly used sparsity-inducing penalty func-
tions, we provide a succinct characterization of the connection between trimmed-Lasso-
like approaches and penalty functions that are coordinate-wise separable, showing that
the trimmed penalties subsume existing coordinate-wise separable penalties, with strict
containment in general.

3. Finally, we describe a variety of exact and heuristic algorithms, both existing and new,
for trimmed Lasso regularized estimation problems. We include a comparison between the
diﬀerent approaches and an accompanying implementation of the algorithms.

1

Introduction

Sparse modeling in linear regression has been a topic of fervent interest in recent years [23, 42].
This interest has taken several forms, from substantial developments in the theory of the Lasso to
advances in algorithms for convex optimization. Throughout there has been a strong emphasis on
the increasingly high-dimensional nature of linear regression problems; in such problems, where the
number of variables p can vastly exceed the number of observations n, sparse modeling techniques
are critical for performing inference.

Context

One of the fundamental approaches to sparse modeling in the usual linear regression model of
y = Xβ + (cid:15), with y ∈ Rn and X ∈ Rn×p, is the best subset selection [57] problem:

min
(cid:107)β(cid:107)0≤k

1
2

(cid:107)y − Xβ(cid:107)2
2,

(1)

∗Authors’ aﬃliation: Sloan School of Management and Operations Research Center, MIT.

Emails: {dbertsim,mcopen,rahulmaz}@mit.edu.

1

which seeks to ﬁnd the best choice of k from among p features that best explain the response in
terms of the least squares loss function. The problem (1) has received extensive attention from
a variety of statistical and optimization perspectives—see for example [14] and references therein.
One can also consider the Lagrangian, or penalized, form of (1), namely,

for a regularization parameter µ > 0. One of the advantages of (1) over (2) is that it oﬀers direct
control over estimators’ sparsity via the discrete parameter k, as opposed to the Lagrangian form
(2) for which the correspondence between the continuous parameter µ and the resulting sparsity of
estimators obtained is not entirely clear. For further discussion, see [65].

Another class of problems that have received considerable attention in the statistics and machine

learning literature is the following:

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + µ(cid:107)β(cid:107)0,

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + R(β),

(2)

(3)

where R(β) is a choice of regularizer which encourages sparsity in β. For example, the popularly
used Lasso [70] takes the form of problem (3) with R(β) = µ(cid:107)β(cid:107)1, where (cid:107) · (cid:107)1 is the (cid:96)1 norm; in
doing so, the Lasso simultaneously selects variables and also performs shrinkage. The Lasso has
seen widespread success across a variety of applications.

In contrast to the convex approach of the Lasso, there also has been been growing interest in
considering richer classes of regularizers R which include nonconvex functions. Examples of such
penalties include the (cid:96)q-penalty (for q ∈ [0, 1]), minimax concave penalty (MCP) [74], and the
smoothly clipped absolute deviation (SCAD) [33], among others. Many of the nonconvex penalty
functions considered are coordinate-wise separable; in other words, R can be decomposed as

R(β) =

ρ(|βi|),

p
(cid:88)

i=1

where ρ(·) is a real-valued function [75]. There has been a variety of evidence suggesting the promise
of such nonconvex approaches in overcoming certain shortcomings of Lasso-like approaches.

One of the central ideas of nonconvex penalty methods used in sparse modeling is that of
creating a continuum of estimation problems which bridge the gap between convex methods for
sparse estimation (such as Lasso) and subset selection in the form (1). However, as noted above,
such a connection does not necessarily oﬀer direct control over the desired level of sparsity of
estimators.

The trimmed Lasso

In contrast with coordinate-wise separable penalties as considered above, we consider a family of
penalties that are not separable across coordinates. One such penalty which forms a principal
object of our study herein is

Tk (β) := min
(cid:107)φ(cid:107)0≤k

(cid:107)φ − β(cid:107)1.

The penalty Tk is a measure of the distance from the set of k-sparse estimators as measured via
the (cid:96)1 norm. In other words, when used in problem (3), the penalty R = Tk controls the amount
of shrinkage towards sparse models.

2

The penalty Tk can equivalently be written as

Tk (β) =

|β(i)|,

p
(cid:88)

i=k+1

where |β(1)| ≥ |β(2)| ≥ · · · ≥ |β(p)| are the sorted entries of β. In words, Tk (β) is the sum of the
absolute values of the p − k smallest magnitude entries of β. The penalty was ﬁrst introduced
in [39, 43, 69, 72]. We refer to this family of penalty functions (over choices of k) as the trimmed
Lasso.1 The case of k = 0 recovers the usual Lasso, as one would suspect. The distinction, of
course, is that for general k, Tk no longer shrinks, or biases towards zero, the k largest entries of β.
Let us consider the least squares loss regularized via the trimmed lasso penalty—this leads to

the following optimization criterion:

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λTk (β) ,

(4)

where λ > 0 is the regularization parameter. The penalty term shrinks the smallest p − k entries of
β and does not impose any penalty on the largest k entries of β. If λ becomes larger, the smallest
p − k entries of β are shrunk further; after a certain threshold—as soon as λ ≥ λ0 for some ﬁnite
λ0—the smallest p − k entries are set to zero. The existence of a ﬁnite λ0 (as stated above) is
an attractive feature of the trimmed Lasso and is known as its exactness property, namely, for λ
suﬃciently large, the problem (4) exactly solves constrained best subset selection as in problem (1)
(c.f. [39]). Note here the contrast with the separable penalty functions which correspond instead
with problem (2); as such, the trimmed Lasso is distinctive in that it oﬀers precise control over the
desired level of sparsity vis-`a-vis the discrete parameter k. Further, it is also notable that many
algorithms developed for separable-penalty estimation problems can be directly adapted for the
trimmed Lasso.

Our objective in studying the trimmed Lasso is distinctive from previous approaches. In par-
ticular, while previous work on the penalty Tk has focused primarily on its use as a tool for
reformulating sparse optimization problems [43, 69] and on how such reformulations can be solved
computationally [39, 72], we instead aim to explore the trimmed Lasso’s structural properties and
its relation to existing sparse modeling techniques.

In particular, a natural question we seek to explore is, what is the connection of the trimmed
Lasso penalty with existing separable penalties commonly used in sparse statistical learning? For
example, the trimmed Lasso bears a close resemblance to the clipped (or capped) Lasso penalty [76],
namely,

p
(cid:88)

i=1

µ min{γ|βi|, 1},

where µ, γ > 0 are parameters (when γ is large, the clipped Lasso approximates µ(cid:107)β(cid:107)0).

Robustness: robust statistics and robust optimization

A signiﬁcant thread woven throughout the consideration of penalty methods for sparse modeling
is the notion of robustness—in short, the ability of a method to perform in the face of noise.
Not surprisingly, the notion of robustness has myriad distinct meanings depending on the context.
Indeed, as Huber, a pioneer in the area of robust statistics, aptly noted:

1The choice of name is our own and is motivated by the least trimmed squares regression estimator, described

below

3

“The word ‘robust’ is loaded with many—sometimes inconsistent—connotations.” [45,
p. 2]

For this reason, we consider robustness from several perspectives—both the robust statistics [45]
and robust optimization [9] viewpoints.

A common premise of the various approaches is as follows: that a robust model should perform
well even under small deviations from its underlying assumptions; and that to achieve such behavior,
some eﬃciency under the assumed model should be sacriﬁced. Not surprisingly in light of Huber’s
prescient observation, the exact manifestation of this idea can take many diﬀerent forms, even if
the initial premise is ostensibly the same.

Robust statistics and the “min-min” approach

One such approach is in the ﬁeld of robust statistics [45, 58, 61]. In this context, the primary as-
sumptions are often probabilistic, i.e. distributional, in nature, and the deviations to be “protected
against” include possibly gross, or arbitrarily bad, errors. Put simply, robust statistics is primary
focused on analyzing and mitigating the inﬂuence of outliers on estimation methods.

There have been a variety of proposals of diﬀerent estimators to achieve this. One that is
particularly relevant for our purposes is that of least trimmed squares (“LTS”) [61]. For ﬁxed
j ∈ {1, . . . , n}, the LTS problem is deﬁned as

where ri(β) = yi −x(cid:48)
iβ are the residuals and r(i)(β) are the sorted residuals given β with |r(1)(β)| ≥
|r(2)(β)| ≥ · · · ≥ |r(n)(β)|. In words, the LTS estimator performs ordinary least squares on the
n − j smallest residuals (discarding the j largest or worst residuals).

Furthermore, it is particularly instructive to express (5) in the equivalent form (c.f. [16])

min
β

n
(cid:88)

i=j+1

|r(i)(β)|2,

min
β

min
I⊆{1,...,n}:
|I|=n−j

(cid:88)

i∈I

|ri(β)|2.

In light of this representation, we refer to LTS as a form of “min-min” robustness. One could also
interpret this min-min robustness as optimistic in the sense the estimation problems (6) and, a
fortiori, (5) allow the modeler to also choose observations to discard.

Other min-min models of robustness

Another approach to robustness which also takes a min-min form like LTS is the classical technique
known as total least squares [38, 54]. For our purposes, we consider total least squares in the form

min
β

min
∆

1
2

(cid:107)y − (X + ∆)β(cid:107)2

2 + η(cid:107)∆(cid:107)2
2,

where (cid:107)∆(cid:107)2 is the usual Frobenius norm of the matrix ∆ and η > 0 is a scalar parameter. In this
framework, one again has an optimistic view on error: ﬁnd the best possible “correction” of the
data matrix X as X + ∆∗ and perform least squares using this corrected data (with η controlling
the ﬂexibility in choice of ∆).

In contrast with the penalized form of (7), one could also consider the problem in a constrained

form such as

min
β

min
∆∈V

1
2

(cid:107)y − (X + ∆)β(cid:107)2
2,

4

(5)

(6)

(7)

(8)

where V ⊆ Rn×p is deﬁned as V = {∆ : (cid:107)∆(cid:107)2 ≤ η(cid:48)} for some η(cid:48) > 0. This problem again has the
min-min form, although now with perturbations ∆ as restricted to the set V.

Robust optimization and the “min-max” approach

We now turn our attention to a diﬀerent approach to the notion of robustness known as robust
optimization [9, 12]. In contrast with robust statistics, robust optimization typically replaces dis-
tributional assumptions with a new primitive, namely, the deterministic notion of an uncertainty
set. Further, in robust optimization one considers a worst-case or pessimistic perspective and the
focus is on perturbations from the nominal model (as opposed to possible gross corruptions as in
robust statistics).

To be precise, one possible robust optimization model for linear regression takes form [9, 15, 73]

min
β

max
∆∈U

1
2

(cid:107)y − (X + ∆)β(cid:107)2
2,

(9)

where U ⊆ Rn×p is a (deterministic) uncertainty set that captures the possible deviations of the
model (from the nominal data X). Note the immediate contrast with the robust models considered
earlier (LTS and total least squares in (5) and (7), respectively) that take the min-min form; instead,
robust optimization focuses on “min-max” robustness. For a related discussion contrasting the min-
min approach with min-max, see [8, 49] and references therein.

One of the attractive features of the min-max formulation is that it gives a re-interpretation of
several statistical regularization methods. For example, the usual Lasso (problem (3) with R = µ(cid:96)1)
can be expressed in the form (9) for a speciﬁc choice of uncertainty set:

Proposition 1.1 (e.g. [9, 73]). Problem (9) with uncertainty set U = {∆ : (cid:107)∆i(cid:107)2 ≤ µ ∀i} is
equivalent to the Lasso, i.e., problem (3) with R(β) = µ(cid:107)β(cid:107)1, where ∆i denotes the ith column of
∆.

For further discussion of the robust optimization approach as applied to statistical problems, see [15]
and references therein.

Other min-max models of robustness

We close our discussion of robustness by considering another example of min-max robustness that
is of particular relevance to the trimmed Lasso. In particular, we consider problem (3) with the
SLOPE (or OWL) penalty [18, 35], namely,

RSLOPE(w)(β) =

wi|β(i)|,

p
(cid:88)

i=1

where w is a (ﬁxed) vector of weights with w1 ≥ w2 ≥ · · · ≥ wp ≥ 0 and w1 > 0. In its simplest
form, the SLOPE penalty has weight vector ˜w, where ˜w1 = · · · = ˜wk = 1, ˜wk+1 = · · · = ˜wp = 0, in
which case we have the identity

RSLOPE( ˜w)(β) = (cid:107)β(cid:107)1 − Tk(β).

There are some apparent similarities but also subtle diﬀerences between the SLOPE penalty and
the trimmed Lasso. From a high level, while the trimmed Lasso focuses on the smallest magnitude
entries of β, the SLOPE penalty in its simplest form focuses on the largest magnitude entries

5

of β. As such, the trimmed Lasso is generally nonconvex, while the SLOPE penalty is always
convex; consequently, the techniques for solving the related estimation problems will necessarily be
diﬀerent.

Finally, we note that the SLOPE penalty can be considered as a min-max model of robustness

for a particular choice of uncertainty set:

Proposition 1.2. Problem (9) with uncertainty set

(cid:26)

U =

∆ :

∆ has at most k nonzero
columns and (cid:107)∆i(cid:107)2 ≤ µ ∀i

(cid:27)

is equivalent to problem (3) with R(β) = µRSLOPE( ˜w)(β), where ˜w1 = · · · = ˜wk = 1 and ˜wk+1 =
· · · = ˜wp = 0.

We return to this particular choice of uncertainty set later. (For completeness, we include a more
general min-max representation of SLOPE in Appendix A.)

Computation and Algorithms

Broadly speaking, there are numerous distinct approaches to algorithms for solving problems of
the form (1)–(3) for various choices of R. We do not attempt to provide a comprehensive list of
such approaches for general R, but we will discuss existing approaches for the trimmed Lasso and
closely related problems. Approaches typically take one of two forms: heuristic or exact.

Heuristic techniques

Heuristic approaches to solving problems (1)–(3) often use techniques from convex optimization [21],
such as proximal gradient descent or coordinate descent (see [33,55]). Typically these techniques are
coupled with an analysis of local or global behavior of the algorithm. For example, global behavior
is often considered under additional restrictive assumptions on the underlying data; unfortunately,
verifying such assumptions can be as diﬃcult as solving the original nonconvex problem.
(For
example, consider the analogy with compressed sensing [25, 30, 32] and the hardness of verifying
whether underlying assumptions hold [5, 71]).

There is also extensive work studying the local behavior (e.g. stationarity) of heuristic ap-
proaches to these problems. For the speciﬁc problems (1) and (2), the behavior of augmented
Lagrangian methods [4, 68] and complementarity constraint techniques [22, 24, 29, 34] have been
considered. For other local approaches, see [52].

Exact techniques

One of the primary drawbacks of heuristic techniques is that it can often be diﬃcult to verify the
degree of suboptimality of the estimators obtained. For this reason, there has been an increasing
interest in studying the behavior of exact algorithms for providing certiﬁably optimal solutions to
problems of the form (1)–(3) [14, 16, 51, 56]. Often these approaches make use of techniques from
mixed integer optimization (“MIO”) [19] which are implemented in a variety of software, e.g. Gurobi
[40]. The tradeoﬀ with such approaches is that they typically carry a heavier computational burden
than convex approaches. For a discussion of the application of MIO in statistics, see [14, 16, 51, 56].

6

What this paper is about

In this paper, we focus on a detailed analysis of the trimmed Lasso, especially with regard to its
properties and its relation to existing methods. In particular, we explore the trimmed Lasso from
two perspectives: that of sparsity as well as that of robustness. We summarize our contributions
as follows:

1. We study the robustness of the trimmed Lasso penalty. In particular, we provide several min-
min robustness representations of it. We ﬁrst show that the same choice of uncertainty set
that leads to the SLOPE penalty in the min-max robust model (9) gives rise to the trimmed
Lasso in the corresponding min-min robust problem (8) (with an additional regularization
term). This gives an interpretation of the SLOPE and trimmed Lasso as a complementary
pair of penalties, one under a pessimistic (min-max) model and the other under an optimistic
(min-min) model.

Moreover, we show another min-min robustness interpretation of the trimmed Lasso by com-
parison with the ordinary Lasso. In doing so, we further highlight the nature of the trimmed
Lasso and its relation to the LTS problem (5).

2. We provide a detailed analysis on the connection between estimation approaches using the
trimmed Lasso and separable penalty functions. In doing so, we show directly how penalties
such as the trimmed Lasso can be viewed as a generalization of such existing approaches in
certain cases. In particular, a trimmed-Lasso-like approach always subsumes its separable
analogue, and the containment is strict in general. We also focus on the speciﬁc case of the
clipped (or capped) Lasso [76]; for this we precisely characterize the relationship and provide
a necessary and suﬃcient condition for the two approaches to be equivalent. In doing so, we
highlight some of the limitations of an approach using a separable penalty function.

3. Finally, we describe a variety of algorithms, both existing and new, for trimmed Lasso esti-
mation problems. We contrast two heuristic approaches for ﬁnding locally optimal solutions
with exact techniques from mixed integer optimization that can be used to produce certiﬁ-
cates of optimality for solutions found via the convex approaches. We also show that the
convex envelope [60] of the trimmed Lasso takes the form

((cid:107)β(cid:107)1 − k)+ ,

where (a)+ := max{0, a}, a “soft-thresholded” variant of the ordinary Lasso. Throughout
this section, we emphasize how techniques from convex optimization can be used to ﬁnd
high-quality solutions to the trimmed Lasso estimation problem. An implementation of the
various algorithms presented herein can be found at

https://github.com/copenhaver/trimmedlasso.

Paper structure

The structure of the paper is as follows. In Section 2, we study several properties of the trimmed
Lasso, provide a few distinct interpretations, and highlight possible generalizations.
In Section
3, we explore the trimmed Lasso in the context of robustness. Then, in Section 4, we study the
relationship between the trimmed Lasso and other nonconvex penalties. In Section 5, we study
the algorithmic implications of the trimmed Lasso. Finally, in Section 6 we share our concluding
thoughts and highlight future directions.

7

2 Structural properties and interpretations

In this section, we provide further background on the trimmed Lasso: its motivations, interpreta-
tions, and generalizations. Our remarks in this section are broadly grouped as follows: in Section
2.1 we summarize the trimmed Lasso’s basic properties as detailed in [39, 43, 69, 72]; we then turn
our attention to an interpretation of the trimmed Lasso as a relaxation of complementarity con-
straints problems from optimization (Section 2.2) and as a variable decomposition method (Section
2.3); ﬁnally, in Sections 2.4 and 2.5 we highlight the key structural features of the trimmed Lasso
by identifying possible generalizations of its deﬁnition and its application. These results augment
the existing literature by giving a deeper understanding of the trimmed Lasso and provide a basis
for further results in Sections 3 and 4.

2.1 Basic observations

Lemma 2.1. For any β,

We begin with a summary of some of the basic properties of the trimmed Lasso as studied in
[39, 43, 69]. First of all, let us also include another representation of Tk:

Tk (β) = min

|βi| = min

(cid:88)

I⊆{1,...,p}:
|I|=p−k

i∈I

z
s. t.

(cid:104)z, |β|(cid:105)
(cid:88)

zi = p − k

i
z ∈ {0, 1}p,

where |β| denotes the vector whose entries are the absolute values of the entries of β.

In other words, the trimmed Lasso can be represented using auxiliary binary variables.

Now let us consider the problem

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λTk (β) ,

(TLλ,k)

where λ > 0 and k ∈ {0, 1, . . . , p} are parameters. Based on the deﬁnition of Tk, we have the
following:

Lemma 2.2. The problem (TLλ,k) can be rewritten exactly in several equivalent forms:

(TLλ,k) = min
β,φ:
(cid:107)φ(cid:107)0≤k

(cid:107)y − Xβ(cid:107)2 + λ(cid:107)β − φ(cid:107)1

(cid:107)y − Xβ(cid:107)2 + λ(cid:107)(cid:15)(cid:107)1

1
2

1
2

1
2

= min
β,φ,(cid:15):
β=φ+(cid:15)
(cid:107)φ(cid:107)0≤k

= min
φ,(cid:15):
(cid:107)φ(cid:107)0≤k

(cid:107)y − X(φ + (cid:15))(cid:107)2 + λ(cid:107)(cid:15)(cid:107)1

min
Tk(β)=0

1
2

(cid:107)y − Xβ(cid:107)2
2.

8

Exact penalization

Based on the deﬁnition of Tk, it follows that Tk(β) = 0 if and only if (cid:107)β(cid:107)0 ≤ k. Therefore, one can
rewrite problem (1) as

In Lagrangian form, this would suggest an approximation for (1) of the form

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λTk(β),

where λ > 0. As noted in the introduction, this approximation is in fact exact (in the sense
of [10, 11]), summarized in the following theorem; for completeness, we include in Appendix B a
full proof that is distinct from that in [39].2

Theorem 2.3 (c.f. [39]). For any ﬁxed k ∈ {0, 1, 2, . . . , p}, η > 0, and problem data y and X,
there exists some λ = λ(y, X) > 0 so that for all λ > λ, the problems

and

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λTk (β) + η(cid:107)β(cid:107)1

min
β
s. t.

1
2 (cid:107)y − Xβ(cid:107)2
(cid:107)β(cid:107)0 ≤ k

2 + η(cid:107)β(cid:107)1

have the same optimal objective value and the same set of optimal solutions.

The direct implication is that trimmed Lasso leads to a continuum (over λ) of relaxations to the
best subset selection problem starting from ordinary least squares estimation; further, best subset
selection lies on this continuum for λ suﬃciently large.

2.2 A complementary constraints viewpoint

We now turn our attention to a new perspective on the trimmed Lasso as considered via mathemat-
ical programming with complementarity constraints (“MPCCs”) [24, 44, 47, 48, 50, 62], sometimes
also referred to as mathematical programs with equilibrium constraints [27]. By studying this
connection, we will show that a penalized form of a common relaxation scheme for MPCCs leads
directly to the trimmed Lasso penalty. This gives a distinctly diﬀerent optimization perspective on
the trimmed Lasso penalty.

As detailed in [22, 24, 34], the problem (1) can be exactly rewritten as

z ∈ [0, 1]p
ziβi = 0.
by the inclusion of auxiliary variables z ∈ [0, 1]p. In essence, the auxiliary variables replace the
combinatorial constraint (cid:107)β(cid:107)0 ≤ k with complementarity constraints of the form ziβi = 0. Of
course, the problem as represented in (10) is still not directly amenable to convex optimization
techniques.

As such, relaxation schemes can be applied to (10). One popular method from the MPCC

literature is the Scholtes-type relaxation [44]; applied to (10) as in [24, 34], this takes the form

(10)

(11)

1
min
2
β,z
s. t. (cid:80)

(cid:107)y − Xβ(cid:107)2
2
i zi = p − k

1
min
2
β,z
s. t. (cid:80)

(cid:107)y − Xβ(cid:107)2
2
i zi = p − k

z ∈ [0, 1]p
|ziβi| ≤ t,

9

2The presence of the additional regularizer η(cid:107)β(cid:107)1 can be interpreted in many ways. For our purposes, it serves to

make the problems well-posed.

where t > 0 is some ﬁxed numerical parameter which controls the strength of the relaxation, with
t = 0 exactly recovering (10).
In the traditional MPCC context, it is standard to study local
optimality and stationarity behavior of solutions to (11) as they relate to the original problem (1),
c.f. [34].

Instead, let us consider a diﬀerent approach. In particular, consider a penalized, or Lagrangian,

form of the Scholtes relaxation (11), namely,

(12)

1
min
2
β,z
s. t. (cid:80)

(cid:107)y − Xβ(cid:107)2

2 + λ

(|ziβi| − t)

(cid:88)

i

i zi = p − k

z ∈ [0, 1]p

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λTk(β) − pλt,

for some ﬁxed λ ≥ 0.3 Observe that we can minimize (12) with respect to z to obtain the equivalent
problem

which is precisely problem (TLλ,(cid:96)) (up to the ﬁxed additive constant). In other words, the trimmed
Lasso can also be viewed as arising directly from a penalized form of the MPCC relaxation, with
auxiliary variables eliminated. This gives another view on Lemma 2.1 which gave a representation
of Tk using auxiliary binary variables.

2.3 Variable decomposition

To better understand the relation of the trimmed Lasso to existing methods, it is also useful to
consider alternative representations. Here we focus on representations which connect it to variable
decomposition methods. Our discussion here is an extended form of related discussions in [39,43,72].
To begin, we return to the ﬁnal representation of the trimmed Lasso problem as shown in

Lemma 2.2, viz.,

(TLλ,k) = min
φ,(cid:15):
(cid:107)φ(cid:107)0≤k

1
2

(cid:107)y − X(φ + (cid:15))(cid:107)2 + λ(cid:107)(cid:15)(cid:107)1.

(13)

We will refer to (TLλ,k) in the form (13) as the split or decomposed representation of the problem.
This is because in this form it is clear that we can think about estimators β found via (TLλ,k) as
being decomposed into two diﬀerent estimators: a sparse component φ and another component (cid:15)
with small (cid:96)1 norm (as controlled via λ).

Several remarks are in order. First, the decomposition of β into β = φ + (cid:15) is truly a de-
composition in that if β∗ is an optimal solution to (TLλ,k) with (φ∗, (cid:15)∗) a corresponding optimal
solution to the split representation of the problem (13), then one must have that φ∗
i = 0 for all
i ∈ {1, . . . , p}. In other words, the supports of φ and (cid:15) do not overlap; therefore, β∗ = φ∗ + (cid:15)∗ is
a genuine decomposition.

i (cid:15)∗

Secondly, the variable decomposition (13) suggests that the problem of ﬁnding the k largest
entries of β (i.e., ﬁnding φ) can be solved as a best subset selection problem with a (possibly
diﬀerent) convex loss function (without (cid:15)). To see this, observe that the problem of ﬁnding φ in
(13) can be written as the problem

3To be precise, this is a weaker relaxation than if we had separate dual variables λi for each constraint |ziβi| ≤ t,

at least in theory.

min
(cid:107)φ(cid:107)0≤k

(cid:101)L(φ),

10

where

(cid:101)L(φ) = min

(cid:15)

1
2

(cid:107)y − X(φ + (cid:15))(cid:107)2

2 + λ(cid:107)(cid:15)(cid:107)1.

Using theory on duality for the Lasso problem [59], one can argue that (cid:101)L is itself a convex loss
function. Hence, the variable decomposition gives some insight into how the largest k loadings for
the trimmed Lasso relates to solving a related sparse estimation problem.

A view towards matrix estimation

Finally, we contend that the variable decomposition of β as a sparse component φ plus a “noise”
component (cid:15) with small norm is a natural and useful analogue of corresponding decompositions in
the matrix estimation literature, such as in factor analysis [3,6,53] and robust Principal Component
Analysis [26]. For the purposes of this paper, we will focus on the analogy with factor analysis.

Factor analysis is a classical multivariate statistical method for decomposing the covariance
structure of random variables; see [13] for an overview of modern approaches to factor analysis.
Given a covariance matrix Σ ∈ Rp×p, one is interested in describing it as the sum of two distinct
components: a low-rank component Θ (corresponding to a low-dimensional covariance structure
common across the variables) and a diagonal component Φ (corresponding to individual variances
unique to each variable)—in symbols, Σ = Θ + Φ.

In reality, this noiseless decomposition is often too restrictive (see e.g. [41,63,67]), and therefore
it is often better to focus on ﬁnding a decomposition Σ = Θ+Φ+N , where N is a noise component
with small norm. As in [13], a corresponding estimation procedure can take the form

min
Θ,Φ
s. t.

(cid:107)Σ − (Θ + Φ)(cid:107)

rank(Θ) ≤ k
Φ = diag(Φ11, . . . , Φpp) (cid:60) 0
Θ (cid:60) 0,

(14)

where the constraint A (cid:60) 0 denotes that A is symmetric, positive semideﬁnite, and (cid:107) · (cid:107) is some
norm. One of the attractive features of the estimation procedure (14) is that for common choices
of (cid:107) · (cid:107), it is possible to completely eliminate the combinatorial rank constraint and the variable Θ
to yield a smooth (nonconvex) optimization problem with compact, convex constraints (see [13] for
details).

This exact same argument can be used to motivate the appearance of the trimmed Lasso penalty.
Indeed, instead of considering estimators β which are exactly k-sparse (i.e., (cid:107)β(cid:107)0 ≤ k), we instead
consider estimators which are approximately k-sparse, i.e., β = φ + (cid:15), where (cid:107)φ(cid:107)0 ≤ k and (cid:15) has
small norm. Given ﬁxed β, such a procedure is precisely

Just as the rank constraint is eliminated from (14), the sparsity constraint can be eliminated from
this to yield a continuous penalty which precisely captures the quality of the approximation β ≈ φ.
The trimmed Lasso uses the choice (cid:107) · (cid:107) = (cid:96)1, although other choices are possible; see Section 2.4.
This analogy with factor analysis is also useful in highlighting additional beneﬁts of the trimmed
Lasso. One of particular note is that it enables the direct application of existing convex optimization
techniques to ﬁnd high-quality solutions to (TLλ,k).

min
(cid:107)φ(cid:107)0≤k

(cid:107)β − φ(cid:107).

11

2.4 Generalizations

We close this section by considering some generalizations of the trimmed Lasso. These are partic-
ularly useful for connecting the trimmed Lasso to other penalties, as we will see later in Section
4.

As noted earlier, the trimmed Lasso measures the distance (in (cid:96)1 norm) from the set of k-sparse
vectors; therefore, it is natural to inquire what properties other measures of distance might carry.
In light of this, we begin with a deﬁnition:

Deﬁnition 2.4. Let k ∈ {0, 1, . . . , p} and g : R+ → R+ be any unbounded, continuous, and strictly
increasing function with g(0) = 0. Deﬁne the corresponding kth projected penalty function, denoted
πg
k, as

It is not diﬃcult to argue that πg

πg
k(β) = min
(cid:107)φ(cid:107)0≤k

(cid:88)

i

g(|φi − βi|).

k has as an equivalent deﬁnition
(cid:88)

πg
k(β) =

g(|β(i)|).

i>k

As an example, πg
k is the trimmed Lasso penalty when g is the absolute value, viz. g(x) = |x|, and
so it is a special case of the projected penalties. Alternatively, suppose g(x) = x2/2. In this case,
we get a trimmed version of the ridge regression penalty: (cid:80)

i>k |β(i)|2/2.

This class of penalty functions has one notable feature, summarized in the following result:4

Proposition 2.5. If g : R+ → R+ is an unbounded, continuous, and strictly increasing function
with g(0) = 0, then for any β, πg
(cid:107)y −

k(β) = 0 if and only if (cid:107)β(cid:107)0 ≤ k. Hence, the problem min
β

1
2

Xβ(cid:107)2

2 + λπg

k(β) converges in objective value to min
(cid:107)β(cid:107)0≤k

(cid:107)y − Xβ(cid:107)2

2 as λ → ∞.

1
2

Therefore, any projected penalty πg

k results in the best subset selection problem (1) asymp-
totically. While the choice of g as the absolute value gives the trimmed Lasso penalty and leads
to exact sparsity in the non-asymptotic regime (c.f. Theorem 2.3) , Proposition 2.5 suggests that
the projected penalty functions have potential utility in attaining approximately sparse estimators.
We will return to the penalties πg
k again in Section 4 to connect the trimmed Lasso to nonconvex
penalty methods.

Before concluding this section, we brieﬂy consider a projected penalty function that is diﬀerent
than the trimmed Lasso. As noted above, if g(x) = x2/2, then the corresponding penalty function
is the trimmed ridge penalty (cid:80)

i>k |β(i)|2/2. The estimation procedure is then
λ
2

(cid:107)y − Xβ(cid:107)2

|β(i)|2,

min
β

2 +

(cid:88)

1
2

i>k

or equivalently in decomposed form (c.f. Section 2.3),5

4An extended statement of the convergence claim is included in Appendix B.
5Interestingly, if one considers this trimmed ridge regression problem and uses convex envelope techniques [21, 60]
2 + τ (cid:107)φ(cid:107)1,

to relax the constraint (cid:107)φ(cid:107)0 ≤ k, the resulting problem takes the form minφ,(cid:15) (cid:107)y − X(φ + (cid:15))(cid:107)2
2/2 + λ(cid:107)(cid:15)(cid:107)2
a sort of “split” variant of the usual elastic net [77], another popular convex method for sparse modeling.

1
2

min
φ,(cid:15):
(cid:107)φ(cid:107)0≤k

(cid:107)y − X(φ + (cid:15))(cid:107)2

2 +

(cid:107)(cid:15)(cid:107)2
2.

λ
2

12

It is not diﬃcult to see that the variable (cid:15) can be eliminated to yield

min
(cid:107)φ(cid:107)0≤k

1
2

(cid:107)A(y − Xφ)(cid:107)2
2 ,

(15)

where A = (I−X(X(cid:48)X+λI)−1X(cid:48))1/2. It follows that the largest k loadings are found via a modiﬁed
best subset selection problem under a diﬀerent loss function—precisely a variant of the (cid:96)2 norm.
This is in the same spirit of observations made in Section 2.3.

Observation 2.6. An obvious question is whether the norm in (15) is genuinely diﬀerent. Observe
that this loss function is the same as the usual (cid:96)2
2 loss if and only if A(cid:48)A is a non-negative multiple
of the identity matrix. It is not diﬃcult to see that this is true iﬀ X(cid:48)X is a non-negative multiple
of the identity. In other words, the loss function in (15) is the same as the usual ridge regression
loss if and only if X is (a scalar multiple of ) an orthogonal design matrix.

2.5 Other applications of the trimmed Lasso: the (Discrete) Dantzig Selector

The above discussion which pertains to the least squares loss data-ﬁdelity term can be generalized
to other loss functions as well. For example, let us consider a data-ﬁdelity term given by the
maximal absolute inner product between the features and residuals, given by (cid:107)X(cid:48)(y − Xβ)(cid:107)∞. An
(cid:96)1-penalized version of this data-ﬁdelity term, popularly known as the Dantzig Selector [17, 46], is
given by the following linear optimization problem:

Estimators found via (16) have statistical properties similar to the Lasso. Further, problem (16)
may be interpreted as an (cid:96)1-approximation to the cardinality constrained version:

(cid:107)X(cid:48)(y − Xβ)(cid:107)∞ + µ(cid:107)β(cid:107)1.

min
β

min
(cid:107)β(cid:107)0≤k

(cid:107)X(cid:48)(y − Xβ)(cid:107)∞,

(16)

(17)

that is, the Discrete Dantzig Selector, recently proposed and studied in [56]. The statistical prop-
erties of (17) are similar to the best-subset selection problem (1), but may be more attractive from
a computational viewpoint as it relies on mixed integer linear optimization as opposed to mixed
integer conic optimization (see [56]).

The trimmed Lasso penalty can also be applied to the data-ﬁdelity term (cid:107)X(cid:48)(y − Xβ)(cid:107)∞,

leading to the following estimator:

(cid:107)X(cid:48)(y − Xβ)(cid:107)∞ + λTk (β) + µ(cid:107)β(cid:107)1.

min
β

Similar to the case of the least squares loss function, the above estimator yields k-sparse solutions
for any µ > 0 and for λ > 0 suﬃciently large.6 While this claim follows a fortiori by appealing to
properties of the Dantzig selector, it nevertheless highlights how any exact penalty method with a
separable penalty function can be turned into a trimmed-style problem which oﬀers direct control
over the sparsity level.

6For the same reason, but instead with the usual Lasso objective, the proof of Theorem 2.3 (see Appendix B)
could be entirely omitted; yet, it is instructive to see in the proof there that the trimmed Lasso truly does set the
smallest entries to zero, and not simply all entries (when λ is large) like the Lasso.

13

3 A perspective on robustness

We now turn our attention to a deeper exploration of the robustness properties of the trimmed
Lasso. We begin by studying the min-min robust analogue of the min-max robust SLOPE penalty;
in doing so, we show under which circumstances this analogue is the trimmed Lasso problem.
Indeed, in such a regime, the trimmed Lasso can be viewed as an optimistic counterpart to the
robust optimization view of the SLOPE penalty. Finally, we turn our attention to an additional
min-min robust interpretation of the trimmed Lasso in direct correspondence with the least trimmed
squares estimator shown in (5), using the ordinary Lasso as our starting point.

3.1 The trimmed Lasso as a min-min robust analogue of SLOPE

We begin by reconsidering the uncertainty set that gave rise to the SLOPE penalty via the min-max
view of robustness as considered in robust optimization:

(cid:26)

U λ

k :=

∆ :

∆ has at most k nonzero
columns and (cid:107)∆i(cid:107)2 ≤ λ ∀i

(cid:27)

.

As per Proposition 1.2, the min-max problem (9), viz.,

is equivalent to the SLOPE-penalized problem

min
β

max
∆∈U λ
k

1
2

(cid:107)y − (X + ∆)β(cid:107)2
2

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λRSLOPE( ˜w)(β).

for the speciﬁc choice of ˜w with ˜w1 = · · · = ˜wk = 1 and ˜wk+1 = · · · = ˜wp = 0.

Let us now consider the form of the min-min robust analogue of the the problem (9) for this
speciﬁc choice of uncertainty set. As per the discussion in Section 1, the min-min analogue takes
the form of problem (8), i.e., a variant of total least squares:

or equivalently as the linearly homogenous problem7

min
β

min
∆∈U λ
k

1
2

(cid:107)y − (X + ∆)β(cid:107)2
2,

min
β

min
∆∈U λ
k

(cid:107)y − (X + ∆)β(cid:107)2.

min
β

min
∆∈U λ
k

(cid:107)y − (X + ∆)β(cid:107)2 + r(β),

It is useful to consider problem (19) with an explicit penalization (or regularization) on β:

where r(·) is, say, a norm (the use of lowercase is to distinguish from the function R in Section 1).
As described in the following theorem, this min-min robustness problem (20) is equivalent to

the trimmed Lasso problem for speciﬁc choices of r. The proof is contained in Appendix B.

7In what follows, the linear homogeneity is useful primarily for simplicity of analysis, c.f. [9, ch. 12]. Indeed, the

conversion to linear homogeneous functions is often hidden in equivalence results like Proposition 1.2.

(18)

(19)

(20)

14

Theorem 3.1. For any k, λ > 0, and norm r, the problem (20) can be rewritten exactly as

min
β

(cid:107)y − Xβ(cid:107)2 + r(β) − λ

|β(i)|

k
(cid:88)

i=1

s. t. λ

|β(i)| ≤ (cid:107)y − Xβ(cid:107)2.

k
(cid:88)

i=1

We have the following as an immediate corollary:

Corollary 3.2. For the choice of r(β) = τ (cid:107)β(cid:107)1, where τ > λ, the problem (20) is precisely

min
β

(cid:107)y − Xβ(cid:107)2 + (τ − λ)(cid:107)β(cid:107)1 + λTk (β)

s. t. λ

|β(i)| ≤ (cid:107)y − Xβ(cid:107)2.

k
(cid:88)

i=1

(21)

In particular, when λ > 0 is small, it is approximately equal (in a precise sense)8 to the trimmed
Lasso problem

min
β

(cid:107)y − Xβ(cid:107)2 + (τ − λ)(cid:107)β(cid:107)1 + λTk (β) .

In words, the min-min problem (20) (with an (cid:96)1 regularization on β) can be written as a variant
of a trimmed Lasso problem, subject to an additional constraint. It is instructive to consider both
the objective and the constraint of problem (21). To begin, the objective has a combined penalty
on β of (τ − λ)(cid:107)β(cid:107)1 + λTk (β). This can be thought of as the more general form of the penalty Tk.
Namely, one can consider the penalty Tx (with 0 ≤ x1 ≤ x2 ≤ · · · ≤ xp ﬁxed) deﬁned as

In this notation, the objective of (21) can be rewritten as (cid:107)y − Xβ(cid:107)2 + Tx(β), with

Tx(β) :=

xi|β(i)|.

p
(cid:88)

i=1

x = (τ − λ, . . . , τ − λ
(cid:125)

(cid:124)

(cid:123)(cid:122)
k times

, τ, . . . , τ
(cid:124) (cid:123)(cid:122) (cid:125)
p−k times

).

In terms of the constraint of problem (21), note that it takes the form of a model-ﬁtting constraint:
namely, λ controls a trade-oﬀ between model ﬁt (cid:107)y − Xβ(cid:107)2 and model complexity measured via
the SLOPE norm (cid:80)k

i=1 |β(i)|.

Having described the structure of problem (21), a few remarks are in order. First of all,
the trimmed Lasso problem (with an additional (cid:96)1 penalty on β) can be interpreted as (a close
approximation to) a min-min robust problem, at least in the regime when λ is small; this provides
an interesting contrast to the sparse-modeling regime when λ is large (c.f. Theorem 2.3). Moreover,
the trimmed Lasso is a min-min robust problem in a way that is the optimistic analogue of its min-
max counterpart, namely, the SLOPE-penalized problem (18). Finally, Theorem 3.1 gives a natural
representation of the trimmed Lasso problem in a way that directly suggests why methods from
diﬀerence-of-convex optimization [2] are relevant (see Section 5).

8For a precise characterization and extended discussion, see Appendix B and Theorem B.2. The informal statement

here is suﬃcient for the purposes of our present discussion.

15

The general SLOPE penalty

Let us brieﬂy remark upon SLOPE in its most general form (with general w); again we will see that
this leads to a more general trimmed Lasso as its (approximate) min-min counterpart. In its most
general form, the SLOPE-penalized problem (18) can be written as the min-max robust problem
(9) with choice of uncertainty set

(cid:40)

U λ

w =

∆ : (cid:107)∆φ(cid:107)2 ≤ λ

wi|φ(i)| ∀φ

(cid:88)

i

(cid:41)

(see Appendix A). In this case, the penalized, homogenized min-min robust counterpart, analogous
to problem (20), can be written as follows:

Proposition 3.3. For any k, λ > 0, and norm r, the problem

min
β

min
∆∈U λ
w

(cid:107)y − (X + ∆)β(cid:107)2 + r(β)

(22)

can be rewritten exactly as

For the choice of r(β) = τ (cid:107)β(cid:107)1, where τ > λw1, the problem (22) is

(cid:107)y − Xβ(cid:107)2 + r(β) − λRSLOPE(w)(β)

min
β
s. t. λRSLOPE(w)(β) ≤ (cid:107)y − Xβ(cid:107)2.

(cid:107)y − Xβ(cid:107)2 + Tτ 1−λw(β)

min
β
s. t. λRSLOPE(w)(β) ≤ (cid:107)y − Xβ(cid:107)2.

In particular, when λ > 0 is suﬃciently small, problem (22) is approximately equal to the generalized
trimmed Lasso problem

min
β

(cid:107)y − Xβ(cid:107)2 + Tτ 1−λw(β).

Put plainly, the general form of the SLOPE penalty leads to a generalized form of the trimmed

Lasso, precisely as was true for the simpliﬁed version considered in Theorem 3.1.

3.2 Another min-min interpretation

We close our discussion of robustness by considering another min-min representation of the trimmed
Lasso. We use the ordinary Lasso problem as our starting point and show how a modiﬁcation in
the same spirit as the min-min robust least trimmed squares estimator in (5) leads directly to the
trimmed Lasso.

To proceed, we begin with the usual Lasso problem

As per Proposition 1.1, this problem is equivalent to the min-max robust problem (9) with uncer-
tainty set U = Lλ = {∆ : (cid:107)∆i(cid:107)2 ≤ λ ∀i}:

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)1.

min
β

max
∆∈Lλ

1
2

(cid:107)y − (X + ∆)β(cid:107)2
2.

16

(23)

(24)

In this view, the usual Lasso (23) can be thought of as a least squares method which takes into
account certain feature-wise adversarial perturbations of the matrix X. The net result is that the
adversarial approach penalizes all loadings equally (with coeﬃcient λ).

Using this setup and Theorem 2.3, we can re-express the trimmed Lasso problem (TLλ,k) in

the equivalent min-min form

min
β

min
I⊆{1,...,p}:
|I|=p−k

max
∆∈Lλ
I

1
2

(cid:107)y − (X + ∆)β(cid:107)2
2,

(25)

where Lλ

I ⊆ Lλ requires that the columns of ∆ ∈ Lλ

I are supported on I:

Lλ

I = {∆ : (cid:107)∆i(cid:107)2 ≤ λ ∀i, ∆i = 0 ∀i /∈ I}.

While the adversarial min-max approach in problem (24) would attempt to “corrupt” all p columns
of X, in estimating β we have the power to optimally discard k out of the p corruptions to the
columns (corresponding to I c). In this sense, the trimmed Lasso in the min-min robust form (25)
acts in a similar spirit to the min-min, robust-statistical least trimmed squares estimator shown in
problem (6).

4 Connection to nonconvex penalty methods

In this section, we explore the connection between the trimmed Lasso and existing, popular noncon-
vex (component-wise separable) penalty functions used for sparse modeling. We begin in Section
4.1 with a brief overview of existing approaches. In Section 4.2 we then highlight how these relate
to the trimmed Lasso, making the connection more concrete with examples in Section 4.3. Then
in Section 4.4 we exactly characterize the connection between the trimmed Lasso and the clipped
Lasso [76]. In doing so, we show that the trimmed Lasso subsumes the clipped Lasso; further, we
provide a necessary and suﬃcient condition for when the containment is strict. Finally, in Section
4.5 we comment on the special case of unbounded penalty functions.

4.1 Setup and Overview

Our focus throughout will be the penalized M -estimation problem of the form

L(β) +

min
β

ρ(|βi|; µ, γ),

p
(cid:88)

i=1

(26)

where µ represents a (continuous) parameter controlling the desired level of sparsity of β and γ is
a parameter controlling the quality of the approximation of the indicator function I{|β| > 0}. A
variety of nonconvex penalty functions and their description in this format is shown in Table 1 (for
a general discussion, see [75]). In particular, for each of these functions we observe that

lim
γ→∞

ρ(|β|; µ, γ) = µ · I{|β| > 0}.

It is particularly important to note the separable nature of the penalty functions appearing in
(26)—namely, each coordinate βi is penalized (via ρ) independently of the other coordinates.

Our primary focus will be on the bounded penalty functions (clipped Lasso, MCP, and SCAD),

all of which take the form

ρ(|β|; µ, γ) = µ min{g(|β|; µ, γ), 1}

(27)

17

Auxiliary Functions

g1(|β|) =

(cid:26) 2γ|β| − γ2β2,
1,

|β| ≤ 1/γ,
|β| > 1/γ.

Deﬁnition

µ min{γ|β|, 1}

µ min{g1(|β|), 1}

µ min{g2(|β|), 1}

Name
Clipped Lasso
[76]
MCP
[74]
SCAD
[33]
(cid:96)q (0 < q < 1)
[36, 37]
Log
[37]

µ|β|1/γ

g2(|β|) =






|β|/(γµ),
β2+(2/γ−4µγ)|β|+1/γ2
4µ−4µ2γ2

|β| ≤ 1/γ,

, 1/γ < |β| ≤ 2µγ − 1/γ,
1,

|β| > 2µγ − 1/γ.

µlog(γ|β| + 1)/log(γ + 1)

Table 1: Nonconvex penalty functions ρ(|β|; µ, γ) represented as in (26). The precise parametric
representation is diﬀerent than their original presentation but they are equivalent. We have taken
care to normalize the diﬀerent penalty functions so that µ is the sparsity parameter and γ corre-
sponds to the approximation of the indicator I{|β| > 0}. For SCAD, it is usually recommended to
set 2µ > 3/γ2.

where g is an increasing function of |β|. We will show that in this case, the problem (26) can be
rewritten exactly as an estimation problem with a (non-separable) trimmed penalty function:

L(β) + µ

min
β

g(|β(i)|)

p
(cid:88)

i=(cid:96)+1

for some (cid:96) ∈ {0, 1, . . . , p} (note the appearance of the projected penalties πg
k as considered in Section
2.4). In the process of doing so, we will also show that, in general, (28) cannot be solved via the
separable-penalty estimation approach of (26), and so the trimmed estimation problem leads to a
richer class of models. Throughout we will often refer to (28) (taken generically over all choices of
(cid:96)) as the trimmed counterpart of the separable estimation problem (26).

4.2 Reformulating the problem (26)

Let us begin by considering penalty functions ρ of the form (27) with g a non-negative, increasing
function of |β|. Observe that for any β we can rewrite (cid:80)p

i=1 min{g(|βi|), 1} as
(cid:41)

min

g(|β(i)|), 1 +

g(|β(i)|), . . . , p − 1 + g(|β(p)|), p

(cid:40) p

(cid:88)

i=1

p
(cid:88)

i=2

(cid:40)

(cid:41)

= min

(cid:96)∈{0,...,p}

(cid:96) +

g(|β(i)|)

.

It follows that (26) can be rewritten exactly as

min
β,
(cid:96)∈{0,...,p}

L(β) + µ

g(|β(i)|) + µ(cid:96)

(cid:88)

i>(cid:96)

(cid:33)

An immediate consequence is the following theorem:

(cid:88)

i>(cid:96)

(cid:32)

18

(28)

(29)

Theorem 4.1. If β∗ is an optimal solution to (26), where ρ(|β|; µ, γ) = µ min{g(|β|; µ, γ), 1}, then
there exists some (cid:96)∗ ∈ {0, . . . , p} so that β∗ is optimal to its trimmed counterpart

L(β) + µ

min
β

g(|β(i)|).

(cid:88)

i>(cid:96)∗

In particular, the choice of (cid:96)∗ = |{i : g(|β∗
to (29), then β∗ in an optimal solution to (26).

i |) ≥ 1}| suﬃces. Conversely, if β∗ is an optimal solution

It follows that the estimation problem (26), which decouples each loading βi in the penalty
function, can be solved using “trimmed” estimation problems of the form (28) with a trimmed
penalty function that couples the loadings and only penalizes the p − (cid:96)∗ smallest. Because the
trimmed penalty function is generally nonconvex by nature, we will focus on comparing it with
other nonconvex penalties for the remainder of the section.

4.3 Trimmed reformulation examples

We now consider the structure of the estimation problem (26) and the corresponding trimmed
estimation problem for the clipped Lasso and MCP penalties. We use the (cid:96)2

2 loss throughout.

Clipped Lasso

The clipped (or capped, or truncated) Lasso penalty [64, 76] takes the component-wise form

Therefore, in our notation, g is a multiple of the absolute value function. A plot of ρ is shown in
Figure 1a. In this case, the estimation problem with (cid:96)2

2 loss is

ρ(|β|; µ, γ) = µ min{γ|β|, 1}.

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + µ

min{γ|βi|, 1}.

(cid:88)

i

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + µγTk (β) .

(30)

(31)

It follows that the corresponding trimmed estimation problem (c.f. Theorem 4.1) is exactly the
trimmed Lasso problem studied earlier, namely,

A distinct advantage of the trimmed Lasso formulation (31) over the traditional clipped Lasso
formulation (30) is that it oﬀers direct control over the desired level of sparsity vis-`a-vis the discrete
parameter k. We perform a deeper analysis of the two problems in Section 4.4.

MCP

The MCP penalty takes the component-wise form

ρ(|β|; µ, γ) = µ min{g(|β|), 1}

where g is any function with g(|β|) = 2γ|β| − γ2β2 whenever |β| ≤ 1/γ and g(|β|) ≥ 1 whenever
|β| > 1/γ. An example of one such g is shown in Table 1. A plot of ρ is shown in Figure 1a. Another
valid choice of g is g(|β|) = max{2γ|β| − γ2β2, γ|β|}. In this case, the trimmed counterpart is

min
β

1
2

(cid:107)y − Xβ(cid:107)2 + µγ

max

(cid:110)
2|β(i)| − γβ2

(i), |β(i)|

(cid:111)

.

(cid:88)

i>(cid:96)

19

µ

0

µ

0

ρCL
ρMCP

1/γ

ρlog
ρ(cid:96)q

1

|β|

(a) Clipped Lasso and MCP

|β|

(b) Log and (cid:96)q

Figure 1: Plots of ρ(|β|; µ, γ) for some of the penalty functions in Table 1.

Note that this problem is amenable to the same class of techniques as applied to the trimmed
Lasso problem in the form (31) because of the increasing nature of g, although the subproblems
with respect to β are no longer convex (although it is a usual MCP estimation problem which is
well-suited to convex optimization approaches; see [55]). Also observe that we can separate the
penalty function into a trimmed Lasso component and another component:

(cid:88)

i>(cid:96)

|β(i)| and

(cid:88)

(cid:16)

i>(cid:96)

|β(i)| − γβ2
(i)

(cid:17)

.

+

Observe that the second component is uniformly bounded above by (p − (cid:96))/(4γ), and so as γ → ∞,
the trimmed Lasso penalty dominates.

4.4 The generality of trimmed estimation

We now turn our focus to more closely studying the relationship between the separable-penalty
estimation problem (26) and its trimmed estimation counterpart. The central problems of interest
are the clipped Lasso and its trimmed counterpart, viz., the trimmed Lasso:9

min
β

min
β

1
2

1
2

(cid:107)y − Xβ(cid:107)2

2 + µ

min{γ|βi|, 1}

(cid:88)

i

(cid:107)y − Xβ(cid:107)2

2 + λT(cid:96) (β) .

(CLµ,γ)

(TLλ,(cid:96))

As per Theorem 4.1, if β∗ is an optimal solution to (CLµ,γ), then β∗ is an optimal solution to
(TLλ,(cid:96)), where λ = µγ and (cid:96) = |{i : |β∗
i | ≥ 1/γ}|. We now consider the converse: given some λ > 0
and (cid:96) ∈ {0, 1, . . . , p} and a solution β∗ to (TLλ,(cid:96)), when does there exist some µ, γ > 0 so that β∗

9One may be concerned about the well-deﬁnedness of such problems (e.g. as guaranteed vis-`a-vis coercivity of the
objective, c.f. [60]). In all the results of Section 4.4, it is possible to add a regularizer η(cid:107)β(cid:107)1 for some ﬁxed η > 0 to
both (CLµ,γ) and (TLλ,(cid:96)) and the results remain valid, mutatis mutandis. The addition of this regularizer implies
coercivity of the objective functions and, consequently, that the minimum is indeed well-deﬁned. For completeness,
we note a technical reason for a choice of η(cid:107)β(cid:107)1 is its positive homogeneity; thus, the proof technique of Lemma 4.3
easily adapts to this modiﬁcation.

20

is an optimal solution to (CLµ,γ)? As the following theorem suggests, the existence of such a γ is
closely connected to an underlying discrete form of “convexity” of the sequence of problems (TLλ,k)
for k ∈ {0, 1, . . . , p}. We will focus on the case when λ = µγ, as this is the natural correspondence
of parameters in light of Theorem 4.1.
Theorem 4.2. If λ > 0, (cid:96) ∈ {0, . . . , p}, and β∗ is an optimal solution to (TLλ,(cid:96)), then there exist
µ, γ > 0 with µγ = λ and so that β∗ is an optimal solution to (CLµ,γ) if and only if
j − (cid:96)e
j − i

Z(TLλ,(cid:96)e) <

Z(TLλ,i) +

(cid:96)e − i
j − i

Z(TLλ,j)

(32)

for all 0 ≤ i < (cid:96)e < j ≤ p, where Z(P) denotes the optimal objective value to optimization problem
(P) and (cid:96)e = min{(cid:96), (cid:107)β∗(cid:107)0}.

Let us note why we refer to the condition in (32) as a discrete analogue of convexity of the
sequence {zk := Z(TLλ,k), k = 0, . . . , p}. In particular, observe that this sequence satisﬁes the
condition of Theorem 4.2 if and only if the function deﬁned as the linear interpolation between the
points (0, z0), (1, z1), . . . , and (p, zp) is strictly convex about the point ((cid:96), z(cid:96)).10

Before proceeding with the proof of the theorem, we state and prove a technical lemma about

the structure of (TLλ,(cid:96)).
Lemma 4.3. Fix λ > 0 and suppose that β∗ is optimal to (TLλ,(cid:96)).
(a) The optimal objective value of (TLλ,(cid:96)) is Z(TLλ,(cid:96)) = ((cid:107)y(cid:107)2
(b) If β∗ is also optimal to (TLλ,(cid:96)(cid:48)), where (cid:96) < (cid:96)(cid:48), then (cid:107)β∗(cid:107)0 ≤ (cid:96) and β∗ is optimal to (TLλ,j)

2 − (cid:107)Xβ∗(cid:107)2

2)/2.

for all integral j with (cid:96) < j < (cid:96)(cid:48).

(c) If κ := (cid:107)β∗(cid:107)0 < (cid:96), then β∗ is also optimal to (TLλ,κ), (TLλ,κ+1), . . . , and (TLλ,(cid:96)−1). Further,

β∗ is not optimal to (TLλ,0), (TLλ,1), . . . , nor (TLλ,κ−1).

Proof. Suppose β∗ is optimal to (TLλ,(cid:96)). Deﬁne

a((cid:15)) := (cid:107)y − (cid:15)Xβ∗(cid:107)2

2/2 + (cid:15)λT(cid:96) (β∗) .

By the optimality of β∗, a((cid:15)) ≥ a(1) for all (cid:15) ≥ 0. As a is a polynomial with degree at most two,
one must have that a(cid:48)(1) = 0. This implies that

a(cid:48)(1) = −(cid:104)y, Xβ∗(cid:105) + (cid:107)Xβ∗(cid:107)2

2 + λT(cid:96) (β∗) = 0.

Adding ((cid:107)y(cid:107)2

2 − (cid:107)Xβ∗(cid:107)2

2)/2 to both sides, the desired result of part (a) follows.
Now suppose that β∗ is also optimal to (TLλ,(cid:96)(cid:48)), where (cid:96)(cid:48) > (cid:96). By part (a), one must necessarily
2)/2. Inspecting Z(TLλ,(cid:96)) − Z(TLλ,(cid:96)(cid:48)), we see

2 − (cid:107)Xβ∗(cid:107)2

have that Z(TLλ,(cid:96)) = Z(TLλ,(cid:96)(cid:48)) = ((cid:107)y(cid:107)2
that

0 = Z(TLλ,(cid:96)) − Z(TLλ,(cid:96)(cid:48)) = λ

(cid:96)(cid:48)
(cid:88)

i=(cid:96)+1

|β∗

(i)|.

Hence, |β∗

((cid:96)+1)| = 0 and therefore (cid:107)β∗(cid:107)0 ≤ (cid:96).

Finally, for any integral j with (cid:96) ≤ j ≤ (cid:96)(cid:48), one always has that Z(TLλ,(cid:96)) ≥ Z(TLλ,j) ≥
Z(TLλ,(cid:96)(cid:48)). As per the preceding argument, Z(TLλ,(cid:96)) = Z(TLλ,(cid:96)) and so Z(TLλ,(cid:96)) = Z(TLλ,j),
and therefore β∗ must also be optimal to (TLλ,j) by applying part (a). This completes part (b).
Part (c) follows from a straightforward inspection of objective functions and using the fact that

Z(TLλ,j) ≥ Z(TLλ,(cid:96)) whenever j ≤ (cid:96).

10To be precise, we mean that the real-valued function that is a linear interpolation of the points has a subdiﬀerential

at the point ((cid:96), z(cid:96)) which is an interval of strictly positive width.

21

Using this lemma, we can now proceed with the proof of the theorem.

Proof of Theorem 4.2. Let zk = Z(TLλ,k) for k ∈ {0, 1, . . . , p}. Suppose that µ, γ > 0 is so that
λ = µγ and β∗ is an optimal solution to (CLµ,γ). Let (cid:96)e = min{(cid:96), (cid:107)β∗(cid:107)0}. Per equation (29), β∗
must be optimal to

min
β

min
k∈{0,...,p}

1
2

(cid:107)y − Xβ(cid:107)2

2 + µk + µγTk (β) .

(33)

Observe that this implies that if k is such that k is a minimizer of minkµk + µγTk (β∗), then β∗
must be optimal to (TLλ,k).

We claim that this observation, combined with Lemma 4.3, implies that

(cid:96)e = arg min
k∈{0,...,p}

µk + µγTk (β∗) .

This can be shown as follows:

(a) Suppose (cid:96) ≤ (cid:107)β∗(cid:107)0 and so (cid:96)e = min{(cid:96), (cid:107)β∗(cid:107)0} = (cid:96). Therefore, by Lemma 4.3(b), β∗ is not

optimal to (TLλ,j) for any j < (cid:96), and thus

min
k∈{0,...,p}

µk + µγTk (β∗) = min

µk + µγTk (β∗) .

k∈{(cid:96),...,p}

If k > (cid:96) is such that k is a minimizer of minkµk + µγTk (β∗), then β∗ must be optimal to
(TLλ,k) (using the observation), and hence by Lemma 4.3(b), (cid:107)β∗(cid:107)0 ≤ (cid:96). Combined with
(cid:96) ≤ (cid:107)β∗(cid:107)0, this implies that (cid:107)β∗(cid:107)0 = (cid:96). Yet then, µ(cid:96) = µ(cid:96) + µγT(cid:96) (β∗) < µk + µγTk (β∗),
contradicting the optimality of k. Therefore, we conclude that (cid:96)e = (cid:96) is the only minimizer of
mink µk + µγTk (β∗).

(b) Now instead suppose that (cid:96)e = (cid:107)β∗(cid:107)0 < (cid:96). Lemma 4.3(c) implies that any optimal solution
k to mink µk + µγTk (β∗) must satisfy k ≥ (cid:107)β∗(cid:107)0 (by the second part combined with the
observation). As before, if k > (cid:107)β∗(cid:107)0 = (cid:96)e, then µk > µ(cid:96)e, and so k cannot be optimal. As a
result, k = (cid:96)e = (cid:107)β∗(cid:107)0 is the unique minimum.

In either case, we have that (cid:96)e is the unique minimizer to mink µk + µγTk (β∗).

It then follows that Z(problem (33)) = z(cid:96)e +µ(cid:96)e. Further, by optimality of β∗, z(cid:96)e +µ(cid:96)e < zi +µi
for all 0 ≤ i ≤ p with i (cid:54)= (cid:96)e. For 0 ≤ i < (cid:96)e, this implies µ < (zi − z(cid:96)e)/((cid:96)e − i) and for j > (cid:96)e,
µ > (z(cid:96)e − zj)/(j − (cid:96)e). In other words, for 0 ≤ i < (cid:96)e < j ≤ p,

This completes the forward direction. The reverse follows in the same way by taking any µ with

z(cid:96)e − zj
j − (cid:96)e

<

zi − z(cid:96)e
(cid:96)e − i

,

i.e., z(cid:96)e <

j − (cid:96)e
j − i

zi +

(cid:96)e − i
j − i

zj.

(cid:18)

µ ∈

max
j>(cid:96)e

z(cid:96)e − zj
j − (cid:96)e

, min
i<(cid:96)e

zi − z(cid:96)e
(cid:96)e − i

(cid:19)

.

We brieﬂy remark upon one implication of the proof of Theorem 4.2. In particular, if β∗ is a

solution to (TLλ,(cid:96)) and (cid:96) < (cid:107)β∗(cid:107)0, then β∗ is not the solution to (TLλ,k) for any k (cid:54)= (cid:96).

An immediate question is whether the convexity condition (32) of Theorem 4.2 always holds.
While the sequence {Z(TLλ,k) : k = 0, 1, . . . , p} is always non-increasing, the following example
shows that the convexity condition need not hold in general; as a result, there exist instances of
the trimmed Lasso problem whose solutions cannot be found by solving a clipped Lasso problem.

22

i m med Lasso
T r
i p p ed Lasso
C l

Figure 2: Stylized relation of clipped Lasso and trimmed Lasso models. Every clipped Lasso model
can be written as a trimmed Lasso model, but the reverse does not hold in general.

Example 4.4. Consider the case when p = n = 2 with

y =

(cid:19)

(cid:18)1
1

and X =

(cid:19)

(cid:18) 1 −1
2

−1

.

Let λ = 1/2 and (cid:96) = 1, and consider minβ (cid:107)y − Xβ(cid:107)2
2/2 + |β(2)|/2 = minβ1,β2(1 − β1 + β2)2/2 + (1 +
β1 − 2β2)2/2 + |β(2)|/2. This has unique optimal solution β∗ = (3/2, 1) with corresponding objective
value z1 = 3/4. One can also compute z0 = Z(TL1/2,0) = 39/40 and z2 = Z(TL1/2,2) = 0. Note
that z1 = 3/4 > (39/40)/2 + (0)/2 = z0/2 + z2/2, and so there do not exist any µ, γ > 0 with
µγ = 1/2 so that β∗ is an optimal solution to (CLµ,γ) by Theorem 4.2. Further, it is possible to
show that β∗ is not an optimal solution to (CLµ,γ) for any choice of µ, γ ≥ 0. (See Appendix B.)

An immediate corollary of this example, combined with Theorem 4.1, is that the class of trimmed
Lasso models contains the class of clipped Lasso models as a proper subset, regardless of whether
we restrict our attention to λ = µγ. In this sense, the trimmed Lasso models comprise a richer set
of models. The relationship is depicted in stylized form in Figure 2.

Limit analysis

It is important to contextualize the results of this section as λ → ∞. This corresponds to γ → ∞
for the clipped Lasso problem, in which case (CLµ,γ) converges to the penalized form of subset
selection:

Note that penalized problems for all of the penalties listed in Table 1 have this as their limit as
γ → ∞. On the other hand, (TLλ,(cid:96)) converges to constrained best subset selection:

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + µ(cid:107)β(cid:107)0.

min
(cid:107)β(cid:107)0≤(cid:96)

1
2

(cid:107)y − Xβ(cid:107)2
2.

(CLµ,∞)

(TL∞,k)

Indeed, from this comparison it now becomes clear why a convexity condition of the form in Theorem
4.2 appears in describing when the clipped Lasso solves the trimmed Lasso problem. In particular,
the conditions under (CLµ,∞) solves the constrained best subset selection problem (TL∞,k) are
precisely those in Theorem 4.2.

4.5 Unbounded penalty functions

We close this section by now considering nonconvex penalty functions which are unbounded and
therefore do not take the form µ min{g(|β|), 1}. Two such examples are the (cid:96)q penalty (0 < q < 1)

23

and the log family of penalties as shown in Table 1 and depicted in Figure 1b. Estimation problems
with these penalties can be cast in the form

min
φ

1
2

(cid:107)y − Xφ(cid:107)2

2 + µ

g(|φi|; γ)

p
(cid:88)

i=1

(34)

γ→∞
−−−→

where µ, γ > 0 are parameters, g is an unbounded and strictly increasing function, and g(|φi|; γ)
I{|φi| > 0}. The change of variables in (34) is intentional and its purpose will become clear shortly.
Observe that because g is now unbounded, there exists some λ = λ(y, X, µ, γ) > 0 so that for

all λ > λ any optimal solution (φ∗, (cid:15)∗) to the problem

min
φ,(cid:15)

1
2

(cid:107)y − X(φ + (cid:15))(cid:107)2

2 + λ(cid:107)(cid:15)(cid:107)1 + µ

g(|φi|; γ)

(35)

p
(cid:88)

i=1

has (cid:15)∗ = 0.11 Therefore, (34) is a special case of (35). We claim that in the limit as γ → ∞ (all
else ﬁxed), that (35) can be written exactly as a trimmed Lasso problem (TLλ,k) for some choice
of k and with the identiﬁcation of variables β = φ + (cid:15).

We summarize this as follows:

Proposition 4.5. As γ → ∞, the penalized estimation problem (34) is a special case of the trimmed
Lasso problem.

Proof. This can be shown in a straightforward manner: namely, as γ → ∞, (35) becomes

which can be in turn written as

min
φ,(cid:15)

1
2

(cid:107)y − X(φ + (cid:15))(cid:107)2

2 + λ(cid:107)(cid:15)(cid:107)1 + µ(cid:107)φ(cid:107)0

1
2

min
φ,(cid:15):
(cid:107)φ(cid:107)0≤k

(cid:107)y − X(φ + (cid:15))(cid:107)2

2 + λ(cid:107)(cid:15)(cid:107)1

for some k ∈ {0, 1, . . . , p}. But as per the observations of Section 2.3, this is exactly (TLλ,k) using
a change of variables β = φ + (cid:15). In the case when λ is suﬃciently large, we necessarily have β = φ
at optimality.

While this result is not surprising (given that as γ → ∞ the problem is (34) is precisely penalized
best subset selection), it is useful for illustrating the connection between (34) and the trimmed
Lasso problem even when the trimmed Lasso parameter λ is not necessarily large: in particular,
(TLλ,k) can be viewed as estimating β as the sum of two components—a sparse component φ
and small-norm (“noise”) component (cid:15). Indeed, in this setup, λ precisely controls the desirable
level of allowed “noise” in β. From this intuitive perspective, it becomes clearer why the trimmed
Lasso type approach represents a continuous connection between best subset selection (λ large)
and ordinary least squares (λ small).

We close this section by making the following observation regarding problem (35). In particular,

observe that regardless of λ, we can rewrite this as

11The proof involves a straightforward modiﬁcation of an argument along the lines of that given in Theorem 2.3.

Also note that we can choose λ so that it is decreasing in γ, ceteris paribus.

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 +

(cid:101)ρ(|βi|)

p
(cid:88)

i=1

24

where (cid:101)ρ(|βi|) is the new penalty function deﬁned as

(cid:101)ρ(|βi|) = min
φ+(cid:15)=βi

λ|(cid:15)| + µg(|φ|; γ).

For the unbounded and concave penalty functions shown in Table 1, this new penalty function
is quasi-concave and can be rewritten easily in closed form. For example, for the (cid:96)q penalty
ρ(|βi|) = µ|βi|1/γ (where γ > 1), the new penalty function is

(cid:101)ρ(|βi|) = min{µ|βi|1/γ, λ|βi|}.

5 Algorithmic Approaches

We now turn our attention to algorithms for estimation with the trimmed Lasso penalty. Our
principle focus throughout will be the same problem considered in Theorem 2.3, namely

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λTk (β) + η(cid:107)β(cid:107)1

(36)

We present three possible approaches to ﬁnding potential solutions to (36): a ﬁrst-order-based
alternating minimization scheme that has accompanying local optimality guarantees and was ﬁrst
studied in [39, 72]; an augmented Lagrangian approach that appears to perform noticeably better,
despite lacking optimality guarantees; and a convex envelope approach. We contrast these methods
with approaches for certifying global optimality of solutions to (36) (described in [69]) and include
an illustrative computational example. Implementations of the various algorithms presented can
be found at

https://github.com/copenhaver/trimmedlasso.

5.1 Upper bounds via convex methods

We start by focusing on the application of convex optimization methods to ﬁnding to ﬁnding
potential solutions to (36). Technical details are contained in Appendix C.

Alternating minimization scheme

We begin with a ﬁrst-order-based approach for obtaining a locally optimal solution of (36) as
described in [39,72]. The key tool in this approach is the theory of diﬀerence of convex optimization
(“DCO”) [1, 2, 66]. Set the following notation:

f (β) = (cid:107)y − Xβ(cid:107)2
f1(β) = (cid:107)y − Xβ(cid:107)2
f2(β) = λ (cid:80)k

i=1 |β(i)|.

2/2 + λTk (β) + η(cid:107)β(cid:107)1,
2/2 + (η + λ)(cid:107)β(cid:107)1,

Let us make a few simple observations:

(a) Problem (36) can be written as min

f (β).

β

(b) For all β, f (β) = f1(β) − f2(β).

(c) The functions f1 and f2 are convex.

25

While simple, these observations enable one to apply the theory of DCO, which focuses precisely

on problems of the form

min
β

f1(β) − f2(β),

In particular, the optimality conditions for such a problem have
where f1 and f2 are convex.
been studied extensively [2]. Let us note that while it may appear that the representation of the
objective f as f1 − f2 might otherwise seem like an artiﬁcial algebraic manipulation, the min-
min representation in Theorem 3.1 shows how such a diﬀerence-of-convex representation can arise
naturally.

We now discuss an associated alternating minimization scheme (or equivalently, a sequential
linearization scheme), shown in Algorithm 1, for ﬁnding local optima of (36). The convergence
properties of Algorithm 1 can be summarized as follows:12
Theorem 5.1 ( [39], Convergence of Algorithm 1). (a) The sequence {f (β(cid:96)) : (cid:96) = 0, 1, . . .}, where

β(cid:96) are as found in Algorithm 1, is non-increasing.

(b) The set {γ(cid:96) : (cid:96) = 0, 1, . . .} is ﬁnite and eventually periodic.

(c) Algorithm 1 converges in a ﬁnite number of iterations to local minimum of (36).

(d) The rate of convergence of f (β(cid:96)) is linear.

Algorithm 1 An alternating scheme for computing a local optimum to (36)

1. Initialize with any β0 ∈ Rp ((cid:96) = 0); for (cid:96) ≥ 0, repeat Steps 2-3 until f (β(cid:96)) = f (β(cid:96)+1).

2. Compute γ(cid:96) as

3. Compute β(cid:96)+1 as

argmax
γ
s. t.

γ(cid:96) ∈

(cid:104)γ, β(cid:96)(cid:105)
(cid:88)

|γi| ≤ λk

i

|γi| ≤ λ ∀i.

(37)

β(cid:96)+1 ∈ argmin

(cid:107)y − Xβ(cid:107)2

2 + (η + λ)(cid:107)β(cid:107)1 − (cid:104)β, γ(cid:96)(cid:105).

(38)

1
2

β

Observation 5.2. Let us return to a remark that preceded Algorithm 1. In particular, we noted
that Algorithm 1 can also be viewed as a sequential linearization approach to solving (36). Namely,
this corresponds to sequentially performing a linearization of f2 (and leaving f1 as is), and then
solving the new convex linearized problem.

Further, let us note why we refer to Algorithm 1 as an alternating minimization scheme. In

particular, in light of the reformulation (43) of (36), we can rewrite (36) exactly as

12To be entirely correct, this result holds for Algorithm 1 with a minor technical modiﬁcation—see details in

Appendix C.

min
β,γ
s. t.

f1(β) − (cid:104)γ, β(cid:105)
(cid:88)

|γi| ≤ λk

(36) =

i

|γi| ≤ λ ∀i.

26

In this sense, if one takes care in performing alternating minimization in β (with γ ﬁxed) and in
γ (with β ﬁxed) (as in Algorithm 1), then a locally optimal solution is guaranteed.

We now turn to how to actually apply Algorithm 1. Observe that the algorithm is quite simple;
in particular, it only requires solving two types of well-structured convex optimization problems.
The ﬁrst such problem, for a ﬁxed β, is shown in (37). This can be solved in closed form by simply
sorting the entries of |β|, i.e., by ﬁnding |β(1)|, . . . , |β(p)|. The second subproblem, shown in (38) for
a ﬁxed γ, is precisely the usual Lasso problem and is amenable to any of the possible algorithms
for the Lasso [31, 42, 70].

Augmented Lagrangian approach

We brieﬂy mention another technique for ﬁnding potential solutions to (36) using an Alternating
Directions Method of Multiplers (ADMM) [20] approach. To our knowledge, the application of
ADMM to the trimmed Lasso problem is novel, although it appears closely related to [68]. We
begin by observing that (36) can be written exactly as

1

2 (cid:107)y − Xβ(cid:107)2

min
β,γ
s. t. β = γ,

2 + η (cid:107)β(cid:107)1 + λTk (γ)

which makes use of the canonical variable splitting. Introducing dual variable q ∈ Rp and parameter
σ > 0, this becomes in augmented Lagrangian form

min
β,γ

max
q

1
2

(cid:107)y − Xβ(cid:107)2

(cid:104)q, β − γ(cid:105) +

2 + η (cid:107)β(cid:107)1 + λTk (γ) +
σ
2

(cid:107)β − γ(cid:107)2
2 .

(39)

The utility of such a reformulation is that it is directly amenable to ADMM, as detailed in
Algorithm 2. While the problem is nonconvex and therefore the ADMM is not guaranteed to
converge, numerical experiments suggest that this approach has superior performance to the DCO-
inspired method considered in Algorithm 1.

We close by commenting on the subproblems that must be solved in Algorithm 2. Step 2 can
be carried out using “hot” starts. Step 3 is the solution of the trimmed Lasso in the orthogonal
design case and can be solved by performed by sorting p numbers; see Appendix C.

Convexiﬁcation approach

We brieﬂy consider the convex relaxation of the problem (36). We begin by computing the convex
envelope [21,60] of Tk on [−1, 1]p (here the choice of [−1, 1]p is standard, such as in the convexiﬁca-
tion of (cid:96)0 over this set which leads to (cid:96)1). The proof follows standard techniques (e.g. computing
the biconjugate [60]) and is omitted.

Lemma 5.3. The convex envelope of Tk on [−1, 1]p is the function Tk deﬁned as

In words, the convex envelope of Tk is a “soft thresholded” version of the Lasso penalty (thresh-
olded at level k). This can be thought of as an alternative way of interpreting the name “trimmed
Lasso.”

Tk(β) = ((cid:107)β(cid:107)1 − k)+ .

27

Algorithm 2 ADMM algorithm for (39)

1. Initialize with any β0, γ0, q0 ∈ Rp and σ > 0. Repeat, for (cid:96) ≥ 0, Steps 2, 3, and 4 until a

desired numerical convergence tolerance is satisﬁed.

β(cid:96)+1 ∈ argmin

(cid:107)y − Xβ(cid:107)2

2 + η(cid:107)β(cid:107)1 +

1
2

β

(cid:104)q(cid:96), β(cid:105) +

(cid:107)β − γ(cid:96)(cid:107)2
2.

σ
2

γ(cid:96)+1 ∈ argmin

λTk (γ) +

(cid:107)β(cid:96)+1 − γ(cid:107)2

2 − (cid:104)q(cid:96), γ(cid:105).

γ

σ
2

4. Set q(cid:96)+1 = q(cid:96) + σ (cid:0)β(cid:96)+1 − γ(cid:96)+1(cid:1).

2. Set

3. Set

precisely

As a result of Lemma 5.3, it follows that the convex analogue of (36), as taken over [−1, 1]p, is

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + η(cid:107)β(cid:107)1 + λ ((cid:107)β(cid:107)1 − k)+ .

(40)

Problem (40) is amenable to a variety of convex optimization techniques such as subgradient descent
[21].

5.2 Certiﬁcates of optimality for (36)

We close our discussion of the algorithmic implications of the trimmed Lasso by discussing tech-
niques for ﬁnding certiﬁably optimal solutions to (36). All approaches presented in the preceding
section ﬁnd potential candidates for solutions to (36), but none is necessarily globally optimal. Let
us return to a representation of (36) that makes use Lemma 2.1:

2 + η(cid:107)β(cid:107)1 + λ(cid:104)z, |β|(cid:105)

min
β,z
s. t.

1
2 (cid:107)y − Xβ(cid:107)2
(cid:88)
zi = p − k

i

z ∈ {0, 1}p.

As noted in [39], this representation of (36) is amenable to mixed integer optimization (“MIO”)
methods [19] for ﬁnding globally optimal solutions to (36), in the same spirit as other MIO-based
approaches to statistical problems [14, 16].

One approach, as described in [69], uses the notion of “big M .” In particular, for M > 0

suﬃciently large, problem (36) can be written exactly as the following linear MIO problem:

28

(cid:107)y − Xβ(cid:107)2

2 + η(cid:107)β(cid:107)1 + λ

(cid:88)

ai

i

min
β,z,a

s. t.

1
2
(cid:88)

i

zi = p − k

z ∈ {0, 1}p
a ≥ β + M z − M 1
a ≥ −β + M z − M 1
a ≥ 0.

(41)

This representation as a linear MIO problem enables the direct application of numerous existing
MIO algorithms (such as [40]).13 Also, let us note that the linear relaxation of (41), i.e., problem
(41) with the constraint z ∈ {0, 1}p replaced with z ∈ [0, 1]p, is the problem

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + η(cid:107)β(cid:107)1 + λ ((cid:107)β(cid:107)1 − M k)+ ,

where we see the convex envelope penalty appear directly. As such, when M is large, the linear
relaxation of (41) is the ordinary Lasso problem minβ

2 + η(cid:107)β(cid:107)1.

1
2 (cid:107)y − Xβ(cid:107)2

5.3 Computational example

Because a rigorous computational comparison is not the primary focus of this paper, we provide a
limited demonstration that describes the behavior of solutions to (36) as computed via the diﬀerent
approaches. Precise computational details are contained in Appendix C.4. We will focus on two
diﬀerent aspects: sparsity and approximation quality.

Sparsity properties

As the motivation for the trimmed Lasso is ostensibly sparse modeling, its sparsity properties are
of particular interest. We consider a problem instance with p = 20, n = 100, k = 2, and signal-to-
noise ratio 10 (the sparsity of the ground truth model βtrue is 10). The relevant coeﬃcient proﬁles
as a function of λ are shown in Figure 3. In this example none of the convex approaches ﬁnds the
optimal two variable solution computed using mixed integer optimization. Further, as one would
expect a priori, the optimal coeﬃcient proﬁles (as well as the ADMM proﬁles) are not continuous
in λ. Finally, note that by design of the algorithms, the alternating minimization and ADMM
approaches yield solutions with sparsity at most k for λ suﬃciently large.

Optimality gap

Another critical question is the degree of suboptimality of solutions found via the convex approaches.
We average optimality gaps across 100 problem instances with p = 20, n = 100, and k = 2; the
relevant results are shown in Figure 4. The results are entirely as one might expect. When λ is
small and the problem is convex or nearly convex, the heuristics perform well. However, this breaks
down as λ increases and the sparsity-inducing nature of the trimmed Lasso penalty comes into play.
Further, we see that the convex envelope approach tends to perform the worst, with the ADMM

13There are certainly other possible representations of (43), such as using special ordered set (SOS) constraints,
see e.g. [14]. Without more sophisticated tuning of M as in [14], the SOS formulations appear to be vastly superior
in terms of time required to prove optimality. The precise formulation essentially takes the form of problem (10). An
SOS-based implementation is provided in the supplementary code as the default method of certifying optimality.

29

Figure 3

30

Figure 4

performing the best of the three heuristics. This is perhaps not surprising, as any solution found
via the ADMM can be guaranteed to be locally optimal by subsequently applying the alternating
minimization scheme of Algorithm 1 to any solution found via Algorithm 2.

Computational burden

Loosely speaking, the heuristic approaches all carry a similar computational cost per iteration,
namely, solving a Lasso-like problem. In contrast, the MIO approach can take signiﬁcantly more
computational resources. However, by design, the MIO approach maintains a suboptimality gap
throughout computation and can therefore be terminated, before optimality is certiﬁed, with a
certiﬁcate of suboptimality. We do not consider any empirical analysis of runtime here.

Other considerations

There are other additional computational considerations that are potentially of interest as well,
but they are primarily beyond the scope of the present work. For example, instead of considering
optimality purely in terms of objective values in (36), there are other critical notions from a sta-
tistical perspective (e.g. ability to recover true sparse models and performance on out-of-sample
data) that would also be necessary to consider across the multiple approaches.

6 Conclusions

In this work, we have studied the trimmed Lasso, a nonconvex adaptation of Lasso that acts as an
exact penalty method for best subset selection. Unlike some other approaches to exact penalization
which use coordinate-wise separable functions, the trimmed Lasso oﬀers direct control of the desired
sparsity k. Further, we emphasized the interpretation of the trimmed Lasso from the perspective

31

of robustness. In doing so, we provided contrasts with the SLOPE penalty as well as comparisons
with estimators from the robust statistics and total least squares literature.

We have also taken care to contextualize the trimmed Lasso within the literature on nonconvex
penalized estimation approaches to sparse modeling, showing that penalties like the trimmed Lasso
can be viewed as a generalization of such approaches in the case when the penalty function is
bounded. In doing so, we also highlighted how precisely the problems were related, with a complete
characterization given in the case of the clipped Lasso.

Finally, we have shown how modern developments in optimization can be brought to bear for
the trimmed Lasso to create convex optimization optimization algorithms that can take advantage
of the signiﬁcant developments in algorithms for Lasso-like problems in recent years.

Our work here raises many interesting questions about further properties of the trimmed Lasso
and the application of similar ideas in other settings. We see two particularly noteworthy directions
of focus: algorithms and statistical properties. For the former, we anticipate that an approach like
trimmed Lasso, which leads to relatively straightforward algorithms that use close analogues from
convex optimization, is simple to interpret and to implement. At the same time, the heuristic
approaches to the trimmed Lasso presented herein carry no more of a computational burden than
solving convex, Lasso-like problems. On the latter front, we anticipate that a deeper analysis of the
statistical properties of estimators attained using the trimmed Lasso would help to illuminate it
in its own right while also further connecting it to existing approaches in the statistical estimation
literature.

Appendix A General min-max representation of SLOPE

For completeness, in this appendix we include the more general representation of the SLOPE
penalty RSLOPE(w) in the same spirit of Proposition 1.2. Here we work with SLOPE in its most
general form, namely,

RSLOPE(w)(β) =

wi|β(i)|,

p
(cid:88)

i=1

where w is a (ﬁxed) vector of weights with w1 ≥ w2 ≥ · · · ≥ wp ≥ 0 and w1 > 0.

To describe the general min-max representation, we ﬁrst set some notation. For a matrix
∆ ∈ Rn×p, we let ν(∆) ∈ Rp be the vector ((cid:107)∆1(cid:107)2, . . . , (cid:107)∆p(cid:107)2) with entries sorted so that ν1 ≥
ν2 ≥ · · · ≥ νp. As usual, for two vectors x and y, we use x ≤ y to denote that coordinate-wise
inequality holds. With this notation, we have the following:

Proposition A.1. Problem (9) with uncertainty set

Uw = {∆ : ν(∆) ≤ w}

is equivalent to problem (3) with R(β) = RSLOPE(w)(β). Further, problem (9) with uncertainty set
Uw = (cid:8)∆ : (cid:107)∆φ(cid:107)2 ≤ RSLOPE(w)(φ) ∀φ(cid:9)

is equivalent to problem (3) with R(β) = RSLOPE(w)(β).

The proof, like the proof of Proposition 1.2, follows basic techniques described in [9] and is

therefore omitted.

32

(42)

(43)

(44)

(45)

Appendix B Additional proofs

This appendix section contains supplemental proofs not contained in the main text.

Proof of Theorem 2.3. Let λ = (cid:107)y(cid:107)2 · (maxj (cid:107)xj(cid:107)2), where xj denotes the jth row of X. We ﬁx
λ > λ, k, and η > 0 throughout the entire proof. We begin by observing that it suﬃces to show
that any solution β to

satisﬁes Tk (β) = 0, or equivalently, (cid:107)β(cid:107)0 ≤ k. As per Lemma 2.1, problem (42) can be rewritten
exactly as

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λTk (β) + η(cid:107)β(cid:107)1

2 + λ(cid:104)z, |β|(cid:105) + η(cid:107)β(cid:107)1

min
β,z
s. t.

1
2 (cid:107)y − Xβ(cid:107)2
(cid:88)
zi = p − k

i

z ∈ {0, 1}p.

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λ(cid:104)z∗, |β|(cid:105) + η(cid:107)β(cid:107)1.

β∗ = proxγR

(cid:0)β∗ − γ(X(cid:48)Xβ∗ − X(cid:48)y)(cid:1) ,

Let (β∗, z∗) be any solution to (43). Observe that necessarily β∗ is also a solution to the problem

Note that, unlike (42), the problem in (44) is readily amenable to an analysis using the theory of
proximal gradient methods [7, 28]. In particular, we must have for any γ > 0 that

where R(β) = η(cid:107)β(cid:107)1 + λ

|βi|. Suppose that Tk (β∗) > 0.

In particular, for some j ∈

{1, . . . , p}, we have β∗

j = 1. Yet, as per (45),14

(cid:88)

i : z∗

i =1
j (cid:54)= 0 and z∗

(cid:12)
(cid:12)β∗

j − γ(cid:104)xj, Xβ∗ − y(cid:105)(cid:12)

(cid:12) > γ(η + λ)

for all γ > 0,

where xj denotes the jth row of X. This implies that

|(cid:104)xj, Xβ∗ − y(cid:105)| ≥ η + λ.

Now, using the deﬁnition of λ, observe that

η + λ ≤ |(cid:104)xj, Xβ∗ − y(cid:105)| ≤ (cid:107)xj(cid:107)2(cid:107)Xβ∗ − y(cid:107)2
≤ (cid:107)xj(cid:107)2(cid:107)y(cid:107) ≤ λ < λ,

which is a contradiction since η > 0. Hence, Tk (β∗) = 0, completing the proof.

14This is valid for the following reason:
j − γx(cid:48)

it must be the case that
(cid:12) > γ(η + λ), for otherwise the soft-thresholding operator at level γ(η + λ) would set this quantity

j(Xβ∗ − y)(cid:12)

(cid:54)= 0 and β∗
j

satisﬁes (45),

since β∗
j

(cid:12)
(cid:12)β∗
to zero.

33

Extended statement of Proposition 2.5

We now include a precise version of the convergence claim in Proposition 2.5. Let us set a standard
notion: we say that β is (cid:15)-optimal (for (cid:15) > 0) to an optimization problem (P) if the optimal
objective value of (P) is within (cid:15) of the objective value of β. We add an additional regularizer
η(cid:107)β(cid:107)1, for η > 0 ﬁxed, to the objective in order to ensure coercivity of the objective functions.

Proposition B.1 (Extended form of Proposition 2.5). Let g : R+ → R+ be an unbounded, contin-
uous, and strictly increasing function with g(0) = 0. Consider the problems

and

min
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λπg

k(β) + η(cid:107)β(cid:107)1

min
(cid:107)β(cid:107)0≤k

1
2

(cid:107)y − Xβ(cid:107)2

2 + η(cid:107)β(cid:107)1.

(46)

(47)

For every (cid:15) > 0, there exists some λ = λ((cid:15)) > 0 so that for all λ > λ,

1. For every optimal β∗ to (46), there is some (cid:98)β so that (cid:107)β∗ − (cid:98)β(cid:107)2 ≤ (cid:15), (cid:98)β is feasible to (47),

and (cid:98)β is (cid:15)-optimal to (47).

2. Every optimal β∗ to (47) is (cid:15)-optimal to (46).

Proof. The proof follows a basic continuity argument that is simpler than the one presented below
in Theorem B.2. For that reason, we do not include a full proof. Observe that the assumptions
If we let (cid:15) > 0 and suppose that β∗ is
on g imply that g−1 is well-deﬁned on, say, g([0, 1]).
2/(2g((cid:15)/p)), and if we deﬁne (cid:98)β to be β∗ with all but the k
optimal to (46), where λ > λ := (cid:107)y(cid:107)2
largest magnitude entries truncated to zero (ties broken arbitrarily), then πg
2/(2λ) and
k(β∗) = (cid:80)p
πg
2/(2λ)) ≤ (cid:15)/p by deﬁnition of λ. Hence,
(cid:107)β∗ − (cid:98)β(cid:107)1 ≤ (cid:15), and all the other claims essentially follow from this.

i − (cid:98)βi|) so that |β∗

i − (cid:98)βi| ≤ g−1((cid:107)y(cid:107)2

k(β∗) ≤ (cid:107)y(cid:107)2

i=1 g(|β∗

Proof of Theorem 3.1. We begin by showing that for any β,

(cid:107)y − (X + ∆)β(cid:107)2 =

(cid:107)y − Xβ(cid:107)2 − λ

|β(i)|

min
∆∈U λ
k

k
(cid:88)

i=1

(cid:33)

+

(cid:32)

where (a)+ := max{0, a}. Fix β and set r = y − Xβ. We assume without loss of generality that
r (cid:54)= 0 and that β (cid:54)= 0. For any ∆, note that (cid:107)r − ∆β(cid:107)2 ≥ 0 and (cid:107)r − ∆β(cid:107)2 ≥ (cid:107)r(cid:107)2 − (cid:107)∆β(cid:107)2 by
the reverse triangle inequality. Now observe that for ∆ ∈ U λ
k ,

(cid:107)∆β(cid:107)2 ≤

|βi|(cid:107)∆i(cid:107)2 ≤

λ|β(i)|.

(cid:88)

i

k
(cid:88)

i=1

Therefore, (cid:107)r − ∆β(cid:107)2 ≥
. Let I ⊆ {1, . . . , p} be a set of k indices which
+
correspond to the k largest entries of β (if |β(k)| = |β(k+1)|, break ties arbitrarily). Deﬁne ∆ ∈ U λ
k
as the matrix whose ith column is

i=1 |β(i)|

(cid:16)

(cid:107)r(cid:107)2 − λ (cid:80)k

(cid:17)

(cid:26) λ sgn(βi)r/(cid:107)r(cid:107)2,
0,

i ∈ I
i /∈ I,

34

(cid:110)

where λ = min
(cid:16)
(cid:107)r(cid:107)2 − λ (cid:80)k

λ, (cid:107)r(cid:107)2/
(cid:17)
i=1 |β(i)|

+

(cid:16)(cid:80)k

i=1 |β(i)|

(cid:17)(cid:111)
.

It is easy to verify that ∆ ∈ U λ

k and (cid:107)r − ∆β(cid:107)2 =

. Combined with the lower bound, we have

(cid:107)y − (X + ∆)β(cid:107)2 =

(cid:107)y − Xβ(cid:107)2 − λ

|β(i)|

min
∆∈U λ
k

k
(cid:88)

i=1

(cid:33)

+

(cid:32)

which completes the ﬁrst claim.

It follows that the problem (20) can be rewritten exactly as

To ﬁnish the proof of the theorem, it suﬃces to show that if β∗ is a solution to (48), then

(cid:32)

min
β

(cid:107)y − Xβ(cid:107)2 − λ

|β(i)|

+ r(β).

(48)

(cid:33)

+

k
(cid:88)

i=1

k
(cid:88)

i=1

(cid:107)y − Xβ∗(cid:107)2 − λ

|β∗

(i)| ≥ 0.

If this is not true, then (cid:107)y − Xβ∗(cid:107)2 − λ (cid:80)k
for 1 > (cid:15) > 0 suﬃciently small, β(cid:15) := (1 − (cid:15))β∗ satisﬁes (cid:107)y − Xβ(cid:15)(cid:107)2 − λ (cid:80)k
turn implies that

(i)| < 0 and so β∗ (cid:54)= 0. However, this implies that
i=1 |(β(cid:15))(i)| < 0. This in

i=1 |β∗

(cid:16)

(cid:107)y − Xβ(cid:15)(cid:107)2 − λ (cid:80)k
(cid:16)
<

(cid:107)y − Xβ∗(cid:107)2 − λ (cid:80)k

(cid:17)
i=1 |(β(cid:15))(i)|
(cid:17)
i=1 |β∗

(i)|

+ r(β(cid:15))
+ r(β∗),

+

+

which contradicts the optimality of β∗. (We have used the absolute homogeneity of the norm r and
that β∗ (cid:54)= 0.) Hence, any optimal β∗ to (48) necessarily satisﬁes (cid:107)y − Xβ∗(cid:107)2 − λ (cid:80)k
(i)| ≥ 0
and so the desired results follows.

i=1 |β∗

N.B. The assumption that r is a norm can be relaxed somewhat (as is clear in the proof),

although the full generality is not necessary for our purposes.

Corollary 3.2 and related discussions

Here we include a precise statement of the “approximate” claim in Corollary 3.2. After the proof,
we include a discussion of related technical issues.

Theorem B.2 (Precise statement of Corollary 3.2). For τ > λ > 0, consider the problems

and

min
β

(cid:107)y − Xβ(cid:107)2 + (τ − λ)(cid:107)β(cid:107)1 + λTk (β)

s. t. λ

|β(i)| ≤ (cid:107)y − Xβ(cid:107)2.

k
(cid:88)

i=1

min
β

(cid:107)y − Xβ(cid:107)2 + (τ − λ)(cid:107)β(cid:107)1 + λTk (β) .

(49)

(50)

For all (cid:15) > 0, there exists λ = λ((cid:15)) > 0 so that whenever λ ∈ (0, λ),

1. Every optimal β∗ to (49) is (cid:15)-optimal to (50).

35

2. For every optimal β∗ to (50), there is some (cid:98)β so that (cid:107)β∗ − (cid:98)β(cid:107)2 ≤ (cid:15), (cid:98)β is feasible to (49),

and (cid:98)β is (cid:15)-optimal to (49).

Proof. Fix τ > 0 throughout. We assume without loss of generality that y (cid:54)= 0, as otherwise the
claim is obvious. We will prove the second claim ﬁrst, as it essentially implies the ﬁrst.

Let us consider two situations. In particular, we consider whether there exists a nonzero optimal

solution to

min
β

(cid:107)y − Xβ(cid:107)2 + τ (cid:107)β(cid:107)1.

(51)

Case 1—existence of nonzero optimal solution to (51)

We ﬁrst consider the case when there exists a nonzero solution to problem (51). We show a few
lemmata:

1. We ﬁrst show that the norm of solutions to (50) are uniformly bounded away from zero,
independent of λ. To proceed, let (cid:98)β be any nonzero optimal solution to (51). Observe that
if β∗ is optimal to (50), then

(cid:107)y − Xβ∗(cid:107)2 + (τ − λ)(cid:107)β∗(cid:107)1 + λTk(β∗) ≤ (cid:107)y − X(cid:98)β(cid:107)2 + (τ − λ)(cid:107)(cid:98)β(cid:107)1 + λTk((cid:98)β)

≤ (cid:107)y − Xβ∗(cid:107)2 + τ (cid:107)β∗(cid:107)1 − λ(cid:107)(cid:98)β(cid:107)1 + λTk((cid:98)β),

implying that (cid:107)(cid:98)β(cid:107)1 − Tk((cid:98)β) ≤ (cid:107)β∗(cid:107)1 − Tk(β∗). In other words, (cid:80)k
(i)| ≤
(cid:107)β∗(cid:107)1. Using the fact that (cid:98)β (cid:54)= 0, we have that any solution β∗ to (50) has strictly positive
norm:

i=1 | (cid:98)β(i)| ≤ (cid:80)k

i=1 |β∗

(cid:107)β∗(cid:107)1 ≥ C > 0,

where C := (cid:80)k

i=1 | (cid:98)β(i)| is a universal constant depending only on τ (and not λ).

2. We now upper bound the norm of solutions to (50). In particular, if β∗ is optimal to (50),

then

(cid:107)y − Xβ∗(cid:107)2 + (τ − λ)(cid:107)β∗(cid:107)1 + λTk(β∗) ≤ (cid:107)y(cid:107)2 + 0 + 0 = (cid:107)y(cid:107)2,
and so (cid:107)β∗(cid:107)1 ≤ (cid:107)y(cid:107)2/(τ −λ). (This bound is not uniform in λ, but if we restrict our attention
to, say λ ≤ τ /2, it is.)

3. We now lower bound the loss for scaled version of optimal solutions. In particular, if σ ∈ [0, 1]

and β∗ is optimal to (50), then by optimality we have that

(cid:107)y − Xβ∗(cid:107)2 + (τ − λ)(cid:107)β∗(cid:107)1 + λTk(β∗) ≤ (cid:107)y − σXβ∗(cid:107)2 + (τ − λ)σ(cid:107)β∗(cid:107)1 + λσTk(β∗),

which in turn implies that

(cid:107)y − σXβ∗(cid:107)2 ≥ (cid:107)y − Xβ∗(cid:107)2 + (τ − λ)(1 − σ)(cid:107)β∗(cid:107)1 + λ(1 − σ)Tk(β∗)

≥ (cid:107)y − Xβ∗(cid:107)2 + (τ − λ)(1 − σ)C ≥ (τ − λ)(1 − σ)C

by combining with the ﬁrst observation.

Using these, we are now ready to proceed. Let (cid:15) > 0; we assume without loss of generality that

(cid:15) < 2(cid:107)y(cid:107)2/τ . Let

λ := min

(cid:26)

(cid:15)τ 3C
4(cid:107)y(cid:107)2(2(cid:107)y(cid:107)2 − (cid:15)τ )

,

τ
2

(cid:27)

.

36

Fix λ ∈ (0, λ) and let β∗ be any optimal solution to (50). Deﬁne

(cid:18)

σ :=

1 −

(cid:19)

(cid:15)τ
2(cid:107)y(cid:107)2

and

(cid:98)β := σβ∗.

We claim that (cid:98)β satisﬁes the desired requirements of the theorem:

1. We ﬁrst argue that (cid:107)β∗ − (cid:98)β(cid:107)2 ≤ (cid:15). Observe that

(cid:107)β∗ − (cid:98)β(cid:107)2 = (cid:15)τ (cid:107)β∗(cid:107)2/(2(cid:107)y(cid:107)2) ≤ (cid:15)τ (cid:107)β∗(cid:107)1/(2(cid:107)y(cid:107)2) ≤ (cid:15)τ (cid:107)y(cid:107)2/(2(cid:107)y(cid:107)2(τ − λ)) ≤ (cid:15).

2. We now show that (cid:98)β is feasible to (49). This requires us to argue that λ (cid:80)k

i=1 | (cid:98)β(i)| ≤

(cid:107)y − X(cid:98)β(cid:107)2. Yet,

λ

k
(cid:88)

i=1

| (cid:98)β(i)| ≤ λ(cid:107)(cid:98)β(cid:107)1 = λσ(cid:107)β∗(cid:107)1 ≤ 2λσ(cid:107)y(cid:107)2/τ ≤

(1 − σ)C

τ
2

≤ (τ − λ)(1 − σ)C ≤ (cid:107)y − σXβ∗(cid:107)2 = (cid:107)y − X(cid:98)β(cid:107)2,

as desired. The only non-obvious step is the inequality 2λσ(cid:107)y(cid:107)2/τ ≤ τ (1 − σ)C/2, which
follows from algebraic manipulations using the deﬁnitions of σ and λ.

3. Finally, we show that (cid:98)β is ((cid:15)(cid:107)X(cid:107)2)-optimal to (49). Indeed, because β∗ is optimal to (50)
which necessarily lowers bound problem (49), we have that the objective value gap between
(cid:98)β and an optimal solution to (49) is at most

(cid:107)y − σXβ∗(cid:107)2 − (cid:107)y − Xβ∗(cid:107)2 + (τ − λ)(σ − 1)(cid:107)β∗(cid:107)1 + λ(σ − 1)Tk(β∗)
≤ (1 − σ)(cid:107)Xβ∗(cid:107)2 + 0 + 0 ≤ (1 − σ)(cid:107)X(cid:107)2(cid:107)β∗(cid:107)2 ≤ 2(1 − σ)(cid:107)X(cid:107)2(cid:107)y(cid:107)2/τ
= 2(cid:15)τ /(2(cid:107)y(cid:107)2)(cid:107)X(cid:107)2(cid:107)y(cid:107)2/τ = (cid:15)(cid:107)X(cid:107)2.

As the choice of (cid:15) > 0 was arbitrary, this completes the proof of claim 2 in the theorem in the case
when 0 is not a solution to (51).

Case 2—no nonzero optimal solution to (51)

In the case when there is no nonzero optimal solution to (51), 0 is optimal and it is the only optimal
point. Our analysis will be similar to the previous approach, with the key diﬀerence being in how
we lower bound the quantity (cid:107)y − σXβ∗(cid:107)2 where β∗ is optimal to (50). Again, we have several
lemmata:

1. As before, if β∗ is optimal to (50), then (cid:107)β∗(cid:107)1 ≤ (cid:107)y(cid:107)2/(τ − λ).

2. We now lower bound the quantity (cid:107)y − σXβ∗(cid:107)2, where β∗ is optimal to (50) and σ ∈ [0, 1].

As such, consider the function

f (σ) := (cid:107)y − σXβ∗(cid:107)2 + στ (cid:107)β∗(cid:107)1.

Because f is convex in σ and the unique optimal solution to (51) is 0, we have that

f (σ) ≥ f (0) + σf (cid:48)(0) ∀σ ∈ [0, 1]

and

f (cid:48)(0) ≥ 0

37

(It is not diﬃcult to argue that f is diﬀerentiable at 0.) An elementary computation shows
that f (cid:48)(0) = τ (cid:107)β∗(cid:107)1 − (cid:104)y, Xβ∗(cid:105)/(cid:107)y(cid:107)2. Therefore, we have that

(cid:107)y − σXβ∗(cid:107)2 + στ (cid:107)β∗(cid:107)1 ≥ (cid:107)y(cid:107)2 + σ (τ (cid:107)β∗(cid:107)1 − (cid:104)y, Xβ∗(cid:105)/(cid:107)y(cid:107)2) ,

implying that

(cid:107)y − σXβ∗(cid:107)2 ≥ (cid:107)y(cid:107)2 − σ(cid:104)y, Xβ∗(cid:105)/(cid:107)y(cid:107)2 ≥ (cid:107)y(cid:107)2 − στ (cid:107)β∗(cid:107)1 ≥ (cid:107)y(cid:107)2 − στ (cid:107)y(cid:107)2/(τ − λ),

with the ﬁnal step following by an application of the previous lemma.

We are now ready to proceed. Let (cid:15) > 0; we assume without loss of generality that (cid:15) < 2(cid:107)y(cid:107)2/τ .

Let

Fix λ ∈ (0, λ) and let β∗ be any optimal solution to (50). Deﬁne

λ := min

(cid:26)

(cid:15)τ 2
4(cid:107)y(cid:107)2 − (cid:15)τ

,

τ
2

(cid:27)

.

(cid:18)

σ :=

1 −

(cid:19)

(cid:15)τ
2(cid:107)y(cid:107)2

and

(cid:98)β := σβ∗.

We claim that (cid:98)β satisﬁes the desired requirements:

1. The proof of the claim that (cid:107)β∗ − (cid:98)β(cid:107)2 ≤ (cid:15) is exactly as before.

2. We now show that (cid:98)β is feasible to (49), which requires a diﬀerent proof. Again this requires

us to argue that λ (cid:80)k

i=1 | (cid:98)β(i)| ≤ (cid:107)y − X(cid:98)β(cid:107)2. Yet,

λ

k
(cid:88)

i=1

| (cid:98)β(i)| ≤ λ(cid:107)(cid:98)β(cid:107)1 = λσ(cid:107)β∗(cid:107)1 ≤ λσ(cid:107)y(cid:107)2/(τ − λ) ≤ (cid:107)y(cid:107)2 − στ (cid:107)y(cid:107)2/(τ − λ)

≤ (cid:107)y − σXβ∗(cid:107)2 = (cid:107)y − X(cid:98)β(cid:107)2,

as desired. The only non-obvious step is the inequality λσ(cid:107)y(cid:107)2/(τ − λ) ≤ (cid:107)y(cid:107)2 − στ (cid:107)y(cid:107)2/(τ −
λ), which follows from algebraic manipulations using the deﬁnitions of σ and λ.

3. Finally, the proof that (cid:98)β is ((cid:15)(cid:107)X(cid:107)2)-optimal to (49) follows in the same way as before.

Therefore, we conclude that in the case when 0 is the unique optimal solution to (51), then

again we have that the claim 2 of the theorem holds.

Finally, we show that claim 1 holds: any solution β∗ to (49) is (cid:15)-optimal to (50). This follows
by letting β be any optimal solution to (50). By applying the entire argument above, we know that
the objective value of some (cid:98)β, feasible to (49) and close to β, is within (cid:15) of the optimal objective
value of (49), i.e., the objective value of β∗, and within (cid:15) of the objective value of (50), i.e., the
objective value of β. This completes the proof.

In short, the key complication is that the quantity (cid:107)y − Xβ∗(cid:107)2 does not need to be uniformly
bounded away from zero for solutions β∗ to problem (50). This is part of the complication of
working with the homogeneous form of the trimmed Lasso problem. For a concrete example, if
one considers the homogeneous Lasso problem with p = n = 1, y = (1), and X = (1), then the
homogeneous Lasso problem minβ (cid:107)y − Xβ(cid:107)2 + η(cid:107)β(cid:107)1 is

|1 − β| + η|β|.

min
β

38

For η ∈ [0, 1], β∗ = 1 is an optimal solution to this problem with corresponding error (cid:107)y−Xβ∗(cid:107) = 0.
If we make an assumption about the behavior of (cid:107)y − Xβ∗(cid:107), then we do not need the setup as
shown above.

Proof of Proposition 3.3. The proof is entirely analogous to that of Theorems 3.1 and B.2 and is
therefore omitted.

Proof of validity of Example 4.4. Let us consider the problem instance where p = n = 2 with

y =

(cid:19)

(cid:18)1
1

and X =

(cid:19)

(cid:18) 1 −1
2

−1

.

Let λ = 1/2 and (cid:96) = 1, and consider the problem

(cid:107)y − Xβ(cid:107)2

min
β

2 + |β(2)| = min
β1,β2

(1 − β1 + β2)2 + (1 + β1 − 2β2)2 + |β(2)|.

(52)

We have omitted the factor of 1/2 as shown in the actual example in the main text in order to
avoid unnecessary complications.

Solving problem (52) and its related counterparts (for (cid:96) ∈ {0, 2}) can rely on convex analysis
In particular, the solution to (52) is

because we can simply enumerate all possible scenarios.
β∗ = (3/2, 1) based on an analysis of two related problems:

(1 − β1 + β2)2 + (1 + β1 − 2β2)2 + |β1|.

(1 − β1 + β2)2 + (1 + β1 − 2β2)2 + |β2|.

min
β1,β2
min
β1,β2

(We should be careful to impose the additional constraints |β1| ≤ |β2| and |β1| ≥ |β2|, respectively,
although a simple argument shows that these constraints are not required in this example.) A
standard convex analysis using the Lasso (e.g. by directly using subdiﬀerentials) shows that the
problems have respective solutions (1/2, 1/2) and (3/2, 1), with the latter having the better objective
value in (52). As such, β∗ is indeed optimal. The solution in the cases of (cid:96) ∈ {0, 2} follows a similarly
standard analysis.

It is perhaps more interesting to study the general case where µ, γ ≥ 0. In particular, we will

show that β∗ = (3/2, 1) is not an optimal solution to the clipped Lasso problem

(1 − β1 + β2)2 + (1 + β1 − 2β2)2 + µ min{γ|β1|, 1} + µ min{γ|β2|, 1}

(53)

min
β1,β2

for any choices of µ and γ. While in general such a problem may be diﬃcult to fully analyze, we
can again rely on localized analysis using convex analysis. To proceed, let

f (β1, β2) = (1 − β1 + β2)2 + (1 + β1 − 2β2)2 + µ min{γ|β1|, 1} + µ min{γ|β2|, 1},

with the parameters µ and γ implicit. We consider the following exhaustive cases:

1. γ > 1 : In this case, f is convex and diﬀerentiable in a neighborhood of β∗. Its gradient at
β∗ is ∇f (β∗) = (0, −1), and therefore β∗ is neither locally optimal nor globally optimal to
problem (53).

2. γ < 2/3 : In this case, f is again convex and diﬀerentiable in a neighborhood of β∗. Its
gradient at β∗ is ∇f (β∗) = (µγ, µγ − 1). Again, this cannot equal (0, 0) and therefore β∗ is
neither locally nor globally optimal to problem (53).

39

3. 2/3 < γ < 1 : In this case, f is again convex and diﬀerentiable in a neighborhood of β∗.
Its gradient at β∗ is ∇f (β∗) = (0, µγ − 1). As a necessary condition for local optimality,
we must have that µγ = 1, implying that µ > 1. Further, if β∗ is optimal to (53), then
f (β∗) ≤ f (0, 0). Yet,

implying that µ ≤ 1/2, in contradiction of µ > 1. Hence, β∗ cannot be optimal to (53).

4. γ = 2/3 : In this case, we make two comparisons, using the points β∗, (0, 0), and (3, 2):

f (β∗) = 1/2 + µ + µγ = 3/2 + µ
f (0, 0) = 2,

f (β∗) = 1/2 + µ + 2µ/3 = 1/2 + 5µ/3
f (0, 0) = 2

f (3, 2) = 2µ.

Assuming optimality of β∗, we have that f (β∗) ≤ f (0, 0), i.e., µ ≤ 9/10; similarly, f (β∗) ≤
f (3, 2), i.e., µ ≥ 3/2. Clearly both cannot hold, and so therefore β∗ cannot be optimal.

5. γ = 1 : Finally, we see that f (β∗) ≤ f (3, 2) would imply that 1/2 + 2µ ≤ 2µ, which is
impossible; hence, β∗ is not optimal to (53). (This argument can clearly also be used in the
case when γ > 1, although it is instructive to see the argument given above in that case.)

In any case, we have that β∗ cannot be a solution to the clipped Lasso problem (53). This completes
the proof of validity of Example 4.4.

Appendix C Supplementary details for Algorithms

This appendix contains further details on algorithms as discussed in Section 5. The presentation
here is primarily self-contained. Note that the alternating minimization scheme based on diﬀerence-
of-convex optimization can be found in [39].

C.1 Alternating minimization scheme

Let us set the following notation:

f (β) = (cid:107)y − Xβ(cid:107)2
f1(β) = (cid:107)y − Xβ(cid:107)2
f2(β) = λ (cid:80)k

i=1 |β(i)|.

2/2 + λTk (β) + η(cid:107)β(cid:107)1,
2/2 + (η + λ)(cid:107)β(cid:107)1,

Deﬁnition C.1. For any function F : Rp → R and (cid:15) ≥ 0, we deﬁne the (cid:15)-subdiﬀerential of F at
β0 ∈ Rp to be the set ∂(cid:15)F (β0) deﬁned as

{γ ∈ Rp : F (β) − F (β0) ≥ (cid:104)γ, β − β0(cid:105) − (cid:15) ∀ β ∈ Rp} .

In particular, when (cid:15) = 0, we refer to ∂0F (β0) as the subdiﬀerential of F at β0, and we will denote
this as ∂F (β0).

Using this deﬁnition, we have the following result precisely characterizing local and global

optima of (36).

40

Theorem C.2. (a) A point β∗ is a local minimum of f if and only if ∂f2(β∗) ⊆ ∂f1(β∗).

(b) A point β∗ is a global minimum of f if and only if ∂(cid:15)f2(β∗) ⊆ ∂(cid:15)f1(β∗) for all (cid:15) ≥ 0.

Proof. This is a direct application of results in [66, Thm. 1]. Part (b) is immediate. The forward
implication of part (a) is immediate as well; the converse implication follows by observing that f2
is a polyhedral convex function [2, Thm. 1(ii)] (see deﬁnition therein).

Let us note that ∂f1 and ∂f2 are both easily computable, and hence, local optimality can be
veriﬁed given some candidate β∗ per Theorem C.2.15 We now discuss the associated alternating
minimization scheme (or equivalently, as a sequential linearization scheme), shown in Algorithm 1
for ﬁnding local optima of (36) by making use of Theorem C.2. Through what follows, we make
use of the standard notion of a conjugate function, deﬁned as follows:

Deﬁnition C.3. For any function F : Rp → R, we deﬁne its conjugate function F ∗ : Rp → R to
be the function

F ∗(γ) = sup
β

(cid:104)γ, β − F (β)(cid:105).

We will make the following minor technical assumption: in step 2) of Algorithm 1, we assume

without loss of generality that the γ(cid:96) so computed satisﬁes the additional criteria:

1. it is an extreme point of the relevant feasible region,

2. and that if ∂f2(β(cid:96)) (cid:54)⊆ ∂f1(β(cid:96)), then γ(cid:96) is chosen such that γ(cid:96) ∈ ∂f2(β(cid:96)) \ ∂f1(β(cid:96)).

Solving (37) with these additional assumptions can nearly be solved in closed form by simply sorting
the entries of |β|, i.e., by ﬁnding |β(1)|, . . . , |β(p)|.We must take some care to ensure that the second
without loss of generality condition on γ is satisﬁed. This is straightforward but tedious; the details
are shown in Appendix C.2.

Using this modiﬁcation, the convergence properties of Algorithm 1 can be proven as follows:

Proof of Theorem 5.1. This is an application of [66, Thms. 3-5]. The only modiﬁcation is in
requiring that γ(cid:96) is chosen so that γ(cid:96) ∈ ∂f2(β∗) \ ∂f1(β∗) if β(cid:96) is not a local minimum of f —
see [66, §3.3] for a motivation and justiﬁcation for such a modiﬁcation. Finally, the correspondence
between γ(cid:96) ∈ ∂f2(β(cid:96)) and (37), and between β(cid:96)+1 ∈ ∂f ∗
1 (γ(cid:96)) and (38), is clear from an elementary
argument applied to subdiﬀerentials of variational formulations of functions.

C.2 Algorithm 1, Step 2

Here we present the details of solving (37) in Algorithm 1 in a way that ensures that the associated
In doing so, we also implicitly study how to verify the
without loss of generality claims hold.
conditions for local optimality (c.f. Theorem C.2). Throughout, we use the sgn function deﬁned as

15For the speciﬁc functions of

interest, verifying local optimality of a candidate β∗ can be performed in
O(p min{n, p} + p log p) operations; the ﬁrst component relates to the computation of X(cid:48)Xβ∗, while the second
captures the sorting of the entries of β∗. See Appendix C.2 for details.

sgn(x) =






1, x > 0
−1, x < 0
0, x = 0.

41

For ﬁxed β, the problem of interest is

max
γ
s. t.

(cid:104)β, γ(cid:105)
(cid:88)

|γi| ≤ λk

i

|γi| ≤ λ ∀i.

We wish to ﬁnd a maximizer γ for which the following hold:

1. γ is an extreme point of the relevant feasible region,

2. and that if ∂f2(β) (cid:54)⊆ ∂f1(β), then γ is such that γ ∈ ∂f2(β) \ ∂f1(β).

As the problem on its own can be solved by sorting the entries of β, the crux of the problem is
ensuring that 2) holds.

Given the highly structured nature of f1 and f2 in our setup, it is simple, albeit tedious, to
ensure that such a condition is satisﬁed. Let I = {i : |βi| = |β(k)|}. If |I| = 1, the optimal solution
is unique, and there is nothing to show. Therefore, we will assume that |I| ≥ 2. We will construct
an optimal solution γ which satisﬁes the desired conditions. First observe that we necessarily must
have that 1) γi = λ sgn(βi) if |βi| > |β(k)| and 2) γi = 0 if |βi| < |β(k)|. We now proceed to deﬁne
the rest of the entries of γ. We consider two cases:

1. First consider the case when |β(k)| > 0. We claim that ∂f2(β) (cid:54)⊆ ∂f1(β). To do so, we will
inspect the ith entries of ∂f1(β) for i ∈ I; as such, let P j
i = {δi : δ ∈ ∂fj(β)} for j ∈ {1, 2}
and i ∈ I (a projection). For each i ∈ I, we have using basic convex analysis that P 1
is
i
a singelton: P 1
i = {(cid:104)Xi, Xβ − y(cid:105) + (η + λ) sgn(βi)}, where Xi is the ith column of X. In
contrast, because |I| ≥ 2, the set P 2
is an interval with strictly positive length for each i ∈ I
i
(it is either [−λ, 0] or [0, λ], depending on whether βi < 0 or βi > 0, respectively). Therefore,
∂f2(β) (cid:54)⊆ ∂f1(β), as claimed.

Fix an arbitrary j ∈ I. Per the above argument, we must have that (cid:104)Xj, Xβ − y(cid:105) + (η +
λ) sgn(βj) (cid:54)= 0 or (cid:104)Xj, Xβ − y(cid:105) + (η + λ) sgn(βj) (cid:54)= λ sgn(βj). In the former case, set γi = 0,
while in the latter case we deﬁne γi = λ sgn(βi) (if both are true, either choice suﬃces). It is
clear that it is possible to ﬁll in the remaining entries of γi for i ∈ I \ {j} in a straightforward
manner so that γ ∈ ∂f2(β). Further, by construction, γ /∈ ∂f1(β), as desired.

i ⊆ P 1

2. Now consider the case when |β(k)| = 0. Using the preceding argument, we see that P 1
is the
i
interval [(cid:104)Xi, Xβ − y(cid:105) − (η + λ), (cid:104)Xi, Xβ − y(cid:105) + η + λ] for i ∈ I. In contrast, P 2
is the interval
i
[−λ, λ] for i ∈ I. If for all i ∈ I one has that P 2
i , then the choice of γi for i ∈ I is
obvious: any optimal extreme point γ of the problem will suﬃce. (Note here that it may or
may not be that ∂f2(β) ⊆ ∂f1(β). This entirely depends on βi for i /∈ I.)
Therefore, we may assume that there exists some j ∈ I so that P 2
(It follows
j
immediately that ∂f2(β) (cid:54)⊆ ∂f1(β).) We must have that (cid:104)Xj, Xβ − y(cid:105) − (η + λ) > −λ
or (cid:104)Xj, Xβ − y(cid:105) + (η + λ) < λ. In the former case, set γi = −λ, while in the latter case we
deﬁne γi = λ (if both are true, either choice suﬃces). It is clear that it is possible to ﬁll in
the remaining entries of γi for i ∈ I \ {j} in a straightforward manner so that γ ∈ ∂f2(β).
By construction, γ /∈ ∂f1(β), as desired.

(cid:54)⊆ P 1
j .

In either case, we have that one can choose γ ∈ ∂f2(β) so that 1) γ is an extreme point
i |γi| ≤ λk, |γi| ≤ λ ∀i} and that 2) γ ∈ ∂f2(β) \ ∂f1(β) whenever

of the feasible region {γ : (cid:80)

42

∂f2(β) (cid:54)⊆ ∂f1(β). This concludes the analysis; thus, we have shown the validity (and computational
feasibility) of the without loss of generality claim present in Algorithm 1. Indeed, per our analysis,
Step 2 in Algorithm 1 can be solved in O(p min{n, p} + p log p) operations (sorting of β in O(p log p)
followed by O(p) conditionals and gradient evaluation in O(np)). In reality, if we keep track of
gradients in Step 3, there is no need to recompute gradients in Step 2, and therefore in practice
Step 2 is of the same complexity of sorting a list of p numbers. (We assume that X(cid:48)y has been
computed oﬄine and store throughout for simplicity.)

C.3 Algorithm 2, Step 3

Here we show how to solve Step 3 in Algorithm 2, namely, solving the orthogonal design trimmed
Lasso problem

min
γ

λTk (γ) +

σ
2

(cid:107)β − γ(cid:107)2

2 − (cid:104)q, γ(cid:105),

(54)

where β and q are ﬁxed. This is solvable in closed form. Let α = β − q/σ. First observe that we
can rewrite (54) as

(54) = min

λTk (γ) + σ(cid:107)γ − α(cid:107)2

2/2

γ

λ(cid:104)z, |γ|(cid:105) + σ(cid:107)γ − α(cid:107)2

2/2

= min
γ,z:
(cid:80)
i zi=p−k
z∈{0,1}p

= min
γ,z:
(cid:80)
i zi=p−k
z∈{0,1}p

(cid:88)

i

(cid:0)λzi|γ| + σ(γi − αi)2/2(cid:1) .

The penultimate step follows via Lemma 2.1. Per this ﬁnal representation, the solution becomes
(If
clear.
|α(k)| = |α(k+1)|, we break ties arbitrarily.) Then a solution γ∗ to (54) is

In particular, let I be a set of k indices of α corresponding to α(1), α(2), . . . , α(k).

(cid:26)

γ∗
i =

αi,
softλ/σ(αi),

i ∈ I
i /∈ I,

where softλ/σ(αi) = sgn(αi) |αi − λ/σ|.

C.4 Computational details

For completeness and reproducibility, we also include all computational details. For Figure 3, the
following parameters were used to generate the test instance: n = 100, p = 20, SNR = 10, julia
seed = 1, η = 0.01, k = 2. The example was generated from the following true model:

1. βtrue is a vector with ten entries equal to 1 and all others equal to zero. (So (cid:107)βtrue(cid:107)0 = 10.)

2. covariance matrix Σ is generated with Σij = .8|i−j|.

3. X ∼ N (0, Σ).

4. (cid:15)i

i.i.d.∼ N (0, β(cid:48)

0Σβ0/SNR)

5. y is then deﬁned as Xβ0 + (cid:15)

43

The 100 examples generated for Figure 4 were using the following parameters: n = 100, p = 20,
SNR = 10, julia seed ∈ {1, . . . , 100}, η = 0.01, k = 2, bigM = 20. MIO using Gurobi solver.
Max iterations: alternating minimization—1000; ADMM (inner)—2000; ADMM (outer)—10000.
ADMM parameters: σ = 1, τ = 0.9. The examples themselves had the same structure as the
previous example. The optimal gaps shown are relative to the objective in (36). The averages
are computed as geometric means (relative to optimal 100%) across the 100 instances, and then
displayed relative to the optimal 100%.

Copenhaver was partially supported by the Department of Defense, Oﬃce of Naval Research,
through the National Defense Science and Engineering Graduate (NDSEG) Fellowship. Mazumder
was partially supported by ONR Grant N000141512342.

Acknowledgments

References

[1] L. T. H. An, “Analyse num´erique des algorithmes de l’optimisation DC. Approches locale et
globale. Codes et simulations num´eriques en grande dimension. Applications,” Ph.D. disserta-
tion, Universit´e de Rouen, 1994.

[2] L. T. H. An and P. D. Tao, “The DC (diﬀerence of convex functions) programming and
DCA revisited with DC models of real world nonconvex optimization problems,” Annals of
Operations Research, vol. 133, pp. 23–46, 2005.

[3] T. Anderson, An Introduction to Multivariate Statistical Analysis, 3rd ed. Wiley, New York,

2003.

[4] R. Andreani, L. Secchin, and P. Silva, “Convergence properties of a second order augmented
Lagrangian method for mathematical programs with complementarity constraints,” 2017.

[5] A. Bandeira, E. Dobriban, D. Mixon, and W. Sawin, “Certifying the Restricted Isometry
Property is hard,” IEEE Transactions in Information Theory, vol. 59, pp. 3448–3450, 2013.

[6] D. Bartholomew, M. Knott, and I. Moustaki, Latent variable models and factor analysis: a

uniﬁed approach. Wiley, 2011.

spaces. Springer, 2011.

[7] H. Bauschke and P. Combettes, Convex analysis and monotone operator theory in Hilbert

[8] A. Beck and A. Ben-Tal, “Duality in robust optimization: primal worst equals dual best,”

Operations Research Letters, vol. 37, no. 1, pp. 1–6, 2009.

[9] A. Ben-Tal, L. E. Ghaoui, and A. Nemirovski, Robust Optimization. Princeton University

Press, 2009.

[10] D. Bertsekas, “Multiplier methods: a survey,” Automatica, vol. 12, no. 2, pp. 133–145, 1976.

[11] ——, Constrained optimization and Lagrange multiplier methods. Academic Press, 2014.

[12] D. Bertsimas, D. Brown, and C. Caramanis, “Theory and applications of robust optimization,”

SIAM Review, vol. 53, no. 3, pp. 464–501, 2011.

44

[13] D. Bertsimas, M. S. Copenhaver, and R. Mazumder, “Certiﬁably optimal low rank factor

analysis,” Journal of Machine Learning Research, vol. 18, no. 29, pp. 1–53, 2017.

[14] D. Bertsimas, A. King, and R. Mazumder, “Best subset selection via a modern optimization

lens,” The Annals of Statistics, vol. 44, no. 2, pp. 813–852, 2016.

[15] D. Bertsimas and M. S. Copenhaver, “Characterization of the equivalence of robustiﬁcation
and regularization in linear and matrix regression,” European Journal of Operational Research,
2017.

[16] D. Bertsimas and R. Mazumder, “Least quantile regression via modern optimization,” The

Annals of Statistics, vol. 42, no. 6, pp. 2494–2525, 2014.

[17] P. J. Bickel, Y. Ritov, and A. B. Tsybakov, “Simultaneous analysis of lasso and dantzig

selector,” The Annals of Statistics, pp. 1705–1732, 2009.

[18] M. Bogdan, E. van den Berg, C. Sabatti, W. Su, and E. Cand`es, “SLOPE: Adaptive variable
selection via convex optimization,” Annals of Applied Statistics, vol. 9, pp. 1103–1140, 2015.

[19] P. Bonami, M. Kilinc, and J. Linderoth, Mixed integer nonlinear programming.

Springer,

2012, ch. Algorithms and software for convex mixed integer nonlinear programs.

[20] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and sta-
tistical learning via the alternating direction method of multipliers,” Foundations and Trends
in Machine Learning, pp. 1–122, 2011.

[21] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge University Press, 2004.

[22] M. Branda, M. Bucher, M. ˇCervinka, and A. Schwartz, “Convergence of a scholtes-type reg-
ularization method for cardinality-constrained optimization problems with an application in
sparse robust portfolio optimization,” arXiv preprint arXiv:1703.10637, 2017.

[23] P. B¨uhlmann and S. Van De Geer, Statistics for high-dimensional data: methods, theory and

applications. Springer, 2011.

[24] O. P. Burdakov, C. Kanzow, and A. Schwartz, “Mathematical programs with cardinality
constraints: Reformulation by complementarity-type conditions and a regularization method,”
SIAM Journal on Optimization, vol. 26, no. 1, pp. 397–425, 2016.

[25] E. Cand`es, J. Romberg, and T. Tao, “Stable signal recovery from incomplete and inaccurate
measurements,” Communications in Pure and Applied Mathematics, vol. 59, pp. 1207–1223,
2005.

[26] E. Cand`es, X. Li, Y. Ma, and J. Wright, “Robust Principal Component Analysis?” Journal

of the ACM, vol. 58, no. 3, pp. 11:1–37, 2011.

[27] B. Colson, P. Marcotte, and G. Savard, “An overview of bilevel optimization,” Annals of

Operations Research, vol. 153, no. 1, pp. 235–256, 2007.

[28] P. Combettes and V. Wajs, “Signal recovering by proximal forward-backward splitting,” Mul-

tiscale Modeling and Simulation, vol. 4, no. 4, pp. 1168–200, 2005.

[29] H. Dong, M. Ahn, and J.-S. Pang, “Structural properties of aﬃne sparsity constraints,” Opti-

mization Online, 2017.

45

[30] D. Donoho, “Compressed sensing,” IEEE Transactions in Information Theory, vol. 52, pp.

1289–1306, 2006.

[31] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani, “Least angle regression,” The Annals of

Statistics, vol. 32, no. 2, pp. 407–99, 2004.

[32] Y. Eldar and G. Kutyniok, Eds., Compressed sensing: theory and applications. Cambridge

University Press, 2012.

[33] J. Fan and R. Li, “Variable selection via nonconcave penalized likelihood and its oracle prop-

erties,” Journal of the American Statistical Association, vol. 96, pp. 1348–1360, 2001.

[34] M. Feng, J. E. Mitchell, J.-S. Pang, X. Shen, and A. W¨achter, “Complementarity formula-
tions of (cid:96)0-norm optimization problems,” Industrial Engineering and Management Sciences.
Technical Report. Northwestern University, Evanston, IL, USA, 2013.

[35] M. Figueiredo and R. Nowak, “Sparse estimation with strongly correlated variables using

ordered weighted (cid:96)1 regularization,” arXiv preprint arXiv:1409.4005, 2014.

[36] I. Frank and J. Friedman, “A statistical view of some chemometrics regression tools,” Tech-

nometrics, vol. 35, pp. 109–148, 1993.

[37] J. Friedman, “Fast sparse regression and classiﬁcation,” 2008, technical report, Department of

Statistics, Stanford University.

[38] G. Golub and C. V. Loan, “An analysis of the total least squares problem,” SIAM Journal of

Numerical Analysis, vol. 17, no. 6, pp. 883–893, 1980.

[39] J.-Y. Gotoh, A. Takeda, and K. Tono, “DC formulations and algorithms for sparse optimization

problems,” Preprint, METR, vol. 27, 2015.

[40] Gurobi Optimization, Inc., “Gurobi optimizer reference manual,” 2016. [Online]. Available:

http://www.gurobi.com

pp. 297–308, 1958.

[41] L. Guttman, “To what extent can communalities reduce rank?” Psychometrika, vol. 23, no. 4,

[42] T. Hastie, R. Tibshirani, and J. Friedman, The elements of statistical learning: data mining,

inference, and prediction. Springer, 2009.

[43] A. B. Hempel and P. J. Goulart, “A novel method for modelling cardinality and rank con-
straints,” in Decision and Control (CDC), 2014 IEEE 53rd Annual Conference on, 2014, pp.
4322–4327.

[44] T. Hoheisel, C. Kanzow, and A. Schwartz, “Theoretical and numerical comparison of relax-
ation methods for mathematical programs with complementarity constraints,” Mathematical
Programming, pp. 1–32, 2013.

[45] P. Huber and E. Ronchetti, Robust statistics, 2nd ed. Wiley, 2009.

[46] G. M. James, P. Radchenko, and J. Lv, “DASSO: connections between the dantzig selector
and lasso,” Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 71,
no. 1, pp. 127–142, 2009.

46

[47] C. Kanzow and A. Schwartz, “A new regularization method for mathematical programs with
complementarity constraints with strong convergence properties,” SIAM Journal on Optimiza-
tion, vol. 23, no. 2, pp. 770–798, 2013.

[48] ——, “The price of inexactness: convergence properties of relaxation methods for mathematical
programs with complementarity constraints revisited,” Mathematics of Operations Research,
vol. 40, no. 2, pp. 253–275, 2014.

[49] K. Klamroth, E. K¨obis, A. Sch¨obel, and C. Tammer, “A uniﬁed approach to uncertain opti-
mization,” European Journal of Operational Research, vol. 260, no. 2, pp. 403–420, 2017.

[50] G.-H. Lin and M. Fukushima, “A modiﬁed relaxation scheme for mathematical programs with
complementarity constraints,” Annals of Operations Research, vol. 133, no. 1, pp. 63–84, 2005.

[51] H. Liu, T. Yao, and R. Li, “Global solutions to folded concave penalized nonconvex learning,”

Annals of Statistics, vol. 44, no. 2, pp. 629–659, 2016.

[52] H. Liu, T. Yao, R. Li, and Y. Ye, “Folded concave penalized sparse linear regression: Sparsity,
statistical performance, and algorithmic theory for local solutions,” Mathematical Program-
ming, pp. 1–34, 2016.

[53] K. Mardia, J. Kent, and J. Bibby, Multivariate analysis. Academic Press, 1979.

[54] I. Markovsky and S. V. Huﬀel, “Overview of total least-squares methods,” Signal Processing,

vol. 87, pp. 2283–2302, 2007.

[55] R. Mazumder, J. Friedman, and T. Hastie, “SparseNet: Coordinate descent with nonconvex
penalties,” Journal of the American Statistical Association, vol. 106, pp. 1125–1138, 2011.

[56] R. Mazumder and P. Radchenko, “The discrete Dantzig selector: Estimating sparse linear
models via mixed integer linear optimization,” IEEE Transactions on Information Theory,
vol. 63, no. 5, pp. 3053–3075, 2017.

[57] A. Miller, Subset selection in regression. CRC Press, 2002.

[58] S. Morgenthaler, “A survey of robust statistics,” Statistical Methods and Applications, vol. 15,

pp. 271–293, 2007.

[59] M. Osborne, B. Presnell, and B. Turlach, “On the lasso and its dual,” Journal of Computational

and Graphical Statistics, vol. 9, no. 2, pp. 319–337, 2000.

[60] R. Rockafeller, Convex analysis. Princeton University Press, 1970.

[61] P. Rousseeuw and A. Leroy, Robust regression and outlier detection. Wiley, 1987.

[62] S. Scholtes and M. St¨ohr, “Exact penalization of mathematical programs with equilibrium

constraints,” SIAM Journal on Control and Optimization, vol. 37, no. 2, pp. 617–652, 1999.

[63] A. Shapiro, “Rank-reducability of a symmetric matrix and sampling theory of minimum trace

factor analysis,” Psychometrika, vol. 47, pp. 187–199, 1982.

[64] X. Shen, W. Pan, and Y. Zhu, “Likelihood-based selection and sharp parameter estimation,”

Journal of the American Statistical Association, vol. 107, no. 497, pp. 223–232, 2012.

47

[65] X. Shen, W. Pan, Y. Zhu, and H. Zhou, “On constrained and regularized high-dimensional
regression,” Annals of the Institute of Statistical Mathematics, vol. 65, no. 5, pp. 807–832,
2013.

[66] P. D. Tao and L. T. H. An, “Convex analysis approach to DC programming: theory, algorithms,

and applications,” Acta Mathematica Vietnamica, vol. 22, pp. 287–355, 1997.

[67] J. Ten-Berge, “Some recent developments in factor analysis and the search for proper commu-
nalities,” in Advances in data science and classiﬁcation. Springer, 1998, pp. 325–334.

[68] Y. Teng, L. Yang, B. Yu, and X. Song, “An augmented Lagrangian proximal alternating

method for sparse discrete optimization problems,” Optimization Online, 2017.

[69] M. Thiao, P. D. Tao, and L. An, “A DC programming approach for sparse eigenvalue problem,”
in Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010,
pp. 1063–1070.

[70] R. Tibshirani, “Regression shrinkage and selection via the Lasso,” Journal of the Royal Sta-

tistical Society, Series B, vol. 58, pp. 267–288, 1996.

[71] A. Tillman and M. Pfetsch, “The computational complexity of the Restricted Isometry Prop-
erty, the nullspace property, and related concepts in compressed sensing,” IEEE Transactions
in Information Theory, vol. 60, pp. 1248–1259, 2014.

[72] K. Tono, A. Takeda, and J.-Y. Gotoh, “Eﬃcient DC algorithm for constrained sparse opti-

mization,” arXiv preprint arXiv:1701.08498, 2017.

[73] H. Xu, C. Caramanis, and S. Mannor, “Robust regression and Lasso,” IEEE Transactions in

Information Theory, vol. 56, no. 7, pp. 3561–74, 2010.

[74] C. Zhang, “Nearly unbiased variable selection under minimax concave penalty,” The Annals

of Statistics, vol. 38, pp. 894–942, 2010.

[75] C.-H. Zhang and T. Zhang, “A general theory of concave regularization for high-dimensional

sparse estimation problems,” Statistical Science, pp. 576–593, 2012.

[76] T. Zhang, “Analysis of multi-stage convex relaxation for sparse regularization,” Journal of

Machine Learning Research, vol. 11, pp. 1081–1107, 2010.

[77] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,” Journal of

the Royal Statistical Society: Series B, vol. 67, no. 2, pp. 301–320, 2005.

48

