Not to Cry Wolf:
Distantly Supervised Multitask Learning in Critical Care

Patrick Schwab 1 Emanuela Keller 2 Carl Muroi 2 David J. Mack 2 Christian Str¨assle 2 Walter Karlen 1

8
1
0
2
 
n
u
J
 
7
 
 
]

G
L
.
s
c
[
 
 
2
v
7
2
0
5
0
.
2
0
8
1
:
v
i
X
r
a

Abstract
Patients in the intensive care unit (ICU) require
constant and close supervision. To assist clinical
staff in this task, hospitals use monitoring sys-
tems that trigger audiovisual alarms if their al-
gorithms indicate that a patient’s condition may
be worsening. However, current monitoring sys-
tems are extremely sensitive to movement arte-
facts and technical errors. As a result,
they
typically trigger hundreds to thousands of false
alarms per patient per day - drowning the impor-
tant alarms in noise and adding to the exhaustion
of clinical staff. In this setting, data is abundantly
available, but obtaining trustworthy annotations
by experts is laborious and expensive. We frame
the problem of false alarm reduction from multi-
variate time series as a machine-learning task and
address it with a novel multitask network archi-
tecture that utilises distant supervision through
multiple related auxiliary tasks in order to reduce
the number of expensive labels required for train-
ing. We show that our approach leads to sig-
niﬁcant improvements over several state-of-the-
art baselines on real-world ICU data and provide
new insights on the importance of task selection
and architectural choices in distantly supervised
multitask learning.

1. Introduction

False alarms are an enormous mental burden for clinical
staff and are extremely dangerous to patients, as alarm
fatigue and desensitisation may lead to clinically important
alarms being missed (Drew et al., 2014). Reportedly,
several hundreds of deaths a year are associated with false
alarms in patient monitoring in the United States alone
(Cvach, 2012).

1Institute of Robotics and Intelligent Systems, ETH Zurich,
Switzerland 2Neurocritical Care Unit, Department of Neuro-
surgery, University Hospital Zurich, Switzerland. Correspon-
dence to: Patrick Schwab <patrick.schwab@hest.ethz.ch>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

Figure 1. Schematic overview of the described problem setting in
critical care. Before an alarm is brought to the attention of clini-
cal staff, an alarm classiﬁcation algorithm could analyse a recent
window of the full set of available data streams in order to identify
whether the alarm was likely caused by an artefact or technical er-
ror and may therefore be reported with a lower degree of urgency.

An intelligent alarm classiﬁcation system could potentially
reduce the burden of a large subset of those false alarms
by assessing which alarms were likely caused by either an
artefact or a technical error and reporting those alarms with
a lower degree of urgency (Figure 1). Roadblocks that have
so far prevented the adoption of machine learning for this
task are the heterogeneity of monitoring systems, the re-
quirement for an extremely high speciﬁcity, to avoid sup-
pressing important alarms, and the prohibitively high cost
associated with obtaining a representative set of clinically
validated labels for each of the manifold alarm types and
monitoring system conﬁgurations in use at hospitals.

We present a semi-supervised approach to false alarm re-
duction that automatically identiﬁes and incorporates a
large amount of distantly supervised auxiliary tasks in or-
der to signiﬁcantly reduce the number of expensive labels
required for training. We demonstrate, on real-world ICU
data, that our approach is able to correctly classify alarms
originating from artefacts and technical errors better than
several state-of-the-art methods for semi-supervised learn-
ing when using just 25, 50 and 100 labelled samples. Be-
sides their importance for clinical practice, our results high-
light the power of distant multitask supervision as a ﬂexi-
ble and effective tool for learning when unlabelled data are
readily available, and shed new light on semi-supervised
learning beyond low-resolution image benchmark datasets.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

Contributions. We subdivide this work along the follow-
ing distinct contributions:

(i) We introduce DSMT-Nets: A novel neural architec-
ture built on the idea of utilising distant supervision
through multiple auxiliary tasks in order to better har-
ness unlabelled data.

(ii) We present a methodology for selecting a large set of
related auxiliary tasks in time series, and a training
procedure that counteracts adverse gradient interac-
tions between auxiliary tasks and the main task.
(iii) We perform extensive quantitative experiments on a
real-world ICU dataset consisting of almost 14,000
alarms in order to evaluate the relative classiﬁcation
performance and label efﬁciency of DSMT-Nets com-
pared to several state-of-the-art methods.

2. Related Work

Background. Driven by widespread efforts to automate
patient monitoring, there has been a recent surge in works
applying machine learning to the vast amounts of data gen-
erated in ICUs. One notable driver is the MIMIC (Saeed
et al., 2011) dataset that has made ICU data accessible to
a large number of researchers. Related works have, for ex-
ample, explored the use of ICU data for tasks such as mor-
tality modelling (Ghassemi et al., 2014), illness assessment
and forecasting (Ghassemi et al., 2015), diagnostic support
(Lipton et al., 2016a), patient state prediction (Cheng et al.,
2017) and learning weaning policies for mechanical ven-
tilation (Prasad et al., 2017). Applying machine-learning
approaches to clinical and physiological data is challeng-
ing, because it is heterogenous, noisy, confounded, sparse
and of high temporal resolution over long periods of time.
These properties are in stark contrast to many of the bench-
mark datasets that machine-learning approaches are typi-
cally developed and evaluated on. Several works therefore
deal with adapting existing machine-learning approaches to
the idiosyncrasies of clinical and physiological data, such
as missingness (Lipton et al., 2016b; Che et al., 2016),
long-term temporal dependencies (Choi et al., 2016), noise
(Schwab et al., 2017), heterogeneity (Libbrecht & Noble,
2015) and sparsity (Lasko et al., 2013). We build on sev-
eral of these innovations in this work.

Alarm Fatigue. The PhysioNet 2015 challenge on false
alarm reduction in electrocardiography (ECG) monitoring
(Clifford et al., 2015) was one of the most notable efforts
to date to address the issue of false alarms in physiological
monitoring. Within the challenge, researchers introduced
several effective approaches to reducing the false alarm rate
of arrhythmia alerts in ECGs (Fallet et al., 2015; Eerik¨ainen
et al., 2015; Krasteva et al., 2016; Plesinger et al., 2016).
However, clinicians in the ICU do not just monitor for ar-
rhythmias, but many adverse events at once using a multi-

tude of different monitoring systems. Typically, these mon-
itoring systems operate in isolation on a single biosignal
and trigger their own distinct sets of alarms. Previous re-
search has shown that there is an opportunity to use data
from other biosignals to identify false alarms in related
waveforms (Aboukhalil et al., 2008). We therefore believe
that a comprehensive solution to alarm fatigue requires an
approach that accounts for the monitoring setup as a whole,
rather than targeting speciﬁc systems or alarms in isolation.

Distant Supervision and Multitask Learning. Multitask
learning has a rich history in healthcare applications and
has, for example, been used for risk prediction in neonatal
intensive care (Saria et al., 2010), drug discovery (Ram-
sundar et al., 2015) and prediction of Clostridium difﬁcile
(Wiens et al., 2016). A way of leveraging multitask learn-
ing to improve label-efﬁciency is to learn jointly from com-
plementary unsupervised auxiliary tasks along with the su-
pervised main task. Existing literature refers to the concept
of applying indirect supervision through auxiliary tasks, be
it for label-efﬁciency or additional predictive performance,
as weak supervision (Papandreou et al., 2015; Oquab et al.,
2015), distant supervision (Zeng et al., 2015; Deriu et al.,
2017) or self-supervision (Fernando et al., 2017; Doersch
& Zisserman, 2017). In particular, (Doersch & Zisserman,
2017) used distantly supervised multitask learning to in-
crease predictive performance in computer vision with up
to four hand-engineered auxiliary tasks. Using auxiliary
tasks in addition to a main task has also been shown to be
a promising approach in reinforcement (Jaderberg et al.,
2016; Aytar et al., 2018) and adversarial learning (Salimans
et al., 2016). Recently, (Laine & Aila, 2017) proposed
to use outputs from the same model at different points in
training and with varying amounts of regularisation as ad-
ditional unsupervised targets for the main task. In contrast
to existing works, we present the ﬁrst approach to distantly
supervised multitask learning that automatically identiﬁes
a large set of related auxiliary tasks from multivariate time
series to jointly learn from labelled and unlabelled data. In
addition, our approach scales to hundreds of auxiliary tasks
in an end-to-end trained neural network.

Semi-supervised Learning. Beside distant supervision,
other state-of-the-art approaches to semi-supervised learn-
ing in neural networks include, broadly, methods based
on (i) reconstruction objectives, such as Variational Auto-
Encoders (VAEs) (Kingma et al., 2014) and Ladder Net-
works (Rasmus et al., 2015), and (ii) adversarial learning
(Springenberg, 2015; Dai et al., 2017; Li et al., 2017).
However, with standard benchmarks consisting primar-
ily of low-resolution image datasets, it is yet unclear to
what degree these method’s results generalise to heteroge-
nous, long-term and high-resolution time series datasets
with informative missingness, as commonly encountered
in healthcare applications.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

(a) DSMT-Net-0

(b) DSMT-Net-3

Figure 2. Two DSMT-Net architectures: (a) one without (DSMT-Net-0) and (b) one with three (DSMT-Net-3) auxiliary tasks. The
number of horizontally aligned multitask blocks Mj (MT-Blocks; red) is variable. Each multitask block hosts its own auxiliary task
aj. An additional bypass connection gives the head block H (blue) direct access to the concatenated hidden states of the perception
blocks Pi (P-Blocks; orange). Each perception block operates on its own input data stream xi. The model incorporates binary missing
indicators mi for each perception block to handle situations where input data streams are missing.

3. Distantly Supervised Multitask Networks

Distantly Supervised Multitask Networks (DSMT-Nets)
are end-to-end trained neural networks that process n het-
erogenous input data streams xi in order to solve a multi-
task learning problem with one main task and k auxiliary
tasks designed to augment the main task. Conceptually,
a DSMT-Net consists of the following components: One
perception block Pi with i ∈ [1, n] for each of the n input
data streams xi, a variable number k of multitask blocks
Mj with j ∈ [1, k], and a single head block H (Figure 2b).
Each of these block types is itself a neural network with
its own parameters and arbitrary architectures and hyperpa-
rameters. The role of the perception blocks Pi is to extract
a hidden feature representation hp,i from their respective
input data streams xi:

hp,i = Pi(xi)

(1)

We separate the perception blocks by input stream xi in
order to be able to model a dynamic set of potentially miss-
ing input data streams. To allow our model to learn miss-
ingness patterns, we follow (Lipton et al., 2016b) and ac-
company each perception block with a missing indicator
mi that is set to 0 if the data stream xi is present and 1
if it is missing. We additionally perform zero-imputation
on the missing perception blocks’ features hp,i. We then
concatenate the features hp,i extracted from the perception
blocks and the corresponding missing indicators mi into a
joint feature representation Pc over all input data streams:

Pc = concatenate(hp,1, m1, ..., hp,n, mn)

(2)

The joint feature representation Pc combines the infor-
mation from all feature representations of the input data
streams and serves as input to the higher level multitask

blocks and the head block. The main role of multitask
blocks Mj is to host auxiliary tasks aj. All multitask blocks
are aligned in parallel in order to minimise the distance
gradients have to propagate through both to the joint fea-
ture representation Pc and from the head block. As output,
each multitask block produces a hidden high-level feature
representation hm,j:

hm,j = Mj(Pc)

(3)

Compared to the straightforward approach of directly ap-
pending the auxiliary tasks to the head block H, the posi-
tioning of multitask blocks below the head block achieves
separation of concerns. In DSMT-Nets, the head block fo-
cuses on learning a hidden feature representation that is op-
timised solely for the main task rather than being forced to
learn a joint feature representation that performs well on
multiple, possibly competing tasks.

The head block H computes the ﬁnal model output y and
further processes the hidden feature representations hm,j
of the multitask blocks via a combinator function (equation
(5)). In addition to the hidden feature representations of
the multitask blocks, the head block retains direct access
to Pc via a bypass connection. We motivate the inclusion
of a bypass connection with the desire to learn hidden fea-
ture representations in multitask blocks that add informa-
tion over Pc (He et al., 2016). Mathematically, we formu-
late the head block H as follows:

y = H(combineMLP(Pc, hm,1, ..., hm,k))

(4)

We note that the DSMT-Net architecture without any mul-
titask blocks corresponds to a na¨ıve supervised neural net-
work over a mixture of expert networks (Jordan & Jacobs,
1994; Shazeer et al., 2017; Schwab et al., 2018) for each
input data stream xi (DSMT-Net-0; Figure 2a).

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

In DSMT-Nets, the combinator
Combinator Function.
function integrates m + 1 data ﬂows from the m multi-
task blocks’ hidden representations as well as the joint fea-
ture representation Pc. We propose a combinator function
(combineMLP) that consists of a single hidden-layer multi-
layer perceptron (MLP) with a dimensionality twice as big
as a single multitask block’s feature representation. As in-
put, the MLP receives the concatenation of all the feature
representations to be integrated:

combineMLP = MLP(concatenate(Pc, h1, ..., hm))

(5)

3.1. Selection of Auxiliary Tasks

One of the most important questions in distantly supervised
learning is how to identify suitable auxiliary tasks. A com-
mon choice of auxiliary task for un- and semi-supervised
learning is reconstruction over the feature and/or hidden
representation space.
Several modern semi-supervised
methods take this approach (Vincent et al., 2008; Kingma
& Welling, 2014; Kingma et al., 2014; Rasmus et al., 2015).
Reconstruction is a convenient choice of auxiliary task be-
cause it is generically applicable to any input data, neu-
ral architecture and predictive task. However, given recent
empirical successes by distant supervision with speciﬁcally
engineered auxiliary tasks (Oquab et al., 2015; Deriu et al.,
2017; Doersch & Zisserman, 2017), we reason that (i) more
”related” tasks might be a better choice of auxiliary task for
semi-supervised learning than reconstruction (Ben-David
& Schuller, 2003) and that (ii) using multiple diverse aux-
iliary tasks might be more effective than just one (Baxter,
2000). Since a predictive feature for a main task is also
a good auxiliary task for learning shared predictive repre-
sentations (Ando & Zhang, 2005), we follow a simple two-
step feature selection methodology (Christ et al., 2016) to
automatically identify a large set of auxiliary tasks that are
closely related to the main task:

1. We extract features from a large pool of manually-
designed features from each input time series. Due
to the large wealth of research in manual feature en-
gineering, there exist vast repositories of such fea-
(Christ et al.,
tures for many data modalities, e.g.
2016). For time series, examples of such features
would be, e.g., the autocorrelation at different lag lev-
els or the power spectral density over a speciﬁc fre-
quency range.

2. We statistically test the extracted features for their im-
portance related to the main task in order to rank the
features by their estimated predictive potential and
determine their relevance. A suitable statistical test
is, for example, a hypothesis test for correlation be-
tween the labels ytrue and the extracted features using
Kendall’s τ (Kendall, 1945).

Using this approach, we are able to identify a large, ranked
list of predictive features suitable for use as target labels
for auxiliary tasks aj in DSMT-Nets. There are two ap-
proaches to choosing a subset of those features as auxiliary
targets: (i) in order of feature importance or (ii) randomly
out of the set of relevant features. The main difference be-
tween the two approaches is that random selection has a
higher expected task diversity as similar tasks are likely to
also rank similarly in terms of importance. There are ar-
guments both for (more information per task) and against
(harder to learn shared feature representation) higher task
diversity. We therefore evaluate both approaches in our ex-
periments.

3.2. Training Distantly Supervised Multitask Networks

A key problem when training neural networks on multi-
ple tasks simultaneously using stochastic gradient descent
is that gradients from the different tasks can interfere ad-
versely (Teh et al., 2017; Doersch & Zisserman, 2017).
We therefore completely disentangle the training of the un-
supervised and supervised tasks in DSMT-Nets.
Instead
of training the auxiliary tasks jointly with the main task,
we alternate between optimising DSMT-Nets for the aux-
iliary tasks and the main task in each epoch, starting with
the auxiliary tasks. At the computational cost of an addi-
tional pass during training, the two-step training procedure
prevents any potential adverse intra-step gradient interac-
tions between the two classes of tasks. To ensure similar
convergence rates for both the main and auxiliary tasks,
we weight the auxiliary tasks such that the total learning
rate for the unsupervised and supervised step are approxi-
mately the same, i.e. a weight of 1
k for each auxiliary task
when there are k auxiliary tasks. A similar training sched-
ule, where generator and discriminator networks are trained
one after another in each iteration, has been proposed to
train generative adversarial networks (GANs) (Goodfellow
et al., 2014).

4. Experiments

We performed extensive quantitative experiments1 on real-
world ICU data using a multitude of different hyperparam-
eter settings in order to answer the following questions:

(1) How do DSMT-Nets perform in terms of predictive
performance and label efﬁciency in multivariate false
alarm detection relative to state-of-the-art methods
for semi-supervised learning?

(2) What is the relationship between the number of aux-
iliary tasks, predictive performance and label efﬁ-
ciency?

1The source code for this work is available online at

https://github.com/d909b/DSMT-Nets.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

(3) What is the importance of the architectural separation
of auxiliary tasks and the main task and the two-step
training procedure in DSMT-Nets?

(4) Is there value in selecting a speciﬁc set of related aux-
iliary tasks for distantly supervised multitask learning
over random selection?

To answer question (1), we systematically evaluated
DSMT-Nets and several baseline models in terms of their
area under the receiver operator curve (AUROC) using
varying amounts of manually classiﬁed labels nlabels =
(12, 25, 50, 100, 500, 1244) and varying amounts of aux-
iliary tasks k = (6, 12, 25, 50, 100) in the DSMT-Nets.
We chose the label subsets at random without stratiﬁca-
tion. The comparison between the models’ performances
when using different levels of labels enable us to judge the
label efﬁciency of the compared models, i.e. which level
of predictive performance they can achieve with a limited
amount of labels. By also changing the amount of auxiliary
tasks used in the models, we are additionally able to assess
the relationship of the number of auxiliary tasks with label
efﬁciency and predictive performance (question (2)).

To answer question (3), we performed an ablation study us-
ing the DSMT-Nets with 100 auxiliary tasks (DSMT-Net-
100) using varying amounts of manually classiﬁed labels
as base models. We then trained the same models without
the two-step training procedure (- two step train). In ad-
dition, we evaluated the performance of a deep Highway
Network (Srivastava et al., 2015) with the same 100 aux-
iliary tasks distributed sequentially among layers (DSMT-
Net-100D) to compare multitask learning in depth against
width. Lastly, we also evaluated a multitask network where
the same 100 auxiliary tasks are placed directly on the head
block (Na¨ıve Multitask Network). Through this process,
we aimed to determine the relative importance of the indi-
vidual design choices introduced in section 3.

To answer question (4), we compared the predictive perfor-
mance of DSMT-Nets using a random selection of all the
signiﬁcant features as determined by our feature selection
methodology to that of DSMT-Nets that use a selection in
order of feature importance. We do so with DSMT-Nets
with 6 (DSMT-Net-6R) and 100 (DSMT-Net-100R) auxil-
iary tasks to additionally assess whether the importance of
auxiliary task selection is sensitive to the number of auxil-
iary tasks.

In total, we trained 2730 distinct model conﬁgurations
in order to gain a better understanding of the empirical
strengths and weaknesses of DSMT-Nets.

4.1. Dataset

We collected biosignal monitoring data from January to
August 2017 (8 months) from consenting patients ad-
mitted to the Neurocritical Care Unit at the University

Figure 3. Two clear examples of arterial blood pressure signals
without (top) and with pronounced artefacts (bottom). Note the
high frequency noise and atypical shape in the artefact sample.

of Zurich, Switzerland. The data included continuous,
evenly-sampled waveforms obtained by electrocardiogra-
phy (ECG; 200 Hz), arterial blood pressure (ART; 100 Hz),
pulse oximetry (PPG and SpO2; 100 Hz) and intracranial
pressure (ICP; 100 Hz) measurements. For this study, we
did not collect or make use of any personal, demographic
or clinical data, such as prior diagnoses, treatments or elec-
tronic health records. To obtain a ground truth assessment
of alarms, we provided clinical staff with a user interface
and instructions2 for annotating alarms that they believed
were caused by artefacts or a technical error (Figure 3).
Because of technical limitations in exporting data from the
biosignal database, we selected the subset of 20 monitoring
days of 14 patients with the highest amount of manually la-
belled alarms for further analysis. The evaluated dataset
encompassed a grand total of 13,938 alarms, yielding an
average rate of 696.9 alarms per patient per day. This num-
ber is in line with alarm rates reported in previous works
(Cvach, 2012). Of all alarms, 46.99% were caused by an
alarm-generating algorithm operating on the ART wave-
form, 33.10% on PPG or derived SpO2, 12.02% on ICP
and 7.89% on either ECG or an ECG-derived signal, such
as the heart rate signal.

Annotations. Out of the whole set of alarms, 1,777
(12.75%) alarms were manually labelled by clinical staff
during the observed period. Because we used multiple an-
notators that were not calibrated to each other’s assess-
ments, we additionally conducted a review over all 1,777
annotations in order to ensure the internal consistency of
the set of annotations as a whole. In this review round, we
found that a total of 603 (33.93%) annotations were incon-
sistent. We subsequently assigned corrected labels to these
alarms. Since label quality is paramount for model training
and validation, we suggest at least one label review round
using the majority vote of a committee of labellers with
clear instructions in order to maintain a sufﬁcient degree
of label consistency. Recent large-scale labelling efforts
in physiological monitoring of arrhythmias (Clifford et al.,
2017) suggest that even more review rounds might be nec-
essary to obtain a gold standard set of labels. In our ﬁnal
label set, 976 (45.08%) out of all annotated alarms are la-
belled as most likely being caused by an artefact or techni-
cal error. We note that a data collection effort of this scale

2Detailed instructions and full qualitative samples can be

found in the supplementary material.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

is extremely expensive and therefore economically infea-
sible for most hospitals, motivating our search for a more
label-efﬁcient approach.

4.2. Evaluation Setup

As input data, we extracted a 40 second window of the time
frame immediately before an alarm was triggered from
each available biosignal. We considered a signal stream
to be missing for a given alarm setting if the last recorded
measurement of that type happened longer than 10 seconds
ago. To reduce the computational resources required for
our experiments, we resampled the input data to 1
of its
16
original sampling rate. In our preliminary evaluation, we
did not see signiﬁcant performance changes when using a
higher sampling rate or a longer context window. Addition-
ally, we standardised the extracted windows of each stream
to the range of [−1, 1] using the maximum and minimum
values encountered in that window.

th

Baselines. To ensure a fair reference, we used the DSMT-
Nets base architecture without any horizontal blocks and
auxiliary tasks as the supervised baseline (DSMT-Net-0,
Figure 2a). Because the supervised baselines have no aux-
iliary tasks, we trained them in a purely supervised manner
on the labelled alarms only.

As a baseline for feature selection, we used the automated
feature extraction and selection approach from (Christ
et al., 2016) to identify a large number (up to 875) of rel-
evant time series features from the multivariate input data.
Note that we followed this process separately for each dis-
tinct amount of labels in order to avoid information leak-
age. We then fed those features to a random forest (RF)
classiﬁer consisting of 4096 trees to produce predictions
(Feature RF). As mentioned in section 3.2, we used the
same feature selection approach to identify suitable auxil-
iary tasks for DSMT-Nets. The Feature RF baseline there-
fore serves as a reference for directly using the identiﬁed
signiﬁcant features to make a prediction.

For comparison to the state-of-the-art in reconstruction-
based semi-supervised learning, we evaluated Ladder Net-
works (Rasmus et al., 2015) on the same dataset. We re-
placed the DSMT-Net components on top of the joint fea-
ture representation Pc with a Ladder Network in order to
use a comparable architecture that is also able to model
missingness and heterogenous input streams.

For comparison to the state-of-the-art in semi-supervised
adversarial
learning, we trained GANs using a semi-
supervised objective function and feature matching (Sali-
mans et al., 2016) on the same dataset. This type of GAN
has been shown to be highly efﬁcacious at semi-supervised
learning in low-resolution image datasets (Salimans et al.,
2016). We trained the generator networks to generate a

context window of multiple high-resolution time series as
input to a DSMT-Net discriminator without any auxiliary
tasks. In terms of architecture, the generator networks used
strided upsampling convolutions.

Hyperparameters. To ensure a fair comparison, we used
a systematic approach to hyperparameter selection for each
evaluated neural network. We trained each model 35 times
with a random choice of the three variable hyperparame-
ters bound to the same ranges (1 − 3 hidden layers, 16 − 32
units/ﬁlters per hidden layer, 25%−85% dropout). We reset
the random seed to the same value for each model in order
to make the search deterministic across training runs, i.e.
all the models were evaluated on exactly the same set of hy-
perparameter values. Note that this setup does not guaran-
tee optimality for any model, however, with respect to the
evaluated hyperparameters, it guarantees the models were
evaluated fairly and given the same amount of scrutiny. To
train the neural network models, we used a learning rate of
0.001 for the ﬁrst ten epochs and 0.0001 afterwards to op-
timise the binary cross-entropy for the main classiﬁcation
output and the mean squared error for all auxiliary tasks.
We additionally used early stopping with a patience of 13
epochs. For the extra hyperparameters in Ladder Networks,
we set the noise level to be ﬁxed at 0.2 at every layer, the
denoising loss weight to 100 for the ﬁrst hidden layer and to
0.1 for every following hidden layer. For the GAN models,
we used a base learning rate of 0.0003 for the discriminator
and a slightly increased learning rate of 0.003 for the gener-
ator to counteract the faster convergence of the discrimina-
tor networks. We trained GANs using an early stopping pa-
tience on the main loss of 650 steps for a minimum of 2500
steps. To choose these extra hyperparameters of GANs and
Ladder Networks, we followed the original author’s pub-
lished conﬁgurations (Rasmus et al., 2015; Salimans et al.,
2016) and adjusted them slightly to ensure they converged.

Architectures. We used the conceptual architecture from
Figure 2 as a base architecture for the DSMT-Nets. As
perception blocks, we employed ResNets (He et al., 2016)
with 1-dimensional convolutions over the time axis for each
input data stream. As head block and multitask blocks,
we used Highway Networks (Srivastava et al., 2015). The
head block hosted a sigmoid binary output y that indicated
whether or not the proposed alarm was likely caused by an
artefact. In addition, we used batch normalisation in the
DSMT-Net blocks.

Metrics. For each approach, we report the AUROC of the
best model encountered over all 35 hyperparameter runs.

Dataset Split. We applied a random split stratiﬁed by
alarm classiﬁcation to the whole set of annotated alarms
to separate the available data into a training (70%, 1244
alarms) and test set (30%, 533 alarms).

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

Table 1. Comparison of the maximum AUROC value across the 35 distinct models (vertical) that we trained using different sets of
hyperparameters and varying amounts of labels (horizontal). We report the AUROC of the best encountered model as calculated on the
test set of 533 alarms. The best results in each column are highlighted in bold.

AUROC with # of Labels
Feature RF
Supervised baseline
Na¨ıve Multitask Network
Ladder Network
Feature Matching GAN

DSMT-Net-6
DSMT-Net-12
DSMT-Net-25
DSMT-Net-50
DSMT-Net-100
- two step train

DSMT-Net-6R
DSMT-Net-100R
DSMT-Net-100D

12
0.567
0.751
0.791
0.791
0.846

0.763
0.739
0.761
0.722
0.720
0.733

0.805
0.790
0.587

25
0.574
0.753
0.804
0.772
0.834

0.839
0.872
0.870
0.847
0.831
0.798

0.851
0.860
0.611

50
0.628
0.806
0.828
0.800
0.834

0.866
0.891
0.886
0.901
0.893
0.785

0.884
0.883
0.722

100
0.822
0.873
0.887
0.842
0.865

0.897
0.890
0.898
0.906
0.907
0.814

0.909
0.909
0.610

500
0.942
0.941
0.941
0.863
0.911

0.924
0.928
0.924
0.926
0.934
0.849

0.921
0.918
0.624

1244
0.955
0.942
0.940
0.868
0.898

0.934
0.933
0.929
0.936
0.934
0.898

0.938
0.932
0.702

5. Results and Discussion

We report the results of our experiments in Table 1 and
discuss them in the following paragraphs.

Predictive Performance. Overall, we found that the label
limit after which the purely supervised approaches consis-
tently outperformed the semi-supervised approaches was
between 100 and 500 labels. The strongest approach when
using all 1244 available labels and the 500 label subset was
the purely supervised Feature RF baseline. Out of all com-
pared methods, DSMT-Nets were the most label-efﬁcient
approach when using 25, 50 and 100 labels. However,
the Feature Matching GAN outperformed the DSMT-Nets
when using just 12 labels. In our experimental setting, the
best DSMT-Nets yielded signiﬁcant improvements in AU-
ROC over both reconstruction-based as well as adversarial
state-of-the-art approaches to semi-supervised learning on
low-resolution image benchmarks. The relative improve-
ments in AUROC amounted to 13.0%, 12.6% and 8.0%
over Ladder Networks and 9.4%, 10.4% and 5.6% over
Feature Matching GANs at 25, 50 and 100 labels, respec-
tively. We note that even Na¨ıve Multitask Networks, that
did not make use of any of the adaptions introduced by
DSMT-Nets, with the exception of two cases outperformed
both Ladder Networks and Feature Matching GANs - sug-
gesting that distant supervision in general is an efﬁcacious
approach to semi-supervised learning in this domain.

Interestingly, most of the evaluated semi-supervised ap-
proaches, with the exception of Na¨ıve Multitask Networks,
were outperformed by their purely supervised counterparts
at lower amounts of labels than one would expect - in many
Indeed, both Feature Matching
cases by a large margin.
GANs as well as Ladder Networks were eclipsed by the
supervised baseline at just 100 labels. This suggests that ei-

ther: (i) Feature Matching GANs and Ladder Networks re-
quire a higher degree of hyperparameter optimisation than
the other evaluated approaches or (ii) the strengths of these
approaches in the domain of low-resolution images do not
generalise to the same degree to the domain of multivariate
high-resolution time series without adaptations. These are
novel ﬁndings given that most other recent evaluations of
state-of-the-art methods in semi-supervised learning have
been conﬁned solely to the low-resolution image domain.
We believe that, in the future, more systematic replication
studies, such as the one presented in this work, are nec-
essary to evaluate the degree to which new methods gen-
eralise beyond benchmark datasets that often do not cover
many practically important data modalities, such as time
series data, and idiosyncrasies, such as missingness, het-
erogeneity, sparsity and noise.

In terms of sensitivity and speciﬁcity, our best models
would have been able to reduce the number of false alarms
brought to the attention of clinical staff with the same
degree of urgency as true alarms with sensitivities of
22.97% (Feature Matching GAN), 40.99% (DSMT-Net-
12), 48.76% (DSMT-Net-50), 63.60% (DSMT-Net-100R),
66.43% (Feature RF) and 76.68% (Feature RF) using, re-
spectively, 12, 25, 50, 100, 500 and 1244 labelled training
samples at a speciﬁcity of 95%. In relative terms, DSMT-
Nets were therefore - with just 100 labels - able to realise
63.60
76.68 = 82.94% of the expected reduction in false alarms
of the Feature RF that was trained on 1,244 labels. This
ﬁnding conﬁrms that a modest data collection effort would
be sufﬁcient to achieve a considerable improvement in false
alarm rates in critical care.

Number of Auxiliary Tasks. In DSMT-Nets with auxil-
iary tasks selected by feature importance, more auxiliary
tasks achieved slightly better performances once sufﬁcient

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

amounts of labels were available. We reason that, because
the head block was trained on labelled samples only, a
greater number of labels was necessary to effectively or-
chestrate the extra information provided by a larger number
of multitask blocks. However, we did not see the same be-
havior in DSMT-Nets with auxiliary tasks selected at ran-
dom. Here, the performances of DSMT-Nets with 6 and
100 auxiliary tasks were comparable across all label levels.

Importance of Adaptions. We found that using DSMT-
Nets trained with auxiliary tasks distributed in depth
(DSMT-Net-100D) performed worse than our proposed ar-
chitecture - demonstrating that parallel alignment of multi-
task blocks is the superior architectural design choice. Sim-
ilarly, DSMT-Net-100 variants without the two step train-
ing procedure (- two step train) consistently failed to reach
the semi-supervised performance of their counterparts with
the two step training procedure enabled (DSMT-Net-100)
for more than 12 labels. This shows that disentangling the
training of the auxiliary and the main task played an in-
tegral role in the strong semi-supervised performance of
DSMT-Nets and further reinforces prior reports that ad-
verse gradient interactions are a key challenge for multi-
task learning in neural networks (Teh et al., 2017; Doersch
& Zisserman, 2017).

Task Selection. We found that random selection in most
cases outperformed selection in order of feature importance
when comparing the DSMT-Net-6 and DSMT-Net-6R vari-
ants. We believe this was the result of increased task diver-
sity when selecting at random from the relevant auxiliary
tasks, as similar features rank close to each other in terms
of feature importance. The fact that this effect was less
pronounced between the same models with more auxiliary
tasks (DSMT-Net-100R and DSMT-Net-100) supports this
theory, as a larger set of tasks will automatically have a
higher diversity due to the limited amount of highly similar
features, thus decreasing the importance of accounting for
diversity in the selection methodology. We therefore con-
clude that task diversity is the dominant factor in selecting
related auxiliary tasks for distant multitask supervision.

6. Limitations

False alarms in the ICU are not solely a technical problem
(Cvach, 2012; Drew et al., 2014). Organisational and pro-
cessual aspects must also be considered to comprehensibly
address this issue in clinical care (Drew et al., 2014). One
such aspect is the question of how to best manage those
alarms that have been ﬂagged as false by an alarm classiﬁ-
cation system. We reason that, due to the inherent possibil-
ity of suppressing a true alarm, a sensible approach would
be to report those errors with a lower degree of urgency,
i.e. with a less pronounced sound, rather than completely
suppressing them (Cvach, 2012).

Another limitation of this work is that we only considered
the detection of alarms that are caused by either artefacts
or technical errors. Alarms that are technically correct,
but clinically require no intervention, are another impor-
tant source of false alarms (Drew et al., 2014) that we did
not analyse in this work. Identifying clinically false alarms
is signiﬁcantly harder than those caused by artefacts and
technical errors, as clinical reasoning requires deep knowl-
edge of a patient’s high-level physiological state, as well as
a signiﬁcant amount of domain knowledge.

Lastly, while the presented distantly supervised approach
to semi-supervised learning performs well on our dataset,
its applicability to other datasets hinges on being able to
determine multiple related auxiliary tasks. We only evalu-
ated distantly supervised multitask learning on time series
data, where large numbers of suitable auxiliary tasks are
readily available through automated feature extraction and
selection (Christ et al., 2016). We hypothesise that it might
not be trivial to ﬁnd large repositories of auxiliary tasks
suitable for distant multitask supervision for all data types.
A comparatively small number of potential auxiliary tasks
have been reported in related works in computer vision
and natural language processing (Blaschko et al., 2010; Xu
et al., 2015; Oquab et al., 2015; Deriu et al., 2017; Do-
ersch & Zisserman, 2017). Finally, our experiments yield
insights into the importance of auxiliary task selection in
DSMT-Nets, but further theoretical analyses are necessary
to understand exactly what types of auxiliary task are useful
to what degree in distantly supervised multitask learning.

7. Conclusion

We present a novel approach to reducing false alarms in
the ICU using data obtained from a dynamic set of mul-
tiple heterogenous biosignal monitors. Unlabelled data is
abundantly available, but obtaining trustworthy expert la-
bels is laborious and expensive in this setting. We intro-
duce a multitask network architecture that leverages distant
supervision through multiple related auxiliary tasks in or-
der to reduce the number of expensive labels required for
training. We develop both a methodology for automati-
cally selecting auxiliary tasks from multivariate time se-
ries as well as an optimised training procedure that coun-
teracts adverse gradient interactions between tasks. Using
a real-world critical care dataset, we demonstrate that our
approach leads to signiﬁcant improvements over several
state-of-the-art baselines. In addition, we found that task
diversity and adverse gradient interactions are key concerns
in distantly supervised multitask learning. Going forward,
we believe that our approach could be applicable to a wide
variety of machine-learning tasks in healthcare for which
obtaining labelled data is a major challenge.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

Acknowledgements

This work was partially funded by the Swiss National Sci-
ence Foundation (SNSF) project No. 167195 within the
National Research Program (NRP) 75 “Big Data” and the
Swiss Commission for Technology and Innovation (CTI)
project No. 25531.

References
Aboukhalil, A., Nielsen, L., Saeed, M., Mark, R. G., and Clifford,
G. D. Reducing false alarm rates for critical arrhythmias using
the arterial blood pressure waveform. Journal of Biomedical
Informatics, 41(3):442–451, 2008.

Ando, R. K. and Zhang, T. A framework for learning predictive
structures from multiple tasks and unlabeled data. Journal of
Machine Learning Research, 6(Nov):1817–1853, 2005.

Aytar, Y., Pfaff, T., Budden, D., Paine, T. L., Wang, Z., and de Fre-
itas, N. Playing hard exploration games by watching youtube.
arXiv preprint arXiv:1805.11592, 2018.

Baxter, J. A model of inductive bias learning. Journal of Artiﬁcial

Intelligence Research, 12(149-198):3, 2000.

Ben-David, S. and Schuller, R. Exploiting task relatedness for
multiple task learning. Lecture Notes in Computer Science, pp.
567–580, 2003.

Blaschko, M., Vedaldi, A., and Zisserman, A. Simultaneous ob-
ject detection and ranking with weak supervision. In Advances
in neural information processing systems, pp. 235–243, 2010.

Che, Z., Purushotham, S., Cho, K., Sontag, D., and Liu, Y. Recur-
rent neural networks for multivariate time series with missing
values. arXiv preprint arXiv:1606.01865, 2016.

Cheng, L.-F., Darnell, G., Chivers, C., Draugelis, M. E., Li,
K., and Engelhardt, B. E. Sparse multi-output Gaussian pro-
arXiv preprint
cesses for medical time series prediction.
arXiv:1703.09112, 2017.

Choi, E., Bahadori, M. T., Schuetz, A., Stewart, W. F., and Sun,
J. Doctor AI: Predicting clinical events via recurrent neural
In Machine Learning for Healthcare Conference,
networks.
pp. 301–318, 2016.

Christ, M., Kempa-Liehr, A. W., and Feindt, M. Distributed and
parallel time series feature extraction for industrial big data ap-
plications. arXiv preprint arXiv:1610.07717, 2016.

Clifford, G., Liu, C., Moody, B., Lehman, L., Silva, I., Li, Q.,
Johnson, A., and Mark, R. G. AF classiﬁcation from a short
single lead ECG recording: The Physionet Computing in Car-
diology Challenge 2017. Computing in Cardiology, 44, 2017.

Clifford, G. D., Silva, I., Moody, B., Li, Q., Kella, D., Shahin,
A., Kooistra, T., Perry, D., and Mark, R. G. The Phys-
ioNet/Computing in Cardiology Challenge 2015: Reducing
false arrhythmia alarms in the ICU. In Computing in Cardi-
ology, pp. 273–276. IEEE, 2015.

Cvach, M. Monitor alarm fatigue: An integrative review. Biomed-
ical Instrumentation & Technology, 46(4):268–277, 2012.

Dai, Z., Yang, Z., Yang, F., Cohen, W. W., and Salakhutdinov,
R. Good Semi-supervised Learning that Requires a Bad GAN.
Advances in Neural Information Processing Systems, 2017.

Deriu, J., Lucchi, A., De Luca, V., Severyn, A., M¨uller, S.,
Cieliebak, M., Hofmann, T., and Jaggi, M. Leveraging large
amounts of weakly supervised data for multi-language senti-
ment classiﬁcation. In Proceedings of the 26th International
Conference on World Wide Web, pp. 1045–1052. International
World Wide Web Conferences Steering Committee, 2017.

Doersch, C. and Zisserman, A. Multi-task self-supervised visual
learning. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2051–2060, 2017.

Drew, B. J., Harris, P., Z`egre-Hemsey, J. K., Mammone, T.,
Schindler, D., Salas-Boni, R., Bai, Y., Tinoco, A., Ding, Q.,
and Hu, X.
Insights into the problem of alarm fatigue with
physiologic monitor devices: A comprehensive observational
study of consecutive intensive care unit patients. PloS one, 9
(10):e110274, 2014.

Eerik¨ainen, L. M., Vanschoren, J., Rooijakkers, M. J., Vullings,
R., and Aarts, R. M. Decreasing the false alarm rate of ar-
rhythmias in intensive care using a machine learning approach.
In Computing in Cardiology, 2015.

Fallet, S., Yazdani, S., and Vesin, J.-M. A multimodal approach
to reduce false arrhythmia alarms in the intensive care unit. In
Computing in Cardiology, 2015.

Fernando, B., Bilen, H., Gavves, E., and Gould, S.

Self-
supervised video representation learning with odd-one-out net-
works. In 2017 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pp. 5729–5738. IEEE, 2017.

Ghassemi, M., Naumann, T., Doshi-Velez, F., Brimmer, N., Joshi,
R., Rumshisky, A., and Szolovits, P. Unfolding physiologi-
cal state: Mortality modelling in intensive care units. In Pro-
ceedings of the 20th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 75–84. ACM,
2014.

Ghassemi, M., Pimentel, M. A., Naumann, T., Brennan, T.,
Clifton, D. A., Szolovits, P., and Feng, M. A multivariate time-
series modeling approach to severity of illness assessment and
forecasting in ICU with sparse, heterogeneous clinical data. In
Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial
Intelligence, pp. 446–453, 2015.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-
Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative
adversarial nets. In Advances in Neural Information Process-
ing Systems, pp. 2672–2680, 2014.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for
image recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 770–778, 2016.

Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo,
Reinforcement
arXiv preprint

J. Z., Silver, D., and Kavukcuoglu, K.
learning with unsupervised auxiliary tasks.
arXiv:1611.05397, 2016.

Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of experts
and the EM algorithm. Neural computation, 6(2):181–214,
1994.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

Kendall, M. G. The treatment of ties in ranking problems.

Biometrika, pp. 239–251, 1945.

Kingma, D. P. and Welling, M. Auto-encoding variational bayes.
International Conference on Learning Representations, 2014.

Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M.
Semi-supervised learning with deep generative models. In Ad-
vances in Neural Information Processing Systems, pp. 3581–
3589, 2014.

Krasteva, V., Jekova, I., Leber, R., Schmid, R., and Ab¨acherli, R.
Real-time arrhythmia detection with supplementary ecg quality
and pulse wave monitoring for the reduction of false alarms in
icus. Physiological measurement, 37(8):1273, 2016.

Laine, S. and Aila, T. Temporal ensembling for semi-supervised
International Conference on Learning Representa-

learning.
tions, 2017.

Lasko, T. A., Denny, J. C., and Levy, M. A. Computational pheno-
type discovery using unsupervised feature learning over noisy,
sparse, and irregular clinical data. PloS one, 8(6):e66341,
2013.

Li, C., Xu, K., Zhu, J., and Zhang, B. Triple generative adversar-
ial nets. Advances in Neural Information Processing Systems,
2017.

Libbrecht, M. W. and Noble, W. S. Machine learning applications
in genetics and genomics. Nature Reviews Genetics, 16(6):321,
2015.

Lipton, Z. C., Kale, D. C., Elkan, C., and Wetzell, R. Learning to
diagnose with LSTM recurrent neural networks. International
Conference on Learning Representations, 2016a.

Lipton, Z. C., Kale, D. C., and Wetzel, R. Directly modeling
missing data in sequences with RNNs: Improved classiﬁcation
In Machine Learning for Healthcare
of clinical time series.
Conference, pp. 253–270, 2016b.

Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Is object localiza-
tion for free? Weakly-supervised learning with convolutional
neural networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 685–694, 2015.

Papandreou, G., Chen, L.-C., Murphy, K. P., and Yuille, A. L.
Weakly-and semi-supervised learning of a deep convolutional
In Proceedings
network for semantic image segmentation.
of the IEEE international conference on computer vision, pp.
1742–1750, 2015.

Plesinger, F., Klimes, P., Halamek, J., and Jurak, P. Taming of the
monitors: reducing false alarms in intensive care units. Physi-
ological measurement, 37(8):1313, 2016.

Prasad, N., Cheng, L.-F., Chivers, C., Draugelis, M., and Engel-
hardt, B. E. A reinforcement learning approach to weaning of
mechanical ventilation in intensive care units. arXiv preprint
arXiv:1704.06300, 2017.

Ramsundar, B., Kearnes, S., Riley, P., Webster, D., Konerding, D.,
and Pande, V. Massively multitask networks for drug discov-
ery. arXiv preprint arXiv:1502.02072, 2015.

Rasmus, A., Berglund, M., Honkala, M., Valpola, H., and Raiko,
In Ad-
T. Semi-supervised learning with ladder networks.
vances in Neural Information Processing Systems, pp. 3546–
3554, 2015.

Saeed, M., Villarroel, M., Reisner, A. T., Clifford, G. D., Lehman,
L.-W., Moody, G., Heldt, T., Kyaw, T. H., Moody, B., and
Mark, R. G. Multiparameter Intelligent Monitoring in Inten-
sive Care II (MIMIC-II): A public-access intensive care unit
database. Critical Care Medicine, 39(5):952, 2011.

Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford,
A., and Chen, X.
Improved techniques for training GANs.
In Advances in Neural Information Processing Systems, pp.
2234–2242, 2016.

Saria, S., Rajani, A. K., Gould, J., Koller, D., and Penn, A. A. In-
tegration of early physiological responses predicts later illness
severity in preterm infants. Science translational medicine, 2
(48):48ra65–48ra65, 2010.

Schwab, P., Scebba, G. C., Zhang, J., Delai, M., and Karlen, W.
Beat by Beat: Classifying Cardiac Arrhythmias with Recurrent
Neural Networks. In Computing in Cardiology, 2017.

Schwab, P., Miladinovic, D., and Karlen, W. Granger-causal at-
tentive mixtures of experts: Learning important features with
neural networks. arXiv preprint arXiv:1802.02195, 2018.

Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hin-
ton, G., and Dean, J. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. arXiv preprint
arXiv:1701.06538, 2017.

Springenberg, J. T. Unsupervised and semi-supervised learn-
ing with categorical generative adversarial networks. arXiv
preprint arXiv:1511.06390, 2015.

Srivastava, R. K., Greff, K., and Schmidhuber, J. Training very
deep networks. In Advances in Neural Information Processing
Systems, pp. 2377–2385, 2015.

Teh, Y., Bapst, V., Pascanu, R., Heess, N., Quan, J., Kirkpatrick,
J., Czarnecki, W. M., and Hadsell, R. Distral: Robust multi-
task reinforcement learning. In Advances in Neural Informa-
tion Processing Systems, pp. 4497–4507, 2017.

Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. Ex-
tracting and composing robust features with denoising autoen-
coders. In Proceedings of the 25th International Conference
on Machine Learning, pp. 1096–1103. ACM, 2008.

Wiens, J., Guttag, J., and Horvitz, E. Patient risk stratiﬁcation
with time-varying parameters: A multitask learning approach.
The Journal of Machine Learning Research, 17(1):2797–2819,
2016.

Xu, J., Schwing, A. G., and Urtasun, R. Learning to segment
under various forms of weak supervision. In Computer Vision
and Pattern Recognition (CVPR), 2015 IEEE Conference on,
pp. 3781–3790. IEEE, 2015.

Zeng, D., Liu, K., Chen, Y., and Zhao, J. Distant supervision
for relation extraction via piecewise convolutional neural net-
In Proceedings of the 2015 Conference on Empiri-
works.
cal Methods in Natural Language Processing, pp. 1753–1762,
2015.

Not to Cry Wolf:
Distantly Supervised Multitask Learning in Critical Care

Patrick Schwab 1 Emanuela Keller 2 Carl Muroi 2 David J. Mack 2 Christian Str¨assle 2 Walter Karlen 1

8
1
0
2
 
n
u
J
 
7
 
 
]

G
L
.
s
c
[
 
 
2
v
7
2
0
5
0
.
2
0
8
1
:
v
i
X
r
a

Abstract
Patients in the intensive care unit (ICU) require
constant and close supervision. To assist clinical
staff in this task, hospitals use monitoring sys-
tems that trigger audiovisual alarms if their al-
gorithms indicate that a patient’s condition may
be worsening. However, current monitoring sys-
tems are extremely sensitive to movement arte-
facts and technical errors. As a result,
they
typically trigger hundreds to thousands of false
alarms per patient per day - drowning the impor-
tant alarms in noise and adding to the exhaustion
of clinical staff. In this setting, data is abundantly
available, but obtaining trustworthy annotations
by experts is laborious and expensive. We frame
the problem of false alarm reduction from multi-
variate time series as a machine-learning task and
address it with a novel multitask network archi-
tecture that utilises distant supervision through
multiple related auxiliary tasks in order to reduce
the number of expensive labels required for train-
ing. We show that our approach leads to sig-
niﬁcant improvements over several state-of-the-
art baselines on real-world ICU data and provide
new insights on the importance of task selection
and architectural choices in distantly supervised
multitask learning.

1. Introduction

False alarms are an enormous mental burden for clinical
staff and are extremely dangerous to patients, as alarm
fatigue and desensitisation may lead to clinically important
alarms being missed (Drew et al., 2014). Reportedly,
several hundreds of deaths a year are associated with false
alarms in patient monitoring in the United States alone
(Cvach, 2012).

1Institute of Robotics and Intelligent Systems, ETH Zurich,
Switzerland 2Neurocritical Care Unit, Department of Neuro-
surgery, University Hospital Zurich, Switzerland. Correspon-
dence to: Patrick Schwab <patrick.schwab@hest.ethz.ch>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

Figure 1. Schematic overview of the described problem setting in
critical care. Before an alarm is brought to the attention of clini-
cal staff, an alarm classiﬁcation algorithm could analyse a recent
window of the full set of available data streams in order to identify
whether the alarm was likely caused by an artefact or technical er-
ror and may therefore be reported with a lower degree of urgency.

An intelligent alarm classiﬁcation system could potentially
reduce the burden of a large subset of those false alarms
by assessing which alarms were likely caused by either an
artefact or a technical error and reporting those alarms with
a lower degree of urgency (Figure 1). Roadblocks that have
so far prevented the adoption of machine learning for this
task are the heterogeneity of monitoring systems, the re-
quirement for an extremely high speciﬁcity, to avoid sup-
pressing important alarms, and the prohibitively high cost
associated with obtaining a representative set of clinically
validated labels for each of the manifold alarm types and
monitoring system conﬁgurations in use at hospitals.

We present a semi-supervised approach to false alarm re-
duction that automatically identiﬁes and incorporates a
large amount of distantly supervised auxiliary tasks in or-
der to signiﬁcantly reduce the number of expensive labels
required for training. We demonstrate, on real-world ICU
data, that our approach is able to correctly classify alarms
originating from artefacts and technical errors better than
several state-of-the-art methods for semi-supervised learn-
ing when using just 25, 50 and 100 labelled samples. Be-
sides their importance for clinical practice, our results high-
light the power of distant multitask supervision as a ﬂexi-
ble and effective tool for learning when unlabelled data are
readily available, and shed new light on semi-supervised
learning beyond low-resolution image benchmark datasets.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

Contributions. We subdivide this work along the follow-
ing distinct contributions:

(i) We introduce DSMT-Nets: A novel neural architec-
ture built on the idea of utilising distant supervision
through multiple auxiliary tasks in order to better har-
ness unlabelled data.

(ii) We present a methodology for selecting a large set of
related auxiliary tasks in time series, and a training
procedure that counteracts adverse gradient interac-
tions between auxiliary tasks and the main task.
(iii) We perform extensive quantitative experiments on a
real-world ICU dataset consisting of almost 14,000
alarms in order to evaluate the relative classiﬁcation
performance and label efﬁciency of DSMT-Nets com-
pared to several state-of-the-art methods.

2. Related Work

Background. Driven by widespread efforts to automate
patient monitoring, there has been a recent surge in works
applying machine learning to the vast amounts of data gen-
erated in ICUs. One notable driver is the MIMIC (Saeed
et al., 2011) dataset that has made ICU data accessible to
a large number of researchers. Related works have, for ex-
ample, explored the use of ICU data for tasks such as mor-
tality modelling (Ghassemi et al., 2014), illness assessment
and forecasting (Ghassemi et al., 2015), diagnostic support
(Lipton et al., 2016a), patient state prediction (Cheng et al.,
2017) and learning weaning policies for mechanical ven-
tilation (Prasad et al., 2017). Applying machine-learning
approaches to clinical and physiological data is challeng-
ing, because it is heterogenous, noisy, confounded, sparse
and of high temporal resolution over long periods of time.
These properties are in stark contrast to many of the bench-
mark datasets that machine-learning approaches are typi-
cally developed and evaluated on. Several works therefore
deal with adapting existing machine-learning approaches to
the idiosyncrasies of clinical and physiological data, such
as missingness (Lipton et al., 2016b; Che et al., 2016),
long-term temporal dependencies (Choi et al., 2016), noise
(Schwab et al., 2017), heterogeneity (Libbrecht & Noble,
2015) and sparsity (Lasko et al., 2013). We build on sev-
eral of these innovations in this work.

Alarm Fatigue. The PhysioNet 2015 challenge on false
alarm reduction in electrocardiography (ECG) monitoring
(Clifford et al., 2015) was one of the most notable efforts
to date to address the issue of false alarms in physiological
monitoring. Within the challenge, researchers introduced
several effective approaches to reducing the false alarm rate
of arrhythmia alerts in ECGs (Fallet et al., 2015; Eerik¨ainen
et al., 2015; Krasteva et al., 2016; Plesinger et al., 2016).
However, clinicians in the ICU do not just monitor for ar-
rhythmias, but many adverse events at once using a multi-

tude of different monitoring systems. Typically, these mon-
itoring systems operate in isolation on a single biosignal
and trigger their own distinct sets of alarms. Previous re-
search has shown that there is an opportunity to use data
from other biosignals to identify false alarms in related
waveforms (Aboukhalil et al., 2008). We therefore believe
that a comprehensive solution to alarm fatigue requires an
approach that accounts for the monitoring setup as a whole,
rather than targeting speciﬁc systems or alarms in isolation.

Distant Supervision and Multitask Learning. Multitask
learning has a rich history in healthcare applications and
has, for example, been used for risk prediction in neonatal
intensive care (Saria et al., 2010), drug discovery (Ram-
sundar et al., 2015) and prediction of Clostridium difﬁcile
(Wiens et al., 2016). A way of leveraging multitask learn-
ing to improve label-efﬁciency is to learn jointly from com-
plementary unsupervised auxiliary tasks along with the su-
pervised main task. Existing literature refers to the concept
of applying indirect supervision through auxiliary tasks, be
it for label-efﬁciency or additional predictive performance,
as weak supervision (Papandreou et al., 2015; Oquab et al.,
2015), distant supervision (Zeng et al., 2015; Deriu et al.,
2017) or self-supervision (Fernando et al., 2017; Doersch
& Zisserman, 2017). In particular, (Doersch & Zisserman,
2017) used distantly supervised multitask learning to in-
crease predictive performance in computer vision with up
to four hand-engineered auxiliary tasks. Using auxiliary
tasks in addition to a main task has also been shown to be
a promising approach in reinforcement (Jaderberg et al.,
2016; Aytar et al., 2018) and adversarial learning (Salimans
et al., 2016). Recently, (Laine & Aila, 2017) proposed
to use outputs from the same model at different points in
training and with varying amounts of regularisation as ad-
ditional unsupervised targets for the main task. In contrast
to existing works, we present the ﬁrst approach to distantly
supervised multitask learning that automatically identiﬁes
a large set of related auxiliary tasks from multivariate time
series to jointly learn from labelled and unlabelled data. In
addition, our approach scales to hundreds of auxiliary tasks
in an end-to-end trained neural network.

Semi-supervised Learning. Beside distant supervision,
other state-of-the-art approaches to semi-supervised learn-
ing in neural networks include, broadly, methods based
on (i) reconstruction objectives, such as Variational Auto-
Encoders (VAEs) (Kingma et al., 2014) and Ladder Net-
works (Rasmus et al., 2015), and (ii) adversarial learning
(Springenberg, 2015; Dai et al., 2017; Li et al., 2017).
However, with standard benchmarks consisting primar-
ily of low-resolution image datasets, it is yet unclear to
what degree these method’s results generalise to heteroge-
nous, long-term and high-resolution time series datasets
with informative missingness, as commonly encountered
in healthcare applications.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

(a) DSMT-Net-0

(b) DSMT-Net-3

Figure 2. Two DSMT-Net architectures: (a) one without (DSMT-Net-0) and (b) one with three (DSMT-Net-3) auxiliary tasks. The
number of horizontally aligned multitask blocks Mj (MT-Blocks; red) is variable. Each multitask block hosts its own auxiliary task
aj. An additional bypass connection gives the head block H (blue) direct access to the concatenated hidden states of the perception
blocks Pi (P-Blocks; orange). Each perception block operates on its own input data stream xi. The model incorporates binary missing
indicators mi for each perception block to handle situations where input data streams are missing.

3. Distantly Supervised Multitask Networks

Distantly Supervised Multitask Networks (DSMT-Nets)
are end-to-end trained neural networks that process n het-
erogenous input data streams xi in order to solve a multi-
task learning problem with one main task and k auxiliary
tasks designed to augment the main task. Conceptually,
a DSMT-Net consists of the following components: One
perception block Pi with i ∈ [1, n] for each of the n input
data streams xi, a variable number k of multitask blocks
Mj with j ∈ [1, k], and a single head block H (Figure 2b).
Each of these block types is itself a neural network with
its own parameters and arbitrary architectures and hyperpa-
rameters. The role of the perception blocks Pi is to extract
a hidden feature representation hp,i from their respective
input data streams xi:

hp,i = Pi(xi)

(1)

We separate the perception blocks by input stream xi in
order to be able to model a dynamic set of potentially miss-
ing input data streams. To allow our model to learn miss-
ingness patterns, we follow (Lipton et al., 2016b) and ac-
company each perception block with a missing indicator
mi that is set to 0 if the data stream xi is present and 1
if it is missing. We additionally perform zero-imputation
on the missing perception blocks’ features hp,i. We then
concatenate the features hp,i extracted from the perception
blocks and the corresponding missing indicators mi into a
joint feature representation Pc over all input data streams:

Pc = concatenate(hp,1, m1, ..., hp,n, mn)

(2)

The joint feature representation Pc combines the infor-
mation from all feature representations of the input data
streams and serves as input to the higher level multitask

blocks and the head block. The main role of multitask
blocks Mj is to host auxiliary tasks aj. All multitask blocks
are aligned in parallel in order to minimise the distance
gradients have to propagate through both to the joint fea-
ture representation Pc and from the head block. As output,
each multitask block produces a hidden high-level feature
representation hm,j:

hm,j = Mj(Pc)

(3)

Compared to the straightforward approach of directly ap-
pending the auxiliary tasks to the head block H, the posi-
tioning of multitask blocks below the head block achieves
separation of concerns. In DSMT-Nets, the head block fo-
cuses on learning a hidden feature representation that is op-
timised solely for the main task rather than being forced to
learn a joint feature representation that performs well on
multiple, possibly competing tasks.

The head block H computes the ﬁnal model output y and
further processes the hidden feature representations hm,j
of the multitask blocks via a combinator function (equation
(5)). In addition to the hidden feature representations of
the multitask blocks, the head block retains direct access
to Pc via a bypass connection. We motivate the inclusion
of a bypass connection with the desire to learn hidden fea-
ture representations in multitask blocks that add informa-
tion over Pc (He et al., 2016). Mathematically, we formu-
late the head block H as follows:

y = H(combineMLP(Pc, hm,1, ..., hm,k))

(4)

We note that the DSMT-Net architecture without any mul-
titask blocks corresponds to a na¨ıve supervised neural net-
work over a mixture of expert networks (Jordan & Jacobs,
1994; Shazeer et al., 2017; Schwab et al., 2018) for each
input data stream xi (DSMT-Net-0; Figure 2a).

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

In DSMT-Nets, the combinator
Combinator Function.
function integrates m + 1 data ﬂows from the m multi-
task blocks’ hidden representations as well as the joint fea-
ture representation Pc. We propose a combinator function
(combineMLP) that consists of a single hidden-layer multi-
layer perceptron (MLP) with a dimensionality twice as big
as a single multitask block’s feature representation. As in-
put, the MLP receives the concatenation of all the feature
representations to be integrated:

combineMLP = MLP(concatenate(Pc, h1, ..., hm))

(5)

3.1. Selection of Auxiliary Tasks

One of the most important questions in distantly supervised
learning is how to identify suitable auxiliary tasks. A com-
mon choice of auxiliary task for un- and semi-supervised
learning is reconstruction over the feature and/or hidden
representation space.
Several modern semi-supervised
methods take this approach (Vincent et al., 2008; Kingma
& Welling, 2014; Kingma et al., 2014; Rasmus et al., 2015).
Reconstruction is a convenient choice of auxiliary task be-
cause it is generically applicable to any input data, neu-
ral architecture and predictive task. However, given recent
empirical successes by distant supervision with speciﬁcally
engineered auxiliary tasks (Oquab et al., 2015; Deriu et al.,
2017; Doersch & Zisserman, 2017), we reason that (i) more
”related” tasks might be a better choice of auxiliary task for
semi-supervised learning than reconstruction (Ben-David
& Schuller, 2003) and that (ii) using multiple diverse aux-
iliary tasks might be more effective than just one (Baxter,
2000). Since a predictive feature for a main task is also
a good auxiliary task for learning shared predictive repre-
sentations (Ando & Zhang, 2005), we follow a simple two-
step feature selection methodology (Christ et al., 2016) to
automatically identify a large set of auxiliary tasks that are
closely related to the main task:

1. We extract features from a large pool of manually-
designed features from each input time series. Due
to the large wealth of research in manual feature en-
gineering, there exist vast repositories of such fea-
(Christ et al.,
tures for many data modalities, e.g.
2016). For time series, examples of such features
would be, e.g., the autocorrelation at different lag lev-
els or the power spectral density over a speciﬁc fre-
quency range.

2. We statistically test the extracted features for their im-
portance related to the main task in order to rank the
features by their estimated predictive potential and
determine their relevance. A suitable statistical test
is, for example, a hypothesis test for correlation be-
tween the labels ytrue and the extracted features using
Kendall’s τ (Kendall, 1945).

Using this approach, we are able to identify a large, ranked
list of predictive features suitable for use as target labels
for auxiliary tasks aj in DSMT-Nets. There are two ap-
proaches to choosing a subset of those features as auxiliary
targets: (i) in order of feature importance or (ii) randomly
out of the set of relevant features. The main difference be-
tween the two approaches is that random selection has a
higher expected task diversity as similar tasks are likely to
also rank similarly in terms of importance. There are ar-
guments both for (more information per task) and against
(harder to learn shared feature representation) higher task
diversity. We therefore evaluate both approaches in our ex-
periments.

3.2. Training Distantly Supervised Multitask Networks

A key problem when training neural networks on multi-
ple tasks simultaneously using stochastic gradient descent
is that gradients from the different tasks can interfere ad-
versely (Teh et al., 2017; Doersch & Zisserman, 2017).
We therefore completely disentangle the training of the un-
supervised and supervised tasks in DSMT-Nets.
Instead
of training the auxiliary tasks jointly with the main task,
we alternate between optimising DSMT-Nets for the aux-
iliary tasks and the main task in each epoch, starting with
the auxiliary tasks. At the computational cost of an addi-
tional pass during training, the two-step training procedure
prevents any potential adverse intra-step gradient interac-
tions between the two classes of tasks. To ensure similar
convergence rates for both the main and auxiliary tasks,
we weight the auxiliary tasks such that the total learning
rate for the unsupervised and supervised step are approxi-
mately the same, i.e. a weight of 1
k for each auxiliary task
when there are k auxiliary tasks. A similar training sched-
ule, where generator and discriminator networks are trained
one after another in each iteration, has been proposed to
train generative adversarial networks (GANs) (Goodfellow
et al., 2014).

4. Experiments

We performed extensive quantitative experiments1 on real-
world ICU data using a multitude of different hyperparam-
eter settings in order to answer the following questions:

(1) How do DSMT-Nets perform in terms of predictive
performance and label efﬁciency in multivariate false
alarm detection relative to state-of-the-art methods
for semi-supervised learning?

(2) What is the relationship between the number of aux-
iliary tasks, predictive performance and label efﬁ-
ciency?

1The source code for this work is available online at

https://github.com/d909b/DSMT-Nets.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

(3) What is the importance of the architectural separation
of auxiliary tasks and the main task and the two-step
training procedure in DSMT-Nets?

(4) Is there value in selecting a speciﬁc set of related aux-
iliary tasks for distantly supervised multitask learning
over random selection?

To answer question (1), we systematically evaluated
DSMT-Nets and several baseline models in terms of their
area under the receiver operator curve (AUROC) using
varying amounts of manually classiﬁed labels nlabels =
(12, 25, 50, 100, 500, 1244) and varying amounts of aux-
iliary tasks k = (6, 12, 25, 50, 100) in the DSMT-Nets.
We chose the label subsets at random without stratiﬁca-
tion. The comparison between the models’ performances
when using different levels of labels enable us to judge the
label efﬁciency of the compared models, i.e. which level
of predictive performance they can achieve with a limited
amount of labels. By also changing the amount of auxiliary
tasks used in the models, we are additionally able to assess
the relationship of the number of auxiliary tasks with label
efﬁciency and predictive performance (question (2)).

To answer question (3), we performed an ablation study us-
ing the DSMT-Nets with 100 auxiliary tasks (DSMT-Net-
100) using varying amounts of manually classiﬁed labels
as base models. We then trained the same models without
the two-step training procedure (- two step train). In ad-
dition, we evaluated the performance of a deep Highway
Network (Srivastava et al., 2015) with the same 100 aux-
iliary tasks distributed sequentially among layers (DSMT-
Net-100D) to compare multitask learning in depth against
width. Lastly, we also evaluated a multitask network where
the same 100 auxiliary tasks are placed directly on the head
block (Na¨ıve Multitask Network). Through this process,
we aimed to determine the relative importance of the indi-
vidual design choices introduced in section 3.

To answer question (4), we compared the predictive perfor-
mance of DSMT-Nets using a random selection of all the
signiﬁcant features as determined by our feature selection
methodology to that of DSMT-Nets that use a selection in
order of feature importance. We do so with DSMT-Nets
with 6 (DSMT-Net-6R) and 100 (DSMT-Net-100R) auxil-
iary tasks to additionally assess whether the importance of
auxiliary task selection is sensitive to the number of auxil-
iary tasks.

In total, we trained 2730 distinct model conﬁgurations
in order to gain a better understanding of the empirical
strengths and weaknesses of DSMT-Nets.

4.1. Dataset

We collected biosignal monitoring data from January to
August 2017 (8 months) from consenting patients ad-
mitted to the Neurocritical Care Unit at the University

Figure 3. Two clear examples of arterial blood pressure signals
without (top) and with pronounced artefacts (bottom). Note the
high frequency noise and atypical shape in the artefact sample.

of Zurich, Switzerland. The data included continuous,
evenly-sampled waveforms obtained by electrocardiogra-
phy (ECG; 200 Hz), arterial blood pressure (ART; 100 Hz),
pulse oximetry (PPG and SpO2; 100 Hz) and intracranial
pressure (ICP; 100 Hz) measurements. For this study, we
did not collect or make use of any personal, demographic
or clinical data, such as prior diagnoses, treatments or elec-
tronic health records. To obtain a ground truth assessment
of alarms, we provided clinical staff with a user interface
and instructions2 for annotating alarms that they believed
were caused by artefacts or a technical error (Figure 3).
Because of technical limitations in exporting data from the
biosignal database, we selected the subset of 20 monitoring
days of 14 patients with the highest amount of manually la-
belled alarms for further analysis. The evaluated dataset
encompassed a grand total of 13,938 alarms, yielding an
average rate of 696.9 alarms per patient per day. This num-
ber is in line with alarm rates reported in previous works
(Cvach, 2012). Of all alarms, 46.99% were caused by an
alarm-generating algorithm operating on the ART wave-
form, 33.10% on PPG or derived SpO2, 12.02% on ICP
and 7.89% on either ECG or an ECG-derived signal, such
as the heart rate signal.

Annotations. Out of the whole set of alarms, 1,777
(12.75%) alarms were manually labelled by clinical staff
during the observed period. Because we used multiple an-
notators that were not calibrated to each other’s assess-
ments, we additionally conducted a review over all 1,777
annotations in order to ensure the internal consistency of
the set of annotations as a whole. In this review round, we
found that a total of 603 (33.93%) annotations were incon-
sistent. We subsequently assigned corrected labels to these
alarms. Since label quality is paramount for model training
and validation, we suggest at least one label review round
using the majority vote of a committee of labellers with
clear instructions in order to maintain a sufﬁcient degree
of label consistency. Recent large-scale labelling efforts
in physiological monitoring of arrhythmias (Clifford et al.,
2017) suggest that even more review rounds might be nec-
essary to obtain a gold standard set of labels. In our ﬁnal
label set, 976 (45.08%) out of all annotated alarms are la-
belled as most likely being caused by an artefact or techni-
cal error. We note that a data collection effort of this scale

2Detailed instructions and full qualitative samples can be

found in the supplementary material.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

is extremely expensive and therefore economically infea-
sible for most hospitals, motivating our search for a more
label-efﬁcient approach.

4.2. Evaluation Setup

As input data, we extracted a 40 second window of the time
frame immediately before an alarm was triggered from
each available biosignal. We considered a signal stream
to be missing for a given alarm setting if the last recorded
measurement of that type happened longer than 10 seconds
ago. To reduce the computational resources required for
our experiments, we resampled the input data to 1
of its
16
original sampling rate. In our preliminary evaluation, we
did not see signiﬁcant performance changes when using a
higher sampling rate or a longer context window. Addition-
ally, we standardised the extracted windows of each stream
to the range of [−1, 1] using the maximum and minimum
values encountered in that window.

th

Baselines. To ensure a fair reference, we used the DSMT-
Nets base architecture without any horizontal blocks and
auxiliary tasks as the supervised baseline (DSMT-Net-0,
Figure 2a). Because the supervised baselines have no aux-
iliary tasks, we trained them in a purely supervised manner
on the labelled alarms only.

As a baseline for feature selection, we used the automated
feature extraction and selection approach from (Christ
et al., 2016) to identify a large number (up to 875) of rel-
evant time series features from the multivariate input data.
Note that we followed this process separately for each dis-
tinct amount of labels in order to avoid information leak-
age. We then fed those features to a random forest (RF)
classiﬁer consisting of 4096 trees to produce predictions
(Feature RF). As mentioned in section 3.2, we used the
same feature selection approach to identify suitable auxil-
iary tasks for DSMT-Nets. The Feature RF baseline there-
fore serves as a reference for directly using the identiﬁed
signiﬁcant features to make a prediction.

For comparison to the state-of-the-art in reconstruction-
based semi-supervised learning, we evaluated Ladder Net-
works (Rasmus et al., 2015) on the same dataset. We re-
placed the DSMT-Net components on top of the joint fea-
ture representation Pc with a Ladder Network in order to
use a comparable architecture that is also able to model
missingness and heterogenous input streams.

For comparison to the state-of-the-art in semi-supervised
adversarial
learning, we trained GANs using a semi-
supervised objective function and feature matching (Sali-
mans et al., 2016) on the same dataset. This type of GAN
has been shown to be highly efﬁcacious at semi-supervised
learning in low-resolution image datasets (Salimans et al.,
2016). We trained the generator networks to generate a

context window of multiple high-resolution time series as
input to a DSMT-Net discriminator without any auxiliary
tasks. In terms of architecture, the generator networks used
strided upsampling convolutions.

Hyperparameters. To ensure a fair comparison, we used
a systematic approach to hyperparameter selection for each
evaluated neural network. We trained each model 35 times
with a random choice of the three variable hyperparame-
ters bound to the same ranges (1 − 3 hidden layers, 16 − 32
units/ﬁlters per hidden layer, 25%−85% dropout). We reset
the random seed to the same value for each model in order
to make the search deterministic across training runs, i.e.
all the models were evaluated on exactly the same set of hy-
perparameter values. Note that this setup does not guaran-
tee optimality for any model, however, with respect to the
evaluated hyperparameters, it guarantees the models were
evaluated fairly and given the same amount of scrutiny. To
train the neural network models, we used a learning rate of
0.001 for the ﬁrst ten epochs and 0.0001 afterwards to op-
timise the binary cross-entropy for the main classiﬁcation
output and the mean squared error for all auxiliary tasks.
We additionally used early stopping with a patience of 13
epochs. For the extra hyperparameters in Ladder Networks,
we set the noise level to be ﬁxed at 0.2 at every layer, the
denoising loss weight to 100 for the ﬁrst hidden layer and to
0.1 for every following hidden layer. For the GAN models,
we used a base learning rate of 0.0003 for the discriminator
and a slightly increased learning rate of 0.003 for the gener-
ator to counteract the faster convergence of the discrimina-
tor networks. We trained GANs using an early stopping pa-
tience on the main loss of 650 steps for a minimum of 2500
steps. To choose these extra hyperparameters of GANs and
Ladder Networks, we followed the original author’s pub-
lished conﬁgurations (Rasmus et al., 2015; Salimans et al.,
2016) and adjusted them slightly to ensure they converged.

Architectures. We used the conceptual architecture from
Figure 2 as a base architecture for the DSMT-Nets. As
perception blocks, we employed ResNets (He et al., 2016)
with 1-dimensional convolutions over the time axis for each
input data stream. As head block and multitask blocks,
we used Highway Networks (Srivastava et al., 2015). The
head block hosted a sigmoid binary output y that indicated
whether or not the proposed alarm was likely caused by an
artefact. In addition, we used batch normalisation in the
DSMT-Net blocks.

Metrics. For each approach, we report the AUROC of the
best model encountered over all 35 hyperparameter runs.

Dataset Split. We applied a random split stratiﬁed by
alarm classiﬁcation to the whole set of annotated alarms
to separate the available data into a training (70%, 1244
alarms) and test set (30%, 533 alarms).

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

Table 1. Comparison of the maximum AUROC value across the 35 distinct models (vertical) that we trained using different sets of
hyperparameters and varying amounts of labels (horizontal). We report the AUROC of the best encountered model as calculated on the
test set of 533 alarms. The best results in each column are highlighted in bold.

AUROC with # of Labels
Feature RF
Supervised baseline
Na¨ıve Multitask Network
Ladder Network
Feature Matching GAN

DSMT-Net-6
DSMT-Net-12
DSMT-Net-25
DSMT-Net-50
DSMT-Net-100
- two step train

DSMT-Net-6R
DSMT-Net-100R
DSMT-Net-100D

12
0.567
0.751
0.791
0.791
0.846

0.763
0.739
0.761
0.722
0.720
0.733

0.805
0.790
0.587

25
0.574
0.753
0.804
0.772
0.834

0.839
0.872
0.870
0.847
0.831
0.798

0.851
0.860
0.611

50
0.628
0.806
0.828
0.800
0.834

0.866
0.891
0.886
0.901
0.893
0.785

0.884
0.883
0.722

100
0.822
0.873
0.887
0.842
0.865

0.897
0.890
0.898
0.906
0.907
0.814

0.909
0.909
0.610

500
0.942
0.941
0.941
0.863
0.911

0.924
0.928
0.924
0.926
0.934
0.849

0.921
0.918
0.624

1244
0.955
0.942
0.940
0.868
0.898

0.934
0.933
0.929
0.936
0.934
0.898

0.938
0.932
0.702

5. Results and Discussion

We report the results of our experiments in Table 1 and
discuss them in the following paragraphs.

Predictive Performance. Overall, we found that the label
limit after which the purely supervised approaches consis-
tently outperformed the semi-supervised approaches was
between 100 and 500 labels. The strongest approach when
using all 1244 available labels and the 500 label subset was
the purely supervised Feature RF baseline. Out of all com-
pared methods, DSMT-Nets were the most label-efﬁcient
approach when using 25, 50 and 100 labels. However,
the Feature Matching GAN outperformed the DSMT-Nets
when using just 12 labels. In our experimental setting, the
best DSMT-Nets yielded signiﬁcant improvements in AU-
ROC over both reconstruction-based as well as adversarial
state-of-the-art approaches to semi-supervised learning on
low-resolution image benchmarks. The relative improve-
ments in AUROC amounted to 13.0%, 12.6% and 8.0%
over Ladder Networks and 9.4%, 10.4% and 5.6% over
Feature Matching GANs at 25, 50 and 100 labels, respec-
tively. We note that even Na¨ıve Multitask Networks, that
did not make use of any of the adaptions introduced by
DSMT-Nets, with the exception of two cases outperformed
both Ladder Networks and Feature Matching GANs - sug-
gesting that distant supervision in general is an efﬁcacious
approach to semi-supervised learning in this domain.

Interestingly, most of the evaluated semi-supervised ap-
proaches, with the exception of Na¨ıve Multitask Networks,
were outperformed by their purely supervised counterparts
at lower amounts of labels than one would expect - in many
Indeed, both Feature Matching
cases by a large margin.
GANs as well as Ladder Networks were eclipsed by the
supervised baseline at just 100 labels. This suggests that ei-

ther: (i) Feature Matching GANs and Ladder Networks re-
quire a higher degree of hyperparameter optimisation than
the other evaluated approaches or (ii) the strengths of these
approaches in the domain of low-resolution images do not
generalise to the same degree to the domain of multivariate
high-resolution time series without adaptations. These are
novel ﬁndings given that most other recent evaluations of
state-of-the-art methods in semi-supervised learning have
been conﬁned solely to the low-resolution image domain.
We believe that, in the future, more systematic replication
studies, such as the one presented in this work, are nec-
essary to evaluate the degree to which new methods gen-
eralise beyond benchmark datasets that often do not cover
many practically important data modalities, such as time
series data, and idiosyncrasies, such as missingness, het-
erogeneity, sparsity and noise.

In terms of sensitivity and speciﬁcity, our best models
would have been able to reduce the number of false alarms
brought to the attention of clinical staff with the same
degree of urgency as true alarms with sensitivities of
22.97% (Feature Matching GAN), 40.99% (DSMT-Net-
12), 48.76% (DSMT-Net-50), 63.60% (DSMT-Net-100R),
66.43% (Feature RF) and 76.68% (Feature RF) using, re-
spectively, 12, 25, 50, 100, 500 and 1244 labelled training
samples at a speciﬁcity of 95%. In relative terms, DSMT-
Nets were therefore - with just 100 labels - able to realise
63.60
76.68 = 82.94% of the expected reduction in false alarms
of the Feature RF that was trained on 1,244 labels. This
ﬁnding conﬁrms that a modest data collection effort would
be sufﬁcient to achieve a considerable improvement in false
alarm rates in critical care.

Number of Auxiliary Tasks. In DSMT-Nets with auxil-
iary tasks selected by feature importance, more auxiliary
tasks achieved slightly better performances once sufﬁcient

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

amounts of labels were available. We reason that, because
the head block was trained on labelled samples only, a
greater number of labels was necessary to effectively or-
chestrate the extra information provided by a larger number
of multitask blocks. However, we did not see the same be-
havior in DSMT-Nets with auxiliary tasks selected at ran-
dom. Here, the performances of DSMT-Nets with 6 and
100 auxiliary tasks were comparable across all label levels.

Importance of Adaptions. We found that using DSMT-
Nets trained with auxiliary tasks distributed in depth
(DSMT-Net-100D) performed worse than our proposed ar-
chitecture - demonstrating that parallel alignment of multi-
task blocks is the superior architectural design choice. Sim-
ilarly, DSMT-Net-100 variants without the two step train-
ing procedure (- two step train) consistently failed to reach
the semi-supervised performance of their counterparts with
the two step training procedure enabled (DSMT-Net-100)
for more than 12 labels. This shows that disentangling the
training of the auxiliary and the main task played an in-
tegral role in the strong semi-supervised performance of
DSMT-Nets and further reinforces prior reports that ad-
verse gradient interactions are a key challenge for multi-
task learning in neural networks (Teh et al., 2017; Doersch
& Zisserman, 2017).

Task Selection. We found that random selection in most
cases outperformed selection in order of feature importance
when comparing the DSMT-Net-6 and DSMT-Net-6R vari-
ants. We believe this was the result of increased task diver-
sity when selecting at random from the relevant auxiliary
tasks, as similar features rank close to each other in terms
of feature importance. The fact that this effect was less
pronounced between the same models with more auxiliary
tasks (DSMT-Net-100R and DSMT-Net-100) supports this
theory, as a larger set of tasks will automatically have a
higher diversity due to the limited amount of highly similar
features, thus decreasing the importance of accounting for
diversity in the selection methodology. We therefore con-
clude that task diversity is the dominant factor in selecting
related auxiliary tasks for distant multitask supervision.

6. Limitations

False alarms in the ICU are not solely a technical problem
(Cvach, 2012; Drew et al., 2014). Organisational and pro-
cessual aspects must also be considered to comprehensibly
address this issue in clinical care (Drew et al., 2014). One
such aspect is the question of how to best manage those
alarms that have been ﬂagged as false by an alarm classiﬁ-
cation system. We reason that, due to the inherent possibil-
ity of suppressing a true alarm, a sensible approach would
be to report those errors with a lower degree of urgency,
i.e. with a less pronounced sound, rather than completely
suppressing them (Cvach, 2012).

Another limitation of this work is that we only considered
the detection of alarms that are caused by either artefacts
or technical errors. Alarms that are technically correct,
but clinically require no intervention, are another impor-
tant source of false alarms (Drew et al., 2014) that we did
not analyse in this work. Identifying clinically false alarms
is signiﬁcantly harder than those caused by artefacts and
technical errors, as clinical reasoning requires deep knowl-
edge of a patient’s high-level physiological state, as well as
a signiﬁcant amount of domain knowledge.

Lastly, while the presented distantly supervised approach
to semi-supervised learning performs well on our dataset,
its applicability to other datasets hinges on being able to
determine multiple related auxiliary tasks. We only evalu-
ated distantly supervised multitask learning on time series
data, where large numbers of suitable auxiliary tasks are
readily available through automated feature extraction and
selection (Christ et al., 2016). We hypothesise that it might
not be trivial to ﬁnd large repositories of auxiliary tasks
suitable for distant multitask supervision for all data types.
A comparatively small number of potential auxiliary tasks
have been reported in related works in computer vision
and natural language processing (Blaschko et al., 2010; Xu
et al., 2015; Oquab et al., 2015; Deriu et al., 2017; Do-
ersch & Zisserman, 2017). Finally, our experiments yield
insights into the importance of auxiliary task selection in
DSMT-Nets, but further theoretical analyses are necessary
to understand exactly what types of auxiliary task are useful
to what degree in distantly supervised multitask learning.

7. Conclusion

We present a novel approach to reducing false alarms in
the ICU using data obtained from a dynamic set of mul-
tiple heterogenous biosignal monitors. Unlabelled data is
abundantly available, but obtaining trustworthy expert la-
bels is laborious and expensive in this setting. We intro-
duce a multitask network architecture that leverages distant
supervision through multiple related auxiliary tasks in or-
der to reduce the number of expensive labels required for
training. We develop both a methodology for automati-
cally selecting auxiliary tasks from multivariate time se-
ries as well as an optimised training procedure that coun-
teracts adverse gradient interactions between tasks. Using
a real-world critical care dataset, we demonstrate that our
approach leads to signiﬁcant improvements over several
state-of-the-art baselines. In addition, we found that task
diversity and adverse gradient interactions are key concerns
in distantly supervised multitask learning. Going forward,
we believe that our approach could be applicable to a wide
variety of machine-learning tasks in healthcare for which
obtaining labelled data is a major challenge.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

Acknowledgements

This work was partially funded by the Swiss National Sci-
ence Foundation (SNSF) project No. 167195 within the
National Research Program (NRP) 75 “Big Data” and the
Swiss Commission for Technology and Innovation (CTI)
project No. 25531.

References
Aboukhalil, A., Nielsen, L., Saeed, M., Mark, R. G., and Clifford,
G. D. Reducing false alarm rates for critical arrhythmias using
the arterial blood pressure waveform. Journal of Biomedical
Informatics, 41(3):442–451, 2008.

Ando, R. K. and Zhang, T. A framework for learning predictive
structures from multiple tasks and unlabeled data. Journal of
Machine Learning Research, 6(Nov):1817–1853, 2005.

Aytar, Y., Pfaff, T., Budden, D., Paine, T. L., Wang, Z., and de Fre-
itas, N. Playing hard exploration games by watching youtube.
arXiv preprint arXiv:1805.11592, 2018.

Baxter, J. A model of inductive bias learning. Journal of Artiﬁcial

Intelligence Research, 12(149-198):3, 2000.

Ben-David, S. and Schuller, R. Exploiting task relatedness for
multiple task learning. Lecture Notes in Computer Science, pp.
567–580, 2003.

Blaschko, M., Vedaldi, A., and Zisserman, A. Simultaneous ob-
ject detection and ranking with weak supervision. In Advances
in neural information processing systems, pp. 235–243, 2010.

Che, Z., Purushotham, S., Cho, K., Sontag, D., and Liu, Y. Recur-
rent neural networks for multivariate time series with missing
values. arXiv preprint arXiv:1606.01865, 2016.

Cheng, L.-F., Darnell, G., Chivers, C., Draugelis, M. E., Li,
K., and Engelhardt, B. E. Sparse multi-output Gaussian pro-
arXiv preprint
cesses for medical time series prediction.
arXiv:1703.09112, 2017.

Choi, E., Bahadori, M. T., Schuetz, A., Stewart, W. F., and Sun,
J. Doctor AI: Predicting clinical events via recurrent neural
In Machine Learning for Healthcare Conference,
networks.
pp. 301–318, 2016.

Christ, M., Kempa-Liehr, A. W., and Feindt, M. Distributed and
parallel time series feature extraction for industrial big data ap-
plications. arXiv preprint arXiv:1610.07717, 2016.

Clifford, G., Liu, C., Moody, B., Lehman, L., Silva, I., Li, Q.,
Johnson, A., and Mark, R. G. AF classiﬁcation from a short
single lead ECG recording: The Physionet Computing in Car-
diology Challenge 2017. Computing in Cardiology, 44, 2017.

Clifford, G. D., Silva, I., Moody, B., Li, Q., Kella, D., Shahin,
A., Kooistra, T., Perry, D., and Mark, R. G. The Phys-
ioNet/Computing in Cardiology Challenge 2015: Reducing
false arrhythmia alarms in the ICU. In Computing in Cardi-
ology, pp. 273–276. IEEE, 2015.

Cvach, M. Monitor alarm fatigue: An integrative review. Biomed-
ical Instrumentation & Technology, 46(4):268–277, 2012.

Dai, Z., Yang, Z., Yang, F., Cohen, W. W., and Salakhutdinov,
R. Good Semi-supervised Learning that Requires a Bad GAN.
Advances in Neural Information Processing Systems, 2017.

Deriu, J., Lucchi, A., De Luca, V., Severyn, A., M¨uller, S.,
Cieliebak, M., Hofmann, T., and Jaggi, M. Leveraging large
amounts of weakly supervised data for multi-language senti-
ment classiﬁcation. In Proceedings of the 26th International
Conference on World Wide Web, pp. 1045–1052. International
World Wide Web Conferences Steering Committee, 2017.

Doersch, C. and Zisserman, A. Multi-task self-supervised visual
learning. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2051–2060, 2017.

Drew, B. J., Harris, P., Z`egre-Hemsey, J. K., Mammone, T.,
Schindler, D., Salas-Boni, R., Bai, Y., Tinoco, A., Ding, Q.,
and Hu, X.
Insights into the problem of alarm fatigue with
physiologic monitor devices: A comprehensive observational
study of consecutive intensive care unit patients. PloS one, 9
(10):e110274, 2014.

Eerik¨ainen, L. M., Vanschoren, J., Rooijakkers, M. J., Vullings,
R., and Aarts, R. M. Decreasing the false alarm rate of ar-
rhythmias in intensive care using a machine learning approach.
In Computing in Cardiology, 2015.

Fallet, S., Yazdani, S., and Vesin, J.-M. A multimodal approach
to reduce false arrhythmia alarms in the intensive care unit. In
Computing in Cardiology, 2015.

Fernando, B., Bilen, H., Gavves, E., and Gould, S.

Self-
supervised video representation learning with odd-one-out net-
works. In 2017 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pp. 5729–5738. IEEE, 2017.

Ghassemi, M., Naumann, T., Doshi-Velez, F., Brimmer, N., Joshi,
R., Rumshisky, A., and Szolovits, P. Unfolding physiologi-
cal state: Mortality modelling in intensive care units. In Pro-
ceedings of the 20th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 75–84. ACM,
2014.

Ghassemi, M., Pimentel, M. A., Naumann, T., Brennan, T.,
Clifton, D. A., Szolovits, P., and Feng, M. A multivariate time-
series modeling approach to severity of illness assessment and
forecasting in ICU with sparse, heterogeneous clinical data. In
Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial
Intelligence, pp. 446–453, 2015.

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-
Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative
adversarial nets. In Advances in Neural Information Process-
ing Systems, pp. 2672–2680, 2014.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for
image recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 770–778, 2016.

Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo,
Reinforcement
arXiv preprint

J. Z., Silver, D., and Kavukcuoglu, K.
learning with unsupervised auxiliary tasks.
arXiv:1611.05397, 2016.

Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of experts
and the EM algorithm. Neural computation, 6(2):181–214,
1994.

Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care

Kendall, M. G. The treatment of ties in ranking problems.

Biometrika, pp. 239–251, 1945.

Kingma, D. P. and Welling, M. Auto-encoding variational bayes.
International Conference on Learning Representations, 2014.

Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M.
Semi-supervised learning with deep generative models. In Ad-
vances in Neural Information Processing Systems, pp. 3581–
3589, 2014.

Krasteva, V., Jekova, I., Leber, R., Schmid, R., and Ab¨acherli, R.
Real-time arrhythmia detection with supplementary ecg quality
and pulse wave monitoring for the reduction of false alarms in
icus. Physiological measurement, 37(8):1273, 2016.

Laine, S. and Aila, T. Temporal ensembling for semi-supervised
International Conference on Learning Representa-

learning.
tions, 2017.

Lasko, T. A., Denny, J. C., and Levy, M. A. Computational pheno-
type discovery using unsupervised feature learning over noisy,
sparse, and irregular clinical data. PloS one, 8(6):e66341,
2013.

Li, C., Xu, K., Zhu, J., and Zhang, B. Triple generative adversar-
ial nets. Advances in Neural Information Processing Systems,
2017.

Libbrecht, M. W. and Noble, W. S. Machine learning applications
in genetics and genomics. Nature Reviews Genetics, 16(6):321,
2015.

Lipton, Z. C., Kale, D. C., Elkan, C., and Wetzell, R. Learning to
diagnose with LSTM recurrent neural networks. International
Conference on Learning Representations, 2016a.

Lipton, Z. C., Kale, D. C., and Wetzel, R. Directly modeling
missing data in sequences with RNNs: Improved classiﬁcation
In Machine Learning for Healthcare
of clinical time series.
Conference, pp. 253–270, 2016b.

Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Is object localiza-
tion for free? Weakly-supervised learning with convolutional
neural networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 685–694, 2015.

Papandreou, G., Chen, L.-C., Murphy, K. P., and Yuille, A. L.
Weakly-and semi-supervised learning of a deep convolutional
In Proceedings
network for semantic image segmentation.
of the IEEE international conference on computer vision, pp.
1742–1750, 2015.

Plesinger, F., Klimes, P., Halamek, J., and Jurak, P. Taming of the
monitors: reducing false alarms in intensive care units. Physi-
ological measurement, 37(8):1313, 2016.

Prasad, N., Cheng, L.-F., Chivers, C., Draugelis, M., and Engel-
hardt, B. E. A reinforcement learning approach to weaning of
mechanical ventilation in intensive care units. arXiv preprint
arXiv:1704.06300, 2017.

Ramsundar, B., Kearnes, S., Riley, P., Webster, D., Konerding, D.,
and Pande, V. Massively multitask networks for drug discov-
ery. arXiv preprint arXiv:1502.02072, 2015.

Rasmus, A., Berglund, M., Honkala, M., Valpola, H., and Raiko,
In Ad-
T. Semi-supervised learning with ladder networks.
vances in Neural Information Processing Systems, pp. 3546–
3554, 2015.

Saeed, M., Villarroel, M., Reisner, A. T., Clifford, G. D., Lehman,
L.-W., Moody, G., Heldt, T., Kyaw, T. H., Moody, B., and
Mark, R. G. Multiparameter Intelligent Monitoring in Inten-
sive Care II (MIMIC-II): A public-access intensive care unit
database. Critical Care Medicine, 39(5):952, 2011.

Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford,
A., and Chen, X.
Improved techniques for training GANs.
In Advances in Neural Information Processing Systems, pp.
2234–2242, 2016.

Saria, S., Rajani, A. K., Gould, J., Koller, D., and Penn, A. A. In-
tegration of early physiological responses predicts later illness
severity in preterm infants. Science translational medicine, 2
(48):48ra65–48ra65, 2010.

Schwab, P., Scebba, G. C., Zhang, J., Delai, M., and Karlen, W.
Beat by Beat: Classifying Cardiac Arrhythmias with Recurrent
Neural Networks. In Computing in Cardiology, 2017.

Schwab, P., Miladinovic, D., and Karlen, W. Granger-causal at-
tentive mixtures of experts: Learning important features with
neural networks. arXiv preprint arXiv:1802.02195, 2018.

Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hin-
ton, G., and Dean, J. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. arXiv preprint
arXiv:1701.06538, 2017.

Springenberg, J. T. Unsupervised and semi-supervised learn-
ing with categorical generative adversarial networks. arXiv
preprint arXiv:1511.06390, 2015.

Srivastava, R. K., Greff, K., and Schmidhuber, J. Training very
deep networks. In Advances in Neural Information Processing
Systems, pp. 2377–2385, 2015.

Teh, Y., Bapst, V., Pascanu, R., Heess, N., Quan, J., Kirkpatrick,
J., Czarnecki, W. M., and Hadsell, R. Distral: Robust multi-
task reinforcement learning. In Advances in Neural Informa-
tion Processing Systems, pp. 4497–4507, 2017.

Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. Ex-
tracting and composing robust features with denoising autoen-
coders. In Proceedings of the 25th International Conference
on Machine Learning, pp. 1096–1103. ACM, 2008.

Wiens, J., Guttag, J., and Horvitz, E. Patient risk stratiﬁcation
with time-varying parameters: A multitask learning approach.
The Journal of Machine Learning Research, 17(1):2797–2819,
2016.

Xu, J., Schwing, A. G., and Urtasun, R. Learning to segment
under various forms of weak supervision. In Computer Vision
and Pattern Recognition (CVPR), 2015 IEEE Conference on,
pp. 3781–3790. IEEE, 2015.

Zeng, D., Liu, K., Chen, Y., and Zhao, J. Distant supervision
for relation extraction via piecewise convolutional neural net-
In Proceedings of the 2015 Conference on Empiri-
works.
cal Methods in Natural Language Processing, pp. 1753–1762,
2015.

