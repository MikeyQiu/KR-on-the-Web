8
1
0
2
 
v
o
N
 
9
2
 
 
]

G
L
.
s
c
[
 
 
4
v
6
3
8
7
0
.
5
0
8
1
:
v
i
X
r
a

Generalized Cross Entropy Loss for Training Deep
Neural Networks with Noisy Labels

Mert R. Sabuncu
Zhilu Zhang
Electrical and Computer Engineering
Meinig School of Biomedical Engineering
Cornell University
zz452@cornell.edu, msabuncu@cornell.edu

Abstract

Deep neural networks (DNNs) have achieved tremendous success in a variety of
applications across many disciplines. Yet, their superior performance comes with
the expensive cost of requiring correctly annotated large-scale datasets. Moreover,
due to DNNs’ rich capacity, errors in training labels can hamper performance. To
combat this problem, mean absolute error (MAE) has recently been proposed as
a noise-robust alternative to the commonly-used categorical cross entropy (CCE)
loss. However, as we show in this paper, MAE can perform poorly with DNNs and
challenging datasets. Here, we present a theoretically grounded set of noise-robust
loss functions that can be seen as a generalization of MAE and CCE. Proposed loss
functions can be readily applied with any existing DNN architecture and algorithm,
while yielding good performance in a wide range of noisy label scenarios. We report
results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-
MNIST datasets and synthetically generated noisy labels.

1

Introduction

The resurrection of neural networks in recent years, together with the recent emergence of large
scale datasets, has enabled super-human performance on many classiﬁcation tasks [21, 28, 30].
However, supervised DNNs often require a large number of training samples to achieve a high level
of performance. For instance, the ImageNet dataset [6] has 3.2 million hand-annotated images.
Although crowdsourcing platforms like Amazon Mechanical Turk have made large-scale annotation
possible, some error during the labeling process is often inevitable, and mislabeled samples can
impair the performance of models trained on these data. Indeed, the sheer capacity of DNNs to
memorize massive data with completely randomly assigned labels [42] proves their susceptibility to
overﬁtting when trained with noisy labels. Hence, an algorithm that is robust against noisy labels
for DNNs is needed to resolve the potential problem. Furthermore, when examples are cheap and
accurate annotations are expensive, it can be more beneﬁcial to have datasets with more but noisier
labels than less but more accurate labels [18].

Classiﬁcation with noisy labels is a widely studied topic [8]. Yet, relatively little attention is given
to directly formulating a noise-robust loss function in the context of DNNs. Our work is motivated
by Ghosh et al. [9] who theoretically showed that mean absolute error (MAE) can be robust against
noisy labels under certain assumptions. However, as we demonstrate below, the robustness of MAE
can concurrently cause increased difﬁculty in training, and lead to performance drop. This limitation
is particularly evident when using DNNs on complicated datasets. To combat this drawback, we
advocate the use of a more general class of noise-robust loss functions, which encompass both MAE
and CCE. Compared to previous methods for DNNs, which often involve extra steps and algorithmic
modiﬁcations, changing only the loss function requires minimal intervention to existing architectures

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.

and algorithms, and thus can be promptly applied. Furthermore, unlike most existing methods, the
proposed loss functions work for both closed-set and open-set noisy labels [40]. Open-set refers to
the situation where samples associated with erroneous labels do not always belong to a ground truth
class contained within the set of known classes in the training data. Conversely, closed-set means that
all labels (erroneous and correct) come from a known set of labels present in the dataset.

The main contributions of this paper are two-fold. First, we propose a novel generalization of CCE
and present a theoretical analysis of proposed loss functions in the context of noisy labels. And
second, we report a thorough empirical evaluation of the proposed loss functions using CIFAR-10,
CIFAR-100 and FASHION-MNIST datasets, and demonstrate signiﬁcant improvement in terms of
classiﬁcation accuracy over the baselines of MAE and CCE, under both closed-set and open-set noisy
labels.

The rest of the paper is organized as follows. Section 2 discusses existing approaches to the problem.
Section 3 introduces our noise-robust loss functions. Section 4 presents and analyzes the experiments
and result. Finally, section 5 concludes our paper.

2 Related Work

Numerous methods have been proposed for learning with noisy labels with DNNs in recent years.
Here, we brieﬂy review the relevant literature. Firstly, Sukhbaatar and Fergus [35] proposed account-
ing for noisy labels with a confusion matrix so that the cross entropy loss becomes

L(θ) =

− log p((cid:101)y = (cid:101)yn|xn, θ) =

− log(

p((cid:101)y = (cid:101)yn|y = i)p(y = i|xn, θ)),

(1)

1
N

N
(cid:88)

n=1

1
N

N
(cid:88)

n=1

c
(cid:88)

i

where c represents number of classes, (cid:101)y represents noisy labels, y represents the latent true labels
and p((cid:101)y = (cid:101)yn|y = i) is the ((cid:101)yn, i)’th component of the confusion matrix. Usually, the real confusion
matrix is unknown. Several methods have been proposed to estimate it [11, 14, 32, 17, 12]. Yet,
accurate estimations can be hard to obtain. Even with the real confusion matrix, training with the
above loss function might be suboptimal for DNNs. Assuming (1) a DNN with enough capacity
to memorize the training set, and (2) a confusion matrix that is diagonally dominant, minimizing
the cross entropy with confusion matrix is equivalent to minimizing the original CCE loss. This
is because the right hand side of Eq. 1 is minimized when p(y = i|xn, θ) = 1 for i = (cid:101)yn and 0
otherwise, ∀ n.

In the context of support vector machines, several theoretically motivated noise-robust loss functions
like the ramp loss, the unhinged loss and the savage loss have been introduced [5, 38, 27]. More
generally, Natarajan et al. [29] presented a way to modify any given surrogate loss function for binary
classiﬁcation to achieve noise-robustness. However, little attention is given to alternative noise robust
loss functions for DNNs. Ghosh et al. [10, 9] proved and empirically demonstrated that MAE is
robust against noisy labels. This paper can be seen as an extension and generalization of their work.

Another popular approach attempts at cleaning up noisy labels. Veit et al. [39] suggested using
a label cleaning network in parallel with a classiﬁcation network to achieve more noise-robust
prediction. However, their method requires a small set of clean labels. Alternatively, one could
gradually replace noisy labels by neural network predictions [33, 36]. Rather than using predictions
for training, Northcutt et al. [31] offered to prune the correct samples based on softmax outputs. As
we demonstrate below, this is similar to one of our approaches. Instead of pruning the dataset once,
our algorithm iteratively prunes the dataset while training until convergence.

Other approaches include treating the true labels as a latent variable and the noisy labels as an
observed variable so that EM-like algorithms can be used to learn true label distribution of the dataset
[41, 18, 37]. Techniques to re-weight conﬁdent samples have also been proposed. Jiang et al. [16]
used a LSTM network on top of a classiﬁcation model to learn the optimal weights on each sample,
while Ren, et al. [34] used a small clean dataset and put more weights on noisy samples which have
gradients closer to that of the clean dataset. In the context of binary classiﬁcation, Liu et al. [24]
derived an optimal importance weighting scheme for noise-robust classiﬁcation. Our method can
also be viewed as re-weighting individual samples; instead of explicitly obtaining weights, we use
the softmax outputs at each iteration as the weightings. Lastly, Azadi et al. [2] proposed a regularizer
that encourages the model to select reliable samples for noise-robustness. Another method that uses

2

knowledge distillation for noisy labels has also been proposed [23]. Both of these methods also
require a smaller clean dataset to work.

3 Generalized Cross Entropy Loss for Noise-Robust Classiﬁcations

3.1 Preliminaries

We consider the problem of k-class classiﬁcation. Let X ⊂ Rd be the feature space and Y =
{1, · · · , c} be the label space. In an ideal scenario, we are given a clean dataset D = {(xi, yi)}n
i=1,
where each (xi, yi) ∈ (X × Y). A classiﬁer is a function that maps input feature space to the label
space f : X → Rc. In this paper, we consider the common case where the function is a DNN with
the softmax output layer. For any loss function L, the (empirical) risk of the classiﬁer f is deﬁned
as RL(f ) = ED[L(f (x), yx)] , where the expectation is over the empirical distribution. The most
commonly used loss for classiﬁcation is cross entropy. In this case, the risk becomes:

RL(f ) = ED[L(f (x; θ), yx)] = −

yij log fj(xi; θ),

(2)

1
n

n
(cid:88)

c
(cid:88)

i=1

j=1

where θ is the set of parameters of the classiﬁer, yij corresponds to the j’th element of one-hot
encoded label of the sample xi, yi = eyi ∈ {0, 1}c such that 1(cid:62)yi = 1 ∀ i, and fj denotes the j’th
element of f . Note that, (cid:80)n
j=1 fj(xi; θ) = 1, and fj(xi; θ) ≥ 0, ∀j, i, θ, since the output layer is a
softmax. The parameters of DNN can be optimized with empirical risk minimization.
We denote a dataset with label noise by Dη = {(xi, (cid:101)yi)}n
respect to each sample such that p((cid:101)yi = k|yi = j, xi) = η(xi)
assumption that noise is conditionally independent of inputs given the true labels so that

i=1 where (cid:101)yi’s are the noisy labels with
jk . In this paper, we make the common

p((cid:101)yi = k|yi = j, xi) = p((cid:101)yi = k|yi = j) = ηjk.
In general, this noise is deﬁned to be class dependent. Noise is uniform with noise rate η, if
ηjk = 1 − η for j = k, and ηjk = η
c−1 for j (cid:54)= k. The risk of classiﬁer with respect to noisy dataset
is then deﬁned as Rη
Let f ∗ be the global minimizer of the risk RL(f ). Then, the empirical risk minimization under loss
function L is deﬁned to be noise tolerant [26] if f ∗ is a global minimum of the noisy risk Rη
A loss function is called symmetric if, for some constant C,

L(f ) = EDη [L(f (x), (cid:101)yx)].

L(f ).

c
(cid:88)

j=1

L(f (x), j) = C,

∀x ∈ X , ∀f.

(3)

c , then under uniform label noise, for any f , Rη

The main contribution of Ghosh et al. [10] is they proved that if loss function is symmetric and
L(f ∗) − Rη
η < c−1
L(f ) ≤ 0. Hence, f ∗ is also the
global minimizer for Rη
L and L is noise tolerant. Moreover, if RL(f ∗) = 0, then L is also noise
tolerant under class dependent noise.

Being a nonsymmetric and unbounded loss function, CCE is sensitive to label noise. On the contrary,
MAE, as a symmetric loss function, is noise robust. For DNNs with a softmax output layer, MAE can
be computed as:

LM AE(f (x), ej) = ||ej − f (x)||1 = 2 − 2fj(x).
With this particular conﬁguration of DNN, the proposed MAE loss is, up to a constant of proportion-
ality, the same as the unhinged loss Lunh(f (x), ej) = 1 − fj(x) [38].

(4)

3.2 Lq Loss for Classiﬁcation

In this section, we will argue that MAE has some drawbacks as a classiﬁcation loss function for
DNNs, which are normally trained on large scale datasets using stochastic gradient based techniques.
Let’s look at the gradient of the loss functions:

∂L(f (xi; θ), yi)
∂θ

=

(cid:40)(cid:80)n
(cid:80)n

n
(cid:88)

i=1

1

fyi (xi;θ) ∇θfyi(xi; θ)

i=1 −
i=1 −∇θfyi(xi; θ)

for CCE
for MAE/unhinged loss.

(5)

3

(a)

(b)

(c)

Figure 1: (a), (b) Test accuracy against number of epochs for training with CCE (orange) and MAE
(blue) loss on clean data with (a) CIFAR-10 and (b) CIFAR-100 datasets. (c) Average softmax
prediction for correctly (solid) and wrongly (dashed) labeled training samples, for CCE (orange) and
Lq (q = 0.7, blue) loss on CIFAR-10 with uniform noise (η = 0.4).

Thus, in CCE, samples with softmax outputs that are less congruent with provided labels, and hence
smaller fyi(xi; θ) or larger 1/fyi(xi; θ), are implicitly weighed more than samples with predictions
that agree more with provided labels in the gradient update. This means that, when training with CCE,
more emphasis is put on difﬁcult samples. This implicit weighting scheme is desirable for training
with clean data, but can cause overﬁtting to noisy labels. Conversely, since the 1/fyi(xi; θ) term is
absent in its gradient, MAE treats every sample equally, which makes it more robust to noisy labels.
However, as we demonstrate empirically, this can lead to signiﬁcantly longer training time before
convergence. Moreover, without the implicit weighting scheme to focus on challenging samples, the
stochasticity involved in the training process can make learning difﬁcult. As a result, classiﬁcation
accuracy might suffer.

To demonstrate this, we conducted a simple experiment using ResNet [13] optimized with the default
setting of Adam [19] on the CIFAR datasets [20]. Fig. 1(a) shows the test accuracy curve when trained
with CCE and MAE respectively on CIFAR-10. As illustrated clearly, it took signiﬁcantly longer
to converge when trained with MAE. In agreement with our analysis, there was also a compromise
in classiﬁcation accuracy due to the increased difﬁculty of learning useful features. These adverse
effects become much more severe when using a more difﬁcult dataset, such as CIFAR-100 (see
Fig. 1(b)). Not only do we observe signiﬁcantly slower convergence, but also a substantial drop in test
accuracy when using MAE. In fact, the maximum test accuracy achieved after 2000 epochs, a long
time after training using CCE has converged, was 38.29%, while CCE achieved an higher accuracy
of 39.92% after merely 7 epochs! Despite its theoretical noise-robustness, due to the shortcoming
during training induced by its noise-robustness, we conclude that MAE is not suitable for DNNs with
challenging datasets like ImageNet.

To exploit the beneﬁts of both the noise-robustness provided by MAE and the implicit weighting
scheme of CCE, we propose using the the negative Box-Cox transformation [4] as a loss function:

Lq(f (x), ej) =

(1 − fj(x)q)
q

,

where q ∈ (0, 1]. Using L’Hôpital’s rule, it can be shown that the proposed loss function is equivalent
to CCE for limq→0 Lq(f (x), ej), and becomes MAE/unhinged loss when q = 1. Hence, this loss is
a generalization of CCE and MAE. Relatedly, Ferrari and Yang [7] viewed the maximization of Eq. 6
as a generalization of maximum likelihood and termed the loss function Lq, which we also adopt.

Theoretically, for any input x, the sum of Lq loss with respect to all classes is bounded by:

Using this bound and under uniform noise with η ≤ 1 − 1

c , we can show (see Appendix)

c − c(1−q)
q

≤

c
(cid:88)

j=1

(1 − fj(x)q)
q

≤

c − 1
q

.

A ≤ (RLq (f ∗) − RLq ( ˆf )) ≤ 0,

4

(6)

(7)

(8)

where A = η[1−c(1−q)]
q(c−1−ηc) < 0, f ∗ is the global minimizer of RLq (f ), and ˆf is the global minimizer
of Rη
(f ). The larger the q, the larger the constant A, and the tighter the bound of Eq. 8. In the
Lq
extreme case of q = 1 (i.e., for MAE), A = 0 and RLq ( ˆf ) = RLq (f ∗). In other words, for q values
approaching 1, the optimum of the noisy risk will yield a risk value (on the clean data) that is close to
( ˆf )) is
f ∗, which implies noise tolerance. It can also be shown that the difference (Rη
Lq
bounded under class dependent noise, provided RLq (f ∗) = 0 and qij < qii ∀i (cid:54)= j (see Thm 2 in
Appendix).

(f ∗) − Rη
Lq

The compromise on noise-robustness when using Lq over MAE prompts an easier learning process.
Let’s look at the gradients of Lq loss to see this:

∂Lq(f (xi; θ), yi)
∂θ

1
fyi(xi; θ)

= fyi(xi; θ)q(−

∇θfyi(xi; θ)) = −fyi(xi; θ)q−1∇θfyi(xi; θ),

where fyi(xi; θ) ∈ [0, 1] ∀ i and q ∈ (0, 1). Thus, relative to CCE, Lq loss weighs each sample by an
additional fyi (xi; θ)q so that less emphasis is put on samples with weak agreement between softmax
outputs and the labels, which should improve robustness against noise. Relative to MAE, a weighting
of fyi(xi; θ)q−1 on each sample can facilitate learning by giving more attention to challenging
datapoints with labels that do not agree with the softmax outputs. On one hand, larger q leads to a
more noise-robust loss function. On the other hand, too large of a q can make optimization strenuous.
Hence, as we will demonstrate empirically below, it is practically useful to set q between 0 and 1,
where a tradeoff equilibrium is achieved between noise-robustness and better learning dynamics.

3.3 Truncated Lq Loss
Since a tighter bound in (cid:80)c
truncated Lq loss:

j=1 L(f (x, j)) would imply stronger noise tolerance, we propose the

Ltrunc(f (x), ej) =

(cid:26)Lq(k)

Lq(f (x), ej)

if fj(x) ≤ k
if fj(x) > k

where 0 < k < 1, and Lq(k) = (1 − kq)/q. Note that, when k → 0, the truncated Lq loss becomes
the normal Lq loss. Assuming k ≥ 1/c, the sum of truncated Lq loss with respect to all classes is
bounded by (see Appendix):

dLq(

) + (c − d)Lq(k) ≤

Ltrunc(f (x), ej) ≤ cLq(k),

(10)

1
d

c
(cid:88)

j=1

where d = max(1, (1−q)1/q
for the truncated Lq loss, Lq(k), is smaller than that for the Lq loss of Eq. 7, if

). It can be veriﬁed that the difference between upper and lower bounds

k

(9)

(11)

d[Lq(k) − Lq(

)] <

1
d

c(1−q) − 1
q

.

As an example, when k ≥ 0.3, the above inequality is satisﬁed for all q and c. When k ≥ 0.2, the
inequality is satisﬁed for all q and c ≥ 10. Since the derived bounds in Eq. 7 and Eq. 10 are tight,
introducing the threshold k can thus lead to a more noise tolerant loss function.

If the softmax output for the provided label is below a threshold, truncated Lq loss becomes a constant.
Thus, the loss gradient is zero for that sample, and it does not contribute to learning dynamics. While
Eq. 10 suggests that a larger threshold k leads to tighter bounds and hence more noise-robustness,
too large of a threshold would precipitate too many discarded samples for training. Ideally, we
would want the algorithm to train with all available clean data and ignore noisy labels. Thus the
optimal choice of k would depend on the noise in the labels. Hence, k can be treated as a (bounded)
hyper-parameter and optimized. In our experiments, we set k = 0.5 that yields a tighter bound for
truncated Lq loss, and which we observed to work well empirically.

A potential problem arises when training directly with this loss function. When the threshold is
relatively large (e.g., k = 0.5 in a 10-class classiﬁcation problem), at the beginning of the training
phase, most of the softmax outputs can be signiﬁcantly smaller than k, resulting in a dramatic drop

5

argmin
θ

n
(cid:88)

i=1

n
(cid:88)

i=1

in the number of effective samples. Moreover, it is suboptimal to prune samples based on softmax
values at the beginning of training. To circumvent the problem, observe that, by deﬁnition of the
truncated Lq loss:

Ltrunc(f (xi; θ), yi) = argmin

viLq(f (xi; θ), yi) + (1 − vi)Lq(k),

(12)

n
(cid:88)

i=1

θ

where vi = 0 if fyi(xi) ≤ k and vi = 1 otherwise, and θ represents the parameters of the classiﬁer.
Optimizing the above loss is the same as optimizing the following:

argmin
θ

viLq(f (xi; θ), yi) − viLq(k) = argmin
θ,w∈[0,1]n

wiLq(f (xi; θ), yi) − Lq(k)

wi,

n
(cid:88)

i=1

n
(cid:88)

i=1

(13)

because for any θ, the optimal wi is 1 if Lq(f (xi; θ), yi) ≤ Lq(k) and 0 if Lq(f (xi; θ), yi) > Lq(k).
Hence, we can optimize the truncated Lq loss by optimizing the right hand side of Eq. 13. If Lq is
convex with respect to the parameters θ, optimizing Eq. 13 is a biconvex optimization problem, and
the alternative convex search (ACS) algorithm [3] can be used to ﬁnd the global minimum. ACS
iteratively optimizes θ and w while keeping the other set of parameters ﬁxed. Despite the high
non-convexity of DNNs, we can apply ACS to ﬁnd a local minimum. We refer to the update of w as
"pruning". At every step of iteration, pruning can be carried out easily by computing f (xi; θ(t)) for
all training samples. Only samples with fyi (xi; θ(t)) ≥ k and Lq(f (xi; θ), yi) ≤ Lq(k) are kept for
updating θ during that iteration (and hence wi = 1 ). The additional computational complexity from
the pruning steps is negligible. Interestingly, the resulting algorithm is similar to that of self-paced
learning [22].

Algorithm 1 ACS for Training with Lq Loss
Input Noisy dataset Dη, total iterations T , threshold k

i = 1 ∀ i

Initialize w(0)
Update θ(0) = argminθ
while t < T do

Update w(t) = argminw
Update θ(t) = argminθ

(cid:80)n

i Lq(f (xi; θ), yi) − Lq(k) (cid:80)n
i=1 w(0)
i=1 w(0)
(cid:80)n
i=1 wiLq(f (xi; θ(t−1)), yi) − Lq(k) (cid:80)n
(cid:80)n
i Lq(f (xi; θ), yi) − Lq(k) (cid:80)n
i=1 w(t)

i=1 w(t)

i

i

i=1 wi [Pruning Step]

Output θ(T )

4 Experiments

The following setup applies to all of the experiments conducted. Noisy datasets were produced by
artiﬁcially corrupting true labels. 10% of the training data was retained for validation. To realistically
mimic a noisy dataset while justiﬁably analyzing the performance of the proposed loss function, only
the training and validation data were contaminated, and test accuracies were computed with respect
to true labels. A mini-batch size of 128 was used. All networks used ReLUs in the hidden layers
and softmax layers at the output. All reported experiments were repeated ﬁve times with random
initialization of neural network parameters and randomly generated noisy labels each time. We
compared the proposed functions with CCE, MAE and also the confusion matrix-corrected CCE, as
shown in Eq. 1. Following [32], we term this "forward correction". All experiments were conducted
with identical optimization procedures and architectures, changing only the loss functions.

4.1 Toward a Better Understanding of Lq Loss

To better grasp the behavior of Lq loss, we implemented different values of q and uniform noise
at different noise levels, and trained ResNet-34 with the default setting of Adam on CIFAR-10.
As shown in Fig. 2, when trained on clean dataset, increasing q not only slowed down the rate of
convergence, but also lowered the classiﬁcation accuracy. More interesting phenomena appeared
when trained on noisy data. When CCE (q = 0) was used, the classiﬁer ﬁrst learned predictive

6

(a)

(b)

(c)

(d)

(e)

(f)

Figure 2: The test accuracy and validation loss against number of epochs for training with Lq loss at
different values of q. (a) and (d): η = 0.0; (b) and (e): η = 0.2; (c) and (f): η = 0.6.

patterns, presumably from the noise-free labels, before overﬁtting strongly to the noisy labels, in
agreement with Arpit et al.’s observations [1]. Training with increased q values delayed overﬁtting
and attained higher classiﬁcation accuracies. One interpretation of this behavior is that the classiﬁer
could learn more about predictive features before overﬁtting. This interpretation is supported by our
plot of the average softmax values with respect to the correctly and wrongly labeled samples on the
training set for CCE and Lq (q = 0.7) loss, and with 40% uniform noise (Fig. 1(c)). For CCE, the
average softmax for wrongly labeled samples remained small at the beginning, but grew quickly when
the model started overﬁtting. Lq loss, on the other hand, resulted in signiﬁcantly smaller softmax
values for wrongly labeled data. This observation further serves as an empirical justiﬁcation for the
use of truncated Lq loss as described in section 3.3.

We also observed that there was a threshold of q beyond which overﬁtting never kicked in before
convergence. When η = 0.2 for instance, training with Lq loss with q = 0.8 produced an overﬁtting-
free training process. Empirically, we noted that, the noisier the data, the larger this threshold is.
However, too large of a q hampers the classiﬁcation accuracy, and thus a larger q is not always
preferred. In general, q can be treated as a hyper-parameter that can be optimized, say via monitoring
validation accuracy. In remaining experiments, we used q = 0.7, which yielded a good compromise
between fast convergence and noise robustness (no overﬁtting was observed for η ≤ 0.5).

4.2 Datasets

CIFAR-10/CIFAR-100: ResNet-34 was used as the classiﬁer optimized with the loss functions
mentioned above. Per-pixel mean subtraction, horizontal random ﬂip and 32 × 32 random crops after
padding with 4 pixels on each side was performed as data preprocessing and augmentation. Following
[15], we used stochastic gradient descent (SGD) with 0.9 momentum, a weight decay of 10−4 and
learning rate of 0.01, and divided it by 10 after 40 and 80 epochs (120 in total) for CIFAR-10, and
after 80 and 120 (150 in total) for CIFAR-100. To ensure a fair comparison, the identical optimization
scheme was used for truncated Lq loss. We trained with the entire dataset for the ﬁrst 40 epochs for
CIFAR-10 and 80 for CIFAR-100, and started pruning and training with the pruned dataset afterwards.
Pruning was done every 10 epochs. To prevent overﬁtting, we used the model at the optimal epoch

7

Table 1: Average test accuracy and standard deviation (5 runs) on experiments with closed-set noise.
We report accuracies of the epoch where validation accuracy is maximum. Forward T and ˆT represent
forward correction with the true and estimated confusion matrices, respectively [32]. q = 0.7 was
used for all experiments with Lq loss and truncated Lq loss. Best 2 accuracies are bold faced.

Datasets

Loss Functions

Uniform Noise
Noise Rate η

FASHION
MNIST

CIFAR-10

CIFAR-100

CCE
MAE
Forward T
Forward ˆT
Lq
Trunc Lq
CCE
MAE
Forward T
Forward ˆT
Lq
Trunc Lq
CCE
MAE
Forward T
Forward ˆT
Lq
Trunc Lq

0.2
93.24 ± 0.12
80.39 ± 4.68
93.64 ± 0.12
93.26 ± 0.10
93.35 ± 0.09
93.21 ± 0.05
86.98 ± 0.44
83.72 ± 3.84
88.63 ± 0.14
87.99 ± 0.36
89.83 ± 0.20
89.7 ± 0.11
58.72 ± 0.26
15.80 ± 1.38
63.16 ± 0.37
39.19 ± 2.61
66.81 ± 0.42
67.61 ± 0.18

0.4
92.09 ± 0.18
79.30 ± 6.20
92.69 ± 0.20
92.24 ± 0.15
92.58 ± 0.11
92.60 ± 0.17
81.88 ± 0.29
67.00 ± 4.45
85.07 ± 0.29
83.25 ± 0.38
87.13 ± 0.22
87.62 ± 0.26
48.20 ± 0.65
9.03 ± 1.54
54.65 ± 0.88
31.05 ± 1.44
61.77 ± 0.24
62.64 ± 0.33

0.6
90.29 ± 0.35
82.41 ± 5.29
91.16 ± 0.16
90.54 ± 0.10
91.30 ± 0.20
91.56 ± 0.16
74.14 ± 0.56
64.21 ± 5.28
79.12 ± 0.64
74.96 ± 0.65
82.54 ± 0.23
82.70 ± 0.23
37.41 ± 0.94
7.74 ± 1.48
44.62 ± 0.82
19.12 ± 1.95
53.16 ± 0.78
54.04 ± 0.56

0.8
86.20 ± 0.68
74.73 ± 5.26
87.59 ± 0.35
85.57 ± 0.86
88.01 ± 0.22
88.33 ± 0.38
53.82 ± 1.04
38.63 ± 2.62
64.30 ± 0.70
54.64 ± 0.44
64.07 ± 1.38
67.92 ± 0.60
18.10 ± 0.82
3.76 ± 0.27
24.83 ± 0.71
8.99 ± 0.58
29.16 ± 0.74
29.60 ± 0.51

0.1
94.06 ± 0.05
74.03 ± 6.32
94.33 ± 0.10
94.09 ± 0.10
93.51 ± 0.17
93.53 ± 0.11
90.69 ± 0.17
82.61 ± 4.81
91.32 ± 0.21
90.52 ± 0.26
90.91 ± 0.22
90.43 ± 0.25
66.54 ± 0.42
13.38 ± 1.84
71.05 ± 0.30
45.96 ± 1.21
68.36 ± 0.42
68.86 ± 0.14

Class Dependent Noise
Noise Rate η

0.2
93.72 ± 0.14
63.03 ± 3.91
94.03 ± 0.11
93.66 ± 0.09
93.24 ± 0.14
93.36 ± 0.07
88.59 ± 0.34
52.93 ± 3.60
90.35 ± 0.26
89.09 ± 0.47
89.33 ± 0.17
89.45 ± 0.29
59.20 ± 0.18
11.50 ± 1.16
71.08 ± 0.22
42.46 ± 2.16
66.59 ± 0.22
66.59 ± 0.23

0.3
92.72 ± 0.21
58.14 ± 0.14
93.91 ± 0.14
93.52 ± 0.16
92.21 ± 0.27
92.76 ± 0.14
86.14 ± 0.40
50.36 ± 5.55
89.25 ± 0.43
86.79 ± 0.36
85.45 ± 0.74
87.10 ± 0.22
51.40 ± 0.16
8.91 ± 0.89
70.76 ± 0.26
38.13 ± 2.97
61.45 ± 0.26
61.87 ± 0.39

0.4
89.82 ± 0.31
56.04 ± 3.76
93.65 ± 0.11
88.53 ± 4.81
89.53 ± 0.53
91.62 ± 0.34
80.11 ±1.44
45.52 ± 0.13
88.12 ± 0.32
83.55 ± 0.58
76.74± 0.61
82.28 ± 0.67
42.74 ± 0.61
8.20 ± 1.04
70.82 ± 0.45
34.44 ± 1.93
47.22 ± 1.15
47.66 ± 0.69

Table 2: Average test accuracy on experiments with CIFAR-10. We replicated the exact experimental
setup as in [40]. The reported accuracies are the average last epoch accuracies after training for 100
epochs. η = 40%. CCE, Forward and method by Wang et al. are adapted for direct comparison.

Noise type
CIFAR-10 + CIFAR-100 (open-set noise)
CIFAR-10 (closed-set noise)

CCE [40]
62.92
62.38

Forward [40] Wang, et al. [40] MAE Lq
79.28
78.15

64.18
77.81

75.06
74.31

71.10
64.79

Trunc Lq
79.55
79.12

based on maximum validation accuracy for pruning. Uniform noise was generated by mapping a true
label to a random label through uniform sampling. Following Patrini, et al. [32] class dependent noise
was generated by mapping TRUCK → AUTOMOBILE, BIRD → AIRPLANE, DEER → HORSE,
and CAT ↔ DOG with probability η for CIFAR-10. For CIFAR-100, we simulated class-dependent
noise by ﬂipping each class into the next circularly with probability η.

We also tested noise-robustness of our loss function on open-set noise using CIFAR-10. For a direct
comparison, we followed the identical setup as described in [40]. For this experiment, the classiﬁer
was trained for only 100 epochs. We observed validation loss plateaued after about 10 epochs, and
hence started pruning the data afterwards at 10-epoch intervals. The open-set noise was generated by
using images from the CIFAR-100 dataset. A random CIFAR-10 label was assigned to these images.

FASHION-MNIST: ResNet-18 was used. The identical data preprocessing, augmentation, and
optimization procedure as in CIFAR-10 was deployed for training. To generate a realistic class
dependent noise, we used the t-SNE [25] plot of the dataset to associated classes with similar
embeddings, and mapped BOOT → SNEAKER , SNEAKER → SANDALS, PULLOVER → SHIRT,
COAT ↔ DRESS with probability η.

4.3 Results and Discussion

Experimental results with closed-set noise is summarized in Table 1. For uniform noise, proposed loss
functions outperformed the baselines signiﬁcantly, including forward correction with the ground truth
confusion matrices. In agreement with our theoretical expectations, truncating the Lq loss enhanced
results. For class dependent noise, in general Forward T offered the best performance, as it relied on
the knowledge of the ground truth confusion matrix. Truncated Lq loss produced similar accuracies
as Forward ˆT for FASHION-MNIST and better results for CIFAR datasets, and outperformed the
other baselines at most noise levels for all datasets. While using Lq loss improved over baselines for
CIFAR-100, no improvements were observed for FASHION-MNIST and CIFAR-10 datasets. We
believe this is in part because very similar classes were grouped together for the confusion matrices
and consquently the DNNs might falsely put high conﬁdence on wrongly labeled samples.

8

In general, classiﬁcation accuracy for both uniform and class dependent noise would be further
improved relative to baselines with optimized q and k values and more number of epochs. Based on
the experimental results, we believe the proposed approach would work well when correctly labeled
data can be differentiated from wrongly labeled data based on softmax outputs, which is often the
case with large-scale data and expressive models. We also observed that MAE performed poorly for
all datasets at all noise levels, presumably because DNNs like ResNet struggled to optimize with
MAE loss, especially on challenging datasets such as CIFAR-100.

Table 2 summarizes the results for open-set noise with CIFAR-10. Following Wang et al. [40], we
reported the last-epoch test accuracy after training for 100 epochs. We also repeated the closed-set
noise experiment with their setup. Using Lq loss noticeably prevented overﬁtting, and using truncated
Lq loss achieved better results than the state-of-the-art method for open-set noise reported in [40].
Moreover, our method is signiﬁcantly easier to implement. Lastly, note that the poor performance of
Lq loss compared to MAE is due to the fact that test accuracy reported here is long after the model
started overﬁtting, since a shallow CNN without data augmentation was deployed for this experiment.

In conclusion, we proposed theoretically grounded and easy-to-use classes of noise-robust loss
functions, the Lq loss and the truncated Lq loss, for classiﬁcation with noisy labels that can be
employed with any existing DNN algorithm. We empirically veriﬁed noise robustness on various
datasets with both closed- and open-set noise scenarios.

This work was supported by NIH R01 grants (R01LM012719 and R01AG053949), the NSF NeuroNex
grant 1707312, and NSF CAREER grant (1748377).

5 Conclusion

Acknowledgments

References

[1] Devansh Arpit, Stanisław Jastrz˛ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio,
Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A
closer look at memorization in deep networks. arXiv preprint arXiv:1706.05394, 2017.

[2] Samaneh Azadi, Jiashi Feng, Stefanie Jegelka, and Trevor Darrell. Auxiliary image regulariza-

tion for deep cnns with noisy labels. arXiv preprint arXiv:1511.07069, 2015.

[3] Mokhtar S Bazaraa, Hanif D Sherali, and Chitharanjan M Shetty. Nonlinear programming:

theory and algorithms. John Wiley & Sons, 2013.

[4] George EP Box and David R Cox. An analysis of transformations. Journal of the Royal

Statistical Society. Series B (Methodological), pages 211–252, 1964.

[5] J Paul Brooks. Support vector machines with the ramp loss and the hard margin loss. Operations

research, 59(2):467–479, 2011.

[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on, pages 248–255. IEEE, 2009.

[7] Davide Ferrari, Yuhong Yang, et al. Maximum lq-likelihood estimation. The Annals of Statistics,

38(2):753–783, 2010.

[8] Benoît Frénay and Michel Verleysen. Classiﬁcation in the presence of label noise: a survey.

IEEE transactions on neural networks and learning systems, 25(5):845–869, 2014.

[9] Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for

deep neural networks. In AAAI, pages 1919–1925, 2017.

[10] Aritra Ghosh, Naresh Manwani, and PS Sastry. Making risk minimization tolerant to label

noise. Neurocomputing, 160:93–107, 2015.

9

[11] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adapta-

tion layer. 2016.

[12] Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou,

Ivor Tsang, Ya Zhang, and
Masashi Sugiyama. Masking: A New Perspective of Noisy Supervision. arXiv preprint
arXiv:1805.08193, 2018.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.

[14] Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to
train deep networks on labels corrupted by severe noise. arXiv preprint arXiv:1802.05300,
2018.

[15] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with
stochastic depth. In European Conference on Computer Vision, pages 646–661. Springer, 2016.

[16] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Regularizing
very deep neural networks on corrupted labels. arXiv preprint arXiv:1712.05055, 2017.

[17] Ishan Jindal, Matthew Nokleby, and Xuewen Chen. Learning deep networks from noisy labels
with dropout regularization. In Data Mining (ICDM), 2016 IEEE 16th International Conference
on, pages 967–972. IEEE, 2016.

[18] Ashish Khetan, Zachary C Lipton, and Anima Anandkumar. Learning from noisy singly-labeled

data. arXiv preprint arXiv:1712.04577, 2017.

[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

2009.

[20] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.

[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems, pages
1097–1105, 2012.

[22] M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In Advances in Neural Information Processing Systems, pages 1189–1197, 2010.

[23] Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Jia Li. Learning from

noisy labels with distillation. arXiv preprint arXiv:1703.02391, 2017.

[24] Tongliang Liu and Dacheng Tao. Classiﬁcation with noisy labels by importance reweighting.
IEEE Transactions on pattern analysis and machine intelligence, 38(3):447–461, 2016.

[25] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine

learning research, 9(Nov):2579–2605, 2008.

[26] Naresh Manwani and PS Sastry. Noise tolerance under risk minimization. IEEE transactions

on cybernetics, 43(3):1146–1151, 2013.

[27] Hamed Masnadi-Shirazi and Nuno Vasconcelos. On the design of loss functions for classi-
ﬁcation: theory, robustness to outliers, and savageboost. In Advances in neural information
processing systems, pages 1049–1056, 2009.

[28] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.

[29] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning
with noisy labels. In Advances in neural information processing systems, pages 1196–1204,
2013.

10

[30] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for
semantic segmentation. In Proceedings of the IEEE International Conference on Computer
Vision, pages 1520–1528, 2015.

[31] Curtis G Northcutt, Tailin Wu, and Isaac L Chuang. Learning with conﬁdent examples: Rank
pruning for robust classiﬁcation with noisy labels. arXiv preprint arXiv:1705.01936, 2017.

[32] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu.
Making deep neural networks robust to label noise: a loss correction approach. stat, 1050:22,
2017.

[33] Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint
arXiv:1412.6596, 2014.

[34] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples

for robust deep learning. arXiv preprint arXiv:1803.09050, 2018.

[35] Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks.

arXiv preprint arXiv:1406.2080, 2(3):4, 2014.

[36] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization

framework for learning with noisy labels. arXiv preprint arXiv:1803.11364, 2018.

[37] Arash Vahdat. Toward robustness against label noise in training deep discriminative neural

networks. In Advances in Neural Information Processing Systems, pages 5601–5610, 2017.

[38] Brendan Van Rooyen, Aditya Menon, and Robert C Williamson. Learning with symmetric
label noise: The importance of being unhinged. In Advances in Neural Information Processing
Systems, pages 10–18, 2015.

[39] Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie.
Learning from noisy large-scale datasets with minimal supervision. In The Conference on
Computer Vision and Pattern Recognition, 2017.

[40] Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao
Xia. Iterative learning with open-set noisy labels. arXiv preprint arXiv:1804.00092, 2018.

[41] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive
noisy labeled data for image classiﬁcation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2691–2699, 2015.

[42] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.

11

Appendix

Lemma 1. limq→0 Lq(f (x), ej) = LC(f (x), ej), where Lq represents the Lq loss, and LC repre-
sents the categorical cross entropy loss.

Proof. from equation 6, and using L’Hôpital’s rule,

lim
q→0

Lq(f (x), ej) = lim
q→0

(1 − fj(x)q)
q

= lim
q→0

d

dq (1 − fj(x)q)
d
dq q

= lim
q→0

−fj(x)q log(fj(x)) = − log(fj(x)) = LC(f (x), ej).

Lemma 2. For any x and q ∈ (0, 1], the sum of Lq loss with respect to all classes is bounded by:

c − c(1−q)
q

≤

c
(cid:88)

j=1

(1 − fj(x)q)
q

≤

c − 1
q

.

(14)

Proof. Observe that, since we have a softmax layer at the end, fj(x) ≤ 1 for all j, and (cid:80)c
1. Now, since q ∈ (0, 1], we have fj(x) ≤ fj(x)q, and (1 − fj(x)) ≥ (1 − fj(x)q). Hence,

j=1 fj(x) =

c
(cid:88)

j=1

(1 − fj(x)q)
q

≤

c
(cid:88)

j=1

(1 − fj(x))
q

=

c − (cid:80)c

j=1 fj(x)
q

=

c − 1
q

.

Moreover, since (cid:80)c
(cid:80)c

j=1(1 − (1/c)q), and

j=1 fj(x)q ≤ (cid:80)c

j=1(1/c)q for all x and q ∈ (0, 1], (cid:80)c

j=1(1 − fj(x)q) ≥

c
(cid:88)

j=1

(1 − fj(x)q)
q

≥

(1 − (1/c)q)
q

=

c − c(1−q)
q

.

c
(cid:88)

j=1

Theorem 1. Under uniform noise with η ≤ 1 − 1
c ,

and

0 ≤ (Rη
Lq

(f ∗) − Rη
Lq

( ˆf )) ≤ A,

A(cid:48) ≤ RLq (f ∗) − RLq ( ˆf ) ≤ 0,

(15)

(16)

where A = η[c(1−q)−1]
the global minimizer of Rη
Lq

q(c−1) ≥ 0, A(cid:48) = η[1−c(1−q)]
(f ).

q(c−1−ηc) < 0, f ∗ is the global minimizer of RLq (f ), and ˆf is

Proof. Recall that for any softmax output f ,

RLq (f ) = ED[Lq(f (x), yx)] = Ex,yx[Lq(f (x), yx)],

and since for uniform noise with noise rate η, ηjk = 1 − η for j = k, and ηjk = η
have

c−1 for j (cid:54)= k, we

12

Rη
Lq

(f ) = ED[Lq(f (x), (cid:101)yx)] = Ex,(cid:101)yx[Lq(f (x), (cid:101)yx)]

= ExEyx|xE
(cid:101)yx|yx,x[Lq(f (x), (cid:101)yx)]
= ExEyx|x[(1 − η)Lq(f (x), yx) +

Lq(f (x), i)]

= ExEyx|x[(1 − η)Lq(f (x), yx) +

Lq(f (x), i) − Lq(f (x), yx))]

= (1 − η)RLq (f ) −

RLq (f ) +

ExEyx|x[

Lq(f (x), i)]

η
c − 1

c
(cid:88)

i=1

= (1 −

)RLq (f ) +

ExEyx|x[

Lq(f (x), i)]

ηc
c − 1

η
c − 1

η
c − 1

η
c − 1

(cid:88)

i(cid:54)=yx
c
(cid:88)
(

i=1

η
c − 1

c
(cid:88)

i=1

Now, from Lemma 2, we have:

(1 −

)RLq (f ) +

ηc
c − 1

η[c − c(1−q)]
q(c − 1)

≤ Rη
Lq

(f ) ≤ (1 −

)RLq (f ) +

ηc
c − 1

η
q

.

We can also write the inequality in terms of RLq (f ):

(Rη
Lq

(f ) −

)/(1 −

) ≤ RLq (f )) ≤ (Rη
Lq

(f ) −

η
q

ηc
c − 1

η[c − c(1−q)]
q(c − 1)

)/(1 −

ηc
c − 1

)

Thus, for ˆf ,

or equivalently,

Rη
Lq

(f ∗) − Rη
Lq

( ˆf ) ≤ A + (1 −

)(RLq (f ∗) − RLq ( ˆf )) ≤ A,

ηc
c − 1

RLq (f ∗) − RLq ( ˆf ) ≥ A(cid:48) + (Rη
Lq

(f ∗) − Rη
Lq

( ˆf ))/(1 −

ηc
c − 1

) ≥ A(cid:48)

q(c−1−ηc) , since η ≤ c−1

q(c−1) ≥ 0 and A(cid:48) = η[1−c(1−q)]

where A = η[c(1−q)−1]
RLq (f ). Lastly, since ˆf is the minimizer of Rη
Lq
RLq (f ∗) − RLq ( ˆf ) ≤ 0 . This completes the proof.
Remark. Note that, when q = 1, A = 0, and f ∗ is also minimizer of risk under uniform noise.
Theorem 2. Under class dependent noise when ηij < (1 − ηi), ∀j (cid:54)= i, ∀i, j ∈ [c], where
ηij = p((cid:101)y = j|y = i), ∀j (cid:54)= i, and (1 − ηi) = p((cid:101)y = i|y = i), if RLq (f ∗) = 0, then

c , and f ∗ is a minimizer of
( ˆf ) ≥ 0, or

(f ), we have that Rη
Lq

(f ∗) − Rη
Lq

0 ≤ (Rη
Lq

(f ∗) − Rη
Lq

( ˆf )) ≤ B,

(17)

ED(1 − ηyx) ≥ 0, f ∗ is the global minimizer of RLq (f ), and ˆf is the global

where B = c1−q−1
q
minimizer of Rη
Lq

(f ).

Proof. For class dependent noise, from Lemma 2, for any soft-max output function f we have

Rη
Lq

(f ) = ED[(1 − ηyx)Lq(f (x), yx)] + ED[

ηyxiLq(f (x), i)]

(cid:88)

i(cid:54)=yx

≤ ED[(1 − ηyx)(

Lq(f (x), i))] + ED[

ηyxiLq(f (x), i)]

c − 1
q

(cid:88)

−

i(cid:54)=yx

(cid:88)

i(cid:54)=yx

=

c − 1
q

ED(1 − ηyx) − ED[

(1 − ηyx − ηyxi)Lq(f (x), i)],

(cid:88)

i(cid:54)=yx

13

and

Hence,

Rη
Lq

(f ) ≥

c − c1−q
q

ED(1 − ηyx) − ED[

(1 − ηyx − ηyxi)Lq(f (x), i)].

(cid:88)

i(cid:54)=yx

(Rη
Lq

(f ∗) − Rη
Lq

( ˆf )) ≤

ED(1 − ηyx)+

c1−q − 1
q
(cid:88)

ED

i(cid:54)=yx

(1 − ηyx − ηyxi)[Lq( ˆf (x), i) − Lq(f ∗(x), i)].

Now, from our assumption that RLq (f ∗) = 0, we have Lq(f ∗(x), yx) = 0. This is only satisﬁed iff
f ∗
i (x) = 0 if i (cid:54)= yx. Hence, Lq(f ∗(x), i) = 1/q ∀i (cid:54)= yx. Moreover,
i (x) = 1 when i = yx, and f ∗
by our assumption, we have (1 − ηyx − ηyxi) > 0. As a result, to derive a upper bound for the
expression above, we need to maximize the second term. Note that by deﬁnition of the Lq loss,
Lq( ˆf (x), i) ≤ 1/q ∀i ∈ [c], and hence the second term is maximized iff Lq( ˆf (x), i) = 1/q ∀i (cid:54)= yx.
This implies that the maximum of the second term is non-positive, so we have

(Rη
Lq

(f ∗) − Rη
Lq

( ˆf )) ≤

c1−q − 1
q

ED(1 − ηyx).

Lastly, since ˆf is the minimizer of Rη
Lq
the proof.

(f ), we have that Rη
Lq

(f ∗) − Rη
Lq

( ˆf ) ≥ 0. This completes

Lemma 3. For any x and q ∈ (0, 1), assuming 1/c ≤ k < 1 where c represents the number of
classes, the sum of truncated Lq loss with respect to all classes is bounded by:

˜dkLq(

) + (c − ˜d)Lq(k) ≤

Ltrunc(f (x), ej) ≤ cLq(k),

(18)

c
(cid:88)

j=1

where ˜d = max(1, (1−q)1/q

).

k

Proof. For the upper bound, by deﬁnition of truncated Lq, Ltrunc(f (x), ej) ≤ Lq(k) for any x and
j. Hence, (cid:80)c

j=1 Ltrunc(f (x), ej) ≤ cLq(k).

For the lower bound, it can be veriﬁed that,

Ltrunc( ˜f (x), ej) ≤

Ltrunc(f (x), ej)

c
(cid:88)

j=1

1
d

c
(cid:88)

j=1

where ˜f (x) = (p, · · · , p, 0, · · · , 0), with p = 1/d ≥ k and d is the number of elements in f (x) with
a value ≤ k. Note that since p > k, 1 ≤ d ≤ 1/k:

c
(cid:88)

j=1

Ltrunc( ˜f (x), ej) = dLq(p) + (c − d)Lq(k) = dLq(

) + (c − d)Lq(k).

1
d

We can get a universal lower bound (that does not depend on f ) by minimizing the above function
with respect to d. To do so, we treat d to be continuous. By deﬁnition of Lq loss, and recall that
0 < q < 1,

1
d

dLq(

) + (c − d)Lq(k) = min

1
min
d
d∈[1,1/k]
We can verify using the second derivative test that the above objective function is convex. As a result,
we can ﬁnd the minimum by taking its derivative. Doing so, we ﬁnd that d = (1−q)1/q
minimizes the
above objective function. Hence, the lower bound is

)q)/q − (1 − kq)/q] = min

d[(kq − (

d[(1 − (

d∈[1,1/k]

d∈[1,1/k]

1
d

k

)q)].

˜dkLq(

) + (c − ˜d)Lq(k) ≤

Ltrunc(f (x), ej),

1
d

c
(cid:88)

j=1

where ˜d = max(1, (1−q)1/q

).

k

Remark. Using Lemma 3, we can prove that the proposed truncated loss leads to more noise robust
training following the same arguments as in Theorem 1 and 2.

14

