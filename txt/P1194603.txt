9
1
0
2
 
b
e
F
 
5
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
8
1
7
0
.
2
0
8
1
:
v
i
X
r
a

The Gaussian Process Autoregressive Regression Model (GPAR)

James Requeima12

Will Tebbutt12

Wessel Bruinsma12

Richard E. Turner1

1University of Cambridge and 2Invenia Labs, Cambridge, UK
{jrr41, wct23, wpb23, ret26}@cam.ac.uk

Abstract

Multi-output regression models must exploit
dependencies between outputs to maximise
predictive performance. The application of
Gaussian processes (GPs) to this setting typi-
cally yields models that are computationally
demanding and have limited representational
power. We present the Gaussian Process Au-
toregressive Regression (GPAR) model, a scal-
able multi-output GP model that is able to
capture nonlinear, possibly input-varying, de-
pendencies between outputs in a simple and
tractable way: the product rule is used to
decompose the joint distribution over the out-
puts into a set of conditionals, each of which
is modelled by a standard GP. GPAR’s eﬃ-
cacy is demonstrated on a variety of synthetic
and real-world problems, outperforming exist-
ing GP models and achieving state-of-the-art
performance on established benchmarks.

1

Introduction

The Gaussian process (GP) probabilistic modelling
framework provides a powerful and popular approach
to nonlinear single-output regression (Rasmussen and
Williams, 2006). The popularity of GP methods stems
from their modularity, tractability, and interpretability:
it is simple to construct rich, nonlinear models by com-
positional covariance function design, which can then
be evaluated in a principled way (e.g. via the marginal
likelihood), before being interpreted in terms of their
component parts. This leads to an attractive plug-
and-play approach to modelling and understanding
data, which is so robust that it can even be automated
(Duvenaud et al., 2013; Sun et al., 2018).

Most regression problems, however, involve multiple
outputs rather than a single one. When modelling such
data, it is key to capture the dependencies between
these outputs. For example, noise in the output space

Proceedings of the 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2019, Naha,
Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by
the author(s).

might be correlated, or, whilst one output might de-
pend on the inputs in a complex (deterministic) way,
it may depend quite simply on other output variables.
In both cases multi-output GP models are required.
There is a plethora of existing multi-output GP models
that can capture linear correlations between output
variables if these correlations are ﬁxed across the in-
put space (Goovaerts, 1997; Wackernagel, 2003; Teh
and Seeger, 2005; Bonilla et al., 2008; Nguyen and
Bonilla, 2014; Dai et al., 2017). However, one of the
main reasons for the popularity of the GP approach is
that a suite of diﬀerent types of nonlinear input depen-
dencies can be modelled, and it is disappointing that
this ﬂexibility is not extended to interactions between
the outputs. There are some approaches that do allow
limited modelling of nonlinear output dependencies
(Wilson et al., 2012; Bruinsma, 2016) but this ﬂexibil-
ity comes from sacriﬁcing tractability, with complex
and computationally demanding approximate inference
and learning schemes now required. This complexity
signiﬁcantly slows down model ﬁtting, evaluation, and
improvement work ﬂow.

What is needed is a ﬂexible and analytically tractable
modelling approach to multi-output regression that
supports plug-and-play modelling and model interpre-
tation. The Gaussian Process Autoregressive Regres-
sion (GPAR) model, introduced in Section 2, achieves
these aims by taking an approach analogous to that em-
ployed by the Neural Autoregressive Density Estimator
(Larochelle and Murray, 2011) for density modelling.
The product rule is used to decompose the distribu-
tion of the outputs given the inputs into a set of one-
dimensional conditional distributions. Critically, these
distributions can be interpreted as a decoupled set of
single-output regression problems, and learning and in-
ference in GPAR therefore amount to a set of standard
single-output GP regression tasks: training is closed
form, fast, and amenable to standard scaling techniques.
GPAR converts the modelling of output dependencies
that are possibly nonlinear and input-dependent into a
set of standard GP covariance function design problems,
constructing expressive, jointly non-Gaussian models
over the outputs. Importantly, we show how GPAR can
capture nonlinear relationships between outputs as well
as structured, input-dependent noise, simply through

The Gaussian Process Autoregressive Regression Model (GPAR)

CO2 C(t)

to some unknown, random function f3; et cetera:

f1( , t)
=
Temperature T (t)

y1(x) = f1(x),
y2(x) = f2(y1(x), x),

f1
f2

p(f1),
p(f2),

Sea Ice I(t)

= f2( ,

, t)

Figure 1: Cartoon motivating a factorisation for the
joint distribution p(I(t), T (t), C(t))

kernel hyperparameter learning. We apply GPAR to
a wide variety of multi-output regression problems,
achieving state-of-the-art results on ﬁve benchmark
tasks.

2 GPAR

Consider the problem of modelling the world’s average
CO2 level C(t), temperature T (t), and Arctic sea ice
extent I(t) as a function of time t. By the greenhouse
eﬀect, one can imagine that the temperature T (t) is a
complicated, stochastic function f1 of CO2 and time:
T (t) = f1(C(t), t). Similarly, one might hypothesise
that the Arctic sea ice extent I(t) can be modelled as
another complicated function f2 of temperature, CO2,
and time: I(t) = f2(T (t), C(t), t). These functional
relationships are depicted in Figure 1 and motivate
the following model where the conditionals model the
postulated underlying functions f1 and f2:

p(I(t), T (t), C(t))

= p(C(t)) p(T (t)

C(t))
(cid:125)

|
(cid:123)(cid:122)
models f1

p(I(t)
(cid:124)

T (t), C(t))
(cid:123)(cid:122)
(cid:125)
models f2

|

.

(cid:124)

More generally, consider the problem of modelling M
outputs y1:M (x) = (y1(x), . . . , yM (x)) as a function of
the input x. Applying the product rule yields1

p(y1:M (x))
y1(x))
= p(y1(x)) p(y2(x)
|
(cid:124)
(cid:125)
(cid:123)(cid:122)
y2(x) as a random
function of y1(x)

· · ·

p(yM (x)
(cid:124)

y1:M −1(x)),
(cid:125)
(cid:123)(cid:122)
yM (x) as a random
function of y1:M −1(x)

|

(1)

which states that y1(x), like CO2, is ﬁrst generated
from x, according to some unknown, random function
f1; that y2(x), like temperature, is then generated
from y1(x) and x, according to some unknown, random
function f2; that y3(x), like the Arctic sea ice extent,
is then generated from y2(x), y1(x), and x, according

1This requires an ordering of the outputs; we will address

this point in Section 2.

∼

∼

∼

...

yM (x) = fM (y1:M −1(x), x)

fM

p(fM ).

GPAR, introduced now, models these unknown func-
tions f1:M with Gaussian processes (GPs). Recall that
a GP f over an index set
deﬁnes a stochastic process,
or process in short, where f (x1), . . . , f (xN ) are jointly
Gaussian distributed for any x1, . . . , xN (Rasmussen
and Williams, 2006). Marginalising out f1:M , we ﬁnd
that GPAR models the conditionals in Equation (1)
with Gaussian processes:

X

ym

y1:m−1

|

∼ GP

(0, km((y1:m−1(x), x), (y1:m−1(x(cid:48)), x(cid:48)))).

(2)

Although the conditionals in Equation (1) are Gaus-
sian, the joint distribution p(y1:N ) is not; moments of
the joint distribution over the outputs are generally
intractable, but samples can be generated by sequen-
tially sampling the conditionals. Figure 2b depicts the
graphical model corresponding to GPAR. Crucially, the
kernels (km) may specify nonlinear, input-dependent
relationships between outputs, which enables GPAR to
model data where outputs inter-depend in complicated
ways.

Returning to the climate modelling example, one might
object that the temperature T (t) at time t does not
just depend the CO2 level C(t) at time t, but in-
stead depends on the entire history of CO2 levels C:
T (t) = f1(C, t). Note: by writing C instead of C(t),
we refer to the entire function C, as opposed to just its
value at t. Similarly, one might object that the Arctic
sea ice extent I(t) at time t does not just depend on
the temperature T (t) and CO2 level C(t) at time t,
but instead depends on the entire history of temper-
atures T and CO2 levels C: T (t) = f2(T, C, t). This
kind of dependency structure, where output ym(x) now
depends on the entirety of all foregoing outputs y1:m
instead of just their value at x, motivates the following
generalisation of GPAR in its form of Equation (2):

ym

y1:m−1

|

∼ GP

(0, km((y1:m−1, x), (y1:m−1, x(cid:48)))),

(3)

which we refer to as nonlocal GPAR, or non-
instantaneous in the temporal setting. Clearly, GPAR
is a special case of nonlocal GPAR. Figure 2a depicts
the graphical model corresponding to nonlocal GPAR.
Nonlocal GPAR will not be evaluated experimentally,
but some theoretical properties will be described.

Inference and learning in GPAR. Inference and
learning in GPAR is simple. Let y(n)
m = ym(x(n)) de-
note an observation for output m. Then, assuming all

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

(a)

f2

y2

f1

y1

f3

y3

(b)

f2

f1

f3

y1(x)

y2(x)

y3(x)

x

(4)

Figure 2: Graphical models corresponding to (a) non-
local GPAR and (b) GPAR

outputs are observed at each input, we ﬁnd

(y(n)

1:M , x(n))N

n=1)

|

p(f1:M
M
(cid:89)

=

m=1

p(fm

|

(y(n)
m )N
n=1
(cid:123)(cid:122)
(cid:125)
(cid:124)
observations
for fm

, (y(n)
(cid:124)

1:m−1, x(n))N
(cid:123)(cid:122)
input locations
of observations

n=1
(cid:125)

),

O

n=1 |

n=1 |

m )N

(y(n)

1:M )N

(x(n))N

(M 3N 3).

meaning that the posterior of fm is computed sim-
ply by conditioning fm on the observations for output
m located at the observations for the foregoing out-
puts and the input, which again is a Gaussian process;
thus, like the prior, the posterior predictive density
also decomposes as a product of Gaussian condition-
als. The evidence log p((y(n)
n=1) decom-
poses similarly; as a consequence, the hyperparam-
eters for fm can be learned simply by maximising
log p((y(n)
n=1). In conclusion, in-
ference and learning in GPAR with M outputs comes
down to inference and learning in M decoupled, one-
dimensional GP regression problems. This shows that
(M N 3), de-
without approximations GPAR scales
pending only linearly on the number of outputs M
rather than cubically, as is the case for general multi-
output GPs, which scale

1:m−1, x(n))N

O
Scaling GPAR. In these one-dimensional GP regres-
sion problems, oﬀ-the-shelf GP scaling techniques may
be applied to trivially scale GPAR to large data sets.
Later we utilise the variational inducing point method
by Titsias (2009), which is theoretically principled
(Matthews et al., 2016) and empirically accurate (Bui
et al., 2016b). This method requires a set of inducing
inputs to be speciﬁed for each Gaussian process. For
the ﬁrst output y1 there is a single input x, which is
time. We use ﬁxed (non-optimised) regularly spaced
inducing inputs, as they are known to perform well for
time series (Bui and Turner, 2014). The second and
following outputs ym, however, require inducing points
to be placed in x and y1, . . . , ym−1. Regular spacing
can be used again for x, but there are choices available
for y1, . . . , ym−1. One approach would be to optimise
these inducing input locations, but instead we use the
posterior predictive means of y1, . . . , ym−1 at x. This
choice accelerates training and was found to yield good
results.

Missing data. When there are missing outputs, the
procedures for inference and learning described above
remain valid as long as for every observation y(n)
m there
are also observations y(n)
m−1:1. We call a data set satis-
fying this property closed downwards. If a data set is
not closed downwards, data may be imputed, e.g. using
the model’s posterior predictive mean, to ensure closed
downwardness; inference and learning then, however,
become approximate. The key conditional indepen-
dence expressed by Figure 2b that results in simple
inference and learning is the following:

Theorem 1. Let a set of observations
downwards. Then yi
the observations for outputs 1, . . . , i and
i + 1, . . . , M .

⊥ D

i+1:M

| D

D

1:i, where

D

be closed
1:i are
D
i+1:M for

Theorem 1 is proved in Appendix A from the sup-
plementary material. Note that Theorem 1 holds for
every graphical model of the form of Figure 2b, mean-
ing that the decomposition into single-output modelling
problems holds for any choice of the conditionals in
Equation (1), not just Gaussians.

Potential deﬁciencies of GPAR. GPAR has two
apparent limitations. First, since the outputs from
earlier dimensions are used as inputs to later dimen-
sions, noisy outputs yield noisy inputs. One possible
mitigating solution is to employ a denoising input
transformation for the kernels, e.g. using the poste-
rior predictive mean of the foregoing outputs as the
input of the next covariance function. We shall refer
to GPAR models employing this approach as D-GPAR.
Second, one needs to choose an ordering of the outputs.
Fortunately, often the data admits a natural ordering;
for example, if the predictions concern a single output
dimension, this should be placed last. Alternatively, if
there is not a natural ordering, one can greedily opti-
mise the evidence with respect to the ordering. This
procedure considers 1
2 M (M + 1) conﬁgurations while
an exhaustive search would consider all M ! possible
conﬁgurations: the best ﬁrst output is selected out
of all M choices, which is then ﬁxed; then, the best
second output is selected out of the remaining M
1
choices, which is then ﬁxed; et cetera. These methods
are examined in Section 6.

−

3 GPAR and Multi-Output GPs

The choice of kernels k1:M for f1:M is crucial to GPAR,
as they determine the types of relationships between
inputs and outputs that can be learned. Particular
choices for k1:M turn out to yield models closely related
to existing work. These connections are made rigorous
by the nonlinear and linear equivalent model discussed
in Appendix B from the supplementary material. We

The Gaussian Process Autoregressive Regression Model (GPAR)

summarise the results here; see also Table 1.

If km depends linearly on the foregoing outputs y1:m−1
at particular x, then a joint Gaussian distribution over
the outputs is induced in the form of a multi-output
GP model (Goovaerts, 1997; Stein, 1999; Wackernagel,
2003; Teh and Seeger, 2005; Bonilla et al., 2008; Nguyen
and Bonilla, 2014) where latent processes are mixed
together according to a matrix, called the mixing ma-
trix, that is lower-triangular (Appendix B). One may
let the dependency of km on y1:m−1 vary with x, in
which case the mixing matrix varies with x, meaning
that correlations between outputs vary with x. This
yields an instance of the Gaussian Process Regression
Network (GPRN) (Wilson et al., 2012) where inference
is fast and closed form. One may even let km de-
pend nonlinearly on y1:m−1, which yields a particularly
structured deep Gaussian process (DGP) (Damianou,
2014; Bui et al., 2016a), potentially with skip connec-
tions from the inputs (Appendix B). Note that GPAR
may be interpreted as a conventional DGP where the
hidden layers are directly observed and correspond to
successive outputs; this connection could potentially
be leveraged to bring machinery developed for DGPs
to GPAR, e.g. to deal with arbitrarily missing data.

One can further let km depend on the entirety of the
foregoing outputs y1:m−1, yielding instances of nonlocal
GPAR. An example of a nonlocal linear kernel is

k((y, x), (y(cid:48), x(cid:48))) =

a(x

z, x(cid:48)

z(cid:48))y(z)y(cid:48)(z(cid:48)) dz dz(cid:48).

−

−

(cid:90)

The nonlocal linear kernel again induces a jointly Gaus-
sian distribution over the outputs in the form of a
convolutional multi-output GP model (Álvarez et al.,
2009; Álvarez and Lawrence, 2009; Bruinsma, 2016)
where latent processes are convolved together accord-
ing to a matrix-valued function, called the convolution
matrix, that is lower-triangular (Appendix B). Again,
one may let the dependency of km on the entirety of
y1:m−1 vary with x, in which case the convolution ma-
trix varies with x, or even let km depend nonlinearly
on the entirety of y1:m−1; an example of a nonlocal
nonlinear kernel is

k(y, y(cid:48)) = σ2 exp

(cid:18)

(cid:90)

−

1
2(cid:96)(z)

(cid:19)

(y(z)

y(cid:48)(z))2 dz

.

−

Henceforth, we shall refer to GPAR with linear de-
pendencies between outputs as GPAR-L, GPAR with
nonlinear dependencies between outputs as GPAR-NL,
and a combination of the two as GPAR-L-NL.

4 Further Related Work

The Gaussian Process Network (Friedman and Nach-
man, 2000) is similar to GPAR, but was instead devel-
oped to identify causal dependencies between variables

in a probabilistic graphical models context rather than
multi-output regression. The work by Yuan (2011) also
discusses a model similar to GPAR, but speciﬁes a
diﬀerent generative procedure for the outputs.

The multi-ﬁdelity modelling literature is closely related
to multi-output modelling. Whereas in a multi-output
regression task we predict all outputs, multi-ﬁdelity
modelling is concerned with predicting a particular
high-ﬁdelity function, incorporating information from
observations from various levels of ﬁdelity. The idea
of iteratively conditioning on lower ﬁdelity models in
the construction of higher ﬁdelity ones is a well-used
strategy (Kennedy and O’Hagan, 2000; Le Gratiet and
Garnier, 2014). The model presented by Perdikaris
et al. (2017) is nearly identical to GPAR applied in the
multi-ﬁdelity framework, but applications outside this
setting have not been considered.

Moreover, GPAR follows a long line of work on the
family of fully visible Bayesian networks (Frey et al.,
1996; Bengio and Bengio, 2000) that decompose the
distribution over the observations according to the
product rule (Equation (1)) and model the resulting
one dimensional conditionals. A number of approaches
use neural networks for this purpose (Neal, 1992; Frey
et al., 1996; Larochelle and Murray, 2011; Theis and
Bethge, 2015; van den Oord et al., 2016). In particular,
if the observations are real-valued, a standard archi-
tecture lets the conditionals be Gaussian with means
encoded by neural networks and ﬁxed variances. Under
broad conditions, if these neural networks are replaced
by Bayesian neural networks with independent Gaus-
sian priors over the weights, we recover GPAR as the
width of the hidden layers goes to inﬁnity (Neal, 1996;
Matthews et al., 2018).

5 Synthetic Data Experiments

GPAR is well suited for problems where there is a
strong functional relationship between outputs and for
problems where observation noise is richly structured
and input dependent. In this section we demonstrate
GPAR’s ability to model both types of phenomena.

First, we test the ability to leverage strong functional
relationships between the outputs. Consider three out-
puts y1, y2, and y3, inter-depending nonlinearly:
x4 + (cid:15)1,

−

sin(10π(x + 1))/(2x + 1)
y1(x) =
y2(x) = cos2(y1(x)) + sin(3x) + (cid:15)2,
y3(x) = y2(x)y2

1(x) + 3x + (cid:15)3,

−

i.i.d.
∼ N

where (cid:15)1, (cid:15)2, (cid:15)3
(0, 0.05). By substituting y1 and
y2 into y3, we see that y3 can be expressed directly in
terms of x, but via a complex function. The dependence
of y3 on y1, y2, however, is much simpler. Therefore, as
GPAR can exploit direct dependencies between y1:2 and

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

GPAR Deps. Between Outputs Kernels for f1:M

Local Linear

Nonlocal Linear

+ dep. on x

Nonlinear

+ dep. on x

+ dep. on x

Nonlinear

+ dep. on x

Related Models

Multi-Output GPs [1]

k1(x, x(cid:48)) + kLinear(y(x), y(x(cid:48)))
k1(x, x(cid:48)) + k2(x, x(cid:48))kLinear(y(x), y(x(cid:48))) GPRN [2]
k1(x, x(cid:48)) + k2(y(x), y(x(cid:48)))
k1(x, x(cid:48)) + k2((x, y(x)), (x(cid:48), y(x(cid:48))))
k1(x, x(cid:48)) + kLinear(y, y(cid:48))
k1(x, x(cid:48)) + k2(x, x(cid:48))kLinear(y, y(cid:48))
k1(x, x(cid:48)) + k2(y, y(cid:48))
k1(x, x(cid:48)) + k2((x, y), (x(cid:48), y(cid:48)))

Deep GPs (DGPs) [3]
DGPs with input connections

Convolutional MOGPs [4]

Table 1: Classiﬁcation of kernels k1:M for f1:M , the resulting dependencies between outputs, and related models.
Here kLinear refers to a linear kernel and k1 and k2 to an exponentiated quadratic (EQ) or rational quadratic (RQ)
kernel (Rasmussen and Williams, 2006). [1]: Goovaerts (1997); Stein (1999); Wackernagel (2003); Teh and Seeger
(2005); Bonilla et al. (2008); Osborne et al. (2008); Nguyen and Bonilla (2014). [2]: Wilson et al. (2012). [3]:
Damianou (2014); Bui et al. (2016a). [4]: Álvarez et al. (2009); Álvarez and Lawrence (2009); Bruinsma (2016).

y3, it should be presented with a much simpliﬁed task
as compared to predicting y3 from x directly. Figure 3
shows plots of independent GPs (IGPs) and GPAR ﬁt
to 30 data points from y1, y2 and y3. Indeed observe
that GPAR is able to learn y2’s dependence on y1, and
y3’s dependence on y1 and y2, whereas the independent
GPs struggle with the complicated structure.

Second, we test GPAR’s ability to capture non-
Gaussian and input-dependent noise. Consider the fol-
lowing three schemes in which two outputs are observed
under various noise conditions: y1(x) = f1(x) + (cid:15)1 and

(1): y2(x) = f2(x) + sin2(2πx)(cid:15)1 + cos2(2πx)(cid:15)2,
(2): y2(x) = f2(x) + sin(π(cid:15)1) + (cid:15)2,
(3): y2(x) = f2(x) + sin(πx)(cid:15)1 + (cid:15)2,

i.i.d.
∼ N

(0, 0.1), and f1 and f2 are compli-
where (cid:15)1, (cid:15)2
cated, nonlinear functions.2 All three schemes have
i.i.d. homoscedastic Gaussian noise in y1. The noise
in y2, however, depends on that in y1 and can be
heteroscadastic. The task for GPAR is to learn the
scheme’s noise structure. Figure 4 visualises the noise
correlations induced by the schemes and the noise struc-
tures learned by GPAR. Observe that GPAR is able
to learn the various noise structures.

6 Real-World Data Experiments

In this section we evaluate GPAR’s performance and
compare to other models on four standard data sets
commonly used to evaluate multi-output models. We
also consider a recently-introduced data set in the ﬁeld
of Bayesian optimisation, which is a downstream ap-
plication area that could beneﬁt from GPAR. Table 2
lists the models against which we compare GPAR. We

2The functions given by f1(x) = − sin(10π(x + 1))/(2x +
5 e2x (θ1 cos(θ2πx) + θ3 cos(θ4πx)) +

1) − x4 and f2(x) = 1
√

2x.

Acronym Model

Independent GPs
Cokriging
Intrinstic Coregionalisation Model [1]
Semi-Parametric Latent Factor Model [2]
Collaborative Multi-Output GPs [3]

IGP
CK
ICM
SLFM
CGP
CMOGP Convolved Multi-Output GP Model [4]
GP Regression Network [5]
GPRN

Table 2: Models against which GPAR is compared. [1]:
Goovaerts (1997); Stein (1999); Wackernagel (2003).
[2]: Teh and Seeger (2005). [3]: Nguyen and Bonilla
(2014). [4]: Álvarez and Lawrence (2011); Álvarez et al.
(2010). [5]: Wilson et al. (2012).

always compare against IGP and CK, ICM, SLFM,
and CGP, and compare against CMOGP and GPRN
if results for the considered task are available. Since
CK and ICM are much simpliﬁed versions of SLFM
(Álvarez et al., 2010; Goovaerts, 1997) and CGP is an
approximation to SLFM, we sometimes omit results
for CK, ICM, and CGP. Implementations can be found
at https://github.com/wesselb/gpar (Python) and
https://github.com/willtebbutt/GPAR.jl (Julia).
Experimental details can be found in Appendix D from
the supplementary material.

Electroencephalogram (EEG) data set.3 This
data set consists of 256 voltage measurements from
7 electrodes placed on a subject’s scalp whilst the
subject is shown a certain image; Zhang et al. (1995)
describe the data collection process in detail. In par-
ticular, we use frontal electrodes FZ and F1–F6 from
the ﬁrst trial on control subject 337. The task is to
predict the last 100 samples for electrodes FZ, F1, and
F2, given that the ﬁrst 156 samples of FZ, F1, and F2

3 The EEG data set can be downloaded at https://

archive.ics.uci.edu/ml/datasets/eeg+database.

The Gaussian Process Autoregressive Regression Model (GPAR)

Figure 3: Synthetic data set with complex output dependencies: GPAR vs independent GPs (IGP) predictions.

Model

IGP

MAE
MAE∗

0.5739
0.5753

CK†

0.51

ICM SLFM CMOGP†

0.4601
0.4114

0.4606
0.4145

0.4552

Model GPRN† GPAR-NL D-GPAR-NL

MAE
MAE∗

0.4525
0.4040

0.4324
0.4168

0.4114
0.3996

Table 4: Results for the Jura data set for IGP, cok-
riging (CK) and ICM with two latent dimensions, the
SLFM with two latent dimensions, CMOGP, GPRN,
∗ Results are obtained by ﬁrst log-
and GPAR.
transforming the data, then performing prediction, and
ﬁnally transforming the predictions back to the original
† Results from Wilson (2014).
domain.

perimental protocol by Goovaerts (1997) also followed
by Álvarez and Lawrence (2011): The training data
comprises 259 data points distributed spatially with
three output variables—nickel, zinc, and cadmium—
and 100 additional data points for which only two of
the three outputs—nickel and zinc—are observed. The
task is to predict cadmium at the locations of those
100 additional data. Performance is evaluated with the
mean absolute error (MAE).

Table 4 shows the results. The comparatively poor
performance of independent GPs highlights the impor-
tance of exploiting correlations between the mineral
concentrations. Furthermore, Table 4 shows that D-
GPAR-NL signiﬁcantly outperforms the other models,
achieving a new state-of-the-art.

Exchange rates data set.5 This data set consists
of the daily exchange rate w.r.t. USD of the top ten
international currencies (CAD, EUR, JPY, GBP, CHF,
AUD, HKD, NZD, KRW, and MXN) and three precious
metals (gold, silver, and platinum) in the year 2007.
The task is to predict CAD on days 50–100, JPY on
days 100–150, and AUD on days 150–200, given that
CAD is observed on days 1–49 and 101–251, JPY on
days 1–99 and 151–251, and AUD on days 1–149 and
201–251; and that all other currencies are observed
throughout the whole year. Performance is measured

5 The exchange rates data set can be downloaded at

http://fx.sauder.ubc.ca.

Figure 4: Correlation between the sample residues
(deviation from the mean) for y1 and y2. Left, middle,
and right plots correspond to schemes (1), (2) and (3)
respectively. Samples are coloured according to input
value x; that is, all samples for a particular x have the
same colour. If the colour pattern is preserved, then
GPAR has successfully captured how the noise in y1
correlates to that in y2.

Model

SMSE MLL

TT

IGP
SLFM
GPAR-NL

1.75
1.06
0.26

2.60
4.00
1.63

2 sec
11 min
5 sec

Table 3: Results for the EEG data set for IGP, the
SLFM with four latent dimensions, and GPAR.

and the whole signals of F3–F6 are observed. Perfor-
mance is measured with the standardised mean squared
error (SMSE), mean log loss (MLL) (Rasmussen and
Williams, 2006), and training time (TT). Figure 5
visualises predictions for electrode F2, and Table 3
quantiﬁes the results. We observe that GPAR-NL out-
performs independent in terms of SMSE and MLL;
note that independent GPs completely fail to provide
an informative prediction. Furthermore, independent
GPs were trained in two seconds, and GPAR-NL took
only three more seconds; in comparison, training SLFM
took 11 minutes.

Jura data set.4 This data set comprises metal con-
centration measurements collected from the topsoil in
a 14.5 km2 region of the Swiss Jura. We follow the ex-

4 The

data

can
//sites.google.com/site/goovaertspierre/
pierregoovaertswebsite/download/.

downloaded

be

at

https:

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

Figure 5: Predictions for electrode F2 from the EEG data set

Model

IGP∗

CMOGP∗ CGP∗ GPAR-L-NL

SMSE 0.5996

0.2427

0.2125

0.0302

Table 5: Experimental results for the exchange rates
∗ These
data set for IGP, CMOGP, CGP, and GPAR.
numbers are taken from Nguyen and Bonilla (2014).

with the SMSE.

Figure 6 visualises GPAR’s prediction for data set, and
Table 5 quantiﬁes the result. We greedily optimise the
evidence w.r.t. the ordering of the outputs to determine
an ordering, and we impute missing data to ensure
closed downwardness of the data. Observe that GPAR
signiﬁcantly outperforms all other models.

Tidal height, wind speed, and air temperature
data set.6 This data set was collected at 5 minute in-
tervals by four weather stations: Bramblemet, Camber-
met, Chimet, and Sotonmet, all located in Southamp-
ton, UK. The task is to predict the air temperature
measured by Cambermet and Chimet from all other
signals. Performance is measured with the SMSE. This
experiment serves two purposes. First, it demonstrates
that it is simple to scale GPAR to large data sets using
oﬀ-the-shelf inducing point techniques for single-output
GP regression. Second, it shows that scaling to large
data sets enables GPAR to better learn dependencies
between outputs, which, importantly, can signiﬁcantly
improve predictions in regions where outputs are par-
tially observed. We utilise the variational inducing
point method by Titsias (2009) as discussed in Sec-
tion 2, with 10 inducing points per day. This data set
is not closed downwards, so we use mean imputation
when training. We use D-GPAR-L and set the tempo-
ral kernel to be a simple EQ, meaning that the model
cannot make long-range predictions, but instead must
exploit correlations between outputs.

Nguyen and Bonilla (2014) consider from this data set
5 days in July 2013, and predict short periods of the
air temperature measured by Cambermet and Chimet
using all other signals. We followed their setup and
predicted the same test set, but instead trained on

6 The data can be downloaded at http://www.
bramblemet.co.uk, http://cambermet.co.uk, http://
www.chimet.co.uk, and http://sotonmet.co.uk.

the whole of July. Even though the additional ob-
servations do not temporally correlate with the test
periods at all, they enable GPAR to better learn the
relationships between the outputs, which, unsurpris-
ingly, signiﬁcantly improved the predictions: using the
whole of July, GPAR achieves SMSE 0.056, compared
to their SMSE 0.107.

The test set used by Nguyen and Bonilla (2014) is
rather small, yielding high-variance test results. We
therefore do not pursue further comparisons on their
train–test split, but instead consider a bigger, more
challenging setup: using as training data 10 days (days
[10, 20), roughly 30 k points), 15 days (days [18, 23),
roughly 47 k points), and the whole of July (roughly
98 k points), make predictions of 4 day periods of the
air temperature measured by Cambermet and Chimet.
Figure 7 visualises the test periods and GPAR’s pre-
dictions for it. Despite the additional observations
not correlating with the test periods, we observe clear,
though dimishining, improvements in the predictions
as the training data is increased.

MLP validation error data set. The ﬁnal data
set is the validation error of a multi-layer perceptron
(MLP) on the MNIST data, trained using categorical
cross-entropy, and set as a function of six hyperpa-
rameters: the number of hidden layers, the number
of neurons per hidden layer, the dropout rate, the
learning rate to use with the ADAM optimizer, the L1
weight penalty, and the L2 weight penalty. This exper-
iment was implemented using code made available by
Hernández-Lobato (2016). An improved model for the
objective surface could translate directly into improved
performance in Bayesian optimisation (Snoek et al.,
2012), as this would enable a more informed search of
the hyperparameter space.

To generate a data set, we sample 291 sets of hyperpa-
rameters randomly from a rectilinear grid and train the
MLP for 21 epochs under each set of hyperparameters,
recording the validation performance after 1, 5, 11, 16,
and 21 epochs. We construct a training set of 175 of
these hyperparameter settings and, crucially, discard
roughly 30% of the validation performance results at 5
epochs at random, and again discard roughly 30% of
those results at 11 epochs, and so forth. The resulting
data set has 175 labels after 1 epoch, 124 after 5, 88

The Gaussian Process Autoregressive Regression Model (GPAR)

Figure 6: Visualisation of the exchange rates data set and CGP’s (black) and GPAR’s (green) predictions for it.
GPAR’s predictions are overlayed on the original ﬁgure by Nguyen and Bonilla (2014).

Figure 7: Visualisation of the air temperature data set and GPAR’s prediction for it. Black circles indicate the
locations of the inducing points.

have learned to exploit the extra information available.
In turn, GPAR noticeably outperforms the SLFM.

7 Conclusion and Future Work

This paper introduced GPAR: a ﬂexible, fast, tractable,
and interpretable approach to multi-output GP re-
gression. GPAR can model (1) nonlinear relation-
ships between outputs and (2) complex output noise.
GPAR can scale to large data sets by trivially lever-
aging scaling techniques for one-dimensional GP re-
gression (Titsias, 2009). In eﬀect, GPAR transforms
high-dimensional data modelling problems into set of
single-output modelling problems, which are the bread
and butter of the GP approach. GPAR was rigorously
tested on a variety of synthetic and real-world prob-
lems, consistently outperforming existing GP models
for multi-output regression. An exciting future appli-
cation of GPAR is to use compositional kernel search
(Lloyd et al., 2014) to automatically learn and explain
dependencies between outputs and inputs. Further
insights into structure of the data could be gained by
decomposing GPAR’s posterior over additive kernel
components (Duvenaud, 2014). These two approaches
could be developed into a useful tool for automatic
structure discovery. Two further exciting future ap-
plications of GPAR are modelling of environmental
phenomena and improving data eﬃciency of existing
Bayesian optimisation tools (Snoek et al., 2012).

Figure 8: Results for the machine learning data set
for a GP, the SLFM with two latent dimensions, and
GPAR

after 11, 64 after 15 and 44 after 21, simulating the
partial completion of the majority of runs. Importantly,
a Bayesian Optimisation system typically exploits only
completed training runs to inform the objective surface,
whereas GPAR can also exploit partially complete runs.

The results presented in Figure 8 show the SMSE in
predicting validation performance at each epoch using
GPAR, the SLFM, and independent GPs on the test
set, averaged over 10 seed for the pseudo-random num-
ber generator used to select which outputs from the
training set to discard. GPs trained independently to
predict performance after a particular number of epochs
perform worse than the SLFM and GPAR, which both

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

Acknowledgements

Richard E. Turner is supported by Google as well as
EPSRC grants EP/M0269571 and EP/L000776/1.

References

Álvarez, M. and Lawrence, N. D. (2009). Sparse con-
volved gaussian processes for multi-output regression.
Advances in Neural Information Processing Systems,
21:57–64.

Álvarez, M. A. and Lawrence, N. D. (2011). Computa-
tionally eﬃcient convolved multiple output Gaussian
processes. Journal of Machine Learning Research,
12:1459–1500.

Álvarez, M. A., Luengo, D., and Lawrence, N. D. (2009).
Latent force models. Artiﬁcial Intelligence and Statis-
tics, 5:9–16.

Álvarez, M. A., Luengo, D., Titsias, M. K., and
Lawrence, N. D. (2010). Eﬃcient multioutput Gaus-
sian processes through variational inducing kernels.
Journal of Machine Learning Research: Workshop
and Conference Proceedings, 9:25–32.

Bengio, Y. and Bengio, S. (2000). Modeling high-
dimensional discrete data with multi-layer neural
networks. In Advances in Neural Information Pro-
cessing Systems, pages 400–406.

Bonilla, E. V., Chai, K. M., and Williams, C. K. I.
(2008). Multi-task Gaussian process prediction. Ad-
vances in Neural Information Processing Systems,
20:153–160.

Bruinsma, W. P. (2016). The generalised gaussian
convolution process model. MPhil thesis, Department
of Engineering, University of Cambridge.

Bui, T. D., Hernández-Lobato, D., Li, Y., Hernández-
Lobato, J. M., and Turner, R. E. (2016a). Deep
gaussian processes for regression using approxi-
mate expectation propagation.
arXiv preprint
arXiv:1602.04133.

Bui, T. D. and Turner, R. E. (2014). Tree-structured
Gaussian process approximations. Advances in Neu-
ral Information Processing Systems, 27:2213–2221.

Bui, T. D., Yan, J., and Turner, R. E. (2016b). A
unifying framework for gaussian process pseudo-point
approximations using power expectation propagation.
arXiv preprint arXiv:1605.07066.

Dai, Z., Álvarez, M. A., and Lawrence, N. D. (2017).
Eﬃcient modeling of latent information in supervised
learning using Gaussian processes. arXiv preprint
arXiv:1705.09862.

Damianou, A. (2014). Deep Gaussian Processes and
Variational Propagation of Uncertainty. PhD thesis,
Department of Neuroscience, University of Sheﬃeld.

Duvenaud, D. (2014). Automatic Model Construction
with Gaussian Processes. PhD thesis, Computational
and Biological Learning Laboratory, University of
Cambridge.

Duvenaud, D., Lloyd, J. R., Grosse, R., Tenenbaum,
J. B., and Ghahramani, Z. (2013). Structure discov-
ery in nonparametric regression through composi-
tional kernel search. In International Conference on
Machine Learning.

Frey, B. J., Hinton, G. E., and Dayan, P. (1996). Does
the wake-sleep algorithm produce good density esti-
mators? In Advances in neural information process-
ing systems, pages 661–667.

Friedman, N. and Nachman, I. (2000). Gaussian process
networks. In Uncertainty in Artiﬁcial Intelligence,
pages 211–219. Morgan Kaufmann Publishers Inc.

Goovaerts, P. (1997). Geostatistics for Natural Re-
sources Evaluation. Oxford University Press, 1 edi-
tion.

Hernández-Lobato, J. M. (2016). Neural networks with
optimal accuracy and speed in their predictions.

Kennedy, M. C. and O’Hagan, A. (2000). Predicting
the output from a complex computer code when fast
approximations are available. Biometrika, 87(1):1–
13.

Koller, D. and Friedman, N. (2009). Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.

Larochelle, H. and Murray, I. (2011). The neural au-
In AISTATS,

toregressive distribution estimator.
volume 1, page 2.

Le Gratiet, L. and Garnier, J. (2014). Recursive co-
kriging model for design of computer experiments
with multiple levels of ﬁdelity. International Journal
for Uncertainty Quantiﬁcation, 4(5).

Lloyd, J. R., Duvenaud, D., Grosse, R., Tenenbaum,
J. B., and Ghahramani, Z. (2014). Automatic con-
struction and natural-language description of non-
parametric regression models. In Association for the
Advancement of Artiﬁcial Intelligence (AAAI).

Matthews, A. G. D. G., Hensman, J., Turner, R. E.,
and Ghahramani, Z. (2016). On sparse variational
methods and the kullback-leibler divergence be-
tween stochastic processes. Artiﬁcial Intelligence
and Statistics, 19.

Matthews, A. G. d. G., Rowland, M., Hron, J., Turner,
R. E., and Ghahramani, Z. (2018). Gaussian pro-
cess behaviour in wide deep neural networks. arXiv
preprint arXiv:1804.11271.

Neal, R. M. (1992). Connectionist learning of belief

networks. Artiﬁcial intelligence, 56(1):71–113.

The Gaussian Process Autoregressive Regression Model (GPAR)

Wilson, A. G. (2014). Covariance Kernels for Fast Au-
tomatic Pattern Discovery and Extrapolation With
Gaussian Processes. PhD thesis, University of Cam-
bridge.

Wilson, A. G., Knowles, D. A., and Ghahramani, Z.
(2012). Gaussian process regression networks. Inter-
national Conference on Machine Learning, 29.

Yuan, C. (2011). Conditional multi-output regression.
In International Joint Conference on Neural Net-
works, pages 189–196. IEEE.

Zhang, X., Begleiter, H., Porjesz, B., Wang, W., and
Litke, A. (1995). Event related potentials during
object recognition tasks. Brain Research Bulletin,
38(6):531–538.

Neal, R. M. (1996). Bayesian learning for neural net-
works, volume 118. Springer Science & Business
Media.

Nguyen, T. V. and Bonilla, E. V. (2014). Collaborative
multi-output Gaussian processes. Conference on
Uncertainty in Artiﬁcial Intelligence, 30.

Nocedal, J. and Wright, S. J. (2006). Numerical Opti-

mization. Springer, second edition.

Osborne, M. A., Roberts, S. J., Rogers, A., Ramchurn,
S. D., and Jennings, N. R. (2008). Towards real-time
information processing of sensor network data us-
ing computationally eﬃcient multi-output Gaussian
processes. In Proceedings of the 7th International
Conference on Information Processing in Sensor Net-
works, IPSN ’08, pages 109–120. IEEE Computer
Society.

Perdikaris, P., Raissi, M., Damianou, A., Lawrence,
N. D., and Karniadakis, G. E. (2017). Nonlinear
information fusion algorithms for data-eﬃcient multi-
ﬁdelity modelling. Proceedings of the Royal Society
A: Mathematical, Physical and Engineering Science,
473.

Rasmussen, C. E. and Williams, C. K. I. (2006). Gaus-
sian Processes for Machine Learning. MIT Press.

Snoek, J., Larochelle, H., and Adams, R. P. (2012).
Practical bayesian optimization of machine learn-
ing algorithms. In Advances in Neural Information
Processing Systems, pages 2951–2959.

Stein, M. (1999).

Interpolation of Spatial Data.

Springer-Verlag New York, 1 edition.

Sun, S., Zhang, G., Wang, C., Zeng, W., Li, J., and
Grosse, R. (2018). Diﬀerentiable compositional ker-
nel learning for gaussian processes. International
Conference on Machine Learning, 35.

Teh, Y. W. and Seeger, M. (2005). Semiparametric
International Workshop on

latent factor models.
Artiﬁcial Intelligence and Statistics, 10.

Theis, L. and Bethge, M. (2015). Generative image
modeling using spatial lstms. In Advances in Neural
Information Processing Systems, pages 1927–1935.

Titsias, M. K. (2009). Variational learning of inducing
variables in sparse Gaussian processes. Artiﬁcial
Intelligence and Statistics, 12:567–574.

van den Oord, A., Kalchbrenner, N., Espeholt, L.,
Vinyals, O., Graves, A., et al. (2016). Conditional
In Ad-
image generation with pixelcnn decoders.
vances in Neural Information Processing Systems,
pages 4790–4798.

Wackernagel, H. (2003). Multivariate Geostatistics.

Springer-Verlag Berlin Heidelberg, 3 edition.

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

A Conditional Independence in

Figure 2b

In this section, we prove the key conditional indepen-
dence in Figure 2b that makes GPAR work:

Theorem 2. Let a set of observations
downwards. Then yi
the observations for outputs 1, . . . , i and
i + 1, . . . , M .7

⊥ D

i+1:M

| D

D

1:i, where

D

be closed
1:i are
D
i+1:M for

To begin with, we review some basic notions concerning
graphical models. Let a path be a sequence of nodes
v1, . . . , vn from some directed graph G where, for each
vi, G either contains an edge from vi to vi+1, or an
edge from vi+1 to vi. If G contains an edge from a to b,
then write a
b to mean the two-node path in which
node b follows node a. Similarly, if G contains an edge
from b to a, then write a
b to mean the two-node
←
path in which b follows a. Write a (cid:10) b to mean either
a

b or b

→

a.

→

←

= v1 (cid:10)

Deﬁnition 1 (Active Path (Deﬁnition 3.6 from Koller
(cid:10) vn be a
and Friedman (2009))). Let
path in a graphical model. Let Z be a subset of the
active
variables from the graphical model. Then, call
given Z if (1) for every v-structure vi−1
vi+1
vi
in
, vi or a descendant of vi is in Z; and (2) no other
P
node in

is in Z.

· · ·

←

→

P

P

P

Deﬁnition 2 (d-Separation (Deﬁnition 3.7 from Koller
and Friedman (2009))). Let X, Y , and Z be three sets
of nodes from a graphical model. Then, call X and Y
d-separated given Z if no path between any x
X and
y

Y is active given Z.

∈

∈

Theorem 3 (d-Separation Implies Conditional Inde-
pendence (Theorem 3.3 from Koller and Friedman
(2009))). Let X, Y , and Z be three sets of nodes from
a graphical model. If X and Y are d-separated given
Z, then X

Z.

Y

⊥

|

Deﬁne the layer of a node in Figure 2b to be

layer(fi) = layer(yi(x)) = i.

We are now ready to prove Theorem 2.

Proof of Theorem 2. For i < j, let
tween any yi(x(cid:48))
∈
be the ﬁrst node in

be a path be-
P
i+1:N . Let yk(ˆx)
such that layer(yk(ˆx)) > i. Then

yi and yj(x)

∈ D

contains

P

P

ym(ˆx)

yk(ˆx) (cid:10)

· · · →

→

· · ·

7 D1:i = {y(n)
j ∈ D : j > i, n ≤ N }.

j

{y(n)

∈ D : j ≤ i, n ≤ N } and Di+1:M =

for some m

i < k.

If yk(ˆx)
ym(ˆx)

∈ D

≤
i+1:N , then, since

1:i, meaning that

∈ D

P
If, on the other hand, yk(ˆx) /
downwards, yk(cid:48)(ˆx) /
≥
cannot be descendant of yk(ˆx), so

∈ D
for all k(cid:48)

∈ D

is closed downwards,

D
is inactive.

, then, since

is closed
D
k. Therefore, yj(x)
must contain

P

yk(cid:48)(ˆx)

yk(cid:48)(cid:48)(ˆx)

fk(cid:48)(cid:48)

· · · →

→

←

→ · · ·

for some m
We conclude that
descendant of yk(cid:48)(cid:48)(ˆx) can be in

k(cid:48) < k(cid:48)(cid:48), which forms a v-structure.
is inactive, because yk(cid:48)(cid:48) (ˆx) nor a

≤

P

.

D

B The Nonlinear and Linear

Equivalent Model

In this section, we construct equivalent models for
GPAR (Lemmas 1 and 2). These models make GPAR’s
connection to other models in the literature explicit.

To begin with, we must introduce some notation and
M )X
M ,
deﬁnitions. For functions A, B :
→ Y
deﬁne composition
B)(x, y) =
is well-deﬁned and as-
A(x, B(
·
M , denote
sociative.
, u). Again, note that
u :
A
◦
B)
(A
◦

u = A(
u). Furthermore, denote

For a function u :

X → Y
u = A
◦

, y)). Note that

M , A
(B

as follows:

X → Y

(
(A

Y
◦

X ×

◦
◦

◦

◦

◦

·

A
(cid:124)

◦ · · · ◦
(cid:123)(cid:122)
n times

A
(cid:125)

= An.

(

(

Y

Y

X ×

X ×

→ Y

→ Y

M )X

M )X

M such that
depends only on (x, y1:i−1),
M , denote
f , and denote N consecutive applications

Consider a function A :
Ai(x, y) :
where A1 = 0. Further let u, y :
T f = u + A
◦
of T by TN .
The expression TM −1 u will be key in constructing
the equivalent models. We show that it is the unique
solution of a functional equation:

X → Y

Proposition 1. The unique solution of y = u + A
is y = TM −1 u.

y

◦

Proof of Proposition 1. First, we show that y = u +
y has a solution, and that this solution is unique.
A
Because Ai(x, y) depends only on (x, y1:i−1), it holds
that

◦

yi = ui + Ai

y = ui + Ai

(y1:i−1, 0),

◦

◦

where (y1:i−1, 0) represents the concatenation of y1:i−1
and M
i + 1 zeros. Thus, yi can uniquely be con-
structed from ui, Ai, and y1:i−1; therefore, y1 exists
and is unique, so y2 exists and is unique: by induction
we ﬁnd that y exists and is unique.

−

The Gaussian Process Autoregressive Regression Model (GPAR)

Second, we show that y = TM −1 u satisﬁes y = u +
y = T y. To show this, we show that (Tn u)i =
A
(Tn−1 u)i for i = 1, . . . , n, for all n. To begin with, we
show the base case, n = 1:

◦

(T u)1 = u1 + A1

u = u1 = (T0 u)1,

◦
since A1 = 0. Finally, suppose that the claim holds for
a particular n. We show that the claim then holds for
n + 1: Let i

n + 1. Then

≤

(Tn+1 u)i = ui + Ai
= ui + Ai

◦

◦

◦

◦

◦

= ui + Ai

(∗)
= ui + Ai

= ui + Ai
= (Tn u)i,

Tn u
((Tn u)1:i−1, (Tn u)i:M )
((Tn−1 u)1:i−1, (Tn u)i:M )

(By assumption)

((Tn−1 u)1:i−1, (Tn−1 u)i:M )
Tn−1 u

where (
) holds because Ai(x, y) depends only on
∗
(x, y1:i−1).

In the linear case, TM −1 u turns out to greatly simplify.

Proposition 2.
TM −1 u = ((cid:80)M −1

i=0 Ai)

u.

◦

If A(x, y) is linear in y,

then

Proof of Proposition 2. If A(x, y) is linear in y, then
distributes over addition. Therefore,
one veriﬁes that

◦
TM −1 u = u + A

= u + A
...
= u + A

◦

◦

◦

TM −2 u
u + A2

◦

TM −3 u

u +

· · ·

+ AM −1

u.

◦

We now use Propositions 1 and 2 to construct a non-
linear and linear equivalent model.

(

GP

Lemma 1 (Nonlinear Equivalent Model). Let A be an
M )X ,
M -dimensional vector-valued process over
(0, kAi) independently, and let
each Ai drawn from
u be an M -dimensional vector-valued process over
,
X
(0, kui) independently. Further-
each ui drawn from
depend only on
more, let Ai(x, y) :
(
(x, y1:i−1), meaning that kAi = kAi(x, y1:i−1, x(cid:48), y(cid:48)
1:i−1),
and let A1 = 0. Denote T f = u + A
f , and denote
N consecutive applications of T by TN . Then

GP
X ×

M )X

→ Y

X ×

Y

Y

◦

yi

y1:i−1

|

∼ GP

y

|

A, u = TM −1 u
(0, kui + kAi(

⇐⇒
, y1:i−1,

·

, y1:i−1)).

Proof of Lemma 1. Since Ai(x, y) depends only on
(x, y1:i−1), it holds by Proposition 1 that any sam-
y, so yi =
A, u satisﬁes yi = ui + Ai
ple from y

|

·

◦

ui + Ai
concatenation of y1:i−1 and M
equivalence now follows.

(y1:i−1, 0), where (y1:i−1, 0) represents the
i + 1 zeros. The

−

◦

Lemma 2 (Linear Equivalent Model). Suppose that
A was instead generated from

(cid:90)

A(x, y)

ˆA =

ˆA(x

z)y(z) dz,

|

−

where ˆA is an (M

, each ˆAi,j drawn from

(0, k ˆAi,j
X
i > j and ˆAi,j = 0 otherwise. Then

GP

×

M )-matrix-valued process over
) independently if

y

A, u =

|

(cid:32)M −1
(cid:88)

(cid:33)

Ai

i=0

u

◦

⇐⇒

yi

y1:i−1

(0, kui + kAi(

, y1:i−1,

, y1:i−1)), (5)

∼ GP

·

·

|
where

1:i−1)

kAi(x, y1:i−1, x(cid:48), y(cid:48)
(cid:90)

i−1
(cid:88)

k ˆAi,j

(x

=

j=1

z, x(cid:48)

z(cid:48))yj(z)y(cid:48)

j(z(cid:48)) dz dz(cid:48).

−

−

Proof of Lemma 2. First, one veriﬁes that Ai(x, y) still
depends only on (x, y1:i−1), and that Ai(x, y) is linear
in y. The result then follows from Lemma 1 and Propo-
sition 2, where the expression for kAi follows from
straightfoward calculation.

As mentioned in the paper, the kernels for f1:M de-
termine the types of relationships between inputs and
outputs that can be learned. Lemmas 1 and 2 make
this explicit: Lemma 1 shows that nonlocal GPAR can
recover a model where M latent GPs u are repeatedly
composed with another latent GP A, where A has a
particular dependency structure, and Lemma 2 shows
that nonlocal GPAR can recover a model where M
latent GPs u are linearly transformed, where the linear
transform T = (cid:80)M −1
i=0 Ai is lower triangular and may
vary with the input.

|

|

|

◦

u, y

T, u = T

In Lemma 2, note that it is not restrictive that T is
lower triangular: Suppose that T were dense. Then,
letting y
T is jointly Gaussian. Hence
y1:i−1, T is a GP whose mean linearly depends upon
yi
y1:i−1 is of the form of
y1:i−1 via T , meaning that yi
Equation (5) where kui may be more complicated. If,
however, ˆA(z) = δ(z)B for some random (M
M )-
matrix B, each Bi,j drawn from
Bi,j ) if i > j and
Bi,j = 0 otherwise, then it is restrictive that T is lower
B, u = (cid:80)M −1
i=0 Biu(x). If
triangular: In this case, y(x)
T = (cid:80)M −1
i=0 Bi were dense, then, letting y
T, u = T u,
y can be represented with Lemma 2 if and only if y
T ’s
covariance can be diagonalised by a constant, invertible,

(0, σ2

N

×

|

|

|

|

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

lower-triangular matrix. This condition does not hold
in general, as Lemma 3 proves.

C Lemma 3

Call functions k1, . . . , kM :
if

X →

R linearly independent

(cid:32)

x :

∀

M
(cid:88)

i=1

(cid:33)

=

⇒

ciki(x) = 0

c1 = . . . = cM = 0.

X →

R be linearly in-
Lemma 3. Let k1, . . . , kM :
dependent and arrange them in a diagonal matrix
K = diag(k1, . . . , kn). Let A be an invertible M
M
matrix such that its columns cannot be permuted into a
triangular matrix. Then there does not exist an invert-
ible triangular matrix T such that T −1BK(x)BTT −T
is diagonal for all x.

×

Proof. Suppose, on the contrary, that such T does exist.
Then two diﬀerent rows ap and aq of A = T −1B share
nonzero elements in some columns C; otherwise, A
would have exactly one nonzero entry in every column—
A is invertible—so A would be the product of a per-
mutation matrix and a diagonal matrix, meaning that
B = T A’s columns could be permuted into a triangu-
lar matrix. Now, by T −1BK(x)BTT −T = AK(x)AT
being diagonal for all x
i ap,iaq,iki(x) = 0 for
∈ X
all x. Therefore, by linear independence of k1, . . . , kN ,
it holds that ap,iaq,i = 0 for all i. But ap,iaq,i
= 0 for
C, which is a contradiction.
any i

, (cid:80)

∈

D Experimental Details

∗

For every experiment, the form of the kernels is deter-
mined by the particular GPAR model used: GPAR-L,
GPAR-NL, or GPAR-L-NL (see Table 2 in the main
paper), potentially with a D-
preﬁx to indicate the de-
noising procedure outlined in “Potential deﬁciencies of
GPAR” in Section 2 of the paper main. For GPAR-NL,
we always used exponentiated quadratic (EQ) kernels,
except for the exchange rates experiment, where we
used rational quadratic (RQ) kernels (Rasmussen and
Williams, 2006). Furthermore, in every problem we
simply expanded according to Equation (1) in the main
paper or greedily optimised the ordering, in both cases
putting the to-be-predicted outputs last. We used
scipy’s implementation of the L-BFGS-B algorithm
(Nocedal and Wright, 2006) to optimise hyperparame-
ters.

9
1
0
2
 
b
e
F
 
5
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
8
1
7
0
.
2
0
8
1
:
v
i
X
r
a

The Gaussian Process Autoregressive Regression Model (GPAR)

James Requeima12

Will Tebbutt12

Wessel Bruinsma12

Richard E. Turner1

1University of Cambridge and 2Invenia Labs, Cambridge, UK
{jrr41, wct23, wpb23, ret26}@cam.ac.uk

Abstract

Multi-output regression models must exploit
dependencies between outputs to maximise
predictive performance. The application of
Gaussian processes (GPs) to this setting typi-
cally yields models that are computationally
demanding and have limited representational
power. We present the Gaussian Process Au-
toregressive Regression (GPAR) model, a scal-
able multi-output GP model that is able to
capture nonlinear, possibly input-varying, de-
pendencies between outputs in a simple and
tractable way: the product rule is used to
decompose the joint distribution over the out-
puts into a set of conditionals, each of which
is modelled by a standard GP. GPAR’s eﬃ-
cacy is demonstrated on a variety of synthetic
and real-world problems, outperforming exist-
ing GP models and achieving state-of-the-art
performance on established benchmarks.

1

Introduction

The Gaussian process (GP) probabilistic modelling
framework provides a powerful and popular approach
to nonlinear single-output regression (Rasmussen and
Williams, 2006). The popularity of GP methods stems
from their modularity, tractability, and interpretability:
it is simple to construct rich, nonlinear models by com-
positional covariance function design, which can then
be evaluated in a principled way (e.g. via the marginal
likelihood), before being interpreted in terms of their
component parts. This leads to an attractive plug-
and-play approach to modelling and understanding
data, which is so robust that it can even be automated
(Duvenaud et al., 2013; Sun et al., 2018).

Most regression problems, however, involve multiple
outputs rather than a single one. When modelling such
data, it is key to capture the dependencies between
these outputs. For example, noise in the output space

Proceedings of the 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2019, Naha,
Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by
the author(s).

might be correlated, or, whilst one output might de-
pend on the inputs in a complex (deterministic) way,
it may depend quite simply on other output variables.
In both cases multi-output GP models are required.
There is a plethora of existing multi-output GP models
that can capture linear correlations between output
variables if these correlations are ﬁxed across the in-
put space (Goovaerts, 1997; Wackernagel, 2003; Teh
and Seeger, 2005; Bonilla et al., 2008; Nguyen and
Bonilla, 2014; Dai et al., 2017). However, one of the
main reasons for the popularity of the GP approach is
that a suite of diﬀerent types of nonlinear input depen-
dencies can be modelled, and it is disappointing that
this ﬂexibility is not extended to interactions between
the outputs. There are some approaches that do allow
limited modelling of nonlinear output dependencies
(Wilson et al., 2012; Bruinsma, 2016) but this ﬂexibil-
ity comes from sacriﬁcing tractability, with complex
and computationally demanding approximate inference
and learning schemes now required. This complexity
signiﬁcantly slows down model ﬁtting, evaluation, and
improvement work ﬂow.

What is needed is a ﬂexible and analytically tractable
modelling approach to multi-output regression that
supports plug-and-play modelling and model interpre-
tation. The Gaussian Process Autoregressive Regres-
sion (GPAR) model, introduced in Section 2, achieves
these aims by taking an approach analogous to that em-
ployed by the Neural Autoregressive Density Estimator
(Larochelle and Murray, 2011) for density modelling.
The product rule is used to decompose the distribu-
tion of the outputs given the inputs into a set of one-
dimensional conditional distributions. Critically, these
distributions can be interpreted as a decoupled set of
single-output regression problems, and learning and in-
ference in GPAR therefore amount to a set of standard
single-output GP regression tasks: training is closed
form, fast, and amenable to standard scaling techniques.
GPAR converts the modelling of output dependencies
that are possibly nonlinear and input-dependent into a
set of standard GP covariance function design problems,
constructing expressive, jointly non-Gaussian models
over the outputs. Importantly, we show how GPAR can
capture nonlinear relationships between outputs as well
as structured, input-dependent noise, simply through

The Gaussian Process Autoregressive Regression Model (GPAR)

CO2 C(t)

to some unknown, random function f3; et cetera:

f1( , t)
=
Temperature T (t)

y1(x) = f1(x),
y2(x) = f2(y1(x), x),

f1
f2

p(f1),
p(f2),

Sea Ice I(t)

= f2( ,

, t)

Figure 1: Cartoon motivating a factorisation for the
joint distribution p(I(t), T (t), C(t))

kernel hyperparameter learning. We apply GPAR to
a wide variety of multi-output regression problems,
achieving state-of-the-art results on ﬁve benchmark
tasks.

2 GPAR

Consider the problem of modelling the world’s average
CO2 level C(t), temperature T (t), and Arctic sea ice
extent I(t) as a function of time t. By the greenhouse
eﬀect, one can imagine that the temperature T (t) is a
complicated, stochastic function f1 of CO2 and time:
T (t) = f1(C(t), t). Similarly, one might hypothesise
that the Arctic sea ice extent I(t) can be modelled as
another complicated function f2 of temperature, CO2,
and time: I(t) = f2(T (t), C(t), t). These functional
relationships are depicted in Figure 1 and motivate
the following model where the conditionals model the
postulated underlying functions f1 and f2:

p(I(t), T (t), C(t))

= p(C(t)) p(T (t)

C(t))
(cid:125)

|
(cid:123)(cid:122)
models f1

p(I(t)
(cid:124)

T (t), C(t))
(cid:123)(cid:122)
(cid:125)
models f2

|

.

(cid:124)

More generally, consider the problem of modelling M
outputs y1:M (x) = (y1(x), . . . , yM (x)) as a function of
the input x. Applying the product rule yields1

p(y1:M (x))
y1(x))
= p(y1(x)) p(y2(x)
|
(cid:124)
(cid:125)
(cid:123)(cid:122)
y2(x) as a random
function of y1(x)

· · ·

p(yM (x)
(cid:124)

y1:M −1(x)),
(cid:125)
(cid:123)(cid:122)
yM (x) as a random
function of y1:M −1(x)

|

(1)

which states that y1(x), like CO2, is ﬁrst generated
from x, according to some unknown, random function
f1; that y2(x), like temperature, is then generated
from y1(x) and x, according to some unknown, random
function f2; that y3(x), like the Arctic sea ice extent,
is then generated from y2(x), y1(x), and x, according

1This requires an ordering of the outputs; we will address

this point in Section 2.

∼

∼

∼

...

yM (x) = fM (y1:M −1(x), x)

fM

p(fM ).

GPAR, introduced now, models these unknown func-
tions f1:M with Gaussian processes (GPs). Recall that
a GP f over an index set
deﬁnes a stochastic process,
or process in short, where f (x1), . . . , f (xN ) are jointly
Gaussian distributed for any x1, . . . , xN (Rasmussen
and Williams, 2006). Marginalising out f1:M , we ﬁnd
that GPAR models the conditionals in Equation (1)
with Gaussian processes:

X

ym

y1:m−1

|

∼ GP

(0, km((y1:m−1(x), x), (y1:m−1(x(cid:48)), x(cid:48)))).

(2)

Although the conditionals in Equation (1) are Gaus-
sian, the joint distribution p(y1:N ) is not; moments of
the joint distribution over the outputs are generally
intractable, but samples can be generated by sequen-
tially sampling the conditionals. Figure 2b depicts the
graphical model corresponding to GPAR. Crucially, the
kernels (km) may specify nonlinear, input-dependent
relationships between outputs, which enables GPAR to
model data where outputs inter-depend in complicated
ways.

Returning to the climate modelling example, one might
object that the temperature T (t) at time t does not
just depend the CO2 level C(t) at time t, but in-
stead depends on the entire history of CO2 levels C:
T (t) = f1(C, t). Note: by writing C instead of C(t),
we refer to the entire function C, as opposed to just its
value at t. Similarly, one might object that the Arctic
sea ice extent I(t) at time t does not just depend on
the temperature T (t) and CO2 level C(t) at time t,
but instead depends on the entire history of temper-
atures T and CO2 levels C: T (t) = f2(T, C, t). This
kind of dependency structure, where output ym(x) now
depends on the entirety of all foregoing outputs y1:m
instead of just their value at x, motivates the following
generalisation of GPAR in its form of Equation (2):

ym

y1:m−1

|

∼ GP

(0, km((y1:m−1, x), (y1:m−1, x(cid:48)))),

(3)

which we refer to as nonlocal GPAR, or non-
instantaneous in the temporal setting. Clearly, GPAR
is a special case of nonlocal GPAR. Figure 2a depicts
the graphical model corresponding to nonlocal GPAR.
Nonlocal GPAR will not be evaluated experimentally,
but some theoretical properties will be described.

Inference and learning in GPAR. Inference and
learning in GPAR is simple. Let y(n)
m = ym(x(n)) de-
note an observation for output m. Then, assuming all

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

(a)

f2

y2

f1

y1

f3

y3

(b)

f2

f1

f3

y1(x)

y2(x)

y3(x)

x

(4)

Figure 2: Graphical models corresponding to (a) non-
local GPAR and (b) GPAR

outputs are observed at each input, we ﬁnd

(y(n)

1:M , x(n))N

n=1)

|

p(f1:M
M
(cid:89)

=

m=1

p(fm

|

(y(n)
m )N
n=1
(cid:123)(cid:122)
(cid:125)
(cid:124)
observations
for fm

, (y(n)
(cid:124)

1:m−1, x(n))N
(cid:123)(cid:122)
input locations
of observations

n=1
(cid:125)

),

O

n=1 |

n=1 |

m )N

(y(n)

1:M )N

(x(n))N

(M 3N 3).

meaning that the posterior of fm is computed sim-
ply by conditioning fm on the observations for output
m located at the observations for the foregoing out-
puts and the input, which again is a Gaussian process;
thus, like the prior, the posterior predictive density
also decomposes as a product of Gaussian condition-
als. The evidence log p((y(n)
n=1) decom-
poses similarly; as a consequence, the hyperparam-
eters for fm can be learned simply by maximising
log p((y(n)
n=1). In conclusion, in-
ference and learning in GPAR with M outputs comes
down to inference and learning in M decoupled, one-
dimensional GP regression problems. This shows that
(M N 3), de-
without approximations GPAR scales
pending only linearly on the number of outputs M
rather than cubically, as is the case for general multi-
output GPs, which scale

1:m−1, x(n))N

O
Scaling GPAR. In these one-dimensional GP regres-
sion problems, oﬀ-the-shelf GP scaling techniques may
be applied to trivially scale GPAR to large data sets.
Later we utilise the variational inducing point method
by Titsias (2009), which is theoretically principled
(Matthews et al., 2016) and empirically accurate (Bui
et al., 2016b). This method requires a set of inducing
inputs to be speciﬁed for each Gaussian process. For
the ﬁrst output y1 there is a single input x, which is
time. We use ﬁxed (non-optimised) regularly spaced
inducing inputs, as they are known to perform well for
time series (Bui and Turner, 2014). The second and
following outputs ym, however, require inducing points
to be placed in x and y1, . . . , ym−1. Regular spacing
can be used again for x, but there are choices available
for y1, . . . , ym−1. One approach would be to optimise
these inducing input locations, but instead we use the
posterior predictive means of y1, . . . , ym−1 at x. This
choice accelerates training and was found to yield good
results.

Missing data. When there are missing outputs, the
procedures for inference and learning described above
remain valid as long as for every observation y(n)
m there
are also observations y(n)
m−1:1. We call a data set satis-
fying this property closed downwards. If a data set is
not closed downwards, data may be imputed, e.g. using
the model’s posterior predictive mean, to ensure closed
downwardness; inference and learning then, however,
become approximate. The key conditional indepen-
dence expressed by Figure 2b that results in simple
inference and learning is the following:

Theorem 1. Let a set of observations
downwards. Then yi
the observations for outputs 1, . . . , i and
i + 1, . . . , M .

⊥ D

i+1:M

| D

D

1:i, where

D

be closed
1:i are
D
i+1:M for

Theorem 1 is proved in Appendix A from the sup-
plementary material. Note that Theorem 1 holds for
every graphical model of the form of Figure 2b, mean-
ing that the decomposition into single-output modelling
problems holds for any choice of the conditionals in
Equation (1), not just Gaussians.

Potential deﬁciencies of GPAR. GPAR has two
apparent limitations. First, since the outputs from
earlier dimensions are used as inputs to later dimen-
sions, noisy outputs yield noisy inputs. One possible
mitigating solution is to employ a denoising input
transformation for the kernels, e.g. using the poste-
rior predictive mean of the foregoing outputs as the
input of the next covariance function. We shall refer
to GPAR models employing this approach as D-GPAR.
Second, one needs to choose an ordering of the outputs.
Fortunately, often the data admits a natural ordering;
for example, if the predictions concern a single output
dimension, this should be placed last. Alternatively, if
there is not a natural ordering, one can greedily opti-
mise the evidence with respect to the ordering. This
procedure considers 1
2 M (M + 1) conﬁgurations while
an exhaustive search would consider all M ! possible
conﬁgurations: the best ﬁrst output is selected out
of all M choices, which is then ﬁxed; then, the best
second output is selected out of the remaining M
1
choices, which is then ﬁxed; et cetera. These methods
are examined in Section 6.

−

3 GPAR and Multi-Output GPs

The choice of kernels k1:M for f1:M is crucial to GPAR,
as they determine the types of relationships between
inputs and outputs that can be learned. Particular
choices for k1:M turn out to yield models closely related
to existing work. These connections are made rigorous
by the nonlinear and linear equivalent model discussed
in Appendix B from the supplementary material. We

The Gaussian Process Autoregressive Regression Model (GPAR)

summarise the results here; see also Table 1.

If km depends linearly on the foregoing outputs y1:m−1
at particular x, then a joint Gaussian distribution over
the outputs is induced in the form of a multi-output
GP model (Goovaerts, 1997; Stein, 1999; Wackernagel,
2003; Teh and Seeger, 2005; Bonilla et al., 2008; Nguyen
and Bonilla, 2014) where latent processes are mixed
together according to a matrix, called the mixing ma-
trix, that is lower-triangular (Appendix B). One may
let the dependency of km on y1:m−1 vary with x, in
which case the mixing matrix varies with x, meaning
that correlations between outputs vary with x. This
yields an instance of the Gaussian Process Regression
Network (GPRN) (Wilson et al., 2012) where inference
is fast and closed form. One may even let km de-
pend nonlinearly on y1:m−1, which yields a particularly
structured deep Gaussian process (DGP) (Damianou,
2014; Bui et al., 2016a), potentially with skip connec-
tions from the inputs (Appendix B). Note that GPAR
may be interpreted as a conventional DGP where the
hidden layers are directly observed and correspond to
successive outputs; this connection could potentially
be leveraged to bring machinery developed for DGPs
to GPAR, e.g. to deal with arbitrarily missing data.

One can further let km depend on the entirety of the
foregoing outputs y1:m−1, yielding instances of nonlocal
GPAR. An example of a nonlocal linear kernel is

k((y, x), (y(cid:48), x(cid:48))) =

a(x

z, x(cid:48)

z(cid:48))y(z)y(cid:48)(z(cid:48)) dz dz(cid:48).

−

−

(cid:90)

The nonlocal linear kernel again induces a jointly Gaus-
sian distribution over the outputs in the form of a
convolutional multi-output GP model (Álvarez et al.,
2009; Álvarez and Lawrence, 2009; Bruinsma, 2016)
where latent processes are convolved together accord-
ing to a matrix-valued function, called the convolution
matrix, that is lower-triangular (Appendix B). Again,
one may let the dependency of km on the entirety of
y1:m−1 vary with x, in which case the convolution ma-
trix varies with x, or even let km depend nonlinearly
on the entirety of y1:m−1; an example of a nonlocal
nonlinear kernel is

k(y, y(cid:48)) = σ2 exp

(cid:18)

(cid:90)

−

1
2(cid:96)(z)

(cid:19)

(y(z)

y(cid:48)(z))2 dz

.

−

Henceforth, we shall refer to GPAR with linear de-
pendencies between outputs as GPAR-L, GPAR with
nonlinear dependencies between outputs as GPAR-NL,
and a combination of the two as GPAR-L-NL.

4 Further Related Work

The Gaussian Process Network (Friedman and Nach-
man, 2000) is similar to GPAR, but was instead devel-
oped to identify causal dependencies between variables

in a probabilistic graphical models context rather than
multi-output regression. The work by Yuan (2011) also
discusses a model similar to GPAR, but speciﬁes a
diﬀerent generative procedure for the outputs.

The multi-ﬁdelity modelling literature is closely related
to multi-output modelling. Whereas in a multi-output
regression task we predict all outputs, multi-ﬁdelity
modelling is concerned with predicting a particular
high-ﬁdelity function, incorporating information from
observations from various levels of ﬁdelity. The idea
of iteratively conditioning on lower ﬁdelity models in
the construction of higher ﬁdelity ones is a well-used
strategy (Kennedy and O’Hagan, 2000; Le Gratiet and
Garnier, 2014). The model presented by Perdikaris
et al. (2017) is nearly identical to GPAR applied in the
multi-ﬁdelity framework, but applications outside this
setting have not been considered.

Moreover, GPAR follows a long line of work on the
family of fully visible Bayesian networks (Frey et al.,
1996; Bengio and Bengio, 2000) that decompose the
distribution over the observations according to the
product rule (Equation (1)) and model the resulting
one dimensional conditionals. A number of approaches
use neural networks for this purpose (Neal, 1992; Frey
et al., 1996; Larochelle and Murray, 2011; Theis and
Bethge, 2015; van den Oord et al., 2016). In particular,
if the observations are real-valued, a standard archi-
tecture lets the conditionals be Gaussian with means
encoded by neural networks and ﬁxed variances. Under
broad conditions, if these neural networks are replaced
by Bayesian neural networks with independent Gaus-
sian priors over the weights, we recover GPAR as the
width of the hidden layers goes to inﬁnity (Neal, 1996;
Matthews et al., 2018).

5 Synthetic Data Experiments

GPAR is well suited for problems where there is a
strong functional relationship between outputs and for
problems where observation noise is richly structured
and input dependent. In this section we demonstrate
GPAR’s ability to model both types of phenomena.

First, we test the ability to leverage strong functional
relationships between the outputs. Consider three out-
puts y1, y2, and y3, inter-depending nonlinearly:
x4 + (cid:15)1,

−

sin(10π(x + 1))/(2x + 1)
y1(x) =
y2(x) = cos2(y1(x)) + sin(3x) + (cid:15)2,
y3(x) = y2(x)y2

1(x) + 3x + (cid:15)3,

−

i.i.d.
∼ N

where (cid:15)1, (cid:15)2, (cid:15)3
(0, 0.05). By substituting y1 and
y2 into y3, we see that y3 can be expressed directly in
terms of x, but via a complex function. The dependence
of y3 on y1, y2, however, is much simpler. Therefore, as
GPAR can exploit direct dependencies between y1:2 and

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

GPAR Deps. Between Outputs Kernels for f1:M

Local Linear

Nonlocal Linear

+ dep. on x

Nonlinear

+ dep. on x

+ dep. on x

Nonlinear

+ dep. on x

Related Models

Multi-Output GPs [1]

k1(x, x(cid:48)) + kLinear(y(x), y(x(cid:48)))
k1(x, x(cid:48)) + k2(x, x(cid:48))kLinear(y(x), y(x(cid:48))) GPRN [2]
k1(x, x(cid:48)) + k2(y(x), y(x(cid:48)))
k1(x, x(cid:48)) + k2((x, y(x)), (x(cid:48), y(x(cid:48))))
k1(x, x(cid:48)) + kLinear(y, y(cid:48))
k1(x, x(cid:48)) + k2(x, x(cid:48))kLinear(y, y(cid:48))
k1(x, x(cid:48)) + k2(y, y(cid:48))
k1(x, x(cid:48)) + k2((x, y), (x(cid:48), y(cid:48)))

Deep GPs (DGPs) [3]
DGPs with input connections

Convolutional MOGPs [4]

Table 1: Classiﬁcation of kernels k1:M for f1:M , the resulting dependencies between outputs, and related models.
Here kLinear refers to a linear kernel and k1 and k2 to an exponentiated quadratic (EQ) or rational quadratic (RQ)
kernel (Rasmussen and Williams, 2006). [1]: Goovaerts (1997); Stein (1999); Wackernagel (2003); Teh and Seeger
(2005); Bonilla et al. (2008); Osborne et al. (2008); Nguyen and Bonilla (2014). [2]: Wilson et al. (2012). [3]:
Damianou (2014); Bui et al. (2016a). [4]: Álvarez et al. (2009); Álvarez and Lawrence (2009); Bruinsma (2016).

y3, it should be presented with a much simpliﬁed task
as compared to predicting y3 from x directly. Figure 3
shows plots of independent GPs (IGPs) and GPAR ﬁt
to 30 data points from y1, y2 and y3. Indeed observe
that GPAR is able to learn y2’s dependence on y1, and
y3’s dependence on y1 and y2, whereas the independent
GPs struggle with the complicated structure.

Second, we test GPAR’s ability to capture non-
Gaussian and input-dependent noise. Consider the fol-
lowing three schemes in which two outputs are observed
under various noise conditions: y1(x) = f1(x) + (cid:15)1 and

(1): y2(x) = f2(x) + sin2(2πx)(cid:15)1 + cos2(2πx)(cid:15)2,
(2): y2(x) = f2(x) + sin(π(cid:15)1) + (cid:15)2,
(3): y2(x) = f2(x) + sin(πx)(cid:15)1 + (cid:15)2,

i.i.d.
∼ N

(0, 0.1), and f1 and f2 are compli-
where (cid:15)1, (cid:15)2
cated, nonlinear functions.2 All three schemes have
i.i.d. homoscedastic Gaussian noise in y1. The noise
in y2, however, depends on that in y1 and can be
heteroscadastic. The task for GPAR is to learn the
scheme’s noise structure. Figure 4 visualises the noise
correlations induced by the schemes and the noise struc-
tures learned by GPAR. Observe that GPAR is able
to learn the various noise structures.

6 Real-World Data Experiments

In this section we evaluate GPAR’s performance and
compare to other models on four standard data sets
commonly used to evaluate multi-output models. We
also consider a recently-introduced data set in the ﬁeld
of Bayesian optimisation, which is a downstream ap-
plication area that could beneﬁt from GPAR. Table 2
lists the models against which we compare GPAR. We

2The functions given by f1(x) = − sin(10π(x + 1))/(2x +
5 e2x (θ1 cos(θ2πx) + θ3 cos(θ4πx)) +

1) − x4 and f2(x) = 1
√

2x.

Acronym Model

Independent GPs
Cokriging
Intrinstic Coregionalisation Model [1]
Semi-Parametric Latent Factor Model [2]
Collaborative Multi-Output GPs [3]

IGP
CK
ICM
SLFM
CGP
CMOGP Convolved Multi-Output GP Model [4]
GP Regression Network [5]
GPRN

Table 2: Models against which GPAR is compared. [1]:
Goovaerts (1997); Stein (1999); Wackernagel (2003).
[2]: Teh and Seeger (2005). [3]: Nguyen and Bonilla
(2014). [4]: Álvarez and Lawrence (2011); Álvarez et al.
(2010). [5]: Wilson et al. (2012).

always compare against IGP and CK, ICM, SLFM,
and CGP, and compare against CMOGP and GPRN
if results for the considered task are available. Since
CK and ICM are much simpliﬁed versions of SLFM
(Álvarez et al., 2010; Goovaerts, 1997) and CGP is an
approximation to SLFM, we sometimes omit results
for CK, ICM, and CGP. Implementations can be found
at https://github.com/wesselb/gpar (Python) and
https://github.com/willtebbutt/GPAR.jl (Julia).
Experimental details can be found in Appendix D from
the supplementary material.

Electroencephalogram (EEG) data set.3 This
data set consists of 256 voltage measurements from
7 electrodes placed on a subject’s scalp whilst the
subject is shown a certain image; Zhang et al. (1995)
describe the data collection process in detail. In par-
ticular, we use frontal electrodes FZ and F1–F6 from
the ﬁrst trial on control subject 337. The task is to
predict the last 100 samples for electrodes FZ, F1, and
F2, given that the ﬁrst 156 samples of FZ, F1, and F2

3 The EEG data set can be downloaded at https://

archive.ics.uci.edu/ml/datasets/eeg+database.

The Gaussian Process Autoregressive Regression Model (GPAR)

Figure 3: Synthetic data set with complex output dependencies: GPAR vs independent GPs (IGP) predictions.

Model

IGP

MAE
MAE∗

0.5739
0.5753

CK†

0.51

ICM SLFM CMOGP†

0.4601
0.4114

0.4606
0.4145

0.4552

Model GPRN† GPAR-NL D-GPAR-NL

MAE
MAE∗

0.4525
0.4040

0.4324
0.4168

0.4114
0.3996

Table 4: Results for the Jura data set for IGP, cok-
riging (CK) and ICM with two latent dimensions, the
SLFM with two latent dimensions, CMOGP, GPRN,
∗ Results are obtained by ﬁrst log-
and GPAR.
transforming the data, then performing prediction, and
ﬁnally transforming the predictions back to the original
† Results from Wilson (2014).
domain.

perimental protocol by Goovaerts (1997) also followed
by Álvarez and Lawrence (2011): The training data
comprises 259 data points distributed spatially with
three output variables—nickel, zinc, and cadmium—
and 100 additional data points for which only two of
the three outputs—nickel and zinc—are observed. The
task is to predict cadmium at the locations of those
100 additional data. Performance is evaluated with the
mean absolute error (MAE).

Table 4 shows the results. The comparatively poor
performance of independent GPs highlights the impor-
tance of exploiting correlations between the mineral
concentrations. Furthermore, Table 4 shows that D-
GPAR-NL signiﬁcantly outperforms the other models,
achieving a new state-of-the-art.

Exchange rates data set.5 This data set consists
of the daily exchange rate w.r.t. USD of the top ten
international currencies (CAD, EUR, JPY, GBP, CHF,
AUD, HKD, NZD, KRW, and MXN) and three precious
metals (gold, silver, and platinum) in the year 2007.
The task is to predict CAD on days 50–100, JPY on
days 100–150, and AUD on days 150–200, given that
CAD is observed on days 1–49 and 101–251, JPY on
days 1–99 and 151–251, and AUD on days 1–149 and
201–251; and that all other currencies are observed
throughout the whole year. Performance is measured

5 The exchange rates data set can be downloaded at

http://fx.sauder.ubc.ca.

Figure 4: Correlation between the sample residues
(deviation from the mean) for y1 and y2. Left, middle,
and right plots correspond to schemes (1), (2) and (3)
respectively. Samples are coloured according to input
value x; that is, all samples for a particular x have the
same colour. If the colour pattern is preserved, then
GPAR has successfully captured how the noise in y1
correlates to that in y2.

Model

SMSE MLL

TT

IGP
SLFM
GPAR-NL

1.75
1.06
0.26

2.60
4.00
1.63

2 sec
11 min
5 sec

Table 3: Results for the EEG data set for IGP, the
SLFM with four latent dimensions, and GPAR.

and the whole signals of F3–F6 are observed. Perfor-
mance is measured with the standardised mean squared
error (SMSE), mean log loss (MLL) (Rasmussen and
Williams, 2006), and training time (TT). Figure 5
visualises predictions for electrode F2, and Table 3
quantiﬁes the results. We observe that GPAR-NL out-
performs independent in terms of SMSE and MLL;
note that independent GPs completely fail to provide
an informative prediction. Furthermore, independent
GPs were trained in two seconds, and GPAR-NL took
only three more seconds; in comparison, training SLFM
took 11 minutes.

Jura data set.4 This data set comprises metal con-
centration measurements collected from the topsoil in
a 14.5 km2 region of the Swiss Jura. We follow the ex-

4 The

data

can
//sites.google.com/site/goovaertspierre/
pierregoovaertswebsite/download/.

downloaded

be

at

https:

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

Figure 5: Predictions for electrode F2 from the EEG data set

Model

IGP∗

CMOGP∗ CGP∗ GPAR-L-NL

SMSE 0.5996

0.2427

0.2125

0.0302

Table 5: Experimental results for the exchange rates
∗ These
data set for IGP, CMOGP, CGP, and GPAR.
numbers are taken from Nguyen and Bonilla (2014).

with the SMSE.

Figure 6 visualises GPAR’s prediction for data set, and
Table 5 quantiﬁes the result. We greedily optimise the
evidence w.r.t. the ordering of the outputs to determine
an ordering, and we impute missing data to ensure
closed downwardness of the data. Observe that GPAR
signiﬁcantly outperforms all other models.

Tidal height, wind speed, and air temperature
data set.6 This data set was collected at 5 minute in-
tervals by four weather stations: Bramblemet, Camber-
met, Chimet, and Sotonmet, all located in Southamp-
ton, UK. The task is to predict the air temperature
measured by Cambermet and Chimet from all other
signals. Performance is measured with the SMSE. This
experiment serves two purposes. First, it demonstrates
that it is simple to scale GPAR to large data sets using
oﬀ-the-shelf inducing point techniques for single-output
GP regression. Second, it shows that scaling to large
data sets enables GPAR to better learn dependencies
between outputs, which, importantly, can signiﬁcantly
improve predictions in regions where outputs are par-
tially observed. We utilise the variational inducing
point method by Titsias (2009) as discussed in Sec-
tion 2, with 10 inducing points per day. This data set
is not closed downwards, so we use mean imputation
when training. We use D-GPAR-L and set the tempo-
ral kernel to be a simple EQ, meaning that the model
cannot make long-range predictions, but instead must
exploit correlations between outputs.

Nguyen and Bonilla (2014) consider from this data set
5 days in July 2013, and predict short periods of the
air temperature measured by Cambermet and Chimet
using all other signals. We followed their setup and
predicted the same test set, but instead trained on

6 The data can be downloaded at http://www.
bramblemet.co.uk, http://cambermet.co.uk, http://
www.chimet.co.uk, and http://sotonmet.co.uk.

the whole of July. Even though the additional ob-
servations do not temporally correlate with the test
periods at all, they enable GPAR to better learn the
relationships between the outputs, which, unsurpris-
ingly, signiﬁcantly improved the predictions: using the
whole of July, GPAR achieves SMSE 0.056, compared
to their SMSE 0.107.

The test set used by Nguyen and Bonilla (2014) is
rather small, yielding high-variance test results. We
therefore do not pursue further comparisons on their
train–test split, but instead consider a bigger, more
challenging setup: using as training data 10 days (days
[10, 20), roughly 30 k points), 15 days (days [18, 23),
roughly 47 k points), and the whole of July (roughly
98 k points), make predictions of 4 day periods of the
air temperature measured by Cambermet and Chimet.
Figure 7 visualises the test periods and GPAR’s pre-
dictions for it. Despite the additional observations
not correlating with the test periods, we observe clear,
though dimishining, improvements in the predictions
as the training data is increased.

MLP validation error data set. The ﬁnal data
set is the validation error of a multi-layer perceptron
(MLP) on the MNIST data, trained using categorical
cross-entropy, and set as a function of six hyperpa-
rameters: the number of hidden layers, the number
of neurons per hidden layer, the dropout rate, the
learning rate to use with the ADAM optimizer, the L1
weight penalty, and the L2 weight penalty. This exper-
iment was implemented using code made available by
Hernández-Lobato (2016). An improved model for the
objective surface could translate directly into improved
performance in Bayesian optimisation (Snoek et al.,
2012), as this would enable a more informed search of
the hyperparameter space.

To generate a data set, we sample 291 sets of hyperpa-
rameters randomly from a rectilinear grid and train the
MLP for 21 epochs under each set of hyperparameters,
recording the validation performance after 1, 5, 11, 16,
and 21 epochs. We construct a training set of 175 of
these hyperparameter settings and, crucially, discard
roughly 30% of the validation performance results at 5
epochs at random, and again discard roughly 30% of
those results at 11 epochs, and so forth. The resulting
data set has 175 labels after 1 epoch, 124 after 5, 88

The Gaussian Process Autoregressive Regression Model (GPAR)

Figure 6: Visualisation of the exchange rates data set and CGP’s (black) and GPAR’s (green) predictions for it.
GPAR’s predictions are overlayed on the original ﬁgure by Nguyen and Bonilla (2014).

Figure 7: Visualisation of the air temperature data set and GPAR’s prediction for it. Black circles indicate the
locations of the inducing points.

have learned to exploit the extra information available.
In turn, GPAR noticeably outperforms the SLFM.

7 Conclusion and Future Work

This paper introduced GPAR: a ﬂexible, fast, tractable,
and interpretable approach to multi-output GP re-
gression. GPAR can model (1) nonlinear relation-
ships between outputs and (2) complex output noise.
GPAR can scale to large data sets by trivially lever-
aging scaling techniques for one-dimensional GP re-
gression (Titsias, 2009). In eﬀect, GPAR transforms
high-dimensional data modelling problems into set of
single-output modelling problems, which are the bread
and butter of the GP approach. GPAR was rigorously
tested on a variety of synthetic and real-world prob-
lems, consistently outperforming existing GP models
for multi-output regression. An exciting future appli-
cation of GPAR is to use compositional kernel search
(Lloyd et al., 2014) to automatically learn and explain
dependencies between outputs and inputs. Further
insights into structure of the data could be gained by
decomposing GPAR’s posterior over additive kernel
components (Duvenaud, 2014). These two approaches
could be developed into a useful tool for automatic
structure discovery. Two further exciting future ap-
plications of GPAR are modelling of environmental
phenomena and improving data eﬃciency of existing
Bayesian optimisation tools (Snoek et al., 2012).

Figure 8: Results for the machine learning data set
for a GP, the SLFM with two latent dimensions, and
GPAR

after 11, 64 after 15 and 44 after 21, simulating the
partial completion of the majority of runs. Importantly,
a Bayesian Optimisation system typically exploits only
completed training runs to inform the objective surface,
whereas GPAR can also exploit partially complete runs.

The results presented in Figure 8 show the SMSE in
predicting validation performance at each epoch using
GPAR, the SLFM, and independent GPs on the test
set, averaged over 10 seed for the pseudo-random num-
ber generator used to select which outputs from the
training set to discard. GPs trained independently to
predict performance after a particular number of epochs
perform worse than the SLFM and GPAR, which both

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

Acknowledgements

Richard E. Turner is supported by Google as well as
EPSRC grants EP/M0269571 and EP/L000776/1.

References

Álvarez, M. and Lawrence, N. D. (2009). Sparse con-
volved gaussian processes for multi-output regression.
Advances in Neural Information Processing Systems,
21:57–64.

Álvarez, M. A. and Lawrence, N. D. (2011). Computa-
tionally eﬃcient convolved multiple output Gaussian
processes. Journal of Machine Learning Research,
12:1459–1500.

Álvarez, M. A., Luengo, D., and Lawrence, N. D. (2009).
Latent force models. Artiﬁcial Intelligence and Statis-
tics, 5:9–16.

Álvarez, M. A., Luengo, D., Titsias, M. K., and
Lawrence, N. D. (2010). Eﬃcient multioutput Gaus-
sian processes through variational inducing kernels.
Journal of Machine Learning Research: Workshop
and Conference Proceedings, 9:25–32.

Bengio, Y. and Bengio, S. (2000). Modeling high-
dimensional discrete data with multi-layer neural
networks. In Advances in Neural Information Pro-
cessing Systems, pages 400–406.

Bonilla, E. V., Chai, K. M., and Williams, C. K. I.
(2008). Multi-task Gaussian process prediction. Ad-
vances in Neural Information Processing Systems,
20:153–160.

Bruinsma, W. P. (2016). The generalised gaussian
convolution process model. MPhil thesis, Department
of Engineering, University of Cambridge.

Bui, T. D., Hernández-Lobato, D., Li, Y., Hernández-
Lobato, J. M., and Turner, R. E. (2016a). Deep
gaussian processes for regression using approxi-
mate expectation propagation.
arXiv preprint
arXiv:1602.04133.

Bui, T. D. and Turner, R. E. (2014). Tree-structured
Gaussian process approximations. Advances in Neu-
ral Information Processing Systems, 27:2213–2221.

Bui, T. D., Yan, J., and Turner, R. E. (2016b). A
unifying framework for gaussian process pseudo-point
approximations using power expectation propagation.
arXiv preprint arXiv:1605.07066.

Dai, Z., Álvarez, M. A., and Lawrence, N. D. (2017).
Eﬃcient modeling of latent information in supervised
learning using Gaussian processes. arXiv preprint
arXiv:1705.09862.

Damianou, A. (2014). Deep Gaussian Processes and
Variational Propagation of Uncertainty. PhD thesis,
Department of Neuroscience, University of Sheﬃeld.

Duvenaud, D. (2014). Automatic Model Construction
with Gaussian Processes. PhD thesis, Computational
and Biological Learning Laboratory, University of
Cambridge.

Duvenaud, D., Lloyd, J. R., Grosse, R., Tenenbaum,
J. B., and Ghahramani, Z. (2013). Structure discov-
ery in nonparametric regression through composi-
tional kernel search. In International Conference on
Machine Learning.

Frey, B. J., Hinton, G. E., and Dayan, P. (1996). Does
the wake-sleep algorithm produce good density esti-
mators? In Advances in neural information process-
ing systems, pages 661–667.

Friedman, N. and Nachman, I. (2000). Gaussian process
networks. In Uncertainty in Artiﬁcial Intelligence,
pages 211–219. Morgan Kaufmann Publishers Inc.

Goovaerts, P. (1997). Geostatistics for Natural Re-
sources Evaluation. Oxford University Press, 1 edi-
tion.

Hernández-Lobato, J. M. (2016). Neural networks with
optimal accuracy and speed in their predictions.

Kennedy, M. C. and O’Hagan, A. (2000). Predicting
the output from a complex computer code when fast
approximations are available. Biometrika, 87(1):1–
13.

Koller, D. and Friedman, N. (2009). Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.

Larochelle, H. and Murray, I. (2011). The neural au-
In AISTATS,

toregressive distribution estimator.
volume 1, page 2.

Le Gratiet, L. and Garnier, J. (2014). Recursive co-
kriging model for design of computer experiments
with multiple levels of ﬁdelity. International Journal
for Uncertainty Quantiﬁcation, 4(5).

Lloyd, J. R., Duvenaud, D., Grosse, R., Tenenbaum,
J. B., and Ghahramani, Z. (2014). Automatic con-
struction and natural-language description of non-
parametric regression models. In Association for the
Advancement of Artiﬁcial Intelligence (AAAI).

Matthews, A. G. D. G., Hensman, J., Turner, R. E.,
and Ghahramani, Z. (2016). On sparse variational
methods and the kullback-leibler divergence be-
tween stochastic processes. Artiﬁcial Intelligence
and Statistics, 19.

Matthews, A. G. d. G., Rowland, M., Hron, J., Turner,
R. E., and Ghahramani, Z. (2018). Gaussian pro-
cess behaviour in wide deep neural networks. arXiv
preprint arXiv:1804.11271.

Neal, R. M. (1992). Connectionist learning of belief

networks. Artiﬁcial intelligence, 56(1):71–113.

The Gaussian Process Autoregressive Regression Model (GPAR)

Wilson, A. G. (2014). Covariance Kernels for Fast Au-
tomatic Pattern Discovery and Extrapolation With
Gaussian Processes. PhD thesis, University of Cam-
bridge.

Wilson, A. G., Knowles, D. A., and Ghahramani, Z.
(2012). Gaussian process regression networks. Inter-
national Conference on Machine Learning, 29.

Yuan, C. (2011). Conditional multi-output regression.
In International Joint Conference on Neural Net-
works, pages 189–196. IEEE.

Zhang, X., Begleiter, H., Porjesz, B., Wang, W., and
Litke, A. (1995). Event related potentials during
object recognition tasks. Brain Research Bulletin,
38(6):531–538.

Neal, R. M. (1996). Bayesian learning for neural net-
works, volume 118. Springer Science & Business
Media.

Nguyen, T. V. and Bonilla, E. V. (2014). Collaborative
multi-output Gaussian processes. Conference on
Uncertainty in Artiﬁcial Intelligence, 30.

Nocedal, J. and Wright, S. J. (2006). Numerical Opti-

mization. Springer, second edition.

Osborne, M. A., Roberts, S. J., Rogers, A., Ramchurn,
S. D., and Jennings, N. R. (2008). Towards real-time
information processing of sensor network data us-
ing computationally eﬃcient multi-output Gaussian
processes. In Proceedings of the 7th International
Conference on Information Processing in Sensor Net-
works, IPSN ’08, pages 109–120. IEEE Computer
Society.

Perdikaris, P., Raissi, M., Damianou, A., Lawrence,
N. D., and Karniadakis, G. E. (2017). Nonlinear
information fusion algorithms for data-eﬃcient multi-
ﬁdelity modelling. Proceedings of the Royal Society
A: Mathematical, Physical and Engineering Science,
473.

Rasmussen, C. E. and Williams, C. K. I. (2006). Gaus-
sian Processes for Machine Learning. MIT Press.

Snoek, J., Larochelle, H., and Adams, R. P. (2012).
Practical bayesian optimization of machine learn-
ing algorithms. In Advances in Neural Information
Processing Systems, pages 2951–2959.

Stein, M. (1999).

Interpolation of Spatial Data.

Springer-Verlag New York, 1 edition.

Sun, S., Zhang, G., Wang, C., Zeng, W., Li, J., and
Grosse, R. (2018). Diﬀerentiable compositional ker-
nel learning for gaussian processes. International
Conference on Machine Learning, 35.

Teh, Y. W. and Seeger, M. (2005). Semiparametric
International Workshop on

latent factor models.
Artiﬁcial Intelligence and Statistics, 10.

Theis, L. and Bethge, M. (2015). Generative image
modeling using spatial lstms. In Advances in Neural
Information Processing Systems, pages 1927–1935.

Titsias, M. K. (2009). Variational learning of inducing
variables in sparse Gaussian processes. Artiﬁcial
Intelligence and Statistics, 12:567–574.

van den Oord, A., Kalchbrenner, N., Espeholt, L.,
Vinyals, O., Graves, A., et al. (2016). Conditional
In Ad-
image generation with pixelcnn decoders.
vances in Neural Information Processing Systems,
pages 4790–4798.

Wackernagel, H. (2003). Multivariate Geostatistics.

Springer-Verlag Berlin Heidelberg, 3 edition.

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

A Conditional Independence in

Figure 2b

In this section, we prove the key conditional indepen-
dence in Figure 2b that makes GPAR work:

Theorem 2. Let a set of observations
downwards. Then yi
the observations for outputs 1, . . . , i and
i + 1, . . . , M .7

⊥ D

i+1:M

| D

D

1:i, where

D

be closed
1:i are
D
i+1:M for

To begin with, we review some basic notions concerning
graphical models. Let a path be a sequence of nodes
v1, . . . , vn from some directed graph G where, for each
vi, G either contains an edge from vi to vi+1, or an
edge from vi+1 to vi. If G contains an edge from a to b,
then write a
b to mean the two-node path in which
node b follows node a. Similarly, if G contains an edge
from b to a, then write a
b to mean the two-node
←
path in which b follows a. Write a (cid:10) b to mean either
a

b or b

→

a.

→

←

= v1 (cid:10)

Deﬁnition 1 (Active Path (Deﬁnition 3.6 from Koller
(cid:10) vn be a
and Friedman (2009))). Let
path in a graphical model. Let Z be a subset of the
active
variables from the graphical model. Then, call
given Z if (1) for every v-structure vi−1
vi+1
vi
in
, vi or a descendant of vi is in Z; and (2) no other
P
node in

is in Z.

· · ·

←

→

P

P

P

Deﬁnition 2 (d-Separation (Deﬁnition 3.7 from Koller
and Friedman (2009))). Let X, Y , and Z be three sets
of nodes from a graphical model. Then, call X and Y
d-separated given Z if no path between any x
X and
y

Y is active given Z.

∈

∈

Theorem 3 (d-Separation Implies Conditional Inde-
pendence (Theorem 3.3 from Koller and Friedman
(2009))). Let X, Y , and Z be three sets of nodes from
a graphical model. If X and Y are d-separated given
Z, then X

Z.

Y

⊥

|

Deﬁne the layer of a node in Figure 2b to be

layer(fi) = layer(yi(x)) = i.

We are now ready to prove Theorem 2.

Proof of Theorem 2. For i < j, let
tween any yi(x(cid:48))
∈
be the ﬁrst node in

be a path be-
P
i+1:N . Let yk(ˆx)
such that layer(yk(ˆx)) > i. Then

yi and yj(x)

∈ D

contains

P

P

ym(ˆx)

yk(ˆx) (cid:10)

· · · →

→

· · ·

7 D1:i = {y(n)
j ∈ D : j > i, n ≤ N }.

j

{y(n)

∈ D : j ≤ i, n ≤ N } and Di+1:M =

for some m

i < k.

If yk(ˆx)
ym(ˆx)

∈ D

≤
i+1:N , then, since

1:i, meaning that

∈ D

P
If, on the other hand, yk(ˆx) /
downwards, yk(cid:48)(ˆx) /
≥
cannot be descendant of yk(ˆx), so

∈ D
for all k(cid:48)

∈ D

is closed downwards,

D
is inactive.

, then, since

is closed
D
k. Therefore, yj(x)
must contain

P

yk(cid:48)(ˆx)

yk(cid:48)(cid:48)(ˆx)

fk(cid:48)(cid:48)

· · · →

→

←

→ · · ·

for some m
We conclude that
descendant of yk(cid:48)(cid:48)(ˆx) can be in

k(cid:48) < k(cid:48)(cid:48), which forms a v-structure.
is inactive, because yk(cid:48)(cid:48) (ˆx) nor a

≤

P

.

D

B The Nonlinear and Linear

Equivalent Model

In this section, we construct equivalent models for
GPAR (Lemmas 1 and 2). These models make GPAR’s
connection to other models in the literature explicit.

To begin with, we must introduce some notation and
M )X
M ,
deﬁnitions. For functions A, B :
→ Y
deﬁne composition
B)(x, y) =
is well-deﬁned and as-
A(x, B(
·
M , denote
sociative.
, u). Again, note that
u :
A
◦
B)
(A
◦

u = A(
u). Furthermore, denote

For a function u :

X → Y
u = A
◦

, y)). Note that

M , A
(B

as follows:

X → Y

(
(A

Y
◦

X ×

◦
◦

◦

◦

◦

·

A
(cid:124)

◦ · · · ◦
(cid:123)(cid:122)
n times

A
(cid:125)

= An.

(

(

Y

Y

X ×

X ×

→ Y

→ Y

M )X

M )X

M such that
depends only on (x, y1:i−1),
M , denote
f , and denote N consecutive applications

Consider a function A :
Ai(x, y) :
where A1 = 0. Further let u, y :
T f = u + A
◦
of T by TN .
The expression TM −1 u will be key in constructing
the equivalent models. We show that it is the unique
solution of a functional equation:

X → Y

Proposition 1. The unique solution of y = u + A
is y = TM −1 u.

y

◦

Proof of Proposition 1. First, we show that y = u +
y has a solution, and that this solution is unique.
A
Because Ai(x, y) depends only on (x, y1:i−1), it holds
that

◦

yi = ui + Ai

y = ui + Ai

(y1:i−1, 0),

◦

◦

where (y1:i−1, 0) represents the concatenation of y1:i−1
and M
i + 1 zeros. Thus, yi can uniquely be con-
structed from ui, Ai, and y1:i−1; therefore, y1 exists
and is unique, so y2 exists and is unique: by induction
we ﬁnd that y exists and is unique.

−

The Gaussian Process Autoregressive Regression Model (GPAR)

Second, we show that y = TM −1 u satisﬁes y = u +
y = T y. To show this, we show that (Tn u)i =
A
(Tn−1 u)i for i = 1, . . . , n, for all n. To begin with, we
show the base case, n = 1:

◦

(T u)1 = u1 + A1

u = u1 = (T0 u)1,

◦
since A1 = 0. Finally, suppose that the claim holds for
a particular n. We show that the claim then holds for
n + 1: Let i

n + 1. Then

≤

(Tn+1 u)i = ui + Ai
= ui + Ai

◦

◦

◦

◦

◦

= ui + Ai

(∗)
= ui + Ai

= ui + Ai
= (Tn u)i,

Tn u
((Tn u)1:i−1, (Tn u)i:M )
((Tn−1 u)1:i−1, (Tn u)i:M )

(By assumption)

((Tn−1 u)1:i−1, (Tn−1 u)i:M )
Tn−1 u

where (
) holds because Ai(x, y) depends only on
∗
(x, y1:i−1).

In the linear case, TM −1 u turns out to greatly simplify.

Proposition 2.
TM −1 u = ((cid:80)M −1

i=0 Ai)

u.

◦

If A(x, y) is linear in y,

then

Proof of Proposition 2. If A(x, y) is linear in y, then
distributes over addition. Therefore,
one veriﬁes that

◦
TM −1 u = u + A

= u + A
...
= u + A

◦

◦

◦

TM −2 u
u + A2

◦

TM −3 u

u +

· · ·

+ AM −1

u.

◦

We now use Propositions 1 and 2 to construct a non-
linear and linear equivalent model.

(

GP

Lemma 1 (Nonlinear Equivalent Model). Let A be an
M )X ,
M -dimensional vector-valued process over
(0, kAi) independently, and let
each Ai drawn from
u be an M -dimensional vector-valued process over
,
X
(0, kui) independently. Further-
each ui drawn from
depend only on
more, let Ai(x, y) :
(
(x, y1:i−1), meaning that kAi = kAi(x, y1:i−1, x(cid:48), y(cid:48)
1:i−1),
and let A1 = 0. Denote T f = u + A
f , and denote
N consecutive applications of T by TN . Then

GP
X ×

M )X

→ Y

X ×

Y

Y

◦

yi

y1:i−1

|

∼ GP

y

|

A, u = TM −1 u
(0, kui + kAi(

⇐⇒
, y1:i−1,

·

, y1:i−1)).

Proof of Lemma 1. Since Ai(x, y) depends only on
(x, y1:i−1), it holds by Proposition 1 that any sam-
y, so yi =
A, u satisﬁes yi = ui + Ai
ple from y

|

·

◦

ui + Ai
concatenation of y1:i−1 and M
equivalence now follows.

(y1:i−1, 0), where (y1:i−1, 0) represents the
i + 1 zeros. The

−

◦

Lemma 2 (Linear Equivalent Model). Suppose that
A was instead generated from

(cid:90)

A(x, y)

ˆA =

ˆA(x

z)y(z) dz,

|

−

where ˆA is an (M

, each ˆAi,j drawn from

(0, k ˆAi,j
X
i > j and ˆAi,j = 0 otherwise. Then

GP

×

M )-matrix-valued process over
) independently if

y

A, u =

|

(cid:32)M −1
(cid:88)

(cid:33)

Ai

i=0

u

◦

⇐⇒

yi

y1:i−1

(0, kui + kAi(

, y1:i−1,

, y1:i−1)), (5)

∼ GP

·

·

|
where

1:i−1)

kAi(x, y1:i−1, x(cid:48), y(cid:48)
(cid:90)

i−1
(cid:88)

k ˆAi,j

(x

=

j=1

z, x(cid:48)

z(cid:48))yj(z)y(cid:48)

j(z(cid:48)) dz dz(cid:48).

−

−

Proof of Lemma 2. First, one veriﬁes that Ai(x, y) still
depends only on (x, y1:i−1), and that Ai(x, y) is linear
in y. The result then follows from Lemma 1 and Propo-
sition 2, where the expression for kAi follows from
straightfoward calculation.

As mentioned in the paper, the kernels for f1:M de-
termine the types of relationships between inputs and
outputs that can be learned. Lemmas 1 and 2 make
this explicit: Lemma 1 shows that nonlocal GPAR can
recover a model where M latent GPs u are repeatedly
composed with another latent GP A, where A has a
particular dependency structure, and Lemma 2 shows
that nonlocal GPAR can recover a model where M
latent GPs u are linearly transformed, where the linear
transform T = (cid:80)M −1
i=0 Ai is lower triangular and may
vary with the input.

|

|

|

◦

u, y

T, u = T

In Lemma 2, note that it is not restrictive that T is
lower triangular: Suppose that T were dense. Then,
letting y
T is jointly Gaussian. Hence
y1:i−1, T is a GP whose mean linearly depends upon
yi
y1:i−1 is of the form of
y1:i−1 via T , meaning that yi
Equation (5) where kui may be more complicated. If,
however, ˆA(z) = δ(z)B for some random (M
M )-
matrix B, each Bi,j drawn from
Bi,j ) if i > j and
Bi,j = 0 otherwise, then it is restrictive that T is lower
B, u = (cid:80)M −1
i=0 Biu(x). If
triangular: In this case, y(x)
T = (cid:80)M −1
i=0 Bi were dense, then, letting y
T, u = T u,
y can be represented with Lemma 2 if and only if y
T ’s
covariance can be diagonalised by a constant, invertible,

(0, σ2

N

×

|

|

|

|

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

lower-triangular matrix. This condition does not hold
in general, as Lemma 3 proves.

C Lemma 3

Call functions k1, . . . , kM :
if

X →

R linearly independent

(cid:32)

x :

∀

M
(cid:88)

i=1

(cid:33)

=

⇒

ciki(x) = 0

c1 = . . . = cM = 0.

X →

R be linearly in-
Lemma 3. Let k1, . . . , kM :
dependent and arrange them in a diagonal matrix
K = diag(k1, . . . , kn). Let A be an invertible M
M
matrix such that its columns cannot be permuted into a
triangular matrix. Then there does not exist an invert-
ible triangular matrix T such that T −1BK(x)BTT −T
is diagonal for all x.

×

Proof. Suppose, on the contrary, that such T does exist.
Then two diﬀerent rows ap and aq of A = T −1B share
nonzero elements in some columns C; otherwise, A
would have exactly one nonzero entry in every column—
A is invertible—so A would be the product of a per-
mutation matrix and a diagonal matrix, meaning that
B = T A’s columns could be permuted into a triangu-
lar matrix. Now, by T −1BK(x)BTT −T = AK(x)AT
being diagonal for all x
i ap,iaq,iki(x) = 0 for
∈ X
all x. Therefore, by linear independence of k1, . . . , kN ,
it holds that ap,iaq,i = 0 for all i. But ap,iaq,i
= 0 for
C, which is a contradiction.
any i

, (cid:80)

∈

D Experimental Details

∗

For every experiment, the form of the kernels is deter-
mined by the particular GPAR model used: GPAR-L,
GPAR-NL, or GPAR-L-NL (see Table 2 in the main
paper), potentially with a D-
preﬁx to indicate the de-
noising procedure outlined in “Potential deﬁciencies of
GPAR” in Section 2 of the paper main. For GPAR-NL,
we always used exponentiated quadratic (EQ) kernels,
except for the exchange rates experiment, where we
used rational quadratic (RQ) kernels (Rasmussen and
Williams, 2006). Furthermore, in every problem we
simply expanded according to Equation (1) in the main
paper or greedily optimised the ordering, in both cases
putting the to-be-predicted outputs last. We used
scipy’s implementation of the L-BFGS-B algorithm
(Nocedal and Wright, 2006) to optimise hyperparame-
ters.

9
1
0
2
 
b
e
F
 
5
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
8
1
7
0
.
2
0
8
1
:
v
i
X
r
a

The Gaussian Process Autoregressive Regression Model (GPAR)

James Requeima12

Will Tebbutt12

Wessel Bruinsma12

Richard E. Turner1

1University of Cambridge and 2Invenia Labs, Cambridge, UK
{jrr41, wct23, wpb23, ret26}@cam.ac.uk

Abstract

Multi-output regression models must exploit
dependencies between outputs to maximise
predictive performance. The application of
Gaussian processes (GPs) to this setting typi-
cally yields models that are computationally
demanding and have limited representational
power. We present the Gaussian Process Au-
toregressive Regression (GPAR) model, a scal-
able multi-output GP model that is able to
capture nonlinear, possibly input-varying, de-
pendencies between outputs in a simple and
tractable way: the product rule is used to
decompose the joint distribution over the out-
puts into a set of conditionals, each of which
is modelled by a standard GP. GPAR’s eﬃ-
cacy is demonstrated on a variety of synthetic
and real-world problems, outperforming exist-
ing GP models and achieving state-of-the-art
performance on established benchmarks.

1

Introduction

The Gaussian process (GP) probabilistic modelling
framework provides a powerful and popular approach
to nonlinear single-output regression (Rasmussen and
Williams, 2006). The popularity of GP methods stems
from their modularity, tractability, and interpretability:
it is simple to construct rich, nonlinear models by com-
positional covariance function design, which can then
be evaluated in a principled way (e.g. via the marginal
likelihood), before being interpreted in terms of their
component parts. This leads to an attractive plug-
and-play approach to modelling and understanding
data, which is so robust that it can even be automated
(Duvenaud et al., 2013; Sun et al., 2018).

Most regression problems, however, involve multiple
outputs rather than a single one. When modelling such
data, it is key to capture the dependencies between
these outputs. For example, noise in the output space

Proceedings of the 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2019, Naha,
Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by
the author(s).

might be correlated, or, whilst one output might de-
pend on the inputs in a complex (deterministic) way,
it may depend quite simply on other output variables.
In both cases multi-output GP models are required.
There is a plethora of existing multi-output GP models
that can capture linear correlations between output
variables if these correlations are ﬁxed across the in-
put space (Goovaerts, 1997; Wackernagel, 2003; Teh
and Seeger, 2005; Bonilla et al., 2008; Nguyen and
Bonilla, 2014; Dai et al., 2017). However, one of the
main reasons for the popularity of the GP approach is
that a suite of diﬀerent types of nonlinear input depen-
dencies can be modelled, and it is disappointing that
this ﬂexibility is not extended to interactions between
the outputs. There are some approaches that do allow
limited modelling of nonlinear output dependencies
(Wilson et al., 2012; Bruinsma, 2016) but this ﬂexibil-
ity comes from sacriﬁcing tractability, with complex
and computationally demanding approximate inference
and learning schemes now required. This complexity
signiﬁcantly slows down model ﬁtting, evaluation, and
improvement work ﬂow.

What is needed is a ﬂexible and analytically tractable
modelling approach to multi-output regression that
supports plug-and-play modelling and model interpre-
tation. The Gaussian Process Autoregressive Regres-
sion (GPAR) model, introduced in Section 2, achieves
these aims by taking an approach analogous to that em-
ployed by the Neural Autoregressive Density Estimator
(Larochelle and Murray, 2011) for density modelling.
The product rule is used to decompose the distribu-
tion of the outputs given the inputs into a set of one-
dimensional conditional distributions. Critically, these
distributions can be interpreted as a decoupled set of
single-output regression problems, and learning and in-
ference in GPAR therefore amount to a set of standard
single-output GP regression tasks: training is closed
form, fast, and amenable to standard scaling techniques.
GPAR converts the modelling of output dependencies
that are possibly nonlinear and input-dependent into a
set of standard GP covariance function design problems,
constructing expressive, jointly non-Gaussian models
over the outputs. Importantly, we show how GPAR can
capture nonlinear relationships between outputs as well
as structured, input-dependent noise, simply through

The Gaussian Process Autoregressive Regression Model (GPAR)

CO2 C(t)

to some unknown, random function f3; et cetera:

f1( , t)
=
Temperature T (t)

y1(x) = f1(x),
y2(x) = f2(y1(x), x),

f1
f2

p(f1),
p(f2),

Sea Ice I(t)

= f2( ,

, t)

Figure 1: Cartoon motivating a factorisation for the
joint distribution p(I(t), T (t), C(t))

kernel hyperparameter learning. We apply GPAR to
a wide variety of multi-output regression problems,
achieving state-of-the-art results on ﬁve benchmark
tasks.

2 GPAR

Consider the problem of modelling the world’s average
CO2 level C(t), temperature T (t), and Arctic sea ice
extent I(t) as a function of time t. By the greenhouse
eﬀect, one can imagine that the temperature T (t) is a
complicated, stochastic function f1 of CO2 and time:
T (t) = f1(C(t), t). Similarly, one might hypothesise
that the Arctic sea ice extent I(t) can be modelled as
another complicated function f2 of temperature, CO2,
and time: I(t) = f2(T (t), C(t), t). These functional
relationships are depicted in Figure 1 and motivate
the following model where the conditionals model the
postulated underlying functions f1 and f2:

p(I(t), T (t), C(t))

= p(C(t)) p(T (t)

C(t))
(cid:125)

|
(cid:123)(cid:122)
models f1

p(I(t)
(cid:124)

T (t), C(t))
(cid:123)(cid:122)
(cid:125)
models f2

|

.

(cid:124)

More generally, consider the problem of modelling M
outputs y1:M (x) = (y1(x), . . . , yM (x)) as a function of
the input x. Applying the product rule yields1

p(y1:M (x))
y1(x))
= p(y1(x)) p(y2(x)
|
(cid:124)
(cid:125)
(cid:123)(cid:122)
y2(x) as a random
function of y1(x)

· · ·

p(yM (x)
(cid:124)

y1:M −1(x)),
(cid:125)
(cid:123)(cid:122)
yM (x) as a random
function of y1:M −1(x)

|

(1)

which states that y1(x), like CO2, is ﬁrst generated
from x, according to some unknown, random function
f1; that y2(x), like temperature, is then generated
from y1(x) and x, according to some unknown, random
function f2; that y3(x), like the Arctic sea ice extent,
is then generated from y2(x), y1(x), and x, according

1This requires an ordering of the outputs; we will address

this point in Section 2.

∼

∼

∼

...

yM (x) = fM (y1:M −1(x), x)

fM

p(fM ).

GPAR, introduced now, models these unknown func-
tions f1:M with Gaussian processes (GPs). Recall that
a GP f over an index set
deﬁnes a stochastic process,
or process in short, where f (x1), . . . , f (xN ) are jointly
Gaussian distributed for any x1, . . . , xN (Rasmussen
and Williams, 2006). Marginalising out f1:M , we ﬁnd
that GPAR models the conditionals in Equation (1)
with Gaussian processes:

X

ym

y1:m−1

|

∼ GP

(0, km((y1:m−1(x), x), (y1:m−1(x(cid:48)), x(cid:48)))).

(2)

Although the conditionals in Equation (1) are Gaus-
sian, the joint distribution p(y1:N ) is not; moments of
the joint distribution over the outputs are generally
intractable, but samples can be generated by sequen-
tially sampling the conditionals. Figure 2b depicts the
graphical model corresponding to GPAR. Crucially, the
kernels (km) may specify nonlinear, input-dependent
relationships between outputs, which enables GPAR to
model data where outputs inter-depend in complicated
ways.

Returning to the climate modelling example, one might
object that the temperature T (t) at time t does not
just depend the CO2 level C(t) at time t, but in-
stead depends on the entire history of CO2 levels C:
T (t) = f1(C, t). Note: by writing C instead of C(t),
we refer to the entire function C, as opposed to just its
value at t. Similarly, one might object that the Arctic
sea ice extent I(t) at time t does not just depend on
the temperature T (t) and CO2 level C(t) at time t,
but instead depends on the entire history of temper-
atures T and CO2 levels C: T (t) = f2(T, C, t). This
kind of dependency structure, where output ym(x) now
depends on the entirety of all foregoing outputs y1:m
instead of just their value at x, motivates the following
generalisation of GPAR in its form of Equation (2):

ym

y1:m−1

|

∼ GP

(0, km((y1:m−1, x), (y1:m−1, x(cid:48)))),

(3)

which we refer to as nonlocal GPAR, or non-
instantaneous in the temporal setting. Clearly, GPAR
is a special case of nonlocal GPAR. Figure 2a depicts
the graphical model corresponding to nonlocal GPAR.
Nonlocal GPAR will not be evaluated experimentally,
but some theoretical properties will be described.

Inference and learning in GPAR. Inference and
learning in GPAR is simple. Let y(n)
m = ym(x(n)) de-
note an observation for output m. Then, assuming all

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

(a)

f2

y2

f1

y1

f3

y3

(b)

f2

f1

f3

y1(x)

y2(x)

y3(x)

x

(4)

Figure 2: Graphical models corresponding to (a) non-
local GPAR and (b) GPAR

outputs are observed at each input, we ﬁnd

(y(n)

1:M , x(n))N

n=1)

|

p(f1:M
M
(cid:89)

=

m=1

p(fm

|

(y(n)
m )N
n=1
(cid:123)(cid:122)
(cid:125)
(cid:124)
observations
for fm

, (y(n)
(cid:124)

1:m−1, x(n))N
(cid:123)(cid:122)
input locations
of observations

n=1
(cid:125)

),

O

n=1 |

n=1 |

m )N

(y(n)

1:M )N

(x(n))N

(M 3N 3).

meaning that the posterior of fm is computed sim-
ply by conditioning fm on the observations for output
m located at the observations for the foregoing out-
puts and the input, which again is a Gaussian process;
thus, like the prior, the posterior predictive density
also decomposes as a product of Gaussian condition-
als. The evidence log p((y(n)
n=1) decom-
poses similarly; as a consequence, the hyperparam-
eters for fm can be learned simply by maximising
log p((y(n)
n=1). In conclusion, in-
ference and learning in GPAR with M outputs comes
down to inference and learning in M decoupled, one-
dimensional GP regression problems. This shows that
(M N 3), de-
without approximations GPAR scales
pending only linearly on the number of outputs M
rather than cubically, as is the case for general multi-
output GPs, which scale

1:m−1, x(n))N

O
Scaling GPAR. In these one-dimensional GP regres-
sion problems, oﬀ-the-shelf GP scaling techniques may
be applied to trivially scale GPAR to large data sets.
Later we utilise the variational inducing point method
by Titsias (2009), which is theoretically principled
(Matthews et al., 2016) and empirically accurate (Bui
et al., 2016b). This method requires a set of inducing
inputs to be speciﬁed for each Gaussian process. For
the ﬁrst output y1 there is a single input x, which is
time. We use ﬁxed (non-optimised) regularly spaced
inducing inputs, as they are known to perform well for
time series (Bui and Turner, 2014). The second and
following outputs ym, however, require inducing points
to be placed in x and y1, . . . , ym−1. Regular spacing
can be used again for x, but there are choices available
for y1, . . . , ym−1. One approach would be to optimise
these inducing input locations, but instead we use the
posterior predictive means of y1, . . . , ym−1 at x. This
choice accelerates training and was found to yield good
results.

Missing data. When there are missing outputs, the
procedures for inference and learning described above
remain valid as long as for every observation y(n)
m there
are also observations y(n)
m−1:1. We call a data set satis-
fying this property closed downwards. If a data set is
not closed downwards, data may be imputed, e.g. using
the model’s posterior predictive mean, to ensure closed
downwardness; inference and learning then, however,
become approximate. The key conditional indepen-
dence expressed by Figure 2b that results in simple
inference and learning is the following:

Theorem 1. Let a set of observations
downwards. Then yi
the observations for outputs 1, . . . , i and
i + 1, . . . , M .

⊥ D

i+1:M

| D

D

1:i, where

D

be closed
1:i are
D
i+1:M for

Theorem 1 is proved in Appendix A from the sup-
plementary material. Note that Theorem 1 holds for
every graphical model of the form of Figure 2b, mean-
ing that the decomposition into single-output modelling
problems holds for any choice of the conditionals in
Equation (1), not just Gaussians.

Potential deﬁciencies of GPAR. GPAR has two
apparent limitations. First, since the outputs from
earlier dimensions are used as inputs to later dimen-
sions, noisy outputs yield noisy inputs. One possible
mitigating solution is to employ a denoising input
transformation for the kernels, e.g. using the poste-
rior predictive mean of the foregoing outputs as the
input of the next covariance function. We shall refer
to GPAR models employing this approach as D-GPAR.
Second, one needs to choose an ordering of the outputs.
Fortunately, often the data admits a natural ordering;
for example, if the predictions concern a single output
dimension, this should be placed last. Alternatively, if
there is not a natural ordering, one can greedily opti-
mise the evidence with respect to the ordering. This
procedure considers 1
2 M (M + 1) conﬁgurations while
an exhaustive search would consider all M ! possible
conﬁgurations: the best ﬁrst output is selected out
of all M choices, which is then ﬁxed; then, the best
second output is selected out of the remaining M
1
choices, which is then ﬁxed; et cetera. These methods
are examined in Section 6.

−

3 GPAR and Multi-Output GPs

The choice of kernels k1:M for f1:M is crucial to GPAR,
as they determine the types of relationships between
inputs and outputs that can be learned. Particular
choices for k1:M turn out to yield models closely related
to existing work. These connections are made rigorous
by the nonlinear and linear equivalent model discussed
in Appendix B from the supplementary material. We

The Gaussian Process Autoregressive Regression Model (GPAR)

summarise the results here; see also Table 1.

If km depends linearly on the foregoing outputs y1:m−1
at particular x, then a joint Gaussian distribution over
the outputs is induced in the form of a multi-output
GP model (Goovaerts, 1997; Stein, 1999; Wackernagel,
2003; Teh and Seeger, 2005; Bonilla et al., 2008; Nguyen
and Bonilla, 2014) where latent processes are mixed
together according to a matrix, called the mixing ma-
trix, that is lower-triangular (Appendix B). One may
let the dependency of km on y1:m−1 vary with x, in
which case the mixing matrix varies with x, meaning
that correlations between outputs vary with x. This
yields an instance of the Gaussian Process Regression
Network (GPRN) (Wilson et al., 2012) where inference
is fast and closed form. One may even let km de-
pend nonlinearly on y1:m−1, which yields a particularly
structured deep Gaussian process (DGP) (Damianou,
2014; Bui et al., 2016a), potentially with skip connec-
tions from the inputs (Appendix B). Note that GPAR
may be interpreted as a conventional DGP where the
hidden layers are directly observed and correspond to
successive outputs; this connection could potentially
be leveraged to bring machinery developed for DGPs
to GPAR, e.g. to deal with arbitrarily missing data.

One can further let km depend on the entirety of the
foregoing outputs y1:m−1, yielding instances of nonlocal
GPAR. An example of a nonlocal linear kernel is

k((y, x), (y(cid:48), x(cid:48))) =

a(x

z, x(cid:48)

z(cid:48))y(z)y(cid:48)(z(cid:48)) dz dz(cid:48).

−

−

(cid:90)

The nonlocal linear kernel again induces a jointly Gaus-
sian distribution over the outputs in the form of a
convolutional multi-output GP model (Álvarez et al.,
2009; Álvarez and Lawrence, 2009; Bruinsma, 2016)
where latent processes are convolved together accord-
ing to a matrix-valued function, called the convolution
matrix, that is lower-triangular (Appendix B). Again,
one may let the dependency of km on the entirety of
y1:m−1 vary with x, in which case the convolution ma-
trix varies with x, or even let km depend nonlinearly
on the entirety of y1:m−1; an example of a nonlocal
nonlinear kernel is

k(y, y(cid:48)) = σ2 exp

(cid:18)

(cid:90)

−

1
2(cid:96)(z)

(cid:19)

(y(z)

y(cid:48)(z))2 dz

.

−

Henceforth, we shall refer to GPAR with linear de-
pendencies between outputs as GPAR-L, GPAR with
nonlinear dependencies between outputs as GPAR-NL,
and a combination of the two as GPAR-L-NL.

4 Further Related Work

The Gaussian Process Network (Friedman and Nach-
man, 2000) is similar to GPAR, but was instead devel-
oped to identify causal dependencies between variables

in a probabilistic graphical models context rather than
multi-output regression. The work by Yuan (2011) also
discusses a model similar to GPAR, but speciﬁes a
diﬀerent generative procedure for the outputs.

The multi-ﬁdelity modelling literature is closely related
to multi-output modelling. Whereas in a multi-output
regression task we predict all outputs, multi-ﬁdelity
modelling is concerned with predicting a particular
high-ﬁdelity function, incorporating information from
observations from various levels of ﬁdelity. The idea
of iteratively conditioning on lower ﬁdelity models in
the construction of higher ﬁdelity ones is a well-used
strategy (Kennedy and O’Hagan, 2000; Le Gratiet and
Garnier, 2014). The model presented by Perdikaris
et al. (2017) is nearly identical to GPAR applied in the
multi-ﬁdelity framework, but applications outside this
setting have not been considered.

Moreover, GPAR follows a long line of work on the
family of fully visible Bayesian networks (Frey et al.,
1996; Bengio and Bengio, 2000) that decompose the
distribution over the observations according to the
product rule (Equation (1)) and model the resulting
one dimensional conditionals. A number of approaches
use neural networks for this purpose (Neal, 1992; Frey
et al., 1996; Larochelle and Murray, 2011; Theis and
Bethge, 2015; van den Oord et al., 2016). In particular,
if the observations are real-valued, a standard archi-
tecture lets the conditionals be Gaussian with means
encoded by neural networks and ﬁxed variances. Under
broad conditions, if these neural networks are replaced
by Bayesian neural networks with independent Gaus-
sian priors over the weights, we recover GPAR as the
width of the hidden layers goes to inﬁnity (Neal, 1996;
Matthews et al., 2018).

5 Synthetic Data Experiments

GPAR is well suited for problems where there is a
strong functional relationship between outputs and for
problems where observation noise is richly structured
and input dependent. In this section we demonstrate
GPAR’s ability to model both types of phenomena.

First, we test the ability to leverage strong functional
relationships between the outputs. Consider three out-
puts y1, y2, and y3, inter-depending nonlinearly:
x4 + (cid:15)1,

−

sin(10π(x + 1))/(2x + 1)
y1(x) =
y2(x) = cos2(y1(x)) + sin(3x) + (cid:15)2,
y3(x) = y2(x)y2

1(x) + 3x + (cid:15)3,

−

i.i.d.
∼ N

where (cid:15)1, (cid:15)2, (cid:15)3
(0, 0.05). By substituting y1 and
y2 into y3, we see that y3 can be expressed directly in
terms of x, but via a complex function. The dependence
of y3 on y1, y2, however, is much simpler. Therefore, as
GPAR can exploit direct dependencies between y1:2 and

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

GPAR Deps. Between Outputs Kernels for f1:M

Local Linear

Nonlocal Linear

+ dep. on x

Nonlinear

+ dep. on x

+ dep. on x

Nonlinear

+ dep. on x

Related Models

Multi-Output GPs [1]

k1(x, x(cid:48)) + kLinear(y(x), y(x(cid:48)))
k1(x, x(cid:48)) + k2(x, x(cid:48))kLinear(y(x), y(x(cid:48))) GPRN [2]
k1(x, x(cid:48)) + k2(y(x), y(x(cid:48)))
k1(x, x(cid:48)) + k2((x, y(x)), (x(cid:48), y(x(cid:48))))
k1(x, x(cid:48)) + kLinear(y, y(cid:48))
k1(x, x(cid:48)) + k2(x, x(cid:48))kLinear(y, y(cid:48))
k1(x, x(cid:48)) + k2(y, y(cid:48))
k1(x, x(cid:48)) + k2((x, y), (x(cid:48), y(cid:48)))

Deep GPs (DGPs) [3]
DGPs with input connections

Convolutional MOGPs [4]

Table 1: Classiﬁcation of kernels k1:M for f1:M , the resulting dependencies between outputs, and related models.
Here kLinear refers to a linear kernel and k1 and k2 to an exponentiated quadratic (EQ) or rational quadratic (RQ)
kernel (Rasmussen and Williams, 2006). [1]: Goovaerts (1997); Stein (1999); Wackernagel (2003); Teh and Seeger
(2005); Bonilla et al. (2008); Osborne et al. (2008); Nguyen and Bonilla (2014). [2]: Wilson et al. (2012). [3]:
Damianou (2014); Bui et al. (2016a). [4]: Álvarez et al. (2009); Álvarez and Lawrence (2009); Bruinsma (2016).

y3, it should be presented with a much simpliﬁed task
as compared to predicting y3 from x directly. Figure 3
shows plots of independent GPs (IGPs) and GPAR ﬁt
to 30 data points from y1, y2 and y3. Indeed observe
that GPAR is able to learn y2’s dependence on y1, and
y3’s dependence on y1 and y2, whereas the independent
GPs struggle with the complicated structure.

Second, we test GPAR’s ability to capture non-
Gaussian and input-dependent noise. Consider the fol-
lowing three schemes in which two outputs are observed
under various noise conditions: y1(x) = f1(x) + (cid:15)1 and

(1): y2(x) = f2(x) + sin2(2πx)(cid:15)1 + cos2(2πx)(cid:15)2,
(2): y2(x) = f2(x) + sin(π(cid:15)1) + (cid:15)2,
(3): y2(x) = f2(x) + sin(πx)(cid:15)1 + (cid:15)2,

i.i.d.
∼ N

(0, 0.1), and f1 and f2 are compli-
where (cid:15)1, (cid:15)2
cated, nonlinear functions.2 All three schemes have
i.i.d. homoscedastic Gaussian noise in y1. The noise
in y2, however, depends on that in y1 and can be
heteroscadastic. The task for GPAR is to learn the
scheme’s noise structure. Figure 4 visualises the noise
correlations induced by the schemes and the noise struc-
tures learned by GPAR. Observe that GPAR is able
to learn the various noise structures.

6 Real-World Data Experiments

In this section we evaluate GPAR’s performance and
compare to other models on four standard data sets
commonly used to evaluate multi-output models. We
also consider a recently-introduced data set in the ﬁeld
of Bayesian optimisation, which is a downstream ap-
plication area that could beneﬁt from GPAR. Table 2
lists the models against which we compare GPAR. We

2The functions given by f1(x) = − sin(10π(x + 1))/(2x +
5 e2x (θ1 cos(θ2πx) + θ3 cos(θ4πx)) +

1) − x4 and f2(x) = 1
√

2x.

Acronym Model

Independent GPs
Cokriging
Intrinstic Coregionalisation Model [1]
Semi-Parametric Latent Factor Model [2]
Collaborative Multi-Output GPs [3]

IGP
CK
ICM
SLFM
CGP
CMOGP Convolved Multi-Output GP Model [4]
GP Regression Network [5]
GPRN

Table 2: Models against which GPAR is compared. [1]:
Goovaerts (1997); Stein (1999); Wackernagel (2003).
[2]: Teh and Seeger (2005). [3]: Nguyen and Bonilla
(2014). [4]: Álvarez and Lawrence (2011); Álvarez et al.
(2010). [5]: Wilson et al. (2012).

always compare against IGP and CK, ICM, SLFM,
and CGP, and compare against CMOGP and GPRN
if results for the considered task are available. Since
CK and ICM are much simpliﬁed versions of SLFM
(Álvarez et al., 2010; Goovaerts, 1997) and CGP is an
approximation to SLFM, we sometimes omit results
for CK, ICM, and CGP. Implementations can be found
at https://github.com/wesselb/gpar (Python) and
https://github.com/willtebbutt/GPAR.jl (Julia).
Experimental details can be found in Appendix D from
the supplementary material.

Electroencephalogram (EEG) data set.3 This
data set consists of 256 voltage measurements from
7 electrodes placed on a subject’s scalp whilst the
subject is shown a certain image; Zhang et al. (1995)
describe the data collection process in detail. In par-
ticular, we use frontal electrodes FZ and F1–F6 from
the ﬁrst trial on control subject 337. The task is to
predict the last 100 samples for electrodes FZ, F1, and
F2, given that the ﬁrst 156 samples of FZ, F1, and F2

3 The EEG data set can be downloaded at https://

archive.ics.uci.edu/ml/datasets/eeg+database.

The Gaussian Process Autoregressive Regression Model (GPAR)

Figure 3: Synthetic data set with complex output dependencies: GPAR vs independent GPs (IGP) predictions.

Model

IGP

MAE
MAE∗

0.5739
0.5753

CK†

0.51

ICM SLFM CMOGP†

0.4601
0.4114

0.4606
0.4145

0.4552

Model GPRN† GPAR-NL D-GPAR-NL

MAE
MAE∗

0.4525
0.4040

0.4324
0.4168

0.4114
0.3996

Table 4: Results for the Jura data set for IGP, cok-
riging (CK) and ICM with two latent dimensions, the
SLFM with two latent dimensions, CMOGP, GPRN,
∗ Results are obtained by ﬁrst log-
and GPAR.
transforming the data, then performing prediction, and
ﬁnally transforming the predictions back to the original
† Results from Wilson (2014).
domain.

perimental protocol by Goovaerts (1997) also followed
by Álvarez and Lawrence (2011): The training data
comprises 259 data points distributed spatially with
three output variables—nickel, zinc, and cadmium—
and 100 additional data points for which only two of
the three outputs—nickel and zinc—are observed. The
task is to predict cadmium at the locations of those
100 additional data. Performance is evaluated with the
mean absolute error (MAE).

Table 4 shows the results. The comparatively poor
performance of independent GPs highlights the impor-
tance of exploiting correlations between the mineral
concentrations. Furthermore, Table 4 shows that D-
GPAR-NL signiﬁcantly outperforms the other models,
achieving a new state-of-the-art.

Exchange rates data set.5 This data set consists
of the daily exchange rate w.r.t. USD of the top ten
international currencies (CAD, EUR, JPY, GBP, CHF,
AUD, HKD, NZD, KRW, and MXN) and three precious
metals (gold, silver, and platinum) in the year 2007.
The task is to predict CAD on days 50–100, JPY on
days 100–150, and AUD on days 150–200, given that
CAD is observed on days 1–49 and 101–251, JPY on
days 1–99 and 151–251, and AUD on days 1–149 and
201–251; and that all other currencies are observed
throughout the whole year. Performance is measured

5 The exchange rates data set can be downloaded at

http://fx.sauder.ubc.ca.

Figure 4: Correlation between the sample residues
(deviation from the mean) for y1 and y2. Left, middle,
and right plots correspond to schemes (1), (2) and (3)
respectively. Samples are coloured according to input
value x; that is, all samples for a particular x have the
same colour. If the colour pattern is preserved, then
GPAR has successfully captured how the noise in y1
correlates to that in y2.

Model

SMSE MLL

TT

IGP
SLFM
GPAR-NL

1.75
1.06
0.26

2.60
4.00
1.63

2 sec
11 min
5 sec

Table 3: Results for the EEG data set for IGP, the
SLFM with four latent dimensions, and GPAR.

and the whole signals of F3–F6 are observed. Perfor-
mance is measured with the standardised mean squared
error (SMSE), mean log loss (MLL) (Rasmussen and
Williams, 2006), and training time (TT). Figure 5
visualises predictions for electrode F2, and Table 3
quantiﬁes the results. We observe that GPAR-NL out-
performs independent in terms of SMSE and MLL;
note that independent GPs completely fail to provide
an informative prediction. Furthermore, independent
GPs were trained in two seconds, and GPAR-NL took
only three more seconds; in comparison, training SLFM
took 11 minutes.

Jura data set.4 This data set comprises metal con-
centration measurements collected from the topsoil in
a 14.5 km2 region of the Swiss Jura. We follow the ex-

4 The

data

can
//sites.google.com/site/goovaertspierre/
pierregoovaertswebsite/download/.

downloaded

be

at

https:

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

Figure 5: Predictions for electrode F2 from the EEG data set

Model

IGP∗

CMOGP∗ CGP∗ GPAR-L-NL

SMSE 0.5996

0.2427

0.2125

0.0302

Table 5: Experimental results for the exchange rates
∗ These
data set for IGP, CMOGP, CGP, and GPAR.
numbers are taken from Nguyen and Bonilla (2014).

with the SMSE.

Figure 6 visualises GPAR’s prediction for data set, and
Table 5 quantiﬁes the result. We greedily optimise the
evidence w.r.t. the ordering of the outputs to determine
an ordering, and we impute missing data to ensure
closed downwardness of the data. Observe that GPAR
signiﬁcantly outperforms all other models.

Tidal height, wind speed, and air temperature
data set.6 This data set was collected at 5 minute in-
tervals by four weather stations: Bramblemet, Camber-
met, Chimet, and Sotonmet, all located in Southamp-
ton, UK. The task is to predict the air temperature
measured by Cambermet and Chimet from all other
signals. Performance is measured with the SMSE. This
experiment serves two purposes. First, it demonstrates
that it is simple to scale GPAR to large data sets using
oﬀ-the-shelf inducing point techniques for single-output
GP regression. Second, it shows that scaling to large
data sets enables GPAR to better learn dependencies
between outputs, which, importantly, can signiﬁcantly
improve predictions in regions where outputs are par-
tially observed. We utilise the variational inducing
point method by Titsias (2009) as discussed in Sec-
tion 2, with 10 inducing points per day. This data set
is not closed downwards, so we use mean imputation
when training. We use D-GPAR-L and set the tempo-
ral kernel to be a simple EQ, meaning that the model
cannot make long-range predictions, but instead must
exploit correlations between outputs.

Nguyen and Bonilla (2014) consider from this data set
5 days in July 2013, and predict short periods of the
air temperature measured by Cambermet and Chimet
using all other signals. We followed their setup and
predicted the same test set, but instead trained on

6 The data can be downloaded at http://www.
bramblemet.co.uk, http://cambermet.co.uk, http://
www.chimet.co.uk, and http://sotonmet.co.uk.

the whole of July. Even though the additional ob-
servations do not temporally correlate with the test
periods at all, they enable GPAR to better learn the
relationships between the outputs, which, unsurpris-
ingly, signiﬁcantly improved the predictions: using the
whole of July, GPAR achieves SMSE 0.056, compared
to their SMSE 0.107.

The test set used by Nguyen and Bonilla (2014) is
rather small, yielding high-variance test results. We
therefore do not pursue further comparisons on their
train–test split, but instead consider a bigger, more
challenging setup: using as training data 10 days (days
[10, 20), roughly 30 k points), 15 days (days [18, 23),
roughly 47 k points), and the whole of July (roughly
98 k points), make predictions of 4 day periods of the
air temperature measured by Cambermet and Chimet.
Figure 7 visualises the test periods and GPAR’s pre-
dictions for it. Despite the additional observations
not correlating with the test periods, we observe clear,
though dimishining, improvements in the predictions
as the training data is increased.

MLP validation error data set. The ﬁnal data
set is the validation error of a multi-layer perceptron
(MLP) on the MNIST data, trained using categorical
cross-entropy, and set as a function of six hyperpa-
rameters: the number of hidden layers, the number
of neurons per hidden layer, the dropout rate, the
learning rate to use with the ADAM optimizer, the L1
weight penalty, and the L2 weight penalty. This exper-
iment was implemented using code made available by
Hernández-Lobato (2016). An improved model for the
objective surface could translate directly into improved
performance in Bayesian optimisation (Snoek et al.,
2012), as this would enable a more informed search of
the hyperparameter space.

To generate a data set, we sample 291 sets of hyperpa-
rameters randomly from a rectilinear grid and train the
MLP for 21 epochs under each set of hyperparameters,
recording the validation performance after 1, 5, 11, 16,
and 21 epochs. We construct a training set of 175 of
these hyperparameter settings and, crucially, discard
roughly 30% of the validation performance results at 5
epochs at random, and again discard roughly 30% of
those results at 11 epochs, and so forth. The resulting
data set has 175 labels after 1 epoch, 124 after 5, 88

The Gaussian Process Autoregressive Regression Model (GPAR)

Figure 6: Visualisation of the exchange rates data set and CGP’s (black) and GPAR’s (green) predictions for it.
GPAR’s predictions are overlayed on the original ﬁgure by Nguyen and Bonilla (2014).

Figure 7: Visualisation of the air temperature data set and GPAR’s prediction for it. Black circles indicate the
locations of the inducing points.

have learned to exploit the extra information available.
In turn, GPAR noticeably outperforms the SLFM.

7 Conclusion and Future Work

This paper introduced GPAR: a ﬂexible, fast, tractable,
and interpretable approach to multi-output GP re-
gression. GPAR can model (1) nonlinear relation-
ships between outputs and (2) complex output noise.
GPAR can scale to large data sets by trivially lever-
aging scaling techniques for one-dimensional GP re-
gression (Titsias, 2009). In eﬀect, GPAR transforms
high-dimensional data modelling problems into set of
single-output modelling problems, which are the bread
and butter of the GP approach. GPAR was rigorously
tested on a variety of synthetic and real-world prob-
lems, consistently outperforming existing GP models
for multi-output regression. An exciting future appli-
cation of GPAR is to use compositional kernel search
(Lloyd et al., 2014) to automatically learn and explain
dependencies between outputs and inputs. Further
insights into structure of the data could be gained by
decomposing GPAR’s posterior over additive kernel
components (Duvenaud, 2014). These two approaches
could be developed into a useful tool for automatic
structure discovery. Two further exciting future ap-
plications of GPAR are modelling of environmental
phenomena and improving data eﬃciency of existing
Bayesian optimisation tools (Snoek et al., 2012).

Figure 8: Results for the machine learning data set
for a GP, the SLFM with two latent dimensions, and
GPAR

after 11, 64 after 15 and 44 after 21, simulating the
partial completion of the majority of runs. Importantly,
a Bayesian Optimisation system typically exploits only
completed training runs to inform the objective surface,
whereas GPAR can also exploit partially complete runs.

The results presented in Figure 8 show the SMSE in
predicting validation performance at each epoch using
GPAR, the SLFM, and independent GPs on the test
set, averaged over 10 seed for the pseudo-random num-
ber generator used to select which outputs from the
training set to discard. GPs trained independently to
predict performance after a particular number of epochs
perform worse than the SLFM and GPAR, which both

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

Acknowledgements

Richard E. Turner is supported by Google as well as
EPSRC grants EP/M0269571 and EP/L000776/1.

References

Álvarez, M. and Lawrence, N. D. (2009). Sparse con-
volved gaussian processes for multi-output regression.
Advances in Neural Information Processing Systems,
21:57–64.

Álvarez, M. A. and Lawrence, N. D. (2011). Computa-
tionally eﬃcient convolved multiple output Gaussian
processes. Journal of Machine Learning Research,
12:1459–1500.

Álvarez, M. A., Luengo, D., and Lawrence, N. D. (2009).
Latent force models. Artiﬁcial Intelligence and Statis-
tics, 5:9–16.

Álvarez, M. A., Luengo, D., Titsias, M. K., and
Lawrence, N. D. (2010). Eﬃcient multioutput Gaus-
sian processes through variational inducing kernels.
Journal of Machine Learning Research: Workshop
and Conference Proceedings, 9:25–32.

Bengio, Y. and Bengio, S. (2000). Modeling high-
dimensional discrete data with multi-layer neural
networks. In Advances in Neural Information Pro-
cessing Systems, pages 400–406.

Bonilla, E. V., Chai, K. M., and Williams, C. K. I.
(2008). Multi-task Gaussian process prediction. Ad-
vances in Neural Information Processing Systems,
20:153–160.

Bruinsma, W. P. (2016). The generalised gaussian
convolution process model. MPhil thesis, Department
of Engineering, University of Cambridge.

Bui, T. D., Hernández-Lobato, D., Li, Y., Hernández-
Lobato, J. M., and Turner, R. E. (2016a). Deep
gaussian processes for regression using approxi-
mate expectation propagation.
arXiv preprint
arXiv:1602.04133.

Bui, T. D. and Turner, R. E. (2014). Tree-structured
Gaussian process approximations. Advances in Neu-
ral Information Processing Systems, 27:2213–2221.

Bui, T. D., Yan, J., and Turner, R. E. (2016b). A
unifying framework for gaussian process pseudo-point
approximations using power expectation propagation.
arXiv preprint arXiv:1605.07066.

Dai, Z., Álvarez, M. A., and Lawrence, N. D. (2017).
Eﬃcient modeling of latent information in supervised
learning using Gaussian processes. arXiv preprint
arXiv:1705.09862.

Damianou, A. (2014). Deep Gaussian Processes and
Variational Propagation of Uncertainty. PhD thesis,
Department of Neuroscience, University of Sheﬃeld.

Duvenaud, D. (2014). Automatic Model Construction
with Gaussian Processes. PhD thesis, Computational
and Biological Learning Laboratory, University of
Cambridge.

Duvenaud, D., Lloyd, J. R., Grosse, R., Tenenbaum,
J. B., and Ghahramani, Z. (2013). Structure discov-
ery in nonparametric regression through composi-
tional kernel search. In International Conference on
Machine Learning.

Frey, B. J., Hinton, G. E., and Dayan, P. (1996). Does
the wake-sleep algorithm produce good density esti-
mators? In Advances in neural information process-
ing systems, pages 661–667.

Friedman, N. and Nachman, I. (2000). Gaussian process
networks. In Uncertainty in Artiﬁcial Intelligence,
pages 211–219. Morgan Kaufmann Publishers Inc.

Goovaerts, P. (1997). Geostatistics for Natural Re-
sources Evaluation. Oxford University Press, 1 edi-
tion.

Hernández-Lobato, J. M. (2016). Neural networks with
optimal accuracy and speed in their predictions.

Kennedy, M. C. and O’Hagan, A. (2000). Predicting
the output from a complex computer code when fast
approximations are available. Biometrika, 87(1):1–
13.

Koller, D. and Friedman, N. (2009). Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.

Larochelle, H. and Murray, I. (2011). The neural au-
In AISTATS,

toregressive distribution estimator.
volume 1, page 2.

Le Gratiet, L. and Garnier, J. (2014). Recursive co-
kriging model for design of computer experiments
with multiple levels of ﬁdelity. International Journal
for Uncertainty Quantiﬁcation, 4(5).

Lloyd, J. R., Duvenaud, D., Grosse, R., Tenenbaum,
J. B., and Ghahramani, Z. (2014). Automatic con-
struction and natural-language description of non-
parametric regression models. In Association for the
Advancement of Artiﬁcial Intelligence (AAAI).

Matthews, A. G. D. G., Hensman, J., Turner, R. E.,
and Ghahramani, Z. (2016). On sparse variational
methods and the kullback-leibler divergence be-
tween stochastic processes. Artiﬁcial Intelligence
and Statistics, 19.

Matthews, A. G. d. G., Rowland, M., Hron, J., Turner,
R. E., and Ghahramani, Z. (2018). Gaussian pro-
cess behaviour in wide deep neural networks. arXiv
preprint arXiv:1804.11271.

Neal, R. M. (1992). Connectionist learning of belief

networks. Artiﬁcial intelligence, 56(1):71–113.

The Gaussian Process Autoregressive Regression Model (GPAR)

Wilson, A. G. (2014). Covariance Kernels for Fast Au-
tomatic Pattern Discovery and Extrapolation With
Gaussian Processes. PhD thesis, University of Cam-
bridge.

Wilson, A. G., Knowles, D. A., and Ghahramani, Z.
(2012). Gaussian process regression networks. Inter-
national Conference on Machine Learning, 29.

Yuan, C. (2011). Conditional multi-output regression.
In International Joint Conference on Neural Net-
works, pages 189–196. IEEE.

Zhang, X., Begleiter, H., Porjesz, B., Wang, W., and
Litke, A. (1995). Event related potentials during
object recognition tasks. Brain Research Bulletin,
38(6):531–538.

Neal, R. M. (1996). Bayesian learning for neural net-
works, volume 118. Springer Science & Business
Media.

Nguyen, T. V. and Bonilla, E. V. (2014). Collaborative
multi-output Gaussian processes. Conference on
Uncertainty in Artiﬁcial Intelligence, 30.

Nocedal, J. and Wright, S. J. (2006). Numerical Opti-

mization. Springer, second edition.

Osborne, M. A., Roberts, S. J., Rogers, A., Ramchurn,
S. D., and Jennings, N. R. (2008). Towards real-time
information processing of sensor network data us-
ing computationally eﬃcient multi-output Gaussian
processes. In Proceedings of the 7th International
Conference on Information Processing in Sensor Net-
works, IPSN ’08, pages 109–120. IEEE Computer
Society.

Perdikaris, P., Raissi, M., Damianou, A., Lawrence,
N. D., and Karniadakis, G. E. (2017). Nonlinear
information fusion algorithms for data-eﬃcient multi-
ﬁdelity modelling. Proceedings of the Royal Society
A: Mathematical, Physical and Engineering Science,
473.

Rasmussen, C. E. and Williams, C. K. I. (2006). Gaus-
sian Processes for Machine Learning. MIT Press.

Snoek, J., Larochelle, H., and Adams, R. P. (2012).
Practical bayesian optimization of machine learn-
ing algorithms. In Advances in Neural Information
Processing Systems, pages 2951–2959.

Stein, M. (1999).

Interpolation of Spatial Data.

Springer-Verlag New York, 1 edition.

Sun, S., Zhang, G., Wang, C., Zeng, W., Li, J., and
Grosse, R. (2018). Diﬀerentiable compositional ker-
nel learning for gaussian processes. International
Conference on Machine Learning, 35.

Teh, Y. W. and Seeger, M. (2005). Semiparametric
International Workshop on

latent factor models.
Artiﬁcial Intelligence and Statistics, 10.

Theis, L. and Bethge, M. (2015). Generative image
modeling using spatial lstms. In Advances in Neural
Information Processing Systems, pages 1927–1935.

Titsias, M. K. (2009). Variational learning of inducing
variables in sparse Gaussian processes. Artiﬁcial
Intelligence and Statistics, 12:567–574.

van den Oord, A., Kalchbrenner, N., Espeholt, L.,
Vinyals, O., Graves, A., et al. (2016). Conditional
In Ad-
image generation with pixelcnn decoders.
vances in Neural Information Processing Systems,
pages 4790–4798.

Wackernagel, H. (2003). Multivariate Geostatistics.

Springer-Verlag Berlin Heidelberg, 3 edition.

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

A Conditional Independence in

Figure 2b

In this section, we prove the key conditional indepen-
dence in Figure 2b that makes GPAR work:

Theorem 2. Let a set of observations
downwards. Then yi
the observations for outputs 1, . . . , i and
i + 1, . . . , M .7

⊥ D

i+1:M

| D

D

1:i, where

D

be closed
1:i are
D
i+1:M for

To begin with, we review some basic notions concerning
graphical models. Let a path be a sequence of nodes
v1, . . . , vn from some directed graph G where, for each
vi, G either contains an edge from vi to vi+1, or an
edge from vi+1 to vi. If G contains an edge from a to b,
then write a
b to mean the two-node path in which
node b follows node a. Similarly, if G contains an edge
from b to a, then write a
b to mean the two-node
←
path in which b follows a. Write a (cid:10) b to mean either
a

b or b

→

a.

→

←

= v1 (cid:10)

Deﬁnition 1 (Active Path (Deﬁnition 3.6 from Koller
(cid:10) vn be a
and Friedman (2009))). Let
path in a graphical model. Let Z be a subset of the
active
variables from the graphical model. Then, call
given Z if (1) for every v-structure vi−1
vi+1
vi
in
, vi or a descendant of vi is in Z; and (2) no other
P
node in

is in Z.

· · ·

←

→

P

P

P

Deﬁnition 2 (d-Separation (Deﬁnition 3.7 from Koller
and Friedman (2009))). Let X, Y , and Z be three sets
of nodes from a graphical model. Then, call X and Y
d-separated given Z if no path between any x
X and
y

Y is active given Z.

∈

∈

Theorem 3 (d-Separation Implies Conditional Inde-
pendence (Theorem 3.3 from Koller and Friedman
(2009))). Let X, Y , and Z be three sets of nodes from
a graphical model. If X and Y are d-separated given
Z, then X

Z.

Y

⊥

|

Deﬁne the layer of a node in Figure 2b to be

layer(fi) = layer(yi(x)) = i.

We are now ready to prove Theorem 2.

Proof of Theorem 2. For i < j, let
tween any yi(x(cid:48))
∈
be the ﬁrst node in

be a path be-
P
i+1:N . Let yk(ˆx)
such that layer(yk(ˆx)) > i. Then

yi and yj(x)

∈ D

contains

P

P

ym(ˆx)

yk(ˆx) (cid:10)

· · · →

→

· · ·

7 D1:i = {y(n)
j ∈ D : j > i, n ≤ N }.

j

{y(n)

∈ D : j ≤ i, n ≤ N } and Di+1:M =

for some m

i < k.

If yk(ˆx)
ym(ˆx)

∈ D

≤
i+1:N , then, since

1:i, meaning that

∈ D

P
If, on the other hand, yk(ˆx) /
downwards, yk(cid:48)(ˆx) /
≥
cannot be descendant of yk(ˆx), so

∈ D
for all k(cid:48)

∈ D

is closed downwards,

D
is inactive.

, then, since

is closed
D
k. Therefore, yj(x)
must contain

P

yk(cid:48)(ˆx)

yk(cid:48)(cid:48)(ˆx)

fk(cid:48)(cid:48)

· · · →

→

←

→ · · ·

for some m
We conclude that
descendant of yk(cid:48)(cid:48)(ˆx) can be in

k(cid:48) < k(cid:48)(cid:48), which forms a v-structure.
is inactive, because yk(cid:48)(cid:48) (ˆx) nor a

≤

P

.

D

B The Nonlinear and Linear

Equivalent Model

In this section, we construct equivalent models for
GPAR (Lemmas 1 and 2). These models make GPAR’s
connection to other models in the literature explicit.

To begin with, we must introduce some notation and
M )X
M ,
deﬁnitions. For functions A, B :
→ Y
deﬁne composition
B)(x, y) =
is well-deﬁned and as-
A(x, B(
·
M , denote
sociative.
, u). Again, note that
u :
A
◦
B)
(A
◦

u = A(
u). Furthermore, denote

For a function u :

X → Y
u = A
◦

, y)). Note that

M , A
(B

as follows:

X → Y

(
(A

Y
◦

X ×

◦
◦

◦

◦

◦

·

A
(cid:124)

◦ · · · ◦
(cid:123)(cid:122)
n times

A
(cid:125)

= An.

(

(

Y

Y

X ×

X ×

→ Y

→ Y

M )X

M )X

M such that
depends only on (x, y1:i−1),
M , denote
f , and denote N consecutive applications

Consider a function A :
Ai(x, y) :
where A1 = 0. Further let u, y :
T f = u + A
◦
of T by TN .
The expression TM −1 u will be key in constructing
the equivalent models. We show that it is the unique
solution of a functional equation:

X → Y

Proposition 1. The unique solution of y = u + A
is y = TM −1 u.

y

◦

Proof of Proposition 1. First, we show that y = u +
y has a solution, and that this solution is unique.
A
Because Ai(x, y) depends only on (x, y1:i−1), it holds
that

◦

yi = ui + Ai

y = ui + Ai

(y1:i−1, 0),

◦

◦

where (y1:i−1, 0) represents the concatenation of y1:i−1
and M
i + 1 zeros. Thus, yi can uniquely be con-
structed from ui, Ai, and y1:i−1; therefore, y1 exists
and is unique, so y2 exists and is unique: by induction
we ﬁnd that y exists and is unique.

−

The Gaussian Process Autoregressive Regression Model (GPAR)

Second, we show that y = TM −1 u satisﬁes y = u +
y = T y. To show this, we show that (Tn u)i =
A
(Tn−1 u)i for i = 1, . . . , n, for all n. To begin with, we
show the base case, n = 1:

◦

(T u)1 = u1 + A1

u = u1 = (T0 u)1,

◦
since A1 = 0. Finally, suppose that the claim holds for
a particular n. We show that the claim then holds for
n + 1: Let i

n + 1. Then

≤

(Tn+1 u)i = ui + Ai
= ui + Ai

◦

◦

◦

◦

◦

= ui + Ai

(∗)
= ui + Ai

= ui + Ai
= (Tn u)i,

Tn u
((Tn u)1:i−1, (Tn u)i:M )
((Tn−1 u)1:i−1, (Tn u)i:M )

(By assumption)

((Tn−1 u)1:i−1, (Tn−1 u)i:M )
Tn−1 u

where (
) holds because Ai(x, y) depends only on
∗
(x, y1:i−1).

In the linear case, TM −1 u turns out to greatly simplify.

Proposition 2.
TM −1 u = ((cid:80)M −1

i=0 Ai)

u.

◦

If A(x, y) is linear in y,

then

Proof of Proposition 2. If A(x, y) is linear in y, then
distributes over addition. Therefore,
one veriﬁes that

◦
TM −1 u = u + A

= u + A
...
= u + A

◦

◦

◦

TM −2 u
u + A2

◦

TM −3 u

u +

· · ·

+ AM −1

u.

◦

We now use Propositions 1 and 2 to construct a non-
linear and linear equivalent model.

(

GP

Lemma 1 (Nonlinear Equivalent Model). Let A be an
M )X ,
M -dimensional vector-valued process over
(0, kAi) independently, and let
each Ai drawn from
u be an M -dimensional vector-valued process over
,
X
(0, kui) independently. Further-
each ui drawn from
depend only on
more, let Ai(x, y) :
(
(x, y1:i−1), meaning that kAi = kAi(x, y1:i−1, x(cid:48), y(cid:48)
1:i−1),
and let A1 = 0. Denote T f = u + A
f , and denote
N consecutive applications of T by TN . Then

GP
X ×

M )X

→ Y

X ×

Y

Y

◦

yi

y1:i−1

|

∼ GP

y

|

A, u = TM −1 u
(0, kui + kAi(

⇐⇒
, y1:i−1,

·

, y1:i−1)).

Proof of Lemma 1. Since Ai(x, y) depends only on
(x, y1:i−1), it holds by Proposition 1 that any sam-
y, so yi =
A, u satisﬁes yi = ui + Ai
ple from y

|

·

◦

ui + Ai
concatenation of y1:i−1 and M
equivalence now follows.

(y1:i−1, 0), where (y1:i−1, 0) represents the
i + 1 zeros. The

−

◦

Lemma 2 (Linear Equivalent Model). Suppose that
A was instead generated from

(cid:90)

A(x, y)

ˆA =

ˆA(x

z)y(z) dz,

|

−

where ˆA is an (M

, each ˆAi,j drawn from

(0, k ˆAi,j
X
i > j and ˆAi,j = 0 otherwise. Then

GP

×

M )-matrix-valued process over
) independently if

y

A, u =

|

(cid:32)M −1
(cid:88)

(cid:33)

Ai

i=0

u

◦

⇐⇒

yi

y1:i−1

(0, kui + kAi(

, y1:i−1,

, y1:i−1)), (5)

∼ GP

·

·

|
where

1:i−1)

kAi(x, y1:i−1, x(cid:48), y(cid:48)
(cid:90)

i−1
(cid:88)

k ˆAi,j

(x

=

j=1

z, x(cid:48)

z(cid:48))yj(z)y(cid:48)

j(z(cid:48)) dz dz(cid:48).

−

−

Proof of Lemma 2. First, one veriﬁes that Ai(x, y) still
depends only on (x, y1:i−1), and that Ai(x, y) is linear
in y. The result then follows from Lemma 1 and Propo-
sition 2, where the expression for kAi follows from
straightfoward calculation.

As mentioned in the paper, the kernels for f1:M de-
termine the types of relationships between inputs and
outputs that can be learned. Lemmas 1 and 2 make
this explicit: Lemma 1 shows that nonlocal GPAR can
recover a model where M latent GPs u are repeatedly
composed with another latent GP A, where A has a
particular dependency structure, and Lemma 2 shows
that nonlocal GPAR can recover a model where M
latent GPs u are linearly transformed, where the linear
transform T = (cid:80)M −1
i=0 Ai is lower triangular and may
vary with the input.

|

|

|

◦

u, y

T, u = T

In Lemma 2, note that it is not restrictive that T is
lower triangular: Suppose that T were dense. Then,
letting y
T is jointly Gaussian. Hence
y1:i−1, T is a GP whose mean linearly depends upon
yi
y1:i−1 is of the form of
y1:i−1 via T , meaning that yi
Equation (5) where kui may be more complicated. If,
however, ˆA(z) = δ(z)B for some random (M
M )-
matrix B, each Bi,j drawn from
Bi,j ) if i > j and
Bi,j = 0 otherwise, then it is restrictive that T is lower
B, u = (cid:80)M −1
i=0 Biu(x). If
triangular: In this case, y(x)
T = (cid:80)M −1
i=0 Bi were dense, then, letting y
T, u = T u,
y can be represented with Lemma 2 if and only if y
T ’s
covariance can be diagonalised by a constant, invertible,

(0, σ2

N

×

|

|

|

|

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

lower-triangular matrix. This condition does not hold
in general, as Lemma 3 proves.

C Lemma 3

Call functions k1, . . . , kM :
if

X →

R linearly independent

(cid:32)

x :

∀

M
(cid:88)

i=1

(cid:33)

=

⇒

ciki(x) = 0

c1 = . . . = cM = 0.

X →

R be linearly in-
Lemma 3. Let k1, . . . , kM :
dependent and arrange them in a diagonal matrix
K = diag(k1, . . . , kn). Let A be an invertible M
M
matrix such that its columns cannot be permuted into a
triangular matrix. Then there does not exist an invert-
ible triangular matrix T such that T −1BK(x)BTT −T
is diagonal for all x.

×

Proof. Suppose, on the contrary, that such T does exist.
Then two diﬀerent rows ap and aq of A = T −1B share
nonzero elements in some columns C; otherwise, A
would have exactly one nonzero entry in every column—
A is invertible—so A would be the product of a per-
mutation matrix and a diagonal matrix, meaning that
B = T A’s columns could be permuted into a triangu-
lar matrix. Now, by T −1BK(x)BTT −T = AK(x)AT
being diagonal for all x
i ap,iaq,iki(x) = 0 for
∈ X
all x. Therefore, by linear independence of k1, . . . , kN ,
it holds that ap,iaq,i = 0 for all i. But ap,iaq,i
= 0 for
C, which is a contradiction.
any i

, (cid:80)

∈

D Experimental Details

∗

For every experiment, the form of the kernels is deter-
mined by the particular GPAR model used: GPAR-L,
GPAR-NL, or GPAR-L-NL (see Table 2 in the main
paper), potentially with a D-
preﬁx to indicate the de-
noising procedure outlined in “Potential deﬁciencies of
GPAR” in Section 2 of the paper main. For GPAR-NL,
we always used exponentiated quadratic (EQ) kernels,
except for the exchange rates experiment, where we
used rational quadratic (RQ) kernels (Rasmussen and
Williams, 2006). Furthermore, in every problem we
simply expanded according to Equation (1) in the main
paper or greedily optimised the ordering, in both cases
putting the to-be-predicted outputs last. We used
scipy’s implementation of the L-BFGS-B algorithm
(Nocedal and Wright, 2006) to optimise hyperparame-
ters.

9
1
0
2
 
b
e
F
 
5
2
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
2
8
1
7
0
.
2
0
8
1
:
v
i
X
r
a

The Gaussian Process Autoregressive Regression Model (GPAR)

James Requeima12

Will Tebbutt12

Wessel Bruinsma12

Richard E. Turner1

1University of Cambridge and 2Invenia Labs, Cambridge, UK
{jrr41, wct23, wpb23, ret26}@cam.ac.uk

Abstract

Multi-output regression models must exploit
dependencies between outputs to maximise
predictive performance. The application of
Gaussian processes (GPs) to this setting typi-
cally yields models that are computationally
demanding and have limited representational
power. We present the Gaussian Process Au-
toregressive Regression (GPAR) model, a scal-
able multi-output GP model that is able to
capture nonlinear, possibly input-varying, de-
pendencies between outputs in a simple and
tractable way: the product rule is used to
decompose the joint distribution over the out-
puts into a set of conditionals, each of which
is modelled by a standard GP. GPAR’s eﬃ-
cacy is demonstrated on a variety of synthetic
and real-world problems, outperforming exist-
ing GP models and achieving state-of-the-art
performance on established benchmarks.

1

Introduction

The Gaussian process (GP) probabilistic modelling
framework provides a powerful and popular approach
to nonlinear single-output regression (Rasmussen and
Williams, 2006). The popularity of GP methods stems
from their modularity, tractability, and interpretability:
it is simple to construct rich, nonlinear models by com-
positional covariance function design, which can then
be evaluated in a principled way (e.g. via the marginal
likelihood), before being interpreted in terms of their
component parts. This leads to an attractive plug-
and-play approach to modelling and understanding
data, which is so robust that it can even be automated
(Duvenaud et al., 2013; Sun et al., 2018).

Most regression problems, however, involve multiple
outputs rather than a single one. When modelling such
data, it is key to capture the dependencies between
these outputs. For example, noise in the output space

Proceedings of the 22nd International Conference on Ar-
tiﬁcial Intelligence and Statistics (AISTATS) 2019, Naha,
Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by
the author(s).

might be correlated, or, whilst one output might de-
pend on the inputs in a complex (deterministic) way,
it may depend quite simply on other output variables.
In both cases multi-output GP models are required.
There is a plethora of existing multi-output GP models
that can capture linear correlations between output
variables if these correlations are ﬁxed across the in-
put space (Goovaerts, 1997; Wackernagel, 2003; Teh
and Seeger, 2005; Bonilla et al., 2008; Nguyen and
Bonilla, 2014; Dai et al., 2017). However, one of the
main reasons for the popularity of the GP approach is
that a suite of diﬀerent types of nonlinear input depen-
dencies can be modelled, and it is disappointing that
this ﬂexibility is not extended to interactions between
the outputs. There are some approaches that do allow
limited modelling of nonlinear output dependencies
(Wilson et al., 2012; Bruinsma, 2016) but this ﬂexibil-
ity comes from sacriﬁcing tractability, with complex
and computationally demanding approximate inference
and learning schemes now required. This complexity
signiﬁcantly slows down model ﬁtting, evaluation, and
improvement work ﬂow.

What is needed is a ﬂexible and analytically tractable
modelling approach to multi-output regression that
supports plug-and-play modelling and model interpre-
tation. The Gaussian Process Autoregressive Regres-
sion (GPAR) model, introduced in Section 2, achieves
these aims by taking an approach analogous to that em-
ployed by the Neural Autoregressive Density Estimator
(Larochelle and Murray, 2011) for density modelling.
The product rule is used to decompose the distribu-
tion of the outputs given the inputs into a set of one-
dimensional conditional distributions. Critically, these
distributions can be interpreted as a decoupled set of
single-output regression problems, and learning and in-
ference in GPAR therefore amount to a set of standard
single-output GP regression tasks: training is closed
form, fast, and amenable to standard scaling techniques.
GPAR converts the modelling of output dependencies
that are possibly nonlinear and input-dependent into a
set of standard GP covariance function design problems,
constructing expressive, jointly non-Gaussian models
over the outputs. Importantly, we show how GPAR can
capture nonlinear relationships between outputs as well
as structured, input-dependent noise, simply through

The Gaussian Process Autoregressive Regression Model (GPAR)

CO2 C(t)

to some unknown, random function f3; et cetera:

f1( , t)
=
Temperature T (t)

y1(x) = f1(x),
y2(x) = f2(y1(x), x),

f1
f2

p(f1),
p(f2),

Sea Ice I(t)

= f2( ,

, t)

Figure 1: Cartoon motivating a factorisation for the
joint distribution p(I(t), T (t), C(t))

kernel hyperparameter learning. We apply GPAR to
a wide variety of multi-output regression problems,
achieving state-of-the-art results on ﬁve benchmark
tasks.

2 GPAR

Consider the problem of modelling the world’s average
CO2 level C(t), temperature T (t), and Arctic sea ice
extent I(t) as a function of time t. By the greenhouse
eﬀect, one can imagine that the temperature T (t) is a
complicated, stochastic function f1 of CO2 and time:
T (t) = f1(C(t), t). Similarly, one might hypothesise
that the Arctic sea ice extent I(t) can be modelled as
another complicated function f2 of temperature, CO2,
and time: I(t) = f2(T (t), C(t), t). These functional
relationships are depicted in Figure 1 and motivate
the following model where the conditionals model the
postulated underlying functions f1 and f2:

p(I(t), T (t), C(t))

= p(C(t)) p(T (t)

C(t))
(cid:125)

|
(cid:123)(cid:122)
models f1

p(I(t)
(cid:124)

T (t), C(t))
(cid:123)(cid:122)
(cid:125)
models f2

|

.

(cid:124)

More generally, consider the problem of modelling M
outputs y1:M (x) = (y1(x), . . . , yM (x)) as a function of
the input x. Applying the product rule yields1

p(y1:M (x))
y1(x))
= p(y1(x)) p(y2(x)
|
(cid:124)
(cid:125)
(cid:123)(cid:122)
y2(x) as a random
function of y1(x)

· · ·

p(yM (x)
(cid:124)

y1:M −1(x)),
(cid:125)
(cid:123)(cid:122)
yM (x) as a random
function of y1:M −1(x)

|

(1)

which states that y1(x), like CO2, is ﬁrst generated
from x, according to some unknown, random function
f1; that y2(x), like temperature, is then generated
from y1(x) and x, according to some unknown, random
function f2; that y3(x), like the Arctic sea ice extent,
is then generated from y2(x), y1(x), and x, according

1This requires an ordering of the outputs; we will address

this point in Section 2.

∼

∼

∼

...

yM (x) = fM (y1:M −1(x), x)

fM

p(fM ).

GPAR, introduced now, models these unknown func-
tions f1:M with Gaussian processes (GPs). Recall that
a GP f over an index set
deﬁnes a stochastic process,
or process in short, where f (x1), . . . , f (xN ) are jointly
Gaussian distributed for any x1, . . . , xN (Rasmussen
and Williams, 2006). Marginalising out f1:M , we ﬁnd
that GPAR models the conditionals in Equation (1)
with Gaussian processes:

X

ym

y1:m−1

|

∼ GP

(0, km((y1:m−1(x), x), (y1:m−1(x(cid:48)), x(cid:48)))).

(2)

Although the conditionals in Equation (1) are Gaus-
sian, the joint distribution p(y1:N ) is not; moments of
the joint distribution over the outputs are generally
intractable, but samples can be generated by sequen-
tially sampling the conditionals. Figure 2b depicts the
graphical model corresponding to GPAR. Crucially, the
kernels (km) may specify nonlinear, input-dependent
relationships between outputs, which enables GPAR to
model data where outputs inter-depend in complicated
ways.

Returning to the climate modelling example, one might
object that the temperature T (t) at time t does not
just depend the CO2 level C(t) at time t, but in-
stead depends on the entire history of CO2 levels C:
T (t) = f1(C, t). Note: by writing C instead of C(t),
we refer to the entire function C, as opposed to just its
value at t. Similarly, one might object that the Arctic
sea ice extent I(t) at time t does not just depend on
the temperature T (t) and CO2 level C(t) at time t,
but instead depends on the entire history of temper-
atures T and CO2 levels C: T (t) = f2(T, C, t). This
kind of dependency structure, where output ym(x) now
depends on the entirety of all foregoing outputs y1:m
instead of just their value at x, motivates the following
generalisation of GPAR in its form of Equation (2):

ym

y1:m−1

|

∼ GP

(0, km((y1:m−1, x), (y1:m−1, x(cid:48)))),

(3)

which we refer to as nonlocal GPAR, or non-
instantaneous in the temporal setting. Clearly, GPAR
is a special case of nonlocal GPAR. Figure 2a depicts
the graphical model corresponding to nonlocal GPAR.
Nonlocal GPAR will not be evaluated experimentally,
but some theoretical properties will be described.

Inference and learning in GPAR. Inference and
learning in GPAR is simple. Let y(n)
m = ym(x(n)) de-
note an observation for output m. Then, assuming all

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

(a)

f2

y2

f1

y1

f3

y3

(b)

f2

f1

f3

y1(x)

y2(x)

y3(x)

x

(4)

Figure 2: Graphical models corresponding to (a) non-
local GPAR and (b) GPAR

outputs are observed at each input, we ﬁnd

(y(n)

1:M , x(n))N

n=1)

|

p(f1:M
M
(cid:89)

=

m=1

p(fm

|

(y(n)
m )N
n=1
(cid:123)(cid:122)
(cid:125)
(cid:124)
observations
for fm

, (y(n)
(cid:124)

1:m−1, x(n))N
(cid:123)(cid:122)
input locations
of observations

n=1
(cid:125)

),

O

n=1 |

n=1 |

m )N

(y(n)

1:M )N

(x(n))N

(M 3N 3).

meaning that the posterior of fm is computed sim-
ply by conditioning fm on the observations for output
m located at the observations for the foregoing out-
puts and the input, which again is a Gaussian process;
thus, like the prior, the posterior predictive density
also decomposes as a product of Gaussian condition-
als. The evidence log p((y(n)
n=1) decom-
poses similarly; as a consequence, the hyperparam-
eters for fm can be learned simply by maximising
log p((y(n)
n=1). In conclusion, in-
ference and learning in GPAR with M outputs comes
down to inference and learning in M decoupled, one-
dimensional GP regression problems. This shows that
(M N 3), de-
without approximations GPAR scales
pending only linearly on the number of outputs M
rather than cubically, as is the case for general multi-
output GPs, which scale

1:m−1, x(n))N

O
Scaling GPAR. In these one-dimensional GP regres-
sion problems, oﬀ-the-shelf GP scaling techniques may
be applied to trivially scale GPAR to large data sets.
Later we utilise the variational inducing point method
by Titsias (2009), which is theoretically principled
(Matthews et al., 2016) and empirically accurate (Bui
et al., 2016b). This method requires a set of inducing
inputs to be speciﬁed for each Gaussian process. For
the ﬁrst output y1 there is a single input x, which is
time. We use ﬁxed (non-optimised) regularly spaced
inducing inputs, as they are known to perform well for
time series (Bui and Turner, 2014). The second and
following outputs ym, however, require inducing points
to be placed in x and y1, . . . , ym−1. Regular spacing
can be used again for x, but there are choices available
for y1, . . . , ym−1. One approach would be to optimise
these inducing input locations, but instead we use the
posterior predictive means of y1, . . . , ym−1 at x. This
choice accelerates training and was found to yield good
results.

Missing data. When there are missing outputs, the
procedures for inference and learning described above
remain valid as long as for every observation y(n)
m there
are also observations y(n)
m−1:1. We call a data set satis-
fying this property closed downwards. If a data set is
not closed downwards, data may be imputed, e.g. using
the model’s posterior predictive mean, to ensure closed
downwardness; inference and learning then, however,
become approximate. The key conditional indepen-
dence expressed by Figure 2b that results in simple
inference and learning is the following:

Theorem 1. Let a set of observations
downwards. Then yi
the observations for outputs 1, . . . , i and
i + 1, . . . , M .

⊥ D

i+1:M

| D

D

1:i, where

D

be closed
1:i are
D
i+1:M for

Theorem 1 is proved in Appendix A from the sup-
plementary material. Note that Theorem 1 holds for
every graphical model of the form of Figure 2b, mean-
ing that the decomposition into single-output modelling
problems holds for any choice of the conditionals in
Equation (1), not just Gaussians.

Potential deﬁciencies of GPAR. GPAR has two
apparent limitations. First, since the outputs from
earlier dimensions are used as inputs to later dimen-
sions, noisy outputs yield noisy inputs. One possible
mitigating solution is to employ a denoising input
transformation for the kernels, e.g. using the poste-
rior predictive mean of the foregoing outputs as the
input of the next covariance function. We shall refer
to GPAR models employing this approach as D-GPAR.
Second, one needs to choose an ordering of the outputs.
Fortunately, often the data admits a natural ordering;
for example, if the predictions concern a single output
dimension, this should be placed last. Alternatively, if
there is not a natural ordering, one can greedily opti-
mise the evidence with respect to the ordering. This
procedure considers 1
2 M (M + 1) conﬁgurations while
an exhaustive search would consider all M ! possible
conﬁgurations: the best ﬁrst output is selected out
of all M choices, which is then ﬁxed; then, the best
second output is selected out of the remaining M
1
choices, which is then ﬁxed; et cetera. These methods
are examined in Section 6.

−

3 GPAR and Multi-Output GPs

The choice of kernels k1:M for f1:M is crucial to GPAR,
as they determine the types of relationships between
inputs and outputs that can be learned. Particular
choices for k1:M turn out to yield models closely related
to existing work. These connections are made rigorous
by the nonlinear and linear equivalent model discussed
in Appendix B from the supplementary material. We

The Gaussian Process Autoregressive Regression Model (GPAR)

summarise the results here; see also Table 1.

If km depends linearly on the foregoing outputs y1:m−1
at particular x, then a joint Gaussian distribution over
the outputs is induced in the form of a multi-output
GP model (Goovaerts, 1997; Stein, 1999; Wackernagel,
2003; Teh and Seeger, 2005; Bonilla et al., 2008; Nguyen
and Bonilla, 2014) where latent processes are mixed
together according to a matrix, called the mixing ma-
trix, that is lower-triangular (Appendix B). One may
let the dependency of km on y1:m−1 vary with x, in
which case the mixing matrix varies with x, meaning
that correlations between outputs vary with x. This
yields an instance of the Gaussian Process Regression
Network (GPRN) (Wilson et al., 2012) where inference
is fast and closed form. One may even let km de-
pend nonlinearly on y1:m−1, which yields a particularly
structured deep Gaussian process (DGP) (Damianou,
2014; Bui et al., 2016a), potentially with skip connec-
tions from the inputs (Appendix B). Note that GPAR
may be interpreted as a conventional DGP where the
hidden layers are directly observed and correspond to
successive outputs; this connection could potentially
be leveraged to bring machinery developed for DGPs
to GPAR, e.g. to deal with arbitrarily missing data.

One can further let km depend on the entirety of the
foregoing outputs y1:m−1, yielding instances of nonlocal
GPAR. An example of a nonlocal linear kernel is

k((y, x), (y(cid:48), x(cid:48))) =

a(x

z, x(cid:48)

z(cid:48))y(z)y(cid:48)(z(cid:48)) dz dz(cid:48).

−

−

(cid:90)

The nonlocal linear kernel again induces a jointly Gaus-
sian distribution over the outputs in the form of a
convolutional multi-output GP model (Álvarez et al.,
2009; Álvarez and Lawrence, 2009; Bruinsma, 2016)
where latent processes are convolved together accord-
ing to a matrix-valued function, called the convolution
matrix, that is lower-triangular (Appendix B). Again,
one may let the dependency of km on the entirety of
y1:m−1 vary with x, in which case the convolution ma-
trix varies with x, or even let km depend nonlinearly
on the entirety of y1:m−1; an example of a nonlocal
nonlinear kernel is

k(y, y(cid:48)) = σ2 exp

(cid:18)

(cid:90)

−

1
2(cid:96)(z)

(cid:19)

(y(z)

y(cid:48)(z))2 dz

.

−

Henceforth, we shall refer to GPAR with linear de-
pendencies between outputs as GPAR-L, GPAR with
nonlinear dependencies between outputs as GPAR-NL,
and a combination of the two as GPAR-L-NL.

4 Further Related Work

The Gaussian Process Network (Friedman and Nach-
man, 2000) is similar to GPAR, but was instead devel-
oped to identify causal dependencies between variables

in a probabilistic graphical models context rather than
multi-output regression. The work by Yuan (2011) also
discusses a model similar to GPAR, but speciﬁes a
diﬀerent generative procedure for the outputs.

The multi-ﬁdelity modelling literature is closely related
to multi-output modelling. Whereas in a multi-output
regression task we predict all outputs, multi-ﬁdelity
modelling is concerned with predicting a particular
high-ﬁdelity function, incorporating information from
observations from various levels of ﬁdelity. The idea
of iteratively conditioning on lower ﬁdelity models in
the construction of higher ﬁdelity ones is a well-used
strategy (Kennedy and O’Hagan, 2000; Le Gratiet and
Garnier, 2014). The model presented by Perdikaris
et al. (2017) is nearly identical to GPAR applied in the
multi-ﬁdelity framework, but applications outside this
setting have not been considered.

Moreover, GPAR follows a long line of work on the
family of fully visible Bayesian networks (Frey et al.,
1996; Bengio and Bengio, 2000) that decompose the
distribution over the observations according to the
product rule (Equation (1)) and model the resulting
one dimensional conditionals. A number of approaches
use neural networks for this purpose (Neal, 1992; Frey
et al., 1996; Larochelle and Murray, 2011; Theis and
Bethge, 2015; van den Oord et al., 2016). In particular,
if the observations are real-valued, a standard archi-
tecture lets the conditionals be Gaussian with means
encoded by neural networks and ﬁxed variances. Under
broad conditions, if these neural networks are replaced
by Bayesian neural networks with independent Gaus-
sian priors over the weights, we recover GPAR as the
width of the hidden layers goes to inﬁnity (Neal, 1996;
Matthews et al., 2018).

5 Synthetic Data Experiments

GPAR is well suited for problems where there is a
strong functional relationship between outputs and for
problems where observation noise is richly structured
and input dependent. In this section we demonstrate
GPAR’s ability to model both types of phenomena.

First, we test the ability to leverage strong functional
relationships between the outputs. Consider three out-
puts y1, y2, and y3, inter-depending nonlinearly:
x4 + (cid:15)1,

−

sin(10π(x + 1))/(2x + 1)
y1(x) =
y2(x) = cos2(y1(x)) + sin(3x) + (cid:15)2,
y3(x) = y2(x)y2

1(x) + 3x + (cid:15)3,

−

i.i.d.
∼ N

where (cid:15)1, (cid:15)2, (cid:15)3
(0, 0.05). By substituting y1 and
y2 into y3, we see that y3 can be expressed directly in
terms of x, but via a complex function. The dependence
of y3 on y1, y2, however, is much simpler. Therefore, as
GPAR can exploit direct dependencies between y1:2 and

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

GPAR Deps. Between Outputs Kernels for f1:M

Local Linear

Nonlocal Linear

+ dep. on x

Nonlinear

+ dep. on x

+ dep. on x

Nonlinear

+ dep. on x

Related Models

Multi-Output GPs [1]

k1(x, x(cid:48)) + kLinear(y(x), y(x(cid:48)))
k1(x, x(cid:48)) + k2(x, x(cid:48))kLinear(y(x), y(x(cid:48))) GPRN [2]
k1(x, x(cid:48)) + k2(y(x), y(x(cid:48)))
k1(x, x(cid:48)) + k2((x, y(x)), (x(cid:48), y(x(cid:48))))
k1(x, x(cid:48)) + kLinear(y, y(cid:48))
k1(x, x(cid:48)) + k2(x, x(cid:48))kLinear(y, y(cid:48))
k1(x, x(cid:48)) + k2(y, y(cid:48))
k1(x, x(cid:48)) + k2((x, y), (x(cid:48), y(cid:48)))

Deep GPs (DGPs) [3]
DGPs with input connections

Convolutional MOGPs [4]

Table 1: Classiﬁcation of kernels k1:M for f1:M , the resulting dependencies between outputs, and related models.
Here kLinear refers to a linear kernel and k1 and k2 to an exponentiated quadratic (EQ) or rational quadratic (RQ)
kernel (Rasmussen and Williams, 2006). [1]: Goovaerts (1997); Stein (1999); Wackernagel (2003); Teh and Seeger
(2005); Bonilla et al. (2008); Osborne et al. (2008); Nguyen and Bonilla (2014). [2]: Wilson et al. (2012). [3]:
Damianou (2014); Bui et al. (2016a). [4]: Álvarez et al. (2009); Álvarez and Lawrence (2009); Bruinsma (2016).

y3, it should be presented with a much simpliﬁed task
as compared to predicting y3 from x directly. Figure 3
shows plots of independent GPs (IGPs) and GPAR ﬁt
to 30 data points from y1, y2 and y3. Indeed observe
that GPAR is able to learn y2’s dependence on y1, and
y3’s dependence on y1 and y2, whereas the independent
GPs struggle with the complicated structure.

Second, we test GPAR’s ability to capture non-
Gaussian and input-dependent noise. Consider the fol-
lowing three schemes in which two outputs are observed
under various noise conditions: y1(x) = f1(x) + (cid:15)1 and

(1): y2(x) = f2(x) + sin2(2πx)(cid:15)1 + cos2(2πx)(cid:15)2,
(2): y2(x) = f2(x) + sin(π(cid:15)1) + (cid:15)2,
(3): y2(x) = f2(x) + sin(πx)(cid:15)1 + (cid:15)2,

i.i.d.
∼ N

(0, 0.1), and f1 and f2 are compli-
where (cid:15)1, (cid:15)2
cated, nonlinear functions.2 All three schemes have
i.i.d. homoscedastic Gaussian noise in y1. The noise
in y2, however, depends on that in y1 and can be
heteroscadastic. The task for GPAR is to learn the
scheme’s noise structure. Figure 4 visualises the noise
correlations induced by the schemes and the noise struc-
tures learned by GPAR. Observe that GPAR is able
to learn the various noise structures.

6 Real-World Data Experiments

In this section we evaluate GPAR’s performance and
compare to other models on four standard data sets
commonly used to evaluate multi-output models. We
also consider a recently-introduced data set in the ﬁeld
of Bayesian optimisation, which is a downstream ap-
plication area that could beneﬁt from GPAR. Table 2
lists the models against which we compare GPAR. We

2The functions given by f1(x) = − sin(10π(x + 1))/(2x +
5 e2x (θ1 cos(θ2πx) + θ3 cos(θ4πx)) +

1) − x4 and f2(x) = 1
√

2x.

Acronym Model

Independent GPs
Cokriging
Intrinstic Coregionalisation Model [1]
Semi-Parametric Latent Factor Model [2]
Collaborative Multi-Output GPs [3]

IGP
CK
ICM
SLFM
CGP
CMOGP Convolved Multi-Output GP Model [4]
GP Regression Network [5]
GPRN

Table 2: Models against which GPAR is compared. [1]:
Goovaerts (1997); Stein (1999); Wackernagel (2003).
[2]: Teh and Seeger (2005). [3]: Nguyen and Bonilla
(2014). [4]: Álvarez and Lawrence (2011); Álvarez et al.
(2010). [5]: Wilson et al. (2012).

always compare against IGP and CK, ICM, SLFM,
and CGP, and compare against CMOGP and GPRN
if results for the considered task are available. Since
CK and ICM are much simpliﬁed versions of SLFM
(Álvarez et al., 2010; Goovaerts, 1997) and CGP is an
approximation to SLFM, we sometimes omit results
for CK, ICM, and CGP. Implementations can be found
at https://github.com/wesselb/gpar (Python) and
https://github.com/willtebbutt/GPAR.jl (Julia).
Experimental details can be found in Appendix D from
the supplementary material.

Electroencephalogram (EEG) data set.3 This
data set consists of 256 voltage measurements from
7 electrodes placed on a subject’s scalp whilst the
subject is shown a certain image; Zhang et al. (1995)
describe the data collection process in detail. In par-
ticular, we use frontal electrodes FZ and F1–F6 from
the ﬁrst trial on control subject 337. The task is to
predict the last 100 samples for electrodes FZ, F1, and
F2, given that the ﬁrst 156 samples of FZ, F1, and F2

3 The EEG data set can be downloaded at https://

archive.ics.uci.edu/ml/datasets/eeg+database.

The Gaussian Process Autoregressive Regression Model (GPAR)

Figure 3: Synthetic data set with complex output dependencies: GPAR vs independent GPs (IGP) predictions.

Model

IGP

MAE
MAE∗

0.5739
0.5753

CK†

0.51

ICM SLFM CMOGP†

0.4601
0.4114

0.4606
0.4145

0.4552

Model GPRN† GPAR-NL D-GPAR-NL

MAE
MAE∗

0.4525
0.4040

0.4324
0.4168

0.4114
0.3996

Table 4: Results for the Jura data set for IGP, cok-
riging (CK) and ICM with two latent dimensions, the
SLFM with two latent dimensions, CMOGP, GPRN,
∗ Results are obtained by ﬁrst log-
and GPAR.
transforming the data, then performing prediction, and
ﬁnally transforming the predictions back to the original
† Results from Wilson (2014).
domain.

perimental protocol by Goovaerts (1997) also followed
by Álvarez and Lawrence (2011): The training data
comprises 259 data points distributed spatially with
three output variables—nickel, zinc, and cadmium—
and 100 additional data points for which only two of
the three outputs—nickel and zinc—are observed. The
task is to predict cadmium at the locations of those
100 additional data. Performance is evaluated with the
mean absolute error (MAE).

Table 4 shows the results. The comparatively poor
performance of independent GPs highlights the impor-
tance of exploiting correlations between the mineral
concentrations. Furthermore, Table 4 shows that D-
GPAR-NL signiﬁcantly outperforms the other models,
achieving a new state-of-the-art.

Exchange rates data set.5 This data set consists
of the daily exchange rate w.r.t. USD of the top ten
international currencies (CAD, EUR, JPY, GBP, CHF,
AUD, HKD, NZD, KRW, and MXN) and three precious
metals (gold, silver, and platinum) in the year 2007.
The task is to predict CAD on days 50–100, JPY on
days 100–150, and AUD on days 150–200, given that
CAD is observed on days 1–49 and 101–251, JPY on
days 1–99 and 151–251, and AUD on days 1–149 and
201–251; and that all other currencies are observed
throughout the whole year. Performance is measured

5 The exchange rates data set can be downloaded at

http://fx.sauder.ubc.ca.

Figure 4: Correlation between the sample residues
(deviation from the mean) for y1 and y2. Left, middle,
and right plots correspond to schemes (1), (2) and (3)
respectively. Samples are coloured according to input
value x; that is, all samples for a particular x have the
same colour. If the colour pattern is preserved, then
GPAR has successfully captured how the noise in y1
correlates to that in y2.

Model

SMSE MLL

TT

IGP
SLFM
GPAR-NL

1.75
1.06
0.26

2.60
4.00
1.63

2 sec
11 min
5 sec

Table 3: Results for the EEG data set for IGP, the
SLFM with four latent dimensions, and GPAR.

and the whole signals of F3–F6 are observed. Perfor-
mance is measured with the standardised mean squared
error (SMSE), mean log loss (MLL) (Rasmussen and
Williams, 2006), and training time (TT). Figure 5
visualises predictions for electrode F2, and Table 3
quantiﬁes the results. We observe that GPAR-NL out-
performs independent in terms of SMSE and MLL;
note that independent GPs completely fail to provide
an informative prediction. Furthermore, independent
GPs were trained in two seconds, and GPAR-NL took
only three more seconds; in comparison, training SLFM
took 11 minutes.

Jura data set.4 This data set comprises metal con-
centration measurements collected from the topsoil in
a 14.5 km2 region of the Swiss Jura. We follow the ex-

4 The

data

can
//sites.google.com/site/goovaertspierre/
pierregoovaertswebsite/download/.

downloaded

be

at

https:

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

Figure 5: Predictions for electrode F2 from the EEG data set

Model

IGP∗

CMOGP∗ CGP∗ GPAR-L-NL

SMSE 0.5996

0.2427

0.2125

0.0302

Table 5: Experimental results for the exchange rates
∗ These
data set for IGP, CMOGP, CGP, and GPAR.
numbers are taken from Nguyen and Bonilla (2014).

with the SMSE.

Figure 6 visualises GPAR’s prediction for data set, and
Table 5 quantiﬁes the result. We greedily optimise the
evidence w.r.t. the ordering of the outputs to determine
an ordering, and we impute missing data to ensure
closed downwardness of the data. Observe that GPAR
signiﬁcantly outperforms all other models.

Tidal height, wind speed, and air temperature
data set.6 This data set was collected at 5 minute in-
tervals by four weather stations: Bramblemet, Camber-
met, Chimet, and Sotonmet, all located in Southamp-
ton, UK. The task is to predict the air temperature
measured by Cambermet and Chimet from all other
signals. Performance is measured with the SMSE. This
experiment serves two purposes. First, it demonstrates
that it is simple to scale GPAR to large data sets using
oﬀ-the-shelf inducing point techniques for single-output
GP regression. Second, it shows that scaling to large
data sets enables GPAR to better learn dependencies
between outputs, which, importantly, can signiﬁcantly
improve predictions in regions where outputs are par-
tially observed. We utilise the variational inducing
point method by Titsias (2009) as discussed in Sec-
tion 2, with 10 inducing points per day. This data set
is not closed downwards, so we use mean imputation
when training. We use D-GPAR-L and set the tempo-
ral kernel to be a simple EQ, meaning that the model
cannot make long-range predictions, but instead must
exploit correlations between outputs.

Nguyen and Bonilla (2014) consider from this data set
5 days in July 2013, and predict short periods of the
air temperature measured by Cambermet and Chimet
using all other signals. We followed their setup and
predicted the same test set, but instead trained on

6 The data can be downloaded at http://www.
bramblemet.co.uk, http://cambermet.co.uk, http://
www.chimet.co.uk, and http://sotonmet.co.uk.

the whole of July. Even though the additional ob-
servations do not temporally correlate with the test
periods at all, they enable GPAR to better learn the
relationships between the outputs, which, unsurpris-
ingly, signiﬁcantly improved the predictions: using the
whole of July, GPAR achieves SMSE 0.056, compared
to their SMSE 0.107.

The test set used by Nguyen and Bonilla (2014) is
rather small, yielding high-variance test results. We
therefore do not pursue further comparisons on their
train–test split, but instead consider a bigger, more
challenging setup: using as training data 10 days (days
[10, 20), roughly 30 k points), 15 days (days [18, 23),
roughly 47 k points), and the whole of July (roughly
98 k points), make predictions of 4 day periods of the
air temperature measured by Cambermet and Chimet.
Figure 7 visualises the test periods and GPAR’s pre-
dictions for it. Despite the additional observations
not correlating with the test periods, we observe clear,
though dimishining, improvements in the predictions
as the training data is increased.

MLP validation error data set. The ﬁnal data
set is the validation error of a multi-layer perceptron
(MLP) on the MNIST data, trained using categorical
cross-entropy, and set as a function of six hyperpa-
rameters: the number of hidden layers, the number
of neurons per hidden layer, the dropout rate, the
learning rate to use with the ADAM optimizer, the L1
weight penalty, and the L2 weight penalty. This exper-
iment was implemented using code made available by
Hernández-Lobato (2016). An improved model for the
objective surface could translate directly into improved
performance in Bayesian optimisation (Snoek et al.,
2012), as this would enable a more informed search of
the hyperparameter space.

To generate a data set, we sample 291 sets of hyperpa-
rameters randomly from a rectilinear grid and train the
MLP for 21 epochs under each set of hyperparameters,
recording the validation performance after 1, 5, 11, 16,
and 21 epochs. We construct a training set of 175 of
these hyperparameter settings and, crucially, discard
roughly 30% of the validation performance results at 5
epochs at random, and again discard roughly 30% of
those results at 11 epochs, and so forth. The resulting
data set has 175 labels after 1 epoch, 124 after 5, 88

The Gaussian Process Autoregressive Regression Model (GPAR)

Figure 6: Visualisation of the exchange rates data set and CGP’s (black) and GPAR’s (green) predictions for it.
GPAR’s predictions are overlayed on the original ﬁgure by Nguyen and Bonilla (2014).

Figure 7: Visualisation of the air temperature data set and GPAR’s prediction for it. Black circles indicate the
locations of the inducing points.

have learned to exploit the extra information available.
In turn, GPAR noticeably outperforms the SLFM.

7 Conclusion and Future Work

This paper introduced GPAR: a ﬂexible, fast, tractable,
and interpretable approach to multi-output GP re-
gression. GPAR can model (1) nonlinear relation-
ships between outputs and (2) complex output noise.
GPAR can scale to large data sets by trivially lever-
aging scaling techniques for one-dimensional GP re-
gression (Titsias, 2009). In eﬀect, GPAR transforms
high-dimensional data modelling problems into set of
single-output modelling problems, which are the bread
and butter of the GP approach. GPAR was rigorously
tested on a variety of synthetic and real-world prob-
lems, consistently outperforming existing GP models
for multi-output regression. An exciting future appli-
cation of GPAR is to use compositional kernel search
(Lloyd et al., 2014) to automatically learn and explain
dependencies between outputs and inputs. Further
insights into structure of the data could be gained by
decomposing GPAR’s posterior over additive kernel
components (Duvenaud, 2014). These two approaches
could be developed into a useful tool for automatic
structure discovery. Two further exciting future ap-
plications of GPAR are modelling of environmental
phenomena and improving data eﬃciency of existing
Bayesian optimisation tools (Snoek et al., 2012).

Figure 8: Results for the machine learning data set
for a GP, the SLFM with two latent dimensions, and
GPAR

after 11, 64 after 15 and 44 after 21, simulating the
partial completion of the majority of runs. Importantly,
a Bayesian Optimisation system typically exploits only
completed training runs to inform the objective surface,
whereas GPAR can also exploit partially complete runs.

The results presented in Figure 8 show the SMSE in
predicting validation performance at each epoch using
GPAR, the SLFM, and independent GPs on the test
set, averaged over 10 seed for the pseudo-random num-
ber generator used to select which outputs from the
training set to discard. GPs trained independently to
predict performance after a particular number of epochs
perform worse than the SLFM and GPAR, which both

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

Acknowledgements

Richard E. Turner is supported by Google as well as
EPSRC grants EP/M0269571 and EP/L000776/1.

References

Álvarez, M. and Lawrence, N. D. (2009). Sparse con-
volved gaussian processes for multi-output regression.
Advances in Neural Information Processing Systems,
21:57–64.

Álvarez, M. A. and Lawrence, N. D. (2011). Computa-
tionally eﬃcient convolved multiple output Gaussian
processes. Journal of Machine Learning Research,
12:1459–1500.

Álvarez, M. A., Luengo, D., and Lawrence, N. D. (2009).
Latent force models. Artiﬁcial Intelligence and Statis-
tics, 5:9–16.

Álvarez, M. A., Luengo, D., Titsias, M. K., and
Lawrence, N. D. (2010). Eﬃcient multioutput Gaus-
sian processes through variational inducing kernels.
Journal of Machine Learning Research: Workshop
and Conference Proceedings, 9:25–32.

Bengio, Y. and Bengio, S. (2000). Modeling high-
dimensional discrete data with multi-layer neural
networks. In Advances in Neural Information Pro-
cessing Systems, pages 400–406.

Bonilla, E. V., Chai, K. M., and Williams, C. K. I.
(2008). Multi-task Gaussian process prediction. Ad-
vances in Neural Information Processing Systems,
20:153–160.

Bruinsma, W. P. (2016). The generalised gaussian
convolution process model. MPhil thesis, Department
of Engineering, University of Cambridge.

Bui, T. D., Hernández-Lobato, D., Li, Y., Hernández-
Lobato, J. M., and Turner, R. E. (2016a). Deep
gaussian processes for regression using approxi-
mate expectation propagation.
arXiv preprint
arXiv:1602.04133.

Bui, T. D. and Turner, R. E. (2014). Tree-structured
Gaussian process approximations. Advances in Neu-
ral Information Processing Systems, 27:2213–2221.

Bui, T. D., Yan, J., and Turner, R. E. (2016b). A
unifying framework for gaussian process pseudo-point
approximations using power expectation propagation.
arXiv preprint arXiv:1605.07066.

Dai, Z., Álvarez, M. A., and Lawrence, N. D. (2017).
Eﬃcient modeling of latent information in supervised
learning using Gaussian processes. arXiv preprint
arXiv:1705.09862.

Damianou, A. (2014). Deep Gaussian Processes and
Variational Propagation of Uncertainty. PhD thesis,
Department of Neuroscience, University of Sheﬃeld.

Duvenaud, D. (2014). Automatic Model Construction
with Gaussian Processes. PhD thesis, Computational
and Biological Learning Laboratory, University of
Cambridge.

Duvenaud, D., Lloyd, J. R., Grosse, R., Tenenbaum,
J. B., and Ghahramani, Z. (2013). Structure discov-
ery in nonparametric regression through composi-
tional kernel search. In International Conference on
Machine Learning.

Frey, B. J., Hinton, G. E., and Dayan, P. (1996). Does
the wake-sleep algorithm produce good density esti-
mators? In Advances in neural information process-
ing systems, pages 661–667.

Friedman, N. and Nachman, I. (2000). Gaussian process
networks. In Uncertainty in Artiﬁcial Intelligence,
pages 211–219. Morgan Kaufmann Publishers Inc.

Goovaerts, P. (1997). Geostatistics for Natural Re-
sources Evaluation. Oxford University Press, 1 edi-
tion.

Hernández-Lobato, J. M. (2016). Neural networks with
optimal accuracy and speed in their predictions.

Kennedy, M. C. and O’Hagan, A. (2000). Predicting
the output from a complex computer code when fast
approximations are available. Biometrika, 87(1):1–
13.

Koller, D. and Friedman, N. (2009). Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.

Larochelle, H. and Murray, I. (2011). The neural au-
In AISTATS,

toregressive distribution estimator.
volume 1, page 2.

Le Gratiet, L. and Garnier, J. (2014). Recursive co-
kriging model for design of computer experiments
with multiple levels of ﬁdelity. International Journal
for Uncertainty Quantiﬁcation, 4(5).

Lloyd, J. R., Duvenaud, D., Grosse, R., Tenenbaum,
J. B., and Ghahramani, Z. (2014). Automatic con-
struction and natural-language description of non-
parametric regression models. In Association for the
Advancement of Artiﬁcial Intelligence (AAAI).

Matthews, A. G. D. G., Hensman, J., Turner, R. E.,
and Ghahramani, Z. (2016). On sparse variational
methods and the kullback-leibler divergence be-
tween stochastic processes. Artiﬁcial Intelligence
and Statistics, 19.

Matthews, A. G. d. G., Rowland, M., Hron, J., Turner,
R. E., and Ghahramani, Z. (2018). Gaussian pro-
cess behaviour in wide deep neural networks. arXiv
preprint arXiv:1804.11271.

Neal, R. M. (1992). Connectionist learning of belief

networks. Artiﬁcial intelligence, 56(1):71–113.

The Gaussian Process Autoregressive Regression Model (GPAR)

Wilson, A. G. (2014). Covariance Kernels for Fast Au-
tomatic Pattern Discovery and Extrapolation With
Gaussian Processes. PhD thesis, University of Cam-
bridge.

Wilson, A. G., Knowles, D. A., and Ghahramani, Z.
(2012). Gaussian process regression networks. Inter-
national Conference on Machine Learning, 29.

Yuan, C. (2011). Conditional multi-output regression.
In International Joint Conference on Neural Net-
works, pages 189–196. IEEE.

Zhang, X., Begleiter, H., Porjesz, B., Wang, W., and
Litke, A. (1995). Event related potentials during
object recognition tasks. Brain Research Bulletin,
38(6):531–538.

Neal, R. M. (1996). Bayesian learning for neural net-
works, volume 118. Springer Science & Business
Media.

Nguyen, T. V. and Bonilla, E. V. (2014). Collaborative
multi-output Gaussian processes. Conference on
Uncertainty in Artiﬁcial Intelligence, 30.

Nocedal, J. and Wright, S. J. (2006). Numerical Opti-

mization. Springer, second edition.

Osborne, M. A., Roberts, S. J., Rogers, A., Ramchurn,
S. D., and Jennings, N. R. (2008). Towards real-time
information processing of sensor network data us-
ing computationally eﬃcient multi-output Gaussian
processes. In Proceedings of the 7th International
Conference on Information Processing in Sensor Net-
works, IPSN ’08, pages 109–120. IEEE Computer
Society.

Perdikaris, P., Raissi, M., Damianou, A., Lawrence,
N. D., and Karniadakis, G. E. (2017). Nonlinear
information fusion algorithms for data-eﬃcient multi-
ﬁdelity modelling. Proceedings of the Royal Society
A: Mathematical, Physical and Engineering Science,
473.

Rasmussen, C. E. and Williams, C. K. I. (2006). Gaus-
sian Processes for Machine Learning. MIT Press.

Snoek, J., Larochelle, H., and Adams, R. P. (2012).
Practical bayesian optimization of machine learn-
ing algorithms. In Advances in Neural Information
Processing Systems, pages 2951–2959.

Stein, M. (1999).

Interpolation of Spatial Data.

Springer-Verlag New York, 1 edition.

Sun, S., Zhang, G., Wang, C., Zeng, W., Li, J., and
Grosse, R. (2018). Diﬀerentiable compositional ker-
nel learning for gaussian processes. International
Conference on Machine Learning, 35.

Teh, Y. W. and Seeger, M. (2005). Semiparametric
International Workshop on

latent factor models.
Artiﬁcial Intelligence and Statistics, 10.

Theis, L. and Bethge, M. (2015). Generative image
modeling using spatial lstms. In Advances in Neural
Information Processing Systems, pages 1927–1935.

Titsias, M. K. (2009). Variational learning of inducing
variables in sparse Gaussian processes. Artiﬁcial
Intelligence and Statistics, 12:567–574.

van den Oord, A., Kalchbrenner, N., Espeholt, L.,
Vinyals, O., Graves, A., et al. (2016). Conditional
In Ad-
image generation with pixelcnn decoders.
vances in Neural Information Processing Systems,
pages 4790–4798.

Wackernagel, H. (2003). Multivariate Geostatistics.

Springer-Verlag Berlin Heidelberg, 3 edition.

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

A Conditional Independence in

Figure 2b

In this section, we prove the key conditional indepen-
dence in Figure 2b that makes GPAR work:

Theorem 2. Let a set of observations
downwards. Then yi
the observations for outputs 1, . . . , i and
i + 1, . . . , M .7

⊥ D

i+1:M

| D

D

1:i, where

D

be closed
1:i are
D
i+1:M for

To begin with, we review some basic notions concerning
graphical models. Let a path be a sequence of nodes
v1, . . . , vn from some directed graph G where, for each
vi, G either contains an edge from vi to vi+1, or an
edge from vi+1 to vi. If G contains an edge from a to b,
then write a
b to mean the two-node path in which
node b follows node a. Similarly, if G contains an edge
from b to a, then write a
b to mean the two-node
←
path in which b follows a. Write a (cid:10) b to mean either
a

b or b

→

a.

→

←

= v1 (cid:10)

Deﬁnition 1 (Active Path (Deﬁnition 3.6 from Koller
(cid:10) vn be a
and Friedman (2009))). Let
path in a graphical model. Let Z be a subset of the
active
variables from the graphical model. Then, call
given Z if (1) for every v-structure vi−1
vi+1
vi
in
, vi or a descendant of vi is in Z; and (2) no other
P
node in

is in Z.

· · ·

←

→

P

P

P

Deﬁnition 2 (d-Separation (Deﬁnition 3.7 from Koller
and Friedman (2009))). Let X, Y , and Z be three sets
of nodes from a graphical model. Then, call X and Y
d-separated given Z if no path between any x
X and
y

Y is active given Z.

∈

∈

Theorem 3 (d-Separation Implies Conditional Inde-
pendence (Theorem 3.3 from Koller and Friedman
(2009))). Let X, Y , and Z be three sets of nodes from
a graphical model. If X and Y are d-separated given
Z, then X

Z.

Y

⊥

|

Deﬁne the layer of a node in Figure 2b to be

layer(fi) = layer(yi(x)) = i.

We are now ready to prove Theorem 2.

Proof of Theorem 2. For i < j, let
tween any yi(x(cid:48))
∈
be the ﬁrst node in

be a path be-
P
i+1:N . Let yk(ˆx)
such that layer(yk(ˆx)) > i. Then

yi and yj(x)

∈ D

contains

P

P

ym(ˆx)

yk(ˆx) (cid:10)

· · · →

→

· · ·

7 D1:i = {y(n)
j ∈ D : j > i, n ≤ N }.

j

{y(n)

∈ D : j ≤ i, n ≤ N } and Di+1:M =

for some m

i < k.

If yk(ˆx)
ym(ˆx)

∈ D

≤
i+1:N , then, since

1:i, meaning that

∈ D

P
If, on the other hand, yk(ˆx) /
downwards, yk(cid:48)(ˆx) /
≥
cannot be descendant of yk(ˆx), so

∈ D
for all k(cid:48)

∈ D

is closed downwards,

D
is inactive.

, then, since

is closed
D
k. Therefore, yj(x)
must contain

P

yk(cid:48)(ˆx)

yk(cid:48)(cid:48)(ˆx)

fk(cid:48)(cid:48)

· · · →

→

←

→ · · ·

for some m
We conclude that
descendant of yk(cid:48)(cid:48)(ˆx) can be in

k(cid:48) < k(cid:48)(cid:48), which forms a v-structure.
is inactive, because yk(cid:48)(cid:48) (ˆx) nor a

≤

P

.

D

B The Nonlinear and Linear

Equivalent Model

In this section, we construct equivalent models for
GPAR (Lemmas 1 and 2). These models make GPAR’s
connection to other models in the literature explicit.

To begin with, we must introduce some notation and
M )X
M ,
deﬁnitions. For functions A, B :
→ Y
deﬁne composition
B)(x, y) =
is well-deﬁned and as-
A(x, B(
·
M , denote
sociative.
, u). Again, note that
u :
A
◦
B)
(A
◦

u = A(
u). Furthermore, denote

For a function u :

X → Y
u = A
◦

, y)). Note that

M , A
(B

as follows:

X → Y

(
(A

Y
◦

X ×

◦
◦

◦

◦

◦

·

A
(cid:124)

◦ · · · ◦
(cid:123)(cid:122)
n times

A
(cid:125)

= An.

(

(

Y

Y

X ×

X ×

→ Y

→ Y

M )X

M )X

M such that
depends only on (x, y1:i−1),
M , denote
f , and denote N consecutive applications

Consider a function A :
Ai(x, y) :
where A1 = 0. Further let u, y :
T f = u + A
◦
of T by TN .
The expression TM −1 u will be key in constructing
the equivalent models. We show that it is the unique
solution of a functional equation:

X → Y

Proposition 1. The unique solution of y = u + A
is y = TM −1 u.

y

◦

Proof of Proposition 1. First, we show that y = u +
y has a solution, and that this solution is unique.
A
Because Ai(x, y) depends only on (x, y1:i−1), it holds
that

◦

yi = ui + Ai

y = ui + Ai

(y1:i−1, 0),

◦

◦

where (y1:i−1, 0) represents the concatenation of y1:i−1
and M
i + 1 zeros. Thus, yi can uniquely be con-
structed from ui, Ai, and y1:i−1; therefore, y1 exists
and is unique, so y2 exists and is unique: by induction
we ﬁnd that y exists and is unique.

−

The Gaussian Process Autoregressive Regression Model (GPAR)

Second, we show that y = TM −1 u satisﬁes y = u +
y = T y. To show this, we show that (Tn u)i =
A
(Tn−1 u)i for i = 1, . . . , n, for all n. To begin with, we
show the base case, n = 1:

◦

(T u)1 = u1 + A1

u = u1 = (T0 u)1,

◦
since A1 = 0. Finally, suppose that the claim holds for
a particular n. We show that the claim then holds for
n + 1: Let i

n + 1. Then

≤

(Tn+1 u)i = ui + Ai
= ui + Ai

◦

◦

◦

◦

◦

= ui + Ai

(∗)
= ui + Ai

= ui + Ai
= (Tn u)i,

Tn u
((Tn u)1:i−1, (Tn u)i:M )
((Tn−1 u)1:i−1, (Tn u)i:M )

(By assumption)

((Tn−1 u)1:i−1, (Tn−1 u)i:M )
Tn−1 u

where (
) holds because Ai(x, y) depends only on
∗
(x, y1:i−1).

In the linear case, TM −1 u turns out to greatly simplify.

Proposition 2.
TM −1 u = ((cid:80)M −1

i=0 Ai)

u.

◦

If A(x, y) is linear in y,

then

Proof of Proposition 2. If A(x, y) is linear in y, then
distributes over addition. Therefore,
one veriﬁes that

◦
TM −1 u = u + A

= u + A
...
= u + A

◦

◦

◦

TM −2 u
u + A2

◦

TM −3 u

u +

· · ·

+ AM −1

u.

◦

We now use Propositions 1 and 2 to construct a non-
linear and linear equivalent model.

(

GP

Lemma 1 (Nonlinear Equivalent Model). Let A be an
M )X ,
M -dimensional vector-valued process over
(0, kAi) independently, and let
each Ai drawn from
u be an M -dimensional vector-valued process over
,
X
(0, kui) independently. Further-
each ui drawn from
depend only on
more, let Ai(x, y) :
(
(x, y1:i−1), meaning that kAi = kAi(x, y1:i−1, x(cid:48), y(cid:48)
1:i−1),
and let A1 = 0. Denote T f = u + A
f , and denote
N consecutive applications of T by TN . Then

GP
X ×

M )X

→ Y

X ×

Y

Y

◦

yi

y1:i−1

|

∼ GP

y

|

A, u = TM −1 u
(0, kui + kAi(

⇐⇒
, y1:i−1,

·

, y1:i−1)).

Proof of Lemma 1. Since Ai(x, y) depends only on
(x, y1:i−1), it holds by Proposition 1 that any sam-
y, so yi =
A, u satisﬁes yi = ui + Ai
ple from y

|

·

◦

ui + Ai
concatenation of y1:i−1 and M
equivalence now follows.

(y1:i−1, 0), where (y1:i−1, 0) represents the
i + 1 zeros. The

−

◦

Lemma 2 (Linear Equivalent Model). Suppose that
A was instead generated from

(cid:90)

A(x, y)

ˆA =

ˆA(x

z)y(z) dz,

|

−

where ˆA is an (M

, each ˆAi,j drawn from

(0, k ˆAi,j
X
i > j and ˆAi,j = 0 otherwise. Then

GP

×

M )-matrix-valued process over
) independently if

y

A, u =

|

(cid:32)M −1
(cid:88)

(cid:33)

Ai

i=0

u

◦

⇐⇒

yi

y1:i−1

(0, kui + kAi(

, y1:i−1,

, y1:i−1)), (5)

∼ GP

·

·

|
where

1:i−1)

kAi(x, y1:i−1, x(cid:48), y(cid:48)
(cid:90)

i−1
(cid:88)

k ˆAi,j

(x

=

j=1

z, x(cid:48)

z(cid:48))yj(z)y(cid:48)

j(z(cid:48)) dz dz(cid:48).

−

−

Proof of Lemma 2. First, one veriﬁes that Ai(x, y) still
depends only on (x, y1:i−1), and that Ai(x, y) is linear
in y. The result then follows from Lemma 1 and Propo-
sition 2, where the expression for kAi follows from
straightfoward calculation.

As mentioned in the paper, the kernels for f1:M de-
termine the types of relationships between inputs and
outputs that can be learned. Lemmas 1 and 2 make
this explicit: Lemma 1 shows that nonlocal GPAR can
recover a model where M latent GPs u are repeatedly
composed with another latent GP A, where A has a
particular dependency structure, and Lemma 2 shows
that nonlocal GPAR can recover a model where M
latent GPs u are linearly transformed, where the linear
transform T = (cid:80)M −1
i=0 Ai is lower triangular and may
vary with the input.

|

|

|

◦

u, y

T, u = T

In Lemma 2, note that it is not restrictive that T is
lower triangular: Suppose that T were dense. Then,
letting y
T is jointly Gaussian. Hence
y1:i−1, T is a GP whose mean linearly depends upon
yi
y1:i−1 is of the form of
y1:i−1 via T , meaning that yi
Equation (5) where kui may be more complicated. If,
however, ˆA(z) = δ(z)B for some random (M
M )-
matrix B, each Bi,j drawn from
Bi,j ) if i > j and
Bi,j = 0 otherwise, then it is restrictive that T is lower
B, u = (cid:80)M −1
i=0 Biu(x). If
triangular: In this case, y(x)
T = (cid:80)M −1
i=0 Bi were dense, then, letting y
T, u = T u,
y can be represented with Lemma 2 if and only if y
T ’s
covariance can be diagonalised by a constant, invertible,

(0, σ2

N

×

|

|

|

|

James Requeima, Will Tebbutt, Wessel Bruisma, and Richard E. Turner

lower-triangular matrix. This condition does not hold
in general, as Lemma 3 proves.

C Lemma 3

Call functions k1, . . . , kM :
if

X →

R linearly independent

(cid:32)

x :

∀

M
(cid:88)

i=1

(cid:33)

=

⇒

ciki(x) = 0

c1 = . . . = cM = 0.

X →

R be linearly in-
Lemma 3. Let k1, . . . , kM :
dependent and arrange them in a diagonal matrix
K = diag(k1, . . . , kn). Let A be an invertible M
M
matrix such that its columns cannot be permuted into a
triangular matrix. Then there does not exist an invert-
ible triangular matrix T such that T −1BK(x)BTT −T
is diagonal for all x.

×

Proof. Suppose, on the contrary, that such T does exist.
Then two diﬀerent rows ap and aq of A = T −1B share
nonzero elements in some columns C; otherwise, A
would have exactly one nonzero entry in every column—
A is invertible—so A would be the product of a per-
mutation matrix and a diagonal matrix, meaning that
B = T A’s columns could be permuted into a triangu-
lar matrix. Now, by T −1BK(x)BTT −T = AK(x)AT
being diagonal for all x
i ap,iaq,iki(x) = 0 for
∈ X
all x. Therefore, by linear independence of k1, . . . , kN ,
it holds that ap,iaq,i = 0 for all i. But ap,iaq,i
= 0 for
C, which is a contradiction.
any i

, (cid:80)

∈

D Experimental Details

∗

For every experiment, the form of the kernels is deter-
mined by the particular GPAR model used: GPAR-L,
GPAR-NL, or GPAR-L-NL (see Table 2 in the main
paper), potentially with a D-
preﬁx to indicate the de-
noising procedure outlined in “Potential deﬁciencies of
GPAR” in Section 2 of the paper main. For GPAR-NL,
we always used exponentiated quadratic (EQ) kernels,
except for the exchange rates experiment, where we
used rational quadratic (RQ) kernels (Rasmussen and
Williams, 2006). Furthermore, in every problem we
simply expanded according to Equation (1) in the main
paper or greedily optimised the ordering, in both cases
putting the to-be-predicted outputs last. We used
scipy’s implementation of the L-BFGS-B algorithm
(Nocedal and Wright, 2006) to optimise hyperparame-
ters.

