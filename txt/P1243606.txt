Vision-based Navigation with Language-based Assistance
via Imitation Learning with Indirect Intervention

Khanh Nguyen
University of Maryland, College Park
kxnguyen@cs.umd.edu

Debadeepta Dey, Chris Brockett, Bill Dolan
Microsoft Research, Redmond
{dedey,Chris.Brockett,billdol}@microsoft.com

9
1
0
2
 
r
p
A
 
6
 
 
]

G
L
.
s
c
[
 
 
4
v
5
5
1
4
0
.
2
1
8
1
:
v
i
X
r
a

Abstract

We present Vision-based Navigation with Language-
based Assistance (VNLA), a grounded vision-language task
where an agent with visual perception is guided via lan-
guage to ﬁnd objects in photorealistic indoor environments.
The task emulates a real-world scenario in that (a) the re-
quester may not know how to navigate to the target objects
and thus makes requests by only specifying high-level end-
goals, and (b) the agent is capable of sensing when it is lost
and querying an advisor, who is more qualiﬁed at the task,
to obtain language subgoals to make progress. To model
language-based assistance, we develop a general frame-
work termed Imitation Learning with Indirect Intervention
(I3L), and propose a solution that is effective on the VNLA
task. Empirical results show that this approach signiﬁcantly
improves the success rate of the learning agent over other
baselines in both seen and unseen environments.

Our code and data are publicly available at https:

//github.com/debadeepta/vnla.

1. Introduction

Rich photorealistic simulators are ﬁnding increasing use
as research testbeds and precursors to real-world embod-
ied agents such as self-driving cars and drones [54, 22, 33].
Recently, growing interest in grounded visual navigation
from natural language is facilitated by the development
of more realistic and complex simulation environments
[2, 36, 9, 47, 60, 59, 52] in place of simple toy environ-
ments [5, 12, 40, 8, 7]. Several variants of this task have
been proposed. In [2], agents learn to execute natural lan-
guage instructions crowd-sourced from humans. [18] train
agents to navigate to answer questions about objects in the
environment. [21] present a scenario where a guide and a
tourist chat to direct the tourist to a destination.

In this paper, we present Vision-based Navigation with
Language-based Assistance (VNLA), a grounded vision-
language task that models a practical scenario: a mobile

agent, equipped with monocular visual perception, is re-
quested via language to assist humans with ﬁnding objects
in indoor environments. A realistic setup of this scenario
must (a) not assume the requester knows how to accom-
plish a task before requesting it, and (b) provide additional
assistance to the agent when it is completely lost and can no
longer make progress on a task. To accomplish (a), instead
of using detailed step-by-step instructions, we request tasks
through high-level instructions that only describe end-goals
(e.g. “Find a pillow in one of the bedrooms”). To fulﬁll (b),
we introduce into the environment an advisor who is present
at all times to assist the agent upon request with low-level
language subgoals such as “Go forward three steps, turn
left”. In VNLA, therefore, the agent must (a) ground the
object referred with the initial end-goal in raw visual inputs,
(b) sense when it is lost and use an assigned budget for re-
questing help, and (c) execute language subgoals to make
progress.

VNLA motivates a novel imitation learning setting that
we term Imitation Learning with Indirect Intervention
(I3L). In conventional Imitation Learning (IL), a learning
agent learns to mimic a teacher, who is only available at
training time, by querying the teacher’s demonstrations on
situations the agent encounters. I3L extends this framework
in two ways. First, an advisor is present in the environment
to assist the agent not only during training time but also at
test time. Second, the advisor assists the agent not by di-
rectly making decisions on the agent’s behalf, but by modi-
fying the environment to inﬂuence its decisions. I3L models
assistance via language subgoals by treating the subgoals
as extra information added to the environment. We devise
an algorithm for the I3L setting that yields signiﬁcant im-
provements over baselines on the VNLA task on both seen
and unseen environments.

The contributions of this paper are:

(a) a new task
VNLA that represents a step closer to real-world applica-
tions of mobile agents accomplishing indoor tasks (b) a
novel IL framework that extends the conventional frame-
work to modeling indirect intervention, and (c) a general
solution to I3L that is shown to be effective on the VNLA

1

Figure 1: An example run in an unseen environment. (a) A bird-eye view of the environment annotated with the agent’s path.
The agent observes the environment only through a ﬁrst-person view. (b) A requester (wearing a hat) asks the agent to “ﬁnd
a towel in the kitchen”. Two towels (pink circle) are in front of the agent but the room is labeled as a “bathroom”. The agent
ignores them without being given the room label. (c) The agent escapes the bathroom but runs into an unfamiliar region.
Sensing that it is lost, the agent signals the advisor (with mustache) for help. The advisor responds with an “easier” low-level
subgoal “turn 60 degrees right, go forward, turn left”. (d) After executing the subgoal, the agent is closer to the kitchen but is
still confused. It thus requests help one more time. After making this request, the agent has exhausted its request budget and
can only rely on its own. (e) Executing the second subgoal helps the agent see the target towel (cyan circle). It successfully
walks to the goal without further assistance. A video demo is at https://youtu.be/Vp6C29qTKQ0.

task. The task is accompanied by a large-scale dataset based
on the photorealistic Matterport3D environment [10, 2].

2. Related Work

Language and robots. Learning to translate natural
language commands to physical actions is well-studied at
the intersection of language and robotics. Proposals in-
clude a variety of grounded parsing models that are trained
from data [42, 43, 37] and models that interact with robots
via natural language queries against a knowledge base [53]
Most relevant to the present work are [45] who ground
natural language to robotic manipulator instructions using
Learning-from-Demonstration (LfD) and [23] who employ
imitation learning of natural language instructions using hu-
mans following directions as demonstration data. In [30],
verbal constraints are used for safe robot navigation in com-
plex real-world environments.

Simulated environments. Simple simulators as Puddle
World Navigation [31] and Rich 3D Blocks World [5, 4]
have facilitated understanding of fundamental representa-
tional and grounding issues by allowing for fast experimen-
tation in easily-managed environments. Game-based and
synthetic environments offer more complex visual contexts
and interaction dynamics [33, 52, 9, 47, 22, 59, 14, 17].
Simulators that are more photo-realistic, realistic-physics
enabled simulators are beginning to be utilized to train real-
world embodied agents [54, 10, 13, 60, 21, 41].

End-to-end learning in rich simulators. [18] present
the “Embodied Question Answering” (EmbodiedQA) task
where an agent explores and answers questions about the
environment. [19] propose a hierarchical solution for this
task where each level of the hierarchy is independently
warmed up with imitation learning and further improved
with reinforcement learning.
[27] and [59] similarly use
reinforcement learning in simulated 3D environments for

successful execution of written instructions. On the vision-
language navigation task [2], cross-modal matching and
self-learning signiﬁcantly improve generalizability to un-
seen environments [24, 58].

Imitation learning. Imitation learning [20, 48, 50, 49,
11, 55] provides an effective learning framework when a
teacher for a task is available or can be simulated. There has
been rich work that focuses on relaxing unrealistic assump-
tions on the teacher. [25, 11, 46, 28, 56] study cases where
teachers provide imperfect demonstrations. [61, 34, 32, 38]
construct policies to minimize the number of queries to
the teacher.
[16] provide language instructions at every
time step to guide meta-policy learning. To the best of our
knowledge, however, no previous work on imitation learn-
ing has explored the case where the agent actively requests
changes to the environment to facilitate its learning process.

3. Vision-based Navigation with Language-

based Assistance

Setup. Our goal is to train an agent, with vision as the per-
ception modality, that can navigate indoors to ﬁnd objects
by requesting and executing language instructions from hu-
mans. The agent is able to “see” the environment via a
monocular camera capturing its ﬁrst-person view as an RGB
image. It is also capable of executing language instructions
and requesting additional help when in need. The camera
image stream and language instructions are the only exter-
nal input signals provided; the agent is not given a map of
the environments or its own location (e.g. via GPS or indoor
localization techniques).

The agent starts at a random location in an indoor envi-
ronment. A requester assigns it an object-ﬁnding task by
sending a high-level end-goal, namely to locate an object
in a particular room (e.g., “Find a cup in one of the bath-
rooms.”). The task is always feasible: there is always an ob-

ject instance in the environment that satisﬁes the end-goal.
The agent is considered to have fulﬁlled the end-goal if it
stops at a location within d meters along the shortest path
to an instance of the desired object. Here d is the success-
radius, a task-speciﬁc hyperparameter. During execution,
the agent may get lost and become unable to progress. We
enable the agent to automatically sense when this happens
and signal an advisor for help.1 The advisor then responds
with language providing a subgoal. The subgoal is a short-
term task that is signiﬁcantly easier to accomplish than the
end-goal. In this work, we consider subgoals that describe
the next k optimal actions (e.g. “Go forward two steps, look
right.”). We assume that strictly following the subgoal helps
the agent make progress.

By specifying the agent’s task with a high-level end-goal,
our setup does not assume the requester knows how to ac-
complish the task before requesting it. This aspect, along
with the agent-advisor interaction, distinguishes our setup
from instruction-following setups [2, 45, 44, 6, 12, 13], in
which the requester provides the agent with detailed se-
quential steps to execute a task only at the beginning.
Constraint formulation. The agent faces a multi-objective
problem: maximizing success rate while minimizing help
requests to the advisor. Since these objectives are in con-
ﬂict, as requesting help more often only helps increase suc-
cess rate, we instead use a hard-constrained formulation:
maximizing success rate without exceeding a budgeted num-
ber of help requests. The hard constraint indirectly speciﬁes
a trade-off ratio between success rate and help requests. The
problem is reduced to single-objective once the constraint is
speciﬁed by users based on their preferences.

4. Imitation Learning with Indirect Interven-

tion

Motivated by the VNLA problem, we introduce Imita-
tion Learning with Indirect Intervention (I3L), which mod-
els (realistic) scenarios where a learning agent is moni-
tored by a more qualiﬁed expert (e.g., a human) and re-
ceives help through an imperfect communication channel
(e.g., language).
Advisor. Conventional Imitation Learning (IL) settings
[20, 48, 50, 49, 11, 55, 56] involve interaction between a
learning agent and a teacher: the agent learns by querying
and imitating demonstrations of the teacher. In I3L, in ad-
dition to interacting with a teacher, the agent also receives
guidance from an advisor. Unlike the teacher, who only in-
teracts with the agent at training time, the advisor assists the
agent during both training and test time.

Intervention. The advisor directs the agent to take a se-
quence of actions through an intervention, which can be di-
rect or indirect. Interventions are direct when the advisor
overwrites the agent’s decisions with its own. By deﬁni-
tion, direct interventions are always executed perfectly, i.e.
the agent always takes actions the advisor wants it to take.
In the case of indirect interventions, the advisor does not
“take over” the agent but instead modiﬁes the environment
to inﬂuence its decisions.2 To utilize indirect interventions,
the agent must learn to interpret them, by mapping them
from signals in the environment to sequences of actions in
its action space. This introduces a new type of error into the
learning process:
intervention interpretation error, which
measures how much the interpretations of the interventions
diverge from the advisor’s original intents.
Formulation. We assume the environment is a Markov de-
cision process with state transition function T . The agent
maintains two policies: a main policy πmain for making de-
cisions on the main task, and a help-requesting policy πhelp
for deciding when the advisor should intervene. We also as-
sume the existence of teacher policies π∗
help, and
an advisor Φ. Teacher policies are only available during
training, while the advisor is always present. Having a pol-
icy πhelp that decides when to ask for help reduces efforts of
the advisor to monitor the agent. However, it does not pre-
vent the advisor from actively intervening when necessary,
because the advisor is able to control πhelp’s decisions by
modifying the environment appropriately. At a state st, if
πhelp decides that the advisor should intervene, the advisor
outputs an indirect intervention that directs the agent to take
a sequence of actions. In this work, we consider the case
when the intervention instructs the agent to take the next k
actions (at, at+1, · · · , at+k−1) suggested by the teacher

main and π∗

at+i = π∗

main(st+i),

(1)

st+i+1 = T (st+i, at+i)

0 ≤ i < k

The state distribution induced by the agent, pagent, depends
on both πmain and πhelp. As in standard imitation learning,
in I3L, the agent’s objective is to minimize expected loss
on the agent-induced state distribution:

ˆπmain, ˆπhelp = arg min

πmain,πhelp

Es∼pagent [L (s, πmain, πhelp)] (2)

where L(., ., .) is a loss function.
Learning to Interpret Indirect Interventions.
I3L can
be viewed as an imitation learning problem in a dynamic
environment, where the environment is altered due to indi-
rect interventions. Provided that teacher policies are well-
deﬁned in the altered environments, an I3L problem can be

1For simplicity, we assume the advisor has perfect knowledge of the en-
vironment, the agent, and the task. In general, as the advisor’s main task is
to help the agent, perfect knowledge is not necessary. The advisor needs to
only possess advantages over the agent (e.g., human-level common sense
or reasoning ability, greater experience at indoor navigation, etc.).

2The direct/indirect distinction is illustrated more tangibly in a physical
agent such as a self-driving car. Turning off automatic driving mode and
taking control of the steering wheel constitutes a direct intervention, while
issuing a verbal command to stop the car represents an indirect intervention
(the command is treated as new information added to the environment).

behavior cloning as special cases. The advisor in I3L-BCUI
intervenes both directly (through behavior cloning) and in-
directly (by modifying the environment) at training time,
but intervenes only indirectly at test time. The teacher in IL
or behavior cloning can be seen as an advisor who is only
available during training and intervenes only directly.
IL
and behavior cloning employ simple help-requesting poli-
cies.
In behavior cloning, the help-requesting policy is
to always have the teacher intervene, since the agent al-
ways lets the teacher make decisions during training. Most
IL algorithms employ a mixed policy as the acting policy
during training, which is equivalent to using a Bernoulli-
distribution sampler as the help-requesting policy.
I3L-
BCUI imposes no restrictions on the help-requesting policy,
which can even be learned from data.

5. Environment and Data

Matterport3D simulator. The Matterport3D dataset [10]
is a large RGB-D dataset for scene understanding in indoor
environments. It contains 10,800 panoramic views inside 90
real building-scale scenes, constructed from 194,400 RGB-
D images. Each scene is a residential building consisting
of multiple rooms and ﬂoor levels, and is annotated with
surface construction, camera poses, and semantic segmen-
tation. Using this dataset, [2] implemented a simulator that
emulates an agent walking in indoor environments. The
pose of the agent is speciﬁed by its viewpoint and orien-
tation (heading angle and elevation angle). Navigation is
accomplished by traversing edges in a pre-deﬁned environ-
ment graph in which edges connect reachable panoramic
viewpoints that are less than 5m apart.
Visual input. The agent’s pose is not provided as input
to the agent. Given a pose, the simulator generates an RGB
image representing the current ﬁrst-person view. The image
is fed into a ResNet-152 [26] pretrained on Imagenet [51]
to extract a mean-pooled feature vector, which serves as the
input to the agent. We use the precomputed image feature
vectors publicly released by [2].
Action space. Following [2], we use a state-independent
action space which consists of six actions: left, right,
up, down, forward and stop. The left, right,
up, down actions rotate the camera by 30 degrees. The
forward action is deﬁned as follows5: executing this ac-
tion takes the agent to the next viewpoint on the shortest
path from the current location to the goals if the viewpoint
lies within 30 degrees of the center of the current view, or
if it lies horizontally within 30 degrees of the center and
the agent cannot bring the viewpoint closer to the center by
looking up or down further; otherwise, executing this action
takes the agent to the viewpoint closest to the center of the

5Our deﬁnition of the forward action, which is different from the
one deﬁned in [2], ensures the navigation teacher never suggests the agent
actions that cause it to deviate from the shortest path to the goals.

Figure 2: Comparison between I3L trained with behavior
cloning under interventions (I3L-BCUI), imitation learning
(IL), and behavior cloning (BC) at training time (left) and
test time (right). Gray dots represent states and arrows rep-
resent actions. Bounding boxes of different colors represent
different environments.

decomposed into a series of IL problems, each of which can
be solved with standard IL algorithms. It turns out, how-
ever, that deﬁning such policies in VNLA is non-trivial.
Even though in VNLA, we use an optimal shortest-path
teacher navigation policy, introducing a subgoal to the envi-
ronment may invalidate this policy. Suppose when an agent
is executing a subgoal, it makes a mistake and deviates from
the trajectory suggested by the subgoal (e.g., ﬁrst turning
right for a subgoal “turn left, go forward”). Then, contin-
uing to follow the subgoal is no longer optimal. Always
following the teacher is also not a good choice because the
agent may learn to ignore the advisor and not be able to
utilize subgoals effectively at test time.

Our solution, which we term as BCUI (Behavior Cloning
Under Interventions), mixes IL with behavior cloning3. In
this approach, the agent uses the teacher policy as the acting
policy (behavior cloning) when executing an intervention (k
steps since the intervention is issued). Thus, the agent never
deviates from the trajectory suggested by the intervention
and thus never encounters conﬂicts between the teacher and
the advisor.4 When no intervention is being executed, the
agent uses the learned policy as the acting policy.
Connection to imitation learning and behavior cloning.
Figure 2 illustrates why I3L trained under BCUI (I3L-
BCUI) is a general framework that subsumes both IL and

3Behavior cloning in IL is equivalent to standard supervised learning in
sequence-to-sequence learning, where during training ground-truth tokens
(instead of predicted tokens) are always used to transition to the next steps.
4A known disadvantage of behavior cloning is that it creates a gap be-
tween training and testing conditions, because at test time the agent acts
on the learned policy. Addressing this problem is left for future work.

Split

Number of data points Number of goals

Train
Dev seen
Dev unseen
Test seen
Test unseen

94,798
4,874
5,005
4,917
5,001

139,757
7,768
8,245
7,470
7,537

Table 1: ASKNAV splits. A data point contains a single
starting viewpoint but multiple goal viewpoints.

current view. We also deﬁne a help-requesting action space
comprising two actions: request and do nothing.
Data Generation. Using annotations provided in the Mat-
terport3D dataset, we construct a dataset for the VNLA
task, called ASKNAV. We use the same environment splits
as [2]: 61 training, 11 development, and 18 test. After ﬁl-
tering out labels that occur less than ﬁve times, are difﬁ-
cult to recognize (e.g., “door frame”), low relevance (e.g.,
“window”) or unknown, we obtain 289 object labels and 26
room labels. We deﬁne each data point as a tuple (environ-
ment, start pose, goal viewpoints, end-goal). An end-goal
is constructed as “Find [O] in [R]”, where [O] is replaced
with “a/an [object label]” (if singular) or “[object label]”
(if plural), and [R] is replaced with “the [room label]” (if
there is one room of the requested label) or “one of the
pluralize([room label])” (if there are multiple rooms
of the requested label). Table 1 summarizes the ASKNAV
dataset. The development and test sets are further divided
into an unseen set and a seen set. The seen set comprises
data points that are generated in the training environments
but do not appear in the training set. The unseen set con-
tains data points generated in the development or test envi-
ronments. The detailed data generation process is described
in the Appendix.

6. Implementation

Notation. The agent maintains two policies: a navigation
policy πnav and a help-requesting policy πask. Each policy is
stochastic, outputting a distribution p over its action space.
An action a is chosen by selecting the maximum probabil-
ity action of or sampling from the output distribution. The
agent is supervised by a navigation teacher π∗
nav and a help-
requesting teacher π∗
ask (both are deterministic policies), and
is assisted by an advisor Φ. A dataset D is provided where
the d-th data point consists of a start viewpoint xstart
, a start
orientation ψstart
d,i }, an end-
goal ed, and the full map Md of the corresponding environ-
ment. At any time, the teachers and the advisor have access
to the agent’s current pose and information provided by the
current data point.
Algorithm. Algorithm 1 describes the overall procedure for
training a VNLA agent. We train the navigation policy un-

, a set of goal viewpoints {xend

d

d

Figure 3: Two decoding passes of the navigation module.
(a) The ﬁrst decoding pass computes the tentative naviga-
tion distribution, which is used as a feature for computing
the help-requesting distribution. (b) The second pass com-
putes the ﬁnal navigation distribution.

t

t

der the I3L-BCUI algorithm (Section 4) and train the help-
requesting policy under behavior cloning. At time step t, the
agent ﬁrst receives a view of the environment from the cur-
rent pose (Line 10). It computes a tentative navigation dis-
tribution pnav
t,1 (Line 11), which is used as an input to com-
pute a help-requesting distribution pask
(Line 12). Since
the help-requesting policy is trained under behavior cloning,
the agent invokes the help-requesting teacher π∗
ask (not the
learned policy πask) to decide if it should request help (Line
13). If the help-requesting teacher decides that the agent
should request help and the help-requesting budget has not
been exhausted, the advisor Φ is invoked to provide help via
a language subgoal gsub
(Lines 14-15). The subgoal is then
prepended to the original end-goal gmain
0,d to form a new end-
goal gmain
(Line 16). If the condition for requesting help
is not met, the end-goal is kept unchanged (Line 19). Af-
ter the help-requesting decision has been made, the agent
computes a ﬁnal navigation distribution pnav
t,2 by invoking
the learned policy πnav the second time. Note that when
computing this distribution, the last help-requesting action
is no longer aask
. The agent selects the
acting navigation policy based on the principle of the I3L-
BCUI algorithm. Speciﬁcally, if the agent has requested
help within the last k steps, i.e. it is still executing a sub-
goal, it uses the teacher policy to act (Line 24). Otherwise,
it samples an action from the ﬁnal navigation distribution
(Line 26). In Lines 28-29, the learned policies are updated
using an online learning algorithm. Finally, the agent tran-
sitions to the next pose according to the taken navigation
action (Line 33).

t−1 but has become aask
t

t

6.1. Agent

We model

the navigation policy πnav and the help-
requesting policy πask as two separate neural network mod-
ules. The navigation module is an encoder-decoder model
[3] with a multiplicative attention mechanism [39] and cov-
erage modeling [57], which encodes an end-goal (a se-

Algorithm 1 VLNA training procedure

d ).

d , ψstart

Reset environment to (xstart
Compute time budget ˆT and help-request budget ˆB.
Initialize current help-request budget b = ˆB.
Initialize anav
Initialize gmain
for t = 1 . . . ˆT do

1: Initialize πnav, πask randomly.
2: k is the number of next actions a subgoal describes.
3: for d = 1 . . . D do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

Receive an image ot of the current view.
pnav
t−1, anav
t,1 = πnav(ot, gmain
t−1, pnav
pask
t = πask(ot, gmain
t = π∗
t = aask∗
aask
ask(pnav
if b > 0 and aask

0 to special action <start>.
d , ψcurr = ψstart
d .

0 , aask
0 = ed, xcurr = xstart

t,1, b, xcurr, ψcurr, {xend

t == request then

t−1, aask
t,1, b)

t−1)

d,i}, Md)

14:
15:

16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35: end for

t = Φ(xcurr, ψcurr, {xend
gsub
gmain
t = gsub
t (cid:12) ed
b ← b − 1

d,i}, Md, k)

else

t = gmain
gmain
t−1

end if
t,2 = πnav(ot, gmain
pnav
anav∗
t = π∗
nav(xcurr, ψcurr, {xend
if requested help within last k steps then

t−1, aask
t )
d,i}, Md)

, anav

t

else

t = anav∗
anav

t

t ∼ pnav
anav
t,2

end if
πask ← UpdatePolicy(πask, pask
πnav ← UpdatePolicy(πnav, pnav
if anav

t == stop then
break

t , aask∗
)
t
t,2, anav∗
)
t

end if
xcurr, ψcurr ← T (xcurr, ψcurr, anav
t )

end for

quence of words) and decodes a sequence of actions. Both
the encoder and decoder are LSTM-based recurrent neural
networks [29]. During time step t, if the end-goal is up-
dated, the encoder generates an attention memory Mt =
(cid:110)

(cid:111)

by recurrently computing

menc

1 , · · · , menc

|gmain
t

|

menc

i = LSTMenc

(cid:0)menc

i−1, gmain
t,i

(cid:1) , 1 ≤ i ≤ |gmain

|

t

(3)

where LSTMenc is the encoding LSTM, gmain
is the embed-
t,i
ding of the i-th word of the end-goal. Otherwise, Mt =
Mt−1. The decoder runs two forward passes to compute
the tentative and the ﬁnal navigation distributions (Figure
3). The i-th decoding pass proceeds as:

(cid:0)hdec
t−1,2, (cid:2)ot; anav
hdec
t,i = LSTMdec
t,i = ATTEND (cid:0)hdec
(cid:1)
hatt
t,i , Mt
t,i = SOFTMAX (cid:0)Wnav
pnav

s hatt
t,i

(cid:1)

t−1; ¯aask
t

(cid:3)(cid:1)

(4)

(5)

(6)

s

where i ∈ {1, 2}, Wnav
is learned parameters, ATTEND(., .)
is the multiplicative attention function, ot is the visual fea-
ture vector of the current view, anav
t−1 is the embedding of the
last navigation action, and

¯aask
t =

(cid:40)

aask
t−1
aask
t

if i = 1,
if i = 2

(7)

is the embedding of the last help-requesting action.

The help-requesting module is a multi-layer feed-
forward neural network with RELU activation functions
and a softmax ﬁnal layer. Its input features are:

• The visual ot and the embedding of the current help-

request budget bt.

• The tentative navigation distribution, pnav
t,1 .
• The tentative navigation decoder states, hdec

t,1 and hatt
t,1.
These features are concatenated and fed into the network to
compute the help-requesting distribution

hask
t = FEED-FORWARDl
t = SOFTMAX (cid:0)Wask
pask

s hask
t

(cid:1)

(cid:0)(cid:2)ot; bt; pnav

t,1 ; hdec

t,1; hatt
t,1

(cid:3)(cid:1) (8)
(9)

s

where Wask
is a learned parameter and FEED-FORWARDl is
a feed-forward network with l hidden layers. During train-
ing, we do not backpropagate errors of the help-requesting
module through its input features. Preliminary experiments
showed that doing so resulted in lower performance.

6.2. Teachers

Navigation teacher.
The navigation teacher always
chooses actions to traverse along the shortest path from
the current viewpoint to the goal viewpoints. This path is
optimal with respect to minimizing the walking distance
to the goals, but is not necessarily optimal in the number
of navigation actions. Given an agent’s pose, the naviga-
tion teacher ﬁrst adjusts the orientation using the camera-
adjusting actions (left, right, up, down) until select-
ing the forward action advances the agent to the next
viewpoint on the shortest path to the goals. The teacher
issues the stop action when one of the goal viewpoints is
reached.
Help-requesting teacher. Even with perfect information
about the environment and the agent, computing an opti-
mal help-requesting teacher policy is expensive because this
policy depends on (a) the agent’s internal state, which lies in
a high-dimensional space and (b) the current learned navi-
gation policy, which changes constantly during training. We
design a heuristic-driven teacher, which decides to request
help when:
(a) The agent deviates from the shortest path by more than
δ meters. The distance from the agent to a path is de-
ﬁned as the distance from its current viewpoint to the
nearest viewpoint on the path.

(b) The agent is “confused”, deﬁned as when the differ-
ence between the entropy of the uniform distribution
and the entropy of the agent’s tentative navigation dis-
tribution pnav
t,1 is smaller than a threshold (cid:15).

(c) The agent has remained at the same viewpoint for the

last µ steps.

(d) The help-request budget is greater than or equal to the

number of remaining steps.

(e) The agent is at a goal viewpoint but the highest-
probability action of the tentative navigation distribu-
tion is forward.

Although this heuristic-based teacher may not be optimal,
our empirical results show that not only is it effective but
it is also easy to imitate. Moreover, imitating a clairvoy-
ant teacher is more sample-efﬁcient (theoretically proven
[49, 56]) and results in safer, more robust policies compared
to maximizing a reward function with reinforcement learn-
ing (empirically shown [15]). The latter approach imposes
weaker constraints on the regularity of the solution and may
produce exploitative but unintuitive policies [1].

6.3. Advisor

Upon receiving a request from the agent, the advisor
queries the navigation teacher for k consecutive steps to
obtain a sequence of k actions (Equation 1). Next, ac-
tions {left, right, up, down, forward, stop} are
mapped to phrases {“turn left”, “turn right”, “look up”,
“look down”, “go forward”, “stop”}, respectively. Then,
repeated actions are aggregated to make the language more
challenging to interpret. For example, to describe a turn-
right action that is repeated X times, the advisor says “turn
Y degrees right” where Y = X × 30 is the total degrees the
agent needs to turn after repeating the turn-right action X
times. Similarly, Z repeated forward actions are phrased
as “go forward Z steps”. The up, down, stop actions are
not aggregated because they are rarely or never repeated. Fi-
nally, action phrases are joined by commas to form the ﬁnal
subgoal (e.g., “turn 60 degrees left, go forward 2 steps”).

6.4. Help-request Budget

Let ˆT be the time budget and B be the help-request bud-
get. Suppose the advisor describes the next k optimal ac-
tions in response to each request. We deﬁne a hyperparam-
eter τ ∈ [0, 1], which is the ratio between the total number
of steps where the agent receives assistance and the time
. Given τ , ˆT and k, we approximate B
budget, i.e. τ ≡ B·k
ˆT
by an integral random variable ˆB

ˆB = (cid:98)B(cid:99) + r
r ∼ BERNOULLI ({B})

(10)

B =

ˆT · τ
k

(cid:105)

(cid:104) ˆB·k
ˆT

where {B} = B − (cid:98)B(cid:99) is the fractional part of B. The ran-
= τ for a ﬁxed ˆT
dom variable r guarantees that Er
and any positive value of k, ensuring fairness when compar-
ing agents interacting with advisors of different ks. Due to
the randomness introduced by r, we evaluate an agent with
multiple samples of ˆB. Detail on how we determine ˆT for
each data point is provided in the appendix.

7. Experimental Setup

Baselines. We compare our learned help-requesting policy
(LEARNED) with the following baseline policies:

• NONE: never requests help.
• FIRST: requests help continuously from the beginning,

• RANDOM: uniformly randomly chooses ˆB steps to re-

up to ˆB.

quest help.

• TEACHER: follows the help-requesting teacher (π∗
ask).
In each experiment, the same help-requesting policy is used
during training and evaluation.
Evaluation metrics. Our primary metrics are success rate,
room-ﬁnding success rate, and navigation error. Success
rate is the fraction of the test set on which the agent suc-
cessfully fulﬁlls the task. Room-ﬁnding success rate is the
fraction of the test set on which the agent’s ﬁnal location
is in the right room type. Navigation error measures the
length of the shortest path from the agent’s end viewpoint
to the goal viewpoints. We evaluate each agent with ﬁve dif-
ferent random seeds and report means with 95% conﬁdence
intervals.
Hyperparameters. See the Appendix for details.

8. Results

Main results. Our main results are presented in Table 2.
Overall, empowering the agent with the ability to ask for
help and assisting it via subgoals greatly boost its perfor-
mance. Requesting help is more useful in unseen environ-
ments, improvements over NONE of all other policies being
higher on TEST UNSEEN than on TEST SEEN. Even a sim-
ple policy like FIRST yields success rate improvements of
12% and 14% over NONE on TEST SEEN and TEST UN-
SEEN respectively. The LEARNED policy outperforms all
agent-agnostic polices (NONE, FIRST, RANDOM), achiev-
ing 9-10% improvement in success rate over RANDOM and
24-28% over NONE. An example run of the LEARNED
agent is shown in Figure 1. The insigniﬁcant performance
gaps between LEARNED and TEACHER indicates that the
latter is not only effective but also easy to imitate6. RAN-
DOM is largely more successful than FIRST, hinting that it

6There is a tradeoff between performance and learnability of the help-
requesting teacher. By varying hyperparameters, we can obtain a teacher
that achieves higher success rate but is harder to imitate.

πask

Success
rate (%) ↑

Room-ﬁnding
success
rate (%) ↑

Mean
navigation
error (m) ↓

Test seen

28.39 ± 0.00
NONE
40.33 ± 0.35
FIRST
RANDOM 42.98 ± 0.44
LEARNED 52.09 ± 0.13

48.97 ± 0.00
59.64 ± 0.22
54.61 ± 0.28
64.84 ± 0.23

6.29 ± 0.00
4.36 ± 0.03
4.53 ± 0.03
3.48 ± 0.01

TEACHER 52.26 ± 0.16

65.42 ± 0.25

3.42 ± 0.01

Test unseen

6.36 ± 0.00
NONE
20.00 ± 0.10
FIRST
RANDOM 25.05 ± 0.31
LEARNED 34.50 ± 0.23

14.34 ± 0.00
30.23 ± 0.40
33.72 ± 0.37
44.50 ± 0.36

11.30 ± 0.00
7.56 ± 0.02
7.09 ± 0.05
5.66 ± 0.02

TEACHER 34.95 ± 0.33

44.85 ± 0.39

5.61 ± 0.02

Table 2:
ASKNAV test sets.

Performance of help-requesting policies on

Advisor Subgoals Train iterations

Test seen

Test unseen

Direct
Direct
Indirect

(cid:55)
(cid:51)
(cid:51)

70k
100k
100k

51.07 ± 0.17 32.19 ± 0.28
52.09 ± 0.13 34.56 ± 0.21
52.09 ± 0.13 34.50 ± 0.23

Table 3: Success rates (%) on ASKNAV test sets of agents
interacting with different advisors. We compare agents that
achieve comparable success rates on the DEV SEEN split.

may be ineffective to request help early too often. Nev-
ertheless, FIRST is better than RANDOM at ﬁnding rooms
on TEST SEEN. This may be because on TEST SEEN, al-
though the complete tasks are previously unseen, the room-
ﬁnding subtasks might have been assigned to the agent dur-
ing training. For example, the agent might have never been
requested to “ﬁnd an armchair in the living room” during
training, but it might have been taught to go to the living
room to ﬁnd other objects. When the agent is asked to ﬁnd
objects in a room it has visited, once the agent recognizes
a familiar action history, it can reach the room by mem-
ory without much additional assistance. As the ﬁrst few
actions are crucial, requesting help early is closer to an op-
timal strategy than randomly requesting in this case.
Effects of subgoals. Subgoals not only serve to direct the
agent, but also act as extra, informative input features. We
hypothesize that the agent still beneﬁts from receiving sub-
goals even when it interacts with an advisor who intervenes
directly (in which case subgoals seem unneeded). To test
this hypothesis, we train agents interacting with a direct ad-
visor, who overwrites the agents’ decisions by its decisions
during interventions. We consider two variants of this advi-
sor: one responds with a subgoal in response to each help
request and the other does not. Table 3 compares these with

πask

Training set

Test seen

Test unseen

NOROOM 43.69 ± 0.37 33.41 ± 0.39
RANDOM
LEARNED NOROOM 53.71 ± 0.19 44.77 ± 0.27
53.85 ± 0.45 41.63 ± 0.24
LEARNED ASKNAV

Table 4: Success rates (%) on NOROOM test sets.

our standard indirect advisor, who at test time sends sub-
goals but does not overwrite the agent’s decisions. Since
success rates on TEST SEEN tend to take a long time to con-
verge, we compare the success rates on TEST UNSEEN of
agents that have comparable success rates on DEV SEEN
(the success rates differ by no more than 0.5%). The two
agents interpreting subgoals face a harder learning problem,
and thus require more iterations to attain success rates on
DEV SEEN comparable to that of the agent not interpreting
subgoals. Receiving subgoals boosts sucess rate by more
than 2% on TEST UNSEEN regardless of whether interven-
tion is direct or indirect.
Does the agent learn to identify objects? The agent might
have only learned to ﬁnd the requested rooms and have
“luckily” stood close to the target objects because there are
only few viewpoints in a room. To verify if the agent has
learned to identify objects after being trained with room
type information, we setup a transfer learning experiment
where an agent trained to fulﬁll end-goals with room types
is evaluated with end-goals without room types. Following
a procedure similar the one used to generate the ASKNAV in
Section 5, we generate a NOROOM dataset, which contains
end-goals without room type information. Each end-goal
in the dataset has the form “Find [O]”, where [O] is an ob-
ject type. Finding any instance of the requested object in
any room satisﬁes the end-goal. The number of goals in
the training split of this dataset is comparable to that of the
ASKNAV dataset (around 140k). More detail is provided
in the Appendix. The results in Table 4 indicate that our
agent, equipped with a learned help-requesting policy and
trained with room types, learns to recognize objects, as it
can ﬁnd objects without room types signiﬁcantly better than
an agent equipped with a random help-requesting policy
and trained speciﬁcally to ﬁnd objects without room types
(+10% on TEST SEEN and +8% on TEST UNSEEN in suc-
cess rate). Unsurprisingly, directly training to ﬁnd objects
without room types yields best results in this setup because
training and test input distributions are not mismatched.

9. Future Work

We are exploring ways to provide more natural, fully-
linguistic question and answer interactions between advisor
and agent, and better theoretical understanding of the I3L
setting and resulting algorithms. We will also be investigat-
ing how to transfer from simulators to real-world robots.

References

[1] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Chris-
tiano, John Schulman, and Dan Man´e. Concrete problems
in ai safety. arXiv preprint arXiv:1606.06565, 2016. 7
[2] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark
Johnson, Niko S¨underhauf, Ian Reid, Stephen Gould, and
Anton van den Hengel. Vision-and-language navigation: In-
terpreting visually-grounded navigation instructions in real
In Proceedings of the IEEE Conference on
environments.
Computer Vision and Pattern Recognition, volume 2, 2018.
1, 2, 3, 4, 5

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. In Proceedings of the International Conference on
Learning Representations, 2015. 5

[4] Yonatan Bisk, Kevin J Shih, Yejin Choi, and Daniel Marcu.
Learning interpretable spatial operations in a rich 3d blocks
world. In Association for the Advancement of Artiﬁcial In-
telligence, 2018. 2

[5] Yonatan Bisk, Deniz Yuret, and Daniel Marcu. Natural lan-
In Proceedings of the
guage communication with robots.
2016 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, pages 751–761, 2016. 1, 2

[6] Valts Blukis, Dipendra Misra, Ross A Knepper, and Yoav
Artzi. Mapping navigation instructions to continuous control
actions with position-visitation prediction. In Conference on
Robot Learning, pages 505–518, 2018. 3

[7] SRK Branavan, David Silver, and Regina Barzilay. Learn-
ing to win by reading manuals in a monte-carlo frame-
work. Journal of Artiﬁcial Intelligence Research, 43:661–
704, 2012. 1

[8] Satchuthananthavale RK Branavan, Harr Chen, Luke S
Zettlemoyer, and Regina Barzilay. Reinforcement learning
In Proceedings of the
for mapping instructions to actions.
Joint Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages 82–90.
Association for Computational Linguistics, 2009. 1

[9] Simon Brodeur, Ethan Perez, Ankesh Anand, Florian
Golemo, Luca Celotti, Florian Strub, Jean Rouat, Hugo
Larochelle, and Aaron Courville. Home: A household mul-
timodal environment. In Proceedings of Advances in Neural
Information Processing Systems, 2017. 1, 2

[10] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-
D data in indoor environments. International Conference on
3D Vision (3DV), 2017. 2, 4

[11] Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal,
Hal Daume III, and John Langford. Learning to search bet-
In Proceedings of the International
ter than your teacher.
Conference of Machine Learning, 2015. 2, 3

[12] David L Chen and Raymond J Mooney. Learning to interpret
natural language navigation instructions from observations.
In Association for the Advancement of Artiﬁcial Intelligence,
volume 2, pages 1–2, 2011. 1, 3

[13] Howard Chen, Alane Shur, Dipendra Misra, Noah Snavely,
Ian Artzi, Yoav, Stephen Gould, and Anton van den Hengel.
Touchdown: Natural language navigation and spatial rea-
soning in visual street environments. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2019. 2, 3

[14] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem
Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu
Nguyen, and Yoshua Bengio. BabyAI: First steps towards
In
grounded language learning with a human in the loop.
Proceedings of the International Conference on Learning
Representations, 2019. 2

[15] Sanjiban Choudhury, Mohak Bhardwaj, Sankalp Arora,
Ashish Kapoor, Gireeja Ranade, Sebastian Scherer, and De-
badeepta Dey. Data-driven planning via imitation learn-
ing. The International Journal of Robotics Research, page
0278364918781001, 2018. 7

[16] John D Co-Reyes, Abhishek Gupta, Suvansh Sanjeev, Nick
Altieri, John DeNero, Pieter Abbeel, and Sergey Levine.
Guiding policies with language via meta-learning. In Pro-
ceedings of the International Conference on Learning Rep-
resentations, 2019. 2

[17] Marc-Alexandre Cˆot´e, ´Akos K´ad´ar, Xingdi Yuan, Ben Ky-
bartas, Tavian Barnes, Emery Fine, James Moore, Matthew
Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay,
and Adam Trischler. Textworld: A learning environment for
text-based games. CoRR, abs/1806.11532, 2018. 2

[18] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee,
Devi Parikh, and Dhruv Batra. Embodied question answer-
ing. Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, 2018. 1, 2

[19] Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh,
and Dhruv Batra. Neural modular control for embodied
question answering. Proceedings of the Conference on Robot
Learning, 2018. 2

[20] Hal Daum´e, John Langford, and Daniel Marcu. Search-
based structured prediction. Machine learning, 75(3):297–
325, 2009. 2, 3

[21] Harm de Vries, Kurt Shuster, Dhruv Batra, Devi Parikh, Ja-
son Weston, and Douwe Kiela. Talk the walk: Navigating
new york city through grounded dialogue. arXiv preprint
arXiv:1807.03367, 2018. 1, 2

[22] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio
Lopez, and Vladlen Koltun. CARLA: An open urban driving
simulator. In Proceedings of the 1st Annual Conference on
Robot Learning, pages 1–16, 2017. 1, 2

[23] Felix Duvallet, Thomas Kollar, and Anthony Stentz.

Im-
itation learning for natural language direction following
In Robotics and Automa-
through unknown environments.
tion (ICRA), 2013 IEEE International Conference on, pages
1047–1053. IEEE, 2013. 2

[24] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach,
Jacob Andreas, Louis-Philippe Morency, Taylor Berg-
Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell.
Speaker-follower models for vision-and-language naviga-
tion. In Neural Information Processing Systems (NeurIPS),
2018. 2

[25] Yang Gao, Ji Lin, Fisher Yu, Sergey Levine, Trevor Dar-
rell, et al. Reinforcement learning from imperfect demon-
strations. Proceedings of the 35th International Conference
on Machine Learning, 2018. 2

[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 4

[27] Karl Moritz Hermann, Felix Hill, Simon Green, Fumin
Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wo-
jciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin,
et al. Grounded language learning in a simulated 3d world.
arXiv preprint arXiv:1706.06551, 2017. 2

[28] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot,
Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew
Sendonaris, Gabriel Dulac-Arnold, et al. Deep q-learning
from demonstrations. Association for the Advancement of
Artiﬁcial Intelligence, 2018. 2

[29] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term
memory. Neural computation, 9(8):1735–1780, 1997. 6
[30] Zhe Hu, Jia Pan, Tingxiang Fan, Ruigang Yang, and Dinesh
Manocha. Safe navigation with human instructions in com-
plex scenes. IEEE Robotics and Automation Letters, 2019.
2

[31] Michael Janner, Karthik Narasimhan, and Regina Barzi-
lay. Representation learning for grounded spatial reasoning.
Transactions of the Association of Computational Linguis-
tics, 6:49–61, 2018. 2

[32] Kshitij Judah, Alan P Fern, Thomas G Dietterich, et al. Ac-
tive lmitation learning: formal and practical reductions to
iid learning. The Journal of Machine Learning Research,
15(1):3925–3963, 2014. 2

[33] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub
Toczek, and Wojciech Ja´skowski. Vizdoom: A doom-based
ai research platform for visual reinforcement learning.
In
Computational Intelligence and Games (CIG), 2016 IEEE
Conference on, pages 1–8. IEEE, 2016. 1, 2

[34] Beomjoon Kim and Joelle Pineau. Maximum mean discrep-
ancy imitation learning. In Robotics: Science and systems,
2013. 2

[35] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In Proceedings of the International
Conference on Learning Representations, 2015. 14

[36] Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu,
Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d
environment for visual ai. arXiv preprint arXiv:1712.05474,
2017. 1

[37] Jayant Krishnamurthy and Tom M Mitchell. Weakly su-
In Proceedings of
pervised training of semantic parsers.
the 2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural Lan-
guage Learning, pages 754–765. Association for Computa-
tional Linguistics, 2012. 2

[38] Michael Laskey, Sam Staszak, Wesley Yu-Shu Hsieh, Jef-
frey Mahler, Florian T Pokorny, Anca D Dragan, and Ken
Goldberg. Shiv: Reducing supervisor burden in dagger using
support vectors for efﬁcient learning from demonstrations in

high dimensional state spaces. In Robotics and Automation
(ICRA), 2016 IEEE International Conference on, pages 462–
469. IEEE, 2016. 2

[39] Minh-Thang Luong, Hieu Pham, and Christopher D Man-
ning. Effective approaches to attention-based neural machine
translation. In Proceedings of Emperical Methods in Natural
Language Processing, 2015. 5

[40] Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers.
Walk the talk: Connecting language, knowledge, and action
in route instructions. In Association for the Advancement of
Artiﬁcial Intelligence, 2006. 1
Savva*,

Oleksandr
Maksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain,
Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi
Parikh, and Dhruv Batra. Habitat: A platform for embodied
ai research. arXiv preprint arXiv:, 2019. 2

Abhishek Kadian*,

[41] Manolis

[42] Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer,
Liefeng Bo, and Dieter Fox. A joint model of language and
perception for grounded attribute learning. In Proceedings
of the International Conference of Machine Learning, 2012.
2

[43] Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and Di-
eter Fox. Learning to parse natural language commands to a
robot control system. In Experimental Robotics, pages 403–
415. Springer, 2013. 2

[44] Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind
Niklasson, Max Shatkhin, and Yoav Artzi. Mapping instruc-
tions to actions in 3d environments with visual goal predic-
tion. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages 2667–2678.
Association for Computational Linguistics, 2018. 3

[45] Dipendra K Misra, Jaeyong Sung, Kevin Lee, and Ashutosh
Saxena. Tell me dave: Contextsensitive grounding of natu-
ral language to mobile manipulation instructions. In in RSS,
2014. 2, 3

[46] Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Woj-
ciech Zaremba, and Pieter Abbeel. Overcoming exploration
In 2018
in reinforcement learning with demonstrations.
IEEE International Conference on Robotics and Automation
(ICRA), pages 6292–6299. IEEE, 2018. 2

[47] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu
Wang, Sanja Fidler, and Antonio Torralba. Virtualhome:
Simulating household activities via programs. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2018. 1, 2

[48] St´ephane Ross and Drew Bagnell. Efﬁcient reductions for
In Proceedings of the thirteenth inter-
imitation learning.
national conference on artiﬁcial intelligence and statistics,
pages 661–668, 2010. 2, 3

[49] Stephane Ross and J Andrew Bagnell. Reinforcement and
imitation learning via interactive no-regret learning. arXiv
preprint arXiv:1406.5979, 2014. 2, 3, 7

[50] St´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A re-
duction of imitation learning and structured prediction to no-
regret online learning. In Proceedings of the fourteenth inter-
national conference on artiﬁcial intelligence and statistics,
pages 627–635, 2011. 2, 3

[51] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al.
Imagenet large
scale visual recognition challenge. International Journal of
Computer Vision, 115(3):211–252, 2015. 4

[52] Manolis Savva, Angel X Chang, Alexey Dosovitskiy,
Thomas Funkhouser, and Vladlen Koltun. Minos: Multi-
modal indoor simulator for navigation in complex environ-
ments. arXiv preprint arXiv:1712.03931, 2017. 1, 2

[53] Ashutosh Saxena, Ashesh Jain, Ozan Sener, Aditya Jami,
Dipendra K Misra, and Hema S Koppula. Robobrain:
Large-scale knowledge engine for robots. arXiv preprint
arXiv:1412.0691, 2014. 2

[54] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish
Kapoor. Airsim: High-ﬁdelity visual and physical simula-
tion for autonomous vehicles. In Field and service robotics,
pages 621–635. Springer, 2018. 1, 2

[55] Amr Sharaf and Hal Daum´e III. Structured prediction via
In Proceedings
learning to search under bandit feedback.
of the 2nd Workshop on Structured Prediction for Natural
Language Processing, pages 17–26, 2017. 2, 3

[56] Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron
Boots, and J Andrew Bagnell. Deeply aggrevated: Differ-
entiable imitation learning for sequential prediction. In Pro-
ceedings of the International Conference of Machine Learn-
ing, 2017. 2, 3, 7

[57] Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and
Hang Li. Modeling coverage for neural machine transla-
tion. In Proceedings of the Association for Computational
Linguistics, 2016. 5

[58] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao,
Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and
Lei Zhang. Reinforced cross-modal matching and self-
supervised imitation learning for vision-language navigation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2019. 2

[59] Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian.
Building generalizable agents with a realistic and rich 3d en-
vironment. In Workshop Track of the International Confer-
ence on Learning Representation, 2018. 1, 2

[60] Fei Xia, Amir R. Zamir, Zhi-Yang He, Alexander Sax, Jiten-
dra Malik, and Silvio Savarese. Gibson env: real-world per-
ception for embodied agents. In Computer Vision and Pat-
tern Recognition (CVPR), 2018 IEEE Conference on. IEEE,
2018. 1, 2, 16

[61] Jiakai Zhang and Kyunghyun Cho. Query-efﬁcient imitation
learning for end-to-end simulated driving. In Association for
the Advancement of Artiﬁcial Intelligence, pages 2891–2897,
2017. 2

Appendix to
Vision-based Navigation with Language-based Assistance
via Imitation Learning with Indirect Intervention

1. Acknowledgements

Algorithm 2 Data sampling procedure.

We would like thank to Microsoft Research Redmond in-
terns Xin Wang, Ziyu Yao, Shrimai Prabhumoye, Shi Feng,
Amr Sharaf, Chen Zhao, and Evan Pu for useful discus-
sions. We are grateful also to Michel Galley and Hal Daum´e
III for their advice and support. Special thanks goes to
Vighnesh Shiv for his meticulous review of the paper. We
also greatly appreciate the CVPR reviewers for their thor-
ough and insightful comments.

2. Data Generation.

ASKNAV dataset. We partition all object instances into
buckets where instances in the same bucket share the same
environment, containing-room label, and object label. For
each bucket, an end-goal is constructed as “Find [O] in [R]”,
where [O] is replaced with “a/an [object label]” (if singular)
or “[object label]” (if plural), and [R] is replaced with “the
[room label]” (if there is one room of the requested label) or
“one of the pluralize([room label])”1 (if there are mul-
tiple rooms of the requested label). We deﬁne the delegate
viewpoint of an object as the closest viewpoint that is in
the same room. To avoid annotation mistakes in the Mat-
terport3D dataset, we ignore object instances that do not lie
in the bounding boxes of their rooms. Goal viewpoints of
an end-goal are delegate viewpoints of all object instances
in the corresponding bucket. All viewpoints in the envi-
ronment are candidates for the start viewpoint. We exclude
candidates that are not reachable from any goal viewpoint
and sample from the remaining candidates at most ﬁve start
viewpoints per room. The initial heading angle is a random
multiple of π
6 (less than 2π) and the initial elevation angle
is always zero.

Each data point is deﬁned as a tuple (environment, start
pose, goal viewpoints, end-goal). For each data point, we
run the navigation teacher to obtain the sequences of actions
that take the agent from the start viewpoint to the goal view-
points. Note that executing those sequences of actions may
not necessarily result in the agent facing the target objects

1We use https://github.com/jazzband/inflect to check

for plurality and perform pluralization.

1: Input: a set of buckets B = {bi} where each bucket contains
valid data points. N is the maximum number of elements to
sample from each bucket (N = 10 for ASKNAV, N = 20 for
NOROOM).

Randomly shufﬂe elements of B.
Mark all environments as ‘not sampled’.
for bucket b in B do

2: Output: a dataset D containing no less than 5000 data points.
3: Initialize D = ∅.
4: while |D| < 5000 and B (cid:54)= ∅ do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
end for
15:
16: end while

s is a random sample of at most N elements of b.
D ← D ∪ s
Remove b from B.
Mark e as ‘already sampled’.

e ← environment of b.
if e was ‘not sampled’ then

end if

in the end. We ﬁlter data points whose start viewpoints are
adjacent to one of the goal viewpoints on the environment
graph, or that require fewer than 5 or more than 25 actions
to reach one of the goal viewpoints. We accumulate valid
data points in all seen environments and sample a fraction
to construct the seen sets. Algorithm 2 describes the sam-
pling procedure. The remaining data points generated from
the training environments form the training set. Similarly,
the unseen sets are samples of data points in the unseen en-
vironments. We ﬁnally remove examples in the seen sets
whose environments do not appear in the training set.

Figures 4, 6, 5, 7 offer more insights into the ASKNAV
dataset. Most common target objects are associated with
many instances in a house (e.g., picture, table, chair, cur-
tain). Goal viewpoints mostly lie in rooms that contain
many objects (e.g., bedroom, kitchen, living room, bath-
room), whereas start viewpoints tend to be in the hallway
because it is spacious and thus includes numerous view-
points. About 85% of paths require at least ten actions to
reach the goal viewpoints.

12

(a) Train

(b) Test seen

(c) Test unseen

Figure 4: Top 20 most common objects in the ASKNAV dataset.

(a) Train

(b) Test seen

(c) Test unseen

Figure 5: Top 20 most common start rooms in the ASKNAV dataset.

(a) Train

(b) Test seen

(c) Test unseen

Figure 6: Top 20 most common goal rooms in the ASKNAV dataset.

Table 5: NOROOM splits.

4. Hyperparameters

(a) Train

(b) Test seen

(c) Test unseen

Figure 7: Distribution of path lengths in the ASKNAV
dataset. Paths are computed by the shortest-path navigation
teacher.

Split

Number of data points Number of goals

Train
Dev seen
Dev unseen
Test seen
Test unseen

78,011
5,018
5,003
5,014
5,010

136,835
9,966
9,318
9,733
10,148

NOROOM dataset. The NOROOM data is generated using
a similar procedure described above. However, since room
types are not provided, each bucket is labeled with only the
environment and the object label. End-goals have the form
“Find [O]” instead of “Find [O] in [R]”. To ensure that the
number of goals of this dataset is comparable to that of the
ASKNAV dataset, we sample at most 12 start viewpoints per
object for each bucket (we sample ﬁve for ASKNAV). Table
5 summarizes the NOROOM dataset splits.

3. Time budget

We set the help-request budget B proportional to the
time budget ˆT , which is the (approximate) number of ac-
tions required to reach the goal viewpoints. During training,
for each data point, we set ˆT to be the rounded average num-
ber of actions needed to move the agent along the shortest
path from the start viewpoint to the goal viewpoints. During
evaluation, because the shortest path needs to be unknown
to the agent, we compute ˆT based on the approximate num-
ber of actions to go optimally from the room type of the
start viewpoint to the room type of the goal viewpoints2.
This quantity is estimated using the training set.

Concretely, suppose we evaluate the agent on a data
point d with starting viewpoint xstart
and goal viewpoints
{xend
d,i }. We deﬁne S as the multiset set of numbers of ac-
tions of training trajectories whose start and goal room types

d

2For the NOROOM dataset, we compute ˆT based on the approximate

number of actions to go from the start room type to the object type.

match those of d

S = {TRAJLEN (cid:0)xstart
, {xend
d(cid:48)
(cid:1) , r (cid:0)xend
(cid:1) = r (cid:0)xstart
r (cid:0)xstart
d(cid:48),i
d(cid:48)

d(cid:48),i(cid:48)}(cid:1) : d(cid:48) ∈ D,
(cid:1)}
(cid:1) = r (cid:0)xend

d,i

d

(11)

(12)

where TRAJLEN(., .) returns the number of actions to move
along the shortest path from a start viewpoint to a set of goal
viewpoints, r(.) returns the room type of a viewpoint, and
D is the training set.

Next, we calculate the 95% upper conﬁdence bound of

the mean number of actions

(cid:40)

T =

min(cupper, Lmax), if |S| > 0
if |S| = 0
Lmax,

(13)

cupper = mean(S) + 1.95 · stdErr(S)

mean(.) and stdErr(.) return the mean and standard error of
a multiset, respectively, and Lmax is a pre-deﬁned constant.
We then run the agent for ˆT = ROUND(T ) steps.

Table 6 summarizes hyperparameters used in our exper-
iments. The navigation module uses unidirectional single-
layer LSTMs as encoder and decoder. We initialize the en-
coder and the decoder by zero vectors. The help-requesting
module is a feed-forward neural network with one hidden
layer. We train the agent with Adam [35] for 105 itera-
tions, using a learning rate of 10−4 without decaying and
a batch size of 100. We regularize the agent with an L2-
norm weight of 5 × 10−4 and a dropout ratio of 0.5. Train-
ing a LEARNED model took about 17 hours on a machine
with a Titan Xp GPU and an Intel 4.00GHz CPU. The help-
requesting ratio (τ ) is 0.4 and the number of actions sug-
gested by the subgoal advisor (k) is 4. The deviation thresh-
old (δ), uncertainty threshold ((cid:15)), and non-moving threshold
(µ) are 8, 1.0, and 9, respectively. The success radius (d) is
ﬁxed at 2 meters. Both the navigation and help-requesting
modules are trained under the maximum log-likelihood ob-
jective, which maximizes the model-estimated probabilities
of actions suggested by the teacher. We evaluate each agent
with ﬁve different random seeds.

5. Qualitative Analysis

We analyze the behavior of an agent that is trained to
learn a help-requesting policy (LEARNED) and is evaluated
with a single random seed. This agent achieves a success
rate of 52.0% on TEST SEEN and 34.5% on TEST UNSEEN.
Overall, the success rate of the agent degrades as the tra-
jectory gets longer (Figure 9). The agent tends to ask for
help early (Figure 8), making more than half of its requests
on the ﬁrst 20% steps. As time advances, the number of
requests decreases. As expected, the agent tends to request

Hyperparameter

Navigation module

LSTM hidden size
Number of LSTM layers
Word embedding size
Navigation action embedding size
Help-requesting action embedding size
Budget embedding size
Image embedding size
Coverage vector size

Help-requesting module

Hidden size
Number of hidden layers
Activation function

Help-requesting teacher

Deviation threshold (δ)
Uncertainty threshold ((cid:15))
Non-moving threshold (µ)
Number of actions suggested by a subgoal (k)

Training

Optimizer
Number of training iterations
Learning rate
Learning rate decay
Batch size
Weight decay (L2-norm regularization)
Dropout ratio
Help-requesting ratio (τ )

Evaluation

Success radius (d)
Number of evaluating random seeds
Maximum time budget (Lmax)

Table 6: Hyperparameters.

Value

512
1
256
32
32
16
2048
10

512
1
RELU

8
1.0
9
4

Adam
105
10−4
No
100
5 × 10−4
0.5
0.4

2
5
25

help more early on TEST UNSEEN than on TEST SEEN. As
shown in Figure 10, the most identiﬁable objects have dis-
tinct invariant features (e.g., sink, curtain), whereas the least
identiﬁable objects greatly vary in shape and color (e.g.,
ﬁreplace, stool, armchair). Mirrors are the easiest objects
to detect in TEST SEEN, possibly because they are usu-
ally in small rooms (e.g., bathroom) and always reﬂect the
camera used by the Matterport3D data collector. On TEST
UNSEEN, they are more difﬁcult to ﬁnd because walking
to the containing-rooms is more challenging. Finding ob-
jects in bathrooms is less challenging because bathrooms
are usually small and have similar layouts and locations
among houses, whereas searching for objects in ofﬁces is
difﬁcult because the corresponding environments are usu-
ally workspaces that contain many similar-looking rooms
(Figure 11). Note that these rankings are subject to sam-
pling biases; for example, they favor objects (or rooms)
whose data points correspond to shorter paths.

Figure 8: Fraction of help requests made over (normalized)
time.

(a)

(b)

Figure 9: Success rate versus number of actions taken by
(a) the navigation teacher and (b) the agent. Error bars are
95% conﬁdence intervals.

Teacher

TEST SEEN

TEST UNSEEN

NONE
Rule (a), (e) (δ = 2)
Rule (a), (e) (δ = 4)
Rule (a), (e) (δ = 8)
Rule (b), (c), (d)
All rules

28.39 ± 0.00
30.36 ± 0.13
39.46 ± 0.05
30.71 ± 0.05
51.89 ± 0.24
52.09 ± 0.13

6.36 ± 0.00
8.49 ± 0.08
8.78 ± 0.04
5.64 ± 0.00
35.52 ± 0.29
34.50 ± 0.23

Table 7: Ablation study on the effectiveness of rules of the
help-requesting teacher. All numbers are success rates (%).
See section 6.2 for speciﬁcations of the rules.

Table 7 shows the effectiveness of different subsets of
rules of the help-requesting teacher. Using only rules (b),
(c), (d), which do not require learning because can be di-

poorly to unseen examples. We nevertheless include rules
(a) and (e) to illustrate that the help-requesting policy can be
taught rules that cannot be executed at test time by a teacher
that has access to ground-truth information. In general, im-
itating a help-requesting teacher allows us to easily transfer
domain knowledge from humans to the agent without re-
striction on the knowledge and on information required to
imitate it.

(a)

(b)

Figure 10: Top ﬁve objects with highest and lowest aver-
age success rates in (a) TEST SEEN and (b) TEST UNSEEN.
Numbers in parentheses are object frequencies. Only ob-
jects appearing more than 50 times are included. Error bars
are 95% conﬁdence intervals.

(a)

(b)

Figure 11: Top ﬁve goal rooms with highest and lowest av-
erage success rates in (a) TEST SEEN and (b) TEST UN-
SEEN. Numbers in parentheses are room frequencies. Only
rooms appearing more than 50 times are included. Error
bars are 95% conﬁdence intervals.

rectly computed at test time without ground-truth informa-
tion, is sufﬁcient to obtain a success rate comparable to
that of using all rules. Using rules (a) and (e), which re-
quire ground-truth information about the environment and
the task, slightly improves the success rate over not request-
ing. Rules (a) and (e) are in fact very difﬁcult to learn con-
sidering the small size of the Matterport3D dataset. Unfor-
tunately, at the time this research was conducted, the Mat-
terport3D simulator was one of the largest scale in the small
pool of indoor simulators with real scenes. The effective-
ness of rules (a) and (e) would be more visible in larger-
scale environments like Gibson [60] but it offered limited
object annotation. Another reason that makes it challenging
to learn rules (a) and (e) is the training-test condition mis-
match. Near the end of training, the agent has memorized
the training examples and rarely makes mistakes. The agent
is thus biased toward not requesting help and generalizes

