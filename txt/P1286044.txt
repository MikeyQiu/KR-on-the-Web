Learning Semantically and Additively Compositional
Distributional Representations

Ran Tian and Naoaki Okazaki and Kentaro Inui
Tohoku University, Japan
{tianran, okazaki, inui}@ecei.tohoku.ac.jp

6
1
0
2
 
n
u
J
 
8
 
 
]
L
C
.
s
c
[
 
 
1
v
1
6
4
2
0
.
6
0
6
1
:
v
i
X
r
a

Abstract

This paper connects a vector-based com-
position model
to a formal semantics,
the Dependency-based Compositional Se-
mantics (DCS). We show theoretical evi-
dence that the vector compositions in our
model conform to the logic of DCS. Ex-
perimentally, we show that vector-based
composition brings a strong ability to
calculate similar phrases as similar vec-
tors, achieving near state-of-the-art on a
wide range of phrase similarity tasks and
relation classiﬁcation; meanwhile, DCS
can guide building vectors for structured
queries that can be directly executed. We
evaluate this utility on sentence comple-
tion task and report a new state-of-the-art.

1

Introduction

A major goal of semantic processing is to map nat-
ural language utterances to representations that fa-
cilitate calculation of meanings, execution of com-
mands, and/or inference of knowledge. Formal
semantics supports such representations by deﬁn-
ing words as some functional units and combining
them via a speciﬁc logic. A simple and illustra-
tive example is the Dependency-based Composi-
tional Semantics (DCS) (Liang et al., 2013). DCS
composes meanings from denotations of words
(i.e. sets of things to which the words apply); say,
the denotations of the concept drug and the event
ban is shown in Figure 1b, where drug is a list
of drug names and ban is a list of the subject-
complement pairs in any ban event; then, a list of
banned drugs can be constructed by ﬁrst taking the
COMP column of all records in ban (projection
“πCOMP”), and then intersecting the results with
drug (intersection “∩”). This procedure deﬁned
how words can be combined to form a meaning.

Better yet, the procedure can be concisely illus-
trated by the DCS tree of “banned drugs” (Fig-
ure 1a), which is similar to a dependency tree but
possesses precise procedural and logical meaning
(Section 2). DCS has been shown useful in ques-
tion answering (Liang et al., 2013) and textual en-
tailment recognition (Tian et al., 2014).

Orthogonal to the formal semantics of DCS,
distributional vector representations are useful in
capturing lexical semantics of words (Turney and
Pantel, 2010; Levy et al., 2015), and progress
is made in combining the word vectors to form
meanings of phrases/sentences (Mitchell and La-
pata, 2010; Baroni and Zamparelli, 2010; Grefen-
stette and Sadrzadeh, 2011; Socher et al., 2012;
Paperno et al., 2014; Hashimoto et al., 2014).
However, less effort is devoted to ﬁnding a link
between vector-based compositions and the com-
position operations in any formal semantics. We
believe that if a link can be found, then symbolic
formulas in the formal semantics will be realized
by vectors composed from word embeddings, such
that similar things are realized by similar vectors;
meanwhile, vectors will acquire formal meanings
that can directly be used in execution or inference
process. Still, to ﬁnd a link is challenging because
any vector compositions that realize such a link
must conform to the logic of the formal semantics.
In this paper, we establish a link between
DCS and certain vector compositions, achieving
a vector-based DCS by replacing denotations of
words with word vectors, and realizing the compo-
sition operations such as intersection and projec-
tion as addition and linear mapping, respectively.
For example, to construct a vector for “banned
drugs”, one takes the word vector vban and mul-
tiply it by a matrix MCOMP, corresponding to the
projection πCOMP; then, one adds the result to the
word vector vdrug to realize the intersection opera-
tion (Figure 1c). We provide a method to train the

Figure 2: DCS tree for a sentence

date list of banned drugs, by sorting its dot prod-
ucts with answer vectors that are also learned by
our model (Figure 1d). This is due to the ability of
our approach to provide a language model that can
is
ﬁnd likely words to ﬁll in the blanks such as “
a banned drug” or “the drug
is banned by . . . ”.
A highlight is the calculation being done as if a
query is “executed” by the DCS tree of “banned
drugs”. We quantitatively evaluate this utility on
sentence completion task (Zweig et al., 2012) and
report a new state-of-the-art (Section 5.4).

2 DCS Trees

DCS composes meanings from denotations, or
sets of things to which words apply. A “thing”
(i.e. element of a denotation) is represented by
a tuple of features of the form Field=Value,
with a ﬁxed inventory of ﬁelds. For example, a
denotation ban might be a set of tuples ban =
{(SUBJ=Canada, COMP=Thalidomide), . . .},
in which each tuple records participants of a ban-
ning event (e.g. Canada banning Thalidomide).

Operations are applied to sets of things to gener-
ate new denotations, for modeling semantic com-
position. An example is the intersection of pet
and fish giving the denotation of “pet ﬁsh”. An-
other necessary operation is projection; by πN we
mean a function mapping a tuple to its value of
the ﬁeld N. For example, πCOMP(ban) is the value
set of the COMP ﬁelds in ban, which consists of
banned objects (i.e. {Thalidomide, . . .}).
In this
paper, we assume a ﬁeld ARG to be names of
things representing themselves, hence for example
πARG(drug) is the set of names of drugs.

For a value set V , we also consider inverse im-
N (V ) := {x | πN(x) ∈ V }. For example,

age π−1

D1 := π−1

SUBJ(πARG(man))

consists of all tuples of the form (SUBJ=x, . . .),
where x is a man’s name (i.e. x ∈ πARG(man)).
Thus, sell ∩ D1 denotes men’s selling events
(i.e. {(SUBJ=John, COMP=Aspirin), . . .} as in
Figure 2). Similarly, the denotation of “banned

Figure 1: (a) The DCS tree of “banned drugs”,
which controls (b) the calculation of its denota-
tion. In this paper, we learn word vectors and ma-
trices such that (c) the same calculation is realized
in distributional semantics. The constructed query
vector can be used to (d) retrieve a list of coarse-
grained candidate answers to that query.

word vectors and linear mappings (i.e. matrices)
jointly from unlabeled corpora.

The rationale for our model is as follows. First,
recent research has shown that additive composi-
tion of word vectors is an approximation to the sit-
uation where two words have overlapping context
(Tian et al., 2015); therefore, it is suitable to im-
plement an “and” or intersection operation (Sec-
tion 3). We design our model such that the resulted
distributional representations are expected to have
additive compositionality. Second, when intersec-
tion is realized as addition, it is natural to imple-
ment projection as linear mapping, as suggested
by the logical interactions between the two oper-
ations (Section 3). Experimentally, we show that
vectors and matrices learned by our model exhibit
favorable characteristics as compared with vectors
trained by GloVe (Pennington et al., 2014) or those
learned from syntactic dependencies (Section 5.1).
Finally, additive composition brings our model
a strong ability to calculate similar vectors for
similar phrases, whereas syntactic-semantic roles
(e.g. SUBJ, COMP) can be distinguished by dif-
ferent projection matrices (e.g. MSUBJ, MCOMP).
We achieve near state-of-the-art performance on a
wide range of phrase similarity tasks (Section 5.2)
and relation classiﬁcation (Section 5.3).

Furthermore, we show that a vector as con-
structed above for “banned drugs” can be used as
a query vector to retrieve a coarse-grained candi-

Figure 3: DCS trees in this work

drugs” as in Figure 1b is formally written as

D2 := drug ∩ π−1

ARG(πCOMP(ban)),

Hence the following denotation

D3 := sell ∩ D1 ∩ π−1

COMP(πARG(D2))

consists of selling events such that the SUBJ is a
man and the COMP is a banned drug.

The calculation above can proceed in a recur-
sive manner controlled by DCS trees. The DCS
tree for the sentence “a man sells banned drugs”
is shown in Figure 2. Formally, a DCS tree is de-
ﬁned as a rooted tree in which nodes are denota-
tions of content words and edges are labeled by
ﬁelds at each ends. Assume a node x has children
y1, . . . , yn, and the edges (x, y1), . . . , (x, yn) are
labeled by (P1, L1), . . . , (Pn, Ln),
respectively.
Then, the denotation [[x]] of the subtree rooted at
x is recursively calculated as

[[x]] := x ∩

π−1
Pi

(πLi([[yi]])).

(1)

n
(cid:92)

i=1

As a result, the denotation of the DCS tree in Fig-
ure 2 is the denotation D3 of “a man sells banned
drugs” as calculated above. DCS can be further
extended to handle phenomena such as quantiﬁers
or superlatives (Liang et al., 2013; Tian et al.,
2014). In this paper, we focus on the basic ver-
sion, but note that it is already expressive enough
to at least partially capture the meanings of a large
portion of phrases and sentences.

DCS trees can be learned from question-answer
pairs and a given database of denotations (Liang
et al., 2013), or they can be extracted from de-
pendency trees if no database is speciﬁed, by tak-
ing advantage of the observation that DCS trees
are similar to dependency trees (Tian et al., 2014).
We use the latter approach, obtaining DCS trees by
rule-based conversion from universal dependency
(UD) trees (McDonald et al., 2013). Therefore,
nodes in a DCS tree are content words in a UD
tree, which are in the form of lemma-POS pairs

(Figure 3). The inventory of ﬁelds is designed
to be ARG, SUBJ, COMP, and all prepositions.
Prepositions are unlike content words which de-
note sets of things, but act as relations which we
treat similarly as SUBJ and COMP. For example,
a prepositional phrase attached to a verb (e.g. play
on the grass) is treated as in Figure 3a. The pres-
ence of two ﬁeld labels on each edge of a DCS
tree makes it convenient for modeling semantics in
several cases, such as a relative clause (Figure 3b).

3 Vector-based DCS

For any content word w, we use a query vector vw
to model its denotation, and an answer vector uw
to model a prototypical element in that denotation.
Query vector v and answer vector u are learned
such that exp(v · u) is proportional to the prob-
ability of u answering the query v. The learning
source is a collection of DCS trees, based on the
idea that the DCS tree of a declarative sentence
usually has non-empty denotation. For exam-
ple, “kids play” means there exists some kid who
plays. Consequently, some element in the play
denotation belongs to π−1
SUBJ(πARG(kid)), and
some element in the kid denotation belongs to
π−1
ARG(πSUBJ(play)). This is a signal to increase
the dot product of uplay and the query vector of
π−1
SUBJ(πARG(kid)), as well as the dot product of
ukid and the query vector of π−1
ARG(πSUBJ(play)).
When optimized on a large corpus, the “typical”
elements of play and kid should be learned by
uplay and ukid, respectively. In general, one has
Theorem 1 Assume the denotation of a DCS tree
is not empty. Given any path from node x to
y, assume edges along the path are labeled by
(P, L), . . . , (K, N). Then, an element in the deno-
tation y belongs to π−1
L (πP(x) . . .).
Therefore, for any two nodes in a DCS tree, the
path from one to another forms a training exam-
ple, which signals increasing the dot product of
the corresponding query and answer vectors.

N (πK(. . . (π−1

It is noteworthy that the above formalization
happens to be closely related to the skip-gram
model (Mikolov et al., 2013b). The skip-gram
learns a target vector vw and a context vector uw
for each word w. It assumes the probability of a
word y co-occurring with a word x in a context
window is proportional to exp(vx · uy). Hence,
if x and y co-occur within a context window, then
one gets a signal to increase vx · uy. If the con-
text window is taken as the same DCS tree, then

the learning of skip-gram and vector-based DCS
will be almost the same, except that the target vec-
tor vx becomes the query vector v, which is no
longer assigned to the word x but the path from
x to y in the DCS tree (e.g. the query vector for
π−1
SUBJ(πARG(kid)) instead of vkid). Therefore,
our model can also be regarded as extending skip-
gram to take account of the changes of meanings
caused by different syntactic-semantic roles.

Additive Composition Word vectors trained by
skip-gram are known to be semantically additive,
such as exhibited in word analogy tasks. An effect
of adding up two skip-gram vectors is further ana-
lyzed in Tian et al. (2015). Namely, the target vec-
tor vw can be regarded as encoding the distribution
of context words surrounding w. If another word
x is given, vw can be decomposed into two parts,
one encodes context words shared with x, and an-
other encodes context words not shared. When vw
and vx are added up, the non-shared part of each of
them tend to cancel out, because non-shared parts
have nearly independent distributions. As a result,
the shared part gets reinforced. An error bound
is derived to estimate how close 1
2 (vw + vx) gets
to the distribution of the shared part. We can see
the same mechanism exists in vector-based DCS.
In a DCS tree, two paths share a context word
if they lead to a same node y; semantically, this
means some element in the denotation y belongs
to both denotations of the two paths (e.g. given the
sentence “kids play balls”, π−1
SUBJ(πARG(kid))
and π−1
COMP(πARG(ball)) both contain a playing
event whose SUBJ is a kid and COMP is a ball).
Therefore, addition of query vectors of two paths
approximates their intersection because the shared
context y gets reinforced.

Projection Generally, for any two denotations
X1, X2 and any projection πN, we have

πN(X1 ∩ X2) ⊆ πN(X1) ∩ πN(X2).

(2)

And the “⊆” can often become “=”, for example
when πN is a one-to-one map or X1 = π−1
N (V )
for some value set V . Therefore, if intersection
is realized by addition, it will be natural to realize
projection by linear mapping because

(v1 + v2)MN = v1MN + v2MN

(3)

holds for any vectors v1, v2 and any matrix MN,
which is parallel to (2). If πN is realized by a ma-
trix MN, then π−1
N should correspond to the in-
verse matrix M −1
N , because πN(π−1
N (V )) = V for

any value set V . So we have realized all composi-
tion operations in DCS.

Query vector of a DCS tree Now, we can deﬁne
the query vector of a DCS tree as parallel to (1):

v[[x]] := vx +

v[[yi]]MLiM −1
Pi

.

(4)

1
n

n
(cid:88)

i=1

4 Training

As described in Section 3, vector-based DCS as-
signs a query vector vw and an answer vector uw
to each content word w. And for each ﬁeld N, it
assigns two matrices MN and M −1
N . For any path
from node x to y sampled from a DCS tree, assume
the edges along are labeled by (P, L), . . . , (K, N).
Then, the dot product vxMPM −1
N · uy
gets a signal to increase.

. . . MKM −1

L

. . . MKM −1

Formally, we adopt the noise-contrastive esti-
mation (Gutmann and Hyv¨arinen, 2012) as used
in the skip-gram model, and mix the paths sam-
pled from DCS trees with artiﬁcially generated
noise. Then, σ(vxMPM −1
N ·uy) mod-
L
els the probability of a training example coming
from DCS trees, where σ(θ) = 1/{1 + exp(−θ)}
is the sigmoid function. The vectors and matri-
ces are trained by maximizing the log-likelihood
of the mixed data. We use stochastic gradient de-
scent (Bottou, 2012) for training. Some important
settings are discussed below.

2

. . . M2l−1M −1
2l

Noise For any vxM1M −1
· uy
obtained from a path of a DCS tree, we generate
noise by randomly choosing an index i ∈ [2, 2l],
and then replacing Mj or M −1
(∀j ≥ i) and uy by
MN(j) or M −1
N(j) and uz, respectively, where N(j)
and z are independently drawn from the marginal
(i.e. unigram) distributions of ﬁelds and words.

j

Update For each data point, when i is the chosen
index above for generating noise, we view indices
j < i as the ”target” part, and j >= i as the ”con-
text”, which is completely replaced by the noise,
as an analogous to the skip-gram model. Then,
at each step we only update one vector and one
matrix from each of the target, context, and noise
part; more speciﬁcally, we only update vx, Mi−1
or M −1
N(i), uy and uz,
at the step. This is much faster than always updat-
ing all matrices.

i−1, Mi or M −1

, MN(i) or M −1

i

Initialization Matrices are initialized as 1
2 (I +
G), where I is the identity matrix; and G and all

Table 1: Top 10 similar words to “book/N”

ARG(πSUBJ(learn)) π−1
π−1

ARG(πCOMP(learn)) π−1

GloVe
books
author
published
novel
memoir
wrote
biography
autobiography
essay
illustrated

no matrix
essay/N
novel/N
memoir/N
books/N
autobiography/N
non-ﬁction/J
reprint/V
publish/V
republish/V
chapbook/N

vecDCS
novel/N
essay/N
anthology/N
publication/N
memoir/N
poem/N
autobiography/N
story/N
pamphlet/N
tale/N

vecUD
essay/N
novel/N
article/N
anthology/N
poem/N
autobiography/N
publication/N
journal/N
memoir/N
pamphlet/N

vectors are initialized with i.i.d. Gaussians of vari-
ance 1/d, where d is the vector dimension. We
ﬁnd that the diagonal component I is necessary to
bring information from vx to uy, whereas the ran-
domness of G makes convergence faster. M −1
N is
initialized as the transpose of MN.

Learning Rate We ﬁnd that the initial learning
rate for vectors can be set to 0.1. But for matrices,
it should be less than 0.0005 otherwise the model
diverges. For stable training, we rescale gradients
when their norms exceed a threshold.

d tr(M ⊥

d tr(M −1

Regularizer During training, MN and M −1
N are
treated as independent matrices. However, we use
the regularizer γ(cid:107)M −1
N MN − 1
N MN)I(cid:107)2 to
drive M −1
N close to the inverse of MN.1 We also
N MN − 1
use κ(cid:107)M ⊥
N MN)I(cid:107)2 to prevent MN
from having too different scales at different direc-
tions (i.e., to drive MN close to orthogonal). We
set γ = 0.001 and κ = 0.0001. Despite the rather
weak regularizer, we ﬁnd that M −1
N can be learned
to be exactly the inverse of MN, and MN can ac-
tually be an orthogonal matrix, showing some se-
mantic regularity (Section 5.1).

5 Experiments

For training vector-based DCS, we use Wikipedia
Extractor2 to extract texts from the 2015-12-01
dump of English Wikipedia3. Then, we use Stan-
ford Parser4 (Klein and Manning, 2003) to parse
all sentences and convert the UD trees into DCS
trees by handwritten rules. We assign a weight to
each path of the DCS trees as follows.

1Problem with the naive regularizer (cid:107)M −1M − I(cid:107)2 is
that, when the scale of M goes larger, it will drive M −1
smaller, which may lead to degeneration. So we scale I ac-
cording to the trace of M −1M .

2http://medialab.di.unipi.it/wiki/

Wikipedia_Extractor

3https://dumps.wikimedia.org/enwiki/
4http://nlp.stanford.edu/software/

lex-parser.shtml

SUBJ(πARG(house)) π−1
π−1

victorian/J
stand/V
vacant/J
18th-century/J
historic/J
old/J
georgian/J
local/J
19th-century/J
tenement/J

teacher/N
skill/N
he/P
she/P
therapist/N
student/N
they/P
mother/N
lesson/N
father/N

COMP(πARG(house))
build/V
rent/V
leave/V
burn down/V
remodel/V
demolish/V
restore/V
renovate/V
rebuild/V
construct/V

skill/N
lesson/N
technique/N
experience/N
ability/N
something/N
knowledge/N
language/N
opportunity/N
instruction/N

π−1

ARG(πin(house))
sit/V
house/N
stand/V
live/V
hang/V
seat/N
stay/V
serve/V
reside/V
hold/V
about(πARG(learn))
otherness/N
intimacy/N
femininity/N
self-awareness/N
life/N
self-expression/N
sadomasochism/N
emptiness/N
criminality/N
masculinity/N

Table 2: Top 10 answers of high dot products

For any path P passing through k intermediate
nodes of degrees n1, . . . , nk, respectively, we set

Weight(P ) :=

(5)

k
(cid:89)

i=1

1
ni − 1

.

Note that ni ≥ 2 because there is a path P passing
through the node; and Weight(P ) = 1 if P con-
sists of a single edge. The equation (5) is intended
to degrade long paths which pass through several
high-valency nodes. We use a random walk algo-
rithm to sample paths such that the expected times
a path is sampled equals its weight. As a result,
the sampled path lengths range from 1 to 19, av-
erage 2.1, with an exponential tail. We convert all
words which are sampled less than 1000 times to
*UNKNOWN*/POS, and all prepositions occurring
less than 10000 times to an *UNKNOWN* ﬁeld.
As a result, we obtain a vocabulary of 109k words
and 211 ﬁeld names.

Using the sampled paths, vectors and matrices
are trained as in Section 4 (vecDCS). The vector
dimension is set to d = 250. We compare with
three baselines: (i) all matrices are ﬁxed to identity
(“no matrix”), in order to investigate the effects
of meaning changes caused by syntactic-semantic
roles and prepositions; (ii) the regularizer enforc-
ing M −1
N to be actually the inverse matrix of MN is
set to γ = 0 (“no inverse”), in order to investigate
the effects of a semantically motivated constraint;
and (iii) applying the same training scheme to UD
trees directly, by modeling UD relations as matri-
ces (“vecUD”). In this case, one edge is assigned
one UD relation rel, so we implement the transfor-

vecDCS

-no matrix
-no inverse

vecUD
GloVe
Grefenstette and Sadrzadeh (2011)
Blacoe and Lapata (2012):RAE
Grefenstette (2013a)
Paperno et al. (2014)
Hashimoto et al. (2014):Waddnl
Kartsaklis and Sadrzadeh (2014)

AN
0.51
0.52
0.47
0.44
0.41
-
0.31
-
-
0.48
-

NN
0.49
0.46
0.43
0.46
0.47
-
0.30
-
-
0.40
-

VO
0.41
0.42
0.38
0.41
0.41
-
0.28
-
-
0.39
-

SVO
0.62
0.62
0.58
0.58
0.60
-
-
-
-

0.43

GS11
0.29
0.29
0.28
0.25
0.23
0.21
-
-
-
0.34
0.41

GS12
0.33
0.33
0.33
0.25
0.17
-
-
0.27
0.36
-
-

Table 3: Spearman’s ρ on phrase similarity

mation from child to parent by Mrel, and from par-
ent to child by M −1
rel . The same hyper-parameters
are used to train vecUD. By comparing vecDCS
with vecUD we investigate if applying the seman-
tics framework of DCS makes any difference. Ad-
ditionally, we compare with the GloVe (6B, 300d)
vector5 (Pennington et al., 2014). Norms of all
word vectors are normalized to 1 and Frobenius
norms of all matrices are normalized to

√

d.

5.1 Qualitative Analysis

We observe several special properties of the vec-
tors and matrices trained by our model.

Words are clustered by POS In terms of cosine
similarity, word vectors trained by vecDCS and
vecUD are clustered by POS tags, probably due
to their interactions with matrices during training.
This is in contrast to the vectors trained by GloVe
or “no matrix” (Table 1).

Matrices show semantic regularity Matrices
learned for ARG, SUBJ and COMP are exactly
orthogonal, and some most frequent prepositions6
are remarkably close. For these matrices, the cor-
responding M −1 also exactly converge to their
It suggests regularities in the semantic
inverse.
space, especially because orthogonal matrices pre-
serve cosine similarity – if MN is orthogonal, two
words x, y and their projections πN(x), πN(y) will
have the same similarity measure, which is seman-
tically reasonable. In contrast, matrices trained by
vecUD are only orthogonal for three UD relations,
namely conj, dep and appos.

Words transformed by matrices To illustrate
the matrices trained by vecDCS, we start from the
query vectors of two words, house and learn,

applying different matrices to them, and show
the 10 answer vectors of the highest dot prod-
ucts (Tabel 2). These are the lists of likely words
take house as a subject, take house as a
which:
in house”, serve as a
complement, ﬁlls into “
subject of learn, serve as a complement of learn,
and ﬁlls into “learn about
”, respectively. As the
table shows, matrices in vecDCS are appropriately
learned to map word vectors to their syntactic-
semantic roles.

5.2 Phrase Similarity

To test if vecDCS has the composition ability to
calculate similar things as similar vectors, we con-
duct evaluation on a wide range of phrase similar-
ity tasks. In these tasks, a system calculates sim-
ilarity scores for pairs of phrases, and the perfor-
mance is evaluated as its correlation with human
annotators, measured by Spearman’s ρ.

Datasets Mitchell and Lapata (2010) create
datasets7 for pairs of three types of two-word
phrases: adjective-nouns (AN) (e.g. “black hair”
and “dark eye”), compound nouns (NN) (e.g. “tax
charge” and “interest rate”) and verb-objects (VO)
(e.g. “ﬁght war” and “win battle”). Each dataset
consists of 108 pairs and each pair is annotated by
18 humans (i.e., 1,944 scores in total). Similarity
scores are integers ranging from 1 to 7. Another
dataset8 is created by extending VO to Subject-
Verb-Object (SVO), and then assessing similari-
ties by crowd sourcing (Kartsaklis and Sadrzadeh,
2014). The dataset GS11 created by Grefen-
stette and Sadrzadeh (2011) (100 pairs, 25 an-
notators) is also of the form SVO, but in each
pair only the verbs are different (e.g. “man pro-

5http://nlp.stanford.edu/projects/

s0453356/

glove/

8http://www.cs.ox.ac.uk/activities/

6of, in, to, for, with, on, as, at, from

compdistmeaning/

7http://homepages.inf.ed.ac.uk/

Message-Topic(e1, e2)
Message-Topic(e1, e2)
Message-Topic(e1, e2)
Component-Whole(e2, e1)
Message-Topic(e1, e2)

It is a monthly [report]1 providing [opinion]2 and advice on current United States government contract issues.
The [report]1 gives an account of the silvicultural [work]2 done in Africa, Asia, Australia, South American and the Caribbean.
NUS today responded to the Government’s [announcement]1 of the long-awaited [review]2 of university funding.
The [review]1 published political [commentary]2 and opinion, but even more than that.
It is a 2004 [book]1 criticizing the political and linguistic [writings]2 of Noam Chomsky.

Table 4: Similar training instances clustered by cosine similarities between features

vecDCS

-no matrix
-no inverse

vecUD
GloVe
Socher et al. (2012)

+3 features

dos Santos et al. (2015)
Xu et al. (2015)

81.2
69.2
79.7
69.2
74.1
79.1
82.4
84.1
85.6

Table 5: F1 on relation classiﬁcation

less, we note that “no matrix” performs as good as
vecDCS, suggesting that meaning changes caused
by syntactic-semantic roles might not be major
factors in these datasets, because the syntactic-
semantic relations are all ﬁxed in each dataset.

5.3 Relation Classiﬁcation

In a relation classiﬁcation task, the relation be-
tween two words in a sentence needs to be clas-
siﬁed; we expect vecDCS to perform better than
“no matrix” on this task because vecDCS can dis-
tinguish the different syntactic-semantic roles of
the two slots the two words ﬁt in. We conﬁrm this
conjecture in this section.

Dataset We use the dataset of SemEval-2010
Task 8 (Hendrickx et al., 2009), in which 9 di-
rected relations (e.g. Cause-Effect) and 1 undi-
rected relation Other are annotated, 8,000 in-
stances for training and 2,717 for test. Perfor-
mance is measured by the 9-class direction-aware
Macro-F1 score excluding Other class.

Our method For any sentence with two words
marked as e1 and e2, we construct the DCS tree
of the sentence, and take the subtree T rooted at
the common ancestor of e1 and e2. We construct
four vectors from T , namely: the query vector for
the subtree rooted at e1 (resp. e2), and the query
vector of the DCS tree obtained from T by re-
rooting it at e1 (resp. e2) (Figure 4). The four
vectors are normalized and concatenated to form
the only feature used to train a classiﬁer. For ve-
cUD, we use the corresponding vectors calculated
from UD trees. For GloVe, we use the word vec-
tor of e1 (resp. e2), and the sum of vectors of all
words within the span [e1, e2) (resp. (e1, e2]) as

Figure 4: For “[smoke]1 cause ﬂight [delay]2”, we
construct (a)(b) from subtrees, and (c)(d) from re-
rooted trees, to form 4 query vectors as feature.

vide/supply money”). The dataset GS12 described
in Grefenstette (2013a) (194 pairs, 50 annotators)
is of the form Adjective-Noun-Verb-Adjective-
Noun (e.g. “local family run/move small hotel”),
where only verbs are different in each pair.

Our method We calculate the cosine similarity
of query vectors corresponding to phrases. For ex-
ample, the query vector for “ﬁght war” is calcu-
lated as vwarMARGM −1
COMP + vﬁght. For vecUD
we use Mnsubj and Mdobj instead of MSUBJ and
MCOMP, respectively. For GloVe we use additive
compositions.

Results As shown in Table 3, vecDCS is com-
petitive on AN, NN, VO, SVO and GS12, con-
sistently outperforming “no inverse”, vecUD and
GloVe, showing strong compositionality. The
weakness of “no inverse” suggests that relaxing
the constraint of inverse matrices may hurt com-
positionaly, though our preliminary examination
on word similarities did not ﬁnd any difference.
The GS11 dataset appears to favor models that can
learn from interactions between the subject and
object arguments, such as the non-linear model
Waddnl in Hashimoto et al. (2014) and the en-
tanglement model in Kartsaklis and Sadrzadeh
(2014). However, these models do not show par-
ticular advantages on other datasets. The recur-
sive autoencoder (RAE) proposed in Socher et al.
(2011) shares an aspect with vecDCS as to con-
struct meanings from parse trees. It is tested by
Blacoe and Lapata (2012) for compositionality,
where vecDCS appears to be better. Neverthe-

“banned drugs”
drug/N
marijuana/N
cannabis/N
trafﬁcking/N
thalidomide/N
smoking/N
narcotic/N
botox/N
doping/N

“banned movies”
bratz/N
porn/N
indecent/N
blockbuster/N
movie/N
idiots/N
blacklist/N
grindhouse/N
doraemon/N

“banned books”
publish/N
unfair/N
obscene/N
samizdat/N
book/N
responsum/N
illegal/N
reclaiming/N
redbook/N

vecDCS

-no matrix
-no inverse

vecUD
N-gram (Various)
Zweig et al. (2012)
Mnih and Teh (2012)
Gubbins and Vlachos (2013)
Mikolov et al. (2013a)

50
60
46
31
39-41
52
55
50
55

Table 6: Answers for composed query vectors

Table 7: Accuracy (%) on sentence completion

the four vectors. Classiﬁer is SVM9 with RBF ker-
nel, C = 2 and Γ = 0.25. The hyper-parameters
are selected by 5-fold cross validation.

Results VecDCS outperforms baselines on rela-
tion classiﬁcation (Table 5). It makes 16 errors in
misclassifying the direction of a relation, as com-
pared to 144 such errors made by “no matrix”, 23
by “no inverse”, 30 by vecUD, and 161 by GloVe.
This suggests that models with syntactic-semantic
transformations (i.e. vecDCS, “no inverse”, and
vecUD) are indeed good at distinguishing the dif-
ferent roles played by e1 and e2. VecDCS scores
moderately lower than the state-of-the-art (Xu et
al., 2015), however we note that these results are
achieved by adding additional features and train-
ing task-speciﬁc neural networks (dos Santos et
al., 2015; Xu et al., 2015). Our method only
uses features constructed from unlabeled corpora.
From this point of view, it is comparable to the
MV-RNN model (without features) in Socher et
al. (2012), and vecDCS actually does better. Ta-
ble 4 shows an example of clustered training in-
stances as assessed by cosine similarities between
their features. It suggests that the features used in
our method can actually cluster similar relations.

5.4 Sentence Completion

If vecDCS can compose query vectors of DCS
trees, one should be able to “execute” the vec-
tors to get a set of answers, as the original DCS
trees can do. This is done by taking dot prod-
ucts with answer vectors and then ranking the an-
swers. Examples are shown in Table 6. Since
query vectors and answer vectors are trained from
unlabeled corpora, we can only obtain a coarse-
grained candidate list. However, it is noteworthy
that despite a common word “banned” shared by
the phrases, their answer lists are largely different,
suggesting that composition actually can be done.
Moreover, some words indeed answer the queries

(e.g. Thalidomide for “banned drugs” and Samiz-
dat for “banned books”).

Quantitatively, we evaluate this utility of exe-
cuting queries on the sentence completion task. In
this task, a sentence is presented with a blank that
need to be ﬁlled in. Five possible words are given
as options for each blank, and a system needs to
choose the correct one. The task can be viewed as
a coarse-grained question answering or an evalua-
tion for language models (Zweig et al., 2012). We
use the MSR sentence completion dataset10 which
consists of 1,040 test questions and a corpus for
training language models. We train vecDCS on
this corpus and use it for evaluation.

Results As shown in Table 7, vecDCS scores
better than the N-gram model and demonstrates
promising performance. However, to our surprise,
“no matrix” shows an even better result which is
the new state-of-the-art. Here we might be fac-
ing the same problem as in the phrase similar-
ity task (Section 5.2); namely, all choices in a
question ﬁll into the same blank and the same
syntactic-semantic role, so the transforming matri-
ces in vecDCS might not be able to distinguish dif-
ferent choices; on the other hand, vecDCS would
suffer more from parsing and POS-tagging errors.
Nonetheless, we believe the result by “no matrix”
reveals a new horizon of sentence completion, and
suggests that composing semantic vectors accord-
ing to DCS trees could be a promising direction.

6 Discussion

We have demonstrated a way to link a vector com-
position model to a formal semantics, combining
the strength of vector representations to calculate
phrase similarities, and the strength of formal se-
mantics to build up structured queries. In this sec-
tion, we discuss several lines of previous research
related to this work.

9https://www.csie.ntu.edu.tw/˜cjlin/

10http://research.microsoft.com/en-us/

libsvm/

projects/scc/

Logic and Distributional Semantics Logic is
necessary for implementing the functional aspects
of meaning and organizing knowledge in a struc-
tured and unambiguous way.
In contrast, distri-
butional semantics provides an elegant methodol-
ogy for assessing semantic similarity and is well
suited for learning from data. There have been re-
peated calls for combining the strength of these
two approaches (Coecke et al., 2010; Baroni et al.,
2014; Liang and Potts, 2015), and several systems
(Lewis and Steedman, 2013; Beltagy et al., 2014;
Tian et al., 2014) have contributed to this direc-
tion. In the remarkable work by Beltagy et al. (to
appear), word and phrase similarities are explicitly
transformed to weighted logical rules that are used
in a probabilistic inference framework. However,
this approach requires considerable amount of en-
gineering, including the generation of rule candi-
dates (e.g. by aligning sentence fragments), con-
verting distributional similarities to weights, and
efﬁciently handling the rules and inference. What
if the distributional representations are equipped
with a logical interface, such that the inference
can be realized by simple vector calculations? We
have shown it possible to realize semantic com-
position; we believe this may lead to signiﬁcant
simpliﬁcation of the system design for combining
logic and distributional semantics.

Compositional Distributional Models There
has been active exploration on how to combine
word vectors such that adequate phrase/sentence
similarities can be assessed (Mitchell and Lapata,
2010, inter alia), and there is nothing new in us-
ing matrices to model changes of meanings. How-
ever, previous model designs mostly rely on lin-
guistic intuitions (Paperno et al., 2014, inter alia),
whereas our model has an exact logic interpreta-
tion. Furthermore, by using additive composition
we enjoy a learning guarantee (Tian et al., 2015).

Vector-based Logic Models This work also
shares the spirit with Grefenstette (2013b) and
Rocktaeschel et al. (2014), in exploring vector cal-
culations that realize logic operations. However,
the previous works did not specify how to inte-
grate contextual distributional information, which
is necessary for calculating semantic similarity.

Formal Semantics Our model
implements a
logic capable of semantic com-
fragment of
position,
largely due to the simple framework
of Dependency-based Compositional Semantics

(Liang et al., 2013). It ﬁts in a long tradition of
logic-based semantics (Montague, 1970; Dowty
et al., 1981; Kamp and Reyle, 1993), with exten-
sive studies on extracting semantics from syntactic
representations such as HPSG (Copestake et al.,
2001; Copestake et al., 2005) and CCG (Baldridge
and Kruijff, 2002; Bos et al., 2004; Steedman,
2012; Artzi et al., 2015; Mineshima et al., 2015).

Logic for Natural Language Inference The
pursue of a logic more suitable for natural lan-
guage inference is also not new.
For exam-
ple, MacCartney and Manning (2008) has imple-
mented a model of natural logic (Lakoff, 1970).
We would not reach the current formalization of
logic of DCS without reading the work by Cal-
vanese et al. (1998), which is an elegant formal-
ization of database semantics in description logic.

Semantic Parsing DCS-related representations
have been actively used in semantic parsing and
we see potential in applying our model. For ex-
ample, Berant and Liang (2014) convert λ-DCS
queries to canonical utterances and assess para-
phrases at the surface level; an alternative could
be using vector-based DCS to bring distributional
similarity directly into calculation of denotations.
We also borrow ideas from previous work, for ex-
ample our training scheme is similar to Guu et al.
(2015) in using paths and composition of matri-
ces, and our method is similar to Poon and Domin-
gos (2009) in building structured knowledge from
clustering syntactic parse of unlabeled data.

Further Applications Regarding the usability
of distributional representations learned by our
model, a strong point is that the representation
takes into account syntactic/structural information
of context. Unlike several previous models (Pad´o
and Lapata, 2007; Levy and Goldberg, 2014;
Pham et al., 2015), our approach learns matrices
at the same time that can extract the information
according to different syntactic-semantic roles. A
related application is selectional preference (Ba-
roni and Lenci, 2010; Lenci, 2011; Van de Cruys,
2014), wherein our model might has potential for
smoothly handling composition.

Reproducibility Find our code at https://
github.com/tianran/vecdcs

Acknowledgments This work was supported by
CREST, JST. We thank the anonymous reviewers
for their valuable comments.

References

[Artzi et al.2015] Yoav Artzi, Kenton Lee, and Luke
Zettlemoyer. 2015. Broad-coverage ccg semantic
parsing with amr. In Proceedings of EMNLP.

[Baldridge and Kruijff2002] Jason

and
Geert-Jan Kruijff. 2002. Coupling ccg and hybrid
In Proceedings of
logic dependency semantics.
ACL.

Baldridge

[Baroni and Lenci2010] Marco Baroni and Alessandro
Lenci. 2010. Distributional memory: A general
framework for corpus-based semantics. Computa-
tional Linguistics, 36(4).

[Baroni and Zamparelli2010] Marco

and
Roberto Zamparelli. 2010. Nouns are vectors, ad-
jectives are matrices: Representing adjective-noun
constructions in semantic space. In Proceedings of
EMNLP.

Baroni

[Baroni et al.2014] Marco Baroni, Raffaella Bernardi,
and Roberto Zamparelli. 2014. Frege in space: A
program for compositional distributional semantics.
Linguistic Issues in Language Technology, 9(6).

[Beltagy et al.2014] Islam Beltagy, Katrin Erk, and
Raymond Mooney. 2014. Probabilistic soft logic
In Proceedings of
for semantic textual similarity.
ACL.

[Beltagy et al.to appear] Islam Beltagy, Stephen Roller,
Pengxiang Cheng, Katrin Erk, and Raymond J.
Mooney.
to appear. Representing meaning with a
combination of logical form and vectors. Computa-
tional Linguistics, special issue on formal distribu-
tional semantics.

[Berant and Liang2014] Jonathan Berant and Percy
Liang. 2014. Semantic parsing via paraphrasing.
In Proceedings of ACL.

[Blacoe and Lapata2012] William Blacoe and Mirella
Lapata. 2012. A comparison of vector-based rep-
resentations for semantic composition. In Proceed-
ings of EMNLP-CoNLL.

[Bos et al.2004] Johan Bos, Stephen Clark, Mark
Steedman, James R. Curran, and Julia Hockenmaier.
2004. Wide-coverage semantic representations from
a ccg parser. In Proceedings of ICCL.

[Bottou2012] L´eon Bottou. 2012. Stochastic gradient
descent tricks. In Gr´egoire Montavon, Genevi`eve B.
Orr, and Klaus-Robert M¨uller, editors, Neural Net-
works: Tricks of the Trade. Springer, Berlin.

[Calvanese et al.1998] Diego Calvanese, Giuseppe De
Giacomo, and Maurizio Lenzerini. 1998. On the
decidability of query containment under constraints.
In Proceedings of the 17th ACM SIGACT SIGMOD
SIGART Symposium on Principles of Database Sys-
tems (PODS98).

[Coecke et al.2010] Bob

Mehrnoosh
Sadrzadeh, and Stephen Clark. 2010. Mathematical
foundations
for a compositional distributional
model of meaning. Linguistic Analysis.

Coecke,

[Copestake et al.2001] Ann Copestake, Alex Las-
2001. An algebra
construction in constraint-based

carides, and Dan Flickinger.
for
grammars. In Proceedings of ACL.

semantic

[Copestake et al.2005] Ann Copestake, Dan Flickinger,
Carl Pollard, and Ivan A. Sag. 2005. Minimal re-
cursion semantics: An introduction. Research on
Language and Computation, 3(2-3).

[dos Santos et al.2015] Cicero dos Santos, Bing Xiang,
and Bowen Zhou. 2015. Classifying relations by
ranking with convolutional neural networks. In Pro-
ceedings of ACL-IJCNLP.

[Dowty et al.1981] David R. Dowty, Robert E. Wall,
and Stanley Peters. 1981. Introduction to Montague
Semantics. Springer Netherlands.

[Grefenstette and Sadrzadeh2011] Edward Grefenstette
2011. Experimen-
and Mehrnoosh Sadrzadeh.
tal support for a categorical compositional distri-
In Proceedings of
butional model of meaning.
EMNLP.

[Grefenstette2013a] Edward Grefenstette.

2013a.
Category-Theoretic Quantitative Compositional
Distributional Models of Natural Language Seman-
tics. PhD thesis.

[Grefenstette2013b] Edward Grefenstette. 2013b. To-
wards a formal distributional semantics: Simulat-
ing logical calculi with tensors. In Proceedings of
*SEM.

[Gubbins and Vlachos2013] Joseph Gubbins and An-
dreas Vlachos. 2013. Dependency language models
for sentence completion. In Proceedings of EMNLP.

[Gutmann and Hyv¨arinen2012] Michael U. Gutmann
and Aapo Hyv¨arinen. 2012. Noise-contrastive es-
timation of unnormalized statistical models, with
J. Mach.
applications to natural image statistics.
Learn. Res., 13(1).

[Guu et al.2015] Kelvin Guu, John Miller, and Percy
Liang. 2015. Traversing knowledge graphs in vec-
tor space. In Proceedings of EMNLP.

[Hashimoto et al.2014] Kazuma Hashimoto, Pontus
Stenetorp, Makoto Miwa, and Yoshimasa Tsu-
ruoka. 2014. Jointly learning word representations
and composition functions using predicate-argument
structures. In Proceedings of EMNLP.

[Hendrickx et al.2009] Iris Hendrickx, Su Nam Kim,
Zornitsa Kozareva, Preslav Nakov, Diarmuid
´O S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti,
2009.
Lorenza Romano, and Stan Szpakowicz.
Semeval-2010 task 8: Multi-way classiﬁcation of se-
mantic relations between pairs of nominals. In Pro-
ceedings of the Workshop on Semantic Evaluations:

Recent Achievements and Future Directions (SEW-
2009).

[Kamp and Reyle1993] Hans Kamp and Uwe Reyle.
1993. From Discourse to Logic. Springer Nether-
lands.

[Kartsaklis and Sadrzadeh2014] Dimitri Kartsaklis and
Mehrnoosh Sadrzadeh. 2014. A study of entangle-
ment in a categorical framework of natural language.
In Proceedings of the 11th Workshop on Quantum
Physics and Logic (QPL).

[Klein and Manning2003] Dan Klein and Christo-
pher D. Manning. 2003. Fast exact inference with
In
a factored model for natural language parsing.
Advances in NIPS.

[Lakoff1970] George Lakoff. 1970. Linguistics and

natural logic. Synthese, 22(1-2).

[Lenci2011] Alessandro Lenci. 2011. Composing and
updating verb argument expectations: A distribu-
In Proceedings of the 2nd
tional semantic model.
Workshop on Cognitive Modeling and Computa-
tional Linguistics.

[Levy and Goldberg2014] Omer Levy and Yoav Gold-
berg. 2014. Dependency-based word embeddings.
In Proceedings of ACL.

[Levy et al.2015] Omer Levy, Yoav Goldberg, and Ido
Dagan. 2015.
Improving distributional similarity
with lessons learned from word embeddings. Trans-
actions of ACL, 3.

[Lewis and Steedman2013] Mike Lewis

and Mark
2013. Combined distributional and

Steedman.
logical semantics. Transactions of ACL, 1.

[Liang and Potts2015] Percy Liang and Christopher
Potts. 2015. Bringing machine learning and compo-
sitional semantics together. Annual Review of Lin-
guistics, 1.

[Liang et al.2013] Percy Liang, Michael I. Jordan, and
Dan Klein. 2013. Learning dependency-based com-
positional semantics. Computational Linguistics,
39(2).

[MacCartney and Manning2008] Bill MacCartney and
Christopher D. Manning. 2008. Modeling semantic
containment and exclusion in natural language infer-
ence. In Proceedings of Coling.

[McDonald et al.2013] Ryan McDonald, Joakim Nivre,
Yvonne Quirmbach-Brundage, Yoav Goldberg, Di-
panjan Das, Kuzman Ganchev, Keith Hall, Slav
Petrov, Hao Zhang, Oscar T¨ackstr¨om, Claudia Be-
dini, N´uria Bertomeu Castell´o, and Jungmee Lee.
2013. Universal dependency annotation for multi-
lingual parsing. In Proceedings ACL.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient es-
timation of word representations in vector space.
arXiv:1301.3781.

[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b.
Distributed representations of words and phrases
and their compositionality. In Advances in NIPS.

[Mineshima et al.2015] Koji Mineshima,

Pascual
Mart´ınez-G´omez, Yusuke Miyao, and Daisuke
inference
Bekki.
In Proceedings of
with compositional semantics.
EMNLP.

Higher-order logical

2015.

[Mitchell and Lapata2010] Jeff Mitchell and Mirella
Lapata. 2010. Composition in distributional mod-
els of semantics. Cognitive Science, 34(8).

[Mnih and Teh2012] Andriy Mnih and Yee Whye Teh.
2012. A fast and simple algorithm for training neu-
ral probabilistic language models. In In Proceedings
of ICML.

[Montague1970] Richard Montague. 1970. Universal

grammar. Theoria, 36.

[Pad´o and Lapata2007] Sebastian Pad´o and Mirella La-
pata. 2007. Dependency-based construction of se-
mantic space models. Computational Linguistics,
33(2).

[Paperno et al.2014] Denis Paperno, Nghia The Pham,
A practical and
and Marco Baroni.
linguistically-motivated approach to compositional
distributional semantics. In Proceedings of ACL.

2014.

[Pennington et al.2014] Jeffrey Pennington, Richard
Socher, and Christopher Manning. 2014. Glove:
Global vectors for word representation. In Proceed-
ings of EMNLP.

[Pham et al.2015] Nghia

The

Pham,

Kruszewski, Angeliki Lazaridou,
Baroni. 2015.
tations for lexical and sentential
c-phrase model. In Proceedings of ACL.

Germ´an
and Marco
Jointly optimizing word represen-
tasks with the

[Poon and Domingos2009] Hoifung Poon and Pedro
Domingos. 2009. Unsupervised semantic parsing.
In Proceedings of EMNLP.

[Rocktaeschel et al.2014] Tim Rocktaeschel, Matko
Bosnjak, Sameer Singh, and Sebastian Riedel.
2014. Low-dimensional embeddings of logic.
In
ACL Workshop on Semantic Parsing (SP’14).

[Socher et al.2011] Richard Socher, Eric H. Huang, Jef-
frey Pennin, Christopher D Manning, and Andrew Y.
Ng. 2011. Dynamic pooling and unfolding recur-
sive autoencoders for paraphrase detection. In Ad-
vances in NIPS.

[Socher et al.2012] Richard Socher, Brody Huval,
Christopher D. Manning, and Andrew Y. Ng.
2012. Semantic compositionality through recursive
matrix-vector spaces. In Proceedings of EMNLP.

[Steedman2012] Mark Steedman. 2012. Taking Scope
- The Natural Semantics of Quantiﬁers. MIT Press.

[Tian et al.2014] Ran Tian, Yusuke Miyao, and Takuya
inference on
Matsuzaki.
dependency-based compositional semantics. In Pro-
ceedings of ACL.

Logical

2014.

[Tian et al.2015] Ran Tian, Naoaki Okazaki, and Ken-
taro Inui. 2015. The mechanism of additive compo-
sition. arXiv:1511.08407.

[Turney and Pantel2010] Peter D. Turney and Patrick
Pantel. 2010. From frequency to meaning: Vec-
tor space models of semantics. Journal of Artiﬁcial
Intelligence Research, 37(1).

[Van de Cruys2014] Tim Van de Cruys. 2014. A neural
network approach to selectional preference acquisi-
tion. In Proceedings of EMNLP.

[Xu et al.2015] Kun Xu, Yansong Feng, Songfang
Huang, and Dongyan Zhao. 2015. Semantic rela-
tion classiﬁcation via convolutional neural networks
In Proceedings of
with simple negative sampling.
EMNLP.

[Zweig et al.2012] Geoffrey Zweig,

John C. Platt,
Christopher Meek, Christopher J.C. Burges, Ainur
Yessenalina, and Qiang Liu. 2012. Computational
approaches to sentence completion. In Proceedings
of ACL.

Learning Semantically and Additively Compositional
Distributional Representations

Ran Tian and Naoaki Okazaki and Kentaro Inui
Tohoku University, Japan
{tianran, okazaki, inui}@ecei.tohoku.ac.jp

6
1
0
2
 
n
u
J
 
8
 
 
]
L
C
.
s
c
[
 
 
1
v
1
6
4
2
0
.
6
0
6
1
:
v
i
X
r
a

Abstract

This paper connects a vector-based com-
position model
to a formal semantics,
the Dependency-based Compositional Se-
mantics (DCS). We show theoretical evi-
dence that the vector compositions in our
model conform to the logic of DCS. Ex-
perimentally, we show that vector-based
composition brings a strong ability to
calculate similar phrases as similar vec-
tors, achieving near state-of-the-art on a
wide range of phrase similarity tasks and
relation classiﬁcation; meanwhile, DCS
can guide building vectors for structured
queries that can be directly executed. We
evaluate this utility on sentence comple-
tion task and report a new state-of-the-art.

1

Introduction

A major goal of semantic processing is to map nat-
ural language utterances to representations that fa-
cilitate calculation of meanings, execution of com-
mands, and/or inference of knowledge. Formal
semantics supports such representations by deﬁn-
ing words as some functional units and combining
them via a speciﬁc logic. A simple and illustra-
tive example is the Dependency-based Composi-
tional Semantics (DCS) (Liang et al., 2013). DCS
composes meanings from denotations of words
(i.e. sets of things to which the words apply); say,
the denotations of the concept drug and the event
ban is shown in Figure 1b, where drug is a list
of drug names and ban is a list of the subject-
complement pairs in any ban event; then, a list of
banned drugs can be constructed by ﬁrst taking the
COMP column of all records in ban (projection
“πCOMP”), and then intersecting the results with
drug (intersection “∩”). This procedure deﬁned
how words can be combined to form a meaning.

Better yet, the procedure can be concisely illus-
trated by the DCS tree of “banned drugs” (Fig-
ure 1a), which is similar to a dependency tree but
possesses precise procedural and logical meaning
(Section 2). DCS has been shown useful in ques-
tion answering (Liang et al., 2013) and textual en-
tailment recognition (Tian et al., 2014).

Orthogonal to the formal semantics of DCS,
distributional vector representations are useful in
capturing lexical semantics of words (Turney and
Pantel, 2010; Levy et al., 2015), and progress
is made in combining the word vectors to form
meanings of phrases/sentences (Mitchell and La-
pata, 2010; Baroni and Zamparelli, 2010; Grefen-
stette and Sadrzadeh, 2011; Socher et al., 2012;
Paperno et al., 2014; Hashimoto et al., 2014).
However, less effort is devoted to ﬁnding a link
between vector-based compositions and the com-
position operations in any formal semantics. We
believe that if a link can be found, then symbolic
formulas in the formal semantics will be realized
by vectors composed from word embeddings, such
that similar things are realized by similar vectors;
meanwhile, vectors will acquire formal meanings
that can directly be used in execution or inference
process. Still, to ﬁnd a link is challenging because
any vector compositions that realize such a link
must conform to the logic of the formal semantics.
In this paper, we establish a link between
DCS and certain vector compositions, achieving
a vector-based DCS by replacing denotations of
words with word vectors, and realizing the compo-
sition operations such as intersection and projec-
tion as addition and linear mapping, respectively.
For example, to construct a vector for “banned
drugs”, one takes the word vector vban and mul-
tiply it by a matrix MCOMP, corresponding to the
projection πCOMP; then, one adds the result to the
word vector vdrug to realize the intersection opera-
tion (Figure 1c). We provide a method to train the

Figure 2: DCS tree for a sentence

date list of banned drugs, by sorting its dot prod-
ucts with answer vectors that are also learned by
our model (Figure 1d). This is due to the ability of
our approach to provide a language model that can
is
ﬁnd likely words to ﬁll in the blanks such as “
a banned drug” or “the drug
is banned by . . . ”.
A highlight is the calculation being done as if a
query is “executed” by the DCS tree of “banned
drugs”. We quantitatively evaluate this utility on
sentence completion task (Zweig et al., 2012) and
report a new state-of-the-art (Section 5.4).

2 DCS Trees

DCS composes meanings from denotations, or
sets of things to which words apply. A “thing”
(i.e. element of a denotation) is represented by
a tuple of features of the form Field=Value,
with a ﬁxed inventory of ﬁelds. For example, a
denotation ban might be a set of tuples ban =
{(SUBJ=Canada, COMP=Thalidomide), . . .},
in which each tuple records participants of a ban-
ning event (e.g. Canada banning Thalidomide).

Operations are applied to sets of things to gener-
ate new denotations, for modeling semantic com-
position. An example is the intersection of pet
and fish giving the denotation of “pet ﬁsh”. An-
other necessary operation is projection; by πN we
mean a function mapping a tuple to its value of
the ﬁeld N. For example, πCOMP(ban) is the value
set of the COMP ﬁelds in ban, which consists of
banned objects (i.e. {Thalidomide, . . .}).
In this
paper, we assume a ﬁeld ARG to be names of
things representing themselves, hence for example
πARG(drug) is the set of names of drugs.

For a value set V , we also consider inverse im-
N (V ) := {x | πN(x) ∈ V }. For example,

age π−1

D1 := π−1

SUBJ(πARG(man))

consists of all tuples of the form (SUBJ=x, . . .),
where x is a man’s name (i.e. x ∈ πARG(man)).
Thus, sell ∩ D1 denotes men’s selling events
(i.e. {(SUBJ=John, COMP=Aspirin), . . .} as in
Figure 2). Similarly, the denotation of “banned

Figure 1: (a) The DCS tree of “banned drugs”,
which controls (b) the calculation of its denota-
tion. In this paper, we learn word vectors and ma-
trices such that (c) the same calculation is realized
in distributional semantics. The constructed query
vector can be used to (d) retrieve a list of coarse-
grained candidate answers to that query.

word vectors and linear mappings (i.e. matrices)
jointly from unlabeled corpora.

The rationale for our model is as follows. First,
recent research has shown that additive composi-
tion of word vectors is an approximation to the sit-
uation where two words have overlapping context
(Tian et al., 2015); therefore, it is suitable to im-
plement an “and” or intersection operation (Sec-
tion 3). We design our model such that the resulted
distributional representations are expected to have
additive compositionality. Second, when intersec-
tion is realized as addition, it is natural to imple-
ment projection as linear mapping, as suggested
by the logical interactions between the two oper-
ations (Section 3). Experimentally, we show that
vectors and matrices learned by our model exhibit
favorable characteristics as compared with vectors
trained by GloVe (Pennington et al., 2014) or those
learned from syntactic dependencies (Section 5.1).
Finally, additive composition brings our model
a strong ability to calculate similar vectors for
similar phrases, whereas syntactic-semantic roles
(e.g. SUBJ, COMP) can be distinguished by dif-
ferent projection matrices (e.g. MSUBJ, MCOMP).
We achieve near state-of-the-art performance on a
wide range of phrase similarity tasks (Section 5.2)
and relation classiﬁcation (Section 5.3).

Furthermore, we show that a vector as con-
structed above for “banned drugs” can be used as
a query vector to retrieve a coarse-grained candi-

Figure 3: DCS trees in this work

drugs” as in Figure 1b is formally written as

D2 := drug ∩ π−1

ARG(πCOMP(ban)),

Hence the following denotation

D3 := sell ∩ D1 ∩ π−1

COMP(πARG(D2))

consists of selling events such that the SUBJ is a
man and the COMP is a banned drug.

The calculation above can proceed in a recur-
sive manner controlled by DCS trees. The DCS
tree for the sentence “a man sells banned drugs”
is shown in Figure 2. Formally, a DCS tree is de-
ﬁned as a rooted tree in which nodes are denota-
tions of content words and edges are labeled by
ﬁelds at each ends. Assume a node x has children
y1, . . . , yn, and the edges (x, y1), . . . , (x, yn) are
labeled by (P1, L1), . . . , (Pn, Ln),
respectively.
Then, the denotation [[x]] of the subtree rooted at
x is recursively calculated as

[[x]] := x ∩

π−1
Pi

(πLi([[yi]])).

(1)

n
(cid:92)

i=1

As a result, the denotation of the DCS tree in Fig-
ure 2 is the denotation D3 of “a man sells banned
drugs” as calculated above. DCS can be further
extended to handle phenomena such as quantiﬁers
or superlatives (Liang et al., 2013; Tian et al.,
2014). In this paper, we focus on the basic ver-
sion, but note that it is already expressive enough
to at least partially capture the meanings of a large
portion of phrases and sentences.

DCS trees can be learned from question-answer
pairs and a given database of denotations (Liang
et al., 2013), or they can be extracted from de-
pendency trees if no database is speciﬁed, by tak-
ing advantage of the observation that DCS trees
are similar to dependency trees (Tian et al., 2014).
We use the latter approach, obtaining DCS trees by
rule-based conversion from universal dependency
(UD) trees (McDonald et al., 2013). Therefore,
nodes in a DCS tree are content words in a UD
tree, which are in the form of lemma-POS pairs

(Figure 3). The inventory of ﬁelds is designed
to be ARG, SUBJ, COMP, and all prepositions.
Prepositions are unlike content words which de-
note sets of things, but act as relations which we
treat similarly as SUBJ and COMP. For example,
a prepositional phrase attached to a verb (e.g. play
on the grass) is treated as in Figure 3a. The pres-
ence of two ﬁeld labels on each edge of a DCS
tree makes it convenient for modeling semantics in
several cases, such as a relative clause (Figure 3b).

3 Vector-based DCS

For any content word w, we use a query vector vw
to model its denotation, and an answer vector uw
to model a prototypical element in that denotation.
Query vector v and answer vector u are learned
such that exp(v · u) is proportional to the prob-
ability of u answering the query v. The learning
source is a collection of DCS trees, based on the
idea that the DCS tree of a declarative sentence
usually has non-empty denotation. For exam-
ple, “kids play” means there exists some kid who
plays. Consequently, some element in the play
denotation belongs to π−1
SUBJ(πARG(kid)), and
some element in the kid denotation belongs to
π−1
ARG(πSUBJ(play)). This is a signal to increase
the dot product of uplay and the query vector of
π−1
SUBJ(πARG(kid)), as well as the dot product of
ukid and the query vector of π−1
ARG(πSUBJ(play)).
When optimized on a large corpus, the “typical”
elements of play and kid should be learned by
uplay and ukid, respectively. In general, one has
Theorem 1 Assume the denotation of a DCS tree
is not empty. Given any path from node x to
y, assume edges along the path are labeled by
(P, L), . . . , (K, N). Then, an element in the deno-
tation y belongs to π−1
L (πP(x) . . .).
Therefore, for any two nodes in a DCS tree, the
path from one to another forms a training exam-
ple, which signals increasing the dot product of
the corresponding query and answer vectors.

N (πK(. . . (π−1

It is noteworthy that the above formalization
happens to be closely related to the skip-gram
model (Mikolov et al., 2013b). The skip-gram
learns a target vector vw and a context vector uw
for each word w. It assumes the probability of a
word y co-occurring with a word x in a context
window is proportional to exp(vx · uy). Hence,
if x and y co-occur within a context window, then
one gets a signal to increase vx · uy. If the con-
text window is taken as the same DCS tree, then

the learning of skip-gram and vector-based DCS
will be almost the same, except that the target vec-
tor vx becomes the query vector v, which is no
longer assigned to the word x but the path from
x to y in the DCS tree (e.g. the query vector for
π−1
SUBJ(πARG(kid)) instead of vkid). Therefore,
our model can also be regarded as extending skip-
gram to take account of the changes of meanings
caused by different syntactic-semantic roles.

Additive Composition Word vectors trained by
skip-gram are known to be semantically additive,
such as exhibited in word analogy tasks. An effect
of adding up two skip-gram vectors is further ana-
lyzed in Tian et al. (2015). Namely, the target vec-
tor vw can be regarded as encoding the distribution
of context words surrounding w. If another word
x is given, vw can be decomposed into two parts,
one encodes context words shared with x, and an-
other encodes context words not shared. When vw
and vx are added up, the non-shared part of each of
them tend to cancel out, because non-shared parts
have nearly independent distributions. As a result,
the shared part gets reinforced. An error bound
is derived to estimate how close 1
2 (vw + vx) gets
to the distribution of the shared part. We can see
the same mechanism exists in vector-based DCS.
In a DCS tree, two paths share a context word
if they lead to a same node y; semantically, this
means some element in the denotation y belongs
to both denotations of the two paths (e.g. given the
sentence “kids play balls”, π−1
SUBJ(πARG(kid))
and π−1
COMP(πARG(ball)) both contain a playing
event whose SUBJ is a kid and COMP is a ball).
Therefore, addition of query vectors of two paths
approximates their intersection because the shared
context y gets reinforced.

Projection Generally, for any two denotations
X1, X2 and any projection πN, we have

πN(X1 ∩ X2) ⊆ πN(X1) ∩ πN(X2).

(2)

And the “⊆” can often become “=”, for example
when πN is a one-to-one map or X1 = π−1
N (V )
for some value set V . Therefore, if intersection
is realized by addition, it will be natural to realize
projection by linear mapping because

(v1 + v2)MN = v1MN + v2MN

(3)

holds for any vectors v1, v2 and any matrix MN,
which is parallel to (2). If πN is realized by a ma-
trix MN, then π−1
N should correspond to the in-
verse matrix M −1
N , because πN(π−1
N (V )) = V for

any value set V . So we have realized all composi-
tion operations in DCS.

Query vector of a DCS tree Now, we can deﬁne
the query vector of a DCS tree as parallel to (1):

v[[x]] := vx +

v[[yi]]MLiM −1
Pi

.

(4)

1
n

n
(cid:88)

i=1

4 Training

As described in Section 3, vector-based DCS as-
signs a query vector vw and an answer vector uw
to each content word w. And for each ﬁeld N, it
assigns two matrices MN and M −1
N . For any path
from node x to y sampled from a DCS tree, assume
the edges along are labeled by (P, L), . . . , (K, N).
Then, the dot product vxMPM −1
N · uy
gets a signal to increase.

. . . MKM −1

L

. . . MKM −1

Formally, we adopt the noise-contrastive esti-
mation (Gutmann and Hyv¨arinen, 2012) as used
in the skip-gram model, and mix the paths sam-
pled from DCS trees with artiﬁcially generated
noise. Then, σ(vxMPM −1
N ·uy) mod-
L
els the probability of a training example coming
from DCS trees, where σ(θ) = 1/{1 + exp(−θ)}
is the sigmoid function. The vectors and matri-
ces are trained by maximizing the log-likelihood
of the mixed data. We use stochastic gradient de-
scent (Bottou, 2012) for training. Some important
settings are discussed below.

2

. . . M2l−1M −1
2l

Noise For any vxM1M −1
· uy
obtained from a path of a DCS tree, we generate
noise by randomly choosing an index i ∈ [2, 2l],
and then replacing Mj or M −1
(∀j ≥ i) and uy by
MN(j) or M −1
N(j) and uz, respectively, where N(j)
and z are independently drawn from the marginal
(i.e. unigram) distributions of ﬁelds and words.

j

Update For each data point, when i is the chosen
index above for generating noise, we view indices
j < i as the ”target” part, and j >= i as the ”con-
text”, which is completely replaced by the noise,
as an analogous to the skip-gram model. Then,
at each step we only update one vector and one
matrix from each of the target, context, and noise
part; more speciﬁcally, we only update vx, Mi−1
or M −1
N(i), uy and uz,
at the step. This is much faster than always updat-
ing all matrices.

i−1, Mi or M −1

, MN(i) or M −1

i

Initialization Matrices are initialized as 1
2 (I +
G), where I is the identity matrix; and G and all

Table 1: Top 10 similar words to “book/N”

ARG(πSUBJ(learn)) π−1
π−1

ARG(πCOMP(learn)) π−1

GloVe
books
author
published
novel
memoir
wrote
biography
autobiography
essay
illustrated

no matrix
essay/N
novel/N
memoir/N
books/N
autobiography/N
non-ﬁction/J
reprint/V
publish/V
republish/V
chapbook/N

vecDCS
novel/N
essay/N
anthology/N
publication/N
memoir/N
poem/N
autobiography/N
story/N
pamphlet/N
tale/N

vecUD
essay/N
novel/N
article/N
anthology/N
poem/N
autobiography/N
publication/N
journal/N
memoir/N
pamphlet/N

vectors are initialized with i.i.d. Gaussians of vari-
ance 1/d, where d is the vector dimension. We
ﬁnd that the diagonal component I is necessary to
bring information from vx to uy, whereas the ran-
domness of G makes convergence faster. M −1
N is
initialized as the transpose of MN.

Learning Rate We ﬁnd that the initial learning
rate for vectors can be set to 0.1. But for matrices,
it should be less than 0.0005 otherwise the model
diverges. For stable training, we rescale gradients
when their norms exceed a threshold.

d tr(M ⊥

d tr(M −1

Regularizer During training, MN and M −1
N are
treated as independent matrices. However, we use
the regularizer γ(cid:107)M −1
N MN − 1
N MN)I(cid:107)2 to
drive M −1
N close to the inverse of MN.1 We also
N MN − 1
use κ(cid:107)M ⊥
N MN)I(cid:107)2 to prevent MN
from having too different scales at different direc-
tions (i.e., to drive MN close to orthogonal). We
set γ = 0.001 and κ = 0.0001. Despite the rather
weak regularizer, we ﬁnd that M −1
N can be learned
to be exactly the inverse of MN, and MN can ac-
tually be an orthogonal matrix, showing some se-
mantic regularity (Section 5.1).

5 Experiments

For training vector-based DCS, we use Wikipedia
Extractor2 to extract texts from the 2015-12-01
dump of English Wikipedia3. Then, we use Stan-
ford Parser4 (Klein and Manning, 2003) to parse
all sentences and convert the UD trees into DCS
trees by handwritten rules. We assign a weight to
each path of the DCS trees as follows.

1Problem with the naive regularizer (cid:107)M −1M − I(cid:107)2 is
that, when the scale of M goes larger, it will drive M −1
smaller, which may lead to degeneration. So we scale I ac-
cording to the trace of M −1M .

2http://medialab.di.unipi.it/wiki/

Wikipedia_Extractor

3https://dumps.wikimedia.org/enwiki/
4http://nlp.stanford.edu/software/

lex-parser.shtml

SUBJ(πARG(house)) π−1
π−1

victorian/J
stand/V
vacant/J
18th-century/J
historic/J
old/J
georgian/J
local/J
19th-century/J
tenement/J

teacher/N
skill/N
he/P
she/P
therapist/N
student/N
they/P
mother/N
lesson/N
father/N

COMP(πARG(house))
build/V
rent/V
leave/V
burn down/V
remodel/V
demolish/V
restore/V
renovate/V
rebuild/V
construct/V

skill/N
lesson/N
technique/N
experience/N
ability/N
something/N
knowledge/N
language/N
opportunity/N
instruction/N

π−1

ARG(πin(house))
sit/V
house/N
stand/V
live/V
hang/V
seat/N
stay/V
serve/V
reside/V
hold/V
about(πARG(learn))
otherness/N
intimacy/N
femininity/N
self-awareness/N
life/N
self-expression/N
sadomasochism/N
emptiness/N
criminality/N
masculinity/N

Table 2: Top 10 answers of high dot products

For any path P passing through k intermediate
nodes of degrees n1, . . . , nk, respectively, we set

Weight(P ) :=

(5)

k
(cid:89)

i=1

1
ni − 1

.

Note that ni ≥ 2 because there is a path P passing
through the node; and Weight(P ) = 1 if P con-
sists of a single edge. The equation (5) is intended
to degrade long paths which pass through several
high-valency nodes. We use a random walk algo-
rithm to sample paths such that the expected times
a path is sampled equals its weight. As a result,
the sampled path lengths range from 1 to 19, av-
erage 2.1, with an exponential tail. We convert all
words which are sampled less than 1000 times to
*UNKNOWN*/POS, and all prepositions occurring
less than 10000 times to an *UNKNOWN* ﬁeld.
As a result, we obtain a vocabulary of 109k words
and 211 ﬁeld names.

Using the sampled paths, vectors and matrices
are trained as in Section 4 (vecDCS). The vector
dimension is set to d = 250. We compare with
three baselines: (i) all matrices are ﬁxed to identity
(“no matrix”), in order to investigate the effects
of meaning changes caused by syntactic-semantic
roles and prepositions; (ii) the regularizer enforc-
ing M −1
N to be actually the inverse matrix of MN is
set to γ = 0 (“no inverse”), in order to investigate
the effects of a semantically motivated constraint;
and (iii) applying the same training scheme to UD
trees directly, by modeling UD relations as matri-
ces (“vecUD”). In this case, one edge is assigned
one UD relation rel, so we implement the transfor-

vecDCS

-no matrix
-no inverse

vecUD
GloVe
Grefenstette and Sadrzadeh (2011)
Blacoe and Lapata (2012):RAE
Grefenstette (2013a)
Paperno et al. (2014)
Hashimoto et al. (2014):Waddnl
Kartsaklis and Sadrzadeh (2014)

AN
0.51
0.52
0.47
0.44
0.41
-
0.31
-
-
0.48
-

NN
0.49
0.46
0.43
0.46
0.47
-
0.30
-
-
0.40
-

VO
0.41
0.42
0.38
0.41
0.41
-
0.28
-
-
0.39
-

SVO
0.62
0.62
0.58
0.58
0.60
-
-
-
-

0.43

GS11
0.29
0.29
0.28
0.25
0.23
0.21
-
-
-
0.34
0.41

GS12
0.33
0.33
0.33
0.25
0.17
-
-
0.27
0.36
-
-

Table 3: Spearman’s ρ on phrase similarity

mation from child to parent by Mrel, and from par-
ent to child by M −1
rel . The same hyper-parameters
are used to train vecUD. By comparing vecDCS
with vecUD we investigate if applying the seman-
tics framework of DCS makes any difference. Ad-
ditionally, we compare with the GloVe (6B, 300d)
vector5 (Pennington et al., 2014). Norms of all
word vectors are normalized to 1 and Frobenius
norms of all matrices are normalized to

√

d.

5.1 Qualitative Analysis

We observe several special properties of the vec-
tors and matrices trained by our model.

Words are clustered by POS In terms of cosine
similarity, word vectors trained by vecDCS and
vecUD are clustered by POS tags, probably due
to their interactions with matrices during training.
This is in contrast to the vectors trained by GloVe
or “no matrix” (Table 1).

Matrices show semantic regularity Matrices
learned for ARG, SUBJ and COMP are exactly
orthogonal, and some most frequent prepositions6
are remarkably close. For these matrices, the cor-
responding M −1 also exactly converge to their
It suggests regularities in the semantic
inverse.
space, especially because orthogonal matrices pre-
serve cosine similarity – if MN is orthogonal, two
words x, y and their projections πN(x), πN(y) will
have the same similarity measure, which is seman-
tically reasonable. In contrast, matrices trained by
vecUD are only orthogonal for three UD relations,
namely conj, dep and appos.

Words transformed by matrices To illustrate
the matrices trained by vecDCS, we start from the
query vectors of two words, house and learn,

applying different matrices to them, and show
the 10 answer vectors of the highest dot prod-
ucts (Tabel 2). These are the lists of likely words
take house as a subject, take house as a
which:
in house”, serve as a
complement, ﬁlls into “
subject of learn, serve as a complement of learn,
and ﬁlls into “learn about
”, respectively. As the
table shows, matrices in vecDCS are appropriately
learned to map word vectors to their syntactic-
semantic roles.

5.2 Phrase Similarity

To test if vecDCS has the composition ability to
calculate similar things as similar vectors, we con-
duct evaluation on a wide range of phrase similar-
ity tasks. In these tasks, a system calculates sim-
ilarity scores for pairs of phrases, and the perfor-
mance is evaluated as its correlation with human
annotators, measured by Spearman’s ρ.

Datasets Mitchell and Lapata (2010) create
datasets7 for pairs of three types of two-word
phrases: adjective-nouns (AN) (e.g. “black hair”
and “dark eye”), compound nouns (NN) (e.g. “tax
charge” and “interest rate”) and verb-objects (VO)
(e.g. “ﬁght war” and “win battle”). Each dataset
consists of 108 pairs and each pair is annotated by
18 humans (i.e., 1,944 scores in total). Similarity
scores are integers ranging from 1 to 7. Another
dataset8 is created by extending VO to Subject-
Verb-Object (SVO), and then assessing similari-
ties by crowd sourcing (Kartsaklis and Sadrzadeh,
2014). The dataset GS11 created by Grefen-
stette and Sadrzadeh (2011) (100 pairs, 25 an-
notators) is also of the form SVO, but in each
pair only the verbs are different (e.g. “man pro-

5http://nlp.stanford.edu/projects/

s0453356/

glove/

8http://www.cs.ox.ac.uk/activities/

6of, in, to, for, with, on, as, at, from

compdistmeaning/

7http://homepages.inf.ed.ac.uk/

Message-Topic(e1, e2)
Message-Topic(e1, e2)
Message-Topic(e1, e2)
Component-Whole(e2, e1)
Message-Topic(e1, e2)

It is a monthly [report]1 providing [opinion]2 and advice on current United States government contract issues.
The [report]1 gives an account of the silvicultural [work]2 done in Africa, Asia, Australia, South American and the Caribbean.
NUS today responded to the Government’s [announcement]1 of the long-awaited [review]2 of university funding.
The [review]1 published political [commentary]2 and opinion, but even more than that.
It is a 2004 [book]1 criticizing the political and linguistic [writings]2 of Noam Chomsky.

Table 4: Similar training instances clustered by cosine similarities between features

vecDCS

-no matrix
-no inverse

vecUD
GloVe
Socher et al. (2012)

+3 features

dos Santos et al. (2015)
Xu et al. (2015)

81.2
69.2
79.7
69.2
74.1
79.1
82.4
84.1
85.6

Table 5: F1 on relation classiﬁcation

less, we note that “no matrix” performs as good as
vecDCS, suggesting that meaning changes caused
by syntactic-semantic roles might not be major
factors in these datasets, because the syntactic-
semantic relations are all ﬁxed in each dataset.

5.3 Relation Classiﬁcation

In a relation classiﬁcation task, the relation be-
tween two words in a sentence needs to be clas-
siﬁed; we expect vecDCS to perform better than
“no matrix” on this task because vecDCS can dis-
tinguish the different syntactic-semantic roles of
the two slots the two words ﬁt in. We conﬁrm this
conjecture in this section.

Dataset We use the dataset of SemEval-2010
Task 8 (Hendrickx et al., 2009), in which 9 di-
rected relations (e.g. Cause-Effect) and 1 undi-
rected relation Other are annotated, 8,000 in-
stances for training and 2,717 for test. Perfor-
mance is measured by the 9-class direction-aware
Macro-F1 score excluding Other class.

Our method For any sentence with two words
marked as e1 and e2, we construct the DCS tree
of the sentence, and take the subtree T rooted at
the common ancestor of e1 and e2. We construct
four vectors from T , namely: the query vector for
the subtree rooted at e1 (resp. e2), and the query
vector of the DCS tree obtained from T by re-
rooting it at e1 (resp. e2) (Figure 4). The four
vectors are normalized and concatenated to form
the only feature used to train a classiﬁer. For ve-
cUD, we use the corresponding vectors calculated
from UD trees. For GloVe, we use the word vec-
tor of e1 (resp. e2), and the sum of vectors of all
words within the span [e1, e2) (resp. (e1, e2]) as

Figure 4: For “[smoke]1 cause ﬂight [delay]2”, we
construct (a)(b) from subtrees, and (c)(d) from re-
rooted trees, to form 4 query vectors as feature.

vide/supply money”). The dataset GS12 described
in Grefenstette (2013a) (194 pairs, 50 annotators)
is of the form Adjective-Noun-Verb-Adjective-
Noun (e.g. “local family run/move small hotel”),
where only verbs are different in each pair.

Our method We calculate the cosine similarity
of query vectors corresponding to phrases. For ex-
ample, the query vector for “ﬁght war” is calcu-
lated as vwarMARGM −1
COMP + vﬁght. For vecUD
we use Mnsubj and Mdobj instead of MSUBJ and
MCOMP, respectively. For GloVe we use additive
compositions.

Results As shown in Table 3, vecDCS is com-
petitive on AN, NN, VO, SVO and GS12, con-
sistently outperforming “no inverse”, vecUD and
GloVe, showing strong compositionality. The
weakness of “no inverse” suggests that relaxing
the constraint of inverse matrices may hurt com-
positionaly, though our preliminary examination
on word similarities did not ﬁnd any difference.
The GS11 dataset appears to favor models that can
learn from interactions between the subject and
object arguments, such as the non-linear model
Waddnl in Hashimoto et al. (2014) and the en-
tanglement model in Kartsaklis and Sadrzadeh
(2014). However, these models do not show par-
ticular advantages on other datasets. The recur-
sive autoencoder (RAE) proposed in Socher et al.
(2011) shares an aspect with vecDCS as to con-
struct meanings from parse trees. It is tested by
Blacoe and Lapata (2012) for compositionality,
where vecDCS appears to be better. Neverthe-

“banned drugs”
drug/N
marijuana/N
cannabis/N
trafﬁcking/N
thalidomide/N
smoking/N
narcotic/N
botox/N
doping/N

“banned movies”
bratz/N
porn/N
indecent/N
blockbuster/N
movie/N
idiots/N
blacklist/N
grindhouse/N
doraemon/N

“banned books”
publish/N
unfair/N
obscene/N
samizdat/N
book/N
responsum/N
illegal/N
reclaiming/N
redbook/N

vecDCS

-no matrix
-no inverse

vecUD
N-gram (Various)
Zweig et al. (2012)
Mnih and Teh (2012)
Gubbins and Vlachos (2013)
Mikolov et al. (2013a)

50
60
46
31
39-41
52
55
50
55

Table 6: Answers for composed query vectors

Table 7: Accuracy (%) on sentence completion

the four vectors. Classiﬁer is SVM9 with RBF ker-
nel, C = 2 and Γ = 0.25. The hyper-parameters
are selected by 5-fold cross validation.

Results VecDCS outperforms baselines on rela-
tion classiﬁcation (Table 5). It makes 16 errors in
misclassifying the direction of a relation, as com-
pared to 144 such errors made by “no matrix”, 23
by “no inverse”, 30 by vecUD, and 161 by GloVe.
This suggests that models with syntactic-semantic
transformations (i.e. vecDCS, “no inverse”, and
vecUD) are indeed good at distinguishing the dif-
ferent roles played by e1 and e2. VecDCS scores
moderately lower than the state-of-the-art (Xu et
al., 2015), however we note that these results are
achieved by adding additional features and train-
ing task-speciﬁc neural networks (dos Santos et
al., 2015; Xu et al., 2015). Our method only
uses features constructed from unlabeled corpora.
From this point of view, it is comparable to the
MV-RNN model (without features) in Socher et
al. (2012), and vecDCS actually does better. Ta-
ble 4 shows an example of clustered training in-
stances as assessed by cosine similarities between
their features. It suggests that the features used in
our method can actually cluster similar relations.

5.4 Sentence Completion

If vecDCS can compose query vectors of DCS
trees, one should be able to “execute” the vec-
tors to get a set of answers, as the original DCS
trees can do. This is done by taking dot prod-
ucts with answer vectors and then ranking the an-
swers. Examples are shown in Table 6. Since
query vectors and answer vectors are trained from
unlabeled corpora, we can only obtain a coarse-
grained candidate list. However, it is noteworthy
that despite a common word “banned” shared by
the phrases, their answer lists are largely different,
suggesting that composition actually can be done.
Moreover, some words indeed answer the queries

(e.g. Thalidomide for “banned drugs” and Samiz-
dat for “banned books”).

Quantitatively, we evaluate this utility of exe-
cuting queries on the sentence completion task. In
this task, a sentence is presented with a blank that
need to be ﬁlled in. Five possible words are given
as options for each blank, and a system needs to
choose the correct one. The task can be viewed as
a coarse-grained question answering or an evalua-
tion for language models (Zweig et al., 2012). We
use the MSR sentence completion dataset10 which
consists of 1,040 test questions and a corpus for
training language models. We train vecDCS on
this corpus and use it for evaluation.

Results As shown in Table 7, vecDCS scores
better than the N-gram model and demonstrates
promising performance. However, to our surprise,
“no matrix” shows an even better result which is
the new state-of-the-art. Here we might be fac-
ing the same problem as in the phrase similar-
ity task (Section 5.2); namely, all choices in a
question ﬁll into the same blank and the same
syntactic-semantic role, so the transforming matri-
ces in vecDCS might not be able to distinguish dif-
ferent choices; on the other hand, vecDCS would
suffer more from parsing and POS-tagging errors.
Nonetheless, we believe the result by “no matrix”
reveals a new horizon of sentence completion, and
suggests that composing semantic vectors accord-
ing to DCS trees could be a promising direction.

6 Discussion

We have demonstrated a way to link a vector com-
position model to a formal semantics, combining
the strength of vector representations to calculate
phrase similarities, and the strength of formal se-
mantics to build up structured queries. In this sec-
tion, we discuss several lines of previous research
related to this work.

9https://www.csie.ntu.edu.tw/˜cjlin/

10http://research.microsoft.com/en-us/

libsvm/

projects/scc/

Logic and Distributional Semantics Logic is
necessary for implementing the functional aspects
of meaning and organizing knowledge in a struc-
tured and unambiguous way.
In contrast, distri-
butional semantics provides an elegant methodol-
ogy for assessing semantic similarity and is well
suited for learning from data. There have been re-
peated calls for combining the strength of these
two approaches (Coecke et al., 2010; Baroni et al.,
2014; Liang and Potts, 2015), and several systems
(Lewis and Steedman, 2013; Beltagy et al., 2014;
Tian et al., 2014) have contributed to this direc-
tion. In the remarkable work by Beltagy et al. (to
appear), word and phrase similarities are explicitly
transformed to weighted logical rules that are used
in a probabilistic inference framework. However,
this approach requires considerable amount of en-
gineering, including the generation of rule candi-
dates (e.g. by aligning sentence fragments), con-
verting distributional similarities to weights, and
efﬁciently handling the rules and inference. What
if the distributional representations are equipped
with a logical interface, such that the inference
can be realized by simple vector calculations? We
have shown it possible to realize semantic com-
position; we believe this may lead to signiﬁcant
simpliﬁcation of the system design for combining
logic and distributional semantics.

Compositional Distributional Models There
has been active exploration on how to combine
word vectors such that adequate phrase/sentence
similarities can be assessed (Mitchell and Lapata,
2010, inter alia), and there is nothing new in us-
ing matrices to model changes of meanings. How-
ever, previous model designs mostly rely on lin-
guistic intuitions (Paperno et al., 2014, inter alia),
whereas our model has an exact logic interpreta-
tion. Furthermore, by using additive composition
we enjoy a learning guarantee (Tian et al., 2015).

Vector-based Logic Models This work also
shares the spirit with Grefenstette (2013b) and
Rocktaeschel et al. (2014), in exploring vector cal-
culations that realize logic operations. However,
the previous works did not specify how to inte-
grate contextual distributional information, which
is necessary for calculating semantic similarity.

Formal Semantics Our model
implements a
logic capable of semantic com-
fragment of
position,
largely due to the simple framework
of Dependency-based Compositional Semantics

(Liang et al., 2013). It ﬁts in a long tradition of
logic-based semantics (Montague, 1970; Dowty
et al., 1981; Kamp and Reyle, 1993), with exten-
sive studies on extracting semantics from syntactic
representations such as HPSG (Copestake et al.,
2001; Copestake et al., 2005) and CCG (Baldridge
and Kruijff, 2002; Bos et al., 2004; Steedman,
2012; Artzi et al., 2015; Mineshima et al., 2015).

Logic for Natural Language Inference The
pursue of a logic more suitable for natural lan-
guage inference is also not new.
For exam-
ple, MacCartney and Manning (2008) has imple-
mented a model of natural logic (Lakoff, 1970).
We would not reach the current formalization of
logic of DCS without reading the work by Cal-
vanese et al. (1998), which is an elegant formal-
ization of database semantics in description logic.

Semantic Parsing DCS-related representations
have been actively used in semantic parsing and
we see potential in applying our model. For ex-
ample, Berant and Liang (2014) convert λ-DCS
queries to canonical utterances and assess para-
phrases at the surface level; an alternative could
be using vector-based DCS to bring distributional
similarity directly into calculation of denotations.
We also borrow ideas from previous work, for ex-
ample our training scheme is similar to Guu et al.
(2015) in using paths and composition of matri-
ces, and our method is similar to Poon and Domin-
gos (2009) in building structured knowledge from
clustering syntactic parse of unlabeled data.

Further Applications Regarding the usability
of distributional representations learned by our
model, a strong point is that the representation
takes into account syntactic/structural information
of context. Unlike several previous models (Pad´o
and Lapata, 2007; Levy and Goldberg, 2014;
Pham et al., 2015), our approach learns matrices
at the same time that can extract the information
according to different syntactic-semantic roles. A
related application is selectional preference (Ba-
roni and Lenci, 2010; Lenci, 2011; Van de Cruys,
2014), wherein our model might has potential for
smoothly handling composition.

Reproducibility Find our code at https://
github.com/tianran/vecdcs

Acknowledgments This work was supported by
CREST, JST. We thank the anonymous reviewers
for their valuable comments.

References

[Artzi et al.2015] Yoav Artzi, Kenton Lee, and Luke
Zettlemoyer. 2015. Broad-coverage ccg semantic
parsing with amr. In Proceedings of EMNLP.

[Baldridge and Kruijff2002] Jason

and
Geert-Jan Kruijff. 2002. Coupling ccg and hybrid
In Proceedings of
logic dependency semantics.
ACL.

Baldridge

[Baroni and Lenci2010] Marco Baroni and Alessandro
Lenci. 2010. Distributional memory: A general
framework for corpus-based semantics. Computa-
tional Linguistics, 36(4).

[Baroni and Zamparelli2010] Marco

and
Roberto Zamparelli. 2010. Nouns are vectors, ad-
jectives are matrices: Representing adjective-noun
constructions in semantic space. In Proceedings of
EMNLP.

Baroni

[Baroni et al.2014] Marco Baroni, Raffaella Bernardi,
and Roberto Zamparelli. 2014. Frege in space: A
program for compositional distributional semantics.
Linguistic Issues in Language Technology, 9(6).

[Beltagy et al.2014] Islam Beltagy, Katrin Erk, and
Raymond Mooney. 2014. Probabilistic soft logic
In Proceedings of
for semantic textual similarity.
ACL.

[Beltagy et al.to appear] Islam Beltagy, Stephen Roller,
Pengxiang Cheng, Katrin Erk, and Raymond J.
Mooney.
to appear. Representing meaning with a
combination of logical form and vectors. Computa-
tional Linguistics, special issue on formal distribu-
tional semantics.

[Berant and Liang2014] Jonathan Berant and Percy
Liang. 2014. Semantic parsing via paraphrasing.
In Proceedings of ACL.

[Blacoe and Lapata2012] William Blacoe and Mirella
Lapata. 2012. A comparison of vector-based rep-
resentations for semantic composition. In Proceed-
ings of EMNLP-CoNLL.

[Bos et al.2004] Johan Bos, Stephen Clark, Mark
Steedman, James R. Curran, and Julia Hockenmaier.
2004. Wide-coverage semantic representations from
a ccg parser. In Proceedings of ICCL.

[Bottou2012] L´eon Bottou. 2012. Stochastic gradient
descent tricks. In Gr´egoire Montavon, Genevi`eve B.
Orr, and Klaus-Robert M¨uller, editors, Neural Net-
works: Tricks of the Trade. Springer, Berlin.

[Calvanese et al.1998] Diego Calvanese, Giuseppe De
Giacomo, and Maurizio Lenzerini. 1998. On the
decidability of query containment under constraints.
In Proceedings of the 17th ACM SIGACT SIGMOD
SIGART Symposium on Principles of Database Sys-
tems (PODS98).

[Coecke et al.2010] Bob

Mehrnoosh
Sadrzadeh, and Stephen Clark. 2010. Mathematical
foundations
for a compositional distributional
model of meaning. Linguistic Analysis.

Coecke,

[Copestake et al.2001] Ann Copestake, Alex Las-
2001. An algebra
construction in constraint-based

carides, and Dan Flickinger.
for
grammars. In Proceedings of ACL.

semantic

[Copestake et al.2005] Ann Copestake, Dan Flickinger,
Carl Pollard, and Ivan A. Sag. 2005. Minimal re-
cursion semantics: An introduction. Research on
Language and Computation, 3(2-3).

[dos Santos et al.2015] Cicero dos Santos, Bing Xiang,
and Bowen Zhou. 2015. Classifying relations by
ranking with convolutional neural networks. In Pro-
ceedings of ACL-IJCNLP.

[Dowty et al.1981] David R. Dowty, Robert E. Wall,
and Stanley Peters. 1981. Introduction to Montague
Semantics. Springer Netherlands.

[Grefenstette and Sadrzadeh2011] Edward Grefenstette
2011. Experimen-
and Mehrnoosh Sadrzadeh.
tal support for a categorical compositional distri-
In Proceedings of
butional model of meaning.
EMNLP.

[Grefenstette2013a] Edward Grefenstette.

2013a.
Category-Theoretic Quantitative Compositional
Distributional Models of Natural Language Seman-
tics. PhD thesis.

[Grefenstette2013b] Edward Grefenstette. 2013b. To-
wards a formal distributional semantics: Simulat-
ing logical calculi with tensors. In Proceedings of
*SEM.

[Gubbins and Vlachos2013] Joseph Gubbins and An-
dreas Vlachos. 2013. Dependency language models
for sentence completion. In Proceedings of EMNLP.

[Gutmann and Hyv¨arinen2012] Michael U. Gutmann
and Aapo Hyv¨arinen. 2012. Noise-contrastive es-
timation of unnormalized statistical models, with
J. Mach.
applications to natural image statistics.
Learn. Res., 13(1).

[Guu et al.2015] Kelvin Guu, John Miller, and Percy
Liang. 2015. Traversing knowledge graphs in vec-
tor space. In Proceedings of EMNLP.

[Hashimoto et al.2014] Kazuma Hashimoto, Pontus
Stenetorp, Makoto Miwa, and Yoshimasa Tsu-
ruoka. 2014. Jointly learning word representations
and composition functions using predicate-argument
structures. In Proceedings of EMNLP.

[Hendrickx et al.2009] Iris Hendrickx, Su Nam Kim,
Zornitsa Kozareva, Preslav Nakov, Diarmuid
´O S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti,
2009.
Lorenza Romano, and Stan Szpakowicz.
Semeval-2010 task 8: Multi-way classiﬁcation of se-
mantic relations between pairs of nominals. In Pro-
ceedings of the Workshop on Semantic Evaluations:

Recent Achievements and Future Directions (SEW-
2009).

[Kamp and Reyle1993] Hans Kamp and Uwe Reyle.
1993. From Discourse to Logic. Springer Nether-
lands.

[Kartsaklis and Sadrzadeh2014] Dimitri Kartsaklis and
Mehrnoosh Sadrzadeh. 2014. A study of entangle-
ment in a categorical framework of natural language.
In Proceedings of the 11th Workshop on Quantum
Physics and Logic (QPL).

[Klein and Manning2003] Dan Klein and Christo-
pher D. Manning. 2003. Fast exact inference with
In
a factored model for natural language parsing.
Advances in NIPS.

[Lakoff1970] George Lakoff. 1970. Linguistics and

natural logic. Synthese, 22(1-2).

[Lenci2011] Alessandro Lenci. 2011. Composing and
updating verb argument expectations: A distribu-
In Proceedings of the 2nd
tional semantic model.
Workshop on Cognitive Modeling and Computa-
tional Linguistics.

[Levy and Goldberg2014] Omer Levy and Yoav Gold-
berg. 2014. Dependency-based word embeddings.
In Proceedings of ACL.

[Levy et al.2015] Omer Levy, Yoav Goldberg, and Ido
Dagan. 2015.
Improving distributional similarity
with lessons learned from word embeddings. Trans-
actions of ACL, 3.

[Lewis and Steedman2013] Mike Lewis

and Mark
2013. Combined distributional and

Steedman.
logical semantics. Transactions of ACL, 1.

[Liang and Potts2015] Percy Liang and Christopher
Potts. 2015. Bringing machine learning and compo-
sitional semantics together. Annual Review of Lin-
guistics, 1.

[Liang et al.2013] Percy Liang, Michael I. Jordan, and
Dan Klein. 2013. Learning dependency-based com-
positional semantics. Computational Linguistics,
39(2).

[MacCartney and Manning2008] Bill MacCartney and
Christopher D. Manning. 2008. Modeling semantic
containment and exclusion in natural language infer-
ence. In Proceedings of Coling.

[McDonald et al.2013] Ryan McDonald, Joakim Nivre,
Yvonne Quirmbach-Brundage, Yoav Goldberg, Di-
panjan Das, Kuzman Ganchev, Keith Hall, Slav
Petrov, Hao Zhang, Oscar T¨ackstr¨om, Claudia Be-
dini, N´uria Bertomeu Castell´o, and Jungmee Lee.
2013. Universal dependency annotation for multi-
lingual parsing. In Proceedings ACL.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efﬁcient es-
timation of word representations in vector space.
arXiv:1301.3781.

[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b.
Distributed representations of words and phrases
and their compositionality. In Advances in NIPS.

[Mineshima et al.2015] Koji Mineshima,

Pascual
Mart´ınez-G´omez, Yusuke Miyao, and Daisuke
inference
Bekki.
In Proceedings of
with compositional semantics.
EMNLP.

Higher-order logical

2015.

[Mitchell and Lapata2010] Jeff Mitchell and Mirella
Lapata. 2010. Composition in distributional mod-
els of semantics. Cognitive Science, 34(8).

[Mnih and Teh2012] Andriy Mnih and Yee Whye Teh.
2012. A fast and simple algorithm for training neu-
ral probabilistic language models. In In Proceedings
of ICML.

[Montague1970] Richard Montague. 1970. Universal

grammar. Theoria, 36.

[Pad´o and Lapata2007] Sebastian Pad´o and Mirella La-
pata. 2007. Dependency-based construction of se-
mantic space models. Computational Linguistics,
33(2).

[Paperno et al.2014] Denis Paperno, Nghia The Pham,
A practical and
and Marco Baroni.
linguistically-motivated approach to compositional
distributional semantics. In Proceedings of ACL.

2014.

[Pennington et al.2014] Jeffrey Pennington, Richard
Socher, and Christopher Manning. 2014. Glove:
Global vectors for word representation. In Proceed-
ings of EMNLP.

[Pham et al.2015] Nghia

The

Pham,

Kruszewski, Angeliki Lazaridou,
Baroni. 2015.
tations for lexical and sentential
c-phrase model. In Proceedings of ACL.

Germ´an
and Marco
Jointly optimizing word represen-
tasks with the

[Poon and Domingos2009] Hoifung Poon and Pedro
Domingos. 2009. Unsupervised semantic parsing.
In Proceedings of EMNLP.

[Rocktaeschel et al.2014] Tim Rocktaeschel, Matko
Bosnjak, Sameer Singh, and Sebastian Riedel.
2014. Low-dimensional embeddings of logic.
In
ACL Workshop on Semantic Parsing (SP’14).

[Socher et al.2011] Richard Socher, Eric H. Huang, Jef-
frey Pennin, Christopher D Manning, and Andrew Y.
Ng. 2011. Dynamic pooling and unfolding recur-
sive autoencoders for paraphrase detection. In Ad-
vances in NIPS.

[Socher et al.2012] Richard Socher, Brody Huval,
Christopher D. Manning, and Andrew Y. Ng.
2012. Semantic compositionality through recursive
matrix-vector spaces. In Proceedings of EMNLP.

[Steedman2012] Mark Steedman. 2012. Taking Scope
- The Natural Semantics of Quantiﬁers. MIT Press.

[Tian et al.2014] Ran Tian, Yusuke Miyao, and Takuya
inference on
Matsuzaki.
dependency-based compositional semantics. In Pro-
ceedings of ACL.

Logical

2014.

[Tian et al.2015] Ran Tian, Naoaki Okazaki, and Ken-
taro Inui. 2015. The mechanism of additive compo-
sition. arXiv:1511.08407.

[Turney and Pantel2010] Peter D. Turney and Patrick
Pantel. 2010. From frequency to meaning: Vec-
tor space models of semantics. Journal of Artiﬁcial
Intelligence Research, 37(1).

[Van de Cruys2014] Tim Van de Cruys. 2014. A neural
network approach to selectional preference acquisi-
tion. In Proceedings of EMNLP.

[Xu et al.2015] Kun Xu, Yansong Feng, Songfang
Huang, and Dongyan Zhao. 2015. Semantic rela-
tion classiﬁcation via convolutional neural networks
In Proceedings of
with simple negative sampling.
EMNLP.

[Zweig et al.2012] Geoffrey Zweig,

John C. Platt,
Christopher Meek, Christopher J.C. Burges, Ainur
Yessenalina, and Qiang Liu. 2012. Computational
approaches to sentence completion. In Proceedings
of ACL.

