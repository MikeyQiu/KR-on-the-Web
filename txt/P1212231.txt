6
1
0
2
 
n
u
J
 
6
2
 
 
]
E
M

.
t
a
t
s
[
 
 
1
v
6
4
0
8
0
.
6
0
6
1
:
v
i
X
r
a

Discriminating sample groups with multi-way
data

Division of Biostatistics, School of Public Health, University of Minnesota,

Division of Biostatistics, School of Public Health, University of Minnesota,

TIANMENG LYU

Minneapolis, MN 55455, USA

ERIC F. LOCK∗

Minneapolis, MN 55455, USA

elock@umn.edu

LYNN E. EBERLY

Minneapolis, MN 55455, USA

Summary

Division of Biostatistics, School of Public Health, University of Minnesota,

High-dimensional linear classiﬁers, such as the support vector machine (SVM) and distance

weighted discrimination (DWD), are commonly used in biomedical research to distinguish groups

of subjects based on a large number of features. However, their use is limited to applications

where a single vector of features is measured for each subject. In practice data are often multi-

way, or measured over multiple dimensions. For example, metabolite abundance may be mea-

sured over multiple regions or tissues, or gene expression may be measured over multiple time

points, for the same subjects. We propose a framework for linear classiﬁcation of high-dimensional

∗To whom correspondence should be addressed.

1

2

T. Lyu and others

multi-way data, in which coeﬃcients can be factorized into weights that are speciﬁc to each di-

mension. More generally, the coeﬃcients for each measurement in a multi-way dataset are as-

sumed to have low-rank structure. This framework extends existing classiﬁcation techniques, and

we have implemented multi-way versions of SVM and DWD. We describe informative simula-

tion results, and apply multi-way DWD to data for two very diﬀerent clinical research studies.

The ﬁrst study uses metabolite magnetic resonance spectroscopy data over multiple brain re-

gions to compare patients with and without spinocerebellar ataxia, the second uses publicly

available gene expression time-course data to compare treatment responses for patients with

multiple sclerosis. Our method improves performance and simpliﬁes interpretation over naive

applications of full rank linear classiﬁcation to multi-way data. An R package is available at

https://github.com/lockEF/MultiwayClassification.

Key words: Classiﬁcation; Distance weighted discrimination; Gene time-course; magnetic resonance spec-

troscopy; Support vector machine; Tensors

1. Introduction

In biomedical research and other ﬁelds, data are often best represented as a multi-way array, also

called a tensor. A multi-way array simply extends the familiar two-way data matrix (e.g., Samples

× Variables) to higher dimensions. Multi-way data frequently arise from molecular proﬁling and

imaging modalities, where data may be measured over multiple body regions, tissue-types, or

developmental time points. Our motivating example is magnetic resonance spectroscopy (MRS)

measurement of the abundance of several metabolites in three brain regions for a common set

of participants: samples × metabolites × regions. We also consider gene expression time-course

data, in which the expression of many genes are measured over multiple time points: samples ×

genes × times.

Discriminating sample groups with multi-way data

3

There are a large number of exploratory factorization and dimension reduction techniques for

multi-way data. A detailed survey of these methods can be found in Kolda and Bader (2009). Two

classical methods are the PARAFAC (Harshman, 1970) decomposition and the Tucker (Tucker,

1966) decomposition, which extend well-known methods such as the singular value decomposition

and principal component analysis for a data matrix. These and similar factorization techniques

are frequently used in practice to analyze neuroimaging data (Cichocki, 2013) and in other bio-

statistical applications (Allen, 2012; Zhou and others, 2015).

In addition to exploratory approaches, there is also a small but growing literature on super-

vised methods for multi-way data, where the interest is to determine the relationship between

an outcome vector and covariates that have multi-way structure. Zhou and others (2013) pro-

posed tensor regression models which have a continuous clinical outcome as the outcome variable

and images that are covariates, formulated as multi-way arrays. In their model, covariate coeﬃ-

cients are assumed to have a PARAFAC structure. An analogous Bayesian formulation for tensor

regression models is described by Miranda and others (2015).

Our interest is in classiﬁcation of a categorical outcome from high-dimensional multi-way

data. Classiﬁcation methods that identify a hyperplane that provides linear separation between

two classes are commonly used in biomedical research to distinguish groups of subjects based

on several features, but these methods assume that each sample’s predictors are in a vector; the

methods can thus not be applied to multi-way data where each sample’s predictors are a matrix.

There has been some work to extend classiﬁers to the multi-way context in machine learning and

computer vision. Ye and others (2004) extended the traditional Fisher’s linear discriminant anal-

ysis (LDA) to two-way tensors and their application focused on dimension reduction of images.

Bauckhage (2007) extended the least mean squares approach for LDA to tensors by assuming

that the projection tensor can be given by the PARAFAC model. Tao and others (2007) proposed

a supervised tensor learning scheme which can be applied using diﬀerent learning methods such

4

T. Lyu and others

as support vector machines (SVM) (Cortes and Vapnik (1995)) and LDA; in their formulation,

coeﬃcients for the hyperplane can be factorized into a single set of weights that are speciﬁc to

each dimension (a rank-1 factorization).

Much of the previous work on supervised learning from multi-way data focuses on the tensor

version of LDA. However, LDA can result in overﬁtting and also the solution is not identiﬁable

when the number of predictors is larger than the sample size, which is frequently the case for

high-throughput biomedical data. An alternative is SVM, which identiﬁes a high-dimensional

hyperplane that separates two classes. The hyperplane is chosen to maximize distance between

cases and controls that are closest to the hyperplane; these samples deﬁne the support vectors.

But as shown in Marron and others (2007), SVM may suﬀer from the data piling problem, which

means if we project the data onto the normal vector of the separating hyperplane then many

points will pile up at the margin. In order to overcome the data piling problem in SVM, Marron

and others (2007) proposed the Distance Weighted Discrimination (DWD) method, which allows

every data point to aﬀect the estimation of the hyperplane.

In this article, we describe a general framework for classifying high-dimensional multi-way

data that extends existing linear classiﬁcation approaches. Our central assumption is that the

multi-way coeﬃcient matrix can be decomposed into patterns that are particular to each dimen-

sion, giving a low-rank representation. The coeﬃcients are estimated by iteratively updating the

weights in each dimension to optimize an objective function. This is shown to improve both in-

terpretation and performance over naive applications of linear classiﬁcation that “vectorize” each

sample’s multi-way structure and treat each array entry as a separate variable. We implement

our extended versions of both SVM and DWD for multi-way data, and ﬁnd that DWD gener-

ally performs better. In applications we illustrate how cross-validation can be used for model

assessment, and how bootstrapping can be used to assess the uncertainty of model estimates.

Previous work on supervised tensor learning has been primarily motivated by imaging data,

Discriminating sample groups with multi-way data

5

and our method can be applied to images. However, we are primarily motivated by applications

where each dimension of a multi-way array has a unique interpretation, and a rank-1 or low-rank

model makes intuitive sense. For the Metabolites × Regions data introduced above, we apply

multi-way DWD to distinguish patients with spinocerebellar ataxia type I (SCA1) from controls;

for the genes × times data we apply multi-way DWD to distinguish good and poor responders

to IFNβ treatment for multiple sclerosis (Baranzini and others, 2005). In both cases multi-way

DWD improves performance over the naive approach and allows for a simpler interpretation of

the results.

2. Methods

2.1 High-dimensional classiﬁcation

Here we brieﬂy describe linear classiﬁcation based on a high-dimensional covariate vector per

sample, before discussing the multi-way case in Section 2.2.

Suppose data are available for n subjects, each belonging to one of two classes which we denote

by +1 and −1. Let xi denote the p×1 vector of covariates for subject i, and let yi denote the class

labels yi ∈ {+1, −1}, i = 1, . . . , n. Deﬁne y = [y1, . . . , yn] : 1 × n and X = [x1, . . . , xn] : p × n.

The goal is to ﬁnd the hyperplane b = [b1, . . . , bp](cid:124) : p × 1 which best separates the two classes.

(cid:124)
i b should provide good discrimination between the two classes.
That is, the projections f (xi) = x

Performance is assessed via an objective function h(y, X, b, Θ), which is to be minimized. The

exact form of the objective function h and additional parameters Θ (if any) depend on the

method. Below we brieﬂy describe the objective functions for SVM and DWD, respectively.

SVM objective: SVM uses the hinge loss function. The optimization problem can be formulated

as

argmin
b,β

1
n

(cid:34)

(cid:88)

i

max(0, 1 − yi(x

(cid:124)
i b − β))

+ λ||b||2,

(cid:35)

where β is an intercept term and λ is a penalty parameter that determines the tradeoﬀ between

6

T. Lyu and others

the size of the hyperplane margin and correct classiﬁcation of the groups on either side of the

hyperplane.

DWD objective: In contrast to SVM, DWD allows every data point (sample) to inﬂuence b

by optimizing the sum of the inverse distances from the data points to the hyperplane. Let Y be

the n × n diagonal matrix with yi’s as the diagonal components. The optimization problem in

DWD can be formulated as

argmin
r ,b,β,ξ

(cid:88)

i

1
ri

+ C1(cid:48)ξ,

where 1 is the vector of 1’s, C is the penalty parameter and ξ is a penalized vector with the

following constraints:

r = Y X (cid:48)b + βy + ξ (cid:62) 0, (cid:107) b (cid:107)(cid:54) 1, ξ (cid:62) 0.

2.2 Naive (full) multi-way classiﬁcation

Now consider classiﬁcation of samples with array data samples × dim1 × dim2. Let xijk denote

the value of measurement under the jth characteristic of dim1 and the kth characteristic in dim2

for subject i where i = 1, . . . , n, j = 1, . . . , p and k = 1, . . . , m. A naive approach to extend linear

classiﬁers to multi-way data is to estimate the coeﬃcient bjk for each dim1,j × dim2,k pair in the

data array. Let

xi = [xi11, . . . , xip1, xi12, . . . , xip2, . . . , xi1m, . . . , xipm](cid:124) : pm × 1

denote the full vector of covariates for subject i, i = 1, . . . , n. Then the hyperplane that is used

to distinguish the two classes is

f (xi) = b11xi11 + b12xi12 + · · · + b1mxi1m + · · · + bp1xip1 + bp2xip2 + · · · + bpmxipm,

We deﬁne this naive approach as the full model and let B : p × m be the coeﬃcient array with

bjk as the (j, k)th component.

Discriminating sample groups with multi-way data

7

2.3 Rank 1 multi-way classiﬁcation

The full model in Section 2.2 estimates diﬀerent coeﬃcients for the same jth characteristic in

dim1 under diﬀerent characteristics in dim2. For example, if dim1 corresponds to metabolites

and dim2 corresponds to brain regions, then the full model estimates diﬀerent coeﬃcients for

the same metabolite, e.g., glucose, measured in diﬀerent brain regions. But the eﬀects of the

same metabolite in diﬀerent brain regions on the classiﬁcation of the two classes are very likely

correlated. The full model does not account for the known multi-way structure of the data and

ignores the possible correlation among the diﬀerent dim1,j × dim2,k pairs; hence, it may result

in less accurate classiﬁcation performance. The proposed multi-way model can be regarded as a

low rank approximation of the full model. The rank 1 multi-way model has the simplest form

among all of the low rank approximations and has a very straightforward interpretation: The

model assumes that the coeﬃcient matrix Bp×m has the rank 1 decomposition

Bp×m = wv(cid:124),

where w = [w1, . . . , wp](cid:124) denotes the vector of weights for dim1 and v = [v1, . . . , vm](cid:124) denotes

the vector of weights for dim2. Under this assumption, the hyperplane to separate the two classes

is:

f (Xi) = w1v1xi11 + w1v2xi12 + . . . + wpv1xip1 + wpv2xip2 + . . . + wpvmxipm

(2.1)

(cid:124)
= (v(cid:124)X
i )w
= (w(cid:124)Xi)v,

where wj, j = 1, . . . , p represents the weight on the jth characteristic of dim1 and vk, k = 1, . . . , m

represents the weight on the kth characteristic of dim2. Since we estimate the weights speciﬁc to

each dimension, and a larger absolute weight usually implies a more important characteristic in

terms of its inﬂuence on classiﬁcation, we interpret the importance of diﬀerent characteristics in

one dimension to be proportional across each level of the other dimension. The full model does

not assume any commonality to the eﬀects of characteristics in dim1 across the levels of dim2,

since the coeﬃcients are estimated separately.

8

T. Lyu and others

2.4 Rank r multi-way classiﬁcation

The rank 1 multi-way model assumes that the coeﬃcient matrix has a rank 1 decomposition,

but sometimes the rank 1 structure may not be able to represent all the information in the true

coeﬃcient matrix. For example, in the metabolites × regions example described in Section 1,

the rank 1 model assumes that there is only one distinguishing proﬁle of metabolites (v) but it

can be weighted diﬀerently across the diﬀerent regions (w). However, in reality, the truth might

be that there are multiple distinguishing metabolite proﬁles (v1, v2, · · · , vr), which should be

weighed diﬀerently across the diﬀerent regions (w1, w2, · · · , wr). Under such circumstances, we

need a more complicated model compared to the rank 1 multi-way model. We propose a rank r

model for the coeﬃcient matrix Bp×m, which can be viewed as a compromise between the full

model and the rank 1 multi-way model. The rank r multi-way model assumes that the coeﬃcient

matrix has the following decomposition:

Bp×m = w1v

(cid:124)

1 + · · · + wrv(cid:124)
r ,

(2.2)

where wz = [wz1, . . . , wzp](cid:124) and vz = [vz1, . . . , vzm](cid:124), z = 1, . . . , r, r < min(p, m).

Note that the rank r model is not immediately identiﬁable in terms of wz and vz for diﬀerent

values of z. However, factorization techniques such as the singular value decomposition (SVD)

can be used to obtain a unique representation of Equation (2.2).

3. Estimation

Here we describe a general approach to estimating the coeﬃcients for multi-way classiﬁcation,

in which an objective function is iteratively optimized over the weights in each dimension. If the

objective function at each iteration is convex, then the overall optimization problem is biconvex,

and our approach can be framed as an Alternate Convex Search (ACS) algorithm (Gorski and

Discriminating sample groups with multi-way data

9

others, 2007). The objective functions of DWD, SVM, and several other linear classiﬁers are

convex. Therefore, we can iteratively optimize their objective functions using the ACS algorithm:

h(y, X, B, Θ) = h(y, (cid:126)X, (cid:126)B, Θ),

where (cid:126)X : pm × n and (cid:126)B : pm × 1 correspond to the vectorized versions of X and B, respectively.

The generic algorithm proceeds by iteratively estimating w and v in B = wv(cid:124).

Below we give the algorithm in detail for multi-way DWD. Details speciﬁc to the application

of SVM to multi-way data are given in the Appendix A. In simulations and in practice, we ﬁnd

that both the basic version and multi-way version of DWD perform better than SVM, and so

we focus on multi-way DWD hereafter. For the rank 1 multi-way model, the multi-way DWD

Step 1: Initialization. Generate the random numbers ˜w0

j , j = 1, . . . , p and ˜v0

k, k = 1, . . . , m

from a uniform distribution with range 0 to 1 and then set the initial values w0 = ˜w0

(cid:107) ˜w0(cid:107) , and

v0 = ˜v0

(cid:107)˜v0(cid:107) where ˜w0 = ( ˜w0

1, . . . , ˜w0

p)(cid:124) and ˜v0 = (˜v0

1, . . . , ˜v0

m)(cid:124). Compute the median of the

pairwise Euclidean distances between the two classes (Marron and others, 2007) and denote it as

algorithm is:

D.

Step 2: Iteration. In the (t + 1)th iteration step, ﬁrst, using vt, create a new dataset Xw

where the observation for each subject i is Xw

i = Xi · vt. Here X is the n × p × m data array, so

Xi is the p × m data matrix for subject i. Then update wt+1 by optimizing the DWD model to

ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = wt+1

1 X w

i1 + wt+1

2 X w

i2 + . . . + wt+1

p X w
ip.

Let dw denote the median of the pairwise Euclidean distances between the two classes in data

Xw; then the penalty parameter C in the DWD model corresponding to Xw is set as 100∗d2
D2

w

.

Second, using wt+1 we apply a similar approach to update vt+1.

Step 3: Convergence. At the end of each iteration step, we compute the coeﬃcients vector as

10

T. Lyu and others

(cid:126)Bt+1 = vt+1 ⊗ wt+1. If the Euclidean distance between (cid:126)Bt and (cid:126)Bt+1 is less than a pre-speciﬁed

threshold (cid:15), then the algorithm stops.

For the rank r model, we add an SVD procedure to assure the model is identiﬁable. The

algorithm is:

Step 1: Initialization. Generate the initial values for w0

z,j, j = 1, . . . , p and v0

z,k, k = 1, . . . , m

and z = 1, . . . , r from a uniform distribution with range 0 to 1. Compute the median of the

pairwise Euclidean distances between the two classes (Marron and others, 2007) and denote it as

D. Compute the coeﬃcient matrix ˜B0 = w0

1 · (v0

1)(cid:124) + . . . + w0

r · (v0

r)(cid:124) where w0

z = (w0

z,1, . . . , w0

z,p)(cid:124)

and v0

z = (v0

z,1, . . . , v0

z,m)(cid:124), z = 1, . . . , r and let B0

v = ˜B0
(cid:107) ˜B0(cid:107)

. The subscript v indicates that in the

ﬁrst iteration we will consider v0 ﬁxed to update w1; then we update v1.

Step 2: Iteration. In the (t + 1)th iteration, compute the SVD of Bt

v: Ut

p×rΣt

r×r(Vt

m×r)(cid:124).

Let vt

z be the zth column of Vt. Create a new dataset Xw where the observation for each

subject i is Xw

r)(cid:124))
i = ((Xi · vt
an rp×1 vector. Then update ˜wt+1 = (cid:0)( ˜wt+1

1)(cid:124), . . . , (Xi · vt

(cid:124)

= (cid:0)X w

i11, . . . , X w
)(cid:124)(cid:1)(cid:124)

r

i1p, . . . , X w

(cid:1)(cid:124)

which is

ir1, . . . , X w
irp
z = (cid:0) ˜wt+1

z1 , . . . , ˜wt+1
zp

(cid:1)(cid:124)

,

)(cid:124), . . . , ( ˜wt+1

1

where ˜wt+1

z = 1, . . . , r by optimizing the DWD objective to ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = ˜wt+1

11 X w

i11 + ˜wt+1

12 X w

i12 + . . . + ˜wt+1

rp X w

irp.

Let dw denote the median of the pairwise Euclidean distances between the two classes in data

Xw; then the penalty parameter C is set as 100·d2
D2

w

. Then update the coeﬃcient matrix:

Bt+1

w = ˜wt+1

1

· (vt

1)(cid:124) + . . . + ˜wt+1

r

· (vt

r)(cid:124).

After an SVD of Bt+1

w , we use an analogous approach to update ˜vt+1, and obtain a new coeﬃcient

matrix

Bt+1

v = w(t+1)

1

· (˜vt+1
1

)(cid:124) + . . . + w(t+1)

r

· (˜vt+1
r

)(cid:124).

Step 3: Convergence. At the end of each iteration step, we compute the coeﬃcients matrix

Discriminating sample groups with multi-way data

11

Bt+1
v

. If the Euclidean distance between (cid:126)Bt

v and (cid:126)Bt+1

v

is less than a pre-speciﬁed threshold (cid:15),

then the algorithm stops.

4. Simulation

4.1 Low rank model simulation and results

We illustrate and compare the full DWD model to the proposed rank 1 multi-way model and

rank r multi-way model in a simulation study. The R package “DWD” (Huang and others,

2012) was used to ﬁt the DWD model in each iteration step. Data were generated under several

conditions, including diﬀerent training sample sizes, diﬀerent multi-way array dimensions, and

diﬀerent structural forms distinguishing the two classes. For all scenarios, a training dataset with

sample size n was generated, with two classes of equal size (n0 = n1 = n/2). We consider the

values n = 40 and n = 100. The predictors have the form of a two-way array of dimensions p × m,

and we generate data under three diﬀerent array dimensions: 15 × 4, 20 × 10, and 500 × 30.

In each training dataset, n0 samples corresponding to class 0 were generated from a multi-

variate normal distribution N (µ0, Σe0), where µ0 is a pm × 1 vector and Σe0 = σ2

e0Ipm×pm. The

other n1 samples corresponding to class 1 were generated from a multivariate normal distribu-

tion N (µ1, Σe1), where µ1 is a pm × 1 vector and Σe1 = σ2

e1Ipm×pm. Under this model the Bayes

classiﬁcation rule, which classiﬁes a subject to class i only if its multivariate density under class

i is highest, takes the form of a linear classiﬁer in which the coeﬃcients are proportional to the

mean diﬀerence µ1 − µ0. This is easily shown by considering the diﬀerence in log density between

the two classes. In practice the generative probability distribution is unknown, but this oracle

rule may be used as a benchmark and motivates the three scenarios with diﬀerent structure in

the mean diﬀerence described below.

In the ﬁrst structural form scenario, the data were generated from the full model. We set

µ0 = (0, . . . , 0)(cid:124) and µ1 was generated from a multivariate normal distribution with mean zero

12

T. Lyu and others

and variance σ2

s Ipm×pm. In the second scenario, the data were generated from the rank 1 model.

We set µ0 = (0, . . . , 0)(cid:124). For µ1, we ﬁrst generated w from a multivariate normal distribution with

mean zero and variance σ2

wIp×p and v from a multivariate normal distribution with mean zero and

variance σ2

vIm×m. Then µ1 was determined by v ⊗ w where ⊗ denotes the Kronecker product. In

the third scenario, the data were generated from the rank 2 model. We ﬁrst generated w0 from a

multivariate normal distribution with mean zero and variance σ2
w0

Ip×p and v0 from a multivariate

normal distribution with mean zero and variance σ2
v0

Im×m and then µ0 was determined by

v0 ⊗ w0. Similarly, µ1 was determined by v1 ⊗ w1 where w1 was generated from a multivariate

normal distribution with mean zero and variance σ2
w1

Ip×p and v1 was from a multivariate normal

distribution with mean zero and variance σ2
v1

Im×m.

Under each scenario, corresponding test data were generated from the same distributions as

the training data with sample sizes n0 = n1 = 50. For each method we assess its misclassiﬁcation

rate on the test data, and the correlation of the estimated hyperplane and the true (Bayes)

hyperplane. The signal-to-noise ratio was adjusted for each scenario such that the misclassiﬁcation

rate of the full model is around 20% when n = 40. Each scenario was replicated 100 times.

The results in Table 1 and Figure 1 show that the model with the best performance (in

terms of the misclassiﬁcation rates and correlation with the truth) is the model from which

the data were generated under each scenario. When the sample size increases from 40 to 100,

the misclassiﬁcation rates are lower and the correlations between the estimated and the true

hyperplanes are higher. From Figure 1, we also observe that when the dimensions increase,

the diﬀerences in misclassiﬁcation rate between the appropriate model and alternative models

increase.

Discriminating sample groups with multi-way data

13

Table 1. Simulation results: “Mis” is the misclassiﬁcation rate and “SE(Mis)” is the standard
error of the misclassiﬁcation rate across the 200 simulated datasets. “Cor” is the correlation
between the estimated linear hyperplane and the true hyperplane. “SE(Cor)” is the standard error
of the correlation. The best correlation and misclassiﬁcation rate in each row are given in bold
font.

n

dimension true model

Mis

SE(Cor)

Mis

SE(Cor)

Mis

full model
SE(Mis) Cor

rank 1 model
SE(Mis) Cor

rank 2 model
SE(Mis) Cor

SE(Cor)

40

15 × 4

100

15 × 4

20 × 10

500 × 30

20 × 10

500 × 30

full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2

0.202
0.196
0.207
0.205
0.212
0.209
0.202
0.215
0.212
0.154
0.159
0.165
0.137
0.148
0.146
0.096
0.114
0.107

0.004
0.010
0.008
0.003
0.008
0.006
0.003
0.005
0.004
0.046
0.122
0.098
0.040
0.094
0.081
0.030
0.060
0.051

0.672
0.669
0.664
0.545
0.530
0.535
0.206
0.200
0.201
0.821
0.801
0.804
0.720
0.704
0.709
0.317
0.309
0.311

0.005
0.012
0.009
0.004
0.008
0.006
0.001
0.002
0.001
0.044
0.127
0.094
0.033
0.099
0.074
0.007
0.038
0.029

0.288
0.156
0.195
0.341
0.127
0.179
0.429
0.008
0.049
0.247
0.138
0.160
0.292
0.086
0.135
0.385
0.001
0.010

0.005
0.009
0.008
0.004
0.007
0.007
0.004
0.001
0.003
0.055
0.123
0.100
0.057
0.068
0.073
0.050
0.004
0.014

0.452
0.798
0.700
0.272
0.799
0.635
0.046
0.692
0.445
0.553
0.900
0.810
0.360
0.921
0.762
0.072
0.853
0.618

0.007
0.014
0.012
0.004
0.010
0.011
0.001
0.005
0.005
0.078
0.150
0.118
0.050
0.063
0.092
0.007
0.037
0.054

0.238
0.178
0.194
0.296
0.159
0.166
0.400
0.026
0.040
0.194
0.150
0.152
0.233
0.108
0.106
0.341
0.005
0.002

0.004
0.009
0.008
0.004
0.008
0.006
0.004
0.003
0.003
0.051
0.123
0.101
0.051
0.082
0.068
0.050
0.011
0.006

0.575
0.720
0.710
0.358
0.689
0.664
0.064
0.545
0.493
0.702
0.842
0.846
0.477
0.842
0.853
0.100
0.734
0.749

0.006
0.012
0.010
0.004
0.009
0.008
0.001
0.005
0.006
0.059
0.138
0.107
0.046
0.082
0.077
0.008
0.050
0.042

Fig. 1. Misclassiﬁcation rates with bars for ± 1.96 standard errors (across the 200 simulated datasets)
under each simulation scenario.

14

T. Lyu and others

4.2 High rank model simulation and results

When the dimension of the multi-way structure is 500 × 30, the eﬀective dimensions of the full

model, rank 1 model and rank 2 model are 500 × 30 = 15000, 500 + 30 = 530 and 500 +

499 + 30 + 29 = 1058 respectively. By this measure, the diﬀerence between the full model and

the rank 2 model is quite large. In order to evaluate the performance of rank r models with

eﬀective dimension in between, additional simulations were done for the multi-way structure with

dimension 500 × 30. We added two more scenarios where the simulated datasets were generated

under the rank 5 and rank 15 models and then applied the rank 5 and rank 15 models to all

datasets generated under the 500 × 30 structure. The results are shown in Figure 1, and illustrate

how diﬀerent low-rank approximations serve as a ﬂexible compromise between the rank 1 and

full models. In particular, as r increases, the performance of the rank r model approaches that

of the full model.

5. Real data analysis

The proposed methods were illustrated in two real data examples.

5.1 Magnetic Resonance Spectroscopy (MRS) data

We consider Magnetic Resonance Spectroscopy (MRS) data for a clinical research project that

enrolled patients with Spinocerebellar Ataxia Type 1 (SCA1) and healthy controls of similar age

and sex distribution. MRS is a non-invasive method using magnetic resonance imaging to quantify

neurochemicals. Here it was used to examine diﬀerences between patients and controls, and

ultimately to track changes in the brains of patients as the disease progresses. Participants were

imaged in a 3 Tesla scanner and the neurochemicals that were quantiﬁed included ascorbate (Asc),

γ-aminobutyric acid (GABA), glucose (Glc), glutamate (Glu), glutathione (GSH), myo-inositol

(Ins), scyllo-inositol (sIns), N-acetyl-aspertate (NAA), total choline (Pcho+GPC), total creatine

Discriminating sample groups with multi-way data

15

(Cr+PCr), total NAA (NAA+NAAG), glutamate plus glutamine (Glu+Gln), and glucose plus

taurine (Glc+Tau).

There were 17 patients and 24 controls enrolled in this study. The concentrations of the same

set of metabolites were measured in three diﬀerent brain regions (Pons, Cerebral Hemisphere,

and Vermis), yielding data with three dimensions: participants × metabolites × regions. Thus,

the data have a multi-way structure. We compared misclassiﬁcation rates for the full model and

the rank 1 multi-way DWD model by leave-one-out cross validation, which is robust to over-

ﬁtting. Each sample was separately left out of the estimation (to be the validation set), all the

other samples were used as training samples to construct the model, and then the model was

tested on the left-out sample. The two models gave the same leave-one-out misclassiﬁcation rate

of 4.88% and similar t-statistics (8.815 vs. 8.354 for rank 1 and full, respectively). The t-statistic

corresponded to testing the null hypothesis that the mean DWD scores of the two groups are

the same, where the DWD score for each sample is calculated from Equation 2.1. The DWD

scores under the rank 1 multi-way DWD model are shown in Figure 2, which shows that the

patients and controls are well separated. Note that one misclassiﬁed case patient scored in the

middle of the controls; this was a presymptomatic patient diagnosed by genetic screening (rather

than presentation of clinical symptoms), so it is reasonable that the rank 1 multiway DWD model

could not classify this patient correctly. The rank 1 multi-way model estimated a single weight for

each metabolite (v) and a single weight for each region (w), thus it has a simpler interpretation

compared to the full model. In order to estimate the 95% conﬁdence intervals of the estimated

weights, 5000 bootstrap samples were generated. For each bootstrap sample, 17 patients and 24

controls were resampled with replacement from the original 17 patients and 24 controls separately.

Then the model was ﬁt to the bootstrap sample to get the estimated weights. The 95% conﬁdence

interval was constructed based on the 2.5% and 97.5% quantiles of all the estimated weights based

on the bootstrap samples. The estimated weights and their 95% bootstrap conﬁdence intervals

16

T. Lyu and others

are shown in Figure 3. The metabolites with large absolute weights are considered important in

distinguishing the ataxia patients from the healthy controls.

Fig. 2. Rank 1 multi-way DWD scores under leave-one-out cross-validation for controls and patients, with
a kernel density estimate for each group.

5.2 Gene Time Course Data

We applied multi-way DWD to classify clinical response to treatment for Multiple Sclerosis (MS)

patients based on gene expression time course data. These data were originally described in

Baranzini and others (2005). Fifty-three patients were given recombinant human interferon beta

(rIFNβ), which is often used to control the symptoms of MS. Gene expression was measured for

76 genes of interest before treatment (baseline) and at 6 follow-up time points over the next two

years (3 months, 6 months, 9 months, 12 months, 18 months, 24 months), yielding a 3-way data

array: patients × genes × times. Afterward, patients were classiﬁed into good responders or poor

responders to rIFNβ based on clinical characteristics. Eﬃcient classiﬁcation of good and poor

Discriminating sample groups with multi-way data

17

Fig. 3. Rank 1 multi-way DWD weights for metabolites (top panel) and regions (bottom panel), with
95% conﬁdence intervals generated from 5000 bootstrap samples.

responders from the gene expression data is desired, for example to guide treatment decisions and

to better understand the IFNβ mechanism. The raw data are publicly available as a supplemental

ﬁle to Baranzini and others (2005).

We consider rank-r multi-way DWD to classify good and poor responders for each of r =

1, . . . , 7. The seven models were compared via leave-one-out cross validation estimation of the

mis-classiﬁcation rate. The rank-1 model, with a single weight for each gene and for each time

point, outperformed the others with the highest t-statistic (t = 7.58) and lowest misclassiﬁcation

rate (16.9%) under cross validation. The full model, with a distinct coeﬃcient for each gene ×

time pair, had a t-statistic of 5.38 and a misclassiﬁcation rate of 22.6% under cross-validation.

18

T. Lyu and others

The DWD scores under leave-one-out cross validation for the rank-1 multi-way model are

shown in Figure 4. This shows substantial but not perfect discrimination between the good and

poor responder groups. The coeﬃcient estimates for each gene and each time point, with 95%

bootstrap conﬁdence intervals, are shown in Figure 5. The four genes with the highest coeﬃcient

were Jak1, Caspase.9, STAT3, and IFN.gRa; the four genes with the highest negative coeﬃcient

were FAS, NFkBIA, IRF6, and ITGB2. The coeﬃcients across time had little variability and no

noticeable patterns. This suggests that the distinction between good and poor responders is not

driven by changes to gene expression in response to INFβ, but rather by baseline diﬀerences in

expression that can be quantiﬁed more precisely over multiple time points. This agrees with the

results in Baranzini and others (2005), who conducted an analysis of variance (ANOVA) for each

gene and report several signiﬁcant response and time eﬀects but no response*time interactions.

Fig. 4. Rank 1 multi-way DWD scores under leave-one-out cross-validation for good and poor treatment
responders, with a kernel density estimate for each group.

An alternative approach to classifying subjects based on gene expression time-course data is

Discriminating sample groups with multi-way data

19

Fig. 5. Rank 1 multi-way DWD weights for genes (top panel) and time points (bottom panel), with 95%
conﬁdence intervals generated from 5000 bootstrap samples.

described in Zhang and others (2013). Their method identiﬁes an optimal direction in time for

each gene using Fisher’s LDA, and then applies SVM or another high-dimensional classiﬁer to the

projections of each gene on its optimal direction. This approach is appropriate when discriminative

patterns over time are diﬀerent for each gene, but it does not explicitly capture patterns that are

shared across multiple genes. They assess classiﬁcation accuracy using the same IFNβ dataset

described above and achieve a minimum cross-validation error rate of approximately 26%, greater

than the error rate of 16.9% achieved by multi-way DWD.

20

T. Lyu and others

6. Discussion

Although data with multi-way structure is common in biomedical research, little work has ad-

dressed classiﬁcation of categorical outcomes from high-dimensional multi-way data. In this arti-

cle, we have proposed a general framework to extend linear classiﬁcation methods to multi-way

data. We mainly focus on the multi-way DWD model because of its ability to overcome the data-

piling problem of SVM and its good performance in simulations. Both the simulation and real

data analysis results show that the multi-way model can improve classiﬁcation accuracy when the

underlying true model has a multi-way structure and can provide a simple and straightforward

interpretation.

other means.

While the simple rank 1 classiﬁcation model performs well in our applications, it may perform

poorly for others. The simulations in Section 4 clearly demonstrate that low-rank models can be

a poor approximation if the underlying signal distinguishing the two classes does not have multi-

way structure. Therefore, we advise that the rank of the model should not be decided upon

blindly; rather, multiple models should be considered and compared via cross-validated errors or

The methodology described in Sections 2 and 3 may be extended in several ways. While

we implement multi-way classiﬁcation for a binary outcome, the framework can also be used to

extend multi-category classiﬁcation methods such as multiclass DWD (Huang and others, 2013).

Also in our implementation we focus on three-way data: samples × dim 1 × dim 2. The general

framework and iterative estimation technique may be extended to more dimensions; for higher-

order arrays the coeﬃcients may be represented as a rank-r PARAFAC decomposition. Sparse

versions of SVM and DWD have been developed, in which negligible coeﬃcients are shrunk to

0 (Bi and others, 2003; Wang and Zou, 2015). Sparse multi-way classiﬁcation, in which some of

the weights in each dimension are shrunk to 0, is another direction of future development.

Discriminating sample groups with multi-way data

21

Acknowledgments

We thank G¨ulin ¨Oz, Uzay Emir, and Dinesh Deelchand for providing the data and feedback for

the MRS application described in Section 5.1. We also thank Hanwen Huang for his help and

advice regarding the DWD package for R.

Conﬂict of Interest: None declared.

This work was supported by the National Institutes of Health grant ULI RR033183/KL2 RR0333182

[to EFL] and grant 1R01NS080816-01A1 [supporting TL and LEE].

Funding

APPENDIX

A. Multi-Way SVM

Details speciﬁc to the application of SVM to multi-way data are given below. For the rank 1

multi-way model, the multi-way SVM algorithm is:

Step 1: Initialization. generate the random numbers w0

j , j = 1, . . . , p and v0

k, k = 1, . . . , m

from a uniform distribution with range 0 to 1 and then set the initial values w0 = (w0

1, . . . , w0

p)(cid:124)

and v0 = (v0

1, . . . , v0

m)(cid:124).

Step 2: Iteration. in the (t + 1)th iteration step, ﬁrst, standardize vt by vt = vt

ﬁx vt, and create a new dataset Xw where the observation for each subject i is Xw

(cid:107)vt(cid:107) and then

i = Xi · vt.

Here X is the n × p × m data array, so Xi is the p × m data matrix for subject i. Then update

wt+1 by optimizing the SVM model to ﬁnd the hyperplane that:

f (Xw

i ) = wt+1

1 X w

i1 + wt+1

2 X w

i2 + . . . + wt+1

p X w
ip.

Second, we standardize and ﬁx wt+1 and then apply a similar approach to update vt+1.

Step 3: Convergence. in the end of each iteration step, we compute the coeﬃcients vector by

22

T. Lyu and others

bt+1 = vt+1 ⊗ wt+1. If the Euclidean diﬀerence between bt and bt+1 is less than a pre-speciﬁed

threshold (cid:15), then the algorithm stops.

For the rank r model, we add an SVD procedure to assure the model is identiﬁable. The

algorithm is:

z = 1, . . . , r.

Step 1: Initialization: generate the initial values for w0

z,j, j = 1, . . . , p and v0

z,k, k = 1, . . . , m

and z = 1, . . . , r from a uniform distribution with range 0 to 1. Compute the coeﬃcient matrix

˜B0 = w0

1 · (v0

1)(cid:124) + . . . + w0

r · (v0

r)(cid:124) where w0

z = (w0

z,1, . . . , w0

z,p)(cid:124) and v0

z = (v0

z,1, . . . , v0

z,m)(cid:124),

Step 2: Iteration: In the (t + 1)th iteration, compute the SVD of Bt

v: Ut

p×rΣt

r×r(Vt

m×r)(cid:124).

Let vt

z be the zth column of Vt. Create a new dataset Xw where the observation for each

subject i is Xw

r)(cid:124))
i = ((Xi · vt
an rp×1 vector. Then update ˜wt+1 = (cid:0)( ˜wt+1

1)(cid:124), . . . , (Xi · vt

(cid:124)

= (cid:0)X w

i11, . . . , X w
)(cid:124)(cid:1)(cid:124)

r

i1p, . . . , X w

(cid:1)(cid:124)

which is

ir1, . . . , X w
irp
z = (cid:0) ˜wt+1

z1 , . . . , ˜wt+1
zp

(cid:1)(cid:124)

,

)(cid:124), . . . , ( ˜wt+1

1

where ˜wt+1

z = 1, . . . , r by optimizing the SVM objective to ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = ˜wt+1

11 X w

i11 + ˜wt+1

12 X w

i12 + . . . + ˜wt+1

rp X w

irp.

Then update the coeﬃcient matrix:

Bt+1

w = ˜wt+1

1

· (vt

1)(cid:124) + . . . + ˜wt+1

r

· (vt

r)(cid:124).

After an SVD of Bt+1

w , we use an analogous approach to update ˜vt+1, and obtain a new coeﬃcient

matrix

Bt+1

v = w(t+1)

1

· (˜vt+1
1

)(cid:124) + . . . + w(t+1)

r

· (˜vt+1
r

)(cid:124).

Step 3: Convergence: in the end of each iteration step, we compute the coeﬃcients matrix

Bt+1
v

. If the Euclidean diﬀerence between (cid:126)Bt

v and (cid:126)Bt+1

v

is less than a pre-speciﬁed threshold (cid:15),

then the algorithm stops.

Note that the convergence of the multi-way SVM model depends more on the starting random

seed than the multi-way DWD model. So we try multiple starting random seeds and select the

REFERENCES

23

solution with the largest objective function value as the best solution.

The comparison of DWD and SVM was shown in Table 2. Only the results based on the model

from which the data were generated were reported. For example, when the data were generated

from the rank 1 model, then the comparison was between the rank 1 multi-way DWD model and

the rank 1 multi-way SVM model.

Table 2. Simulation results: “Mis” is the misclassiﬁcation rate and “SE(Mis)” is the standard
error of the misclassiﬁcation rate across the 200 simulated datasets. “Cor” is the absolute value
of the correlation between the estimated linear hyperplane and the true hyperplane (we take the
absolute value because SVM does not deﬁne a positive and negative class). “SE(Cor)” is the
standard error of the absolute correlation.

dimension true model

Mis

SE(Mis) Cor

SE(Cor)

Mis

SE(Mis) Cor

SE(Cor)

DWD

SVM

15 × 4

20 × 10

500 × 30

full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2

0.202
0.156
0.194
0.205
0.127
0.166
0.202
0.008
0.040

0.004
0.009
0.008
0.003
0.007
0.006
0.003
0.001
0.003

0.672
0.799
0.710
0.545
0.799
0.664
0.206
0.692
0.493

0.005
0.014
0.010
0.004
0.010
0.008
0.001
0.005
0.006

0.239
0.211
0.244
0.218
0.206
0.228
0.201
0.024
0.066

0.004
0.010
0.008
0.004
0.009
0.007
0.003
0.003
0.005

0.575
0.620
0.550
0.511
0.556
0.490
0.206
0.577
0.415

0.006
0.015
0.011
0.004
0.014
0.009
0.001
0.007
0.006

References

Allen, Genevera. (2012). Sparse higher-order principal components analysis. In: International

Conference on Artiﬁcial Intelligence and Statistics. pp. 27–36.

Baranzini, Sergio E, Mousavi, Parvin, Rio, Jordi, Caillier, Stacy J, Stillman,

Althea, Villoslada, Pablo, Wyatt, Matthew M, Comabella, Manuel, Greller,

Larry D, Somogyi, Roland and others. (2005). Transcription-based prediction of response

to IFNβ using supervised computational methods. PLoS Biol 3(1), e2.

Bauckhage, Christian. (2007). Robust tensor classiﬁers for color object recognition. In: Image

24

REFERENCES

Analysis and Recognition. Springer, pp. 352–363.

Bi, Jinbo, Bennett, Kristin, Embrechts, Mark, Breneman, Curt and Song, Minghu.

(2003). Dimensionality reduction via sparse support vector machines. The Journal of Machine

Learning Research 3, 1229–1243.

arXiv preprint arXiv:1305.0395 .

ing 20(3), 273–297.

Cichocki, Andrzej. (2013). Tensor decompositions: A new concept in brain data analysis?

Cortes, Corinna and Vapnik, Vladimir. (1995). Support-vector networks. Machine learn-

Gorski, Jochen, Pfeuffer, Frank and Klamroth, Kathrin. (2007). Biconvex sets and

optimization with biconvex functions: a survey and extensions. Mathematical Methods of Op-

erations Research 66(3), 373–407.

Harshman, Richard A. (1970). Foundations of the parafac procedure: Models and conditions

for an “explanatory” multi-modal factor analysis. UCLA Working Papers in Phonetics 16,

1–84.

Huang, Hanwen, Liu, Yufeng, Du, Ying, Perou, Charles M, Hayes, D Neil, Todd,

Michael J and Marron, James Stephen. (2013). Multiclass distance-weighted discrimi-

nation. Journal of Computational and Graphical Statistics 22(4), 953–969.

Huang, Hanwen, Lu, Xiaosun, Liu, Yufeng, Haaland, Perry and Marron, JS. (2012).

R/DWD: distance-weighted discrimination for classiﬁcation, visualization and batch adjust-

ment. Bioinformatics 28(8), 1182–1183.

Kolda, Tamara G and Bader, Brett W. (2009). Tensor decompositions and applications.

SIAM review 51(3), 455–500.

REFERENCES

25

Marron, J S, Todd, Michael J and Ahn, Jeongyoun. (2007). Distance-weighted discrim-

ination. Journal of the American Statistical Association 102(480), 1267–1271.

Miranda, Michelle, Zhu, Hongtu and Ibrahim, Joseph G. (2015). TPRM: Tensor par-

tition regression models with applications in imaging biomarker detection. arXiv preprint

arXiv:1505.05482 .

Tao, Dacheng, Li, Xuelong, Wu, Xindong, Hu, Weiming and Maybank, Stephen J.

(2007). Supervised tensor learning. Knowl. Inf. Syst. 13(1), 1–42.

Tucker, Ledyard R. (1966). Some mathematical notes on three-mode factor analysis. Psy-

chometrika 31(3), 279–311.

Wang, Boxiang and Zou, Hui. (2015). Sparse distance weighted discrimination. Journal of

Computational and Graphical Statistics (just-accepted), 00–00.

Ye, Jieping, Janardan, Ravi and Li, Qi. (2004). Two-dimensional linear discriminant anal-

ysis. In: Advances in Neural Information Processing Systems, Volume 17. pp. 1569–1576.

Zhang, Yuping, Tibshirani, Robert and Davis, Ronald. (2013). Classiﬁcation of patients

from time-course gene expression. Biostatistics 14(1), 87–98.

Zhou, Hua, Li, Lexin and Zhu, Hongtu. (2013). Tensor regression with applications in

neuroimaging data analysis. Journal of the American Statistical Association 108(502), 540–

552.

Zhou, Jing, Bhattacharya, Anirban, Herring, Amy H and Dunson, David B. (2015).

Bayesian factorizations of big sparse tensors. Journal of the American Statistical Associa-

tion 110(512), 1562–1576.

6
1
0
2
 
n
u
J
 
6
2
 
 
]
E
M

.
t
a
t
s
[
 
 
1
v
6
4
0
8
0
.
6
0
6
1
:
v
i
X
r
a

Discriminating sample groups with multi-way
data

Division of Biostatistics, School of Public Health, University of Minnesota,

Division of Biostatistics, School of Public Health, University of Minnesota,

TIANMENG LYU

Minneapolis, MN 55455, USA

ERIC F. LOCK∗

Minneapolis, MN 55455, USA

elock@umn.edu

LYNN E. EBERLY

Minneapolis, MN 55455, USA

Summary

Division of Biostatistics, School of Public Health, University of Minnesota,

High-dimensional linear classiﬁers, such as the support vector machine (SVM) and distance

weighted discrimination (DWD), are commonly used in biomedical research to distinguish groups

of subjects based on a large number of features. However, their use is limited to applications

where a single vector of features is measured for each subject. In practice data are often multi-

way, or measured over multiple dimensions. For example, metabolite abundance may be mea-

sured over multiple regions or tissues, or gene expression may be measured over multiple time

points, for the same subjects. We propose a framework for linear classiﬁcation of high-dimensional

∗To whom correspondence should be addressed.

1

2

T. Lyu and others

multi-way data, in which coeﬃcients can be factorized into weights that are speciﬁc to each di-

mension. More generally, the coeﬃcients for each measurement in a multi-way dataset are as-

sumed to have low-rank structure. This framework extends existing classiﬁcation techniques, and

we have implemented multi-way versions of SVM and DWD. We describe informative simula-

tion results, and apply multi-way DWD to data for two very diﬀerent clinical research studies.

The ﬁrst study uses metabolite magnetic resonance spectroscopy data over multiple brain re-

gions to compare patients with and without spinocerebellar ataxia, the second uses publicly

available gene expression time-course data to compare treatment responses for patients with

multiple sclerosis. Our method improves performance and simpliﬁes interpretation over naive

applications of full rank linear classiﬁcation to multi-way data. An R package is available at

https://github.com/lockEF/MultiwayClassification.

Key words: Classiﬁcation; Distance weighted discrimination; Gene time-course; magnetic resonance spec-

troscopy; Support vector machine; Tensors

1. Introduction

In biomedical research and other ﬁelds, data are often best represented as a multi-way array, also

called a tensor. A multi-way array simply extends the familiar two-way data matrix (e.g., Samples

× Variables) to higher dimensions. Multi-way data frequently arise from molecular proﬁling and

imaging modalities, where data may be measured over multiple body regions, tissue-types, or

developmental time points. Our motivating example is magnetic resonance spectroscopy (MRS)

measurement of the abundance of several metabolites in three brain regions for a common set

of participants: samples × metabolites × regions. We also consider gene expression time-course

data, in which the expression of many genes are measured over multiple time points: samples ×

genes × times.

Discriminating sample groups with multi-way data

3

There are a large number of exploratory factorization and dimension reduction techniques for

multi-way data. A detailed survey of these methods can be found in Kolda and Bader (2009). Two

classical methods are the PARAFAC (Harshman, 1970) decomposition and the Tucker (Tucker,

1966) decomposition, which extend well-known methods such as the singular value decomposition

and principal component analysis for a data matrix. These and similar factorization techniques

are frequently used in practice to analyze neuroimaging data (Cichocki, 2013) and in other bio-

statistical applications (Allen, 2012; Zhou and others, 2015).

In addition to exploratory approaches, there is also a small but growing literature on super-

vised methods for multi-way data, where the interest is to determine the relationship between

an outcome vector and covariates that have multi-way structure. Zhou and others (2013) pro-

posed tensor regression models which have a continuous clinical outcome as the outcome variable

and images that are covariates, formulated as multi-way arrays. In their model, covariate coeﬃ-

cients are assumed to have a PARAFAC structure. An analogous Bayesian formulation for tensor

regression models is described by Miranda and others (2015).

Our interest is in classiﬁcation of a categorical outcome from high-dimensional multi-way

data. Classiﬁcation methods that identify a hyperplane that provides linear separation between

two classes are commonly used in biomedical research to distinguish groups of subjects based

on several features, but these methods assume that each sample’s predictors are in a vector; the

methods can thus not be applied to multi-way data where each sample’s predictors are a matrix.

There has been some work to extend classiﬁers to the multi-way context in machine learning and

computer vision. Ye and others (2004) extended the traditional Fisher’s linear discriminant anal-

ysis (LDA) to two-way tensors and their application focused on dimension reduction of images.

Bauckhage (2007) extended the least mean squares approach for LDA to tensors by assuming

that the projection tensor can be given by the PARAFAC model. Tao and others (2007) proposed

a supervised tensor learning scheme which can be applied using diﬀerent learning methods such

4

T. Lyu and others

as support vector machines (SVM) (Cortes and Vapnik (1995)) and LDA; in their formulation,

coeﬃcients for the hyperplane can be factorized into a single set of weights that are speciﬁc to

each dimension (a rank-1 factorization).

Much of the previous work on supervised learning from multi-way data focuses on the tensor

version of LDA. However, LDA can result in overﬁtting and also the solution is not identiﬁable

when the number of predictors is larger than the sample size, which is frequently the case for

high-throughput biomedical data. An alternative is SVM, which identiﬁes a high-dimensional

hyperplane that separates two classes. The hyperplane is chosen to maximize distance between

cases and controls that are closest to the hyperplane; these samples deﬁne the support vectors.

But as shown in Marron and others (2007), SVM may suﬀer from the data piling problem, which

means if we project the data onto the normal vector of the separating hyperplane then many

points will pile up at the margin. In order to overcome the data piling problem in SVM, Marron

and others (2007) proposed the Distance Weighted Discrimination (DWD) method, which allows

every data point to aﬀect the estimation of the hyperplane.

In this article, we describe a general framework for classifying high-dimensional multi-way

data that extends existing linear classiﬁcation approaches. Our central assumption is that the

multi-way coeﬃcient matrix can be decomposed into patterns that are particular to each dimen-

sion, giving a low-rank representation. The coeﬃcients are estimated by iteratively updating the

weights in each dimension to optimize an objective function. This is shown to improve both in-

terpretation and performance over naive applications of linear classiﬁcation that “vectorize” each

sample’s multi-way structure and treat each array entry as a separate variable. We implement

our extended versions of both SVM and DWD for multi-way data, and ﬁnd that DWD gener-

ally performs better. In applications we illustrate how cross-validation can be used for model

assessment, and how bootstrapping can be used to assess the uncertainty of model estimates.

Previous work on supervised tensor learning has been primarily motivated by imaging data,

Discriminating sample groups with multi-way data

5

and our method can be applied to images. However, we are primarily motivated by applications

where each dimension of a multi-way array has a unique interpretation, and a rank-1 or low-rank

model makes intuitive sense. For the Metabolites × Regions data introduced above, we apply

multi-way DWD to distinguish patients with spinocerebellar ataxia type I (SCA1) from controls;

for the genes × times data we apply multi-way DWD to distinguish good and poor responders

to IFNβ treatment for multiple sclerosis (Baranzini and others, 2005). In both cases multi-way

DWD improves performance over the naive approach and allows for a simpler interpretation of

the results.

2. Methods

2.1 High-dimensional classiﬁcation

Here we brieﬂy describe linear classiﬁcation based on a high-dimensional covariate vector per

sample, before discussing the multi-way case in Section 2.2.

Suppose data are available for n subjects, each belonging to one of two classes which we denote

by +1 and −1. Let xi denote the p×1 vector of covariates for subject i, and let yi denote the class

labels yi ∈ {+1, −1}, i = 1, . . . , n. Deﬁne y = [y1, . . . , yn] : 1 × n and X = [x1, . . . , xn] : p × n.

The goal is to ﬁnd the hyperplane b = [b1, . . . , bp](cid:124) : p × 1 which best separates the two classes.

(cid:124)
i b should provide good discrimination between the two classes.
That is, the projections f (xi) = x

Performance is assessed via an objective function h(y, X, b, Θ), which is to be minimized. The

exact form of the objective function h and additional parameters Θ (if any) depend on the

method. Below we brieﬂy describe the objective functions for SVM and DWD, respectively.

SVM objective: SVM uses the hinge loss function. The optimization problem can be formulated

as

argmin
b,β

1
n

(cid:34)

(cid:88)

i

max(0, 1 − yi(x

(cid:124)
i b − β))

+ λ||b||2,

(cid:35)

where β is an intercept term and λ is a penalty parameter that determines the tradeoﬀ between

6

T. Lyu and others

the size of the hyperplane margin and correct classiﬁcation of the groups on either side of the

hyperplane.

DWD objective: In contrast to SVM, DWD allows every data point (sample) to inﬂuence b

by optimizing the sum of the inverse distances from the data points to the hyperplane. Let Y be

the n × n diagonal matrix with yi’s as the diagonal components. The optimization problem in

DWD can be formulated as

argmin
r ,b,β,ξ

(cid:88)

i

1
ri

+ C1(cid:48)ξ,

where 1 is the vector of 1’s, C is the penalty parameter and ξ is a penalized vector with the

following constraints:

r = Y X (cid:48)b + βy + ξ (cid:62) 0, (cid:107) b (cid:107)(cid:54) 1, ξ (cid:62) 0.

2.2 Naive (full) multi-way classiﬁcation

Now consider classiﬁcation of samples with array data samples × dim1 × dim2. Let xijk denote

the value of measurement under the jth characteristic of dim1 and the kth characteristic in dim2

for subject i where i = 1, . . . , n, j = 1, . . . , p and k = 1, . . . , m. A naive approach to extend linear

classiﬁers to multi-way data is to estimate the coeﬃcient bjk for each dim1,j × dim2,k pair in the

data array. Let

xi = [xi11, . . . , xip1, xi12, . . . , xip2, . . . , xi1m, . . . , xipm](cid:124) : pm × 1

denote the full vector of covariates for subject i, i = 1, . . . , n. Then the hyperplane that is used

to distinguish the two classes is

f (xi) = b11xi11 + b12xi12 + · · · + b1mxi1m + · · · + bp1xip1 + bp2xip2 + · · · + bpmxipm,

We deﬁne this naive approach as the full model and let B : p × m be the coeﬃcient array with

bjk as the (j, k)th component.

Discriminating sample groups with multi-way data

7

2.3 Rank 1 multi-way classiﬁcation

The full model in Section 2.2 estimates diﬀerent coeﬃcients for the same jth characteristic in

dim1 under diﬀerent characteristics in dim2. For example, if dim1 corresponds to metabolites

and dim2 corresponds to brain regions, then the full model estimates diﬀerent coeﬃcients for

the same metabolite, e.g., glucose, measured in diﬀerent brain regions. But the eﬀects of the

same metabolite in diﬀerent brain regions on the classiﬁcation of the two classes are very likely

correlated. The full model does not account for the known multi-way structure of the data and

ignores the possible correlation among the diﬀerent dim1,j × dim2,k pairs; hence, it may result

in less accurate classiﬁcation performance. The proposed multi-way model can be regarded as a

low rank approximation of the full model. The rank 1 multi-way model has the simplest form

among all of the low rank approximations and has a very straightforward interpretation: The

model assumes that the coeﬃcient matrix Bp×m has the rank 1 decomposition

Bp×m = wv(cid:124),

where w = [w1, . . . , wp](cid:124) denotes the vector of weights for dim1 and v = [v1, . . . , vm](cid:124) denotes

the vector of weights for dim2. Under this assumption, the hyperplane to separate the two classes

is:

f (Xi) = w1v1xi11 + w1v2xi12 + . . . + wpv1xip1 + wpv2xip2 + . . . + wpvmxipm

(2.1)

(cid:124)
= (v(cid:124)X
i )w
= (w(cid:124)Xi)v,

where wj, j = 1, . . . , p represents the weight on the jth characteristic of dim1 and vk, k = 1, . . . , m

represents the weight on the kth characteristic of dim2. Since we estimate the weights speciﬁc to

each dimension, and a larger absolute weight usually implies a more important characteristic in

terms of its inﬂuence on classiﬁcation, we interpret the importance of diﬀerent characteristics in

one dimension to be proportional across each level of the other dimension. The full model does

not assume any commonality to the eﬀects of characteristics in dim1 across the levels of dim2,

since the coeﬃcients are estimated separately.

8

T. Lyu and others

2.4 Rank r multi-way classiﬁcation

The rank 1 multi-way model assumes that the coeﬃcient matrix has a rank 1 decomposition,

but sometimes the rank 1 structure may not be able to represent all the information in the true

coeﬃcient matrix. For example, in the metabolites × regions example described in Section 1,

the rank 1 model assumes that there is only one distinguishing proﬁle of metabolites (v) but it

can be weighted diﬀerently across the diﬀerent regions (w). However, in reality, the truth might

be that there are multiple distinguishing metabolite proﬁles (v1, v2, · · · , vr), which should be

weighed diﬀerently across the diﬀerent regions (w1, w2, · · · , wr). Under such circumstances, we

need a more complicated model compared to the rank 1 multi-way model. We propose a rank r

model for the coeﬃcient matrix Bp×m, which can be viewed as a compromise between the full

model and the rank 1 multi-way model. The rank r multi-way model assumes that the coeﬃcient

matrix has the following decomposition:

Bp×m = w1v

(cid:124)

1 + · · · + wrv(cid:124)
r ,

(2.2)

where wz = [wz1, . . . , wzp](cid:124) and vz = [vz1, . . . , vzm](cid:124), z = 1, . . . , r, r < min(p, m).

Note that the rank r model is not immediately identiﬁable in terms of wz and vz for diﬀerent

values of z. However, factorization techniques such as the singular value decomposition (SVD)

can be used to obtain a unique representation of Equation (2.2).

3. Estimation

Here we describe a general approach to estimating the coeﬃcients for multi-way classiﬁcation,

in which an objective function is iteratively optimized over the weights in each dimension. If the

objective function at each iteration is convex, then the overall optimization problem is biconvex,

and our approach can be framed as an Alternate Convex Search (ACS) algorithm (Gorski and

Discriminating sample groups with multi-way data

9

others, 2007). The objective functions of DWD, SVM, and several other linear classiﬁers are

convex. Therefore, we can iteratively optimize their objective functions using the ACS algorithm:

h(y, X, B, Θ) = h(y, (cid:126)X, (cid:126)B, Θ),

where (cid:126)X : pm × n and (cid:126)B : pm × 1 correspond to the vectorized versions of X and B, respectively.

The generic algorithm proceeds by iteratively estimating w and v in B = wv(cid:124).

Below we give the algorithm in detail for multi-way DWD. Details speciﬁc to the application

of SVM to multi-way data are given in the Appendix A. In simulations and in practice, we ﬁnd

that both the basic version and multi-way version of DWD perform better than SVM, and so

we focus on multi-way DWD hereafter. For the rank 1 multi-way model, the multi-way DWD

Step 1: Initialization. Generate the random numbers ˜w0

j , j = 1, . . . , p and ˜v0

k, k = 1, . . . , m

from a uniform distribution with range 0 to 1 and then set the initial values w0 = ˜w0

(cid:107) ˜w0(cid:107) , and

v0 = ˜v0

(cid:107)˜v0(cid:107) where ˜w0 = ( ˜w0

1, . . . , ˜w0

p)(cid:124) and ˜v0 = (˜v0

1, . . . , ˜v0

m)(cid:124). Compute the median of the

pairwise Euclidean distances between the two classes (Marron and others, 2007) and denote it as

algorithm is:

D.

Step 2: Iteration. In the (t + 1)th iteration step, ﬁrst, using vt, create a new dataset Xw

where the observation for each subject i is Xw

i = Xi · vt. Here X is the n × p × m data array, so

Xi is the p × m data matrix for subject i. Then update wt+1 by optimizing the DWD model to

ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = wt+1

1 X w

i1 + wt+1

2 X w

i2 + . . . + wt+1

p X w
ip.

Let dw denote the median of the pairwise Euclidean distances between the two classes in data

Xw; then the penalty parameter C in the DWD model corresponding to Xw is set as 100∗d2
D2

w

.

Second, using wt+1 we apply a similar approach to update vt+1.

Step 3: Convergence. At the end of each iteration step, we compute the coeﬃcients vector as

10

T. Lyu and others

(cid:126)Bt+1 = vt+1 ⊗ wt+1. If the Euclidean distance between (cid:126)Bt and (cid:126)Bt+1 is less than a pre-speciﬁed

threshold (cid:15), then the algorithm stops.

For the rank r model, we add an SVD procedure to assure the model is identiﬁable. The

algorithm is:

Step 1: Initialization. Generate the initial values for w0

z,j, j = 1, . . . , p and v0

z,k, k = 1, . . . , m

and z = 1, . . . , r from a uniform distribution with range 0 to 1. Compute the median of the

pairwise Euclidean distances between the two classes (Marron and others, 2007) and denote it as

D. Compute the coeﬃcient matrix ˜B0 = w0

1 · (v0

1)(cid:124) + . . . + w0

r · (v0

r)(cid:124) where w0

z = (w0

z,1, . . . , w0

z,p)(cid:124)

and v0

z = (v0

z,1, . . . , v0

z,m)(cid:124), z = 1, . . . , r and let B0

v = ˜B0
(cid:107) ˜B0(cid:107)

. The subscript v indicates that in the

ﬁrst iteration we will consider v0 ﬁxed to update w1; then we update v1.

Step 2: Iteration. In the (t + 1)th iteration, compute the SVD of Bt

v: Ut

p×rΣt

r×r(Vt

m×r)(cid:124).

Let vt

z be the zth column of Vt. Create a new dataset Xw where the observation for each

subject i is Xw

r)(cid:124))
i = ((Xi · vt
an rp×1 vector. Then update ˜wt+1 = (cid:0)( ˜wt+1

1)(cid:124), . . . , (Xi · vt

(cid:124)

= (cid:0)X w

i11, . . . , X w
)(cid:124)(cid:1)(cid:124)

r

i1p, . . . , X w

(cid:1)(cid:124)

which is

ir1, . . . , X w
irp
z = (cid:0) ˜wt+1

z1 , . . . , ˜wt+1
zp

(cid:1)(cid:124)

,

)(cid:124), . . . , ( ˜wt+1

1

where ˜wt+1

z = 1, . . . , r by optimizing the DWD objective to ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = ˜wt+1

11 X w

i11 + ˜wt+1

12 X w

i12 + . . . + ˜wt+1

rp X w

irp.

Let dw denote the median of the pairwise Euclidean distances between the two classes in data

Xw; then the penalty parameter C is set as 100·d2
D2

w

. Then update the coeﬃcient matrix:

Bt+1

w = ˜wt+1

1

· (vt

1)(cid:124) + . . . + ˜wt+1

r

· (vt

r)(cid:124).

After an SVD of Bt+1

w , we use an analogous approach to update ˜vt+1, and obtain a new coeﬃcient

matrix

Bt+1

v = w(t+1)

1

· (˜vt+1
1

)(cid:124) + . . . + w(t+1)

r

· (˜vt+1
r

)(cid:124).

Step 3: Convergence. At the end of each iteration step, we compute the coeﬃcients matrix

Discriminating sample groups with multi-way data

11

Bt+1
v

. If the Euclidean distance between (cid:126)Bt

v and (cid:126)Bt+1

v

is less than a pre-speciﬁed threshold (cid:15),

then the algorithm stops.

4. Simulation

4.1 Low rank model simulation and results

We illustrate and compare the full DWD model to the proposed rank 1 multi-way model and

rank r multi-way model in a simulation study. The R package “DWD” (Huang and others,

2012) was used to ﬁt the DWD model in each iteration step. Data were generated under several

conditions, including diﬀerent training sample sizes, diﬀerent multi-way array dimensions, and

diﬀerent structural forms distinguishing the two classes. For all scenarios, a training dataset with

sample size n was generated, with two classes of equal size (n0 = n1 = n/2). We consider the

values n = 40 and n = 100. The predictors have the form of a two-way array of dimensions p × m,

and we generate data under three diﬀerent array dimensions: 15 × 4, 20 × 10, and 500 × 30.

In each training dataset, n0 samples corresponding to class 0 were generated from a multi-

variate normal distribution N (µ0, Σe0), where µ0 is a pm × 1 vector and Σe0 = σ2

e0Ipm×pm. The

other n1 samples corresponding to class 1 were generated from a multivariate normal distribu-

tion N (µ1, Σe1), where µ1 is a pm × 1 vector and Σe1 = σ2

e1Ipm×pm. Under this model the Bayes

classiﬁcation rule, which classiﬁes a subject to class i only if its multivariate density under class

i is highest, takes the form of a linear classiﬁer in which the coeﬃcients are proportional to the

mean diﬀerence µ1 − µ0. This is easily shown by considering the diﬀerence in log density between

the two classes. In practice the generative probability distribution is unknown, but this oracle

rule may be used as a benchmark and motivates the three scenarios with diﬀerent structure in

the mean diﬀerence described below.

In the ﬁrst structural form scenario, the data were generated from the full model. We set

µ0 = (0, . . . , 0)(cid:124) and µ1 was generated from a multivariate normal distribution with mean zero

12

T. Lyu and others

and variance σ2

s Ipm×pm. In the second scenario, the data were generated from the rank 1 model.

We set µ0 = (0, . . . , 0)(cid:124). For µ1, we ﬁrst generated w from a multivariate normal distribution with

mean zero and variance σ2

wIp×p and v from a multivariate normal distribution with mean zero and

variance σ2

vIm×m. Then µ1 was determined by v ⊗ w where ⊗ denotes the Kronecker product. In

the third scenario, the data were generated from the rank 2 model. We ﬁrst generated w0 from a

multivariate normal distribution with mean zero and variance σ2
w0

Ip×p and v0 from a multivariate

normal distribution with mean zero and variance σ2
v0

Im×m and then µ0 was determined by

v0 ⊗ w0. Similarly, µ1 was determined by v1 ⊗ w1 where w1 was generated from a multivariate

normal distribution with mean zero and variance σ2
w1

Ip×p and v1 was from a multivariate normal

distribution with mean zero and variance σ2
v1

Im×m.

Under each scenario, corresponding test data were generated from the same distributions as

the training data with sample sizes n0 = n1 = 50. For each method we assess its misclassiﬁcation

rate on the test data, and the correlation of the estimated hyperplane and the true (Bayes)

hyperplane. The signal-to-noise ratio was adjusted for each scenario such that the misclassiﬁcation

rate of the full model is around 20% when n = 40. Each scenario was replicated 100 times.

The results in Table 1 and Figure 1 show that the model with the best performance (in

terms of the misclassiﬁcation rates and correlation with the truth) is the model from which

the data were generated under each scenario. When the sample size increases from 40 to 100,

the misclassiﬁcation rates are lower and the correlations between the estimated and the true

hyperplanes are higher. From Figure 1, we also observe that when the dimensions increase,

the diﬀerences in misclassiﬁcation rate between the appropriate model and alternative models

increase.

Discriminating sample groups with multi-way data

13

Table 1. Simulation results: “Mis” is the misclassiﬁcation rate and “SE(Mis)” is the standard
error of the misclassiﬁcation rate across the 200 simulated datasets. “Cor” is the correlation
between the estimated linear hyperplane and the true hyperplane. “SE(Cor)” is the standard error
of the correlation. The best correlation and misclassiﬁcation rate in each row are given in bold
font.

n

dimension true model

Mis

SE(Cor)

Mis

SE(Cor)

Mis

full model
SE(Mis) Cor

rank 1 model
SE(Mis) Cor

rank 2 model
SE(Mis) Cor

SE(Cor)

40

15 × 4

100

15 × 4

20 × 10

500 × 30

20 × 10

500 × 30

full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2

0.202
0.196
0.207
0.205
0.212
0.209
0.202
0.215
0.212
0.154
0.159
0.165
0.137
0.148
0.146
0.096
0.114
0.107

0.004
0.010
0.008
0.003
0.008
0.006
0.003
0.005
0.004
0.046
0.122
0.098
0.040
0.094
0.081
0.030
0.060
0.051

0.672
0.669
0.664
0.545
0.530
0.535
0.206
0.200
0.201
0.821
0.801
0.804
0.720
0.704
0.709
0.317
0.309
0.311

0.005
0.012
0.009
0.004
0.008
0.006
0.001
0.002
0.001
0.044
0.127
0.094
0.033
0.099
0.074
0.007
0.038
0.029

0.288
0.156
0.195
0.341
0.127
0.179
0.429
0.008
0.049
0.247
0.138
0.160
0.292
0.086
0.135
0.385
0.001
0.010

0.005
0.009
0.008
0.004
0.007
0.007
0.004
0.001
0.003
0.055
0.123
0.100
0.057
0.068
0.073
0.050
0.004
0.014

0.452
0.798
0.700
0.272
0.799
0.635
0.046
0.692
0.445
0.553
0.900
0.810
0.360
0.921
0.762
0.072
0.853
0.618

0.007
0.014
0.012
0.004
0.010
0.011
0.001
0.005
0.005
0.078
0.150
0.118
0.050
0.063
0.092
0.007
0.037
0.054

0.238
0.178
0.194
0.296
0.159
0.166
0.400
0.026
0.040
0.194
0.150
0.152
0.233
0.108
0.106
0.341
0.005
0.002

0.004
0.009
0.008
0.004
0.008
0.006
0.004
0.003
0.003
0.051
0.123
0.101
0.051
0.082
0.068
0.050
0.011
0.006

0.575
0.720
0.710
0.358
0.689
0.664
0.064
0.545
0.493
0.702
0.842
0.846
0.477
0.842
0.853
0.100
0.734
0.749

0.006
0.012
0.010
0.004
0.009
0.008
0.001
0.005
0.006
0.059
0.138
0.107
0.046
0.082
0.077
0.008
0.050
0.042

Fig. 1. Misclassiﬁcation rates with bars for ± 1.96 standard errors (across the 200 simulated datasets)
under each simulation scenario.

14

T. Lyu and others

4.2 High rank model simulation and results

When the dimension of the multi-way structure is 500 × 30, the eﬀective dimensions of the full

model, rank 1 model and rank 2 model are 500 × 30 = 15000, 500 + 30 = 530 and 500 +

499 + 30 + 29 = 1058 respectively. By this measure, the diﬀerence between the full model and

the rank 2 model is quite large. In order to evaluate the performance of rank r models with

eﬀective dimension in between, additional simulations were done for the multi-way structure with

dimension 500 × 30. We added two more scenarios where the simulated datasets were generated

under the rank 5 and rank 15 models and then applied the rank 5 and rank 15 models to all

datasets generated under the 500 × 30 structure. The results are shown in Figure 1, and illustrate

how diﬀerent low-rank approximations serve as a ﬂexible compromise between the rank 1 and

full models. In particular, as r increases, the performance of the rank r model approaches that

of the full model.

5. Real data analysis

The proposed methods were illustrated in two real data examples.

5.1 Magnetic Resonance Spectroscopy (MRS) data

We consider Magnetic Resonance Spectroscopy (MRS) data for a clinical research project that

enrolled patients with Spinocerebellar Ataxia Type 1 (SCA1) and healthy controls of similar age

and sex distribution. MRS is a non-invasive method using magnetic resonance imaging to quantify

neurochemicals. Here it was used to examine diﬀerences between patients and controls, and

ultimately to track changes in the brains of patients as the disease progresses. Participants were

imaged in a 3 Tesla scanner and the neurochemicals that were quantiﬁed included ascorbate (Asc),

γ-aminobutyric acid (GABA), glucose (Glc), glutamate (Glu), glutathione (GSH), myo-inositol

(Ins), scyllo-inositol (sIns), N-acetyl-aspertate (NAA), total choline (Pcho+GPC), total creatine

Discriminating sample groups with multi-way data

15

(Cr+PCr), total NAA (NAA+NAAG), glutamate plus glutamine (Glu+Gln), and glucose plus

taurine (Glc+Tau).

There were 17 patients and 24 controls enrolled in this study. The concentrations of the same

set of metabolites were measured in three diﬀerent brain regions (Pons, Cerebral Hemisphere,

and Vermis), yielding data with three dimensions: participants × metabolites × regions. Thus,

the data have a multi-way structure. We compared misclassiﬁcation rates for the full model and

the rank 1 multi-way DWD model by leave-one-out cross validation, which is robust to over-

ﬁtting. Each sample was separately left out of the estimation (to be the validation set), all the

other samples were used as training samples to construct the model, and then the model was

tested on the left-out sample. The two models gave the same leave-one-out misclassiﬁcation rate

of 4.88% and similar t-statistics (8.815 vs. 8.354 for rank 1 and full, respectively). The t-statistic

corresponded to testing the null hypothesis that the mean DWD scores of the two groups are

the same, where the DWD score for each sample is calculated from Equation 2.1. The DWD

scores under the rank 1 multi-way DWD model are shown in Figure 2, which shows that the

patients and controls are well separated. Note that one misclassiﬁed case patient scored in the

middle of the controls; this was a presymptomatic patient diagnosed by genetic screening (rather

than presentation of clinical symptoms), so it is reasonable that the rank 1 multiway DWD model

could not classify this patient correctly. The rank 1 multi-way model estimated a single weight for

each metabolite (v) and a single weight for each region (w), thus it has a simpler interpretation

compared to the full model. In order to estimate the 95% conﬁdence intervals of the estimated

weights, 5000 bootstrap samples were generated. For each bootstrap sample, 17 patients and 24

controls were resampled with replacement from the original 17 patients and 24 controls separately.

Then the model was ﬁt to the bootstrap sample to get the estimated weights. The 95% conﬁdence

interval was constructed based on the 2.5% and 97.5% quantiles of all the estimated weights based

on the bootstrap samples. The estimated weights and their 95% bootstrap conﬁdence intervals

16

T. Lyu and others

are shown in Figure 3. The metabolites with large absolute weights are considered important in

distinguishing the ataxia patients from the healthy controls.

Fig. 2. Rank 1 multi-way DWD scores under leave-one-out cross-validation for controls and patients, with
a kernel density estimate for each group.

5.2 Gene Time Course Data

We applied multi-way DWD to classify clinical response to treatment for Multiple Sclerosis (MS)

patients based on gene expression time course data. These data were originally described in

Baranzini and others (2005). Fifty-three patients were given recombinant human interferon beta

(rIFNβ), which is often used to control the symptoms of MS. Gene expression was measured for

76 genes of interest before treatment (baseline) and at 6 follow-up time points over the next two

years (3 months, 6 months, 9 months, 12 months, 18 months, 24 months), yielding a 3-way data

array: patients × genes × times. Afterward, patients were classiﬁed into good responders or poor

responders to rIFNβ based on clinical characteristics. Eﬃcient classiﬁcation of good and poor

Discriminating sample groups with multi-way data

17

Fig. 3. Rank 1 multi-way DWD weights for metabolites (top panel) and regions (bottom panel), with
95% conﬁdence intervals generated from 5000 bootstrap samples.

responders from the gene expression data is desired, for example to guide treatment decisions and

to better understand the IFNβ mechanism. The raw data are publicly available as a supplemental

ﬁle to Baranzini and others (2005).

We consider rank-r multi-way DWD to classify good and poor responders for each of r =

1, . . . , 7. The seven models were compared via leave-one-out cross validation estimation of the

mis-classiﬁcation rate. The rank-1 model, with a single weight for each gene and for each time

point, outperformed the others with the highest t-statistic (t = 7.58) and lowest misclassiﬁcation

rate (16.9%) under cross validation. The full model, with a distinct coeﬃcient for each gene ×

time pair, had a t-statistic of 5.38 and a misclassiﬁcation rate of 22.6% under cross-validation.

18

T. Lyu and others

The DWD scores under leave-one-out cross validation for the rank-1 multi-way model are

shown in Figure 4. This shows substantial but not perfect discrimination between the good and

poor responder groups. The coeﬃcient estimates for each gene and each time point, with 95%

bootstrap conﬁdence intervals, are shown in Figure 5. The four genes with the highest coeﬃcient

were Jak1, Caspase.9, STAT3, and IFN.gRa; the four genes with the highest negative coeﬃcient

were FAS, NFkBIA, IRF6, and ITGB2. The coeﬃcients across time had little variability and no

noticeable patterns. This suggests that the distinction between good and poor responders is not

driven by changes to gene expression in response to INFβ, but rather by baseline diﬀerences in

expression that can be quantiﬁed more precisely over multiple time points. This agrees with the

results in Baranzini and others (2005), who conducted an analysis of variance (ANOVA) for each

gene and report several signiﬁcant response and time eﬀects but no response*time interactions.

Fig. 4. Rank 1 multi-way DWD scores under leave-one-out cross-validation for good and poor treatment
responders, with a kernel density estimate for each group.

An alternative approach to classifying subjects based on gene expression time-course data is

Discriminating sample groups with multi-way data

19

Fig. 5. Rank 1 multi-way DWD weights for genes (top panel) and time points (bottom panel), with 95%
conﬁdence intervals generated from 5000 bootstrap samples.

described in Zhang and others (2013). Their method identiﬁes an optimal direction in time for

each gene using Fisher’s LDA, and then applies SVM or another high-dimensional classiﬁer to the

projections of each gene on its optimal direction. This approach is appropriate when discriminative

patterns over time are diﬀerent for each gene, but it does not explicitly capture patterns that are

shared across multiple genes. They assess classiﬁcation accuracy using the same IFNβ dataset

described above and achieve a minimum cross-validation error rate of approximately 26%, greater

than the error rate of 16.9% achieved by multi-way DWD.

20

T. Lyu and others

6. Discussion

Although data with multi-way structure is common in biomedical research, little work has ad-

dressed classiﬁcation of categorical outcomes from high-dimensional multi-way data. In this arti-

cle, we have proposed a general framework to extend linear classiﬁcation methods to multi-way

data. We mainly focus on the multi-way DWD model because of its ability to overcome the data-

piling problem of SVM and its good performance in simulations. Both the simulation and real

data analysis results show that the multi-way model can improve classiﬁcation accuracy when the

underlying true model has a multi-way structure and can provide a simple and straightforward

interpretation.

other means.

While the simple rank 1 classiﬁcation model performs well in our applications, it may perform

poorly for others. The simulations in Section 4 clearly demonstrate that low-rank models can be

a poor approximation if the underlying signal distinguishing the two classes does not have multi-

way structure. Therefore, we advise that the rank of the model should not be decided upon

blindly; rather, multiple models should be considered and compared via cross-validated errors or

The methodology described in Sections 2 and 3 may be extended in several ways. While

we implement multi-way classiﬁcation for a binary outcome, the framework can also be used to

extend multi-category classiﬁcation methods such as multiclass DWD (Huang and others, 2013).

Also in our implementation we focus on three-way data: samples × dim 1 × dim 2. The general

framework and iterative estimation technique may be extended to more dimensions; for higher-

order arrays the coeﬃcients may be represented as a rank-r PARAFAC decomposition. Sparse

versions of SVM and DWD have been developed, in which negligible coeﬃcients are shrunk to

0 (Bi and others, 2003; Wang and Zou, 2015). Sparse multi-way classiﬁcation, in which some of

the weights in each dimension are shrunk to 0, is another direction of future development.

Discriminating sample groups with multi-way data

21

Acknowledgments

We thank G¨ulin ¨Oz, Uzay Emir, and Dinesh Deelchand for providing the data and feedback for

the MRS application described in Section 5.1. We also thank Hanwen Huang for his help and

advice regarding the DWD package for R.

Conﬂict of Interest: None declared.

This work was supported by the National Institutes of Health grant ULI RR033183/KL2 RR0333182

[to EFL] and grant 1R01NS080816-01A1 [supporting TL and LEE].

Funding

APPENDIX

A. Multi-Way SVM

Details speciﬁc to the application of SVM to multi-way data are given below. For the rank 1

multi-way model, the multi-way SVM algorithm is:

Step 1: Initialization. generate the random numbers w0

j , j = 1, . . . , p and v0

k, k = 1, . . . , m

from a uniform distribution with range 0 to 1 and then set the initial values w0 = (w0

1, . . . , w0

p)(cid:124)

and v0 = (v0

1, . . . , v0

m)(cid:124).

Step 2: Iteration. in the (t + 1)th iteration step, ﬁrst, standardize vt by vt = vt

ﬁx vt, and create a new dataset Xw where the observation for each subject i is Xw

(cid:107)vt(cid:107) and then

i = Xi · vt.

Here X is the n × p × m data array, so Xi is the p × m data matrix for subject i. Then update

wt+1 by optimizing the SVM model to ﬁnd the hyperplane that:

f (Xw

i ) = wt+1

1 X w

i1 + wt+1

2 X w

i2 + . . . + wt+1

p X w
ip.

Second, we standardize and ﬁx wt+1 and then apply a similar approach to update vt+1.

Step 3: Convergence. in the end of each iteration step, we compute the coeﬃcients vector by

22

T. Lyu and others

bt+1 = vt+1 ⊗ wt+1. If the Euclidean diﬀerence between bt and bt+1 is less than a pre-speciﬁed

threshold (cid:15), then the algorithm stops.

For the rank r model, we add an SVD procedure to assure the model is identiﬁable. The

algorithm is:

z = 1, . . . , r.

Step 1: Initialization: generate the initial values for w0

z,j, j = 1, . . . , p and v0

z,k, k = 1, . . . , m

and z = 1, . . . , r from a uniform distribution with range 0 to 1. Compute the coeﬃcient matrix

˜B0 = w0

1 · (v0

1)(cid:124) + . . . + w0

r · (v0

r)(cid:124) where w0

z = (w0

z,1, . . . , w0

z,p)(cid:124) and v0

z = (v0

z,1, . . . , v0

z,m)(cid:124),

Step 2: Iteration: In the (t + 1)th iteration, compute the SVD of Bt

v: Ut

p×rΣt

r×r(Vt

m×r)(cid:124).

Let vt

z be the zth column of Vt. Create a new dataset Xw where the observation for each

subject i is Xw

r)(cid:124))
i = ((Xi · vt
an rp×1 vector. Then update ˜wt+1 = (cid:0)( ˜wt+1

1)(cid:124), . . . , (Xi · vt

(cid:124)

= (cid:0)X w

i11, . . . , X w
)(cid:124)(cid:1)(cid:124)

r

i1p, . . . , X w

(cid:1)(cid:124)

which is

ir1, . . . , X w
irp
z = (cid:0) ˜wt+1

z1 , . . . , ˜wt+1
zp

(cid:1)(cid:124)

,

)(cid:124), . . . , ( ˜wt+1

1

where ˜wt+1

z = 1, . . . , r by optimizing the SVM objective to ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = ˜wt+1

11 X w

i11 + ˜wt+1

12 X w

i12 + . . . + ˜wt+1

rp X w

irp.

Then update the coeﬃcient matrix:

Bt+1

w = ˜wt+1

1

· (vt

1)(cid:124) + . . . + ˜wt+1

r

· (vt

r)(cid:124).

After an SVD of Bt+1

w , we use an analogous approach to update ˜vt+1, and obtain a new coeﬃcient

matrix

Bt+1

v = w(t+1)

1

· (˜vt+1
1

)(cid:124) + . . . + w(t+1)

r

· (˜vt+1
r

)(cid:124).

Step 3: Convergence: in the end of each iteration step, we compute the coeﬃcients matrix

Bt+1
v

. If the Euclidean diﬀerence between (cid:126)Bt

v and (cid:126)Bt+1

v

is less than a pre-speciﬁed threshold (cid:15),

then the algorithm stops.

Note that the convergence of the multi-way SVM model depends more on the starting random

seed than the multi-way DWD model. So we try multiple starting random seeds and select the

REFERENCES

23

solution with the largest objective function value as the best solution.

The comparison of DWD and SVM was shown in Table 2. Only the results based on the model

from which the data were generated were reported. For example, when the data were generated

from the rank 1 model, then the comparison was between the rank 1 multi-way DWD model and

the rank 1 multi-way SVM model.

Table 2. Simulation results: “Mis” is the misclassiﬁcation rate and “SE(Mis)” is the standard
error of the misclassiﬁcation rate across the 200 simulated datasets. “Cor” is the absolute value
of the correlation between the estimated linear hyperplane and the true hyperplane (we take the
absolute value because SVM does not deﬁne a positive and negative class). “SE(Cor)” is the
standard error of the absolute correlation.

dimension true model

Mis

SE(Mis) Cor

SE(Cor)

Mis

SE(Mis) Cor

SE(Cor)

DWD

SVM

15 × 4

20 × 10

500 × 30

full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2

0.202
0.156
0.194
0.205
0.127
0.166
0.202
0.008
0.040

0.004
0.009
0.008
0.003
0.007
0.006
0.003
0.001
0.003

0.672
0.799
0.710
0.545
0.799
0.664
0.206
0.692
0.493

0.005
0.014
0.010
0.004
0.010
0.008
0.001
0.005
0.006

0.239
0.211
0.244
0.218
0.206
0.228
0.201
0.024
0.066

0.004
0.010
0.008
0.004
0.009
0.007
0.003
0.003
0.005

0.575
0.620
0.550
0.511
0.556
0.490
0.206
0.577
0.415

0.006
0.015
0.011
0.004
0.014
0.009
0.001
0.007
0.006

References

Allen, Genevera. (2012). Sparse higher-order principal components analysis. In: International

Conference on Artiﬁcial Intelligence and Statistics. pp. 27–36.

Baranzini, Sergio E, Mousavi, Parvin, Rio, Jordi, Caillier, Stacy J, Stillman,

Althea, Villoslada, Pablo, Wyatt, Matthew M, Comabella, Manuel, Greller,

Larry D, Somogyi, Roland and others. (2005). Transcription-based prediction of response

to IFNβ using supervised computational methods. PLoS Biol 3(1), e2.

Bauckhage, Christian. (2007). Robust tensor classiﬁers for color object recognition. In: Image

24

REFERENCES

Analysis and Recognition. Springer, pp. 352–363.

Bi, Jinbo, Bennett, Kristin, Embrechts, Mark, Breneman, Curt and Song, Minghu.

(2003). Dimensionality reduction via sparse support vector machines. The Journal of Machine

Learning Research 3, 1229–1243.

arXiv preprint arXiv:1305.0395 .

ing 20(3), 273–297.

Cichocki, Andrzej. (2013). Tensor decompositions: A new concept in brain data analysis?

Cortes, Corinna and Vapnik, Vladimir. (1995). Support-vector networks. Machine learn-

Gorski, Jochen, Pfeuffer, Frank and Klamroth, Kathrin. (2007). Biconvex sets and

optimization with biconvex functions: a survey and extensions. Mathematical Methods of Op-

erations Research 66(3), 373–407.

Harshman, Richard A. (1970). Foundations of the parafac procedure: Models and conditions

for an “explanatory” multi-modal factor analysis. UCLA Working Papers in Phonetics 16,

1–84.

Huang, Hanwen, Liu, Yufeng, Du, Ying, Perou, Charles M, Hayes, D Neil, Todd,

Michael J and Marron, James Stephen. (2013). Multiclass distance-weighted discrimi-

nation. Journal of Computational and Graphical Statistics 22(4), 953–969.

Huang, Hanwen, Lu, Xiaosun, Liu, Yufeng, Haaland, Perry and Marron, JS. (2012).

R/DWD: distance-weighted discrimination for classiﬁcation, visualization and batch adjust-

ment. Bioinformatics 28(8), 1182–1183.

Kolda, Tamara G and Bader, Brett W. (2009). Tensor decompositions and applications.

SIAM review 51(3), 455–500.

REFERENCES

25

Marron, J S, Todd, Michael J and Ahn, Jeongyoun. (2007). Distance-weighted discrim-

ination. Journal of the American Statistical Association 102(480), 1267–1271.

Miranda, Michelle, Zhu, Hongtu and Ibrahim, Joseph G. (2015). TPRM: Tensor par-

tition regression models with applications in imaging biomarker detection. arXiv preprint

arXiv:1505.05482 .

Tao, Dacheng, Li, Xuelong, Wu, Xindong, Hu, Weiming and Maybank, Stephen J.

(2007). Supervised tensor learning. Knowl. Inf. Syst. 13(1), 1–42.

Tucker, Ledyard R. (1966). Some mathematical notes on three-mode factor analysis. Psy-

chometrika 31(3), 279–311.

Wang, Boxiang and Zou, Hui. (2015). Sparse distance weighted discrimination. Journal of

Computational and Graphical Statistics (just-accepted), 00–00.

Ye, Jieping, Janardan, Ravi and Li, Qi. (2004). Two-dimensional linear discriminant anal-

ysis. In: Advances in Neural Information Processing Systems, Volume 17. pp. 1569–1576.

Zhang, Yuping, Tibshirani, Robert and Davis, Ronald. (2013). Classiﬁcation of patients

from time-course gene expression. Biostatistics 14(1), 87–98.

Zhou, Hua, Li, Lexin and Zhu, Hongtu. (2013). Tensor regression with applications in

neuroimaging data analysis. Journal of the American Statistical Association 108(502), 540–

552.

Zhou, Jing, Bhattacharya, Anirban, Herring, Amy H and Dunson, David B. (2015).

Bayesian factorizations of big sparse tensors. Journal of the American Statistical Associa-

tion 110(512), 1562–1576.

6
1
0
2
 
n
u
J
 
6
2
 
 
]
E
M

.
t
a
t
s
[
 
 
1
v
6
4
0
8
0
.
6
0
6
1
:
v
i
X
r
a

Discriminating sample groups with multi-way
data

Division of Biostatistics, School of Public Health, University of Minnesota,

Division of Biostatistics, School of Public Health, University of Minnesota,

TIANMENG LYU

Minneapolis, MN 55455, USA

ERIC F. LOCK∗

Minneapolis, MN 55455, USA

elock@umn.edu

LYNN E. EBERLY

Minneapolis, MN 55455, USA

Summary

Division of Biostatistics, School of Public Health, University of Minnesota,

High-dimensional linear classiﬁers, such as the support vector machine (SVM) and distance

weighted discrimination (DWD), are commonly used in biomedical research to distinguish groups

of subjects based on a large number of features. However, their use is limited to applications

where a single vector of features is measured for each subject. In practice data are often multi-

way, or measured over multiple dimensions. For example, metabolite abundance may be mea-

sured over multiple regions or tissues, or gene expression may be measured over multiple time

points, for the same subjects. We propose a framework for linear classiﬁcation of high-dimensional

∗To whom correspondence should be addressed.

1

2

T. Lyu and others

multi-way data, in which coeﬃcients can be factorized into weights that are speciﬁc to each di-

mension. More generally, the coeﬃcients for each measurement in a multi-way dataset are as-

sumed to have low-rank structure. This framework extends existing classiﬁcation techniques, and

we have implemented multi-way versions of SVM and DWD. We describe informative simula-

tion results, and apply multi-way DWD to data for two very diﬀerent clinical research studies.

The ﬁrst study uses metabolite magnetic resonance spectroscopy data over multiple brain re-

gions to compare patients with and without spinocerebellar ataxia, the second uses publicly

available gene expression time-course data to compare treatment responses for patients with

multiple sclerosis. Our method improves performance and simpliﬁes interpretation over naive

applications of full rank linear classiﬁcation to multi-way data. An R package is available at

https://github.com/lockEF/MultiwayClassification.

Key words: Classiﬁcation; Distance weighted discrimination; Gene time-course; magnetic resonance spec-

troscopy; Support vector machine; Tensors

1. Introduction

In biomedical research and other ﬁelds, data are often best represented as a multi-way array, also

called a tensor. A multi-way array simply extends the familiar two-way data matrix (e.g., Samples

× Variables) to higher dimensions. Multi-way data frequently arise from molecular proﬁling and

imaging modalities, where data may be measured over multiple body regions, tissue-types, or

developmental time points. Our motivating example is magnetic resonance spectroscopy (MRS)

measurement of the abundance of several metabolites in three brain regions for a common set

of participants: samples × metabolites × regions. We also consider gene expression time-course

data, in which the expression of many genes are measured over multiple time points: samples ×

genes × times.

Discriminating sample groups with multi-way data

3

There are a large number of exploratory factorization and dimension reduction techniques for

multi-way data. A detailed survey of these methods can be found in Kolda and Bader (2009). Two

classical methods are the PARAFAC (Harshman, 1970) decomposition and the Tucker (Tucker,

1966) decomposition, which extend well-known methods such as the singular value decomposition

and principal component analysis for a data matrix. These and similar factorization techniques

are frequently used in practice to analyze neuroimaging data (Cichocki, 2013) and in other bio-

statistical applications (Allen, 2012; Zhou and others, 2015).

In addition to exploratory approaches, there is also a small but growing literature on super-

vised methods for multi-way data, where the interest is to determine the relationship between

an outcome vector and covariates that have multi-way structure. Zhou and others (2013) pro-

posed tensor regression models which have a continuous clinical outcome as the outcome variable

and images that are covariates, formulated as multi-way arrays. In their model, covariate coeﬃ-

cients are assumed to have a PARAFAC structure. An analogous Bayesian formulation for tensor

regression models is described by Miranda and others (2015).

Our interest is in classiﬁcation of a categorical outcome from high-dimensional multi-way

data. Classiﬁcation methods that identify a hyperplane that provides linear separation between

two classes are commonly used in biomedical research to distinguish groups of subjects based

on several features, but these methods assume that each sample’s predictors are in a vector; the

methods can thus not be applied to multi-way data where each sample’s predictors are a matrix.

There has been some work to extend classiﬁers to the multi-way context in machine learning and

computer vision. Ye and others (2004) extended the traditional Fisher’s linear discriminant anal-

ysis (LDA) to two-way tensors and their application focused on dimension reduction of images.

Bauckhage (2007) extended the least mean squares approach for LDA to tensors by assuming

that the projection tensor can be given by the PARAFAC model. Tao and others (2007) proposed

a supervised tensor learning scheme which can be applied using diﬀerent learning methods such

4

T. Lyu and others

as support vector machines (SVM) (Cortes and Vapnik (1995)) and LDA; in their formulation,

coeﬃcients for the hyperplane can be factorized into a single set of weights that are speciﬁc to

each dimension (a rank-1 factorization).

Much of the previous work on supervised learning from multi-way data focuses on the tensor

version of LDA. However, LDA can result in overﬁtting and also the solution is not identiﬁable

when the number of predictors is larger than the sample size, which is frequently the case for

high-throughput biomedical data. An alternative is SVM, which identiﬁes a high-dimensional

hyperplane that separates two classes. The hyperplane is chosen to maximize distance between

cases and controls that are closest to the hyperplane; these samples deﬁne the support vectors.

But as shown in Marron and others (2007), SVM may suﬀer from the data piling problem, which

means if we project the data onto the normal vector of the separating hyperplane then many

points will pile up at the margin. In order to overcome the data piling problem in SVM, Marron

and others (2007) proposed the Distance Weighted Discrimination (DWD) method, which allows

every data point to aﬀect the estimation of the hyperplane.

In this article, we describe a general framework for classifying high-dimensional multi-way

data that extends existing linear classiﬁcation approaches. Our central assumption is that the

multi-way coeﬃcient matrix can be decomposed into patterns that are particular to each dimen-

sion, giving a low-rank representation. The coeﬃcients are estimated by iteratively updating the

weights in each dimension to optimize an objective function. This is shown to improve both in-

terpretation and performance over naive applications of linear classiﬁcation that “vectorize” each

sample’s multi-way structure and treat each array entry as a separate variable. We implement

our extended versions of both SVM and DWD for multi-way data, and ﬁnd that DWD gener-

ally performs better. In applications we illustrate how cross-validation can be used for model

assessment, and how bootstrapping can be used to assess the uncertainty of model estimates.

Previous work on supervised tensor learning has been primarily motivated by imaging data,

Discriminating sample groups with multi-way data

5

and our method can be applied to images. However, we are primarily motivated by applications

where each dimension of a multi-way array has a unique interpretation, and a rank-1 or low-rank

model makes intuitive sense. For the Metabolites × Regions data introduced above, we apply

multi-way DWD to distinguish patients with spinocerebellar ataxia type I (SCA1) from controls;

for the genes × times data we apply multi-way DWD to distinguish good and poor responders

to IFNβ treatment for multiple sclerosis (Baranzini and others, 2005). In both cases multi-way

DWD improves performance over the naive approach and allows for a simpler interpretation of

the results.

2. Methods

2.1 High-dimensional classiﬁcation

Here we brieﬂy describe linear classiﬁcation based on a high-dimensional covariate vector per

sample, before discussing the multi-way case in Section 2.2.

Suppose data are available for n subjects, each belonging to one of two classes which we denote

by +1 and −1. Let xi denote the p×1 vector of covariates for subject i, and let yi denote the class

labels yi ∈ {+1, −1}, i = 1, . . . , n. Deﬁne y = [y1, . . . , yn] : 1 × n and X = [x1, . . . , xn] : p × n.

The goal is to ﬁnd the hyperplane b = [b1, . . . , bp](cid:124) : p × 1 which best separates the two classes.

(cid:124)
i b should provide good discrimination between the two classes.
That is, the projections f (xi) = x

Performance is assessed via an objective function h(y, X, b, Θ), which is to be minimized. The

exact form of the objective function h and additional parameters Θ (if any) depend on the

method. Below we brieﬂy describe the objective functions for SVM and DWD, respectively.

SVM objective: SVM uses the hinge loss function. The optimization problem can be formulated

as

argmin
b,β

1
n

(cid:34)

(cid:88)

i

max(0, 1 − yi(x

(cid:124)
i b − β))

+ λ||b||2,

(cid:35)

where β is an intercept term and λ is a penalty parameter that determines the tradeoﬀ between

6

T. Lyu and others

the size of the hyperplane margin and correct classiﬁcation of the groups on either side of the

hyperplane.

DWD objective: In contrast to SVM, DWD allows every data point (sample) to inﬂuence b

by optimizing the sum of the inverse distances from the data points to the hyperplane. Let Y be

the n × n diagonal matrix with yi’s as the diagonal components. The optimization problem in

DWD can be formulated as

argmin
r ,b,β,ξ

(cid:88)

i

1
ri

+ C1(cid:48)ξ,

where 1 is the vector of 1’s, C is the penalty parameter and ξ is a penalized vector with the

following constraints:

r = Y X (cid:48)b + βy + ξ (cid:62) 0, (cid:107) b (cid:107)(cid:54) 1, ξ (cid:62) 0.

2.2 Naive (full) multi-way classiﬁcation

Now consider classiﬁcation of samples with array data samples × dim1 × dim2. Let xijk denote

the value of measurement under the jth characteristic of dim1 and the kth characteristic in dim2

for subject i where i = 1, . . . , n, j = 1, . . . , p and k = 1, . . . , m. A naive approach to extend linear

classiﬁers to multi-way data is to estimate the coeﬃcient bjk for each dim1,j × dim2,k pair in the

data array. Let

xi = [xi11, . . . , xip1, xi12, . . . , xip2, . . . , xi1m, . . . , xipm](cid:124) : pm × 1

denote the full vector of covariates for subject i, i = 1, . . . , n. Then the hyperplane that is used

to distinguish the two classes is

f (xi) = b11xi11 + b12xi12 + · · · + b1mxi1m + · · · + bp1xip1 + bp2xip2 + · · · + bpmxipm,

We deﬁne this naive approach as the full model and let B : p × m be the coeﬃcient array with

bjk as the (j, k)th component.

Discriminating sample groups with multi-way data

7

2.3 Rank 1 multi-way classiﬁcation

The full model in Section 2.2 estimates diﬀerent coeﬃcients for the same jth characteristic in

dim1 under diﬀerent characteristics in dim2. For example, if dim1 corresponds to metabolites

and dim2 corresponds to brain regions, then the full model estimates diﬀerent coeﬃcients for

the same metabolite, e.g., glucose, measured in diﬀerent brain regions. But the eﬀects of the

same metabolite in diﬀerent brain regions on the classiﬁcation of the two classes are very likely

correlated. The full model does not account for the known multi-way structure of the data and

ignores the possible correlation among the diﬀerent dim1,j × dim2,k pairs; hence, it may result

in less accurate classiﬁcation performance. The proposed multi-way model can be regarded as a

low rank approximation of the full model. The rank 1 multi-way model has the simplest form

among all of the low rank approximations and has a very straightforward interpretation: The

model assumes that the coeﬃcient matrix Bp×m has the rank 1 decomposition

Bp×m = wv(cid:124),

where w = [w1, . . . , wp](cid:124) denotes the vector of weights for dim1 and v = [v1, . . . , vm](cid:124) denotes

the vector of weights for dim2. Under this assumption, the hyperplane to separate the two classes

is:

f (Xi) = w1v1xi11 + w1v2xi12 + . . . + wpv1xip1 + wpv2xip2 + . . . + wpvmxipm

(2.1)

(cid:124)
= (v(cid:124)X
i )w
= (w(cid:124)Xi)v,

where wj, j = 1, . . . , p represents the weight on the jth characteristic of dim1 and vk, k = 1, . . . , m

represents the weight on the kth characteristic of dim2. Since we estimate the weights speciﬁc to

each dimension, and a larger absolute weight usually implies a more important characteristic in

terms of its inﬂuence on classiﬁcation, we interpret the importance of diﬀerent characteristics in

one dimension to be proportional across each level of the other dimension. The full model does

not assume any commonality to the eﬀects of characteristics in dim1 across the levels of dim2,

since the coeﬃcients are estimated separately.

8

T. Lyu and others

2.4 Rank r multi-way classiﬁcation

The rank 1 multi-way model assumes that the coeﬃcient matrix has a rank 1 decomposition,

but sometimes the rank 1 structure may not be able to represent all the information in the true

coeﬃcient matrix. For example, in the metabolites × regions example described in Section 1,

the rank 1 model assumes that there is only one distinguishing proﬁle of metabolites (v) but it

can be weighted diﬀerently across the diﬀerent regions (w). However, in reality, the truth might

be that there are multiple distinguishing metabolite proﬁles (v1, v2, · · · , vr), which should be

weighed diﬀerently across the diﬀerent regions (w1, w2, · · · , wr). Under such circumstances, we

need a more complicated model compared to the rank 1 multi-way model. We propose a rank r

model for the coeﬃcient matrix Bp×m, which can be viewed as a compromise between the full

model and the rank 1 multi-way model. The rank r multi-way model assumes that the coeﬃcient

matrix has the following decomposition:

Bp×m = w1v

(cid:124)

1 + · · · + wrv(cid:124)
r ,

(2.2)

where wz = [wz1, . . . , wzp](cid:124) and vz = [vz1, . . . , vzm](cid:124), z = 1, . . . , r, r < min(p, m).

Note that the rank r model is not immediately identiﬁable in terms of wz and vz for diﬀerent

values of z. However, factorization techniques such as the singular value decomposition (SVD)

can be used to obtain a unique representation of Equation (2.2).

3. Estimation

Here we describe a general approach to estimating the coeﬃcients for multi-way classiﬁcation,

in which an objective function is iteratively optimized over the weights in each dimension. If the

objective function at each iteration is convex, then the overall optimization problem is biconvex,

and our approach can be framed as an Alternate Convex Search (ACS) algorithm (Gorski and

Discriminating sample groups with multi-way data

9

others, 2007). The objective functions of DWD, SVM, and several other linear classiﬁers are

convex. Therefore, we can iteratively optimize their objective functions using the ACS algorithm:

h(y, X, B, Θ) = h(y, (cid:126)X, (cid:126)B, Θ),

where (cid:126)X : pm × n and (cid:126)B : pm × 1 correspond to the vectorized versions of X and B, respectively.

The generic algorithm proceeds by iteratively estimating w and v in B = wv(cid:124).

Below we give the algorithm in detail for multi-way DWD. Details speciﬁc to the application

of SVM to multi-way data are given in the Appendix A. In simulations and in practice, we ﬁnd

that both the basic version and multi-way version of DWD perform better than SVM, and so

we focus on multi-way DWD hereafter. For the rank 1 multi-way model, the multi-way DWD

Step 1: Initialization. Generate the random numbers ˜w0

j , j = 1, . . . , p and ˜v0

k, k = 1, . . . , m

from a uniform distribution with range 0 to 1 and then set the initial values w0 = ˜w0

(cid:107) ˜w0(cid:107) , and

v0 = ˜v0

(cid:107)˜v0(cid:107) where ˜w0 = ( ˜w0

1, . . . , ˜w0

p)(cid:124) and ˜v0 = (˜v0

1, . . . , ˜v0

m)(cid:124). Compute the median of the

pairwise Euclidean distances between the two classes (Marron and others, 2007) and denote it as

algorithm is:

D.

Step 2: Iteration. In the (t + 1)th iteration step, ﬁrst, using vt, create a new dataset Xw

where the observation for each subject i is Xw

i = Xi · vt. Here X is the n × p × m data array, so

Xi is the p × m data matrix for subject i. Then update wt+1 by optimizing the DWD model to

ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = wt+1

1 X w

i1 + wt+1

2 X w

i2 + . . . + wt+1

p X w
ip.

Let dw denote the median of the pairwise Euclidean distances between the two classes in data

Xw; then the penalty parameter C in the DWD model corresponding to Xw is set as 100∗d2
D2

w

.

Second, using wt+1 we apply a similar approach to update vt+1.

Step 3: Convergence. At the end of each iteration step, we compute the coeﬃcients vector as

10

T. Lyu and others

(cid:126)Bt+1 = vt+1 ⊗ wt+1. If the Euclidean distance between (cid:126)Bt and (cid:126)Bt+1 is less than a pre-speciﬁed

threshold (cid:15), then the algorithm stops.

For the rank r model, we add an SVD procedure to assure the model is identiﬁable. The

algorithm is:

Step 1: Initialization. Generate the initial values for w0

z,j, j = 1, . . . , p and v0

z,k, k = 1, . . . , m

and z = 1, . . . , r from a uniform distribution with range 0 to 1. Compute the median of the

pairwise Euclidean distances between the two classes (Marron and others, 2007) and denote it as

D. Compute the coeﬃcient matrix ˜B0 = w0

1 · (v0

1)(cid:124) + . . . + w0

r · (v0

r)(cid:124) where w0

z = (w0

z,1, . . . , w0

z,p)(cid:124)

and v0

z = (v0

z,1, . . . , v0

z,m)(cid:124), z = 1, . . . , r and let B0

v = ˜B0
(cid:107) ˜B0(cid:107)

. The subscript v indicates that in the

ﬁrst iteration we will consider v0 ﬁxed to update w1; then we update v1.

Step 2: Iteration. In the (t + 1)th iteration, compute the SVD of Bt

v: Ut

p×rΣt

r×r(Vt

m×r)(cid:124).

Let vt

z be the zth column of Vt. Create a new dataset Xw where the observation for each

subject i is Xw

r)(cid:124))
i = ((Xi · vt
an rp×1 vector. Then update ˜wt+1 = (cid:0)( ˜wt+1

1)(cid:124), . . . , (Xi · vt

(cid:124)

= (cid:0)X w

i11, . . . , X w
)(cid:124)(cid:1)(cid:124)

r

i1p, . . . , X w

(cid:1)(cid:124)

which is

ir1, . . . , X w
irp
z = (cid:0) ˜wt+1

z1 , . . . , ˜wt+1
zp

(cid:1)(cid:124)

,

)(cid:124), . . . , ( ˜wt+1

1

where ˜wt+1

z = 1, . . . , r by optimizing the DWD objective to ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = ˜wt+1

11 X w

i11 + ˜wt+1

12 X w

i12 + . . . + ˜wt+1

rp X w

irp.

Let dw denote the median of the pairwise Euclidean distances between the two classes in data

Xw; then the penalty parameter C is set as 100·d2
D2

w

. Then update the coeﬃcient matrix:

Bt+1

w = ˜wt+1

1

· (vt

1)(cid:124) + . . . + ˜wt+1

r

· (vt

r)(cid:124).

After an SVD of Bt+1

w , we use an analogous approach to update ˜vt+1, and obtain a new coeﬃcient

matrix

Bt+1

v = w(t+1)

1

· (˜vt+1
1

)(cid:124) + . . . + w(t+1)

r

· (˜vt+1
r

)(cid:124).

Step 3: Convergence. At the end of each iteration step, we compute the coeﬃcients matrix

Discriminating sample groups with multi-way data

11

Bt+1
v

. If the Euclidean distance between (cid:126)Bt

v and (cid:126)Bt+1

v

is less than a pre-speciﬁed threshold (cid:15),

then the algorithm stops.

4. Simulation

4.1 Low rank model simulation and results

We illustrate and compare the full DWD model to the proposed rank 1 multi-way model and

rank r multi-way model in a simulation study. The R package “DWD” (Huang and others,

2012) was used to ﬁt the DWD model in each iteration step. Data were generated under several

conditions, including diﬀerent training sample sizes, diﬀerent multi-way array dimensions, and

diﬀerent structural forms distinguishing the two classes. For all scenarios, a training dataset with

sample size n was generated, with two classes of equal size (n0 = n1 = n/2). We consider the

values n = 40 and n = 100. The predictors have the form of a two-way array of dimensions p × m,

and we generate data under three diﬀerent array dimensions: 15 × 4, 20 × 10, and 500 × 30.

In each training dataset, n0 samples corresponding to class 0 were generated from a multi-

variate normal distribution N (µ0, Σe0), where µ0 is a pm × 1 vector and Σe0 = σ2

e0Ipm×pm. The

other n1 samples corresponding to class 1 were generated from a multivariate normal distribu-

tion N (µ1, Σe1), where µ1 is a pm × 1 vector and Σe1 = σ2

e1Ipm×pm. Under this model the Bayes

classiﬁcation rule, which classiﬁes a subject to class i only if its multivariate density under class

i is highest, takes the form of a linear classiﬁer in which the coeﬃcients are proportional to the

mean diﬀerence µ1 − µ0. This is easily shown by considering the diﬀerence in log density between

the two classes. In practice the generative probability distribution is unknown, but this oracle

rule may be used as a benchmark and motivates the three scenarios with diﬀerent structure in

the mean diﬀerence described below.

In the ﬁrst structural form scenario, the data were generated from the full model. We set

µ0 = (0, . . . , 0)(cid:124) and µ1 was generated from a multivariate normal distribution with mean zero

12

T. Lyu and others

and variance σ2

s Ipm×pm. In the second scenario, the data were generated from the rank 1 model.

We set µ0 = (0, . . . , 0)(cid:124). For µ1, we ﬁrst generated w from a multivariate normal distribution with

mean zero and variance σ2

wIp×p and v from a multivariate normal distribution with mean zero and

variance σ2

vIm×m. Then µ1 was determined by v ⊗ w where ⊗ denotes the Kronecker product. In

the third scenario, the data were generated from the rank 2 model. We ﬁrst generated w0 from a

multivariate normal distribution with mean zero and variance σ2
w0

Ip×p and v0 from a multivariate

normal distribution with mean zero and variance σ2
v0

Im×m and then µ0 was determined by

v0 ⊗ w0. Similarly, µ1 was determined by v1 ⊗ w1 where w1 was generated from a multivariate

normal distribution with mean zero and variance σ2
w1

Ip×p and v1 was from a multivariate normal

distribution with mean zero and variance σ2
v1

Im×m.

Under each scenario, corresponding test data were generated from the same distributions as

the training data with sample sizes n0 = n1 = 50. For each method we assess its misclassiﬁcation

rate on the test data, and the correlation of the estimated hyperplane and the true (Bayes)

hyperplane. The signal-to-noise ratio was adjusted for each scenario such that the misclassiﬁcation

rate of the full model is around 20% when n = 40. Each scenario was replicated 100 times.

The results in Table 1 and Figure 1 show that the model with the best performance (in

terms of the misclassiﬁcation rates and correlation with the truth) is the model from which

the data were generated under each scenario. When the sample size increases from 40 to 100,

the misclassiﬁcation rates are lower and the correlations between the estimated and the true

hyperplanes are higher. From Figure 1, we also observe that when the dimensions increase,

the diﬀerences in misclassiﬁcation rate between the appropriate model and alternative models

increase.

Discriminating sample groups with multi-way data

13

Table 1. Simulation results: “Mis” is the misclassiﬁcation rate and “SE(Mis)” is the standard
error of the misclassiﬁcation rate across the 200 simulated datasets. “Cor” is the correlation
between the estimated linear hyperplane and the true hyperplane. “SE(Cor)” is the standard error
of the correlation. The best correlation and misclassiﬁcation rate in each row are given in bold
font.

n

dimension true model

Mis

SE(Cor)

Mis

SE(Cor)

Mis

full model
SE(Mis) Cor

rank 1 model
SE(Mis) Cor

rank 2 model
SE(Mis) Cor

SE(Cor)

40

15 × 4

100

15 × 4

20 × 10

500 × 30

20 × 10

500 × 30

full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2

0.202
0.196
0.207
0.205
0.212
0.209
0.202
0.215
0.212
0.154
0.159
0.165
0.137
0.148
0.146
0.096
0.114
0.107

0.004
0.010
0.008
0.003
0.008
0.006
0.003
0.005
0.004
0.046
0.122
0.098
0.040
0.094
0.081
0.030
0.060
0.051

0.672
0.669
0.664
0.545
0.530
0.535
0.206
0.200
0.201
0.821
0.801
0.804
0.720
0.704
0.709
0.317
0.309
0.311

0.005
0.012
0.009
0.004
0.008
0.006
0.001
0.002
0.001
0.044
0.127
0.094
0.033
0.099
0.074
0.007
0.038
0.029

0.288
0.156
0.195
0.341
0.127
0.179
0.429
0.008
0.049
0.247
0.138
0.160
0.292
0.086
0.135
0.385
0.001
0.010

0.005
0.009
0.008
0.004
0.007
0.007
0.004
0.001
0.003
0.055
0.123
0.100
0.057
0.068
0.073
0.050
0.004
0.014

0.452
0.798
0.700
0.272
0.799
0.635
0.046
0.692
0.445
0.553
0.900
0.810
0.360
0.921
0.762
0.072
0.853
0.618

0.007
0.014
0.012
0.004
0.010
0.011
0.001
0.005
0.005
0.078
0.150
0.118
0.050
0.063
0.092
0.007
0.037
0.054

0.238
0.178
0.194
0.296
0.159
0.166
0.400
0.026
0.040
0.194
0.150
0.152
0.233
0.108
0.106
0.341
0.005
0.002

0.004
0.009
0.008
0.004
0.008
0.006
0.004
0.003
0.003
0.051
0.123
0.101
0.051
0.082
0.068
0.050
0.011
0.006

0.575
0.720
0.710
0.358
0.689
0.664
0.064
0.545
0.493
0.702
0.842
0.846
0.477
0.842
0.853
0.100
0.734
0.749

0.006
0.012
0.010
0.004
0.009
0.008
0.001
0.005
0.006
0.059
0.138
0.107
0.046
0.082
0.077
0.008
0.050
0.042

Fig. 1. Misclassiﬁcation rates with bars for ± 1.96 standard errors (across the 200 simulated datasets)
under each simulation scenario.

14

T. Lyu and others

4.2 High rank model simulation and results

When the dimension of the multi-way structure is 500 × 30, the eﬀective dimensions of the full

model, rank 1 model and rank 2 model are 500 × 30 = 15000, 500 + 30 = 530 and 500 +

499 + 30 + 29 = 1058 respectively. By this measure, the diﬀerence between the full model and

the rank 2 model is quite large. In order to evaluate the performance of rank r models with

eﬀective dimension in between, additional simulations were done for the multi-way structure with

dimension 500 × 30. We added two more scenarios where the simulated datasets were generated

under the rank 5 and rank 15 models and then applied the rank 5 and rank 15 models to all

datasets generated under the 500 × 30 structure. The results are shown in Figure 1, and illustrate

how diﬀerent low-rank approximations serve as a ﬂexible compromise between the rank 1 and

full models. In particular, as r increases, the performance of the rank r model approaches that

of the full model.

5. Real data analysis

The proposed methods were illustrated in two real data examples.

5.1 Magnetic Resonance Spectroscopy (MRS) data

We consider Magnetic Resonance Spectroscopy (MRS) data for a clinical research project that

enrolled patients with Spinocerebellar Ataxia Type 1 (SCA1) and healthy controls of similar age

and sex distribution. MRS is a non-invasive method using magnetic resonance imaging to quantify

neurochemicals. Here it was used to examine diﬀerences between patients and controls, and

ultimately to track changes in the brains of patients as the disease progresses. Participants were

imaged in a 3 Tesla scanner and the neurochemicals that were quantiﬁed included ascorbate (Asc),

γ-aminobutyric acid (GABA), glucose (Glc), glutamate (Glu), glutathione (GSH), myo-inositol

(Ins), scyllo-inositol (sIns), N-acetyl-aspertate (NAA), total choline (Pcho+GPC), total creatine

Discriminating sample groups with multi-way data

15

(Cr+PCr), total NAA (NAA+NAAG), glutamate plus glutamine (Glu+Gln), and glucose plus

taurine (Glc+Tau).

There were 17 patients and 24 controls enrolled in this study. The concentrations of the same

set of metabolites were measured in three diﬀerent brain regions (Pons, Cerebral Hemisphere,

and Vermis), yielding data with three dimensions: participants × metabolites × regions. Thus,

the data have a multi-way structure. We compared misclassiﬁcation rates for the full model and

the rank 1 multi-way DWD model by leave-one-out cross validation, which is robust to over-

ﬁtting. Each sample was separately left out of the estimation (to be the validation set), all the

other samples were used as training samples to construct the model, and then the model was

tested on the left-out sample. The two models gave the same leave-one-out misclassiﬁcation rate

of 4.88% and similar t-statistics (8.815 vs. 8.354 for rank 1 and full, respectively). The t-statistic

corresponded to testing the null hypothesis that the mean DWD scores of the two groups are

the same, where the DWD score for each sample is calculated from Equation 2.1. The DWD

scores under the rank 1 multi-way DWD model are shown in Figure 2, which shows that the

patients and controls are well separated. Note that one misclassiﬁed case patient scored in the

middle of the controls; this was a presymptomatic patient diagnosed by genetic screening (rather

than presentation of clinical symptoms), so it is reasonable that the rank 1 multiway DWD model

could not classify this patient correctly. The rank 1 multi-way model estimated a single weight for

each metabolite (v) and a single weight for each region (w), thus it has a simpler interpretation

compared to the full model. In order to estimate the 95% conﬁdence intervals of the estimated

weights, 5000 bootstrap samples were generated. For each bootstrap sample, 17 patients and 24

controls were resampled with replacement from the original 17 patients and 24 controls separately.

Then the model was ﬁt to the bootstrap sample to get the estimated weights. The 95% conﬁdence

interval was constructed based on the 2.5% and 97.5% quantiles of all the estimated weights based

on the bootstrap samples. The estimated weights and their 95% bootstrap conﬁdence intervals

16

T. Lyu and others

are shown in Figure 3. The metabolites with large absolute weights are considered important in

distinguishing the ataxia patients from the healthy controls.

Fig. 2. Rank 1 multi-way DWD scores under leave-one-out cross-validation for controls and patients, with
a kernel density estimate for each group.

5.2 Gene Time Course Data

We applied multi-way DWD to classify clinical response to treatment for Multiple Sclerosis (MS)

patients based on gene expression time course data. These data were originally described in

Baranzini and others (2005). Fifty-three patients were given recombinant human interferon beta

(rIFNβ), which is often used to control the symptoms of MS. Gene expression was measured for

76 genes of interest before treatment (baseline) and at 6 follow-up time points over the next two

years (3 months, 6 months, 9 months, 12 months, 18 months, 24 months), yielding a 3-way data

array: patients × genes × times. Afterward, patients were classiﬁed into good responders or poor

responders to rIFNβ based on clinical characteristics. Eﬃcient classiﬁcation of good and poor

Discriminating sample groups with multi-way data

17

Fig. 3. Rank 1 multi-way DWD weights for metabolites (top panel) and regions (bottom panel), with
95% conﬁdence intervals generated from 5000 bootstrap samples.

responders from the gene expression data is desired, for example to guide treatment decisions and

to better understand the IFNβ mechanism. The raw data are publicly available as a supplemental

ﬁle to Baranzini and others (2005).

We consider rank-r multi-way DWD to classify good and poor responders for each of r =

1, . . . , 7. The seven models were compared via leave-one-out cross validation estimation of the

mis-classiﬁcation rate. The rank-1 model, with a single weight for each gene and for each time

point, outperformed the others with the highest t-statistic (t = 7.58) and lowest misclassiﬁcation

rate (16.9%) under cross validation. The full model, with a distinct coeﬃcient for each gene ×

time pair, had a t-statistic of 5.38 and a misclassiﬁcation rate of 22.6% under cross-validation.

18

T. Lyu and others

The DWD scores under leave-one-out cross validation for the rank-1 multi-way model are

shown in Figure 4. This shows substantial but not perfect discrimination between the good and

poor responder groups. The coeﬃcient estimates for each gene and each time point, with 95%

bootstrap conﬁdence intervals, are shown in Figure 5. The four genes with the highest coeﬃcient

were Jak1, Caspase.9, STAT3, and IFN.gRa; the four genes with the highest negative coeﬃcient

were FAS, NFkBIA, IRF6, and ITGB2. The coeﬃcients across time had little variability and no

noticeable patterns. This suggests that the distinction between good and poor responders is not

driven by changes to gene expression in response to INFβ, but rather by baseline diﬀerences in

expression that can be quantiﬁed more precisely over multiple time points. This agrees with the

results in Baranzini and others (2005), who conducted an analysis of variance (ANOVA) for each

gene and report several signiﬁcant response and time eﬀects but no response*time interactions.

Fig. 4. Rank 1 multi-way DWD scores under leave-one-out cross-validation for good and poor treatment
responders, with a kernel density estimate for each group.

An alternative approach to classifying subjects based on gene expression time-course data is

Discriminating sample groups with multi-way data

19

Fig. 5. Rank 1 multi-way DWD weights for genes (top panel) and time points (bottom panel), with 95%
conﬁdence intervals generated from 5000 bootstrap samples.

described in Zhang and others (2013). Their method identiﬁes an optimal direction in time for

each gene using Fisher’s LDA, and then applies SVM or another high-dimensional classiﬁer to the

projections of each gene on its optimal direction. This approach is appropriate when discriminative

patterns over time are diﬀerent for each gene, but it does not explicitly capture patterns that are

shared across multiple genes. They assess classiﬁcation accuracy using the same IFNβ dataset

described above and achieve a minimum cross-validation error rate of approximately 26%, greater

than the error rate of 16.9% achieved by multi-way DWD.

20

T. Lyu and others

6. Discussion

Although data with multi-way structure is common in biomedical research, little work has ad-

dressed classiﬁcation of categorical outcomes from high-dimensional multi-way data. In this arti-

cle, we have proposed a general framework to extend linear classiﬁcation methods to multi-way

data. We mainly focus on the multi-way DWD model because of its ability to overcome the data-

piling problem of SVM and its good performance in simulations. Both the simulation and real

data analysis results show that the multi-way model can improve classiﬁcation accuracy when the

underlying true model has a multi-way structure and can provide a simple and straightforward

interpretation.

other means.

While the simple rank 1 classiﬁcation model performs well in our applications, it may perform

poorly for others. The simulations in Section 4 clearly demonstrate that low-rank models can be

a poor approximation if the underlying signal distinguishing the two classes does not have multi-

way structure. Therefore, we advise that the rank of the model should not be decided upon

blindly; rather, multiple models should be considered and compared via cross-validated errors or

The methodology described in Sections 2 and 3 may be extended in several ways. While

we implement multi-way classiﬁcation for a binary outcome, the framework can also be used to

extend multi-category classiﬁcation methods such as multiclass DWD (Huang and others, 2013).

Also in our implementation we focus on three-way data: samples × dim 1 × dim 2. The general

framework and iterative estimation technique may be extended to more dimensions; for higher-

order arrays the coeﬃcients may be represented as a rank-r PARAFAC decomposition. Sparse

versions of SVM and DWD have been developed, in which negligible coeﬃcients are shrunk to

0 (Bi and others, 2003; Wang and Zou, 2015). Sparse multi-way classiﬁcation, in which some of

the weights in each dimension are shrunk to 0, is another direction of future development.

Discriminating sample groups with multi-way data

21

Acknowledgments

We thank G¨ulin ¨Oz, Uzay Emir, and Dinesh Deelchand for providing the data and feedback for

the MRS application described in Section 5.1. We also thank Hanwen Huang for his help and

advice regarding the DWD package for R.

Conﬂict of Interest: None declared.

This work was supported by the National Institutes of Health grant ULI RR033183/KL2 RR0333182

[to EFL] and grant 1R01NS080816-01A1 [supporting TL and LEE].

Funding

APPENDIX

A. Multi-Way SVM

Details speciﬁc to the application of SVM to multi-way data are given below. For the rank 1

multi-way model, the multi-way SVM algorithm is:

Step 1: Initialization. generate the random numbers w0

j , j = 1, . . . , p and v0

k, k = 1, . . . , m

from a uniform distribution with range 0 to 1 and then set the initial values w0 = (w0

1, . . . , w0

p)(cid:124)

and v0 = (v0

1, . . . , v0

m)(cid:124).

Step 2: Iteration. in the (t + 1)th iteration step, ﬁrst, standardize vt by vt = vt

ﬁx vt, and create a new dataset Xw where the observation for each subject i is Xw

(cid:107)vt(cid:107) and then

i = Xi · vt.

Here X is the n × p × m data array, so Xi is the p × m data matrix for subject i. Then update

wt+1 by optimizing the SVM model to ﬁnd the hyperplane that:

f (Xw

i ) = wt+1

1 X w

i1 + wt+1

2 X w

i2 + . . . + wt+1

p X w
ip.

Second, we standardize and ﬁx wt+1 and then apply a similar approach to update vt+1.

Step 3: Convergence. in the end of each iteration step, we compute the coeﬃcients vector by

22

T. Lyu and others

bt+1 = vt+1 ⊗ wt+1. If the Euclidean diﬀerence between bt and bt+1 is less than a pre-speciﬁed

threshold (cid:15), then the algorithm stops.

For the rank r model, we add an SVD procedure to assure the model is identiﬁable. The

algorithm is:

z = 1, . . . , r.

Step 1: Initialization: generate the initial values for w0

z,j, j = 1, . . . , p and v0

z,k, k = 1, . . . , m

and z = 1, . . . , r from a uniform distribution with range 0 to 1. Compute the coeﬃcient matrix

˜B0 = w0

1 · (v0

1)(cid:124) + . . . + w0

r · (v0

r)(cid:124) where w0

z = (w0

z,1, . . . , w0

z,p)(cid:124) and v0

z = (v0

z,1, . . . , v0

z,m)(cid:124),

Step 2: Iteration: In the (t + 1)th iteration, compute the SVD of Bt

v: Ut

p×rΣt

r×r(Vt

m×r)(cid:124).

Let vt

z be the zth column of Vt. Create a new dataset Xw where the observation for each

subject i is Xw

r)(cid:124))
i = ((Xi · vt
an rp×1 vector. Then update ˜wt+1 = (cid:0)( ˜wt+1

1)(cid:124), . . . , (Xi · vt

(cid:124)

= (cid:0)X w

i11, . . . , X w
)(cid:124)(cid:1)(cid:124)

r

i1p, . . . , X w

(cid:1)(cid:124)

which is

ir1, . . . , X w
irp
z = (cid:0) ˜wt+1

z1 , . . . , ˜wt+1
zp

(cid:1)(cid:124)

,

)(cid:124), . . . , ( ˜wt+1

1

where ˜wt+1

z = 1, . . . , r by optimizing the SVM objective to ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = ˜wt+1

11 X w

i11 + ˜wt+1

12 X w

i12 + . . . + ˜wt+1

rp X w

irp.

Then update the coeﬃcient matrix:

Bt+1

w = ˜wt+1

1

· (vt

1)(cid:124) + . . . + ˜wt+1

r

· (vt

r)(cid:124).

After an SVD of Bt+1

w , we use an analogous approach to update ˜vt+1, and obtain a new coeﬃcient

matrix

Bt+1

v = w(t+1)

1

· (˜vt+1
1

)(cid:124) + . . . + w(t+1)

r

· (˜vt+1
r

)(cid:124).

Step 3: Convergence: in the end of each iteration step, we compute the coeﬃcients matrix

Bt+1
v

. If the Euclidean diﬀerence between (cid:126)Bt

v and (cid:126)Bt+1

v

is less than a pre-speciﬁed threshold (cid:15),

then the algorithm stops.

Note that the convergence of the multi-way SVM model depends more on the starting random

seed than the multi-way DWD model. So we try multiple starting random seeds and select the

REFERENCES

23

solution with the largest objective function value as the best solution.

The comparison of DWD and SVM was shown in Table 2. Only the results based on the model

from which the data were generated were reported. For example, when the data were generated

from the rank 1 model, then the comparison was between the rank 1 multi-way DWD model and

the rank 1 multi-way SVM model.

Table 2. Simulation results: “Mis” is the misclassiﬁcation rate and “SE(Mis)” is the standard
error of the misclassiﬁcation rate across the 200 simulated datasets. “Cor” is the absolute value
of the correlation between the estimated linear hyperplane and the true hyperplane (we take the
absolute value because SVM does not deﬁne a positive and negative class). “SE(Cor)” is the
standard error of the absolute correlation.

dimension true model

Mis

SE(Mis) Cor

SE(Cor)

Mis

SE(Mis) Cor

SE(Cor)

DWD

SVM

15 × 4

20 × 10

500 × 30

full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2

0.202
0.156
0.194
0.205
0.127
0.166
0.202
0.008
0.040

0.004
0.009
0.008
0.003
0.007
0.006
0.003
0.001
0.003

0.672
0.799
0.710
0.545
0.799
0.664
0.206
0.692
0.493

0.005
0.014
0.010
0.004
0.010
0.008
0.001
0.005
0.006

0.239
0.211
0.244
0.218
0.206
0.228
0.201
0.024
0.066

0.004
0.010
0.008
0.004
0.009
0.007
0.003
0.003
0.005

0.575
0.620
0.550
0.511
0.556
0.490
0.206
0.577
0.415

0.006
0.015
0.011
0.004
0.014
0.009
0.001
0.007
0.006

References

Allen, Genevera. (2012). Sparse higher-order principal components analysis. In: International

Conference on Artiﬁcial Intelligence and Statistics. pp. 27–36.

Baranzini, Sergio E, Mousavi, Parvin, Rio, Jordi, Caillier, Stacy J, Stillman,

Althea, Villoslada, Pablo, Wyatt, Matthew M, Comabella, Manuel, Greller,

Larry D, Somogyi, Roland and others. (2005). Transcription-based prediction of response

to IFNβ using supervised computational methods. PLoS Biol 3(1), e2.

Bauckhage, Christian. (2007). Robust tensor classiﬁers for color object recognition. In: Image

24

REFERENCES

Analysis and Recognition. Springer, pp. 352–363.

Bi, Jinbo, Bennett, Kristin, Embrechts, Mark, Breneman, Curt and Song, Minghu.

(2003). Dimensionality reduction via sparse support vector machines. The Journal of Machine

Learning Research 3, 1229–1243.

arXiv preprint arXiv:1305.0395 .

ing 20(3), 273–297.

Cichocki, Andrzej. (2013). Tensor decompositions: A new concept in brain data analysis?

Cortes, Corinna and Vapnik, Vladimir. (1995). Support-vector networks. Machine learn-

Gorski, Jochen, Pfeuffer, Frank and Klamroth, Kathrin. (2007). Biconvex sets and

optimization with biconvex functions: a survey and extensions. Mathematical Methods of Op-

erations Research 66(3), 373–407.

Harshman, Richard A. (1970). Foundations of the parafac procedure: Models and conditions

for an “explanatory” multi-modal factor analysis. UCLA Working Papers in Phonetics 16,

1–84.

Huang, Hanwen, Liu, Yufeng, Du, Ying, Perou, Charles M, Hayes, D Neil, Todd,

Michael J and Marron, James Stephen. (2013). Multiclass distance-weighted discrimi-

nation. Journal of Computational and Graphical Statistics 22(4), 953–969.

Huang, Hanwen, Lu, Xiaosun, Liu, Yufeng, Haaland, Perry and Marron, JS. (2012).

R/DWD: distance-weighted discrimination for classiﬁcation, visualization and batch adjust-

ment. Bioinformatics 28(8), 1182–1183.

Kolda, Tamara G and Bader, Brett W. (2009). Tensor decompositions and applications.

SIAM review 51(3), 455–500.

REFERENCES

25

Marron, J S, Todd, Michael J and Ahn, Jeongyoun. (2007). Distance-weighted discrim-

ination. Journal of the American Statistical Association 102(480), 1267–1271.

Miranda, Michelle, Zhu, Hongtu and Ibrahim, Joseph G. (2015). TPRM: Tensor par-

tition regression models with applications in imaging biomarker detection. arXiv preprint

arXiv:1505.05482 .

Tao, Dacheng, Li, Xuelong, Wu, Xindong, Hu, Weiming and Maybank, Stephen J.

(2007). Supervised tensor learning. Knowl. Inf. Syst. 13(1), 1–42.

Tucker, Ledyard R. (1966). Some mathematical notes on three-mode factor analysis. Psy-

chometrika 31(3), 279–311.

Wang, Boxiang and Zou, Hui. (2015). Sparse distance weighted discrimination. Journal of

Computational and Graphical Statistics (just-accepted), 00–00.

Ye, Jieping, Janardan, Ravi and Li, Qi. (2004). Two-dimensional linear discriminant anal-

ysis. In: Advances in Neural Information Processing Systems, Volume 17. pp. 1569–1576.

Zhang, Yuping, Tibshirani, Robert and Davis, Ronald. (2013). Classiﬁcation of patients

from time-course gene expression. Biostatistics 14(1), 87–98.

Zhou, Hua, Li, Lexin and Zhu, Hongtu. (2013). Tensor regression with applications in

neuroimaging data analysis. Journal of the American Statistical Association 108(502), 540–

552.

Zhou, Jing, Bhattacharya, Anirban, Herring, Amy H and Dunson, David B. (2015).

Bayesian factorizations of big sparse tensors. Journal of the American Statistical Associa-

tion 110(512), 1562–1576.

6
1
0
2
 
n
u
J
 
6
2
 
 
]
E
M

.
t
a
t
s
[
 
 
1
v
6
4
0
8
0
.
6
0
6
1
:
v
i
X
r
a

Discriminating sample groups with multi-way
data

Division of Biostatistics, School of Public Health, University of Minnesota,

Division of Biostatistics, School of Public Health, University of Minnesota,

TIANMENG LYU

Minneapolis, MN 55455, USA

ERIC F. LOCK∗

Minneapolis, MN 55455, USA

elock@umn.edu

LYNN E. EBERLY

Minneapolis, MN 55455, USA

Summary

Division of Biostatistics, School of Public Health, University of Minnesota,

High-dimensional linear classiﬁers, such as the support vector machine (SVM) and distance

weighted discrimination (DWD), are commonly used in biomedical research to distinguish groups

of subjects based on a large number of features. However, their use is limited to applications

where a single vector of features is measured for each subject. In practice data are often multi-

way, or measured over multiple dimensions. For example, metabolite abundance may be mea-

sured over multiple regions or tissues, or gene expression may be measured over multiple time

points, for the same subjects. We propose a framework for linear classiﬁcation of high-dimensional

∗To whom correspondence should be addressed.

1

2

T. Lyu and others

multi-way data, in which coeﬃcients can be factorized into weights that are speciﬁc to each di-

mension. More generally, the coeﬃcients for each measurement in a multi-way dataset are as-

sumed to have low-rank structure. This framework extends existing classiﬁcation techniques, and

we have implemented multi-way versions of SVM and DWD. We describe informative simula-

tion results, and apply multi-way DWD to data for two very diﬀerent clinical research studies.

The ﬁrst study uses metabolite magnetic resonance spectroscopy data over multiple brain re-

gions to compare patients with and without spinocerebellar ataxia, the second uses publicly

available gene expression time-course data to compare treatment responses for patients with

multiple sclerosis. Our method improves performance and simpliﬁes interpretation over naive

applications of full rank linear classiﬁcation to multi-way data. An R package is available at

https://github.com/lockEF/MultiwayClassification.

Key words: Classiﬁcation; Distance weighted discrimination; Gene time-course; magnetic resonance spec-

troscopy; Support vector machine; Tensors

1. Introduction

In biomedical research and other ﬁelds, data are often best represented as a multi-way array, also

called a tensor. A multi-way array simply extends the familiar two-way data matrix (e.g., Samples

× Variables) to higher dimensions. Multi-way data frequently arise from molecular proﬁling and

imaging modalities, where data may be measured over multiple body regions, tissue-types, or

developmental time points. Our motivating example is magnetic resonance spectroscopy (MRS)

measurement of the abundance of several metabolites in three brain regions for a common set

of participants: samples × metabolites × regions. We also consider gene expression time-course

data, in which the expression of many genes are measured over multiple time points: samples ×

genes × times.

Discriminating sample groups with multi-way data

3

There are a large number of exploratory factorization and dimension reduction techniques for

multi-way data. A detailed survey of these methods can be found in Kolda and Bader (2009). Two

classical methods are the PARAFAC (Harshman, 1970) decomposition and the Tucker (Tucker,

1966) decomposition, which extend well-known methods such as the singular value decomposition

and principal component analysis for a data matrix. These and similar factorization techniques

are frequently used in practice to analyze neuroimaging data (Cichocki, 2013) and in other bio-

statistical applications (Allen, 2012; Zhou and others, 2015).

In addition to exploratory approaches, there is also a small but growing literature on super-

vised methods for multi-way data, where the interest is to determine the relationship between

an outcome vector and covariates that have multi-way structure. Zhou and others (2013) pro-

posed tensor regression models which have a continuous clinical outcome as the outcome variable

and images that are covariates, formulated as multi-way arrays. In their model, covariate coeﬃ-

cients are assumed to have a PARAFAC structure. An analogous Bayesian formulation for tensor

regression models is described by Miranda and others (2015).

Our interest is in classiﬁcation of a categorical outcome from high-dimensional multi-way

data. Classiﬁcation methods that identify a hyperplane that provides linear separation between

two classes are commonly used in biomedical research to distinguish groups of subjects based

on several features, but these methods assume that each sample’s predictors are in a vector; the

methods can thus not be applied to multi-way data where each sample’s predictors are a matrix.

There has been some work to extend classiﬁers to the multi-way context in machine learning and

computer vision. Ye and others (2004) extended the traditional Fisher’s linear discriminant anal-

ysis (LDA) to two-way tensors and their application focused on dimension reduction of images.

Bauckhage (2007) extended the least mean squares approach for LDA to tensors by assuming

that the projection tensor can be given by the PARAFAC model. Tao and others (2007) proposed

a supervised tensor learning scheme which can be applied using diﬀerent learning methods such

4

T. Lyu and others

as support vector machines (SVM) (Cortes and Vapnik (1995)) and LDA; in their formulation,

coeﬃcients for the hyperplane can be factorized into a single set of weights that are speciﬁc to

each dimension (a rank-1 factorization).

Much of the previous work on supervised learning from multi-way data focuses on the tensor

version of LDA. However, LDA can result in overﬁtting and also the solution is not identiﬁable

when the number of predictors is larger than the sample size, which is frequently the case for

high-throughput biomedical data. An alternative is SVM, which identiﬁes a high-dimensional

hyperplane that separates two classes. The hyperplane is chosen to maximize distance between

cases and controls that are closest to the hyperplane; these samples deﬁne the support vectors.

But as shown in Marron and others (2007), SVM may suﬀer from the data piling problem, which

means if we project the data onto the normal vector of the separating hyperplane then many

points will pile up at the margin. In order to overcome the data piling problem in SVM, Marron

and others (2007) proposed the Distance Weighted Discrimination (DWD) method, which allows

every data point to aﬀect the estimation of the hyperplane.

In this article, we describe a general framework for classifying high-dimensional multi-way

data that extends existing linear classiﬁcation approaches. Our central assumption is that the

multi-way coeﬃcient matrix can be decomposed into patterns that are particular to each dimen-

sion, giving a low-rank representation. The coeﬃcients are estimated by iteratively updating the

weights in each dimension to optimize an objective function. This is shown to improve both in-

terpretation and performance over naive applications of linear classiﬁcation that “vectorize” each

sample’s multi-way structure and treat each array entry as a separate variable. We implement

our extended versions of both SVM and DWD for multi-way data, and ﬁnd that DWD gener-

ally performs better. In applications we illustrate how cross-validation can be used for model

assessment, and how bootstrapping can be used to assess the uncertainty of model estimates.

Previous work on supervised tensor learning has been primarily motivated by imaging data,

Discriminating sample groups with multi-way data

5

and our method can be applied to images. However, we are primarily motivated by applications

where each dimension of a multi-way array has a unique interpretation, and a rank-1 or low-rank

model makes intuitive sense. For the Metabolites × Regions data introduced above, we apply

multi-way DWD to distinguish patients with spinocerebellar ataxia type I (SCA1) from controls;

for the genes × times data we apply multi-way DWD to distinguish good and poor responders

to IFNβ treatment for multiple sclerosis (Baranzini and others, 2005). In both cases multi-way

DWD improves performance over the naive approach and allows for a simpler interpretation of

the results.

2. Methods

2.1 High-dimensional classiﬁcation

Here we brieﬂy describe linear classiﬁcation based on a high-dimensional covariate vector per

sample, before discussing the multi-way case in Section 2.2.

Suppose data are available for n subjects, each belonging to one of two classes which we denote

by +1 and −1. Let xi denote the p×1 vector of covariates for subject i, and let yi denote the class

labels yi ∈ {+1, −1}, i = 1, . . . , n. Deﬁne y = [y1, . . . , yn] : 1 × n and X = [x1, . . . , xn] : p × n.

The goal is to ﬁnd the hyperplane b = [b1, . . . , bp](cid:124) : p × 1 which best separates the two classes.

(cid:124)
i b should provide good discrimination between the two classes.
That is, the projections f (xi) = x

Performance is assessed via an objective function h(y, X, b, Θ), which is to be minimized. The

exact form of the objective function h and additional parameters Θ (if any) depend on the

method. Below we brieﬂy describe the objective functions for SVM and DWD, respectively.

SVM objective: SVM uses the hinge loss function. The optimization problem can be formulated

as

argmin
b,β

1
n

(cid:34)

(cid:88)

i

max(0, 1 − yi(x

(cid:124)
i b − β))

+ λ||b||2,

(cid:35)

where β is an intercept term and λ is a penalty parameter that determines the tradeoﬀ between

6

T. Lyu and others

the size of the hyperplane margin and correct classiﬁcation of the groups on either side of the

hyperplane.

DWD objective: In contrast to SVM, DWD allows every data point (sample) to inﬂuence b

by optimizing the sum of the inverse distances from the data points to the hyperplane. Let Y be

the n × n diagonal matrix with yi’s as the diagonal components. The optimization problem in

DWD can be formulated as

argmin
r ,b,β,ξ

(cid:88)

i

1
ri

+ C1(cid:48)ξ,

where 1 is the vector of 1’s, C is the penalty parameter and ξ is a penalized vector with the

following constraints:

r = Y X (cid:48)b + βy + ξ (cid:62) 0, (cid:107) b (cid:107)(cid:54) 1, ξ (cid:62) 0.

2.2 Naive (full) multi-way classiﬁcation

Now consider classiﬁcation of samples with array data samples × dim1 × dim2. Let xijk denote

the value of measurement under the jth characteristic of dim1 and the kth characteristic in dim2

for subject i where i = 1, . . . , n, j = 1, . . . , p and k = 1, . . . , m. A naive approach to extend linear

classiﬁers to multi-way data is to estimate the coeﬃcient bjk for each dim1,j × dim2,k pair in the

data array. Let

xi = [xi11, . . . , xip1, xi12, . . . , xip2, . . . , xi1m, . . . , xipm](cid:124) : pm × 1

denote the full vector of covariates for subject i, i = 1, . . . , n. Then the hyperplane that is used

to distinguish the two classes is

f (xi) = b11xi11 + b12xi12 + · · · + b1mxi1m + · · · + bp1xip1 + bp2xip2 + · · · + bpmxipm,

We deﬁne this naive approach as the full model and let B : p × m be the coeﬃcient array with

bjk as the (j, k)th component.

Discriminating sample groups with multi-way data

7

2.3 Rank 1 multi-way classiﬁcation

The full model in Section 2.2 estimates diﬀerent coeﬃcients for the same jth characteristic in

dim1 under diﬀerent characteristics in dim2. For example, if dim1 corresponds to metabolites

and dim2 corresponds to brain regions, then the full model estimates diﬀerent coeﬃcients for

the same metabolite, e.g., glucose, measured in diﬀerent brain regions. But the eﬀects of the

same metabolite in diﬀerent brain regions on the classiﬁcation of the two classes are very likely

correlated. The full model does not account for the known multi-way structure of the data and

ignores the possible correlation among the diﬀerent dim1,j × dim2,k pairs; hence, it may result

in less accurate classiﬁcation performance. The proposed multi-way model can be regarded as a

low rank approximation of the full model. The rank 1 multi-way model has the simplest form

among all of the low rank approximations and has a very straightforward interpretation: The

model assumes that the coeﬃcient matrix Bp×m has the rank 1 decomposition

Bp×m = wv(cid:124),

where w = [w1, . . . , wp](cid:124) denotes the vector of weights for dim1 and v = [v1, . . . , vm](cid:124) denotes

the vector of weights for dim2. Under this assumption, the hyperplane to separate the two classes

is:

f (Xi) = w1v1xi11 + w1v2xi12 + . . . + wpv1xip1 + wpv2xip2 + . . . + wpvmxipm

(2.1)

(cid:124)
= (v(cid:124)X
i )w
= (w(cid:124)Xi)v,

where wj, j = 1, . . . , p represents the weight on the jth characteristic of dim1 and vk, k = 1, . . . , m

represents the weight on the kth characteristic of dim2. Since we estimate the weights speciﬁc to

each dimension, and a larger absolute weight usually implies a more important characteristic in

terms of its inﬂuence on classiﬁcation, we interpret the importance of diﬀerent characteristics in

one dimension to be proportional across each level of the other dimension. The full model does

not assume any commonality to the eﬀects of characteristics in dim1 across the levels of dim2,

since the coeﬃcients are estimated separately.

8

T. Lyu and others

2.4 Rank r multi-way classiﬁcation

The rank 1 multi-way model assumes that the coeﬃcient matrix has a rank 1 decomposition,

but sometimes the rank 1 structure may not be able to represent all the information in the true

coeﬃcient matrix. For example, in the metabolites × regions example described in Section 1,

the rank 1 model assumes that there is only one distinguishing proﬁle of metabolites (v) but it

can be weighted diﬀerently across the diﬀerent regions (w). However, in reality, the truth might

be that there are multiple distinguishing metabolite proﬁles (v1, v2, · · · , vr), which should be

weighed diﬀerently across the diﬀerent regions (w1, w2, · · · , wr). Under such circumstances, we

need a more complicated model compared to the rank 1 multi-way model. We propose a rank r

model for the coeﬃcient matrix Bp×m, which can be viewed as a compromise between the full

model and the rank 1 multi-way model. The rank r multi-way model assumes that the coeﬃcient

matrix has the following decomposition:

Bp×m = w1v

(cid:124)

1 + · · · + wrv(cid:124)
r ,

(2.2)

where wz = [wz1, . . . , wzp](cid:124) and vz = [vz1, . . . , vzm](cid:124), z = 1, . . . , r, r < min(p, m).

Note that the rank r model is not immediately identiﬁable in terms of wz and vz for diﬀerent

values of z. However, factorization techniques such as the singular value decomposition (SVD)

can be used to obtain a unique representation of Equation (2.2).

3. Estimation

Here we describe a general approach to estimating the coeﬃcients for multi-way classiﬁcation,

in which an objective function is iteratively optimized over the weights in each dimension. If the

objective function at each iteration is convex, then the overall optimization problem is biconvex,

and our approach can be framed as an Alternate Convex Search (ACS) algorithm (Gorski and

Discriminating sample groups with multi-way data

9

others, 2007). The objective functions of DWD, SVM, and several other linear classiﬁers are

convex. Therefore, we can iteratively optimize their objective functions using the ACS algorithm:

h(y, X, B, Θ) = h(y, (cid:126)X, (cid:126)B, Θ),

where (cid:126)X : pm × n and (cid:126)B : pm × 1 correspond to the vectorized versions of X and B, respectively.

The generic algorithm proceeds by iteratively estimating w and v in B = wv(cid:124).

Below we give the algorithm in detail for multi-way DWD. Details speciﬁc to the application

of SVM to multi-way data are given in the Appendix A. In simulations and in practice, we ﬁnd

that both the basic version and multi-way version of DWD perform better than SVM, and so

we focus on multi-way DWD hereafter. For the rank 1 multi-way model, the multi-way DWD

Step 1: Initialization. Generate the random numbers ˜w0

j , j = 1, . . . , p and ˜v0

k, k = 1, . . . , m

from a uniform distribution with range 0 to 1 and then set the initial values w0 = ˜w0

(cid:107) ˜w0(cid:107) , and

v0 = ˜v0

(cid:107)˜v0(cid:107) where ˜w0 = ( ˜w0

1, . . . , ˜w0

p)(cid:124) and ˜v0 = (˜v0

1, . . . , ˜v0

m)(cid:124). Compute the median of the

pairwise Euclidean distances between the two classes (Marron and others, 2007) and denote it as

algorithm is:

D.

Step 2: Iteration. In the (t + 1)th iteration step, ﬁrst, using vt, create a new dataset Xw

where the observation for each subject i is Xw

i = Xi · vt. Here X is the n × p × m data array, so

Xi is the p × m data matrix for subject i. Then update wt+1 by optimizing the DWD model to

ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = wt+1

1 X w

i1 + wt+1

2 X w

i2 + . . . + wt+1

p X w
ip.

Let dw denote the median of the pairwise Euclidean distances between the two classes in data

Xw; then the penalty parameter C in the DWD model corresponding to Xw is set as 100∗d2
D2

w

.

Second, using wt+1 we apply a similar approach to update vt+1.

Step 3: Convergence. At the end of each iteration step, we compute the coeﬃcients vector as

10

T. Lyu and others

(cid:126)Bt+1 = vt+1 ⊗ wt+1. If the Euclidean distance between (cid:126)Bt and (cid:126)Bt+1 is less than a pre-speciﬁed

threshold (cid:15), then the algorithm stops.

For the rank r model, we add an SVD procedure to assure the model is identiﬁable. The

algorithm is:

Step 1: Initialization. Generate the initial values for w0

z,j, j = 1, . . . , p and v0

z,k, k = 1, . . . , m

and z = 1, . . . , r from a uniform distribution with range 0 to 1. Compute the median of the

pairwise Euclidean distances between the two classes (Marron and others, 2007) and denote it as

D. Compute the coeﬃcient matrix ˜B0 = w0

1 · (v0

1)(cid:124) + . . . + w0

r · (v0

r)(cid:124) where w0

z = (w0

z,1, . . . , w0

z,p)(cid:124)

and v0

z = (v0

z,1, . . . , v0

z,m)(cid:124), z = 1, . . . , r and let B0

v = ˜B0
(cid:107) ˜B0(cid:107)

. The subscript v indicates that in the

ﬁrst iteration we will consider v0 ﬁxed to update w1; then we update v1.

Step 2: Iteration. In the (t + 1)th iteration, compute the SVD of Bt

v: Ut

p×rΣt

r×r(Vt

m×r)(cid:124).

Let vt

z be the zth column of Vt. Create a new dataset Xw where the observation for each

subject i is Xw

r)(cid:124))
i = ((Xi · vt
an rp×1 vector. Then update ˜wt+1 = (cid:0)( ˜wt+1

1)(cid:124), . . . , (Xi · vt

(cid:124)

= (cid:0)X w

i11, . . . , X w
)(cid:124)(cid:1)(cid:124)

r

i1p, . . . , X w

(cid:1)(cid:124)

which is

ir1, . . . , X w
irp
z = (cid:0) ˜wt+1

z1 , . . . , ˜wt+1
zp

(cid:1)(cid:124)

,

)(cid:124), . . . , ( ˜wt+1

1

where ˜wt+1

z = 1, . . . , r by optimizing the DWD objective to ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = ˜wt+1

11 X w

i11 + ˜wt+1

12 X w

i12 + . . . + ˜wt+1

rp X w

irp.

Let dw denote the median of the pairwise Euclidean distances between the two classes in data

Xw; then the penalty parameter C is set as 100·d2
D2

w

. Then update the coeﬃcient matrix:

Bt+1

w = ˜wt+1

1

· (vt

1)(cid:124) + . . . + ˜wt+1

r

· (vt

r)(cid:124).

After an SVD of Bt+1

w , we use an analogous approach to update ˜vt+1, and obtain a new coeﬃcient

matrix

Bt+1

v = w(t+1)

1

· (˜vt+1
1

)(cid:124) + . . . + w(t+1)

r

· (˜vt+1
r

)(cid:124).

Step 3: Convergence. At the end of each iteration step, we compute the coeﬃcients matrix

Discriminating sample groups with multi-way data

11

Bt+1
v

. If the Euclidean distance between (cid:126)Bt

v and (cid:126)Bt+1

v

is less than a pre-speciﬁed threshold (cid:15),

then the algorithm stops.

4. Simulation

4.1 Low rank model simulation and results

We illustrate and compare the full DWD model to the proposed rank 1 multi-way model and

rank r multi-way model in a simulation study. The R package “DWD” (Huang and others,

2012) was used to ﬁt the DWD model in each iteration step. Data were generated under several

conditions, including diﬀerent training sample sizes, diﬀerent multi-way array dimensions, and

diﬀerent structural forms distinguishing the two classes. For all scenarios, a training dataset with

sample size n was generated, with two classes of equal size (n0 = n1 = n/2). We consider the

values n = 40 and n = 100. The predictors have the form of a two-way array of dimensions p × m,

and we generate data under three diﬀerent array dimensions: 15 × 4, 20 × 10, and 500 × 30.

In each training dataset, n0 samples corresponding to class 0 were generated from a multi-

variate normal distribution N (µ0, Σe0), where µ0 is a pm × 1 vector and Σe0 = σ2

e0Ipm×pm. The

other n1 samples corresponding to class 1 were generated from a multivariate normal distribu-

tion N (µ1, Σe1), where µ1 is a pm × 1 vector and Σe1 = σ2

e1Ipm×pm. Under this model the Bayes

classiﬁcation rule, which classiﬁes a subject to class i only if its multivariate density under class

i is highest, takes the form of a linear classiﬁer in which the coeﬃcients are proportional to the

mean diﬀerence µ1 − µ0. This is easily shown by considering the diﬀerence in log density between

the two classes. In practice the generative probability distribution is unknown, but this oracle

rule may be used as a benchmark and motivates the three scenarios with diﬀerent structure in

the mean diﬀerence described below.

In the ﬁrst structural form scenario, the data were generated from the full model. We set

µ0 = (0, . . . , 0)(cid:124) and µ1 was generated from a multivariate normal distribution with mean zero

12

T. Lyu and others

and variance σ2

s Ipm×pm. In the second scenario, the data were generated from the rank 1 model.

We set µ0 = (0, . . . , 0)(cid:124). For µ1, we ﬁrst generated w from a multivariate normal distribution with

mean zero and variance σ2

wIp×p and v from a multivariate normal distribution with mean zero and

variance σ2

vIm×m. Then µ1 was determined by v ⊗ w where ⊗ denotes the Kronecker product. In

the third scenario, the data were generated from the rank 2 model. We ﬁrst generated w0 from a

multivariate normal distribution with mean zero and variance σ2
w0

Ip×p and v0 from a multivariate

normal distribution with mean zero and variance σ2
v0

Im×m and then µ0 was determined by

v0 ⊗ w0. Similarly, µ1 was determined by v1 ⊗ w1 where w1 was generated from a multivariate

normal distribution with mean zero and variance σ2
w1

Ip×p and v1 was from a multivariate normal

distribution with mean zero and variance σ2
v1

Im×m.

Under each scenario, corresponding test data were generated from the same distributions as

the training data with sample sizes n0 = n1 = 50. For each method we assess its misclassiﬁcation

rate on the test data, and the correlation of the estimated hyperplane and the true (Bayes)

hyperplane. The signal-to-noise ratio was adjusted for each scenario such that the misclassiﬁcation

rate of the full model is around 20% when n = 40. Each scenario was replicated 100 times.

The results in Table 1 and Figure 1 show that the model with the best performance (in

terms of the misclassiﬁcation rates and correlation with the truth) is the model from which

the data were generated under each scenario. When the sample size increases from 40 to 100,

the misclassiﬁcation rates are lower and the correlations between the estimated and the true

hyperplanes are higher. From Figure 1, we also observe that when the dimensions increase,

the diﬀerences in misclassiﬁcation rate between the appropriate model and alternative models

increase.

Discriminating sample groups with multi-way data

13

Table 1. Simulation results: “Mis” is the misclassiﬁcation rate and “SE(Mis)” is the standard
error of the misclassiﬁcation rate across the 200 simulated datasets. “Cor” is the correlation
between the estimated linear hyperplane and the true hyperplane. “SE(Cor)” is the standard error
of the correlation. The best correlation and misclassiﬁcation rate in each row are given in bold
font.

n

dimension true model

Mis

SE(Cor)

Mis

SE(Cor)

Mis

full model
SE(Mis) Cor

rank 1 model
SE(Mis) Cor

rank 2 model
SE(Mis) Cor

SE(Cor)

40

15 × 4

100

15 × 4

20 × 10

500 × 30

20 × 10

500 × 30

full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2

0.202
0.196
0.207
0.205
0.212
0.209
0.202
0.215
0.212
0.154
0.159
0.165
0.137
0.148
0.146
0.096
0.114
0.107

0.004
0.010
0.008
0.003
0.008
0.006
0.003
0.005
0.004
0.046
0.122
0.098
0.040
0.094
0.081
0.030
0.060
0.051

0.672
0.669
0.664
0.545
0.530
0.535
0.206
0.200
0.201
0.821
0.801
0.804
0.720
0.704
0.709
0.317
0.309
0.311

0.005
0.012
0.009
0.004
0.008
0.006
0.001
0.002
0.001
0.044
0.127
0.094
0.033
0.099
0.074
0.007
0.038
0.029

0.288
0.156
0.195
0.341
0.127
0.179
0.429
0.008
0.049
0.247
0.138
0.160
0.292
0.086
0.135
0.385
0.001
0.010

0.005
0.009
0.008
0.004
0.007
0.007
0.004
0.001
0.003
0.055
0.123
0.100
0.057
0.068
0.073
0.050
0.004
0.014

0.452
0.798
0.700
0.272
0.799
0.635
0.046
0.692
0.445
0.553
0.900
0.810
0.360
0.921
0.762
0.072
0.853
0.618

0.007
0.014
0.012
0.004
0.010
0.011
0.001
0.005
0.005
0.078
0.150
0.118
0.050
0.063
0.092
0.007
0.037
0.054

0.238
0.178
0.194
0.296
0.159
0.166
0.400
0.026
0.040
0.194
0.150
0.152
0.233
0.108
0.106
0.341
0.005
0.002

0.004
0.009
0.008
0.004
0.008
0.006
0.004
0.003
0.003
0.051
0.123
0.101
0.051
0.082
0.068
0.050
0.011
0.006

0.575
0.720
0.710
0.358
0.689
0.664
0.064
0.545
0.493
0.702
0.842
0.846
0.477
0.842
0.853
0.100
0.734
0.749

0.006
0.012
0.010
0.004
0.009
0.008
0.001
0.005
0.006
0.059
0.138
0.107
0.046
0.082
0.077
0.008
0.050
0.042

Fig. 1. Misclassiﬁcation rates with bars for ± 1.96 standard errors (across the 200 simulated datasets)
under each simulation scenario.

14

T. Lyu and others

4.2 High rank model simulation and results

When the dimension of the multi-way structure is 500 × 30, the eﬀective dimensions of the full

model, rank 1 model and rank 2 model are 500 × 30 = 15000, 500 + 30 = 530 and 500 +

499 + 30 + 29 = 1058 respectively. By this measure, the diﬀerence between the full model and

the rank 2 model is quite large. In order to evaluate the performance of rank r models with

eﬀective dimension in between, additional simulations were done for the multi-way structure with

dimension 500 × 30. We added two more scenarios where the simulated datasets were generated

under the rank 5 and rank 15 models and then applied the rank 5 and rank 15 models to all

datasets generated under the 500 × 30 structure. The results are shown in Figure 1, and illustrate

how diﬀerent low-rank approximations serve as a ﬂexible compromise between the rank 1 and

full models. In particular, as r increases, the performance of the rank r model approaches that

of the full model.

5. Real data analysis

The proposed methods were illustrated in two real data examples.

5.1 Magnetic Resonance Spectroscopy (MRS) data

We consider Magnetic Resonance Spectroscopy (MRS) data for a clinical research project that

enrolled patients with Spinocerebellar Ataxia Type 1 (SCA1) and healthy controls of similar age

and sex distribution. MRS is a non-invasive method using magnetic resonance imaging to quantify

neurochemicals. Here it was used to examine diﬀerences between patients and controls, and

ultimately to track changes in the brains of patients as the disease progresses. Participants were

imaged in a 3 Tesla scanner and the neurochemicals that were quantiﬁed included ascorbate (Asc),

γ-aminobutyric acid (GABA), glucose (Glc), glutamate (Glu), glutathione (GSH), myo-inositol

(Ins), scyllo-inositol (sIns), N-acetyl-aspertate (NAA), total choline (Pcho+GPC), total creatine

Discriminating sample groups with multi-way data

15

(Cr+PCr), total NAA (NAA+NAAG), glutamate plus glutamine (Glu+Gln), and glucose plus

taurine (Glc+Tau).

There were 17 patients and 24 controls enrolled in this study. The concentrations of the same

set of metabolites were measured in three diﬀerent brain regions (Pons, Cerebral Hemisphere,

and Vermis), yielding data with three dimensions: participants × metabolites × regions. Thus,

the data have a multi-way structure. We compared misclassiﬁcation rates for the full model and

the rank 1 multi-way DWD model by leave-one-out cross validation, which is robust to over-

ﬁtting. Each sample was separately left out of the estimation (to be the validation set), all the

other samples were used as training samples to construct the model, and then the model was

tested on the left-out sample. The two models gave the same leave-one-out misclassiﬁcation rate

of 4.88% and similar t-statistics (8.815 vs. 8.354 for rank 1 and full, respectively). The t-statistic

corresponded to testing the null hypothesis that the mean DWD scores of the two groups are

the same, where the DWD score for each sample is calculated from Equation 2.1. The DWD

scores under the rank 1 multi-way DWD model are shown in Figure 2, which shows that the

patients and controls are well separated. Note that one misclassiﬁed case patient scored in the

middle of the controls; this was a presymptomatic patient diagnosed by genetic screening (rather

than presentation of clinical symptoms), so it is reasonable that the rank 1 multiway DWD model

could not classify this patient correctly. The rank 1 multi-way model estimated a single weight for

each metabolite (v) and a single weight for each region (w), thus it has a simpler interpretation

compared to the full model. In order to estimate the 95% conﬁdence intervals of the estimated

weights, 5000 bootstrap samples were generated. For each bootstrap sample, 17 patients and 24

controls were resampled with replacement from the original 17 patients and 24 controls separately.

Then the model was ﬁt to the bootstrap sample to get the estimated weights. The 95% conﬁdence

interval was constructed based on the 2.5% and 97.5% quantiles of all the estimated weights based

on the bootstrap samples. The estimated weights and their 95% bootstrap conﬁdence intervals

16

T. Lyu and others

are shown in Figure 3. The metabolites with large absolute weights are considered important in

distinguishing the ataxia patients from the healthy controls.

Fig. 2. Rank 1 multi-way DWD scores under leave-one-out cross-validation for controls and patients, with
a kernel density estimate for each group.

5.2 Gene Time Course Data

We applied multi-way DWD to classify clinical response to treatment for Multiple Sclerosis (MS)

patients based on gene expression time course data. These data were originally described in

Baranzini and others (2005). Fifty-three patients were given recombinant human interferon beta

(rIFNβ), which is often used to control the symptoms of MS. Gene expression was measured for

76 genes of interest before treatment (baseline) and at 6 follow-up time points over the next two

years (3 months, 6 months, 9 months, 12 months, 18 months, 24 months), yielding a 3-way data

array: patients × genes × times. Afterward, patients were classiﬁed into good responders or poor

responders to rIFNβ based on clinical characteristics. Eﬃcient classiﬁcation of good and poor

Discriminating sample groups with multi-way data

17

Fig. 3. Rank 1 multi-way DWD weights for metabolites (top panel) and regions (bottom panel), with
95% conﬁdence intervals generated from 5000 bootstrap samples.

responders from the gene expression data is desired, for example to guide treatment decisions and

to better understand the IFNβ mechanism. The raw data are publicly available as a supplemental

ﬁle to Baranzini and others (2005).

We consider rank-r multi-way DWD to classify good and poor responders for each of r =

1, . . . , 7. The seven models were compared via leave-one-out cross validation estimation of the

mis-classiﬁcation rate. The rank-1 model, with a single weight for each gene and for each time

point, outperformed the others with the highest t-statistic (t = 7.58) and lowest misclassiﬁcation

rate (16.9%) under cross validation. The full model, with a distinct coeﬃcient for each gene ×

time pair, had a t-statistic of 5.38 and a misclassiﬁcation rate of 22.6% under cross-validation.

18

T. Lyu and others

The DWD scores under leave-one-out cross validation for the rank-1 multi-way model are

shown in Figure 4. This shows substantial but not perfect discrimination between the good and

poor responder groups. The coeﬃcient estimates for each gene and each time point, with 95%

bootstrap conﬁdence intervals, are shown in Figure 5. The four genes with the highest coeﬃcient

were Jak1, Caspase.9, STAT3, and IFN.gRa; the four genes with the highest negative coeﬃcient

were FAS, NFkBIA, IRF6, and ITGB2. The coeﬃcients across time had little variability and no

noticeable patterns. This suggests that the distinction between good and poor responders is not

driven by changes to gene expression in response to INFβ, but rather by baseline diﬀerences in

expression that can be quantiﬁed more precisely over multiple time points. This agrees with the

results in Baranzini and others (2005), who conducted an analysis of variance (ANOVA) for each

gene and report several signiﬁcant response and time eﬀects but no response*time interactions.

Fig. 4. Rank 1 multi-way DWD scores under leave-one-out cross-validation for good and poor treatment
responders, with a kernel density estimate for each group.

An alternative approach to classifying subjects based on gene expression time-course data is

Discriminating sample groups with multi-way data

19

Fig. 5. Rank 1 multi-way DWD weights for genes (top panel) and time points (bottom panel), with 95%
conﬁdence intervals generated from 5000 bootstrap samples.

described in Zhang and others (2013). Their method identiﬁes an optimal direction in time for

each gene using Fisher’s LDA, and then applies SVM or another high-dimensional classiﬁer to the

projections of each gene on its optimal direction. This approach is appropriate when discriminative

patterns over time are diﬀerent for each gene, but it does not explicitly capture patterns that are

shared across multiple genes. They assess classiﬁcation accuracy using the same IFNβ dataset

described above and achieve a minimum cross-validation error rate of approximately 26%, greater

than the error rate of 16.9% achieved by multi-way DWD.

20

T. Lyu and others

6. Discussion

Although data with multi-way structure is common in biomedical research, little work has ad-

dressed classiﬁcation of categorical outcomes from high-dimensional multi-way data. In this arti-

cle, we have proposed a general framework to extend linear classiﬁcation methods to multi-way

data. We mainly focus on the multi-way DWD model because of its ability to overcome the data-

piling problem of SVM and its good performance in simulations. Both the simulation and real

data analysis results show that the multi-way model can improve classiﬁcation accuracy when the

underlying true model has a multi-way structure and can provide a simple and straightforward

interpretation.

other means.

While the simple rank 1 classiﬁcation model performs well in our applications, it may perform

poorly for others. The simulations in Section 4 clearly demonstrate that low-rank models can be

a poor approximation if the underlying signal distinguishing the two classes does not have multi-

way structure. Therefore, we advise that the rank of the model should not be decided upon

blindly; rather, multiple models should be considered and compared via cross-validated errors or

The methodology described in Sections 2 and 3 may be extended in several ways. While

we implement multi-way classiﬁcation for a binary outcome, the framework can also be used to

extend multi-category classiﬁcation methods such as multiclass DWD (Huang and others, 2013).

Also in our implementation we focus on three-way data: samples × dim 1 × dim 2. The general

framework and iterative estimation technique may be extended to more dimensions; for higher-

order arrays the coeﬃcients may be represented as a rank-r PARAFAC decomposition. Sparse

versions of SVM and DWD have been developed, in which negligible coeﬃcients are shrunk to

0 (Bi and others, 2003; Wang and Zou, 2015). Sparse multi-way classiﬁcation, in which some of

the weights in each dimension are shrunk to 0, is another direction of future development.

Discriminating sample groups with multi-way data

21

Acknowledgments

We thank G¨ulin ¨Oz, Uzay Emir, and Dinesh Deelchand for providing the data and feedback for

the MRS application described in Section 5.1. We also thank Hanwen Huang for his help and

advice regarding the DWD package for R.

Conﬂict of Interest: None declared.

This work was supported by the National Institutes of Health grant ULI RR033183/KL2 RR0333182

[to EFL] and grant 1R01NS080816-01A1 [supporting TL and LEE].

Funding

APPENDIX

A. Multi-Way SVM

Details speciﬁc to the application of SVM to multi-way data are given below. For the rank 1

multi-way model, the multi-way SVM algorithm is:

Step 1: Initialization. generate the random numbers w0

j , j = 1, . . . , p and v0

k, k = 1, . . . , m

from a uniform distribution with range 0 to 1 and then set the initial values w0 = (w0

1, . . . , w0

p)(cid:124)

and v0 = (v0

1, . . . , v0

m)(cid:124).

Step 2: Iteration. in the (t + 1)th iteration step, ﬁrst, standardize vt by vt = vt

ﬁx vt, and create a new dataset Xw where the observation for each subject i is Xw

(cid:107)vt(cid:107) and then

i = Xi · vt.

Here X is the n × p × m data array, so Xi is the p × m data matrix for subject i. Then update

wt+1 by optimizing the SVM model to ﬁnd the hyperplane that:

f (Xw

i ) = wt+1

1 X w

i1 + wt+1

2 X w

i2 + . . . + wt+1

p X w
ip.

Second, we standardize and ﬁx wt+1 and then apply a similar approach to update vt+1.

Step 3: Convergence. in the end of each iteration step, we compute the coeﬃcients vector by

22

T. Lyu and others

bt+1 = vt+1 ⊗ wt+1. If the Euclidean diﬀerence between bt and bt+1 is less than a pre-speciﬁed

threshold (cid:15), then the algorithm stops.

For the rank r model, we add an SVD procedure to assure the model is identiﬁable. The

algorithm is:

z = 1, . . . , r.

Step 1: Initialization: generate the initial values for w0

z,j, j = 1, . . . , p and v0

z,k, k = 1, . . . , m

and z = 1, . . . , r from a uniform distribution with range 0 to 1. Compute the coeﬃcient matrix

˜B0 = w0

1 · (v0

1)(cid:124) + . . . + w0

r · (v0

r)(cid:124) where w0

z = (w0

z,1, . . . , w0

z,p)(cid:124) and v0

z = (v0

z,1, . . . , v0

z,m)(cid:124),

Step 2: Iteration: In the (t + 1)th iteration, compute the SVD of Bt

v: Ut

p×rΣt

r×r(Vt

m×r)(cid:124).

Let vt

z be the zth column of Vt. Create a new dataset Xw where the observation for each

subject i is Xw

r)(cid:124))
i = ((Xi · vt
an rp×1 vector. Then update ˜wt+1 = (cid:0)( ˜wt+1

1)(cid:124), . . . , (Xi · vt

(cid:124)

= (cid:0)X w

i11, . . . , X w
)(cid:124)(cid:1)(cid:124)

r

i1p, . . . , X w

(cid:1)(cid:124)

which is

ir1, . . . , X w
irp
z = (cid:0) ˜wt+1

z1 , . . . , ˜wt+1
zp

(cid:1)(cid:124)

,

)(cid:124), . . . , ( ˜wt+1

1

where ˜wt+1

z = 1, . . . , r by optimizing the SVM objective to ﬁnd the hyperplane deﬁned by:

f (Xw

i ) = ˜wt+1

11 X w

i11 + ˜wt+1

12 X w

i12 + . . . + ˜wt+1

rp X w

irp.

Then update the coeﬃcient matrix:

Bt+1

w = ˜wt+1

1

· (vt

1)(cid:124) + . . . + ˜wt+1

r

· (vt

r)(cid:124).

After an SVD of Bt+1

w , we use an analogous approach to update ˜vt+1, and obtain a new coeﬃcient

matrix

Bt+1

v = w(t+1)

1

· (˜vt+1
1

)(cid:124) + . . . + w(t+1)

r

· (˜vt+1
r

)(cid:124).

Step 3: Convergence: in the end of each iteration step, we compute the coeﬃcients matrix

Bt+1
v

. If the Euclidean diﬀerence between (cid:126)Bt

v and (cid:126)Bt+1

v

is less than a pre-speciﬁed threshold (cid:15),

then the algorithm stops.

Note that the convergence of the multi-way SVM model depends more on the starting random

seed than the multi-way DWD model. So we try multiple starting random seeds and select the

REFERENCES

23

solution with the largest objective function value as the best solution.

The comparison of DWD and SVM was shown in Table 2. Only the results based on the model

from which the data were generated were reported. For example, when the data were generated

from the rank 1 model, then the comparison was between the rank 1 multi-way DWD model and

the rank 1 multi-way SVM model.

Table 2. Simulation results: “Mis” is the misclassiﬁcation rate and “SE(Mis)” is the standard
error of the misclassiﬁcation rate across the 200 simulated datasets. “Cor” is the absolute value
of the correlation between the estimated linear hyperplane and the true hyperplane (we take the
absolute value because SVM does not deﬁne a positive and negative class). “SE(Cor)” is the
standard error of the absolute correlation.

dimension true model

Mis

SE(Mis) Cor

SE(Cor)

Mis

SE(Mis) Cor

SE(Cor)

DWD

SVM

15 × 4

20 × 10

500 × 30

full
rank 1
rank 2
full
rank 1
rank 2
full
rank 1
rank 2

0.202
0.156
0.194
0.205
0.127
0.166
0.202
0.008
0.040

0.004
0.009
0.008
0.003
0.007
0.006
0.003
0.001
0.003

0.672
0.799
0.710
0.545
0.799
0.664
0.206
0.692
0.493

0.005
0.014
0.010
0.004
0.010
0.008
0.001
0.005
0.006

0.239
0.211
0.244
0.218
0.206
0.228
0.201
0.024
0.066

0.004
0.010
0.008
0.004
0.009
0.007
0.003
0.003
0.005

0.575
0.620
0.550
0.511
0.556
0.490
0.206
0.577
0.415

0.006
0.015
0.011
0.004
0.014
0.009
0.001
0.007
0.006

References

Allen, Genevera. (2012). Sparse higher-order principal components analysis. In: International

Conference on Artiﬁcial Intelligence and Statistics. pp. 27–36.

Baranzini, Sergio E, Mousavi, Parvin, Rio, Jordi, Caillier, Stacy J, Stillman,

Althea, Villoslada, Pablo, Wyatt, Matthew M, Comabella, Manuel, Greller,

Larry D, Somogyi, Roland and others. (2005). Transcription-based prediction of response

to IFNβ using supervised computational methods. PLoS Biol 3(1), e2.

Bauckhage, Christian. (2007). Robust tensor classiﬁers for color object recognition. In: Image

24

REFERENCES

Analysis and Recognition. Springer, pp. 352–363.

Bi, Jinbo, Bennett, Kristin, Embrechts, Mark, Breneman, Curt and Song, Minghu.

(2003). Dimensionality reduction via sparse support vector machines. The Journal of Machine

Learning Research 3, 1229–1243.

arXiv preprint arXiv:1305.0395 .

ing 20(3), 273–297.

Cichocki, Andrzej. (2013). Tensor decompositions: A new concept in brain data analysis?

Cortes, Corinna and Vapnik, Vladimir. (1995). Support-vector networks. Machine learn-

Gorski, Jochen, Pfeuffer, Frank and Klamroth, Kathrin. (2007). Biconvex sets and

optimization with biconvex functions: a survey and extensions. Mathematical Methods of Op-

erations Research 66(3), 373–407.

Harshman, Richard A. (1970). Foundations of the parafac procedure: Models and conditions

for an “explanatory” multi-modal factor analysis. UCLA Working Papers in Phonetics 16,

1–84.

Huang, Hanwen, Liu, Yufeng, Du, Ying, Perou, Charles M, Hayes, D Neil, Todd,

Michael J and Marron, James Stephen. (2013). Multiclass distance-weighted discrimi-

nation. Journal of Computational and Graphical Statistics 22(4), 953–969.

Huang, Hanwen, Lu, Xiaosun, Liu, Yufeng, Haaland, Perry and Marron, JS. (2012).

R/DWD: distance-weighted discrimination for classiﬁcation, visualization and batch adjust-

ment. Bioinformatics 28(8), 1182–1183.

Kolda, Tamara G and Bader, Brett W. (2009). Tensor decompositions and applications.

SIAM review 51(3), 455–500.

REFERENCES

25

Marron, J S, Todd, Michael J and Ahn, Jeongyoun. (2007). Distance-weighted discrim-

ination. Journal of the American Statistical Association 102(480), 1267–1271.

Miranda, Michelle, Zhu, Hongtu and Ibrahim, Joseph G. (2015). TPRM: Tensor par-

tition regression models with applications in imaging biomarker detection. arXiv preprint

arXiv:1505.05482 .

Tao, Dacheng, Li, Xuelong, Wu, Xindong, Hu, Weiming and Maybank, Stephen J.

(2007). Supervised tensor learning. Knowl. Inf. Syst. 13(1), 1–42.

Tucker, Ledyard R. (1966). Some mathematical notes on three-mode factor analysis. Psy-

chometrika 31(3), 279–311.

Wang, Boxiang and Zou, Hui. (2015). Sparse distance weighted discrimination. Journal of

Computational and Graphical Statistics (just-accepted), 00–00.

Ye, Jieping, Janardan, Ravi and Li, Qi. (2004). Two-dimensional linear discriminant anal-

ysis. In: Advances in Neural Information Processing Systems, Volume 17. pp. 1569–1576.

Zhang, Yuping, Tibshirani, Robert and Davis, Ronald. (2013). Classiﬁcation of patients

from time-course gene expression. Biostatistics 14(1), 87–98.

Zhou, Hua, Li, Lexin and Zhu, Hongtu. (2013). Tensor regression with applications in

neuroimaging data analysis. Journal of the American Statistical Association 108(502), 540–

552.

Zhou, Jing, Bhattacharya, Anirban, Herring, Amy H and Dunson, David B. (2015).

Bayesian factorizations of big sparse tensors. Journal of the American Statistical Associa-

tion 110(512), 1562–1576.

