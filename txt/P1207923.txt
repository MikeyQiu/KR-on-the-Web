CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

1

Partial Membership Latent Dirichlet Allocation

Chao Chen, Member, IEEE, Alina Zare, Senior Member, IEEE, Huy N. Trinh, Gbenga O. Omotara, J. Tory
Cobb, Senior Member, IEEE, and Timotius A. Lagaunne

6
1
0
2
 
c
e
D
 
8
2
 
 
]

V
C
.
s
c
[
 
 
1
v
6
3
9
8
0
.
2
1
6
1
:
v
i
X
r
a

Abstract—Topic models (e.g., pLSA, LDA, sLDA) have been
widely used for segmenting imagery. However, these models are
conﬁned to crisp segmentation, forcing a visual word (i.e., an image
patch) to belong to one and only one topic. Yet, there are many
images in which some regions cannot be assigned a crisp categorical
label (e.g., transition regions between a foggy sky and the ground or
between sand and water at a beach). In these cases, a visual word
is best represented with partial memberships across multiple topics.
To address this, we present a partial membership latent Dirichlet
allocation (PM-LDA) model and an associated parameter estimation
algorithm. This model can be useful for imagery where a visual
word may be a mixture of multiple topics. Experimental results on
visual and sonar imagery show that PM-LDA can produce both
crisp and soft semantic image segmentations; a capability previous
topic modeling methods do not have.

Index Terms—latent dirichlet allocation, partial membership,

image segmentation, soft image segmentation, topic model.

I. INTRODUCTION

T HE goal of unsupervised semantic image segmentation

is to divide an image into semantically distinct coherent
regions, i.e., regions corresponding to objects or parts of objects.
Inspired by the success of Latent Dirichlet Allocation (LDA) [1]
in discovering semantically meaningful topics from document
collections, many have successfully applied LDA or its variants
to image segmentation [2]–[6]. It has been used in a wide range
of computer vision applications, such as object recognition and
tracking and image retrieval [5], [7]–[9]. Yet, in many images, the
widely used crisp image segmentation methods fail to perform.
Speciﬁcally, these methods poorly address imagery in which there
are smooth gradients and transition regions. For example, consider
the photograph in Fig. 1a where the gradually thinning fog blurs
the boundary between the foggy sky and the mountain, a sharp
boundary between the “fog” and “mountain” topics does not exist.
Similarly, notice the lack of a sharp boundary given the gradually
fading sunlight in Fig. 1b or the gradually vanishing sand ripples

C. Chen, H. Trinh, G. O. Omotara, and T. Lagaunne are with the Department
of Electrical and Computer Engineering, University of Missouri, Columbia, MO,
65203. E-mail: ccwwf@mail.missouri.edu

A. Zare is with the Department of Electrical and Computer Engineering,

University of Florida, Gainesville, FL, 32611. E-mail: azare@ece.uﬂ.edu
J. Tory Cobb is with Naval Surface Warfare Center, Panama City, FL.
The authors graciously thank the Ofﬁce of Naval Research, Code 321, for
funding this research. Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily reﬂect
the views of the Ofﬁce of Naval Research.

Manuscript received December 28, 2016.

shown in the Synthetic Aperture SONAR (SAS) image of the sea
ﬂoor in Fig. 1c. In this paper, we present Partial Membership
Latent Dirichlet Allocation (PM-LDA) to address unsupervised
semantic image segmentation in the case of imagery with regions
of transition.

(a)

(b)

(c)

Fig. 1: Imagery with regions of gradual transition. (a) Image
with gradual transition from fog to mountain. (b) Sunset image
with gradual transition from sun to sky. (c) SONAR image with
gradually vanishing sand ripples.

Unsupervised semantic image segmentation methods differ
from traditional (i.e., non-hierarchical/ﬂat) segmentation methods
(e.g., normalized cuts algorithm [7]) by estimating and describing
additional inter-segment relationships. Namely, these methods
over-segment
imagery and then group the resulting “visual
words” into topic clusters such that small segments from the
same object class can be combined into a complete object and
provide a comprehensive organization of the larger scene. In
other words, unsupervised semantic image segmentation methods
cluster imagery hierarchically in which the lower level corresponds
to an over-segmentation of the imagery and the higher level
groups the over-segmented pieces into topic clusters. In addition
to the direct application of LDA to visual words, methods that
consider spatial structure have also been developed. These include
the Spatial Latent Topic model (Spatial-LTM) and Spatial LDA
(sLDA) model [3], [4]. Zhao, et al.[5] developed a Topic Random
Field (TRF) model to tackle the problems caused by discarding
spatial information and information loss by feature quantization.
Andreetto, et al.[6] proposed an Afﬁnity-based Latent Dirichlet
Allocation (A-LDA). Yet, under these existing topic models, a
visual word is only assigned to one topic (i.e., the word-topic
assignment is a binary indicator). In this paper, we generalize
LDA to allow for partial memberships.

II. PARTIAL MEMBERSHIP MODELS

Partial membership models and algorithms have been previously
developed in the literature. One prevalent partial membership

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

2

(a) BPM

(b) LDA

(c) PM-LDA

Fig. 2: (a) Graphical model for BPM. (b) Graphical model for LDA. (c) Graphical model for PM-LDA

approach is the Fuzzy C-means algorithm (FCM) [10] and, in
particular, FCM extensions for image segmentation [11]–[13].
FCM is a centroid-based clustering method which iteratively
minimizes the following objective function,

J =

N
(cid:88)

K
(cid:88)

n=1

k=1

nkd2(xn, µk)
zm

(1)

where znk represents the (partial) membership of the nth data
point xn to cluster k. The membership values are constrained
such that (cid:80)K
k=1 znk = 1 and znk ≥ 0. The m > 1 is the fuzziﬁer
parameter which controls the degree to which memberships are
mixed or non-binary, µk is the prototype representing cluster k,
and d2(xn, µk) is generally taken to be the squared Euclidean
distance between xn and µk.

Although the FCM has been extensively used, until recently,
probabilistic interpretations of FCM had not been developed. These
interpretations were contributed by Heller et al.by introducing the
Bayesian Partial Membership model (BPM) [14] and Glenn et
al.through the introduction of Bayesian Fuzzy Clustering (BFC)
[15], [16]. Both of these Bayesian models for partial membership
methods are non-hierarchical. In contrast, the proposed PM-LDA
extends these models to a hierarchical approach that allows for
semantic image segmentation like LDA. BFC and BPM are
reviewed in the following sub-sections.

A. Bayesian Partial Membership Model

In a ﬁnite mixture model, the data likelihood of xn, is

p(xn|β) =

πkpk(xn|βk),

(2)

K
(cid:88)

k=1

where {πk}K
k=1 are the mixture weights and β = {β1, β2, ..., βK}
are the mixture component parameters. pk(xn|βk) is the kth
mixture component with parameters βk. In this model, a data
point is assumed to come from one (and only one) of the K
mixture components. Thus, given its component assignment, zn,
the probability of a data point, xn, is deﬁned as p(xn|zn, β) =
(cid:81)K
k=1 znk = 1, and
zn = [zn1, zn2, ..., znK] is the binary membership vector. If znk =

k=1 pk(xn|βk)znk where znk ∈ {0, 1}, (cid:80)K

1, the data point xn is assumed to have been drawn from mixture
component k.

In order to obtain a model allowing multiple cluster member-
ships for a data point, the constraint znk ∈ {0, 1} is relaxed to
znk ∈ [0, 1]. The modiﬁed constraints and the inclusion of prior
distributions for several key parameters results in the Bayesian
Partial Membership model,

p(π, s, zn, xn|α, λ, β) = p(π|α)p(s|λ)p(zn|πs)

K
(cid:89)

k=1

pk(xn|βk)znk ,

(3)

where znk ∈ [0, 1], (cid:80)K
k=1 znk = 1, and π is the cluster mixing
proportion assumed to be distributed according to a Dirichlet
distribution with parameter α, i.e., π ∼ Dir(α). The scaling
factor, s, determines the level of cluster mixing and is distributed
according to an exponential distribution with mean 1/λ, s ∼
exp(λ), and zn ∼ Dir(πs) is the membership vector for data
point xn.

As shown in [14], if each of the mixture components are expo-
nential family distributions of the same type, then p(xn|zn, β) =
(cid:81)K
k=1 znk = 1, can be

k=1 pk(xn|βk)znk with znk ∈ [0, 1] and (cid:80)K

written as:

p(xn|zn, β) = Expon

znkηk

.

(4)

(cid:33)

(cid:32)

(cid:88)

k

This indicates that the data generating distribution for xn is of the
same exponential family distribution as the original K clusters,
but with new natural parameters (cid:80)
k znkηk. The new parameters
are a convex combination of the natural parameters, ηk, of the
original clusters weighted by znk. This provides the powerful (and
convenient) ability to sample directly from the unique mixture
distribution for each data point if the natural parameters of the
original clusters and the membership vector for the data point
are known. A graphical model of BPM is shown in Fig. 2a. The
generative process of BPM is described as follows:

1. Draw mixing proportion π from a Dirichlet distribution,

2. Draw scaling constant s from an exponential distribution

π ∼ Dir(α).

s ∼ exp(λ) = λe−λs.

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

3

3. For each data point xn

Given the cluster prototype µk and membership znk, the

(a) Draw the membership vector zn from a Dirichlet

distribution, zn ∼ Dir(sπ).
(b) Draw a data point xn ∼ Expon((cid:80)

k znkηk).

distribution of data point xn is described as
(cid:34)(cid:32) K
(cid:88)

(cid:40)

p(xn|zn, µk) ∝ exp

−

B. Bayesian Fuzzy Clustering Model

Bayesian Fuzzy Clustering (BFC) also models membership
vectors as Dirichlet distributed random variables. Input data points
are then modeled as Gaussian random variables. Since Gaussian
distributions are of the exponential family and using the result in
(4), the natural parameters for the Gaussian representing each data
point are convex combinations of the natural parameters of the
individual clusters weighted by the associated partial membership
values. In order to incorporate the fuzziﬁer parameter used in
FCM as shown in (1), each membership value is raised to the
mth power. This results in the data likelihood distribution (named
in [15] as the Fuzzy Data Likelihood (FDL)),

p(X|Z, Y) =

N
(cid:89)

n=1

1
Z(zn, m, Y)

K
(cid:89)

k=1

N (xn|µk, Q = zm

nkI)

with

and a prior distribution for the cluster membership (called Fuzzy
Cluster Prior (FCP)),

˜p(Z|Y ) =

Z(zn, m, Y)

z−mp/2
nk

Dir(zn|α)

(6)

N
(cid:89)

n=1

(cid:33)

(cid:32) K
(cid:89)

k=1

and a Gaussian prior distribution on cluster prototypes

p(Y) =

N (µk|µy, Σy),

(7)

K
(cid:89)

k=1

where Z(zn, m, Y) is a normalization constant, µk is the mean
of the kth Gaussian cluster, p is the dimensionality of the data, µy
and Σy are the sample mean and sample covariance, respectively.
The joint likelihood of data and parameters

p(X, Z, Y) = p(X|Z, Y)˜p(Z|Y)p(Y)
K
(cid:88)

N
(cid:88)

(cid:40)

∝ exp

−

nk (xn − µk)T (xn − µk)
zm

(cid:41)

1
2

N
(cid:89)

K
(cid:89)

n=1

k=1

n=1

k=1
(cid:40)

zαk−1
nk

exp

−

1
2

K
(cid:88)

k=1

(cid:0)µk − µy

(cid:1)T

Σ−1
y

(cid:0)µk − µy

(cid:1)

.

(cid:33)

zm
nk

xT

n xn

(cid:33)(cid:35)(cid:41)

k=1

1
2
(cid:32) K
(cid:88)

k=1

−2xT
n

zm
nkµk

+

nkµT
zm

k µk

.

(9)

(cid:33)

(cid:32) K
(cid:88)

k=1

Let Q = (cid:80)K

k=1 zm

nkI, and µ =

Equation (9) can be rewritten as

(cid:80)K

1
k=1 zm
nk

(cid:16)(cid:80)K

k=1 zm

nkµk

(cid:17)

,

p(xn|zn, µk) ∝ exp

−

(cid:2)xT

n Qxn − 2xT

(cid:26)

(cid:40)

1
2

1
2

(cid:34)(cid:32) K
(cid:88)

k=1

(cid:27)
n Qµ + µT Qµ(cid:3)
(cid:35)(cid:41)

(cid:33)

× exp

−

nkµT
zm

k µk

− µT Qµ

,

then

p(xn|zn, µk) = N (xn|µ, Q).

For data point xn, the generative process implicitly deﬁned in the
BFC is as follows:

(a) Draw the membership vector zn from the FCP.
(b) Draw a data point xn from a Gaussian distribution, xn ∼

(10)

(11)

Q =

zm
nkI

and

µ =

K
(cid:88)

k=1

1
k=1 zm
nk

(cid:80)K

(cid:32) K
(cid:88)

k=1

(cid:33)

zm
nkµk

.

The FCP is an improper prior which is uninformative about
the membership prior belief. Thus, is it unclear how to sample
memberships from the FCP in [15]. However, it is also noted
in [15] that the FCP can be converted to a proper prior by
(cid:16)(cid:81)K
replacing the second term
with a product of
Inverse-Gamma distribution with shape parameter as mp/2 − 1
with a small scale parameter.

k=1 z−mp/2

nk

(cid:17)

C. Uniﬁed Partial Membership Model

Considering both the BPM and BFC, a uniﬁed model which
combines the two models is developed in this paper. A comparison
of the terms in the BFC and BPM models is shown in Table I. The
major difference between the BPM and BFC models is how the
two approaches model the degree of mixing between clusters. The
BFC uses both a ﬁxed fuzziﬁer parameter and the membership
prior FCP to control the degree of mixing between clusters. In
contrast, in the BPM, the degree of mixing between clusters is
controlled only through the scaling hyper-parameter, s, found in
the prior distribution on partial membership values. The BPM
explicitly deﬁnes the cluster mixing proportion π while the BFC
does not. In terms of the data generating distribution, coefﬁcients

(cid:41)

(8)

Y = {µ1, · · · , µK} ,

(5)

N (˙|µ, Q), with

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

4

of the convex combination for BFC are raised to the m power
while the BPM does not.

can be computed as

TABLE I: Table comparison between BFC, BPM and the unifying
model.

BPM

m = 1

Unifying
model
m

s ∼ exp(λ)

s ∼ exp(λ)

π ∼ Dir(α)

π ∼ Dir(α)

fuzziﬁer
Scaling
Factor
Mixing
Prop.

BFC

m

N/A

N/A

of

Coeff.
Data
Generating
Distribution

Membership z ∼ FCP

z ∼ Dir(πs)

z ∼ Modi-
ﬁed FCP

zm
nk
k zm
nk

(cid:80)

zm
nk
k zm
nk

(cid:80)

, m = 1

zm
nk
k zm
nk

(cid:80)

In this paper, a uniﬁed model combining BFC and BPM is
proposed to combine the two approaches and investigate the effect
of fuzziﬁer m and scaling factor s on the membership mixing
level. The uniﬁed model is composed of an exponential prior on
scaling factor deﬁned as

p(s|λ) = λe−λs,

(12)

a Dirichlet prior on mixing proportion deﬁned as

p(π|α) =

(πk)(αk−1),

(13)

(cid:16)(cid:80)K

Γ

k=1 αk

(cid:17)

(cid:81)K

k=1 Γ(αk)

K
(cid:89)

k=1

a data likelihood in the same form of the FDL, deﬁned as

p(X|Z, Y) =

N
(cid:89)

n=1

1
Z(zn, m, Y)

K
(cid:89)

k=1

N (xn|µk, Q = zm

nkI),

(14)

and a prior distribution for the cluster membership deﬁned as

˜p(Z|Y) =

Z(zn, m, Y)

Dir(zn|sπ),

(15)

N
(cid:89)

n=1

(cid:33)

z−mp/2
nk

(cid:32) K
(cid:89)

k=1

which modiﬁes the FCP in the BFC by replacing the Dirichlet
parameter with sπ. The joint likelihood of the unifying model

p(X, Z, π, s|Y) = p(X|Z, Y)˜p(Z|Y)p(π)p(s)
N
(cid:26)
(cid:89)

K
(cid:89)

(2π)−p/2 exp

−

nk(xn − µk)T (xn − µk)
zm

(cid:27)

=

1
2

(znk)(sπk−1)

(cid:16)(cid:80)K

Γ

k=1 αk

(cid:17)

(cid:81)K

k=1 Γ(αk)

n=1

k=1
(cid:16)(cid:80)K

Γ

(cid:17)

k=1 sπk

(cid:81)K

k=1 Γ(sπk)

K
(cid:89)

k=1

K
(cid:89)

k=1

(πk)(αk−1)λe−λs.

(16)

(cid:27)

(17)

Thus, the log of this likelihood is

ln p(X, Z, Y, s, π) = const +
N
(cid:88)

K
(cid:88)

(cid:26)

−

nk(xn − µk)T (xn − µk)
zm

+

(cid:27)

N
(cid:88)

(cid:26)

n=1

1
2

(cid:33)

(cid:33)

K
(cid:88)

k=1
K
(cid:88)

k=1

n=1

ln Γ

k=1
(cid:32) K
(cid:88)

k=1
(cid:32) K
(cid:88)

k=1
ln λ − λs.

K
(cid:88)

k=1
K
(cid:88)

k=1

ln Γ

αk

−

ln Γ (αk) +

(αk − 1) ln πk +

sπk

−

ln Γ(sπk) +

(sπk − 1) ln znk

+

Figure 3 illustrates the effect of m and s on the membership
mixing level in the uniﬁed model. To generate this ﬁgure, two
Gaussian clusters were considered, N (0, 1) and N (1, 1). A
sequences of input data points, X, were generated in range
[−0.5, 1.5] with increment 0.05, and a set of membership values
for the ﬁrst cluster, Z, were generated in range [0, 1] with
increment 0.05 (resulting in the membership values in the second
cluster being 1-Z). The fuzziﬁer m was varied to be −1, −0.5,
0, 0.5, 1, 2, 5, and 10. The scaling factor s was varied to be
0.1, 0.5, 1, 2, and 5. The parameters λ, π, and α were set to
be 1, [0.5, 0.5], and [1, 1], respectively. Given this generated data
set and parameters, the log of joint likelihood in the unifying
model was computed and shown in Figure 3. Here, the X−axis
denotes the membership in the ﬁrst cluster ranging from 0 to 1
and the Y −axis denotes the input data point value, x, ranging
from −0.5 to 1.5. As shown in each column where the fuzziﬁer
m is ﬁxed, as s increases, mixing memberships correspond to
higher log likelihood values. Memberships with high log likelihood
values (bright yellow region) are gradually moved from the left
and right margins (corresponding to crisp memberships) to the
horizontal center (corresponding to highly mixed memberships).
The fuzziﬁer m determines the vertical position of the parameters
with high log likelihood values. As shown in each row, when m
increases, data points with high log likelihood values are gradually
expanded to the top and bottom margins. This ﬁgure illustrates

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

5

that the scaling factor s and the fuzziﬁer m impact resulting
partial membership values in different ways. One observation
is that the scaling factor s impacts the cluster mixing level by
controlling the prior knowledge on memberships and the fuzziﬁer
m by controlling the percentage of mixing data points.

The generative processes for both the BFC and BPM models
are similar. The main difference between the BPM and BFC
models is that the BFC uses both a ﬁxed fuzziﬁer parameter and a
scaling parameter to control the degree of mixing between topics.
In contrast, in the BPM, the degree of mixing between topics is
controlled only through a scaling hyper-parameter, s, found in
the prior distribution on partial membership values. The uniﬁed
model incorporates both of these parameters to allow a larger
degree of freedom in controlling the mixing between topics.

III. PARTIAL MEMBERSHIP LDA

To generalize LDA by allowing for partial memberships, a PM-
LDA model is proposed in this paper, where partial memberships
are introduced to the standard LDA in a similar fashion to the
developed uniﬁed model. Both the fuzziﬁer m and the scaling
factor s are incorporated in the proposed PM-LDA model. In
PM-LDA, the random variable associated with a data point is
assumed to be distributed according to multiple topics with a
continuous partial membership in each topic. First, we consider
the simplest case where the fuzziﬁer m = 1. Given, m = 1, the
PM-LDA model is

p(πd, sd, zdn, xdn|α, λ, β) = p(πd|α)p(sd|λ)p(zdn|πdsd)

K
(cid:89)

k=1

pk(xdn|βk)zdnk

(18)

where xdn is the nth word in document d, zdn is the partial
membership vector of xdn, πd ∼ Dir(α) and sd ∼ exp(λ)
are the topic proportion and the level of topic mixing in
document d, respectively. The parameter α corresponds to the
topic composition across a document. For example, in Fig. 1c, the
image may be composed of 40% “sand ripple” topic and 60% “ﬂat
sand” topic (i.e., α = [0.4, 0.6]). The parameter λ controls how
similar the partial membership vector of each word is expected to
be to the topic distribution of the document. For example, a small
λ would correspond to most words in a document to have partial
membership vectors very close to πd. During image segmentation,
a small λ generally corresponds to large transition regions (e.g.,
transition from “ﬂat sand” to “sand ripple” comprises most of the
image). For a large λ, the partial membership vectors for each
word can vary signiﬁcantly from the document mixing proportions.
In general, a large λ corresponds to very narrow (tending towards
crisp) transition regions during image segmentation (e.g., the SAS
image may have 39% of the visual words as pure “sand ripple”,
59% as pure “ﬂat sand”, and only 2% mixed).

The vector zdn ∼ Dir(πdsd) represents the partial memberships
of data point xdn in each of the K topics. If each topic distribution

(a)

(b)

Fig. 4: Partial membership data generating distributions. In (a), two
Gaussian topics with µ1 = [−4, −4]; µ2 = [6, 6], Σ1 = Σ2 = I.
In (b), two Gaussian topics with µ1 = [−4, −4]; µ2 = [6, 6],
Σ1 = [4, 0; 0, 1], Σ2 = [1, 0; 0, 4] [14].

is assumed to be of the exponential family, pk(·|βk) = Expon(ηk),
then using the result in (4), p(xdn|zdn, β) = Expon((cid:80)
k zdnkηk).
The graphical model for PM-LDA is shown in Fig. 2c and the
generative procedure of PM-LDA is described as follows.

1. For each document Xd, draw topic proportions πd ∼

2. Draw scaling factor sd from an exponential distribution

Dir(α).

sd ∼ exp(λ) = λe−λs.

3. For each word xdn

(a) Draw the membership vector zdn ∼ Dir(πdsd).
(b) Draw word xdn ∼ Expon((cid:80)

k zdnkηk).

For any value of the fuzzifer m, the above step 3(b) is modiﬁed
ηk). zdnk in the last

to be: Draw word xdn ∼ Expon((cid:80)
term of Equation (18) is correspondingly modiﬁed as

zm
dnk
j zm
dnj

(cid:80)

k

.

(cid:80)

zm
dnk
j zm
dnj

In PM-LDA, the membership zdn is drawn from a Dirichlet
distribution which is in contrast to a multinomial distribution as
used in LDA. With the inﬁnite number of possible values for zdn,
the word generating distributions in PM-LDA are expanded from
only K generating distributions (as in LDA) to inﬁnitely many.
Fig. 4 illustrates this using two Gaussian topic distributions, where
the membership value to one topic is varied from 0 to 1 with an
increment 0.1. The two original topics are shown as the Gaussian
distributions at either end. In LDA, words are generated from only
the two original topic distributions. In PM-LDA, words can be
generated from any (of the inﬁnitely many) convex combinations
of the topic distributions. As the scaling factor s → 0, the PM-
LDA model will degrade to the LDA model.

Given the hyperparameters Ψ = {α, λ, β}, the full PM-LDA

model over all words in the dth document is:

p(πd, sd, Zd, Xd|α, λ, β)

(19)

= p(πd|α)p(sd|λ)

p(xdn|zdn, β)p(zdn|πdsd).

Nd(cid:89)

n=1

where πd are the topic proportions, scaling factor sd, partial
membership vectors Zd = {zdn}Nd
n=1 and a set of Nd words Xd
for document d. The log of (19) when considering the speciﬁc
forms chosen in our model, is shown in (20).

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

6

Fig. 3: Log likelihood values of the uniﬁed model with different x, m and s parameter settings. To generate this ﬁgure, the uniﬁed
model was assumed to have two Gaussian clusters, N (0, 1) and N (1, 1). π = [0.5, 0.5], α = [1, 1]. The Y −axis denotes the value
of an input data point, x, which varies from −0.5 to 1.5 with increment 0.05. The X−axis denotes the membership in the ﬁrst
cluster varied from 0 to 1. For each row, the scaling factor s is ﬁxed and the fuzziﬁer m is varied to be different values. For each
column, the fuzziﬁer m is ﬁxed and the scaling factor s is varied to be different values. The color indicates the log likelihood value
for the ﬁxed data set with a given membership vector, m and s value. Yellow corresponds to a high log likelihood value and blue
corrsponds to a low log likelihood value.

Ld = ln(p(πd, sd, Zd, Xd|α, λ, β)) = ln Γ

αk

−

ln Γ (αk) +

(αk − 1) ln πdk + ln λ − λsd

+

ln p(xdn|zdn, β) +

ln Γ

sdπdk

−

ln Γ(sdπdk) +

(sdπdk − 1) ln zdnk

(20)

(cid:27)

Nd(cid:88)

n=1

(cid:32) K
(cid:88)

k=1

(cid:33)

(cid:33)

K
(cid:88)

k=1
K
(cid:88)

k=1

Nd(cid:88)

(cid:26)

n=1

(cid:32) K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

(cid:80)D

During parameter estimation, our goal is to maximize L =
d=1 Ld by estimating all the model parameters πd, sd, zdn, β.
In this paper, we employ a Metropolis within Gibbs [15], [17]
sampling approach.

where D = {X1, X2, ..., XD} includes all training documents
and Π, S, M include all of the topic proportions, scaling factors
and membership vectors, respectively.

IV. PARAMETER ESTIMATION FOR PM-LDA

The goal of parameter estimation is to maximize the following

posterior distribution,

p(Π, S, M, β|D, α, λ) ∝ p(Π, S, M, D|α, λ, β),

(21)

A Metropolis within Gibbs sampler is employed to perform the
MAP inference which can generate samples from the posterior
distribution in (21), [15], [17]. An outline of the sampler is
provided in Alg. 1. The sampler is a simple and straight-forward
implementation composed of a series of draws from candidate
distributions for each parameter and then evaluation of the
candidate in the appropriate acceptance ratio. Our implementation

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

7

of the sampler has been posted online.1 In our current imple-
mentation, we consider the topic distributions to be Gaussian
with different means, µk, but identical diagonal and isotropic
covariance matrices, Σk = σ2I.

imagery.

Algorithm 1 Metropolis-within-Gibbs Sampling Method for
Parameter Estimation
Input: A corpus D, the number of topics K, and the number of

Output: Collection of all samples: Π(t), S(t), M(t), β(t) =
K , Σ(t)

2 ..., µ(t)

2 , Σ(t)

(cid:111)
.

(cid:110)

K

sampling iterations T

1 , µ(t)
1 , Σ(t)
µ(t)
1: for t = 1 : T do
2:
3:

for d = 1 : D do

4:

5:
6:

7:
8:
9:

10:

11:
12:

(cid:110)
1, p(π†,s(t−1),Z(t−1),X|Ψ)p(π(t−1)|α)
p(π(t−1),s(t−1),Z(t−1),X|Ψ)p(π†|α)

Sample πd: Draw candidate: π† ∼ Dir(α)
Accept candidate with probability:
aπ = min
Sample sd: Draw candidate: s† ∼ exp(λ)
Accept candidate with probability:
as = min
for n = 1 : Nd do

(cid:110)
1, p(π(t),s†,Z(t−1),X|Ψ)p(s(t−1)|λ)
p(π(t),s(t−1),Z(t−1),X|Ψ)p(s†|λ)

(cid:111)

(cid:111)

Sample zdn: Draw candidate: z†
Accept candidate with probability:
n,xn|Ψ)
az = min

p(π(t),s(t),z†
p(π(t),s(t),z(t−1)

1,

(cid:110)

,xn|Ψ)

n

(cid:111)

n ∼ Dir(1K)

end for

end for
for k = 1 : K do

Sample µk: Draw proposal: µ†
k ∼ N (·|µD, f ΣD)
µD and ΣD are mean and covariance of the data
Accept candidate with probability:
(cid:17)

(cid:16)

p

p

(cid:16)

Π(t),S(t),M(t),D|µ†
k
(t−1)
Π(t),S(t),M(t),D|µ
k

N (µ
(cid:17)

(t−1)
k
N (µ†

|µD,ΣD)

(cid:27)

k|µD,ΣD)

ak = min

1,

(cid:26)

end for
Sample covariance matrices Σ = σ2I:
Draw candidate from:
σ2 = 1
2
Accept candidate with probability:

(cid:26)

aΣ = min

1,

p(Π(t),S(t),M(t),D|Σ†)
p(Π(t),S(t),M(t),D|Σ(t−1))

(cid:27)

.

(cid:8)maxxn d2(xn − µD) − minxn d2(xn − µD)(cid:9)

13: end for

The proposed Metropolis within Gibbs scheme will return the
full distribution of parameter values given the desired posterior.
We use the MAP sample (i.e., the sample with the largest log
posterior value) as the ﬁnal estimate, {Π∗, S∗, M∗, µ∗, Σ∗}.

V. DATA & EXPERIMENTAL RESULTS
In this section, we show results of image segmentation on two
datasets: (i) Synthetic Aperture SONAR (SAS) imagery and (ii)
Sunset imagery,. In the following experiments, unless otherwise
speciﬁed, the fuzziﬁer m value is set to 1.

1Code can be found at: https://github.com/TigerSense/PMLDA

A. Synthetic Aperture Sonar (SAS) Imagery Dataset

The ﬁrst set of experiments considers segmentation of SAS

1) SAS sub-image segmentation and comparison to LDA and
FCM: Our ﬁrst experiment considers segmentation of a subset of
four sub-images from our SAS image database (shown in the ﬁrst
column in Fig. 5). For each image, we simply compute the average
intensity value and entropy within a 21 × 21 window as feature
values. The average intensity value is scaled (×10) to roughly
the same magnitude of the average entropy value. Each image is
divided into multiple documents using a sliding window approach.
A document consists of all of the feature vectors associated with
each pixel (i.e., visual words) in the window. The number of
topics in this dataset is set to 3. For LDA, a dictionary of size
100 is built by clustering all the computed feature values using
the K-means. FCM results with m = 1.5. Parameters for LDA
and FCM were selected manually to provide the best results. Due
to the lack of ground-truth, qualitative segmentation results in
Fig. 5 is provided. In the ﬁrst row, Subﬁgures (b), (c), and (d)
show the partial membership maps in the “dark ﬂat sand” , “sand
ripple” , and “bright ﬂat sand” topics using PM-LDA, respectively.
Subﬁgures (f), (g), and (h) show the partial membership maps in
each of the three clusters using FCM, respectively. In (b) - (d)
and (f) - (h), the color indicates the degree of membership of a
visual word in a topic where red corresponds to a full membership
of 1 and dark blue color corresponds to a membership value of
0. The LDA result is shown in (e) where color indicates topic
assignment. Subﬁgures in Row 2-4 follow the same subﬁgure
captions in Row 1.

From the experimental results, we can see that PM-LDA
achieves much better results than FCM and LDA. As shown
in Fig. 5c and 5k, the segmentation results of PM-LDA show
a gradual change from “sand ripple” to “dark ﬂat sand”. FCM
captures the gradual transition to some extent but is not able to
clearly differentiate between clusters. For example, as shown in
Fig. 5o - 5p and Fig. 5w - 5x, using FCM, the rippled region in
Images 2 and 3 is assigned to 2 clusters with nearly equal partial
memberships. As LDA cannot generate partial memberships, in
Fig. 5e and 5m, Image 1 and 2 are simply partitioned into different
topics using LDA. Yet, by comparing Fig. 5u with 5t and Fig. 5ac
with 5ab, we can see that on Image 3 and 4 that do not contain
transition regions, LDA achieves similar segmentation result to
PM-LDA.

2) Complete SAS image segmentation and comparison to LDA
and sLDA: Experiments are then extended to the complete
SONAR images. Five complete high-frequency SAS (indicated as
HF-00, HF-01,..., HF-04, are segmented into 200 superpixels [18]
and each superpixel is considered as a document. In addition to
the mean and entropy features used in the previous experiment,
a “sand ripple”ﬁlter response is also used as the third feature
in this experiment. The ﬁlter is built based on the sand ripple
characterization algorithm proposed in [19]. It is hypothesized

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

8

(a) Image 1

(b) PM-LDA:1 (c) PM-LDA:2

(d) PM-LDA:3

(e) LDA

(f) FCM:1

(g) FCM:2

(h) FCM:3

(i) Image 2

(j) PM-LDA:1

(k) PM-LDA:2

(l) PM-LDA:3

(m) LDA

(n) FCM:1

(o) FCM:2

(p) FCM:3

(q) Image 3

(r) PM-LDA:1

(s) PM-LDA:2

(t) PM-LDA:3

(u) LDA

(v) FCM:1

(w) FCM:2

(x) FCM:3

(y) Image 4

(z) PM-LDA:1 (aa) PM-LDA:2 (ab) PM-LDA:3

(ac) LDA

(ad) FCM:1

(ae) FCM:2

(af) FCM:3

Fig. 5: Segmentation results of Image 1 - 4 using PM-LDA, FCM and LDA. (a): SAS Image. (b)-(d): PM-LDA partial membership
map in the “dark ﬂat sand,” “sand ripple,” “bright ﬂat sand” topics, respectively. (e): LDA result where color indicates topic label.
(f)-(h): FCM partial membership map in the ﬁrst, second, and third cluster, respectively. Subﬁgure captions in Row 2 - 4 follow those
in Row 1. In PM-LDA and FCM results, color indicates the degree of membership of a visual word in a topic or cluster.

is a rippled region with certain ripple
that each superpixel
frequency, fripple (number of ripples per meter). The sand ripple
characterization algorithm is applied to each superpixel to estimate
its ripple frequency. As the range resolution of the sonar imagery
is 0.025m, a complete ripple length L can be computed as
40/fripple. To capture the ripple repeating pattern, a ﬁlter is built
as [−1, 0, 1, −1, 0, 1], where 1 and 0 are matrices with height of
11 and width of (cid:100)L/3(cid:101). The ﬁlter is applied to the corresponding
superpixel and the ﬁlter response is used as the third feature for
that superpixel. Non-rippled superpixels have low ﬁlter responses
and ripple superpixels have high ﬁlter response.

In this experiment, we compare PM-LDA with LDA and sLDA
by running the parameter estimation on ﬁve SONAR images
simultaneously. For PM-LDA, the hyper-parameter λ and α is set
to be 0.5 and 1K. For sLDA, σ is set to be 0.1 and each document
is repeated for 4 times. The topic number is set to K = 3. The
segmentation results of PM-LDA, LDA, and sLDA are shown in
Fig. 6. Column (a) are the ﬁve complete SONAR images HF-00
to HF-04 with super-pixel boundaries. Column (b)-(d) are the
PM-LDA results, which represent the partial membership maps in
“sand ripple”, “sea grass” and “dark ﬂat sand” topic, respectively.
Column (e) and (f) are the LDA and sLDA results, respectively.
The color indicates the topic number. As shown in Fig. 6, PM-
LDA achieves similar segmentation results to LDA and sLDA
on SONAR imagery. All of them are able to learn the “sand
ripple”, “sea grass”, and “dark ﬂat sand” topics, and for each

topic, the corresponding regions localized by PM-LDA, LDA,
and sLDA are similar to each other. For example, “sand ripple”
region learned by PM-LDA (the red regions in Column (b) are
almost the same as “sand ripple” region learned by LDA and
sLDA (the dark blue regions in Column (e) and (f)). However,
PM-LDA has an exclusive capability of localizing the gradual
transition regions between different topics while both LDA and
sLDA only provide crispy boundaries. These partial membership
values mostly occur at the boundary between two topics. Thus,
PM-LDA is able to identify when the feature vector contains
information from multiple topics (as the feature vector is being
computed over a window that contains more than one topic). This
is a powerful result showing the effectiveness of PM-LDA in
providing semantic image understanding.

3) Varying scaling factor s: As discussed in Section III,
the scaling factor sd determines the similarity of the partial
membership vector of each word, zdn, to the topic proportion πd.
In this experiment, we investigated the effect of s by estimating the
memberships and topics with ﬁxed topic proportion. A subregion
consisting of three superpixels [20] is used in this experiment and
shown in Fig. 7. Each superpixel is treated as a document. The
topic proportion πd is set to be [1, 1, 1]/3 and the scaling factor s
is varied to be 3, 10, 300, 30000. The membership estimation
results are shown in Figure 8. Column (a)-(c) represent the
membership maps in “sand ripple”, “sea-grass”, and “dark ﬂat
sand” topic, respectively. In this set of superpixels, there are

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

9

(a) SONAR imagery
with super-pixel
boundary

(b) PM-LDA:
sand ripple

(c) PM-LDA:
sea grass

(d) PM-LDA:
dark ﬂat sand

(e) LDA

(f) sLDA

Fig. 6: Segmentation results of PM-LDA, LDA, and sLDA. Column (a) represents the original images HF-00 to HF-04 with super-pixel
boundaries. Column (b)-(d) are the PM-LDA results, which represent the partial membership maps in “sand ripple”, “sea grass” and
“dark ﬂat sand” topic, respectively. Column (e) and (f) are the LDA and sLDA results, respectively. The color indicates the topic
number.

no regions of “sea-grass” which PM-LDA correctly determines.
As can be seen, within each superpixel, as the scaling factor s
increases, the partial memberships gradually approach the topic
proportion [1, 1, 1]/3 and the membership map becomes more
smooth. This can be considered as a way of incorporating spatial
information. Visual words in a superpixel are spatial neighbors.
They are assumed to have similar attributes and should have
similar membership vectors. By using superpixels as documents
and increasing the value of scaling factor s, spatial information
is implicitly incorporated to PM-LDA.

4) Varying the number of superpixels: In the following exper-
iment, we study the effect of number of superpixels. SONAR

Fig. 7: A subregion of three superpixels

Imagery HF-00 is selected for this experiment. The number of
superpixel is varied to be 20, 48, and 200. The scaling factor s
is varied to be 1, 100, 500, and 1000. In contrast to the previous

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

10

(a) s = 3

(a) s = 10

(a) s = 300

(a) s = 30000

(b)

(b)

(b)

(b)

(c)

(c)

(c)

(c)

Fig. 8: Partial membership maps with varying s. Each row shows
the estimated membership maps of the three estimated topics. The
black contour indicates the superpixel boundary. The superpixels
are results published in [20].

experiment where the topic proportion is ﬁxed, the topic proportion
is learned in this experiment. The experimental results are shown
in Fig. 9 - 11. The three ﬁgures show the partial membership
maps in “sea-grass”, “dark ﬂat sand”, and ”sand ripple” topic,
respectively, with varying number of superpixels and varying
scaling factor s,.

In Fig. 9 - 10, at each row, when the scaling factor s increases,
especially when s changes from 500 to 1000, a few superpixels
within the red region (as shown in Column s = 1) change their
color from red to yellow, thus the red region (when s = 1)
becomes less and less continuous. Based on the discussion in above
experiment, with superpixels as documents, a large scaling factor
s should enhance the spatial continuity. However, this requires
a strict prerequisite that the topic proportions are accurately
estimated and consistent across superpixels that belong to the
same topic. Otherwise, matching a large scaling factor with a
superpixel will damage the membership estimation by pushing the
membership vector towards an incorrect topic proportion vector.
As illustrated, increasing the number of superpixels and matching
a large scaling factor does not guarantee an improved membership
estimation.

5) Varying fuzziﬁer: In this experiment, we investigate the effect
of the fuzziﬁer m using SAS image HF-00 with 48 superpixels.
m is varied to be 1, 2, and 3, respectively. Experimental results
are shown in Figure 12. In each column, as m increases, some
more highly mixed partial membership values are present around
boundary areas. /

B. Sunset Dataset

Finally, experimental results on Sunset dataset show the ability
of PM-LDA to perform partial membership segmentation given
visual natural imagery. 27 sunset themed images from Flickr (with
the necessary permissions)2 3 were used. Features used in this
experiment are 3 × 3 Gaussian ﬁlter (σ = 1) responses on LAB
channels, 3 × 3 Gaussian ﬁlter response (σ = 2) on blue channel,
ﬁrst order derivative of Gaussian along y-axis on L channel and
the log transform of blue channel. Each image is segmented into
15 superpixels using normalized cuts. The number of topics is set
to be 3. Experiments are run on 27 images simultaneously and
some of the experimental results are shown in Fig. 13. Columns
2-4 show the segmentation results of PM-LDA. Column 5 is the
LDA results with 3 topics. Comparing Column 3 and Column
5 in Fig. 13, we can see that PM-LDA can generate continuous
partial membership according to the extent to which the sky is
colored by sunlight. The partial membership map illustrates how
the topic gradually shift from one to the other. In contrast, LDA
can only produce 0-1 segmentation.

VI. CONCLUSION

In this paper the PM-LDA model

is introduced for soft
image segmentation. PM-LDA improves upon the LDA model by
introducing a partial membership rather than requiring a single
topic label for each word. Experimental results on three image
datasets demonstrate the capacity of PM-LDA model in both
soft and crisp image segmentation. Future work will include
developing a more efﬁcient sampling approach, e.g., a collapsed
Gibbs sampler, to accelerate the parameter estimation procedure.

VII. ACKNOWLEDGEMENTS

The authors graciously thank the Ofﬁce of Naval Research,
Code 321, for funding this research. Any opinions, ﬁndings, and
conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reﬂect the views of
the Ofﬁce of Naval Research.

REFERENCES

[1] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” Journal

of Machine Learning Research, vol. 3, pp. 993–1022, 2003. 1

[2] B. C. Russell, W. T. Freeman, A. Efros, J. Sivic, and A. Zisserman,
“Using multiple segmentations to discover objects and their extent in
image collections,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2006, pp. 1605–1614. 1

[3] L. Cao and L. Fei-Fei, “Spatially coherent latent topic model for concurrent
segmentation and classiﬁcation of objects and scenes,” in IEEE International
Conference on Computer Vision, 2007, pp. 1–8. 1

[4] X. Wang and E. Grimson, “Spatial latent dirichlet allocation,” in Advances

in Neural Information Processing Systems, 2008, pp. 1577–1584. 1

[5] B. Zhao, L. Fei-Fei, and E. P. Xing, “Image segmentation with topic random
ﬁeld,” in European Conference on Computer Vision, 2010, pp. 785–798. 1

2Photo can be found at: https://www.ﬂickr.com/photos/aoa-/6104409480/
3Photo can be found at: https://www.ﬂickr.com/photos/frenchdave/8482336933/

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

11

20 superpixels

48 superpixels

20 superpixels

48 superpixels

200 superpixels

s=1

s=100

s=500

s=1000

Fig. 9: Partial membership maps for HF-00 with varying number of superpixels and varying scaling factor s for topic 1 - sea grass.
Column 2 to 5 correspond to scaling factor s = 1, 100, 500, 1000, respectively.

200 superpixels
s=1

s=100

s=500

s=1000

Fig. 10: Partial membership maps of HF-00 with varying number of superpixels and varying scaling factor s for topic 2 - dark ﬂat
sand. Column 1 to 5 correspond to scaling factor s = 1, 100, 500, 1000, respectively.

[6] M. Andreetto, L. Zelnik-Manor, and P. Perona, “Unsupervised learning of
categorical segments in image collections,” IEEE Transactions on Pattern

Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1842–1855, 2012. 1
[7] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

12

20 superpixels

48 superpixels

200 superpixels
s=1

s=100

s=500

s=1000

Fig. 11: Partial membership maps of HF-00 with varying number of superpixels and varying scaling factor s for topic 3 - sand ripple.
Column 1 to 5 correspond to scaling factor s = 1, 100, 500, 1000, respectively.

extended synthetic aperture sonar model and parallel sampling method,”
IEEE Transaction on Geoscience and Remote Sensing., vol. 53, no. 10, pp.
5547–5559, 2015. 7

[20] J. T. Cobb and A. Zare, “Multi-image texton selection for sonar image
seabed co-segmentation,” in SPIE, vol. 8709, no. 87090H, June 2013. 8, 10

Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8,
pp. 888–905, 2000. 1

[8] D. Comaniciu and P. Meer, “Mean shift: A robust approach toward feature
space analysis,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 24, no. 5, pp. 603–619, 2002. 1

[9] P. F. Felzenszwalb and D. P. Huttenlocher, “Efﬁcient graph-based image
segmentation,” International Journal of Computer Vision, vol. 59, no. 2, pp.
167–181, 2004. 1

[10] J. C. Bezdek, R. Ehrlich, and W. Full, “Fcm: The fuzzy c-means clustering
algorithm,” Computers & Geosciences, vol. 10, no. 2, pp. 191–203, 1984. 2
[11] S. Naz, H. Majeed, and H. Irshad, “Image segmentation using fuzzy
clustering: A survey,” in International Conference on Emerging Technologies
(ICET), Oct 2010, pp. 181–186. 2

[12] S. Krinidis and V. Chatzis, “A robust fuzzy local information c-means
clustering algorithm,” IEEE Transactions on Image Processing, vol. 19,
no. 5, pp. 1328–1337, May 2010. 2

[13] S. Krinidis and M. Krinidis, “Generalised fuzzy local information c-means
clustering algorithm,” Electronics Letters, vol. 48, no. 23, pp. 1468–1470,
November 2012. 2

[14] K. A. Heller, S. Williamson, and Z. Ghahramani, “Statistical models for
partial membership,” in International Conference on Machine Learning,
2008, pp. 392–399. 2, 5

[15] T. Glenn, A. Zare, and P. Gader, “Bayesian fuzzy clustering,” IEEE
Transaction on Fuzzy Systems, no. 8, pp. 1545–1561, 2015. 2, 3, 6
[16] T. C. Glenn, “Context-dependent detection in hyperspectral imagery,” 2013.

2

[17] C. Robert and G. Casella, Monte Carlo statistical methods. Springer Science

& Business Media, 2013. 6

[18] J. T. Cobb and A. Zare, “Boundary detection and superpixel formation in
synthetic aperture sonar imagery,” in International Conference on SAS and
SAR, Sept. 2014. 7

[19] C. Chen, A. Zare, and J. T. Cobb, “Sand ripple characterization using an

PLACE
PHOTO
HERE

Chao Chen received the B.S. and M.S. degree in control
theory both from Xidian University, Xi’an, China, in
2007 and 2010, respectively, and the Ph.D. degree from
University of Missouri - Columbia, in 2016. Her research
interests include sparse coding, Bayesian inference, and
synthetic aperture sonar imagery analysis.

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

13

(a)m = 1

(a)m = 2

(a)m = 3

(b)

(b)

(b)

(c)

(c)

(c)

Fig. 12: The partial membership maps of HF-00 with varying fuzziﬁer. Row 1-3 represent the partial membership maps of HF-00
with fuzziﬁer = 1, 2, and 3, respectively. The superpixel number is 48.

Gbenga Omotara is currently pursuing an undergrad-
uate degree in Electrical and Computer Engineering at
the University of Missouri, Columbia, MO, USA. His
interests include machine learning and computational
intelligence.

PLACE
PHOTO
HERE

PLACE
PHOTO
HERE

Alina Zare (S’07–M’08–SM’13) received the Ph.D.
degree from the University of Florida, Gainesville, in
2008. She is currently an Associate Professor with the
Department of Electrical and Computer Engineering,
University of Florida. Her research interests include
machine learning, computational intelligence, Bayesian
methods, sparsity promotion, image analysis, pattern
recognition, hyperspectral image analysis, and remote
sensing. Alina Zare is a recipient of the 2014 National
Science Foundation CAREER award and the 2014 Na-
tional Geospatial-Intelligence Agency New Investigator
Program Award. Alina Zare is an Associate Editor of the IEEE Transactions on
Geoscience and Remote Sensing.

Huy N. Trinh received the B.S. degree in Computer
Science at University of Missouri, Columbia, USA, in
2015. He is currently a Graduate Research Assistant
working toward the M.S degree in the Department of
Computer Science, University of Missouri, Columbia,
USA. His research interests include machine learning,
image segmentation, computational intelligent, cloud
computing and networks.

PLACE
PHOTO
HERE

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

14

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

Fig. 13: Examples of segmentation result on Sunset dataset. (a): Sunset Image 1. (f): Sunset Image 2. (k): Sunset Image 3. (b)-(d),
(g)-(i), and (l)-(n) are the PM-LDA partial membership maps in the estimated three topics for Sunset Image 1, Sunset Image 2, and
Sunset Image 3, respectively. The color indicates the degree of membership of a visual word in a topic or cluster. (e), (j), and (o) are
the LDA results where color indicates the topic.

PLACE
PHOTO
HERE

J. Tory Cobb (S’99–M’01–SM’13) received the B.S.
degree in electrical engineering from the United States
Coast Guard Academy, New London, CT, USA in 1994,
the M.S. degree in electrical engineering from Auburn
University, Auburn, AL, USA, in 2001, and the Ph.D.
degree from the University of Florida, Gainesville, FL,
USA, in 2011. From 1994 to 1999 he was an active
duty ofﬁcer in the Coast Guard. Since 2001 he has been
employed as a Research Engineer at the Naval Surface
Warfare Center, Panama City, FL, USA. He has served
as Principal Investigator or Co-Principal Investigator for
various automatic target recognition and sensor fusion projects funded by the
Ofﬁce of Naval Research. His research interests include statistical modeling
of sonar signals with applications to automatic target recognition, automated
environmental characterization of seabeds in side-look sonar images, and sonar
image segmentation algorithm development. Dr. Cobb is an Associate Editor of
the IEEE Journal of Oceanic Engineering.

Timotius A. Lagaunne is currently pursuing an under-
graduate degree in Mathematics at the University of
Missouri Columbia, MO, USA. His interests include ma-
chine learning, computational intelligence, data science
and natural language processing

PLACE
PHOTO
HERE

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

1

Partial Membership Latent Dirichlet Allocation

Chao Chen, Member, IEEE, Alina Zare, Senior Member, IEEE, Huy N. Trinh, Gbenga O. Omotara, J. Tory
Cobb, Senior Member, IEEE, and Timotius A. Lagaunne

6
1
0
2
 
c
e
D
 
8
2
 
 
]

V
C
.
s
c
[
 
 
1
v
6
3
9
8
0
.
2
1
6
1
:
v
i
X
r
a

Abstract—Topic models (e.g., pLSA, LDA, sLDA) have been
widely used for segmenting imagery. However, these models are
conﬁned to crisp segmentation, forcing a visual word (i.e., an image
patch) to belong to one and only one topic. Yet, there are many
images in which some regions cannot be assigned a crisp categorical
label (e.g., transition regions between a foggy sky and the ground or
between sand and water at a beach). In these cases, a visual word
is best represented with partial memberships across multiple topics.
To address this, we present a partial membership latent Dirichlet
allocation (PM-LDA) model and an associated parameter estimation
algorithm. This model can be useful for imagery where a visual
word may be a mixture of multiple topics. Experimental results on
visual and sonar imagery show that PM-LDA can produce both
crisp and soft semantic image segmentations; a capability previous
topic modeling methods do not have.

Index Terms—latent dirichlet allocation, partial membership,

image segmentation, soft image segmentation, topic model.

I. INTRODUCTION

T HE goal of unsupervised semantic image segmentation

is to divide an image into semantically distinct coherent
regions, i.e., regions corresponding to objects or parts of objects.
Inspired by the success of Latent Dirichlet Allocation (LDA) [1]
in discovering semantically meaningful topics from document
collections, many have successfully applied LDA or its variants
to image segmentation [2]–[6]. It has been used in a wide range
of computer vision applications, such as object recognition and
tracking and image retrieval [5], [7]–[9]. Yet, in many images, the
widely used crisp image segmentation methods fail to perform.
Speciﬁcally, these methods poorly address imagery in which there
are smooth gradients and transition regions. For example, consider
the photograph in Fig. 1a where the gradually thinning fog blurs
the boundary between the foggy sky and the mountain, a sharp
boundary between the “fog” and “mountain” topics does not exist.
Similarly, notice the lack of a sharp boundary given the gradually
fading sunlight in Fig. 1b or the gradually vanishing sand ripples

C. Chen, H. Trinh, G. O. Omotara, and T. Lagaunne are with the Department
of Electrical and Computer Engineering, University of Missouri, Columbia, MO,
65203. E-mail: ccwwf@mail.missouri.edu

A. Zare is with the Department of Electrical and Computer Engineering,

University of Florida, Gainesville, FL, 32611. E-mail: azare@ece.uﬂ.edu
J. Tory Cobb is with Naval Surface Warfare Center, Panama City, FL.
The authors graciously thank the Ofﬁce of Naval Research, Code 321, for
funding this research. Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily reﬂect
the views of the Ofﬁce of Naval Research.

Manuscript received December 28, 2016.

shown in the Synthetic Aperture SONAR (SAS) image of the sea
ﬂoor in Fig. 1c. In this paper, we present Partial Membership
Latent Dirichlet Allocation (PM-LDA) to address unsupervised
semantic image segmentation in the case of imagery with regions
of transition.

(a)

(b)

(c)

Fig. 1: Imagery with regions of gradual transition. (a) Image
with gradual transition from fog to mountain. (b) Sunset image
with gradual transition from sun to sky. (c) SONAR image with
gradually vanishing sand ripples.

Unsupervised semantic image segmentation methods differ
from traditional (i.e., non-hierarchical/ﬂat) segmentation methods
(e.g., normalized cuts algorithm [7]) by estimating and describing
additional inter-segment relationships. Namely, these methods
over-segment
imagery and then group the resulting “visual
words” into topic clusters such that small segments from the
same object class can be combined into a complete object and
provide a comprehensive organization of the larger scene. In
other words, unsupervised semantic image segmentation methods
cluster imagery hierarchically in which the lower level corresponds
to an over-segmentation of the imagery and the higher level
groups the over-segmented pieces into topic clusters. In addition
to the direct application of LDA to visual words, methods that
consider spatial structure have also been developed. These include
the Spatial Latent Topic model (Spatial-LTM) and Spatial LDA
(sLDA) model [3], [4]. Zhao, et al.[5] developed a Topic Random
Field (TRF) model to tackle the problems caused by discarding
spatial information and information loss by feature quantization.
Andreetto, et al.[6] proposed an Afﬁnity-based Latent Dirichlet
Allocation (A-LDA). Yet, under these existing topic models, a
visual word is only assigned to one topic (i.e., the word-topic
assignment is a binary indicator). In this paper, we generalize
LDA to allow for partial memberships.

II. PARTIAL MEMBERSHIP MODELS

Partial membership models and algorithms have been previously
developed in the literature. One prevalent partial membership

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

2

(a) BPM

(b) LDA

(c) PM-LDA

Fig. 2: (a) Graphical model for BPM. (b) Graphical model for LDA. (c) Graphical model for PM-LDA

approach is the Fuzzy C-means algorithm (FCM) [10] and, in
particular, FCM extensions for image segmentation [11]–[13].
FCM is a centroid-based clustering method which iteratively
minimizes the following objective function,

J =

N
(cid:88)

K
(cid:88)

n=1

k=1

nkd2(xn, µk)
zm

(1)

where znk represents the (partial) membership of the nth data
point xn to cluster k. The membership values are constrained
such that (cid:80)K
k=1 znk = 1 and znk ≥ 0. The m > 1 is the fuzziﬁer
parameter which controls the degree to which memberships are
mixed or non-binary, µk is the prototype representing cluster k,
and d2(xn, µk) is generally taken to be the squared Euclidean
distance between xn and µk.

Although the FCM has been extensively used, until recently,
probabilistic interpretations of FCM had not been developed. These
interpretations were contributed by Heller et al.by introducing the
Bayesian Partial Membership model (BPM) [14] and Glenn et
al.through the introduction of Bayesian Fuzzy Clustering (BFC)
[15], [16]. Both of these Bayesian models for partial membership
methods are non-hierarchical. In contrast, the proposed PM-LDA
extends these models to a hierarchical approach that allows for
semantic image segmentation like LDA. BFC and BPM are
reviewed in the following sub-sections.

A. Bayesian Partial Membership Model

In a ﬁnite mixture model, the data likelihood of xn, is

p(xn|β) =

πkpk(xn|βk),

(2)

K
(cid:88)

k=1

where {πk}K
k=1 are the mixture weights and β = {β1, β2, ..., βK}
are the mixture component parameters. pk(xn|βk) is the kth
mixture component with parameters βk. In this model, a data
point is assumed to come from one (and only one) of the K
mixture components. Thus, given its component assignment, zn,
the probability of a data point, xn, is deﬁned as p(xn|zn, β) =
(cid:81)K
k=1 znk = 1, and
zn = [zn1, zn2, ..., znK] is the binary membership vector. If znk =

k=1 pk(xn|βk)znk where znk ∈ {0, 1}, (cid:80)K

1, the data point xn is assumed to have been drawn from mixture
component k.

In order to obtain a model allowing multiple cluster member-
ships for a data point, the constraint znk ∈ {0, 1} is relaxed to
znk ∈ [0, 1]. The modiﬁed constraints and the inclusion of prior
distributions for several key parameters results in the Bayesian
Partial Membership model,

p(π, s, zn, xn|α, λ, β) = p(π|α)p(s|λ)p(zn|πs)

K
(cid:89)

k=1

pk(xn|βk)znk ,

(3)

where znk ∈ [0, 1], (cid:80)K
k=1 znk = 1, and π is the cluster mixing
proportion assumed to be distributed according to a Dirichlet
distribution with parameter α, i.e., π ∼ Dir(α). The scaling
factor, s, determines the level of cluster mixing and is distributed
according to an exponential distribution with mean 1/λ, s ∼
exp(λ), and zn ∼ Dir(πs) is the membership vector for data
point xn.

As shown in [14], if each of the mixture components are expo-
nential family distributions of the same type, then p(xn|zn, β) =
(cid:81)K
k=1 znk = 1, can be

k=1 pk(xn|βk)znk with znk ∈ [0, 1] and (cid:80)K

written as:

p(xn|zn, β) = Expon

znkηk

.

(4)

(cid:33)

(cid:32)

(cid:88)

k

This indicates that the data generating distribution for xn is of the
same exponential family distribution as the original K clusters,
but with new natural parameters (cid:80)
k znkηk. The new parameters
are a convex combination of the natural parameters, ηk, of the
original clusters weighted by znk. This provides the powerful (and
convenient) ability to sample directly from the unique mixture
distribution for each data point if the natural parameters of the
original clusters and the membership vector for the data point
are known. A graphical model of BPM is shown in Fig. 2a. The
generative process of BPM is described as follows:

1. Draw mixing proportion π from a Dirichlet distribution,

2. Draw scaling constant s from an exponential distribution

π ∼ Dir(α).

s ∼ exp(λ) = λe−λs.

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

3

3. For each data point xn

Given the cluster prototype µk and membership znk, the

(a) Draw the membership vector zn from a Dirichlet

distribution, zn ∼ Dir(sπ).
(b) Draw a data point xn ∼ Expon((cid:80)

k znkηk).

distribution of data point xn is described as
(cid:34)(cid:32) K
(cid:88)

(cid:40)

p(xn|zn, µk) ∝ exp

−

B. Bayesian Fuzzy Clustering Model

Bayesian Fuzzy Clustering (BFC) also models membership
vectors as Dirichlet distributed random variables. Input data points
are then modeled as Gaussian random variables. Since Gaussian
distributions are of the exponential family and using the result in
(4), the natural parameters for the Gaussian representing each data
point are convex combinations of the natural parameters of the
individual clusters weighted by the associated partial membership
values. In order to incorporate the fuzziﬁer parameter used in
FCM as shown in (1), each membership value is raised to the
mth power. This results in the data likelihood distribution (named
in [15] as the Fuzzy Data Likelihood (FDL)),

p(X|Z, Y) =

N
(cid:89)

n=1

1
Z(zn, m, Y)

K
(cid:89)

k=1

N (xn|µk, Q = zm

nkI)

with

and a prior distribution for the cluster membership (called Fuzzy
Cluster Prior (FCP)),

˜p(Z|Y ) =

Z(zn, m, Y)

z−mp/2
nk

Dir(zn|α)

(6)

N
(cid:89)

n=1

(cid:33)

(cid:32) K
(cid:89)

k=1

and a Gaussian prior distribution on cluster prototypes

p(Y) =

N (µk|µy, Σy),

(7)

K
(cid:89)

k=1

where Z(zn, m, Y) is a normalization constant, µk is the mean
of the kth Gaussian cluster, p is the dimensionality of the data, µy
and Σy are the sample mean and sample covariance, respectively.
The joint likelihood of data and parameters

p(X, Z, Y) = p(X|Z, Y)˜p(Z|Y)p(Y)
K
(cid:88)

N
(cid:88)

(cid:40)

∝ exp

−

nk (xn − µk)T (xn − µk)
zm

(cid:41)

1
2

N
(cid:89)

K
(cid:89)

n=1

k=1

n=1

k=1
(cid:40)

zαk−1
nk

exp

−

1
2

K
(cid:88)

k=1

(cid:0)µk − µy

(cid:1)T

Σ−1
y

(cid:0)µk − µy

(cid:1)

.

(cid:33)

zm
nk

xT

n xn

(cid:33)(cid:35)(cid:41)

k=1

1
2
(cid:32) K
(cid:88)

k=1

−2xT
n

zm
nkµk

+

nkµT
zm

k µk

.

(9)

(cid:33)

(cid:32) K
(cid:88)

k=1

Let Q = (cid:80)K

k=1 zm

nkI, and µ =

Equation (9) can be rewritten as

(cid:80)K

1
k=1 zm
nk

(cid:16)(cid:80)K

k=1 zm

nkµk

(cid:17)

,

p(xn|zn, µk) ∝ exp

−

(cid:2)xT

n Qxn − 2xT

(cid:26)

(cid:40)

1
2

1
2

(cid:34)(cid:32) K
(cid:88)

k=1

(cid:27)
n Qµ + µT Qµ(cid:3)
(cid:35)(cid:41)

(cid:33)

× exp

−

nkµT
zm

k µk

− µT Qµ

,

then

p(xn|zn, µk) = N (xn|µ, Q).

For data point xn, the generative process implicitly deﬁned in the
BFC is as follows:

(a) Draw the membership vector zn from the FCP.
(b) Draw a data point xn from a Gaussian distribution, xn ∼

(10)

(11)

Q =

zm
nkI

and

µ =

K
(cid:88)

k=1

1
k=1 zm
nk

(cid:80)K

(cid:32) K
(cid:88)

k=1

(cid:33)

zm
nkµk

.

The FCP is an improper prior which is uninformative about
the membership prior belief. Thus, is it unclear how to sample
memberships from the FCP in [15]. However, it is also noted
in [15] that the FCP can be converted to a proper prior by
(cid:16)(cid:81)K
replacing the second term
with a product of
Inverse-Gamma distribution with shape parameter as mp/2 − 1
with a small scale parameter.

k=1 z−mp/2

nk

(cid:17)

C. Uniﬁed Partial Membership Model

Considering both the BPM and BFC, a uniﬁed model which
combines the two models is developed in this paper. A comparison
of the terms in the BFC and BPM models is shown in Table I. The
major difference between the BPM and BFC models is how the
two approaches model the degree of mixing between clusters. The
BFC uses both a ﬁxed fuzziﬁer parameter and the membership
prior FCP to control the degree of mixing between clusters. In
contrast, in the BPM, the degree of mixing between clusters is
controlled only through the scaling hyper-parameter, s, found in
the prior distribution on partial membership values. The BPM
explicitly deﬁnes the cluster mixing proportion π while the BFC
does not. In terms of the data generating distribution, coefﬁcients

(cid:41)

(8)

Y = {µ1, · · · , µK} ,

(5)

N (˙|µ, Q), with

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

4

of the convex combination for BFC are raised to the m power
while the BPM does not.

can be computed as

TABLE I: Table comparison between BFC, BPM and the unifying
model.

BPM

m = 1

Unifying
model
m

s ∼ exp(λ)

s ∼ exp(λ)

π ∼ Dir(α)

π ∼ Dir(α)

fuzziﬁer
Scaling
Factor
Mixing
Prop.

BFC

m

N/A

N/A

of

Coeff.
Data
Generating
Distribution

Membership z ∼ FCP

z ∼ Dir(πs)

z ∼ Modi-
ﬁed FCP

zm
nk
k zm
nk

(cid:80)

zm
nk
k zm
nk

(cid:80)

, m = 1

zm
nk
k zm
nk

(cid:80)

In this paper, a uniﬁed model combining BFC and BPM is
proposed to combine the two approaches and investigate the effect
of fuzziﬁer m and scaling factor s on the membership mixing
level. The uniﬁed model is composed of an exponential prior on
scaling factor deﬁned as

p(s|λ) = λe−λs,

(12)

a Dirichlet prior on mixing proportion deﬁned as

p(π|α) =

(πk)(αk−1),

(13)

(cid:16)(cid:80)K

Γ

k=1 αk

(cid:17)

(cid:81)K

k=1 Γ(αk)

K
(cid:89)

k=1

a data likelihood in the same form of the FDL, deﬁned as

p(X|Z, Y) =

N
(cid:89)

n=1

1
Z(zn, m, Y)

K
(cid:89)

k=1

N (xn|µk, Q = zm

nkI),

(14)

and a prior distribution for the cluster membership deﬁned as

˜p(Z|Y) =

Z(zn, m, Y)

Dir(zn|sπ),

(15)

N
(cid:89)

n=1

(cid:33)

z−mp/2
nk

(cid:32) K
(cid:89)

k=1

which modiﬁes the FCP in the BFC by replacing the Dirichlet
parameter with sπ. The joint likelihood of the unifying model

p(X, Z, π, s|Y) = p(X|Z, Y)˜p(Z|Y)p(π)p(s)
N
(cid:26)
(cid:89)

K
(cid:89)

(2π)−p/2 exp

−

nk(xn − µk)T (xn − µk)
zm

(cid:27)

=

1
2

(znk)(sπk−1)

(cid:16)(cid:80)K

Γ

k=1 αk

(cid:17)

(cid:81)K

k=1 Γ(αk)

n=1

k=1
(cid:16)(cid:80)K

Γ

(cid:17)

k=1 sπk

(cid:81)K

k=1 Γ(sπk)

K
(cid:89)

k=1

K
(cid:89)

k=1

(πk)(αk−1)λe−λs.

(16)

(cid:27)

(17)

Thus, the log of this likelihood is

ln p(X, Z, Y, s, π) = const +
N
(cid:88)

K
(cid:88)

(cid:26)

−

nk(xn − µk)T (xn − µk)
zm

+

(cid:27)

N
(cid:88)

(cid:26)

n=1

1
2

(cid:33)

(cid:33)

K
(cid:88)

k=1
K
(cid:88)

k=1

n=1

ln Γ

k=1
(cid:32) K
(cid:88)

k=1
(cid:32) K
(cid:88)

k=1
ln λ − λs.

K
(cid:88)

k=1
K
(cid:88)

k=1

ln Γ

αk

−

ln Γ (αk) +

(αk − 1) ln πk +

sπk

−

ln Γ(sπk) +

(sπk − 1) ln znk

+

Figure 3 illustrates the effect of m and s on the membership
mixing level in the uniﬁed model. To generate this ﬁgure, two
Gaussian clusters were considered, N (0, 1) and N (1, 1). A
sequences of input data points, X, were generated in range
[−0.5, 1.5] with increment 0.05, and a set of membership values
for the ﬁrst cluster, Z, were generated in range [0, 1] with
increment 0.05 (resulting in the membership values in the second
cluster being 1-Z). The fuzziﬁer m was varied to be −1, −0.5,
0, 0.5, 1, 2, 5, and 10. The scaling factor s was varied to be
0.1, 0.5, 1, 2, and 5. The parameters λ, π, and α were set to
be 1, [0.5, 0.5], and [1, 1], respectively. Given this generated data
set and parameters, the log of joint likelihood in the unifying
model was computed and shown in Figure 3. Here, the X−axis
denotes the membership in the ﬁrst cluster ranging from 0 to 1
and the Y −axis denotes the input data point value, x, ranging
from −0.5 to 1.5. As shown in each column where the fuzziﬁer
m is ﬁxed, as s increases, mixing memberships correspond to
higher log likelihood values. Memberships with high log likelihood
values (bright yellow region) are gradually moved from the left
and right margins (corresponding to crisp memberships) to the
horizontal center (corresponding to highly mixed memberships).
The fuzziﬁer m determines the vertical position of the parameters
with high log likelihood values. As shown in each row, when m
increases, data points with high log likelihood values are gradually
expanded to the top and bottom margins. This ﬁgure illustrates

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

5

that the scaling factor s and the fuzziﬁer m impact resulting
partial membership values in different ways. One observation
is that the scaling factor s impacts the cluster mixing level by
controlling the prior knowledge on memberships and the fuzziﬁer
m by controlling the percentage of mixing data points.

The generative processes for both the BFC and BPM models
are similar. The main difference between the BPM and BFC
models is that the BFC uses both a ﬁxed fuzziﬁer parameter and a
scaling parameter to control the degree of mixing between topics.
In contrast, in the BPM, the degree of mixing between topics is
controlled only through a scaling hyper-parameter, s, found in
the prior distribution on partial membership values. The uniﬁed
model incorporates both of these parameters to allow a larger
degree of freedom in controlling the mixing between topics.

III. PARTIAL MEMBERSHIP LDA

To generalize LDA by allowing for partial memberships, a PM-
LDA model is proposed in this paper, where partial memberships
are introduced to the standard LDA in a similar fashion to the
developed uniﬁed model. Both the fuzziﬁer m and the scaling
factor s are incorporated in the proposed PM-LDA model. In
PM-LDA, the random variable associated with a data point is
assumed to be distributed according to multiple topics with a
continuous partial membership in each topic. First, we consider
the simplest case where the fuzziﬁer m = 1. Given, m = 1, the
PM-LDA model is

p(πd, sd, zdn, xdn|α, λ, β) = p(πd|α)p(sd|λ)p(zdn|πdsd)

K
(cid:89)

k=1

pk(xdn|βk)zdnk

(18)

where xdn is the nth word in document d, zdn is the partial
membership vector of xdn, πd ∼ Dir(α) and sd ∼ exp(λ)
are the topic proportion and the level of topic mixing in
document d, respectively. The parameter α corresponds to the
topic composition across a document. For example, in Fig. 1c, the
image may be composed of 40% “sand ripple” topic and 60% “ﬂat
sand” topic (i.e., α = [0.4, 0.6]). The parameter λ controls how
similar the partial membership vector of each word is expected to
be to the topic distribution of the document. For example, a small
λ would correspond to most words in a document to have partial
membership vectors very close to πd. During image segmentation,
a small λ generally corresponds to large transition regions (e.g.,
transition from “ﬂat sand” to “sand ripple” comprises most of the
image). For a large λ, the partial membership vectors for each
word can vary signiﬁcantly from the document mixing proportions.
In general, a large λ corresponds to very narrow (tending towards
crisp) transition regions during image segmentation (e.g., the SAS
image may have 39% of the visual words as pure “sand ripple”,
59% as pure “ﬂat sand”, and only 2% mixed).

The vector zdn ∼ Dir(πdsd) represents the partial memberships
of data point xdn in each of the K topics. If each topic distribution

(a)

(b)

Fig. 4: Partial membership data generating distributions. In (a), two
Gaussian topics with µ1 = [−4, −4]; µ2 = [6, 6], Σ1 = Σ2 = I.
In (b), two Gaussian topics with µ1 = [−4, −4]; µ2 = [6, 6],
Σ1 = [4, 0; 0, 1], Σ2 = [1, 0; 0, 4] [14].

is assumed to be of the exponential family, pk(·|βk) = Expon(ηk),
then using the result in (4), p(xdn|zdn, β) = Expon((cid:80)
k zdnkηk).
The graphical model for PM-LDA is shown in Fig. 2c and the
generative procedure of PM-LDA is described as follows.

1. For each document Xd, draw topic proportions πd ∼

2. Draw scaling factor sd from an exponential distribution

Dir(α).

sd ∼ exp(λ) = λe−λs.

3. For each word xdn

(a) Draw the membership vector zdn ∼ Dir(πdsd).
(b) Draw word xdn ∼ Expon((cid:80)

k zdnkηk).

For any value of the fuzzifer m, the above step 3(b) is modiﬁed
ηk). zdnk in the last

to be: Draw word xdn ∼ Expon((cid:80)
term of Equation (18) is correspondingly modiﬁed as

zm
dnk
j zm
dnj

(cid:80)

k

.

(cid:80)

zm
dnk
j zm
dnj

In PM-LDA, the membership zdn is drawn from a Dirichlet
distribution which is in contrast to a multinomial distribution as
used in LDA. With the inﬁnite number of possible values for zdn,
the word generating distributions in PM-LDA are expanded from
only K generating distributions (as in LDA) to inﬁnitely many.
Fig. 4 illustrates this using two Gaussian topic distributions, where
the membership value to one topic is varied from 0 to 1 with an
increment 0.1. The two original topics are shown as the Gaussian
distributions at either end. In LDA, words are generated from only
the two original topic distributions. In PM-LDA, words can be
generated from any (of the inﬁnitely many) convex combinations
of the topic distributions. As the scaling factor s → 0, the PM-
LDA model will degrade to the LDA model.

Given the hyperparameters Ψ = {α, λ, β}, the full PM-LDA

model over all words in the dth document is:

p(πd, sd, Zd, Xd|α, λ, β)

(19)

= p(πd|α)p(sd|λ)

p(xdn|zdn, β)p(zdn|πdsd).

Nd(cid:89)

n=1

where πd are the topic proportions, scaling factor sd, partial
membership vectors Zd = {zdn}Nd
n=1 and a set of Nd words Xd
for document d. The log of (19) when considering the speciﬁc
forms chosen in our model, is shown in (20).

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

6

Fig. 3: Log likelihood values of the uniﬁed model with different x, m and s parameter settings. To generate this ﬁgure, the uniﬁed
model was assumed to have two Gaussian clusters, N (0, 1) and N (1, 1). π = [0.5, 0.5], α = [1, 1]. The Y −axis denotes the value
of an input data point, x, which varies from −0.5 to 1.5 with increment 0.05. The X−axis denotes the membership in the ﬁrst
cluster varied from 0 to 1. For each row, the scaling factor s is ﬁxed and the fuzziﬁer m is varied to be different values. For each
column, the fuzziﬁer m is ﬁxed and the scaling factor s is varied to be different values. The color indicates the log likelihood value
for the ﬁxed data set with a given membership vector, m and s value. Yellow corresponds to a high log likelihood value and blue
corrsponds to a low log likelihood value.

Ld = ln(p(πd, sd, Zd, Xd|α, λ, β)) = ln Γ

αk

−

ln Γ (αk) +

(αk − 1) ln πdk + ln λ − λsd

+

ln p(xdn|zdn, β) +

ln Γ

sdπdk

−

ln Γ(sdπdk) +

(sdπdk − 1) ln zdnk

(20)

(cid:27)

Nd(cid:88)

n=1

(cid:32) K
(cid:88)

k=1

(cid:33)

(cid:33)

K
(cid:88)

k=1
K
(cid:88)

k=1

Nd(cid:88)

(cid:26)

n=1

(cid:32) K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

(cid:80)D

During parameter estimation, our goal is to maximize L =
d=1 Ld by estimating all the model parameters πd, sd, zdn, β.
In this paper, we employ a Metropolis within Gibbs [15], [17]
sampling approach.

where D = {X1, X2, ..., XD} includes all training documents
and Π, S, M include all of the topic proportions, scaling factors
and membership vectors, respectively.

IV. PARAMETER ESTIMATION FOR PM-LDA

The goal of parameter estimation is to maximize the following

posterior distribution,

p(Π, S, M, β|D, α, λ) ∝ p(Π, S, M, D|α, λ, β),

(21)

A Metropolis within Gibbs sampler is employed to perform the
MAP inference which can generate samples from the posterior
distribution in (21), [15], [17]. An outline of the sampler is
provided in Alg. 1. The sampler is a simple and straight-forward
implementation composed of a series of draws from candidate
distributions for each parameter and then evaluation of the
candidate in the appropriate acceptance ratio. Our implementation

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

7

of the sampler has been posted online.1 In our current imple-
mentation, we consider the topic distributions to be Gaussian
with different means, µk, but identical diagonal and isotropic
covariance matrices, Σk = σ2I.

imagery.

Algorithm 1 Metropolis-within-Gibbs Sampling Method for
Parameter Estimation
Input: A corpus D, the number of topics K, and the number of

Output: Collection of all samples: Π(t), S(t), M(t), β(t) =
K , Σ(t)

2 ..., µ(t)

2 , Σ(t)

(cid:111)
.

(cid:110)

K

sampling iterations T

1 , µ(t)
1 , Σ(t)
µ(t)
1: for t = 1 : T do
2:
3:

for d = 1 : D do

4:

5:
6:

7:
8:
9:

10:

11:
12:

(cid:110)
1, p(π†,s(t−1),Z(t−1),X|Ψ)p(π(t−1)|α)
p(π(t−1),s(t−1),Z(t−1),X|Ψ)p(π†|α)

Sample πd: Draw candidate: π† ∼ Dir(α)
Accept candidate with probability:
aπ = min
Sample sd: Draw candidate: s† ∼ exp(λ)
Accept candidate with probability:
as = min
for n = 1 : Nd do

(cid:110)
1, p(π(t),s†,Z(t−1),X|Ψ)p(s(t−1)|λ)
p(π(t),s(t−1),Z(t−1),X|Ψ)p(s†|λ)

(cid:111)

(cid:111)

Sample zdn: Draw candidate: z†
Accept candidate with probability:
n,xn|Ψ)
az = min

p(π(t),s(t),z†
p(π(t),s(t),z(t−1)

1,

(cid:110)

,xn|Ψ)

n

(cid:111)

n ∼ Dir(1K)

end for

end for
for k = 1 : K do

Sample µk: Draw proposal: µ†
k ∼ N (·|µD, f ΣD)
µD and ΣD are mean and covariance of the data
Accept candidate with probability:
(cid:17)

(cid:16)

p

p

(cid:16)

Π(t),S(t),M(t),D|µ†
k
(t−1)
Π(t),S(t),M(t),D|µ
k

N (µ
(cid:17)

(t−1)
k
N (µ†

|µD,ΣD)

(cid:27)

k|µD,ΣD)

ak = min

1,

(cid:26)

end for
Sample covariance matrices Σ = σ2I:
Draw candidate from:
σ2 = 1
2
Accept candidate with probability:

(cid:26)

aΣ = min

1,

p(Π(t),S(t),M(t),D|Σ†)
p(Π(t),S(t),M(t),D|Σ(t−1))

(cid:27)

.

(cid:8)maxxn d2(xn − µD) − minxn d2(xn − µD)(cid:9)

13: end for

The proposed Metropolis within Gibbs scheme will return the
full distribution of parameter values given the desired posterior.
We use the MAP sample (i.e., the sample with the largest log
posterior value) as the ﬁnal estimate, {Π∗, S∗, M∗, µ∗, Σ∗}.

V. DATA & EXPERIMENTAL RESULTS
In this section, we show results of image segmentation on two
datasets: (i) Synthetic Aperture SONAR (SAS) imagery and (ii)
Sunset imagery,. In the following experiments, unless otherwise
speciﬁed, the fuzziﬁer m value is set to 1.

1Code can be found at: https://github.com/TigerSense/PMLDA

A. Synthetic Aperture Sonar (SAS) Imagery Dataset

The ﬁrst set of experiments considers segmentation of SAS

1) SAS sub-image segmentation and comparison to LDA and
FCM: Our ﬁrst experiment considers segmentation of a subset of
four sub-images from our SAS image database (shown in the ﬁrst
column in Fig. 5). For each image, we simply compute the average
intensity value and entropy within a 21 × 21 window as feature
values. The average intensity value is scaled (×10) to roughly
the same magnitude of the average entropy value. Each image is
divided into multiple documents using a sliding window approach.
A document consists of all of the feature vectors associated with
each pixel (i.e., visual words) in the window. The number of
topics in this dataset is set to 3. For LDA, a dictionary of size
100 is built by clustering all the computed feature values using
the K-means. FCM results with m = 1.5. Parameters for LDA
and FCM were selected manually to provide the best results. Due
to the lack of ground-truth, qualitative segmentation results in
Fig. 5 is provided. In the ﬁrst row, Subﬁgures (b), (c), and (d)
show the partial membership maps in the “dark ﬂat sand” , “sand
ripple” , and “bright ﬂat sand” topics using PM-LDA, respectively.
Subﬁgures (f), (g), and (h) show the partial membership maps in
each of the three clusters using FCM, respectively. In (b) - (d)
and (f) - (h), the color indicates the degree of membership of a
visual word in a topic where red corresponds to a full membership
of 1 and dark blue color corresponds to a membership value of
0. The LDA result is shown in (e) where color indicates topic
assignment. Subﬁgures in Row 2-4 follow the same subﬁgure
captions in Row 1.

From the experimental results, we can see that PM-LDA
achieves much better results than FCM and LDA. As shown
in Fig. 5c and 5k, the segmentation results of PM-LDA show
a gradual change from “sand ripple” to “dark ﬂat sand”. FCM
captures the gradual transition to some extent but is not able to
clearly differentiate between clusters. For example, as shown in
Fig. 5o - 5p and Fig. 5w - 5x, using FCM, the rippled region in
Images 2 and 3 is assigned to 2 clusters with nearly equal partial
memberships. As LDA cannot generate partial memberships, in
Fig. 5e and 5m, Image 1 and 2 are simply partitioned into different
topics using LDA. Yet, by comparing Fig. 5u with 5t and Fig. 5ac
with 5ab, we can see that on Image 3 and 4 that do not contain
transition regions, LDA achieves similar segmentation result to
PM-LDA.

2) Complete SAS image segmentation and comparison to LDA
and sLDA: Experiments are then extended to the complete
SONAR images. Five complete high-frequency SAS (indicated as
HF-00, HF-01,..., HF-04, are segmented into 200 superpixels [18]
and each superpixel is considered as a document. In addition to
the mean and entropy features used in the previous experiment,
a “sand ripple”ﬁlter response is also used as the third feature
in this experiment. The ﬁlter is built based on the sand ripple
characterization algorithm proposed in [19]. It is hypothesized

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

8

(a) Image 1

(b) PM-LDA:1 (c) PM-LDA:2

(d) PM-LDA:3

(e) LDA

(f) FCM:1

(g) FCM:2

(h) FCM:3

(i) Image 2

(j) PM-LDA:1

(k) PM-LDA:2

(l) PM-LDA:3

(m) LDA

(n) FCM:1

(o) FCM:2

(p) FCM:3

(q) Image 3

(r) PM-LDA:1

(s) PM-LDA:2

(t) PM-LDA:3

(u) LDA

(v) FCM:1

(w) FCM:2

(x) FCM:3

(y) Image 4

(z) PM-LDA:1 (aa) PM-LDA:2 (ab) PM-LDA:3

(ac) LDA

(ad) FCM:1

(ae) FCM:2

(af) FCM:3

Fig. 5: Segmentation results of Image 1 - 4 using PM-LDA, FCM and LDA. (a): SAS Image. (b)-(d): PM-LDA partial membership
map in the “dark ﬂat sand,” “sand ripple,” “bright ﬂat sand” topics, respectively. (e): LDA result where color indicates topic label.
(f)-(h): FCM partial membership map in the ﬁrst, second, and third cluster, respectively. Subﬁgure captions in Row 2 - 4 follow those
in Row 1. In PM-LDA and FCM results, color indicates the degree of membership of a visual word in a topic or cluster.

is a rippled region with certain ripple
that each superpixel
frequency, fripple (number of ripples per meter). The sand ripple
characterization algorithm is applied to each superpixel to estimate
its ripple frequency. As the range resolution of the sonar imagery
is 0.025m, a complete ripple length L can be computed as
40/fripple. To capture the ripple repeating pattern, a ﬁlter is built
as [−1, 0, 1, −1, 0, 1], where 1 and 0 are matrices with height of
11 and width of (cid:100)L/3(cid:101). The ﬁlter is applied to the corresponding
superpixel and the ﬁlter response is used as the third feature for
that superpixel. Non-rippled superpixels have low ﬁlter responses
and ripple superpixels have high ﬁlter response.

In this experiment, we compare PM-LDA with LDA and sLDA
by running the parameter estimation on ﬁve SONAR images
simultaneously. For PM-LDA, the hyper-parameter λ and α is set
to be 0.5 and 1K. For sLDA, σ is set to be 0.1 and each document
is repeated for 4 times. The topic number is set to K = 3. The
segmentation results of PM-LDA, LDA, and sLDA are shown in
Fig. 6. Column (a) are the ﬁve complete SONAR images HF-00
to HF-04 with super-pixel boundaries. Column (b)-(d) are the
PM-LDA results, which represent the partial membership maps in
“sand ripple”, “sea grass” and “dark ﬂat sand” topic, respectively.
Column (e) and (f) are the LDA and sLDA results, respectively.
The color indicates the topic number. As shown in Fig. 6, PM-
LDA achieves similar segmentation results to LDA and sLDA
on SONAR imagery. All of them are able to learn the “sand
ripple”, “sea grass”, and “dark ﬂat sand” topics, and for each

topic, the corresponding regions localized by PM-LDA, LDA,
and sLDA are similar to each other. For example, “sand ripple”
region learned by PM-LDA (the red regions in Column (b) are
almost the same as “sand ripple” region learned by LDA and
sLDA (the dark blue regions in Column (e) and (f)). However,
PM-LDA has an exclusive capability of localizing the gradual
transition regions between different topics while both LDA and
sLDA only provide crispy boundaries. These partial membership
values mostly occur at the boundary between two topics. Thus,
PM-LDA is able to identify when the feature vector contains
information from multiple topics (as the feature vector is being
computed over a window that contains more than one topic). This
is a powerful result showing the effectiveness of PM-LDA in
providing semantic image understanding.

3) Varying scaling factor s: As discussed in Section III,
the scaling factor sd determines the similarity of the partial
membership vector of each word, zdn, to the topic proportion πd.
In this experiment, we investigated the effect of s by estimating the
memberships and topics with ﬁxed topic proportion. A subregion
consisting of three superpixels [20] is used in this experiment and
shown in Fig. 7. Each superpixel is treated as a document. The
topic proportion πd is set to be [1, 1, 1]/3 and the scaling factor s
is varied to be 3, 10, 300, 30000. The membership estimation
results are shown in Figure 8. Column (a)-(c) represent the
membership maps in “sand ripple”, “sea-grass”, and “dark ﬂat
sand” topic, respectively. In this set of superpixels, there are

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

9

(a) SONAR imagery
with super-pixel
boundary

(b) PM-LDA:
sand ripple

(c) PM-LDA:
sea grass

(d) PM-LDA:
dark ﬂat sand

(e) LDA

(f) sLDA

Fig. 6: Segmentation results of PM-LDA, LDA, and sLDA. Column (a) represents the original images HF-00 to HF-04 with super-pixel
boundaries. Column (b)-(d) are the PM-LDA results, which represent the partial membership maps in “sand ripple”, “sea grass” and
“dark ﬂat sand” topic, respectively. Column (e) and (f) are the LDA and sLDA results, respectively. The color indicates the topic
number.

no regions of “sea-grass” which PM-LDA correctly determines.
As can be seen, within each superpixel, as the scaling factor s
increases, the partial memberships gradually approach the topic
proportion [1, 1, 1]/3 and the membership map becomes more
smooth. This can be considered as a way of incorporating spatial
information. Visual words in a superpixel are spatial neighbors.
They are assumed to have similar attributes and should have
similar membership vectors. By using superpixels as documents
and increasing the value of scaling factor s, spatial information
is implicitly incorporated to PM-LDA.

4) Varying the number of superpixels: In the following exper-
iment, we study the effect of number of superpixels. SONAR

Fig. 7: A subregion of three superpixels

Imagery HF-00 is selected for this experiment. The number of
superpixel is varied to be 20, 48, and 200. The scaling factor s
is varied to be 1, 100, 500, and 1000. In contrast to the previous

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

10

(a) s = 3

(a) s = 10

(a) s = 300

(a) s = 30000

(b)

(b)

(b)

(b)

(c)

(c)

(c)

(c)

Fig. 8: Partial membership maps with varying s. Each row shows
the estimated membership maps of the three estimated topics. The
black contour indicates the superpixel boundary. The superpixels
are results published in [20].

experiment where the topic proportion is ﬁxed, the topic proportion
is learned in this experiment. The experimental results are shown
in Fig. 9 - 11. The three ﬁgures show the partial membership
maps in “sea-grass”, “dark ﬂat sand”, and ”sand ripple” topic,
respectively, with varying number of superpixels and varying
scaling factor s,.

In Fig. 9 - 10, at each row, when the scaling factor s increases,
especially when s changes from 500 to 1000, a few superpixels
within the red region (as shown in Column s = 1) change their
color from red to yellow, thus the red region (when s = 1)
becomes less and less continuous. Based on the discussion in above
experiment, with superpixels as documents, a large scaling factor
s should enhance the spatial continuity. However, this requires
a strict prerequisite that the topic proportions are accurately
estimated and consistent across superpixels that belong to the
same topic. Otherwise, matching a large scaling factor with a
superpixel will damage the membership estimation by pushing the
membership vector towards an incorrect topic proportion vector.
As illustrated, increasing the number of superpixels and matching
a large scaling factor does not guarantee an improved membership
estimation.

5) Varying fuzziﬁer: In this experiment, we investigate the effect
of the fuzziﬁer m using SAS image HF-00 with 48 superpixels.
m is varied to be 1, 2, and 3, respectively. Experimental results
are shown in Figure 12. In each column, as m increases, some
more highly mixed partial membership values are present around
boundary areas. /

B. Sunset Dataset

Finally, experimental results on Sunset dataset show the ability
of PM-LDA to perform partial membership segmentation given
visual natural imagery. 27 sunset themed images from Flickr (with
the necessary permissions)2 3 were used. Features used in this
experiment are 3 × 3 Gaussian ﬁlter (σ = 1) responses on LAB
channels, 3 × 3 Gaussian ﬁlter response (σ = 2) on blue channel,
ﬁrst order derivative of Gaussian along y-axis on L channel and
the log transform of blue channel. Each image is segmented into
15 superpixels using normalized cuts. The number of topics is set
to be 3. Experiments are run on 27 images simultaneously and
some of the experimental results are shown in Fig. 13. Columns
2-4 show the segmentation results of PM-LDA. Column 5 is the
LDA results with 3 topics. Comparing Column 3 and Column
5 in Fig. 13, we can see that PM-LDA can generate continuous
partial membership according to the extent to which the sky is
colored by sunlight. The partial membership map illustrates how
the topic gradually shift from one to the other. In contrast, LDA
can only produce 0-1 segmentation.

VI. CONCLUSION

In this paper the PM-LDA model

is introduced for soft
image segmentation. PM-LDA improves upon the LDA model by
introducing a partial membership rather than requiring a single
topic label for each word. Experimental results on three image
datasets demonstrate the capacity of PM-LDA model in both
soft and crisp image segmentation. Future work will include
developing a more efﬁcient sampling approach, e.g., a collapsed
Gibbs sampler, to accelerate the parameter estimation procedure.

VII. ACKNOWLEDGEMENTS

The authors graciously thank the Ofﬁce of Naval Research,
Code 321, for funding this research. Any opinions, ﬁndings, and
conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reﬂect the views of
the Ofﬁce of Naval Research.

REFERENCES

[1] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” Journal

of Machine Learning Research, vol. 3, pp. 993–1022, 2003. 1

[2] B. C. Russell, W. T. Freeman, A. Efros, J. Sivic, and A. Zisserman,
“Using multiple segmentations to discover objects and their extent in
image collections,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2006, pp. 1605–1614. 1

[3] L. Cao and L. Fei-Fei, “Spatially coherent latent topic model for concurrent
segmentation and classiﬁcation of objects and scenes,” in IEEE International
Conference on Computer Vision, 2007, pp. 1–8. 1

[4] X. Wang and E. Grimson, “Spatial latent dirichlet allocation,” in Advances

in Neural Information Processing Systems, 2008, pp. 1577–1584. 1

[5] B. Zhao, L. Fei-Fei, and E. P. Xing, “Image segmentation with topic random
ﬁeld,” in European Conference on Computer Vision, 2010, pp. 785–798. 1

2Photo can be found at: https://www.ﬂickr.com/photos/aoa-/6104409480/
3Photo can be found at: https://www.ﬂickr.com/photos/frenchdave/8482336933/

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

11

20 superpixels

48 superpixels

20 superpixels

48 superpixels

200 superpixels

s=1

s=100

s=500

s=1000

Fig. 9: Partial membership maps for HF-00 with varying number of superpixels and varying scaling factor s for topic 1 - sea grass.
Column 2 to 5 correspond to scaling factor s = 1, 100, 500, 1000, respectively.

200 superpixels
s=1

s=100

s=500

s=1000

Fig. 10: Partial membership maps of HF-00 with varying number of superpixels and varying scaling factor s for topic 2 - dark ﬂat
sand. Column 1 to 5 correspond to scaling factor s = 1, 100, 500, 1000, respectively.

[6] M. Andreetto, L. Zelnik-Manor, and P. Perona, “Unsupervised learning of
categorical segments in image collections,” IEEE Transactions on Pattern

Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1842–1855, 2012. 1
[7] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

12

20 superpixels

48 superpixels

200 superpixels
s=1

s=100

s=500

s=1000

Fig. 11: Partial membership maps of HF-00 with varying number of superpixels and varying scaling factor s for topic 3 - sand ripple.
Column 1 to 5 correspond to scaling factor s = 1, 100, 500, 1000, respectively.

extended synthetic aperture sonar model and parallel sampling method,”
IEEE Transaction on Geoscience and Remote Sensing., vol. 53, no. 10, pp.
5547–5559, 2015. 7

[20] J. T. Cobb and A. Zare, “Multi-image texton selection for sonar image
seabed co-segmentation,” in SPIE, vol. 8709, no. 87090H, June 2013. 8, 10

Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 8,
pp. 888–905, 2000. 1

[8] D. Comaniciu and P. Meer, “Mean shift: A robust approach toward feature
space analysis,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 24, no. 5, pp. 603–619, 2002. 1

[9] P. F. Felzenszwalb and D. P. Huttenlocher, “Efﬁcient graph-based image
segmentation,” International Journal of Computer Vision, vol. 59, no. 2, pp.
167–181, 2004. 1

[10] J. C. Bezdek, R. Ehrlich, and W. Full, “Fcm: The fuzzy c-means clustering
algorithm,” Computers & Geosciences, vol. 10, no. 2, pp. 191–203, 1984. 2
[11] S. Naz, H. Majeed, and H. Irshad, “Image segmentation using fuzzy
clustering: A survey,” in International Conference on Emerging Technologies
(ICET), Oct 2010, pp. 181–186. 2

[12] S. Krinidis and V. Chatzis, “A robust fuzzy local information c-means
clustering algorithm,” IEEE Transactions on Image Processing, vol. 19,
no. 5, pp. 1328–1337, May 2010. 2

[13] S. Krinidis and M. Krinidis, “Generalised fuzzy local information c-means
clustering algorithm,” Electronics Letters, vol. 48, no. 23, pp. 1468–1470,
November 2012. 2

[14] K. A. Heller, S. Williamson, and Z. Ghahramani, “Statistical models for
partial membership,” in International Conference on Machine Learning,
2008, pp. 392–399. 2, 5

[15] T. Glenn, A. Zare, and P. Gader, “Bayesian fuzzy clustering,” IEEE
Transaction on Fuzzy Systems, no. 8, pp. 1545–1561, 2015. 2, 3, 6
[16] T. C. Glenn, “Context-dependent detection in hyperspectral imagery,” 2013.

2

[17] C. Robert and G. Casella, Monte Carlo statistical methods. Springer Science

& Business Media, 2013. 6

[18] J. T. Cobb and A. Zare, “Boundary detection and superpixel formation in
synthetic aperture sonar imagery,” in International Conference on SAS and
SAR, Sept. 2014. 7

[19] C. Chen, A. Zare, and J. T. Cobb, “Sand ripple characterization using an

PLACE
PHOTO
HERE

Chao Chen received the B.S. and M.S. degree in control
theory both from Xidian University, Xi’an, China, in
2007 and 2010, respectively, and the Ph.D. degree from
University of Missouri - Columbia, in 2016. Her research
interests include sparse coding, Bayesian inference, and
synthetic aperture sonar imagery analysis.

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

13

(a)m = 1

(a)m = 2

(a)m = 3

(b)

(b)

(b)

(c)

(c)

(c)

Fig. 12: The partial membership maps of HF-00 with varying fuzziﬁer. Row 1-3 represent the partial membership maps of HF-00
with fuzziﬁer = 1, 2, and 3, respectively. The superpixel number is 48.

Gbenga Omotara is currently pursuing an undergrad-
uate degree in Electrical and Computer Engineering at
the University of Missouri, Columbia, MO, USA. His
interests include machine learning and computational
intelligence.

PLACE
PHOTO
HERE

PLACE
PHOTO
HERE

Alina Zare (S’07–M’08–SM’13) received the Ph.D.
degree from the University of Florida, Gainesville, in
2008. She is currently an Associate Professor with the
Department of Electrical and Computer Engineering,
University of Florida. Her research interests include
machine learning, computational intelligence, Bayesian
methods, sparsity promotion, image analysis, pattern
recognition, hyperspectral image analysis, and remote
sensing. Alina Zare is a recipient of the 2014 National
Science Foundation CAREER award and the 2014 Na-
tional Geospatial-Intelligence Agency New Investigator
Program Award. Alina Zare is an Associate Editor of the IEEE Transactions on
Geoscience and Remote Sensing.

Huy N. Trinh received the B.S. degree in Computer
Science at University of Missouri, Columbia, USA, in
2015. He is currently a Graduate Research Assistant
working toward the M.S degree in the Department of
Computer Science, University of Missouri, Columbia,
USA. His research interests include machine learning,
image segmentation, computational intelligent, cloud
computing and networks.

PLACE
PHOTO
HERE

CHEN et al.: PARTIAL MEMBERSHIP LATENT DIRICHLET ALLOCATION

14

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

(j)

(k)

(l)

(m)

(n)

(o)

Fig. 13: Examples of segmentation result on Sunset dataset. (a): Sunset Image 1. (f): Sunset Image 2. (k): Sunset Image 3. (b)-(d),
(g)-(i), and (l)-(n) are the PM-LDA partial membership maps in the estimated three topics for Sunset Image 1, Sunset Image 2, and
Sunset Image 3, respectively. The color indicates the degree of membership of a visual word in a topic or cluster. (e), (j), and (o) are
the LDA results where color indicates the topic.

PLACE
PHOTO
HERE

J. Tory Cobb (S’99–M’01–SM’13) received the B.S.
degree in electrical engineering from the United States
Coast Guard Academy, New London, CT, USA in 1994,
the M.S. degree in electrical engineering from Auburn
University, Auburn, AL, USA, in 2001, and the Ph.D.
degree from the University of Florida, Gainesville, FL,
USA, in 2011. From 1994 to 1999 he was an active
duty ofﬁcer in the Coast Guard. Since 2001 he has been
employed as a Research Engineer at the Naval Surface
Warfare Center, Panama City, FL, USA. He has served
as Principal Investigator or Co-Principal Investigator for
various automatic target recognition and sensor fusion projects funded by the
Ofﬁce of Naval Research. His research interests include statistical modeling
of sonar signals with applications to automatic target recognition, automated
environmental characterization of seabeds in side-look sonar images, and sonar
image segmentation algorithm development. Dr. Cobb is an Associate Editor of
the IEEE Journal of Oceanic Engineering.

Timotius A. Lagaunne is currently pursuing an under-
graduate degree in Mathematics at the University of
Missouri Columbia, MO, USA. His interests include ma-
chine learning, computational intelligence, data science
and natural language processing

PLACE
PHOTO
HERE

