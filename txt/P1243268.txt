Phrase Localization and Visual Relationship Detection with Comprehensive
Image-Language Cues

Bryan A. Plummer

Arun Mallya

Christopher M. Cervantes

Julia Hockenmaier

Svetlana Lazebnik
University of Illinois at Urbana-Champaign
{bplumme2, amallya2, ccervan2, juliahmr, slazebni}@illinois.edu

7
1
0
2
 
g
u
A
 
9
 
 
]

V
C
.
s
c
[
 
 
4
v
1
4
6
6
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

This paper presents a framework for localization or
grounding of phrases in images using a large collection
of linguistic and visual cues. We model the appearance,
size, and position of entity bounding boxes, adjectives that
contain attribute information, and spatial relationships be-
tween pairs of entities connected by verbs or prepositions.
Special attention is given to relationships between people
and clothing or body part mentions, as they are useful for
distinguishing individuals. We automatically learn weights
for combining these cues and at test time, perform joint in-
ference over all phrases in a caption. The resulting system
produces state of the art performance on phrase localiza-
tion on the Flickr30k Entities dataset [33] and visual rela-
tionship detection on the Stanford VRD dataset [27].1

1. Introduction

Today’s deep features can give reliable signals about a
broad range of content in natural images, leading to ad-
vances in image-language tasks such as automatic cap-
tioning [6, 14, 16, 17, 42] and visual question answer-
ing [1, 8, 44]. A basic building block for such tasks is lo-
calization or grounding of individual phrases [6, 16, 17, 28,
33, 40, 42]. A number of datasets with phrase grounding
information have been released, including Flickr30k Enti-
ties [33], ReferIt [18], Google Referring Expressions [29],
and Visual Genome [21]. However, grounding remains
challenging due to open-ended vocabularies, highly unbal-
anced training data, prevalence of hard-to-localize entities
like clothing and body parts, as well as the subtlety and va-
riety of linguistic cues that can be used for localization.

The goal of this paper is to accurately localize a bound-
ing box for each entity (noun phrase) mentioned in a caption
for a particular test image. We propose a joint localization
objective for this task using a learned combination of single-
phrase and phrase-pair cues. Evaluation is performed on the

1Code: https://github.com/BryanPlummer/pl-clc

Figure 1: Left: an image and caption, together with ground truth bounding
boxes of entities (noun phrases). Right: a list of all the cues used by our
system, with corresponding phrases from the sentence.

challenging recent Flickr30K Entities dataset [33], which
provides ground truth bounding boxes for each entity in the
ﬁve captions of the original Flickr30K dataset [43].

Figure 1 introduces the components of our system using
an example image and caption. Given a noun phrase ex-
tracted from the caption, e.g., red and blue umbrella, we ob-
tain single-phrase cue scores for each candidate box based
on appearance (modeled with a phrase-region embedding as
well as object detectors for common classes), size, position,
and attributes (adjectives). If a pair of entities is connected
by a verb (man carries a baby) or a preposition (woman
in a red jacket), we also score the pair of corresponding
candidate boxes using a spatial model. In addition, actions
may modify the appearance of either the subject or the ob-
ject (e.g., a man carrying a baby has a characteristic appear-
ance, as does a baby being carried). To account for this, we
learn subject-verb and verb-object appearance models for
the constituent entities. We give special treatment to rela-
tionships between people, clothing, and body parts, as these
are commonly used for describing individuals, and are also
among the hardest entities for existing approaches to local-
ize. To extract as complete a set of relationships as possible,
we use natural language processing (NLP) tools to resolve
pronoun references within a sentence: e.g., by analyzing the

Adjectives Verbs

Method

Ours

(a) NonlinearSP [40]
GroundeR [34]
MCB [8]
SCRC [12]
SMPL [41]
RtP [33]

(b) Scene Graph [15]
ReferIt [18]
Google RefExp [29]

Single Phrase Cues

Phrase-Region Candidate Candidate Object
Compatibility
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
–
–
(cid:88)

Position
(cid:88)
–
–
–
(cid:88)
–
–
–
(cid:88)
(cid:88)

Detectors
(cid:88)*
–
–
–
–
–
(cid:88)*
(cid:88)
(cid:88)
–

Size
(cid:88)
–
–
–
–
–
(cid:88)
–
(cid:88)
(cid:88)

(cid:88)
–
–
–
–
–
(cid:88)*
(cid:88)
(cid:88)*
–

Phrase-Pair Spatial Cues
Relative
Position
(cid:88)
–
–
–
–
(cid:88)*
–
(cid:88)
(cid:88)
–

Clothing &
Body Parts
(cid:88)
–
–
–
–
–
–
–
–
–

Inference
Joint
Localization
(cid:88)
–
–
–
–
(cid:88)
–
(cid:88)
–
–

(cid:88)
–
–
–
–
–
–
–
–
–

Table 1: Comparison of cues for phrase-to-region grounding. (a) Models applied to phrase localization on Flickr30K Entities. (b) Models on related tasks.
* indicates that the cue is used in a limited fashion, i.e. [18, 33] restricted their adjective cues to colors, [41] only modeled possessive pronoun phrase-pair
spatial cues ignoring verb and prepositional phrases, [33] and we limit the object detectors to 20 common categories.

sentence A man puts his hand around a woman, we can de-
termine that the hand belongs to the man and introduce the
respective pairwise term into our objective.

Table 1 compares the cues used in our work to those in
other recent papers on phrase localization and related tasks
like image retrieval and referring expression understanding.
To date, other methods applied to the Flickr30K Entities
dataset [8, 12, 34, 40, 41] have used a limited set of single-
phrase cues. Information from the rest of the caption, like
verbs and prepositions indicating spatial relationships, has
been ignored. One exception is Wang et al. [41], who tried
to relate multiple phrases to each other, but limited their re-
lationships only to those indicated by possessive pronouns,
not personal ones. By contrast, we use pronoun cues to the
full extent by performing pronominal coreference. Also,
ours is the only work in this area incorporating the visual
aspect of verbs. Our formulation is most similar to that
of [33], but with a larger set of cues, learned combination
weights, and a global optimization method for simultane-
ously localizing all the phrases in a sentence.

In addition to our experiments on phrase localization, we
also adapt our method to the recently introduced task of
visual relationship detection (VRD) on the Stanford VRD
dataset [27]. Given a test image, the goal of VRD is to de-
tect all entities and relationships present and output them in
the form (subject, predicate, object) with the correspond-
ing bounding boxes. By contrast with phrase localization,
where we are given a set of entities and relationships that
are in the image, in VRD we do not know a priori which
objects or relationships might be present. On this task, our
model shows signiﬁcant performance gains over prior work,
with especially acute differences in zero-shot detection due
to modeling cues with a vision-language embedding. This
adaptability to never-before-seen examples is also a notable
distinction between our approach and prior methods on re-
lated tasks (e.g. [7, 15, 18, 20]), which typically train their
models on a set of predeﬁned object categories, providing
no support for out-of-vocabulary entities.

Section 2 discusses our global objective function for si-
multaneously localizing all phrases from the sentence and
describes the procedure for learning combination weights.
Section 3.1 details how we parse sentences to extract enti-
ties, relationships, and other relevant linguistic cues. Sec-
tions 3.2 and 3.3 deﬁne single-phrase and phrase-pair cost
functions between linguistic and visual cues. Section 4
presents an in-depth evaluation of our cues on Flickr30K
Entities [33]. Lastly, Section 5 presents the adaptation of
our method to the VRD task [27].

2. Phrase localization approach

We follow the task deﬁnition used in [8, 12, 33, 34, 40,
41]: At test time, we are given an image and a caption with
a set of entities (noun phrases), and we need to localize each
entity with a bounding box. Section 2.1 describes our infer-
ence formulation, and Section 2.2 describes our procedure
for learning the weights of different cues.

2.1. Joint phrase localization

For each image-language cue derived from a single
phrase or a pair of phrases (Figure 1), we deﬁne a cue-
speciﬁc cost function that measures its compatibility with an
image region (small values indicate high compatibility). We
will describe the cost functions in detail in Section 3; here,
we give our test-time optimization framework for jointly lo-
calizing all phrases from a sentence.

Given a single phrase p from a test sentence, we score
each region (bounding box) proposal b from the test image
based on a linear combination of cue-speciﬁc cost functions
φ{1,··· ,KS }(p, b) with learned weights wS:

S(p,b;wS)=

1s(p)φs(p,b)wS
s ,

(1)

KS(cid:88)

s=1

where 1s(p) is an indicator function for the availability of
cue s for phrase p (e.g., an adjective cue would be avail-
able for the phrase blue socks, but would be unavailable for

socks by itself). As will be described in Section 3.2, we use
14 single-phrase cost functions: region-phrase compatibil-
ity score, phrase position, phrase size (one for each of the
eight phrase types of [33]), object detector score, adjective,
subject-verb, and verb-object scores.

For a pair of phrases with some relationship r =
(p, rel, p(cid:48)) and candidate regions b and b(cid:48), an analogous
scoring function is given by a weighted combination of pair-
wise costs ψ{1,··· ,KQ}(r, b, b(cid:48)):

Q(r,b,b(cid:48);wQ)=

1q(r)ψq(r,b,b(cid:48))wQ
q .

(2)

KQ
(cid:88)

q=1

We use three pairwise cost functions corresponding to spa-
tial classiﬁers for verb, preposition, and clothing and body
parts relationships (Section 3.3).

We train all cue-speciﬁc cost functions on the training set
and the combination weights on the validation set. At test
time, given an image and a list of phrases {p1, · · · , pN },
we ﬁrst retrieve top M candidate boxes for each phrase pi
using Eq. (1). Our goal is then to select one bounding box
bi out of the M candidates per each phrase pi such that the
following objective is minimized:




(cid:88)



pi

min
b1,···,bN

S(pi,bi) +

Q(rij,bi,bj)

(3)

(cid:88)

rij =(pi,relij ,pj )






where phrases pi and pj (and respective boxes bi and bj)
are related by some relationship relij. This is a binary
quadratic programming formulation inspired by [38]; we
relax and solve it using a sequential QP solver in MAT-
LAB. The solution gives a single bounding box hypothe-
sis for each phrase. Performance is evaluated using Re-
call@1, or proportion of phrases where the selected box has
Intersection-over-Union (IOU) ≥ 0.5 with the ground truth.

2.2. Learning scoring function weights

We learn the weights wS and wQ in Eqs. (1) and (2) by
directly optimizing recall on the validation set. We start by
ﬁnding the unary weights wS that maximize the number of
correctly localized phrases:

We optimize Eq. (4) using a derivative-free direct search
method [22] (MATLAB’s fminsearch). We randomly ini-
tialize the weights, keep the best weights after 20 runs based
on validation set performance (takes just a few minutes to
learn weights for all single phrase cues in our experiments).
Next, we ﬁx wS and learn the weights wQ over phrase-
pair cues in the validation set. To this end, we formulate an
objective analogous to Eq. (4) for maximizing the number
of correctly localized region pairs. Similar to Eq. (5), we
deﬁne the function ˆρ(r; w) to return the best pair of boxes
for the relationship r = (p, rel, p(cid:48)):

ˆρ(r;w)= min
b,b(cid:48)∈B

S(p,b;wS)+S(p(cid:48),b(cid:48);wS)+Q(r,b,b(cid:48);w). (6)

Then our pairwise objective function is

wQ = arg max

IP airIOU ≥0.5(ρ∗

k, ˆρ(rk; w)),

(7)

M
(cid:88)

k=1

w

k).

where M is the number of phrase pairs with a relation-
ship, IP airIOU ≥0.5 returns the number of correctly local-
ized boxes (0, 1, or 2), and ρ∗
k is the ground truth box pair
for the relationship rk = (pk, relk, p(cid:48)

Note that we also attempted to learn the weights wS and
wQ using standard approaches such as rank-SVM [13], but
found our proposed direct search formulation to work bet-
ter. In phrase localization, due to its Recall@1 evaluation
criterion, only the correctness of one best-scoring candidate
region for each phrase matters, unlike in typical detection
scenarios, where one would like all positive examples to
have better scores than all negative examples. The VRD
task of Section 5 is a more conventional detection task, so
there we found rank-SVM to be more appropriate.

3. Cues for phrase-region grounding

Section 3.1 describes how we extract linguistic cues from
sentences. Sections 3.2 and 3.3 give our deﬁnitions of the
two types of cost functions used in Eqs. (1) and (2): sin-
gle phrase cues (SPC) measure the compatibility of a given
phrase with a candidate bounding box, and phrase pair cues
(PPC) ensure that pairs of related phrases are localized in a
spatially coherent manner.

wS = arg max

w

N
(cid:88)

i=1

1IOU ≥0.5(b∗

i , ˆb(pi; w)),

(4)

3.1. Extracting linguistic cues from captions

where N is the number of phrases in the training set,
1IOU ≥0.5 is an indicator function returning 1 if the two
boxes have IOU ≥ 0.5, b∗
i is the ground truth bounding box
for phrase pi, ˆb(p; w) returns the most likely box candidate
for phrase p under the current weights, or, more formally,
given a set of candidate boxes B,

ˆb(p; w) = min
b∈B

S(p, b; w).

(5)

The Flickr30k Entities dataset provides annotations for
Noun Phrase (NP) chunks corresponding to entities, but lin-
guistic cues corresponding to adjectives, verbs, and preposi-
tions must be extracted from the captions using NLP tools.
Once these cues are extracted, they will be translated into
visually relevant constraints for grounding.
In particular,
we will learn specialized detectors for adjectives, subject-
verb, and verb-object relationships (Section 3.2). Also, be-
cause pairs of entities connected by a verb or preposition

have constrained layout, we will train classiﬁers to score
pairs of boxes based on spatial information (Section 3.3).

Adjectives are part of NP chunks so identifying them is
trivial. To extract other cues, such as verbs and preposi-
tions that may indicate actions and spatial relationships, we
obtain a constituent parse tree for each sentence using the
Stanford parser [37]. Then, for possible relational phrases
(prepositional and verb phrases), we use the method of Fi-
dler et al. [7], where we start at the relational phrase and
then traverse up the tree and to the left until we reach a noun
phrase node, which will correspond to the ﬁrst entity in an
(entity1, rel, entity2) tuple. The second entity is given by
the ﬁrst noun phrase node on the right side of the relational
phrase in the parse tree. For example, given the sentence A
boy running in a ﬁeld with a dog, the extracted NP chunks
would be a boy, a ﬁeld, a dog. The relational phrases would
be (a boy, running in, a ﬁeld) and (a boy, with, a dog).

Notice that a single relational phrase can give rise to mul-
tiple relationship cues. Thus, from (a boy, running in, a
ﬁeld), we extract the verb relation (boy, running, ﬁeld) and
prepositional relation (boy, in, ﬁeld). An exception to this
is a relational phrase where the ﬁrst entity is a person and
the second one is of the clothing or body part type,2 e.g.,
(a boy, running in, a jacket). For this case, we create a sin-
gle special pairwise relation (boy, jacket) that assumes that
the second entity is attached to the ﬁrst one and the exact
relationship words do not matter, i.e., (a boy, running in, a
jacket) and (a boy, wearing, a jacket) are considered to be
the same. The attachment assumption can fail for phrases
like (a boy, looking at, a jacket), but such cases are rare.

Finally, since pronouns in Flickr30k Entities are not
annotated, we attempt to perform pronominal coreference
(i.e., creating a link between a pronoun and the phrase
it refers to) in order to extract a more complete set of
cues. As an example, given the sentence Ducks feed them-
selves, initially we can only extract the subject-verb cue
(ducks, f eed), but we don’t know who or what they are
feeding. Pronominal coreference resolution tells us that the
ducks are themselves eating and not, say, feeding ducklings.
We use a simple rule-based method similar to knowledge-
poor methods [11, 31]. Given lists of pronouns by type,3 our
rules attach each pronoun with at most one non-pronominal
mention that occurs earlier in the sentence (an antecedent).
We assume that subject and object pronouns often refer to
the main subject (e.g. [A dog] laying on the ground looks
up at the dog standing over [him]), reﬂexive and recipro-
cal pronouns refer to the nearest antecedent (e.g. [A tennis
player] readies [herself].), and indeﬁnite pronouns do not
refer to a previously described entity. It must be noted that

2Each NP chunk from the Flickr30K dataset is classiﬁed into one of

eight phrase types based on the dictionaries of [33].

3Relevant pronoun types are subject, object, reﬂexive, reciprocal, rela-

tive, and indeﬁnite.

compared with verb and prepositional relationships, rela-
tively few additional cues are extracted using this procedure
(432 pronoun relationships in the test set and 13,163 in the
train set, while the counts for the other relationships are on
the order of 10K and 300K).

3.2. Single Phrase Cues (SPCs)

Region-phrase compatibility: This is the most basic cue
relating phrases to image regions based on appearance. It
is applied to every test phrase (i.e., its indicator function
in Eq. (1) is always 1). Given phrase p and region b, the
cost φCCA(p, b) is given by the cosine distance between
p and b in a joint embedding space learned using normal-
ized Canonical Correlation Analysis (CCA) [10]. We use
the same procedure as [33]. Regions are represented by the
fc7 activations of a Fast-RCNN model [9] ﬁne-tuned using
the union of the PASCAL 2007 and 2012 trainval sets [5].
After removing stopwords, phrases are represented by the
HGLMM ﬁsher vector encoding [19] of word2vec [30].

Candidate position: The location of a bounding box in
an image has been shown to be predictive of the kinds of
phrases it may refer to [4, 12, 18, 23]. We learn location
models for each of the eight broad phrase types speciﬁed
in [33]: people, clothing, body parts, vehicles, animals,
scenes, and a catch-all “other.” We represent a bounding
box by its centroid normalized by the image size, the per-
centage of the image covered by the box, and its aspect
ratio, resulting in a 4-dim. feature vector. We then train
a support vector machine (SVM) with a radial basis func-
tion (RBF) kernel using LIBSVM [2]. We randomly sample
EdgeBox [46] proposals with IOU < 0.5 with the ground
truth boxes for negative examples. Our scoring function is

φpos(p, b) = − log(SVMtype(p)(b)),

where SVMtype(p) returns the probability that box b is of the
phrase type type(p) (we use Platt scaling [32] to convert the
SVM output to a probability).

Candidate size: People have a bias towards describing
larger, more salient objects, leading prior work to consider
the size of a candidate box in their models [7, 18, 33]. We
follow the procedure of [33], so that given a box b with di-
mensions normalized by the image size, we have

φsizetype(p) (p, b) = 1 − bwidth × bheight.

Unlike phrase position, this cost function does not use a
trained SVM per phrase type. Instead, each phrase type is
its own feature and the corresponding indicator function re-
turns 1 if that phrase belongs to the associated type.

Detectors: CCA embeddings are limited in their ability to
localize objects because they must account for a wide range
of phrases and because they do not use negative examples

during training. To compensate for this, we use Fast R-
CNN [9] to learn three networks for common object cate-
gories, attributes, and actions. Once a detector is trained, its
score for a region proposal b is

φdet(p, b) = − log(softmaxdet(p, b)),

where softmaxdet(p, b) returns the output of the softmax
layer for the object class corresponding to p. We manu-
ally create dictionaries to map phrases to detector categories
(e.g., man, woman, etc. map to ‘person’), and the indicator
function for each detector returns 1 only if one of the words
in the phrase exists in its dictionary. If multiple detectors
for a single cue type are appropriate for a phrase (e.g., a
black and white shirt would have two adjective detectors
ﬁre, one for each color), the scores are averaged. Below,
we describe the three detector networks used in our model.
Complete dictionaries can be found in Appendix B.

Objects: We use the dictionary of [33] to map nouns to the
20 PASCAL object categories [5] and ﬁne-tune the network
on the union of the PASCAL VOC 2007 and 2012 trainval
sets. At test time, when we run a detector for a phrase that
maps to one of these object categories, we also use bound-
ing box regression to reﬁne the original region proposals.
Regression is not used for the other networks below.

Adjectives: Adjectives found in phrases, especially color,
provide valuable attribute information for localization [7,
15, 18, 33]. The Flickr30K Entities baseline approach [33]
used a network trained for 11 colors. As a generalization
of that, we create a list of adjectives that occur at least 100
times in the training set of Flickr30k. After grouping to-
gether similar words and ﬁltering out non-visual terms (e.g.,
adventurous), we are left with a dictionary of 83 adjectives.
As in [33], we consider color terms describing people (black
man, white girl) to be separate categories.

Subject-Verb and Verb-Object: Verbs can modify the ap-
pearance of both the subject and the object in a relation. For
example, knowing that a person is riding a horse can give
us better appearance models for ﬁnding both the person and
the horse [35, 36]. As we did with adjectives, we collect
verbs that occur at least 100 times in the training set, group
together similar words, and ﬁlter out those that don’t have
a clear visual aspect, resulting in a dictionary of 58 verbs.
Since a person running looks different than a dog running,
we subdivide our verb categories by phrase type of the sub-
ject (resp. object) if that phrase type occurs with the verb
at least 30 times in the train set. For example, if there are
enough animal-running occurrences, we create a new cate-
gory with instances of all animals running. For the remain-
ing phrases, we train a catch-all detector over all the phrases
related to that verb. Following [35], we train separate detec-
tors for subject-verb and verb-object relationships, resulting
in dictionary sizes of 191 (resp. 225). We also attempted to

learn subject-verb-object detectors as in [35, 36], but did not
see a further improvement.

3.3. Phrase-Pair Cues (PPCs)

So far, we have discussed cues pertaining to a single
phrase, but relationships between pairs of phrases can also
provide cues about their relative position. We denote such
relationships as tuples (pleft , rel, pright ) with left, right in-
dicating on which side of the relationship the phrases oc-
cur. As discussed in Section 3.1, we consider three distinct
types of relationships: verbs (man, riding, horse), preposi-
tions (man, on, horse), and clothing and body parts (man,
wearing, hat). For each of the three relationship types, we
group phrases referring to people but treat all other phrases
as distinct, and then gather all relationships that occur at
least 30 times in the training set. Then we learn a spatial
relationship model as follows. Given a pair of boxes with
coordinates b = (x, y, w, h) and b(cid:48) = (x(cid:48), y(cid:48), w(cid:48), h(cid:48)), we
compute a four-dim. feature

[(x − x(cid:48))/w, (y − y(cid:48))/h, w(cid:48)/w, h(cid:48)/h] ,

(8)

and concatenate it with combined SPC scores S(pleft , b),
S(pright , b(cid:48)) from Eq. (1). To obtain negative examples, we
randomly sample from other box pairings with IOU < 0.5
with the ground truth regions from that image. We train
an RBF SVM classiﬁer with Platt scaling [32] to obtain a
probability output. This is similar to the method of [15], but
rather than learning a Gaussian Mixture Model using only
positive data, we learn a more discriminative model. Be-
low are details on the three types of relationship classiﬁers.
Complete dictionaries can be found in Appendix C.

Verbs: Starting with our dictionary of 58 verb detectors
and following the above procedure of identifying all rela-
tionships that occur at least 30 times in the training set, we
end up with 260 (pleft , relverb, pright ) SVM classiﬁers.

Prepositions: We ﬁrst gather a list of prepositions that oc-
cur at least 100 times in the training set, combine simi-
lar words, and ﬁlter out words that do not indicate a clear
spatial relationship. This yields eight prepositions (in, on,
under, behind, across, between, onto, and near) and 216
(pleft , relprep, pright ) relationships.

Clothing and body part attachment: We collect
(pleft , relc&bp, pright ) relationships where the left phrase is
always a person and the right phrase is from the clothing or
body part type and learn 207 such classiﬁers. As discussed
in Section 3.1, this relationship type takes precedence over
any verb or preposition relationships that may also hold be-
tween the same phrases.

Method

Accuracy

(a)

(b)

(c)

Single-phrase cues
CCA
CCA+Det
CCA+Det+Size
CCA+Det+Size+Adj
CCA+Det+Size+Adj+Verbs
CCA+Det+Size+Adj+Verbs+Pos (SPC)
Phrase pair cues
SPC+Verbs
SPC+Verbs+Preps
SPC+Verbs+Preps+C&BP (SPC+PPC)
State of the art
SMPL [41]
NonlinearSP [40]
GroundeR [34]
MCB [8]
RtP [33]

43.09
45.29
51.45
52.63
54.51
55.49

55.53
55.62
55.85

42.08
43.89
47.81
48.69
50.89

Table 2: Phrase-region grounding performance on the Flickr30k Entities
dataset. (a) Performance of our single-phrase cues (Sec. 3.2). (b) Further
improvements by adding our pairwise cues (Sec. 3.3). (c) Accuracies of
competing state-of-the-art methods. This comparison excludes concurrent
work that was published after our initial submission [3].
4. Experiments on Flickr30k Entities

4.1. Implementation details

We utilize the provided train/test/val split of 29,873
training, 1,000 validation, and 1,000 testing images [33].
Following [33], our region proposals are given by the top
200 EdgeBox [46] proposals per image. At test time, given
a sentence and an image, we ﬁrst use Eq. (1) to ﬁnd the
top 30 candidate regions for each phrase after performing
non-maximum suppression using a 0.8 IOU threshold. Re-
stricted to these candidates, we optimize Eq. (2) to ﬁnd a
globally consistent mapping of phrases to regions.

Consistent with [33], we only evaluate localization for
phrases with a ground truth bounding box.
If multiple
bounding boxes are associated with a phrase (e.g., four in-
dividual boxes for four men), we represent the phrase as the
union of its boxes. For each image and phrase in the test set,
the predicted box must have at least 0.5 IOU with its ground
truth box to be deemed successfully localized. As only a
single candidate is selected for each phrase, we report the
proportion of correctly localized phrases (i.e. Recall@1).

4.2. Results

Table 2 reports our overall localization accuracy for com-
binations of cues and compares our performance to the state
of the art. Object detectors, reported on the second line of
Table 2(a), show a 2% overall gain over the CCA baseline.
This includes the gain from the detector score as well as the
bounding box regressor trained with the detector in the Fast
R-CNN framework [9]. Adding adjective, verb, and size
cues improves accuracy by a further 9%. Our last cue in Ta-
ble 2(a), position, provides an additional 1% improvement.
We can see from Table 2(b) that the spatial cues give only
a small overall boost in accuracy on the test set, but that

is due to the relatively small number of phrases to which
they apply. In Table 4 we will show that the localization
improvement on the affected phrases is much larger.

Table 2(c) compares our performance to the state of
the art. The method most similar to ours is our earlier
model [33], which we call RtP here. RtP relies on a subset
of our single-phrase cues (region-phrase CCA, size, object
detectors, and color adjectives), and localizes each phrase
separately. The closest version of our current model to
RtP is CCA+Det+Size+Adj, which replaces the 11 colors
of [33] with our more general model for 83 adjectives, and
obtains almost 2% better performance. Our full model is
5% better than RtP. It is also worth noting that a rank-SVM
model [13] for learning cue combination weights gave us
8% worse performance than the direct search scheme of
Section 2.2.

Table 3 breaks down the comparison by phrase type.
Our model has the highest accuracy on most phrase types,
with scenes being the most notable exception, for which
GroundeR [34] does better. However, GroundeR uses Se-
lective Search proposals [39], which have an upper bound
performance that is 7% higher on scene phrases despite us-
ing half as many proposals. Although body parts have the
lowest localization accuracy at 25.24%, this represents an
8% improvement in accuracy over prior methods. However,
only around 62% of body part phrases have a box with high
enough IOU with the ground truth, showing a major area of
weakness of category-independent proposal methods.
In-
deed, if we were to augment our EdgeBox region proposals
with ground truth boxes, we would get an overall improve-
ment in accuracy of about 9% for the full system.

Since many of the cues apply to a small subset of the
phrases, Table 4 details the performance of cues over only
the phrases they affect. As a baseline, we compare against
the combination of cues available for all phrases: region-
phrase CCA, position, and size. To have a consistent set of
regions, the baseline also uses improved boxes from bound-
ing box regressors trained along with the object detectors.
As a result, the object detectors provide less than 2% gain
over the baseline for the phrases on which they are used,
suggesting that the regression provides the majority of the
gain from CCA to CCA+Det in Table 2. This also conﬁrms
that there is signiﬁcant room for improvement in select-
ing candidate regions. By contrast, adjective, subject-verb,
and verb-object detectors show signiﬁcant gains, improving
over the baseline by 6-7%.

The right side of Table 4 shows the improvement on
phrases due to phrase pair cues. Here, we separate the
phrases that occur on the left side of the relationship, which
corresponds to the subject, from the phrases on the right
side. Our results show that the subject, is generally eas-
ier to localize. On the other hand, clothing and body parts
show up mainly on the right side of relationships and they

People

Clothing

Body Parts

Animals

Vehicles

Instruments

Scene

#Test

SMPL [41]
GroundeR [34]
RtP [33]
SPC+PPC (ours)
Upper Bound

5,656

57.89
61.00
64.73
71.69
97.72

2,306

34.61
38.12
46.88
50.95
83.13

523

15.87
10.33
17.21
25.24
61.57

518

55.98
62.55
65.83
76.25
91.89

400

52.25
68.75
68.75
66.50
94.00

162

23.46
36.42
37.65
35.80
82.10

Other

3,374

26.23
29.08
31.77
35.98
81.06

1,619

34.22
58.18
51.39
51.51
84.37

Table 3: Comparison of phrase localization performance over phrase types. Upper Bound refers to the proportion of phrases of each type for which there
exists a region proposal having at least 0.5 IOU with the ground truth.

Method

Baseline
+Cue

#Test
#Train

Single Phrase Cues (SPC)

Phrase-Pair Cues (PPC)

Object
Detectors

Adjectives

Subject-
Verb

Verb-
Object

74.25
75.78

4,059
114,748

57.71
64.35

3,809
110,415

69.68
75.53

3,094
94,353

Verbs

Prepositions

Clothing &
Body Parts

Left

78.32
78.94

867
26,254

Right

51.05
51.33

858
25,898

Left

68.97
69.74

780
23,973

Right

55.01
56.14

778
23,903

Left

81.01
82.86

1,464
42,084

Right

50.72
52.23

1,591
45,496

40.70
47.62

2,398
71,336

Table 4: Breakdown of performance for individual cues restricted only to test phrases to which they apply.
For SPC, Baseline is given by
CCA+Position+Size. For PPC, Baseline is the full SPC model. For all comparisons, we use the improved boxes from bounding box regression on top
of object detector output. PPC evaluation is split by which side of the relationship the phrases occur on. The bottom two rows show the numbers of affected
phrases in the test and training sets. For reference, there are 14.5k visual phrases in the test set and 427k visual phrases in the train set.

tend to be small. It is also less likely that such phrases will
have good candidate boxes – recall from Table 3 that body
parts have a performance upper bound of only 62%. Al-
though they affect relatively few test phrases, all three of our
relationship classiﬁers show consistent gains over the SPC
model. This is encouraging given that many of the relation-
ships that are used on the validation set to learn our model
parameters do not occur in the test set (and vice versa).

Figure 2 provides a qualitative comparison of our output
with the RtP model [33]. In the ﬁrst example, the prediction
for the dog is improved due to the subject-verb classiﬁer for
dog jumping. For the second example, pronominal corefer-
ence resolution (Section 3.1) links each other to two men,
telling us that not only is a man hitting something, but also
that another man is being hit. In the third example, the RtP
model is not able to locate the woman’s blue stripes in her
hair despite having a model for blue. Our adjective detec-
tors take into account stripes as well as blue, allowing us
to correctly localize the phrase, even though we still fail
to localize the hair. Since the blue stripes and hair should
co-locate, a method for obtaining co-referent entities would
further improve performance on such cases. In the last ex-
ample, the RtP model makes the same incorrect prediction
for the two men. However, our spatial relationship between
the ﬁrst man and his gray sweater helps us correctly localize
him. We also improve our prediction for the shopping cart.

5. Visual Relationship Detection

In this section, we adapt our framework to the recently
introduced Visual Relationship Detection (VRD) bench-
mark of Lu et al. [27]. Given a test image without any text
annotations, the task of VRD is to detect all entities and
relationships present and output them in the form (subject,

predicate, object) with the corresponding bounding boxes.
A relationship detection is judged to be correct if it exists
in the image and both the subject and object boxes have
IOU ≥ 0.5 with their respective ground truth. In contrast to
phrase grounding, where we are given a set of entities and
relationships that are assumed to be in the image, here we
do not know a priori which objects or relationships might
be present. On the other hand, the VRD dataset is easier
than Flickr30K Entities in that it has a limited vocabulary
of 100 object classes and 70 predicates annotated in 4000
training and 1000 test images.

Given the small ﬁxed class vocabulary, it would seem
advantageous to train 100 object detectors on this dataset,
as was done by Lu et al. [27]. However, the training set
is relatively small, the class distribution is unbalanced, and
there is no validation set. Thus, we found that training de-
tectors and then relationship models on the same images
causes overﬁtting because the detector scores on the train-
ing images are overconﬁdent. We obtain better results by
training all appearance models using CCA, which also takes
into account semantic similarity between category names
and is trivially extendable to previously unseen categories.
Here, we use fc7 features from a Fast RCNN model trained
on MSCOCO [26] due to the larger range of categories
than PASCAL, and word2vec for object and predicate class
names. We train the following CCA models:

1. CCA(entity box, entity class name): this is the equiv-
alent to region-phrase CCA in Section 3.2 and is used
to score both candidate subject and object boxes.
2. CCA(subject box, [subject class name, predicate class
name]): analogous to subject-verb classiﬁers of Sec-
tion 3.2. The 300-dimensional word2vec features of
subject and predicate class names are concatenated.
3. CCA(object box, [predicate class name, object class

Figure 2: Example results on Flickr30k Entities comparing our SPC+PPC model’s output with the RtP model [33]. See text for discussion.

name]): analogous to verb-object classiﬁers of Section
3.2.

4. CCA(union box, predicate class name):

this model
measures the compatibility between the bounding box
of both subject and object and the predicate name.
5. CCA(union box, [subject class name, predicate class

name, object class name]).

Note that models 4 and 5 had no analogue in our phrase
localization system. On that task, entities were known to be
in the image and relationships simply provided constraints,
while here we need to predict which relationships exist. To
make predictions for predicates and relationships (which is
the goal of models 4 and 5), it helps to see both the subject
and object regions. Union box features were also less useful
for phrase localization due to the larger vocabularies and
relative scarcity of relationships in that task.

Each candidate relationship gets six CCA scores (model
1 above is applied both to the subject and the object). In
addition, we compute size and position scores as in Sec-
tion 3.2 for subject and object, and a score for a pairwise
spatial SVM trained to predict the predicate based on the
four-dimensional feature of Eq.
(8). This yields an 11-
dim. feature vector. By contrast with phrase localization,
our features for VRD are dense (always available for every
relationship).

In Section 2.2 we found feature weights by maximiz-
ing our recall metric. Here we have a more conventional
detection task, so we obtain better performance by train-
ing a linear rank-SVM model [13] to enforce that correctly
detected relationships are ranked higher than negative de-
tections (where either box has < 0.5 IOU with the ground
truth). We use the test set object detections (just the boxes,
not the scores) provided by [27] to directly compare per-
formance with the same candidate regions. During testing,

we produce a score for every ordered pair of detected boxes
and all possible predicates, and retain the top 10 predicted
relationships per pair of (subject, object) boxes.

Consistent with [27], Table 5 reports recall, R@{100,
50}, or the portion of correctly localized relationships in the
top 100 (resp. 50) ranked relationships in the image. The
right side shows performance for relationships that have not
been encountered in the training set. Our method clearly
outperforms that of Lu et al. [27], which uses separate vi-
sual, language, and relationship likelihood cues. We also
outperform Zhang et al. [45], which combines object detec-
tors, visual appearance, and object position in a single neu-
ral network. We observe that cues based on object class and
relative subject-object position provide a noticeable boost
in performance. Further, due to using CCA with multi-
modal embeddings, we generalize better to unseen relation-
ships. Qualitative examples and associated discussion can
be found in Appendix A.

6. Conclusion

This paper introduced a framework incorporating a com-
prehensive collection of image- and language-based cues
for visual grounding and demonstrated signiﬁcant gains
over the state of the art on two tasks: phrase localization on
Flickr30k Entities and relationship detection on the VRD
dataset. For the latter task, we got particularly pronounced
gains for the zero-shot learning scenario. In future work, we
would like to train a single network for combining multiple
cues. Doing this in a uniﬁed end-to-end fashion is chal-
lenging, since one needs to ﬁnd the right balance between
parameter sharing and specialization or ﬁne-tuning required
by individual cues. To this end, our work provides a strong
baseline and can help to inform future approaches.

Method

Phrase Det.

Rel. Det.

R@100

R@50

R@100

R@50

Zero-shot Phrase Det.
R@100

R@50

Zero-shot Rel. Det.
R@50
R@100

(a)

(b)

Visual Only Model [27]
Visual + Language +
Likelihood Model [27]
VTransE [45]
CCA
CCA + Size
CCA + Size + Position

2.61

17.03

22.42
15.36
15.85
20.70

2.24

16.17

19.42
11.38
11.72
16.89

1.85

14.70

15.20
13.69
14.05
18.37

1.58

13.86

14.07
10.08
10.36
15.08

1.12

3.75

3.51
12.40
12.92
15.23

0.95

3.36

2.65
7.78
8.04
10.86

0.78

3.52

2.14
11.12
11.46
13.43

0.67

3.13

1.71
6.59
6.76
9.67

Table 5: Relationship detection recall at different thresholds (R@{100,50}). CCA refers to the combination of six CCA models (see text). Position refers
to the combination of individual box position and pairwise spatial classiﬁers. This comparison excludes concurrent work that was published after our initial
submission [24, 25].

Acknowledgments. This work was partially supported by
NSF grants 1053856, 1205627, 1405883, 1302438, and
1563727, Xerox UAC, the Sloan Foundation, and a Google
Research Award.

[12] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Dar-
rell. Natural language object retrieval. In CVPR, 2016. 2,
4

[13] T. Joachims. Training linear svms in linear time. In SIGKDD,

References

[1] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
Zitnick, and D. Parikh. Vqa: Visual question answering. In
ICCV, 2015. 1

[2] C.-C. Chang and C.-J. Lin.

LIBSVM: A library for
support vector machines. ACM Transactions on Intelli-
gent Systems and Technology, 2:27:1–27:27, 2011. Soft-
ware available at http://www.csie.ntu.edu.tw/
˜cjlin/libsvm. 4

[3] K. Chen, R. Kovvuri, J. Gao, and R. Nevatia. MSRC: Mul-
timodal spatial regression with semantic context for phrase
grounding. In ICMR, 2017. 6

[4] S. K. Divvala, D. Hoiem, J. H. Hays, A. A. Efros, and
M. Heber. An empirical study of context in object detection.
In CVPR, 2009. 4

[5] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The PASCAL Visual Object Classes
Challenge 2012 (VOC2012) Results.
http://www.pascal-
network.org/challenges/VOC/voc2012/workshop/index.html,
2012. 4, 5

[6] H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dol-
lar, J. Gao, X. He, M. Mitchell, J. Platt, L. Zitnick, and
In
G. Zweig. From captions to visual concepts and back.
CVPR, 2015. 1

[7] S. Fidler, A. Sharma, and R. Urtasun. A sentence is worth a

thousand pixels. In CVPR, 2013. 2, 4, 5

[8] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and
M. Rohrbach. Multimodal compact bilinear pooling for vi-
sual question answering and visual grounding. In EMNLP,
2016. 1, 2, 6

[9] R. Girshick. Fast r-cnn. In ICCV, 2015. 4, 5, 6
[10] Y. Gong, Q. Ke, M. Isard, and S. Lazebnik. A multi-view em-
bedding space for modeling internet images, tags, and their
semantics. IJCV, 106(2):210–233, 2014. 4

[11] S. Harabagiu and S. Maiorano. Knowledge-lean corefer-
ence resolution and its relation to textual cohesion and co-
herence. In Proceedings of the ACL-99 Workshop on the re-
lation of discourse/dialogue structure and reference, pages
29–38, 1999. 4

2006. 3, 6, 8

[14] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully
convolutional localization networks for dense captioning. In
CVPR, 2016. 1

[15] J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. A. Shamma,
Image retrieval using scene

M. Bernstein, and L. Fei-Fei.
graphs. In CVPR, 2015. 2, 5

[16] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
In CVPR, 2015.

ments for generating image descriptions.
1

[17] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment em-
beddings for bidirectional image sentence mapping. In NIPS,
2014. 1

[18] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg.
Referitgame: Referring to objects in photographs of natural
scenes. In EMNLP, 2014. 1, 2, 4, 5

[19] B. Klein, G. Lev, G. Sadeh, and L. Wolf. Associating neu-
ral word embeddings with deep image representations using
ﬁsher vector. In CVPR, 2015. 4

[20] C. Kong, D. Lin, M. Bansal, R. Urtasun, and S. Fidler. What
are you talking about? text-to-image coreference. In CVPR,
2014. 2

[21] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bern-
stein, and L. Fei-Fei. Visual genome: Connecting language
and vision using crowdsourced dense image annotations.
IJCV, 2017. 1

[22] J. C. Lagarias, J. A. Reeds, M. H. Wright, and P. E.
Wright. Convergence properties of the nelder-mead simplex
method in low dimensions. SIAM Journal of Optimization,
9(1):112147, 1998. 3

[23] L.-J. Li, H. Su, Y. Lim, and L. Fei-Fei. Object bank: An
object-level image representation for high-level visual recog-
nition. IJCV, 107(1):20–39, 2014. 4

[24] Y. Li, W. Ouyang, X. Wang, and X. Tang. ViP-CNN: Visual
phrase guided convolutional neural network. In CVPR, 2017.
9

[25] X. Liang, L. Lee, and E. P. Xing. Deep variation-structured
reinforcement learning for visual relationship and attribute
detection. In CVPR, 2017. 9

[43] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From im-
age descriptions to visual denotations: New similarity met-
rics for semantic inference over event descriptions. TACL,
2:67–78, 2014. 1

[44] L. Yu, E. Park, A. C. Berg, and T. L. Berg. Visual Madlibs:
Fill in the blank Image Generation and Question Answering.
In ICCV, 2015. 1

[45] H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua. Visual
translation embedding network for visual relation detection.
In CVPR, 2017. 8, 9

[46] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object

proposals from edges. In ECCV, 2014. 4, 6

[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Com-
mon objects in context. In ECCV, 2014. 7

[27] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual rela-
tionship detection with language priors. In ECCV, 2016. 1,
2, 7, 8, 9

[28] L. Ma, Z. Lu, L. Shang, and H. Li. Multimodal convolutional
neural networks for matching image and sentence. In ICCV,
2015. 1

[29] J. Mao, J. Huang, A. Toshev, O. Camburu, A. Yuille, and
K. Murphy. Generation and comprehension of unambiguous
object descriptions. In CVPR, 2016. 1, 2

[30] T. Mikolov, K. Chen, G. Corrado, and J. Dean.

Efﬁ-
cient estimation of word representations in vector space.
arXiv:1301.3781, 2013. 4

[31] R. Mitkov. Robust pronoun resolution with limited knowl-
edge. In Proceedings of the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics and 17th International
Conference on Computational Linguistics-Volume 2, pages
869–875. Association for Computational Linguistics, 1998.
4

[32] J. C. Platt. Probabilistic outputs for support vector machines
and comparisons to regularized likelihood methods. In Ad-
vances in Large Margin Classiﬁers, pages 61–74. MIT Press,
1999. 4, 5

[33] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
to-sentence models. IJCV, 123(1):74–93, 2017. 1, 2, 3, 4, 5,
6, 7, 8

[34] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and
B. Schiele. Grounding of textual phrases in images by re-
construction. In ECCV, 2016. 2, 6, 7

[35] F. Sadeghi, S. K. Divvala, and A. Farhadi. Viske: Visual
knowledge extraction and question answering by visual ver-
iﬁcation of relation phrases. In CVPR, 2015. 5

[36] M. A. Sadeghi and A. Farhadi. Recognition using visual

phrases. In CVPR, 2011. 5

[37] R. Socher, J. Bauer, C. D. Manning, and A. Y. Ng. Parsing
With Compositional Vector Grammars. In ACL, 2013. 4
[38] J. Tighe, M. Niethammer, and S. Lazebnik. Scene pars-
ing with object instances and occlusion ordering. In CVPR,
2014. 3

[39] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.
Selective search for object recognition. IJCV, 104(2), 2013.
6

[40] L. Wang, Y. Li, and S. Lazebnik. Learning deep structure-
In CVPR, 2016. 1, 2,

preserving image-text embeddings.
6

[41] M. Wang, M. Azab, N. Kojima, R. Mihalcea, and J. Deng.
Structured matching for phrase localization. In ECCV, 2016.
2, 6, 7

[42] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov,
R. Zemel, and Y. Bengio. Show, attend and tell: Neural im-
age caption generation with visual attention. In ICML, 2015.
1

A. Visualization of detected relationships (VRD Dataset)

Below are some example detections on the VRD test set. Figure 3 shows some of the highly conﬁdent and correctly
localized detections. We detect different types of relationships - spatial (post, behind, car), (sky, above, laptop), (laptop, on,
table), clothing (person, wear, hat), (person, has, shorts), and actions (person, ride, skateboard).

Figure 3: Highly conﬁdent and correctly localized relationships on the VRD dataset.

Figure 4 shows detections which were marked as negatives by the evaluation code as these relationships were not annotated
in the corresponding images. However, note that these predictions are logically correct. The mouse is indeed next to the laptop
(leftmost, ﬁrst row), and the laptop is under the sky (middle, ﬁrst row). Further, in the leftmost, second row image of Figure 3,
the relationship (person, has, shorts) was marked as present, whereas the middle, second row image in Figure 4 has (person,
has, hat) marked as absent, which indicates a lapse in annotation.

Figure 5 shows examples of wrongly detected relationships. Some of these relationships are logically implausible such as
(hat, hold, surfboard) (leftmost, ﬁrst row), while others such as (jeans, on, table) (middle, ﬁrst row), while plausible, aren’t
contextually true in the image. Other failure modes include incorrect detections such as the sky in the (rightmost, ﬁrst row)
image and the phone in the (leftmost, second row) image.

Figure 4: Plausible and logically correct detected relationships, penalized as negatives due to lack of annotations in the VRD dataset.

Figure 5: Falsely detected relationships on the VRD dataset. Mistakes are either due to incorrect localization of objects, prediction of implausible relation-
ships, contextually incorrect relationships, or a combination of mistakes.

B. List of detector classes from Flickr30k Entities

B.1. Adjectives

people-white 3)

2)
9) wet

1) white
8)
grassy
15) professional 16) brown
22) people-blue
23) male
29) bald
30) cold
36) green
37) young
43) paved
44)
teenage
50) military
51) purple
57) dark
58) beautiful
64) chinese
65)
little
72) wooden
71) colorful
78) multicolored 79) bearded

indian

4)
11)

empty
red

5)
new
12) people-red
19) african
26) blond

female
10) colored
17) people-blond 18) snowy
25) oriental
24)
32) people-yellow 33) shirtless
31) blue
39) dark-haired
38) dirt
46) pink
45) cloudy
53) hard
52) asian
60) adult
59) sandy
67) old
tan
66)
74) plastic
full
73)
81) short
80) huge

40) dark-skinned 41) orange
47) older
rocky
54)
light
61) golden
68) concrete
tall
75)
82) high

48)
55) hooded
62) elderly
69) outdoors
76) striped
top
83)

people-black

6)
black
13) sunny
20)
indoor
27) people-green 28) crowded
34) american

7)
14) smiling
21) gray

35) hot
42) younger
49) urban
56) yellow
63) bright
70)
77) middle-aged

long

B.2. Subject-Verb

2)
animals-catching
1)
7)
animals-holding
6)
animals-sleeping
12)
11)
bodyparts-holding 17)
16)
22)
clothing-eating
21)
27)
clothing-posing
26)
32)
clothing-sitting
31)
37)
clothing-walking
36)
42)
other-eating
41)
47)
other-playing
46)
52)
other-running
51)
57)
other-standing
56)
62)
other-writing
61)
67)
people-cooking
66)
72)
people-drinking
71)
77)
people-ﬁshing
76)
82)
people-hugging
81)
87)
people-kneeling
86)
92)
people-pointing
91)
97)
96)
people-riding
102) people-skiing
101) people-sitting
106) people-smoking
107) people-splashing
111) people-swimming 112) people-swinging
116) people-walking
121) scene-holding
126) scene-standing
131) vehicles-running
136) playing
141)
jumping
146) posing
151) smiling
156)
throwing
161) cleaning
166) hit
171) pointing
176) writing
181) digging
186) sweeping
191)

4)
animals-digging
animals-climbing 3)
9)
animals-playing
animals-jumping
8)
14)
animals-standing
animals-splashing 13)
19)
bodyparts-walking
18)
bodyparts-sitting
24)
clothing-jumping
23)
clothing-holding
29)
clothing-riding
28)
clothing-reading
clothing-smiling
34)
33)
clothing-sleeping
instruments-singing 39)
38)
clothing-working
44)
other-holding
43)
other-ﬂying
49)
other-posing
48)
other-pointing
54)
other-sitting
53)
other-singing
59)
other-throwing
58)
other-talking
64)
people-catching
63)
people-blowing
69)
people-dancing
68)
people-cutting
74)
people-eating
73)
people-driving
79)
people-hiking
78)
people-ﬂying
84)
people-jumping
83)
people-juggling
89)
people-painting
88)
people-laughing
94)
people-pushing
93)
people-posing
99)
98)
people-serving
people-running
104) people-sliding
103) people-sleeping
109) people-surﬁng
108) people-standing
114) people-throwing
113) people-talking
119) people-writing
118) people-working
124) scene-running
123) scene-reading
129) vehicles-driving
128) scene-walking
134) sitting
133) vehicles-throwing
139)
138) walking
144) performing
143)
talking
149)
148) hiking
reading
154) pushing
153) sleeping
159) cooking
158) driving
164)
163) swinging
laughing
169) ﬂying
168)
juggling
174) drinking
173) sliding
179) kneeling
178) catching
184) surﬁng
183) shopping
189) drawing
reaching
188)

animals-ﬂying
5)
animals-ﬁghting
animals-sitting
10)
animals-running
animals-walking
15)
animals-swimming
clothing-dancing
clothing-climbing
20)
clothing-playing
clothing-performing 25)
clothing-singing
30)
clothing-running
clothing-talking
35)
clothing-standing
other-drinking
40)
other-cooking
other-performing
45)
other-jumping
other-riding
50)
other-reading
other-smiling
55)
other-sleeping
other-working
60)
other-walking
people-climbing
65)
people-cleaning
people-drawing
70)
people-digging
people-ﬁghting
75)
people-falling
people-holding
80)
people-hit
people-kissing
85)
people-kicking
people-playing
90)
people-performing
95)
people-reaching
people-reading
100) people-singing
people-shopping
105) people-smiling
110) people-sweeping
115) people-touches
120) scene-eating
125) scene-sitting
130) vehicles-holding
135) holding
140)
riding
145) eating
150) dancing
155) swimming
160) cutting
165) kicking
170) kissing
175) ﬁshing
180) hugging
185) waving
190) splashing

117) people-waving
122) scene-playing
127) scene-talking
132) vehicles-sitting
137) standing
142) working
147) climbing
152) singing
157) painting
162) serving
167) ﬁghting
172) blowing
177) skiing
182) smoking
187)

running

touches

falling

B.3. Verb-Object

1)
5)
9)
13)
17)
21)
25)
29)
33)
37)
41)
45)
49)
53)
57)
61)
65)
69)
73)
77)
81)
85)
89)
93)
97)
101)
105)
109)
113)
117)
121)
125)
129)
133)
137)
141)
145)
149)
153)
157)
161)
165)
169)
173)
177)
181)
185)
189)
193)
197)
201)
205)
209)
213)
217)
221)
225)

other-blowing
scene-cleaning
bodyparts-cooking
clothing-dancing
other-digging
scene-drinking
other-eating
scene-falling
scene-ﬂying
animals-holding
other-holding
people-hugging
other-jumping
other-kicking
other-kneeling
other-painting
people-performing
instruments-playing
vehicles-playing
scene-pointing
people-posing
vehicles-pushing
people-reading
scene-riding
clothing-running
vehicles-running
instruments-singing
bodyparts-sitting
people-sitting
bodyparts-sleeping
other-sliding
other-smiling
scene-splashing
other-standing
scene-surﬁng
scene-swimming
people-talking
scene-throwing
bodyparts-walking
scene-walking
people-waving
scene-working
holding
running
talking
climbing
smiling
swimming
cooking
swinging
ﬁghting
pointing
ﬁshing
kneeling
shopping
falling
touches

other-catching
2)
bodyparts-climbing
6)
other-cooking
10)
other-dancing
14)
scene-digging
18)
other-driving
22)
people-eating
26)
other-ﬁghting
30)
scene-hiking
34)
bodyparts-holding
38)
people-holding
42)
other-juggling
46)
people-jumping
50)
people-kicking
54)
scene-kneeling
58)
scene-painting
62)
scene-performing
66)
other-playing
70)
bodyparts-pointing
74)
bodyparts-posing
78)
scene-posing
82)
other-reaching
86)
animals-riding
90)
vehicles-riding
94)
other-running
98)
other-serving
102)
other-singing
106)
clothing-sitting
110)
scene-sitting
114)
other-sleeping
118)
scene-sliding
122)
people-smiling
126)
animals-standing
130)
people-standing
134)
other-sweeping
138)
other-swinging
142)
scene-talking
146)
bodyparts-touches
150)
clothing-walking
154)
vehicles-walking
158)
clothing-working
162)
vehicles-working
166)
playing
170)
riding
174)
performing
178)
hiking
182)
singing
186)
throwing
190)
cutting
194)
laughing
198)
juggling
202)
206)
blowing
210) writing
hugging
214)
surﬁng
218)
reaching
222)

scene-catching
3)
other-climbing
7)
bodyparts-cutting
11)
people-dancing
15)
other-drawing
19)
scene-driving
23)
scene-eating
27)
scene-ﬁshing
31)
other-hit
35)
clothing-holding
39)
scene-holding
43)
animals-jumping
47)
scene-jumping
51)
people-kissing
55)
other-laughing
59)
instruments-performing
63)
animals-playing
67)
people-playing
71)
other-pointing
75)
clothing-posing
79)
other-pushing
83)
scene-reaching
87)
other-riding
91)
animals-running
95)
people-running
99)
people-serving
103)
people-singing
107)
instruments-sitting
111)
vehicles-sitting
115)
people-sleeping
119)
bodyparts-smiling
123)
scene-smiling
127)
bodyparts-standing
131)
scene-standing
135)
scene-sweeping
139)
clothing-talking
143)
other-throwing
147)
other-touches
151)
other-walking
155)
bodyparts-waving
159)
other-working
163)
other-writing
167)
standing
171)
jumping
175)
eating
179)
reading
183)
sleeping
187)
painting
191)
cleaning
195)
kicking
199)
ﬂying
203)
sliding
207)
skiing
211)
215)
digging
219) waving
drawing
223)

other-cleaning
scene-climbing
other-cutting
scene-dancing
other-drinking
vehicles-driving
other-falling
other-ﬂying
people-hit
instruments-holding
vehicles-holding
bodyparts-jumping
vehicles-jumping
scene-kissing
people-laughing
other-performing
clothing-playing
scene-playing
people-pointing
other-posing
people-pushing
other-reading
people-riding
bodyparts-running
scene-running
other-shopping
animals-sitting
other-sitting
scene-skiing
scene-sleeping
clothing-smiling
other-smoking
clothing-standing
vehicles-standing
other-swimming
other-talking
people-throwing
animals-walking
people-walking
other-waving
people-working
sitting

4)
8)
12)
16)
20)
24)
28)
32)
36)
40)
44)
48)
52)
56)
60)
64)
68)
72)
76)
80)
84)
88)
92)
96)
100)
104)
108)
112)
116)
120)
124)
128)
132)
136)
140)
144)
148)
152)
156)
160)
164)
168)
172) walking
176) working
180)
184)
188)
192)
196)
200)
204)
208)
212)
216)
220)
224)

posing
dancing
pushing
driving
serving
hit
kissing
drinking
catching
smoking
sweeping
splashing

C. List of phrase-pair relationships from Flickr30k Entities
C.1. Verbs

1)
5)
9)
13)
17)
21)
25)
29)
33)
37)
41)
45)
49)
53)
57)
61)
65)
69)
73)
77)
81)
85)
89)
93)
97)
101) people-playing-keyboard
105) people-playing-sand
109) people-playing-stage
113) people-playing-trumpet
117) people-posing-people
121) people-pushing-stroller
125) people-reading-paper
129) people-riding-bikes
133) people-riding-horses
137) people-riding-scooter
141) people-riding-unicycle
145) people-running-ﬁeld
149) people-running-sidewalk
153) people-serving-food
157) people-sitting-beach
161) people-sitting-bike
165) people-sitting-chair
169) people-sitting-desk
173) people-sitting-horse
177) people-sitting-people
181) people-sitting-steps
185) people-sitting-table
189) people-sitting-water
193) people-sleeping-grass
197) people-smiling-people
201) people-standing-bridge
205) people-standing-door
209) people-standing-grass
213) people-standing-platform
217) people-standing-rocks
221) people-standing-stage
225) people-standing-wall
229) people-swinging-bat
233) people-talking-people
237) people-throwing-people
241) people-walking-bridge
245) people-walking-dogs
249) people-walking-path
253) people-walking-snow
257) people-walking-wall

dog-holding-stick
2)
dog-catching-frisbee
3)
dog-jumping-people
6)
dog-jumping-hurdle
7)
dog-running-ﬁeld
10)
dog-running-beach
11)
dog-swimming-water
14)
dog-running-water
15)
dogs-running-grass
18)
dogs-running-ﬁeld
19)
people-cleaning-dishes
22)
people-catching-wave
23)
people-climbing-rocks
26)
people-climbing-rock+wall
27)
people-cutting-cake
30)
people-cooking-food
31)
people-drinking-beer
34)
people-digging-snow
35)
people-hit-ball
38)
people-eating-table
39)
people-holding-box
42)
people-holding-book
43)
people-holding-drink
46)
people-holding-dog
47)
people-holding-football
50)
people-holding-ﬂowers
51)
people-holding-people
54)
people-holding-object
55)
people-holding-signs
58)
people-holding-sign
59)
people-hugging-people
people-holding-tennis+racket 62)
63)
people-jumping-hurdle
66)
people-jumping-bike
67)
people-jumping-rock
70)
people-jumping-ramp
71)
people-kicking-ball
74)
people-jumping-water
75)
people-laughing-people
78)
people-kissing-people
79)
people-playing-accordion
82)
people-performing-stage
83)
people-playing-beach
86)
people-playing-basketball
87)
people-playing-drum
90)
people-playing-dog
91)
people-playing-fountain
94)
people-playing-football
95)
98)
people-playing-guitars
people-playing-guitar
99)
103) people-playing-piano
102) people-playing-people
106) people-playing-saxophone 107) people-playing-snow
110) people-playing-swing
114) people-playing-violin
118) people-posing-picture
122) people-reading-book
126) people-riding-bicycle
130) people-riding-bull
134) people-riding-motorbike
138) people-riding-skateboard
142) people-riding-wave
146) people-running-grass
150) people-running-street
154) people-singing-guitar
158) people-sitting-bed
162) people-sitting-blanket
166) people-sitting-chairs
170) people-sitting-dock
174) people-sitting-ledge
178) people-sitting-rock
182) people-sitting-stool
186) people-sitting-tables
190) people-sleeping-bench
194) people-sleeping-people
198) people-smoking-cigarette
202) people-standing-building
206) people-standing-doorway
210) people-standing-ladder
214) people-standing-podium
218) people-standing-sidewalk
222) people-standing-street
226) people-standing-water
230) people-swinging-swing
234) people-talking-phone
238) people-walking-beach
242) people-walking-building
246) people-walking-ﬁeld
250) people-walking-people
254) people-walking-stairs
258) people-walking-water

dog-jumping-frisbee
4)
dog-jumping-ball
dog-playing-ball
8)
dog-jumping-water
dog-running-snow
12)
dog-running-grass
dogs-playing-snow
16)
dogs-playing-grass
people-catching-ball
20)
people-blowing-bubbles
people-climbing-rock
24)
people-climbing-mountain
people-climbing-wall
28)
people-climbing-tree
people-dancing-stage
32)
people-dancing-people
people-eating-meal
36)
people-eating-food
people-holding-ball
40)
people-hit-tennis+ball
people-holding-cup
44)
people-holding-camera
people-holding-ﬂags
48)
people-holding-ﬂag
people-holding-microphone
52)
people-holding-guitar
people-holding-shovel
56)
people-holding-rope
people-holding-stick
60)
people-holding-something
people-jumping-bed
64)
people-jumping-ball
people-jumping-pool
people-jumping-people
68)
people-jumping-trampoline
people-jumping-swimming+pool 72)
people-kicking-soccer+ball
76)
people-kicking-people
people-performing-people
80)
people-painting-picture
people-playing-ball
84)
people-playing-bagpipes
people-playing-cello
88)
people-playing-board+game
people-playing-ﬂute
92)
people-playing-drums
96)
people-playing-frisbee
people-playing-game
100) people-playing-instruments
people-playing-instrument
104) people-playing-pool
108) people-playing-soccer
112) people-playing-toys
116) people-playing-water
120) people-pushing-people
124) people-reading-newspaper
128) people-riding-bike
132) people-riding-horse
136) people-riding-people
140) people-riding-surfboard
144) people-running-beach
148) people-running-road
152) people-running-water
156) people-singing-people
160) people-sitting-benches
164) people-sitting-building
168) people-sitting-curb
172) people-sitting-grass
176) people-sitting-park+bench
180) people-sitting-sidewalk
184) people-sitting-swing
188) people-sitting-wall
192) people-sleeping-couch
196) people-sliding-slide
200) people-standing-boat
204) people-standing-counter
208) people-standing-ﬁeld
212) people-standing-people
216) people-standing-rock
220) people-standing-snow
224) people-standing-tree
228) people-swimming-pool
232) people-talking-microphone
236) people-throwing-frisbee
240) people-walking-bike
244) people-walking-dog
248) people-walking-hill
252) people-walking-sidewalk
256) people-walking-trail
260) people-working-people

111) people-playing-toy
115) people-playing-volleyball
119) people-pushing-cart
123) people-reading-magazine
127) people-riding-bicycles
131) people-riding-dirt+bike
135) people-riding-motorcycle
139) people-riding-street
143) people-running-ball
147) people-running-people
151) people-running-track
155) people-singing-microphone
159) people-sitting-bench
163) people-sitting-boat
167) people-sitting-couch
171) people-sitting-ﬂoor
175) people-sitting-motorcycle
179) people-sitting-rocks
183) people-sitting-street
187) people-sitting-tree
191) people-sleeping-chair
195) people-sliding-base
199) people-standing-beach
203) people-standing-car
207) people-standing-fence
211) people-standing-line
215) people-standing-road
219) people-standing-sign
223) people-standing-table
227) people-surﬁng-wave
231) people-talking-cellphone
235) people-throwing-ball
239) people-walking-bicycle
243) people-walking-city+street
247) people-walking-grass
251) people-walking-road
255) people-walking-street
259) people-working-machine

C.2. Prepositions

1)
5)
9)
13)
17)
21)
25)
29)
33)
37)
41)
45)
49)
53)
57)
61)
65)
69)
73)
77)
81)
85)
89)
93)
97)
101)
105)
109)
113)
117)
121)
125)
129)
133)
137)
141)
145)
149)
153)
157)
161)
165)
169)
173)
177)
181)
185)
189)
193)
197)
201)
205)
209)
213)

ball-in-mouth
dog-in-ball
dog-in-grass
dog-in-water
dog-on-leash
dogs-in-snow
hands-in-people
people-across-street
people-behind-people
people-in-ball
people-in-blanket
people-in-camera
people-in-cart
people-in-colors
people-in-doorway
people-in-football
people-in-guitar
people-in-kitchen
people-in-mirror
people-in-park
people-in-room
people-in-street
people-in-towel
people-in-tub
people-near-beach
people-near-fence
people-near-pole
people-near-table
people-near-window
people-on-bed
people-on-bicycles
people-on-board
people-on-bus
people-on-city+street
people-on-curb
people-on-ﬁeld
people-on-hill
people-on-ladder
people-on-mat
people-on-park+bench
people-on-phone
people-on-raft
people-on-road
people-on-rope
people-on-scooter
people-on-skateboard
people-on-soccer+ﬁeld
people-on-step
people-on-street
people-on-tire+swing
people-on-trampoline
people-on-water
something-in-mouth
tattoo-on-people

2)
6)
10)
14)
18)
22)
26)
30)
34)
38)
42)
46)
50)
54)
58)
62)
66)
70)
74)
78)
82)
86)
90)
94)
98)
102)
106)
110)
114)
118)
122)
126)
130)
134)
138)
142)
146)
150)
154)
158)
162)
166)
170)
174)
178)
182)
186)
190)
194)
198)
202)
206)
210)
214)

bicycle-on-street
dog-in-collar
dog-in-snow
dog-on-beach
dogs-in-dogs
dogs-in-water
object-in-mouth
people-behind-building
people-between-people
people-in-bed
people-in-boat
people-in-cane
people-in-chair
people-in-dirt
people-in-face+paint
people-in-fountain
people-in-highchair
people-in-lake
people-in-mud
people-in-people
people-in-sand
people-in-stroller
people-in-toy
people-in-water
people-near-brick+wall
people-near-fountain
people-near-road
people-near-tree
people-on-back
people-on-bench
people-on-bike
people-on-boat
people-on-cellphone
people-on-cliff
people-on-deck
people-on-ﬂoor
people-on-horse
people-on-lawn
people-on-motorcycle
people-on-path
people-on-pier
people-on-rail
people-on-rock
people-on-sand
people-on-shore
people-on-sled
people-on-sofa
people-on-steps
people-on-surfboard
people-on-track
people-on-tree
people-on-wave
stick-in-mouth
tennis+ball-in-mouth

3)
7)
11)
15)
19)
23)
27)
31)
35)
39)
43)
47)
51)
55)
59)
63)
67)
71)
75)
79)
83)
87)
91)
95)
99)
103)
107)
111)
115)
119)
123)
127)
131)
135)
139)
143)
147)
151)
155)
159)
163)
167)
171)
175)
179)
183)
187)
191)
195)
199)
203)
207)
211)
215)

boat-in-water
dog-in-dog
dog-in-stick
dog-on-grass
dogs-in-ﬁeld
dogs-on-grass
one-in-shirt
people-behind-counter
people-in-area
people-in-bicycle
people-in-body+water
people-in-canoe
people-in-chairs
people-in-dog
people-in-ﬁeld
people-in-gear
people-in-instruments
people-in-line
people-in-number
people-in-pool
people-in-snow
people-in-swimming+pool
people-in-toys
people-in-wheelchair
people-near-building
people-near-lake
people-near-sidewalk
people-near-wall
people-on-balcony
people-on-benches
people-on-bikes
people-on-bridge
people-on-chair
people-on-computer
people-on-dock
people-on-grass
people-on-horses
people-on-ledge
people-on-motorcycles
people-on-pavement
people-on-platform
people-on-railing
people-on-rocks
people-on-scaffold
people-on-side+road
people-on-slide
people-on-stage
people-on-stilts
people-on-swing
people-on-trail
people-on-walkway
people-under-tree
street-in-people
toy-in-mouth

building-in-people
4)
dog-in-ﬁeld
8)
dog-in-toy
12)
dog-on-hind+legs
16)
dogs-in-grass
20)
guitar-in-people
24)
other-in-shirt
28)
people-behind-fence
32)
people-in-back
36)
people-in-bike
40)
people-in-building
44)
people-in-car
48)
people-in-cigarette
52)
people-in-dogs
56)
people-in-ﬂowers
60)
people-in-grass
64)
people-in-kayak
68)
people-in-microphone
72)
people-in-ocean
76)
people-in-river
80)
people-in-soccer+ball
84)
people-in-swing
88)
people-in-tree
92)
people-in-yard
96)
people-near-car
100)
people-near-people
104)
people-near-street
108)
people-near-water
112)
people-on-beach
116)
people-on-bicycle
120)
people-on-blanket
124)
people-on-building
128)
people-on-chairs
132)
people-on-couch
136)
people-on-fence
140)
people-on-grill
144)
people-on-ice
148)
people-on-machine
152)
people-on-mountain
156)
people-on-people
160)
people-on-porch
164)
people-on-ramp
168)
people-on-roof
172)
people-on-scaffolding
176)
people-on-sidewalk
180)
people-on-snowboard
184)
people-on-stairs
188)
people-on-stool
192)
people-on-table
196)
people-on-train
200)
people-on-wall
204)
shirt-in-people
208)
212)
table-in-people
216) wall-in-grafﬁti

C.3. Clothing and Body Part Attachment

1)
5)
9)
13)
17)
21)
25)
29)
33)
37)
41)
45)
49)
53)
57)
61)
65)
69)
73)
77)
81)
85)
89)
93)
97)
101)
105)
109)
113)
117)
121)
125)
129)
133)
137)
141)
145)
149)
153)
157)
161)
165)
169)
173)
177)
181)
185)
189)
193)
197)
201)
205)

people-apron
people-backpack
people-ball+cap
people-bathing+suit
people-beret
people-black+shirt
people-blue
people-brown+jacket
people-camouﬂage
people-clothing
people-costume
people-curly+hair
people-dress+shirt
people-faces
people-ﬂip-ﬂops
people-goggles
people-hair
people-harness
people-headband
people-heels
people-jacket
people-jersey
people-kilt
people-leather+jacket
people-life+jacket
people-mohawk
people-nose
people-orange+jacket
people-outﬁt
people-pants
people-pink+coat
people-pink+outﬁt
people-plaid+shirt
people-purple+shirt
people-red-hair
people-rock+face
people-scarf
people-shoe
people-shorts
people-skirts
people-snowshoes
people-striped+shirt
people-suspenders
people-swimming+trunks
people-t-shirts
people-tank
people-teeth
people-top
people-tuxedo
people-uniform
people-wedding+dress
people-winter+clothes

2)
6)
10)
14)
18)
22)
26)
30)
34)
38)
42)
46)
50)
54)
58)
62)
66)
70)
74)
78)
82)
86)
90)
94)
98)
102)
106)
110)
114)
118)
122)
126)
130)
134)
138)
142)
146)
150)
154)
158)
162)
166)
170)
174)
178)
182)
186)
190)
194)
198)
202)
206)

people-aprons
people-backpacks
people-bandanna
people-bathing+suits
people-bikini
people-black+white
people-body
people-brown+shirt
people-cap
people-coat
people-costumes
people-denim+jacket
people-dresses
people-feet
people-garb
people-gold
people-haircut
people-hat
people-headphones
people-helmet
people-jackets
people-jerseys
people-knees
people-leg
people-life+jackets
people-mouth
people-orange
people-orange+shirt
people-outﬁts
people-people
people-pink+dress
people-pink+pants
people-polo+shirt
people-purse
people-ring
people-safety+vest
people-scrubs
people-shoes
people-shoulder
people-sleeveless+shirt
people-snowsuit
people-suit
people-sweater
people-swimsuit
people-tan+jacket
people-tank+top
people-thumbs
people-tops
people-umbrella
people-uniforms
people-wetsuit
people-winter+clothing

3)
7)
11)
15)
19)
23)
27)
31)
35)
39)
43)
47)
51)
55)
59)
63)
67)
71)
75)
79)
83)
87)
91)
95)
99)
103)
107)
111)
115)
119)
123)
127)
131)
135)
139)
143)
147)
151)
155)
159)
163)
167)
171)
175)
179)
183)
187)
191)
195)
199)
203)
207)

people-arms
people-bag
people-baseball+cap
people-beanie
people-bikinis
people-blond-hair
people-boots
people-business+attire
people-checkered+shirt
people-coats
people-cowboy+hat
people-dreadlocks
people-eyes
people-ﬁnger
people-glasses
people-gray
people-hand
people-hats
people-heads
people-helmets
people-jean+shorts
people-jumpsuit
people-lab+coat
people-legs
people-makeup
people-mustache
people-orange+dress
people-orange+vest
people-overalls
people-pigtails
people-pink+hat
people-pink+shirt
people-ponytail
people-red
people-robe
people-safety+vests
people-shirt
people-shopping+bag
people-shoulders
people-smile
people-socks
people-suits
people-sweatshirt
people-swimsuits
people-tan+pants
people-tattoo
people-tie
people-trunks
people-umbrellas
people-vest
people-white
people-yellow

4)
8)
12)
16)
20)
24)
28)
32)
36)
40)
44)
48)
52)
56)
60)
64)
68)
72)
76)
80)
84)
88)
92)
96)
100)
104)
108)
112)
116)
120)
124)
128)
132)
136)
140)
144)
148)
152)
156)
160)
164)
168)
172)
176)
180)
184)
188)
192)
196)
200)
204)

people-attire
people-bags
people-baseball+uniform
people-beard
people-black
people-blouse
people-brown
people-business+suit
people-clothes
people-collared+shirt
people-cowboy+hats
people-dress
people-face
people-ﬁngers
people-gloves
people-green
people-hands
people-head
people-headscarf
people-hoodie
people-jeans
people-khaki+pants
people-lap
people-leotard
people-mask
people-necklace
people-orange+hat
people-orange+vests
people-pajamas
people-pink
people-pink+jacket
people-pink+sweater
people-purple
people-red+white
people-robes
people-sandals
people-shirts
people-shopping+bags
people-skirt
people-sneakers
people-straw+hat
people-sunglasses
people-swim+trunks
people-t-shirt
people-tan+shirt
people-tattoos
people-tongue
people-turban
people-underwear
people-vests
people-wig

Phrase Localization and Visual Relationship Detection with Comprehensive
Image-Language Cues

Bryan A. Plummer

Arun Mallya

Christopher M. Cervantes

Julia Hockenmaier

Svetlana Lazebnik
University of Illinois at Urbana-Champaign
{bplumme2, amallya2, ccervan2, juliahmr, slazebni}@illinois.edu

7
1
0
2
 
g
u
A
 
9
 
 
]

V
C
.
s
c
[
 
 
4
v
1
4
6
6
0
.
1
1
6
1
:
v
i
X
r
a

Abstract

This paper presents a framework for localization or
grounding of phrases in images using a large collection
of linguistic and visual cues. We model the appearance,
size, and position of entity bounding boxes, adjectives that
contain attribute information, and spatial relationships be-
tween pairs of entities connected by verbs or prepositions.
Special attention is given to relationships between people
and clothing or body part mentions, as they are useful for
distinguishing individuals. We automatically learn weights
for combining these cues and at test time, perform joint in-
ference over all phrases in a caption. The resulting system
produces state of the art performance on phrase localiza-
tion on the Flickr30k Entities dataset [33] and visual rela-
tionship detection on the Stanford VRD dataset [27].1

1. Introduction

Today’s deep features can give reliable signals about a
broad range of content in natural images, leading to ad-
vances in image-language tasks such as automatic cap-
tioning [6, 14, 16, 17, 42] and visual question answer-
ing [1, 8, 44]. A basic building block for such tasks is lo-
calization or grounding of individual phrases [6, 16, 17, 28,
33, 40, 42]. A number of datasets with phrase grounding
information have been released, including Flickr30k Enti-
ties [33], ReferIt [18], Google Referring Expressions [29],
and Visual Genome [21]. However, grounding remains
challenging due to open-ended vocabularies, highly unbal-
anced training data, prevalence of hard-to-localize entities
like clothing and body parts, as well as the subtlety and va-
riety of linguistic cues that can be used for localization.

The goal of this paper is to accurately localize a bound-
ing box for each entity (noun phrase) mentioned in a caption
for a particular test image. We propose a joint localization
objective for this task using a learned combination of single-
phrase and phrase-pair cues. Evaluation is performed on the

1Code: https://github.com/BryanPlummer/pl-clc

Figure 1: Left: an image and caption, together with ground truth bounding
boxes of entities (noun phrases). Right: a list of all the cues used by our
system, with corresponding phrases from the sentence.

challenging recent Flickr30K Entities dataset [33], which
provides ground truth bounding boxes for each entity in the
ﬁve captions of the original Flickr30K dataset [43].

Figure 1 introduces the components of our system using
an example image and caption. Given a noun phrase ex-
tracted from the caption, e.g., red and blue umbrella, we ob-
tain single-phrase cue scores for each candidate box based
on appearance (modeled with a phrase-region embedding as
well as object detectors for common classes), size, position,
and attributes (adjectives). If a pair of entities is connected
by a verb (man carries a baby) or a preposition (woman
in a red jacket), we also score the pair of corresponding
candidate boxes using a spatial model. In addition, actions
may modify the appearance of either the subject or the ob-
ject (e.g., a man carrying a baby has a characteristic appear-
ance, as does a baby being carried). To account for this, we
learn subject-verb and verb-object appearance models for
the constituent entities. We give special treatment to rela-
tionships between people, clothing, and body parts, as these
are commonly used for describing individuals, and are also
among the hardest entities for existing approaches to local-
ize. To extract as complete a set of relationships as possible,
we use natural language processing (NLP) tools to resolve
pronoun references within a sentence: e.g., by analyzing the

Adjectives Verbs

Method

Ours

(a) NonlinearSP [40]
GroundeR [34]
MCB [8]
SCRC [12]
SMPL [41]
RtP [33]

(b) Scene Graph [15]
ReferIt [18]
Google RefExp [29]

Single Phrase Cues

Phrase-Region Candidate Candidate Object
Compatibility
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
–
–
(cid:88)

Position
(cid:88)
–
–
–
(cid:88)
–
–
–
(cid:88)
(cid:88)

Detectors
(cid:88)*
–
–
–
–
–
(cid:88)*
(cid:88)
(cid:88)
–

Size
(cid:88)
–
–
–
–
–
(cid:88)
–
(cid:88)
(cid:88)

(cid:88)
–
–
–
–
–
(cid:88)*
(cid:88)
(cid:88)*
–

Phrase-Pair Spatial Cues
Relative
Position
(cid:88)
–
–
–
–
(cid:88)*
–
(cid:88)
(cid:88)
–

Clothing &
Body Parts
(cid:88)
–
–
–
–
–
–
–
–
–

Inference
Joint
Localization
(cid:88)
–
–
–
–
(cid:88)
–
(cid:88)
–
–

(cid:88)
–
–
–
–
–
–
–
–
–

Table 1: Comparison of cues for phrase-to-region grounding. (a) Models applied to phrase localization on Flickr30K Entities. (b) Models on related tasks.
* indicates that the cue is used in a limited fashion, i.e. [18, 33] restricted their adjective cues to colors, [41] only modeled possessive pronoun phrase-pair
spatial cues ignoring verb and prepositional phrases, [33] and we limit the object detectors to 20 common categories.

sentence A man puts his hand around a woman, we can de-
termine that the hand belongs to the man and introduce the
respective pairwise term into our objective.

Table 1 compares the cues used in our work to those in
other recent papers on phrase localization and related tasks
like image retrieval and referring expression understanding.
To date, other methods applied to the Flickr30K Entities
dataset [8, 12, 34, 40, 41] have used a limited set of single-
phrase cues. Information from the rest of the caption, like
verbs and prepositions indicating spatial relationships, has
been ignored. One exception is Wang et al. [41], who tried
to relate multiple phrases to each other, but limited their re-
lationships only to those indicated by possessive pronouns,
not personal ones. By contrast, we use pronoun cues to the
full extent by performing pronominal coreference. Also,
ours is the only work in this area incorporating the visual
aspect of verbs. Our formulation is most similar to that
of [33], but with a larger set of cues, learned combination
weights, and a global optimization method for simultane-
ously localizing all the phrases in a sentence.

In addition to our experiments on phrase localization, we
also adapt our method to the recently introduced task of
visual relationship detection (VRD) on the Stanford VRD
dataset [27]. Given a test image, the goal of VRD is to de-
tect all entities and relationships present and output them in
the form (subject, predicate, object) with the correspond-
ing bounding boxes. By contrast with phrase localization,
where we are given a set of entities and relationships that
are in the image, in VRD we do not know a priori which
objects or relationships might be present. On this task, our
model shows signiﬁcant performance gains over prior work,
with especially acute differences in zero-shot detection due
to modeling cues with a vision-language embedding. This
adaptability to never-before-seen examples is also a notable
distinction between our approach and prior methods on re-
lated tasks (e.g. [7, 15, 18, 20]), which typically train their
models on a set of predeﬁned object categories, providing
no support for out-of-vocabulary entities.

Section 2 discusses our global objective function for si-
multaneously localizing all phrases from the sentence and
describes the procedure for learning combination weights.
Section 3.1 details how we parse sentences to extract enti-
ties, relationships, and other relevant linguistic cues. Sec-
tions 3.2 and 3.3 deﬁne single-phrase and phrase-pair cost
functions between linguistic and visual cues. Section 4
presents an in-depth evaluation of our cues on Flickr30K
Entities [33]. Lastly, Section 5 presents the adaptation of
our method to the VRD task [27].

2. Phrase localization approach

We follow the task deﬁnition used in [8, 12, 33, 34, 40,
41]: At test time, we are given an image and a caption with
a set of entities (noun phrases), and we need to localize each
entity with a bounding box. Section 2.1 describes our infer-
ence formulation, and Section 2.2 describes our procedure
for learning the weights of different cues.

2.1. Joint phrase localization

For each image-language cue derived from a single
phrase or a pair of phrases (Figure 1), we deﬁne a cue-
speciﬁc cost function that measures its compatibility with an
image region (small values indicate high compatibility). We
will describe the cost functions in detail in Section 3; here,
we give our test-time optimization framework for jointly lo-
calizing all phrases from a sentence.

Given a single phrase p from a test sentence, we score
each region (bounding box) proposal b from the test image
based on a linear combination of cue-speciﬁc cost functions
φ{1,··· ,KS }(p, b) with learned weights wS:

S(p,b;wS)=

1s(p)φs(p,b)wS
s ,

(1)

KS(cid:88)

s=1

where 1s(p) is an indicator function for the availability of
cue s for phrase p (e.g., an adjective cue would be avail-
able for the phrase blue socks, but would be unavailable for

socks by itself). As will be described in Section 3.2, we use
14 single-phrase cost functions: region-phrase compatibil-
ity score, phrase position, phrase size (one for each of the
eight phrase types of [33]), object detector score, adjective,
subject-verb, and verb-object scores.

For a pair of phrases with some relationship r =
(p, rel, p(cid:48)) and candidate regions b and b(cid:48), an analogous
scoring function is given by a weighted combination of pair-
wise costs ψ{1,··· ,KQ}(r, b, b(cid:48)):

Q(r,b,b(cid:48);wQ)=

1q(r)ψq(r,b,b(cid:48))wQ
q .

(2)

KQ
(cid:88)

q=1

We use three pairwise cost functions corresponding to spa-
tial classiﬁers for verb, preposition, and clothing and body
parts relationships (Section 3.3).

We train all cue-speciﬁc cost functions on the training set
and the combination weights on the validation set. At test
time, given an image and a list of phrases {p1, · · · , pN },
we ﬁrst retrieve top M candidate boxes for each phrase pi
using Eq. (1). Our goal is then to select one bounding box
bi out of the M candidates per each phrase pi such that the
following objective is minimized:




(cid:88)



pi

min
b1,···,bN

S(pi,bi) +

Q(rij,bi,bj)

(3)

(cid:88)

rij =(pi,relij ,pj )






where phrases pi and pj (and respective boxes bi and bj)
are related by some relationship relij. This is a binary
quadratic programming formulation inspired by [38]; we
relax and solve it using a sequential QP solver in MAT-
LAB. The solution gives a single bounding box hypothe-
sis for each phrase. Performance is evaluated using Re-
call@1, or proportion of phrases where the selected box has
Intersection-over-Union (IOU) ≥ 0.5 with the ground truth.

2.2. Learning scoring function weights

We learn the weights wS and wQ in Eqs. (1) and (2) by
directly optimizing recall on the validation set. We start by
ﬁnding the unary weights wS that maximize the number of
correctly localized phrases:

We optimize Eq. (4) using a derivative-free direct search
method [22] (MATLAB’s fminsearch). We randomly ini-
tialize the weights, keep the best weights after 20 runs based
on validation set performance (takes just a few minutes to
learn weights for all single phrase cues in our experiments).
Next, we ﬁx wS and learn the weights wQ over phrase-
pair cues in the validation set. To this end, we formulate an
objective analogous to Eq. (4) for maximizing the number
of correctly localized region pairs. Similar to Eq. (5), we
deﬁne the function ˆρ(r; w) to return the best pair of boxes
for the relationship r = (p, rel, p(cid:48)):

ˆρ(r;w)= min
b,b(cid:48)∈B

S(p,b;wS)+S(p(cid:48),b(cid:48);wS)+Q(r,b,b(cid:48);w). (6)

Then our pairwise objective function is

wQ = arg max

IP airIOU ≥0.5(ρ∗

k, ˆρ(rk; w)),

(7)

M
(cid:88)

k=1

w

k).

where M is the number of phrase pairs with a relation-
ship, IP airIOU ≥0.5 returns the number of correctly local-
ized boxes (0, 1, or 2), and ρ∗
k is the ground truth box pair
for the relationship rk = (pk, relk, p(cid:48)

Note that we also attempted to learn the weights wS and
wQ using standard approaches such as rank-SVM [13], but
found our proposed direct search formulation to work bet-
ter. In phrase localization, due to its Recall@1 evaluation
criterion, only the correctness of one best-scoring candidate
region for each phrase matters, unlike in typical detection
scenarios, where one would like all positive examples to
have better scores than all negative examples. The VRD
task of Section 5 is a more conventional detection task, so
there we found rank-SVM to be more appropriate.

3. Cues for phrase-region grounding

Section 3.1 describes how we extract linguistic cues from
sentences. Sections 3.2 and 3.3 give our deﬁnitions of the
two types of cost functions used in Eqs. (1) and (2): sin-
gle phrase cues (SPC) measure the compatibility of a given
phrase with a candidate bounding box, and phrase pair cues
(PPC) ensure that pairs of related phrases are localized in a
spatially coherent manner.

wS = arg max

w

N
(cid:88)

i=1

1IOU ≥0.5(b∗

i , ˆb(pi; w)),

(4)

3.1. Extracting linguistic cues from captions

where N is the number of phrases in the training set,
1IOU ≥0.5 is an indicator function returning 1 if the two
boxes have IOU ≥ 0.5, b∗
i is the ground truth bounding box
for phrase pi, ˆb(p; w) returns the most likely box candidate
for phrase p under the current weights, or, more formally,
given a set of candidate boxes B,

ˆb(p; w) = min
b∈B

S(p, b; w).

(5)

The Flickr30k Entities dataset provides annotations for
Noun Phrase (NP) chunks corresponding to entities, but lin-
guistic cues corresponding to adjectives, verbs, and preposi-
tions must be extracted from the captions using NLP tools.
Once these cues are extracted, they will be translated into
visually relevant constraints for grounding.
In particular,
we will learn specialized detectors for adjectives, subject-
verb, and verb-object relationships (Section 3.2). Also, be-
cause pairs of entities connected by a verb or preposition

have constrained layout, we will train classiﬁers to score
pairs of boxes based on spatial information (Section 3.3).

Adjectives are part of NP chunks so identifying them is
trivial. To extract other cues, such as verbs and preposi-
tions that may indicate actions and spatial relationships, we
obtain a constituent parse tree for each sentence using the
Stanford parser [37]. Then, for possible relational phrases
(prepositional and verb phrases), we use the method of Fi-
dler et al. [7], where we start at the relational phrase and
then traverse up the tree and to the left until we reach a noun
phrase node, which will correspond to the ﬁrst entity in an
(entity1, rel, entity2) tuple. The second entity is given by
the ﬁrst noun phrase node on the right side of the relational
phrase in the parse tree. For example, given the sentence A
boy running in a ﬁeld with a dog, the extracted NP chunks
would be a boy, a ﬁeld, a dog. The relational phrases would
be (a boy, running in, a ﬁeld) and (a boy, with, a dog).

Notice that a single relational phrase can give rise to mul-
tiple relationship cues. Thus, from (a boy, running in, a
ﬁeld), we extract the verb relation (boy, running, ﬁeld) and
prepositional relation (boy, in, ﬁeld). An exception to this
is a relational phrase where the ﬁrst entity is a person and
the second one is of the clothing or body part type,2 e.g.,
(a boy, running in, a jacket). For this case, we create a sin-
gle special pairwise relation (boy, jacket) that assumes that
the second entity is attached to the ﬁrst one and the exact
relationship words do not matter, i.e., (a boy, running in, a
jacket) and (a boy, wearing, a jacket) are considered to be
the same. The attachment assumption can fail for phrases
like (a boy, looking at, a jacket), but such cases are rare.

Finally, since pronouns in Flickr30k Entities are not
annotated, we attempt to perform pronominal coreference
(i.e., creating a link between a pronoun and the phrase
it refers to) in order to extract a more complete set of
cues. As an example, given the sentence Ducks feed them-
selves, initially we can only extract the subject-verb cue
(ducks, f eed), but we don’t know who or what they are
feeding. Pronominal coreference resolution tells us that the
ducks are themselves eating and not, say, feeding ducklings.
We use a simple rule-based method similar to knowledge-
poor methods [11, 31]. Given lists of pronouns by type,3 our
rules attach each pronoun with at most one non-pronominal
mention that occurs earlier in the sentence (an antecedent).
We assume that subject and object pronouns often refer to
the main subject (e.g. [A dog] laying on the ground looks
up at the dog standing over [him]), reﬂexive and recipro-
cal pronouns refer to the nearest antecedent (e.g. [A tennis
player] readies [herself].), and indeﬁnite pronouns do not
refer to a previously described entity. It must be noted that

2Each NP chunk from the Flickr30K dataset is classiﬁed into one of

eight phrase types based on the dictionaries of [33].

3Relevant pronoun types are subject, object, reﬂexive, reciprocal, rela-

tive, and indeﬁnite.

compared with verb and prepositional relationships, rela-
tively few additional cues are extracted using this procedure
(432 pronoun relationships in the test set and 13,163 in the
train set, while the counts for the other relationships are on
the order of 10K and 300K).

3.2. Single Phrase Cues (SPCs)

Region-phrase compatibility: This is the most basic cue
relating phrases to image regions based on appearance. It
is applied to every test phrase (i.e., its indicator function
in Eq. (1) is always 1). Given phrase p and region b, the
cost φCCA(p, b) is given by the cosine distance between
p and b in a joint embedding space learned using normal-
ized Canonical Correlation Analysis (CCA) [10]. We use
the same procedure as [33]. Regions are represented by the
fc7 activations of a Fast-RCNN model [9] ﬁne-tuned using
the union of the PASCAL 2007 and 2012 trainval sets [5].
After removing stopwords, phrases are represented by the
HGLMM ﬁsher vector encoding [19] of word2vec [30].

Candidate position: The location of a bounding box in
an image has been shown to be predictive of the kinds of
phrases it may refer to [4, 12, 18, 23]. We learn location
models for each of the eight broad phrase types speciﬁed
in [33]: people, clothing, body parts, vehicles, animals,
scenes, and a catch-all “other.” We represent a bounding
box by its centroid normalized by the image size, the per-
centage of the image covered by the box, and its aspect
ratio, resulting in a 4-dim. feature vector. We then train
a support vector machine (SVM) with a radial basis func-
tion (RBF) kernel using LIBSVM [2]. We randomly sample
EdgeBox [46] proposals with IOU < 0.5 with the ground
truth boxes for negative examples. Our scoring function is

φpos(p, b) = − log(SVMtype(p)(b)),

where SVMtype(p) returns the probability that box b is of the
phrase type type(p) (we use Platt scaling [32] to convert the
SVM output to a probability).

Candidate size: People have a bias towards describing
larger, more salient objects, leading prior work to consider
the size of a candidate box in their models [7, 18, 33]. We
follow the procedure of [33], so that given a box b with di-
mensions normalized by the image size, we have

φsizetype(p) (p, b) = 1 − bwidth × bheight.

Unlike phrase position, this cost function does not use a
trained SVM per phrase type. Instead, each phrase type is
its own feature and the corresponding indicator function re-
turns 1 if that phrase belongs to the associated type.

Detectors: CCA embeddings are limited in their ability to
localize objects because they must account for a wide range
of phrases and because they do not use negative examples

during training. To compensate for this, we use Fast R-
CNN [9] to learn three networks for common object cate-
gories, attributes, and actions. Once a detector is trained, its
score for a region proposal b is

φdet(p, b) = − log(softmaxdet(p, b)),

where softmaxdet(p, b) returns the output of the softmax
layer for the object class corresponding to p. We manu-
ally create dictionaries to map phrases to detector categories
(e.g., man, woman, etc. map to ‘person’), and the indicator
function for each detector returns 1 only if one of the words
in the phrase exists in its dictionary. If multiple detectors
for a single cue type are appropriate for a phrase (e.g., a
black and white shirt would have two adjective detectors
ﬁre, one for each color), the scores are averaged. Below,
we describe the three detector networks used in our model.
Complete dictionaries can be found in Appendix B.

Objects: We use the dictionary of [33] to map nouns to the
20 PASCAL object categories [5] and ﬁne-tune the network
on the union of the PASCAL VOC 2007 and 2012 trainval
sets. At test time, when we run a detector for a phrase that
maps to one of these object categories, we also use bound-
ing box regression to reﬁne the original region proposals.
Regression is not used for the other networks below.

Adjectives: Adjectives found in phrases, especially color,
provide valuable attribute information for localization [7,
15, 18, 33]. The Flickr30K Entities baseline approach [33]
used a network trained for 11 colors. As a generalization
of that, we create a list of adjectives that occur at least 100
times in the training set of Flickr30k. After grouping to-
gether similar words and ﬁltering out non-visual terms (e.g.,
adventurous), we are left with a dictionary of 83 adjectives.
As in [33], we consider color terms describing people (black
man, white girl) to be separate categories.

Subject-Verb and Verb-Object: Verbs can modify the ap-
pearance of both the subject and the object in a relation. For
example, knowing that a person is riding a horse can give
us better appearance models for ﬁnding both the person and
the horse [35, 36]. As we did with adjectives, we collect
verbs that occur at least 100 times in the training set, group
together similar words, and ﬁlter out those that don’t have
a clear visual aspect, resulting in a dictionary of 58 verbs.
Since a person running looks different than a dog running,
we subdivide our verb categories by phrase type of the sub-
ject (resp. object) if that phrase type occurs with the verb
at least 30 times in the train set. For example, if there are
enough animal-running occurrences, we create a new cate-
gory with instances of all animals running. For the remain-
ing phrases, we train a catch-all detector over all the phrases
related to that verb. Following [35], we train separate detec-
tors for subject-verb and verb-object relationships, resulting
in dictionary sizes of 191 (resp. 225). We also attempted to

learn subject-verb-object detectors as in [35, 36], but did not
see a further improvement.

3.3. Phrase-Pair Cues (PPCs)

So far, we have discussed cues pertaining to a single
phrase, but relationships between pairs of phrases can also
provide cues about their relative position. We denote such
relationships as tuples (pleft , rel, pright ) with left, right in-
dicating on which side of the relationship the phrases oc-
cur. As discussed in Section 3.1, we consider three distinct
types of relationships: verbs (man, riding, horse), preposi-
tions (man, on, horse), and clothing and body parts (man,
wearing, hat). For each of the three relationship types, we
group phrases referring to people but treat all other phrases
as distinct, and then gather all relationships that occur at
least 30 times in the training set. Then we learn a spatial
relationship model as follows. Given a pair of boxes with
coordinates b = (x, y, w, h) and b(cid:48) = (x(cid:48), y(cid:48), w(cid:48), h(cid:48)), we
compute a four-dim. feature

[(x − x(cid:48))/w, (y − y(cid:48))/h, w(cid:48)/w, h(cid:48)/h] ,

(8)

and concatenate it with combined SPC scores S(pleft , b),
S(pright , b(cid:48)) from Eq. (1). To obtain negative examples, we
randomly sample from other box pairings with IOU < 0.5
with the ground truth regions from that image. We train
an RBF SVM classiﬁer with Platt scaling [32] to obtain a
probability output. This is similar to the method of [15], but
rather than learning a Gaussian Mixture Model using only
positive data, we learn a more discriminative model. Be-
low are details on the three types of relationship classiﬁers.
Complete dictionaries can be found in Appendix C.

Verbs: Starting with our dictionary of 58 verb detectors
and following the above procedure of identifying all rela-
tionships that occur at least 30 times in the training set, we
end up with 260 (pleft , relverb, pright ) SVM classiﬁers.

Prepositions: We ﬁrst gather a list of prepositions that oc-
cur at least 100 times in the training set, combine simi-
lar words, and ﬁlter out words that do not indicate a clear
spatial relationship. This yields eight prepositions (in, on,
under, behind, across, between, onto, and near) and 216
(pleft , relprep, pright ) relationships.

Clothing and body part attachment: We collect
(pleft , relc&bp, pright ) relationships where the left phrase is
always a person and the right phrase is from the clothing or
body part type and learn 207 such classiﬁers. As discussed
in Section 3.1, this relationship type takes precedence over
any verb or preposition relationships that may also hold be-
tween the same phrases.

Method

Accuracy

(a)

(b)

(c)

Single-phrase cues
CCA
CCA+Det
CCA+Det+Size
CCA+Det+Size+Adj
CCA+Det+Size+Adj+Verbs
CCA+Det+Size+Adj+Verbs+Pos (SPC)
Phrase pair cues
SPC+Verbs
SPC+Verbs+Preps
SPC+Verbs+Preps+C&BP (SPC+PPC)
State of the art
SMPL [41]
NonlinearSP [40]
GroundeR [34]
MCB [8]
RtP [33]

43.09
45.29
51.45
52.63
54.51
55.49

55.53
55.62
55.85

42.08
43.89
47.81
48.69
50.89

Table 2: Phrase-region grounding performance on the Flickr30k Entities
dataset. (a) Performance of our single-phrase cues (Sec. 3.2). (b) Further
improvements by adding our pairwise cues (Sec. 3.3). (c) Accuracies of
competing state-of-the-art methods. This comparison excludes concurrent
work that was published after our initial submission [3].
4. Experiments on Flickr30k Entities

4.1. Implementation details

We utilize the provided train/test/val split of 29,873
training, 1,000 validation, and 1,000 testing images [33].
Following [33], our region proposals are given by the top
200 EdgeBox [46] proposals per image. At test time, given
a sentence and an image, we ﬁrst use Eq. (1) to ﬁnd the
top 30 candidate regions for each phrase after performing
non-maximum suppression using a 0.8 IOU threshold. Re-
stricted to these candidates, we optimize Eq. (2) to ﬁnd a
globally consistent mapping of phrases to regions.

Consistent with [33], we only evaluate localization for
phrases with a ground truth bounding box.
If multiple
bounding boxes are associated with a phrase (e.g., four in-
dividual boxes for four men), we represent the phrase as the
union of its boxes. For each image and phrase in the test set,
the predicted box must have at least 0.5 IOU with its ground
truth box to be deemed successfully localized. As only a
single candidate is selected for each phrase, we report the
proportion of correctly localized phrases (i.e. Recall@1).

4.2. Results

Table 2 reports our overall localization accuracy for com-
binations of cues and compares our performance to the state
of the art. Object detectors, reported on the second line of
Table 2(a), show a 2% overall gain over the CCA baseline.
This includes the gain from the detector score as well as the
bounding box regressor trained with the detector in the Fast
R-CNN framework [9]. Adding adjective, verb, and size
cues improves accuracy by a further 9%. Our last cue in Ta-
ble 2(a), position, provides an additional 1% improvement.
We can see from Table 2(b) that the spatial cues give only
a small overall boost in accuracy on the test set, but that

is due to the relatively small number of phrases to which
they apply. In Table 4 we will show that the localization
improvement on the affected phrases is much larger.

Table 2(c) compares our performance to the state of
the art. The method most similar to ours is our earlier
model [33], which we call RtP here. RtP relies on a subset
of our single-phrase cues (region-phrase CCA, size, object
detectors, and color adjectives), and localizes each phrase
separately. The closest version of our current model to
RtP is CCA+Det+Size+Adj, which replaces the 11 colors
of [33] with our more general model for 83 adjectives, and
obtains almost 2% better performance. Our full model is
5% better than RtP. It is also worth noting that a rank-SVM
model [13] for learning cue combination weights gave us
8% worse performance than the direct search scheme of
Section 2.2.

Table 3 breaks down the comparison by phrase type.
Our model has the highest accuracy on most phrase types,
with scenes being the most notable exception, for which
GroundeR [34] does better. However, GroundeR uses Se-
lective Search proposals [39], which have an upper bound
performance that is 7% higher on scene phrases despite us-
ing half as many proposals. Although body parts have the
lowest localization accuracy at 25.24%, this represents an
8% improvement in accuracy over prior methods. However,
only around 62% of body part phrases have a box with high
enough IOU with the ground truth, showing a major area of
weakness of category-independent proposal methods.
In-
deed, if we were to augment our EdgeBox region proposals
with ground truth boxes, we would get an overall improve-
ment in accuracy of about 9% for the full system.

Since many of the cues apply to a small subset of the
phrases, Table 4 details the performance of cues over only
the phrases they affect. As a baseline, we compare against
the combination of cues available for all phrases: region-
phrase CCA, position, and size. To have a consistent set of
regions, the baseline also uses improved boxes from bound-
ing box regressors trained along with the object detectors.
As a result, the object detectors provide less than 2% gain
over the baseline for the phrases on which they are used,
suggesting that the regression provides the majority of the
gain from CCA to CCA+Det in Table 2. This also conﬁrms
that there is signiﬁcant room for improvement in select-
ing candidate regions. By contrast, adjective, subject-verb,
and verb-object detectors show signiﬁcant gains, improving
over the baseline by 6-7%.

The right side of Table 4 shows the improvement on
phrases due to phrase pair cues. Here, we separate the
phrases that occur on the left side of the relationship, which
corresponds to the subject, from the phrases on the right
side. Our results show that the subject, is generally eas-
ier to localize. On the other hand, clothing and body parts
show up mainly on the right side of relationships and they

People

Clothing

Body Parts

Animals

Vehicles

Instruments

Scene

#Test

SMPL [41]
GroundeR [34]
RtP [33]
SPC+PPC (ours)
Upper Bound

5,656

57.89
61.00
64.73
71.69
97.72

2,306

34.61
38.12
46.88
50.95
83.13

523

15.87
10.33
17.21
25.24
61.57

518

55.98
62.55
65.83
76.25
91.89

400

52.25
68.75
68.75
66.50
94.00

162

23.46
36.42
37.65
35.80
82.10

Other

3,374

26.23
29.08
31.77
35.98
81.06

1,619

34.22
58.18
51.39
51.51
84.37

Table 3: Comparison of phrase localization performance over phrase types. Upper Bound refers to the proportion of phrases of each type for which there
exists a region proposal having at least 0.5 IOU with the ground truth.

Method

Baseline
+Cue

#Test
#Train

Single Phrase Cues (SPC)

Phrase-Pair Cues (PPC)

Object
Detectors

Adjectives

Subject-
Verb

Verb-
Object

74.25
75.78

4,059
114,748

57.71
64.35

3,809
110,415

69.68
75.53

3,094
94,353

Verbs

Prepositions

Clothing &
Body Parts

Left

78.32
78.94

867
26,254

Right

51.05
51.33

858
25,898

Left

68.97
69.74

780
23,973

Right

55.01
56.14

778
23,903

Left

81.01
82.86

1,464
42,084

Right

50.72
52.23

1,591
45,496

40.70
47.62

2,398
71,336

Table 4: Breakdown of performance for individual cues restricted only to test phrases to which they apply.
For SPC, Baseline is given by
CCA+Position+Size. For PPC, Baseline is the full SPC model. For all comparisons, we use the improved boxes from bounding box regression on top
of object detector output. PPC evaluation is split by which side of the relationship the phrases occur on. The bottom two rows show the numbers of affected
phrases in the test and training sets. For reference, there are 14.5k visual phrases in the test set and 427k visual phrases in the train set.

tend to be small. It is also less likely that such phrases will
have good candidate boxes – recall from Table 3 that body
parts have a performance upper bound of only 62%. Al-
though they affect relatively few test phrases, all three of our
relationship classiﬁers show consistent gains over the SPC
model. This is encouraging given that many of the relation-
ships that are used on the validation set to learn our model
parameters do not occur in the test set (and vice versa).

Figure 2 provides a qualitative comparison of our output
with the RtP model [33]. In the ﬁrst example, the prediction
for the dog is improved due to the subject-verb classiﬁer for
dog jumping. For the second example, pronominal corefer-
ence resolution (Section 3.1) links each other to two men,
telling us that not only is a man hitting something, but also
that another man is being hit. In the third example, the RtP
model is not able to locate the woman’s blue stripes in her
hair despite having a model for blue. Our adjective detec-
tors take into account stripes as well as blue, allowing us
to correctly localize the phrase, even though we still fail
to localize the hair. Since the blue stripes and hair should
co-locate, a method for obtaining co-referent entities would
further improve performance on such cases. In the last ex-
ample, the RtP model makes the same incorrect prediction
for the two men. However, our spatial relationship between
the ﬁrst man and his gray sweater helps us correctly localize
him. We also improve our prediction for the shopping cart.

5. Visual Relationship Detection

In this section, we adapt our framework to the recently
introduced Visual Relationship Detection (VRD) bench-
mark of Lu et al. [27]. Given a test image without any text
annotations, the task of VRD is to detect all entities and
relationships present and output them in the form (subject,

predicate, object) with the corresponding bounding boxes.
A relationship detection is judged to be correct if it exists
in the image and both the subject and object boxes have
IOU ≥ 0.5 with their respective ground truth. In contrast to
phrase grounding, where we are given a set of entities and
relationships that are assumed to be in the image, here we
do not know a priori which objects or relationships might
be present. On the other hand, the VRD dataset is easier
than Flickr30K Entities in that it has a limited vocabulary
of 100 object classes and 70 predicates annotated in 4000
training and 1000 test images.

Given the small ﬁxed class vocabulary, it would seem
advantageous to train 100 object detectors on this dataset,
as was done by Lu et al. [27]. However, the training set
is relatively small, the class distribution is unbalanced, and
there is no validation set. Thus, we found that training de-
tectors and then relationship models on the same images
causes overﬁtting because the detector scores on the train-
ing images are overconﬁdent. We obtain better results by
training all appearance models using CCA, which also takes
into account semantic similarity between category names
and is trivially extendable to previously unseen categories.
Here, we use fc7 features from a Fast RCNN model trained
on MSCOCO [26] due to the larger range of categories
than PASCAL, and word2vec for object and predicate class
names. We train the following CCA models:

1. CCA(entity box, entity class name): this is the equiv-
alent to region-phrase CCA in Section 3.2 and is used
to score both candidate subject and object boxes.
2. CCA(subject box, [subject class name, predicate class
name]): analogous to subject-verb classiﬁers of Sec-
tion 3.2. The 300-dimensional word2vec features of
subject and predicate class names are concatenated.
3. CCA(object box, [predicate class name, object class

Figure 2: Example results on Flickr30k Entities comparing our SPC+PPC model’s output with the RtP model [33]. See text for discussion.

name]): analogous to verb-object classiﬁers of Section
3.2.

4. CCA(union box, predicate class name):

this model
measures the compatibility between the bounding box
of both subject and object and the predicate name.
5. CCA(union box, [subject class name, predicate class

name, object class name]).

Note that models 4 and 5 had no analogue in our phrase
localization system. On that task, entities were known to be
in the image and relationships simply provided constraints,
while here we need to predict which relationships exist. To
make predictions for predicates and relationships (which is
the goal of models 4 and 5), it helps to see both the subject
and object regions. Union box features were also less useful
for phrase localization due to the larger vocabularies and
relative scarcity of relationships in that task.

Each candidate relationship gets six CCA scores (model
1 above is applied both to the subject and the object). In
addition, we compute size and position scores as in Sec-
tion 3.2 for subject and object, and a score for a pairwise
spatial SVM trained to predict the predicate based on the
four-dimensional feature of Eq.
(8). This yields an 11-
dim. feature vector. By contrast with phrase localization,
our features for VRD are dense (always available for every
relationship).

In Section 2.2 we found feature weights by maximiz-
ing our recall metric. Here we have a more conventional
detection task, so we obtain better performance by train-
ing a linear rank-SVM model [13] to enforce that correctly
detected relationships are ranked higher than negative de-
tections (where either box has < 0.5 IOU with the ground
truth). We use the test set object detections (just the boxes,
not the scores) provided by [27] to directly compare per-
formance with the same candidate regions. During testing,

we produce a score for every ordered pair of detected boxes
and all possible predicates, and retain the top 10 predicted
relationships per pair of (subject, object) boxes.

Consistent with [27], Table 5 reports recall, R@{100,
50}, or the portion of correctly localized relationships in the
top 100 (resp. 50) ranked relationships in the image. The
right side shows performance for relationships that have not
been encountered in the training set. Our method clearly
outperforms that of Lu et al. [27], which uses separate vi-
sual, language, and relationship likelihood cues. We also
outperform Zhang et al. [45], which combines object detec-
tors, visual appearance, and object position in a single neu-
ral network. We observe that cues based on object class and
relative subject-object position provide a noticeable boost
in performance. Further, due to using CCA with multi-
modal embeddings, we generalize better to unseen relation-
ships. Qualitative examples and associated discussion can
be found in Appendix A.

6. Conclusion

This paper introduced a framework incorporating a com-
prehensive collection of image- and language-based cues
for visual grounding and demonstrated signiﬁcant gains
over the state of the art on two tasks: phrase localization on
Flickr30k Entities and relationship detection on the VRD
dataset. For the latter task, we got particularly pronounced
gains for the zero-shot learning scenario. In future work, we
would like to train a single network for combining multiple
cues. Doing this in a uniﬁed end-to-end fashion is chal-
lenging, since one needs to ﬁnd the right balance between
parameter sharing and specialization or ﬁne-tuning required
by individual cues. To this end, our work provides a strong
baseline and can help to inform future approaches.

Method

Phrase Det.

Rel. Det.

R@100

R@50

R@100

R@50

Zero-shot Phrase Det.
R@100

R@50

Zero-shot Rel. Det.
R@50
R@100

(a)

(b)

Visual Only Model [27]
Visual + Language +
Likelihood Model [27]
VTransE [45]
CCA
CCA + Size
CCA + Size + Position

2.61

17.03

22.42
15.36
15.85
20.70

2.24

16.17

19.42
11.38
11.72
16.89

1.85

14.70

15.20
13.69
14.05
18.37

1.58

13.86

14.07
10.08
10.36
15.08

1.12

3.75

3.51
12.40
12.92
15.23

0.95

3.36

2.65
7.78
8.04
10.86

0.78

3.52

2.14
11.12
11.46
13.43

0.67

3.13

1.71
6.59
6.76
9.67

Table 5: Relationship detection recall at different thresholds (R@{100,50}). CCA refers to the combination of six CCA models (see text). Position refers
to the combination of individual box position and pairwise spatial classiﬁers. This comparison excludes concurrent work that was published after our initial
submission [24, 25].

Acknowledgments. This work was partially supported by
NSF grants 1053856, 1205627, 1405883, 1302438, and
1563727, Xerox UAC, the Sloan Foundation, and a Google
Research Award.

[12] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Dar-
rell. Natural language object retrieval. In CVPR, 2016. 2,
4

[13] T. Joachims. Training linear svms in linear time. In SIGKDD,

References

[1] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
Zitnick, and D. Parikh. Vqa: Visual question answering. In
ICCV, 2015. 1

[2] C.-C. Chang and C.-J. Lin.

LIBSVM: A library for
support vector machines. ACM Transactions on Intelli-
gent Systems and Technology, 2:27:1–27:27, 2011. Soft-
ware available at http://www.csie.ntu.edu.tw/
˜cjlin/libsvm. 4

[3] K. Chen, R. Kovvuri, J. Gao, and R. Nevatia. MSRC: Mul-
timodal spatial regression with semantic context for phrase
grounding. In ICMR, 2017. 6

[4] S. K. Divvala, D. Hoiem, J. H. Hays, A. A. Efros, and
M. Heber. An empirical study of context in object detection.
In CVPR, 2009. 4

[5] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
and A. Zisserman. The PASCAL Visual Object Classes
Challenge 2012 (VOC2012) Results.
http://www.pascal-
network.org/challenges/VOC/voc2012/workshop/index.html,
2012. 4, 5

[6] H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dol-
lar, J. Gao, X. He, M. Mitchell, J. Platt, L. Zitnick, and
In
G. Zweig. From captions to visual concepts and back.
CVPR, 2015. 1

[7] S. Fidler, A. Sharma, and R. Urtasun. A sentence is worth a

thousand pixels. In CVPR, 2013. 2, 4, 5

[8] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and
M. Rohrbach. Multimodal compact bilinear pooling for vi-
sual question answering and visual grounding. In EMNLP,
2016. 1, 2, 6

[9] R. Girshick. Fast r-cnn. In ICCV, 2015. 4, 5, 6
[10] Y. Gong, Q. Ke, M. Isard, and S. Lazebnik. A multi-view em-
bedding space for modeling internet images, tags, and their
semantics. IJCV, 106(2):210–233, 2014. 4

[11] S. Harabagiu and S. Maiorano. Knowledge-lean corefer-
ence resolution and its relation to textual cohesion and co-
herence. In Proceedings of the ACL-99 Workshop on the re-
lation of discourse/dialogue structure and reference, pages
29–38, 1999. 4

2006. 3, 6, 8

[14] J. Johnson, A. Karpathy, and L. Fei-Fei. Densecap: Fully
convolutional localization networks for dense captioning. In
CVPR, 2016. 1

[15] J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. A. Shamma,
Image retrieval using scene

M. Bernstein, and L. Fei-Fei.
graphs. In CVPR, 2015. 2, 5

[16] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
In CVPR, 2015.

ments for generating image descriptions.
1

[17] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment em-
beddings for bidirectional image sentence mapping. In NIPS,
2014. 1

[18] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg.
Referitgame: Referring to objects in photographs of natural
scenes. In EMNLP, 2014. 1, 2, 4, 5

[19] B. Klein, G. Lev, G. Sadeh, and L. Wolf. Associating neu-
ral word embeddings with deep image representations using
ﬁsher vector. In CVPR, 2015. 4

[20] C. Kong, D. Lin, M. Bansal, R. Urtasun, and S. Fidler. What
are you talking about? text-to-image coreference. In CVPR,
2014. 2

[21] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz,
S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bern-
stein, and L. Fei-Fei. Visual genome: Connecting language
and vision using crowdsourced dense image annotations.
IJCV, 2017. 1

[22] J. C. Lagarias, J. A. Reeds, M. H. Wright, and P. E.
Wright. Convergence properties of the nelder-mead simplex
method in low dimensions. SIAM Journal of Optimization,
9(1):112147, 1998. 3

[23] L.-J. Li, H. Su, Y. Lim, and L. Fei-Fei. Object bank: An
object-level image representation for high-level visual recog-
nition. IJCV, 107(1):20–39, 2014. 4

[24] Y. Li, W. Ouyang, X. Wang, and X. Tang. ViP-CNN: Visual
phrase guided convolutional neural network. In CVPR, 2017.
9

[25] X. Liang, L. Lee, and E. P. Xing. Deep variation-structured
reinforcement learning for visual relationship and attribute
detection. In CVPR, 2017. 9

[43] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From im-
age descriptions to visual denotations: New similarity met-
rics for semantic inference over event descriptions. TACL,
2:67–78, 2014. 1

[44] L. Yu, E. Park, A. C. Berg, and T. L. Berg. Visual Madlibs:
Fill in the blank Image Generation and Question Answering.
In ICCV, 2015. 1

[45] H. Zhang, Z. Kyaw, S.-F. Chang, and T.-S. Chua. Visual
translation embedding network for visual relation detection.
In CVPR, 2017. 8, 9

[46] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object

proposals from edges. In ECCV, 2014. 4, 6

[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Com-
mon objects in context. In ECCV, 2014. 7

[27] C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual rela-
tionship detection with language priors. In ECCV, 2016. 1,
2, 7, 8, 9

[28] L. Ma, Z. Lu, L. Shang, and H. Li. Multimodal convolutional
neural networks for matching image and sentence. In ICCV,
2015. 1

[29] J. Mao, J. Huang, A. Toshev, O. Camburu, A. Yuille, and
K. Murphy. Generation and comprehension of unambiguous
object descriptions. In CVPR, 2016. 1, 2

[30] T. Mikolov, K. Chen, G. Corrado, and J. Dean.

Efﬁ-
cient estimation of word representations in vector space.
arXiv:1301.3781, 2013. 4

[31] R. Mitkov. Robust pronoun resolution with limited knowl-
edge. In Proceedings of the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics and 17th International
Conference on Computational Linguistics-Volume 2, pages
869–875. Association for Computational Linguistics, 1998.
4

[32] J. C. Platt. Probabilistic outputs for support vector machines
and comparisons to regularized likelihood methods. In Ad-
vances in Large Margin Classiﬁers, pages 61–74. MIT Press,
1999. 4, 5

[33] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
to-sentence models. IJCV, 123(1):74–93, 2017. 1, 2, 3, 4, 5,
6, 7, 8

[34] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and
B. Schiele. Grounding of textual phrases in images by re-
construction. In ECCV, 2016. 2, 6, 7

[35] F. Sadeghi, S. K. Divvala, and A. Farhadi. Viske: Visual
knowledge extraction and question answering by visual ver-
iﬁcation of relation phrases. In CVPR, 2015. 5

[36] M. A. Sadeghi and A. Farhadi. Recognition using visual

phrases. In CVPR, 2011. 5

[37] R. Socher, J. Bauer, C. D. Manning, and A. Y. Ng. Parsing
With Compositional Vector Grammars. In ACL, 2013. 4
[38] J. Tighe, M. Niethammer, and S. Lazebnik. Scene pars-
ing with object instances and occlusion ordering. In CVPR,
2014. 3

[39] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.
Selective search for object recognition. IJCV, 104(2), 2013.
6

[40] L. Wang, Y. Li, and S. Lazebnik. Learning deep structure-
In CVPR, 2016. 1, 2,

preserving image-text embeddings.
6

[41] M. Wang, M. Azab, N. Kojima, R. Mihalcea, and J. Deng.
Structured matching for phrase localization. In ECCV, 2016.
2, 6, 7

[42] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov,
R. Zemel, and Y. Bengio. Show, attend and tell: Neural im-
age caption generation with visual attention. In ICML, 2015.
1

A. Visualization of detected relationships (VRD Dataset)

Below are some example detections on the VRD test set. Figure 3 shows some of the highly conﬁdent and correctly
localized detections. We detect different types of relationships - spatial (post, behind, car), (sky, above, laptop), (laptop, on,
table), clothing (person, wear, hat), (person, has, shorts), and actions (person, ride, skateboard).

Figure 3: Highly conﬁdent and correctly localized relationships on the VRD dataset.

Figure 4 shows detections which were marked as negatives by the evaluation code as these relationships were not annotated
in the corresponding images. However, note that these predictions are logically correct. The mouse is indeed next to the laptop
(leftmost, ﬁrst row), and the laptop is under the sky (middle, ﬁrst row). Further, in the leftmost, second row image of Figure 3,
the relationship (person, has, shorts) was marked as present, whereas the middle, second row image in Figure 4 has (person,
has, hat) marked as absent, which indicates a lapse in annotation.

Figure 5 shows examples of wrongly detected relationships. Some of these relationships are logically implausible such as
(hat, hold, surfboard) (leftmost, ﬁrst row), while others such as (jeans, on, table) (middle, ﬁrst row), while plausible, aren’t
contextually true in the image. Other failure modes include incorrect detections such as the sky in the (rightmost, ﬁrst row)
image and the phone in the (leftmost, second row) image.

Figure 4: Plausible and logically correct detected relationships, penalized as negatives due to lack of annotations in the VRD dataset.

Figure 5: Falsely detected relationships on the VRD dataset. Mistakes are either due to incorrect localization of objects, prediction of implausible relation-
ships, contextually incorrect relationships, or a combination of mistakes.

B. List of detector classes from Flickr30k Entities

B.1. Adjectives

people-white 3)

2)
9) wet

1) white
8)
grassy
15) professional 16) brown
22) people-blue
23) male
29) bald
30) cold
36) green
37) young
43) paved
44)
teenage
50) military
51) purple
57) dark
58) beautiful
64) chinese
65)
little
72) wooden
71) colorful
78) multicolored 79) bearded

indian

4)
11)

empty
red

5)
new
12) people-red
19) african
26) blond

female
10) colored
17) people-blond 18) snowy
25) oriental
24)
32) people-yellow 33) shirtless
31) blue
39) dark-haired
38) dirt
46) pink
45) cloudy
53) hard
52) asian
60) adult
59) sandy
67) old
tan
66)
74) plastic
full
73)
81) short
80) huge

40) dark-skinned 41) orange
47) older
rocky
54)
light
61) golden
68) concrete
tall
75)
82) high

48)
55) hooded
62) elderly
69) outdoors
76) striped
top
83)

people-black

6)
black
13) sunny
20)
indoor
27) people-green 28) crowded
34) american

7)
14) smiling
21) gray

35) hot
42) younger
49) urban
56) yellow
63) bright
70)
77) middle-aged

long

B.2. Subject-Verb

2)
animals-catching
1)
7)
animals-holding
6)
animals-sleeping
12)
11)
bodyparts-holding 17)
16)
22)
clothing-eating
21)
27)
clothing-posing
26)
32)
clothing-sitting
31)
37)
clothing-walking
36)
42)
other-eating
41)
47)
other-playing
46)
52)
other-running
51)
57)
other-standing
56)
62)
other-writing
61)
67)
people-cooking
66)
72)
people-drinking
71)
77)
people-ﬁshing
76)
82)
people-hugging
81)
87)
people-kneeling
86)
92)
people-pointing
91)
97)
96)
people-riding
102) people-skiing
101) people-sitting
106) people-smoking
107) people-splashing
111) people-swimming 112) people-swinging
116) people-walking
121) scene-holding
126) scene-standing
131) vehicles-running
136) playing
141)
jumping
146) posing
151) smiling
156)
throwing
161) cleaning
166) hit
171) pointing
176) writing
181) digging
186) sweeping
191)

4)
animals-digging
animals-climbing 3)
9)
animals-playing
animals-jumping
8)
14)
animals-standing
animals-splashing 13)
19)
bodyparts-walking
18)
bodyparts-sitting
24)
clothing-jumping
23)
clothing-holding
29)
clothing-riding
28)
clothing-reading
clothing-smiling
34)
33)
clothing-sleeping
instruments-singing 39)
38)
clothing-working
44)
other-holding
43)
other-ﬂying
49)
other-posing
48)
other-pointing
54)
other-sitting
53)
other-singing
59)
other-throwing
58)
other-talking
64)
people-catching
63)
people-blowing
69)
people-dancing
68)
people-cutting
74)
people-eating
73)
people-driving
79)
people-hiking
78)
people-ﬂying
84)
people-jumping
83)
people-juggling
89)
people-painting
88)
people-laughing
94)
people-pushing
93)
people-posing
99)
98)
people-serving
people-running
104) people-sliding
103) people-sleeping
109) people-surﬁng
108) people-standing
114) people-throwing
113) people-talking
119) people-writing
118) people-working
124) scene-running
123) scene-reading
129) vehicles-driving
128) scene-walking
134) sitting
133) vehicles-throwing
139)
138) walking
144) performing
143)
talking
149)
148) hiking
reading
154) pushing
153) sleeping
159) cooking
158) driving
164)
163) swinging
laughing
169) ﬂying
168)
juggling
174) drinking
173) sliding
179) kneeling
178) catching
184) surﬁng
183) shopping
189) drawing
reaching
188)

animals-ﬂying
5)
animals-ﬁghting
animals-sitting
10)
animals-running
animals-walking
15)
animals-swimming
clothing-dancing
clothing-climbing
20)
clothing-playing
clothing-performing 25)
clothing-singing
30)
clothing-running
clothing-talking
35)
clothing-standing
other-drinking
40)
other-cooking
other-performing
45)
other-jumping
other-riding
50)
other-reading
other-smiling
55)
other-sleeping
other-working
60)
other-walking
people-climbing
65)
people-cleaning
people-drawing
70)
people-digging
people-ﬁghting
75)
people-falling
people-holding
80)
people-hit
people-kissing
85)
people-kicking
people-playing
90)
people-performing
95)
people-reaching
people-reading
100) people-singing
people-shopping
105) people-smiling
110) people-sweeping
115) people-touches
120) scene-eating
125) scene-sitting
130) vehicles-holding
135) holding
140)
riding
145) eating
150) dancing
155) swimming
160) cutting
165) kicking
170) kissing
175) ﬁshing
180) hugging
185) waving
190) splashing

117) people-waving
122) scene-playing
127) scene-talking
132) vehicles-sitting
137) standing
142) working
147) climbing
152) singing
157) painting
162) serving
167) ﬁghting
172) blowing
177) skiing
182) smoking
187)

running

touches

falling

B.3. Verb-Object

1)
5)
9)
13)
17)
21)
25)
29)
33)
37)
41)
45)
49)
53)
57)
61)
65)
69)
73)
77)
81)
85)
89)
93)
97)
101)
105)
109)
113)
117)
121)
125)
129)
133)
137)
141)
145)
149)
153)
157)
161)
165)
169)
173)
177)
181)
185)
189)
193)
197)
201)
205)
209)
213)
217)
221)
225)

other-blowing
scene-cleaning
bodyparts-cooking
clothing-dancing
other-digging
scene-drinking
other-eating
scene-falling
scene-ﬂying
animals-holding
other-holding
people-hugging
other-jumping
other-kicking
other-kneeling
other-painting
people-performing
instruments-playing
vehicles-playing
scene-pointing
people-posing
vehicles-pushing
people-reading
scene-riding
clothing-running
vehicles-running
instruments-singing
bodyparts-sitting
people-sitting
bodyparts-sleeping
other-sliding
other-smiling
scene-splashing
other-standing
scene-surﬁng
scene-swimming
people-talking
scene-throwing
bodyparts-walking
scene-walking
people-waving
scene-working
holding
running
talking
climbing
smiling
swimming
cooking
swinging
ﬁghting
pointing
ﬁshing
kneeling
shopping
falling
touches

other-catching
2)
bodyparts-climbing
6)
other-cooking
10)
other-dancing
14)
scene-digging
18)
other-driving
22)
people-eating
26)
other-ﬁghting
30)
scene-hiking
34)
bodyparts-holding
38)
people-holding
42)
other-juggling
46)
people-jumping
50)
people-kicking
54)
scene-kneeling
58)
scene-painting
62)
scene-performing
66)
other-playing
70)
bodyparts-pointing
74)
bodyparts-posing
78)
scene-posing
82)
other-reaching
86)
animals-riding
90)
vehicles-riding
94)
other-running
98)
other-serving
102)
other-singing
106)
clothing-sitting
110)
scene-sitting
114)
other-sleeping
118)
scene-sliding
122)
people-smiling
126)
animals-standing
130)
people-standing
134)
other-sweeping
138)
other-swinging
142)
scene-talking
146)
bodyparts-touches
150)
clothing-walking
154)
vehicles-walking
158)
clothing-working
162)
vehicles-working
166)
playing
170)
riding
174)
performing
178)
hiking
182)
singing
186)
throwing
190)
cutting
194)
laughing
198)
juggling
202)
206)
blowing
210) writing
hugging
214)
surﬁng
218)
reaching
222)

scene-catching
3)
other-climbing
7)
bodyparts-cutting
11)
people-dancing
15)
other-drawing
19)
scene-driving
23)
scene-eating
27)
scene-ﬁshing
31)
other-hit
35)
clothing-holding
39)
scene-holding
43)
animals-jumping
47)
scene-jumping
51)
people-kissing
55)
other-laughing
59)
instruments-performing
63)
animals-playing
67)
people-playing
71)
other-pointing
75)
clothing-posing
79)
other-pushing
83)
scene-reaching
87)
other-riding
91)
animals-running
95)
people-running
99)
people-serving
103)
people-singing
107)
instruments-sitting
111)
vehicles-sitting
115)
people-sleeping
119)
bodyparts-smiling
123)
scene-smiling
127)
bodyparts-standing
131)
scene-standing
135)
scene-sweeping
139)
clothing-talking
143)
other-throwing
147)
other-touches
151)
other-walking
155)
bodyparts-waving
159)
other-working
163)
other-writing
167)
standing
171)
jumping
175)
eating
179)
reading
183)
sleeping
187)
painting
191)
cleaning
195)
kicking
199)
ﬂying
203)
sliding
207)
skiing
211)
215)
digging
219) waving
drawing
223)

other-cleaning
scene-climbing
other-cutting
scene-dancing
other-drinking
vehicles-driving
other-falling
other-ﬂying
people-hit
instruments-holding
vehicles-holding
bodyparts-jumping
vehicles-jumping
scene-kissing
people-laughing
other-performing
clothing-playing
scene-playing
people-pointing
other-posing
people-pushing
other-reading
people-riding
bodyparts-running
scene-running
other-shopping
animals-sitting
other-sitting
scene-skiing
scene-sleeping
clothing-smiling
other-smoking
clothing-standing
vehicles-standing
other-swimming
other-talking
people-throwing
animals-walking
people-walking
other-waving
people-working
sitting

4)
8)
12)
16)
20)
24)
28)
32)
36)
40)
44)
48)
52)
56)
60)
64)
68)
72)
76)
80)
84)
88)
92)
96)
100)
104)
108)
112)
116)
120)
124)
128)
132)
136)
140)
144)
148)
152)
156)
160)
164)
168)
172) walking
176) working
180)
184)
188)
192)
196)
200)
204)
208)
212)
216)
220)
224)

posing
dancing
pushing
driving
serving
hit
kissing
drinking
catching
smoking
sweeping
splashing

C. List of phrase-pair relationships from Flickr30k Entities
C.1. Verbs

1)
5)
9)
13)
17)
21)
25)
29)
33)
37)
41)
45)
49)
53)
57)
61)
65)
69)
73)
77)
81)
85)
89)
93)
97)
101) people-playing-keyboard
105) people-playing-sand
109) people-playing-stage
113) people-playing-trumpet
117) people-posing-people
121) people-pushing-stroller
125) people-reading-paper
129) people-riding-bikes
133) people-riding-horses
137) people-riding-scooter
141) people-riding-unicycle
145) people-running-ﬁeld
149) people-running-sidewalk
153) people-serving-food
157) people-sitting-beach
161) people-sitting-bike
165) people-sitting-chair
169) people-sitting-desk
173) people-sitting-horse
177) people-sitting-people
181) people-sitting-steps
185) people-sitting-table
189) people-sitting-water
193) people-sleeping-grass
197) people-smiling-people
201) people-standing-bridge
205) people-standing-door
209) people-standing-grass
213) people-standing-platform
217) people-standing-rocks
221) people-standing-stage
225) people-standing-wall
229) people-swinging-bat
233) people-talking-people
237) people-throwing-people
241) people-walking-bridge
245) people-walking-dogs
249) people-walking-path
253) people-walking-snow
257) people-walking-wall

dog-holding-stick
2)
dog-catching-frisbee
3)
dog-jumping-people
6)
dog-jumping-hurdle
7)
dog-running-ﬁeld
10)
dog-running-beach
11)
dog-swimming-water
14)
dog-running-water
15)
dogs-running-grass
18)
dogs-running-ﬁeld
19)
people-cleaning-dishes
22)
people-catching-wave
23)
people-climbing-rocks
26)
people-climbing-rock+wall
27)
people-cutting-cake
30)
people-cooking-food
31)
people-drinking-beer
34)
people-digging-snow
35)
people-hit-ball
38)
people-eating-table
39)
people-holding-box
42)
people-holding-book
43)
people-holding-drink
46)
people-holding-dog
47)
people-holding-football
50)
people-holding-ﬂowers
51)
people-holding-people
54)
people-holding-object
55)
people-holding-signs
58)
people-holding-sign
59)
people-hugging-people
people-holding-tennis+racket 62)
63)
people-jumping-hurdle
66)
people-jumping-bike
67)
people-jumping-rock
70)
people-jumping-ramp
71)
people-kicking-ball
74)
people-jumping-water
75)
people-laughing-people
78)
people-kissing-people
79)
people-playing-accordion
82)
people-performing-stage
83)
people-playing-beach
86)
people-playing-basketball
87)
people-playing-drum
90)
people-playing-dog
91)
people-playing-fountain
94)
people-playing-football
95)
98)
people-playing-guitars
people-playing-guitar
99)
103) people-playing-piano
102) people-playing-people
106) people-playing-saxophone 107) people-playing-snow
110) people-playing-swing
114) people-playing-violin
118) people-posing-picture
122) people-reading-book
126) people-riding-bicycle
130) people-riding-bull
134) people-riding-motorbike
138) people-riding-skateboard
142) people-riding-wave
146) people-running-grass
150) people-running-street
154) people-singing-guitar
158) people-sitting-bed
162) people-sitting-blanket
166) people-sitting-chairs
170) people-sitting-dock
174) people-sitting-ledge
178) people-sitting-rock
182) people-sitting-stool
186) people-sitting-tables
190) people-sleeping-bench
194) people-sleeping-people
198) people-smoking-cigarette
202) people-standing-building
206) people-standing-doorway
210) people-standing-ladder
214) people-standing-podium
218) people-standing-sidewalk
222) people-standing-street
226) people-standing-water
230) people-swinging-swing
234) people-talking-phone
238) people-walking-beach
242) people-walking-building
246) people-walking-ﬁeld
250) people-walking-people
254) people-walking-stairs
258) people-walking-water

dog-jumping-frisbee
4)
dog-jumping-ball
dog-playing-ball
8)
dog-jumping-water
dog-running-snow
12)
dog-running-grass
dogs-playing-snow
16)
dogs-playing-grass
people-catching-ball
20)
people-blowing-bubbles
people-climbing-rock
24)
people-climbing-mountain
people-climbing-wall
28)
people-climbing-tree
people-dancing-stage
32)
people-dancing-people
people-eating-meal
36)
people-eating-food
people-holding-ball
40)
people-hit-tennis+ball
people-holding-cup
44)
people-holding-camera
people-holding-ﬂags
48)
people-holding-ﬂag
people-holding-microphone
52)
people-holding-guitar
people-holding-shovel
56)
people-holding-rope
people-holding-stick
60)
people-holding-something
people-jumping-bed
64)
people-jumping-ball
people-jumping-pool
people-jumping-people
68)
people-jumping-trampoline
people-jumping-swimming+pool 72)
people-kicking-soccer+ball
76)
people-kicking-people
people-performing-people
80)
people-painting-picture
people-playing-ball
84)
people-playing-bagpipes
people-playing-cello
88)
people-playing-board+game
people-playing-ﬂute
92)
people-playing-drums
96)
people-playing-frisbee
people-playing-game
100) people-playing-instruments
people-playing-instrument
104) people-playing-pool
108) people-playing-soccer
112) people-playing-toys
116) people-playing-water
120) people-pushing-people
124) people-reading-newspaper
128) people-riding-bike
132) people-riding-horse
136) people-riding-people
140) people-riding-surfboard
144) people-running-beach
148) people-running-road
152) people-running-water
156) people-singing-people
160) people-sitting-benches
164) people-sitting-building
168) people-sitting-curb
172) people-sitting-grass
176) people-sitting-park+bench
180) people-sitting-sidewalk
184) people-sitting-swing
188) people-sitting-wall
192) people-sleeping-couch
196) people-sliding-slide
200) people-standing-boat
204) people-standing-counter
208) people-standing-ﬁeld
212) people-standing-people
216) people-standing-rock
220) people-standing-snow
224) people-standing-tree
228) people-swimming-pool
232) people-talking-microphone
236) people-throwing-frisbee
240) people-walking-bike
244) people-walking-dog
248) people-walking-hill
252) people-walking-sidewalk
256) people-walking-trail
260) people-working-people

111) people-playing-toy
115) people-playing-volleyball
119) people-pushing-cart
123) people-reading-magazine
127) people-riding-bicycles
131) people-riding-dirt+bike
135) people-riding-motorcycle
139) people-riding-street
143) people-running-ball
147) people-running-people
151) people-running-track
155) people-singing-microphone
159) people-sitting-bench
163) people-sitting-boat
167) people-sitting-couch
171) people-sitting-ﬂoor
175) people-sitting-motorcycle
179) people-sitting-rocks
183) people-sitting-street
187) people-sitting-tree
191) people-sleeping-chair
195) people-sliding-base
199) people-standing-beach
203) people-standing-car
207) people-standing-fence
211) people-standing-line
215) people-standing-road
219) people-standing-sign
223) people-standing-table
227) people-surﬁng-wave
231) people-talking-cellphone
235) people-throwing-ball
239) people-walking-bicycle
243) people-walking-city+street
247) people-walking-grass
251) people-walking-road
255) people-walking-street
259) people-working-machine

C.2. Prepositions

1)
5)
9)
13)
17)
21)
25)
29)
33)
37)
41)
45)
49)
53)
57)
61)
65)
69)
73)
77)
81)
85)
89)
93)
97)
101)
105)
109)
113)
117)
121)
125)
129)
133)
137)
141)
145)
149)
153)
157)
161)
165)
169)
173)
177)
181)
185)
189)
193)
197)
201)
205)
209)
213)

ball-in-mouth
dog-in-ball
dog-in-grass
dog-in-water
dog-on-leash
dogs-in-snow
hands-in-people
people-across-street
people-behind-people
people-in-ball
people-in-blanket
people-in-camera
people-in-cart
people-in-colors
people-in-doorway
people-in-football
people-in-guitar
people-in-kitchen
people-in-mirror
people-in-park
people-in-room
people-in-street
people-in-towel
people-in-tub
people-near-beach
people-near-fence
people-near-pole
people-near-table
people-near-window
people-on-bed
people-on-bicycles
people-on-board
people-on-bus
people-on-city+street
people-on-curb
people-on-ﬁeld
people-on-hill
people-on-ladder
people-on-mat
people-on-park+bench
people-on-phone
people-on-raft
people-on-road
people-on-rope
people-on-scooter
people-on-skateboard
people-on-soccer+ﬁeld
people-on-step
people-on-street
people-on-tire+swing
people-on-trampoline
people-on-water
something-in-mouth
tattoo-on-people

2)
6)
10)
14)
18)
22)
26)
30)
34)
38)
42)
46)
50)
54)
58)
62)
66)
70)
74)
78)
82)
86)
90)
94)
98)
102)
106)
110)
114)
118)
122)
126)
130)
134)
138)
142)
146)
150)
154)
158)
162)
166)
170)
174)
178)
182)
186)
190)
194)
198)
202)
206)
210)
214)

bicycle-on-street
dog-in-collar
dog-in-snow
dog-on-beach
dogs-in-dogs
dogs-in-water
object-in-mouth
people-behind-building
people-between-people
people-in-bed
people-in-boat
people-in-cane
people-in-chair
people-in-dirt
people-in-face+paint
people-in-fountain
people-in-highchair
people-in-lake
people-in-mud
people-in-people
people-in-sand
people-in-stroller
people-in-toy
people-in-water
people-near-brick+wall
people-near-fountain
people-near-road
people-near-tree
people-on-back
people-on-bench
people-on-bike
people-on-boat
people-on-cellphone
people-on-cliff
people-on-deck
people-on-ﬂoor
people-on-horse
people-on-lawn
people-on-motorcycle
people-on-path
people-on-pier
people-on-rail
people-on-rock
people-on-sand
people-on-shore
people-on-sled
people-on-sofa
people-on-steps
people-on-surfboard
people-on-track
people-on-tree
people-on-wave
stick-in-mouth
tennis+ball-in-mouth

3)
7)
11)
15)
19)
23)
27)
31)
35)
39)
43)
47)
51)
55)
59)
63)
67)
71)
75)
79)
83)
87)
91)
95)
99)
103)
107)
111)
115)
119)
123)
127)
131)
135)
139)
143)
147)
151)
155)
159)
163)
167)
171)
175)
179)
183)
187)
191)
195)
199)
203)
207)
211)
215)

boat-in-water
dog-in-dog
dog-in-stick
dog-on-grass
dogs-in-ﬁeld
dogs-on-grass
one-in-shirt
people-behind-counter
people-in-area
people-in-bicycle
people-in-body+water
people-in-canoe
people-in-chairs
people-in-dog
people-in-ﬁeld
people-in-gear
people-in-instruments
people-in-line
people-in-number
people-in-pool
people-in-snow
people-in-swimming+pool
people-in-toys
people-in-wheelchair
people-near-building
people-near-lake
people-near-sidewalk
people-near-wall
people-on-balcony
people-on-benches
people-on-bikes
people-on-bridge
people-on-chair
people-on-computer
people-on-dock
people-on-grass
people-on-horses
people-on-ledge
people-on-motorcycles
people-on-pavement
people-on-platform
people-on-railing
people-on-rocks
people-on-scaffold
people-on-side+road
people-on-slide
people-on-stage
people-on-stilts
people-on-swing
people-on-trail
people-on-walkway
people-under-tree
street-in-people
toy-in-mouth

building-in-people
4)
dog-in-ﬁeld
8)
dog-in-toy
12)
dog-on-hind+legs
16)
dogs-in-grass
20)
guitar-in-people
24)
other-in-shirt
28)
people-behind-fence
32)
people-in-back
36)
people-in-bike
40)
people-in-building
44)
people-in-car
48)
people-in-cigarette
52)
people-in-dogs
56)
people-in-ﬂowers
60)
people-in-grass
64)
people-in-kayak
68)
people-in-microphone
72)
people-in-ocean
76)
people-in-river
80)
people-in-soccer+ball
84)
people-in-swing
88)
people-in-tree
92)
people-in-yard
96)
people-near-car
100)
people-near-people
104)
people-near-street
108)
people-near-water
112)
people-on-beach
116)
people-on-bicycle
120)
people-on-blanket
124)
people-on-building
128)
people-on-chairs
132)
people-on-couch
136)
people-on-fence
140)
people-on-grill
144)
people-on-ice
148)
people-on-machine
152)
people-on-mountain
156)
people-on-people
160)
people-on-porch
164)
people-on-ramp
168)
people-on-roof
172)
people-on-scaffolding
176)
people-on-sidewalk
180)
people-on-snowboard
184)
people-on-stairs
188)
people-on-stool
192)
people-on-table
196)
people-on-train
200)
people-on-wall
204)
shirt-in-people
208)
212)
table-in-people
216) wall-in-grafﬁti

C.3. Clothing and Body Part Attachment

1)
5)
9)
13)
17)
21)
25)
29)
33)
37)
41)
45)
49)
53)
57)
61)
65)
69)
73)
77)
81)
85)
89)
93)
97)
101)
105)
109)
113)
117)
121)
125)
129)
133)
137)
141)
145)
149)
153)
157)
161)
165)
169)
173)
177)
181)
185)
189)
193)
197)
201)
205)

people-apron
people-backpack
people-ball+cap
people-bathing+suit
people-beret
people-black+shirt
people-blue
people-brown+jacket
people-camouﬂage
people-clothing
people-costume
people-curly+hair
people-dress+shirt
people-faces
people-ﬂip-ﬂops
people-goggles
people-hair
people-harness
people-headband
people-heels
people-jacket
people-jersey
people-kilt
people-leather+jacket
people-life+jacket
people-mohawk
people-nose
people-orange+jacket
people-outﬁt
people-pants
people-pink+coat
people-pink+outﬁt
people-plaid+shirt
people-purple+shirt
people-red-hair
people-rock+face
people-scarf
people-shoe
people-shorts
people-skirts
people-snowshoes
people-striped+shirt
people-suspenders
people-swimming+trunks
people-t-shirts
people-tank
people-teeth
people-top
people-tuxedo
people-uniform
people-wedding+dress
people-winter+clothes

2)
6)
10)
14)
18)
22)
26)
30)
34)
38)
42)
46)
50)
54)
58)
62)
66)
70)
74)
78)
82)
86)
90)
94)
98)
102)
106)
110)
114)
118)
122)
126)
130)
134)
138)
142)
146)
150)
154)
158)
162)
166)
170)
174)
178)
182)
186)
190)
194)
198)
202)
206)

people-aprons
people-backpacks
people-bandanna
people-bathing+suits
people-bikini
people-black+white
people-body
people-brown+shirt
people-cap
people-coat
people-costumes
people-denim+jacket
people-dresses
people-feet
people-garb
people-gold
people-haircut
people-hat
people-headphones
people-helmet
people-jackets
people-jerseys
people-knees
people-leg
people-life+jackets
people-mouth
people-orange
people-orange+shirt
people-outﬁts
people-people
people-pink+dress
people-pink+pants
people-polo+shirt
people-purse
people-ring
people-safety+vest
people-scrubs
people-shoes
people-shoulder
people-sleeveless+shirt
people-snowsuit
people-suit
people-sweater
people-swimsuit
people-tan+jacket
people-tank+top
people-thumbs
people-tops
people-umbrella
people-uniforms
people-wetsuit
people-winter+clothing

3)
7)
11)
15)
19)
23)
27)
31)
35)
39)
43)
47)
51)
55)
59)
63)
67)
71)
75)
79)
83)
87)
91)
95)
99)
103)
107)
111)
115)
119)
123)
127)
131)
135)
139)
143)
147)
151)
155)
159)
163)
167)
171)
175)
179)
183)
187)
191)
195)
199)
203)
207)

people-arms
people-bag
people-baseball+cap
people-beanie
people-bikinis
people-blond-hair
people-boots
people-business+attire
people-checkered+shirt
people-coats
people-cowboy+hat
people-dreadlocks
people-eyes
people-ﬁnger
people-glasses
people-gray
people-hand
people-hats
people-heads
people-helmets
people-jean+shorts
people-jumpsuit
people-lab+coat
people-legs
people-makeup
people-mustache
people-orange+dress
people-orange+vest
people-overalls
people-pigtails
people-pink+hat
people-pink+shirt
people-ponytail
people-red
people-robe
people-safety+vests
people-shirt
people-shopping+bag
people-shoulders
people-smile
people-socks
people-suits
people-sweatshirt
people-swimsuits
people-tan+pants
people-tattoo
people-tie
people-trunks
people-umbrellas
people-vest
people-white
people-yellow

4)
8)
12)
16)
20)
24)
28)
32)
36)
40)
44)
48)
52)
56)
60)
64)
68)
72)
76)
80)
84)
88)
92)
96)
100)
104)
108)
112)
116)
120)
124)
128)
132)
136)
140)
144)
148)
152)
156)
160)
164)
168)
172)
176)
180)
184)
188)
192)
196)
200)
204)

people-attire
people-bags
people-baseball+uniform
people-beard
people-black
people-blouse
people-brown
people-business+suit
people-clothes
people-collared+shirt
people-cowboy+hats
people-dress
people-face
people-ﬁngers
people-gloves
people-green
people-hands
people-head
people-headscarf
people-hoodie
people-jeans
people-khaki+pants
people-lap
people-leotard
people-mask
people-necklace
people-orange+hat
people-orange+vests
people-pajamas
people-pink
people-pink+jacket
people-pink+sweater
people-purple
people-red+white
people-robes
people-sandals
people-shirts
people-shopping+bags
people-skirt
people-sneakers
people-straw+hat
people-sunglasses
people-swim+trunks
people-t-shirt
people-tan+shirt
people-tattoos
people-tongue
people-turban
people-underwear
people-vests
people-wig

