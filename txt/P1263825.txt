8
1
0
2
 
t
c
O
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
6
9
5
9
0
.
2
0
8
1
:
v
i
X
r
a

Tunability: Importance of Hyperparameters of Machine
Learning Algorithms

by Philipp Probst, Anne-Laure Boulesteix and Bernd Bischl

October 23, 2018

Abstract

Modern supervised machine learning algorithms involve hyperparameters that have to
be set before running them. Options for setting hyperparameters are default values from
the software package, manual conﬁguration by the user or conﬁguring them for optimal
predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly,
we formalize the problem of tuning from a statistical point of view, deﬁne data-based de-
faults and suggest general measures quantifying the tunability of hyperparameters of al-
gorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets
from the OpenML platform and six common machine learning algorithms. We apply our
measures to assess the tunability of their parameters. Our results yield default values for
hyperparameters and enable users to decide whether it is worth conducting a possibly time
consuming tuning strategy, to focus on the most important hyperparameters and to chose
adequate hyperparameter spaces for tuning.

1

Introduction

Machine learning (ML) algorithms such as gradient boosting, random forest and neural net-
works for regression and classiﬁcation involve a number of hyperparameters that have to
be set before running them. In contrast to direct, ﬁrst-level model parameters, which are
determined during training, these second-level tuning parameters often have to be carefully
optimized to achieve maximal performance. A related problem exists in many other algo-
rithmic areas, e.g., control parameters in evolutionary algorithms (Eiben and Smit, 2011).
In order to select an appropriate hyperparameter conﬁguration for a speciﬁc dataset
at hand, users of ML algorithms can resort to default values of hyperparameters that are
speciﬁed in implementing software packages or manually conﬁgure them, for example, based
on recommendations from the literature, experience or trial-and-error.

Alternatively, one can use hyperparameter tuning strategies, which are data-dependent,
second-level optimization procedures (Guyon et al., 2010), which try to minimize the ex-
pected generalization error of the inducing algorithm over a hyperparameter search space
of considered candidate conﬁgurations, usually by evaluating predictions on an independent
test set, or by running a resampling scheme such as cross-validation (Bischl et al., 2012).
For a recent overview of tuning strategies, see, e.g., Luo (2016).

These search strategies range from simple grid or random search (Bergstra and Bengio,
2012) to more complex, iterative procedures such as Bayesian optimization (Hutter et al.,
2011; Snoek et al., 2012; Bischl et al., 2017) or iterated F-racing (Birattari et al., 2010;
Lang et al., 2017).

In addition to selecting an eﬃcient tuning strategy, the set of tunable hyperparameters
and their corresponding ranges, scales and potential prior distributions for subsequent sam-
pling have to be determined by the user. Some hyperparameters might be safely set to
default values, if they work well across many diﬀerent scenarios. Wrong decisions in these
areas can inhibit either the quality of the resulting model or at the very least the eﬃciency
and fast convergence of the tuning procedure. This creates a burden for:

1

1. ML users – Which hyperparameters should be tuned and in which ranges?

2. Designers of ML algorithms – How do I deﬁne robust defaults?

We argue that many users, especially if they do not have years of practical experience in
the ﬁeld, here often rely on heuristics or spurious knowledge. It should also be noted that
designers of fully automated tuning frameworks face at least very similar problems. It is
not clear how these questions should be addressed in a data-dependent, automated, optimal
and objective manner. In other words, the scientiﬁc community not only misses answers to
these questions for many algorithms but also a systematic framework, methods and criteria,
which are required to answer these questions.

With the present paper we aim at ﬁlling this gap and formalize the problem of parameter
tuning from a statistical point of view, in order to simplify the tuning process for less
experienced users and to optimize decision making for more advanced processes.

After presenting related literature in section 2, we deﬁne theoretical measures for assess-
ing the impact of tuning in section 3. For this purpose we (i) deﬁne the concept of default
hyperparameters, (ii) suggest measures for quantiﬁying the tunability of the whole algorithm
and speciﬁc hyperparameters based on the diﬀerences between the performance of default
hyperparameters and the performance of the hyperparameters when this hyperparameter is
set to an optimal value. Then we (iii) address the tunability of hyperparameter combinations
and joint gains, (iv) provide theoretical deﬁnitions for an appropriate hyperparameter space
on which tuning should be executed and (v) propose procedures to estimate these quantities
based on the results of a benchmark study with random hyperparameter conﬁgurations with
the help of surrogate models. In sections 4 and 5 we illustrate these concepts and methods
through an application. For this purpose we use benchmark results of six machine learning
algorithms with diﬀerent hyperparameters which were evaluated on 38 datasets from the
OpenML platform. Finally, in the last section 6 we conclude and discuss the results.

2 Related literature

To the best of our knowledge, only a limited amount of articles address the problem of
tunability and generation of tuning search spaces. Bergstra and Bengio (2012) compute the
relevance of the hyperparameters of neural networks and conclude that some are important
on all datasets, while others are only important on some datasets. Their conclusion is
primarily visual and used as an argument for why random search works better than grid
search when tuning neural networks.

A speciﬁc study for decision trees was conducted by Mantovani et al. (2016) who ap-
ply standard tuning techniques to decision trees on 102 datasets and calculate diﬀerences
of accuracy between the tuned algorithm and the algorithm with default hyperparameter
settings.

A diﬀerent approach is proposed by Hutter et al. (2013), which aims at identifying the
most important hyperparameters via forward selection. In the same vein, Fawcett and Hoos
(2016) present an ablation analysis technique, which aims at identifying the hyperparameters
that contribute the most to improved performance after tuning. For each of the considered
hyperparameters, they compute the performance gain that can be achieved by changing
its value from the initial value to the value speciﬁed in the target conﬁguration which was
determined by the tuning strategy. This procedure is iterated in a greedy forward search.

A more general framework for measuring the importance of single hyperparameters is
presented by Hutter et al. (2014). After having used a tuning strategy such as sequential
model-based optimization, a functional ANOVA approach is used for measuring the impor-
tance of hyperparameters.

These works concentrate on the importance of hyperparameters on single datasets,
mainly to retrospectively explain what happened during an already concluded tuning pro-
cess. Our main focus is the generalization across multiple datasets in order to facilitate
better general understanding of hyperparameter eﬀects and better decision making for fu-
ture experiments. In a recent paper van Rijn and Hutter (2017) pose very similar questions
to ours to assess the importance of hyperparameters across datasets. We compare it to our
approach in section 6.

2

Our framework is based on using surrogate models, also sometimes called empirical
performance models, which allow estimating the performance of arbitrary hyperparameter
conﬁgurations based on a limited number of prior experiments. The idea of surrogate models
is far from new, as it constitutes the central idea of Bayesian optimization for hyperparameter
search but is also used, for example, in Biedenkapp et al. (2017) for increasing the speed of
an ablation analysis and by Eggensperger et al. (2018) for speeding up the benchmarking of
tuning strategies.

3 Methods for Estimation of Defaults, Tunability and
Ranges

3.1 General notation

Consider a target variable Y , a feature vector X, and an unknown joint distribution P on
(X, Y ), from which we have sampled a dataset T of n observations. A machine learning (ML)
algorithm now learns the functional relationship between X and Y by producing a prediction
model ˆf (X, θ), controlled by the k-dimensional hyperparameter conﬁguration θ = (θ1, ..., θk)
from the hyperparameter search space Θ = Θ1 × ... × Θk. In order to measure prediction
performance pointwise between the true label Y and its prediction ˆf (X, θ), we deﬁne a loss
function L(Y, ˆf (X, θ)). We are naturally interested in estimating the expected risk of the
inducing algorithm, w.r.t. θ on new data, also sampled from P: R(θ) = E(L(Y, ˆf (X, θ))|P).
This mapping encodes, given a certain data distribution, a certain learning algorithm and a
certain performance measure, the numerical quality for any hyperparameter conﬁguration θ.
Given m diﬀerent datasets (or data distributions) P1, ..., Pm, we arrive at m hyperparameter
risk mappings

R(j)(θ) := E(L(Y, ˆf (X, θ))|Pj),

j = 1, ..., m.

For now, we assume all R(j)(θ) to be known, and show how to estimate them in section 3.7.

3.2 Optimal conﬁguration per dataset and optimal defaults

We ﬁrst deﬁne the best hyperparameter conﬁguration for dataset j as

θ(j)(cid:63) := arg min

R(j)(θ).

θ∈Θ

Defaults settings are supposed to work well across many diﬀerent datasets and are usually
provided by software packages, in an often ad hoc or heuristic manner. We propose to deﬁne
an optimal default conﬁguration, based on an extensive number of empirical experiments on
m diﬀerent benchmark datasets, by

(1)

(2)

(3)

θ(cid:63) := arg min

g(R(1)(θ), ..., R(m)(θ)).

θ∈Θ

Here, g is a summary function that has to be speciﬁed. Selecting the mean (or median
as a more robust candidate) would imply minimizing the average (or median) risk over all
datasets.

The measures R(j)(θ) could potentially be scaled appropriately beforehand in order to
make them more commensurable between datasets, e.g., one could scale all R(j)(θ) to [0, 1]
by substracting the result of a very simple baseline like a featureless dummy predictor and
dividing this diﬀerence by the absolute diﬀerence between the risk of the best possible
result (as an approximation of the Bayes error) and the result of the very simple baseline
predictor. Or one could produce a statistical z-score by subtracting the mean and dividing
by the standard deviation from all experimental results on the same dataset (Feurer et al.,
2018).

The appropriateness of the scaling highly depends on the performance measure that
is used. One could, for example, argue that the AUC does not have to be scaled as an
improvement from 0.5 to 0.6 can possibly be seen as important as an improvement from 0.8
to 0.9. On the other hand, averaging the mean squared error on several datasets does not

3

make a lot of sense, as the scale of the outcome of diﬀerent regression problems can be very
diﬀerent. Then scaling or using another measure such as R2 seems essential.

3.3 Measuring overall tunability of a ML algorithm

A general measure of the tunability of an algorithm per dataset can then be computed
based on the diﬀerence between the risk of an overall reference conﬁguration (e.g., either the
software defaults or deﬁnition (3)) and the risk of the best possible conﬁguration on that
dataset:

d(j) := R(j)(θ(cid:63)) − R(j)(θ(j)(cid:63)), for j = 1, ..., m.

(4)

For each algorithm, this gives rise to an empirical distribution of performance diﬀerences
over datasets, which might be directly visualized or summarized to an aggregated tunability
measure d by using mean, median or quantiles.

3.4 Measuring tunability of a speciﬁc hyperparameter

The best hyperparameter value for one parameter i on dataset j, when all other parameters
are set to defaults from θ(cid:63) := (θ(cid:63)
k), is denoted by

1, ..., θ(cid:63)

θ(j)(cid:63)
i

:= arg min
θ∈Θ,θl=θ(cid:63)

l ∀l(cid:54)=i

R(j)(θ).

A natural measure for tunability of the i-th parameter on dataset j is then the diﬀerence

in risk between the above and our default reference conﬁguration:
:= R(j)(θ(cid:63)) − R(j)(θ(j)(cid:63)

), for j = 1, ..., m, i = 1, ..., k.

d(j)
i

i

Furthermore, we deﬁne d(j),rel

d(j) as the fraction of performance gain, when we only
tune i compared to tuning the complete algorithm, on dataset j. Again, one can calculate
the mean, the median or quantiles of these two diﬀerences over the n datasets, to get a
notion of the overall tunability di of this parameter.

i

i

= d(j)

3.5 Tunability of hyperparamater combinations and joint gains

Let us now consider two hyperparameters indexed as i1 and i2. To measure the tunability
with respect to these two parameters, we deﬁne

θ(j)(cid:63)
i1,i2

:=

arg min

R(j)(θ),

θ∈Θ,θl=θ(cid:63)

l ∀l(cid:54)∈{i1,i2}

i.e., the θ-vector containing the default values for all hyperparameters other than i1 and i2,
and the optimal combination of values for the i1-th and i2-th components of θ.

Analogously to the previous section, we can now deﬁne the tunability of the set (i1, i2)

as the gain over the reference default on dataset j as:

d(j)
i1,i2

:=R(j)(θ∗) − R(j)(θ(j)(cid:63)
i1,i2

).

The joint gain which can be expected when tuning not only one of the two hyperparam-

eters individually, but both of them jointly, on a dataset j, can be expressed by:

g(j)
i1,i2

:= min{(R(j)(θ(j)(cid:63)

)), (R(j)(θ(j)(cid:63)

))} − R(j)(θ(j)(cid:63)
i1,i2

).

i2

i1

Furthermore, one could be interested in whether this joint gain could simply be reached
by tuning both parameters i1 and i2 in a univariate fashion sequentially, either in the order
i1 → i2 or i2 → i1, and what order would be preferable. For this purpose one could compare
the risk of the hyperparameter value that results when tuning them together R(j)(θ(j)(cid:63)
) with
i1,i2
the risks of the hyperparameter values that are obtained when tuning them sequentially, that
means R(j)(θ(j)(cid:63)

), which is done for example in Waldron et al. (2011).

) or R(j)(θ(j)(cid:63)

i1→i2

i2→i1

Again, all these measures should be summarized across datasets, resulting in di1,i2 and
gi1,i2. Of course, these approaches can be further generalized by considering combinations
of more than two parameters.

(5)

(6)

(7)

(8)

(9)

4

3.6 Optimal hyperparameter ranges for tuning

A reasonable hyperparameter space Θ(cid:63) for tuning should include the optimal conﬁguration
θ(j)(cid:63) for dataset j with high probability. We denote the p-quantile of the distribution of one
parameter regarding the best hyperparameters on each dataset (θ(1)(cid:63))i, ..., (θ(m)(cid:63))i as qi,p.
The hyperparameter tuning space can then be deﬁned as:

Θ(cid:63) := {θ ∈ Θ|∀i ∈ {1, ..., k} : θi ≥ qi,p1 ∧ θi ≤ qi,p2} ,
with p1 and p2 being quantiles which can be set for example to the 5 % quantile and the
95 % quantile. This avoids focusing too much on outlier datasets and makes the deﬁnition
of the space independent from the number of datasets.

(10)

The deﬁnition above is only valid for numerical hyperparameters. In case of categorical
variables one could use similar rules, for example only including hyperparameter values that
were at least once or in at least 10 % of the datasets the best possible hyperparameter
setting.

3.7 Practical estimation

In order to practically apply the previously deﬁned concepts, two remaining issues need to be
addressed: a) We need to discuss how to obtain R(j)(θ); and b) in (2) and (3) a multivariate
optimization problem (the minimization) needs to be solved1.

For a) we estimate R(j)(θ) by using surrogate models ˆR(j)(θ), and replace the original
quantity by its estimator in all previous formulas. Surrogate models for each dataset j are
based on a meta dataset. This is created by evaluating a large number of conﬁgurations of the
respective ML method. The surrogate regression model then learns to map a hyperparameter
conﬁguration to estimated performance. For b) we solve the optimization problem – now
cheap to evaluate, because of the surrogate models – through black-box optimization.

4 Experimental setup

In this section we give an overview about the experimental setup that is used for obtaining
surrogate models, tunability measures and tuning spaces.

4.1 Datasets from the OpenML platform

Recently, the OpenML project (Vanschoren et al., 2013) has been created as a ﬂexible online
platform that allows ML scientists to share their data, corresponding tasks and results of
diﬀerent ML algorithms. We use a speciﬁc subset of carefully curated classiﬁcation datasets
from the OpenML platform called OpenML100 (Bischl et al., 2017). For our study we only
use the 38 binary classiﬁcation tasks that do not contain any missing values.

4.2 ML Algorithms

The algorithms considered in this paper are common methods for supervised learning. We
examine elastic net (glmnet), decision tree (rpart), k-nearest neighbors (kknn), support
vector machine (svm), random forest (ranger) and gradient boosting (xgboost). For more
details about the used software packages see Kühn et al. (2018). An overview of their
considered hyperparameters is displayed in Table 1, including respective data types, box-
constraints and a potential transformation function.

In the case of xgboost, the underlying package only supports numerical features, so we
opted for a dummy feature encoding for categorical features, which is performed internally
by the underlying packages for svm and glmnet.

Some hyperparameters of the algorithms are dependent on others. We take into account
these dependencies and, for example, only sample a value for gamma for the support vector
machine if the radial kernel was sampled beforehand.

1All other previous optimization problems are univariate or two-dimensional and can simply be addressed by

a simple technique like a ﬁne grid search

5

Algorithm Hyperparameter
glmnet

Type Lower Upper Trafo

alpha
lambda

cp
maxdepth
minbucket
minsplit
-
k

kernel
cost
gamma
degree

rpart

kknn

svm

ranger

xgboost

num.trees
replace
sample.fraction
mtry
respect.unordered.factors
min.node.size

nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

numeric
numeric

numeric
integer
integer
integer
-
integer

discrete
numeric
numeric
integer

integer
logical
numeric
numeric
logical
numeric

integer
numeric
numeric
discrete
integer
numeric
numeric
numeric
numeric
numeric

0
-10

0
1
1
1

1

-
-10
-10
2

1
-
0.1
0
-
0

1
-10
0.1
-
1
0
0
0
-10
-10

1
10

1
30
60
60

30

-
10
10
5

2000
-
1
1
-
1

5000
0
1
-
15
7
1
1
10
10

-
2x

-
-
-
-

-

-
2x
2x
-

-
2x
-
-
-
2x
-
-
2x
2x

-
-
-
x · p
-
nx

Table 1: Hyperparameters of the algorithms. p refers to the number of variables and n to the number
of observations. The columns Lower and Upper indicate the regions from which samples of these hyper-
parameters are drawn. The transformation function in the trafo column, if any, indicates how the values
are transformed according to this function. The exponential transformation is applied to obtain more
candidate values in regions with smaller hyperparameters because for these hyperparameters the perfor-
mance diﬀerences between smaller values are potentially bigger than for bigger values. The mtry value
in ranger that is drawn from [0, 1] is transformed for each dataset separately. After having chosen the
dataset, the value is multiplied by the number of variables and afterwards rounded up. Similarly, for the
min.node.size the value x is transformed by the formula [nx] with n being the number of observations
of the dataset, to obtain a positive integer values with higher probability for smaller values (the value is
ﬁnally rounded to obtain integer values).

6

4.3 Performance estimation

Several measures are regarded throughout this paper, either for evaluating our considered
classiﬁcation models that should be tuned, or for evaluating our surrogate regression models.
As no optimal measure exists, we will compare several of them. In the classiﬁcation case,
we consider AUC, accuracy and brier score. In the case of surrogate regression, we consider
R2, which is directly proportional to the regular mean squared error but scaled to [0,1] and
explains the gain over a constant model estimating the overall mean of all data points. We
also compute Kendall’s tau as a ranking based measure for regression.

The performance estimation for the diﬀerent hyperparameter experiments is computed
through 10-fold cross-validation. For the comparison of surrogate models 10 times repeated
10-fold cross-validation is used.

4.4 Random Bot sampling strategy for meta data

To reliably estimate our surrogate models we need enough evaluated conﬁgurations per
classiﬁer and data set. We sample these points from independent uniform distributions
where the respective support for each parameter is displayed in Table 1. Here, uniform
refers to the untransformed scale, so we sample uniformly from the interval [Lower, Upper ]
of Table 1.

In order to properly facilitate the automatic computation of a large database of hyper-
In an embarrassingly
parameter experiments, we implemented a so called OpenML bot.
parallel manner it chooses in each iteration a random dataset, a random classiﬁcation al-
gorithm, samples a random conﬁguration and evaluates it via cross-validation. A subset
of 500000 experiments for each algorithm and all datasets are used for our analysis here.2
More technical details regarding the random bot, its setup and results can be obtained in
Kühn et al. (2018), furthermore, for simple and permanent access the results of the bot are
stored in a ﬁgshare repository (Kühn et al., 2018).

4.5 Optimizing surrogates to obtain optimal defaults

Random search is also used for our black-box optimization problems in section 3.7. For
the estimation of the defaults for each algorithm we randomly sample 100000 points in
the hyperparameter space as deﬁned in Table 1 and determine the conﬁguration with the
minimal average risk. The same strategy with 100000 random points is used to obtain
the best hyperparameter setting on each dataset that is needed for the estimation of the
tunability of an algorithm. For the estimation of the tunability of single hyperparameters we
also use 100000 random points for each parameter, while for the tunability of combination
of hyperparameters we only use 10000 random points to reduce runtime as this should be
enough to cover 2-dimensional hyperparameter spaces.

Of course one has to be careful with overﬁtting here, as our new defaults are chosen
with the help of the same datasets that are used to determine the performance. Therefore,
we also evaluate our approach via a “10-fold cross-validation across datasets”. Here, we
repeatedly calculate the optimal defaults based on 90% “training datasets” and evaluate the
package defaults and our optimal defaults – the latter induced from the training data sets –
on the surrogate models of the remaining 10% “test datasets”, and compare their diﬀerence
in performance.

4.6 The problem of hyperparameter dependency

Some parameters are dependent on other superordinate hyperparameters and are only rele-
vant if the parameter value of this superordinate parameter was set to a speciﬁc value. For
example gamma in svm only makes sense if the kernel was set to “radial“ or degree only
makes sense if the kernel was set to “polynomial“. Some of these subordinate parameters
might be invalid/inactive in the reference default conﬁguration, rendering it impossible to
univariately tune them in order to compute their tunability score. In such a case we set the

230 for each dataset for kknn

7

superordinate parameter to a value which makes the subordinate parameter active, compute
the optimal defaults for the rest of the parameters and compute the tunability score for the
subordinate parameter with these defaults.

4.7 Software details

All our experiments are executed in R and are run through a combination of custom code
from our random bot Kühn et al. (2018), the OpenML R package (Casalicchio et al., 2017),
mlr (Bischl et al., 2016) and batchtools (Lang et al., 2017) for parallelization. All results
are uploaded to the OpenML platform and there publicly available for further analysis.
mlr is also used to compare and ﬁt all surrogate regression models. The fully reproducible
R code for all computations and analyses of our paper can be found on the github page:
https://github.com/PhilippPro/tunability. We also provide an interactive shiny app
under https://philipppro.shinyapps.io/tunability/, which displays all results of the
following section in a potentially more convenient, interactive fashion and which can simply
be accessed through a web browser.

5 Results and discussion

We calculate all results for AUC, accuracy and brier score but mainly discuss AUC results
here. Tables and ﬁgures for the other measures can be accessed in the Appendix and in our
interactive shiny application.

5.1 Surrogate models

We compare diﬀerent possible regression models as candidates for our surrogate models: the
linear model (lm), a simple decision tree (rpart), k nearest-neighbors (kknn) and random
forest (ranger)3 All algorithms are run with their default settings. We calculate 10 times
repeated 10-fold cross-validated regression performance measures R2 and Kendall’s tau per
dataset, and average these across all datasets4. Results for AUC are displayed in Figure
1. A good overall performance is achieved by ranger with qualitatively similar results for
other classiﬁcation performance measures (see Appendix). In the following we use random
forest as surrogate model because it performs reasonably well and is already an established
algorithm for surrogate models in the literature (Eggensperger et al., 2014; Hutter et al.,
2013).

5.1.1 Optimal defaults and tunability

Table 2 displays our mean tunability results for the algorithms as deﬁned in formula (4)
w.r.t. package defaults (Def.P column) and our optimal defaults (Def.O). It also displays
the improvement per algorithm when moving from package defaults to optimal defaults
(Improv), which was positive overall. This also holds for svm and ranger although the
package defaults are data dependent, which we currently cannot model (gamma = 1/p for
p for ranger). From now on, when discussing tunability, we will only do
svm and mtry =
this w.r.t. our optimal defaults.

√

Clearly, some algorithms such as glmnet and svm are much more tunable than the others,
while ranger is the algorithm with the smallest tunability, which is in line with common
In Figure 2 modiﬁed boxplots of the tunabilities are
knowledge in the web community.
depicted. For each ML algorithm, some outliers are visible, which indicates that tuning has
a much higher impact on some speciﬁc datasets.

3We also tried cubist (Kuhn et al., 2016), which provided good results but the algorithm had some technical
problems for some combinations of datasets and algorithms. We did not include gaussian process which is one of
the standard algorithms for surrogate models as it cannot handle categorical variables.

4In case of kknn four datasets did not provide results for one of the surrogate models and were not used.

8

Figure 1: Average performances over the datasets of diﬀerent surrogate models (target: AUC)
for diﬀerent algorithms (that were presented in 4.2).

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.045
0.024
0.025
0.012
0.025
0.006
0.014
0.042
0.004
0.006
0.029
0.014

0.069
glmnet
0.038
rpart
0.031
kknn
svm 0.056
0.010
0.043

0.037
0.016
0.006
0.048
0.007
0.017

ranger
xgboost

Impr-CV
0.032
0.022
0.025
0.008
0.003
0.026

Table 2: Overall tunability (regarding AUC) with the package defaults (Tun.P) and the new
defaults (Tun.O) as reference, cross-validated tunability (Tun.O-CV), average improvement (Im-
prov) and cross-validated average improvement (Impr-CV) obtained by using new defaults com-
pared to old defaults. The (cross-validated) improvement can be calculated by the (rounded)
diﬀerence between Tun.P and Tun.O (Tun.O-CV).

9

Figure 2: Boxplots of the tunabilities (AUC) of the diﬀerent algorithms. The upper and lower
whiskers (upper and lower line of the boxplot rectangle) are in our case deﬁned as the 0.1
and 0.9 quantiles of the tunability scores. The 0.9 quantile indicates how much performance
improvement can be expected on at least 10% of datasets. One outlier of glmnet (value 0.5) is
not shown.

5.1.2 Tunability of speciﬁc hyperparameters

In Table 3 the mean tunability (regarding the AUC) of single hyperparameters as deﬁned
in Equation (6) in section 3.4 can be seen. From here on, we will refer to tunability only
with respect to optimal defaults.

For glmnet lambda seems to be more tunable than alpha.

In rpart the minbucket
and minsplit parameters seem to be the most important ones for tuning. k in the kknn
algorithm is very tunable w.r.t. package defaults, but not regarding optimal defaults. In svm
the biggest gain in performance can be achieved by tuning the kernel, gamma or degree,
while the cost parameter does not seem to be very tunable. In ranger mtry is the most
tunable parameter. For xgboost there are two parameters that are quite tunable: eta and
the booster. booster speciﬁes if a tree or a linear model is trained. The cross-validated
results can be seen in Table 10 in the Appendix, they are quite similar to the non cross-
validated results and for all parameters slightly higher.

Instead of looking only at the average, as in Table 3, one could also be interested in the
distribution of the tunability of each dataset. As an example, Figure 3 shows the tunability
of each parameter of ranger in a boxplot. This gives a more in-depth insight into the
tunability, makes it possible to detect outliers and to examine the skewness.

5.1.3 Hyperparameter space for tuning

The hyperparameter space for tuning, as deﬁned in Equation (10) in section 3.6 and based
on the 0.05 and 0.95 quantiles, is displayed in Table 3. All optimal defaults are contained
in this hyperparameter space while some of the package defaults are not.

As an example, Figure 4 displays the full histogram of the best values of mtry of the
random forest over all datasets. Note that for quite a few data sets much higher values than
the package defaults seem advantageous. Analogous histograms for other parameters are
available through the shiny app.

10

Parameter Def.P

q0.05

q0.95

1
0

7

0.01
30
7
20

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

radial
1
1/p
3

radial
682.478
0.005
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
0

sample.fraction
mtry

√

983
FALSE
0.703
p · 0.257
FALSE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

30

0
21
12
24

0.403
0.004

Def.O Tun.P Tun.O
0.024
0.069
0.006
0.038
0.021
0.034
0.012
0.038
0.002
0.025
0.002
0.004
0.006
0.005
0.004
0.004
0.006
0.031
0.006
0.031
0.042
0.056
0.024
0.030
0.006
0.016
0.022
0.030
0.014
0.008
0.006
0.010
0.001
0.001
0.001
0.002
0.002
0.004
0.003
0.006
0.000
0.000
0.001
0.001
0.014
0.043
0.002
0.004
0.005
0.006
0.002
0.004
0.008
0.015
0.001
0.001
0.002
0.008
0.001
0.006
0.001
0.008
0.002
0.003
0.002
0.003

4168
0.018
0.839
gbtree
13
2.06
0.752
0.585
0.982
1.113

0.009
0.001

0
12.1
3.85
5

9.95

0.981
0.147

0.008
27
41.6
49.15

30

0.002
0.003
2

920.582
18.195
4

206.35

1740.15

0.323
0.035

0.974
0.692

0.007

0.513

920.7
0.002
0.545

5.6
1.295
0.419
0.335
0.008
0.002

4550.95
0.355
0.958

14
6.984
0.864
0.886
29.755
6.105

Table 3: Defaults (package defaults (Def.P) and optimal defaults (Def.O)), tunability of the hy-
perparameters with the package defaults (Tun.P) and our optimal defaults (Tun.O) as reference
and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters of the algorithms.

11

Figure 3: Boxplots of the tunabilities of the diﬀerent parameters of ranger. Same deﬁnition of
whiskers as in Figure 2.

Figure 4: Histogram of best parameter values for mtry of random forest over all considered data
sets.

12

5.1.4 Tunability of hyperparameter combinations

As an example, Table 4 displays the average tunability di1,i2 of all 2-way hyperparameter
combinations for rpart. Obviously, the increased ﬂexibility in tuning a 2-way combination
enables larger improvements when compared with the tunability of one of the respective
individual parameters.

In Table 5 the joint gain of tuning two hyperparameters gi1,i2 instead of only the best
as deﬁned in section 3.5 can be seen. The parameters minsplit and minbucket have the
biggest joint eﬀect, which is not very surprising, as they are closely related: minsplit is
the minimum number of observations that must exist in a node in order for a split to be
attempted and minbucket the minimum number of observations in any terminal leaf node.
If a higher value of minsplit than the default performs better on a dataset it is possibly
not enough to set it higher without also increasing minbucket, so the strong relationship is
quite clear. Again, further ﬁgures for other algorithms are available through the shiny app.

cp
maxdepth
minbucket
minsplit

0.002

cp maxdepth minbucket minsplit
0.004
0.005
0.011
0.004

0.006
0.007
0.006

0.003
0.002

Table 4: Tunability di1,i2 of hyperparameters of rpart, diagonal shows tunability of the single
hyperparameters.

cp
maxdepth
minbucket

0.0007

maxdepth minbucket minsplit
0.0004
0.0019
0.0055

0.0005
0.0014

Table 5: Joint gain gi1,i2 of tuning two hyperparameters instead of the most important in rpart.

6 Conclusion and Discussion

Our paper provides concise and intuitive deﬁnitions for optimal defaults of ML algorithms
and the impact of tuning them either jointly, tuning individual parameters or combinations,
all based on the general concept of surrogate empirical performance models. Tunability
values as deﬁned in our framework are easily and directly interpretable as how much per-
formance can be gained by tuning this hyperparameter?. This allows direct comparability of
the tunability values across diﬀerent algorithms.

In an extensive OpenML benchmark, we computed optimal defaults for elastic net, de-
cision tree, k-nearest neighbors, SVM, random forest and xgboost and quantiﬁed their tun-
ability and the tunability of their individual parameters. This – to the best of our knowledge
– has never been provided before in such a principled manner. Our results are often in line
with common knowledge from literature and our method itself now allows an analogous
analysis for other or more complex methods.

Our framework is based on the concept of default hyperparameter values, which can be
seen both as an advantage (default values are a valuable output of the approach) and as
an inconvenience (the determination of the default values is an additional analysis step and
needed as a reference point for most of our measures).

We now compare our method with van Rijn and Hutter (2017). In contrast to us, they
apply the functional ANOVA framework from Hutter et al. (2014) on a surrogate random
forest to assess the importance of hyperparameters regarding empirical performance of a
support vector machine, random forest and adaboost, which results in numerical importance

13

scores for individual hyperparameters. Their numerical scores are - in our opinion - less
directly interpretable, but they do not rely on defaults as a reference point, which one
might see as an advantage. They also propose a method for calculating hyperparameter
priors, combine it with the tuning procedure hyperband, and assess the performance of this
new tuning procedure. In contrast, we deﬁne and calculate ranges for all hyperparameters.
Setting ranges for the tuning space can be seen as a special case of a prior distribution - the
uniform distribution on the speciﬁed hyperparameter space. Regarding the experimental
setup, we compute more hyperparameter runs (around 2.5 million vs. 250000), but consider
only the 38 binary classiﬁcation datasets of OpenML100 while van Rijn and Hutter (2017)
use all the 100 datasets which also contain multiclass datasets. We evaluate the performance
of diﬀerent surrogate models by 10 times repeated 10-fold cross-validation to choose an
appropriate model and to assure that it performs reasonably well.

Our study has some limitations that could be addressed in the future: a) We only con-
sidered binary classiﬁcation, where we tried to include a wider variety of datasets from
diﬀerent domains. In principle this is not a restriction as our methods can easily be applied
to multiclass classiﬁcation, regression, survival analysis or even algorithms not from machine
learning whose empirical performance is reliably measurable on a problem instance. b) Uni-
form random sampling of hyperparameters might not scale enough for very high dimensional
spaces, and a smarter sequential technique might be in order here, see (Bossek et al., 2015)
for an potential approach of sampling across problem instances to learn optimal mappings
from problem characteristics to algorithm conﬁgurations. c) We currently are learning static
defaults, which cannot depend on dataset characteristics (like number of features, or fur-
ther statistical measures) as in meta-learning. Doing so might improve performance results
of optimal defaults considerably, but would require a more complicated approach. d) Our
approach still needs initial ranges to be set, in order to run our sampling procedure. Only
based on these wider ranges we can then compute more precise, closer ranges.

Acknowledgements

We would like to thank Joaquin Vanschoren for support regarding the OpenML platform
and Andreas Müller, Jan van Rijn, Janek Thomas and Florian Pﬁsterer for reviewing and
useful comments. Thanks to Jenny Lee for language editing.

References

J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of

Machine Learning Research, 13:281–305, 2012.

A. Biedenkapp, M. T. Lindauer, K. Eggensperger, F. Hutter, C. Fawcett, and H. H. Hoos.
In AAAI, pages

Eﬃcient parameter importance analysis via ablation with surrogates.
773–779, 2017.

M. Birattari, Z. Yuan, P. Balaprakash, and T. Stützle. F-Race and iterated F-Race: An
overview. In Experimental Methods for the Analysis of Optimization Algorithms, pages
311–336. Springer, 2010.

B. Bischl, O. Mersmann, H. Trautmann, and C. Weihs. Resampling methods for meta-model
validation with recommendations for evolutionary computation. Evolutionary Computa-
tion, 20(2):249–275, 2012.

B. Bischl, M. Lang, L. Kotthoﬀ, J. Schiﬀner, J. Richter, E. Studerus, G. Casalicchio, and
Z. M. Jones. mlr: Machine learning in R. Journal of Machine Learning Research, 17
(170):1–5, 2016.

B. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. van Rijn,
and J. Vanschoren. OpenML benchmarking suites and the OpenML100. ArXiv preprint
arXiv:1708.03731, 2017. URL https://arxiv.org/abs/1708.03731.

14

B. Bischl, J. Richter, J. Bossek, D. Horn, J. Thomas, and M. Lang. mlrMBO: A modular
framework for model-based optimization of expensive black-box functions. ArXiv preprint
arXiv:1703.03373, 2017. URL https://arxiv.org/abs/1703.03373.

J. Bossek, B. Bischl, T. Wagner, and G. Rudolph. Learning feature-parameter mappings
for parameter tuning via the proﬁle expected improvement. In Proceedings of the 2015
Annual Conference on Genetic and Evolutionary Computation, pages 1319–1326. ACM,
2015.

G. Casalicchio, J. Bossek, M. Lang, D. Kirchhoﬀ, P. Kerschke, B. Hofner, H. Seibold, J. Van-
schoren, and B. Bischl. OpenML: An R package to connect to the machine learning
platform OpenML. Computational Statistics, 32(3):1–15, 2017.

K. Eggensperger, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Surrogate benchmarks for
hyperparameter optimization.
In Proceedings of the 2014 International Conference on
Meta-learning and Algorithm Selection-Volume 1201, pages 24–31. CEUR-WS. org, 2014.

K. Eggensperger, M. Lindauer, H. H. Hoos, F. Hutter, and K. Leyton-Brown. Eﬃcient
benchmarking of algorithm conﬁgurators via model-based surrogates. Machine Learning,
pages 1–27, 2018.

A. E. Eiben and S. K. Smit. Parameter tuning for conﬁguring and analyzing evolutionary

algorithms. Swarm and Evolutionary Computation, 1(1):19–31, 2011.

C. Fawcett and H. H. Hoos. Analysing diﬀerences between algorithm conﬁgurations through

ablation. Journal of Heuristics, 22(4):431–458, 2016.

M. Feurer, B. Letham, and E. Bakshy. Scalable meta-learning for bayesian optimization.

arXiv preprint 1802.02219, 2018. URL https://arxiv.org/abs/1802.02219.

I. Guyon, A. Saﬀari, G. Dror, and G. Cawley. Model selection: Beyond the bayesian/fre-

quentist divide. Journal of Machine Learning Research, 11(Jan):61–87, 2010.

F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for
general algorithm conﬁguration. In International Conference on Learning and Intelligent
Optimization, pages 507–523. Springer, 2011.

F. Hutter, H. H. Hoos, and K. Leyton-Brown. Identifying key algorithm parameters and
instance features using forward selection. In International Conference on Learning and
Intelligent Optimization, pages 364–381. Springer, 2013.

F. Hutter, H. Hoos, and K. Leyton-Brown. An eﬃcient approach for assessing hyperparam-
eter importance. In ICML, volume 32 of JMLR Workshop and Conference Proceedings,
pages 754–762, 2014.

D. Kühn, P. Probst, J. Thomas, and B. Bischl. Automatic Exploration of Machine
Learning Experiments on OpenML. ArXiv preprint arXiv:1806.10961, 2018. URL
https://arxiv.org/abs/1806.10961.

M. Kuhn, S. Weston, C. Keefer, and N. Coulter. Cubist: Rule- and instance-based regression

modeling, 2016. R package version 0.0.19.

D.

Kühn,
bot

P.
Probst,
benchmark

J.
data

Thomas,
(ﬁnal

and
B.
subset),

Bischl.
2018.

R
https://figshare.com/articles/OpenML_R_Bot_Benchmark_Data_final_subset_/5882230/2.

OpenML
URL

M. Lang, B. Bischl, and D. Surmann. batchtools: Tools for R to work on batch systems.

The Journal of Open Source Software, 2(10), 2017.

G. Luo. A review of automatic selection methods for machine learning algorithms and hyper-
parameter values. Network Modeling Analysis in Health Informatics and Bioinformatics,
5(1):1–16, 2016.

15

R. G. Mantovani, T. Horváth, R. Cerri, A. Carvalho, and J. Vanschoren. Hyper-parameter
In Brazilian Conference on Intelligent

tuning of a decision tree induction algorithm.
Systems (BRACIS 2016), 2016.

J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pages 2951–
2959, 2012.

J. N. van Rijn and F. Hutter. Hyperparameter importance across datasets. ArXiv preprint

arXiv:1710.04725, 2017. URL https://arxiv.org/abs/1710.04725.

J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo. OpenML: Networked science in

machine learning. SIGKDD Explorations, 15(2):49–60, 2013.

L. Waldron, M. Pintilie, M.-S. Tsao, F. A. Shepherd, C. Huttenhower, and I. Jurisica.
Optimized application of penalized regression methods to diverse genomic data. Bioin-
formatics, 27(24):3399–3406, 2011.

16

Appendix A. Results for accuracy and brier score

Figure 5: Same as ﬁgure 1 but with accuracy as target measure. Average performances over
the datasets of diﬀerent surrogate models (target: accuracy) for diﬀerent algorithms (that were
presented in 4.2).

Figure 6: Same as ﬁgure 1 but with brier score as target measure. Average performances over
the datasets of diﬀerent surrogate models (target: brier score) for diﬀerent algorithms (that were
presented in 4.2).

17

Figure 7: Boxplots of the tunabilities (accuracy) of the diﬀerent algorithms.

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.023
0.019
0.008
0.012
0.013
0.008
0.011
0.030
0.009
0.007
0.023
0.011

0.042
glmnet
0.020
rpart
0.021
kknn
svm 0.041
0.016
0.034

0.042
0.014
0.010
0.041
0.009
0.012

ranger
xgboost

Impr-CV
0.001
0.005
0.010
-0.001
0.006
0.022

Table 6: Tunability measures as in table 2, but calculated for the accuracy. Overall tunabil-
ity (regarding accuracy) with the package defaults (Def.P) and the optimal defaults (Def.O)
as reference points, cross-validated tunability (Def.O-CV), average improvement (Improv) and
cross-validated average improvement (Impr-CV) obtained by using new defaults compared to
old defaults. The (cross-validated) improvement can be calculated by the (rounded) diﬀerence
between Def.P and Def.O (Def.O-CV).

18

Parameter Def.P

Def.O Tun.P Tun.O q0.05

q0.95

7

14

2

30

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

1
0

0.01
30
7
20

0.252
0.005

0.002
19
5
13

radial
1
1/p
3

radial
936.982
0.002
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
1

sample.fraction
mtry

√

162
FALSE
0.76
p · 0.432
TRUE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

3342
0.031
0.89
gbtree
14
1.264
0.712
0.827
2.224
0.021

0.042
0.022
0.029
0.020
0.013
0.004
0.005
0.002
0.021
0.021
0.041
0.019
0.019
0.024
0.005
0.016
0.001
0.004
0.003
0.010
0.001
0.001
0.034
0.004
0.005
0.003
0.008
0.001
0.009
0.005
0.006
0.002
0.003

0.019
0.010
0.017
0.012
0.008
0.004
0.006
0.003
0.008
0.008
0.030
0.018
0.003
0.020
0.014
0.007
0.001
0.001
0.003
0.003
0.000
0.002
0.011
0.002
0.005
0.002
0.005
0.001
0.002
0.001
0.001
0.002
0.002

0.015
0.001

0
10
1.85
6.7

0.979
0.223

0.528
28
43.15
47.6

0.025
0.007
2

943.704
276.02
4

203.5

1908.25

0.257
0.081

0.971
0.867

0.009

0.453

1360
0.002
0.555

3
1.061
0.334
0.348
0.004
0.003

4847.15
0.445
0.964

13
7.502
0.887
0.857
5.837
2.904

Table 7: Tunability measures for single hyperparameters and tuning spaces as in table 3, but
calculated for the accuracy. Defaults (package defaults (Def.P) and own calculated defaults
(Def.O)), tunability of the hyperparameters with the package defaults (Tun.P) and our new
defaults (Tun.O) as reference and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters
of the algorithms.

19

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.011
0.010
0.006
0.009
0.009
0.003
0.008
0.018
0.010
0.005
0.018
0.009

0.022
glmnet
0.015
rpart
0.012
kknn
svm 0.026
0.015
0.027

0.020
0.011
0.003
0.023
0.006
0.011

ranger
xgboost

Impr-CV
0.001
0.004
0.009
0.003
0.009
0.016

Table 8: Tunability measures as in table 2, but calculated for the brier score. Overall tunability
(regarding brier score) with the package defaults (Def.P) and the optimal defaults (Def.O)
as reference points, cross-validated tunability (Def.O-CV), average improvement (Improv) and
cross-validated average improvement (Impr-CV) obtained by using new defaults compared to
old defaults. The (cross-validated) improvement can be calculated by the (rounded) diﬀerence
between Def.P and Def.O (Def.O-CV).

Figure 8: Boxplots of the tunabilities (brier score) of the diﬀerent algorithms.

20

Parameter Def.P

q0.05

q0.95

1
0

7

0.01
30
7
20

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

radial
1
1/p
3

radial
950.787
0.005
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
1

sample.fraction
mtry

√

198
FALSE
0.667
p · 0.666
TRUE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

19

0.997
0.004

0.001
13
12
18

Def.O Tun.P Tun.O
0.010
0.022
0.005
0.009
0.007
0.014
0.009
0.015
0.003
0.009
0.002
0.002
0.006
0.004
0.002
0.002
0.003
0.012
0.003
0.012
0.018
0.026
0.011
0.013
0.002
0.012
0.012
0.015
0.009
0.003
0.005
0.015
0.001
0.001
0.001
0.002
0.003
0.002
0.002
0.010
0.000
0.000
0.001
0.001
0.009
0.027
0.002
0.004
0.005
0.004
0.002
0.002
0.004
0.009
0.001
0.001
0.002
0.007
0.002
0.004
0.001
0.004
0.003
0.002
0.004
0.003

2563
0.052
0.873
gbtree
11
1.75
0.713
0.638
0.101
0.894

0.003
0.001

0
9
1
7

0.974
0.051

0.035
27.15
44.1
49.15

4.85

30

0.002
0.001
2

963.81
4.759
4

187.85

1568.25

0.317
0.072

0.964
0.954

0.008

0.394

2018.55
0.003
0.447

4780.05
0.436
0.951

2.6
1.277
0.354
0.363
0.006
0.003

13
5.115
0.922
0.916
28.032
2.68

Table 9: Tunability measures for single hyperparameters and tuning spaces as in table 3, but
calculated for the brier score. Defaults (package defaults (Def.P) and own calculated defaults
(Def.O)), tunability of the hyperparameters with the package defaults (Tun.P) and our new
defaults (Tun.O) as reference and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters
of the algorithms.

21

Measure

AUC

Accuracy

Brier score

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k

Parameter Tun.O Tun.O-CV Tun.O Tun.O-CV Tun.O Tun.O-CV
0.020
0.019
0.015
0.010
0.018
0.017
0.011
0.012
0.005
0.008
0.003
0.004
0.006
0.006
0.003
0.003
0.003
0.008
0.003
0.008
0.023
0.030
0.016
0.018
0.002
0.003
0.016
0.020
0.014
0.014
0.006
0.007
0.001
0.001
0.001
0.001
0.003
0.003
0.003
0.003
0.000
0.000
0.001
0.002
0.011
0.011
0.002
0.002
0.006
0.005
0.002
0.002
0.004
0.005
0.001
0.001
0.003
0.002
0.002
0.001
0.002
0.001
0.004
0.002
0.004
0.002

0.024
0.006
0.021
0.012
0.002
0.002
0.006
0.004
0.006
0.006
svm 0.042
0.024
0.006
0.022
0.014
0.006
0.001
0.001
0.002
0.003
0.000
0.001
0.014
0.002
0.005
0.002
0.008
0.001
0.002
0.001
0.001
0.002
0.002

0.037
0.006
0.034
0.016
0.002
0.002
0.009
0.004
0.006
0.006
0.048
0.030
0.006
0.028
0.020
0.007
0.002
0.002
0.002
0.004
0.000
0.001
0.017
0.002
0.006
0.002
0.008
0.001
0.003
0.002
0.001
0.003
0.004

0.010
0.005
0.007
0.009
0.003
0.002
0.006
0.002
0.003
0.003
0.018
0.011
0.002
0.012
0.009
0.005
0.001
0.001
0.003
0.002
0.000
0.001
0.009
0.002
0.005
0.002
0.004
0.001
0.002
0.002
0.001
0.003
0.004

0.042
0.026
0.039
0.014
0.008
0.004
0.007
0.003
0.010
0.010
0.041
0.031
0.003
0.031
0.027
0.009
0.003
0.002
0.003
0.005
0.001
0.002
0.012
0.003
0.006
0.002
0.005
0.001
0.002
0.001
0.001
0.003
0.003

kernel
cost
gamma
degree
ranger
num.trees
replace
sample.fraction
mtry
respect.unordered.factors
min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

Table 10: Tunability with calculated defaults as reference without (Tun.O) and with (Tun.O-CV)
cross-validation for AUC, accuracy and brier score

22

8
1
0
2
 
t
c
O
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
6
9
5
9
0
.
2
0
8
1
:
v
i
X
r
a

Tunability: Importance of Hyperparameters of Machine
Learning Algorithms

by Philipp Probst, Anne-Laure Boulesteix and Bernd Bischl

October 23, 2018

Abstract

Modern supervised machine learning algorithms involve hyperparameters that have to
be set before running them. Options for setting hyperparameters are default values from
the software package, manual conﬁguration by the user or conﬁguring them for optimal
predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly,
we formalize the problem of tuning from a statistical point of view, deﬁne data-based de-
faults and suggest general measures quantifying the tunability of hyperparameters of al-
gorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets
from the OpenML platform and six common machine learning algorithms. We apply our
measures to assess the tunability of their parameters. Our results yield default values for
hyperparameters and enable users to decide whether it is worth conducting a possibly time
consuming tuning strategy, to focus on the most important hyperparameters and to chose
adequate hyperparameter spaces for tuning.

1

Introduction

Machine learning (ML) algorithms such as gradient boosting, random forest and neural net-
works for regression and classiﬁcation involve a number of hyperparameters that have to
be set before running them. In contrast to direct, ﬁrst-level model parameters, which are
determined during training, these second-level tuning parameters often have to be carefully
optimized to achieve maximal performance. A related problem exists in many other algo-
rithmic areas, e.g., control parameters in evolutionary algorithms (Eiben and Smit, 2011).
In order to select an appropriate hyperparameter conﬁguration for a speciﬁc dataset
at hand, users of ML algorithms can resort to default values of hyperparameters that are
speciﬁed in implementing software packages or manually conﬁgure them, for example, based
on recommendations from the literature, experience or trial-and-error.

Alternatively, one can use hyperparameter tuning strategies, which are data-dependent,
second-level optimization procedures (Guyon et al., 2010), which try to minimize the ex-
pected generalization error of the inducing algorithm over a hyperparameter search space
of considered candidate conﬁgurations, usually by evaluating predictions on an independent
test set, or by running a resampling scheme such as cross-validation (Bischl et al., 2012).
For a recent overview of tuning strategies, see, e.g., Luo (2016).

These search strategies range from simple grid or random search (Bergstra and Bengio,
2012) to more complex, iterative procedures such as Bayesian optimization (Hutter et al.,
2011; Snoek et al., 2012; Bischl et al., 2017) or iterated F-racing (Birattari et al., 2010;
Lang et al., 2017).

In addition to selecting an eﬃcient tuning strategy, the set of tunable hyperparameters
and their corresponding ranges, scales and potential prior distributions for subsequent sam-
pling have to be determined by the user. Some hyperparameters might be safely set to
default values, if they work well across many diﬀerent scenarios. Wrong decisions in these
areas can inhibit either the quality of the resulting model or at the very least the eﬃciency
and fast convergence of the tuning procedure. This creates a burden for:

1

1. ML users – Which hyperparameters should be tuned and in which ranges?

2. Designers of ML algorithms – How do I deﬁne robust defaults?

We argue that many users, especially if they do not have years of practical experience in
the ﬁeld, here often rely on heuristics or spurious knowledge. It should also be noted that
designers of fully automated tuning frameworks face at least very similar problems. It is
not clear how these questions should be addressed in a data-dependent, automated, optimal
and objective manner. In other words, the scientiﬁc community not only misses answers to
these questions for many algorithms but also a systematic framework, methods and criteria,
which are required to answer these questions.

With the present paper we aim at ﬁlling this gap and formalize the problem of parameter
tuning from a statistical point of view, in order to simplify the tuning process for less
experienced users and to optimize decision making for more advanced processes.

After presenting related literature in section 2, we deﬁne theoretical measures for assess-
ing the impact of tuning in section 3. For this purpose we (i) deﬁne the concept of default
hyperparameters, (ii) suggest measures for quantiﬁying the tunability of the whole algorithm
and speciﬁc hyperparameters based on the diﬀerences between the performance of default
hyperparameters and the performance of the hyperparameters when this hyperparameter is
set to an optimal value. Then we (iii) address the tunability of hyperparameter combinations
and joint gains, (iv) provide theoretical deﬁnitions for an appropriate hyperparameter space
on which tuning should be executed and (v) propose procedures to estimate these quantities
based on the results of a benchmark study with random hyperparameter conﬁgurations with
the help of surrogate models. In sections 4 and 5 we illustrate these concepts and methods
through an application. For this purpose we use benchmark results of six machine learning
algorithms with diﬀerent hyperparameters which were evaluated on 38 datasets from the
OpenML platform. Finally, in the last section 6 we conclude and discuss the results.

2 Related literature

To the best of our knowledge, only a limited amount of articles address the problem of
tunability and generation of tuning search spaces. Bergstra and Bengio (2012) compute the
relevance of the hyperparameters of neural networks and conclude that some are important
on all datasets, while others are only important on some datasets. Their conclusion is
primarily visual and used as an argument for why random search works better than grid
search when tuning neural networks.

A speciﬁc study for decision trees was conducted by Mantovani et al. (2016) who ap-
ply standard tuning techniques to decision trees on 102 datasets and calculate diﬀerences
of accuracy between the tuned algorithm and the algorithm with default hyperparameter
settings.

A diﬀerent approach is proposed by Hutter et al. (2013), which aims at identifying the
most important hyperparameters via forward selection. In the same vein, Fawcett and Hoos
(2016) present an ablation analysis technique, which aims at identifying the hyperparameters
that contribute the most to improved performance after tuning. For each of the considered
hyperparameters, they compute the performance gain that can be achieved by changing
its value from the initial value to the value speciﬁed in the target conﬁguration which was
determined by the tuning strategy. This procedure is iterated in a greedy forward search.

A more general framework for measuring the importance of single hyperparameters is
presented by Hutter et al. (2014). After having used a tuning strategy such as sequential
model-based optimization, a functional ANOVA approach is used for measuring the impor-
tance of hyperparameters.

These works concentrate on the importance of hyperparameters on single datasets,
mainly to retrospectively explain what happened during an already concluded tuning pro-
cess. Our main focus is the generalization across multiple datasets in order to facilitate
better general understanding of hyperparameter eﬀects and better decision making for fu-
ture experiments. In a recent paper van Rijn and Hutter (2017) pose very similar questions
to ours to assess the importance of hyperparameters across datasets. We compare it to our
approach in section 6.

2

Our framework is based on using surrogate models, also sometimes called empirical
performance models, which allow estimating the performance of arbitrary hyperparameter
conﬁgurations based on a limited number of prior experiments. The idea of surrogate models
is far from new, as it constitutes the central idea of Bayesian optimization for hyperparameter
search but is also used, for example, in Biedenkapp et al. (2017) for increasing the speed of
an ablation analysis and by Eggensperger et al. (2018) for speeding up the benchmarking of
tuning strategies.

3 Methods for Estimation of Defaults, Tunability and
Ranges

3.1 General notation

Consider a target variable Y , a feature vector X, and an unknown joint distribution P on
(X, Y ), from which we have sampled a dataset T of n observations. A machine learning (ML)
algorithm now learns the functional relationship between X and Y by producing a prediction
model ˆf (X, θ), controlled by the k-dimensional hyperparameter conﬁguration θ = (θ1, ..., θk)
from the hyperparameter search space Θ = Θ1 × ... × Θk. In order to measure prediction
performance pointwise between the true label Y and its prediction ˆf (X, θ), we deﬁne a loss
function L(Y, ˆf (X, θ)). We are naturally interested in estimating the expected risk of the
inducing algorithm, w.r.t. θ on new data, also sampled from P: R(θ) = E(L(Y, ˆf (X, θ))|P).
This mapping encodes, given a certain data distribution, a certain learning algorithm and a
certain performance measure, the numerical quality for any hyperparameter conﬁguration θ.
Given m diﬀerent datasets (or data distributions) P1, ..., Pm, we arrive at m hyperparameter
risk mappings

R(j)(θ) := E(L(Y, ˆf (X, θ))|Pj),

j = 1, ..., m.

For now, we assume all R(j)(θ) to be known, and show how to estimate them in section 3.7.

3.2 Optimal conﬁguration per dataset and optimal defaults

We ﬁrst deﬁne the best hyperparameter conﬁguration for dataset j as

θ(j)(cid:63) := arg min

R(j)(θ).

θ∈Θ

Defaults settings are supposed to work well across many diﬀerent datasets and are usually
provided by software packages, in an often ad hoc or heuristic manner. We propose to deﬁne
an optimal default conﬁguration, based on an extensive number of empirical experiments on
m diﬀerent benchmark datasets, by

(1)

(2)

(3)

θ(cid:63) := arg min

g(R(1)(θ), ..., R(m)(θ)).

θ∈Θ

Here, g is a summary function that has to be speciﬁed. Selecting the mean (or median
as a more robust candidate) would imply minimizing the average (or median) risk over all
datasets.

The measures R(j)(θ) could potentially be scaled appropriately beforehand in order to
make them more commensurable between datasets, e.g., one could scale all R(j)(θ) to [0, 1]
by substracting the result of a very simple baseline like a featureless dummy predictor and
dividing this diﬀerence by the absolute diﬀerence between the risk of the best possible
result (as an approximation of the Bayes error) and the result of the very simple baseline
predictor. Or one could produce a statistical z-score by subtracting the mean and dividing
by the standard deviation from all experimental results on the same dataset (Feurer et al.,
2018).

The appropriateness of the scaling highly depends on the performance measure that
is used. One could, for example, argue that the AUC does not have to be scaled as an
improvement from 0.5 to 0.6 can possibly be seen as important as an improvement from 0.8
to 0.9. On the other hand, averaging the mean squared error on several datasets does not

3

make a lot of sense, as the scale of the outcome of diﬀerent regression problems can be very
diﬀerent. Then scaling or using another measure such as R2 seems essential.

3.3 Measuring overall tunability of a ML algorithm

A general measure of the tunability of an algorithm per dataset can then be computed
based on the diﬀerence between the risk of an overall reference conﬁguration (e.g., either the
software defaults or deﬁnition (3)) and the risk of the best possible conﬁguration on that
dataset:

d(j) := R(j)(θ(cid:63)) − R(j)(θ(j)(cid:63)), for j = 1, ..., m.

(4)

For each algorithm, this gives rise to an empirical distribution of performance diﬀerences
over datasets, which might be directly visualized or summarized to an aggregated tunability
measure d by using mean, median or quantiles.

3.4 Measuring tunability of a speciﬁc hyperparameter

The best hyperparameter value for one parameter i on dataset j, when all other parameters
are set to defaults from θ(cid:63) := (θ(cid:63)
k), is denoted by

1, ..., θ(cid:63)

θ(j)(cid:63)
i

:= arg min
θ∈Θ,θl=θ(cid:63)

l ∀l(cid:54)=i

R(j)(θ).

A natural measure for tunability of the i-th parameter on dataset j is then the diﬀerence

in risk between the above and our default reference conﬁguration:
:= R(j)(θ(cid:63)) − R(j)(θ(j)(cid:63)

), for j = 1, ..., m, i = 1, ..., k.

d(j)
i

i

Furthermore, we deﬁne d(j),rel

d(j) as the fraction of performance gain, when we only
tune i compared to tuning the complete algorithm, on dataset j. Again, one can calculate
the mean, the median or quantiles of these two diﬀerences over the n datasets, to get a
notion of the overall tunability di of this parameter.

i

i

= d(j)

3.5 Tunability of hyperparamater combinations and joint gains

Let us now consider two hyperparameters indexed as i1 and i2. To measure the tunability
with respect to these two parameters, we deﬁne

θ(j)(cid:63)
i1,i2

:=

arg min

R(j)(θ),

θ∈Θ,θl=θ(cid:63)

l ∀l(cid:54)∈{i1,i2}

i.e., the θ-vector containing the default values for all hyperparameters other than i1 and i2,
and the optimal combination of values for the i1-th and i2-th components of θ.

Analogously to the previous section, we can now deﬁne the tunability of the set (i1, i2)

as the gain over the reference default on dataset j as:

d(j)
i1,i2

:=R(j)(θ∗) − R(j)(θ(j)(cid:63)
i1,i2

).

The joint gain which can be expected when tuning not only one of the two hyperparam-

eters individually, but both of them jointly, on a dataset j, can be expressed by:

g(j)
i1,i2

:= min{(R(j)(θ(j)(cid:63)

)), (R(j)(θ(j)(cid:63)

))} − R(j)(θ(j)(cid:63)
i1,i2

).

i2

i1

Furthermore, one could be interested in whether this joint gain could simply be reached
by tuning both parameters i1 and i2 in a univariate fashion sequentially, either in the order
i1 → i2 or i2 → i1, and what order would be preferable. For this purpose one could compare
the risk of the hyperparameter value that results when tuning them together R(j)(θ(j)(cid:63)
) with
i1,i2
the risks of the hyperparameter values that are obtained when tuning them sequentially, that
means R(j)(θ(j)(cid:63)

), which is done for example in Waldron et al. (2011).

) or R(j)(θ(j)(cid:63)

i1→i2

i2→i1

Again, all these measures should be summarized across datasets, resulting in di1,i2 and
gi1,i2. Of course, these approaches can be further generalized by considering combinations
of more than two parameters.

(5)

(6)

(7)

(8)

(9)

4

3.6 Optimal hyperparameter ranges for tuning

A reasonable hyperparameter space Θ(cid:63) for tuning should include the optimal conﬁguration
θ(j)(cid:63) for dataset j with high probability. We denote the p-quantile of the distribution of one
parameter regarding the best hyperparameters on each dataset (θ(1)(cid:63))i, ..., (θ(m)(cid:63))i as qi,p.
The hyperparameter tuning space can then be deﬁned as:

Θ(cid:63) := {θ ∈ Θ|∀i ∈ {1, ..., k} : θi ≥ qi,p1 ∧ θi ≤ qi,p2} ,
with p1 and p2 being quantiles which can be set for example to the 5 % quantile and the
95 % quantile. This avoids focusing too much on outlier datasets and makes the deﬁnition
of the space independent from the number of datasets.

(10)

The deﬁnition above is only valid for numerical hyperparameters. In case of categorical
variables one could use similar rules, for example only including hyperparameter values that
were at least once or in at least 10 % of the datasets the best possible hyperparameter
setting.

3.7 Practical estimation

In order to practically apply the previously deﬁned concepts, two remaining issues need to be
addressed: a) We need to discuss how to obtain R(j)(θ); and b) in (2) and (3) a multivariate
optimization problem (the minimization) needs to be solved1.

For a) we estimate R(j)(θ) by using surrogate models ˆR(j)(θ), and replace the original
quantity by its estimator in all previous formulas. Surrogate models for each dataset j are
based on a meta dataset. This is created by evaluating a large number of conﬁgurations of the
respective ML method. The surrogate regression model then learns to map a hyperparameter
conﬁguration to estimated performance. For b) we solve the optimization problem – now
cheap to evaluate, because of the surrogate models – through black-box optimization.

4 Experimental setup

In this section we give an overview about the experimental setup that is used for obtaining
surrogate models, tunability measures and tuning spaces.

4.1 Datasets from the OpenML platform

Recently, the OpenML project (Vanschoren et al., 2013) has been created as a ﬂexible online
platform that allows ML scientists to share their data, corresponding tasks and results of
diﬀerent ML algorithms. We use a speciﬁc subset of carefully curated classiﬁcation datasets
from the OpenML platform called OpenML100 (Bischl et al., 2017). For our study we only
use the 38 binary classiﬁcation tasks that do not contain any missing values.

4.2 ML Algorithms

The algorithms considered in this paper are common methods for supervised learning. We
examine elastic net (glmnet), decision tree (rpart), k-nearest neighbors (kknn), support
vector machine (svm), random forest (ranger) and gradient boosting (xgboost). For more
details about the used software packages see Kühn et al. (2018). An overview of their
considered hyperparameters is displayed in Table 1, including respective data types, box-
constraints and a potential transformation function.

In the case of xgboost, the underlying package only supports numerical features, so we
opted for a dummy feature encoding for categorical features, which is performed internally
by the underlying packages for svm and glmnet.

Some hyperparameters of the algorithms are dependent on others. We take into account
these dependencies and, for example, only sample a value for gamma for the support vector
machine if the radial kernel was sampled beforehand.

1All other previous optimization problems are univariate or two-dimensional and can simply be addressed by

a simple technique like a ﬁne grid search

5

Algorithm Hyperparameter
glmnet

Type Lower Upper Trafo

alpha
lambda

cp
maxdepth
minbucket
minsplit
-
k

kernel
cost
gamma
degree

rpart

kknn

svm

ranger

xgboost

num.trees
replace
sample.fraction
mtry
respect.unordered.factors
min.node.size

nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

numeric
numeric

numeric
integer
integer
integer
-
integer

discrete
numeric
numeric
integer

integer
logical
numeric
numeric
logical
numeric

integer
numeric
numeric
discrete
integer
numeric
numeric
numeric
numeric
numeric

0
-10

0
1
1
1

1

-
-10
-10
2

1
-
0.1
0
-
0

1
-10
0.1
-
1
0
0
0
-10
-10

1
10

1
30
60
60

30

-
10
10
5

2000
-
1
1
-
1

5000
0
1
-
15
7
1
1
10
10

-
2x

-
-
-
-

-

-
2x
2x
-

-
2x
-
-
-
2x
-
-
2x
2x

-
-
-
x · p
-
nx

Table 1: Hyperparameters of the algorithms. p refers to the number of variables and n to the number
of observations. The columns Lower and Upper indicate the regions from which samples of these hyper-
parameters are drawn. The transformation function in the trafo column, if any, indicates how the values
are transformed according to this function. The exponential transformation is applied to obtain more
candidate values in regions with smaller hyperparameters because for these hyperparameters the perfor-
mance diﬀerences between smaller values are potentially bigger than for bigger values. The mtry value
in ranger that is drawn from [0, 1] is transformed for each dataset separately. After having chosen the
dataset, the value is multiplied by the number of variables and afterwards rounded up. Similarly, for the
min.node.size the value x is transformed by the formula [nx] with n being the number of observations
of the dataset, to obtain a positive integer values with higher probability for smaller values (the value is
ﬁnally rounded to obtain integer values).

6

4.3 Performance estimation

Several measures are regarded throughout this paper, either for evaluating our considered
classiﬁcation models that should be tuned, or for evaluating our surrogate regression models.
As no optimal measure exists, we will compare several of them. In the classiﬁcation case,
we consider AUC, accuracy and brier score. In the case of surrogate regression, we consider
R2, which is directly proportional to the regular mean squared error but scaled to [0,1] and
explains the gain over a constant model estimating the overall mean of all data points. We
also compute Kendall’s tau as a ranking based measure for regression.

The performance estimation for the diﬀerent hyperparameter experiments is computed
through 10-fold cross-validation. For the comparison of surrogate models 10 times repeated
10-fold cross-validation is used.

4.4 Random Bot sampling strategy for meta data

To reliably estimate our surrogate models we need enough evaluated conﬁgurations per
classiﬁer and data set. We sample these points from independent uniform distributions
where the respective support for each parameter is displayed in Table 1. Here, uniform
refers to the untransformed scale, so we sample uniformly from the interval [Lower, Upper ]
of Table 1.

In order to properly facilitate the automatic computation of a large database of hyper-
In an embarrassingly
parameter experiments, we implemented a so called OpenML bot.
parallel manner it chooses in each iteration a random dataset, a random classiﬁcation al-
gorithm, samples a random conﬁguration and evaluates it via cross-validation. A subset
of 500000 experiments for each algorithm and all datasets are used for our analysis here.2
More technical details regarding the random bot, its setup and results can be obtained in
Kühn et al. (2018), furthermore, for simple and permanent access the results of the bot are
stored in a ﬁgshare repository (Kühn et al., 2018).

4.5 Optimizing surrogates to obtain optimal defaults

Random search is also used for our black-box optimization problems in section 3.7. For
the estimation of the defaults for each algorithm we randomly sample 100000 points in
the hyperparameter space as deﬁned in Table 1 and determine the conﬁguration with the
minimal average risk. The same strategy with 100000 random points is used to obtain
the best hyperparameter setting on each dataset that is needed for the estimation of the
tunability of an algorithm. For the estimation of the tunability of single hyperparameters we
also use 100000 random points for each parameter, while for the tunability of combination
of hyperparameters we only use 10000 random points to reduce runtime as this should be
enough to cover 2-dimensional hyperparameter spaces.

Of course one has to be careful with overﬁtting here, as our new defaults are chosen
with the help of the same datasets that are used to determine the performance. Therefore,
we also evaluate our approach via a “10-fold cross-validation across datasets”. Here, we
repeatedly calculate the optimal defaults based on 90% “training datasets” and evaluate the
package defaults and our optimal defaults – the latter induced from the training data sets –
on the surrogate models of the remaining 10% “test datasets”, and compare their diﬀerence
in performance.

4.6 The problem of hyperparameter dependency

Some parameters are dependent on other superordinate hyperparameters and are only rele-
vant if the parameter value of this superordinate parameter was set to a speciﬁc value. For
example gamma in svm only makes sense if the kernel was set to “radial“ or degree only
makes sense if the kernel was set to “polynomial“. Some of these subordinate parameters
might be invalid/inactive in the reference default conﬁguration, rendering it impossible to
univariately tune them in order to compute their tunability score. In such a case we set the

230 for each dataset for kknn

7

superordinate parameter to a value which makes the subordinate parameter active, compute
the optimal defaults for the rest of the parameters and compute the tunability score for the
subordinate parameter with these defaults.

4.7 Software details

All our experiments are executed in R and are run through a combination of custom code
from our random bot Kühn et al. (2018), the OpenML R package (Casalicchio et al., 2017),
mlr (Bischl et al., 2016) and batchtools (Lang et al., 2017) for parallelization. All results
are uploaded to the OpenML platform and there publicly available for further analysis.
mlr is also used to compare and ﬁt all surrogate regression models. The fully reproducible
R code for all computations and analyses of our paper can be found on the github page:
https://github.com/PhilippPro/tunability. We also provide an interactive shiny app
under https://philipppro.shinyapps.io/tunability/, which displays all results of the
following section in a potentially more convenient, interactive fashion and which can simply
be accessed through a web browser.

5 Results and discussion

We calculate all results for AUC, accuracy and brier score but mainly discuss AUC results
here. Tables and ﬁgures for the other measures can be accessed in the Appendix and in our
interactive shiny application.

5.1 Surrogate models

We compare diﬀerent possible regression models as candidates for our surrogate models: the
linear model (lm), a simple decision tree (rpart), k nearest-neighbors (kknn) and random
forest (ranger)3 All algorithms are run with their default settings. We calculate 10 times
repeated 10-fold cross-validated regression performance measures R2 and Kendall’s tau per
dataset, and average these across all datasets4. Results for AUC are displayed in Figure
1. A good overall performance is achieved by ranger with qualitatively similar results for
other classiﬁcation performance measures (see Appendix). In the following we use random
forest as surrogate model because it performs reasonably well and is already an established
algorithm for surrogate models in the literature (Eggensperger et al., 2014; Hutter et al.,
2013).

5.1.1 Optimal defaults and tunability

Table 2 displays our mean tunability results for the algorithms as deﬁned in formula (4)
w.r.t. package defaults (Def.P column) and our optimal defaults (Def.O). It also displays
the improvement per algorithm when moving from package defaults to optimal defaults
(Improv), which was positive overall. This also holds for svm and ranger although the
package defaults are data dependent, which we currently cannot model (gamma = 1/p for
p for ranger). From now on, when discussing tunability, we will only do
svm and mtry =
this w.r.t. our optimal defaults.

√

Clearly, some algorithms such as glmnet and svm are much more tunable than the others,
while ranger is the algorithm with the smallest tunability, which is in line with common
In Figure 2 modiﬁed boxplots of the tunabilities are
knowledge in the web community.
depicted. For each ML algorithm, some outliers are visible, which indicates that tuning has
a much higher impact on some speciﬁc datasets.

3We also tried cubist (Kuhn et al., 2016), which provided good results but the algorithm had some technical
problems for some combinations of datasets and algorithms. We did not include gaussian process which is one of
the standard algorithms for surrogate models as it cannot handle categorical variables.

4In case of kknn four datasets did not provide results for one of the surrogate models and were not used.

8

Figure 1: Average performances over the datasets of diﬀerent surrogate models (target: AUC)
for diﬀerent algorithms (that were presented in 4.2).

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.045
0.024
0.025
0.012
0.025
0.006
0.014
0.042
0.004
0.006
0.029
0.014

0.069
glmnet
0.038
rpart
0.031
kknn
svm 0.056
0.010
0.043

0.037
0.016
0.006
0.048
0.007
0.017

ranger
xgboost

Impr-CV
0.032
0.022
0.025
0.008
0.003
0.026

Table 2: Overall tunability (regarding AUC) with the package defaults (Tun.P) and the new
defaults (Tun.O) as reference, cross-validated tunability (Tun.O-CV), average improvement (Im-
prov) and cross-validated average improvement (Impr-CV) obtained by using new defaults com-
pared to old defaults. The (cross-validated) improvement can be calculated by the (rounded)
diﬀerence between Tun.P and Tun.O (Tun.O-CV).

9

Figure 2: Boxplots of the tunabilities (AUC) of the diﬀerent algorithms. The upper and lower
whiskers (upper and lower line of the boxplot rectangle) are in our case deﬁned as the 0.1
and 0.9 quantiles of the tunability scores. The 0.9 quantile indicates how much performance
improvement can be expected on at least 10% of datasets. One outlier of glmnet (value 0.5) is
not shown.

5.1.2 Tunability of speciﬁc hyperparameters

In Table 3 the mean tunability (regarding the AUC) of single hyperparameters as deﬁned
in Equation (6) in section 3.4 can be seen. From here on, we will refer to tunability only
with respect to optimal defaults.

For glmnet lambda seems to be more tunable than alpha.

In rpart the minbucket
and minsplit parameters seem to be the most important ones for tuning. k in the kknn
algorithm is very tunable w.r.t. package defaults, but not regarding optimal defaults. In svm
the biggest gain in performance can be achieved by tuning the kernel, gamma or degree,
while the cost parameter does not seem to be very tunable. In ranger mtry is the most
tunable parameter. For xgboost there are two parameters that are quite tunable: eta and
the booster. booster speciﬁes if a tree or a linear model is trained. The cross-validated
results can be seen in Table 10 in the Appendix, they are quite similar to the non cross-
validated results and for all parameters slightly higher.

Instead of looking only at the average, as in Table 3, one could also be interested in the
distribution of the tunability of each dataset. As an example, Figure 3 shows the tunability
of each parameter of ranger in a boxplot. This gives a more in-depth insight into the
tunability, makes it possible to detect outliers and to examine the skewness.

5.1.3 Hyperparameter space for tuning

The hyperparameter space for tuning, as deﬁned in Equation (10) in section 3.6 and based
on the 0.05 and 0.95 quantiles, is displayed in Table 3. All optimal defaults are contained
in this hyperparameter space while some of the package defaults are not.

As an example, Figure 4 displays the full histogram of the best values of mtry of the
random forest over all datasets. Note that for quite a few data sets much higher values than
the package defaults seem advantageous. Analogous histograms for other parameters are
available through the shiny app.

10

Parameter Def.P

q0.05

q0.95

1
0

7

0.01
30
7
20

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

radial
1
1/p
3

radial
682.478
0.005
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
0

sample.fraction
mtry

√

983
FALSE
0.703
p · 0.257
FALSE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

30

0
21
12
24

0.403
0.004

Def.O Tun.P Tun.O
0.024
0.069
0.006
0.038
0.021
0.034
0.012
0.038
0.002
0.025
0.002
0.004
0.006
0.005
0.004
0.004
0.006
0.031
0.006
0.031
0.042
0.056
0.024
0.030
0.006
0.016
0.022
0.030
0.014
0.008
0.006
0.010
0.001
0.001
0.001
0.002
0.002
0.004
0.003
0.006
0.000
0.000
0.001
0.001
0.014
0.043
0.002
0.004
0.005
0.006
0.002
0.004
0.008
0.015
0.001
0.001
0.002
0.008
0.001
0.006
0.001
0.008
0.002
0.003
0.002
0.003

4168
0.018
0.839
gbtree
13
2.06
0.752
0.585
0.982
1.113

0.009
0.001

0
12.1
3.85
5

9.95

0.981
0.147

0.008
27
41.6
49.15

30

0.002
0.003
2

920.582
18.195
4

206.35

1740.15

0.323
0.035

0.974
0.692

0.007

0.513

920.7
0.002
0.545

5.6
1.295
0.419
0.335
0.008
0.002

4550.95
0.355
0.958

14
6.984
0.864
0.886
29.755
6.105

Table 3: Defaults (package defaults (Def.P) and optimal defaults (Def.O)), tunability of the hy-
perparameters with the package defaults (Tun.P) and our optimal defaults (Tun.O) as reference
and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters of the algorithms.

11

Figure 3: Boxplots of the tunabilities of the diﬀerent parameters of ranger. Same deﬁnition of
whiskers as in Figure 2.

Figure 4: Histogram of best parameter values for mtry of random forest over all considered data
sets.

12

5.1.4 Tunability of hyperparameter combinations

As an example, Table 4 displays the average tunability di1,i2 of all 2-way hyperparameter
combinations for rpart. Obviously, the increased ﬂexibility in tuning a 2-way combination
enables larger improvements when compared with the tunability of one of the respective
individual parameters.

In Table 5 the joint gain of tuning two hyperparameters gi1,i2 instead of only the best
as deﬁned in section 3.5 can be seen. The parameters minsplit and minbucket have the
biggest joint eﬀect, which is not very surprising, as they are closely related: minsplit is
the minimum number of observations that must exist in a node in order for a split to be
attempted and minbucket the minimum number of observations in any terminal leaf node.
If a higher value of minsplit than the default performs better on a dataset it is possibly
not enough to set it higher without also increasing minbucket, so the strong relationship is
quite clear. Again, further ﬁgures for other algorithms are available through the shiny app.

cp
maxdepth
minbucket
minsplit

0.002

cp maxdepth minbucket minsplit
0.004
0.005
0.011
0.004

0.006
0.007
0.006

0.003
0.002

Table 4: Tunability di1,i2 of hyperparameters of rpart, diagonal shows tunability of the single
hyperparameters.

cp
maxdepth
minbucket

0.0007

maxdepth minbucket minsplit
0.0004
0.0019
0.0055

0.0005
0.0014

Table 5: Joint gain gi1,i2 of tuning two hyperparameters instead of the most important in rpart.

6 Conclusion and Discussion

Our paper provides concise and intuitive deﬁnitions for optimal defaults of ML algorithms
and the impact of tuning them either jointly, tuning individual parameters or combinations,
all based on the general concept of surrogate empirical performance models. Tunability
values as deﬁned in our framework are easily and directly interpretable as how much per-
formance can be gained by tuning this hyperparameter?. This allows direct comparability of
the tunability values across diﬀerent algorithms.

In an extensive OpenML benchmark, we computed optimal defaults for elastic net, de-
cision tree, k-nearest neighbors, SVM, random forest and xgboost and quantiﬁed their tun-
ability and the tunability of their individual parameters. This – to the best of our knowledge
– has never been provided before in such a principled manner. Our results are often in line
with common knowledge from literature and our method itself now allows an analogous
analysis for other or more complex methods.

Our framework is based on the concept of default hyperparameter values, which can be
seen both as an advantage (default values are a valuable output of the approach) and as
an inconvenience (the determination of the default values is an additional analysis step and
needed as a reference point for most of our measures).

We now compare our method with van Rijn and Hutter (2017). In contrast to us, they
apply the functional ANOVA framework from Hutter et al. (2014) on a surrogate random
forest to assess the importance of hyperparameters regarding empirical performance of a
support vector machine, random forest and adaboost, which results in numerical importance

13

scores for individual hyperparameters. Their numerical scores are - in our opinion - less
directly interpretable, but they do not rely on defaults as a reference point, which one
might see as an advantage. They also propose a method for calculating hyperparameter
priors, combine it with the tuning procedure hyperband, and assess the performance of this
new tuning procedure. In contrast, we deﬁne and calculate ranges for all hyperparameters.
Setting ranges for the tuning space can be seen as a special case of a prior distribution - the
uniform distribution on the speciﬁed hyperparameter space. Regarding the experimental
setup, we compute more hyperparameter runs (around 2.5 million vs. 250000), but consider
only the 38 binary classiﬁcation datasets of OpenML100 while van Rijn and Hutter (2017)
use all the 100 datasets which also contain multiclass datasets. We evaluate the performance
of diﬀerent surrogate models by 10 times repeated 10-fold cross-validation to choose an
appropriate model and to assure that it performs reasonably well.

Our study has some limitations that could be addressed in the future: a) We only con-
sidered binary classiﬁcation, where we tried to include a wider variety of datasets from
diﬀerent domains. In principle this is not a restriction as our methods can easily be applied
to multiclass classiﬁcation, regression, survival analysis or even algorithms not from machine
learning whose empirical performance is reliably measurable on a problem instance. b) Uni-
form random sampling of hyperparameters might not scale enough for very high dimensional
spaces, and a smarter sequential technique might be in order here, see (Bossek et al., 2015)
for an potential approach of sampling across problem instances to learn optimal mappings
from problem characteristics to algorithm conﬁgurations. c) We currently are learning static
defaults, which cannot depend on dataset characteristics (like number of features, or fur-
ther statistical measures) as in meta-learning. Doing so might improve performance results
of optimal defaults considerably, but would require a more complicated approach. d) Our
approach still needs initial ranges to be set, in order to run our sampling procedure. Only
based on these wider ranges we can then compute more precise, closer ranges.

Acknowledgements

We would like to thank Joaquin Vanschoren for support regarding the OpenML platform
and Andreas Müller, Jan van Rijn, Janek Thomas and Florian Pﬁsterer for reviewing and
useful comments. Thanks to Jenny Lee for language editing.

References

J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of

Machine Learning Research, 13:281–305, 2012.

A. Biedenkapp, M. T. Lindauer, K. Eggensperger, F. Hutter, C. Fawcett, and H. H. Hoos.
In AAAI, pages

Eﬃcient parameter importance analysis via ablation with surrogates.
773–779, 2017.

M. Birattari, Z. Yuan, P. Balaprakash, and T. Stützle. F-Race and iterated F-Race: An
overview. In Experimental Methods for the Analysis of Optimization Algorithms, pages
311–336. Springer, 2010.

B. Bischl, O. Mersmann, H. Trautmann, and C. Weihs. Resampling methods for meta-model
validation with recommendations for evolutionary computation. Evolutionary Computa-
tion, 20(2):249–275, 2012.

B. Bischl, M. Lang, L. Kotthoﬀ, J. Schiﬀner, J. Richter, E. Studerus, G. Casalicchio, and
Z. M. Jones. mlr: Machine learning in R. Journal of Machine Learning Research, 17
(170):1–5, 2016.

B. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. van Rijn,
and J. Vanschoren. OpenML benchmarking suites and the OpenML100. ArXiv preprint
arXiv:1708.03731, 2017. URL https://arxiv.org/abs/1708.03731.

14

B. Bischl, J. Richter, J. Bossek, D. Horn, J. Thomas, and M. Lang. mlrMBO: A modular
framework for model-based optimization of expensive black-box functions. ArXiv preprint
arXiv:1703.03373, 2017. URL https://arxiv.org/abs/1703.03373.

J. Bossek, B. Bischl, T. Wagner, and G. Rudolph. Learning feature-parameter mappings
for parameter tuning via the proﬁle expected improvement. In Proceedings of the 2015
Annual Conference on Genetic and Evolutionary Computation, pages 1319–1326. ACM,
2015.

G. Casalicchio, J. Bossek, M. Lang, D. Kirchhoﬀ, P. Kerschke, B. Hofner, H. Seibold, J. Van-
schoren, and B. Bischl. OpenML: An R package to connect to the machine learning
platform OpenML. Computational Statistics, 32(3):1–15, 2017.

K. Eggensperger, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Surrogate benchmarks for
hyperparameter optimization.
In Proceedings of the 2014 International Conference on
Meta-learning and Algorithm Selection-Volume 1201, pages 24–31. CEUR-WS. org, 2014.

K. Eggensperger, M. Lindauer, H. H. Hoos, F. Hutter, and K. Leyton-Brown. Eﬃcient
benchmarking of algorithm conﬁgurators via model-based surrogates. Machine Learning,
pages 1–27, 2018.

A. E. Eiben and S. K. Smit. Parameter tuning for conﬁguring and analyzing evolutionary

algorithms. Swarm and Evolutionary Computation, 1(1):19–31, 2011.

C. Fawcett and H. H. Hoos. Analysing diﬀerences between algorithm conﬁgurations through

ablation. Journal of Heuristics, 22(4):431–458, 2016.

M. Feurer, B. Letham, and E. Bakshy. Scalable meta-learning for bayesian optimization.

arXiv preprint 1802.02219, 2018. URL https://arxiv.org/abs/1802.02219.

I. Guyon, A. Saﬀari, G. Dror, and G. Cawley. Model selection: Beyond the bayesian/fre-

quentist divide. Journal of Machine Learning Research, 11(Jan):61–87, 2010.

F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for
general algorithm conﬁguration. In International Conference on Learning and Intelligent
Optimization, pages 507–523. Springer, 2011.

F. Hutter, H. H. Hoos, and K. Leyton-Brown. Identifying key algorithm parameters and
instance features using forward selection. In International Conference on Learning and
Intelligent Optimization, pages 364–381. Springer, 2013.

F. Hutter, H. Hoos, and K. Leyton-Brown. An eﬃcient approach for assessing hyperparam-
eter importance. In ICML, volume 32 of JMLR Workshop and Conference Proceedings,
pages 754–762, 2014.

D. Kühn, P. Probst, J. Thomas, and B. Bischl. Automatic Exploration of Machine
Learning Experiments on OpenML. ArXiv preprint arXiv:1806.10961, 2018. URL
https://arxiv.org/abs/1806.10961.

M. Kuhn, S. Weston, C. Keefer, and N. Coulter. Cubist: Rule- and instance-based regression

modeling, 2016. R package version 0.0.19.

D.

Kühn,
bot

P.
Probst,
benchmark

J.
data

Thomas,
(ﬁnal

and
B.
subset),

Bischl.
2018.

R
https://figshare.com/articles/OpenML_R_Bot_Benchmark_Data_final_subset_/5882230/2.

OpenML
URL

M. Lang, B. Bischl, and D. Surmann. batchtools: Tools for R to work on batch systems.

The Journal of Open Source Software, 2(10), 2017.

G. Luo. A review of automatic selection methods for machine learning algorithms and hyper-
parameter values. Network Modeling Analysis in Health Informatics and Bioinformatics,
5(1):1–16, 2016.

15

R. G. Mantovani, T. Horváth, R. Cerri, A. Carvalho, and J. Vanschoren. Hyper-parameter
In Brazilian Conference on Intelligent

tuning of a decision tree induction algorithm.
Systems (BRACIS 2016), 2016.

J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pages 2951–
2959, 2012.

J. N. van Rijn and F. Hutter. Hyperparameter importance across datasets. ArXiv preprint

arXiv:1710.04725, 2017. URL https://arxiv.org/abs/1710.04725.

J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo. OpenML: Networked science in

machine learning. SIGKDD Explorations, 15(2):49–60, 2013.

L. Waldron, M. Pintilie, M.-S. Tsao, F. A. Shepherd, C. Huttenhower, and I. Jurisica.
Optimized application of penalized regression methods to diverse genomic data. Bioin-
formatics, 27(24):3399–3406, 2011.

16

Appendix A. Results for accuracy and brier score

Figure 5: Same as ﬁgure 1 but with accuracy as target measure. Average performances over
the datasets of diﬀerent surrogate models (target: accuracy) for diﬀerent algorithms (that were
presented in 4.2).

Figure 6: Same as ﬁgure 1 but with brier score as target measure. Average performances over
the datasets of diﬀerent surrogate models (target: brier score) for diﬀerent algorithms (that were
presented in 4.2).

17

Figure 7: Boxplots of the tunabilities (accuracy) of the diﬀerent algorithms.

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.023
0.019
0.008
0.012
0.013
0.008
0.011
0.030
0.009
0.007
0.023
0.011

0.042
glmnet
0.020
rpart
0.021
kknn
svm 0.041
0.016
0.034

0.042
0.014
0.010
0.041
0.009
0.012

ranger
xgboost

Impr-CV
0.001
0.005
0.010
-0.001
0.006
0.022

Table 6: Tunability measures as in table 2, but calculated for the accuracy. Overall tunabil-
ity (regarding accuracy) with the package defaults (Def.P) and the optimal defaults (Def.O)
as reference points, cross-validated tunability (Def.O-CV), average improvement (Improv) and
cross-validated average improvement (Impr-CV) obtained by using new defaults compared to
old defaults. The (cross-validated) improvement can be calculated by the (rounded) diﬀerence
between Def.P and Def.O (Def.O-CV).

18

Parameter Def.P

Def.O Tun.P Tun.O q0.05

q0.95

7

14

2

30

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

1
0

0.01
30
7
20

0.252
0.005

0.002
19
5
13

radial
1
1/p
3

radial
936.982
0.002
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
1

sample.fraction
mtry

√

162
FALSE
0.76
p · 0.432
TRUE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

3342
0.031
0.89
gbtree
14
1.264
0.712
0.827
2.224
0.021

0.042
0.022
0.029
0.020
0.013
0.004
0.005
0.002
0.021
0.021
0.041
0.019
0.019
0.024
0.005
0.016
0.001
0.004
0.003
0.010
0.001
0.001
0.034
0.004
0.005
0.003
0.008
0.001
0.009
0.005
0.006
0.002
0.003

0.019
0.010
0.017
0.012
0.008
0.004
0.006
0.003
0.008
0.008
0.030
0.018
0.003
0.020
0.014
0.007
0.001
0.001
0.003
0.003
0.000
0.002
0.011
0.002
0.005
0.002
0.005
0.001
0.002
0.001
0.001
0.002
0.002

0.015
0.001

0
10
1.85
6.7

0.979
0.223

0.528
28
43.15
47.6

0.025
0.007
2

943.704
276.02
4

203.5

1908.25

0.257
0.081

0.971
0.867

0.009

0.453

1360
0.002
0.555

3
1.061
0.334
0.348
0.004
0.003

4847.15
0.445
0.964

13
7.502
0.887
0.857
5.837
2.904

Table 7: Tunability measures for single hyperparameters and tuning spaces as in table 3, but
calculated for the accuracy. Defaults (package defaults (Def.P) and own calculated defaults
(Def.O)), tunability of the hyperparameters with the package defaults (Tun.P) and our new
defaults (Tun.O) as reference and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters
of the algorithms.

19

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.011
0.010
0.006
0.009
0.009
0.003
0.008
0.018
0.010
0.005
0.018
0.009

0.022
glmnet
0.015
rpart
0.012
kknn
svm 0.026
0.015
0.027

0.020
0.011
0.003
0.023
0.006
0.011

ranger
xgboost

Impr-CV
0.001
0.004
0.009
0.003
0.009
0.016

Table 8: Tunability measures as in table 2, but calculated for the brier score. Overall tunability
(regarding brier score) with the package defaults (Def.P) and the optimal defaults (Def.O)
as reference points, cross-validated tunability (Def.O-CV), average improvement (Improv) and
cross-validated average improvement (Impr-CV) obtained by using new defaults compared to
old defaults. The (cross-validated) improvement can be calculated by the (rounded) diﬀerence
between Def.P and Def.O (Def.O-CV).

Figure 8: Boxplots of the tunabilities (brier score) of the diﬀerent algorithms.

20

Parameter Def.P

q0.05

q0.95

1
0

7

0.01
30
7
20

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

radial
1
1/p
3

radial
950.787
0.005
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
1

sample.fraction
mtry

√

198
FALSE
0.667
p · 0.666
TRUE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

19

0.997
0.004

0.001
13
12
18

Def.O Tun.P Tun.O
0.010
0.022
0.005
0.009
0.007
0.014
0.009
0.015
0.003
0.009
0.002
0.002
0.006
0.004
0.002
0.002
0.003
0.012
0.003
0.012
0.018
0.026
0.011
0.013
0.002
0.012
0.012
0.015
0.009
0.003
0.005
0.015
0.001
0.001
0.001
0.002
0.003
0.002
0.002
0.010
0.000
0.000
0.001
0.001
0.009
0.027
0.002
0.004
0.005
0.004
0.002
0.002
0.004
0.009
0.001
0.001
0.002
0.007
0.002
0.004
0.001
0.004
0.003
0.002
0.004
0.003

2563
0.052
0.873
gbtree
11
1.75
0.713
0.638
0.101
0.894

0.003
0.001

0
9
1
7

0.974
0.051

0.035
27.15
44.1
49.15

4.85

30

0.002
0.001
2

963.81
4.759
4

187.85

1568.25

0.317
0.072

0.964
0.954

0.008

0.394

2018.55
0.003
0.447

4780.05
0.436
0.951

2.6
1.277
0.354
0.363
0.006
0.003

13
5.115
0.922
0.916
28.032
2.68

Table 9: Tunability measures for single hyperparameters and tuning spaces as in table 3, but
calculated for the brier score. Defaults (package defaults (Def.P) and own calculated defaults
(Def.O)), tunability of the hyperparameters with the package defaults (Tun.P) and our new
defaults (Tun.O) as reference and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters
of the algorithms.

21

Measure

AUC

Accuracy

Brier score

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k

Parameter Tun.O Tun.O-CV Tun.O Tun.O-CV Tun.O Tun.O-CV
0.020
0.019
0.015
0.010
0.018
0.017
0.011
0.012
0.005
0.008
0.003
0.004
0.006
0.006
0.003
0.003
0.003
0.008
0.003
0.008
0.023
0.030
0.016
0.018
0.002
0.003
0.016
0.020
0.014
0.014
0.006
0.007
0.001
0.001
0.001
0.001
0.003
0.003
0.003
0.003
0.000
0.000
0.001
0.002
0.011
0.011
0.002
0.002
0.006
0.005
0.002
0.002
0.004
0.005
0.001
0.001
0.003
0.002
0.002
0.001
0.002
0.001
0.004
0.002
0.004
0.002

0.024
0.006
0.021
0.012
0.002
0.002
0.006
0.004
0.006
0.006
svm 0.042
0.024
0.006
0.022
0.014
0.006
0.001
0.001
0.002
0.003
0.000
0.001
0.014
0.002
0.005
0.002
0.008
0.001
0.002
0.001
0.001
0.002
0.002

0.037
0.006
0.034
0.016
0.002
0.002
0.009
0.004
0.006
0.006
0.048
0.030
0.006
0.028
0.020
0.007
0.002
0.002
0.002
0.004
0.000
0.001
0.017
0.002
0.006
0.002
0.008
0.001
0.003
0.002
0.001
0.003
0.004

0.010
0.005
0.007
0.009
0.003
0.002
0.006
0.002
0.003
0.003
0.018
0.011
0.002
0.012
0.009
0.005
0.001
0.001
0.003
0.002
0.000
0.001
0.009
0.002
0.005
0.002
0.004
0.001
0.002
0.002
0.001
0.003
0.004

0.042
0.026
0.039
0.014
0.008
0.004
0.007
0.003
0.010
0.010
0.041
0.031
0.003
0.031
0.027
0.009
0.003
0.002
0.003
0.005
0.001
0.002
0.012
0.003
0.006
0.002
0.005
0.001
0.002
0.001
0.001
0.003
0.003

kernel
cost
gamma
degree
ranger
num.trees
replace
sample.fraction
mtry
respect.unordered.factors
min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

Table 10: Tunability with calculated defaults as reference without (Tun.O) and with (Tun.O-CV)
cross-validation for AUC, accuracy and brier score

22

8
1
0
2
 
t
c
O
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
6
9
5
9
0
.
2
0
8
1
:
v
i
X
r
a

Tunability: Importance of Hyperparameters of Machine
Learning Algorithms

by Philipp Probst, Anne-Laure Boulesteix and Bernd Bischl

October 23, 2018

Abstract

Modern supervised machine learning algorithms involve hyperparameters that have to
be set before running them. Options for setting hyperparameters are default values from
the software package, manual conﬁguration by the user or conﬁguring them for optimal
predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly,
we formalize the problem of tuning from a statistical point of view, deﬁne data-based de-
faults and suggest general measures quantifying the tunability of hyperparameters of al-
gorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets
from the OpenML platform and six common machine learning algorithms. We apply our
measures to assess the tunability of their parameters. Our results yield default values for
hyperparameters and enable users to decide whether it is worth conducting a possibly time
consuming tuning strategy, to focus on the most important hyperparameters and to chose
adequate hyperparameter spaces for tuning.

1

Introduction

Machine learning (ML) algorithms such as gradient boosting, random forest and neural net-
works for regression and classiﬁcation involve a number of hyperparameters that have to
be set before running them. In contrast to direct, ﬁrst-level model parameters, which are
determined during training, these second-level tuning parameters often have to be carefully
optimized to achieve maximal performance. A related problem exists in many other algo-
rithmic areas, e.g., control parameters in evolutionary algorithms (Eiben and Smit, 2011).
In order to select an appropriate hyperparameter conﬁguration for a speciﬁc dataset
at hand, users of ML algorithms can resort to default values of hyperparameters that are
speciﬁed in implementing software packages or manually conﬁgure them, for example, based
on recommendations from the literature, experience or trial-and-error.

Alternatively, one can use hyperparameter tuning strategies, which are data-dependent,
second-level optimization procedures (Guyon et al., 2010), which try to minimize the ex-
pected generalization error of the inducing algorithm over a hyperparameter search space
of considered candidate conﬁgurations, usually by evaluating predictions on an independent
test set, or by running a resampling scheme such as cross-validation (Bischl et al., 2012).
For a recent overview of tuning strategies, see, e.g., Luo (2016).

These search strategies range from simple grid or random search (Bergstra and Bengio,
2012) to more complex, iterative procedures such as Bayesian optimization (Hutter et al.,
2011; Snoek et al., 2012; Bischl et al., 2017) or iterated F-racing (Birattari et al., 2010;
Lang et al., 2017).

In addition to selecting an eﬃcient tuning strategy, the set of tunable hyperparameters
and their corresponding ranges, scales and potential prior distributions for subsequent sam-
pling have to be determined by the user. Some hyperparameters might be safely set to
default values, if they work well across many diﬀerent scenarios. Wrong decisions in these
areas can inhibit either the quality of the resulting model or at the very least the eﬃciency
and fast convergence of the tuning procedure. This creates a burden for:

1

1. ML users – Which hyperparameters should be tuned and in which ranges?

2. Designers of ML algorithms – How do I deﬁne robust defaults?

We argue that many users, especially if they do not have years of practical experience in
the ﬁeld, here often rely on heuristics or spurious knowledge. It should also be noted that
designers of fully automated tuning frameworks face at least very similar problems. It is
not clear how these questions should be addressed in a data-dependent, automated, optimal
and objective manner. In other words, the scientiﬁc community not only misses answers to
these questions for many algorithms but also a systematic framework, methods and criteria,
which are required to answer these questions.

With the present paper we aim at ﬁlling this gap and formalize the problem of parameter
tuning from a statistical point of view, in order to simplify the tuning process for less
experienced users and to optimize decision making for more advanced processes.

After presenting related literature in section 2, we deﬁne theoretical measures for assess-
ing the impact of tuning in section 3. For this purpose we (i) deﬁne the concept of default
hyperparameters, (ii) suggest measures for quantiﬁying the tunability of the whole algorithm
and speciﬁc hyperparameters based on the diﬀerences between the performance of default
hyperparameters and the performance of the hyperparameters when this hyperparameter is
set to an optimal value. Then we (iii) address the tunability of hyperparameter combinations
and joint gains, (iv) provide theoretical deﬁnitions for an appropriate hyperparameter space
on which tuning should be executed and (v) propose procedures to estimate these quantities
based on the results of a benchmark study with random hyperparameter conﬁgurations with
the help of surrogate models. In sections 4 and 5 we illustrate these concepts and methods
through an application. For this purpose we use benchmark results of six machine learning
algorithms with diﬀerent hyperparameters which were evaluated on 38 datasets from the
OpenML platform. Finally, in the last section 6 we conclude and discuss the results.

2 Related literature

To the best of our knowledge, only a limited amount of articles address the problem of
tunability and generation of tuning search spaces. Bergstra and Bengio (2012) compute the
relevance of the hyperparameters of neural networks and conclude that some are important
on all datasets, while others are only important on some datasets. Their conclusion is
primarily visual and used as an argument for why random search works better than grid
search when tuning neural networks.

A speciﬁc study for decision trees was conducted by Mantovani et al. (2016) who ap-
ply standard tuning techniques to decision trees on 102 datasets and calculate diﬀerences
of accuracy between the tuned algorithm and the algorithm with default hyperparameter
settings.

A diﬀerent approach is proposed by Hutter et al. (2013), which aims at identifying the
most important hyperparameters via forward selection. In the same vein, Fawcett and Hoos
(2016) present an ablation analysis technique, which aims at identifying the hyperparameters
that contribute the most to improved performance after tuning. For each of the considered
hyperparameters, they compute the performance gain that can be achieved by changing
its value from the initial value to the value speciﬁed in the target conﬁguration which was
determined by the tuning strategy. This procedure is iterated in a greedy forward search.

A more general framework for measuring the importance of single hyperparameters is
presented by Hutter et al. (2014). After having used a tuning strategy such as sequential
model-based optimization, a functional ANOVA approach is used for measuring the impor-
tance of hyperparameters.

These works concentrate on the importance of hyperparameters on single datasets,
mainly to retrospectively explain what happened during an already concluded tuning pro-
cess. Our main focus is the generalization across multiple datasets in order to facilitate
better general understanding of hyperparameter eﬀects and better decision making for fu-
ture experiments. In a recent paper van Rijn and Hutter (2017) pose very similar questions
to ours to assess the importance of hyperparameters across datasets. We compare it to our
approach in section 6.

2

Our framework is based on using surrogate models, also sometimes called empirical
performance models, which allow estimating the performance of arbitrary hyperparameter
conﬁgurations based on a limited number of prior experiments. The idea of surrogate models
is far from new, as it constitutes the central idea of Bayesian optimization for hyperparameter
search but is also used, for example, in Biedenkapp et al. (2017) for increasing the speed of
an ablation analysis and by Eggensperger et al. (2018) for speeding up the benchmarking of
tuning strategies.

3 Methods for Estimation of Defaults, Tunability and
Ranges

3.1 General notation

Consider a target variable Y , a feature vector X, and an unknown joint distribution P on
(X, Y ), from which we have sampled a dataset T of n observations. A machine learning (ML)
algorithm now learns the functional relationship between X and Y by producing a prediction
model ˆf (X, θ), controlled by the k-dimensional hyperparameter conﬁguration θ = (θ1, ..., θk)
from the hyperparameter search space Θ = Θ1 × ... × Θk. In order to measure prediction
performance pointwise between the true label Y and its prediction ˆf (X, θ), we deﬁne a loss
function L(Y, ˆf (X, θ)). We are naturally interested in estimating the expected risk of the
inducing algorithm, w.r.t. θ on new data, also sampled from P: R(θ) = E(L(Y, ˆf (X, θ))|P).
This mapping encodes, given a certain data distribution, a certain learning algorithm and a
certain performance measure, the numerical quality for any hyperparameter conﬁguration θ.
Given m diﬀerent datasets (or data distributions) P1, ..., Pm, we arrive at m hyperparameter
risk mappings

R(j)(θ) := E(L(Y, ˆf (X, θ))|Pj),

j = 1, ..., m.

For now, we assume all R(j)(θ) to be known, and show how to estimate them in section 3.7.

3.2 Optimal conﬁguration per dataset and optimal defaults

We ﬁrst deﬁne the best hyperparameter conﬁguration for dataset j as

θ(j)(cid:63) := arg min

R(j)(θ).

θ∈Θ

Defaults settings are supposed to work well across many diﬀerent datasets and are usually
provided by software packages, in an often ad hoc or heuristic manner. We propose to deﬁne
an optimal default conﬁguration, based on an extensive number of empirical experiments on
m diﬀerent benchmark datasets, by

(1)

(2)

(3)

θ(cid:63) := arg min

g(R(1)(θ), ..., R(m)(θ)).

θ∈Θ

Here, g is a summary function that has to be speciﬁed. Selecting the mean (or median
as a more robust candidate) would imply minimizing the average (or median) risk over all
datasets.

The measures R(j)(θ) could potentially be scaled appropriately beforehand in order to
make them more commensurable between datasets, e.g., one could scale all R(j)(θ) to [0, 1]
by substracting the result of a very simple baseline like a featureless dummy predictor and
dividing this diﬀerence by the absolute diﬀerence between the risk of the best possible
result (as an approximation of the Bayes error) and the result of the very simple baseline
predictor. Or one could produce a statistical z-score by subtracting the mean and dividing
by the standard deviation from all experimental results on the same dataset (Feurer et al.,
2018).

The appropriateness of the scaling highly depends on the performance measure that
is used. One could, for example, argue that the AUC does not have to be scaled as an
improvement from 0.5 to 0.6 can possibly be seen as important as an improvement from 0.8
to 0.9. On the other hand, averaging the mean squared error on several datasets does not

3

make a lot of sense, as the scale of the outcome of diﬀerent regression problems can be very
diﬀerent. Then scaling or using another measure such as R2 seems essential.

3.3 Measuring overall tunability of a ML algorithm

A general measure of the tunability of an algorithm per dataset can then be computed
based on the diﬀerence between the risk of an overall reference conﬁguration (e.g., either the
software defaults or deﬁnition (3)) and the risk of the best possible conﬁguration on that
dataset:

d(j) := R(j)(θ(cid:63)) − R(j)(θ(j)(cid:63)), for j = 1, ..., m.

(4)

For each algorithm, this gives rise to an empirical distribution of performance diﬀerences
over datasets, which might be directly visualized or summarized to an aggregated tunability
measure d by using mean, median or quantiles.

3.4 Measuring tunability of a speciﬁc hyperparameter

The best hyperparameter value for one parameter i on dataset j, when all other parameters
are set to defaults from θ(cid:63) := (θ(cid:63)
k), is denoted by

1, ..., θ(cid:63)

θ(j)(cid:63)
i

:= arg min
θ∈Θ,θl=θ(cid:63)

l ∀l(cid:54)=i

R(j)(θ).

A natural measure for tunability of the i-th parameter on dataset j is then the diﬀerence

in risk between the above and our default reference conﬁguration:
:= R(j)(θ(cid:63)) − R(j)(θ(j)(cid:63)

), for j = 1, ..., m, i = 1, ..., k.

d(j)
i

i

Furthermore, we deﬁne d(j),rel

d(j) as the fraction of performance gain, when we only
tune i compared to tuning the complete algorithm, on dataset j. Again, one can calculate
the mean, the median or quantiles of these two diﬀerences over the n datasets, to get a
notion of the overall tunability di of this parameter.

i

i

= d(j)

3.5 Tunability of hyperparamater combinations and joint gains

Let us now consider two hyperparameters indexed as i1 and i2. To measure the tunability
with respect to these two parameters, we deﬁne

θ(j)(cid:63)
i1,i2

:=

arg min

R(j)(θ),

θ∈Θ,θl=θ(cid:63)

l ∀l(cid:54)∈{i1,i2}

i.e., the θ-vector containing the default values for all hyperparameters other than i1 and i2,
and the optimal combination of values for the i1-th and i2-th components of θ.

Analogously to the previous section, we can now deﬁne the tunability of the set (i1, i2)

as the gain over the reference default on dataset j as:

d(j)
i1,i2

:=R(j)(θ∗) − R(j)(θ(j)(cid:63)
i1,i2

).

The joint gain which can be expected when tuning not only one of the two hyperparam-

eters individually, but both of them jointly, on a dataset j, can be expressed by:

g(j)
i1,i2

:= min{(R(j)(θ(j)(cid:63)

)), (R(j)(θ(j)(cid:63)

))} − R(j)(θ(j)(cid:63)
i1,i2

).

i2

i1

Furthermore, one could be interested in whether this joint gain could simply be reached
by tuning both parameters i1 and i2 in a univariate fashion sequentially, either in the order
i1 → i2 or i2 → i1, and what order would be preferable. For this purpose one could compare
the risk of the hyperparameter value that results when tuning them together R(j)(θ(j)(cid:63)
) with
i1,i2
the risks of the hyperparameter values that are obtained when tuning them sequentially, that
means R(j)(θ(j)(cid:63)

), which is done for example in Waldron et al. (2011).

) or R(j)(θ(j)(cid:63)

i1→i2

i2→i1

Again, all these measures should be summarized across datasets, resulting in di1,i2 and
gi1,i2. Of course, these approaches can be further generalized by considering combinations
of more than two parameters.

(5)

(6)

(7)

(8)

(9)

4

3.6 Optimal hyperparameter ranges for tuning

A reasonable hyperparameter space Θ(cid:63) for tuning should include the optimal conﬁguration
θ(j)(cid:63) for dataset j with high probability. We denote the p-quantile of the distribution of one
parameter regarding the best hyperparameters on each dataset (θ(1)(cid:63))i, ..., (θ(m)(cid:63))i as qi,p.
The hyperparameter tuning space can then be deﬁned as:

Θ(cid:63) := {θ ∈ Θ|∀i ∈ {1, ..., k} : θi ≥ qi,p1 ∧ θi ≤ qi,p2} ,
with p1 and p2 being quantiles which can be set for example to the 5 % quantile and the
95 % quantile. This avoids focusing too much on outlier datasets and makes the deﬁnition
of the space independent from the number of datasets.

(10)

The deﬁnition above is only valid for numerical hyperparameters. In case of categorical
variables one could use similar rules, for example only including hyperparameter values that
were at least once or in at least 10 % of the datasets the best possible hyperparameter
setting.

3.7 Practical estimation

In order to practically apply the previously deﬁned concepts, two remaining issues need to be
addressed: a) We need to discuss how to obtain R(j)(θ); and b) in (2) and (3) a multivariate
optimization problem (the minimization) needs to be solved1.

For a) we estimate R(j)(θ) by using surrogate models ˆR(j)(θ), and replace the original
quantity by its estimator in all previous formulas. Surrogate models for each dataset j are
based on a meta dataset. This is created by evaluating a large number of conﬁgurations of the
respective ML method. The surrogate regression model then learns to map a hyperparameter
conﬁguration to estimated performance. For b) we solve the optimization problem – now
cheap to evaluate, because of the surrogate models – through black-box optimization.

4 Experimental setup

In this section we give an overview about the experimental setup that is used for obtaining
surrogate models, tunability measures and tuning spaces.

4.1 Datasets from the OpenML platform

Recently, the OpenML project (Vanschoren et al., 2013) has been created as a ﬂexible online
platform that allows ML scientists to share their data, corresponding tasks and results of
diﬀerent ML algorithms. We use a speciﬁc subset of carefully curated classiﬁcation datasets
from the OpenML platform called OpenML100 (Bischl et al., 2017). For our study we only
use the 38 binary classiﬁcation tasks that do not contain any missing values.

4.2 ML Algorithms

The algorithms considered in this paper are common methods for supervised learning. We
examine elastic net (glmnet), decision tree (rpart), k-nearest neighbors (kknn), support
vector machine (svm), random forest (ranger) and gradient boosting (xgboost). For more
details about the used software packages see Kühn et al. (2018). An overview of their
considered hyperparameters is displayed in Table 1, including respective data types, box-
constraints and a potential transformation function.

In the case of xgboost, the underlying package only supports numerical features, so we
opted for a dummy feature encoding for categorical features, which is performed internally
by the underlying packages for svm and glmnet.

Some hyperparameters of the algorithms are dependent on others. We take into account
these dependencies and, for example, only sample a value for gamma for the support vector
machine if the radial kernel was sampled beforehand.

1All other previous optimization problems are univariate or two-dimensional and can simply be addressed by

a simple technique like a ﬁne grid search

5

Algorithm Hyperparameter
glmnet

Type Lower Upper Trafo

alpha
lambda

cp
maxdepth
minbucket
minsplit
-
k

kernel
cost
gamma
degree

rpart

kknn

svm

ranger

xgboost

num.trees
replace
sample.fraction
mtry
respect.unordered.factors
min.node.size

nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

numeric
numeric

numeric
integer
integer
integer
-
integer

discrete
numeric
numeric
integer

integer
logical
numeric
numeric
logical
numeric

integer
numeric
numeric
discrete
integer
numeric
numeric
numeric
numeric
numeric

0
-10

0
1
1
1

1

-
-10
-10
2

1
-
0.1
0
-
0

1
-10
0.1
-
1
0
0
0
-10
-10

1
10

1
30
60
60

30

-
10
10
5

2000
-
1
1
-
1

5000
0
1
-
15
7
1
1
10
10

-
2x

-
-
-
-

-

-
2x
2x
-

-
2x
-
-
-
2x
-
-
2x
2x

-
-
-
x · p
-
nx

Table 1: Hyperparameters of the algorithms. p refers to the number of variables and n to the number
of observations. The columns Lower and Upper indicate the regions from which samples of these hyper-
parameters are drawn. The transformation function in the trafo column, if any, indicates how the values
are transformed according to this function. The exponential transformation is applied to obtain more
candidate values in regions with smaller hyperparameters because for these hyperparameters the perfor-
mance diﬀerences between smaller values are potentially bigger than for bigger values. The mtry value
in ranger that is drawn from [0, 1] is transformed for each dataset separately. After having chosen the
dataset, the value is multiplied by the number of variables and afterwards rounded up. Similarly, for the
min.node.size the value x is transformed by the formula [nx] with n being the number of observations
of the dataset, to obtain a positive integer values with higher probability for smaller values (the value is
ﬁnally rounded to obtain integer values).

6

4.3 Performance estimation

Several measures are regarded throughout this paper, either for evaluating our considered
classiﬁcation models that should be tuned, or for evaluating our surrogate regression models.
As no optimal measure exists, we will compare several of them. In the classiﬁcation case,
we consider AUC, accuracy and brier score. In the case of surrogate regression, we consider
R2, which is directly proportional to the regular mean squared error but scaled to [0,1] and
explains the gain over a constant model estimating the overall mean of all data points. We
also compute Kendall’s tau as a ranking based measure for regression.

The performance estimation for the diﬀerent hyperparameter experiments is computed
through 10-fold cross-validation. For the comparison of surrogate models 10 times repeated
10-fold cross-validation is used.

4.4 Random Bot sampling strategy for meta data

To reliably estimate our surrogate models we need enough evaluated conﬁgurations per
classiﬁer and data set. We sample these points from independent uniform distributions
where the respective support for each parameter is displayed in Table 1. Here, uniform
refers to the untransformed scale, so we sample uniformly from the interval [Lower, Upper ]
of Table 1.

In order to properly facilitate the automatic computation of a large database of hyper-
In an embarrassingly
parameter experiments, we implemented a so called OpenML bot.
parallel manner it chooses in each iteration a random dataset, a random classiﬁcation al-
gorithm, samples a random conﬁguration and evaluates it via cross-validation. A subset
of 500000 experiments for each algorithm and all datasets are used for our analysis here.2
More technical details regarding the random bot, its setup and results can be obtained in
Kühn et al. (2018), furthermore, for simple and permanent access the results of the bot are
stored in a ﬁgshare repository (Kühn et al., 2018).

4.5 Optimizing surrogates to obtain optimal defaults

Random search is also used for our black-box optimization problems in section 3.7. For
the estimation of the defaults for each algorithm we randomly sample 100000 points in
the hyperparameter space as deﬁned in Table 1 and determine the conﬁguration with the
minimal average risk. The same strategy with 100000 random points is used to obtain
the best hyperparameter setting on each dataset that is needed for the estimation of the
tunability of an algorithm. For the estimation of the tunability of single hyperparameters we
also use 100000 random points for each parameter, while for the tunability of combination
of hyperparameters we only use 10000 random points to reduce runtime as this should be
enough to cover 2-dimensional hyperparameter spaces.

Of course one has to be careful with overﬁtting here, as our new defaults are chosen
with the help of the same datasets that are used to determine the performance. Therefore,
we also evaluate our approach via a “10-fold cross-validation across datasets”. Here, we
repeatedly calculate the optimal defaults based on 90% “training datasets” and evaluate the
package defaults and our optimal defaults – the latter induced from the training data sets –
on the surrogate models of the remaining 10% “test datasets”, and compare their diﬀerence
in performance.

4.6 The problem of hyperparameter dependency

Some parameters are dependent on other superordinate hyperparameters and are only rele-
vant if the parameter value of this superordinate parameter was set to a speciﬁc value. For
example gamma in svm only makes sense if the kernel was set to “radial“ or degree only
makes sense if the kernel was set to “polynomial“. Some of these subordinate parameters
might be invalid/inactive in the reference default conﬁguration, rendering it impossible to
univariately tune them in order to compute their tunability score. In such a case we set the

230 for each dataset for kknn

7

superordinate parameter to a value which makes the subordinate parameter active, compute
the optimal defaults for the rest of the parameters and compute the tunability score for the
subordinate parameter with these defaults.

4.7 Software details

All our experiments are executed in R and are run through a combination of custom code
from our random bot Kühn et al. (2018), the OpenML R package (Casalicchio et al., 2017),
mlr (Bischl et al., 2016) and batchtools (Lang et al., 2017) for parallelization. All results
are uploaded to the OpenML platform and there publicly available for further analysis.
mlr is also used to compare and ﬁt all surrogate regression models. The fully reproducible
R code for all computations and analyses of our paper can be found on the github page:
https://github.com/PhilippPro/tunability. We also provide an interactive shiny app
under https://philipppro.shinyapps.io/tunability/, which displays all results of the
following section in a potentially more convenient, interactive fashion and which can simply
be accessed through a web browser.

5 Results and discussion

We calculate all results for AUC, accuracy and brier score but mainly discuss AUC results
here. Tables and ﬁgures for the other measures can be accessed in the Appendix and in our
interactive shiny application.

5.1 Surrogate models

We compare diﬀerent possible regression models as candidates for our surrogate models: the
linear model (lm), a simple decision tree (rpart), k nearest-neighbors (kknn) and random
forest (ranger)3 All algorithms are run with their default settings. We calculate 10 times
repeated 10-fold cross-validated regression performance measures R2 and Kendall’s tau per
dataset, and average these across all datasets4. Results for AUC are displayed in Figure
1. A good overall performance is achieved by ranger with qualitatively similar results for
other classiﬁcation performance measures (see Appendix). In the following we use random
forest as surrogate model because it performs reasonably well and is already an established
algorithm for surrogate models in the literature (Eggensperger et al., 2014; Hutter et al.,
2013).

5.1.1 Optimal defaults and tunability

Table 2 displays our mean tunability results for the algorithms as deﬁned in formula (4)
w.r.t. package defaults (Def.P column) and our optimal defaults (Def.O). It also displays
the improvement per algorithm when moving from package defaults to optimal defaults
(Improv), which was positive overall. This also holds for svm and ranger although the
package defaults are data dependent, which we currently cannot model (gamma = 1/p for
p for ranger). From now on, when discussing tunability, we will only do
svm and mtry =
this w.r.t. our optimal defaults.

√

Clearly, some algorithms such as glmnet and svm are much more tunable than the others,
while ranger is the algorithm with the smallest tunability, which is in line with common
In Figure 2 modiﬁed boxplots of the tunabilities are
knowledge in the web community.
depicted. For each ML algorithm, some outliers are visible, which indicates that tuning has
a much higher impact on some speciﬁc datasets.

3We also tried cubist (Kuhn et al., 2016), which provided good results but the algorithm had some technical
problems for some combinations of datasets and algorithms. We did not include gaussian process which is one of
the standard algorithms for surrogate models as it cannot handle categorical variables.

4In case of kknn four datasets did not provide results for one of the surrogate models and were not used.

8

Figure 1: Average performances over the datasets of diﬀerent surrogate models (target: AUC)
for diﬀerent algorithms (that were presented in 4.2).

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.045
0.024
0.025
0.012
0.025
0.006
0.014
0.042
0.004
0.006
0.029
0.014

0.069
glmnet
0.038
rpart
0.031
kknn
svm 0.056
0.010
0.043

0.037
0.016
0.006
0.048
0.007
0.017

ranger
xgboost

Impr-CV
0.032
0.022
0.025
0.008
0.003
0.026

Table 2: Overall tunability (regarding AUC) with the package defaults (Tun.P) and the new
defaults (Tun.O) as reference, cross-validated tunability (Tun.O-CV), average improvement (Im-
prov) and cross-validated average improvement (Impr-CV) obtained by using new defaults com-
pared to old defaults. The (cross-validated) improvement can be calculated by the (rounded)
diﬀerence between Tun.P and Tun.O (Tun.O-CV).

9

Figure 2: Boxplots of the tunabilities (AUC) of the diﬀerent algorithms. The upper and lower
whiskers (upper and lower line of the boxplot rectangle) are in our case deﬁned as the 0.1
and 0.9 quantiles of the tunability scores. The 0.9 quantile indicates how much performance
improvement can be expected on at least 10% of datasets. One outlier of glmnet (value 0.5) is
not shown.

5.1.2 Tunability of speciﬁc hyperparameters

In Table 3 the mean tunability (regarding the AUC) of single hyperparameters as deﬁned
in Equation (6) in section 3.4 can be seen. From here on, we will refer to tunability only
with respect to optimal defaults.

For glmnet lambda seems to be more tunable than alpha.

In rpart the minbucket
and minsplit parameters seem to be the most important ones for tuning. k in the kknn
algorithm is very tunable w.r.t. package defaults, but not regarding optimal defaults. In svm
the biggest gain in performance can be achieved by tuning the kernel, gamma or degree,
while the cost parameter does not seem to be very tunable. In ranger mtry is the most
tunable parameter. For xgboost there are two parameters that are quite tunable: eta and
the booster. booster speciﬁes if a tree or a linear model is trained. The cross-validated
results can be seen in Table 10 in the Appendix, they are quite similar to the non cross-
validated results and for all parameters slightly higher.

Instead of looking only at the average, as in Table 3, one could also be interested in the
distribution of the tunability of each dataset. As an example, Figure 3 shows the tunability
of each parameter of ranger in a boxplot. This gives a more in-depth insight into the
tunability, makes it possible to detect outliers and to examine the skewness.

5.1.3 Hyperparameter space for tuning

The hyperparameter space for tuning, as deﬁned in Equation (10) in section 3.6 and based
on the 0.05 and 0.95 quantiles, is displayed in Table 3. All optimal defaults are contained
in this hyperparameter space while some of the package defaults are not.

As an example, Figure 4 displays the full histogram of the best values of mtry of the
random forest over all datasets. Note that for quite a few data sets much higher values than
the package defaults seem advantageous. Analogous histograms for other parameters are
available through the shiny app.

10

Parameter Def.P

q0.05

q0.95

1
0

7

0.01
30
7
20

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

radial
1
1/p
3

radial
682.478
0.005
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
0

sample.fraction
mtry

√

983
FALSE
0.703
p · 0.257
FALSE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

30

0
21
12
24

0.403
0.004

Def.O Tun.P Tun.O
0.024
0.069
0.006
0.038
0.021
0.034
0.012
0.038
0.002
0.025
0.002
0.004
0.006
0.005
0.004
0.004
0.006
0.031
0.006
0.031
0.042
0.056
0.024
0.030
0.006
0.016
0.022
0.030
0.014
0.008
0.006
0.010
0.001
0.001
0.001
0.002
0.002
0.004
0.003
0.006
0.000
0.000
0.001
0.001
0.014
0.043
0.002
0.004
0.005
0.006
0.002
0.004
0.008
0.015
0.001
0.001
0.002
0.008
0.001
0.006
0.001
0.008
0.002
0.003
0.002
0.003

4168
0.018
0.839
gbtree
13
2.06
0.752
0.585
0.982
1.113

0.009
0.001

0
12.1
3.85
5

9.95

0.981
0.147

0.008
27
41.6
49.15

30

0.002
0.003
2

920.582
18.195
4

206.35

1740.15

0.323
0.035

0.974
0.692

0.007

0.513

920.7
0.002
0.545

5.6
1.295
0.419
0.335
0.008
0.002

4550.95
0.355
0.958

14
6.984
0.864
0.886
29.755
6.105

Table 3: Defaults (package defaults (Def.P) and optimal defaults (Def.O)), tunability of the hy-
perparameters with the package defaults (Tun.P) and our optimal defaults (Tun.O) as reference
and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters of the algorithms.

11

Figure 3: Boxplots of the tunabilities of the diﬀerent parameters of ranger. Same deﬁnition of
whiskers as in Figure 2.

Figure 4: Histogram of best parameter values for mtry of random forest over all considered data
sets.

12

5.1.4 Tunability of hyperparameter combinations

As an example, Table 4 displays the average tunability di1,i2 of all 2-way hyperparameter
combinations for rpart. Obviously, the increased ﬂexibility in tuning a 2-way combination
enables larger improvements when compared with the tunability of one of the respective
individual parameters.

In Table 5 the joint gain of tuning two hyperparameters gi1,i2 instead of only the best
as deﬁned in section 3.5 can be seen. The parameters minsplit and minbucket have the
biggest joint eﬀect, which is not very surprising, as they are closely related: minsplit is
the minimum number of observations that must exist in a node in order for a split to be
attempted and minbucket the minimum number of observations in any terminal leaf node.
If a higher value of minsplit than the default performs better on a dataset it is possibly
not enough to set it higher without also increasing minbucket, so the strong relationship is
quite clear. Again, further ﬁgures for other algorithms are available through the shiny app.

cp
maxdepth
minbucket
minsplit

0.002

cp maxdepth minbucket minsplit
0.004
0.005
0.011
0.004

0.006
0.007
0.006

0.003
0.002

Table 4: Tunability di1,i2 of hyperparameters of rpart, diagonal shows tunability of the single
hyperparameters.

cp
maxdepth
minbucket

0.0007

maxdepth minbucket minsplit
0.0004
0.0019
0.0055

0.0005
0.0014

Table 5: Joint gain gi1,i2 of tuning two hyperparameters instead of the most important in rpart.

6 Conclusion and Discussion

Our paper provides concise and intuitive deﬁnitions for optimal defaults of ML algorithms
and the impact of tuning them either jointly, tuning individual parameters or combinations,
all based on the general concept of surrogate empirical performance models. Tunability
values as deﬁned in our framework are easily and directly interpretable as how much per-
formance can be gained by tuning this hyperparameter?. This allows direct comparability of
the tunability values across diﬀerent algorithms.

In an extensive OpenML benchmark, we computed optimal defaults for elastic net, de-
cision tree, k-nearest neighbors, SVM, random forest and xgboost and quantiﬁed their tun-
ability and the tunability of their individual parameters. This – to the best of our knowledge
– has never been provided before in such a principled manner. Our results are often in line
with common knowledge from literature and our method itself now allows an analogous
analysis for other or more complex methods.

Our framework is based on the concept of default hyperparameter values, which can be
seen both as an advantage (default values are a valuable output of the approach) and as
an inconvenience (the determination of the default values is an additional analysis step and
needed as a reference point for most of our measures).

We now compare our method with van Rijn and Hutter (2017). In contrast to us, they
apply the functional ANOVA framework from Hutter et al. (2014) on a surrogate random
forest to assess the importance of hyperparameters regarding empirical performance of a
support vector machine, random forest and adaboost, which results in numerical importance

13

scores for individual hyperparameters. Their numerical scores are - in our opinion - less
directly interpretable, but they do not rely on defaults as a reference point, which one
might see as an advantage. They also propose a method for calculating hyperparameter
priors, combine it with the tuning procedure hyperband, and assess the performance of this
new tuning procedure. In contrast, we deﬁne and calculate ranges for all hyperparameters.
Setting ranges for the tuning space can be seen as a special case of a prior distribution - the
uniform distribution on the speciﬁed hyperparameter space. Regarding the experimental
setup, we compute more hyperparameter runs (around 2.5 million vs. 250000), but consider
only the 38 binary classiﬁcation datasets of OpenML100 while van Rijn and Hutter (2017)
use all the 100 datasets which also contain multiclass datasets. We evaluate the performance
of diﬀerent surrogate models by 10 times repeated 10-fold cross-validation to choose an
appropriate model and to assure that it performs reasonably well.

Our study has some limitations that could be addressed in the future: a) We only con-
sidered binary classiﬁcation, where we tried to include a wider variety of datasets from
diﬀerent domains. In principle this is not a restriction as our methods can easily be applied
to multiclass classiﬁcation, regression, survival analysis or even algorithms not from machine
learning whose empirical performance is reliably measurable on a problem instance. b) Uni-
form random sampling of hyperparameters might not scale enough for very high dimensional
spaces, and a smarter sequential technique might be in order here, see (Bossek et al., 2015)
for an potential approach of sampling across problem instances to learn optimal mappings
from problem characteristics to algorithm conﬁgurations. c) We currently are learning static
defaults, which cannot depend on dataset characteristics (like number of features, or fur-
ther statistical measures) as in meta-learning. Doing so might improve performance results
of optimal defaults considerably, but would require a more complicated approach. d) Our
approach still needs initial ranges to be set, in order to run our sampling procedure. Only
based on these wider ranges we can then compute more precise, closer ranges.

Acknowledgements

We would like to thank Joaquin Vanschoren for support regarding the OpenML platform
and Andreas Müller, Jan van Rijn, Janek Thomas and Florian Pﬁsterer for reviewing and
useful comments. Thanks to Jenny Lee for language editing.

References

J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of

Machine Learning Research, 13:281–305, 2012.

A. Biedenkapp, M. T. Lindauer, K. Eggensperger, F. Hutter, C. Fawcett, and H. H. Hoos.
In AAAI, pages

Eﬃcient parameter importance analysis via ablation with surrogates.
773–779, 2017.

M. Birattari, Z. Yuan, P. Balaprakash, and T. Stützle. F-Race and iterated F-Race: An
overview. In Experimental Methods for the Analysis of Optimization Algorithms, pages
311–336. Springer, 2010.

B. Bischl, O. Mersmann, H. Trautmann, and C. Weihs. Resampling methods for meta-model
validation with recommendations for evolutionary computation. Evolutionary Computa-
tion, 20(2):249–275, 2012.

B. Bischl, M. Lang, L. Kotthoﬀ, J. Schiﬀner, J. Richter, E. Studerus, G. Casalicchio, and
Z. M. Jones. mlr: Machine learning in R. Journal of Machine Learning Research, 17
(170):1–5, 2016.

B. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. van Rijn,
and J. Vanschoren. OpenML benchmarking suites and the OpenML100. ArXiv preprint
arXiv:1708.03731, 2017. URL https://arxiv.org/abs/1708.03731.

14

B. Bischl, J. Richter, J. Bossek, D. Horn, J. Thomas, and M. Lang. mlrMBO: A modular
framework for model-based optimization of expensive black-box functions. ArXiv preprint
arXiv:1703.03373, 2017. URL https://arxiv.org/abs/1703.03373.

J. Bossek, B. Bischl, T. Wagner, and G. Rudolph. Learning feature-parameter mappings
for parameter tuning via the proﬁle expected improvement. In Proceedings of the 2015
Annual Conference on Genetic and Evolutionary Computation, pages 1319–1326. ACM,
2015.

G. Casalicchio, J. Bossek, M. Lang, D. Kirchhoﬀ, P. Kerschke, B. Hofner, H. Seibold, J. Van-
schoren, and B. Bischl. OpenML: An R package to connect to the machine learning
platform OpenML. Computational Statistics, 32(3):1–15, 2017.

K. Eggensperger, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Surrogate benchmarks for
hyperparameter optimization.
In Proceedings of the 2014 International Conference on
Meta-learning and Algorithm Selection-Volume 1201, pages 24–31. CEUR-WS. org, 2014.

K. Eggensperger, M. Lindauer, H. H. Hoos, F. Hutter, and K. Leyton-Brown. Eﬃcient
benchmarking of algorithm conﬁgurators via model-based surrogates. Machine Learning,
pages 1–27, 2018.

A. E. Eiben and S. K. Smit. Parameter tuning for conﬁguring and analyzing evolutionary

algorithms. Swarm and Evolutionary Computation, 1(1):19–31, 2011.

C. Fawcett and H. H. Hoos. Analysing diﬀerences between algorithm conﬁgurations through

ablation. Journal of Heuristics, 22(4):431–458, 2016.

M. Feurer, B. Letham, and E. Bakshy. Scalable meta-learning for bayesian optimization.

arXiv preprint 1802.02219, 2018. URL https://arxiv.org/abs/1802.02219.

I. Guyon, A. Saﬀari, G. Dror, and G. Cawley. Model selection: Beyond the bayesian/fre-

quentist divide. Journal of Machine Learning Research, 11(Jan):61–87, 2010.

F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for
general algorithm conﬁguration. In International Conference on Learning and Intelligent
Optimization, pages 507–523. Springer, 2011.

F. Hutter, H. H. Hoos, and K. Leyton-Brown. Identifying key algorithm parameters and
instance features using forward selection. In International Conference on Learning and
Intelligent Optimization, pages 364–381. Springer, 2013.

F. Hutter, H. Hoos, and K. Leyton-Brown. An eﬃcient approach for assessing hyperparam-
eter importance. In ICML, volume 32 of JMLR Workshop and Conference Proceedings,
pages 754–762, 2014.

D. Kühn, P. Probst, J. Thomas, and B. Bischl. Automatic Exploration of Machine
Learning Experiments on OpenML. ArXiv preprint arXiv:1806.10961, 2018. URL
https://arxiv.org/abs/1806.10961.

M. Kuhn, S. Weston, C. Keefer, and N. Coulter. Cubist: Rule- and instance-based regression

modeling, 2016. R package version 0.0.19.

D.

Kühn,
bot

P.
Probst,
benchmark

J.
data

Thomas,
(ﬁnal

and
B.
subset),

Bischl.
2018.

R
https://figshare.com/articles/OpenML_R_Bot_Benchmark_Data_final_subset_/5882230/2.

OpenML
URL

M. Lang, B. Bischl, and D. Surmann. batchtools: Tools for R to work on batch systems.

The Journal of Open Source Software, 2(10), 2017.

G. Luo. A review of automatic selection methods for machine learning algorithms and hyper-
parameter values. Network Modeling Analysis in Health Informatics and Bioinformatics,
5(1):1–16, 2016.

15

R. G. Mantovani, T. Horváth, R. Cerri, A. Carvalho, and J. Vanschoren. Hyper-parameter
In Brazilian Conference on Intelligent

tuning of a decision tree induction algorithm.
Systems (BRACIS 2016), 2016.

J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pages 2951–
2959, 2012.

J. N. van Rijn and F. Hutter. Hyperparameter importance across datasets. ArXiv preprint

arXiv:1710.04725, 2017. URL https://arxiv.org/abs/1710.04725.

J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo. OpenML: Networked science in

machine learning. SIGKDD Explorations, 15(2):49–60, 2013.

L. Waldron, M. Pintilie, M.-S. Tsao, F. A. Shepherd, C. Huttenhower, and I. Jurisica.
Optimized application of penalized regression methods to diverse genomic data. Bioin-
formatics, 27(24):3399–3406, 2011.

16

Appendix A. Results for accuracy and brier score

Figure 5: Same as ﬁgure 1 but with accuracy as target measure. Average performances over
the datasets of diﬀerent surrogate models (target: accuracy) for diﬀerent algorithms (that were
presented in 4.2).

Figure 6: Same as ﬁgure 1 but with brier score as target measure. Average performances over
the datasets of diﬀerent surrogate models (target: brier score) for diﬀerent algorithms (that were
presented in 4.2).

17

Figure 7: Boxplots of the tunabilities (accuracy) of the diﬀerent algorithms.

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.023
0.019
0.008
0.012
0.013
0.008
0.011
0.030
0.009
0.007
0.023
0.011

0.042
glmnet
0.020
rpart
0.021
kknn
svm 0.041
0.016
0.034

0.042
0.014
0.010
0.041
0.009
0.012

ranger
xgboost

Impr-CV
0.001
0.005
0.010
-0.001
0.006
0.022

Table 6: Tunability measures as in table 2, but calculated for the accuracy. Overall tunabil-
ity (regarding accuracy) with the package defaults (Def.P) and the optimal defaults (Def.O)
as reference points, cross-validated tunability (Def.O-CV), average improvement (Improv) and
cross-validated average improvement (Impr-CV) obtained by using new defaults compared to
old defaults. The (cross-validated) improvement can be calculated by the (rounded) diﬀerence
between Def.P and Def.O (Def.O-CV).

18

Parameter Def.P

Def.O Tun.P Tun.O q0.05

q0.95

7

14

2

30

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

1
0

0.01
30
7
20

0.252
0.005

0.002
19
5
13

radial
1
1/p
3

radial
936.982
0.002
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
1

sample.fraction
mtry

√

162
FALSE
0.76
p · 0.432
TRUE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

3342
0.031
0.89
gbtree
14
1.264
0.712
0.827
2.224
0.021

0.042
0.022
0.029
0.020
0.013
0.004
0.005
0.002
0.021
0.021
0.041
0.019
0.019
0.024
0.005
0.016
0.001
0.004
0.003
0.010
0.001
0.001
0.034
0.004
0.005
0.003
0.008
0.001
0.009
0.005
0.006
0.002
0.003

0.019
0.010
0.017
0.012
0.008
0.004
0.006
0.003
0.008
0.008
0.030
0.018
0.003
0.020
0.014
0.007
0.001
0.001
0.003
0.003
0.000
0.002
0.011
0.002
0.005
0.002
0.005
0.001
0.002
0.001
0.001
0.002
0.002

0.015
0.001

0
10
1.85
6.7

0.979
0.223

0.528
28
43.15
47.6

0.025
0.007
2

943.704
276.02
4

203.5

1908.25

0.257
0.081

0.971
0.867

0.009

0.453

1360
0.002
0.555

3
1.061
0.334
0.348
0.004
0.003

4847.15
0.445
0.964

13
7.502
0.887
0.857
5.837
2.904

Table 7: Tunability measures for single hyperparameters and tuning spaces as in table 3, but
calculated for the accuracy. Defaults (package defaults (Def.P) and own calculated defaults
(Def.O)), tunability of the hyperparameters with the package defaults (Tun.P) and our new
defaults (Tun.O) as reference and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters
of the algorithms.

19

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.011
0.010
0.006
0.009
0.009
0.003
0.008
0.018
0.010
0.005
0.018
0.009

0.022
glmnet
0.015
rpart
0.012
kknn
svm 0.026
0.015
0.027

0.020
0.011
0.003
0.023
0.006
0.011

ranger
xgboost

Impr-CV
0.001
0.004
0.009
0.003
0.009
0.016

Table 8: Tunability measures as in table 2, but calculated for the brier score. Overall tunability
(regarding brier score) with the package defaults (Def.P) and the optimal defaults (Def.O)
as reference points, cross-validated tunability (Def.O-CV), average improvement (Improv) and
cross-validated average improvement (Impr-CV) obtained by using new defaults compared to
old defaults. The (cross-validated) improvement can be calculated by the (rounded) diﬀerence
between Def.P and Def.O (Def.O-CV).

Figure 8: Boxplots of the tunabilities (brier score) of the diﬀerent algorithms.

20

Parameter Def.P

q0.05

q0.95

1
0

7

0.01
30
7
20

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

radial
1
1/p
3

radial
950.787
0.005
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
1

sample.fraction
mtry

√

198
FALSE
0.667
p · 0.666
TRUE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

19

0.997
0.004

0.001
13
12
18

Def.O Tun.P Tun.O
0.010
0.022
0.005
0.009
0.007
0.014
0.009
0.015
0.003
0.009
0.002
0.002
0.006
0.004
0.002
0.002
0.003
0.012
0.003
0.012
0.018
0.026
0.011
0.013
0.002
0.012
0.012
0.015
0.009
0.003
0.005
0.015
0.001
0.001
0.001
0.002
0.003
0.002
0.002
0.010
0.000
0.000
0.001
0.001
0.009
0.027
0.002
0.004
0.005
0.004
0.002
0.002
0.004
0.009
0.001
0.001
0.002
0.007
0.002
0.004
0.001
0.004
0.003
0.002
0.004
0.003

2563
0.052
0.873
gbtree
11
1.75
0.713
0.638
0.101
0.894

0.003
0.001

0
9
1
7

0.974
0.051

0.035
27.15
44.1
49.15

4.85

30

0.002
0.001
2

963.81
4.759
4

187.85

1568.25

0.317
0.072

0.964
0.954

0.008

0.394

2018.55
0.003
0.447

4780.05
0.436
0.951

2.6
1.277
0.354
0.363
0.006
0.003

13
5.115
0.922
0.916
28.032
2.68

Table 9: Tunability measures for single hyperparameters and tuning spaces as in table 3, but
calculated for the brier score. Defaults (package defaults (Def.P) and own calculated defaults
(Def.O)), tunability of the hyperparameters with the package defaults (Tun.P) and our new
defaults (Tun.O) as reference and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters
of the algorithms.

21

Measure

AUC

Accuracy

Brier score

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k

Parameter Tun.O Tun.O-CV Tun.O Tun.O-CV Tun.O Tun.O-CV
0.020
0.019
0.015
0.010
0.018
0.017
0.011
0.012
0.005
0.008
0.003
0.004
0.006
0.006
0.003
0.003
0.003
0.008
0.003
0.008
0.023
0.030
0.016
0.018
0.002
0.003
0.016
0.020
0.014
0.014
0.006
0.007
0.001
0.001
0.001
0.001
0.003
0.003
0.003
0.003
0.000
0.000
0.001
0.002
0.011
0.011
0.002
0.002
0.006
0.005
0.002
0.002
0.004
0.005
0.001
0.001
0.003
0.002
0.002
0.001
0.002
0.001
0.004
0.002
0.004
0.002

0.024
0.006
0.021
0.012
0.002
0.002
0.006
0.004
0.006
0.006
svm 0.042
0.024
0.006
0.022
0.014
0.006
0.001
0.001
0.002
0.003
0.000
0.001
0.014
0.002
0.005
0.002
0.008
0.001
0.002
0.001
0.001
0.002
0.002

0.037
0.006
0.034
0.016
0.002
0.002
0.009
0.004
0.006
0.006
0.048
0.030
0.006
0.028
0.020
0.007
0.002
0.002
0.002
0.004
0.000
0.001
0.017
0.002
0.006
0.002
0.008
0.001
0.003
0.002
0.001
0.003
0.004

0.010
0.005
0.007
0.009
0.003
0.002
0.006
0.002
0.003
0.003
0.018
0.011
0.002
0.012
0.009
0.005
0.001
0.001
0.003
0.002
0.000
0.001
0.009
0.002
0.005
0.002
0.004
0.001
0.002
0.002
0.001
0.003
0.004

0.042
0.026
0.039
0.014
0.008
0.004
0.007
0.003
0.010
0.010
0.041
0.031
0.003
0.031
0.027
0.009
0.003
0.002
0.003
0.005
0.001
0.002
0.012
0.003
0.006
0.002
0.005
0.001
0.002
0.001
0.001
0.003
0.003

kernel
cost
gamma
degree
ranger
num.trees
replace
sample.fraction
mtry
respect.unordered.factors
min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

Table 10: Tunability with calculated defaults as reference without (Tun.O) and with (Tun.O-CV)
cross-validation for AUC, accuracy and brier score

22

8
1
0
2
 
t
c
O
 
2
2
 
 
]
L
M

.
t
a
t
s
[
 
 
3
v
6
9
5
9
0
.
2
0
8
1
:
v
i
X
r
a

Tunability: Importance of Hyperparameters of Machine
Learning Algorithms

by Philipp Probst, Anne-Laure Boulesteix and Bernd Bischl

October 23, 2018

Abstract

Modern supervised machine learning algorithms involve hyperparameters that have to
be set before running them. Options for setting hyperparameters are default values from
the software package, manual conﬁguration by the user or conﬁguring them for optimal
predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly,
we formalize the problem of tuning from a statistical point of view, deﬁne data-based de-
faults and suggest general measures quantifying the tunability of hyperparameters of al-
gorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets
from the OpenML platform and six common machine learning algorithms. We apply our
measures to assess the tunability of their parameters. Our results yield default values for
hyperparameters and enable users to decide whether it is worth conducting a possibly time
consuming tuning strategy, to focus on the most important hyperparameters and to chose
adequate hyperparameter spaces for tuning.

1

Introduction

Machine learning (ML) algorithms such as gradient boosting, random forest and neural net-
works for regression and classiﬁcation involve a number of hyperparameters that have to
be set before running them. In contrast to direct, ﬁrst-level model parameters, which are
determined during training, these second-level tuning parameters often have to be carefully
optimized to achieve maximal performance. A related problem exists in many other algo-
rithmic areas, e.g., control parameters in evolutionary algorithms (Eiben and Smit, 2011).
In order to select an appropriate hyperparameter conﬁguration for a speciﬁc dataset
at hand, users of ML algorithms can resort to default values of hyperparameters that are
speciﬁed in implementing software packages or manually conﬁgure them, for example, based
on recommendations from the literature, experience or trial-and-error.

Alternatively, one can use hyperparameter tuning strategies, which are data-dependent,
second-level optimization procedures (Guyon et al., 2010), which try to minimize the ex-
pected generalization error of the inducing algorithm over a hyperparameter search space
of considered candidate conﬁgurations, usually by evaluating predictions on an independent
test set, or by running a resampling scheme such as cross-validation (Bischl et al., 2012).
For a recent overview of tuning strategies, see, e.g., Luo (2016).

These search strategies range from simple grid or random search (Bergstra and Bengio,
2012) to more complex, iterative procedures such as Bayesian optimization (Hutter et al.,
2011; Snoek et al., 2012; Bischl et al., 2017) or iterated F-racing (Birattari et al., 2010;
Lang et al., 2017).

In addition to selecting an eﬃcient tuning strategy, the set of tunable hyperparameters
and their corresponding ranges, scales and potential prior distributions for subsequent sam-
pling have to be determined by the user. Some hyperparameters might be safely set to
default values, if they work well across many diﬀerent scenarios. Wrong decisions in these
areas can inhibit either the quality of the resulting model or at the very least the eﬃciency
and fast convergence of the tuning procedure. This creates a burden for:

1

1. ML users – Which hyperparameters should be tuned and in which ranges?

2. Designers of ML algorithms – How do I deﬁne robust defaults?

We argue that many users, especially if they do not have years of practical experience in
the ﬁeld, here often rely on heuristics or spurious knowledge. It should also be noted that
designers of fully automated tuning frameworks face at least very similar problems. It is
not clear how these questions should be addressed in a data-dependent, automated, optimal
and objective manner. In other words, the scientiﬁc community not only misses answers to
these questions for many algorithms but also a systematic framework, methods and criteria,
which are required to answer these questions.

With the present paper we aim at ﬁlling this gap and formalize the problem of parameter
tuning from a statistical point of view, in order to simplify the tuning process for less
experienced users and to optimize decision making for more advanced processes.

After presenting related literature in section 2, we deﬁne theoretical measures for assess-
ing the impact of tuning in section 3. For this purpose we (i) deﬁne the concept of default
hyperparameters, (ii) suggest measures for quantiﬁying the tunability of the whole algorithm
and speciﬁc hyperparameters based on the diﬀerences between the performance of default
hyperparameters and the performance of the hyperparameters when this hyperparameter is
set to an optimal value. Then we (iii) address the tunability of hyperparameter combinations
and joint gains, (iv) provide theoretical deﬁnitions for an appropriate hyperparameter space
on which tuning should be executed and (v) propose procedures to estimate these quantities
based on the results of a benchmark study with random hyperparameter conﬁgurations with
the help of surrogate models. In sections 4 and 5 we illustrate these concepts and methods
through an application. For this purpose we use benchmark results of six machine learning
algorithms with diﬀerent hyperparameters which were evaluated on 38 datasets from the
OpenML platform. Finally, in the last section 6 we conclude and discuss the results.

2 Related literature

To the best of our knowledge, only a limited amount of articles address the problem of
tunability and generation of tuning search spaces. Bergstra and Bengio (2012) compute the
relevance of the hyperparameters of neural networks and conclude that some are important
on all datasets, while others are only important on some datasets. Their conclusion is
primarily visual and used as an argument for why random search works better than grid
search when tuning neural networks.

A speciﬁc study for decision trees was conducted by Mantovani et al. (2016) who ap-
ply standard tuning techniques to decision trees on 102 datasets and calculate diﬀerences
of accuracy between the tuned algorithm and the algorithm with default hyperparameter
settings.

A diﬀerent approach is proposed by Hutter et al. (2013), which aims at identifying the
most important hyperparameters via forward selection. In the same vein, Fawcett and Hoos
(2016) present an ablation analysis technique, which aims at identifying the hyperparameters
that contribute the most to improved performance after tuning. For each of the considered
hyperparameters, they compute the performance gain that can be achieved by changing
its value from the initial value to the value speciﬁed in the target conﬁguration which was
determined by the tuning strategy. This procedure is iterated in a greedy forward search.

A more general framework for measuring the importance of single hyperparameters is
presented by Hutter et al. (2014). After having used a tuning strategy such as sequential
model-based optimization, a functional ANOVA approach is used for measuring the impor-
tance of hyperparameters.

These works concentrate on the importance of hyperparameters on single datasets,
mainly to retrospectively explain what happened during an already concluded tuning pro-
cess. Our main focus is the generalization across multiple datasets in order to facilitate
better general understanding of hyperparameter eﬀects and better decision making for fu-
ture experiments. In a recent paper van Rijn and Hutter (2017) pose very similar questions
to ours to assess the importance of hyperparameters across datasets. We compare it to our
approach in section 6.

2

Our framework is based on using surrogate models, also sometimes called empirical
performance models, which allow estimating the performance of arbitrary hyperparameter
conﬁgurations based on a limited number of prior experiments. The idea of surrogate models
is far from new, as it constitutes the central idea of Bayesian optimization for hyperparameter
search but is also used, for example, in Biedenkapp et al. (2017) for increasing the speed of
an ablation analysis and by Eggensperger et al. (2018) for speeding up the benchmarking of
tuning strategies.

3 Methods for Estimation of Defaults, Tunability and
Ranges

3.1 General notation

Consider a target variable Y , a feature vector X, and an unknown joint distribution P on
(X, Y ), from which we have sampled a dataset T of n observations. A machine learning (ML)
algorithm now learns the functional relationship between X and Y by producing a prediction
model ˆf (X, θ), controlled by the k-dimensional hyperparameter conﬁguration θ = (θ1, ..., θk)
from the hyperparameter search space Θ = Θ1 × ... × Θk. In order to measure prediction
performance pointwise between the true label Y and its prediction ˆf (X, θ), we deﬁne a loss
function L(Y, ˆf (X, θ)). We are naturally interested in estimating the expected risk of the
inducing algorithm, w.r.t. θ on new data, also sampled from P: R(θ) = E(L(Y, ˆf (X, θ))|P).
This mapping encodes, given a certain data distribution, a certain learning algorithm and a
certain performance measure, the numerical quality for any hyperparameter conﬁguration θ.
Given m diﬀerent datasets (or data distributions) P1, ..., Pm, we arrive at m hyperparameter
risk mappings

R(j)(θ) := E(L(Y, ˆf (X, θ))|Pj),

j = 1, ..., m.

For now, we assume all R(j)(θ) to be known, and show how to estimate them in section 3.7.

3.2 Optimal conﬁguration per dataset and optimal defaults

We ﬁrst deﬁne the best hyperparameter conﬁguration for dataset j as

θ(j)(cid:63) := arg min

R(j)(θ).

θ∈Θ

Defaults settings are supposed to work well across many diﬀerent datasets and are usually
provided by software packages, in an often ad hoc or heuristic manner. We propose to deﬁne
an optimal default conﬁguration, based on an extensive number of empirical experiments on
m diﬀerent benchmark datasets, by

(1)

(2)

(3)

θ(cid:63) := arg min

g(R(1)(θ), ..., R(m)(θ)).

θ∈Θ

Here, g is a summary function that has to be speciﬁed. Selecting the mean (or median
as a more robust candidate) would imply minimizing the average (or median) risk over all
datasets.

The measures R(j)(θ) could potentially be scaled appropriately beforehand in order to
make them more commensurable between datasets, e.g., one could scale all R(j)(θ) to [0, 1]
by substracting the result of a very simple baseline like a featureless dummy predictor and
dividing this diﬀerence by the absolute diﬀerence between the risk of the best possible
result (as an approximation of the Bayes error) and the result of the very simple baseline
predictor. Or one could produce a statistical z-score by subtracting the mean and dividing
by the standard deviation from all experimental results on the same dataset (Feurer et al.,
2018).

The appropriateness of the scaling highly depends on the performance measure that
is used. One could, for example, argue that the AUC does not have to be scaled as an
improvement from 0.5 to 0.6 can possibly be seen as important as an improvement from 0.8
to 0.9. On the other hand, averaging the mean squared error on several datasets does not

3

make a lot of sense, as the scale of the outcome of diﬀerent regression problems can be very
diﬀerent. Then scaling or using another measure such as R2 seems essential.

3.3 Measuring overall tunability of a ML algorithm

A general measure of the tunability of an algorithm per dataset can then be computed
based on the diﬀerence between the risk of an overall reference conﬁguration (e.g., either the
software defaults or deﬁnition (3)) and the risk of the best possible conﬁguration on that
dataset:

d(j) := R(j)(θ(cid:63)) − R(j)(θ(j)(cid:63)), for j = 1, ..., m.

(4)

For each algorithm, this gives rise to an empirical distribution of performance diﬀerences
over datasets, which might be directly visualized or summarized to an aggregated tunability
measure d by using mean, median or quantiles.

3.4 Measuring tunability of a speciﬁc hyperparameter

The best hyperparameter value for one parameter i on dataset j, when all other parameters
are set to defaults from θ(cid:63) := (θ(cid:63)
k), is denoted by

1, ..., θ(cid:63)

θ(j)(cid:63)
i

:= arg min
θ∈Θ,θl=θ(cid:63)

l ∀l(cid:54)=i

R(j)(θ).

A natural measure for tunability of the i-th parameter on dataset j is then the diﬀerence

in risk between the above and our default reference conﬁguration:
:= R(j)(θ(cid:63)) − R(j)(θ(j)(cid:63)

), for j = 1, ..., m, i = 1, ..., k.

d(j)
i

i

Furthermore, we deﬁne d(j),rel

d(j) as the fraction of performance gain, when we only
tune i compared to tuning the complete algorithm, on dataset j. Again, one can calculate
the mean, the median or quantiles of these two diﬀerences over the n datasets, to get a
notion of the overall tunability di of this parameter.

i

= d(j)

i

3.5 Tunability of hyperparamater combinations and joint gains

Let us now consider two hyperparameters indexed as i1 and i2. To measure the tunability
with respect to these two parameters, we deﬁne

θ(j)(cid:63)
i1,i2

:=

arg min

R(j)(θ),

θ∈Θ,θl=θ(cid:63)

l ∀l(cid:54)∈{i1,i2}

i.e., the θ-vector containing the default values for all hyperparameters other than i1 and i2,
and the optimal combination of values for the i1-th and i2-th components of θ.

Analogously to the previous section, we can now deﬁne the tunability of the set (i1, i2)

as the gain over the reference default on dataset j as:

d(j)
i1,i2

:=R(j)(θ∗) − R(j)(θ(j)(cid:63)
i1,i2

).

The joint gain which can be expected when tuning not only one of the two hyperparam-

eters individually, but both of them jointly, on a dataset j, can be expressed by:

g(j)
i1,i2

:= min{(R(j)(θ(j)(cid:63)

)), (R(j)(θ(j)(cid:63)

))} − R(j)(θ(j)(cid:63)
i1,i2

).

i2

i1

Furthermore, one could be interested in whether this joint gain could simply be reached
by tuning both parameters i1 and i2 in a univariate fashion sequentially, either in the order
i1 → i2 or i2 → i1, and what order would be preferable. For this purpose one could compare
the risk of the hyperparameter value that results when tuning them together R(j)(θ(j)(cid:63)
) with
i1,i2
the risks of the hyperparameter values that are obtained when tuning them sequentially, that
means R(j)(θ(j)(cid:63)

), which is done for example in Waldron et al. (2011).

) or R(j)(θ(j)(cid:63)

i1→i2

i2→i1

Again, all these measures should be summarized across datasets, resulting in di1,i2 and
gi1,i2. Of course, these approaches can be further generalized by considering combinations
of more than two parameters.

(5)

(6)

(7)

(8)

(9)

4

3.6 Optimal hyperparameter ranges for tuning

A reasonable hyperparameter space Θ(cid:63) for tuning should include the optimal conﬁguration
θ(j)(cid:63) for dataset j with high probability. We denote the p-quantile of the distribution of one
parameter regarding the best hyperparameters on each dataset (θ(1)(cid:63))i, ..., (θ(m)(cid:63))i as qi,p.
The hyperparameter tuning space can then be deﬁned as:

Θ(cid:63) := {θ ∈ Θ|∀i ∈ {1, ..., k} : θi ≥ qi,p1 ∧ θi ≤ qi,p2} ,
with p1 and p2 being quantiles which can be set for example to the 5 % quantile and the
95 % quantile. This avoids focusing too much on outlier datasets and makes the deﬁnition
of the space independent from the number of datasets.

(10)

The deﬁnition above is only valid for numerical hyperparameters. In case of categorical
variables one could use similar rules, for example only including hyperparameter values that
were at least once or in at least 10 % of the datasets the best possible hyperparameter
setting.

3.7 Practical estimation

In order to practically apply the previously deﬁned concepts, two remaining issues need to be
addressed: a) We need to discuss how to obtain R(j)(θ); and b) in (2) and (3) a multivariate
optimization problem (the minimization) needs to be solved1.

For a) we estimate R(j)(θ) by using surrogate models ˆR(j)(θ), and replace the original
quantity by its estimator in all previous formulas. Surrogate models for each dataset j are
based on a meta dataset. This is created by evaluating a large number of conﬁgurations of the
respective ML method. The surrogate regression model then learns to map a hyperparameter
conﬁguration to estimated performance. For b) we solve the optimization problem – now
cheap to evaluate, because of the surrogate models – through black-box optimization.

4 Experimental setup

In this section we give an overview about the experimental setup that is used for obtaining
surrogate models, tunability measures and tuning spaces.

4.1 Datasets from the OpenML platform

Recently, the OpenML project (Vanschoren et al., 2013) has been created as a ﬂexible online
platform that allows ML scientists to share their data, corresponding tasks and results of
diﬀerent ML algorithms. We use a speciﬁc subset of carefully curated classiﬁcation datasets
from the OpenML platform called OpenML100 (Bischl et al., 2017). For our study we only
use the 38 binary classiﬁcation tasks that do not contain any missing values.

4.2 ML Algorithms

The algorithms considered in this paper are common methods for supervised learning. We
examine elastic net (glmnet), decision tree (rpart), k-nearest neighbors (kknn), support
vector machine (svm), random forest (ranger) and gradient boosting (xgboost). For more
details about the used software packages see Kühn et al. (2018). An overview of their
considered hyperparameters is displayed in Table 1, including respective data types, box-
constraints and a potential transformation function.

In the case of xgboost, the underlying package only supports numerical features, so we
opted for a dummy feature encoding for categorical features, which is performed internally
by the underlying packages for svm and glmnet.

Some hyperparameters of the algorithms are dependent on others. We take into account
these dependencies and, for example, only sample a value for gamma for the support vector
machine if the radial kernel was sampled beforehand.

1All other previous optimization problems are univariate or two-dimensional and can simply be addressed by

a simple technique like a ﬁne grid search

5

Algorithm Hyperparameter
glmnet

Type Lower Upper Trafo

alpha
lambda

cp
maxdepth
minbucket
minsplit
-
k

kernel
cost
gamma
degree

rpart

kknn

svm

ranger

xgboost

num.trees
replace
sample.fraction
mtry
respect.unordered.factors
min.node.size

nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

numeric
numeric

numeric
integer
integer
integer
-
integer

discrete
numeric
numeric
integer

integer
logical
numeric
numeric
logical
numeric

integer
numeric
numeric
discrete
integer
numeric
numeric
numeric
numeric
numeric

0
-10

0
1
1
1

1

-
-10
-10
2

1
-
0.1
0
-
0

1
-10
0.1
-
1
0
0
0
-10
-10

1
10

1
30
60
60

30

-
10
10
5

2000
-
1
1
-
1

5000
0
1
-
15
7
1
1
10
10

-
2x

-
-
-
-

-

-
2x
2x
-

-
2x
-
-
-
2x
-
-
2x
2x

-
-
-
x · p
-
nx

Table 1: Hyperparameters of the algorithms. p refers to the number of variables and n to the number
of observations. The columns Lower and Upper indicate the regions from which samples of these hyper-
parameters are drawn. The transformation function in the trafo column, if any, indicates how the values
are transformed according to this function. The exponential transformation is applied to obtain more
candidate values in regions with smaller hyperparameters because for these hyperparameters the perfor-
mance diﬀerences between smaller values are potentially bigger than for bigger values. The mtry value
in ranger that is drawn from [0, 1] is transformed for each dataset separately. After having chosen the
dataset, the value is multiplied by the number of variables and afterwards rounded up. Similarly, for the
min.node.size the value x is transformed by the formula [nx] with n being the number of observations
of the dataset, to obtain a positive integer values with higher probability for smaller values (the value is
ﬁnally rounded to obtain integer values).

6

4.3 Performance estimation

Several measures are regarded throughout this paper, either for evaluating our considered
classiﬁcation models that should be tuned, or for evaluating our surrogate regression models.
As no optimal measure exists, we will compare several of them. In the classiﬁcation case,
we consider AUC, accuracy and brier score. In the case of surrogate regression, we consider
R2, which is directly proportional to the regular mean squared error but scaled to [0,1] and
explains the gain over a constant model estimating the overall mean of all data points. We
also compute Kendall’s tau as a ranking based measure for regression.

The performance estimation for the diﬀerent hyperparameter experiments is computed
through 10-fold cross-validation. For the comparison of surrogate models 10 times repeated
10-fold cross-validation is used.

4.4 Random Bot sampling strategy for meta data

To reliably estimate our surrogate models we need enough evaluated conﬁgurations per
classiﬁer and data set. We sample these points from independent uniform distributions
where the respective support for each parameter is displayed in Table 1. Here, uniform
refers to the untransformed scale, so we sample uniformly from the interval [Lower, Upper ]
of Table 1.

In order to properly facilitate the automatic computation of a large database of hyper-
In an embarrassingly
parameter experiments, we implemented a so called OpenML bot.
parallel manner it chooses in each iteration a random dataset, a random classiﬁcation al-
gorithm, samples a random conﬁguration and evaluates it via cross-validation. A subset
of 500000 experiments for each algorithm and all datasets are used for our analysis here.2
More technical details regarding the random bot, its setup and results can be obtained in
Kühn et al. (2018), furthermore, for simple and permanent access the results of the bot are
stored in a ﬁgshare repository (Kühn et al., 2018).

4.5 Optimizing surrogates to obtain optimal defaults

Random search is also used for our black-box optimization problems in section 3.7. For
the estimation of the defaults for each algorithm we randomly sample 100000 points in
the hyperparameter space as deﬁned in Table 1 and determine the conﬁguration with the
minimal average risk. The same strategy with 100000 random points is used to obtain
the best hyperparameter setting on each dataset that is needed for the estimation of the
tunability of an algorithm. For the estimation of the tunability of single hyperparameters we
also use 100000 random points for each parameter, while for the tunability of combination
of hyperparameters we only use 10000 random points to reduce runtime as this should be
enough to cover 2-dimensional hyperparameter spaces.

Of course one has to be careful with overﬁtting here, as our new defaults are chosen
with the help of the same datasets that are used to determine the performance. Therefore,
we also evaluate our approach via a “10-fold cross-validation across datasets”. Here, we
repeatedly calculate the optimal defaults based on 90% “training datasets” and evaluate the
package defaults and our optimal defaults – the latter induced from the training data sets –
on the surrogate models of the remaining 10% “test datasets”, and compare their diﬀerence
in performance.

4.6 The problem of hyperparameter dependency

Some parameters are dependent on other superordinate hyperparameters and are only rele-
vant if the parameter value of this superordinate parameter was set to a speciﬁc value. For
example gamma in svm only makes sense if the kernel was set to “radial“ or degree only
makes sense if the kernel was set to “polynomial“. Some of these subordinate parameters
might be invalid/inactive in the reference default conﬁguration, rendering it impossible to
univariately tune them in order to compute their tunability score. In such a case we set the

230 for each dataset for kknn

7

superordinate parameter to a value which makes the subordinate parameter active, compute
the optimal defaults for the rest of the parameters and compute the tunability score for the
subordinate parameter with these defaults.

4.7 Software details

All our experiments are executed in R and are run through a combination of custom code
from our random bot Kühn et al. (2018), the OpenML R package (Casalicchio et al., 2017),
mlr (Bischl et al., 2016) and batchtools (Lang et al., 2017) for parallelization. All results
are uploaded to the OpenML platform and there publicly available for further analysis.
mlr is also used to compare and ﬁt all surrogate regression models. The fully reproducible
R code for all computations and analyses of our paper can be found on the github page:
https://github.com/PhilippPro/tunability. We also provide an interactive shiny app
under https://philipppro.shinyapps.io/tunability/, which displays all results of the
following section in a potentially more convenient, interactive fashion and which can simply
be accessed through a web browser.

5 Results and discussion

We calculate all results for AUC, accuracy and brier score but mainly discuss AUC results
here. Tables and ﬁgures for the other measures can be accessed in the Appendix and in our
interactive shiny application.

5.1 Surrogate models

We compare diﬀerent possible regression models as candidates for our surrogate models: the
linear model (lm), a simple decision tree (rpart), k nearest-neighbors (kknn) and random
forest (ranger)3 All algorithms are run with their default settings. We calculate 10 times
repeated 10-fold cross-validated regression performance measures R2 and Kendall’s tau per
dataset, and average these across all datasets4. Results for AUC are displayed in Figure
1. A good overall performance is achieved by ranger with qualitatively similar results for
other classiﬁcation performance measures (see Appendix). In the following we use random
forest as surrogate model because it performs reasonably well and is already an established
algorithm for surrogate models in the literature (Eggensperger et al., 2014; Hutter et al.,
2013).

5.1.1 Optimal defaults and tunability

Table 2 displays our mean tunability results for the algorithms as deﬁned in formula (4)
w.r.t. package defaults (Def.P column) and our optimal defaults (Def.O). It also displays
the improvement per algorithm when moving from package defaults to optimal defaults
(Improv), which was positive overall. This also holds for svm and ranger although the
package defaults are data dependent, which we currently cannot model (gamma = 1/p for
p for ranger). From now on, when discussing tunability, we will only do
svm and mtry =
this w.r.t. our optimal defaults.

√

Clearly, some algorithms such as glmnet and svm are much more tunable than the others,
while ranger is the algorithm with the smallest tunability, which is in line with common
In Figure 2 modiﬁed boxplots of the tunabilities are
knowledge in the web community.
depicted. For each ML algorithm, some outliers are visible, which indicates that tuning has
a much higher impact on some speciﬁc datasets.

3We also tried cubist (Kuhn et al., 2016), which provided good results but the algorithm had some technical
problems for some combinations of datasets and algorithms. We did not include gaussian process which is one of
the standard algorithms for surrogate models as it cannot handle categorical variables.

4In case of kknn four datasets did not provide results for one of the surrogate models and were not used.

8

Figure 1: Average performances over the datasets of diﬀerent surrogate models (target: AUC)
for diﬀerent algorithms (that were presented in 4.2).

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.045
0.024
0.025
0.012
0.025
0.006
0.014
0.042
0.004
0.006
0.029
0.014

0.069
glmnet
0.038
rpart
0.031
kknn
svm 0.056
0.010
0.043

0.037
0.016
0.006
0.048
0.007
0.017

ranger
xgboost

Impr-CV
0.032
0.022
0.025
0.008
0.003
0.026

Table 2: Overall tunability (regarding AUC) with the package defaults (Tun.P) and the new
defaults (Tun.O) as reference, cross-validated tunability (Tun.O-CV), average improvement (Im-
prov) and cross-validated average improvement (Impr-CV) obtained by using new defaults com-
pared to old defaults. The (cross-validated) improvement can be calculated by the (rounded)
diﬀerence between Tun.P and Tun.O (Tun.O-CV).

9

Figure 2: Boxplots of the tunabilities (AUC) of the diﬀerent algorithms. The upper and lower
whiskers (upper and lower line of the boxplot rectangle) are in our case deﬁned as the 0.1
and 0.9 quantiles of the tunability scores. The 0.9 quantile indicates how much performance
improvement can be expected on at least 10% of datasets. One outlier of glmnet (value 0.5) is
not shown.

5.1.2 Tunability of speciﬁc hyperparameters

In Table 3 the mean tunability (regarding the AUC) of single hyperparameters as deﬁned
in Equation (6) in section 3.4 can be seen. From here on, we will refer to tunability only
with respect to optimal defaults.

For glmnet lambda seems to be more tunable than alpha.

In rpart the minbucket
and minsplit parameters seem to be the most important ones for tuning. k in the kknn
algorithm is very tunable w.r.t. package defaults, but not regarding optimal defaults. In svm
the biggest gain in performance can be achieved by tuning the kernel, gamma or degree,
while the cost parameter does not seem to be very tunable. In ranger mtry is the most
tunable parameter. For xgboost there are two parameters that are quite tunable: eta and
the booster. booster speciﬁes if a tree or a linear model is trained. The cross-validated
results can be seen in Table 10 in the Appendix, they are quite similar to the non cross-
validated results and for all parameters slightly higher.

Instead of looking only at the average, as in Table 3, one could also be interested in the
distribution of the tunability of each dataset. As an example, Figure 3 shows the tunability
of each parameter of ranger in a boxplot. This gives a more in-depth insight into the
tunability, makes it possible to detect outliers and to examine the skewness.

5.1.3 Hyperparameter space for tuning

The hyperparameter space for tuning, as deﬁned in Equation (10) in section 3.6 and based
on the 0.05 and 0.95 quantiles, is displayed in Table 3. All optimal defaults are contained
in this hyperparameter space while some of the package defaults are not.

As an example, Figure 4 displays the full histogram of the best values of mtry of the
random forest over all datasets. Note that for quite a few data sets much higher values than
the package defaults seem advantageous. Analogous histograms for other parameters are
available through the shiny app.

10

Parameter Def.P

q0.05

q0.95

1
0

7

0.01
30
7
20

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

radial
1
1/p
3

radial
682.478
0.005
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
0

sample.fraction
mtry

√

983
FALSE
0.703
p · 0.257
FALSE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

30

0
21
12
24

0.403
0.004

Def.O Tun.P Tun.O
0.024
0.069
0.006
0.038
0.021
0.034
0.012
0.038
0.002
0.025
0.002
0.004
0.006
0.005
0.004
0.004
0.006
0.031
0.006
0.031
0.042
0.056
0.024
0.030
0.006
0.016
0.022
0.030
0.014
0.008
0.006
0.010
0.001
0.001
0.001
0.002
0.002
0.004
0.003
0.006
0.000
0.000
0.001
0.001
0.014
0.043
0.002
0.004
0.005
0.006
0.002
0.004
0.008
0.015
0.001
0.001
0.002
0.008
0.001
0.006
0.001
0.008
0.002
0.003
0.002
0.003

4168
0.018
0.839
gbtree
13
2.06
0.752
0.585
0.982
1.113

0.009
0.001

0
12.1
3.85
5

9.95

0.981
0.147

0.008
27
41.6
49.15

30

0.002
0.003
2

920.582
18.195
4

206.35

1740.15

0.323
0.035

0.974
0.692

0.007

0.513

920.7
0.002
0.545

5.6
1.295
0.419
0.335
0.008
0.002

4550.95
0.355
0.958

14
6.984
0.864
0.886
29.755
6.105

Table 3: Defaults (package defaults (Def.P) and optimal defaults (Def.O)), tunability of the hy-
perparameters with the package defaults (Tun.P) and our optimal defaults (Tun.O) as reference
and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters of the algorithms.

11

Figure 3: Boxplots of the tunabilities of the diﬀerent parameters of ranger. Same deﬁnition of
whiskers as in Figure 2.

Figure 4: Histogram of best parameter values for mtry of random forest over all considered data
sets.

12

5.1.4 Tunability of hyperparameter combinations

As an example, Table 4 displays the average tunability di1,i2 of all 2-way hyperparameter
combinations for rpart. Obviously, the increased ﬂexibility in tuning a 2-way combination
enables larger improvements when compared with the tunability of one of the respective
individual parameters.

In Table 5 the joint gain of tuning two hyperparameters gi1,i2 instead of only the best
as deﬁned in section 3.5 can be seen. The parameters minsplit and minbucket have the
biggest joint eﬀect, which is not very surprising, as they are closely related: minsplit is
the minimum number of observations that must exist in a node in order for a split to be
attempted and minbucket the minimum number of observations in any terminal leaf node.
If a higher value of minsplit than the default performs better on a dataset it is possibly
not enough to set it higher without also increasing minbucket, so the strong relationship is
quite clear. Again, further ﬁgures for other algorithms are available through the shiny app.

cp
maxdepth
minbucket
minsplit

0.002

cp maxdepth minbucket minsplit
0.004
0.005
0.011
0.004

0.006
0.007
0.006

0.003
0.002

Table 4: Tunability di1,i2 of hyperparameters of rpart, diagonal shows tunability of the single
hyperparameters.

cp
maxdepth
minbucket

0.0007

maxdepth minbucket minsplit
0.0004
0.0019
0.0055

0.0005
0.0014

Table 5: Joint gain gi1,i2 of tuning two hyperparameters instead of the most important in rpart.

6 Conclusion and Discussion

Our paper provides concise and intuitive deﬁnitions for optimal defaults of ML algorithms
and the impact of tuning them either jointly, tuning individual parameters or combinations,
all based on the general concept of surrogate empirical performance models. Tunability
values as deﬁned in our framework are easily and directly interpretable as how much per-
formance can be gained by tuning this hyperparameter?. This allows direct comparability of
the tunability values across diﬀerent algorithms.

In an extensive OpenML benchmark, we computed optimal defaults for elastic net, de-
cision tree, k-nearest neighbors, SVM, random forest and xgboost and quantiﬁed their tun-
ability and the tunability of their individual parameters. This – to the best of our knowledge
– has never been provided before in such a principled manner. Our results are often in line
with common knowledge from literature and our method itself now allows an analogous
analysis for other or more complex methods.

Our framework is based on the concept of default hyperparameter values, which can be
seen both as an advantage (default values are a valuable output of the approach) and as
an inconvenience (the determination of the default values is an additional analysis step and
needed as a reference point for most of our measures).

We now compare our method with van Rijn and Hutter (2017). In contrast to us, they
apply the functional ANOVA framework from Hutter et al. (2014) on a surrogate random
forest to assess the importance of hyperparameters regarding empirical performance of a
support vector machine, random forest and adaboost, which results in numerical importance

13

scores for individual hyperparameters. Their numerical scores are - in our opinion - less
directly interpretable, but they do not rely on defaults as a reference point, which one
might see as an advantage. They also propose a method for calculating hyperparameter
priors, combine it with the tuning procedure hyperband, and assess the performance of this
new tuning procedure. In contrast, we deﬁne and calculate ranges for all hyperparameters.
Setting ranges for the tuning space can be seen as a special case of a prior distribution - the
uniform distribution on the speciﬁed hyperparameter space. Regarding the experimental
setup, we compute more hyperparameter runs (around 2.5 million vs. 250000), but consider
only the 38 binary classiﬁcation datasets of OpenML100 while van Rijn and Hutter (2017)
use all the 100 datasets which also contain multiclass datasets. We evaluate the performance
of diﬀerent surrogate models by 10 times repeated 10-fold cross-validation to choose an
appropriate model and to assure that it performs reasonably well.

Our study has some limitations that could be addressed in the future: a) We only con-
sidered binary classiﬁcation, where we tried to include a wider variety of datasets from
diﬀerent domains. In principle this is not a restriction as our methods can easily be applied
to multiclass classiﬁcation, regression, survival analysis or even algorithms not from machine
learning whose empirical performance is reliably measurable on a problem instance. b) Uni-
form random sampling of hyperparameters might not scale enough for very high dimensional
spaces, and a smarter sequential technique might be in order here, see (Bossek et al., 2015)
for an potential approach of sampling across problem instances to learn optimal mappings
from problem characteristics to algorithm conﬁgurations. c) We currently are learning static
defaults, which cannot depend on dataset characteristics (like number of features, or fur-
ther statistical measures) as in meta-learning. Doing so might improve performance results
of optimal defaults considerably, but would require a more complicated approach. d) Our
approach still needs initial ranges to be set, in order to run our sampling procedure. Only
based on these wider ranges we can then compute more precise, closer ranges.

Acknowledgements

We would like to thank Joaquin Vanschoren for support regarding the OpenML platform
and Andreas Müller, Jan van Rijn, Janek Thomas and Florian Pﬁsterer for reviewing and
useful comments. Thanks to Jenny Lee for language editing.

References

J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of

Machine Learning Research, 13:281–305, 2012.

A. Biedenkapp, M. T. Lindauer, K. Eggensperger, F. Hutter, C. Fawcett, and H. H. Hoos.
In AAAI, pages

Eﬃcient parameter importance analysis via ablation with surrogates.
773–779, 2017.

M. Birattari, Z. Yuan, P. Balaprakash, and T. Stützle. F-Race and iterated F-Race: An
overview. In Experimental Methods for the Analysis of Optimization Algorithms, pages
311–336. Springer, 2010.

B. Bischl, O. Mersmann, H. Trautmann, and C. Weihs. Resampling methods for meta-model
validation with recommendations for evolutionary computation. Evolutionary Computa-
tion, 20(2):249–275, 2012.

B. Bischl, M. Lang, L. Kotthoﬀ, J. Schiﬀner, J. Richter, E. Studerus, G. Casalicchio, and
Z. M. Jones. mlr: Machine learning in R. Journal of Machine Learning Research, 17
(170):1–5, 2016.

B. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. van Rijn,
and J. Vanschoren. OpenML benchmarking suites and the OpenML100. ArXiv preprint
arXiv:1708.03731, 2017. URL https://arxiv.org/abs/1708.03731.

14

B. Bischl, J. Richter, J. Bossek, D. Horn, J. Thomas, and M. Lang. mlrMBO: A modular
framework for model-based optimization of expensive black-box functions. ArXiv preprint
arXiv:1703.03373, 2017. URL https://arxiv.org/abs/1703.03373.

J. Bossek, B. Bischl, T. Wagner, and G. Rudolph. Learning feature-parameter mappings
for parameter tuning via the proﬁle expected improvement. In Proceedings of the 2015
Annual Conference on Genetic and Evolutionary Computation, pages 1319–1326. ACM,
2015.

G. Casalicchio, J. Bossek, M. Lang, D. Kirchhoﬀ, P. Kerschke, B. Hofner, H. Seibold, J. Van-
schoren, and B. Bischl. OpenML: An R package to connect to the machine learning
platform OpenML. Computational Statistics, 32(3):1–15, 2017.

K. Eggensperger, F. Hutter, H. H. Hoos, and K. Leyton-Brown. Surrogate benchmarks for
hyperparameter optimization.
In Proceedings of the 2014 International Conference on
Meta-learning and Algorithm Selection-Volume 1201, pages 24–31. CEUR-WS. org, 2014.

K. Eggensperger, M. Lindauer, H. H. Hoos, F. Hutter, and K. Leyton-Brown. Eﬃcient
benchmarking of algorithm conﬁgurators via model-based surrogates. Machine Learning,
pages 1–27, 2018.

A. E. Eiben and S. K. Smit. Parameter tuning for conﬁguring and analyzing evolutionary

algorithms. Swarm and Evolutionary Computation, 1(1):19–31, 2011.

C. Fawcett and H. H. Hoos. Analysing diﬀerences between algorithm conﬁgurations through

ablation. Journal of Heuristics, 22(4):431–458, 2016.

M. Feurer, B. Letham, and E. Bakshy. Scalable meta-learning for bayesian optimization.

arXiv preprint 1802.02219, 2018. URL https://arxiv.org/abs/1802.02219.

I. Guyon, A. Saﬀari, G. Dror, and G. Cawley. Model selection: Beyond the bayesian/fre-

quentist divide. Journal of Machine Learning Research, 11(Jan):61–87, 2010.

F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for
general algorithm conﬁguration. In International Conference on Learning and Intelligent
Optimization, pages 507–523. Springer, 2011.

F. Hutter, H. H. Hoos, and K. Leyton-Brown. Identifying key algorithm parameters and
instance features using forward selection. In International Conference on Learning and
Intelligent Optimization, pages 364–381. Springer, 2013.

F. Hutter, H. Hoos, and K. Leyton-Brown. An eﬃcient approach for assessing hyperparam-
eter importance. In ICML, volume 32 of JMLR Workshop and Conference Proceedings,
pages 754–762, 2014.

D. Kühn, P. Probst, J. Thomas, and B. Bischl. Automatic Exploration of Machine
Learning Experiments on OpenML. ArXiv preprint arXiv:1806.10961, 2018. URL
https://arxiv.org/abs/1806.10961.

M. Kuhn, S. Weston, C. Keefer, and N. Coulter. Cubist: Rule- and instance-based regression

modeling, 2016. R package version 0.0.19.

D.

Kühn,
bot

P.
Probst,
benchmark

J.
data

Thomas,
(ﬁnal

and
B.
subset),

Bischl.
2018.

R
https://figshare.com/articles/OpenML_R_Bot_Benchmark_Data_final_subset_/5882230/2.

OpenML
URL

M. Lang, B. Bischl, and D. Surmann. batchtools: Tools for R to work on batch systems.

The Journal of Open Source Software, 2(10), 2017.

G. Luo. A review of automatic selection methods for machine learning algorithms and hyper-
parameter values. Network Modeling Analysis in Health Informatics and Bioinformatics,
5(1):1–16, 2016.

15

R. G. Mantovani, T. Horváth, R. Cerri, A. Carvalho, and J. Vanschoren. Hyper-parameter
In Brazilian Conference on Intelligent

tuning of a decision tree induction algorithm.
Systems (BRACIS 2016), 2016.

J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pages 2951–
2959, 2012.

J. N. van Rijn and F. Hutter. Hyperparameter importance across datasets. ArXiv preprint

arXiv:1710.04725, 2017. URL https://arxiv.org/abs/1710.04725.

J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo. OpenML: Networked science in

machine learning. SIGKDD Explorations, 15(2):49–60, 2013.

L. Waldron, M. Pintilie, M.-S. Tsao, F. A. Shepherd, C. Huttenhower, and I. Jurisica.
Optimized application of penalized regression methods to diverse genomic data. Bioin-
formatics, 27(24):3399–3406, 2011.

16

Appendix A. Results for accuracy and brier score

Figure 5: Same as ﬁgure 1 but with accuracy as target measure. Average performances over
the datasets of diﬀerent surrogate models (target: accuracy) for diﬀerent algorithms (that were
presented in 4.2).

Figure 6: Same as ﬁgure 1 but with brier score as target measure. Average performances over
the datasets of diﬀerent surrogate models (target: brier score) for diﬀerent algorithms (that were
presented in 4.2).

17

Figure 7: Boxplots of the tunabilities (accuracy) of the diﬀerent algorithms.

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.023
0.019
0.008
0.012
0.013
0.008
0.011
0.030
0.009
0.007
0.023
0.011

0.042
glmnet
0.020
rpart
0.021
kknn
svm 0.041
0.016
0.034

0.042
0.014
0.010
0.041
0.009
0.012

ranger
xgboost

Impr-CV
0.001
0.005
0.010
-0.001
0.006
0.022

Table 6: Tunability measures as in table 2, but calculated for the accuracy. Overall tunabil-
ity (regarding accuracy) with the package defaults (Def.P) and the optimal defaults (Def.O)
as reference points, cross-validated tunability (Def.O-CV), average improvement (Improv) and
cross-validated average improvement (Impr-CV) obtained by using new defaults compared to
old defaults. The (cross-validated) improvement can be calculated by the (rounded) diﬀerence
between Def.P and Def.O (Def.O-CV).

18

Parameter Def.P

Def.O Tun.P Tun.O q0.05

q0.95

7

14

2

30

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

1
0

0.01
30
7
20

0.252
0.005

0.002
19
5
13

radial
1
1/p
3

radial
936.982
0.002
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
1

sample.fraction
mtry

√

162
FALSE
0.76
p · 0.432
TRUE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

3342
0.031
0.89
gbtree
14
1.264
0.712
0.827
2.224
0.021

0.042
0.022
0.029
0.020
0.013
0.004
0.005
0.002
0.021
0.021
0.041
0.019
0.019
0.024
0.005
0.016
0.001
0.004
0.003
0.010
0.001
0.001
0.034
0.004
0.005
0.003
0.008
0.001
0.009
0.005
0.006
0.002
0.003

0.019
0.010
0.017
0.012
0.008
0.004
0.006
0.003
0.008
0.008
0.030
0.018
0.003
0.020
0.014
0.007
0.001
0.001
0.003
0.003
0.000
0.002
0.011
0.002
0.005
0.002
0.005
0.001
0.002
0.001
0.001
0.002
0.002

0.015
0.001

0
10
1.85
6.7

0.979
0.223

0.528
28
43.15
47.6

0.025
0.007
2

943.704
276.02
4

203.5

1908.25

0.257
0.081

0.971
0.867

0.009

0.453

1360
0.002
0.555

3
1.061
0.334
0.348
0.004
0.003

4847.15
0.445
0.964

13
7.502
0.887
0.857
5.837
2.904

Table 7: Tunability measures for single hyperparameters and tuning spaces as in table 3, but
calculated for the accuracy. Defaults (package defaults (Def.P) and own calculated defaults
(Def.O)), tunability of the hyperparameters with the package defaults (Tun.P) and our new
defaults (Tun.O) as reference and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters
of the algorithms.

19

Algorithm Tun.P Tun.O Tun.O-CV Improv
0.011
0.010
0.006
0.009
0.009
0.003
0.008
0.018
0.010
0.005
0.018
0.009

0.022
glmnet
0.015
rpart
0.012
kknn
svm 0.026
0.015
0.027

0.020
0.011
0.003
0.023
0.006
0.011

ranger
xgboost

Impr-CV
0.001
0.004
0.009
0.003
0.009
0.016

Table 8: Tunability measures as in table 2, but calculated for the brier score. Overall tunability
(regarding brier score) with the package defaults (Def.P) and the optimal defaults (Def.O)
as reference points, cross-validated tunability (Def.O-CV), average improvement (Improv) and
cross-validated average improvement (Impr-CV) obtained by using new defaults compared to
old defaults. The (cross-validated) improvement can be calculated by the (rounded) diﬀerence
between Def.P and Def.O (Def.O-CV).

Figure 8: Boxplots of the tunabilities (brier score) of the diﬀerent algorithms.

20

Parameter Def.P

q0.05

q0.95

1
0

7

0.01
30
7
20

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k
svm
kernel
cost
gamma
degree
ranger
num.trees

radial
1
1/p
3

radial
950.787
0.005
3

500
replace TRUE
1
p
respect.unordered.factors TRUE
1

sample.fraction
mtry

√

198
FALSE
0.667
p · 0.666
TRUE
1

min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

500
0.3
1
gbtree
6
1
1
1
1
1

19

0.997
0.004

0.001
13
12
18

Def.O Tun.P Tun.O
0.010
0.022
0.005
0.009
0.007
0.014
0.009
0.015
0.003
0.009
0.002
0.002
0.006
0.004
0.002
0.002
0.003
0.012
0.003
0.012
0.018
0.026
0.011
0.013
0.002
0.012
0.012
0.015
0.009
0.003
0.005
0.015
0.001
0.001
0.001
0.002
0.003
0.002
0.002
0.010
0.000
0.000
0.001
0.001
0.009
0.027
0.002
0.004
0.005
0.004
0.002
0.002
0.004
0.009
0.001
0.001
0.002
0.007
0.002
0.004
0.001
0.004
0.003
0.002
0.004
0.003

2563
0.052
0.873
gbtree
11
1.75
0.713
0.638
0.101
0.894

0.003
0.001

0
9
1
7

0.974
0.051

0.035
27.15
44.1
49.15

4.85

30

0.002
0.001
2

963.81
4.759
4

187.85

1568.25

0.317
0.072

0.964
0.954

0.008

0.394

2018.55
0.003
0.447

4780.05
0.436
0.951

2.6
1.277
0.354
0.363
0.006
0.003

13
5.115
0.922
0.916
28.032
2.68

Table 9: Tunability measures for single hyperparameters and tuning spaces as in table 3, but
calculated for the brier score. Defaults (package defaults (Def.P) and own calculated defaults
(Def.O)), tunability of the hyperparameters with the package defaults (Tun.P) and our new
defaults (Tun.O) as reference and tuning space quantiles (q0.05 and q0.95) for diﬀerent parameters
of the algorithms.

21

Measure

AUC

Accuracy

Brier score

glmnet
alpha
lambda
rpart
cp
maxdepth
minbucket
minsplit
kknn
k

Parameter Tun.O Tun.O-CV Tun.O Tun.O-CV Tun.O Tun.O-CV
0.020
0.019
0.015
0.010
0.018
0.017
0.011
0.012
0.005
0.008
0.003
0.004
0.006
0.006
0.003
0.003
0.003
0.008
0.003
0.008
0.023
0.030
0.016
0.018
0.002
0.003
0.016
0.020
0.014
0.014
0.006
0.007
0.001
0.001
0.001
0.001
0.003
0.003
0.003
0.003
0.000
0.000
0.001
0.002
0.011
0.011
0.002
0.002
0.006
0.005
0.002
0.002
0.004
0.005
0.001
0.001
0.003
0.002
0.002
0.001
0.002
0.001
0.004
0.002
0.004
0.002

0.024
0.006
0.021
0.012
0.002
0.002
0.006
0.004
0.006
0.006
svm 0.042
0.024
0.006
0.022
0.014
0.006
0.001
0.001
0.002
0.003
0.000
0.001
0.014
0.002
0.005
0.002
0.008
0.001
0.002
0.001
0.001
0.002
0.002

0.037
0.006
0.034
0.016
0.002
0.002
0.009
0.004
0.006
0.006
0.048
0.030
0.006
0.028
0.020
0.007
0.002
0.002
0.002
0.004
0.000
0.001
0.017
0.002
0.006
0.002
0.008
0.001
0.003
0.002
0.001
0.003
0.004

0.010
0.005
0.007
0.009
0.003
0.002
0.006
0.002
0.003
0.003
0.018
0.011
0.002
0.012
0.009
0.005
0.001
0.001
0.003
0.002
0.000
0.001
0.009
0.002
0.005
0.002
0.004
0.001
0.002
0.002
0.001
0.003
0.004

0.042
0.026
0.039
0.014
0.008
0.004
0.007
0.003
0.010
0.010
0.041
0.031
0.003
0.031
0.027
0.009
0.003
0.002
0.003
0.005
0.001
0.002
0.012
0.003
0.006
0.002
0.005
0.001
0.002
0.001
0.001
0.003
0.003

kernel
cost
gamma
degree
ranger
num.trees
replace
sample.fraction
mtry
respect.unordered.factors
min.node.size
xgboost
nrounds
eta
subsample
booster
max_depth
min_child_weight
colsample_bytree
colsample_bylevel
lambda
alpha

Table 10: Tunability with calculated defaults as reference without (Tun.O) and with (Tun.O-CV)
cross-validation for AUC, accuracy and brier score

22

