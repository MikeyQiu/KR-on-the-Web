Sequential Score Adaptation with Extreme Value Theory
for Robust Railway Track Inspection

Xavier Gibert
University of Maryland
College Park, MD
gibert@umiacs.umd.edu

Vishal M. Patel
Rutgers University
Piscataway, NJ
vishal.m.patel@rutgers.edu

Rama Chellappa
University of Maryland
College Park, MD
rama@umiacs.umd.edu

5
1
0
2
 
t
c
O
 
0
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
2
8
5
0
.
0
1
5
1
:
v
i
X
r
a

Abstract

Periodic inspections are necessary to keep railroad
tracks in state of good repair and prevent train accidents.
Automatic track inspection using machine vision technology
has become a very effective inspection tool. Because of its
non-contact nature, this technology can be deployed on vir-
tually any railway vehicle to continuously survey the tracks
and send exception reports to track maintenance person-
nel. However, as appearance and imaging conditions vary,
false alarm rates can dramatically change, making it difﬁ-
cult to select a good operating point. In this paper, we use
extreme value theory (EVT) within a Bayesian framework
to optimally adjust the sensitivity of anomaly detectors. We
show that by approximating the lower tail of the probabil-
ity density function (PDF) of the scores with an Exponential
distribution (a special case of the Generalized Pareto distri-
bution), and using the Gamma conjugate prior learned from
the training data, it is possible to reduce the variability in
false alarm rate and improve the overall performance. This
method has shown an increase in the defect detection rate
of rail fasteners in the presence of clutter (at PFA 0.1%)
from 95.40% to 99.26% on the 85-mile Northeast Corridor
(NEC) 2012-2013 concrete tie dataset.

1. Introduction

In sequential inspection problems, such as visual rail-
way track inspection, a video feed is streamed from one or
more cameras to a detection system, and we are interested
in designing a detector that can ﬁnd abnormal patterns in
such data. There is a limit to the number of false alarms
that the operator can handle, so it is necessary to select the
optimal operating point at which the false alarm rate does
not exceed such limit. Indeed, most of the data that an au-
tonomous inspection vehicle will collect will be discarded
without anyone ever looking at it. Therefore, an excessively
high false alarm rate will result in a waste of storage space

Figure 1. Deﬁnition of basic track elements.

and bandwidth. The only relevant images are the ones that
correspond to unexpected patterns, so we are actually inter-
ested in ﬁnding such anomalous patterns.

Anomaly detection is a hypotheses testing problem in
which the null hypothesis is that an image is normal and the
alternative hypothesis is that it is anomalous. Due to the
complexity of the scene and image formation process, both
hypothesis are composite, with nuisance parameters arising
from changes in illumination, occlusion, background clut-
ter, and many other uncontrollable factors. Rather than try-
ing to model each of these variables individually, in this
paper we adapt the detection scores with the objective of
reducing the variability in type I error rate. The is known
as constant false alarm rate (CFAR) detection. We adopt
the Bayesian view that such parameters are random vari-
ables with one realization per image. The images have a
natural order based on the time they were captured at, so
the sequence of these random parameters forms a random
process. A key observation is that this random process has
strong long-term dependencies. The effect of such slowly
varying nuisance parameters is that false alarms are con-
centrated in small segments of the image sequence.

1

2.8171

2.2172

2.1372

2.2761

2.7332

-1.5259

-0.8281

-0.7909

-0.7995

-0.5839

-0.2813

-0.8813

-0.8373

-0.5157

1.4479

-2.0874

-2.1373

-2.3936

-2.8944

-2.5422

(a)

(b)

(c)

(d)

Figure 2. Examples of fastener scores (a) Good fasteners with
high scores (b) Good fasteners with low scores (c) Defective fas-
teners with high scores (d) Defective fasteners with low scores

Figure 1 shows the deﬁnitions of several track compo-
nents. In this paper, we focus on fastener inspection. Fig-
ure 2 shows examples of good and defective fasteners and
their detection scores generated by the multi-task learning
method [3] of Gibert et al. Although most fasteners have
high scores and most defective ones have low scores, when
good fasteners have low scores, there is an underlying phe-
nomenon that causes scores of nearby images to also be low.
The rest of the paper is organized as follows. In Sec-
tion 2 we review related works. The algorithm is described
in Section 3. Experimental results are described in Sec-
tion 4. Section 5 concludes the paper with a brief summary
and discussion.

2. Background

2.1. Robust Anomaly Detection

The presence of outliers is a challenge that many com-
puter vision systems have to deal with. The RANdom
SAmple Consensus (RANSAC) algorithm [2] has been used
in many applications for removing outliers when ﬁtting a
model to data. This method is specially useful when most of
the samples follow a linear model plus additive i.i.d. Gaus-
sian noise, but a few samples with gross errors do not fol-
low this model. However, in many applications, it not clear

which samples should be treated as inliers and which of
them are outliers. For instance, in big data applications,
the data just appears to have a distribution with long tails
that decay at slower rate than the corresponding Gaussian
distribution that best ﬁts the data in the least squares sense.
Indeed, what appears to be an outlier in feature space may
just be a normal sample that has been subject to some kind
of degradation for which the feature extractor was not de-
signed for. These degradation modes may include impulse
noise, partial occlusion, and in some cases, changes in ap-
pearance due to blur, shadows, or pose. In anomaly detec-
tion problems, the samples of interest are those in the tail
of such data distribution. Therefore, any method that dis-
cards outliers have the potential of discarding anomalies, so
in order to successfully ﬁnd anomalies in such images it is
necessary to use other methods.

The ﬁeld of robust statistics [7, 10] provides the tools
for estimation of unknown quantities when the underlying
probability distribution is non-Gaussian and it is not known
exactly. In practice, the data can be modeled as the mix-
ture of a Gaussian distribution and a heavy-tailed distribu-
tion (the contaminated Gaussian model). In this case, it is
desirable to design an estimator whose performance is min-
imax over a family of distributions that includes the Gaus-
sian as a special case. There are basically three types of ro-
bust estimates: M-estimates[6] (Maximum likelihood type),
L-estimates (Linear combination of order statistics), and R-
estimates (Estimates derived from rank tests).

In supervised learning problems, there is a distinction on
how to handle outliers at training time vs. testing time. Su-
pervision at training time usually mitigates the problem of
outliers as it is possible to manually select the inliers. The
use of the (cid:96)1 minimization promotes a sparse representation
of the data. The solution of the (cid:96)1 minimization is the Max-
imum Likelihood Estimate of the location parameter when
the data follows a Laplacian distribution, and a straightfor-
ward way of robustifying a regression procedure is by re-
placing the (cid:96)2 norm in the cost function by the (cid:96)1 norm. A
related L-estimator that results from such (cid:96)1 optimization is
the Least Median of Squares (LMS), which was introduced
in the computer vision ﬁeld by Kim et al. [8]. The draw-
back of the LMS is that the median estimator’s efﬁciency is
only 2
π = 0.637 when the true distribution is Gaussian. The
M-estimator based on the Huber loss function[6]




1
2

t2

ρ(t) =

for |t| < k

(1)

k2



k|t| −

for |t| ≥ k

1
2
is more ﬂexible because it has the sample mean (k = ∞)
and sample median (k = 0) as special cases and it can be
tuned to handle different degrees of contamination in the
contaminated Gaussian model. However, since this estima-
tor depends on a scale parameter k (unlike L-estimators,

which are scale-invariant), it is necessary to ﬁrst estimate
this parameter using a robust scale estimator.

function of the excesses),

2.2. Extreme Value Theory for Adaptive Anomaly

Detection

Due to illumination and viewpoint changes, clutter dis-
tribution, and other image degradation, the distribution of
features extracted from images at test time, does not match
what was observed during training. Moreover, such distri-
bution may not be stationary, but slowly changes over time,
so a ﬁxed threshold would result in large variability in the
false alarm rate. Broadwater and Chellappa[1] proposed
a technique to ﬁnd adaptive thresholds for Constant False
Alarm Rate (CFAR) detectors based on Extreme Value The-
ory (EVT) [5] that can be used even when limited training
data is available. EVT is applicable to problems where the
probability of a rare event must be estimated even if such a
rare event has never occurred. Scheirer et al. [12, 13] also
used EVT for score normalization and showed its applica-
bility to sensor fusion problems.

For completeness, we recall the EVT basic results below.
Let X1, . . . , Xn be i.i.d. samples from an unknown distri-
bution F and Mn = max(X1, . . . , Xn), the maximum of n
i.i.d. variables. The fundamental EVT theorem, the Fisher-
Tippett-Gnedenko theorem[5], states that if there exist a se-
quence of pairs of real numbers (an, bn) such that an > 0
for all n and a distribution function Λ(x) such that

lim
n→∞

P

(cid:18) Mn − bn
an

(cid:19)

≤ x

= Λ(x),

(2)

for all x at which Λ(x) is continuous, then the limit distribu-
tion Λ(x) belongs to either the Gumbel, the Fr´echet or the
Weibull family. These three families can be grouped into
the Generalized Extreme Value Distribution (GEVD)

Λ(x; µ, σ, ξ) = exp

−

1 + ξ

,

(3)

(cid:40)

(cid:20)

(cid:19)(cid:21)−1/ξ(cid:41)

(cid:18) x − µ
σ

where µ ∈ R is the location parameter, σ > 0 the scale
parameter and ξ ∈ R the shape parameter. The Gumbel
distribution is a special case of the GEVD when ξ = 0, the
Fr´echet when ξ > 0, and the Weibull when ξ < 0. When
the limiting distribution exists, we say that F (x) lies in the
“domain of attraction” of Λ(x).

In many practical applications, we are interested in the
tail distribution of the distribution F . Given an upper
threshold u, we select the Nn samples that exceed such
threshold and deﬁne the excesses Y1, . . . , YNn as Yi =
Xj − n, where i is the excess index and j is the index of the
original sample. The probability of exceeding the thresh-
old is λ = 1 − F (u). For sufﬁciently large u, the upper
tail distribution function Fu(y) (the conditional distribution

Fu(y) =

F (u + y) − F (u)
1 − F (u)

(4)

can be approximated by a Generalized Pareto Distribution

G(y; σ, ξ) = 1 −

1 +

,

y > 0.

(5)

(cid:18)

(cid:19)−1/ξ

ξy
σ

+

where σ > 0, ξ ∈ R, and x+ = max(x, 0). This ap-
proximation is justiﬁed by the Pickands theorem[11], which
states that

inf
ξ

lim
u↑ωF

inf
σ

sup
y>0

|Fu(y) − G(y; σ, ξ)| = 0

(6)

if and only if F is in the domain of attraction of the GEVD.
Note that the exponential distribution is a special case of the
GPD for ξ = 0, i.e. G(y; σ, 0) = 1 − e−y/σ.

These results can be extended to the multivariate case,
for example to model the tail distribution of the maximum
of a cluster of observations. Under stationarity of observa-
tions, this can be achieved by incorporating both the tail of
the marginal distribution and the so-called extremal index.
Let {Xn : n ≥ 1} be a (strictly) stationary sequence of r.v.’s
with marginal distribution F . Then, for sufﬁciently large n

P {Mn ≤ un} ≈ F nθ(un),

(7)

where un is any high threshold such that n(1−F (un)) con-
verges to a positive number as n → ∞ and θ is a ﬁxed
number in [0, 1]. θ is the extremal index that measures the
strength of dependence of {Xn}. If {Xn} are independent,
then θ = 1. On the other hand, if {Xn} are highly depen-
dent, then θ ≈ 0. A method for estimating the extremal
index for a real-valued Markov chain was proposed by Yun
[15].

3. Proposed Approach

In this section we describe our approach for normalizing
the scores of an anomaly detector deployed in an application
in which the distribution of the normal samples gradually
changes over time. This may be caused by changes in illu-
mination, change in view-point, addition or removal of clut-
ter, or other uncontrollable factors. The approach is similar
to the method proposed by Broadwater and Chellappa[1] in
which an adaptive threshold is estimated from the GPD ﬁt
to the upper tail of the distribution after removing the out-
liers or targets using a Kolmogorov-Smirnov statistical test.
The difference is that our method is Bayesian and we work
with sequential data and estimate the adaptive threshold for
each sample.

3.1. Bayesian Model

We want to adapt the scores of an anomaly detector ap-
plied to a sequence of images so that, when we apply a given
threshold, we get an approximate CFAR. The images have
been collected from a moving vehicle, so the environmen-
tal conditions and clutter distribution are not stationary, but
slowly change over time. In EVT-based threshold estima-
tion, it is necessary to estimate the parameters σ and ξ of
the GPD from the upper- or lower-tail of the empirical dis-
tribution. For the rest of this paper we will refer to the up-
per tail of the distribution of the random variable X, but the
same applies to the lower tail since the lower tail of X is
the upper tail of Z = −X. The threshold u needs to be
set high enough so that the tail of F (x) converges in dis-
tribution to the GPD. However, since we are dealing with a
non-stationary random process, we need to work on a small
window centered at the sample of interest. This window
needs to be long enough so that we can ﬁt the parameters of
the GPD to its tail (for example the largest 5% of the sam-
ples), but short enough that the distribution has not changed
much. In applications in which the dynamics of the pro-
cess change quickly, our options are rather limited. If we
ﬁt a GPD to the extreme samples of a short window, the
estimated threshold has so much variance that the resulting
performance is worse than using a ﬁxed threshold. On the
other hand, if the window is too long, the threshold does not
adapt at all. For example, if we use a window of 100 sam-
ples and select the upper threshold to the 95th percentile,
we would only have 5 samples to estimate the 2 parameters
of the GPD, resulting in severe overﬁtting.

To overcome this limitation, we will make one simpli-
ﬁcation by ﬁxing ξ = 0, so we only need to estimate one
parameter instead of two. Under ξ = 0, the GPD reduces to
the exponential distribution

G(y; σ, ξ = 0) = 1 − e−y/σ.

(8)

For convenience, we apply the parameterization λ = 1/σ
and write the Exponential in its canonical form

G(y; λ) = 1 − e−λy
g(y; λ) = λe−λy.

(9)

(10)

As opposed to the general case of the GPD, the Exponen-
tial distribution is a member of the exponential family, so it
has a non-trivial sufﬁcient statistic from which we can eas-
ily compute the maximum likelihood estimate (MLE) of its
parameter. Its conjugate prior is the Gamma distribution,

π(λ; α, β) =

λα−1e−βλ,

(11)

βα
Γ(α)

Algorithm 1 EVT training algorithm.
1: procedure TRAIN(T , pu, w0)
2:
3:

n ← 0, s ← 0
(cid:46) Initialize sufﬁcient statistics
for all (x, y) ∈ T do (cid:46) Training set T contains x

scores, y labels

threshold

g ← {xi | yi = 0}
u ← u | #{gi > u} = #g pu

(cid:46) Select negative samples
(cid:46) Find upper

t ← {gi | gi > u} - u
n ← n + #t
s ← s + (cid:80) t

(cid:46) Extract upper tail
(cid:46) Update counts
(cid:46) Update sum

4:
5:

6:
7:
8:

end for
α0 ← 1 + w0
β0 ← w0 s
n
return α0, β0

9:
10:
11:
12:
13: end procedure

(cid:46) Parameters of the Gamma prior

Gamma(λ; α0, β0) prior can be computed as

α1 = α0 + n
n
(cid:88)

β1 = β0 +

yi.

i=1

(12)

(13)

Moreover, the maximum a posteriori (MAP) estimate has
the closed from (cid:98)λ = β
α−1 . This simpliﬁed model allows us
to derive a very fast adaptation algorithm that we describe
in the following section. This approximation works well
in practice, especially when the scores are trained with a
sparsity promoting loss function such as the hinge loss.

3.2. Training

Our training set T contains a number of sequences of
scores x with their corresponding sequences of labels y.
During training, we compute the sufﬁcient statistics n and
s for all the samples that are not labeled as anomalies (the
sufﬁcient statistic is all we need to characterize the Gamma
prior distribution). We then re-scale them to limit the ef-
fect of this prior. Effectively, we use w0 pseudo-samples
instead n (the number of samples in the training set). This
is necessary because n is usually a very large number, and
computing α0 and β0 with it would result in a very strong
prior that would introduce too much bias in the MAP esti-
mate.

The steps of the training procedure are described in Al-
gorithm 1. The parameter pu is the probability of the tail,
and w0 is the weight in sample counts that we assign to the
training set. In our experiments we used pu = 0.05 and
w0 = 400.

3.3. Proposed Adaptive Thresholding Algorithm

the non-informative (improper) prior is given by α = 1,
β = 0, and the parameters of the Gamma posterior under a

During testing, we ﬁrst perform a series of Kolmogorov-
Smirnov (KS) tests [14] to ﬁnd and remove anomalies. The

Algorithm 2 EVT adaptive thresholding algorithm

1: procedure ADAPTSCORES(x, α0, β0, pu, pf , w1, L,

(cid:98)λ0 ← β0
α0−1
y ← sort desc(x)

(cid:46) MLE in training set
(cid:46) Sort scores in descending

na)

order

k ← #y pu
for i ← 1, na do

scores, y labels

(cid:46) Training set T contains x

u ← yi+k
t ← {yi, . . . , yi+k} − u
Dn,i = supx∈t

(cid:46) Find upper threshold
(cid:46) Extract upper tail
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:46) Compute
(cid:12) (cid:98)Gn(x) − G(x; λ)

KS statistic
end for
ˆi ← mini{Dn,i}
u(cid:48) ← yˆi
t ← {yˆi, . . . , yˆi+k} − u
α1 ← α0 + w1
β1 ← β0 + w1
for i ← 1, n do

(cid:80) t

#t

(cid:46) Estimate number of outliers
(cid:46) Set outlier rejection threshold
(cid:46) Extract upper tail

w ← xi−(L−1)/2:i+(L−1)/2

(cid:46) Window

u ← u | #{wi > u} = #w pu

(cid:46) Find upper

centered at sample xi

threshold

t ← {wi | wi > u} - u
α ← α1 + #t
β ← β1 + (cid:80) t
(cid:98)λ ← β
α−1
yi ← xi + u − (cid:98)λ log(pf /pu)

(cid:46) Extract upper tail
(cid:46) Posterior
(cid:46) Posterior
(cid:46) MAP estimate
(cid:46) Adapt score

(cid:46) Adapted scores

2:
3:

4:

5:

6:
7:

8:

9:
10:
11:
12:

13:
14:
15:
16:

17:

18:
19:
20:

21:

22:
23:
24:
25: end procedure

end for
return y

KS statistic

Dn = sup
x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)Gn(x) − G(x; (cid:98)λ)
(cid:12)

(14)

measures the dissimilarity between distributions G(x; (cid:98)λ)
and (cid:98)Gn(x). G(x; (cid:98)λ) is the GPD in (9) and

(cid:98)Gn(x) = 1 −

I(Xi ≤ x)

(15)

1
n

n
(cid:88)

i=1

where I(x) is a standard indicator function, is the empirical
tail CDF. The KS test requires estimating a threshold Kα
for rejecting (with conﬁdence 1 − α) the hypothesis that
nDn > Kα.
the observed data does not ﬁt G with the test
The result from Lilliefors[9] shows that the KS test is biased
when the reference distribution G is not precisely known (in
this case, (cid:98)λ is estimated from the training data). However,
as noted in [1], it is not necessary to identify the exact value

√

of α for the purpose of removing anomalies and outliers.
Instead, we ﬁrst compute Dn with all the samples in the tail.
We call this Dn,1. We then remove the largest sample and
we compute Dn,2 using the remaining samples. We keep
iterating until we get Dn,na . Finally, we select the value of
i that minimizes Dn,i.

After removing the anomalies, we use the prior estimated
during training to compute the posterior for the whole se-
quence. This posterior is used as the prior for estimating the
tail distribution on each shift of a window centered on each
of the samples. The details of the adaptation procedure are
described in Algorithm 2. The input to the adaptation pro-
cedure is a sequence of scores x, the parameters of the prior
Gamma distribution α0 and β0, the size of the upper tail pu,
the target false alarm rate pf , the weight w1 that we assign
to the the prior contribution of the whole sequence, the win-
dow length L, and the maximum number of anomalies na
in the sequence. The output sequence y has been adapted
so that when it is thresholded at 0, the false alarm rate is pf .
In our experiments, we have used pu = 0.05, pf = 0.001,
w1 = 100, L = 101, and na = 12.

4. Experimental Results

To validate the effectiveness of the proposed approach,
we have used the 340 sequences of fastener detections cor-
responding to each of the 4 cameras in each of the 85 miles
of the Amtrak NEC concrete tie dataset introduced in [4].
This dataset contains a total of 203,287 ties and each tie is
divided in 4 regions (left ﬁeld, left gage, right gage, and
right ﬁeld), so the total number of images is 813,148. The
detection problem consists in determining whether an im-
age contains a fastener attached to one of the rails. The
dataset contains bounding boxes for all the images that are
known to contain a defect. The total number of defects is
1,087 (0.13% of all the fasteners). The defective fastener
class contains two subclasses: broken fastener and missing
fastener.

We have used the scores generated by the multi-task
learning (MTL) detector described in [3]. This detector
uses deep learning with multiple tasks that are trained in
parallel. The reason for using multiple tasks is to prevent
overﬁtting. By sharing a common low-level representation
between the fastener inspection task and a separate mate-
rial classiﬁcation task, there is a data ampliﬁcation effect
that results in better generalization for both classiﬁers. We
also compare the performance with the baseline single-task
learning (STL) method in [4]. The raw data was provided
by Amtrak, and the authors of [3, 4] provided the output
of their detectors as well as the codes to evaluate the per-
formance. This detector produces a scalar-valued score for
each image by spatially pooling all the detections in the im-
age. Scores are high when the image contains a good fas-
tener, and low when the fastener is either missing or broken.

Figure 2 shows several detection examples of the MTL de-
tector.

To facilitate the evaluation of fastener detection perfor-
mance under difﬁcult scenarios, whenever the fastener is not
directly attached to the rail or tie, or when for some reason
a fastener is not visible at all, those ties are marked as unin-
spectable with a special label. Depending on the value of
such label, the dataset is divided into 3 subsets:

• Clear ties: 200,763 ties (1,037 ties with at least one

defect).

• Clear ties plus switches: 201,856 ties (1,045 ties with
at least one defect). See Figure 3 for an example of a
switch section.

• All ties: 203,287 ties (1,052 ties with at least one de-
fect). This includes switches, and ties for which some
fasteners are not visible because they are covered by
ballast or a lubricator. See Figures 4 and 5 for exam-
ples of high ballast and lubricator sections.

Figure 4. Example of section marked as ballast.

Figure 3. Example of section marked as switch.

For training, we use all the available data after setting
aside the sequence being tested. Table 1 and Figure 6 show
the detection results on the normalized scores. The overall
improvement is signiﬁcant. The detection rate on the whole
dataset at P F A = 0.1% increases from 95.40% to 99.26%.
This is a 6× reduction in the missed rate. Moreover, the per-
formance on the clear tie subset does not degrade at all. The
running time of our EVT adaptation algorithm implemented
in MATLAB1 for adapting all 813,148 scores is only of 17
seconds on a Mid-2012 MacBook Pro with a 2.5 GHz Intel
Core i5 processor, so this dramatic improvement comes at
negligible computational cost (running the detector process
takes several hours).

1The code and data used in this section is available at
https://github.com/xavigibert/EvtTrack

Figure 5. Example of section marked as lubricator.

5. Conclusions

In this paper, we presented a new algorithm that nor-
malizes scores from a sequential anomaly detector with
the objective of harmonizing its false alarm rate. Extreme
value theory provides a solid foundation from which adap-
tive thresholding algorithms can be derived. When working
with sequences of images, we need to take advantage of the
statistical dependencies of nuisance parameters of nearby
images. If we discard such dependencies and treat each im-
age in the sequence independently, the performance suffers.
The CFAR detection approach proposed in this paper
has applicability beyond railway track inspection from a
moving vehicle. It could be used, for example, in surveil-
lance video to remove bursts of false alarms caused by sun
glare, insects, rain or fog. Its computational cost is negligi-
ble compared to that of the underlying detector, so this ap-
proach can be easily retroﬁtted to existing detectors already

Condition

PFA MTL + EVT MTL[3]

STL[4]

Fastener (only clear ties)

Fastener (clear + switch)

Fastener (all ties)

0.1%
0.02%

0.1%
0.02%

0.1%
0.02%

99.91%
97.20%

99.54%
93.80%

99.26%
93.47%

99.91% 98.41%
96.74% 93.19%

98.43% 94.54%
89.35% 88.70%

95.40% 87.38%
87.76%

–

Table 1. Fastener detection results before and after score normalization.

[12] W. Scheirer, A. Rocha, R. Micheals, and T. Boult. Robust fu-
sion: Extreme value theory for recognition score normaliza-
tion. In European Conference on Computer Vision (ECCV),
pages 481–495. Springer, 2010. 3

[13] W. J. Scheirer, A. Rocha, R. J. Micheals, and T. E. Boult.
Meta-recognition: The theory and practice of recognition
score analysis. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 33(8):1689–1695, 2011. 3

[14] A. Stuart, J. Ord, and S. Arnold. Kendall’s Advanced Theory
of Statistics, Volume 2A: Classical Inference and the Linear
Model. Hodder Arnold, London, U.K., 1999. 4

[15] S. Yun. The extremal index of a higher-order stationary
markov chain. The Annals of Applied Probability, 8(2):408–
437, may 1998. 3

in operation.

Acknowledgements

The authors thank Amtrak, ENSCO, Inc. and the Federal
Railroad Administration for providing the data used in this
paper.

References

[1] J. Broadwater and R. Chellappa. Adaptive threshold estima-
tion via extreme value theory. IEEE Transactions on Signal
Processing, 58(2):490–500, 2010. 3, 5

[2] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: A paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
ACM, 24(6):381–395, 1981. 2

[3] X. Gibert, V. M. Patel, and R. Chellappa. Deep multi-task
learning for railway track inspection. arXiv:1509.05267,
2015. 2, 5, 7

[4] X. Gibert, V. M. Patel, and R. Chellappa. Robust fastener
detection for autonomous visual railway track inspection. In
IEEE Winter Conference on Applications of Computer Vision
(WACV), 2015. 5, 7

[5] E. Gumbel. Statistics of Extremes. Columbia University

Press, New York, 1958. 3

[6] P. J. Huber. Robust estimation of a location parameter. The
Annals of Mathematical Statistics, 35(1):73–101, 1964. 2
[7] P. J. Huber and E. M. Ronchetti. Robust Statistics. Wiley se-
ries in probability and statistics. John Wiley & Sons, Hobo-
ken, New Jersey, second edition, 2009. 2

[8] D. Y. Kim, J. J. Kim, P. Meer, D. Mintz, and A. Rosenfeld.
Robust computer vision: A least median of squares based
In in Proc. of Image Understanding Workshop,
approach.
pages 1117–1134, 1989. 2

[9] H. W. Lilliefors. On the Kolmogorov-Smirnov test for nor-
mality with mean and variance unknown. Journal of the
American Statistical Association, 62(318):399–402, 1967. 5
[10] R. A. Maronna, D. R. Martin, and V. J. Yohai. Robust Statis-
tics: Theory and Methods. Wiley series in probability and
statistics. John Wiley & Sons, Chichester, England, 2006. 2
[11] J. Pickands. Statistical inference using extreme order statis-
tics. The Annals of Statistics, 3(1):119–131, jan 1975. 3

(a)

(b)

(c)

Figure 6. ROC curves comparing defective fastener detection performance on the 85-mile testing set using normalized vs. unnormalized
scores (a) on the clear ties subset (b) on the clear with with switches subset (c) on all ties. Detections are per image (each tie has 4 images).

Sequential Score Adaptation with Extreme Value Theory
for Robust Railway Track Inspection

Xavier Gibert
University of Maryland
College Park, MD
gibert@umiacs.umd.edu

Vishal M. Patel
Rutgers University
Piscataway, NJ
vishal.m.patel@rutgers.edu

Rama Chellappa
University of Maryland
College Park, MD
rama@umiacs.umd.edu

5
1
0
2
 
t
c
O
 
0
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
2
8
5
0
.
0
1
5
1
:
v
i
X
r
a

Abstract

Periodic inspections are necessary to keep railroad
tracks in state of good repair and prevent train accidents.
Automatic track inspection using machine vision technology
has become a very effective inspection tool. Because of its
non-contact nature, this technology can be deployed on vir-
tually any railway vehicle to continuously survey the tracks
and send exception reports to track maintenance person-
nel. However, as appearance and imaging conditions vary,
false alarm rates can dramatically change, making it difﬁ-
cult to select a good operating point. In this paper, we use
extreme value theory (EVT) within a Bayesian framework
to optimally adjust the sensitivity of anomaly detectors. We
show that by approximating the lower tail of the probabil-
ity density function (PDF) of the scores with an Exponential
distribution (a special case of the Generalized Pareto distri-
bution), and using the Gamma conjugate prior learned from
the training data, it is possible to reduce the variability in
false alarm rate and improve the overall performance. This
method has shown an increase in the defect detection rate
of rail fasteners in the presence of clutter (at PFA 0.1%)
from 95.40% to 99.26% on the 85-mile Northeast Corridor
(NEC) 2012-2013 concrete tie dataset.

1. Introduction

In sequential inspection problems, such as visual rail-
way track inspection, a video feed is streamed from one or
more cameras to a detection system, and we are interested
in designing a detector that can ﬁnd abnormal patterns in
such data. There is a limit to the number of false alarms
that the operator can handle, so it is necessary to select the
optimal operating point at which the false alarm rate does
not exceed such limit. Indeed, most of the data that an au-
tonomous inspection vehicle will collect will be discarded
without anyone ever looking at it. Therefore, an excessively
high false alarm rate will result in a waste of storage space

Figure 1. Deﬁnition of basic track elements.

and bandwidth. The only relevant images are the ones that
correspond to unexpected patterns, so we are actually inter-
ested in ﬁnding such anomalous patterns.

Anomaly detection is a hypotheses testing problem in
which the null hypothesis is that an image is normal and the
alternative hypothesis is that it is anomalous. Due to the
complexity of the scene and image formation process, both
hypothesis are composite, with nuisance parameters arising
from changes in illumination, occlusion, background clut-
ter, and many other uncontrollable factors. Rather than try-
ing to model each of these variables individually, in this
paper we adapt the detection scores with the objective of
reducing the variability in type I error rate. The is known
as constant false alarm rate (CFAR) detection. We adopt
the Bayesian view that such parameters are random vari-
ables with one realization per image. The images have a
natural order based on the time they were captured at, so
the sequence of these random parameters forms a random
process. A key observation is that this random process has
strong long-term dependencies. The effect of such slowly
varying nuisance parameters is that false alarms are con-
centrated in small segments of the image sequence.

1

2.8171

2.2172

2.1372

2.2761

2.7332

-1.5259

-0.8281

-0.7909

-0.7995

-0.5839

-0.2813

-0.8813

-0.8373

-0.5157

1.4479

-2.0874

-2.1373

-2.3936

-2.8944

-2.5422

(a)

(b)

(c)

(d)

Figure 2. Examples of fastener scores (a) Good fasteners with
high scores (b) Good fasteners with low scores (c) Defective fas-
teners with high scores (d) Defective fasteners with low scores

Figure 1 shows the deﬁnitions of several track compo-
nents. In this paper, we focus on fastener inspection. Fig-
ure 2 shows examples of good and defective fasteners and
their detection scores generated by the multi-task learning
method [3] of Gibert et al. Although most fasteners have
high scores and most defective ones have low scores, when
good fasteners have low scores, there is an underlying phe-
nomenon that causes scores of nearby images to also be low.
The rest of the paper is organized as follows. In Sec-
tion 2 we review related works. The algorithm is described
in Section 3. Experimental results are described in Sec-
tion 4. Section 5 concludes the paper with a brief summary
and discussion.

2. Background

2.1. Robust Anomaly Detection

The presence of outliers is a challenge that many com-
puter vision systems have to deal with. The RANdom
SAmple Consensus (RANSAC) algorithm [2] has been used
in many applications for removing outliers when ﬁtting a
model to data. This method is specially useful when most of
the samples follow a linear model plus additive i.i.d. Gaus-
sian noise, but a few samples with gross errors do not fol-
low this model. However, in many applications, it not clear

which samples should be treated as inliers and which of
them are outliers. For instance, in big data applications,
the data just appears to have a distribution with long tails
that decay at slower rate than the corresponding Gaussian
distribution that best ﬁts the data in the least squares sense.
Indeed, what appears to be an outlier in feature space may
just be a normal sample that has been subject to some kind
of degradation for which the feature extractor was not de-
signed for. These degradation modes may include impulse
noise, partial occlusion, and in some cases, changes in ap-
pearance due to blur, shadows, or pose. In anomaly detec-
tion problems, the samples of interest are those in the tail
of such data distribution. Therefore, any method that dis-
cards outliers have the potential of discarding anomalies, so
in order to successfully ﬁnd anomalies in such images it is
necessary to use other methods.

The ﬁeld of robust statistics [7, 10] provides the tools
for estimation of unknown quantities when the underlying
probability distribution is non-Gaussian and it is not known
exactly. In practice, the data can be modeled as the mix-
ture of a Gaussian distribution and a heavy-tailed distribu-
tion (the contaminated Gaussian model). In this case, it is
desirable to design an estimator whose performance is min-
imax over a family of distributions that includes the Gaus-
sian as a special case. There are basically three types of ro-
bust estimates: M-estimates[6] (Maximum likelihood type),
L-estimates (Linear combination of order statistics), and R-
estimates (Estimates derived from rank tests).

In supervised learning problems, there is a distinction on
how to handle outliers at training time vs. testing time. Su-
pervision at training time usually mitigates the problem of
outliers as it is possible to manually select the inliers. The
use of the (cid:96)1 minimization promotes a sparse representation
of the data. The solution of the (cid:96)1 minimization is the Max-
imum Likelihood Estimate of the location parameter when
the data follows a Laplacian distribution, and a straightfor-
ward way of robustifying a regression procedure is by re-
placing the (cid:96)2 norm in the cost function by the (cid:96)1 norm. A
related L-estimator that results from such (cid:96)1 optimization is
the Least Median of Squares (LMS), which was introduced
in the computer vision ﬁeld by Kim et al. [8]. The draw-
back of the LMS is that the median estimator’s efﬁciency is
only 2
π = 0.637 when the true distribution is Gaussian. The
M-estimator based on the Huber loss function[6]




1
2

t2

ρ(t) =

for |t| < k

(1)

k2



k|t| −

for |t| ≥ k

1
2
is more ﬂexible because it has the sample mean (k = ∞)
and sample median (k = 0) as special cases and it can be
tuned to handle different degrees of contamination in the
contaminated Gaussian model. However, since this estima-
tor depends on a scale parameter k (unlike L-estimators,

which are scale-invariant), it is necessary to ﬁrst estimate
this parameter using a robust scale estimator.

function of the excesses),

2.2. Extreme Value Theory for Adaptive Anomaly

Detection

Due to illumination and viewpoint changes, clutter dis-
tribution, and other image degradation, the distribution of
features extracted from images at test time, does not match
what was observed during training. Moreover, such distri-
bution may not be stationary, but slowly changes over time,
so a ﬁxed threshold would result in large variability in the
false alarm rate. Broadwater and Chellappa[1] proposed
a technique to ﬁnd adaptive thresholds for Constant False
Alarm Rate (CFAR) detectors based on Extreme Value The-
ory (EVT) [5] that can be used even when limited training
data is available. EVT is applicable to problems where the
probability of a rare event must be estimated even if such a
rare event has never occurred. Scheirer et al. [12, 13] also
used EVT for score normalization and showed its applica-
bility to sensor fusion problems.

For completeness, we recall the EVT basic results below.
Let X1, . . . , Xn be i.i.d. samples from an unknown distri-
bution F and Mn = max(X1, . . . , Xn), the maximum of n
i.i.d. variables. The fundamental EVT theorem, the Fisher-
Tippett-Gnedenko theorem[5], states that if there exist a se-
quence of pairs of real numbers (an, bn) such that an > 0
for all n and a distribution function Λ(x) such that

lim
n→∞

P

(cid:18) Mn − bn
an

(cid:19)

≤ x

= Λ(x),

(2)

for all x at which Λ(x) is continuous, then the limit distribu-
tion Λ(x) belongs to either the Gumbel, the Fr´echet or the
Weibull family. These three families can be grouped into
the Generalized Extreme Value Distribution (GEVD)

Λ(x; µ, σ, ξ) = exp

−

1 + ξ

,

(3)

(cid:40)

(cid:20)

(cid:19)(cid:21)−1/ξ(cid:41)

(cid:18) x − µ
σ

where µ ∈ R is the location parameter, σ > 0 the scale
parameter and ξ ∈ R the shape parameter. The Gumbel
distribution is a special case of the GEVD when ξ = 0, the
Fr´echet when ξ > 0, and the Weibull when ξ < 0. When
the limiting distribution exists, we say that F (x) lies in the
“domain of attraction” of Λ(x).

In many practical applications, we are interested in the
tail distribution of the distribution F . Given an upper
threshold u, we select the Nn samples that exceed such
threshold and deﬁne the excesses Y1, . . . , YNn as Yi =
Xj − n, where i is the excess index and j is the index of the
original sample. The probability of exceeding the thresh-
old is λ = 1 − F (u). For sufﬁciently large u, the upper
tail distribution function Fu(y) (the conditional distribution

Fu(y) =

F (u + y) − F (u)
1 − F (u)

(4)

can be approximated by a Generalized Pareto Distribution

G(y; σ, ξ) = 1 −

1 +

,

y > 0.

(5)

(cid:18)

(cid:19)−1/ξ

ξy
σ

+

where σ > 0, ξ ∈ R, and x+ = max(x, 0). This ap-
proximation is justiﬁed by the Pickands theorem[11], which
states that

inf
ξ

lim
u↑ωF

inf
σ

sup
y>0

|Fu(y) − G(y; σ, ξ)| = 0

(6)

if and only if F is in the domain of attraction of the GEVD.
Note that the exponential distribution is a special case of the
GPD for ξ = 0, i.e. G(y; σ, 0) = 1 − e−y/σ.

These results can be extended to the multivariate case,
for example to model the tail distribution of the maximum
of a cluster of observations. Under stationarity of observa-
tions, this can be achieved by incorporating both the tail of
the marginal distribution and the so-called extremal index.
Let {Xn : n ≥ 1} be a (strictly) stationary sequence of r.v.’s
with marginal distribution F . Then, for sufﬁciently large n

P {Mn ≤ un} ≈ F nθ(un),

(7)

where un is any high threshold such that n(1−F (un)) con-
verges to a positive number as n → ∞ and θ is a ﬁxed
number in [0, 1]. θ is the extremal index that measures the
strength of dependence of {Xn}. If {Xn} are independent,
then θ = 1. On the other hand, if {Xn} are highly depen-
dent, then θ ≈ 0. A method for estimating the extremal
index for a real-valued Markov chain was proposed by Yun
[15].

3. Proposed Approach

In this section we describe our approach for normalizing
the scores of an anomaly detector deployed in an application
in which the distribution of the normal samples gradually
changes over time. This may be caused by changes in illu-
mination, change in view-point, addition or removal of clut-
ter, or other uncontrollable factors. The approach is similar
to the method proposed by Broadwater and Chellappa[1] in
which an adaptive threshold is estimated from the GPD ﬁt
to the upper tail of the distribution after removing the out-
liers or targets using a Kolmogorov-Smirnov statistical test.
The difference is that our method is Bayesian and we work
with sequential data and estimate the adaptive threshold for
each sample.

3.1. Bayesian Model

We want to adapt the scores of an anomaly detector ap-
plied to a sequence of images so that, when we apply a given
threshold, we get an approximate CFAR. The images have
been collected from a moving vehicle, so the environmen-
tal conditions and clutter distribution are not stationary, but
slowly change over time. In EVT-based threshold estima-
tion, it is necessary to estimate the parameters σ and ξ of
the GPD from the upper- or lower-tail of the empirical dis-
tribution. For the rest of this paper we will refer to the up-
per tail of the distribution of the random variable X, but the
same applies to the lower tail since the lower tail of X is
the upper tail of Z = −X. The threshold u needs to be
set high enough so that the tail of F (x) converges in dis-
tribution to the GPD. However, since we are dealing with a
non-stationary random process, we need to work on a small
window centered at the sample of interest. This window
needs to be long enough so that we can ﬁt the parameters of
the GPD to its tail (for example the largest 5% of the sam-
ples), but short enough that the distribution has not changed
much. In applications in which the dynamics of the pro-
cess change quickly, our options are rather limited. If we
ﬁt a GPD to the extreme samples of a short window, the
estimated threshold has so much variance that the resulting
performance is worse than using a ﬁxed threshold. On the
other hand, if the window is too long, the threshold does not
adapt at all. For example, if we use a window of 100 sam-
ples and select the upper threshold to the 95th percentile,
we would only have 5 samples to estimate the 2 parameters
of the GPD, resulting in severe overﬁtting.

To overcome this limitation, we will make one simpli-
ﬁcation by ﬁxing ξ = 0, so we only need to estimate one
parameter instead of two. Under ξ = 0, the GPD reduces to
the exponential distribution

G(y; σ, ξ = 0) = 1 − e−y/σ.

(8)

For convenience, we apply the parameterization λ = 1/σ
and write the Exponential in its canonical form

G(y; λ) = 1 − e−λy
g(y; λ) = λe−λy.

(9)

(10)

As opposed to the general case of the GPD, the Exponen-
tial distribution is a member of the exponential family, so it
has a non-trivial sufﬁcient statistic from which we can eas-
ily compute the maximum likelihood estimate (MLE) of its
parameter. Its conjugate prior is the Gamma distribution,

π(λ; α, β) =

λα−1e−βλ,

(11)

βα
Γ(α)

Algorithm 1 EVT training algorithm.
1: procedure TRAIN(T , pu, w0)
2:
3:

n ← 0, s ← 0
(cid:46) Initialize sufﬁcient statistics
for all (x, y) ∈ T do (cid:46) Training set T contains x

scores, y labels

threshold

g ← {xi | yi = 0}
u ← u | #{gi > u} = #g pu

(cid:46) Select negative samples
(cid:46) Find upper

t ← {gi | gi > u} - u
n ← n + #t
s ← s + (cid:80) t

(cid:46) Extract upper tail
(cid:46) Update counts
(cid:46) Update sum

4:
5:

6:
7:
8:

end for
α0 ← 1 + w0
β0 ← w0 s
n
return α0, β0

9:
10:
11:
12:
13: end procedure

(cid:46) Parameters of the Gamma prior

Gamma(λ; α0, β0) prior can be computed as

α1 = α0 + n
n
(cid:88)

β1 = β0 +

yi.

i=1

(12)

(13)

Moreover, the maximum a posteriori (MAP) estimate has
the closed from (cid:98)λ = β
α−1 . This simpliﬁed model allows us
to derive a very fast adaptation algorithm that we describe
in the following section. This approximation works well
in practice, especially when the scores are trained with a
sparsity promoting loss function such as the hinge loss.

3.2. Training

Our training set T contains a number of sequences of
scores x with their corresponding sequences of labels y.
During training, we compute the sufﬁcient statistics n and
s for all the samples that are not labeled as anomalies (the
sufﬁcient statistic is all we need to characterize the Gamma
prior distribution). We then re-scale them to limit the ef-
fect of this prior. Effectively, we use w0 pseudo-samples
instead n (the number of samples in the training set). This
is necessary because n is usually a very large number, and
computing α0 and β0 with it would result in a very strong
prior that would introduce too much bias in the MAP esti-
mate.

The steps of the training procedure are described in Al-
gorithm 1. The parameter pu is the probability of the tail,
and w0 is the weight in sample counts that we assign to the
training set. In our experiments we used pu = 0.05 and
w0 = 400.

3.3. Proposed Adaptive Thresholding Algorithm

the non-informative (improper) prior is given by α = 1,
β = 0, and the parameters of the Gamma posterior under a

During testing, we ﬁrst perform a series of Kolmogorov-
Smirnov (KS) tests [14] to ﬁnd and remove anomalies. The

Algorithm 2 EVT adaptive thresholding algorithm

1: procedure ADAPTSCORES(x, α0, β0, pu, pf , w1, L,

(cid:98)λ0 ← β0
α0−1
y ← sort desc(x)

(cid:46) MLE in training set
(cid:46) Sort scores in descending

na)

order

k ← #y pu
for i ← 1, na do

scores, y labels

(cid:46) Training set T contains x

u ← yi+k
t ← {yi, . . . , yi+k} − u
Dn,i = supx∈t

(cid:46) Find upper threshold
(cid:46) Extract upper tail
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:46) Compute
(cid:12) (cid:98)Gn(x) − G(x; λ)

KS statistic
end for
ˆi ← mini{Dn,i}
u(cid:48) ← yˆi
t ← {yˆi, . . . , yˆi+k} − u
α1 ← α0 + w1
β1 ← β0 + w1
for i ← 1, n do

(cid:80) t

#t

(cid:46) Estimate number of outliers
(cid:46) Set outlier rejection threshold
(cid:46) Extract upper tail

w ← xi−(L−1)/2:i+(L−1)/2

(cid:46) Window

u ← u | #{wi > u} = #w pu

(cid:46) Find upper

centered at sample xi

threshold

t ← {wi | wi > u} - u
α ← α1 + #t
β ← β1 + (cid:80) t
(cid:98)λ ← β
α−1
yi ← xi + u − (cid:98)λ log(pf /pu)

(cid:46) Extract upper tail
(cid:46) Posterior
(cid:46) Posterior
(cid:46) MAP estimate
(cid:46) Adapt score

(cid:46) Adapted scores

2:
3:

4:

5:

6:
7:

8:

9:
10:
11:
12:

13:
14:
15:
16:

17:

18:
19:
20:

21:

22:
23:
24:
25: end procedure

end for
return y

KS statistic

Dn = sup
x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)Gn(x) − G(x; (cid:98)λ)
(cid:12)

(14)

measures the dissimilarity between distributions G(x; (cid:98)λ)
and (cid:98)Gn(x). G(x; (cid:98)λ) is the GPD in (9) and

(cid:98)Gn(x) = 1 −

I(Xi ≤ x)

(15)

1
n

n
(cid:88)

i=1

where I(x) is a standard indicator function, is the empirical
tail CDF. The KS test requires estimating a threshold Kα
for rejecting (with conﬁdence 1 − α) the hypothesis that
nDn > Kα.
the observed data does not ﬁt G with the test
The result from Lilliefors[9] shows that the KS test is biased
when the reference distribution G is not precisely known (in
this case, (cid:98)λ is estimated from the training data). However,
as noted in [1], it is not necessary to identify the exact value

√

of α for the purpose of removing anomalies and outliers.
Instead, we ﬁrst compute Dn with all the samples in the tail.
We call this Dn,1. We then remove the largest sample and
we compute Dn,2 using the remaining samples. We keep
iterating until we get Dn,na . Finally, we select the value of
i that minimizes Dn,i.

After removing the anomalies, we use the prior estimated
during training to compute the posterior for the whole se-
quence. This posterior is used as the prior for estimating the
tail distribution on each shift of a window centered on each
of the samples. The details of the adaptation procedure are
described in Algorithm 2. The input to the adaptation pro-
cedure is a sequence of scores x, the parameters of the prior
Gamma distribution α0 and β0, the size of the upper tail pu,
the target false alarm rate pf , the weight w1 that we assign
to the the prior contribution of the whole sequence, the win-
dow length L, and the maximum number of anomalies na
in the sequence. The output sequence y has been adapted
so that when it is thresholded at 0, the false alarm rate is pf .
In our experiments, we have used pu = 0.05, pf = 0.001,
w1 = 100, L = 101, and na = 12.

4. Experimental Results

To validate the effectiveness of the proposed approach,
we have used the 340 sequences of fastener detections cor-
responding to each of the 4 cameras in each of the 85 miles
of the Amtrak NEC concrete tie dataset introduced in [4].
This dataset contains a total of 203,287 ties and each tie is
divided in 4 regions (left ﬁeld, left gage, right gage, and
right ﬁeld), so the total number of images is 813,148. The
detection problem consists in determining whether an im-
age contains a fastener attached to one of the rails. The
dataset contains bounding boxes for all the images that are
known to contain a defect. The total number of defects is
1,087 (0.13% of all the fasteners). The defective fastener
class contains two subclasses: broken fastener and missing
fastener.

We have used the scores generated by the multi-task
learning (MTL) detector described in [3]. This detector
uses deep learning with multiple tasks that are trained in
parallel. The reason for using multiple tasks is to prevent
overﬁtting. By sharing a common low-level representation
between the fastener inspection task and a separate mate-
rial classiﬁcation task, there is a data ampliﬁcation effect
that results in better generalization for both classiﬁers. We
also compare the performance with the baseline single-task
learning (STL) method in [4]. The raw data was provided
by Amtrak, and the authors of [3, 4] provided the output
of their detectors as well as the codes to evaluate the per-
formance. This detector produces a scalar-valued score for
each image by spatially pooling all the detections in the im-
age. Scores are high when the image contains a good fas-
tener, and low when the fastener is either missing or broken.

Figure 2 shows several detection examples of the MTL de-
tector.

To facilitate the evaluation of fastener detection perfor-
mance under difﬁcult scenarios, whenever the fastener is not
directly attached to the rail or tie, or when for some reason
a fastener is not visible at all, those ties are marked as unin-
spectable with a special label. Depending on the value of
such label, the dataset is divided into 3 subsets:

• Clear ties: 200,763 ties (1,037 ties with at least one

defect).

• Clear ties plus switches: 201,856 ties (1,045 ties with
at least one defect). See Figure 3 for an example of a
switch section.

• All ties: 203,287 ties (1,052 ties with at least one de-
fect). This includes switches, and ties for which some
fasteners are not visible because they are covered by
ballast or a lubricator. See Figures 4 and 5 for exam-
ples of high ballast and lubricator sections.

Figure 4. Example of section marked as ballast.

Figure 3. Example of section marked as switch.

For training, we use all the available data after setting
aside the sequence being tested. Table 1 and Figure 6 show
the detection results on the normalized scores. The overall
improvement is signiﬁcant. The detection rate on the whole
dataset at P F A = 0.1% increases from 95.40% to 99.26%.
This is a 6× reduction in the missed rate. Moreover, the per-
formance on the clear tie subset does not degrade at all. The
running time of our EVT adaptation algorithm implemented
in MATLAB1 for adapting all 813,148 scores is only of 17
seconds on a Mid-2012 MacBook Pro with a 2.5 GHz Intel
Core i5 processor, so this dramatic improvement comes at
negligible computational cost (running the detector process
takes several hours).

1The code and data used in this section is available at
https://github.com/xavigibert/EvtTrack

Figure 5. Example of section marked as lubricator.

5. Conclusions

In this paper, we presented a new algorithm that nor-
malizes scores from a sequential anomaly detector with
the objective of harmonizing its false alarm rate. Extreme
value theory provides a solid foundation from which adap-
tive thresholding algorithms can be derived. When working
with sequences of images, we need to take advantage of the
statistical dependencies of nuisance parameters of nearby
images. If we discard such dependencies and treat each im-
age in the sequence independently, the performance suffers.
The CFAR detection approach proposed in this paper
has applicability beyond railway track inspection from a
moving vehicle. It could be used, for example, in surveil-
lance video to remove bursts of false alarms caused by sun
glare, insects, rain or fog. Its computational cost is negligi-
ble compared to that of the underlying detector, so this ap-
proach can be easily retroﬁtted to existing detectors already

Condition

PFA MTL + EVT MTL[3]

STL[4]

Fastener (only clear ties)

Fastener (clear + switch)

Fastener (all ties)

0.1%
0.02%

0.1%
0.02%

0.1%
0.02%

99.91%
97.20%

99.54%
93.80%

99.26%
93.47%

99.91% 98.41%
96.74% 93.19%

98.43% 94.54%
89.35% 88.70%

95.40% 87.38%
87.76%

–

Table 1. Fastener detection results before and after score normalization.

[12] W. Scheirer, A. Rocha, R. Micheals, and T. Boult. Robust fu-
sion: Extreme value theory for recognition score normaliza-
tion. In European Conference on Computer Vision (ECCV),
pages 481–495. Springer, 2010. 3

[13] W. J. Scheirer, A. Rocha, R. J. Micheals, and T. E. Boult.
Meta-recognition: The theory and practice of recognition
score analysis. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 33(8):1689–1695, 2011. 3

[14] A. Stuart, J. Ord, and S. Arnold. Kendall’s Advanced Theory
of Statistics, Volume 2A: Classical Inference and the Linear
Model. Hodder Arnold, London, U.K., 1999. 4

[15] S. Yun. The extremal index of a higher-order stationary
markov chain. The Annals of Applied Probability, 8(2):408–
437, may 1998. 3

in operation.

Acknowledgements

The authors thank Amtrak, ENSCO, Inc. and the Federal
Railroad Administration for providing the data used in this
paper.

References

[1] J. Broadwater and R. Chellappa. Adaptive threshold estima-
tion via extreme value theory. IEEE Transactions on Signal
Processing, 58(2):490–500, 2010. 3, 5

[2] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: A paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
ACM, 24(6):381–395, 1981. 2

[3] X. Gibert, V. M. Patel, and R. Chellappa. Deep multi-task
learning for railway track inspection. arXiv:1509.05267,
2015. 2, 5, 7

[4] X. Gibert, V. M. Patel, and R. Chellappa. Robust fastener
detection for autonomous visual railway track inspection. In
IEEE Winter Conference on Applications of Computer Vision
(WACV), 2015. 5, 7

[5] E. Gumbel. Statistics of Extremes. Columbia University

Press, New York, 1958. 3

[6] P. J. Huber. Robust estimation of a location parameter. The
Annals of Mathematical Statistics, 35(1):73–101, 1964. 2
[7] P. J. Huber and E. M. Ronchetti. Robust Statistics. Wiley se-
ries in probability and statistics. John Wiley & Sons, Hobo-
ken, New Jersey, second edition, 2009. 2

[8] D. Y. Kim, J. J. Kim, P. Meer, D. Mintz, and A. Rosenfeld.
Robust computer vision: A least median of squares based
In in Proc. of Image Understanding Workshop,
approach.
pages 1117–1134, 1989. 2

[9] H. W. Lilliefors. On the Kolmogorov-Smirnov test for nor-
mality with mean and variance unknown. Journal of the
American Statistical Association, 62(318):399–402, 1967. 5
[10] R. A. Maronna, D. R. Martin, and V. J. Yohai. Robust Statis-
tics: Theory and Methods. Wiley series in probability and
statistics. John Wiley & Sons, Chichester, England, 2006. 2
[11] J. Pickands. Statistical inference using extreme order statis-
tics. The Annals of Statistics, 3(1):119–131, jan 1975. 3

(a)

(b)

(c)

Figure 6. ROC curves comparing defective fastener detection performance on the 85-mile testing set using normalized vs. unnormalized
scores (a) on the clear ties subset (b) on the clear with with switches subset (c) on all ties. Detections are per image (each tie has 4 images).

Sequential Score Adaptation with Extreme Value Theory
for Robust Railway Track Inspection

Xavier Gibert
University of Maryland
College Park, MD
gibert@umiacs.umd.edu

Vishal M. Patel
Rutgers University
Piscataway, NJ
vishal.m.patel@rutgers.edu

Rama Chellappa
University of Maryland
College Park, MD
rama@umiacs.umd.edu

5
1
0
2
 
t
c
O
 
0
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
2
8
5
0
.
0
1
5
1
:
v
i
X
r
a

Abstract

Periodic inspections are necessary to keep railroad
tracks in state of good repair and prevent train accidents.
Automatic track inspection using machine vision technology
has become a very effective inspection tool. Because of its
non-contact nature, this technology can be deployed on vir-
tually any railway vehicle to continuously survey the tracks
and send exception reports to track maintenance person-
nel. However, as appearance and imaging conditions vary,
false alarm rates can dramatically change, making it difﬁ-
cult to select a good operating point. In this paper, we use
extreme value theory (EVT) within a Bayesian framework
to optimally adjust the sensitivity of anomaly detectors. We
show that by approximating the lower tail of the probabil-
ity density function (PDF) of the scores with an Exponential
distribution (a special case of the Generalized Pareto distri-
bution), and using the Gamma conjugate prior learned from
the training data, it is possible to reduce the variability in
false alarm rate and improve the overall performance. This
method has shown an increase in the defect detection rate
of rail fasteners in the presence of clutter (at PFA 0.1%)
from 95.40% to 99.26% on the 85-mile Northeast Corridor
(NEC) 2012-2013 concrete tie dataset.

1. Introduction

In sequential inspection problems, such as visual rail-
way track inspection, a video feed is streamed from one or
more cameras to a detection system, and we are interested
in designing a detector that can ﬁnd abnormal patterns in
such data. There is a limit to the number of false alarms
that the operator can handle, so it is necessary to select the
optimal operating point at which the false alarm rate does
not exceed such limit. Indeed, most of the data that an au-
tonomous inspection vehicle will collect will be discarded
without anyone ever looking at it. Therefore, an excessively
high false alarm rate will result in a waste of storage space

Figure 1. Deﬁnition of basic track elements.

and bandwidth. The only relevant images are the ones that
correspond to unexpected patterns, so we are actually inter-
ested in ﬁnding such anomalous patterns.

Anomaly detection is a hypotheses testing problem in
which the null hypothesis is that an image is normal and the
alternative hypothesis is that it is anomalous. Due to the
complexity of the scene and image formation process, both
hypothesis are composite, with nuisance parameters arising
from changes in illumination, occlusion, background clut-
ter, and many other uncontrollable factors. Rather than try-
ing to model each of these variables individually, in this
paper we adapt the detection scores with the objective of
reducing the variability in type I error rate. The is known
as constant false alarm rate (CFAR) detection. We adopt
the Bayesian view that such parameters are random vari-
ables with one realization per image. The images have a
natural order based on the time they were captured at, so
the sequence of these random parameters forms a random
process. A key observation is that this random process has
strong long-term dependencies. The effect of such slowly
varying nuisance parameters is that false alarms are con-
centrated in small segments of the image sequence.

1

2.8171

2.2172

2.1372

2.2761

2.7332

-1.5259

-0.8281

-0.7909

-0.7995

-0.5839

-0.2813

-0.8813

-0.8373

-0.5157

1.4479

-2.0874

-2.1373

-2.3936

-2.8944

-2.5422

(a)

(b)

(c)

(d)

Figure 2. Examples of fastener scores (a) Good fasteners with
high scores (b) Good fasteners with low scores (c) Defective fas-
teners with high scores (d) Defective fasteners with low scores

Figure 1 shows the deﬁnitions of several track compo-
nents. In this paper, we focus on fastener inspection. Fig-
ure 2 shows examples of good and defective fasteners and
their detection scores generated by the multi-task learning
method [3] of Gibert et al. Although most fasteners have
high scores and most defective ones have low scores, when
good fasteners have low scores, there is an underlying phe-
nomenon that causes scores of nearby images to also be low.
The rest of the paper is organized as follows. In Sec-
tion 2 we review related works. The algorithm is described
in Section 3. Experimental results are described in Sec-
tion 4. Section 5 concludes the paper with a brief summary
and discussion.

2. Background

2.1. Robust Anomaly Detection

The presence of outliers is a challenge that many com-
puter vision systems have to deal with. The RANdom
SAmple Consensus (RANSAC) algorithm [2] has been used
in many applications for removing outliers when ﬁtting a
model to data. This method is specially useful when most of
the samples follow a linear model plus additive i.i.d. Gaus-
sian noise, but a few samples with gross errors do not fol-
low this model. However, in many applications, it not clear

which samples should be treated as inliers and which of
them are outliers. For instance, in big data applications,
the data just appears to have a distribution with long tails
that decay at slower rate than the corresponding Gaussian
distribution that best ﬁts the data in the least squares sense.
Indeed, what appears to be an outlier in feature space may
just be a normal sample that has been subject to some kind
of degradation for which the feature extractor was not de-
signed for. These degradation modes may include impulse
noise, partial occlusion, and in some cases, changes in ap-
pearance due to blur, shadows, or pose. In anomaly detec-
tion problems, the samples of interest are those in the tail
of such data distribution. Therefore, any method that dis-
cards outliers have the potential of discarding anomalies, so
in order to successfully ﬁnd anomalies in such images it is
necessary to use other methods.

The ﬁeld of robust statistics [7, 10] provides the tools
for estimation of unknown quantities when the underlying
probability distribution is non-Gaussian and it is not known
exactly. In practice, the data can be modeled as the mix-
ture of a Gaussian distribution and a heavy-tailed distribu-
tion (the contaminated Gaussian model). In this case, it is
desirable to design an estimator whose performance is min-
imax over a family of distributions that includes the Gaus-
sian as a special case. There are basically three types of ro-
bust estimates: M-estimates[6] (Maximum likelihood type),
L-estimates (Linear combination of order statistics), and R-
estimates (Estimates derived from rank tests).

In supervised learning problems, there is a distinction on
how to handle outliers at training time vs. testing time. Su-
pervision at training time usually mitigates the problem of
outliers as it is possible to manually select the inliers. The
use of the (cid:96)1 minimization promotes a sparse representation
of the data. The solution of the (cid:96)1 minimization is the Max-
imum Likelihood Estimate of the location parameter when
the data follows a Laplacian distribution, and a straightfor-
ward way of robustifying a regression procedure is by re-
placing the (cid:96)2 norm in the cost function by the (cid:96)1 norm. A
related L-estimator that results from such (cid:96)1 optimization is
the Least Median of Squares (LMS), which was introduced
in the computer vision ﬁeld by Kim et al. [8]. The draw-
back of the LMS is that the median estimator’s efﬁciency is
only 2
π = 0.637 when the true distribution is Gaussian. The
M-estimator based on the Huber loss function[6]




1
2

t2

ρ(t) =

for |t| < k

(1)

k2



k|t| −

for |t| ≥ k

1
2
is more ﬂexible because it has the sample mean (k = ∞)
and sample median (k = 0) as special cases and it can be
tuned to handle different degrees of contamination in the
contaminated Gaussian model. However, since this estima-
tor depends on a scale parameter k (unlike L-estimators,

which are scale-invariant), it is necessary to ﬁrst estimate
this parameter using a robust scale estimator.

function of the excesses),

2.2. Extreme Value Theory for Adaptive Anomaly

Detection

Due to illumination and viewpoint changes, clutter dis-
tribution, and other image degradation, the distribution of
features extracted from images at test time, does not match
what was observed during training. Moreover, such distri-
bution may not be stationary, but slowly changes over time,
so a ﬁxed threshold would result in large variability in the
false alarm rate. Broadwater and Chellappa[1] proposed
a technique to ﬁnd adaptive thresholds for Constant False
Alarm Rate (CFAR) detectors based on Extreme Value The-
ory (EVT) [5] that can be used even when limited training
data is available. EVT is applicable to problems where the
probability of a rare event must be estimated even if such a
rare event has never occurred. Scheirer et al. [12, 13] also
used EVT for score normalization and showed its applica-
bility to sensor fusion problems.

For completeness, we recall the EVT basic results below.
Let X1, . . . , Xn be i.i.d. samples from an unknown distri-
bution F and Mn = max(X1, . . . , Xn), the maximum of n
i.i.d. variables. The fundamental EVT theorem, the Fisher-
Tippett-Gnedenko theorem[5], states that if there exist a se-
quence of pairs of real numbers (an, bn) such that an > 0
for all n and a distribution function Λ(x) such that

lim
n→∞

P

(cid:18) Mn − bn
an

(cid:19)

≤ x

= Λ(x),

(2)

for all x at which Λ(x) is continuous, then the limit distribu-
tion Λ(x) belongs to either the Gumbel, the Fr´echet or the
Weibull family. These three families can be grouped into
the Generalized Extreme Value Distribution (GEVD)

Λ(x; µ, σ, ξ) = exp

−

1 + ξ

,

(3)

(cid:40)

(cid:20)

(cid:19)(cid:21)−1/ξ(cid:41)

(cid:18) x − µ
σ

where µ ∈ R is the location parameter, σ > 0 the scale
parameter and ξ ∈ R the shape parameter. The Gumbel
distribution is a special case of the GEVD when ξ = 0, the
Fr´echet when ξ > 0, and the Weibull when ξ < 0. When
the limiting distribution exists, we say that F (x) lies in the
“domain of attraction” of Λ(x).

In many practical applications, we are interested in the
tail distribution of the distribution F . Given an upper
threshold u, we select the Nn samples that exceed such
threshold and deﬁne the excesses Y1, . . . , YNn as Yi =
Xj − n, where i is the excess index and j is the index of the
original sample. The probability of exceeding the thresh-
old is λ = 1 − F (u). For sufﬁciently large u, the upper
tail distribution function Fu(y) (the conditional distribution

Fu(y) =

F (u + y) − F (u)
1 − F (u)

(4)

can be approximated by a Generalized Pareto Distribution

G(y; σ, ξ) = 1 −

1 +

,

y > 0.

(5)

(cid:18)

(cid:19)−1/ξ

ξy
σ

+

where σ > 0, ξ ∈ R, and x+ = max(x, 0). This ap-
proximation is justiﬁed by the Pickands theorem[11], which
states that

inf
ξ

lim
u↑ωF

inf
σ

sup
y>0

|Fu(y) − G(y; σ, ξ)| = 0

(6)

if and only if F is in the domain of attraction of the GEVD.
Note that the exponential distribution is a special case of the
GPD for ξ = 0, i.e. G(y; σ, 0) = 1 − e−y/σ.

These results can be extended to the multivariate case,
for example to model the tail distribution of the maximum
of a cluster of observations. Under stationarity of observa-
tions, this can be achieved by incorporating both the tail of
the marginal distribution and the so-called extremal index.
Let {Xn : n ≥ 1} be a (strictly) stationary sequence of r.v.’s
with marginal distribution F . Then, for sufﬁciently large n

P {Mn ≤ un} ≈ F nθ(un),

(7)

where un is any high threshold such that n(1−F (un)) con-
verges to a positive number as n → ∞ and θ is a ﬁxed
number in [0, 1]. θ is the extremal index that measures the
strength of dependence of {Xn}. If {Xn} are independent,
then θ = 1. On the other hand, if {Xn} are highly depen-
dent, then θ ≈ 0. A method for estimating the extremal
index for a real-valued Markov chain was proposed by Yun
[15].

3. Proposed Approach

In this section we describe our approach for normalizing
the scores of an anomaly detector deployed in an application
in which the distribution of the normal samples gradually
changes over time. This may be caused by changes in illu-
mination, change in view-point, addition or removal of clut-
ter, or other uncontrollable factors. The approach is similar
to the method proposed by Broadwater and Chellappa[1] in
which an adaptive threshold is estimated from the GPD ﬁt
to the upper tail of the distribution after removing the out-
liers or targets using a Kolmogorov-Smirnov statistical test.
The difference is that our method is Bayesian and we work
with sequential data and estimate the adaptive threshold for
each sample.

3.1. Bayesian Model

We want to adapt the scores of an anomaly detector ap-
plied to a sequence of images so that, when we apply a given
threshold, we get an approximate CFAR. The images have
been collected from a moving vehicle, so the environmen-
tal conditions and clutter distribution are not stationary, but
slowly change over time. In EVT-based threshold estima-
tion, it is necessary to estimate the parameters σ and ξ of
the GPD from the upper- or lower-tail of the empirical dis-
tribution. For the rest of this paper we will refer to the up-
per tail of the distribution of the random variable X, but the
same applies to the lower tail since the lower tail of X is
the upper tail of Z = −X. The threshold u needs to be
set high enough so that the tail of F (x) converges in dis-
tribution to the GPD. However, since we are dealing with a
non-stationary random process, we need to work on a small
window centered at the sample of interest. This window
needs to be long enough so that we can ﬁt the parameters of
the GPD to its tail (for example the largest 5% of the sam-
ples), but short enough that the distribution has not changed
much. In applications in which the dynamics of the pro-
cess change quickly, our options are rather limited. If we
ﬁt a GPD to the extreme samples of a short window, the
estimated threshold has so much variance that the resulting
performance is worse than using a ﬁxed threshold. On the
other hand, if the window is too long, the threshold does not
adapt at all. For example, if we use a window of 100 sam-
ples and select the upper threshold to the 95th percentile,
we would only have 5 samples to estimate the 2 parameters
of the GPD, resulting in severe overﬁtting.

To overcome this limitation, we will make one simpli-
ﬁcation by ﬁxing ξ = 0, so we only need to estimate one
parameter instead of two. Under ξ = 0, the GPD reduces to
the exponential distribution

G(y; σ, ξ = 0) = 1 − e−y/σ.

(8)

For convenience, we apply the parameterization λ = 1/σ
and write the Exponential in its canonical form

G(y; λ) = 1 − e−λy
g(y; λ) = λe−λy.

(9)

(10)

As opposed to the general case of the GPD, the Exponen-
tial distribution is a member of the exponential family, so it
has a non-trivial sufﬁcient statistic from which we can eas-
ily compute the maximum likelihood estimate (MLE) of its
parameter. Its conjugate prior is the Gamma distribution,

π(λ; α, β) =

λα−1e−βλ,

(11)

βα
Γ(α)

Algorithm 1 EVT training algorithm.
1: procedure TRAIN(T , pu, w0)
2:
3:

n ← 0, s ← 0
(cid:46) Initialize sufﬁcient statistics
for all (x, y) ∈ T do (cid:46) Training set T contains x

scores, y labels

threshold

g ← {xi | yi = 0}
u ← u | #{gi > u} = #g pu

(cid:46) Select negative samples
(cid:46) Find upper

t ← {gi | gi > u} - u
n ← n + #t
s ← s + (cid:80) t

(cid:46) Extract upper tail
(cid:46) Update counts
(cid:46) Update sum

4:
5:

6:
7:
8:

end for
α0 ← 1 + w0
β0 ← w0 s
n
return α0, β0

9:
10:
11:
12:
13: end procedure

(cid:46) Parameters of the Gamma prior

Gamma(λ; α0, β0) prior can be computed as

α1 = α0 + n
n
(cid:88)

β1 = β0 +

yi.

i=1

(12)

(13)

Moreover, the maximum a posteriori (MAP) estimate has
the closed from (cid:98)λ = β
α−1 . This simpliﬁed model allows us
to derive a very fast adaptation algorithm that we describe
in the following section. This approximation works well
in practice, especially when the scores are trained with a
sparsity promoting loss function such as the hinge loss.

3.2. Training

Our training set T contains a number of sequences of
scores x with their corresponding sequences of labels y.
During training, we compute the sufﬁcient statistics n and
s for all the samples that are not labeled as anomalies (the
sufﬁcient statistic is all we need to characterize the Gamma
prior distribution). We then re-scale them to limit the ef-
fect of this prior. Effectively, we use w0 pseudo-samples
instead n (the number of samples in the training set). This
is necessary because n is usually a very large number, and
computing α0 and β0 with it would result in a very strong
prior that would introduce too much bias in the MAP esti-
mate.

The steps of the training procedure are described in Al-
gorithm 1. The parameter pu is the probability of the tail,
and w0 is the weight in sample counts that we assign to the
training set. In our experiments we used pu = 0.05 and
w0 = 400.

3.3. Proposed Adaptive Thresholding Algorithm

the non-informative (improper) prior is given by α = 1,
β = 0, and the parameters of the Gamma posterior under a

During testing, we ﬁrst perform a series of Kolmogorov-
Smirnov (KS) tests [14] to ﬁnd and remove anomalies. The

Algorithm 2 EVT adaptive thresholding algorithm

1: procedure ADAPTSCORES(x, α0, β0, pu, pf , w1, L,

(cid:98)λ0 ← β0
α0−1
y ← sort desc(x)

(cid:46) MLE in training set
(cid:46) Sort scores in descending

na)

order

k ← #y pu
for i ← 1, na do

scores, y labels

(cid:46) Training set T contains x

u ← yi+k
t ← {yi, . . . , yi+k} − u
Dn,i = supx∈t

(cid:46) Find upper threshold
(cid:46) Extract upper tail
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:46) Compute
(cid:12) (cid:98)Gn(x) − G(x; λ)

KS statistic
end for
ˆi ← mini{Dn,i}
u(cid:48) ← yˆi
t ← {yˆi, . . . , yˆi+k} − u
α1 ← α0 + w1
β1 ← β0 + w1
for i ← 1, n do

(cid:80) t

#t

(cid:46) Estimate number of outliers
(cid:46) Set outlier rejection threshold
(cid:46) Extract upper tail

w ← xi−(L−1)/2:i+(L−1)/2

(cid:46) Window

u ← u | #{wi > u} = #w pu

(cid:46) Find upper

centered at sample xi

threshold

t ← {wi | wi > u} - u
α ← α1 + #t
β ← β1 + (cid:80) t
(cid:98)λ ← β
α−1
yi ← xi + u − (cid:98)λ log(pf /pu)

(cid:46) Extract upper tail
(cid:46) Posterior
(cid:46) Posterior
(cid:46) MAP estimate
(cid:46) Adapt score

(cid:46) Adapted scores

2:
3:

4:

5:

6:
7:

8:

9:
10:
11:
12:

13:
14:
15:
16:

17:

18:
19:
20:

21:

22:
23:
24:
25: end procedure

end for
return y

KS statistic

Dn = sup
x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)Gn(x) − G(x; (cid:98)λ)
(cid:12)

(14)

measures the dissimilarity between distributions G(x; (cid:98)λ)
and (cid:98)Gn(x). G(x; (cid:98)λ) is the GPD in (9) and

(cid:98)Gn(x) = 1 −

I(Xi ≤ x)

(15)

1
n

n
(cid:88)

i=1

where I(x) is a standard indicator function, is the empirical
tail CDF. The KS test requires estimating a threshold Kα
for rejecting (with conﬁdence 1 − α) the hypothesis that
nDn > Kα.
the observed data does not ﬁt G with the test
The result from Lilliefors[9] shows that the KS test is biased
when the reference distribution G is not precisely known (in
this case, (cid:98)λ is estimated from the training data). However,
as noted in [1], it is not necessary to identify the exact value

√

of α for the purpose of removing anomalies and outliers.
Instead, we ﬁrst compute Dn with all the samples in the tail.
We call this Dn,1. We then remove the largest sample and
we compute Dn,2 using the remaining samples. We keep
iterating until we get Dn,na . Finally, we select the value of
i that minimizes Dn,i.

After removing the anomalies, we use the prior estimated
during training to compute the posterior for the whole se-
quence. This posterior is used as the prior for estimating the
tail distribution on each shift of a window centered on each
of the samples. The details of the adaptation procedure are
described in Algorithm 2. The input to the adaptation pro-
cedure is a sequence of scores x, the parameters of the prior
Gamma distribution α0 and β0, the size of the upper tail pu,
the target false alarm rate pf , the weight w1 that we assign
to the the prior contribution of the whole sequence, the win-
dow length L, and the maximum number of anomalies na
in the sequence. The output sequence y has been adapted
so that when it is thresholded at 0, the false alarm rate is pf .
In our experiments, we have used pu = 0.05, pf = 0.001,
w1 = 100, L = 101, and na = 12.

4. Experimental Results

To validate the effectiveness of the proposed approach,
we have used the 340 sequences of fastener detections cor-
responding to each of the 4 cameras in each of the 85 miles
of the Amtrak NEC concrete tie dataset introduced in [4].
This dataset contains a total of 203,287 ties and each tie is
divided in 4 regions (left ﬁeld, left gage, right gage, and
right ﬁeld), so the total number of images is 813,148. The
detection problem consists in determining whether an im-
age contains a fastener attached to one of the rails. The
dataset contains bounding boxes for all the images that are
known to contain a defect. The total number of defects is
1,087 (0.13% of all the fasteners). The defective fastener
class contains two subclasses: broken fastener and missing
fastener.

We have used the scores generated by the multi-task
learning (MTL) detector described in [3]. This detector
uses deep learning with multiple tasks that are trained in
parallel. The reason for using multiple tasks is to prevent
overﬁtting. By sharing a common low-level representation
between the fastener inspection task and a separate mate-
rial classiﬁcation task, there is a data ampliﬁcation effect
that results in better generalization for both classiﬁers. We
also compare the performance with the baseline single-task
learning (STL) method in [4]. The raw data was provided
by Amtrak, and the authors of [3, 4] provided the output
of their detectors as well as the codes to evaluate the per-
formance. This detector produces a scalar-valued score for
each image by spatially pooling all the detections in the im-
age. Scores are high when the image contains a good fas-
tener, and low when the fastener is either missing or broken.

Figure 2 shows several detection examples of the MTL de-
tector.

To facilitate the evaluation of fastener detection perfor-
mance under difﬁcult scenarios, whenever the fastener is not
directly attached to the rail or tie, or when for some reason
a fastener is not visible at all, those ties are marked as unin-
spectable with a special label. Depending on the value of
such label, the dataset is divided into 3 subsets:

• Clear ties: 200,763 ties (1,037 ties with at least one

defect).

• Clear ties plus switches: 201,856 ties (1,045 ties with
at least one defect). See Figure 3 for an example of a
switch section.

• All ties: 203,287 ties (1,052 ties with at least one de-
fect). This includes switches, and ties for which some
fasteners are not visible because they are covered by
ballast or a lubricator. See Figures 4 and 5 for exam-
ples of high ballast and lubricator sections.

Figure 4. Example of section marked as ballast.

Figure 3. Example of section marked as switch.

For training, we use all the available data after setting
aside the sequence being tested. Table 1 and Figure 6 show
the detection results on the normalized scores. The overall
improvement is signiﬁcant. The detection rate on the whole
dataset at P F A = 0.1% increases from 95.40% to 99.26%.
This is a 6× reduction in the missed rate. Moreover, the per-
formance on the clear tie subset does not degrade at all. The
running time of our EVT adaptation algorithm implemented
in MATLAB1 for adapting all 813,148 scores is only of 17
seconds on a Mid-2012 MacBook Pro with a 2.5 GHz Intel
Core i5 processor, so this dramatic improvement comes at
negligible computational cost (running the detector process
takes several hours).

1The code and data used in this section is available at
https://github.com/xavigibert/EvtTrack

Figure 5. Example of section marked as lubricator.

5. Conclusions

In this paper, we presented a new algorithm that nor-
malizes scores from a sequential anomaly detector with
the objective of harmonizing its false alarm rate. Extreme
value theory provides a solid foundation from which adap-
tive thresholding algorithms can be derived. When working
with sequences of images, we need to take advantage of the
statistical dependencies of nuisance parameters of nearby
images. If we discard such dependencies and treat each im-
age in the sequence independently, the performance suffers.
The CFAR detection approach proposed in this paper
has applicability beyond railway track inspection from a
moving vehicle. It could be used, for example, in surveil-
lance video to remove bursts of false alarms caused by sun
glare, insects, rain or fog. Its computational cost is negligi-
ble compared to that of the underlying detector, so this ap-
proach can be easily retroﬁtted to existing detectors already

Condition

PFA MTL + EVT MTL[3]

STL[4]

Fastener (only clear ties)

Fastener (clear + switch)

Fastener (all ties)

0.1%
0.02%

0.1%
0.02%

0.1%
0.02%

99.91%
97.20%

99.54%
93.80%

99.26%
93.47%

99.91% 98.41%
96.74% 93.19%

98.43% 94.54%
89.35% 88.70%

95.40% 87.38%
87.76%

–

Table 1. Fastener detection results before and after score normalization.

[12] W. Scheirer, A. Rocha, R. Micheals, and T. Boult. Robust fu-
sion: Extreme value theory for recognition score normaliza-
tion. In European Conference on Computer Vision (ECCV),
pages 481–495. Springer, 2010. 3

[13] W. J. Scheirer, A. Rocha, R. J. Micheals, and T. E. Boult.
Meta-recognition: The theory and practice of recognition
score analysis. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 33(8):1689–1695, 2011. 3

[14] A. Stuart, J. Ord, and S. Arnold. Kendall’s Advanced Theory
of Statistics, Volume 2A: Classical Inference and the Linear
Model. Hodder Arnold, London, U.K., 1999. 4

[15] S. Yun. The extremal index of a higher-order stationary
markov chain. The Annals of Applied Probability, 8(2):408–
437, may 1998. 3

in operation.

Acknowledgements

The authors thank Amtrak, ENSCO, Inc. and the Federal
Railroad Administration for providing the data used in this
paper.

References

[1] J. Broadwater and R. Chellappa. Adaptive threshold estima-
tion via extreme value theory. IEEE Transactions on Signal
Processing, 58(2):490–500, 2010. 3, 5

[2] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: A paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
ACM, 24(6):381–395, 1981. 2

[3] X. Gibert, V. M. Patel, and R. Chellappa. Deep multi-task
learning for railway track inspection. arXiv:1509.05267,
2015. 2, 5, 7

[4] X. Gibert, V. M. Patel, and R. Chellappa. Robust fastener
detection for autonomous visual railway track inspection. In
IEEE Winter Conference on Applications of Computer Vision
(WACV), 2015. 5, 7

[5] E. Gumbel. Statistics of Extremes. Columbia University

Press, New York, 1958. 3

[6] P. J. Huber. Robust estimation of a location parameter. The
Annals of Mathematical Statistics, 35(1):73–101, 1964. 2
[7] P. J. Huber and E. M. Ronchetti. Robust Statistics. Wiley se-
ries in probability and statistics. John Wiley & Sons, Hobo-
ken, New Jersey, second edition, 2009. 2

[8] D. Y. Kim, J. J. Kim, P. Meer, D. Mintz, and A. Rosenfeld.
Robust computer vision: A least median of squares based
In in Proc. of Image Understanding Workshop,
approach.
pages 1117–1134, 1989. 2

[9] H. W. Lilliefors. On the Kolmogorov-Smirnov test for nor-
mality with mean and variance unknown. Journal of the
American Statistical Association, 62(318):399–402, 1967. 5
[10] R. A. Maronna, D. R. Martin, and V. J. Yohai. Robust Statis-
tics: Theory and Methods. Wiley series in probability and
statistics. John Wiley & Sons, Chichester, England, 2006. 2
[11] J. Pickands. Statistical inference using extreme order statis-
tics. The Annals of Statistics, 3(1):119–131, jan 1975. 3

(a)

(b)

(c)

Figure 6. ROC curves comparing defective fastener detection performance on the 85-mile testing set using normalized vs. unnormalized
scores (a) on the clear ties subset (b) on the clear with with switches subset (c) on all ties. Detections are per image (each tie has 4 images).

Sequential Score Adaptation with Extreme Value Theory
for Robust Railway Track Inspection

Xavier Gibert
University of Maryland
College Park, MD
gibert@umiacs.umd.edu

Vishal M. Patel
Rutgers University
Piscataway, NJ
vishal.m.patel@rutgers.edu

Rama Chellappa
University of Maryland
College Park, MD
rama@umiacs.umd.edu

5
1
0
2
 
t
c
O
 
0
2
 
 
]

V
C
.
s
c
[
 
 
1
v
2
2
8
5
0
.
0
1
5
1
:
v
i
X
r
a

Abstract

Periodic inspections are necessary to keep railroad
tracks in state of good repair and prevent train accidents.
Automatic track inspection using machine vision technology
has become a very effective inspection tool. Because of its
non-contact nature, this technology can be deployed on vir-
tually any railway vehicle to continuously survey the tracks
and send exception reports to track maintenance person-
nel. However, as appearance and imaging conditions vary,
false alarm rates can dramatically change, making it difﬁ-
cult to select a good operating point. In this paper, we use
extreme value theory (EVT) within a Bayesian framework
to optimally adjust the sensitivity of anomaly detectors. We
show that by approximating the lower tail of the probabil-
ity density function (PDF) of the scores with an Exponential
distribution (a special case of the Generalized Pareto distri-
bution), and using the Gamma conjugate prior learned from
the training data, it is possible to reduce the variability in
false alarm rate and improve the overall performance. This
method has shown an increase in the defect detection rate
of rail fasteners in the presence of clutter (at PFA 0.1%)
from 95.40% to 99.26% on the 85-mile Northeast Corridor
(NEC) 2012-2013 concrete tie dataset.

1. Introduction

In sequential inspection problems, such as visual rail-
way track inspection, a video feed is streamed from one or
more cameras to a detection system, and we are interested
in designing a detector that can ﬁnd abnormal patterns in
such data. There is a limit to the number of false alarms
that the operator can handle, so it is necessary to select the
optimal operating point at which the false alarm rate does
not exceed such limit. Indeed, most of the data that an au-
tonomous inspection vehicle will collect will be discarded
without anyone ever looking at it. Therefore, an excessively
high false alarm rate will result in a waste of storage space

Figure 1. Deﬁnition of basic track elements.

and bandwidth. The only relevant images are the ones that
correspond to unexpected patterns, so we are actually inter-
ested in ﬁnding such anomalous patterns.

Anomaly detection is a hypotheses testing problem in
which the null hypothesis is that an image is normal and the
alternative hypothesis is that it is anomalous. Due to the
complexity of the scene and image formation process, both
hypothesis are composite, with nuisance parameters arising
from changes in illumination, occlusion, background clut-
ter, and many other uncontrollable factors. Rather than try-
ing to model each of these variables individually, in this
paper we adapt the detection scores with the objective of
reducing the variability in type I error rate. The is known
as constant false alarm rate (CFAR) detection. We adopt
the Bayesian view that such parameters are random vari-
ables with one realization per image. The images have a
natural order based on the time they were captured at, so
the sequence of these random parameters forms a random
process. A key observation is that this random process has
strong long-term dependencies. The effect of such slowly
varying nuisance parameters is that false alarms are con-
centrated in small segments of the image sequence.

1

2.8171

2.2172

2.1372

2.2761

2.7332

-1.5259

-0.8281

-0.7909

-0.7995

-0.5839

-0.2813

-0.8813

-0.8373

-0.5157

1.4479

-2.0874

-2.1373

-2.3936

-2.8944

-2.5422

(a)

(b)

(c)

(d)

Figure 2. Examples of fastener scores (a) Good fasteners with
high scores (b) Good fasteners with low scores (c) Defective fas-
teners with high scores (d) Defective fasteners with low scores

Figure 1 shows the deﬁnitions of several track compo-
nents. In this paper, we focus on fastener inspection. Fig-
ure 2 shows examples of good and defective fasteners and
their detection scores generated by the multi-task learning
method [3] of Gibert et al. Although most fasteners have
high scores and most defective ones have low scores, when
good fasteners have low scores, there is an underlying phe-
nomenon that causes scores of nearby images to also be low.
The rest of the paper is organized as follows. In Sec-
tion 2 we review related works. The algorithm is described
in Section 3. Experimental results are described in Sec-
tion 4. Section 5 concludes the paper with a brief summary
and discussion.

2. Background

2.1. Robust Anomaly Detection

The presence of outliers is a challenge that many com-
puter vision systems have to deal with. The RANdom
SAmple Consensus (RANSAC) algorithm [2] has been used
in many applications for removing outliers when ﬁtting a
model to data. This method is specially useful when most of
the samples follow a linear model plus additive i.i.d. Gaus-
sian noise, but a few samples with gross errors do not fol-
low this model. However, in many applications, it not clear

which samples should be treated as inliers and which of
them are outliers. For instance, in big data applications,
the data just appears to have a distribution with long tails
that decay at slower rate than the corresponding Gaussian
distribution that best ﬁts the data in the least squares sense.
Indeed, what appears to be an outlier in feature space may
just be a normal sample that has been subject to some kind
of degradation for which the feature extractor was not de-
signed for. These degradation modes may include impulse
noise, partial occlusion, and in some cases, changes in ap-
pearance due to blur, shadows, or pose. In anomaly detec-
tion problems, the samples of interest are those in the tail
of such data distribution. Therefore, any method that dis-
cards outliers have the potential of discarding anomalies, so
in order to successfully ﬁnd anomalies in such images it is
necessary to use other methods.

The ﬁeld of robust statistics [7, 10] provides the tools
for estimation of unknown quantities when the underlying
probability distribution is non-Gaussian and it is not known
exactly. In practice, the data can be modeled as the mix-
ture of a Gaussian distribution and a heavy-tailed distribu-
tion (the contaminated Gaussian model). In this case, it is
desirable to design an estimator whose performance is min-
imax over a family of distributions that includes the Gaus-
sian as a special case. There are basically three types of ro-
bust estimates: M-estimates[6] (Maximum likelihood type),
L-estimates (Linear combination of order statistics), and R-
estimates (Estimates derived from rank tests).

In supervised learning problems, there is a distinction on
how to handle outliers at training time vs. testing time. Su-
pervision at training time usually mitigates the problem of
outliers as it is possible to manually select the inliers. The
use of the (cid:96)1 minimization promotes a sparse representation
of the data. The solution of the (cid:96)1 minimization is the Max-
imum Likelihood Estimate of the location parameter when
the data follows a Laplacian distribution, and a straightfor-
ward way of robustifying a regression procedure is by re-
placing the (cid:96)2 norm in the cost function by the (cid:96)1 norm. A
related L-estimator that results from such (cid:96)1 optimization is
the Least Median of Squares (LMS), which was introduced
in the computer vision ﬁeld by Kim et al. [8]. The draw-
back of the LMS is that the median estimator’s efﬁciency is
only 2
π = 0.637 when the true distribution is Gaussian. The
M-estimator based on the Huber loss function[6]




1
2

t2

ρ(t) =

for |t| < k

(1)

k2



k|t| −

for |t| ≥ k

1
2
is more ﬂexible because it has the sample mean (k = ∞)
and sample median (k = 0) as special cases and it can be
tuned to handle different degrees of contamination in the
contaminated Gaussian model. However, since this estima-
tor depends on a scale parameter k (unlike L-estimators,

which are scale-invariant), it is necessary to ﬁrst estimate
this parameter using a robust scale estimator.

function of the excesses),

2.2. Extreme Value Theory for Adaptive Anomaly

Detection

Due to illumination and viewpoint changes, clutter dis-
tribution, and other image degradation, the distribution of
features extracted from images at test time, does not match
what was observed during training. Moreover, such distri-
bution may not be stationary, but slowly changes over time,
so a ﬁxed threshold would result in large variability in the
false alarm rate. Broadwater and Chellappa[1] proposed
a technique to ﬁnd adaptive thresholds for Constant False
Alarm Rate (CFAR) detectors based on Extreme Value The-
ory (EVT) [5] that can be used even when limited training
data is available. EVT is applicable to problems where the
probability of a rare event must be estimated even if such a
rare event has never occurred. Scheirer et al. [12, 13] also
used EVT for score normalization and showed its applica-
bility to sensor fusion problems.

For completeness, we recall the EVT basic results below.
Let X1, . . . , Xn be i.i.d. samples from an unknown distri-
bution F and Mn = max(X1, . . . , Xn), the maximum of n
i.i.d. variables. The fundamental EVT theorem, the Fisher-
Tippett-Gnedenko theorem[5], states that if there exist a se-
quence of pairs of real numbers (an, bn) such that an > 0
for all n and a distribution function Λ(x) such that

lim
n→∞

P

(cid:18) Mn − bn
an

(cid:19)

≤ x

= Λ(x),

(2)

for all x at which Λ(x) is continuous, then the limit distribu-
tion Λ(x) belongs to either the Gumbel, the Fr´echet or the
Weibull family. These three families can be grouped into
the Generalized Extreme Value Distribution (GEVD)

Λ(x; µ, σ, ξ) = exp

−

1 + ξ

,

(3)

(cid:40)

(cid:20)

(cid:19)(cid:21)−1/ξ(cid:41)

(cid:18) x − µ
σ

where µ ∈ R is the location parameter, σ > 0 the scale
parameter and ξ ∈ R the shape parameter. The Gumbel
distribution is a special case of the GEVD when ξ = 0, the
Fr´echet when ξ > 0, and the Weibull when ξ < 0. When
the limiting distribution exists, we say that F (x) lies in the
“domain of attraction” of Λ(x).

In many practical applications, we are interested in the
tail distribution of the distribution F . Given an upper
threshold u, we select the Nn samples that exceed such
threshold and deﬁne the excesses Y1, . . . , YNn as Yi =
Xj − n, where i is the excess index and j is the index of the
original sample. The probability of exceeding the thresh-
old is λ = 1 − F (u). For sufﬁciently large u, the upper
tail distribution function Fu(y) (the conditional distribution

Fu(y) =

F (u + y) − F (u)
1 − F (u)

(4)

can be approximated by a Generalized Pareto Distribution

G(y; σ, ξ) = 1 −

1 +

,

y > 0.

(5)

(cid:18)

(cid:19)−1/ξ

ξy
σ

+

where σ > 0, ξ ∈ R, and x+ = max(x, 0). This ap-
proximation is justiﬁed by the Pickands theorem[11], which
states that

inf
ξ

lim
u↑ωF

inf
σ

sup
y>0

|Fu(y) − G(y; σ, ξ)| = 0

(6)

if and only if F is in the domain of attraction of the GEVD.
Note that the exponential distribution is a special case of the
GPD for ξ = 0, i.e. G(y; σ, 0) = 1 − e−y/σ.

These results can be extended to the multivariate case,
for example to model the tail distribution of the maximum
of a cluster of observations. Under stationarity of observa-
tions, this can be achieved by incorporating both the tail of
the marginal distribution and the so-called extremal index.
Let {Xn : n ≥ 1} be a (strictly) stationary sequence of r.v.’s
with marginal distribution F . Then, for sufﬁciently large n

P {Mn ≤ un} ≈ F nθ(un),

(7)

where un is any high threshold such that n(1−F (un)) con-
verges to a positive number as n → ∞ and θ is a ﬁxed
number in [0, 1]. θ is the extremal index that measures the
strength of dependence of {Xn}. If {Xn} are independent,
then θ = 1. On the other hand, if {Xn} are highly depen-
dent, then θ ≈ 0. A method for estimating the extremal
index for a real-valued Markov chain was proposed by Yun
[15].

3. Proposed Approach

In this section we describe our approach for normalizing
the scores of an anomaly detector deployed in an application
in which the distribution of the normal samples gradually
changes over time. This may be caused by changes in illu-
mination, change in view-point, addition or removal of clut-
ter, or other uncontrollable factors. The approach is similar
to the method proposed by Broadwater and Chellappa[1] in
which an adaptive threshold is estimated from the GPD ﬁt
to the upper tail of the distribution after removing the out-
liers or targets using a Kolmogorov-Smirnov statistical test.
The difference is that our method is Bayesian and we work
with sequential data and estimate the adaptive threshold for
each sample.

3.1. Bayesian Model

We want to adapt the scores of an anomaly detector ap-
plied to a sequence of images so that, when we apply a given
threshold, we get an approximate CFAR. The images have
been collected from a moving vehicle, so the environmen-
tal conditions and clutter distribution are not stationary, but
slowly change over time. In EVT-based threshold estima-
tion, it is necessary to estimate the parameters σ and ξ of
the GPD from the upper- or lower-tail of the empirical dis-
tribution. For the rest of this paper we will refer to the up-
per tail of the distribution of the random variable X, but the
same applies to the lower tail since the lower tail of X is
the upper tail of Z = −X. The threshold u needs to be
set high enough so that the tail of F (x) converges in dis-
tribution to the GPD. However, since we are dealing with a
non-stationary random process, we need to work on a small
window centered at the sample of interest. This window
needs to be long enough so that we can ﬁt the parameters of
the GPD to its tail (for example the largest 5% of the sam-
ples), but short enough that the distribution has not changed
much. In applications in which the dynamics of the pro-
cess change quickly, our options are rather limited. If we
ﬁt a GPD to the extreme samples of a short window, the
estimated threshold has so much variance that the resulting
performance is worse than using a ﬁxed threshold. On the
other hand, if the window is too long, the threshold does not
adapt at all. For example, if we use a window of 100 sam-
ples and select the upper threshold to the 95th percentile,
we would only have 5 samples to estimate the 2 parameters
of the GPD, resulting in severe overﬁtting.

To overcome this limitation, we will make one simpli-
ﬁcation by ﬁxing ξ = 0, so we only need to estimate one
parameter instead of two. Under ξ = 0, the GPD reduces to
the exponential distribution

G(y; σ, ξ = 0) = 1 − e−y/σ.

(8)

For convenience, we apply the parameterization λ = 1/σ
and write the Exponential in its canonical form

G(y; λ) = 1 − e−λy
g(y; λ) = λe−λy.

(9)

(10)

As opposed to the general case of the GPD, the Exponen-
tial distribution is a member of the exponential family, so it
has a non-trivial sufﬁcient statistic from which we can eas-
ily compute the maximum likelihood estimate (MLE) of its
parameter. Its conjugate prior is the Gamma distribution,

π(λ; α, β) =

λα−1e−βλ,

(11)

βα
Γ(α)

Algorithm 1 EVT training algorithm.
1: procedure TRAIN(T , pu, w0)
2:
3:

n ← 0, s ← 0
(cid:46) Initialize sufﬁcient statistics
for all (x, y) ∈ T do (cid:46) Training set T contains x

scores, y labels

threshold

g ← {xi | yi = 0}
u ← u | #{gi > u} = #g pu

(cid:46) Select negative samples
(cid:46) Find upper

t ← {gi | gi > u} - u
n ← n + #t
s ← s + (cid:80) t

(cid:46) Extract upper tail
(cid:46) Update counts
(cid:46) Update sum

4:
5:

6:
7:
8:

end for
α0 ← 1 + w0
β0 ← w0 s
n
return α0, β0

9:
10:
11:
12:
13: end procedure

(cid:46) Parameters of the Gamma prior

Gamma(λ; α0, β0) prior can be computed as

α1 = α0 + n
n
(cid:88)

β1 = β0 +

yi.

i=1

(12)

(13)

Moreover, the maximum a posteriori (MAP) estimate has
the closed from (cid:98)λ = β
α−1 . This simpliﬁed model allows us
to derive a very fast adaptation algorithm that we describe
in the following section. This approximation works well
in practice, especially when the scores are trained with a
sparsity promoting loss function such as the hinge loss.

3.2. Training

Our training set T contains a number of sequences of
scores x with their corresponding sequences of labels y.
During training, we compute the sufﬁcient statistics n and
s for all the samples that are not labeled as anomalies (the
sufﬁcient statistic is all we need to characterize the Gamma
prior distribution). We then re-scale them to limit the ef-
fect of this prior. Effectively, we use w0 pseudo-samples
instead n (the number of samples in the training set). This
is necessary because n is usually a very large number, and
computing α0 and β0 with it would result in a very strong
prior that would introduce too much bias in the MAP esti-
mate.

The steps of the training procedure are described in Al-
gorithm 1. The parameter pu is the probability of the tail,
and w0 is the weight in sample counts that we assign to the
training set. In our experiments we used pu = 0.05 and
w0 = 400.

3.3. Proposed Adaptive Thresholding Algorithm

the non-informative (improper) prior is given by α = 1,
β = 0, and the parameters of the Gamma posterior under a

During testing, we ﬁrst perform a series of Kolmogorov-
Smirnov (KS) tests [14] to ﬁnd and remove anomalies. The

Algorithm 2 EVT adaptive thresholding algorithm

1: procedure ADAPTSCORES(x, α0, β0, pu, pf , w1, L,

(cid:98)λ0 ← β0
α0−1
y ← sort desc(x)

(cid:46) MLE in training set
(cid:46) Sort scores in descending

na)

order

k ← #y pu
for i ← 1, na do

scores, y labels

(cid:46) Training set T contains x

u ← yi+k
t ← {yi, . . . , yi+k} − u
Dn,i = supx∈t

(cid:46) Find upper threshold
(cid:46) Extract upper tail
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:46) Compute
(cid:12) (cid:98)Gn(x) − G(x; λ)

KS statistic
end for
ˆi ← mini{Dn,i}
u(cid:48) ← yˆi
t ← {yˆi, . . . , yˆi+k} − u
α1 ← α0 + w1
β1 ← β0 + w1
for i ← 1, n do

(cid:80) t

#t

(cid:46) Estimate number of outliers
(cid:46) Set outlier rejection threshold
(cid:46) Extract upper tail

w ← xi−(L−1)/2:i+(L−1)/2

(cid:46) Window

u ← u | #{wi > u} = #w pu

(cid:46) Find upper

centered at sample xi

threshold

t ← {wi | wi > u} - u
α ← α1 + #t
β ← β1 + (cid:80) t
(cid:98)λ ← β
α−1
yi ← xi + u − (cid:98)λ log(pf /pu)

(cid:46) Extract upper tail
(cid:46) Posterior
(cid:46) Posterior
(cid:46) MAP estimate
(cid:46) Adapt score

(cid:46) Adapted scores

2:
3:

4:

5:

6:
7:

8:

9:
10:
11:
12:

13:
14:
15:
16:

17:

18:
19:
20:

21:

22:
23:
24:
25: end procedure

end for
return y

KS statistic

Dn = sup
x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:98)Gn(x) − G(x; (cid:98)λ)
(cid:12)

(14)

measures the dissimilarity between distributions G(x; (cid:98)λ)
and (cid:98)Gn(x). G(x; (cid:98)λ) is the GPD in (9) and

(cid:98)Gn(x) = 1 −

I(Xi ≤ x)

(15)

1
n

n
(cid:88)

i=1

where I(x) is a standard indicator function, is the empirical
tail CDF. The KS test requires estimating a threshold Kα
for rejecting (with conﬁdence 1 − α) the hypothesis that
nDn > Kα.
the observed data does not ﬁt G with the test
The result from Lilliefors[9] shows that the KS test is biased
when the reference distribution G is not precisely known (in
this case, (cid:98)λ is estimated from the training data). However,
as noted in [1], it is not necessary to identify the exact value

√

of α for the purpose of removing anomalies and outliers.
Instead, we ﬁrst compute Dn with all the samples in the tail.
We call this Dn,1. We then remove the largest sample and
we compute Dn,2 using the remaining samples. We keep
iterating until we get Dn,na . Finally, we select the value of
i that minimizes Dn,i.

After removing the anomalies, we use the prior estimated
during training to compute the posterior for the whole se-
quence. This posterior is used as the prior for estimating the
tail distribution on each shift of a window centered on each
of the samples. The details of the adaptation procedure are
described in Algorithm 2. The input to the adaptation pro-
cedure is a sequence of scores x, the parameters of the prior
Gamma distribution α0 and β0, the size of the upper tail pu,
the target false alarm rate pf , the weight w1 that we assign
to the the prior contribution of the whole sequence, the win-
dow length L, and the maximum number of anomalies na
in the sequence. The output sequence y has been adapted
so that when it is thresholded at 0, the false alarm rate is pf .
In our experiments, we have used pu = 0.05, pf = 0.001,
w1 = 100, L = 101, and na = 12.

4. Experimental Results

To validate the effectiveness of the proposed approach,
we have used the 340 sequences of fastener detections cor-
responding to each of the 4 cameras in each of the 85 miles
of the Amtrak NEC concrete tie dataset introduced in [4].
This dataset contains a total of 203,287 ties and each tie is
divided in 4 regions (left ﬁeld, left gage, right gage, and
right ﬁeld), so the total number of images is 813,148. The
detection problem consists in determining whether an im-
age contains a fastener attached to one of the rails. The
dataset contains bounding boxes for all the images that are
known to contain a defect. The total number of defects is
1,087 (0.13% of all the fasteners). The defective fastener
class contains two subclasses: broken fastener and missing
fastener.

We have used the scores generated by the multi-task
learning (MTL) detector described in [3]. This detector
uses deep learning with multiple tasks that are trained in
parallel. The reason for using multiple tasks is to prevent
overﬁtting. By sharing a common low-level representation
between the fastener inspection task and a separate mate-
rial classiﬁcation task, there is a data ampliﬁcation effect
that results in better generalization for both classiﬁers. We
also compare the performance with the baseline single-task
learning (STL) method in [4]. The raw data was provided
by Amtrak, and the authors of [3, 4] provided the output
of their detectors as well as the codes to evaluate the per-
formance. This detector produces a scalar-valued score for
each image by spatially pooling all the detections in the im-
age. Scores are high when the image contains a good fas-
tener, and low when the fastener is either missing or broken.

Figure 2 shows several detection examples of the MTL de-
tector.

To facilitate the evaluation of fastener detection perfor-
mance under difﬁcult scenarios, whenever the fastener is not
directly attached to the rail or tie, or when for some reason
a fastener is not visible at all, those ties are marked as unin-
spectable with a special label. Depending on the value of
such label, the dataset is divided into 3 subsets:

• Clear ties: 200,763 ties (1,037 ties with at least one

defect).

• Clear ties plus switches: 201,856 ties (1,045 ties with
at least one defect). See Figure 3 for an example of a
switch section.

• All ties: 203,287 ties (1,052 ties with at least one de-
fect). This includes switches, and ties for which some
fasteners are not visible because they are covered by
ballast or a lubricator. See Figures 4 and 5 for exam-
ples of high ballast and lubricator sections.

Figure 4. Example of section marked as ballast.

Figure 3. Example of section marked as switch.

For training, we use all the available data after setting
aside the sequence being tested. Table 1 and Figure 6 show
the detection results on the normalized scores. The overall
improvement is signiﬁcant. The detection rate on the whole
dataset at P F A = 0.1% increases from 95.40% to 99.26%.
This is a 6× reduction in the missed rate. Moreover, the per-
formance on the clear tie subset does not degrade at all. The
running time of our EVT adaptation algorithm implemented
in MATLAB1 for adapting all 813,148 scores is only of 17
seconds on a Mid-2012 MacBook Pro with a 2.5 GHz Intel
Core i5 processor, so this dramatic improvement comes at
negligible computational cost (running the detector process
takes several hours).

1The code and data used in this section is available at
https://github.com/xavigibert/EvtTrack

Figure 5. Example of section marked as lubricator.

5. Conclusions

In this paper, we presented a new algorithm that nor-
malizes scores from a sequential anomaly detector with
the objective of harmonizing its false alarm rate. Extreme
value theory provides a solid foundation from which adap-
tive thresholding algorithms can be derived. When working
with sequences of images, we need to take advantage of the
statistical dependencies of nuisance parameters of nearby
images. If we discard such dependencies and treat each im-
age in the sequence independently, the performance suffers.
The CFAR detection approach proposed in this paper
has applicability beyond railway track inspection from a
moving vehicle. It could be used, for example, in surveil-
lance video to remove bursts of false alarms caused by sun
glare, insects, rain or fog. Its computational cost is negligi-
ble compared to that of the underlying detector, so this ap-
proach can be easily retroﬁtted to existing detectors already

Condition

PFA MTL + EVT MTL[3]

STL[4]

Fastener (only clear ties)

Fastener (clear + switch)

Fastener (all ties)

0.1%
0.02%

0.1%
0.02%

0.1%
0.02%

99.91%
97.20%

99.54%
93.80%

99.26%
93.47%

99.91% 98.41%
96.74% 93.19%

98.43% 94.54%
89.35% 88.70%

95.40% 87.38%
87.76%

–

Table 1. Fastener detection results before and after score normalization.

[12] W. Scheirer, A. Rocha, R. Micheals, and T. Boult. Robust fu-
sion: Extreme value theory for recognition score normaliza-
tion. In European Conference on Computer Vision (ECCV),
pages 481–495. Springer, 2010. 3

[13] W. J. Scheirer, A. Rocha, R. J. Micheals, and T. E. Boult.
Meta-recognition: The theory and practice of recognition
score analysis. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 33(8):1689–1695, 2011. 3

[14] A. Stuart, J. Ord, and S. Arnold. Kendall’s Advanced Theory
of Statistics, Volume 2A: Classical Inference and the Linear
Model. Hodder Arnold, London, U.K., 1999. 4

[15] S. Yun. The extremal index of a higher-order stationary
markov chain. The Annals of Applied Probability, 8(2):408–
437, may 1998. 3

in operation.

Acknowledgements

The authors thank Amtrak, ENSCO, Inc. and the Federal
Railroad Administration for providing the data used in this
paper.

References

[1] J. Broadwater and R. Chellappa. Adaptive threshold estima-
tion via extreme value theory. IEEE Transactions on Signal
Processing, 58(2):490–500, 2010. 3, 5

[2] M. A. Fischler and R. C. Bolles. Random sample consen-
sus: A paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
ACM, 24(6):381–395, 1981. 2

[3] X. Gibert, V. M. Patel, and R. Chellappa. Deep multi-task
learning for railway track inspection. arXiv:1509.05267,
2015. 2, 5, 7

[4] X. Gibert, V. M. Patel, and R. Chellappa. Robust fastener
detection for autonomous visual railway track inspection. In
IEEE Winter Conference on Applications of Computer Vision
(WACV), 2015. 5, 7

[5] E. Gumbel. Statistics of Extremes. Columbia University

Press, New York, 1958. 3

[6] P. J. Huber. Robust estimation of a location parameter. The
Annals of Mathematical Statistics, 35(1):73–101, 1964. 2
[7] P. J. Huber and E. M. Ronchetti. Robust Statistics. Wiley se-
ries in probability and statistics. John Wiley & Sons, Hobo-
ken, New Jersey, second edition, 2009. 2

[8] D. Y. Kim, J. J. Kim, P. Meer, D. Mintz, and A. Rosenfeld.
Robust computer vision: A least median of squares based
In in Proc. of Image Understanding Workshop,
approach.
pages 1117–1134, 1989. 2

[9] H. W. Lilliefors. On the Kolmogorov-Smirnov test for nor-
mality with mean and variance unknown. Journal of the
American Statistical Association, 62(318):399–402, 1967. 5
[10] R. A. Maronna, D. R. Martin, and V. J. Yohai. Robust Statis-
tics: Theory and Methods. Wiley series in probability and
statistics. John Wiley & Sons, Chichester, England, 2006. 2
[11] J. Pickands. Statistical inference using extreme order statis-
tics. The Annals of Statistics, 3(1):119–131, jan 1975. 3

(a)

(b)

(c)

Figure 6. ROC curves comparing defective fastener detection performance on the 85-mile testing set using normalized vs. unnormalized
scores (a) on the clear ties subset (b) on the clear with with switches subset (c) on all ties. Detections are per image (each tie has 4 images).

