9
1
0
2
 
c
e
D
 
5
 
 
]

G
L
.
s
c
[
 
 
1
v
8
6
3
2
0
.
2
1
9
1
:
v
i
X
r
a

Inter-Level Cooperation in Hierarchical Reinforcement Learning

Abdul Rahman Kreidieh∗
UC Berkeley

Samyak Parajuli
UC Berkeley

Nathan Lichtlé
ÃĽcole Normale SupÃľrieure

Yiling You
UC Berkeley

Rayyan Nasr
American University of Beirut

Alexandre M. Bayen
UC Berkeley

ABSTRACT
This article presents a novel algorithm for promoting cooperation
between internal actors in a goal-conditioned hierarchical reinforce-
ment learning (HRL) policy. Current techniques for HRL policy
optimization treat the higher and lower level policies as separate
entities which are trained to maximize different objective func-
tions, rendering the HRL problem formulation more similar to a
general sum game than a single-agent task. Within this setting,
we hypothesize that improved cooperation between the internal
agents of a hierarchy can simplify the credit assignment problem
from the perspective of the high-level policies, thereby leading to
significant improvements to training in situations where intricate
sets of action primitives must be performed to yield improvements
in performance. In order to promote cooperation within this set-
ting, we propose the inclusion of a connected gradient term to the
gradient computations of the higher level policies1. Our method is
demonstrated to achieve superior results to existing techniques in
a set of difficult long time horizon tasks.

KEYWORDS
hierarchical reinforcement learning; multi-agent learning; learning
agent-to-agent interactions

1 INTRODUCTION
Hierarchical models for neural networks, such as feudal networks [4]
and its variants [14, 20, 32], have served as a valuable tool for solv-
ing tasks with large time horizons and/or sparse rewards. These
models achieve superior performance by temporally abstracting the
rate at which actions from various levels of the hierarchy are issued,
thereby allowing for higher level policies to more easily perceive
the effects of actions over larger time intervals. The actions of the
higher level policies are passed to the levels below them as a goal
д, and the low-level policies are then trained to achieve these goals
via a goal-conditioned reward function rд(·). This often serves as a
denser reward than would be provided by the environment.

While the temporal abstraction of various levels of a policy
can improve training in certain settings, it also separates the once
single-agent policy into a set of policies. Specifically, the policy
in an n-level hierarchy is broken apart into n policies, each of

∗Corresponding author; email: aboudy@berkeley.edu
1For the purposes of reproducibility, the presented algorithms, environments, and
results are available online at: https://github.com/AboudyKreidieh/h-baselines.

Manuscript submitted to Proc. of the 19th International Conference on Autonomous Agents
and Multiagent Systems (AAMAS 2020), B. An, N. Yorke-Smith, A. El Fallah Seghrouchni,
G. Sukthankar (eds.), 2020
© 2020 International Foundation for Autonomous Agents and Multiagent Systems
(www.ifaamas.org). All rights reserved.
https://doi.org/doi

which are trained to maximize a separate reward function. The
consequences of this decomposition can be severe, as it transforms
the once single agent problem into a non-cooperative Markov game
with compatible or conflicting interests [12, 23], and as a result
yields some of the same complications as experienced in multi-
agent policy optimization, namely non-stationarity [2, 8, 34] and
the need for cooperative behaviors to emerge [3, 29, 30].

The present article aims to address the second of the two limiting
factors of multi-agent learning in HRL presented in the previous
paragraph, namely the absence of implicit channels through which
cooperation is encouraged between agents in a hierarchy. Simply
put, this article attempts to answer the question: Can cooperation
between agents in a hierarchy improve the overall performance
of said hierarchy? In order to so do, we present a novel optimiza-
tion scheme for hierarchical policies. Our approach attempts to
encourage cooperation between the various levels of the hierar-
chy by propagating the gradients of the losses of the lower level
policies through the upper level policies as well. This serves to in-
dicate to the higher level policies whether the commands they are
issuing to the level below them are reasonable and/or achievable.
This technique, which more closely resembles single agent policy
optimization for hierarchical models, more readily promotes coop-
eration between levels of the policy, and as a result can significantly
improve sample efficiency and training performance.
The key contributions of this article are as follows:

• In order to solidfy the connections between goal-conditioned
hierarchical tasks and Markov games, we begin by formulat-
ing the HRL problem as a Markov game and demonstrate a
potential limiting feature of existing policy update schemes
in the setting.

• Next, we derive a new off-policy gradient algorithm for
higher level policies in two-level hierarchical policies. Our
technique, called hierarchical reinforcement learning with
connected gradients (HRL-CG), is empirically demonstrated
to achieve superior performance for existing algorithms.
• Our technique introduces a new hyperparameter to the train-
ing procedure (λ), which can add a layer of complexity to the
hyperparameter tuning procedure. As a result, we dedicate
a portion of this article on developing an intuition of the
effects this term can have on the evolution of the various
policies and on how these effects can help dictate the choice
of λ.

The remainder of the article is organized as follows. Section 2
provides a background on MDPs, reinforcement learning, HRL, and
presents the hierarchical model utilized for the rest of the article.
Section 3 motivates the study of HRL under the lens of multi-agent
systems by formally defining the goal-conditioned HRL tasks as

Manuscript submitted to AAMAS, 2020

A. Kreidieh et al.

a multi-agent game. Section 4 introduces the connected-gradient
formulation and compares it with multi-agent learning. Section 5
finds connections with our work and others involving improved
coordination with gradients. And finally, Section 6 analyzes our
model with other hierarchical algorithms for popular tasks.

2 BACKGROUND
2.1 Reinforcement learning and MDPs
RL problems are generally studied as a Markov decision problem
(MDP) [1], defined by the tuple:

MDP = (S, A, P, r, ρ0, γ ,T )
where S ⊆ Rn is an n-dimensional state space, A ⊆ Rm an m-
dimensional action space, P : S × A × S → R+ a transition
probability function, r : S → R a bounded reward function, ρ0 :
S → R+ an initial state distribution, γ ∈ (0, 1] a discount factor,
and T a time horizon.

(1)

In a MDP, an agent receives sensory inputs st ∈ S from the
environment and interacts with this environment by performing
actions at ∈ A. The agent’s actions are often defined by a policy
πθ : S × A → R+ parametrized by θ . The objective of the agent
is to learn an optimal policy: θ ∗ := argmaxθ η(πθ ), where η(πθ ) =
(cid:205)T

i=0 γ iri is the expected discounted return.

2.2 Actor-critic algorithms
Several algorithms have been proposed for computing an optimal
policy under the aforementioned formulation. With gradient based
methods, the policy is updated by taking the gradient of the ex-
pected return ∇θ η(θ ) [27]. In a specific class of actor-critic methods
particularly well suited for continuous action spaces, the gradient of
a continuous policy, or actor, is computed through the deterministic
policy gradient algorithm [22]:

(2)

(cid:2)∇aQϕ (s, a)|a=π (s)∇θ πθ (s)(cid:3)

∇θ η(θ ) = Es∼pπ
where Qϕ (s, a) = Esi ∼pπ ,ai ∼π [(cid:205)T
i=t γ iri |s, a] is the expected return
from a starting state s given an action a. This value function is
estimated by an additional function approximator, known as a
critic, which is learned using temporal difference learning [26, 33],
an update rule based on the Bellman equation [1]:
ε(st , at , st +1) = (cid:2)Qϕ (st , at ) − y(cid:3) 2

(3)

where:

y = rt + γ ¯Qϕ (st +1, πθ (st +1))
and ¯Qϕ is a target Q function whose parameters are periodically
updated with the most recent ϕ. This target term is meant to help
stabilize learning [18].

(4)

2.3 Hierarchical reinforcement learning
One drawback of the generic RL formulation is its inability to effec-
tively handle long-term credit assignment problems, particularly
in the presence of sparse reward functions. An example of this is
the game Montezuma’s Revenge, where classic techniques such as
DQN fail to produce relevant high-level behaviors from exploring
short-term primitive actions [13].

c

дt

· · ·

· · ·

st

at

st +1

at +1

st +i

at +i

Environment

Figure 1: A schematic representation of the hierarchical
model architecture and algorithm used in this article. The
Manager network issues new commands дt every k time
steps to the Worker. The Worker then perform environment
actions at to accomplish the goal issued by the Manger.

In response to the above limitation, hierarchical reinforcement
learning (HRL) proposes methods for decomposing complex tasks
into simpler subproblems that can more readily be tackled by low-
level action primitives. In HRL, the controller is decomposed into a
high-level controller policy in charge of planning over long time
horizons, and a low-level controller policy in charge of execut-
ing actions within the environment. The high-level controller is
decoupled from the low-level controller by operating at a lower
temporal resolution and passing either options [28] or goals [4] to
the lower-level at said frequency. This compresses the large time
horizon problem from the perspective of the high-level controller
in a much smaller one, and allows the low-level policy to produce
action primitives that support short time horizon tasks as well.

2.4 Goal-conditioned HRL
Several HRL frameworks have been proposed to facilitate and/or
encourage the decomposition of decision-making and execution
during training [4, 6, 21, 28]. In this article, we consider a two-
level goal-conditioned hierarchy presented in [20] (see Figure 1),
which itself is similar to the feudal networks formulation by Dayan
and Hinton [4]. This network consists of a high-level, or Manager,
policy πm that computes and outputs goals дt ∼ πm (st , c) every
k time steps, and a low-level, or Worker, policy πw that takes as
inputs the current state and the assigned goals and is encouraged
to perform actions at ∼ πw (st , дt ) that satisfy these goals via an
intrinsic reward function rw (st , дt , st +1). The contextual term, c,
parametrizes the environmental objective (e.g. desired position to
move to), and consequently is passed both to the Manager policy
as well as the environmental reward function rm (st , c).

We utilize an intrinsic reward function that serves to characterize
the goals as desired relative changes in observations. The intrinsic
reward function is accordingly:

rw (st , дt , st +1) = −||st + дt − st +1||2

(5)

In order to maintain the same absolute position of the goal regard-
less of state change, a fixed goal-transition function h(st , дt , st +1) =
st +дt −st +1 is used in between goal-updates by the Manager policy.
The goal transition function is accordingly defined as:

дt +1 =

(cid:40)

πm (st , c)
st + дt − st +1

if t mod k = 0
otherwise

(6)

Inter-Level Cooperation in Hierarchical Reinforcement Learning

Manuscript submitted to AAMAS, 2020

c

дt

c

дt

∇θmηm

∇θw ηw

· · ·

· · ·

· · ·

· · ·

∇θmηw

st

at

st +1

at +1

st +i

at +i

st

at

st +1

at +1

st +i

at +i

Environment

Environment

Figure 2: A schematic representation of the actor policy updates for both the Manager and the Worker policies. Left: The actor
policies of both the Manager and Worker networks are trained to maximize separate reward functions, as indicated by the
gradient update steps in red. No gradients are shared between the two. Right: In order to encourage cooperation between the
two policies, we propose the inclusion of additional gradient term that propagates the rewards associated with the Worker
policies through the Manager as well.

Notation Meaning

πx (·, ·)
ηx (·)

Actor network
Expected discounted return. A function of the trainable
actor parameters of the level.
Value function

Vx (·, ·)
Qx (·, ·, ·) Q-function / critic
θx
ϕx
st
at
дt
c

Actor trainable parameters
Critic trainable parameters
observation at time t
Worker action at time t
Manager action (goal) at time t
Contextual term. Assigns context-specific environmen-
tal goals.
Reward for a given state, goal/context, next state
Time horizon
Manager goal-assignment period

rx (·, ·, ·)
T
k

Table 1: Main Notations. The x variables in the notation may
be m for the Manager and w for the Worker.

3 CONNECTIONS BETWEEN HIERARCHICAL

AND MULTI-AGENT RL

Prior to advancing to the proposed methods of this article, we begin
by motivating the study of goal-conditioned hierarchical reinforce-
ment learning under the lens of multi-agent systems. We do so by
formulating the HRL task as a sequence of non-stationary MDPs.
Further characterizations of the task as a multi-agent problem also
emerge under the formulation of the naive optimization scheme in
Section 4.1 and below where we highlight similar challenges faced
in both areas:

Non-stationarity. In multi-agent RL, several independent agents
interact with an environment as well as potentially one another. In
this setting, the MDP of any individual agent i is:
MDPi = (Si , Ai , Pi , ri , ρ0,i , γ ,T , π −i
θ
where, in addition to the variables depicted in Section 2.1, the
environment is further characterized by the policies of all other

(7)

)

agents within the network π −i
θ , which evolve as training progresses.
This additional evolving term contributes to the nonstationarity of
the environment from the perspective of a particular agent since
it is always changing, a factor that can be seen as invalidating the
use of standard off-policy temporal difference learning of the value
functions (Eq. (3)) in multiagent settings [17]. Returning to the
hierarchical model architecture depicted in Section 2.4, it can be
seen that the Manager and Worker policies, πm and πw , exist under
similar environmental settings as those experienced in multi-agent
systems, with the MDP of each policy expressed as:
MDPm = (S, Am, P, rm, ρ0, γ ,T /k, πw )

(8)

MDPw = (S, Aw , P, rw , ρ0, γ , k, πm )
As a result, the aforementioned non-stationarity effects of off-
policy training in MARL are a potential limiting factor in HRL as
well, since the lower level policy is constantly changing from the
perspective of the higher level one.

(9)

Credit Assignment Problem. The Credit Assignment Problem
deals with determining the contribution of a particular part of
a system component to its overall success. This is a common issue
in single agent learning, where it is difficult to determine which ac-
tion was critical in achieving the goal. In multi-agent learning, this
problem is exacerbated because there are many more components
operating on the environment and can also result in problems such
as the âĂĲLazyAgent ProblemâĂİ [25]. Hierarchical reinforcement
learning attempts to deal with the credit assignment problem by
introducing abstractions through higher-level policies that look at
experiences over a temporally extended period. However, there is
still a "hierarchical credit assignment problem" as mentioned in
[5], which deals with determining which abstracted node led to the
reward.

4 CONNECTED HIERARCHICAL MODELS
Looking to the multi-agent nature of hierarchical reinforcement
learning, this article attempts to address one potential limiting fac-
tor when training hierarchical policies, namely the degree to which
cooperation between internal agents is encouraged. In response to

Manuscript submitted to AAMAS, 2020

A. Kreidieh et al.

this, this section introduces the connected gradient formulation for
training higher-level policies in two-level goal-conditioned hier-
archies. The effects of this formulation on subsequent training is
empirically studied in later sections.

4.1 Naive HRL optimization scheme
We begin by providing the naive HRL optimization scheme. For the
purposes of this study, we formulate the policy update procedure
as a concurrent training between the Manager and Worker.

As mentioned in Section 2.4, the Manager in this setting is
rewarded based on the original environmental reward function:
rm (st , c). Under the actor-critic formulation depicted in Section 2.2,
and assuming that the contextual term c is also passed to the actor
and critic, the policy gradient procedure for the Manager’s actor
policy is:

∇θm ηm = Es∼pπ

(cid:2)∇aQm (s, c, a)|a=πm (s,c)∇θm πm (s, c)(cid:3)

(10)

where the transition probability pπ = pπm, πw is a function of both
the Manager and Worker policies.

Unlike the Manager, the Worker policy is motivated to follow
the goals set by the Manager via an intrinsic reward based on the
distance between the current observation and the goal observation:
rw (st , дt , st +1). For this article, we use:

rw (st , дt , st +1) = −||st + дt − st +1||2

(11)

The choice of Worker reward function is not studied in the present
article, however, for later sections we emphasize that this reward
function is explicitly defined within the algorithm.

As a result of this dissimilar intrinsic reward rw , the Worker

policy is assigned a separate gradient update procedure:
(cid:2)∇aQw (s, д, a)|a=πw (s,д)∇θw πw (s, д)(cid:3)

∇θw ηw = Es∼pπ

(12)

The above actor update procedures are depicted in Fig. 2 (left
section). Notably, no gradients under this formulation are shared
between the Manager and Worker policies, further highlighting the
multi-agent nature of this task. The lack of shared gradients results
in a situation whereby the Manager policy does not receive direct
feedback on an agent’s ability to perform certain goals, a factor that
can have a significant impact on the potential value of the specified
goal. The unconstrained nature of the Manager’s evolution can also
result in situations whereby the Worker policy chases an unstable
target as the Manager attempts to identify effective goals. As we
expand on below, our connected gradient formulation allows the
model to more clearly disambiguate which lower level actions are
contributing to greater rewards, thereby reducing the impacts of
the credit assignment problem and in general performing more
stable updates.

4.2 HRL with connected gradients
In order to promote cooperation between the Manager and Worker
policies as training progresses, we define a new gradient update
procedure that propagates the losses experienced by the Worker
through the Manager as well, see Fig. 2 (right section). To that end,
we begin by modifying the expected return of the Manager policy
ηm to include a weighted term for the Worker expected return as

well. Our new expected return, η′

m , is:

η′
m

= Es∼pπ

= ηm + ληw
T /k

(cid:213)





+ λEs∼pπ

t =0

(cid:2)γ t (rm (st , c, πm (s, c))(cid:3)







γ t rw (st , д, πw (st , д))

(cid:34) k
(cid:213)

(cid:35)

t =0

(13)

where λ is a weighting term whose effect is explored in Section 6.4.
Under this new definition of the loss function, the Worker loss
is propagated through the Manager’s trainable parameters by re-
placing the goal term within the reward function of the Worker
with the direct output from the Manager’s policy. Accordingly, the
transition of the д term in Eq. (13) is defined in Eq. (6).

Solving for the gradient of Eq. (13) with respect to the Manager
policy’s parameters θm , we derive the new connected-gradient
formulation of the HRL problem as:

∇θm η′
m

= Es∼pπ

(cid:2)∇aQm (s, c, a)|a=πm (s,c)∇θm πm (s, c)(cid:3)

(cid:20)

+ λEs∼pπ

∇θm дt ∇д

(cid:0)r (д, st , πw (дt , st ))

+ πw (д, st )∇aQw (дt , st , a)|a=πw (дt ,st )

(14)

(cid:21)

(cid:1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дt

The above equation is derived in Appendix A. Breaking it apart,
we can see that it consists of three terms. The first and third term
computes the gradient of the critic policies Qm and Qw with respect
to the parameters θm , respectively. The second term computes
the gradient of the Worker-specific reward with respect to the
parameters θm . While this second term at first glance may seem
unusual, it is worth remembering that this reward function is in
fact a design feature within the HRL formulation, and in the case
of this article is explicitly depicted in Eq. (11).

5 RELATED WORK
We focus on related work that deal with differentiable coordination
in multi-agent systems.

Recent work has proposed a hierarchical policy gradient that
promotes the use of a combined Manager/Worker loss [15]. Our
work differentiates from this work primarily in three respects. First
off, our derivation of the gradient applies to actor-critic algorithms,
which have been empirically demonstrated to be significantly more
data-efficient [10, 11], and, being off-policy algorithms, may suffer
even more greatly from the non-stationary effects associated with
poor cooperation [20]. Moreover, their implementation does not
propose the use of weighting term λ when combining the two gra-
dients. This term, as we detail in Section 6.4, can have a significant
effect of improving training performance. Finally, we take the gra-
dient of the return of the Manager with respect to the Worker in
order to use connected gradients to propagate information up from
the Worker to the Manager, whereas their purpose is to calculate
the gradient of the trajectory with respect to both Manager and
Worker.

Our work can also be compared to communication in multi-
agent systems in works such as [24] and namely the end to end
Differentiable Inter-Agent Learning (DIAL) method proposed in [9].

Inter-Level Cooperation in Hierarchical Reinforcement Learning

Manuscript submitted to AAMAS, 2020

no meta

HRL

HRL-CG, λ = 0.01

(a) Environment

(b) Evaluation Rewards

(c) Intrinsic Rewards

(d) Legend

Figure 3: AntGather results.

Our work does not aim to learn an explicit communication channel;
however, it is motivated by the same principle - that letting gradi-
ents flow across agents results in richer feedback. Furthermore, we
differ in the fact that we structure the problem as a hierarchical re-
inforcement learning one creating a more constrained and directed
objective. Finally, our gradients are propagated from the Worker
up to the Manager and not arbitrarily among agents.

Finally, we mentioned that goal-conditioned hierarchical rein-
forcement learning has been a useful paradigm in increasing co-
operation between multiple levels of a hierarchy; a proposed im-
provement has been using representation learning through a goal
embedding. This idea has been proposed in [19] and can be seen as
an extended form of communication, wherein they try to convey
the most relevant information in a compressed format to induce
greater cooperation.

6 EXPERIMENTS
In this section, we detail the experimental setup and training pro-
cedure, and present results of various continuous control tasks.

6.1 Environments
The performance of the proposed method is demonstrated on two
classic sparse reward tasks. These environments were all simulated
using the MuJoCo physics engine for model-based control [31].
The time horizon in both tasks is set to 500 steps. The tasks are as
follows:

6.1.1 Ant Gather. This task is a direct replication of the envi-
ronment presented in [7]. In this task, a quadrupedal (Ant) agent
is placed in a 20x20 space with 8 apples and 8 bombs. The agent
receives a reward of +1 or collecting an apple and -1 for collecting
a bomb; all other actions yield a reward of 0. Results are reported
over the average of the past 100 episodes.

6.1.2 Ant Maze. This task is a direct replication of the environ-
ment presented in [20]. In this task, immovable blocks are placed to
confine the agent to a U-shaped corridor. The agent is initialized at
position (0, 0), and assigned an (x, y) position in the network that
it is expected to reach (this (x, y) position is what we refer to as
c in our formulation). The agent is rewarded with its negative L2
distance from this position.

During the training procedure, the agent is assigned (x, y) values
between the range (−4, −4)x(20, 20) at the start of every episode.
The performance of the agent is evaluated every 50,000 steps at the
positions (16, 0), (16, 16), and (0, 16) based on a “success” metric,

defined as achieved if the agent is within an L2 distance of 5 from
the target at the final step. This evaluation metric is averaged across
50 episodes.

6.2 Experimental setup
Experiments were conducted using the TD3 learning algorithm [10],
a variant of the DDPG [16] algorithm for continuous control that
utilizes duel value functions and delaying policy updates to reduce
overestimation bias. Results are reported over 10 random seeds of
the simulator and the network initialization. The choice of hyper-
parameters are as follows:

• Network shapes of (256, 256) for the actor and critic of both
controllers with ReLU nonlinearities at the hidden layers and
tanh nonlinearity at the output layer of the actors. The output
of the actors are scaled to match the desired action space.

• Adam optimizer; learning rate: 3e-4 for actor and critic of both

controllers

• Soft update targets τ = 5e-3 for both controllers
• Discount γ = 0.99 for both controllers.
• Replay buffer size: 200,000 for both controllers.
• Lower-level critic updates 1 environment step and delay actor

and target critic updates every 2 steps.

• Higher-level critic updates 10 environment steps and delay actor

and target critic updates every 20 steps.

• Huber loss function for the critic of both controllers.
• No gradient clipping.
• Exploration (for both controllers):

– Initial purely random exploration for the first 10,000 time steps
– Gaussian noise of the form N (0, 0.1×action range) for all other

steps

• Reward scale of 0.1 for AntMaze and 10 for AntGather.
• Connected gradient weights λ are provided in the next sections.

6.3 Comparative analysis
We test our proposed algorithm against two baseline methods:

• no-meta: a generic fully connected network to validate the need

for hierarchical models to solve these tasks, and

• HRL: the generic formulation of the hierarchical reinforcement
learning optimization scheme. This can also be considered as a
special case of our HRL-CG algorithm with λ set to 0.

For our algorithm, HRL-CG, we set λ = 0.01 and λ = 0.005 for the
AntGather and AntMaze environments, respectively.

Manuscript submitted to AAMAS, 2020

A. Kreidieh et al.

no meta

HRL

HRL-CG, λ = 5e-3

(a) Environment

(b) AntMaze [16,0]

(c) AntMaze [16,16]

(d) AntMaze [0,16]

(e) Intrinsic Rewards

Figure 4: AntMaze results.

(a) Environmental Rewards

(b) Intrinsic Rewards

Figure 5: Effect of choice of connected gradient weight on
training performance in AntGather.

Figures 3 and 4 depict the training performance of both and
Manger and Worker polices in the AntGather and AntMaze envi-
ronments, respectively. In both cases, the use of connected gradients
results in significant improvements on the performance of the pol-
icy within the environment. In the AntGather environment, while
the HRL-CG policy originally progresses more slowly than the HRL
algorithm, it is ultimately capable of reaching a maximum return
of 4.10 ± 0.18 across 10 policies, far exceeding the HRL algorithm’s
maximum return of 3.04 ± 0.54. The HRL-CG algorithm also ex-
hibits more stable training in this setting, yielding a much smaller
variance in performance between seeds.

Similar improvements can be seen in the AntMaze tasks. In this
case, the HRL-CG performs approximately equivalently to the HRL
algorithm for simple goals, in this case moving to (16,0), i.e. simply
moving forward. However, for the more difficult tasks and ones
that require a greater deal of exploration, the HRL-CG algorithm
converges to its maximal value far before the HRL algorithm is
capable of doing so.

The improvements in environmental returns in both tasks are
equally coupled with a set of improvements in the intrinsic re-
turns, thereby validating the use of connected gradients to improve
cooperation between the two policies.

6.4 Choice of connected-gradient weight
In this section, we explore the effects and implications of using dif-
ferent values of λ for the connected-gradient weight. Fig. 5 depicts
the effect of the choice of λ on the performance of both the Manager
and Worker policies in the AntGather environment. When the value
of λ equals 0, the algorithm is akin to the naive goal-conditioned
HRL scheme as explained in Section 2.4. For small values of λ, we
witness improvements in intrinsic rewards that do not result in

noticeable changes to the performance of the training algorithm.
As we increase the λ term; however, the increase in coordination
begins to slow down improvements in environmental returns at the
early stages of training. The corresponding increase in cooperation,
however, results in a situation whereby the Manager policy can
confidently identify the effects of local goals on the actions the
Worker performs, ultimately allowing the overall policy to train
much more efficiently. This initial slowing in training; however,
begins to intensify for larger λ terms in which the values of the
Worker gradient send too strong of a signal to the Manager policy,
thereby slowing down or stopping training completely. Interest-
ingly enough, this does not result in significant improvement in
the intrinsic returns from the most high-performing λ term. These
results suggest that moderate values of λ are necessary to yield
optimal results.

7 CONCLUSIONS AND FUTURE WORK
In this work, we propose connections between multi agent and hier-
archical reinforcement learning that motivates our novel method of
inducing cooperation between hierarchies. We provide a derivation
of the gradient of a Manager policy with respect to its Workers for
an actor-critic formulation as well as introducing a λ weighting
term for this gradient which controls the level of coordination. We
find that optimizing using this method results in consistently better
results, particularly for more difficult tasks.
For future work, we would like to expand our framework of inter-
level coordination by using more principles proven to be effective in
multi-agent systems, particularly with two primary objectives. First,
we intend to observe the effects of using a communication channel
for more explicit feedback. We also would like to implement a critic
that is centralized through hierarchies. Finally, we would like to test
these approaches for competitive as well as collaborative scenarios.

A DERIVATION OF CONNECTING MANAGER

GRADIENTS

In this section, we derive an analytic expression of the gradient of
the Manager policy in a two-level goal-conditioned hierarchy with
respect to both the losses associated with the high level and low
level policies. In mathematical terms, we are trying to derive an
expression for the weighted summation of the derivation of both
losses, expressed as follows:

∇θm η′
m

= ∇θm

(ηm + ληw ) = ∇θm ηm + λ∇θm ηw

(15)

Inter-Level Cooperation in Hierarchical Reinforcement Learning

Manuscript submitted to AAMAS, 2020

where λ is a weighting term and ηm and ηw are the expected returns
assigned to the Manger and Worker policies, respectively. More
specifically, these two terms are:
(cid:104)(cid:205)T /k
t =0

(cid:2)γ t (rm (skt , c, πm (skt , c))(cid:3) (cid:105) = ∫

S ρ0(st )Vm (st , c)dst

ηm = Es∼pπ

(16)

ηw = Es∼pπ

t =0 γ t rw (st , дt , πw (st , дt ))

S ρ0(st )Vw (st , дt )dst

(17)

(cid:105) = ∫

(cid:104)(cid:205)k

Here, under the actor-critic formulation we replace the expected
return under a given starting state with the value functions Vm and
Vw This is integrated over the distribution of initial states ρ0(·).

Following the results by [22], we can express the first term in

Eq. (14) as:

∇θw ηw = Es∼pπ

∇aQw (s, д, a)|a=πw (s,д)∇θw πw (s, д)

(18)

(cid:105)

(cid:104)

We now expand the second term of the gradient into a function
of the Manager and Worker actor (πm , πw ) and critic (Qm , Qw )
policies and their trainable parameters. In order to propagate the
loss associated with the Worker through the policy parameters of
the Manager, we assume that the goals assigned to the Worker дt are
not fixed variables, but rather temporally abstracted outputs from
the Manager policy πm , and may be updated in between decisions
by the Manager via a transition function h. Mathematically, the
goal transition is defined as:

(cid:40)

дt (θm ) =

πm (st )
h(st −1, дt −1(θm ), st ) otherwise

if t mod k = 0

For the purposes of simplicity, we choose express the Manager
output term дt (θm ) as дt from now on.

We begin by computing the partial derivative of the Worker

value function with respect to the parameters of the Manager:
∇θmVw (дt , st )

= ∇θm Qw (дt , st , πw (дt , st ))

= ∇θm

(cid:18)
r (дt , st , πw (дt , st ))

S

G
= ∇θm r (дt , st , πw (дt , st ))
∫

∫

+ γ ∇θm

G

S

∫

∫

+

γpw (д′, s ′|дt , st , πw (дt , st ))Vw (д′, s ′)ds ′dд′

(cid:19)

The second element, pw,2, is the state transition probability from

the MDP formulation of the task, i.e.

pw,2(s ′|дt , st , πw (дt , st )) = p(s ′|st , πw (дt , st ))

(23)

Combining Eq. (21)-(23) into Eq. (20), we get:

∇θmVw (дt , st )

= ∇θm r (дt , st , πw (дt , st ))
∫

∫

+ γ ∇θm

G

S

(cid:18)
pw,1(д′|s ′, дt , st , πw (дt , st ))

= ∇θm r (дt , st , πw (дt , st ))
∫

∫

+ γ ∇θm

∫

+ γ ∇θm

G∩{дt +1 }

S
∫

(G∩{дt +1 })c

S
= ∇θm r (дt , st , πw (дt , st ))
∫

+ γ ∇θm

S

pw,2(s ′|дt , st , πw (дt , st ))Vw (д′, s ′)ds ′dд′

(cid:19)

1 · p(s ′|st , πw (дt , st ))Vw (д′, s ′)ds ′dд′

0 · p(s ′|st , πw (дt , st ))Vw (д′, s ′)ds ′dд′

p(s ′|st , πw (дt , st ))Vw (дt +1, s ′)ds ′

(24)

(19)

Continuing the derivation of ∇θmVw from Eq. (24), we get,

∇θmVw (дt , st )

= ∇θm r (дt , st , πw (дt , st ))
∫

S
= ∇θm r (дt , st , πw (дt , st ))

+ γ ∇θm

p(s ′|st , πw (дt , st ))Vw (дt +1, s ′)ds ′

∇θm p(s ′|st , πw (дt , st ))Vw (дt +1, s ′)ds ′

= ∇θm дt ∇дr (д, st , πw (дt , st ))|д=дt

+ ∇θm дt ∇дπw (д, st )|д=дt ∇ar (дt , st , a)|a=πw (дt ,st )
(cid:18)
Vw (дt +1, s ′)∇θm дt ∇дπw (д, st )|д=дt

+ γ

∫

∇ap(s ′|st , a)|a=πw (дt ,st )ds ′

(cid:19)

∫

+ γ

S

S

S

∫

+ γ

pw (д′, s ′|дt , st , πw (дt , st ))Vw (д′, s ′)ds ′dд′

p(s ′|st , πw (дt , st ))∇θmVw (дt +1, s ′)ds ′

(20)

= ∇θm дt ∇д

(cid:18)
r (д, st , πw (дt , st ))

where G and S are the goal and environment state spaces, re-
spectively, and pw (·, ·|·, ·, ·) is the probability distribution of the
next state from the perspective of the Worker given the current
state and action. Expanding the latter term, we get:

pw (д′, s ′|дt , st , πw (дt , st ))

= pw,1(д′|s ′, дt , st , πw (дt , st ))pw,2(s ′|дt , st , πw (дt , st ))
The first element, pw 1, is the probability distribution of the next
goal, and is deterministic with respect to the conditional variables.
Specifically:

(21)

pw,1(д′|дt , st , πw (дt , st )) =

(cid:40)

if д′ = дt +1
1
0 otherwise

(22)

+ πw (д, st )∇ar (дt , st , a)|a=πw (дt ,st )

∫

+ γ

S

Vw (дt +1, s ′)πw (д, st )∇ap(s ′|st , a)|a=πw (дt ,st )ds ′

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дt

∫

+ γ

S

= ∇θm дt ∇д
∫

∫

+ γ

S

p(s ′|st , πw (дt , st ))∇θmVw (дt +1, s ′)ds ′
(cid:18)
r (дt , st , a)

(cid:18)
r (д, st , πw (дt , st )) + πw (д, st )∇a

+ γ

Vw (дt +1, s ′)p(s ′|st , a)ds ′

S

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)a=πw (дt ,st )

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дt

p(s ′|st , πw (дt , st ))∇θmVw (дt +1, s ′)ds ′

(25)

Manuscript submitted to AAMAS, 2020

= ∇θm дt ∇д

(cid:18)
r (д, st , πw (дt , st ))

+ πw (д, st )∇aQw (дt , st , a)|a=πw (дt ,st )

∫

+ γ

S

p(s ′|st , πw (дt , st ))∇θmVw (дt +1, s ′)ds ′

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дt

(26)

Iterating this formula, we have,

∇θmVw (дt , st )

= ∇θm дt ∇д

(cid:18)
r (д, st , πw (дt , st ))

+ πw (д, st )∇aQw (дt , st , a)|a=πw (дt ,st )

∫

+ γ

S

p(st +1|st , πw (дt , st ))∇θmVw (дt +1, st +1)dst +1

= ∇θm дt ∇д

(cid:18)
r (д, st , πw (дt , st ))

+ πw (д, st )∇aQw (дt , st , a)|a=πw (дt ,st )

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дt

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дt

∫

+ γ

S

p(st +1|st , πw (дt , st ))∇θm дt +1∇д

(cid:18)
r (д, st +1, πw (дt +1, st +1))

+ πw (д, st +1)∇aQw (дt +1, st +1, a)|a=πw (дt +1,st +1)

+ γ 2

∫

∫

S

S

(cid:18)
p(st +1|st , πw (дt , st ))p(st +2|st +1, πw (дt +1, st +1))

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дt +1

dst +1

(cid:19)

∇θmVw (дt +2, st +2)dst +2dst +1
...

=

∞
(cid:213)

n=0

∫

γ n

∫

· · ·

S
S
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)
(cid:123)(cid:122)
n times

(cid:124)

(cid:32)n−1
(cid:214)

k =0

p(st +k +1|st +k , πw (дt +k , st +k ))

(cid:33)

× ∇θm дt +n ∇д

(cid:18)
r (д, st +n, πw (дt +n, st +n ))

+ πw (д, st +n )∇aQw (дt +n, st +n, a)|a=πw (дt +n,st +n )

dst +n · · · dst +1

(cid:19)(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дt +n

(27)

Taking the gradient of the expected Worker value function, we

get,

∇θm ηw

∫

S

= ∇θm
∫

=

=

S
∫

S

ρ0(s0)

ρ0(s0)Vw (д0, s0)ds0

ρ0(s0)∇θm Vw (д0, s0)ds0

∫

γ n

· · ·

∫

(cid:34) (cid:32)n−1
(cid:214)

S
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124)

(cid:123)(cid:122)
n times

S
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)

k =0

∞
(cid:213)

n=0

(cid:18)

× ∇д

r (д, sn, πw (дn, sn ))

p(sk +1 |sk, πw (дk, sk ))

∇θm дn

(cid:33)

+ πw (д, sn )∇a Qw (дn, sn, a)|a=πw (дn,sn )

dsn · · · ds0

(cid:19) (cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дn

A. Kreidieh et al.

∞
(cid:213)

∫

=

∫

· · ·

n=0

S
S
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124)
(cid:125)
(cid:123)(cid:122)
n+1 times

γ npθm,θw (τ )∇θm дn ∇д

r (д, sn, πw (дn, sn ))

(cid:18)

+ πw (д, sn )∇a Qw (дn, sn, a)|a=πw (дn,sn )

dsn · · · ds0

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дn

= Eτ ∼pθm , θw

(τ )

∇θm дt ∇д

r (д, st , πw (дt , st ))

(cid:20)

(cid:18)

+ πw (д, st )∇a Qw (дt , st , a)|a=πw (дt , st )

(cid:21)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дt

(28)

where τ = (s0, a0, s1, a1, . . . , sn ) is a trajectory and µθm,θw ,n (τ ) is
the (improper) discounted probability of witnessing a trajectory a
set of policy parameters θm and θw .

The final representation of the connected gradient formulation

is then:

∇θm η′
m
= Es∼pπ

(cid:2)∇aQm (s, c, a)|a=πm (s,c)∇θm πm (s, c)(cid:3)
(cid:18)
r (д, st , πw (дt , st ))

∇θm дt ∇д

(cid:20)

τ ∼pθm ,θw (τ )

+ E

+ πw (д, st )∇aQw (дt , st , a)|a=πw (дt ,st )

(29)

(cid:21)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)д=дt

REFERENCES
[1] Richard Bellman. 1957. A Markovian decision process. Journal of Mathematics

and Mechanics (1957), 679–684.

[2] Lucian Busoniu, Robert Babuska, and Bart De Schutter. 2006. Multi-agent rein-
forcement learning: A survey. In 2006 9th International Conference on Control,
Automation, Robotics and Vision. IEEE, 1–6.

[3] Caroline Claus and Craig Boutilier. 1998. The dynamics of reinforcement learning

in cooperative multiagent systems. AAAI/IAAI 1998, 746-752 (1998), 2.

[4] Peter Dayan and Geoffrey E Hinton. 1993. Feudal reinforcement learning. In

Advances in neural information processing systems. 271–278.

[5] Thomas G. Dietterich. 1998. The MAXQ Method for Hierarchical Reinforcement
Learning. In In Proceedings of the Fifteenth International Conference on Machine
Learning. Morgan Kaufmann, 118–126.

[6] Thomas G Dietterich. 2000. Hierarchical reinforcement learning with the MAXQ
value function decomposition. Journal of Artificial Intelligence Research 13 (2000),
227–303.

[7] Carlos Florensa, Yan Duan, and Pieter Abbeel. 2017. Stochastic Neural Networks
for Hierarchical Reinforcement Learning. arxiv preprint arxiv:1704.03012 (2017).
[8] Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras,
Philip HS Torr, Pushmeet Kohli, and Shimon Whiteson. 2017. Stabilising ex-
perience replay for deep multi-agent reinforcement learning. arXiv preprint
arXiv:1702.08887 (2017).

[9] Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson.
2016. Learning to Communicate with Deep Multi-Agent Reinforcement Learning.
CoRR abs/1605.06676 (2016). arXiv:1605.06676 http://arxiv.org/abs/1605.06676
[10] Scott Fujimoto, Herke van Hoof, and David Meger. 2018. Addressing function
approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477
(2018).

[11] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft
actor-critic: Off-policy maximum entropy deep reinforcement learning with a
stochastic actor. arXiv preprint arXiv:1801.01290 (2018).

[12] Junling Hu and Michael P Wellman. 2003. Nash Q-learning for general-sum
stochastic games. Journal of machine learning research 4, Nov (2003), 1039–1069.
[13] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum.
2016. Hierarchical deep reinforcement learning: Integrating temporal abstraction
and intrinsic motivation. In Advances in neural information processing systems.
3675–3683.

[14] Andrew Levy, Robert Platt, and Kate Saenko. 2017. Hierarchical actor-critic.

arXiv preprint arXiv:1712.00948 (2017).

[15] Alexander C Li, Carlos Florensa, Ignasi Clavera, and Pieter Abbeel. 2019. Sub-
policy Adaptation for Hierarchical Reinforcement Learning. arXiv preprint
arXiv:1906.05862 (2019).

Inter-Level Cooperation in Hierarchical Reinforcement Learning

Manuscript submitted to AAMAS, 2020

[16] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,
Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with
deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).

[17] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor
Mordatch. 2017. Multi-agent actor-critic for mixed cooperative-competitive
environments. In Advances in Neural Information Processing Systems. 6379–6390.
[18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg
Ostrovski, et al. 2015. Human-level control through deep reinforcement learning.
Nature 518, 7540 (2015), 529.

[19] Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. 2018. Near-
Optimal Representation Learning for Hierarchical Reinforcement Learning. CoRR
abs/1810.01257 (2018). arXiv:1810.01257 http://arxiv.org/abs/1810.01257
[20] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. 2018. Data-
efficient hierarchical reinforcement learning. In Advances in Neural Information
Processing Systems. 3303–3313.

[21] Ronald Parr and Stuart J Russell. 1998. Reinforcement learning with hierarchies

of machines. In Advances in neural information processing systems. 1043–1049.

[22] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
Martin Riedmiller. 2014. Deterministic policy gradient algorithms. In ICML.
[23] Dipti Srinivasan. 2010. Innovations in Multi-Agent Systems and Application–1.

Vol. 310. Springer.

[24] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2016. Learning Mul-
tiagent Communication with Backpropagation. CoRR abs/1605.07736 (2016).
arXiv:1605.07736 http://arxiv.org/abs/1605.07736

[25] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Viní-
cius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z.
Leibo, Karl Tuyls, and Thore Graepel. 2017. Value-Decomposition Networks For

Cooperative Multi-Agent Learning. CoRR abs/1706.05296 (2017). arXiv:1706.05296
http://arxiv.org/abs/1706.05296

[26] Richard S Sutton. 1988. Learning to predict by the methods of temporal differences.

Machine learning 3, 1 (1988), 9–44.

[27] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. 2000.
Policy gradient methods for reinforcement learning with function approximation.
In Advances in neural information processing systems. 1057–1063.

[28] Richard S Sutton, Doina Precup, and Satinder Singh. 1999. Between MDPs and
semi-MDPs: A framework for temporal abstraction in reinforcement learning.
Artificial intelligence 112, 1-2 (1999), 181–211.

[29] Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Kor-
jus, Juhan Aru, Jaan Aru, and Raul Vicente. 2017. Multiagent cooperation and
competition with deep reinforcement learning. PloS one 12, 4 (2017), e0172395.
[30] Ming Tan. 1993. Multi-agent reinforcement learning: Independent vs. cooperative
agents. In Proceedings of the tenth international conference on machine learning.
330–337.

[31] Emanuel Todorov, Tom Erez, and Yuval Tassa. 2012. Mujoco: A physics engine
for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent
Robots and Systems. IEEE, 5026–5033.

[32] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max
Jaderberg, David Silver, and Koray Kavukcuoglu. 2017. Feudal networks for
hierarchical reinforcement learning. arXiv preprint arXiv:1703.01161 (2017).
[33] Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine learning

8, 3-4 (1992), 279–292.

[34] Michael Weinberg and Jeffrey S Rosenschein. 2004. Best-response multiagent
learning in non-stationary environments. In Proceedings of the Third International
Joint Conference on Autonomous Agents and Multiagent Systems-Volume 2. IEEE
Computer Society, 506–513.

