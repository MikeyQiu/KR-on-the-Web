8
1
0
2
 
v
o
N
 
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
5
4
5
0
.
2
0
8
1
:
v
i
X
r
a

Mapping Images to Scene Graphs with
Permutation-Invariant Structured Prediction

Roei Herzig∗
Tel Aviv University
roeiherzig@mail.tau.ac.il

Moshiko Raboh∗
Tel Aviv University
mosheraboh@mail.tau.ac.il

Gal Chechik
Bar-Ilan University, NVIDIA Research
gal.chechik@biu.ac.il

Jonathan Berant
Tel Aviv University, AI2
joberant@cs.tau.ac.il

Amir Globerson
Tel Aviv University
gamir@post.tau.ac.il

Abstract

Machine understanding of complex images is a key goal of artiﬁcial intelligence.
One challenge underlying this task is that visual scenes contain multiple inter-
related objects, and that global context plays an important role in interpreting
the scene. A natural modeling framework for capturing such effects is structured
prediction, which optimizes over complex labels, while modeling within-label
interactions. However, it is unclear what principles should guide the design of a
structured prediction model that utilizes the power of deep learning components.
Here we propose a design principle for such architectures that follows from a
natural requirement of permutation invariance. We prove a necessary and sufﬁ-
cient characterization for architectures that follow this invariance, and discuss its
implication on model design. Finally, we show that the resulting model achieves
new state-of-the-art results on the Visual Genome scene-graph labeling benchmark,
outperforming all recent approaches.

1

Introduction

Understanding the semantics of a complex visual scene is a fundamental problem in machine
perception. It often requires recognizing multiple objects in a scene, together with their spatial and
functional relations. The set of objects and relations is sometimes represented as a graph, connecting
objects (nodes) with their relations (edges) and is known as a scene graph (Figure 1). Scene graphs
provide a compact representation of the semantics of an image, and can be useful for semantic-level
interpretation and reasoning about a visual scene Johnson et al. (2018). Scene-graph prediction is the
problem of inferring the joint set of objects and their relations in a visual scene.

Since objects and relations are inter-dependent (e.g., a person and chair are more likely to be in relation
“sitting on” than “eating”), a scene graph predictor should capture this dependence in order to improve
prediction accuracy. This goal is a special case of a more general problem, namely, inferring multiple
inter-dependent labels, which is the research focus of the ﬁeld of structured prediction. Structured
prediction has attracted considerable attention because it applies to many learning problems and

∗Equal Contribution.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Figure 1: An image and its scene graph from the Visual Genome dataset (Krishna et al., 2017). The scene graph
captures the entities in the image (nodes, blue circles) like dog and their relations (edges, red circles) like (cid:10)hat,
on, dog(cid:11).

poses unique theoretical and algorithmic challenges (e.g., see Belanger et al., 2017; Chen et al., 2015;
Taskar et al., 2004). It is therefore a natural approach for predicting scene graphs from images.

Structured prediction models typically deﬁne a score function s(x, y) that quantiﬁes how well a
label assignment y is compatible with an input x. In the case of understanding complex visual
scenes, x is an image, and y is a complex label containing the labels of objects detected in an image
and the labels of their relations. In this setup, the inference task amounts to ﬁnding the label that
maximizes the compatibility score y∗ = arg maxy s(x, y). This score-based approach separates a
scoring component – implemented by a parametric model, from an optimization component – aimed
at ﬁnding a label that maximizes that score. Unfortunately, for a general scoring function s(·), the
space of possible label assignments grows exponentially with input size. For instance, for scene
graphs the set of possible object label assignments is too large even for relatively simple images,
since the vocabulary of candidate objects may contain thousands of objects. As a result, inferring the
label assignment that maximizes a scoring function is computationally hard in the general case.

An alternative approach to score-based methods is to map an input x to a structured output y with
a “black box" neural network, without explicitly deﬁning a score function. This raises a natural
question: what is the right architecture for such a network? Here we take an axiomatic approach and
argue that one important property such networks should satisfy is invariance to a particular type of
input permutation. We then prove that this invariance is equivalent to imposing certain structural
constraints on the architecture of the network, and describe architectures that satisfy these constraints.

To evaluate our approach, we ﬁrst demonstrate on a synthetic dataset that respecting permutation
invariance is important, because models that violate this invariance need more training data, despite
having a comparable model size. Then, we tackle the problem of scene graph generation. We describe
a model that satisﬁes the permutation invariance property, and show that it achieves state-of-the-art
results on the competitive Visual Genome benchmark (Krishna et al., 2017), demonstrating the power
of our new design principle.

In summary, the novel contributions of this paper are: a) Deriving sufﬁcient and necessary conditions
for graph-permutation invariance in deep structured prediction architectures. b) Empirically demon-
strating the beneﬁt of graph-permutation invariance. c) Developing a state-of-the-art model for scene
graph prediction on a large dataset of complex visual scenes.

2 Structured Prediction

Scored-based methods in structured prediction deﬁne a function s(x, y) that quantiﬁes the degree
to which y is compatible with x, and infer a label by maximizing s(x, y) (e.g., see Belanger et al.,
2017; Chen et al., 2015; Lafferty et al., 2001; Meshi et al., 2010; Taskar et al., 2004). Most score
functions previously used decompose as a sum over simpler functions, s(x, y) = (cid:80)
i fi(x, y), making
it possible to optimize maxy fi(x, y) efﬁciently. This local maximization forms the basic building
block of algorithms for approximately maximizing s(x, y). One way to decompose the score function
is to restrict each fi(x, y) to depend only on a small subset of the y variables.

The renewed interest in deep learning led to efforts to integrate deep networks with structured predic-
tion, including modeling the fi functions as deep networks. In this context, the most widely-used
score functions are singleton fi(yi, x) and pairwise fij(yi, yj, x). The early work taking this approach
used a two-stage architecture, learning the local scores independently of the structured prediction

2

Figure 2: Left: Graph permutation invariance. A graph labeling function F is graph permutation invariant
(GPI) if permuting the node features maintains the output. Right: a schematic representation of the GPI
architecture in Theorem 1. Singleton features zi are omitted for simplicity. (a) First, the features zi,j are
processed element-wise by φ. (b) Features are summed to create a vector si, which is concatenated with zi. (c)
A representation of the entire graph is created by applying α n times and summing the created vector. (d) The
graph representation is then ﬁnally processed by ρ together with zk.

goal (Chen et al., 2014; Farabet et al., 2013). Later studies considered end-to-end architectures where
the inference algorithm is part of the computation graph (Chen et al., 2015; Pei et al., 2015; Schwing
& Urtasun, 2015; Zheng et al., 2015). Recent studies go beyond pairwise scores, also modelling
global factors (Belanger et al., 2017; Gygli et al., 2017).

Score-based methods provide several advantages. First, they allow intuitive speciﬁcation of local
dependencies between labels and how these translate to global dependencies. Second, for linear
score functions, the learning problem has natural convex surrogates Lafferty et al. (2001); Taskar
et al. (2004). Third, inference in large label spaces is sometimes possible via exact algorithms or
empirically accurate approximations. However, with the advent of deep scoring functions s(x, y; w),
learning is no longer convex. Thus, it is worthwhile to rethink the architecture of structured prediction
models, and consider models that map inputs x to outputs y directly without explicitly maximizing a
score function. We would like these models to enjoy the expressivity and predictive power of neural
networks, while maintaining the ability to specify local dependencies between labels in a ﬂexible
manner. In the next section, we present such an approach and consider a natural question: what
should be the properties of a deep neural network used for structured prediction.

3 Permutation-Invariant Structured Prediction

In what follows we deﬁne the permutation-invariance property for structured prediction models, and
argue that permutation invariance is a natural principle for designing their architecture.

We ﬁrst introduce our notation. We focus on structures with pairwise interactions, because they are
simpler in terms of notation and are sufﬁcient for describing the structure in many problems. We
denote a structured label by y = [y1, . . . , yn]. In a score-based approach, the score is deﬁned via a
set of singleton scores fi(yi, x) and pairwise scores fij(yi, yj, x), where the overall score s(x, y) is
the sum of these scores. For brevity, we denote fij = fij(yi, yj, x) and fi = fi(yi, x). An inference
algorithm takes as input the local scores fi, fij and outputs an assignment that maximizes s(x, y).
We can thus view inference as a black-box that takes node-dependent and edge-dependent inputs
(i.e., the scores fi, fij) and returns a label y, even without an explicit score function s(x, y). While
numerous inference algorithms exist for this setup, including belief propagation (BP) and mean ﬁeld,
here we develop a framework for a deep labeling algorithm (we avoid the term “inference” since the
algorithm does not explicitly maximize a score function). Such an algorithm will be a black-box,
taking the f functions as input and the labels y1, . . . , yn as output. We next ask what architecture
such an algorithm should have.

We follow with several deﬁnitions. A graph labeling function F : (V, E) → Y is a function whose
input is an ordered set of node features V = [z1, . . . , zn] and an ordered set of edge features
E = [z1,2 . . . , zi,j, . . . , zn,n−1]. For example, zi can be the array of values fi, and zi,j can be
the table of values fi,j. Assume zi ∈ Rd and zi,j ∈ Re. The output of F is a set of node labels
y = [y1, . . . , yn]. Thus, algorithms such as BP are graph labeling functions. However, graph labeling
functions do not necessarily maximize a score function. We denote the joint set of node features and
edge features by z (i.e., a set of n + n(n − 1) = n2 vectors). In Section 3.1 we discuss extensions to
this case where only a subset of the edges is available.

3

1, y∗

2, y∗

1, y∗

2, y∗

A natural requirement is that the function F produces the same result when given the same features,
up to a permutation of the input. For example, consider a label space with three variables y1, y2, y3,
and assume that F takes as input z = (z1, z2, z3, z12, z13, z23) = (f1, f2, f3, f12, f13, f23), and
outputs a label y = (y∗
3). When F is given an input that is permuted in a consistent way, say,
z(cid:48) = (f2, f1, f3, f21, f23, f13), this deﬁnes exactly the same input. Hence, the output should still be
y = (y∗
3). Most inference algorithms, including BP and mean ﬁeld, satisfy this symmetry
requirement by design, but this property is not guaranteed in general in a deep model. Here, our
goal is to design a deep learning black-box, and hence we wish to guarantee invariance to input
permutations. A black-box that violates this invariance “wastes” capacity on learning it at training
time, which increases sample complexity, as shown in Sec. 5.1. We proceed to formally deﬁne the
permutation invariance property.
Deﬁnition 1. Let z be a set of node features and edge features, and let σ be a permutation of
{1, . . . , n}. We deﬁne σ(z) to be a new set of node and edge features given by [σ(z)]i = zσ(i) and
[σ(z)]i,j = zσ(i),σ(j).

We also use the notation σ([y1, . . . , yn]) = [yσ(1), . . . , yσ(n)] for permuting the labels. Namely, σ
applied to a set of labels yields the same labels, only permuted by σ. Be aware that applying σ to
the input features is different from permuting labels, because edge input features must permuted in a
way that is consistent with permuting node input features. We now provide our key deﬁnition of a
function whose output is invariant to permutations of the input. See Figure 2 (left).
Deﬁnition 2. A graph labeling function F is said to be graph-permutation invariant (GPI), if for
all permutations σ of {1, . . . , n} and for all z it satisﬁes: F(σ(z)) = σ(F(z)).

3.1 Characterizing Permutation Invariance

Motivated by the above discussion, we ask: what structure is necessary and sufﬁcient to guarantee
that F is GPI? Note that a function F takes as input an ordered set z. Therefore its output on z
could certainly differ from its output on σ(z). To achieve permutation invariance, F should contain
certain symmetries. For instance, one permutation invariant architecture could be to deﬁne yi = g(zi)
for any function g, but this architecture is too restrictive and does not cover all permutation invariant
functions. Theorem 1 below provides a complete characterization (see Figure 2 for the corresponding
architecture). Intuitively, the architecture in Theorem 1 is such that it can aggregate information from
the entire graph, and do so in a permutation invariant manner.
Theorem 1. Let F be a graph labeling function. Then F is graph-permutation invariant if and only
if there exist functions α, ρ, φ such that for all k = 1, . . . , n:

[F(z)]k = ρ

zk,

α

zi,

φ(zi, zi,j, zj)



 ,

(1)









n
(cid:88)

i=1

(cid:88)

j(cid:54)=i

where φ : R2d+e → RL, α : Rd+L → RW and ρ : RW +d → R.

Proof. First, we show that any F satisfying the conditions of Theorem 1 is GPI. Namely, for any
permutation σ, [F(σ(z))]k = [F(z)]σ(k). To see this, write [F(σ(z))]k using Eq. 1 and Deﬁnition 1:

[F(σ(z))]k = ρ(zσ(k),

α(zσ(i),

φ(zσ(i), zσ(i),σ(j), zσ(j)))).

(2)

(cid:88)

i

(cid:88)

j(cid:54)=i

The second argument of ρ above is invariant under σ, because it is a sum over nodes and their
neighbors, which is invariant under permutation. Thus Eq. 2 is equal to:

ρ(zσ(k),

α(zi,

φ(zi, zi,j, zj))) = [F(z)]σ(k)

(cid:88)

i

(cid:88)

j(cid:54)=i

where equality follows from Eq. 1. We thus proved that Eq. 1 implies graph permutation invariance.

Next, we prove that any given GPI function F0 can be expressed as a function F in Eq. 1. Namely,
we show how to deﬁne φ, α and ρ that can implement F0. Note that in this direction of the proof the
function F0 is a black-box. Namely, we only know that it is GPI, but do not assume anything else
about its implementation.

4

The key idea is to construct φ, α such that the second argument of ρ in Eq. 1 contains the information
about all the graph features z. Then, the function ρ corresponds to an application of F0 to this
representation, followed by extracting the label yk. To simplify notation assume edge features are
scalar (e = 1). The extension to vectors is simple, but involves more indexing.

We assume WLOG that the black-box function F0 is a function only of the pairwise features zi,j
(otherwise, we can always augment the pairwise features with the singleton features). Since zi,j ∈ R
we use a matrix Rn,n to denote all the pairwise features.

Finally, we assume that our implementation of F0 will take additional node features zk such that no
two nodes have the same feature (i.e., the features identify the node).

Our goal is thus to show that there exist functions α, φ, ρ such that the function in Eq. 2 applied to Z
yields the same labels as F0(Z).

Let H be a hash function with L buckets mapping node features zi to an index (bucket). Assume
that H is perfect (this can be achieved for a large enough L). Deﬁne φ to map the pairwise
features to a vector of size L. Let 1 [j] be a one-hot vector of dimension RL, with one in the
jth coordinate. Recall that we consider scalar zi,j so that φ is indeed in RL, and deﬁne φ as:
φ(zi, zi,j, zj) = 1 [H(zj)] zi,j, i.e., φ “stores” zi,j in the unique bucket for node j.
Let si = (cid:80)
zi,j ∈E φ(zi, zi,j, zj) be the second argument of α in Eq. 1 (si ∈ RL). Then, since all
zj are distinct, si stores all the pairwise features for neighbors of i in unique positions within its
L coordinates. Since si(H(zk)) contains the feature zi,k whereas sj(H(zk)) contains the feature
zj,k, we cannot simply sum the si, since we would lose the information of which edges the features
originated from. Instead, we deﬁne α to map si to RL×L such that each feature is mapped to a
distinct location. Formally:

α(zi, si) = 1 [H(zi)] sT
i
α outputs a matrix that is all zeros except for the features corresponding to node i that are stored in
row H(zi). The matrix M = (cid:80)
i α(zi, si) (namely, the second argument of ρ in Eq. 1) is a matrix
with all the edge features in the graph including the graph structure.

(3)

.

To complete the construction we set ρ to have the same outcome as F0. We ﬁrst discard rows and
columns in M that do not correspond to original nodes (reducing M to dimension n × n). Then, we
use the reduced matrix as the input z to the black-box F0.

Assume for simplicity that M does not need to be contracted (this merely introduces another indexing
step). Then M corresponds to the original matrix Z of pairwise features, with both rows and
columns permuted according to H. We will thus use M as input to the function F0. Since F0 is
GPI, this means that the label for node k will be given by F0(M ) in position H(zk). Thus we set
ρ(zk, M ) = [F0(M )]H(zk), and by the argument above this equals [F0(Z)]k, implying that the
above α, φ and ρ indeed implement F0.

Extension to general graphs So far, we discussed complete graphs, where edges correspond
to valid feature pairs. However, many graphs of interest might be incomplete. For example, an
n-variable chain graph in sequence labeling has only n − 1 edges. For such graphs, the input to F
would not contain all zi,j pairs but rather only features corresponding to valid edges of the graph, and
we are only interested in invariances that preserve the graph structure, namely, the automorphisms
of the graph. Thus, the desired invariance is that σ(F(z)) = F(σ(z)), where σ is not an arbitrary
permutation but an automorphism. It is easy to see that a simple variant of Theorem 1 holds in
this case. All we need to do is replace in Eq. 2 the sum (cid:80)
j∈N (i), where N (i) are the
neighbors of node i in the graph. The arguments are then similar to the proof above.

j(cid:54)=i with (cid:80)

Implications of Theorem 1 Our result has interesting implications for deep structured prediction.
First, it highlights that the fact that the architecture “collects” information from all different edges
of the graph, in an invariant fashion via the α, φ functions. Speciﬁcally, the functions φ (after
summation) aggregate all the features around a given node, and then α (after summation) can
collect them. Thus, these functions can provide a summary of the entire graph that is sufﬁcient for
downstream algorithms. This is different from one round of message passing algorithms which would
not be sufﬁcient for collecting global graph information. Note that the dimensions of φ, α may need
to be large to aggregate all graph information (e.g., by hashing all the features as in the proof of
Theorem 1), but the architecture itself can be shallow.

5

Second, the architecture is parallelizable, as all φ functions can be applied simultaneously. This is in
contrast to recurrent models Zellers et al. (2017) which are harder to parallelize and are thus slower
in practice.

Finally, the theorem suggests several common architectural structures that can be used within GPI.
We brieﬂy mention two of these. 1) Attention: Attention is a powerful component in deep learning
architectures (Bahdanau et al., 2015), but most inference algorithms do not use attention. Intuitively,
in attention each node i aggregates features of neighbors through a weighted sum, where the weight
is a function of the neighbor’s relevance. For example, the label of an entity in an image may depend
more strongly on entities that are spatially closer. Attention can be naturally implemented in our
GPI characterization, and we provide a full derivation for this implementation in the appendix. It
plays a key role in our scene graph model described below. 2) RNNs: Because GPI functions are
closed under composition, for any GPI function F we can run F iteratively by providing the output
of one step of F as part of the input to the next step and maintain GPI. This results in a recurrent
architecture, which we use in our scene graph model.

4 Related Work

The concept of architectural invariance was recently proposed in DEEPSETS (Zaheer et al., 2017).
The invariance we consider is much less restrictive: the architecture does not need to be invariant
to all permutations of singleton and pairwise features, just those consistent with a graph re-labeling.
This characterization results in a substantially different set of possible architectures.

Deep structured prediction. There has been signiﬁcant recent interest in extending deep learning
to structured prediction tasks. Much of this work has been on semantic segmentation, where
convolutional networks (Shelhamer et al., 2017) became a standard approach for obtaining “singleton
scores” and various approaches were proposed for adding structure on top. Most of these approaches
used variants of message passing algorithms, unrolled into a computation graph (Xu et al., 2017).
Some studies parameterized parts of the message passing algorithm and learned its parameters (Lin
et al., 2015). Recently, gradient descent has also been used for maximizing score functions (Belanger
et al., 2017; Gygli et al., 2017). An alternative to deep structured prediction is greedy decoding,
inferring each label at a time based on previous labels. This approach has been popular in sequence-
based applications (e.g., parsing (Chen & Manning, 2014)), relying on the sequential structure of the
input, where BiLSTMs are effectively applied. Another related line of work is applying deep learning
to graph-based problems, such as TSP (Bello et al., 2016; Gilmer et al., 2017; Khalil et al., 2017).
Clearly, the notion of graph invariance is important in these, as highlighted in (Gilmer et al., 2017).
They however do not specify a general architecture that satisﬁes invariance as we do here, and in
fact focus on message passing architectures, which we strictly generalize. Furthermore, our focus is
on the more general problem of structured prediction, rather than speciﬁc graph-based optimization
problems.

Scene graph prediction. Extracting scene graphs from images provides a semantic representation
that can later be used for reasoning, question answering, and image retrieval (Johnson et al., 2015;
Lu et al., 2016; Raposo et al., 2017). It is at the forefront of machine vision research, integrating
challenges like object detection, action recognition and detection of human-object interactions (Liao
et al., 2016; Plummer et al., 2017). Prior work on scene graph predictions used neural message
passing algorithms (Xu et al., 2017) as well as prior knowledge in the form of word embeddings
(Lu et al., 2016). Other work suggested to predict graphs directly from pixels in an end-to-end
manner Newell & Deng (2017). NeuralMotif (Zellers et al., 2017), currently the state-of-the-art
model for scene graph prediction on Visual Genome, employs an RNN that provides global context
by sequentially reading the independent predictions for each entity and relation and then reﬁnes those
predictions. The NEURALMOTIF model maintains GPI by ﬁxing the order in which the RNN reads
its inputs and thus only a single order is allowed. However, this ﬁxed order is not guaranteed to be
optimal.

5 Experimental Evaluation

We empirically evaluate the beneﬁt of GPI architectures. First, using a synthetic graph-labeling task,
and then for the problem of mapping images to scene graphs.

6

Figure 3: Accuracy as a function of sample size for graph labeling. Right is a zoomed in version of left.

5.1 Synthetic Graph Labeling

We start with studying GPI on a synthetic problem, deﬁned as follows. An input graph G = (V, E)
is given, where each node i ∈ V is assigned to one of K sets. The set for node i is denoted by
Γ(i). The goal is to compute for each node the number of neighbors that belong to the same set.
Namely, the label of a node is yi = (cid:80)
j∈N (i) 1[Γ(i) = Γ(j)]. We generated random graphs with 10
nodes (larger graphs produced similar results) by sampling each edge independently and uniformly,
and sampling Γ(i) for every node uniformly from {1, . . . , K}. The node features zi ∈ {0, 1}K are
one-hot vectors of Γ(i) and the edge features zi,j ∈ {0, 1} indicate whether ij ∈ E. We compare two
standard non-GPI architectures and one GPI architecture: (a) A GPI-architecture for graph prediction,
described in detail in Section 5.2. We used the basic version without attention and RNN. (b) LSTM:
We replace (cid:80) φ(·) and (cid:80) α(·), which perform aggregation in Theorem 1 with two LSTMs with
a state size of 200 that read their input in random order. (c) A fully-connected (FC) feed-forward
network with 2 hidden layers of 1000 nodes each. The input to the fully connected model is a
concatenation of all node and pairwise features. The output is all node predictions. The focus of the
experiment is to study sample complexity. Therefore, for a fair comparison, we use the same number
of parameters for all models.

Figure 3, shows the results, demonstrating that GPI requires far fewer samples to converge to the
correct solution. This illustrates the advantage of an architecture with the correct inductive bias for
the problem.

5.2 Scene-Graph Classiﬁcation

We evaluate the GPI approach on the motivating task of this paper, inferring scene graphs from
images (Figure 1). In this problem, the input is an image annotated with a set of bounding boxes for
the entities in the image.2 The goal is to label each bounding box with the correct entity category and
every pair of entities with their relation, such that they form a coherent scene graph.

We begin by describing our Scene Graph Predictor (SGP) model. We aim to predict two types of
variables. The ﬁrst is entity variables [y1, . . . , yn] for all bounding boxes. Each yi can take one of
L values (e.g., “dog”, “man”). The second is relation variables [yn+1, . . . , yn2] for every pair of
bounding boxes. Each such yj can take one of R values (e.g., “on”, “near”). Our graph connects
variables that are expected to be inter-related. It contains two types of edges: 1) entity-entity edge
connecting every two entity variables (yi and yj for 1 ≤ i (cid:54)= j ≤ n. 2) entity-relation edges
connecting every relation variable yk (where k > n) to its two entity variables. Thus, our graph is not
a complete graph and our goal is to design an architecture that will be invariant to any automorphism
of the graph, such as permutations of the entity variables.

For the input features z, we used the features learned by the baseline model from Zellers et al.
(2017).3 Speciﬁcally, the entity features zi included (1) The conﬁdence probabilities of all entities
for yi as learned by the baseline model. (2) Bounding box information given as (left, bottom,
width, height); (3) The number of smaller entities (also bigger); (4) The number of entities to
the left, right, above and below. (5) The number of entities with higher and with lower conﬁdence;

2For simplicity, we focus on the task where boxes are given.
3The baseline does not use any LSTM or context, and is thus unrelated to the main contribution of Zellers

et al. (2017).

7

Constrained Evaluation

SGCls

PredCls

Unconstrained Evaluation
SGCls

PredCls

R@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100

Lu et al., 2016 (Lu et al., 2016)
Xu et al., 2017 (Xu et al., 2017)
Pixel2Graph (Newell & Deng, 2017)
Graph R-CNN (Yang et al., 2018)
Neural Motifs (Zellers et al., 2017)
Baseline (Zellers et al., 2017)
No Attention
Neighbor Attention
Linguistic

11.8
21.7
-
29.6
35.8
34.6
35.3
35.7
36.5

14.1
24.4
-
31.6
36.5
35.3
37.2
38.5
38.8

35.0
44.8
-
54.2
65.2
63.7
64.5
64.6
65.1

27.9
53.0
-
59.1
67.1
65.6
66.3
66.6
66.9

-
-
26.5
-
44.5
43.4
44.1
44.7
45.5

-
-
30.0
-
47.7
46.6
48.5
49.9
50.8

-
-
68.0
-
81.1
78.8
79.7
80.0
80.8

-
-
75.2
-
88.3
85.9
86.7
87.1
88.2

Table 1: Test set results for graph-constrained evaluation (i.e., the returned triplets must be consistent with a
scene graph) and for unconstrained evaluation (triplets need not be consistent with a scene graph).

(6) For the linguistic model only: word embedding of the most probable class. Word vectors were
learned with GLOVE from the ground-truth captions of Visual Genome.
Similarly, the relation features zj ∈ RR contained the probabilities of relation entities for the relation
j. For the Linguistic model, these features were extended to include word embedding of the most
probable class. For entity-entity pairwise features zi,j, we use the relation probability for each pair.
Because the output of SGP are probability distributions over entities and relations, we use them as an
the input z to SGP, once again in a recurrent manner and maintain GPI.

We next describe the main components of the GPI architecture. First, we focus on the parts that
output the entity labels. φent is the network that integrates features for two entity variables yi
and yj. It simply takes zi, zj and zi,j as input, and outputs a vector of dimension n1. Next, the
network αent takes as input the outputs of φent for all neighbors of an entity, and uses the attention
mechanism described above to output a vector of dimension n2. Finally, the ρent network takes these
n2 dimensional vectors and outputs L logits predicting the entity value. The ρrel network takes as
input the αent representation of the two entities, as well as zi,j and transforms the output into R
logits. See appendix for speciﬁc network architectures.

5.2.1 Experimental Setup and Results

Dataset. We evaluated our approach on Visual Genome (VG) (Krishna et al., 2017), a dataset with
108,077 images annotated with bounding boxes, entities and relations. On average, images have 12
entities and 7 relations per image. For a proper comparison with previous results (Newell & Deng,
2017; Xu et al., 2017; Zellers et al., 2017), we used the data from (Xu et al., 2017), including the
train and test splits. For evaluation, we used the same 150 entities and 50 relations as in (Newell
& Deng, 2017; Xu et al., 2017; Zellers et al., 2017). To tune hyper-parameters, we also split the
training data into two by randomly selecting 5K examples, resulting in a ﬁnal 70K/5K/32K split for
train/validation/test sets.

Training. All networks were trained using Adam (Kingma & Ba, 2014) with batch size 20. Hyper-
parameter values below were chosen based on the validation set. The SGP loss function was the sum
of cross-entropy losses over all entities and relations in the image. In the loss, we penalized entities
4 times more strongly than relations, and penalized negative relations 10 times more weakly than
positive relations.

Evaluation.
In (Xu et al., 2017) three different evaluation settings were considered. Here we
focus on two of these: (1) SGCls: Given ground-truth bounding boxes for entities, predict all entity
categories and relations categories. (2) PredCls: Given bounding boxes annotated with entity labels,
predict all relations. Following (Lu et al., 2016), we used Recall@K as the evaluation metric. It
measures the fraction of correct ground-truth triplets that appear within the K most conﬁdent triplets
proposed by the model. Two evaluation protocols are used in the literature differing in whether they
enforce graph constraints over model predictions. The ﬁrst graph-constrained protocol requires that
the top-K triplets assign one consistent class per entity and relation. The second unconstrained
protocol does not enforce any such constraints. We report results on both protocols, following (Zellers
et al., 2017).

8

Figure 4: (a) An input image with bounding boxes from VG. (b) The ground-truth scene graph. (c) The
Baseline fails to recognize some entities (tail and tree) and relations (in front of instead of looking at). (d)
GPI:LINGUISTIC ﬁxes most incorrect LP predictions. (e) Window is the most signiﬁcant neighbor of Tree. (f)
The entity bird receives substantial attention, while Tree and building are less informative.

Models and baselines. We compare four variants of our GPI approach with the reported results
of four baselines that are currently the state-of-the-art on various scene graph prediction problems
(all models use the same data split and pre-processing as (Xu et al., 2017)): 1) LU ET AL., 2016
(LU ET AL., 2016): This work leverages word embeddings to ﬁne-tune the likelihood of predicted
relations. 2) XU ET AL, 2017 (XU ET AL., 2017): This model passes messages between entities and
relations, and iteratively reﬁnes the feature map used for prediction. 3) NEWELL & DENG, 2017
(NEWELL & DENG, 2017): The PIXEL2GRAPH model uses associative embeddings (Newell et al.,
2017) to produce a full graph from the image. 4) YANG ET AL., 2018 (YANG ET AL., 2018): The
GRAPH R-CNN model uses object-relation regularities to sparsify and reason over scene graphs. 5)
ZELLERS ET AL., 2017 (ZELLERS ET AL., 2017): The NEURALMOTIF method encodes global
context for capturing high-order motifs in scene graphs, and the BASELINE outputs the entities and
relations distributions without using the global context. The following variants of GPI were compared:
1) GPI: NO ATTENTION: Our GPI model, but with no attention mechanism. Instead, following
Theorem 1, we simply sum the features. 2) GPI: NEIGHBORATTENTION: Our GPI model, with
attention over neighbors features. 3) GPI: LINGUISTIC: Same as GPI: NEIGHBORATTENTION but
also concatenating the word embedding vector, as described above.

Results. Table 1 shows recall@50 and recall@100 for three variants of our approach, and compared
with ﬁve baselines. All GPI variants performs well, with LINGUISTIC outperforming all baselines
for SGCls and being comparable to the state-of-the-art model for PredCls. Note that PredCl is an
easier task, which makes less use of the structure, hence it is not surprising that GPI achieves similar
accuracy to Zellers et al. (2017). Figure 4 illustrates the model behavior. Predicting isolated labels
with zi (4c) mislabels several entities, but these are corrected at the ﬁnal output (4d). Figure 4e
shows that the system learned to attend more to nearby entities (the window and building are closer
to the tree), and 4f shows that stronger attention is learned for the class bird, presumably because it is
usually more informative than common classes like tree.

Implementation details. The φ and α networks were each implemented as a single fully-connected
(FC) layer with a 500-dimensional outputs. ρ was implemented as a FC network with 3 500-
dimensional hidden layers, with one 150-dimensional output for the entity probabilities, and one
51-dimensional output for relation probabilities. The attention mechanism was implemented as a
network like to φ and α, receiving the same inputs, but using the output scores for the attention . The
full code is available at https://github.com/shikorab/SceneGraph

6 Conclusion

We presented a deep learning approach to structured prediction, which constrains the architecture
to be invariant to structurally identical inputs. As in score-based methods, our approach relies on
pairwise features, capable of describing inter-label correlations, and thus inheriting the intuitive
aspect of score-based approaches. However, instead of maximizing a score function (which leads
to computationally-hard inference), we directly produce an output that is invariant to equivalent
representations of the pairwise terms.

9

This axiomatic approach to model architecture can be extended in many ways. For image labeling,
geometric invariances (shift or rotation) may be desired.
In other cases, invariance to feature
permutations may be desirable. We leave the derivation of the corresponding architectures to future
work. Finally, there may be cases where the invariant structure is unknown and should be discovered
from data, which is related to work on lifting graphical models Bui et al. (2013). It would be interesting
to explore algorithms that discover and use such symmetries for deep structured prediction.

This work was supported by the ISF Centers of Excellence grant, and by the Yandex Initiative in
Machine Learning. Work by GC was performed while at Google Brain Research.

Acknowledgements

References

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and

translate. In International Conference on Learning Representations (ICLR), 2015.

Belanger, David, Yang, Bishan, and McCallum, Andrew. End-to-end learning for structured prediction
energy networks. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70, pp. 429–439. PMLR, 2017.

Bello, Irwan, Pham, Hieu, Le, Quoc V, Norouzi, Mohammad, and Bengio, Samy. Neural combinato-

rial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Bui, Hung Hai, Huynh, Tuyen N., and Riedel, Sebastian. Automorphism groups of graphical models
and lifted variational inference. In Proceedings of the Twenty-Ninth Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’13, pp. 132–141, 2013.

Chen, Danqi and Manning, Christopher. A fast and accurate dependency parser using neural
networks. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 740–750, 2014.

Chen, Liang Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Se-
mantic image segmentation with deep convolutional nets and fully connected CRFs. In Proceedings
of the Second International Conference on Learning Representations, 2014.

Chen, Liang Chieh, Schwing, Alexander G, Yuille, Alan L, and Urtasun, Raquel. Learning deep

structured models. In Proc. ICML, 2015.

Farabet, Clement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical
features for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):
1915–1929, 2013.

Gilmer, Justin, Schoenholz, Samuel S, Riley, Patrick F, Vinyals, Oriol, and Dahl, George E. Neural

message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.

Gygli, Michael, Norouzi, Mohammad, and Angelova, Anelia. Deep value networks learn to evaluate
and iteratively reﬁne structured outputs. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine
Learning Research, pp. 1341–1351, International Convention Centre, Sydney, Australia, 2017.
PMLR.

Johnson, Justin, Krishna, Ranjay, Stark, Michael, Li, Li-Jia, Shamma, David A., Bernstein, Michael S.,
In Proc. Conf. Comput. Vision Pattern

Image retrieval using scene graphs.

and Li, Fei-Fei.
Recognition, pp. 3668–3678, 2015.

Johnson, Justin, Gupta, Agrim, and Fei-Fei, Li. Image generation from scene graphs. arXiv preprint

arXiv:1804.01622, 2018.

Khalil, Elias, Dai, Hanjun, Zhang, Yuyu, Dilkina, Bistra, and Song, Le. Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp.
6351–6361, 2017.

10

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint

arXiv: 1412.6980, abs/1412.6980, 2014.

Krishna, Ranjay, Zhu, Yuke, Groth, Oliver, Johnson, Justin, Hata, Kenji, Kravitz, Joshua, Chen,
Stephanie, Kalantidis, Yannis, Li, Li-Jia, Shamma, David A, et al. Visual genome: Connecting
International Journal of
language and vision using crowdsourced dense image annotations.
Computer Vision, 123(1):32–73, 2017.

Lafferty, J., McCallum, A., and Pereira, F. Conditional random ﬁelds: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning, pp. 282–289, 2001.

Liao, Wentong, Yang, Michael Ying, Ackermann, Hanno, and Rosenhahn, Bodo. On support relations

and semantic scene graphs. arXiv preprint arXiv:1609.05834, 2016.

Lin, Guosheng, Shen, Chunhua, Reid, Ian, and van den Hengel, Anton. Deeply learning the messages
in message passing inference. In Advances in Neural Information Processing Systems, pp. 361–369,
2015.

Lu, Cewu, Krishna, Ranjay, Bernstein, Michael S., and Li, Fei-Fei. Visual relationship detection with

language priors. In European Conf. Comput. Vision, pp. 852–869, 2016.

Meshi, O., Sontag, D., Jaakkola, T., and Globerson, A. Learning efﬁciently with approximate
In Proceedings of the 27th International Conference on Machine

inference via dual losses.
Learning, pp. 783–790, New York, NY, USA, 2010. ACM.

Newell, Alejandro and Deng, Jia. Pixels to graphs by associative embedding. In Advances in Neural
Information Processing Systems 30 (to appear), pp. 1172–1180. Curran Associates, Inc., 2017.

Newell, Alejandro, Huang, Zhiao, and Deng, Jia. Associative embedding: End-to-end learning for
joint detection and grouping. In Neural Inform. Process. Syst., pp. 2274–2284. Curran Associates,
Inc., 2017.

Pei, Wenzhe, Ge, Tao, and Chang, Baobao. An effective neural network model for graph-based de-
pendency parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computationa
Linguistics, pp. 313–322, 2015.

Plummer, Bryan A., Mallya, Arun, Cervantes, Christopher M., Hockenmaier, Julia, and Lazebnik,
Svetlana. Phrase localization and visual relationship detection with comprehensive image-language
cues. In ICCV, pp. 1946–1955, 2017.

Raposo, David, Santoro, Adam, Barrett, David, Pascanu, Razvan, Lillicrap, Timothy, and Battaglia,
Peter. Discovering objects and their relations from entangled scene representations. arXiv preprint
arXiv:1702.05068, 2017.

Schwing, Alexander G and Urtasun, Raquel. Fully connected deep structured networks. ArXiv

e-prints, 2015.

Shelhamer, Evan, Long, Jonathan, and Darrell, Trevor. Fully convolutional networks for semantic

segmentation. Proc. Conf. Comput. Vision Pattern Recognition, 39(4):640–651, 2017.

Taskar, B., Guestrin, C., and Koller, D. Max margin Markov networks. In Thrun, S., Saul, L., and
Schölkopf, B. (eds.), Advances in Neural Information Processing Systems 16, pp. 25–32. MIT
Press, Cambridge, MA, 2004.

Xu, Danfei, Zhu, Yuke, Choy, Christopher B., and Fei-Fei, Li. Scene Graph Generation by Iterative
Message Passing. In Proc. Conf. Comput. Vision Pattern Recognition, pp. 3097–3106, 2017.

Yang, Jianwei, Lu, Jiasen, Lee, Stefan, Batra, Dhruv, and Parikh, Devi. Graph R-CNN for scene

graph generation. In European Conf. Comput. Vision, pp. 690–706, 2018.

Zaheer, Manzil, Kottur, Satwik, Ravanbakhsh, Siamak, Poczos, Barnabas, Salakhutdinov, Ruslan R,
and Smola, Alexander J. Deep sets. In Advances in Neural Information Processing Systems 30, pp.
3394–3404. Curran Associates, Inc., 2017.

11

Zellers, Rowan, Yatskar, Mark, Thomson, Sam, and Choi, Yejin. Neural motifs: Scene graph parsing

with global context. arXiv preprint arXiv:1711.06640, abs/1711.06640, 2017.

Zheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav, Su, Zhizhong, Du,
Dalong, Huang, Chang, and Torr, Philip HS. Conditional random ﬁelds as recurrent neural networks.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 1529–1537, 2015.

7 Supplementary Material

This supplementary material includes: (1) Visual illustration of the proof of Theorem 1. (2) Explaining
how to integrate an attention mechanism in our GPI framework. (3) Additional evaluation method to
further analyze and compare our work with baselines.

7.1 Theorem 1: Illustration of Proof

Figure 5: Illustration of the proof of Theorem 1 using a speciﬁc construction example. Here H is a hash function
of size L = 5 such that H(1) = 1, H(3) = 2, H(2) = 4, G is a three-node input graph, and zi,j ∈ R are the
pairwise features (in purple) of G. (a) φ is applied to each zi,j. Each application yields a vector in R5. The three
dark yellow columns correspond to φ(z1,1), φ(z1,2) and φ(z1,3). Then, all vectors φ(zi,j) are summed over j
to obtain three si vectors. (b) α’s (blue matrices) are an outer product between 1 [H(zi)] and si resulting in a
matrix of zeros except one row. The dark blue matrix corresponds for α(z1, s1). (c) All α’s are summed to a
5 × 5 matrix, isomorphic to the original zi,j matrix.

7.2 Characterizing Permutation Invariance: Attention

Attention is a powerful component which naturally can be introduced into our GPI model. We now
show how attention can be introduced in our framework. Formally, we learn attention weights for the
neighbors j of a node i, which scale the features zi,j of that neighbor. We can also learn different
attention weights for individual features of each neighbor in a similar way.

Let wi,j ∈ R be an attention mask specifying the weight that node i gives to node j:

wi,j(zi, zi,j, zj) = eβ(zi,zi,j ,zj )/

eβ(zi,zi,t,zt)

(4)

(cid:88)

t

where β can be any scalar-valued function of its arguments (e.g., a dot product of zi and zj as in
standard attention models). To introduce attention we wish α ∈ Re to have the form of weighting
wi,j over neighboring feature vectors zi,j, namely, α = (cid:80)
To achieve this form we extend φ by a single entry, deﬁning φ ∈ Re+1 (namely we set
L = e + 1) as φ1:e(zi, zi,j, zj) = eβ(zi,zi,j ,zj )zi,j (here φ1:e are the ﬁrst e elements of φ)

j(cid:54)=i wi,jzi,j.

12

and φe+1(zi, zi,j; zj) = eβ(zi,zi,j ,zj ). We keep the deﬁnition of si = (cid:80)
we deﬁne α = si,1:e
si,e+1
over neighboring feature vectors zi,j:

j(cid:54)=i φ(zi, zi,j, zj). Next,
and substitute si and φ to obtain the desired form as attention weights wi,j

α(zi, si) =

si,1:e
si,e+1

(cid:88)

=

j(cid:54)=i

eβ(zi,zi,j ,zj )zi,j
(cid:80)
j(cid:54)=i eβ(zi,zi,j ,zj )

(cid:88)

=

j(cid:54)=i

wi,jzi,j

A similar approach can be applied over α and ρ to model attention over the outputs of α as well
(graph nodes).

7.3 Scene Graph Results

In the main paper, we described the results for the two prediction tasks: SGCls and PredCls, as
deﬁned in section 5.2.1: "Experimental Setup and Results". To further analyze our module, we
compare the best variant, GPI: LINGUISTIC, per relation to two baselines: (Lu et al., 2016) and Xu
et al. (2017). Table 2, speciﬁes the PredCls recall@5 of the 20-top frequent relation classes. The GPI
module performs better in almost all the relations classes.

RELATION

(LU ET AL., 2016)

(XU ET AL., 2017)

LINGUISTIC

99.71
98.03
80.38
82.47
98.47
85.16
31.85
49.19
61.50
79.35
28.64
31.74
26.09
8.45
54.08

ON
HAS
IN
OF
WEARING
NEAR
WITH
ABOVE
HOLDING
BEHIND
UNDER
SITTING ON
IN FRONT OF
ATTACHED TO
AT
HANGING FROM 0.0
OVER
FOR
RIDING

9.26
12.20
72.43

99.25
97.25
88.30
96.75
98.23
96.81
88.10
79.73
80.67
92.32
52.73
50.17
59.63
29.58
70.41
0.0
0.0
31.71
89.72

99.3
98.7
95.9
98.1
99.6
95.4
94.2
83.9
95.5
91.2
83.2
90.4
74.9
77.4
80.9
74.1
62.4
45.1
96.1

Table 2: Recall@5 of PredCls for the 20-top relations ranked by their frequency, as in (Xu et al., 2017)

13

8
1
0
2
 
v
o
N
 
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
5
4
5
0
.
2
0
8
1
:
v
i
X
r
a

Mapping Images to Scene Graphs with
Permutation-Invariant Structured Prediction

Roei Herzig∗
Tel Aviv University
roeiherzig@mail.tau.ac.il

Moshiko Raboh∗
Tel Aviv University
mosheraboh@mail.tau.ac.il

Gal Chechik
Bar-Ilan University, NVIDIA Research
gal.chechik@biu.ac.il

Jonathan Berant
Tel Aviv University, AI2
joberant@cs.tau.ac.il

Amir Globerson
Tel Aviv University
gamir@post.tau.ac.il

Abstract

Machine understanding of complex images is a key goal of artiﬁcial intelligence.
One challenge underlying this task is that visual scenes contain multiple inter-
related objects, and that global context plays an important role in interpreting
the scene. A natural modeling framework for capturing such effects is structured
prediction, which optimizes over complex labels, while modeling within-label
interactions. However, it is unclear what principles should guide the design of a
structured prediction model that utilizes the power of deep learning components.
Here we propose a design principle for such architectures that follows from a
natural requirement of permutation invariance. We prove a necessary and sufﬁ-
cient characterization for architectures that follow this invariance, and discuss its
implication on model design. Finally, we show that the resulting model achieves
new state-of-the-art results on the Visual Genome scene-graph labeling benchmark,
outperforming all recent approaches.

1

Introduction

Understanding the semantics of a complex visual scene is a fundamental problem in machine
perception. It often requires recognizing multiple objects in a scene, together with their spatial and
functional relations. The set of objects and relations is sometimes represented as a graph, connecting
objects (nodes) with their relations (edges) and is known as a scene graph (Figure 1). Scene graphs
provide a compact representation of the semantics of an image, and can be useful for semantic-level
interpretation and reasoning about a visual scene Johnson et al. (2018). Scene-graph prediction is the
problem of inferring the joint set of objects and their relations in a visual scene.

Since objects and relations are inter-dependent (e.g., a person and chair are more likely to be in relation
“sitting on” than “eating”), a scene graph predictor should capture this dependence in order to improve
prediction accuracy. This goal is a special case of a more general problem, namely, inferring multiple
inter-dependent labels, which is the research focus of the ﬁeld of structured prediction. Structured
prediction has attracted considerable attention because it applies to many learning problems and

∗Equal Contribution.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Figure 1: An image and its scene graph from the Visual Genome dataset (Krishna et al., 2017). The scene graph
captures the entities in the image (nodes, blue circles) like dog and their relations (edges, red circles) like (cid:10)hat,
on, dog(cid:11).

poses unique theoretical and algorithmic challenges (e.g., see Belanger et al., 2017; Chen et al., 2015;
Taskar et al., 2004). It is therefore a natural approach for predicting scene graphs from images.

Structured prediction models typically deﬁne a score function s(x, y) that quantiﬁes how well a
label assignment y is compatible with an input x. In the case of understanding complex visual
scenes, x is an image, and y is a complex label containing the labels of objects detected in an image
and the labels of their relations. In this setup, the inference task amounts to ﬁnding the label that
maximizes the compatibility score y∗ = arg maxy s(x, y). This score-based approach separates a
scoring component – implemented by a parametric model, from an optimization component – aimed
at ﬁnding a label that maximizes that score. Unfortunately, for a general scoring function s(·), the
space of possible label assignments grows exponentially with input size. For instance, for scene
graphs the set of possible object label assignments is too large even for relatively simple images,
since the vocabulary of candidate objects may contain thousands of objects. As a result, inferring the
label assignment that maximizes a scoring function is computationally hard in the general case.

An alternative approach to score-based methods is to map an input x to a structured output y with
a “black box" neural network, without explicitly deﬁning a score function. This raises a natural
question: what is the right architecture for such a network? Here we take an axiomatic approach and
argue that one important property such networks should satisfy is invariance to a particular type of
input permutation. We then prove that this invariance is equivalent to imposing certain structural
constraints on the architecture of the network, and describe architectures that satisfy these constraints.

To evaluate our approach, we ﬁrst demonstrate on a synthetic dataset that respecting permutation
invariance is important, because models that violate this invariance need more training data, despite
having a comparable model size. Then, we tackle the problem of scene graph generation. We describe
a model that satisﬁes the permutation invariance property, and show that it achieves state-of-the-art
results on the competitive Visual Genome benchmark (Krishna et al., 2017), demonstrating the power
of our new design principle.

In summary, the novel contributions of this paper are: a) Deriving sufﬁcient and necessary conditions
for graph-permutation invariance in deep structured prediction architectures. b) Empirically demon-
strating the beneﬁt of graph-permutation invariance. c) Developing a state-of-the-art model for scene
graph prediction on a large dataset of complex visual scenes.

2 Structured Prediction

Scored-based methods in structured prediction deﬁne a function s(x, y) that quantiﬁes the degree
to which y is compatible with x, and infer a label by maximizing s(x, y) (e.g., see Belanger et al.,
2017; Chen et al., 2015; Lafferty et al., 2001; Meshi et al., 2010; Taskar et al., 2004). Most score
functions previously used decompose as a sum over simpler functions, s(x, y) = (cid:80)
i fi(x, y), making
it possible to optimize maxy fi(x, y) efﬁciently. This local maximization forms the basic building
block of algorithms for approximately maximizing s(x, y). One way to decompose the score function
is to restrict each fi(x, y) to depend only on a small subset of the y variables.

The renewed interest in deep learning led to efforts to integrate deep networks with structured predic-
tion, including modeling the fi functions as deep networks. In this context, the most widely-used
score functions are singleton fi(yi, x) and pairwise fij(yi, yj, x). The early work taking this approach
used a two-stage architecture, learning the local scores independently of the structured prediction

2

Figure 2: Left: Graph permutation invariance. A graph labeling function F is graph permutation invariant
(GPI) if permuting the node features maintains the output. Right: a schematic representation of the GPI
architecture in Theorem 1. Singleton features zi are omitted for simplicity. (a) First, the features zi,j are
processed element-wise by φ. (b) Features are summed to create a vector si, which is concatenated with zi. (c)
A representation of the entire graph is created by applying α n times and summing the created vector. (d) The
graph representation is then ﬁnally processed by ρ together with zk.

goal (Chen et al., 2014; Farabet et al., 2013). Later studies considered end-to-end architectures where
the inference algorithm is part of the computation graph (Chen et al., 2015; Pei et al., 2015; Schwing
& Urtasun, 2015; Zheng et al., 2015). Recent studies go beyond pairwise scores, also modelling
global factors (Belanger et al., 2017; Gygli et al., 2017).

Score-based methods provide several advantages. First, they allow intuitive speciﬁcation of local
dependencies between labels and how these translate to global dependencies. Second, for linear
score functions, the learning problem has natural convex surrogates Lafferty et al. (2001); Taskar
et al. (2004). Third, inference in large label spaces is sometimes possible via exact algorithms or
empirically accurate approximations. However, with the advent of deep scoring functions s(x, y; w),
learning is no longer convex. Thus, it is worthwhile to rethink the architecture of structured prediction
models, and consider models that map inputs x to outputs y directly without explicitly maximizing a
score function. We would like these models to enjoy the expressivity and predictive power of neural
networks, while maintaining the ability to specify local dependencies between labels in a ﬂexible
manner. In the next section, we present such an approach and consider a natural question: what
should be the properties of a deep neural network used for structured prediction.

3 Permutation-Invariant Structured Prediction

In what follows we deﬁne the permutation-invariance property for structured prediction models, and
argue that permutation invariance is a natural principle for designing their architecture.

We ﬁrst introduce our notation. We focus on structures with pairwise interactions, because they are
simpler in terms of notation and are sufﬁcient for describing the structure in many problems. We
denote a structured label by y = [y1, . . . , yn]. In a score-based approach, the score is deﬁned via a
set of singleton scores fi(yi, x) and pairwise scores fij(yi, yj, x), where the overall score s(x, y) is
the sum of these scores. For brevity, we denote fij = fij(yi, yj, x) and fi = fi(yi, x). An inference
algorithm takes as input the local scores fi, fij and outputs an assignment that maximizes s(x, y).
We can thus view inference as a black-box that takes node-dependent and edge-dependent inputs
(i.e., the scores fi, fij) and returns a label y, even without an explicit score function s(x, y). While
numerous inference algorithms exist for this setup, including belief propagation (BP) and mean ﬁeld,
here we develop a framework for a deep labeling algorithm (we avoid the term “inference” since the
algorithm does not explicitly maximize a score function). Such an algorithm will be a black-box,
taking the f functions as input and the labels y1, . . . , yn as output. We next ask what architecture
such an algorithm should have.

We follow with several deﬁnitions. A graph labeling function F : (V, E) → Y is a function whose
input is an ordered set of node features V = [z1, . . . , zn] and an ordered set of edge features
E = [z1,2 . . . , zi,j, . . . , zn,n−1]. For example, zi can be the array of values fi, and zi,j can be
the table of values fi,j. Assume zi ∈ Rd and zi,j ∈ Re. The output of F is a set of node labels
y = [y1, . . . , yn]. Thus, algorithms such as BP are graph labeling functions. However, graph labeling
functions do not necessarily maximize a score function. We denote the joint set of node features and
edge features by z (i.e., a set of n + n(n − 1) = n2 vectors). In Section 3.1 we discuss extensions to
this case where only a subset of the edges is available.

3

1, y∗

2, y∗

1, y∗

2, y∗

A natural requirement is that the function F produces the same result when given the same features,
up to a permutation of the input. For example, consider a label space with three variables y1, y2, y3,
and assume that F takes as input z = (z1, z2, z3, z12, z13, z23) = (f1, f2, f3, f12, f13, f23), and
outputs a label y = (y∗
3). When F is given an input that is permuted in a consistent way, say,
z(cid:48) = (f2, f1, f3, f21, f23, f13), this deﬁnes exactly the same input. Hence, the output should still be
y = (y∗
3). Most inference algorithms, including BP and mean ﬁeld, satisfy this symmetry
requirement by design, but this property is not guaranteed in general in a deep model. Here, our
goal is to design a deep learning black-box, and hence we wish to guarantee invariance to input
permutations. A black-box that violates this invariance “wastes” capacity on learning it at training
time, which increases sample complexity, as shown in Sec. 5.1. We proceed to formally deﬁne the
permutation invariance property.
Deﬁnition 1. Let z be a set of node features and edge features, and let σ be a permutation of
{1, . . . , n}. We deﬁne σ(z) to be a new set of node and edge features given by [σ(z)]i = zσ(i) and
[σ(z)]i,j = zσ(i),σ(j).

We also use the notation σ([y1, . . . , yn]) = [yσ(1), . . . , yσ(n)] for permuting the labels. Namely, σ
applied to a set of labels yields the same labels, only permuted by σ. Be aware that applying σ to
the input features is different from permuting labels, because edge input features must permuted in a
way that is consistent with permuting node input features. We now provide our key deﬁnition of a
function whose output is invariant to permutations of the input. See Figure 2 (left).
Deﬁnition 2. A graph labeling function F is said to be graph-permutation invariant (GPI), if for
all permutations σ of {1, . . . , n} and for all z it satisﬁes: F(σ(z)) = σ(F(z)).

3.1 Characterizing Permutation Invariance

Motivated by the above discussion, we ask: what structure is necessary and sufﬁcient to guarantee
that F is GPI? Note that a function F takes as input an ordered set z. Therefore its output on z
could certainly differ from its output on σ(z). To achieve permutation invariance, F should contain
certain symmetries. For instance, one permutation invariant architecture could be to deﬁne yi = g(zi)
for any function g, but this architecture is too restrictive and does not cover all permutation invariant
functions. Theorem 1 below provides a complete characterization (see Figure 2 for the corresponding
architecture). Intuitively, the architecture in Theorem 1 is such that it can aggregate information from
the entire graph, and do so in a permutation invariant manner.
Theorem 1. Let F be a graph labeling function. Then F is graph-permutation invariant if and only
if there exist functions α, ρ, φ such that for all k = 1, . . . , n:

[F(z)]k = ρ

zk,

α

zi,

φ(zi, zi,j, zj)



 ,

(1)









n
(cid:88)

i=1

(cid:88)

j(cid:54)=i

where φ : R2d+e → RL, α : Rd+L → RW and ρ : RW +d → R.

Proof. First, we show that any F satisfying the conditions of Theorem 1 is GPI. Namely, for any
permutation σ, [F(σ(z))]k = [F(z)]σ(k). To see this, write [F(σ(z))]k using Eq. 1 and Deﬁnition 1:

[F(σ(z))]k = ρ(zσ(k),

α(zσ(i),

φ(zσ(i), zσ(i),σ(j), zσ(j)))).

(2)

(cid:88)

i

(cid:88)

j(cid:54)=i

The second argument of ρ above is invariant under σ, because it is a sum over nodes and their
neighbors, which is invariant under permutation. Thus Eq. 2 is equal to:

ρ(zσ(k),

α(zi,

φ(zi, zi,j, zj))) = [F(z)]σ(k)

(cid:88)

i

(cid:88)

j(cid:54)=i

where equality follows from Eq. 1. We thus proved that Eq. 1 implies graph permutation invariance.

Next, we prove that any given GPI function F0 can be expressed as a function F in Eq. 1. Namely,
we show how to deﬁne φ, α and ρ that can implement F0. Note that in this direction of the proof the
function F0 is a black-box. Namely, we only know that it is GPI, but do not assume anything else
about its implementation.

4

The key idea is to construct φ, α such that the second argument of ρ in Eq. 1 contains the information
about all the graph features z. Then, the function ρ corresponds to an application of F0 to this
representation, followed by extracting the label yk. To simplify notation assume edge features are
scalar (e = 1). The extension to vectors is simple, but involves more indexing.

We assume WLOG that the black-box function F0 is a function only of the pairwise features zi,j
(otherwise, we can always augment the pairwise features with the singleton features). Since zi,j ∈ R
we use a matrix Rn,n to denote all the pairwise features.

Finally, we assume that our implementation of F0 will take additional node features zk such that no
two nodes have the same feature (i.e., the features identify the node).

Our goal is thus to show that there exist functions α, φ, ρ such that the function in Eq. 2 applied to Z
yields the same labels as F0(Z).

Let H be a hash function with L buckets mapping node features zi to an index (bucket). Assume
that H is perfect (this can be achieved for a large enough L). Deﬁne φ to map the pairwise
features to a vector of size L. Let 1 [j] be a one-hot vector of dimension RL, with one in the
jth coordinate. Recall that we consider scalar zi,j so that φ is indeed in RL, and deﬁne φ as:
φ(zi, zi,j, zj) = 1 [H(zj)] zi,j, i.e., φ “stores” zi,j in the unique bucket for node j.
Let si = (cid:80)
zi,j ∈E φ(zi, zi,j, zj) be the second argument of α in Eq. 1 (si ∈ RL). Then, since all
zj are distinct, si stores all the pairwise features for neighbors of i in unique positions within its
L coordinates. Since si(H(zk)) contains the feature zi,k whereas sj(H(zk)) contains the feature
zj,k, we cannot simply sum the si, since we would lose the information of which edges the features
originated from. Instead, we deﬁne α to map si to RL×L such that each feature is mapped to a
distinct location. Formally:

α(zi, si) = 1 [H(zi)] sT
i
α outputs a matrix that is all zeros except for the features corresponding to node i that are stored in
row H(zi). The matrix M = (cid:80)
i α(zi, si) (namely, the second argument of ρ in Eq. 1) is a matrix
with all the edge features in the graph including the graph structure.

(3)

.

To complete the construction we set ρ to have the same outcome as F0. We ﬁrst discard rows and
columns in M that do not correspond to original nodes (reducing M to dimension n × n). Then, we
use the reduced matrix as the input z to the black-box F0.

Assume for simplicity that M does not need to be contracted (this merely introduces another indexing
step). Then M corresponds to the original matrix Z of pairwise features, with both rows and
columns permuted according to H. We will thus use M as input to the function F0. Since F0 is
GPI, this means that the label for node k will be given by F0(M ) in position H(zk). Thus we set
ρ(zk, M ) = [F0(M )]H(zk), and by the argument above this equals [F0(Z)]k, implying that the
above α, φ and ρ indeed implement F0.

Extension to general graphs So far, we discussed complete graphs, where edges correspond
to valid feature pairs. However, many graphs of interest might be incomplete. For example, an
n-variable chain graph in sequence labeling has only n − 1 edges. For such graphs, the input to F
would not contain all zi,j pairs but rather only features corresponding to valid edges of the graph, and
we are only interested in invariances that preserve the graph structure, namely, the automorphisms
of the graph. Thus, the desired invariance is that σ(F(z)) = F(σ(z)), where σ is not an arbitrary
permutation but an automorphism. It is easy to see that a simple variant of Theorem 1 holds in
this case. All we need to do is replace in Eq. 2 the sum (cid:80)
j∈N (i), where N (i) are the
neighbors of node i in the graph. The arguments are then similar to the proof above.

j(cid:54)=i with (cid:80)

Implications of Theorem 1 Our result has interesting implications for deep structured prediction.
First, it highlights that the fact that the architecture “collects” information from all different edges
of the graph, in an invariant fashion via the α, φ functions. Speciﬁcally, the functions φ (after
summation) aggregate all the features around a given node, and then α (after summation) can
collect them. Thus, these functions can provide a summary of the entire graph that is sufﬁcient for
downstream algorithms. This is different from one round of message passing algorithms which would
not be sufﬁcient for collecting global graph information. Note that the dimensions of φ, α may need
to be large to aggregate all graph information (e.g., by hashing all the features as in the proof of
Theorem 1), but the architecture itself can be shallow.

5

Second, the architecture is parallelizable, as all φ functions can be applied simultaneously. This is in
contrast to recurrent models Zellers et al. (2017) which are harder to parallelize and are thus slower
in practice.

Finally, the theorem suggests several common architectural structures that can be used within GPI.
We brieﬂy mention two of these. 1) Attention: Attention is a powerful component in deep learning
architectures (Bahdanau et al., 2015), but most inference algorithms do not use attention. Intuitively,
in attention each node i aggregates features of neighbors through a weighted sum, where the weight
is a function of the neighbor’s relevance. For example, the label of an entity in an image may depend
more strongly on entities that are spatially closer. Attention can be naturally implemented in our
GPI characterization, and we provide a full derivation for this implementation in the appendix. It
plays a key role in our scene graph model described below. 2) RNNs: Because GPI functions are
closed under composition, for any GPI function F we can run F iteratively by providing the output
of one step of F as part of the input to the next step and maintain GPI. This results in a recurrent
architecture, which we use in our scene graph model.

4 Related Work

The concept of architectural invariance was recently proposed in DEEPSETS (Zaheer et al., 2017).
The invariance we consider is much less restrictive: the architecture does not need to be invariant
to all permutations of singleton and pairwise features, just those consistent with a graph re-labeling.
This characterization results in a substantially different set of possible architectures.

Deep structured prediction. There has been signiﬁcant recent interest in extending deep learning
to structured prediction tasks. Much of this work has been on semantic segmentation, where
convolutional networks (Shelhamer et al., 2017) became a standard approach for obtaining “singleton
scores” and various approaches were proposed for adding structure on top. Most of these approaches
used variants of message passing algorithms, unrolled into a computation graph (Xu et al., 2017).
Some studies parameterized parts of the message passing algorithm and learned its parameters (Lin
et al., 2015). Recently, gradient descent has also been used for maximizing score functions (Belanger
et al., 2017; Gygli et al., 2017). An alternative to deep structured prediction is greedy decoding,
inferring each label at a time based on previous labels. This approach has been popular in sequence-
based applications (e.g., parsing (Chen & Manning, 2014)), relying on the sequential structure of the
input, where BiLSTMs are effectively applied. Another related line of work is applying deep learning
to graph-based problems, such as TSP (Bello et al., 2016; Gilmer et al., 2017; Khalil et al., 2017).
Clearly, the notion of graph invariance is important in these, as highlighted in (Gilmer et al., 2017).
They however do not specify a general architecture that satisﬁes invariance as we do here, and in
fact focus on message passing architectures, which we strictly generalize. Furthermore, our focus is
on the more general problem of structured prediction, rather than speciﬁc graph-based optimization
problems.

Scene graph prediction. Extracting scene graphs from images provides a semantic representation
that can later be used for reasoning, question answering, and image retrieval (Johnson et al., 2015;
Lu et al., 2016; Raposo et al., 2017). It is at the forefront of machine vision research, integrating
challenges like object detection, action recognition and detection of human-object interactions (Liao
et al., 2016; Plummer et al., 2017). Prior work on scene graph predictions used neural message
passing algorithms (Xu et al., 2017) as well as prior knowledge in the form of word embeddings
(Lu et al., 2016). Other work suggested to predict graphs directly from pixels in an end-to-end
manner Newell & Deng (2017). NeuralMotif (Zellers et al., 2017), currently the state-of-the-art
model for scene graph prediction on Visual Genome, employs an RNN that provides global context
by sequentially reading the independent predictions for each entity and relation and then reﬁnes those
predictions. The NEURALMOTIF model maintains GPI by ﬁxing the order in which the RNN reads
its inputs and thus only a single order is allowed. However, this ﬁxed order is not guaranteed to be
optimal.

5 Experimental Evaluation

We empirically evaluate the beneﬁt of GPI architectures. First, using a synthetic graph-labeling task,
and then for the problem of mapping images to scene graphs.

6

Figure 3: Accuracy as a function of sample size for graph labeling. Right is a zoomed in version of left.

5.1 Synthetic Graph Labeling

We start with studying GPI on a synthetic problem, deﬁned as follows. An input graph G = (V, E)
is given, where each node i ∈ V is assigned to one of K sets. The set for node i is denoted by
Γ(i). The goal is to compute for each node the number of neighbors that belong to the same set.
Namely, the label of a node is yi = (cid:80)
j∈N (i) 1[Γ(i) = Γ(j)]. We generated random graphs with 10
nodes (larger graphs produced similar results) by sampling each edge independently and uniformly,
and sampling Γ(i) for every node uniformly from {1, . . . , K}. The node features zi ∈ {0, 1}K are
one-hot vectors of Γ(i) and the edge features zi,j ∈ {0, 1} indicate whether ij ∈ E. We compare two
standard non-GPI architectures and one GPI architecture: (a) A GPI-architecture for graph prediction,
described in detail in Section 5.2. We used the basic version without attention and RNN. (b) LSTM:
We replace (cid:80) φ(·) and (cid:80) α(·), which perform aggregation in Theorem 1 with two LSTMs with
a state size of 200 that read their input in random order. (c) A fully-connected (FC) feed-forward
network with 2 hidden layers of 1000 nodes each. The input to the fully connected model is a
concatenation of all node and pairwise features. The output is all node predictions. The focus of the
experiment is to study sample complexity. Therefore, for a fair comparison, we use the same number
of parameters for all models.

Figure 3, shows the results, demonstrating that GPI requires far fewer samples to converge to the
correct solution. This illustrates the advantage of an architecture with the correct inductive bias for
the problem.

5.2 Scene-Graph Classiﬁcation

We evaluate the GPI approach on the motivating task of this paper, inferring scene graphs from
images (Figure 1). In this problem, the input is an image annotated with a set of bounding boxes for
the entities in the image.2 The goal is to label each bounding box with the correct entity category and
every pair of entities with their relation, such that they form a coherent scene graph.

We begin by describing our Scene Graph Predictor (SGP) model. We aim to predict two types of
variables. The ﬁrst is entity variables [y1, . . . , yn] for all bounding boxes. Each yi can take one of
L values (e.g., “dog”, “man”). The second is relation variables [yn+1, . . . , yn2] for every pair of
bounding boxes. Each such yj can take one of R values (e.g., “on”, “near”). Our graph connects
variables that are expected to be inter-related. It contains two types of edges: 1) entity-entity edge
connecting every two entity variables (yi and yj for 1 ≤ i (cid:54)= j ≤ n. 2) entity-relation edges
connecting every relation variable yk (where k > n) to its two entity variables. Thus, our graph is not
a complete graph and our goal is to design an architecture that will be invariant to any automorphism
of the graph, such as permutations of the entity variables.

For the input features z, we used the features learned by the baseline model from Zellers et al.
(2017).3 Speciﬁcally, the entity features zi included (1) The conﬁdence probabilities of all entities
for yi as learned by the baseline model. (2) Bounding box information given as (left, bottom,
width, height); (3) The number of smaller entities (also bigger); (4) The number of entities to
the left, right, above and below. (5) The number of entities with higher and with lower conﬁdence;

2For simplicity, we focus on the task where boxes are given.
3The baseline does not use any LSTM or context, and is thus unrelated to the main contribution of Zellers

et al. (2017).

7

Constrained Evaluation

SGCls

PredCls

Unconstrained Evaluation
SGCls

PredCls

R@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100

Lu et al., 2016 (Lu et al., 2016)
Xu et al., 2017 (Xu et al., 2017)
Pixel2Graph (Newell & Deng, 2017)
Graph R-CNN (Yang et al., 2018)
Neural Motifs (Zellers et al., 2017)
Baseline (Zellers et al., 2017)
No Attention
Neighbor Attention
Linguistic

11.8
21.7
-
29.6
35.8
34.6
35.3
35.7
36.5

14.1
24.4
-
31.6
36.5
35.3
37.2
38.5
38.8

35.0
44.8
-
54.2
65.2
63.7
64.5
64.6
65.1

27.9
53.0
-
59.1
67.1
65.6
66.3
66.6
66.9

-
-
26.5
-
44.5
43.4
44.1
44.7
45.5

-
-
30.0
-
47.7
46.6
48.5
49.9
50.8

-
-
68.0
-
81.1
78.8
79.7
80.0
80.8

-
-
75.2
-
88.3
85.9
86.7
87.1
88.2

Table 1: Test set results for graph-constrained evaluation (i.e., the returned triplets must be consistent with a
scene graph) and for unconstrained evaluation (triplets need not be consistent with a scene graph).

(6) For the linguistic model only: word embedding of the most probable class. Word vectors were
learned with GLOVE from the ground-truth captions of Visual Genome.
Similarly, the relation features zj ∈ RR contained the probabilities of relation entities for the relation
j. For the Linguistic model, these features were extended to include word embedding of the most
probable class. For entity-entity pairwise features zi,j, we use the relation probability for each pair.
Because the output of SGP are probability distributions over entities and relations, we use them as an
the input z to SGP, once again in a recurrent manner and maintain GPI.

We next describe the main components of the GPI architecture. First, we focus on the parts that
output the entity labels. φent is the network that integrates features for two entity variables yi
and yj. It simply takes zi, zj and zi,j as input, and outputs a vector of dimension n1. Next, the
network αent takes as input the outputs of φent for all neighbors of an entity, and uses the attention
mechanism described above to output a vector of dimension n2. Finally, the ρent network takes these
n2 dimensional vectors and outputs L logits predicting the entity value. The ρrel network takes as
input the αent representation of the two entities, as well as zi,j and transforms the output into R
logits. See appendix for speciﬁc network architectures.

5.2.1 Experimental Setup and Results

Dataset. We evaluated our approach on Visual Genome (VG) (Krishna et al., 2017), a dataset with
108,077 images annotated with bounding boxes, entities and relations. On average, images have 12
entities and 7 relations per image. For a proper comparison with previous results (Newell & Deng,
2017; Xu et al., 2017; Zellers et al., 2017), we used the data from (Xu et al., 2017), including the
train and test splits. For evaluation, we used the same 150 entities and 50 relations as in (Newell
& Deng, 2017; Xu et al., 2017; Zellers et al., 2017). To tune hyper-parameters, we also split the
training data into two by randomly selecting 5K examples, resulting in a ﬁnal 70K/5K/32K split for
train/validation/test sets.

Training. All networks were trained using Adam (Kingma & Ba, 2014) with batch size 20. Hyper-
parameter values below were chosen based on the validation set. The SGP loss function was the sum
of cross-entropy losses over all entities and relations in the image. In the loss, we penalized entities
4 times more strongly than relations, and penalized negative relations 10 times more weakly than
positive relations.

Evaluation.
In (Xu et al., 2017) three different evaluation settings were considered. Here we
focus on two of these: (1) SGCls: Given ground-truth bounding boxes for entities, predict all entity
categories and relations categories. (2) PredCls: Given bounding boxes annotated with entity labels,
predict all relations. Following (Lu et al., 2016), we used Recall@K as the evaluation metric. It
measures the fraction of correct ground-truth triplets that appear within the K most conﬁdent triplets
proposed by the model. Two evaluation protocols are used in the literature differing in whether they
enforce graph constraints over model predictions. The ﬁrst graph-constrained protocol requires that
the top-K triplets assign one consistent class per entity and relation. The second unconstrained
protocol does not enforce any such constraints. We report results on both protocols, following (Zellers
et al., 2017).

8

Figure 4: (a) An input image with bounding boxes from VG. (b) The ground-truth scene graph. (c) The
Baseline fails to recognize some entities (tail and tree) and relations (in front of instead of looking at). (d)
GPI:LINGUISTIC ﬁxes most incorrect LP predictions. (e) Window is the most signiﬁcant neighbor of Tree. (f)
The entity bird receives substantial attention, while Tree and building are less informative.

Models and baselines. We compare four variants of our GPI approach with the reported results
of four baselines that are currently the state-of-the-art on various scene graph prediction problems
(all models use the same data split and pre-processing as (Xu et al., 2017)): 1) LU ET AL., 2016
(LU ET AL., 2016): This work leverages word embeddings to ﬁne-tune the likelihood of predicted
relations. 2) XU ET AL, 2017 (XU ET AL., 2017): This model passes messages between entities and
relations, and iteratively reﬁnes the feature map used for prediction. 3) NEWELL & DENG, 2017
(NEWELL & DENG, 2017): The PIXEL2GRAPH model uses associative embeddings (Newell et al.,
2017) to produce a full graph from the image. 4) YANG ET AL., 2018 (YANG ET AL., 2018): The
GRAPH R-CNN model uses object-relation regularities to sparsify and reason over scene graphs. 5)
ZELLERS ET AL., 2017 (ZELLERS ET AL., 2017): The NEURALMOTIF method encodes global
context for capturing high-order motifs in scene graphs, and the BASELINE outputs the entities and
relations distributions without using the global context. The following variants of GPI were compared:
1) GPI: NO ATTENTION: Our GPI model, but with no attention mechanism. Instead, following
Theorem 1, we simply sum the features. 2) GPI: NEIGHBORATTENTION: Our GPI model, with
attention over neighbors features. 3) GPI: LINGUISTIC: Same as GPI: NEIGHBORATTENTION but
also concatenating the word embedding vector, as described above.

Results. Table 1 shows recall@50 and recall@100 for three variants of our approach, and compared
with ﬁve baselines. All GPI variants performs well, with LINGUISTIC outperforming all baselines
for SGCls and being comparable to the state-of-the-art model for PredCls. Note that PredCl is an
easier task, which makes less use of the structure, hence it is not surprising that GPI achieves similar
accuracy to Zellers et al. (2017). Figure 4 illustrates the model behavior. Predicting isolated labels
with zi (4c) mislabels several entities, but these are corrected at the ﬁnal output (4d). Figure 4e
shows that the system learned to attend more to nearby entities (the window and building are closer
to the tree), and 4f shows that stronger attention is learned for the class bird, presumably because it is
usually more informative than common classes like tree.

Implementation details. The φ and α networks were each implemented as a single fully-connected
(FC) layer with a 500-dimensional outputs. ρ was implemented as a FC network with 3 500-
dimensional hidden layers, with one 150-dimensional output for the entity probabilities, and one
51-dimensional output for relation probabilities. The attention mechanism was implemented as a
network like to φ and α, receiving the same inputs, but using the output scores for the attention . The
full code is available at https://github.com/shikorab/SceneGraph

6 Conclusion

We presented a deep learning approach to structured prediction, which constrains the architecture
to be invariant to structurally identical inputs. As in score-based methods, our approach relies on
pairwise features, capable of describing inter-label correlations, and thus inheriting the intuitive
aspect of score-based approaches. However, instead of maximizing a score function (which leads
to computationally-hard inference), we directly produce an output that is invariant to equivalent
representations of the pairwise terms.

9

This axiomatic approach to model architecture can be extended in many ways. For image labeling,
geometric invariances (shift or rotation) may be desired.
In other cases, invariance to feature
permutations may be desirable. We leave the derivation of the corresponding architectures to future
work. Finally, there may be cases where the invariant structure is unknown and should be discovered
from data, which is related to work on lifting graphical models Bui et al. (2013). It would be interesting
to explore algorithms that discover and use such symmetries for deep structured prediction.

This work was supported by the ISF Centers of Excellence grant, and by the Yandex Initiative in
Machine Learning. Work by GC was performed while at Google Brain Research.

Acknowledgements

References

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and

translate. In International Conference on Learning Representations (ICLR), 2015.

Belanger, David, Yang, Bishan, and McCallum, Andrew. End-to-end learning for structured prediction
energy networks. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70, pp. 429–439. PMLR, 2017.

Bello, Irwan, Pham, Hieu, Le, Quoc V, Norouzi, Mohammad, and Bengio, Samy. Neural combinato-

rial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Bui, Hung Hai, Huynh, Tuyen N., and Riedel, Sebastian. Automorphism groups of graphical models
and lifted variational inference. In Proceedings of the Twenty-Ninth Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’13, pp. 132–141, 2013.

Chen, Danqi and Manning, Christopher. A fast and accurate dependency parser using neural
networks. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 740–750, 2014.

Chen, Liang Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Se-
mantic image segmentation with deep convolutional nets and fully connected CRFs. In Proceedings
of the Second International Conference on Learning Representations, 2014.

Chen, Liang Chieh, Schwing, Alexander G, Yuille, Alan L, and Urtasun, Raquel. Learning deep

structured models. In Proc. ICML, 2015.

Farabet, Clement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical
features for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):
1915–1929, 2013.

Gilmer, Justin, Schoenholz, Samuel S, Riley, Patrick F, Vinyals, Oriol, and Dahl, George E. Neural

message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.

Gygli, Michael, Norouzi, Mohammad, and Angelova, Anelia. Deep value networks learn to evaluate
and iteratively reﬁne structured outputs. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine
Learning Research, pp. 1341–1351, International Convention Centre, Sydney, Australia, 2017.
PMLR.

Johnson, Justin, Krishna, Ranjay, Stark, Michael, Li, Li-Jia, Shamma, David A., Bernstein, Michael S.,
In Proc. Conf. Comput. Vision Pattern

Image retrieval using scene graphs.

and Li, Fei-Fei.
Recognition, pp. 3668–3678, 2015.

Johnson, Justin, Gupta, Agrim, and Fei-Fei, Li. Image generation from scene graphs. arXiv preprint

arXiv:1804.01622, 2018.

Khalil, Elias, Dai, Hanjun, Zhang, Yuyu, Dilkina, Bistra, and Song, Le. Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp.
6351–6361, 2017.

10

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint

arXiv: 1412.6980, abs/1412.6980, 2014.

Krishna, Ranjay, Zhu, Yuke, Groth, Oliver, Johnson, Justin, Hata, Kenji, Kravitz, Joshua, Chen,
Stephanie, Kalantidis, Yannis, Li, Li-Jia, Shamma, David A, et al. Visual genome: Connecting
International Journal of
language and vision using crowdsourced dense image annotations.
Computer Vision, 123(1):32–73, 2017.

Lafferty, J., McCallum, A., and Pereira, F. Conditional random ﬁelds: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning, pp. 282–289, 2001.

Liao, Wentong, Yang, Michael Ying, Ackermann, Hanno, and Rosenhahn, Bodo. On support relations

and semantic scene graphs. arXiv preprint arXiv:1609.05834, 2016.

Lin, Guosheng, Shen, Chunhua, Reid, Ian, and van den Hengel, Anton. Deeply learning the messages
in message passing inference. In Advances in Neural Information Processing Systems, pp. 361–369,
2015.

Lu, Cewu, Krishna, Ranjay, Bernstein, Michael S., and Li, Fei-Fei. Visual relationship detection with

language priors. In European Conf. Comput. Vision, pp. 852–869, 2016.

Meshi, O., Sontag, D., Jaakkola, T., and Globerson, A. Learning efﬁciently with approximate
In Proceedings of the 27th International Conference on Machine

inference via dual losses.
Learning, pp. 783–790, New York, NY, USA, 2010. ACM.

Newell, Alejandro and Deng, Jia. Pixels to graphs by associative embedding. In Advances in Neural
Information Processing Systems 30 (to appear), pp. 1172–1180. Curran Associates, Inc., 2017.

Newell, Alejandro, Huang, Zhiao, and Deng, Jia. Associative embedding: End-to-end learning for
joint detection and grouping. In Neural Inform. Process. Syst., pp. 2274–2284. Curran Associates,
Inc., 2017.

Pei, Wenzhe, Ge, Tao, and Chang, Baobao. An effective neural network model for graph-based de-
pendency parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computationa
Linguistics, pp. 313–322, 2015.

Plummer, Bryan A., Mallya, Arun, Cervantes, Christopher M., Hockenmaier, Julia, and Lazebnik,
Svetlana. Phrase localization and visual relationship detection with comprehensive image-language
cues. In ICCV, pp. 1946–1955, 2017.

Raposo, David, Santoro, Adam, Barrett, David, Pascanu, Razvan, Lillicrap, Timothy, and Battaglia,
Peter. Discovering objects and their relations from entangled scene representations. arXiv preprint
arXiv:1702.05068, 2017.

Schwing, Alexander G and Urtasun, Raquel. Fully connected deep structured networks. ArXiv

e-prints, 2015.

Shelhamer, Evan, Long, Jonathan, and Darrell, Trevor. Fully convolutional networks for semantic

segmentation. Proc. Conf. Comput. Vision Pattern Recognition, 39(4):640–651, 2017.

Taskar, B., Guestrin, C., and Koller, D. Max margin Markov networks. In Thrun, S., Saul, L., and
Schölkopf, B. (eds.), Advances in Neural Information Processing Systems 16, pp. 25–32. MIT
Press, Cambridge, MA, 2004.

Xu, Danfei, Zhu, Yuke, Choy, Christopher B., and Fei-Fei, Li. Scene Graph Generation by Iterative
Message Passing. In Proc. Conf. Comput. Vision Pattern Recognition, pp. 3097–3106, 2017.

Yang, Jianwei, Lu, Jiasen, Lee, Stefan, Batra, Dhruv, and Parikh, Devi. Graph R-CNN for scene

graph generation. In European Conf. Comput. Vision, pp. 690–706, 2018.

Zaheer, Manzil, Kottur, Satwik, Ravanbakhsh, Siamak, Poczos, Barnabas, Salakhutdinov, Ruslan R,
and Smola, Alexander J. Deep sets. In Advances in Neural Information Processing Systems 30, pp.
3394–3404. Curran Associates, Inc., 2017.

11

Zellers, Rowan, Yatskar, Mark, Thomson, Sam, and Choi, Yejin. Neural motifs: Scene graph parsing

with global context. arXiv preprint arXiv:1711.06640, abs/1711.06640, 2017.

Zheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav, Su, Zhizhong, Du,
Dalong, Huang, Chang, and Torr, Philip HS. Conditional random ﬁelds as recurrent neural networks.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 1529–1537, 2015.

7 Supplementary Material

This supplementary material includes: (1) Visual illustration of the proof of Theorem 1. (2) Explaining
how to integrate an attention mechanism in our GPI framework. (3) Additional evaluation method to
further analyze and compare our work with baselines.

7.1 Theorem 1: Illustration of Proof

Figure 5: Illustration of the proof of Theorem 1 using a speciﬁc construction example. Here H is a hash function
of size L = 5 such that H(1) = 1, H(3) = 2, H(2) = 4, G is a three-node input graph, and zi,j ∈ R are the
pairwise features (in purple) of G. (a) φ is applied to each zi,j. Each application yields a vector in R5. The three
dark yellow columns correspond to φ(z1,1), φ(z1,2) and φ(z1,3). Then, all vectors φ(zi,j) are summed over j
to obtain three si vectors. (b) α’s (blue matrices) are an outer product between 1 [H(zi)] and si resulting in a
matrix of zeros except one row. The dark blue matrix corresponds for α(z1, s1). (c) All α’s are summed to a
5 × 5 matrix, isomorphic to the original zi,j matrix.

7.2 Characterizing Permutation Invariance: Attention

Attention is a powerful component which naturally can be introduced into our GPI model. We now
show how attention can be introduced in our framework. Formally, we learn attention weights for the
neighbors j of a node i, which scale the features zi,j of that neighbor. We can also learn different
attention weights for individual features of each neighbor in a similar way.

Let wi,j ∈ R be an attention mask specifying the weight that node i gives to node j:

wi,j(zi, zi,j, zj) = eβ(zi,zi,j ,zj )/

eβ(zi,zi,t,zt)

(4)

(cid:88)

t

where β can be any scalar-valued function of its arguments (e.g., a dot product of zi and zj as in
standard attention models). To introduce attention we wish α ∈ Re to have the form of weighting
wi,j over neighboring feature vectors zi,j, namely, α = (cid:80)
To achieve this form we extend φ by a single entry, deﬁning φ ∈ Re+1 (namely we set
L = e + 1) as φ1:e(zi, zi,j, zj) = eβ(zi,zi,j ,zj )zi,j (here φ1:e are the ﬁrst e elements of φ)

j(cid:54)=i wi,jzi,j.

12

and φe+1(zi, zi,j; zj) = eβ(zi,zi,j ,zj ). We keep the deﬁnition of si = (cid:80)
we deﬁne α = si,1:e
si,e+1
over neighboring feature vectors zi,j:

j(cid:54)=i φ(zi, zi,j, zj). Next,
and substitute si and φ to obtain the desired form as attention weights wi,j

α(zi, si) =

si,1:e
si,e+1

(cid:88)

=

j(cid:54)=i

eβ(zi,zi,j ,zj )zi,j
(cid:80)
j(cid:54)=i eβ(zi,zi,j ,zj )

(cid:88)

=

j(cid:54)=i

wi,jzi,j

A similar approach can be applied over α and ρ to model attention over the outputs of α as well
(graph nodes).

7.3 Scene Graph Results

In the main paper, we described the results for the two prediction tasks: SGCls and PredCls, as
deﬁned in section 5.2.1: "Experimental Setup and Results". To further analyze our module, we
compare the best variant, GPI: LINGUISTIC, per relation to two baselines: (Lu et al., 2016) and Xu
et al. (2017). Table 2, speciﬁes the PredCls recall@5 of the 20-top frequent relation classes. The GPI
module performs better in almost all the relations classes.

RELATION

(LU ET AL., 2016)

(XU ET AL., 2017)

LINGUISTIC

99.71
98.03
80.38
82.47
98.47
85.16
31.85
49.19
61.50
79.35
28.64
31.74
26.09
8.45
54.08

ON
HAS
IN
OF
WEARING
NEAR
WITH
ABOVE
HOLDING
BEHIND
UNDER
SITTING ON
IN FRONT OF
ATTACHED TO
AT
HANGING FROM 0.0
OVER
FOR
RIDING

9.26
12.20
72.43

99.25
97.25
88.30
96.75
98.23
96.81
88.10
79.73
80.67
92.32
52.73
50.17
59.63
29.58
70.41
0.0
0.0
31.71
89.72

99.3
98.7
95.9
98.1
99.6
95.4
94.2
83.9
95.5
91.2
83.2
90.4
74.9
77.4
80.9
74.1
62.4
45.1
96.1

Table 2: Recall@5 of PredCls for the 20-top relations ranked by their frequency, as in (Xu et al., 2017)

13

8
1
0
2
 
v
o
N
 
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
5
4
5
0
.
2
0
8
1
:
v
i
X
r
a

Mapping Images to Scene Graphs with
Permutation-Invariant Structured Prediction

Roei Herzig∗
Tel Aviv University
roeiherzig@mail.tau.ac.il

Moshiko Raboh∗
Tel Aviv University
mosheraboh@mail.tau.ac.il

Gal Chechik
Bar-Ilan University, NVIDIA Research
gal.chechik@biu.ac.il

Jonathan Berant
Tel Aviv University, AI2
joberant@cs.tau.ac.il

Amir Globerson
Tel Aviv University
gamir@post.tau.ac.il

Abstract

Machine understanding of complex images is a key goal of artiﬁcial intelligence.
One challenge underlying this task is that visual scenes contain multiple inter-
related objects, and that global context plays an important role in interpreting
the scene. A natural modeling framework for capturing such effects is structured
prediction, which optimizes over complex labels, while modeling within-label
interactions. However, it is unclear what principles should guide the design of a
structured prediction model that utilizes the power of deep learning components.
Here we propose a design principle for such architectures that follows from a
natural requirement of permutation invariance. We prove a necessary and sufﬁ-
cient characterization for architectures that follow this invariance, and discuss its
implication on model design. Finally, we show that the resulting model achieves
new state-of-the-art results on the Visual Genome scene-graph labeling benchmark,
outperforming all recent approaches.

1

Introduction

Understanding the semantics of a complex visual scene is a fundamental problem in machine
perception. It often requires recognizing multiple objects in a scene, together with their spatial and
functional relations. The set of objects and relations is sometimes represented as a graph, connecting
objects (nodes) with their relations (edges) and is known as a scene graph (Figure 1). Scene graphs
provide a compact representation of the semantics of an image, and can be useful for semantic-level
interpretation and reasoning about a visual scene Johnson et al. (2018). Scene-graph prediction is the
problem of inferring the joint set of objects and their relations in a visual scene.

Since objects and relations are inter-dependent (e.g., a person and chair are more likely to be in relation
“sitting on” than “eating”), a scene graph predictor should capture this dependence in order to improve
prediction accuracy. This goal is a special case of a more general problem, namely, inferring multiple
inter-dependent labels, which is the research focus of the ﬁeld of structured prediction. Structured
prediction has attracted considerable attention because it applies to many learning problems and

∗Equal Contribution.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Figure 1: An image and its scene graph from the Visual Genome dataset (Krishna et al., 2017). The scene graph
captures the entities in the image (nodes, blue circles) like dog and their relations (edges, red circles) like (cid:10)hat,
on, dog(cid:11).

poses unique theoretical and algorithmic challenges (e.g., see Belanger et al., 2017; Chen et al., 2015;
Taskar et al., 2004). It is therefore a natural approach for predicting scene graphs from images.

Structured prediction models typically deﬁne a score function s(x, y) that quantiﬁes how well a
label assignment y is compatible with an input x. In the case of understanding complex visual
scenes, x is an image, and y is a complex label containing the labels of objects detected in an image
and the labels of their relations. In this setup, the inference task amounts to ﬁnding the label that
maximizes the compatibility score y∗ = arg maxy s(x, y). This score-based approach separates a
scoring component – implemented by a parametric model, from an optimization component – aimed
at ﬁnding a label that maximizes that score. Unfortunately, for a general scoring function s(·), the
space of possible label assignments grows exponentially with input size. For instance, for scene
graphs the set of possible object label assignments is too large even for relatively simple images,
since the vocabulary of candidate objects may contain thousands of objects. As a result, inferring the
label assignment that maximizes a scoring function is computationally hard in the general case.

An alternative approach to score-based methods is to map an input x to a structured output y with
a “black box" neural network, without explicitly deﬁning a score function. This raises a natural
question: what is the right architecture for such a network? Here we take an axiomatic approach and
argue that one important property such networks should satisfy is invariance to a particular type of
input permutation. We then prove that this invariance is equivalent to imposing certain structural
constraints on the architecture of the network, and describe architectures that satisfy these constraints.

To evaluate our approach, we ﬁrst demonstrate on a synthetic dataset that respecting permutation
invariance is important, because models that violate this invariance need more training data, despite
having a comparable model size. Then, we tackle the problem of scene graph generation. We describe
a model that satisﬁes the permutation invariance property, and show that it achieves state-of-the-art
results on the competitive Visual Genome benchmark (Krishna et al., 2017), demonstrating the power
of our new design principle.

In summary, the novel contributions of this paper are: a) Deriving sufﬁcient and necessary conditions
for graph-permutation invariance in deep structured prediction architectures. b) Empirically demon-
strating the beneﬁt of graph-permutation invariance. c) Developing a state-of-the-art model for scene
graph prediction on a large dataset of complex visual scenes.

2 Structured Prediction

Scored-based methods in structured prediction deﬁne a function s(x, y) that quantiﬁes the degree
to which y is compatible with x, and infer a label by maximizing s(x, y) (e.g., see Belanger et al.,
2017; Chen et al., 2015; Lafferty et al., 2001; Meshi et al., 2010; Taskar et al., 2004). Most score
functions previously used decompose as a sum over simpler functions, s(x, y) = (cid:80)
i fi(x, y), making
it possible to optimize maxy fi(x, y) efﬁciently. This local maximization forms the basic building
block of algorithms for approximately maximizing s(x, y). One way to decompose the score function
is to restrict each fi(x, y) to depend only on a small subset of the y variables.

The renewed interest in deep learning led to efforts to integrate deep networks with structured predic-
tion, including modeling the fi functions as deep networks. In this context, the most widely-used
score functions are singleton fi(yi, x) and pairwise fij(yi, yj, x). The early work taking this approach
used a two-stage architecture, learning the local scores independently of the structured prediction

2

Figure 2: Left: Graph permutation invariance. A graph labeling function F is graph permutation invariant
(GPI) if permuting the node features maintains the output. Right: a schematic representation of the GPI
architecture in Theorem 1. Singleton features zi are omitted for simplicity. (a) First, the features zi,j are
processed element-wise by φ. (b) Features are summed to create a vector si, which is concatenated with zi. (c)
A representation of the entire graph is created by applying α n times and summing the created vector. (d) The
graph representation is then ﬁnally processed by ρ together with zk.

goal (Chen et al., 2014; Farabet et al., 2013). Later studies considered end-to-end architectures where
the inference algorithm is part of the computation graph (Chen et al., 2015; Pei et al., 2015; Schwing
& Urtasun, 2015; Zheng et al., 2015). Recent studies go beyond pairwise scores, also modelling
global factors (Belanger et al., 2017; Gygli et al., 2017).

Score-based methods provide several advantages. First, they allow intuitive speciﬁcation of local
dependencies between labels and how these translate to global dependencies. Second, for linear
score functions, the learning problem has natural convex surrogates Lafferty et al. (2001); Taskar
et al. (2004). Third, inference in large label spaces is sometimes possible via exact algorithms or
empirically accurate approximations. However, with the advent of deep scoring functions s(x, y; w),
learning is no longer convex. Thus, it is worthwhile to rethink the architecture of structured prediction
models, and consider models that map inputs x to outputs y directly without explicitly maximizing a
score function. We would like these models to enjoy the expressivity and predictive power of neural
networks, while maintaining the ability to specify local dependencies between labels in a ﬂexible
manner. In the next section, we present such an approach and consider a natural question: what
should be the properties of a deep neural network used for structured prediction.

3 Permutation-Invariant Structured Prediction

In what follows we deﬁne the permutation-invariance property for structured prediction models, and
argue that permutation invariance is a natural principle for designing their architecture.

We ﬁrst introduce our notation. We focus on structures with pairwise interactions, because they are
simpler in terms of notation and are sufﬁcient for describing the structure in many problems. We
denote a structured label by y = [y1, . . . , yn]. In a score-based approach, the score is deﬁned via a
set of singleton scores fi(yi, x) and pairwise scores fij(yi, yj, x), where the overall score s(x, y) is
the sum of these scores. For brevity, we denote fij = fij(yi, yj, x) and fi = fi(yi, x). An inference
algorithm takes as input the local scores fi, fij and outputs an assignment that maximizes s(x, y).
We can thus view inference as a black-box that takes node-dependent and edge-dependent inputs
(i.e., the scores fi, fij) and returns a label y, even without an explicit score function s(x, y). While
numerous inference algorithms exist for this setup, including belief propagation (BP) and mean ﬁeld,
here we develop a framework for a deep labeling algorithm (we avoid the term “inference” since the
algorithm does not explicitly maximize a score function). Such an algorithm will be a black-box,
taking the f functions as input and the labels y1, . . . , yn as output. We next ask what architecture
such an algorithm should have.

We follow with several deﬁnitions. A graph labeling function F : (V, E) → Y is a function whose
input is an ordered set of node features V = [z1, . . . , zn] and an ordered set of edge features
E = [z1,2 . . . , zi,j, . . . , zn,n−1]. For example, zi can be the array of values fi, and zi,j can be
the table of values fi,j. Assume zi ∈ Rd and zi,j ∈ Re. The output of F is a set of node labels
y = [y1, . . . , yn]. Thus, algorithms such as BP are graph labeling functions. However, graph labeling
functions do not necessarily maximize a score function. We denote the joint set of node features and
edge features by z (i.e., a set of n + n(n − 1) = n2 vectors). In Section 3.1 we discuss extensions to
this case where only a subset of the edges is available.

3

1, y∗

2, y∗

1, y∗

2, y∗

A natural requirement is that the function F produces the same result when given the same features,
up to a permutation of the input. For example, consider a label space with three variables y1, y2, y3,
and assume that F takes as input z = (z1, z2, z3, z12, z13, z23) = (f1, f2, f3, f12, f13, f23), and
outputs a label y = (y∗
3). When F is given an input that is permuted in a consistent way, say,
z(cid:48) = (f2, f1, f3, f21, f23, f13), this deﬁnes exactly the same input. Hence, the output should still be
y = (y∗
3). Most inference algorithms, including BP and mean ﬁeld, satisfy this symmetry
requirement by design, but this property is not guaranteed in general in a deep model. Here, our
goal is to design a deep learning black-box, and hence we wish to guarantee invariance to input
permutations. A black-box that violates this invariance “wastes” capacity on learning it at training
time, which increases sample complexity, as shown in Sec. 5.1. We proceed to formally deﬁne the
permutation invariance property.
Deﬁnition 1. Let z be a set of node features and edge features, and let σ be a permutation of
{1, . . . , n}. We deﬁne σ(z) to be a new set of node and edge features given by [σ(z)]i = zσ(i) and
[σ(z)]i,j = zσ(i),σ(j).

We also use the notation σ([y1, . . . , yn]) = [yσ(1), . . . , yσ(n)] for permuting the labels. Namely, σ
applied to a set of labels yields the same labels, only permuted by σ. Be aware that applying σ to
the input features is different from permuting labels, because edge input features must permuted in a
way that is consistent with permuting node input features. We now provide our key deﬁnition of a
function whose output is invariant to permutations of the input. See Figure 2 (left).
Deﬁnition 2. A graph labeling function F is said to be graph-permutation invariant (GPI), if for
all permutations σ of {1, . . . , n} and for all z it satisﬁes: F(σ(z)) = σ(F(z)).

3.1 Characterizing Permutation Invariance

Motivated by the above discussion, we ask: what structure is necessary and sufﬁcient to guarantee
that F is GPI? Note that a function F takes as input an ordered set z. Therefore its output on z
could certainly differ from its output on σ(z). To achieve permutation invariance, F should contain
certain symmetries. For instance, one permutation invariant architecture could be to deﬁne yi = g(zi)
for any function g, but this architecture is too restrictive and does not cover all permutation invariant
functions. Theorem 1 below provides a complete characterization (see Figure 2 for the corresponding
architecture). Intuitively, the architecture in Theorem 1 is such that it can aggregate information from
the entire graph, and do so in a permutation invariant manner.
Theorem 1. Let F be a graph labeling function. Then F is graph-permutation invariant if and only
if there exist functions α, ρ, φ such that for all k = 1, . . . , n:

[F(z)]k = ρ

zk,

α

zi,

φ(zi, zi,j, zj)



 ,

(1)









n
(cid:88)

i=1

(cid:88)

j(cid:54)=i

where φ : R2d+e → RL, α : Rd+L → RW and ρ : RW +d → R.

Proof. First, we show that any F satisfying the conditions of Theorem 1 is GPI. Namely, for any
permutation σ, [F(σ(z))]k = [F(z)]σ(k). To see this, write [F(σ(z))]k using Eq. 1 and Deﬁnition 1:

[F(σ(z))]k = ρ(zσ(k),

α(zσ(i),

φ(zσ(i), zσ(i),σ(j), zσ(j)))).

(2)

(cid:88)

i

(cid:88)

j(cid:54)=i

The second argument of ρ above is invariant under σ, because it is a sum over nodes and their
neighbors, which is invariant under permutation. Thus Eq. 2 is equal to:

ρ(zσ(k),

α(zi,

φ(zi, zi,j, zj))) = [F(z)]σ(k)

(cid:88)

i

(cid:88)

j(cid:54)=i

where equality follows from Eq. 1. We thus proved that Eq. 1 implies graph permutation invariance.

Next, we prove that any given GPI function F0 can be expressed as a function F in Eq. 1. Namely,
we show how to deﬁne φ, α and ρ that can implement F0. Note that in this direction of the proof the
function F0 is a black-box. Namely, we only know that it is GPI, but do not assume anything else
about its implementation.

4

The key idea is to construct φ, α such that the second argument of ρ in Eq. 1 contains the information
about all the graph features z. Then, the function ρ corresponds to an application of F0 to this
representation, followed by extracting the label yk. To simplify notation assume edge features are
scalar (e = 1). The extension to vectors is simple, but involves more indexing.

We assume WLOG that the black-box function F0 is a function only of the pairwise features zi,j
(otherwise, we can always augment the pairwise features with the singleton features). Since zi,j ∈ R
we use a matrix Rn,n to denote all the pairwise features.

Finally, we assume that our implementation of F0 will take additional node features zk such that no
two nodes have the same feature (i.e., the features identify the node).

Our goal is thus to show that there exist functions α, φ, ρ such that the function in Eq. 2 applied to Z
yields the same labels as F0(Z).

Let H be a hash function with L buckets mapping node features zi to an index (bucket). Assume
that H is perfect (this can be achieved for a large enough L). Deﬁne φ to map the pairwise
features to a vector of size L. Let 1 [j] be a one-hot vector of dimension RL, with one in the
jth coordinate. Recall that we consider scalar zi,j so that φ is indeed in RL, and deﬁne φ as:
φ(zi, zi,j, zj) = 1 [H(zj)] zi,j, i.e., φ “stores” zi,j in the unique bucket for node j.
Let si = (cid:80)
zi,j ∈E φ(zi, zi,j, zj) be the second argument of α in Eq. 1 (si ∈ RL). Then, since all
zj are distinct, si stores all the pairwise features for neighbors of i in unique positions within its
L coordinates. Since si(H(zk)) contains the feature zi,k whereas sj(H(zk)) contains the feature
zj,k, we cannot simply sum the si, since we would lose the information of which edges the features
originated from. Instead, we deﬁne α to map si to RL×L such that each feature is mapped to a
distinct location. Formally:

α(zi, si) = 1 [H(zi)] sT
i
α outputs a matrix that is all zeros except for the features corresponding to node i that are stored in
row H(zi). The matrix M = (cid:80)
i α(zi, si) (namely, the second argument of ρ in Eq. 1) is a matrix
with all the edge features in the graph including the graph structure.

(3)

.

To complete the construction we set ρ to have the same outcome as F0. We ﬁrst discard rows and
columns in M that do not correspond to original nodes (reducing M to dimension n × n). Then, we
use the reduced matrix as the input z to the black-box F0.

Assume for simplicity that M does not need to be contracted (this merely introduces another indexing
step). Then M corresponds to the original matrix Z of pairwise features, with both rows and
columns permuted according to H. We will thus use M as input to the function F0. Since F0 is
GPI, this means that the label for node k will be given by F0(M ) in position H(zk). Thus we set
ρ(zk, M ) = [F0(M )]H(zk), and by the argument above this equals [F0(Z)]k, implying that the
above α, φ and ρ indeed implement F0.

Extension to general graphs So far, we discussed complete graphs, where edges correspond
to valid feature pairs. However, many graphs of interest might be incomplete. For example, an
n-variable chain graph in sequence labeling has only n − 1 edges. For such graphs, the input to F
would not contain all zi,j pairs but rather only features corresponding to valid edges of the graph, and
we are only interested in invariances that preserve the graph structure, namely, the automorphisms
of the graph. Thus, the desired invariance is that σ(F(z)) = F(σ(z)), where σ is not an arbitrary
permutation but an automorphism. It is easy to see that a simple variant of Theorem 1 holds in
this case. All we need to do is replace in Eq. 2 the sum (cid:80)
j∈N (i), where N (i) are the
neighbors of node i in the graph. The arguments are then similar to the proof above.

j(cid:54)=i with (cid:80)

Implications of Theorem 1 Our result has interesting implications for deep structured prediction.
First, it highlights that the fact that the architecture “collects” information from all different edges
of the graph, in an invariant fashion via the α, φ functions. Speciﬁcally, the functions φ (after
summation) aggregate all the features around a given node, and then α (after summation) can
collect them. Thus, these functions can provide a summary of the entire graph that is sufﬁcient for
downstream algorithms. This is different from one round of message passing algorithms which would
not be sufﬁcient for collecting global graph information. Note that the dimensions of φ, α may need
to be large to aggregate all graph information (e.g., by hashing all the features as in the proof of
Theorem 1), but the architecture itself can be shallow.

5

Second, the architecture is parallelizable, as all φ functions can be applied simultaneously. This is in
contrast to recurrent models Zellers et al. (2017) which are harder to parallelize and are thus slower
in practice.

Finally, the theorem suggests several common architectural structures that can be used within GPI.
We brieﬂy mention two of these. 1) Attention: Attention is a powerful component in deep learning
architectures (Bahdanau et al., 2015), but most inference algorithms do not use attention. Intuitively,
in attention each node i aggregates features of neighbors through a weighted sum, where the weight
is a function of the neighbor’s relevance. For example, the label of an entity in an image may depend
more strongly on entities that are spatially closer. Attention can be naturally implemented in our
GPI characterization, and we provide a full derivation for this implementation in the appendix. It
plays a key role in our scene graph model described below. 2) RNNs: Because GPI functions are
closed under composition, for any GPI function F we can run F iteratively by providing the output
of one step of F as part of the input to the next step and maintain GPI. This results in a recurrent
architecture, which we use in our scene graph model.

4 Related Work

The concept of architectural invariance was recently proposed in DEEPSETS (Zaheer et al., 2017).
The invariance we consider is much less restrictive: the architecture does not need to be invariant
to all permutations of singleton and pairwise features, just those consistent with a graph re-labeling.
This characterization results in a substantially different set of possible architectures.

Deep structured prediction. There has been signiﬁcant recent interest in extending deep learning
to structured prediction tasks. Much of this work has been on semantic segmentation, where
convolutional networks (Shelhamer et al., 2017) became a standard approach for obtaining “singleton
scores” and various approaches were proposed for adding structure on top. Most of these approaches
used variants of message passing algorithms, unrolled into a computation graph (Xu et al., 2017).
Some studies parameterized parts of the message passing algorithm and learned its parameters (Lin
et al., 2015). Recently, gradient descent has also been used for maximizing score functions (Belanger
et al., 2017; Gygli et al., 2017). An alternative to deep structured prediction is greedy decoding,
inferring each label at a time based on previous labels. This approach has been popular in sequence-
based applications (e.g., parsing (Chen & Manning, 2014)), relying on the sequential structure of the
input, where BiLSTMs are effectively applied. Another related line of work is applying deep learning
to graph-based problems, such as TSP (Bello et al., 2016; Gilmer et al., 2017; Khalil et al., 2017).
Clearly, the notion of graph invariance is important in these, as highlighted in (Gilmer et al., 2017).
They however do not specify a general architecture that satisﬁes invariance as we do here, and in
fact focus on message passing architectures, which we strictly generalize. Furthermore, our focus is
on the more general problem of structured prediction, rather than speciﬁc graph-based optimization
problems.

Scene graph prediction. Extracting scene graphs from images provides a semantic representation
that can later be used for reasoning, question answering, and image retrieval (Johnson et al., 2015;
Lu et al., 2016; Raposo et al., 2017). It is at the forefront of machine vision research, integrating
challenges like object detection, action recognition and detection of human-object interactions (Liao
et al., 2016; Plummer et al., 2017). Prior work on scene graph predictions used neural message
passing algorithms (Xu et al., 2017) as well as prior knowledge in the form of word embeddings
(Lu et al., 2016). Other work suggested to predict graphs directly from pixels in an end-to-end
manner Newell & Deng (2017). NeuralMotif (Zellers et al., 2017), currently the state-of-the-art
model for scene graph prediction on Visual Genome, employs an RNN that provides global context
by sequentially reading the independent predictions for each entity and relation and then reﬁnes those
predictions. The NEURALMOTIF model maintains GPI by ﬁxing the order in which the RNN reads
its inputs and thus only a single order is allowed. However, this ﬁxed order is not guaranteed to be
optimal.

5 Experimental Evaluation

We empirically evaluate the beneﬁt of GPI architectures. First, using a synthetic graph-labeling task,
and then for the problem of mapping images to scene graphs.

6

Figure 3: Accuracy as a function of sample size for graph labeling. Right is a zoomed in version of left.

5.1 Synthetic Graph Labeling

We start with studying GPI on a synthetic problem, deﬁned as follows. An input graph G = (V, E)
is given, where each node i ∈ V is assigned to one of K sets. The set for node i is denoted by
Γ(i). The goal is to compute for each node the number of neighbors that belong to the same set.
Namely, the label of a node is yi = (cid:80)
j∈N (i) 1[Γ(i) = Γ(j)]. We generated random graphs with 10
nodes (larger graphs produced similar results) by sampling each edge independently and uniformly,
and sampling Γ(i) for every node uniformly from {1, . . . , K}. The node features zi ∈ {0, 1}K are
one-hot vectors of Γ(i) and the edge features zi,j ∈ {0, 1} indicate whether ij ∈ E. We compare two
standard non-GPI architectures and one GPI architecture: (a) A GPI-architecture for graph prediction,
described in detail in Section 5.2. We used the basic version without attention and RNN. (b) LSTM:
We replace (cid:80) φ(·) and (cid:80) α(·), which perform aggregation in Theorem 1 with two LSTMs with
a state size of 200 that read their input in random order. (c) A fully-connected (FC) feed-forward
network with 2 hidden layers of 1000 nodes each. The input to the fully connected model is a
concatenation of all node and pairwise features. The output is all node predictions. The focus of the
experiment is to study sample complexity. Therefore, for a fair comparison, we use the same number
of parameters for all models.

Figure 3, shows the results, demonstrating that GPI requires far fewer samples to converge to the
correct solution. This illustrates the advantage of an architecture with the correct inductive bias for
the problem.

5.2 Scene-Graph Classiﬁcation

We evaluate the GPI approach on the motivating task of this paper, inferring scene graphs from
images (Figure 1). In this problem, the input is an image annotated with a set of bounding boxes for
the entities in the image.2 The goal is to label each bounding box with the correct entity category and
every pair of entities with their relation, such that they form a coherent scene graph.

We begin by describing our Scene Graph Predictor (SGP) model. We aim to predict two types of
variables. The ﬁrst is entity variables [y1, . . . , yn] for all bounding boxes. Each yi can take one of
L values (e.g., “dog”, “man”). The second is relation variables [yn+1, . . . , yn2] for every pair of
bounding boxes. Each such yj can take one of R values (e.g., “on”, “near”). Our graph connects
variables that are expected to be inter-related. It contains two types of edges: 1) entity-entity edge
connecting every two entity variables (yi and yj for 1 ≤ i (cid:54)= j ≤ n. 2) entity-relation edges
connecting every relation variable yk (where k > n) to its two entity variables. Thus, our graph is not
a complete graph and our goal is to design an architecture that will be invariant to any automorphism
of the graph, such as permutations of the entity variables.

For the input features z, we used the features learned by the baseline model from Zellers et al.
(2017).3 Speciﬁcally, the entity features zi included (1) The conﬁdence probabilities of all entities
for yi as learned by the baseline model. (2) Bounding box information given as (left, bottom,
width, height); (3) The number of smaller entities (also bigger); (4) The number of entities to
the left, right, above and below. (5) The number of entities with higher and with lower conﬁdence;

2For simplicity, we focus on the task where boxes are given.
3The baseline does not use any LSTM or context, and is thus unrelated to the main contribution of Zellers

et al. (2017).

7

Constrained Evaluation

SGCls

PredCls

Unconstrained Evaluation
SGCls

PredCls

R@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100

Lu et al., 2016 (Lu et al., 2016)
Xu et al., 2017 (Xu et al., 2017)
Pixel2Graph (Newell & Deng, 2017)
Graph R-CNN (Yang et al., 2018)
Neural Motifs (Zellers et al., 2017)
Baseline (Zellers et al., 2017)
No Attention
Neighbor Attention
Linguistic

11.8
21.7
-
29.6
35.8
34.6
35.3
35.7
36.5

14.1
24.4
-
31.6
36.5
35.3
37.2
38.5
38.8

35.0
44.8
-
54.2
65.2
63.7
64.5
64.6
65.1

27.9
53.0
-
59.1
67.1
65.6
66.3
66.6
66.9

-
-
26.5
-
44.5
43.4
44.1
44.7
45.5

-
-
30.0
-
47.7
46.6
48.5
49.9
50.8

-
-
68.0
-
81.1
78.8
79.7
80.0
80.8

-
-
75.2
-
88.3
85.9
86.7
87.1
88.2

Table 1: Test set results for graph-constrained evaluation (i.e., the returned triplets must be consistent with a
scene graph) and for unconstrained evaluation (triplets need not be consistent with a scene graph).

(6) For the linguistic model only: word embedding of the most probable class. Word vectors were
learned with GLOVE from the ground-truth captions of Visual Genome.
Similarly, the relation features zj ∈ RR contained the probabilities of relation entities for the relation
j. For the Linguistic model, these features were extended to include word embedding of the most
probable class. For entity-entity pairwise features zi,j, we use the relation probability for each pair.
Because the output of SGP are probability distributions over entities and relations, we use them as an
the input z to SGP, once again in a recurrent manner and maintain GPI.

We next describe the main components of the GPI architecture. First, we focus on the parts that
output the entity labels. φent is the network that integrates features for two entity variables yi
and yj. It simply takes zi, zj and zi,j as input, and outputs a vector of dimension n1. Next, the
network αent takes as input the outputs of φent for all neighbors of an entity, and uses the attention
mechanism described above to output a vector of dimension n2. Finally, the ρent network takes these
n2 dimensional vectors and outputs L logits predicting the entity value. The ρrel network takes as
input the αent representation of the two entities, as well as zi,j and transforms the output into R
logits. See appendix for speciﬁc network architectures.

5.2.1 Experimental Setup and Results

Dataset. We evaluated our approach on Visual Genome (VG) (Krishna et al., 2017), a dataset with
108,077 images annotated with bounding boxes, entities and relations. On average, images have 12
entities and 7 relations per image. For a proper comparison with previous results (Newell & Deng,
2017; Xu et al., 2017; Zellers et al., 2017), we used the data from (Xu et al., 2017), including the
train and test splits. For evaluation, we used the same 150 entities and 50 relations as in (Newell
& Deng, 2017; Xu et al., 2017; Zellers et al., 2017). To tune hyper-parameters, we also split the
training data into two by randomly selecting 5K examples, resulting in a ﬁnal 70K/5K/32K split for
train/validation/test sets.

Training. All networks were trained using Adam (Kingma & Ba, 2014) with batch size 20. Hyper-
parameter values below were chosen based on the validation set. The SGP loss function was the sum
of cross-entropy losses over all entities and relations in the image. In the loss, we penalized entities
4 times more strongly than relations, and penalized negative relations 10 times more weakly than
positive relations.

Evaluation.
In (Xu et al., 2017) three different evaluation settings were considered. Here we
focus on two of these: (1) SGCls: Given ground-truth bounding boxes for entities, predict all entity
categories and relations categories. (2) PredCls: Given bounding boxes annotated with entity labels,
predict all relations. Following (Lu et al., 2016), we used Recall@K as the evaluation metric. It
measures the fraction of correct ground-truth triplets that appear within the K most conﬁdent triplets
proposed by the model. Two evaluation protocols are used in the literature differing in whether they
enforce graph constraints over model predictions. The ﬁrst graph-constrained protocol requires that
the top-K triplets assign one consistent class per entity and relation. The second unconstrained
protocol does not enforce any such constraints. We report results on both protocols, following (Zellers
et al., 2017).

8

Figure 4: (a) An input image with bounding boxes from VG. (b) The ground-truth scene graph. (c) The
Baseline fails to recognize some entities (tail and tree) and relations (in front of instead of looking at). (d)
GPI:LINGUISTIC ﬁxes most incorrect LP predictions. (e) Window is the most signiﬁcant neighbor of Tree. (f)
The entity bird receives substantial attention, while Tree and building are less informative.

Models and baselines. We compare four variants of our GPI approach with the reported results
of four baselines that are currently the state-of-the-art on various scene graph prediction problems
(all models use the same data split and pre-processing as (Xu et al., 2017)): 1) LU ET AL., 2016
(LU ET AL., 2016): This work leverages word embeddings to ﬁne-tune the likelihood of predicted
relations. 2) XU ET AL, 2017 (XU ET AL., 2017): This model passes messages between entities and
relations, and iteratively reﬁnes the feature map used for prediction. 3) NEWELL & DENG, 2017
(NEWELL & DENG, 2017): The PIXEL2GRAPH model uses associative embeddings (Newell et al.,
2017) to produce a full graph from the image. 4) YANG ET AL., 2018 (YANG ET AL., 2018): The
GRAPH R-CNN model uses object-relation regularities to sparsify and reason over scene graphs. 5)
ZELLERS ET AL., 2017 (ZELLERS ET AL., 2017): The NEURALMOTIF method encodes global
context for capturing high-order motifs in scene graphs, and the BASELINE outputs the entities and
relations distributions without using the global context. The following variants of GPI were compared:
1) GPI: NO ATTENTION: Our GPI model, but with no attention mechanism. Instead, following
Theorem 1, we simply sum the features. 2) GPI: NEIGHBORATTENTION: Our GPI model, with
attention over neighbors features. 3) GPI: LINGUISTIC: Same as GPI: NEIGHBORATTENTION but
also concatenating the word embedding vector, as described above.

Results. Table 1 shows recall@50 and recall@100 for three variants of our approach, and compared
with ﬁve baselines. All GPI variants performs well, with LINGUISTIC outperforming all baselines
for SGCls and being comparable to the state-of-the-art model for PredCls. Note that PredCl is an
easier task, which makes less use of the structure, hence it is not surprising that GPI achieves similar
accuracy to Zellers et al. (2017). Figure 4 illustrates the model behavior. Predicting isolated labels
with zi (4c) mislabels several entities, but these are corrected at the ﬁnal output (4d). Figure 4e
shows that the system learned to attend more to nearby entities (the window and building are closer
to the tree), and 4f shows that stronger attention is learned for the class bird, presumably because it is
usually more informative than common classes like tree.

Implementation details. The φ and α networks were each implemented as a single fully-connected
(FC) layer with a 500-dimensional outputs. ρ was implemented as a FC network with 3 500-
dimensional hidden layers, with one 150-dimensional output for the entity probabilities, and one
51-dimensional output for relation probabilities. The attention mechanism was implemented as a
network like to φ and α, receiving the same inputs, but using the output scores for the attention . The
full code is available at https://github.com/shikorab/SceneGraph

6 Conclusion

We presented a deep learning approach to structured prediction, which constrains the architecture
to be invariant to structurally identical inputs. As in score-based methods, our approach relies on
pairwise features, capable of describing inter-label correlations, and thus inheriting the intuitive
aspect of score-based approaches. However, instead of maximizing a score function (which leads
to computationally-hard inference), we directly produce an output that is invariant to equivalent
representations of the pairwise terms.

9

This axiomatic approach to model architecture can be extended in many ways. For image labeling,
geometric invariances (shift or rotation) may be desired.
In other cases, invariance to feature
permutations may be desirable. We leave the derivation of the corresponding architectures to future
work. Finally, there may be cases where the invariant structure is unknown and should be discovered
from data, which is related to work on lifting graphical models Bui et al. (2013). It would be interesting
to explore algorithms that discover and use such symmetries for deep structured prediction.

This work was supported by the ISF Centers of Excellence grant, and by the Yandex Initiative in
Machine Learning. Work by GC was performed while at Google Brain Research.

Acknowledgements

References

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and

translate. In International Conference on Learning Representations (ICLR), 2015.

Belanger, David, Yang, Bishan, and McCallum, Andrew. End-to-end learning for structured prediction
energy networks. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70, pp. 429–439. PMLR, 2017.

Bello, Irwan, Pham, Hieu, Le, Quoc V, Norouzi, Mohammad, and Bengio, Samy. Neural combinato-

rial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Bui, Hung Hai, Huynh, Tuyen N., and Riedel, Sebastian. Automorphism groups of graphical models
and lifted variational inference. In Proceedings of the Twenty-Ninth Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’13, pp. 132–141, 2013.

Chen, Danqi and Manning, Christopher. A fast and accurate dependency parser using neural
networks. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 740–750, 2014.

Chen, Liang Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Se-
mantic image segmentation with deep convolutional nets and fully connected CRFs. In Proceedings
of the Second International Conference on Learning Representations, 2014.

Chen, Liang Chieh, Schwing, Alexander G, Yuille, Alan L, and Urtasun, Raquel. Learning deep

structured models. In Proc. ICML, 2015.

Farabet, Clement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical
features for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):
1915–1929, 2013.

Gilmer, Justin, Schoenholz, Samuel S, Riley, Patrick F, Vinyals, Oriol, and Dahl, George E. Neural

message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.

Gygli, Michael, Norouzi, Mohammad, and Angelova, Anelia. Deep value networks learn to evaluate
and iteratively reﬁne structured outputs. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine
Learning Research, pp. 1341–1351, International Convention Centre, Sydney, Australia, 2017.
PMLR.

Johnson, Justin, Krishna, Ranjay, Stark, Michael, Li, Li-Jia, Shamma, David A., Bernstein, Michael S.,
In Proc. Conf. Comput. Vision Pattern

Image retrieval using scene graphs.

and Li, Fei-Fei.
Recognition, pp. 3668–3678, 2015.

Johnson, Justin, Gupta, Agrim, and Fei-Fei, Li. Image generation from scene graphs. arXiv preprint

arXiv:1804.01622, 2018.

Khalil, Elias, Dai, Hanjun, Zhang, Yuyu, Dilkina, Bistra, and Song, Le. Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp.
6351–6361, 2017.

10

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint

arXiv: 1412.6980, abs/1412.6980, 2014.

Krishna, Ranjay, Zhu, Yuke, Groth, Oliver, Johnson, Justin, Hata, Kenji, Kravitz, Joshua, Chen,
Stephanie, Kalantidis, Yannis, Li, Li-Jia, Shamma, David A, et al. Visual genome: Connecting
International Journal of
language and vision using crowdsourced dense image annotations.
Computer Vision, 123(1):32–73, 2017.

Lafferty, J., McCallum, A., and Pereira, F. Conditional random ﬁelds: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning, pp. 282–289, 2001.

Liao, Wentong, Yang, Michael Ying, Ackermann, Hanno, and Rosenhahn, Bodo. On support relations

and semantic scene graphs. arXiv preprint arXiv:1609.05834, 2016.

Lin, Guosheng, Shen, Chunhua, Reid, Ian, and van den Hengel, Anton. Deeply learning the messages
in message passing inference. In Advances in Neural Information Processing Systems, pp. 361–369,
2015.

Lu, Cewu, Krishna, Ranjay, Bernstein, Michael S., and Li, Fei-Fei. Visual relationship detection with

language priors. In European Conf. Comput. Vision, pp. 852–869, 2016.

Meshi, O., Sontag, D., Jaakkola, T., and Globerson, A. Learning efﬁciently with approximate
In Proceedings of the 27th International Conference on Machine

inference via dual losses.
Learning, pp. 783–790, New York, NY, USA, 2010. ACM.

Newell, Alejandro and Deng, Jia. Pixels to graphs by associative embedding. In Advances in Neural
Information Processing Systems 30 (to appear), pp. 1172–1180. Curran Associates, Inc., 2017.

Newell, Alejandro, Huang, Zhiao, and Deng, Jia. Associative embedding: End-to-end learning for
joint detection and grouping. In Neural Inform. Process. Syst., pp. 2274–2284. Curran Associates,
Inc., 2017.

Pei, Wenzhe, Ge, Tao, and Chang, Baobao. An effective neural network model for graph-based de-
pendency parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computationa
Linguistics, pp. 313–322, 2015.

Plummer, Bryan A., Mallya, Arun, Cervantes, Christopher M., Hockenmaier, Julia, and Lazebnik,
Svetlana. Phrase localization and visual relationship detection with comprehensive image-language
cues. In ICCV, pp. 1946–1955, 2017.

Raposo, David, Santoro, Adam, Barrett, David, Pascanu, Razvan, Lillicrap, Timothy, and Battaglia,
Peter. Discovering objects and their relations from entangled scene representations. arXiv preprint
arXiv:1702.05068, 2017.

Schwing, Alexander G and Urtasun, Raquel. Fully connected deep structured networks. ArXiv

e-prints, 2015.

Shelhamer, Evan, Long, Jonathan, and Darrell, Trevor. Fully convolutional networks for semantic

segmentation. Proc. Conf. Comput. Vision Pattern Recognition, 39(4):640–651, 2017.

Taskar, B., Guestrin, C., and Koller, D. Max margin Markov networks. In Thrun, S., Saul, L., and
Schölkopf, B. (eds.), Advances in Neural Information Processing Systems 16, pp. 25–32. MIT
Press, Cambridge, MA, 2004.

Xu, Danfei, Zhu, Yuke, Choy, Christopher B., and Fei-Fei, Li. Scene Graph Generation by Iterative
Message Passing. In Proc. Conf. Comput. Vision Pattern Recognition, pp. 3097–3106, 2017.

Yang, Jianwei, Lu, Jiasen, Lee, Stefan, Batra, Dhruv, and Parikh, Devi. Graph R-CNN for scene

graph generation. In European Conf. Comput. Vision, pp. 690–706, 2018.

Zaheer, Manzil, Kottur, Satwik, Ravanbakhsh, Siamak, Poczos, Barnabas, Salakhutdinov, Ruslan R,
and Smola, Alexander J. Deep sets. In Advances in Neural Information Processing Systems 30, pp.
3394–3404. Curran Associates, Inc., 2017.

11

Zellers, Rowan, Yatskar, Mark, Thomson, Sam, and Choi, Yejin. Neural motifs: Scene graph parsing

with global context. arXiv preprint arXiv:1711.06640, abs/1711.06640, 2017.

Zheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav, Su, Zhizhong, Du,
Dalong, Huang, Chang, and Torr, Philip HS. Conditional random ﬁelds as recurrent neural networks.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 1529–1537, 2015.

7 Supplementary Material

This supplementary material includes: (1) Visual illustration of the proof of Theorem 1. (2) Explaining
how to integrate an attention mechanism in our GPI framework. (3) Additional evaluation method to
further analyze and compare our work with baselines.

7.1 Theorem 1: Illustration of Proof

Figure 5: Illustration of the proof of Theorem 1 using a speciﬁc construction example. Here H is a hash function
of size L = 5 such that H(1) = 1, H(3) = 2, H(2) = 4, G is a three-node input graph, and zi,j ∈ R are the
pairwise features (in purple) of G. (a) φ is applied to each zi,j. Each application yields a vector in R5. The three
dark yellow columns correspond to φ(z1,1), φ(z1,2) and φ(z1,3). Then, all vectors φ(zi,j) are summed over j
to obtain three si vectors. (b) α’s (blue matrices) are an outer product between 1 [H(zi)] and si resulting in a
matrix of zeros except one row. The dark blue matrix corresponds for α(z1, s1). (c) All α’s are summed to a
5 × 5 matrix, isomorphic to the original zi,j matrix.

7.2 Characterizing Permutation Invariance: Attention

Attention is a powerful component which naturally can be introduced into our GPI model. We now
show how attention can be introduced in our framework. Formally, we learn attention weights for the
neighbors j of a node i, which scale the features zi,j of that neighbor. We can also learn different
attention weights for individual features of each neighbor in a similar way.

Let wi,j ∈ R be an attention mask specifying the weight that node i gives to node j:

wi,j(zi, zi,j, zj) = eβ(zi,zi,j ,zj )/

eβ(zi,zi,t,zt)

(4)

(cid:88)

t

where β can be any scalar-valued function of its arguments (e.g., a dot product of zi and zj as in
standard attention models). To introduce attention we wish α ∈ Re to have the form of weighting
wi,j over neighboring feature vectors zi,j, namely, α = (cid:80)
To achieve this form we extend φ by a single entry, deﬁning φ ∈ Re+1 (namely we set
L = e + 1) as φ1:e(zi, zi,j, zj) = eβ(zi,zi,j ,zj )zi,j (here φ1:e are the ﬁrst e elements of φ)

j(cid:54)=i wi,jzi,j.

12

and φe+1(zi, zi,j; zj) = eβ(zi,zi,j ,zj ). We keep the deﬁnition of si = (cid:80)
we deﬁne α = si,1:e
si,e+1
over neighboring feature vectors zi,j:

j(cid:54)=i φ(zi, zi,j, zj). Next,
and substitute si and φ to obtain the desired form as attention weights wi,j

α(zi, si) =

si,1:e
si,e+1

(cid:88)

=

j(cid:54)=i

eβ(zi,zi,j ,zj )zi,j
(cid:80)
j(cid:54)=i eβ(zi,zi,j ,zj )

(cid:88)

=

j(cid:54)=i

wi,jzi,j

A similar approach can be applied over α and ρ to model attention over the outputs of α as well
(graph nodes).

7.3 Scene Graph Results

In the main paper, we described the results for the two prediction tasks: SGCls and PredCls, as
deﬁned in section 5.2.1: "Experimental Setup and Results". To further analyze our module, we
compare the best variant, GPI: LINGUISTIC, per relation to two baselines: (Lu et al., 2016) and Xu
et al. (2017). Table 2, speciﬁes the PredCls recall@5 of the 20-top frequent relation classes. The GPI
module performs better in almost all the relations classes.

RELATION

(LU ET AL., 2016)

(XU ET AL., 2017)

LINGUISTIC

99.71
98.03
80.38
82.47
98.47
85.16
31.85
49.19
61.50
79.35
28.64
31.74
26.09
8.45
54.08

ON
HAS
IN
OF
WEARING
NEAR
WITH
ABOVE
HOLDING
BEHIND
UNDER
SITTING ON
IN FRONT OF
ATTACHED TO
AT
HANGING FROM 0.0
OVER
FOR
RIDING

9.26
12.20
72.43

99.25
97.25
88.30
96.75
98.23
96.81
88.10
79.73
80.67
92.32
52.73
50.17
59.63
29.58
70.41
0.0
0.0
31.71
89.72

99.3
98.7
95.9
98.1
99.6
95.4
94.2
83.9
95.5
91.2
83.2
90.4
74.9
77.4
80.9
74.1
62.4
45.1
96.1

Table 2: Recall@5 of PredCls for the 20-top relations ranked by their frequency, as in (Xu et al., 2017)

13

8
1
0
2
 
v
o
N
 
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
5
4
5
0
.
2
0
8
1
:
v
i
X
r
a

Mapping Images to Scene Graphs with
Permutation-Invariant Structured Prediction

Roei Herzig∗
Tel Aviv University
roeiherzig@mail.tau.ac.il

Moshiko Raboh∗
Tel Aviv University
mosheraboh@mail.tau.ac.il

Gal Chechik
Bar-Ilan University, NVIDIA Research
gal.chechik@biu.ac.il

Jonathan Berant
Tel Aviv University, AI2
joberant@cs.tau.ac.il

Amir Globerson
Tel Aviv University
gamir@post.tau.ac.il

Abstract

Machine understanding of complex images is a key goal of artiﬁcial intelligence.
One challenge underlying this task is that visual scenes contain multiple inter-
related objects, and that global context plays an important role in interpreting
the scene. A natural modeling framework for capturing such effects is structured
prediction, which optimizes over complex labels, while modeling within-label
interactions. However, it is unclear what principles should guide the design of a
structured prediction model that utilizes the power of deep learning components.
Here we propose a design principle for such architectures that follows from a
natural requirement of permutation invariance. We prove a necessary and sufﬁ-
cient characterization for architectures that follow this invariance, and discuss its
implication on model design. Finally, we show that the resulting model achieves
new state-of-the-art results on the Visual Genome scene-graph labeling benchmark,
outperforming all recent approaches.

1

Introduction

Understanding the semantics of a complex visual scene is a fundamental problem in machine
perception. It often requires recognizing multiple objects in a scene, together with their spatial and
functional relations. The set of objects and relations is sometimes represented as a graph, connecting
objects (nodes) with their relations (edges) and is known as a scene graph (Figure 1). Scene graphs
provide a compact representation of the semantics of an image, and can be useful for semantic-level
interpretation and reasoning about a visual scene Johnson et al. (2018). Scene-graph prediction is the
problem of inferring the joint set of objects and their relations in a visual scene.

Since objects and relations are inter-dependent (e.g., a person and chair are more likely to be in relation
“sitting on” than “eating”), a scene graph predictor should capture this dependence in order to improve
prediction accuracy. This goal is a special case of a more general problem, namely, inferring multiple
inter-dependent labels, which is the research focus of the ﬁeld of structured prediction. Structured
prediction has attracted considerable attention because it applies to many learning problems and

∗Equal Contribution.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Figure 1: An image and its scene graph from the Visual Genome dataset (Krishna et al., 2017). The scene graph
captures the entities in the image (nodes, blue circles) like dog and their relations (edges, red circles) like (cid:10)hat,
on, dog(cid:11).

poses unique theoretical and algorithmic challenges (e.g., see Belanger et al., 2017; Chen et al., 2015;
Taskar et al., 2004). It is therefore a natural approach for predicting scene graphs from images.

Structured prediction models typically deﬁne a score function s(x, y) that quantiﬁes how well a
label assignment y is compatible with an input x. In the case of understanding complex visual
scenes, x is an image, and y is a complex label containing the labels of objects detected in an image
and the labels of their relations. In this setup, the inference task amounts to ﬁnding the label that
maximizes the compatibility score y∗ = arg maxy s(x, y). This score-based approach separates a
scoring component – implemented by a parametric model, from an optimization component – aimed
at ﬁnding a label that maximizes that score. Unfortunately, for a general scoring function s(·), the
space of possible label assignments grows exponentially with input size. For instance, for scene
graphs the set of possible object label assignments is too large even for relatively simple images,
since the vocabulary of candidate objects may contain thousands of objects. As a result, inferring the
label assignment that maximizes a scoring function is computationally hard in the general case.

An alternative approach to score-based methods is to map an input x to a structured output y with
a “black box" neural network, without explicitly deﬁning a score function. This raises a natural
question: what is the right architecture for such a network? Here we take an axiomatic approach and
argue that one important property such networks should satisfy is invariance to a particular type of
input permutation. We then prove that this invariance is equivalent to imposing certain structural
constraints on the architecture of the network, and describe architectures that satisfy these constraints.

To evaluate our approach, we ﬁrst demonstrate on a synthetic dataset that respecting permutation
invariance is important, because models that violate this invariance need more training data, despite
having a comparable model size. Then, we tackle the problem of scene graph generation. We describe
a model that satisﬁes the permutation invariance property, and show that it achieves state-of-the-art
results on the competitive Visual Genome benchmark (Krishna et al., 2017), demonstrating the power
of our new design principle.

In summary, the novel contributions of this paper are: a) Deriving sufﬁcient and necessary conditions
for graph-permutation invariance in deep structured prediction architectures. b) Empirically demon-
strating the beneﬁt of graph-permutation invariance. c) Developing a state-of-the-art model for scene
graph prediction on a large dataset of complex visual scenes.

2 Structured Prediction

Scored-based methods in structured prediction deﬁne a function s(x, y) that quantiﬁes the degree
to which y is compatible with x, and infer a label by maximizing s(x, y) (e.g., see Belanger et al.,
2017; Chen et al., 2015; Lafferty et al., 2001; Meshi et al., 2010; Taskar et al., 2004). Most score
functions previously used decompose as a sum over simpler functions, s(x, y) = (cid:80)
i fi(x, y), making
it possible to optimize maxy fi(x, y) efﬁciently. This local maximization forms the basic building
block of algorithms for approximately maximizing s(x, y). One way to decompose the score function
is to restrict each fi(x, y) to depend only on a small subset of the y variables.

The renewed interest in deep learning led to efforts to integrate deep networks with structured predic-
tion, including modeling the fi functions as deep networks. In this context, the most widely-used
score functions are singleton fi(yi, x) and pairwise fij(yi, yj, x). The early work taking this approach
used a two-stage architecture, learning the local scores independently of the structured prediction

2

Figure 2: Left: Graph permutation invariance. A graph labeling function F is graph permutation invariant
(GPI) if permuting the node features maintains the output. Right: a schematic representation of the GPI
architecture in Theorem 1. Singleton features zi are omitted for simplicity. (a) First, the features zi,j are
processed element-wise by φ. (b) Features are summed to create a vector si, which is concatenated with zi. (c)
A representation of the entire graph is created by applying α n times and summing the created vector. (d) The
graph representation is then ﬁnally processed by ρ together with zk.

goal (Chen et al., 2014; Farabet et al., 2013). Later studies considered end-to-end architectures where
the inference algorithm is part of the computation graph (Chen et al., 2015; Pei et al., 2015; Schwing
& Urtasun, 2015; Zheng et al., 2015). Recent studies go beyond pairwise scores, also modelling
global factors (Belanger et al., 2017; Gygli et al., 2017).

Score-based methods provide several advantages. First, they allow intuitive speciﬁcation of local
dependencies between labels and how these translate to global dependencies. Second, for linear
score functions, the learning problem has natural convex surrogates Lafferty et al. (2001); Taskar
et al. (2004). Third, inference in large label spaces is sometimes possible via exact algorithms or
empirically accurate approximations. However, with the advent of deep scoring functions s(x, y; w),
learning is no longer convex. Thus, it is worthwhile to rethink the architecture of structured prediction
models, and consider models that map inputs x to outputs y directly without explicitly maximizing a
score function. We would like these models to enjoy the expressivity and predictive power of neural
networks, while maintaining the ability to specify local dependencies between labels in a ﬂexible
manner. In the next section, we present such an approach and consider a natural question: what
should be the properties of a deep neural network used for structured prediction.

3 Permutation-Invariant Structured Prediction

In what follows we deﬁne the permutation-invariance property for structured prediction models, and
argue that permutation invariance is a natural principle for designing their architecture.

We ﬁrst introduce our notation. We focus on structures with pairwise interactions, because they are
simpler in terms of notation and are sufﬁcient for describing the structure in many problems. We
denote a structured label by y = [y1, . . . , yn]. In a score-based approach, the score is deﬁned via a
set of singleton scores fi(yi, x) and pairwise scores fij(yi, yj, x), where the overall score s(x, y) is
the sum of these scores. For brevity, we denote fij = fij(yi, yj, x) and fi = fi(yi, x). An inference
algorithm takes as input the local scores fi, fij and outputs an assignment that maximizes s(x, y).
We can thus view inference as a black-box that takes node-dependent and edge-dependent inputs
(i.e., the scores fi, fij) and returns a label y, even without an explicit score function s(x, y). While
numerous inference algorithms exist for this setup, including belief propagation (BP) and mean ﬁeld,
here we develop a framework for a deep labeling algorithm (we avoid the term “inference” since the
algorithm does not explicitly maximize a score function). Such an algorithm will be a black-box,
taking the f functions as input and the labels y1, . . . , yn as output. We next ask what architecture
such an algorithm should have.

We follow with several deﬁnitions. A graph labeling function F : (V, E) → Y is a function whose
input is an ordered set of node features V = [z1, . . . , zn] and an ordered set of edge features
E = [z1,2 . . . , zi,j, . . . , zn,n−1]. For example, zi can be the array of values fi, and zi,j can be
the table of values fi,j. Assume zi ∈ Rd and zi,j ∈ Re. The output of F is a set of node labels
y = [y1, . . . , yn]. Thus, algorithms such as BP are graph labeling functions. However, graph labeling
functions do not necessarily maximize a score function. We denote the joint set of node features and
edge features by z (i.e., a set of n + n(n − 1) = n2 vectors). In Section 3.1 we discuss extensions to
this case where only a subset of the edges is available.

3

1, y∗

2, y∗

1, y∗

2, y∗

A natural requirement is that the function F produces the same result when given the same features,
up to a permutation of the input. For example, consider a label space with three variables y1, y2, y3,
and assume that F takes as input z = (z1, z2, z3, z12, z13, z23) = (f1, f2, f3, f12, f13, f23), and
outputs a label y = (y∗
3). When F is given an input that is permuted in a consistent way, say,
z(cid:48) = (f2, f1, f3, f21, f23, f13), this deﬁnes exactly the same input. Hence, the output should still be
y = (y∗
3). Most inference algorithms, including BP and mean ﬁeld, satisfy this symmetry
requirement by design, but this property is not guaranteed in general in a deep model. Here, our
goal is to design a deep learning black-box, and hence we wish to guarantee invariance to input
permutations. A black-box that violates this invariance “wastes” capacity on learning it at training
time, which increases sample complexity, as shown in Sec. 5.1. We proceed to formally deﬁne the
permutation invariance property.
Deﬁnition 1. Let z be a set of node features and edge features, and let σ be a permutation of
{1, . . . , n}. We deﬁne σ(z) to be a new set of node and edge features given by [σ(z)]i = zσ(i) and
[σ(z)]i,j = zσ(i),σ(j).

We also use the notation σ([y1, . . . , yn]) = [yσ(1), . . . , yσ(n)] for permuting the labels. Namely, σ
applied to a set of labels yields the same labels, only permuted by σ. Be aware that applying σ to
the input features is different from permuting labels, because edge input features must permuted in a
way that is consistent with permuting node input features. We now provide our key deﬁnition of a
function whose output is invariant to permutations of the input. See Figure 2 (left).
Deﬁnition 2. A graph labeling function F is said to be graph-permutation invariant (GPI), if for
all permutations σ of {1, . . . , n} and for all z it satisﬁes: F(σ(z)) = σ(F(z)).

3.1 Characterizing Permutation Invariance

Motivated by the above discussion, we ask: what structure is necessary and sufﬁcient to guarantee
that F is GPI? Note that a function F takes as input an ordered set z. Therefore its output on z
could certainly differ from its output on σ(z). To achieve permutation invariance, F should contain
certain symmetries. For instance, one permutation invariant architecture could be to deﬁne yi = g(zi)
for any function g, but this architecture is too restrictive and does not cover all permutation invariant
functions. Theorem 1 below provides a complete characterization (see Figure 2 for the corresponding
architecture). Intuitively, the architecture in Theorem 1 is such that it can aggregate information from
the entire graph, and do so in a permutation invariant manner.
Theorem 1. Let F be a graph labeling function. Then F is graph-permutation invariant if and only
if there exist functions α, ρ, φ such that for all k = 1, . . . , n:

[F(z)]k = ρ

zk,

α

zi,

φ(zi, zi,j, zj)



 ,

(1)









n
(cid:88)

i=1

(cid:88)

j(cid:54)=i

where φ : R2d+e → RL, α : Rd+L → RW and ρ : RW +d → R.

Proof. First, we show that any F satisfying the conditions of Theorem 1 is GPI. Namely, for any
permutation σ, [F(σ(z))]k = [F(z)]σ(k). To see this, write [F(σ(z))]k using Eq. 1 and Deﬁnition 1:

[F(σ(z))]k = ρ(zσ(k),

α(zσ(i),

φ(zσ(i), zσ(i),σ(j), zσ(j)))).

(2)

(cid:88)

i

(cid:88)

j(cid:54)=i

The second argument of ρ above is invariant under σ, because it is a sum over nodes and their
neighbors, which is invariant under permutation. Thus Eq. 2 is equal to:

ρ(zσ(k),

α(zi,

φ(zi, zi,j, zj))) = [F(z)]σ(k)

(cid:88)

i

(cid:88)

j(cid:54)=i

where equality follows from Eq. 1. We thus proved that Eq. 1 implies graph permutation invariance.

Next, we prove that any given GPI function F0 can be expressed as a function F in Eq. 1. Namely,
we show how to deﬁne φ, α and ρ that can implement F0. Note that in this direction of the proof the
function F0 is a black-box. Namely, we only know that it is GPI, but do not assume anything else
about its implementation.

4

The key idea is to construct φ, α such that the second argument of ρ in Eq. 1 contains the information
about all the graph features z. Then, the function ρ corresponds to an application of F0 to this
representation, followed by extracting the label yk. To simplify notation assume edge features are
scalar (e = 1). The extension to vectors is simple, but involves more indexing.

We assume WLOG that the black-box function F0 is a function only of the pairwise features zi,j
(otherwise, we can always augment the pairwise features with the singleton features). Since zi,j ∈ R
we use a matrix Rn,n to denote all the pairwise features.

Finally, we assume that our implementation of F0 will take additional node features zk such that no
two nodes have the same feature (i.e., the features identify the node).

Our goal is thus to show that there exist functions α, φ, ρ such that the function in Eq. 2 applied to Z
yields the same labels as F0(Z).

Let H be a hash function with L buckets mapping node features zi to an index (bucket). Assume
that H is perfect (this can be achieved for a large enough L). Deﬁne φ to map the pairwise
features to a vector of size L. Let 1 [j] be a one-hot vector of dimension RL, with one in the
jth coordinate. Recall that we consider scalar zi,j so that φ is indeed in RL, and deﬁne φ as:
φ(zi, zi,j, zj) = 1 [H(zj)] zi,j, i.e., φ “stores” zi,j in the unique bucket for node j.
Let si = (cid:80)
zi,j ∈E φ(zi, zi,j, zj) be the second argument of α in Eq. 1 (si ∈ RL). Then, since all
zj are distinct, si stores all the pairwise features for neighbors of i in unique positions within its
L coordinates. Since si(H(zk)) contains the feature zi,k whereas sj(H(zk)) contains the feature
zj,k, we cannot simply sum the si, since we would lose the information of which edges the features
originated from. Instead, we deﬁne α to map si to RL×L such that each feature is mapped to a
distinct location. Formally:

α(zi, si) = 1 [H(zi)] sT
i
α outputs a matrix that is all zeros except for the features corresponding to node i that are stored in
row H(zi). The matrix M = (cid:80)
i α(zi, si) (namely, the second argument of ρ in Eq. 1) is a matrix
with all the edge features in the graph including the graph structure.

(3)

.

To complete the construction we set ρ to have the same outcome as F0. We ﬁrst discard rows and
columns in M that do not correspond to original nodes (reducing M to dimension n × n). Then, we
use the reduced matrix as the input z to the black-box F0.

Assume for simplicity that M does not need to be contracted (this merely introduces another indexing
step). Then M corresponds to the original matrix Z of pairwise features, with both rows and
columns permuted according to H. We will thus use M as input to the function F0. Since F0 is
GPI, this means that the label for node k will be given by F0(M ) in position H(zk). Thus we set
ρ(zk, M ) = [F0(M )]H(zk), and by the argument above this equals [F0(Z)]k, implying that the
above α, φ and ρ indeed implement F0.

Extension to general graphs So far, we discussed complete graphs, where edges correspond
to valid feature pairs. However, many graphs of interest might be incomplete. For example, an
n-variable chain graph in sequence labeling has only n − 1 edges. For such graphs, the input to F
would not contain all zi,j pairs but rather only features corresponding to valid edges of the graph, and
we are only interested in invariances that preserve the graph structure, namely, the automorphisms
of the graph. Thus, the desired invariance is that σ(F(z)) = F(σ(z)), where σ is not an arbitrary
permutation but an automorphism. It is easy to see that a simple variant of Theorem 1 holds in
this case. All we need to do is replace in Eq. 2 the sum (cid:80)
j∈N (i), where N (i) are the
neighbors of node i in the graph. The arguments are then similar to the proof above.

j(cid:54)=i with (cid:80)

Implications of Theorem 1 Our result has interesting implications for deep structured prediction.
First, it highlights that the fact that the architecture “collects” information from all different edges
of the graph, in an invariant fashion via the α, φ functions. Speciﬁcally, the functions φ (after
summation) aggregate all the features around a given node, and then α (after summation) can
collect them. Thus, these functions can provide a summary of the entire graph that is sufﬁcient for
downstream algorithms. This is different from one round of message passing algorithms which would
not be sufﬁcient for collecting global graph information. Note that the dimensions of φ, α may need
to be large to aggregate all graph information (e.g., by hashing all the features as in the proof of
Theorem 1), but the architecture itself can be shallow.

5

Second, the architecture is parallelizable, as all φ functions can be applied simultaneously. This is in
contrast to recurrent models Zellers et al. (2017) which are harder to parallelize and are thus slower
in practice.

Finally, the theorem suggests several common architectural structures that can be used within GPI.
We brieﬂy mention two of these. 1) Attention: Attention is a powerful component in deep learning
architectures (Bahdanau et al., 2015), but most inference algorithms do not use attention. Intuitively,
in attention each node i aggregates features of neighbors through a weighted sum, where the weight
is a function of the neighbor’s relevance. For example, the label of an entity in an image may depend
more strongly on entities that are spatially closer. Attention can be naturally implemented in our
GPI characterization, and we provide a full derivation for this implementation in the appendix. It
plays a key role in our scene graph model described below. 2) RNNs: Because GPI functions are
closed under composition, for any GPI function F we can run F iteratively by providing the output
of one step of F as part of the input to the next step and maintain GPI. This results in a recurrent
architecture, which we use in our scene graph model.

4 Related Work

The concept of architectural invariance was recently proposed in DEEPSETS (Zaheer et al., 2017).
The invariance we consider is much less restrictive: the architecture does not need to be invariant
to all permutations of singleton and pairwise features, just those consistent with a graph re-labeling.
This characterization results in a substantially different set of possible architectures.

Deep structured prediction. There has been signiﬁcant recent interest in extending deep learning
to structured prediction tasks. Much of this work has been on semantic segmentation, where
convolutional networks (Shelhamer et al., 2017) became a standard approach for obtaining “singleton
scores” and various approaches were proposed for adding structure on top. Most of these approaches
used variants of message passing algorithms, unrolled into a computation graph (Xu et al., 2017).
Some studies parameterized parts of the message passing algorithm and learned its parameters (Lin
et al., 2015). Recently, gradient descent has also been used for maximizing score functions (Belanger
et al., 2017; Gygli et al., 2017). An alternative to deep structured prediction is greedy decoding,
inferring each label at a time based on previous labels. This approach has been popular in sequence-
based applications (e.g., parsing (Chen & Manning, 2014)), relying on the sequential structure of the
input, where BiLSTMs are effectively applied. Another related line of work is applying deep learning
to graph-based problems, such as TSP (Bello et al., 2016; Gilmer et al., 2017; Khalil et al., 2017).
Clearly, the notion of graph invariance is important in these, as highlighted in (Gilmer et al., 2017).
They however do not specify a general architecture that satisﬁes invariance as we do here, and in
fact focus on message passing architectures, which we strictly generalize. Furthermore, our focus is
on the more general problem of structured prediction, rather than speciﬁc graph-based optimization
problems.

Scene graph prediction. Extracting scene graphs from images provides a semantic representation
that can later be used for reasoning, question answering, and image retrieval (Johnson et al., 2015;
Lu et al., 2016; Raposo et al., 2017). It is at the forefront of machine vision research, integrating
challenges like object detection, action recognition and detection of human-object interactions (Liao
et al., 2016; Plummer et al., 2017). Prior work on scene graph predictions used neural message
passing algorithms (Xu et al., 2017) as well as prior knowledge in the form of word embeddings
(Lu et al., 2016). Other work suggested to predict graphs directly from pixels in an end-to-end
manner Newell & Deng (2017). NeuralMotif (Zellers et al., 2017), currently the state-of-the-art
model for scene graph prediction on Visual Genome, employs an RNN that provides global context
by sequentially reading the independent predictions for each entity and relation and then reﬁnes those
predictions. The NEURALMOTIF model maintains GPI by ﬁxing the order in which the RNN reads
its inputs and thus only a single order is allowed. However, this ﬁxed order is not guaranteed to be
optimal.

5 Experimental Evaluation

We empirically evaluate the beneﬁt of GPI architectures. First, using a synthetic graph-labeling task,
and then for the problem of mapping images to scene graphs.

6

Figure 3: Accuracy as a function of sample size for graph labeling. Right is a zoomed in version of left.

5.1 Synthetic Graph Labeling

We start with studying GPI on a synthetic problem, deﬁned as follows. An input graph G = (V, E)
is given, where each node i ∈ V is assigned to one of K sets. The set for node i is denoted by
Γ(i). The goal is to compute for each node the number of neighbors that belong to the same set.
Namely, the label of a node is yi = (cid:80)
j∈N (i) 1[Γ(i) = Γ(j)]. We generated random graphs with 10
nodes (larger graphs produced similar results) by sampling each edge independently and uniformly,
and sampling Γ(i) for every node uniformly from {1, . . . , K}. The node features zi ∈ {0, 1}K are
one-hot vectors of Γ(i) and the edge features zi,j ∈ {0, 1} indicate whether ij ∈ E. We compare two
standard non-GPI architectures and one GPI architecture: (a) A GPI-architecture for graph prediction,
described in detail in Section 5.2. We used the basic version without attention and RNN. (b) LSTM:
We replace (cid:80) φ(·) and (cid:80) α(·), which perform aggregation in Theorem 1 with two LSTMs with
a state size of 200 that read their input in random order. (c) A fully-connected (FC) feed-forward
network with 2 hidden layers of 1000 nodes each. The input to the fully connected model is a
concatenation of all node and pairwise features. The output is all node predictions. The focus of the
experiment is to study sample complexity. Therefore, for a fair comparison, we use the same number
of parameters for all models.

Figure 3, shows the results, demonstrating that GPI requires far fewer samples to converge to the
correct solution. This illustrates the advantage of an architecture with the correct inductive bias for
the problem.

5.2 Scene-Graph Classiﬁcation

We evaluate the GPI approach on the motivating task of this paper, inferring scene graphs from
images (Figure 1). In this problem, the input is an image annotated with a set of bounding boxes for
the entities in the image.2 The goal is to label each bounding box with the correct entity category and
every pair of entities with their relation, such that they form a coherent scene graph.

We begin by describing our Scene Graph Predictor (SGP) model. We aim to predict two types of
variables. The ﬁrst is entity variables [y1, . . . , yn] for all bounding boxes. Each yi can take one of
L values (e.g., “dog”, “man”). The second is relation variables [yn+1, . . . , yn2] for every pair of
bounding boxes. Each such yj can take one of R values (e.g., “on”, “near”). Our graph connects
variables that are expected to be inter-related. It contains two types of edges: 1) entity-entity edge
connecting every two entity variables (yi and yj for 1 ≤ i (cid:54)= j ≤ n. 2) entity-relation edges
connecting every relation variable yk (where k > n) to its two entity variables. Thus, our graph is not
a complete graph and our goal is to design an architecture that will be invariant to any automorphism
of the graph, such as permutations of the entity variables.

For the input features z, we used the features learned by the baseline model from Zellers et al.
(2017).3 Speciﬁcally, the entity features zi included (1) The conﬁdence probabilities of all entities
for yi as learned by the baseline model. (2) Bounding box information given as (left, bottom,
width, height); (3) The number of smaller entities (also bigger); (4) The number of entities to
the left, right, above and below. (5) The number of entities with higher and with lower conﬁdence;

2For simplicity, we focus on the task where boxes are given.
3The baseline does not use any LSTM or context, and is thus unrelated to the main contribution of Zellers

et al. (2017).

7

Constrained Evaluation

SGCls

PredCls

Unconstrained Evaluation
SGCls

PredCls

R@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100

Lu et al., 2016 (Lu et al., 2016)
Xu et al., 2017 (Xu et al., 2017)
Pixel2Graph (Newell & Deng, 2017)
Graph R-CNN (Yang et al., 2018)
Neural Motifs (Zellers et al., 2017)
Baseline (Zellers et al., 2017)
No Attention
Neighbor Attention
Linguistic

11.8
21.7
-
29.6
35.8
34.6
35.3
35.7
36.5

14.1
24.4
-
31.6
36.5
35.3
37.2
38.5
38.8

35.0
44.8
-
54.2
65.2
63.7
64.5
64.6
65.1

27.9
53.0
-
59.1
67.1
65.6
66.3
66.6
66.9

-
-
26.5
-
44.5
43.4
44.1
44.7
45.5

-
-
30.0
-
47.7
46.6
48.5
49.9
50.8

-
-
68.0
-
81.1
78.8
79.7
80.0
80.8

-
-
75.2
-
88.3
85.9
86.7
87.1
88.2

Table 1: Test set results for graph-constrained evaluation (i.e., the returned triplets must be consistent with a
scene graph) and for unconstrained evaluation (triplets need not be consistent with a scene graph).

(6) For the linguistic model only: word embedding of the most probable class. Word vectors were
learned with GLOVE from the ground-truth captions of Visual Genome.
Similarly, the relation features zj ∈ RR contained the probabilities of relation entities for the relation
j. For the Linguistic model, these features were extended to include word embedding of the most
probable class. For entity-entity pairwise features zi,j, we use the relation probability for each pair.
Because the output of SGP are probability distributions over entities and relations, we use them as an
the input z to SGP, once again in a recurrent manner and maintain GPI.

We next describe the main components of the GPI architecture. First, we focus on the parts that
output the entity labels. φent is the network that integrates features for two entity variables yi
and yj. It simply takes zi, zj and zi,j as input, and outputs a vector of dimension n1. Next, the
network αent takes as input the outputs of φent for all neighbors of an entity, and uses the attention
mechanism described above to output a vector of dimension n2. Finally, the ρent network takes these
n2 dimensional vectors and outputs L logits predicting the entity value. The ρrel network takes as
input the αent representation of the two entities, as well as zi,j and transforms the output into R
logits. See appendix for speciﬁc network architectures.

5.2.1 Experimental Setup and Results

Dataset. We evaluated our approach on Visual Genome (VG) (Krishna et al., 2017), a dataset with
108,077 images annotated with bounding boxes, entities and relations. On average, images have 12
entities and 7 relations per image. For a proper comparison with previous results (Newell & Deng,
2017; Xu et al., 2017; Zellers et al., 2017), we used the data from (Xu et al., 2017), including the
train and test splits. For evaluation, we used the same 150 entities and 50 relations as in (Newell
& Deng, 2017; Xu et al., 2017; Zellers et al., 2017). To tune hyper-parameters, we also split the
training data into two by randomly selecting 5K examples, resulting in a ﬁnal 70K/5K/32K split for
train/validation/test sets.

Training. All networks were trained using Adam (Kingma & Ba, 2014) with batch size 20. Hyper-
parameter values below were chosen based on the validation set. The SGP loss function was the sum
of cross-entropy losses over all entities and relations in the image. In the loss, we penalized entities
4 times more strongly than relations, and penalized negative relations 10 times more weakly than
positive relations.

Evaluation.
In (Xu et al., 2017) three different evaluation settings were considered. Here we
focus on two of these: (1) SGCls: Given ground-truth bounding boxes for entities, predict all entity
categories and relations categories. (2) PredCls: Given bounding boxes annotated with entity labels,
predict all relations. Following (Lu et al., 2016), we used Recall@K as the evaluation metric. It
measures the fraction of correct ground-truth triplets that appear within the K most conﬁdent triplets
proposed by the model. Two evaluation protocols are used in the literature differing in whether they
enforce graph constraints over model predictions. The ﬁrst graph-constrained protocol requires that
the top-K triplets assign one consistent class per entity and relation. The second unconstrained
protocol does not enforce any such constraints. We report results on both protocols, following (Zellers
et al., 2017).

8

Figure 4: (a) An input image with bounding boxes from VG. (b) The ground-truth scene graph. (c) The
Baseline fails to recognize some entities (tail and tree) and relations (in front of instead of looking at). (d)
GPI:LINGUISTIC ﬁxes most incorrect LP predictions. (e) Window is the most signiﬁcant neighbor of Tree. (f)
The entity bird receives substantial attention, while Tree and building are less informative.

Models and baselines. We compare four variants of our GPI approach with the reported results
of four baselines that are currently the state-of-the-art on various scene graph prediction problems
(all models use the same data split and pre-processing as (Xu et al., 2017)): 1) LU ET AL., 2016
(LU ET AL., 2016): This work leverages word embeddings to ﬁne-tune the likelihood of predicted
relations. 2) XU ET AL, 2017 (XU ET AL., 2017): This model passes messages between entities and
relations, and iteratively reﬁnes the feature map used for prediction. 3) NEWELL & DENG, 2017
(NEWELL & DENG, 2017): The PIXEL2GRAPH model uses associative embeddings (Newell et al.,
2017) to produce a full graph from the image. 4) YANG ET AL., 2018 (YANG ET AL., 2018): The
GRAPH R-CNN model uses object-relation regularities to sparsify and reason over scene graphs. 5)
ZELLERS ET AL., 2017 (ZELLERS ET AL., 2017): The NEURALMOTIF method encodes global
context for capturing high-order motifs in scene graphs, and the BASELINE outputs the entities and
relations distributions without using the global context. The following variants of GPI were compared:
1) GPI: NO ATTENTION: Our GPI model, but with no attention mechanism. Instead, following
Theorem 1, we simply sum the features. 2) GPI: NEIGHBORATTENTION: Our GPI model, with
attention over neighbors features. 3) GPI: LINGUISTIC: Same as GPI: NEIGHBORATTENTION but
also concatenating the word embedding vector, as described above.

Results. Table 1 shows recall@50 and recall@100 for three variants of our approach, and compared
with ﬁve baselines. All GPI variants performs well, with LINGUISTIC outperforming all baselines
for SGCls and being comparable to the state-of-the-art model for PredCls. Note that PredCl is an
easier task, which makes less use of the structure, hence it is not surprising that GPI achieves similar
accuracy to Zellers et al. (2017). Figure 4 illustrates the model behavior. Predicting isolated labels
with zi (4c) mislabels several entities, but these are corrected at the ﬁnal output (4d). Figure 4e
shows that the system learned to attend more to nearby entities (the window and building are closer
to the tree), and 4f shows that stronger attention is learned for the class bird, presumably because it is
usually more informative than common classes like tree.

Implementation details. The φ and α networks were each implemented as a single fully-connected
(FC) layer with a 500-dimensional outputs. ρ was implemented as a FC network with 3 500-
dimensional hidden layers, with one 150-dimensional output for the entity probabilities, and one
51-dimensional output for relation probabilities. The attention mechanism was implemented as a
network like to φ and α, receiving the same inputs, but using the output scores for the attention . The
full code is available at https://github.com/shikorab/SceneGraph

6 Conclusion

We presented a deep learning approach to structured prediction, which constrains the architecture
to be invariant to structurally identical inputs. As in score-based methods, our approach relies on
pairwise features, capable of describing inter-label correlations, and thus inheriting the intuitive
aspect of score-based approaches. However, instead of maximizing a score function (which leads
to computationally-hard inference), we directly produce an output that is invariant to equivalent
representations of the pairwise terms.

9

This axiomatic approach to model architecture can be extended in many ways. For image labeling,
geometric invariances (shift or rotation) may be desired.
In other cases, invariance to feature
permutations may be desirable. We leave the derivation of the corresponding architectures to future
work. Finally, there may be cases where the invariant structure is unknown and should be discovered
from data, which is related to work on lifting graphical models Bui et al. (2013). It would be interesting
to explore algorithms that discover and use such symmetries for deep structured prediction.

This work was supported by the ISF Centers of Excellence grant, and by the Yandex Initiative in
Machine Learning. Work by GC was performed while at Google Brain Research.

Acknowledgements

References

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and

translate. In International Conference on Learning Representations (ICLR), 2015.

Belanger, David, Yang, Bishan, and McCallum, Andrew. End-to-end learning for structured prediction
energy networks. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70, pp. 429–439. PMLR, 2017.

Bello, Irwan, Pham, Hieu, Le, Quoc V, Norouzi, Mohammad, and Bengio, Samy. Neural combinato-

rial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Bui, Hung Hai, Huynh, Tuyen N., and Riedel, Sebastian. Automorphism groups of graphical models
and lifted variational inference. In Proceedings of the Twenty-Ninth Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’13, pp. 132–141, 2013.

Chen, Danqi and Manning, Christopher. A fast and accurate dependency parser using neural
networks. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 740–750, 2014.

Chen, Liang Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Se-
mantic image segmentation with deep convolutional nets and fully connected CRFs. In Proceedings
of the Second International Conference on Learning Representations, 2014.

Chen, Liang Chieh, Schwing, Alexander G, Yuille, Alan L, and Urtasun, Raquel. Learning deep

structured models. In Proc. ICML, 2015.

Farabet, Clement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical
features for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):
1915–1929, 2013.

Gilmer, Justin, Schoenholz, Samuel S, Riley, Patrick F, Vinyals, Oriol, and Dahl, George E. Neural

message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.

Gygli, Michael, Norouzi, Mohammad, and Angelova, Anelia. Deep value networks learn to evaluate
and iteratively reﬁne structured outputs. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine
Learning Research, pp. 1341–1351, International Convention Centre, Sydney, Australia, 2017.
PMLR.

Johnson, Justin, Krishna, Ranjay, Stark, Michael, Li, Li-Jia, Shamma, David A., Bernstein, Michael S.,
In Proc. Conf. Comput. Vision Pattern

Image retrieval using scene graphs.

and Li, Fei-Fei.
Recognition, pp. 3668–3678, 2015.

Johnson, Justin, Gupta, Agrim, and Fei-Fei, Li. Image generation from scene graphs. arXiv preprint

arXiv:1804.01622, 2018.

Khalil, Elias, Dai, Hanjun, Zhang, Yuyu, Dilkina, Bistra, and Song, Le. Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp.
6351–6361, 2017.

10

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint

arXiv: 1412.6980, abs/1412.6980, 2014.

Krishna, Ranjay, Zhu, Yuke, Groth, Oliver, Johnson, Justin, Hata, Kenji, Kravitz, Joshua, Chen,
Stephanie, Kalantidis, Yannis, Li, Li-Jia, Shamma, David A, et al. Visual genome: Connecting
International Journal of
language and vision using crowdsourced dense image annotations.
Computer Vision, 123(1):32–73, 2017.

Lafferty, J., McCallum, A., and Pereira, F. Conditional random ﬁelds: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning, pp. 282–289, 2001.

Liao, Wentong, Yang, Michael Ying, Ackermann, Hanno, and Rosenhahn, Bodo. On support relations

and semantic scene graphs. arXiv preprint arXiv:1609.05834, 2016.

Lin, Guosheng, Shen, Chunhua, Reid, Ian, and van den Hengel, Anton. Deeply learning the messages
in message passing inference. In Advances in Neural Information Processing Systems, pp. 361–369,
2015.

Lu, Cewu, Krishna, Ranjay, Bernstein, Michael S., and Li, Fei-Fei. Visual relationship detection with

language priors. In European Conf. Comput. Vision, pp. 852–869, 2016.

Meshi, O., Sontag, D., Jaakkola, T., and Globerson, A. Learning efﬁciently with approximate
In Proceedings of the 27th International Conference on Machine

inference via dual losses.
Learning, pp. 783–790, New York, NY, USA, 2010. ACM.

Newell, Alejandro and Deng, Jia. Pixels to graphs by associative embedding. In Advances in Neural
Information Processing Systems 30 (to appear), pp. 1172–1180. Curran Associates, Inc., 2017.

Newell, Alejandro, Huang, Zhiao, and Deng, Jia. Associative embedding: End-to-end learning for
joint detection and grouping. In Neural Inform. Process. Syst., pp. 2274–2284. Curran Associates,
Inc., 2017.

Pei, Wenzhe, Ge, Tao, and Chang, Baobao. An effective neural network model for graph-based de-
pendency parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computationa
Linguistics, pp. 313–322, 2015.

Plummer, Bryan A., Mallya, Arun, Cervantes, Christopher M., Hockenmaier, Julia, and Lazebnik,
Svetlana. Phrase localization and visual relationship detection with comprehensive image-language
cues. In ICCV, pp. 1946–1955, 2017.

Raposo, David, Santoro, Adam, Barrett, David, Pascanu, Razvan, Lillicrap, Timothy, and Battaglia,
Peter. Discovering objects and their relations from entangled scene representations. arXiv preprint
arXiv:1702.05068, 2017.

Schwing, Alexander G and Urtasun, Raquel. Fully connected deep structured networks. ArXiv

e-prints, 2015.

Shelhamer, Evan, Long, Jonathan, and Darrell, Trevor. Fully convolutional networks for semantic

segmentation. Proc. Conf. Comput. Vision Pattern Recognition, 39(4):640–651, 2017.

Taskar, B., Guestrin, C., and Koller, D. Max margin Markov networks. In Thrun, S., Saul, L., and
Schölkopf, B. (eds.), Advances in Neural Information Processing Systems 16, pp. 25–32. MIT
Press, Cambridge, MA, 2004.

Xu, Danfei, Zhu, Yuke, Choy, Christopher B., and Fei-Fei, Li. Scene Graph Generation by Iterative
Message Passing. In Proc. Conf. Comput. Vision Pattern Recognition, pp. 3097–3106, 2017.

Yang, Jianwei, Lu, Jiasen, Lee, Stefan, Batra, Dhruv, and Parikh, Devi. Graph R-CNN for scene

graph generation. In European Conf. Comput. Vision, pp. 690–706, 2018.

Zaheer, Manzil, Kottur, Satwik, Ravanbakhsh, Siamak, Poczos, Barnabas, Salakhutdinov, Ruslan R,
and Smola, Alexander J. Deep sets. In Advances in Neural Information Processing Systems 30, pp.
3394–3404. Curran Associates, Inc., 2017.

11

Zellers, Rowan, Yatskar, Mark, Thomson, Sam, and Choi, Yejin. Neural motifs: Scene graph parsing

with global context. arXiv preprint arXiv:1711.06640, abs/1711.06640, 2017.

Zheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav, Su, Zhizhong, Du,
Dalong, Huang, Chang, and Torr, Philip HS. Conditional random ﬁelds as recurrent neural networks.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 1529–1537, 2015.

7 Supplementary Material

This supplementary material includes: (1) Visual illustration of the proof of Theorem 1. (2) Explaining
how to integrate an attention mechanism in our GPI framework. (3) Additional evaluation method to
further analyze and compare our work with baselines.

7.1 Theorem 1: Illustration of Proof

Figure 5: Illustration of the proof of Theorem 1 using a speciﬁc construction example. Here H is a hash function
of size L = 5 such that H(1) = 1, H(3) = 2, H(2) = 4, G is a three-node input graph, and zi,j ∈ R are the
pairwise features (in purple) of G. (a) φ is applied to each zi,j. Each application yields a vector in R5. The three
dark yellow columns correspond to φ(z1,1), φ(z1,2) and φ(z1,3). Then, all vectors φ(zi,j) are summed over j
to obtain three si vectors. (b) α’s (blue matrices) are an outer product between 1 [H(zi)] and si resulting in a
matrix of zeros except one row. The dark blue matrix corresponds for α(z1, s1). (c) All α’s are summed to a
5 × 5 matrix, isomorphic to the original zi,j matrix.

7.2 Characterizing Permutation Invariance: Attention

Attention is a powerful component which naturally can be introduced into our GPI model. We now
show how attention can be introduced in our framework. Formally, we learn attention weights for the
neighbors j of a node i, which scale the features zi,j of that neighbor. We can also learn different
attention weights for individual features of each neighbor in a similar way.

Let wi,j ∈ R be an attention mask specifying the weight that node i gives to node j:

wi,j(zi, zi,j, zj) = eβ(zi,zi,j ,zj )/

eβ(zi,zi,t,zt)

(4)

(cid:88)

t

where β can be any scalar-valued function of its arguments (e.g., a dot product of zi and zj as in
standard attention models). To introduce attention we wish α ∈ Re to have the form of weighting
wi,j over neighboring feature vectors zi,j, namely, α = (cid:80)
To achieve this form we extend φ by a single entry, deﬁning φ ∈ Re+1 (namely we set
L = e + 1) as φ1:e(zi, zi,j, zj) = eβ(zi,zi,j ,zj )zi,j (here φ1:e are the ﬁrst e elements of φ)

j(cid:54)=i wi,jzi,j.

12

and φe+1(zi, zi,j; zj) = eβ(zi,zi,j ,zj ). We keep the deﬁnition of si = (cid:80)
we deﬁne α = si,1:e
si,e+1
over neighboring feature vectors zi,j:

j(cid:54)=i φ(zi, zi,j, zj). Next,
and substitute si and φ to obtain the desired form as attention weights wi,j

α(zi, si) =

si,1:e
si,e+1

(cid:88)

=

j(cid:54)=i

eβ(zi,zi,j ,zj )zi,j
(cid:80)
j(cid:54)=i eβ(zi,zi,j ,zj )

(cid:88)

=

j(cid:54)=i

wi,jzi,j

A similar approach can be applied over α and ρ to model attention over the outputs of α as well
(graph nodes).

7.3 Scene Graph Results

In the main paper, we described the results for the two prediction tasks: SGCls and PredCls, as
deﬁned in section 5.2.1: "Experimental Setup and Results". To further analyze our module, we
compare the best variant, GPI: LINGUISTIC, per relation to two baselines: (Lu et al., 2016) and Xu
et al. (2017). Table 2, speciﬁes the PredCls recall@5 of the 20-top frequent relation classes. The GPI
module performs better in almost all the relations classes.

RELATION

(LU ET AL., 2016)

(XU ET AL., 2017)

LINGUISTIC

99.71
98.03
80.38
82.47
98.47
85.16
31.85
49.19
61.50
79.35
28.64
31.74
26.09
8.45
54.08

ON
HAS
IN
OF
WEARING
NEAR
WITH
ABOVE
HOLDING
BEHIND
UNDER
SITTING ON
IN FRONT OF
ATTACHED TO
AT
HANGING FROM 0.0
OVER
FOR
RIDING

9.26
12.20
72.43

99.25
97.25
88.30
96.75
98.23
96.81
88.10
79.73
80.67
92.32
52.73
50.17
59.63
29.58
70.41
0.0
0.0
31.71
89.72

99.3
98.7
95.9
98.1
99.6
95.4
94.2
83.9
95.5
91.2
83.2
90.4
74.9
77.4
80.9
74.1
62.4
45.1
96.1

Table 2: Recall@5 of PredCls for the 20-top relations ranked by their frequency, as in (Xu et al., 2017)

13

8
1
0
2
 
v
o
N
 
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
5
4
5
0
.
2
0
8
1
:
v
i
X
r
a

Mapping Images to Scene Graphs with
Permutation-Invariant Structured Prediction

Roei Herzig∗
Tel Aviv University
roeiherzig@mail.tau.ac.il

Moshiko Raboh∗
Tel Aviv University
mosheraboh@mail.tau.ac.il

Gal Chechik
Bar-Ilan University, NVIDIA Research
gal.chechik@biu.ac.il

Jonathan Berant
Tel Aviv University, AI2
joberant@cs.tau.ac.il

Amir Globerson
Tel Aviv University
gamir@post.tau.ac.il

Abstract

Machine understanding of complex images is a key goal of artiﬁcial intelligence.
One challenge underlying this task is that visual scenes contain multiple inter-
related objects, and that global context plays an important role in interpreting
the scene. A natural modeling framework for capturing such effects is structured
prediction, which optimizes over complex labels, while modeling within-label
interactions. However, it is unclear what principles should guide the design of a
structured prediction model that utilizes the power of deep learning components.
Here we propose a design principle for such architectures that follows from a
natural requirement of permutation invariance. We prove a necessary and sufﬁ-
cient characterization for architectures that follow this invariance, and discuss its
implication on model design. Finally, we show that the resulting model achieves
new state-of-the-art results on the Visual Genome scene-graph labeling benchmark,
outperforming all recent approaches.

1

Introduction

Understanding the semantics of a complex visual scene is a fundamental problem in machine
perception. It often requires recognizing multiple objects in a scene, together with their spatial and
functional relations. The set of objects and relations is sometimes represented as a graph, connecting
objects (nodes) with their relations (edges) and is known as a scene graph (Figure 1). Scene graphs
provide a compact representation of the semantics of an image, and can be useful for semantic-level
interpretation and reasoning about a visual scene Johnson et al. (2018). Scene-graph prediction is the
problem of inferring the joint set of objects and their relations in a visual scene.

Since objects and relations are inter-dependent (e.g., a person and chair are more likely to be in relation
“sitting on” than “eating”), a scene graph predictor should capture this dependence in order to improve
prediction accuracy. This goal is a special case of a more general problem, namely, inferring multiple
inter-dependent labels, which is the research focus of the ﬁeld of structured prediction. Structured
prediction has attracted considerable attention because it applies to many learning problems and

∗Equal Contribution.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Figure 1: An image and its scene graph from the Visual Genome dataset (Krishna et al., 2017). The scene graph
captures the entities in the image (nodes, blue circles) like dog and their relations (edges, red circles) like (cid:10)hat,
on, dog(cid:11).

poses unique theoretical and algorithmic challenges (e.g., see Belanger et al., 2017; Chen et al., 2015;
Taskar et al., 2004). It is therefore a natural approach for predicting scene graphs from images.

Structured prediction models typically deﬁne a score function s(x, y) that quantiﬁes how well a
label assignment y is compatible with an input x. In the case of understanding complex visual
scenes, x is an image, and y is a complex label containing the labels of objects detected in an image
and the labels of their relations. In this setup, the inference task amounts to ﬁnding the label that
maximizes the compatibility score y∗ = arg maxy s(x, y). This score-based approach separates a
scoring component – implemented by a parametric model, from an optimization component – aimed
at ﬁnding a label that maximizes that score. Unfortunately, for a general scoring function s(·), the
space of possible label assignments grows exponentially with input size. For instance, for scene
graphs the set of possible object label assignments is too large even for relatively simple images,
since the vocabulary of candidate objects may contain thousands of objects. As a result, inferring the
label assignment that maximizes a scoring function is computationally hard in the general case.

An alternative approach to score-based methods is to map an input x to a structured output y with
a “black box" neural network, without explicitly deﬁning a score function. This raises a natural
question: what is the right architecture for such a network? Here we take an axiomatic approach and
argue that one important property such networks should satisfy is invariance to a particular type of
input permutation. We then prove that this invariance is equivalent to imposing certain structural
constraints on the architecture of the network, and describe architectures that satisfy these constraints.

To evaluate our approach, we ﬁrst demonstrate on a synthetic dataset that respecting permutation
invariance is important, because models that violate this invariance need more training data, despite
having a comparable model size. Then, we tackle the problem of scene graph generation. We describe
a model that satisﬁes the permutation invariance property, and show that it achieves state-of-the-art
results on the competitive Visual Genome benchmark (Krishna et al., 2017), demonstrating the power
of our new design principle.

In summary, the novel contributions of this paper are: a) Deriving sufﬁcient and necessary conditions
for graph-permutation invariance in deep structured prediction architectures. b) Empirically demon-
strating the beneﬁt of graph-permutation invariance. c) Developing a state-of-the-art model for scene
graph prediction on a large dataset of complex visual scenes.

2 Structured Prediction

Scored-based methods in structured prediction deﬁne a function s(x, y) that quantiﬁes the degree
to which y is compatible with x, and infer a label by maximizing s(x, y) (e.g., see Belanger et al.,
2017; Chen et al., 2015; Lafferty et al., 2001; Meshi et al., 2010; Taskar et al., 2004). Most score
functions previously used decompose as a sum over simpler functions, s(x, y) = (cid:80)
i fi(x, y), making
it possible to optimize maxy fi(x, y) efﬁciently. This local maximization forms the basic building
block of algorithms for approximately maximizing s(x, y). One way to decompose the score function
is to restrict each fi(x, y) to depend only on a small subset of the y variables.

The renewed interest in deep learning led to efforts to integrate deep networks with structured predic-
tion, including modeling the fi functions as deep networks. In this context, the most widely-used
score functions are singleton fi(yi, x) and pairwise fij(yi, yj, x). The early work taking this approach
used a two-stage architecture, learning the local scores independently of the structured prediction

2

Figure 2: Left: Graph permutation invariance. A graph labeling function F is graph permutation invariant
(GPI) if permuting the node features maintains the output. Right: a schematic representation of the GPI
architecture in Theorem 1. Singleton features zi are omitted for simplicity. (a) First, the features zi,j are
processed element-wise by φ. (b) Features are summed to create a vector si, which is concatenated with zi. (c)
A representation of the entire graph is created by applying α n times and summing the created vector. (d) The
graph representation is then ﬁnally processed by ρ together with zk.

goal (Chen et al., 2014; Farabet et al., 2013). Later studies considered end-to-end architectures where
the inference algorithm is part of the computation graph (Chen et al., 2015; Pei et al., 2015; Schwing
& Urtasun, 2015; Zheng et al., 2015). Recent studies go beyond pairwise scores, also modelling
global factors (Belanger et al., 2017; Gygli et al., 2017).

Score-based methods provide several advantages. First, they allow intuitive speciﬁcation of local
dependencies between labels and how these translate to global dependencies. Second, for linear
score functions, the learning problem has natural convex surrogates Lafferty et al. (2001); Taskar
et al. (2004). Third, inference in large label spaces is sometimes possible via exact algorithms or
empirically accurate approximations. However, with the advent of deep scoring functions s(x, y; w),
learning is no longer convex. Thus, it is worthwhile to rethink the architecture of structured prediction
models, and consider models that map inputs x to outputs y directly without explicitly maximizing a
score function. We would like these models to enjoy the expressivity and predictive power of neural
networks, while maintaining the ability to specify local dependencies between labels in a ﬂexible
manner. In the next section, we present such an approach and consider a natural question: what
should be the properties of a deep neural network used for structured prediction.

3 Permutation-Invariant Structured Prediction

In what follows we deﬁne the permutation-invariance property for structured prediction models, and
argue that permutation invariance is a natural principle for designing their architecture.

We ﬁrst introduce our notation. We focus on structures with pairwise interactions, because they are
simpler in terms of notation and are sufﬁcient for describing the structure in many problems. We
denote a structured label by y = [y1, . . . , yn]. In a score-based approach, the score is deﬁned via a
set of singleton scores fi(yi, x) and pairwise scores fij(yi, yj, x), where the overall score s(x, y) is
the sum of these scores. For brevity, we denote fij = fij(yi, yj, x) and fi = fi(yi, x). An inference
algorithm takes as input the local scores fi, fij and outputs an assignment that maximizes s(x, y).
We can thus view inference as a black-box that takes node-dependent and edge-dependent inputs
(i.e., the scores fi, fij) and returns a label y, even without an explicit score function s(x, y). While
numerous inference algorithms exist for this setup, including belief propagation (BP) and mean ﬁeld,
here we develop a framework for a deep labeling algorithm (we avoid the term “inference” since the
algorithm does not explicitly maximize a score function). Such an algorithm will be a black-box,
taking the f functions as input and the labels y1, . . . , yn as output. We next ask what architecture
such an algorithm should have.

We follow with several deﬁnitions. A graph labeling function F : (V, E) → Y is a function whose
input is an ordered set of node features V = [z1, . . . , zn] and an ordered set of edge features
E = [z1,2 . . . , zi,j, . . . , zn,n−1]. For example, zi can be the array of values fi, and zi,j can be
the table of values fi,j. Assume zi ∈ Rd and zi,j ∈ Re. The output of F is a set of node labels
y = [y1, . . . , yn]. Thus, algorithms such as BP are graph labeling functions. However, graph labeling
functions do not necessarily maximize a score function. We denote the joint set of node features and
edge features by z (i.e., a set of n + n(n − 1) = n2 vectors). In Section 3.1 we discuss extensions to
this case where only a subset of the edges is available.

3

1, y∗

2, y∗

1, y∗

2, y∗

A natural requirement is that the function F produces the same result when given the same features,
up to a permutation of the input. For example, consider a label space with three variables y1, y2, y3,
and assume that F takes as input z = (z1, z2, z3, z12, z13, z23) = (f1, f2, f3, f12, f13, f23), and
outputs a label y = (y∗
3). When F is given an input that is permuted in a consistent way, say,
z(cid:48) = (f2, f1, f3, f21, f23, f13), this deﬁnes exactly the same input. Hence, the output should still be
y = (y∗
3). Most inference algorithms, including BP and mean ﬁeld, satisfy this symmetry
requirement by design, but this property is not guaranteed in general in a deep model. Here, our
goal is to design a deep learning black-box, and hence we wish to guarantee invariance to input
permutations. A black-box that violates this invariance “wastes” capacity on learning it at training
time, which increases sample complexity, as shown in Sec. 5.1. We proceed to formally deﬁne the
permutation invariance property.
Deﬁnition 1. Let z be a set of node features and edge features, and let σ be a permutation of
{1, . . . , n}. We deﬁne σ(z) to be a new set of node and edge features given by [σ(z)]i = zσ(i) and
[σ(z)]i,j = zσ(i),σ(j).

We also use the notation σ([y1, . . . , yn]) = [yσ(1), . . . , yσ(n)] for permuting the labels. Namely, σ
applied to a set of labels yields the same labels, only permuted by σ. Be aware that applying σ to
the input features is different from permuting labels, because edge input features must permuted in a
way that is consistent with permuting node input features. We now provide our key deﬁnition of a
function whose output is invariant to permutations of the input. See Figure 2 (left).
Deﬁnition 2. A graph labeling function F is said to be graph-permutation invariant (GPI), if for
all permutations σ of {1, . . . , n} and for all z it satisﬁes: F(σ(z)) = σ(F(z)).

3.1 Characterizing Permutation Invariance

Motivated by the above discussion, we ask: what structure is necessary and sufﬁcient to guarantee
that F is GPI? Note that a function F takes as input an ordered set z. Therefore its output on z
could certainly differ from its output on σ(z). To achieve permutation invariance, F should contain
certain symmetries. For instance, one permutation invariant architecture could be to deﬁne yi = g(zi)
for any function g, but this architecture is too restrictive and does not cover all permutation invariant
functions. Theorem 1 below provides a complete characterization (see Figure 2 for the corresponding
architecture). Intuitively, the architecture in Theorem 1 is such that it can aggregate information from
the entire graph, and do so in a permutation invariant manner.
Theorem 1. Let F be a graph labeling function. Then F is graph-permutation invariant if and only
if there exist functions α, ρ, φ such that for all k = 1, . . . , n:

[F(z)]k = ρ

zk,

α

zi,

φ(zi, zi,j, zj)



 ,

(1)









n
(cid:88)

i=1

(cid:88)

j(cid:54)=i

where φ : R2d+e → RL, α : Rd+L → RW and ρ : RW +d → R.

Proof. First, we show that any F satisfying the conditions of Theorem 1 is GPI. Namely, for any
permutation σ, [F(σ(z))]k = [F(z)]σ(k). To see this, write [F(σ(z))]k using Eq. 1 and Deﬁnition 1:

[F(σ(z))]k = ρ(zσ(k),

α(zσ(i),

φ(zσ(i), zσ(i),σ(j), zσ(j)))).

(2)

(cid:88)

i

(cid:88)

j(cid:54)=i

The second argument of ρ above is invariant under σ, because it is a sum over nodes and their
neighbors, which is invariant under permutation. Thus Eq. 2 is equal to:

ρ(zσ(k),

α(zi,

φ(zi, zi,j, zj))) = [F(z)]σ(k)

(cid:88)

i

(cid:88)

j(cid:54)=i

where equality follows from Eq. 1. We thus proved that Eq. 1 implies graph permutation invariance.

Next, we prove that any given GPI function F0 can be expressed as a function F in Eq. 1. Namely,
we show how to deﬁne φ, α and ρ that can implement F0. Note that in this direction of the proof the
function F0 is a black-box. Namely, we only know that it is GPI, but do not assume anything else
about its implementation.

4

The key idea is to construct φ, α such that the second argument of ρ in Eq. 1 contains the information
about all the graph features z. Then, the function ρ corresponds to an application of F0 to this
representation, followed by extracting the label yk. To simplify notation assume edge features are
scalar (e = 1). The extension to vectors is simple, but involves more indexing.

We assume WLOG that the black-box function F0 is a function only of the pairwise features zi,j
(otherwise, we can always augment the pairwise features with the singleton features). Since zi,j ∈ R
we use a matrix Rn,n to denote all the pairwise features.

Finally, we assume that our implementation of F0 will take additional node features zk such that no
two nodes have the same feature (i.e., the features identify the node).

Our goal is thus to show that there exist functions α, φ, ρ such that the function in Eq. 2 applied to Z
yields the same labels as F0(Z).

Let H be a hash function with L buckets mapping node features zi to an index (bucket). Assume
that H is perfect (this can be achieved for a large enough L). Deﬁne φ to map the pairwise
features to a vector of size L. Let 1 [j] be a one-hot vector of dimension RL, with one in the
jth coordinate. Recall that we consider scalar zi,j so that φ is indeed in RL, and deﬁne φ as:
φ(zi, zi,j, zj) = 1 [H(zj)] zi,j, i.e., φ “stores” zi,j in the unique bucket for node j.
Let si = (cid:80)
zi,j ∈E φ(zi, zi,j, zj) be the second argument of α in Eq. 1 (si ∈ RL). Then, since all
zj are distinct, si stores all the pairwise features for neighbors of i in unique positions within its
L coordinates. Since si(H(zk)) contains the feature zi,k whereas sj(H(zk)) contains the feature
zj,k, we cannot simply sum the si, since we would lose the information of which edges the features
originated from. Instead, we deﬁne α to map si to RL×L such that each feature is mapped to a
distinct location. Formally:

α(zi, si) = 1 [H(zi)] sT
i
α outputs a matrix that is all zeros except for the features corresponding to node i that are stored in
row H(zi). The matrix M = (cid:80)
i α(zi, si) (namely, the second argument of ρ in Eq. 1) is a matrix
with all the edge features in the graph including the graph structure.

(3)

.

To complete the construction we set ρ to have the same outcome as F0. We ﬁrst discard rows and
columns in M that do not correspond to original nodes (reducing M to dimension n × n). Then, we
use the reduced matrix as the input z to the black-box F0.

Assume for simplicity that M does not need to be contracted (this merely introduces another indexing
step). Then M corresponds to the original matrix Z of pairwise features, with both rows and
columns permuted according to H. We will thus use M as input to the function F0. Since F0 is
GPI, this means that the label for node k will be given by F0(M ) in position H(zk). Thus we set
ρ(zk, M ) = [F0(M )]H(zk), and by the argument above this equals [F0(Z)]k, implying that the
above α, φ and ρ indeed implement F0.

Extension to general graphs So far, we discussed complete graphs, where edges correspond
to valid feature pairs. However, many graphs of interest might be incomplete. For example, an
n-variable chain graph in sequence labeling has only n − 1 edges. For such graphs, the input to F
would not contain all zi,j pairs but rather only features corresponding to valid edges of the graph, and
we are only interested in invariances that preserve the graph structure, namely, the automorphisms
of the graph. Thus, the desired invariance is that σ(F(z)) = F(σ(z)), where σ is not an arbitrary
permutation but an automorphism. It is easy to see that a simple variant of Theorem 1 holds in
this case. All we need to do is replace in Eq. 2 the sum (cid:80)
j∈N (i), where N (i) are the
neighbors of node i in the graph. The arguments are then similar to the proof above.

j(cid:54)=i with (cid:80)

Implications of Theorem 1 Our result has interesting implications for deep structured prediction.
First, it highlights that the fact that the architecture “collects” information from all different edges
of the graph, in an invariant fashion via the α, φ functions. Speciﬁcally, the functions φ (after
summation) aggregate all the features around a given node, and then α (after summation) can
collect them. Thus, these functions can provide a summary of the entire graph that is sufﬁcient for
downstream algorithms. This is different from one round of message passing algorithms which would
not be sufﬁcient for collecting global graph information. Note that the dimensions of φ, α may need
to be large to aggregate all graph information (e.g., by hashing all the features as in the proof of
Theorem 1), but the architecture itself can be shallow.

5

Second, the architecture is parallelizable, as all φ functions can be applied simultaneously. This is in
contrast to recurrent models Zellers et al. (2017) which are harder to parallelize and are thus slower
in practice.

Finally, the theorem suggests several common architectural structures that can be used within GPI.
We brieﬂy mention two of these. 1) Attention: Attention is a powerful component in deep learning
architectures (Bahdanau et al., 2015), but most inference algorithms do not use attention. Intuitively,
in attention each node i aggregates features of neighbors through a weighted sum, where the weight
is a function of the neighbor’s relevance. For example, the label of an entity in an image may depend
more strongly on entities that are spatially closer. Attention can be naturally implemented in our
GPI characterization, and we provide a full derivation for this implementation in the appendix. It
plays a key role in our scene graph model described below. 2) RNNs: Because GPI functions are
closed under composition, for any GPI function F we can run F iteratively by providing the output
of one step of F as part of the input to the next step and maintain GPI. This results in a recurrent
architecture, which we use in our scene graph model.

4 Related Work

The concept of architectural invariance was recently proposed in DEEPSETS (Zaheer et al., 2017).
The invariance we consider is much less restrictive: the architecture does not need to be invariant
to all permutations of singleton and pairwise features, just those consistent with a graph re-labeling.
This characterization results in a substantially different set of possible architectures.

Deep structured prediction. There has been signiﬁcant recent interest in extending deep learning
to structured prediction tasks. Much of this work has been on semantic segmentation, where
convolutional networks (Shelhamer et al., 2017) became a standard approach for obtaining “singleton
scores” and various approaches were proposed for adding structure on top. Most of these approaches
used variants of message passing algorithms, unrolled into a computation graph (Xu et al., 2017).
Some studies parameterized parts of the message passing algorithm and learned its parameters (Lin
et al., 2015). Recently, gradient descent has also been used for maximizing score functions (Belanger
et al., 2017; Gygli et al., 2017). An alternative to deep structured prediction is greedy decoding,
inferring each label at a time based on previous labels. This approach has been popular in sequence-
based applications (e.g., parsing (Chen & Manning, 2014)), relying on the sequential structure of the
input, where BiLSTMs are effectively applied. Another related line of work is applying deep learning
to graph-based problems, such as TSP (Bello et al., 2016; Gilmer et al., 2017; Khalil et al., 2017).
Clearly, the notion of graph invariance is important in these, as highlighted in (Gilmer et al., 2017).
They however do not specify a general architecture that satisﬁes invariance as we do here, and in
fact focus on message passing architectures, which we strictly generalize. Furthermore, our focus is
on the more general problem of structured prediction, rather than speciﬁc graph-based optimization
problems.

Scene graph prediction. Extracting scene graphs from images provides a semantic representation
that can later be used for reasoning, question answering, and image retrieval (Johnson et al., 2015;
Lu et al., 2016; Raposo et al., 2017). It is at the forefront of machine vision research, integrating
challenges like object detection, action recognition and detection of human-object interactions (Liao
et al., 2016; Plummer et al., 2017). Prior work on scene graph predictions used neural message
passing algorithms (Xu et al., 2017) as well as prior knowledge in the form of word embeddings
(Lu et al., 2016). Other work suggested to predict graphs directly from pixels in an end-to-end
manner Newell & Deng (2017). NeuralMotif (Zellers et al., 2017), currently the state-of-the-art
model for scene graph prediction on Visual Genome, employs an RNN that provides global context
by sequentially reading the independent predictions for each entity and relation and then reﬁnes those
predictions. The NEURALMOTIF model maintains GPI by ﬁxing the order in which the RNN reads
its inputs and thus only a single order is allowed. However, this ﬁxed order is not guaranteed to be
optimal.

5 Experimental Evaluation

We empirically evaluate the beneﬁt of GPI architectures. First, using a synthetic graph-labeling task,
and then for the problem of mapping images to scene graphs.

6

Figure 3: Accuracy as a function of sample size for graph labeling. Right is a zoomed in version of left.

5.1 Synthetic Graph Labeling

We start with studying GPI on a synthetic problem, deﬁned as follows. An input graph G = (V, E)
is given, where each node i ∈ V is assigned to one of K sets. The set for node i is denoted by
Γ(i). The goal is to compute for each node the number of neighbors that belong to the same set.
Namely, the label of a node is yi = (cid:80)
j∈N (i) 1[Γ(i) = Γ(j)]. We generated random graphs with 10
nodes (larger graphs produced similar results) by sampling each edge independently and uniformly,
and sampling Γ(i) for every node uniformly from {1, . . . , K}. The node features zi ∈ {0, 1}K are
one-hot vectors of Γ(i) and the edge features zi,j ∈ {0, 1} indicate whether ij ∈ E. We compare two
standard non-GPI architectures and one GPI architecture: (a) A GPI-architecture for graph prediction,
described in detail in Section 5.2. We used the basic version without attention and RNN. (b) LSTM:
We replace (cid:80) φ(·) and (cid:80) α(·), which perform aggregation in Theorem 1 with two LSTMs with
a state size of 200 that read their input in random order. (c) A fully-connected (FC) feed-forward
network with 2 hidden layers of 1000 nodes each. The input to the fully connected model is a
concatenation of all node and pairwise features. The output is all node predictions. The focus of the
experiment is to study sample complexity. Therefore, for a fair comparison, we use the same number
of parameters for all models.

Figure 3, shows the results, demonstrating that GPI requires far fewer samples to converge to the
correct solution. This illustrates the advantage of an architecture with the correct inductive bias for
the problem.

5.2 Scene-Graph Classiﬁcation

We evaluate the GPI approach on the motivating task of this paper, inferring scene graphs from
images (Figure 1). In this problem, the input is an image annotated with a set of bounding boxes for
the entities in the image.2 The goal is to label each bounding box with the correct entity category and
every pair of entities with their relation, such that they form a coherent scene graph.

We begin by describing our Scene Graph Predictor (SGP) model. We aim to predict two types of
variables. The ﬁrst is entity variables [y1, . . . , yn] for all bounding boxes. Each yi can take one of
L values (e.g., “dog”, “man”). The second is relation variables [yn+1, . . . , yn2] for every pair of
bounding boxes. Each such yj can take one of R values (e.g., “on”, “near”). Our graph connects
variables that are expected to be inter-related. It contains two types of edges: 1) entity-entity edge
connecting every two entity variables (yi and yj for 1 ≤ i (cid:54)= j ≤ n. 2) entity-relation edges
connecting every relation variable yk (where k > n) to its two entity variables. Thus, our graph is not
a complete graph and our goal is to design an architecture that will be invariant to any automorphism
of the graph, such as permutations of the entity variables.

For the input features z, we used the features learned by the baseline model from Zellers et al.
(2017).3 Speciﬁcally, the entity features zi included (1) The conﬁdence probabilities of all entities
for yi as learned by the baseline model. (2) Bounding box information given as (left, bottom,
width, height); (3) The number of smaller entities (also bigger); (4) The number of entities to
the left, right, above and below. (5) The number of entities with higher and with lower conﬁdence;

2For simplicity, we focus on the task where boxes are given.
3The baseline does not use any LSTM or context, and is thus unrelated to the main contribution of Zellers

et al. (2017).

7

Constrained Evaluation

SGCls

PredCls

Unconstrained Evaluation
SGCls

PredCls

R@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100

Lu et al., 2016 (Lu et al., 2016)
Xu et al., 2017 (Xu et al., 2017)
Pixel2Graph (Newell & Deng, 2017)
Graph R-CNN (Yang et al., 2018)
Neural Motifs (Zellers et al., 2017)
Baseline (Zellers et al., 2017)
No Attention
Neighbor Attention
Linguistic

11.8
21.7
-
29.6
35.8
34.6
35.3
35.7
36.5

14.1
24.4
-
31.6
36.5
35.3
37.2
38.5
38.8

35.0
44.8
-
54.2
65.2
63.7
64.5
64.6
65.1

27.9
53.0
-
59.1
67.1
65.6
66.3
66.6
66.9

-
-
26.5
-
44.5
43.4
44.1
44.7
45.5

-
-
30.0
-
47.7
46.6
48.5
49.9
50.8

-
-
68.0
-
81.1
78.8
79.7
80.0
80.8

-
-
75.2
-
88.3
85.9
86.7
87.1
88.2

Table 1: Test set results for graph-constrained evaluation (i.e., the returned triplets must be consistent with a
scene graph) and for unconstrained evaluation (triplets need not be consistent with a scene graph).

(6) For the linguistic model only: word embedding of the most probable class. Word vectors were
learned with GLOVE from the ground-truth captions of Visual Genome.
Similarly, the relation features zj ∈ RR contained the probabilities of relation entities for the relation
j. For the Linguistic model, these features were extended to include word embedding of the most
probable class. For entity-entity pairwise features zi,j, we use the relation probability for each pair.
Because the output of SGP are probability distributions over entities and relations, we use them as an
the input z to SGP, once again in a recurrent manner and maintain GPI.

We next describe the main components of the GPI architecture. First, we focus on the parts that
output the entity labels. φent is the network that integrates features for two entity variables yi
and yj. It simply takes zi, zj and zi,j as input, and outputs a vector of dimension n1. Next, the
network αent takes as input the outputs of φent for all neighbors of an entity, and uses the attention
mechanism described above to output a vector of dimension n2. Finally, the ρent network takes these
n2 dimensional vectors and outputs L logits predicting the entity value. The ρrel network takes as
input the αent representation of the two entities, as well as zi,j and transforms the output into R
logits. See appendix for speciﬁc network architectures.

5.2.1 Experimental Setup and Results

Dataset. We evaluated our approach on Visual Genome (VG) (Krishna et al., 2017), a dataset with
108,077 images annotated with bounding boxes, entities and relations. On average, images have 12
entities and 7 relations per image. For a proper comparison with previous results (Newell & Deng,
2017; Xu et al., 2017; Zellers et al., 2017), we used the data from (Xu et al., 2017), including the
train and test splits. For evaluation, we used the same 150 entities and 50 relations as in (Newell
& Deng, 2017; Xu et al., 2017; Zellers et al., 2017). To tune hyper-parameters, we also split the
training data into two by randomly selecting 5K examples, resulting in a ﬁnal 70K/5K/32K split for
train/validation/test sets.

Training. All networks were trained using Adam (Kingma & Ba, 2014) with batch size 20. Hyper-
parameter values below were chosen based on the validation set. The SGP loss function was the sum
of cross-entropy losses over all entities and relations in the image. In the loss, we penalized entities
4 times more strongly than relations, and penalized negative relations 10 times more weakly than
positive relations.

Evaluation.
In (Xu et al., 2017) three different evaluation settings were considered. Here we
focus on two of these: (1) SGCls: Given ground-truth bounding boxes for entities, predict all entity
categories and relations categories. (2) PredCls: Given bounding boxes annotated with entity labels,
predict all relations. Following (Lu et al., 2016), we used Recall@K as the evaluation metric. It
measures the fraction of correct ground-truth triplets that appear within the K most conﬁdent triplets
proposed by the model. Two evaluation protocols are used in the literature differing in whether they
enforce graph constraints over model predictions. The ﬁrst graph-constrained protocol requires that
the top-K triplets assign one consistent class per entity and relation. The second unconstrained
protocol does not enforce any such constraints. We report results on both protocols, following (Zellers
et al., 2017).

8

Figure 4: (a) An input image with bounding boxes from VG. (b) The ground-truth scene graph. (c) The
Baseline fails to recognize some entities (tail and tree) and relations (in front of instead of looking at). (d)
GPI:LINGUISTIC ﬁxes most incorrect LP predictions. (e) Window is the most signiﬁcant neighbor of Tree. (f)
The entity bird receives substantial attention, while Tree and building are less informative.

Models and baselines. We compare four variants of our GPI approach with the reported results
of four baselines that are currently the state-of-the-art on various scene graph prediction problems
(all models use the same data split and pre-processing as (Xu et al., 2017)): 1) LU ET AL., 2016
(LU ET AL., 2016): This work leverages word embeddings to ﬁne-tune the likelihood of predicted
relations. 2) XU ET AL, 2017 (XU ET AL., 2017): This model passes messages between entities and
relations, and iteratively reﬁnes the feature map used for prediction. 3) NEWELL & DENG, 2017
(NEWELL & DENG, 2017): The PIXEL2GRAPH model uses associative embeddings (Newell et al.,
2017) to produce a full graph from the image. 4) YANG ET AL., 2018 (YANG ET AL., 2018): The
GRAPH R-CNN model uses object-relation regularities to sparsify and reason over scene graphs. 5)
ZELLERS ET AL., 2017 (ZELLERS ET AL., 2017): The NEURALMOTIF method encodes global
context for capturing high-order motifs in scene graphs, and the BASELINE outputs the entities and
relations distributions without using the global context. The following variants of GPI were compared:
1) GPI: NO ATTENTION: Our GPI model, but with no attention mechanism. Instead, following
Theorem 1, we simply sum the features. 2) GPI: NEIGHBORATTENTION: Our GPI model, with
attention over neighbors features. 3) GPI: LINGUISTIC: Same as GPI: NEIGHBORATTENTION but
also concatenating the word embedding vector, as described above.

Results. Table 1 shows recall@50 and recall@100 for three variants of our approach, and compared
with ﬁve baselines. All GPI variants performs well, with LINGUISTIC outperforming all baselines
for SGCls and being comparable to the state-of-the-art model for PredCls. Note that PredCl is an
easier task, which makes less use of the structure, hence it is not surprising that GPI achieves similar
accuracy to Zellers et al. (2017). Figure 4 illustrates the model behavior. Predicting isolated labels
with zi (4c) mislabels several entities, but these are corrected at the ﬁnal output (4d). Figure 4e
shows that the system learned to attend more to nearby entities (the window and building are closer
to the tree), and 4f shows that stronger attention is learned for the class bird, presumably because it is
usually more informative than common classes like tree.

Implementation details. The φ and α networks were each implemented as a single fully-connected
(FC) layer with a 500-dimensional outputs. ρ was implemented as a FC network with 3 500-
dimensional hidden layers, with one 150-dimensional output for the entity probabilities, and one
51-dimensional output for relation probabilities. The attention mechanism was implemented as a
network like to φ and α, receiving the same inputs, but using the output scores for the attention . The
full code is available at https://github.com/shikorab/SceneGraph

6 Conclusion

We presented a deep learning approach to structured prediction, which constrains the architecture
to be invariant to structurally identical inputs. As in score-based methods, our approach relies on
pairwise features, capable of describing inter-label correlations, and thus inheriting the intuitive
aspect of score-based approaches. However, instead of maximizing a score function (which leads
to computationally-hard inference), we directly produce an output that is invariant to equivalent
representations of the pairwise terms.

9

This axiomatic approach to model architecture can be extended in many ways. For image labeling,
geometric invariances (shift or rotation) may be desired.
In other cases, invariance to feature
permutations may be desirable. We leave the derivation of the corresponding architectures to future
work. Finally, there may be cases where the invariant structure is unknown and should be discovered
from data, which is related to work on lifting graphical models Bui et al. (2013). It would be interesting
to explore algorithms that discover and use such symmetries for deep structured prediction.

This work was supported by the ISF Centers of Excellence grant, and by the Yandex Initiative in
Machine Learning. Work by GC was performed while at Google Brain Research.

Acknowledgements

References

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and

translate. In International Conference on Learning Representations (ICLR), 2015.

Belanger, David, Yang, Bishan, and McCallum, Andrew. End-to-end learning for structured prediction
energy networks. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70, pp. 429–439. PMLR, 2017.

Bello, Irwan, Pham, Hieu, Le, Quoc V, Norouzi, Mohammad, and Bengio, Samy. Neural combinato-

rial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Bui, Hung Hai, Huynh, Tuyen N., and Riedel, Sebastian. Automorphism groups of graphical models
and lifted variational inference. In Proceedings of the Twenty-Ninth Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’13, pp. 132–141, 2013.

Chen, Danqi and Manning, Christopher. A fast and accurate dependency parser using neural
networks. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 740–750, 2014.

Chen, Liang Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Se-
mantic image segmentation with deep convolutional nets and fully connected CRFs. In Proceedings
of the Second International Conference on Learning Representations, 2014.

Chen, Liang Chieh, Schwing, Alexander G, Yuille, Alan L, and Urtasun, Raquel. Learning deep

structured models. In Proc. ICML, 2015.

Farabet, Clement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical
features for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):
1915–1929, 2013.

Gilmer, Justin, Schoenholz, Samuel S, Riley, Patrick F, Vinyals, Oriol, and Dahl, George E. Neural

message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.

Gygli, Michael, Norouzi, Mohammad, and Angelova, Anelia. Deep value networks learn to evaluate
and iteratively reﬁne structured outputs. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine
Learning Research, pp. 1341–1351, International Convention Centre, Sydney, Australia, 2017.
PMLR.

Johnson, Justin, Krishna, Ranjay, Stark, Michael, Li, Li-Jia, Shamma, David A., Bernstein, Michael S.,
In Proc. Conf. Comput. Vision Pattern

Image retrieval using scene graphs.

and Li, Fei-Fei.
Recognition, pp. 3668–3678, 2015.

Johnson, Justin, Gupta, Agrim, and Fei-Fei, Li. Image generation from scene graphs. arXiv preprint

arXiv:1804.01622, 2018.

Khalil, Elias, Dai, Hanjun, Zhang, Yuyu, Dilkina, Bistra, and Song, Le. Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp.
6351–6361, 2017.

10

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint

arXiv: 1412.6980, abs/1412.6980, 2014.

Krishna, Ranjay, Zhu, Yuke, Groth, Oliver, Johnson, Justin, Hata, Kenji, Kravitz, Joshua, Chen,
Stephanie, Kalantidis, Yannis, Li, Li-Jia, Shamma, David A, et al. Visual genome: Connecting
International Journal of
language and vision using crowdsourced dense image annotations.
Computer Vision, 123(1):32–73, 2017.

Lafferty, J., McCallum, A., and Pereira, F. Conditional random ﬁelds: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning, pp. 282–289, 2001.

Liao, Wentong, Yang, Michael Ying, Ackermann, Hanno, and Rosenhahn, Bodo. On support relations

and semantic scene graphs. arXiv preprint arXiv:1609.05834, 2016.

Lin, Guosheng, Shen, Chunhua, Reid, Ian, and van den Hengel, Anton. Deeply learning the messages
in message passing inference. In Advances in Neural Information Processing Systems, pp. 361–369,
2015.

Lu, Cewu, Krishna, Ranjay, Bernstein, Michael S., and Li, Fei-Fei. Visual relationship detection with

language priors. In European Conf. Comput. Vision, pp. 852–869, 2016.

Meshi, O., Sontag, D., Jaakkola, T., and Globerson, A. Learning efﬁciently with approximate
In Proceedings of the 27th International Conference on Machine

inference via dual losses.
Learning, pp. 783–790, New York, NY, USA, 2010. ACM.

Newell, Alejandro and Deng, Jia. Pixels to graphs by associative embedding. In Advances in Neural
Information Processing Systems 30 (to appear), pp. 1172–1180. Curran Associates, Inc., 2017.

Newell, Alejandro, Huang, Zhiao, and Deng, Jia. Associative embedding: End-to-end learning for
joint detection and grouping. In Neural Inform. Process. Syst., pp. 2274–2284. Curran Associates,
Inc., 2017.

Pei, Wenzhe, Ge, Tao, and Chang, Baobao. An effective neural network model for graph-based de-
pendency parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computationa
Linguistics, pp. 313–322, 2015.

Plummer, Bryan A., Mallya, Arun, Cervantes, Christopher M., Hockenmaier, Julia, and Lazebnik,
Svetlana. Phrase localization and visual relationship detection with comprehensive image-language
cues. In ICCV, pp. 1946–1955, 2017.

Raposo, David, Santoro, Adam, Barrett, David, Pascanu, Razvan, Lillicrap, Timothy, and Battaglia,
Peter. Discovering objects and their relations from entangled scene representations. arXiv preprint
arXiv:1702.05068, 2017.

Schwing, Alexander G and Urtasun, Raquel. Fully connected deep structured networks. ArXiv

e-prints, 2015.

Shelhamer, Evan, Long, Jonathan, and Darrell, Trevor. Fully convolutional networks for semantic

segmentation. Proc. Conf. Comput. Vision Pattern Recognition, 39(4):640–651, 2017.

Taskar, B., Guestrin, C., and Koller, D. Max margin Markov networks. In Thrun, S., Saul, L., and
Schölkopf, B. (eds.), Advances in Neural Information Processing Systems 16, pp. 25–32. MIT
Press, Cambridge, MA, 2004.

Xu, Danfei, Zhu, Yuke, Choy, Christopher B., and Fei-Fei, Li. Scene Graph Generation by Iterative
Message Passing. In Proc. Conf. Comput. Vision Pattern Recognition, pp. 3097–3106, 2017.

Yang, Jianwei, Lu, Jiasen, Lee, Stefan, Batra, Dhruv, and Parikh, Devi. Graph R-CNN for scene

graph generation. In European Conf. Comput. Vision, pp. 690–706, 2018.

Zaheer, Manzil, Kottur, Satwik, Ravanbakhsh, Siamak, Poczos, Barnabas, Salakhutdinov, Ruslan R,
and Smola, Alexander J. Deep sets. In Advances in Neural Information Processing Systems 30, pp.
3394–3404. Curran Associates, Inc., 2017.

11

Zellers, Rowan, Yatskar, Mark, Thomson, Sam, and Choi, Yejin. Neural motifs: Scene graph parsing

with global context. arXiv preprint arXiv:1711.06640, abs/1711.06640, 2017.

Zheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav, Su, Zhizhong, Du,
Dalong, Huang, Chang, and Torr, Philip HS. Conditional random ﬁelds as recurrent neural networks.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 1529–1537, 2015.

7 Supplementary Material

This supplementary material includes: (1) Visual illustration of the proof of Theorem 1. (2) Explaining
how to integrate an attention mechanism in our GPI framework. (3) Additional evaluation method to
further analyze and compare our work with baselines.

7.1 Theorem 1: Illustration of Proof

Figure 5: Illustration of the proof of Theorem 1 using a speciﬁc construction example. Here H is a hash function
of size L = 5 such that H(1) = 1, H(3) = 2, H(2) = 4, G is a three-node input graph, and zi,j ∈ R are the
pairwise features (in purple) of G. (a) φ is applied to each zi,j. Each application yields a vector in R5. The three
dark yellow columns correspond to φ(z1,1), φ(z1,2) and φ(z1,3). Then, all vectors φ(zi,j) are summed over j
to obtain three si vectors. (b) α’s (blue matrices) are an outer product between 1 [H(zi)] and si resulting in a
matrix of zeros except one row. The dark blue matrix corresponds for α(z1, s1). (c) All α’s are summed to a
5 × 5 matrix, isomorphic to the original zi,j matrix.

7.2 Characterizing Permutation Invariance: Attention

Attention is a powerful component which naturally can be introduced into our GPI model. We now
show how attention can be introduced in our framework. Formally, we learn attention weights for the
neighbors j of a node i, which scale the features zi,j of that neighbor. We can also learn different
attention weights for individual features of each neighbor in a similar way.

Let wi,j ∈ R be an attention mask specifying the weight that node i gives to node j:

wi,j(zi, zi,j, zj) = eβ(zi,zi,j ,zj )/

eβ(zi,zi,t,zt)

(4)

(cid:88)

t

where β can be any scalar-valued function of its arguments (e.g., a dot product of zi and zj as in
standard attention models). To introduce attention we wish α ∈ Re to have the form of weighting
wi,j over neighboring feature vectors zi,j, namely, α = (cid:80)
To achieve this form we extend φ by a single entry, deﬁning φ ∈ Re+1 (namely we set
L = e + 1) as φ1:e(zi, zi,j, zj) = eβ(zi,zi,j ,zj )zi,j (here φ1:e are the ﬁrst e elements of φ)

j(cid:54)=i wi,jzi,j.

12

and φe+1(zi, zi,j; zj) = eβ(zi,zi,j ,zj ). We keep the deﬁnition of si = (cid:80)
we deﬁne α = si,1:e
si,e+1
over neighboring feature vectors zi,j:

j(cid:54)=i φ(zi, zi,j, zj). Next,
and substitute si and φ to obtain the desired form as attention weights wi,j

α(zi, si) =

si,1:e
si,e+1

(cid:88)

=

j(cid:54)=i

eβ(zi,zi,j ,zj )zi,j
(cid:80)
j(cid:54)=i eβ(zi,zi,j ,zj )

(cid:88)

=

j(cid:54)=i

wi,jzi,j

A similar approach can be applied over α and ρ to model attention over the outputs of α as well
(graph nodes).

7.3 Scene Graph Results

In the main paper, we described the results for the two prediction tasks: SGCls and PredCls, as
deﬁned in section 5.2.1: "Experimental Setup and Results". To further analyze our module, we
compare the best variant, GPI: LINGUISTIC, per relation to two baselines: (Lu et al., 2016) and Xu
et al. (2017). Table 2, speciﬁes the PredCls recall@5 of the 20-top frequent relation classes. The GPI
module performs better in almost all the relations classes.

RELATION

(LU ET AL., 2016)

(XU ET AL., 2017)

LINGUISTIC

99.71
98.03
80.38
82.47
98.47
85.16
31.85
49.19
61.50
79.35
28.64
31.74
26.09
8.45
54.08

ON
HAS
IN
OF
WEARING
NEAR
WITH
ABOVE
HOLDING
BEHIND
UNDER
SITTING ON
IN FRONT OF
ATTACHED TO
AT
HANGING FROM 0.0
OVER
FOR
RIDING

9.26
12.20
72.43

99.25
97.25
88.30
96.75
98.23
96.81
88.10
79.73
80.67
92.32
52.73
50.17
59.63
29.58
70.41
0.0
0.0
31.71
89.72

99.3
98.7
95.9
98.1
99.6
95.4
94.2
83.9
95.5
91.2
83.2
90.4
74.9
77.4
80.9
74.1
62.4
45.1
96.1

Table 2: Recall@5 of PredCls for the 20-top relations ranked by their frequency, as in (Xu et al., 2017)

13

8
1
0
2
 
v
o
N
 
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
5
4
5
0
.
2
0
8
1
:
v
i
X
r
a

Mapping Images to Scene Graphs with
Permutation-Invariant Structured Prediction

Roei Herzig∗
Tel Aviv University
roeiherzig@mail.tau.ac.il

Moshiko Raboh∗
Tel Aviv University
mosheraboh@mail.tau.ac.il

Gal Chechik
Bar-Ilan University, NVIDIA Research
gal.chechik@biu.ac.il

Jonathan Berant
Tel Aviv University, AI2
joberant@cs.tau.ac.il

Amir Globerson
Tel Aviv University
gamir@post.tau.ac.il

Abstract

Machine understanding of complex images is a key goal of artiﬁcial intelligence.
One challenge underlying this task is that visual scenes contain multiple inter-
related objects, and that global context plays an important role in interpreting
the scene. A natural modeling framework for capturing such effects is structured
prediction, which optimizes over complex labels, while modeling within-label
interactions. However, it is unclear what principles should guide the design of a
structured prediction model that utilizes the power of deep learning components.
Here we propose a design principle for such architectures that follows from a
natural requirement of permutation invariance. We prove a necessary and sufﬁ-
cient characterization for architectures that follow this invariance, and discuss its
implication on model design. Finally, we show that the resulting model achieves
new state-of-the-art results on the Visual Genome scene-graph labeling benchmark,
outperforming all recent approaches.

1

Introduction

Understanding the semantics of a complex visual scene is a fundamental problem in machine
perception. It often requires recognizing multiple objects in a scene, together with their spatial and
functional relations. The set of objects and relations is sometimes represented as a graph, connecting
objects (nodes) with their relations (edges) and is known as a scene graph (Figure 1). Scene graphs
provide a compact representation of the semantics of an image, and can be useful for semantic-level
interpretation and reasoning about a visual scene Johnson et al. (2018). Scene-graph prediction is the
problem of inferring the joint set of objects and their relations in a visual scene.

Since objects and relations are inter-dependent (e.g., a person and chair are more likely to be in relation
“sitting on” than “eating”), a scene graph predictor should capture this dependence in order to improve
prediction accuracy. This goal is a special case of a more general problem, namely, inferring multiple
inter-dependent labels, which is the research focus of the ﬁeld of structured prediction. Structured
prediction has attracted considerable attention because it applies to many learning problems and

∗Equal Contribution.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Figure 1: An image and its scene graph from the Visual Genome dataset (Krishna et al., 2017). The scene graph
captures the entities in the image (nodes, blue circles) like dog and their relations (edges, red circles) like (cid:10)hat,
on, dog(cid:11).

poses unique theoretical and algorithmic challenges (e.g., see Belanger et al., 2017; Chen et al., 2015;
Taskar et al., 2004). It is therefore a natural approach for predicting scene graphs from images.

Structured prediction models typically deﬁne a score function s(x, y) that quantiﬁes how well a
label assignment y is compatible with an input x. In the case of understanding complex visual
scenes, x is an image, and y is a complex label containing the labels of objects detected in an image
and the labels of their relations. In this setup, the inference task amounts to ﬁnding the label that
maximizes the compatibility score y∗ = arg maxy s(x, y). This score-based approach separates a
scoring component – implemented by a parametric model, from an optimization component – aimed
at ﬁnding a label that maximizes that score. Unfortunately, for a general scoring function s(·), the
space of possible label assignments grows exponentially with input size. For instance, for scene
graphs the set of possible object label assignments is too large even for relatively simple images,
since the vocabulary of candidate objects may contain thousands of objects. As a result, inferring the
label assignment that maximizes a scoring function is computationally hard in the general case.

An alternative approach to score-based methods is to map an input x to a structured output y with
a “black box" neural network, without explicitly deﬁning a score function. This raises a natural
question: what is the right architecture for such a network? Here we take an axiomatic approach and
argue that one important property such networks should satisfy is invariance to a particular type of
input permutation. We then prove that this invariance is equivalent to imposing certain structural
constraints on the architecture of the network, and describe architectures that satisfy these constraints.

To evaluate our approach, we ﬁrst demonstrate on a synthetic dataset that respecting permutation
invariance is important, because models that violate this invariance need more training data, despite
having a comparable model size. Then, we tackle the problem of scene graph generation. We describe
a model that satisﬁes the permutation invariance property, and show that it achieves state-of-the-art
results on the competitive Visual Genome benchmark (Krishna et al., 2017), demonstrating the power
of our new design principle.

In summary, the novel contributions of this paper are: a) Deriving sufﬁcient and necessary conditions
for graph-permutation invariance in deep structured prediction architectures. b) Empirically demon-
strating the beneﬁt of graph-permutation invariance. c) Developing a state-of-the-art model for scene
graph prediction on a large dataset of complex visual scenes.

2 Structured Prediction

Scored-based methods in structured prediction deﬁne a function s(x, y) that quantiﬁes the degree
to which y is compatible with x, and infer a label by maximizing s(x, y) (e.g., see Belanger et al.,
2017; Chen et al., 2015; Lafferty et al., 2001; Meshi et al., 2010; Taskar et al., 2004). Most score
functions previously used decompose as a sum over simpler functions, s(x, y) = (cid:80)
i fi(x, y), making
it possible to optimize maxy fi(x, y) efﬁciently. This local maximization forms the basic building
block of algorithms for approximately maximizing s(x, y). One way to decompose the score function
is to restrict each fi(x, y) to depend only on a small subset of the y variables.

The renewed interest in deep learning led to efforts to integrate deep networks with structured predic-
tion, including modeling the fi functions as deep networks. In this context, the most widely-used
score functions are singleton fi(yi, x) and pairwise fij(yi, yj, x). The early work taking this approach
used a two-stage architecture, learning the local scores independently of the structured prediction

2

Figure 2: Left: Graph permutation invariance. A graph labeling function F is graph permutation invariant
(GPI) if permuting the node features maintains the output. Right: a schematic representation of the GPI
architecture in Theorem 1. Singleton features zi are omitted for simplicity. (a) First, the features zi,j are
processed element-wise by φ. (b) Features are summed to create a vector si, which is concatenated with zi. (c)
A representation of the entire graph is created by applying α n times and summing the created vector. (d) The
graph representation is then ﬁnally processed by ρ together with zk.

goal (Chen et al., 2014; Farabet et al., 2013). Later studies considered end-to-end architectures where
the inference algorithm is part of the computation graph (Chen et al., 2015; Pei et al., 2015; Schwing
& Urtasun, 2015; Zheng et al., 2015). Recent studies go beyond pairwise scores, also modelling
global factors (Belanger et al., 2017; Gygli et al., 2017).

Score-based methods provide several advantages. First, they allow intuitive speciﬁcation of local
dependencies between labels and how these translate to global dependencies. Second, for linear
score functions, the learning problem has natural convex surrogates Lafferty et al. (2001); Taskar
et al. (2004). Third, inference in large label spaces is sometimes possible via exact algorithms or
empirically accurate approximations. However, with the advent of deep scoring functions s(x, y; w),
learning is no longer convex. Thus, it is worthwhile to rethink the architecture of structured prediction
models, and consider models that map inputs x to outputs y directly without explicitly maximizing a
score function. We would like these models to enjoy the expressivity and predictive power of neural
networks, while maintaining the ability to specify local dependencies between labels in a ﬂexible
manner. In the next section, we present such an approach and consider a natural question: what
should be the properties of a deep neural network used for structured prediction.

3 Permutation-Invariant Structured Prediction

In what follows we deﬁne the permutation-invariance property for structured prediction models, and
argue that permutation invariance is a natural principle for designing their architecture.

We ﬁrst introduce our notation. We focus on structures with pairwise interactions, because they are
simpler in terms of notation and are sufﬁcient for describing the structure in many problems. We
denote a structured label by y = [y1, . . . , yn]. In a score-based approach, the score is deﬁned via a
set of singleton scores fi(yi, x) and pairwise scores fij(yi, yj, x), where the overall score s(x, y) is
the sum of these scores. For brevity, we denote fij = fij(yi, yj, x) and fi = fi(yi, x). An inference
algorithm takes as input the local scores fi, fij and outputs an assignment that maximizes s(x, y).
We can thus view inference as a black-box that takes node-dependent and edge-dependent inputs
(i.e., the scores fi, fij) and returns a label y, even without an explicit score function s(x, y). While
numerous inference algorithms exist for this setup, including belief propagation (BP) and mean ﬁeld,
here we develop a framework for a deep labeling algorithm (we avoid the term “inference” since the
algorithm does not explicitly maximize a score function). Such an algorithm will be a black-box,
taking the f functions as input and the labels y1, . . . , yn as output. We next ask what architecture
such an algorithm should have.

We follow with several deﬁnitions. A graph labeling function F : (V, E) → Y is a function whose
input is an ordered set of node features V = [z1, . . . , zn] and an ordered set of edge features
E = [z1,2 . . . , zi,j, . . . , zn,n−1]. For example, zi can be the array of values fi, and zi,j can be
the table of values fi,j. Assume zi ∈ Rd and zi,j ∈ Re. The output of F is a set of node labels
y = [y1, . . . , yn]. Thus, algorithms such as BP are graph labeling functions. However, graph labeling
functions do not necessarily maximize a score function. We denote the joint set of node features and
edge features by z (i.e., a set of n + n(n − 1) = n2 vectors). In Section 3.1 we discuss extensions to
this case where only a subset of the edges is available.

3

1, y∗

2, y∗

1, y∗

2, y∗

A natural requirement is that the function F produces the same result when given the same features,
up to a permutation of the input. For example, consider a label space with three variables y1, y2, y3,
and assume that F takes as input z = (z1, z2, z3, z12, z13, z23) = (f1, f2, f3, f12, f13, f23), and
outputs a label y = (y∗
3). When F is given an input that is permuted in a consistent way, say,
z(cid:48) = (f2, f1, f3, f21, f23, f13), this deﬁnes exactly the same input. Hence, the output should still be
y = (y∗
3). Most inference algorithms, including BP and mean ﬁeld, satisfy this symmetry
requirement by design, but this property is not guaranteed in general in a deep model. Here, our
goal is to design a deep learning black-box, and hence we wish to guarantee invariance to input
permutations. A black-box that violates this invariance “wastes” capacity on learning it at training
time, which increases sample complexity, as shown in Sec. 5.1. We proceed to formally deﬁne the
permutation invariance property.
Deﬁnition 1. Let z be a set of node features and edge features, and let σ be a permutation of
{1, . . . , n}. We deﬁne σ(z) to be a new set of node and edge features given by [σ(z)]i = zσ(i) and
[σ(z)]i,j = zσ(i),σ(j).

We also use the notation σ([y1, . . . , yn]) = [yσ(1), . . . , yσ(n)] for permuting the labels. Namely, σ
applied to a set of labels yields the same labels, only permuted by σ. Be aware that applying σ to
the input features is different from permuting labels, because edge input features must permuted in a
way that is consistent with permuting node input features. We now provide our key deﬁnition of a
function whose output is invariant to permutations of the input. See Figure 2 (left).
Deﬁnition 2. A graph labeling function F is said to be graph-permutation invariant (GPI), if for
all permutations σ of {1, . . . , n} and for all z it satisﬁes: F(σ(z)) = σ(F(z)).

3.1 Characterizing Permutation Invariance

Motivated by the above discussion, we ask: what structure is necessary and sufﬁcient to guarantee
that F is GPI? Note that a function F takes as input an ordered set z. Therefore its output on z
could certainly differ from its output on σ(z). To achieve permutation invariance, F should contain
certain symmetries. For instance, one permutation invariant architecture could be to deﬁne yi = g(zi)
for any function g, but this architecture is too restrictive and does not cover all permutation invariant
functions. Theorem 1 below provides a complete characterization (see Figure 2 for the corresponding
architecture). Intuitively, the architecture in Theorem 1 is such that it can aggregate information from
the entire graph, and do so in a permutation invariant manner.
Theorem 1. Let F be a graph labeling function. Then F is graph-permutation invariant if and only
if there exist functions α, ρ, φ such that for all k = 1, . . . , n:

[F(z)]k = ρ

zk,

α

zi,

φ(zi, zi,j, zj)



 ,

(1)









n
(cid:88)

i=1

(cid:88)

j(cid:54)=i

where φ : R2d+e → RL, α : Rd+L → RW and ρ : RW +d → R.

Proof. First, we show that any F satisfying the conditions of Theorem 1 is GPI. Namely, for any
permutation σ, [F(σ(z))]k = [F(z)]σ(k). To see this, write [F(σ(z))]k using Eq. 1 and Deﬁnition 1:

[F(σ(z))]k = ρ(zσ(k),

α(zσ(i),

φ(zσ(i), zσ(i),σ(j), zσ(j)))).

(2)

(cid:88)

i

(cid:88)

j(cid:54)=i

The second argument of ρ above is invariant under σ, because it is a sum over nodes and their
neighbors, which is invariant under permutation. Thus Eq. 2 is equal to:

ρ(zσ(k),

α(zi,

φ(zi, zi,j, zj))) = [F(z)]σ(k)

(cid:88)

i

(cid:88)

j(cid:54)=i

where equality follows from Eq. 1. We thus proved that Eq. 1 implies graph permutation invariance.

Next, we prove that any given GPI function F0 can be expressed as a function F in Eq. 1. Namely,
we show how to deﬁne φ, α and ρ that can implement F0. Note that in this direction of the proof the
function F0 is a black-box. Namely, we only know that it is GPI, but do not assume anything else
about its implementation.

4

The key idea is to construct φ, α such that the second argument of ρ in Eq. 1 contains the information
about all the graph features z. Then, the function ρ corresponds to an application of F0 to this
representation, followed by extracting the label yk. To simplify notation assume edge features are
scalar (e = 1). The extension to vectors is simple, but involves more indexing.

We assume WLOG that the black-box function F0 is a function only of the pairwise features zi,j
(otherwise, we can always augment the pairwise features with the singleton features). Since zi,j ∈ R
we use a matrix Rn,n to denote all the pairwise features.

Finally, we assume that our implementation of F0 will take additional node features zk such that no
two nodes have the same feature (i.e., the features identify the node).

Our goal is thus to show that there exist functions α, φ, ρ such that the function in Eq. 2 applied to Z
yields the same labels as F0(Z).

Let H be a hash function with L buckets mapping node features zi to an index (bucket). Assume
that H is perfect (this can be achieved for a large enough L). Deﬁne φ to map the pairwise
features to a vector of size L. Let 1 [j] be a one-hot vector of dimension RL, with one in the
jth coordinate. Recall that we consider scalar zi,j so that φ is indeed in RL, and deﬁne φ as:
φ(zi, zi,j, zj) = 1 [H(zj)] zi,j, i.e., φ “stores” zi,j in the unique bucket for node j.
Let si = (cid:80)
zi,j ∈E φ(zi, zi,j, zj) be the second argument of α in Eq. 1 (si ∈ RL). Then, since all
zj are distinct, si stores all the pairwise features for neighbors of i in unique positions within its
L coordinates. Since si(H(zk)) contains the feature zi,k whereas sj(H(zk)) contains the feature
zj,k, we cannot simply sum the si, since we would lose the information of which edges the features
originated from. Instead, we deﬁne α to map si to RL×L such that each feature is mapped to a
distinct location. Formally:

α(zi, si) = 1 [H(zi)] sT
i
α outputs a matrix that is all zeros except for the features corresponding to node i that are stored in
row H(zi). The matrix M = (cid:80)
i α(zi, si) (namely, the second argument of ρ in Eq. 1) is a matrix
with all the edge features in the graph including the graph structure.

(3)

.

To complete the construction we set ρ to have the same outcome as F0. We ﬁrst discard rows and
columns in M that do not correspond to original nodes (reducing M to dimension n × n). Then, we
use the reduced matrix as the input z to the black-box F0.

Assume for simplicity that M does not need to be contracted (this merely introduces another indexing
step). Then M corresponds to the original matrix Z of pairwise features, with both rows and
columns permuted according to H. We will thus use M as input to the function F0. Since F0 is
GPI, this means that the label for node k will be given by F0(M ) in position H(zk). Thus we set
ρ(zk, M ) = [F0(M )]H(zk), and by the argument above this equals [F0(Z)]k, implying that the
above α, φ and ρ indeed implement F0.

Extension to general graphs So far, we discussed complete graphs, where edges correspond
to valid feature pairs. However, many graphs of interest might be incomplete. For example, an
n-variable chain graph in sequence labeling has only n − 1 edges. For such graphs, the input to F
would not contain all zi,j pairs but rather only features corresponding to valid edges of the graph, and
we are only interested in invariances that preserve the graph structure, namely, the automorphisms
of the graph. Thus, the desired invariance is that σ(F(z)) = F(σ(z)), where σ is not an arbitrary
permutation but an automorphism. It is easy to see that a simple variant of Theorem 1 holds in
this case. All we need to do is replace in Eq. 2 the sum (cid:80)
j∈N (i), where N (i) are the
neighbors of node i in the graph. The arguments are then similar to the proof above.

j(cid:54)=i with (cid:80)

Implications of Theorem 1 Our result has interesting implications for deep structured prediction.
First, it highlights that the fact that the architecture “collects” information from all different edges
of the graph, in an invariant fashion via the α, φ functions. Speciﬁcally, the functions φ (after
summation) aggregate all the features around a given node, and then α (after summation) can
collect them. Thus, these functions can provide a summary of the entire graph that is sufﬁcient for
downstream algorithms. This is different from one round of message passing algorithms which would
not be sufﬁcient for collecting global graph information. Note that the dimensions of φ, α may need
to be large to aggregate all graph information (e.g., by hashing all the features as in the proof of
Theorem 1), but the architecture itself can be shallow.

5

Second, the architecture is parallelizable, as all φ functions can be applied simultaneously. This is in
contrast to recurrent models Zellers et al. (2017) which are harder to parallelize and are thus slower
in practice.

Finally, the theorem suggests several common architectural structures that can be used within GPI.
We brieﬂy mention two of these. 1) Attention: Attention is a powerful component in deep learning
architectures (Bahdanau et al., 2015), but most inference algorithms do not use attention. Intuitively,
in attention each node i aggregates features of neighbors through a weighted sum, where the weight
is a function of the neighbor’s relevance. For example, the label of an entity in an image may depend
more strongly on entities that are spatially closer. Attention can be naturally implemented in our
GPI characterization, and we provide a full derivation for this implementation in the appendix. It
plays a key role in our scene graph model described below. 2) RNNs: Because GPI functions are
closed under composition, for any GPI function F we can run F iteratively by providing the output
of one step of F as part of the input to the next step and maintain GPI. This results in a recurrent
architecture, which we use in our scene graph model.

4 Related Work

The concept of architectural invariance was recently proposed in DEEPSETS (Zaheer et al., 2017).
The invariance we consider is much less restrictive: the architecture does not need to be invariant
to all permutations of singleton and pairwise features, just those consistent with a graph re-labeling.
This characterization results in a substantially different set of possible architectures.

Deep structured prediction. There has been signiﬁcant recent interest in extending deep learning
to structured prediction tasks. Much of this work has been on semantic segmentation, where
convolutional networks (Shelhamer et al., 2017) became a standard approach for obtaining “singleton
scores” and various approaches were proposed for adding structure on top. Most of these approaches
used variants of message passing algorithms, unrolled into a computation graph (Xu et al., 2017).
Some studies parameterized parts of the message passing algorithm and learned its parameters (Lin
et al., 2015). Recently, gradient descent has also been used for maximizing score functions (Belanger
et al., 2017; Gygli et al., 2017). An alternative to deep structured prediction is greedy decoding,
inferring each label at a time based on previous labels. This approach has been popular in sequence-
based applications (e.g., parsing (Chen & Manning, 2014)), relying on the sequential structure of the
input, where BiLSTMs are effectively applied. Another related line of work is applying deep learning
to graph-based problems, such as TSP (Bello et al., 2016; Gilmer et al., 2017; Khalil et al., 2017).
Clearly, the notion of graph invariance is important in these, as highlighted in (Gilmer et al., 2017).
They however do not specify a general architecture that satisﬁes invariance as we do here, and in
fact focus on message passing architectures, which we strictly generalize. Furthermore, our focus is
on the more general problem of structured prediction, rather than speciﬁc graph-based optimization
problems.

Scene graph prediction. Extracting scene graphs from images provides a semantic representation
that can later be used for reasoning, question answering, and image retrieval (Johnson et al., 2015;
Lu et al., 2016; Raposo et al., 2017). It is at the forefront of machine vision research, integrating
challenges like object detection, action recognition and detection of human-object interactions (Liao
et al., 2016; Plummer et al., 2017). Prior work on scene graph predictions used neural message
passing algorithms (Xu et al., 2017) as well as prior knowledge in the form of word embeddings
(Lu et al., 2016). Other work suggested to predict graphs directly from pixels in an end-to-end
manner Newell & Deng (2017). NeuralMotif (Zellers et al., 2017), currently the state-of-the-art
model for scene graph prediction on Visual Genome, employs an RNN that provides global context
by sequentially reading the independent predictions for each entity and relation and then reﬁnes those
predictions. The NEURALMOTIF model maintains GPI by ﬁxing the order in which the RNN reads
its inputs and thus only a single order is allowed. However, this ﬁxed order is not guaranteed to be
optimal.

5 Experimental Evaluation

We empirically evaluate the beneﬁt of GPI architectures. First, using a synthetic graph-labeling task,
and then for the problem of mapping images to scene graphs.

6

Figure 3: Accuracy as a function of sample size for graph labeling. Right is a zoomed in version of left.

5.1 Synthetic Graph Labeling

We start with studying GPI on a synthetic problem, deﬁned as follows. An input graph G = (V, E)
is given, where each node i ∈ V is assigned to one of K sets. The set for node i is denoted by
Γ(i). The goal is to compute for each node the number of neighbors that belong to the same set.
Namely, the label of a node is yi = (cid:80)
j∈N (i) 1[Γ(i) = Γ(j)]. We generated random graphs with 10
nodes (larger graphs produced similar results) by sampling each edge independently and uniformly,
and sampling Γ(i) for every node uniformly from {1, . . . , K}. The node features zi ∈ {0, 1}K are
one-hot vectors of Γ(i) and the edge features zi,j ∈ {0, 1} indicate whether ij ∈ E. We compare two
standard non-GPI architectures and one GPI architecture: (a) A GPI-architecture for graph prediction,
described in detail in Section 5.2. We used the basic version without attention and RNN. (b) LSTM:
We replace (cid:80) φ(·) and (cid:80) α(·), which perform aggregation in Theorem 1 with two LSTMs with
a state size of 200 that read their input in random order. (c) A fully-connected (FC) feed-forward
network with 2 hidden layers of 1000 nodes each. The input to the fully connected model is a
concatenation of all node and pairwise features. The output is all node predictions. The focus of the
experiment is to study sample complexity. Therefore, for a fair comparison, we use the same number
of parameters for all models.

Figure 3, shows the results, demonstrating that GPI requires far fewer samples to converge to the
correct solution. This illustrates the advantage of an architecture with the correct inductive bias for
the problem.

5.2 Scene-Graph Classiﬁcation

We evaluate the GPI approach on the motivating task of this paper, inferring scene graphs from
images (Figure 1). In this problem, the input is an image annotated with a set of bounding boxes for
the entities in the image.2 The goal is to label each bounding box with the correct entity category and
every pair of entities with their relation, such that they form a coherent scene graph.

We begin by describing our Scene Graph Predictor (SGP) model. We aim to predict two types of
variables. The ﬁrst is entity variables [y1, . . . , yn] for all bounding boxes. Each yi can take one of
L values (e.g., “dog”, “man”). The second is relation variables [yn+1, . . . , yn2] for every pair of
bounding boxes. Each such yj can take one of R values (e.g., “on”, “near”). Our graph connects
variables that are expected to be inter-related. It contains two types of edges: 1) entity-entity edge
connecting every two entity variables (yi and yj for 1 ≤ i (cid:54)= j ≤ n. 2) entity-relation edges
connecting every relation variable yk (where k > n) to its two entity variables. Thus, our graph is not
a complete graph and our goal is to design an architecture that will be invariant to any automorphism
of the graph, such as permutations of the entity variables.

For the input features z, we used the features learned by the baseline model from Zellers et al.
(2017).3 Speciﬁcally, the entity features zi included (1) The conﬁdence probabilities of all entities
for yi as learned by the baseline model. (2) Bounding box information given as (left, bottom,
width, height); (3) The number of smaller entities (also bigger); (4) The number of entities to
the left, right, above and below. (5) The number of entities with higher and with lower conﬁdence;

2For simplicity, we focus on the task where boxes are given.
3The baseline does not use any LSTM or context, and is thus unrelated to the main contribution of Zellers

et al. (2017).

7

Constrained Evaluation

SGCls

PredCls

Unconstrained Evaluation
SGCls

PredCls

R@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100

Lu et al., 2016 (Lu et al., 2016)
Xu et al., 2017 (Xu et al., 2017)
Pixel2Graph (Newell & Deng, 2017)
Graph R-CNN (Yang et al., 2018)
Neural Motifs (Zellers et al., 2017)
Baseline (Zellers et al., 2017)
No Attention
Neighbor Attention
Linguistic

11.8
21.7
-
29.6
35.8
34.6
35.3
35.7
36.5

14.1
24.4
-
31.6
36.5
35.3
37.2
38.5
38.8

35.0
44.8
-
54.2
65.2
63.7
64.5
64.6
65.1

27.9
53.0
-
59.1
67.1
65.6
66.3
66.6
66.9

-
-
26.5
-
44.5
43.4
44.1
44.7
45.5

-
-
30.0
-
47.7
46.6
48.5
49.9
50.8

-
-
68.0
-
81.1
78.8
79.7
80.0
80.8

-
-
75.2
-
88.3
85.9
86.7
87.1
88.2

Table 1: Test set results for graph-constrained evaluation (i.e., the returned triplets must be consistent with a
scene graph) and for unconstrained evaluation (triplets need not be consistent with a scene graph).

(6) For the linguistic model only: word embedding of the most probable class. Word vectors were
learned with GLOVE from the ground-truth captions of Visual Genome.
Similarly, the relation features zj ∈ RR contained the probabilities of relation entities for the relation
j. For the Linguistic model, these features were extended to include word embedding of the most
probable class. For entity-entity pairwise features zi,j, we use the relation probability for each pair.
Because the output of SGP are probability distributions over entities and relations, we use them as an
the input z to SGP, once again in a recurrent manner and maintain GPI.

We next describe the main components of the GPI architecture. First, we focus on the parts that
output the entity labels. φent is the network that integrates features for two entity variables yi
and yj. It simply takes zi, zj and zi,j as input, and outputs a vector of dimension n1. Next, the
network αent takes as input the outputs of φent for all neighbors of an entity, and uses the attention
mechanism described above to output a vector of dimension n2. Finally, the ρent network takes these
n2 dimensional vectors and outputs L logits predicting the entity value. The ρrel network takes as
input the αent representation of the two entities, as well as zi,j and transforms the output into R
logits. See appendix for speciﬁc network architectures.

5.2.1 Experimental Setup and Results

Dataset. We evaluated our approach on Visual Genome (VG) (Krishna et al., 2017), a dataset with
108,077 images annotated with bounding boxes, entities and relations. On average, images have 12
entities and 7 relations per image. For a proper comparison with previous results (Newell & Deng,
2017; Xu et al., 2017; Zellers et al., 2017), we used the data from (Xu et al., 2017), including the
train and test splits. For evaluation, we used the same 150 entities and 50 relations as in (Newell
& Deng, 2017; Xu et al., 2017; Zellers et al., 2017). To tune hyper-parameters, we also split the
training data into two by randomly selecting 5K examples, resulting in a ﬁnal 70K/5K/32K split for
train/validation/test sets.

Training. All networks were trained using Adam (Kingma & Ba, 2014) with batch size 20. Hyper-
parameter values below were chosen based on the validation set. The SGP loss function was the sum
of cross-entropy losses over all entities and relations in the image. In the loss, we penalized entities
4 times more strongly than relations, and penalized negative relations 10 times more weakly than
positive relations.

Evaluation.
In (Xu et al., 2017) three different evaluation settings were considered. Here we
focus on two of these: (1) SGCls: Given ground-truth bounding boxes for entities, predict all entity
categories and relations categories. (2) PredCls: Given bounding boxes annotated with entity labels,
predict all relations. Following (Lu et al., 2016), we used Recall@K as the evaluation metric. It
measures the fraction of correct ground-truth triplets that appear within the K most conﬁdent triplets
proposed by the model. Two evaluation protocols are used in the literature differing in whether they
enforce graph constraints over model predictions. The ﬁrst graph-constrained protocol requires that
the top-K triplets assign one consistent class per entity and relation. The second unconstrained
protocol does not enforce any such constraints. We report results on both protocols, following (Zellers
et al., 2017).

8

Figure 4: (a) An input image with bounding boxes from VG. (b) The ground-truth scene graph. (c) The
Baseline fails to recognize some entities (tail and tree) and relations (in front of instead of looking at). (d)
GPI:LINGUISTIC ﬁxes most incorrect LP predictions. (e) Window is the most signiﬁcant neighbor of Tree. (f)
The entity bird receives substantial attention, while Tree and building are less informative.

Models and baselines. We compare four variants of our GPI approach with the reported results
of four baselines that are currently the state-of-the-art on various scene graph prediction problems
(all models use the same data split and pre-processing as (Xu et al., 2017)): 1) LU ET AL., 2016
(LU ET AL., 2016): This work leverages word embeddings to ﬁne-tune the likelihood of predicted
relations. 2) XU ET AL, 2017 (XU ET AL., 2017): This model passes messages between entities and
relations, and iteratively reﬁnes the feature map used for prediction. 3) NEWELL & DENG, 2017
(NEWELL & DENG, 2017): The PIXEL2GRAPH model uses associative embeddings (Newell et al.,
2017) to produce a full graph from the image. 4) YANG ET AL., 2018 (YANG ET AL., 2018): The
GRAPH R-CNN model uses object-relation regularities to sparsify and reason over scene graphs. 5)
ZELLERS ET AL., 2017 (ZELLERS ET AL., 2017): The NEURALMOTIF method encodes global
context for capturing high-order motifs in scene graphs, and the BASELINE outputs the entities and
relations distributions without using the global context. The following variants of GPI were compared:
1) GPI: NO ATTENTION: Our GPI model, but with no attention mechanism. Instead, following
Theorem 1, we simply sum the features. 2) GPI: NEIGHBORATTENTION: Our GPI model, with
attention over neighbors features. 3) GPI: LINGUISTIC: Same as GPI: NEIGHBORATTENTION but
also concatenating the word embedding vector, as described above.

Results. Table 1 shows recall@50 and recall@100 for three variants of our approach, and compared
with ﬁve baselines. All GPI variants performs well, with LINGUISTIC outperforming all baselines
for SGCls and being comparable to the state-of-the-art model for PredCls. Note that PredCl is an
easier task, which makes less use of the structure, hence it is not surprising that GPI achieves similar
accuracy to Zellers et al. (2017). Figure 4 illustrates the model behavior. Predicting isolated labels
with zi (4c) mislabels several entities, but these are corrected at the ﬁnal output (4d). Figure 4e
shows that the system learned to attend more to nearby entities (the window and building are closer
to the tree), and 4f shows that stronger attention is learned for the class bird, presumably because it is
usually more informative than common classes like tree.

Implementation details. The φ and α networks were each implemented as a single fully-connected
(FC) layer with a 500-dimensional outputs. ρ was implemented as a FC network with 3 500-
dimensional hidden layers, with one 150-dimensional output for the entity probabilities, and one
51-dimensional output for relation probabilities. The attention mechanism was implemented as a
network like to φ and α, receiving the same inputs, but using the output scores for the attention . The
full code is available at https://github.com/shikorab/SceneGraph

6 Conclusion

We presented a deep learning approach to structured prediction, which constrains the architecture
to be invariant to structurally identical inputs. As in score-based methods, our approach relies on
pairwise features, capable of describing inter-label correlations, and thus inheriting the intuitive
aspect of score-based approaches. However, instead of maximizing a score function (which leads
to computationally-hard inference), we directly produce an output that is invariant to equivalent
representations of the pairwise terms.

9

This axiomatic approach to model architecture can be extended in many ways. For image labeling,
geometric invariances (shift or rotation) may be desired.
In other cases, invariance to feature
permutations may be desirable. We leave the derivation of the corresponding architectures to future
work. Finally, there may be cases where the invariant structure is unknown and should be discovered
from data, which is related to work on lifting graphical models Bui et al. (2013). It would be interesting
to explore algorithms that discover and use such symmetries for deep structured prediction.

This work was supported by the ISF Centers of Excellence grant, and by the Yandex Initiative in
Machine Learning. Work by GC was performed while at Google Brain Research.

Acknowledgements

References

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and

translate. In International Conference on Learning Representations (ICLR), 2015.

Belanger, David, Yang, Bishan, and McCallum, Andrew. End-to-end learning for structured prediction
energy networks. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70, pp. 429–439. PMLR, 2017.

Bello, Irwan, Pham, Hieu, Le, Quoc V, Norouzi, Mohammad, and Bengio, Samy. Neural combinato-

rial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Bui, Hung Hai, Huynh, Tuyen N., and Riedel, Sebastian. Automorphism groups of graphical models
and lifted variational inference. In Proceedings of the Twenty-Ninth Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’13, pp. 132–141, 2013.

Chen, Danqi and Manning, Christopher. A fast and accurate dependency parser using neural
networks. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 740–750, 2014.

Chen, Liang Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Se-
mantic image segmentation with deep convolutional nets and fully connected CRFs. In Proceedings
of the Second International Conference on Learning Representations, 2014.

Chen, Liang Chieh, Schwing, Alexander G, Yuille, Alan L, and Urtasun, Raquel. Learning deep

structured models. In Proc. ICML, 2015.

Farabet, Clement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical
features for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):
1915–1929, 2013.

Gilmer, Justin, Schoenholz, Samuel S, Riley, Patrick F, Vinyals, Oriol, and Dahl, George E. Neural

message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.

Gygli, Michael, Norouzi, Mohammad, and Angelova, Anelia. Deep value networks learn to evaluate
and iteratively reﬁne structured outputs. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine
Learning Research, pp. 1341–1351, International Convention Centre, Sydney, Australia, 2017.
PMLR.

Johnson, Justin, Krishna, Ranjay, Stark, Michael, Li, Li-Jia, Shamma, David A., Bernstein, Michael S.,
In Proc. Conf. Comput. Vision Pattern

Image retrieval using scene graphs.

and Li, Fei-Fei.
Recognition, pp. 3668–3678, 2015.

Johnson, Justin, Gupta, Agrim, and Fei-Fei, Li. Image generation from scene graphs. arXiv preprint

arXiv:1804.01622, 2018.

Khalil, Elias, Dai, Hanjun, Zhang, Yuyu, Dilkina, Bistra, and Song, Le. Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp.
6351–6361, 2017.

10

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint

arXiv: 1412.6980, abs/1412.6980, 2014.

Krishna, Ranjay, Zhu, Yuke, Groth, Oliver, Johnson, Justin, Hata, Kenji, Kravitz, Joshua, Chen,
Stephanie, Kalantidis, Yannis, Li, Li-Jia, Shamma, David A, et al. Visual genome: Connecting
International Journal of
language and vision using crowdsourced dense image annotations.
Computer Vision, 123(1):32–73, 2017.

Lafferty, J., McCallum, A., and Pereira, F. Conditional random ﬁelds: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning, pp. 282–289, 2001.

Liao, Wentong, Yang, Michael Ying, Ackermann, Hanno, and Rosenhahn, Bodo. On support relations

and semantic scene graphs. arXiv preprint arXiv:1609.05834, 2016.

Lin, Guosheng, Shen, Chunhua, Reid, Ian, and van den Hengel, Anton. Deeply learning the messages
in message passing inference. In Advances in Neural Information Processing Systems, pp. 361–369,
2015.

Lu, Cewu, Krishna, Ranjay, Bernstein, Michael S., and Li, Fei-Fei. Visual relationship detection with

language priors. In European Conf. Comput. Vision, pp. 852–869, 2016.

Meshi, O., Sontag, D., Jaakkola, T., and Globerson, A. Learning efﬁciently with approximate
In Proceedings of the 27th International Conference on Machine

inference via dual losses.
Learning, pp. 783–790, New York, NY, USA, 2010. ACM.

Newell, Alejandro and Deng, Jia. Pixels to graphs by associative embedding. In Advances in Neural
Information Processing Systems 30 (to appear), pp. 1172–1180. Curran Associates, Inc., 2017.

Newell, Alejandro, Huang, Zhiao, and Deng, Jia. Associative embedding: End-to-end learning for
joint detection and grouping. In Neural Inform. Process. Syst., pp. 2274–2284. Curran Associates,
Inc., 2017.

Pei, Wenzhe, Ge, Tao, and Chang, Baobao. An effective neural network model for graph-based de-
pendency parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computationa
Linguistics, pp. 313–322, 2015.

Plummer, Bryan A., Mallya, Arun, Cervantes, Christopher M., Hockenmaier, Julia, and Lazebnik,
Svetlana. Phrase localization and visual relationship detection with comprehensive image-language
cues. In ICCV, pp. 1946–1955, 2017.

Raposo, David, Santoro, Adam, Barrett, David, Pascanu, Razvan, Lillicrap, Timothy, and Battaglia,
Peter. Discovering objects and their relations from entangled scene representations. arXiv preprint
arXiv:1702.05068, 2017.

Schwing, Alexander G and Urtasun, Raquel. Fully connected deep structured networks. ArXiv

e-prints, 2015.

Shelhamer, Evan, Long, Jonathan, and Darrell, Trevor. Fully convolutional networks for semantic

segmentation. Proc. Conf. Comput. Vision Pattern Recognition, 39(4):640–651, 2017.

Taskar, B., Guestrin, C., and Koller, D. Max margin Markov networks. In Thrun, S., Saul, L., and
Schölkopf, B. (eds.), Advances in Neural Information Processing Systems 16, pp. 25–32. MIT
Press, Cambridge, MA, 2004.

Xu, Danfei, Zhu, Yuke, Choy, Christopher B., and Fei-Fei, Li. Scene Graph Generation by Iterative
Message Passing. In Proc. Conf. Comput. Vision Pattern Recognition, pp. 3097–3106, 2017.

Yang, Jianwei, Lu, Jiasen, Lee, Stefan, Batra, Dhruv, and Parikh, Devi. Graph R-CNN for scene

graph generation. In European Conf. Comput. Vision, pp. 690–706, 2018.

Zaheer, Manzil, Kottur, Satwik, Ravanbakhsh, Siamak, Poczos, Barnabas, Salakhutdinov, Ruslan R,
and Smola, Alexander J. Deep sets. In Advances in Neural Information Processing Systems 30, pp.
3394–3404. Curran Associates, Inc., 2017.

11

Zellers, Rowan, Yatskar, Mark, Thomson, Sam, and Choi, Yejin. Neural motifs: Scene graph parsing

with global context. arXiv preprint arXiv:1711.06640, abs/1711.06640, 2017.

Zheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav, Su, Zhizhong, Du,
Dalong, Huang, Chang, and Torr, Philip HS. Conditional random ﬁelds as recurrent neural networks.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 1529–1537, 2015.

7 Supplementary Material

This supplementary material includes: (1) Visual illustration of the proof of Theorem 1. (2) Explaining
how to integrate an attention mechanism in our GPI framework. (3) Additional evaluation method to
further analyze and compare our work with baselines.

7.1 Theorem 1: Illustration of Proof

Figure 5: Illustration of the proof of Theorem 1 using a speciﬁc construction example. Here H is a hash function
of size L = 5 such that H(1) = 1, H(3) = 2, H(2) = 4, G is a three-node input graph, and zi,j ∈ R are the
pairwise features (in purple) of G. (a) φ is applied to each zi,j. Each application yields a vector in R5. The three
dark yellow columns correspond to φ(z1,1), φ(z1,2) and φ(z1,3). Then, all vectors φ(zi,j) are summed over j
to obtain three si vectors. (b) α’s (blue matrices) are an outer product between 1 [H(zi)] and si resulting in a
matrix of zeros except one row. The dark blue matrix corresponds for α(z1, s1). (c) All α’s are summed to a
5 × 5 matrix, isomorphic to the original zi,j matrix.

7.2 Characterizing Permutation Invariance: Attention

Attention is a powerful component which naturally can be introduced into our GPI model. We now
show how attention can be introduced in our framework. Formally, we learn attention weights for the
neighbors j of a node i, which scale the features zi,j of that neighbor. We can also learn different
attention weights for individual features of each neighbor in a similar way.

Let wi,j ∈ R be an attention mask specifying the weight that node i gives to node j:

wi,j(zi, zi,j, zj) = eβ(zi,zi,j ,zj )/

eβ(zi,zi,t,zt)

(4)

(cid:88)

t

where β can be any scalar-valued function of its arguments (e.g., a dot product of zi and zj as in
standard attention models). To introduce attention we wish α ∈ Re to have the form of weighting
wi,j over neighboring feature vectors zi,j, namely, α = (cid:80)
To achieve this form we extend φ by a single entry, deﬁning φ ∈ Re+1 (namely we set
L = e + 1) as φ1:e(zi, zi,j, zj) = eβ(zi,zi,j ,zj )zi,j (here φ1:e are the ﬁrst e elements of φ)

j(cid:54)=i wi,jzi,j.

12

and φe+1(zi, zi,j; zj) = eβ(zi,zi,j ,zj ). We keep the deﬁnition of si = (cid:80)
we deﬁne α = si,1:e
si,e+1
over neighboring feature vectors zi,j:

j(cid:54)=i φ(zi, zi,j, zj). Next,
and substitute si and φ to obtain the desired form as attention weights wi,j

α(zi, si) =

si,1:e
si,e+1

(cid:88)

=

j(cid:54)=i

eβ(zi,zi,j ,zj )zi,j
(cid:80)
j(cid:54)=i eβ(zi,zi,j ,zj )

(cid:88)

=

j(cid:54)=i

wi,jzi,j

A similar approach can be applied over α and ρ to model attention over the outputs of α as well
(graph nodes).

7.3 Scene Graph Results

In the main paper, we described the results for the two prediction tasks: SGCls and PredCls, as
deﬁned in section 5.2.1: "Experimental Setup and Results". To further analyze our module, we
compare the best variant, GPI: LINGUISTIC, per relation to two baselines: (Lu et al., 2016) and Xu
et al. (2017). Table 2, speciﬁes the PredCls recall@5 of the 20-top frequent relation classes. The GPI
module performs better in almost all the relations classes.

RELATION

(LU ET AL., 2016)

(XU ET AL., 2017)

LINGUISTIC

99.71
98.03
80.38
82.47
98.47
85.16
31.85
49.19
61.50
79.35
28.64
31.74
26.09
8.45
54.08

ON
HAS
IN
OF
WEARING
NEAR
WITH
ABOVE
HOLDING
BEHIND
UNDER
SITTING ON
IN FRONT OF
ATTACHED TO
AT
HANGING FROM 0.0
OVER
FOR
RIDING

9.26
12.20
72.43

99.25
97.25
88.30
96.75
98.23
96.81
88.10
79.73
80.67
92.32
52.73
50.17
59.63
29.58
70.41
0.0
0.0
31.71
89.72

99.3
98.7
95.9
98.1
99.6
95.4
94.2
83.9
95.5
91.2
83.2
90.4
74.9
77.4
80.9
74.1
62.4
45.1
96.1

Table 2: Recall@5 of PredCls for the 20-top relations ranked by their frequency, as in (Xu et al., 2017)

13

8
1
0
2
 
v
o
N
 
1
 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
1
5
4
5
0
.
2
0
8
1
:
v
i
X
r
a

Mapping Images to Scene Graphs with
Permutation-Invariant Structured Prediction

Roei Herzig∗
Tel Aviv University
roeiherzig@mail.tau.ac.il

Moshiko Raboh∗
Tel Aviv University
mosheraboh@mail.tau.ac.il

Gal Chechik
Bar-Ilan University, NVIDIA Research
gal.chechik@biu.ac.il

Jonathan Berant
Tel Aviv University, AI2
joberant@cs.tau.ac.il

Amir Globerson
Tel Aviv University
gamir@post.tau.ac.il

Abstract

Machine understanding of complex images is a key goal of artiﬁcial intelligence.
One challenge underlying this task is that visual scenes contain multiple inter-
related objects, and that global context plays an important role in interpreting
the scene. A natural modeling framework for capturing such effects is structured
prediction, which optimizes over complex labels, while modeling within-label
interactions. However, it is unclear what principles should guide the design of a
structured prediction model that utilizes the power of deep learning components.
Here we propose a design principle for such architectures that follows from a
natural requirement of permutation invariance. We prove a necessary and sufﬁ-
cient characterization for architectures that follow this invariance, and discuss its
implication on model design. Finally, we show that the resulting model achieves
new state-of-the-art results on the Visual Genome scene-graph labeling benchmark,
outperforming all recent approaches.

1

Introduction

Understanding the semantics of a complex visual scene is a fundamental problem in machine
perception. It often requires recognizing multiple objects in a scene, together with their spatial and
functional relations. The set of objects and relations is sometimes represented as a graph, connecting
objects (nodes) with their relations (edges) and is known as a scene graph (Figure 1). Scene graphs
provide a compact representation of the semantics of an image, and can be useful for semantic-level
interpretation and reasoning about a visual scene Johnson et al. (2018). Scene-graph prediction is the
problem of inferring the joint set of objects and their relations in a visual scene.

Since objects and relations are inter-dependent (e.g., a person and chair are more likely to be in relation
“sitting on” than “eating”), a scene graph predictor should capture this dependence in order to improve
prediction accuracy. This goal is a special case of a more general problem, namely, inferring multiple
inter-dependent labels, which is the research focus of the ﬁeld of structured prediction. Structured
prediction has attracted considerable attention because it applies to many learning problems and

∗Equal Contribution.

32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.

Figure 1: An image and its scene graph from the Visual Genome dataset (Krishna et al., 2017). The scene graph
captures the entities in the image (nodes, blue circles) like dog and their relations (edges, red circles) like (cid:10)hat,
on, dog(cid:11).

poses unique theoretical and algorithmic challenges (e.g., see Belanger et al., 2017; Chen et al., 2015;
Taskar et al., 2004). It is therefore a natural approach for predicting scene graphs from images.

Structured prediction models typically deﬁne a score function s(x, y) that quantiﬁes how well a
label assignment y is compatible with an input x. In the case of understanding complex visual
scenes, x is an image, and y is a complex label containing the labels of objects detected in an image
and the labels of their relations. In this setup, the inference task amounts to ﬁnding the label that
maximizes the compatibility score y∗ = arg maxy s(x, y). This score-based approach separates a
scoring component – implemented by a parametric model, from an optimization component – aimed
at ﬁnding a label that maximizes that score. Unfortunately, for a general scoring function s(·), the
space of possible label assignments grows exponentially with input size. For instance, for scene
graphs the set of possible object label assignments is too large even for relatively simple images,
since the vocabulary of candidate objects may contain thousands of objects. As a result, inferring the
label assignment that maximizes a scoring function is computationally hard in the general case.

An alternative approach to score-based methods is to map an input x to a structured output y with
a “black box" neural network, without explicitly deﬁning a score function. This raises a natural
question: what is the right architecture for such a network? Here we take an axiomatic approach and
argue that one important property such networks should satisfy is invariance to a particular type of
input permutation. We then prove that this invariance is equivalent to imposing certain structural
constraints on the architecture of the network, and describe architectures that satisfy these constraints.

To evaluate our approach, we ﬁrst demonstrate on a synthetic dataset that respecting permutation
invariance is important, because models that violate this invariance need more training data, despite
having a comparable model size. Then, we tackle the problem of scene graph generation. We describe
a model that satisﬁes the permutation invariance property, and show that it achieves state-of-the-art
results on the competitive Visual Genome benchmark (Krishna et al., 2017), demonstrating the power
of our new design principle.

In summary, the novel contributions of this paper are: a) Deriving sufﬁcient and necessary conditions
for graph-permutation invariance in deep structured prediction architectures. b) Empirically demon-
strating the beneﬁt of graph-permutation invariance. c) Developing a state-of-the-art model for scene
graph prediction on a large dataset of complex visual scenes.

2 Structured Prediction

Scored-based methods in structured prediction deﬁne a function s(x, y) that quantiﬁes the degree
to which y is compatible with x, and infer a label by maximizing s(x, y) (e.g., see Belanger et al.,
2017; Chen et al., 2015; Lafferty et al., 2001; Meshi et al., 2010; Taskar et al., 2004). Most score
functions previously used decompose as a sum over simpler functions, s(x, y) = (cid:80)
i fi(x, y), making
it possible to optimize maxy fi(x, y) efﬁciently. This local maximization forms the basic building
block of algorithms for approximately maximizing s(x, y). One way to decompose the score function
is to restrict each fi(x, y) to depend only on a small subset of the y variables.

The renewed interest in deep learning led to efforts to integrate deep networks with structured predic-
tion, including modeling the fi functions as deep networks. In this context, the most widely-used
score functions are singleton fi(yi, x) and pairwise fij(yi, yj, x). The early work taking this approach
used a two-stage architecture, learning the local scores independently of the structured prediction

2

Figure 2: Left: Graph permutation invariance. A graph labeling function F is graph permutation invariant
(GPI) if permuting the node features maintains the output. Right: a schematic representation of the GPI
architecture in Theorem 1. Singleton features zi are omitted for simplicity. (a) First, the features zi,j are
processed element-wise by φ. (b) Features are summed to create a vector si, which is concatenated with zi. (c)
A representation of the entire graph is created by applying α n times and summing the created vector. (d) The
graph representation is then ﬁnally processed by ρ together with zk.

goal (Chen et al., 2014; Farabet et al., 2013). Later studies considered end-to-end architectures where
the inference algorithm is part of the computation graph (Chen et al., 2015; Pei et al., 2015; Schwing
& Urtasun, 2015; Zheng et al., 2015). Recent studies go beyond pairwise scores, also modelling
global factors (Belanger et al., 2017; Gygli et al., 2017).

Score-based methods provide several advantages. First, they allow intuitive speciﬁcation of local
dependencies between labels and how these translate to global dependencies. Second, for linear
score functions, the learning problem has natural convex surrogates Lafferty et al. (2001); Taskar
et al. (2004). Third, inference in large label spaces is sometimes possible via exact algorithms or
empirically accurate approximations. However, with the advent of deep scoring functions s(x, y; w),
learning is no longer convex. Thus, it is worthwhile to rethink the architecture of structured prediction
models, and consider models that map inputs x to outputs y directly without explicitly maximizing a
score function. We would like these models to enjoy the expressivity and predictive power of neural
networks, while maintaining the ability to specify local dependencies between labels in a ﬂexible
manner. In the next section, we present such an approach and consider a natural question: what
should be the properties of a deep neural network used for structured prediction.

3 Permutation-Invariant Structured Prediction

In what follows we deﬁne the permutation-invariance property for structured prediction models, and
argue that permutation invariance is a natural principle for designing their architecture.

We ﬁrst introduce our notation. We focus on structures with pairwise interactions, because they are
simpler in terms of notation and are sufﬁcient for describing the structure in many problems. We
denote a structured label by y = [y1, . . . , yn]. In a score-based approach, the score is deﬁned via a
set of singleton scores fi(yi, x) and pairwise scores fij(yi, yj, x), where the overall score s(x, y) is
the sum of these scores. For brevity, we denote fij = fij(yi, yj, x) and fi = fi(yi, x). An inference
algorithm takes as input the local scores fi, fij and outputs an assignment that maximizes s(x, y).
We can thus view inference as a black-box that takes node-dependent and edge-dependent inputs
(i.e., the scores fi, fij) and returns a label y, even without an explicit score function s(x, y). While
numerous inference algorithms exist for this setup, including belief propagation (BP) and mean ﬁeld,
here we develop a framework for a deep labeling algorithm (we avoid the term “inference” since the
algorithm does not explicitly maximize a score function). Such an algorithm will be a black-box,
taking the f functions as input and the labels y1, . . . , yn as output. We next ask what architecture
such an algorithm should have.

We follow with several deﬁnitions. A graph labeling function F : (V, E) → Y is a function whose
input is an ordered set of node features V = [z1, . . . , zn] and an ordered set of edge features
E = [z1,2 . . . , zi,j, . . . , zn,n−1]. For example, zi can be the array of values fi, and zi,j can be
the table of values fi,j. Assume zi ∈ Rd and zi,j ∈ Re. The output of F is a set of node labels
y = [y1, . . . , yn]. Thus, algorithms such as BP are graph labeling functions. However, graph labeling
functions do not necessarily maximize a score function. We denote the joint set of node features and
edge features by z (i.e., a set of n + n(n − 1) = n2 vectors). In Section 3.1 we discuss extensions to
this case where only a subset of the edges is available.

3

1, y∗

2, y∗

1, y∗

2, y∗

A natural requirement is that the function F produces the same result when given the same features,
up to a permutation of the input. For example, consider a label space with three variables y1, y2, y3,
and assume that F takes as input z = (z1, z2, z3, z12, z13, z23) = (f1, f2, f3, f12, f13, f23), and
outputs a label y = (y∗
3). When F is given an input that is permuted in a consistent way, say,
z(cid:48) = (f2, f1, f3, f21, f23, f13), this deﬁnes exactly the same input. Hence, the output should still be
y = (y∗
3). Most inference algorithms, including BP and mean ﬁeld, satisfy this symmetry
requirement by design, but this property is not guaranteed in general in a deep model. Here, our
goal is to design a deep learning black-box, and hence we wish to guarantee invariance to input
permutations. A black-box that violates this invariance “wastes” capacity on learning it at training
time, which increases sample complexity, as shown in Sec. 5.1. We proceed to formally deﬁne the
permutation invariance property.
Deﬁnition 1. Let z be a set of node features and edge features, and let σ be a permutation of
{1, . . . , n}. We deﬁne σ(z) to be a new set of node and edge features given by [σ(z)]i = zσ(i) and
[σ(z)]i,j = zσ(i),σ(j).

We also use the notation σ([y1, . . . , yn]) = [yσ(1), . . . , yσ(n)] for permuting the labels. Namely, σ
applied to a set of labels yields the same labels, only permuted by σ. Be aware that applying σ to
the input features is different from permuting labels, because edge input features must permuted in a
way that is consistent with permuting node input features. We now provide our key deﬁnition of a
function whose output is invariant to permutations of the input. See Figure 2 (left).
Deﬁnition 2. A graph labeling function F is said to be graph-permutation invariant (GPI), if for
all permutations σ of {1, . . . , n} and for all z it satisﬁes: F(σ(z)) = σ(F(z)).

3.1 Characterizing Permutation Invariance

Motivated by the above discussion, we ask: what structure is necessary and sufﬁcient to guarantee
that F is GPI? Note that a function F takes as input an ordered set z. Therefore its output on z
could certainly differ from its output on σ(z). To achieve permutation invariance, F should contain
certain symmetries. For instance, one permutation invariant architecture could be to deﬁne yi = g(zi)
for any function g, but this architecture is too restrictive and does not cover all permutation invariant
functions. Theorem 1 below provides a complete characterization (see Figure 2 for the corresponding
architecture). Intuitively, the architecture in Theorem 1 is such that it can aggregate information from
the entire graph, and do so in a permutation invariant manner.
Theorem 1. Let F be a graph labeling function. Then F is graph-permutation invariant if and only
if there exist functions α, ρ, φ such that for all k = 1, . . . , n:

[F(z)]k = ρ

zk,

α

zi,

φ(zi, zi,j, zj)



 ,

(1)









n
(cid:88)

i=1

(cid:88)

j(cid:54)=i

where φ : R2d+e → RL, α : Rd+L → RW and ρ : RW +d → R.

Proof. First, we show that any F satisfying the conditions of Theorem 1 is GPI. Namely, for any
permutation σ, [F(σ(z))]k = [F(z)]σ(k). To see this, write [F(σ(z))]k using Eq. 1 and Deﬁnition 1:

[F(σ(z))]k = ρ(zσ(k),

α(zσ(i),

φ(zσ(i), zσ(i),σ(j), zσ(j)))).

(2)

(cid:88)

i

(cid:88)

j(cid:54)=i

The second argument of ρ above is invariant under σ, because it is a sum over nodes and their
neighbors, which is invariant under permutation. Thus Eq. 2 is equal to:

ρ(zσ(k),

α(zi,

φ(zi, zi,j, zj))) = [F(z)]σ(k)

(cid:88)

i

(cid:88)

j(cid:54)=i

where equality follows from Eq. 1. We thus proved that Eq. 1 implies graph permutation invariance.

Next, we prove that any given GPI function F0 can be expressed as a function F in Eq. 1. Namely,
we show how to deﬁne φ, α and ρ that can implement F0. Note that in this direction of the proof the
function F0 is a black-box. Namely, we only know that it is GPI, but do not assume anything else
about its implementation.

4

The key idea is to construct φ, α such that the second argument of ρ in Eq. 1 contains the information
about all the graph features z. Then, the function ρ corresponds to an application of F0 to this
representation, followed by extracting the label yk. To simplify notation assume edge features are
scalar (e = 1). The extension to vectors is simple, but involves more indexing.

We assume WLOG that the black-box function F0 is a function only of the pairwise features zi,j
(otherwise, we can always augment the pairwise features with the singleton features). Since zi,j ∈ R
we use a matrix Rn,n to denote all the pairwise features.

Finally, we assume that our implementation of F0 will take additional node features zk such that no
two nodes have the same feature (i.e., the features identify the node).

Our goal is thus to show that there exist functions α, φ, ρ such that the function in Eq. 2 applied to Z
yields the same labels as F0(Z).

Let H be a hash function with L buckets mapping node features zi to an index (bucket). Assume
that H is perfect (this can be achieved for a large enough L). Deﬁne φ to map the pairwise
features to a vector of size L. Let 1 [j] be a one-hot vector of dimension RL, with one in the
jth coordinate. Recall that we consider scalar zi,j so that φ is indeed in RL, and deﬁne φ as:
φ(zi, zi,j, zj) = 1 [H(zj)] zi,j, i.e., φ “stores” zi,j in the unique bucket for node j.
Let si = (cid:80)
zi,j ∈E φ(zi, zi,j, zj) be the second argument of α in Eq. 1 (si ∈ RL). Then, since all
zj are distinct, si stores all the pairwise features for neighbors of i in unique positions within its
L coordinates. Since si(H(zk)) contains the feature zi,k whereas sj(H(zk)) contains the feature
zj,k, we cannot simply sum the si, since we would lose the information of which edges the features
originated from. Instead, we deﬁne α to map si to RL×L such that each feature is mapped to a
distinct location. Formally:

α(zi, si) = 1 [H(zi)] sT
i
α outputs a matrix that is all zeros except for the features corresponding to node i that are stored in
row H(zi). The matrix M = (cid:80)
i α(zi, si) (namely, the second argument of ρ in Eq. 1) is a matrix
with all the edge features in the graph including the graph structure.

(3)

.

To complete the construction we set ρ to have the same outcome as F0. We ﬁrst discard rows and
columns in M that do not correspond to original nodes (reducing M to dimension n × n). Then, we
use the reduced matrix as the input z to the black-box F0.

Assume for simplicity that M does not need to be contracted (this merely introduces another indexing
step). Then M corresponds to the original matrix Z of pairwise features, with both rows and
columns permuted according to H. We will thus use M as input to the function F0. Since F0 is
GPI, this means that the label for node k will be given by F0(M ) in position H(zk). Thus we set
ρ(zk, M ) = [F0(M )]H(zk), and by the argument above this equals [F0(Z)]k, implying that the
above α, φ and ρ indeed implement F0.

Extension to general graphs So far, we discussed complete graphs, where edges correspond
to valid feature pairs. However, many graphs of interest might be incomplete. For example, an
n-variable chain graph in sequence labeling has only n − 1 edges. For such graphs, the input to F
would not contain all zi,j pairs but rather only features corresponding to valid edges of the graph, and
we are only interested in invariances that preserve the graph structure, namely, the automorphisms
of the graph. Thus, the desired invariance is that σ(F(z)) = F(σ(z)), where σ is not an arbitrary
permutation but an automorphism. It is easy to see that a simple variant of Theorem 1 holds in
this case. All we need to do is replace in Eq. 2 the sum (cid:80)
j∈N (i), where N (i) are the
neighbors of node i in the graph. The arguments are then similar to the proof above.

j(cid:54)=i with (cid:80)

Implications of Theorem 1 Our result has interesting implications for deep structured prediction.
First, it highlights that the fact that the architecture “collects” information from all different edges
of the graph, in an invariant fashion via the α, φ functions. Speciﬁcally, the functions φ (after
summation) aggregate all the features around a given node, and then α (after summation) can
collect them. Thus, these functions can provide a summary of the entire graph that is sufﬁcient for
downstream algorithms. This is different from one round of message passing algorithms which would
not be sufﬁcient for collecting global graph information. Note that the dimensions of φ, α may need
to be large to aggregate all graph information (e.g., by hashing all the features as in the proof of
Theorem 1), but the architecture itself can be shallow.

5

Second, the architecture is parallelizable, as all φ functions can be applied simultaneously. This is in
contrast to recurrent models Zellers et al. (2017) which are harder to parallelize and are thus slower
in practice.

Finally, the theorem suggests several common architectural structures that can be used within GPI.
We brieﬂy mention two of these. 1) Attention: Attention is a powerful component in deep learning
architectures (Bahdanau et al., 2015), but most inference algorithms do not use attention. Intuitively,
in attention each node i aggregates features of neighbors through a weighted sum, where the weight
is a function of the neighbor’s relevance. For example, the label of an entity in an image may depend
more strongly on entities that are spatially closer. Attention can be naturally implemented in our
GPI characterization, and we provide a full derivation for this implementation in the appendix. It
plays a key role in our scene graph model described below. 2) RNNs: Because GPI functions are
closed under composition, for any GPI function F we can run F iteratively by providing the output
of one step of F as part of the input to the next step and maintain GPI. This results in a recurrent
architecture, which we use in our scene graph model.

4 Related Work

The concept of architectural invariance was recently proposed in DEEPSETS (Zaheer et al., 2017).
The invariance we consider is much less restrictive: the architecture does not need to be invariant
to all permutations of singleton and pairwise features, just those consistent with a graph re-labeling.
This characterization results in a substantially different set of possible architectures.

Deep structured prediction. There has been signiﬁcant recent interest in extending deep learning
to structured prediction tasks. Much of this work has been on semantic segmentation, where
convolutional networks (Shelhamer et al., 2017) became a standard approach for obtaining “singleton
scores” and various approaches were proposed for adding structure on top. Most of these approaches
used variants of message passing algorithms, unrolled into a computation graph (Xu et al., 2017).
Some studies parameterized parts of the message passing algorithm and learned its parameters (Lin
et al., 2015). Recently, gradient descent has also been used for maximizing score functions (Belanger
et al., 2017; Gygli et al., 2017). An alternative to deep structured prediction is greedy decoding,
inferring each label at a time based on previous labels. This approach has been popular in sequence-
based applications (e.g., parsing (Chen & Manning, 2014)), relying on the sequential structure of the
input, where BiLSTMs are effectively applied. Another related line of work is applying deep learning
to graph-based problems, such as TSP (Bello et al., 2016; Gilmer et al., 2017; Khalil et al., 2017).
Clearly, the notion of graph invariance is important in these, as highlighted in (Gilmer et al., 2017).
They however do not specify a general architecture that satisﬁes invariance as we do here, and in
fact focus on message passing architectures, which we strictly generalize. Furthermore, our focus is
on the more general problem of structured prediction, rather than speciﬁc graph-based optimization
problems.

Scene graph prediction. Extracting scene graphs from images provides a semantic representation
that can later be used for reasoning, question answering, and image retrieval (Johnson et al., 2015;
Lu et al., 2016; Raposo et al., 2017). It is at the forefront of machine vision research, integrating
challenges like object detection, action recognition and detection of human-object interactions (Liao
et al., 2016; Plummer et al., 2017). Prior work on scene graph predictions used neural message
passing algorithms (Xu et al., 2017) as well as prior knowledge in the form of word embeddings
(Lu et al., 2016). Other work suggested to predict graphs directly from pixels in an end-to-end
manner Newell & Deng (2017). NeuralMotif (Zellers et al., 2017), currently the state-of-the-art
model for scene graph prediction on Visual Genome, employs an RNN that provides global context
by sequentially reading the independent predictions for each entity and relation and then reﬁnes those
predictions. The NEURALMOTIF model maintains GPI by ﬁxing the order in which the RNN reads
its inputs and thus only a single order is allowed. However, this ﬁxed order is not guaranteed to be
optimal.

5 Experimental Evaluation

We empirically evaluate the beneﬁt of GPI architectures. First, using a synthetic graph-labeling task,
and then for the problem of mapping images to scene graphs.

6

Figure 3: Accuracy as a function of sample size for graph labeling. Right is a zoomed in version of left.

5.1 Synthetic Graph Labeling

We start with studying GPI on a synthetic problem, deﬁned as follows. An input graph G = (V, E)
is given, where each node i ∈ V is assigned to one of K sets. The set for node i is denoted by
Γ(i). The goal is to compute for each node the number of neighbors that belong to the same set.
Namely, the label of a node is yi = (cid:80)
j∈N (i) 1[Γ(i) = Γ(j)]. We generated random graphs with 10
nodes (larger graphs produced similar results) by sampling each edge independently and uniformly,
and sampling Γ(i) for every node uniformly from {1, . . . , K}. The node features zi ∈ {0, 1}K are
one-hot vectors of Γ(i) and the edge features zi,j ∈ {0, 1} indicate whether ij ∈ E. We compare two
standard non-GPI architectures and one GPI architecture: (a) A GPI-architecture for graph prediction,
described in detail in Section 5.2. We used the basic version without attention and RNN. (b) LSTM:
We replace (cid:80) φ(·) and (cid:80) α(·), which perform aggregation in Theorem 1 with two LSTMs with
a state size of 200 that read their input in random order. (c) A fully-connected (FC) feed-forward
network with 2 hidden layers of 1000 nodes each. The input to the fully connected model is a
concatenation of all node and pairwise features. The output is all node predictions. The focus of the
experiment is to study sample complexity. Therefore, for a fair comparison, we use the same number
of parameters for all models.

Figure 3, shows the results, demonstrating that GPI requires far fewer samples to converge to the
correct solution. This illustrates the advantage of an architecture with the correct inductive bias for
the problem.

5.2 Scene-Graph Classiﬁcation

We evaluate the GPI approach on the motivating task of this paper, inferring scene graphs from
images (Figure 1). In this problem, the input is an image annotated with a set of bounding boxes for
the entities in the image.2 The goal is to label each bounding box with the correct entity category and
every pair of entities with their relation, such that they form a coherent scene graph.

We begin by describing our Scene Graph Predictor (SGP) model. We aim to predict two types of
variables. The ﬁrst is entity variables [y1, . . . , yn] for all bounding boxes. Each yi can take one of
L values (e.g., “dog”, “man”). The second is relation variables [yn+1, . . . , yn2] for every pair of
bounding boxes. Each such yj can take one of R values (e.g., “on”, “near”). Our graph connects
variables that are expected to be inter-related. It contains two types of edges: 1) entity-entity edge
connecting every two entity variables (yi and yj for 1 ≤ i (cid:54)= j ≤ n. 2) entity-relation edges
connecting every relation variable yk (where k > n) to its two entity variables. Thus, our graph is not
a complete graph and our goal is to design an architecture that will be invariant to any automorphism
of the graph, such as permutations of the entity variables.

For the input features z, we used the features learned by the baseline model from Zellers et al.
(2017).3 Speciﬁcally, the entity features zi included (1) The conﬁdence probabilities of all entities
for yi as learned by the baseline model. (2) Bounding box information given as (left, bottom,
width, height); (3) The number of smaller entities (also bigger); (4) The number of entities to
the left, right, above and below. (5) The number of entities with higher and with lower conﬁdence;

2For simplicity, we focus on the task where boxes are given.
3The baseline does not use any LSTM or context, and is thus unrelated to the main contribution of Zellers

et al. (2017).

7

Constrained Evaluation

SGCls

PredCls

Unconstrained Evaluation
SGCls

PredCls

R@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100

Lu et al., 2016 (Lu et al., 2016)
Xu et al., 2017 (Xu et al., 2017)
Pixel2Graph (Newell & Deng, 2017)
Graph R-CNN (Yang et al., 2018)
Neural Motifs (Zellers et al., 2017)
Baseline (Zellers et al., 2017)
No Attention
Neighbor Attention
Linguistic

11.8
21.7
-
29.6
35.8
34.6
35.3
35.7
36.5

14.1
24.4
-
31.6
36.5
35.3
37.2
38.5
38.8

35.0
44.8
-
54.2
65.2
63.7
64.5
64.6
65.1

27.9
53.0
-
59.1
67.1
65.6
66.3
66.6
66.9

-
-
26.5
-
44.5
43.4
44.1
44.7
45.5

-
-
30.0
-
47.7
46.6
48.5
49.9
50.8

-
-
68.0
-
81.1
78.8
79.7
80.0
80.8

-
-
75.2
-
88.3
85.9
86.7
87.1
88.2

Table 1: Test set results for graph-constrained evaluation (i.e., the returned triplets must be consistent with a
scene graph) and for unconstrained evaluation (triplets need not be consistent with a scene graph).

(6) For the linguistic model only: word embedding of the most probable class. Word vectors were
learned with GLOVE from the ground-truth captions of Visual Genome.
Similarly, the relation features zj ∈ RR contained the probabilities of relation entities for the relation
j. For the Linguistic model, these features were extended to include word embedding of the most
probable class. For entity-entity pairwise features zi,j, we use the relation probability for each pair.
Because the output of SGP are probability distributions over entities and relations, we use them as an
the input z to SGP, once again in a recurrent manner and maintain GPI.

We next describe the main components of the GPI architecture. First, we focus on the parts that
output the entity labels. φent is the network that integrates features for two entity variables yi
and yj. It simply takes zi, zj and zi,j as input, and outputs a vector of dimension n1. Next, the
network αent takes as input the outputs of φent for all neighbors of an entity, and uses the attention
mechanism described above to output a vector of dimension n2. Finally, the ρent network takes these
n2 dimensional vectors and outputs L logits predicting the entity value. The ρrel network takes as
input the αent representation of the two entities, as well as zi,j and transforms the output into R
logits. See appendix for speciﬁc network architectures.

5.2.1 Experimental Setup and Results

Dataset. We evaluated our approach on Visual Genome (VG) (Krishna et al., 2017), a dataset with
108,077 images annotated with bounding boxes, entities and relations. On average, images have 12
entities and 7 relations per image. For a proper comparison with previous results (Newell & Deng,
2017; Xu et al., 2017; Zellers et al., 2017), we used the data from (Xu et al., 2017), including the
train and test splits. For evaluation, we used the same 150 entities and 50 relations as in (Newell
& Deng, 2017; Xu et al., 2017; Zellers et al., 2017). To tune hyper-parameters, we also split the
training data into two by randomly selecting 5K examples, resulting in a ﬁnal 70K/5K/32K split for
train/validation/test sets.

Training. All networks were trained using Adam (Kingma & Ba, 2014) with batch size 20. Hyper-
parameter values below were chosen based on the validation set. The SGP loss function was the sum
of cross-entropy losses over all entities and relations in the image. In the loss, we penalized entities
4 times more strongly than relations, and penalized negative relations 10 times more weakly than
positive relations.

Evaluation.
In (Xu et al., 2017) three different evaluation settings were considered. Here we
focus on two of these: (1) SGCls: Given ground-truth bounding boxes for entities, predict all entity
categories and relations categories. (2) PredCls: Given bounding boxes annotated with entity labels,
predict all relations. Following (Lu et al., 2016), we used Recall@K as the evaluation metric. It
measures the fraction of correct ground-truth triplets that appear within the K most conﬁdent triplets
proposed by the model. Two evaluation protocols are used in the literature differing in whether they
enforce graph constraints over model predictions. The ﬁrst graph-constrained protocol requires that
the top-K triplets assign one consistent class per entity and relation. The second unconstrained
protocol does not enforce any such constraints. We report results on both protocols, following (Zellers
et al., 2017).

8

Figure 4: (a) An input image with bounding boxes from VG. (b) The ground-truth scene graph. (c) The
Baseline fails to recognize some entities (tail and tree) and relations (in front of instead of looking at). (d)
GPI:LINGUISTIC ﬁxes most incorrect LP predictions. (e) Window is the most signiﬁcant neighbor of Tree. (f)
The entity bird receives substantial attention, while Tree and building are less informative.

Models and baselines. We compare four variants of our GPI approach with the reported results
of four baselines that are currently the state-of-the-art on various scene graph prediction problems
(all models use the same data split and pre-processing as (Xu et al., 2017)): 1) LU ET AL., 2016
(LU ET AL., 2016): This work leverages word embeddings to ﬁne-tune the likelihood of predicted
relations. 2) XU ET AL, 2017 (XU ET AL., 2017): This model passes messages between entities and
relations, and iteratively reﬁnes the feature map used for prediction. 3) NEWELL & DENG, 2017
(NEWELL & DENG, 2017): The PIXEL2GRAPH model uses associative embeddings (Newell et al.,
2017) to produce a full graph from the image. 4) YANG ET AL., 2018 (YANG ET AL., 2018): The
GRAPH R-CNN model uses object-relation regularities to sparsify and reason over scene graphs. 5)
ZELLERS ET AL., 2017 (ZELLERS ET AL., 2017): The NEURALMOTIF method encodes global
context for capturing high-order motifs in scene graphs, and the BASELINE outputs the entities and
relations distributions without using the global context. The following variants of GPI were compared:
1) GPI: NO ATTENTION: Our GPI model, but with no attention mechanism. Instead, following
Theorem 1, we simply sum the features. 2) GPI: NEIGHBORATTENTION: Our GPI model, with
attention over neighbors features. 3) GPI: LINGUISTIC: Same as GPI: NEIGHBORATTENTION but
also concatenating the word embedding vector, as described above.

Results. Table 1 shows recall@50 and recall@100 for three variants of our approach, and compared
with ﬁve baselines. All GPI variants performs well, with LINGUISTIC outperforming all baselines
for SGCls and being comparable to the state-of-the-art model for PredCls. Note that PredCl is an
easier task, which makes less use of the structure, hence it is not surprising that GPI achieves similar
accuracy to Zellers et al. (2017). Figure 4 illustrates the model behavior. Predicting isolated labels
with zi (4c) mislabels several entities, but these are corrected at the ﬁnal output (4d). Figure 4e
shows that the system learned to attend more to nearby entities (the window and building are closer
to the tree), and 4f shows that stronger attention is learned for the class bird, presumably because it is
usually more informative than common classes like tree.

Implementation details. The φ and α networks were each implemented as a single fully-connected
(FC) layer with a 500-dimensional outputs. ρ was implemented as a FC network with 3 500-
dimensional hidden layers, with one 150-dimensional output for the entity probabilities, and one
51-dimensional output for relation probabilities. The attention mechanism was implemented as a
network like to φ and α, receiving the same inputs, but using the output scores for the attention . The
full code is available at https://github.com/shikorab/SceneGraph

6 Conclusion

We presented a deep learning approach to structured prediction, which constrains the architecture
to be invariant to structurally identical inputs. As in score-based methods, our approach relies on
pairwise features, capable of describing inter-label correlations, and thus inheriting the intuitive
aspect of score-based approaches. However, instead of maximizing a score function (which leads
to computationally-hard inference), we directly produce an output that is invariant to equivalent
representations of the pairwise terms.

9

This axiomatic approach to model architecture can be extended in many ways. For image labeling,
geometric invariances (shift or rotation) may be desired.
In other cases, invariance to feature
permutations may be desirable. We leave the derivation of the corresponding architectures to future
work. Finally, there may be cases where the invariant structure is unknown and should be discovered
from data, which is related to work on lifting graphical models Bui et al. (2013). It would be interesting
to explore algorithms that discover and use such symmetries for deep structured prediction.

This work was supported by the ISF Centers of Excellence grant, and by the Yandex Initiative in
Machine Learning. Work by GC was performed while at Google Brain Research.

Acknowledgements

References

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and

translate. In International Conference on Learning Representations (ICLR), 2015.

Belanger, David, Yang, Bishan, and McCallum, Andrew. End-to-end learning for structured prediction
energy networks. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70, pp. 429–439. PMLR, 2017.

Bello, Irwan, Pham, Hieu, Le, Quoc V, Norouzi, Mohammad, and Bengio, Samy. Neural combinato-

rial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.

Bui, Hung Hai, Huynh, Tuyen N., and Riedel, Sebastian. Automorphism groups of graphical models
and lifted variational inference. In Proceedings of the Twenty-Ninth Conference on Uncertainty in
Artiﬁcial Intelligence, UAI’13, pp. 132–141, 2013.

Chen, Danqi and Manning, Christopher. A fast and accurate dependency parser using neural
networks. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 740–750, 2014.

Chen, Liang Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Se-
mantic image segmentation with deep convolutional nets and fully connected CRFs. In Proceedings
of the Second International Conference on Learning Representations, 2014.

Chen, Liang Chieh, Schwing, Alexander G, Yuille, Alan L, and Urtasun, Raquel. Learning deep

structured models. In Proc. ICML, 2015.

Farabet, Clement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical
features for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):
1915–1929, 2013.

Gilmer, Justin, Schoenholz, Samuel S, Riley, Patrick F, Vinyals, Oriol, and Dahl, George E. Neural

message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.

Gygli, Michael, Norouzi, Mohammad, and Angelova, Anelia. Deep value networks learn to evaluate
and iteratively reﬁne structured outputs. In Precup, Doina and Teh, Yee Whye (eds.), Proceedings
of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine
Learning Research, pp. 1341–1351, International Convention Centre, Sydney, Australia, 2017.
PMLR.

Johnson, Justin, Krishna, Ranjay, Stark, Michael, Li, Li-Jia, Shamma, David A., Bernstein, Michael S.,
In Proc. Conf. Comput. Vision Pattern

Image retrieval using scene graphs.

and Li, Fei-Fei.
Recognition, pp. 3668–3678, 2015.

Johnson, Justin, Gupta, Agrim, and Fei-Fei, Li. Image generation from scene graphs. arXiv preprint

arXiv:1804.01622, 2018.

Khalil, Elias, Dai, Hanjun, Zhang, Yuyu, Dilkina, Bistra, and Song, Le. Learning combinatorial
optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp.
6351–6361, 2017.

10

Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint

arXiv: 1412.6980, abs/1412.6980, 2014.

Krishna, Ranjay, Zhu, Yuke, Groth, Oliver, Johnson, Justin, Hata, Kenji, Kravitz, Joshua, Chen,
Stephanie, Kalantidis, Yannis, Li, Li-Jia, Shamma, David A, et al. Visual genome: Connecting
International Journal of
language and vision using crowdsourced dense image annotations.
Computer Vision, 123(1):32–73, 2017.

Lafferty, J., McCallum, A., and Pereira, F. Conditional random ﬁelds: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning, pp. 282–289, 2001.

Liao, Wentong, Yang, Michael Ying, Ackermann, Hanno, and Rosenhahn, Bodo. On support relations

and semantic scene graphs. arXiv preprint arXiv:1609.05834, 2016.

Lin, Guosheng, Shen, Chunhua, Reid, Ian, and van den Hengel, Anton. Deeply learning the messages
in message passing inference. In Advances in Neural Information Processing Systems, pp. 361–369,
2015.

Lu, Cewu, Krishna, Ranjay, Bernstein, Michael S., and Li, Fei-Fei. Visual relationship detection with

language priors. In European Conf. Comput. Vision, pp. 852–869, 2016.

Meshi, O., Sontag, D., Jaakkola, T., and Globerson, A. Learning efﬁciently with approximate
In Proceedings of the 27th International Conference on Machine

inference via dual losses.
Learning, pp. 783–790, New York, NY, USA, 2010. ACM.

Newell, Alejandro and Deng, Jia. Pixels to graphs by associative embedding. In Advances in Neural
Information Processing Systems 30 (to appear), pp. 1172–1180. Curran Associates, Inc., 2017.

Newell, Alejandro, Huang, Zhiao, and Deng, Jia. Associative embedding: End-to-end learning for
joint detection and grouping. In Neural Inform. Process. Syst., pp. 2274–2284. Curran Associates,
Inc., 2017.

Pei, Wenzhe, Ge, Tao, and Chang, Baobao. An effective neural network model for graph-based de-
pendency parsing. In Proceedings of the 53rd Annual Meeting of the Association for Computationa
Linguistics, pp. 313–322, 2015.

Plummer, Bryan A., Mallya, Arun, Cervantes, Christopher M., Hockenmaier, Julia, and Lazebnik,
Svetlana. Phrase localization and visual relationship detection with comprehensive image-language
cues. In ICCV, pp. 1946–1955, 2017.

Raposo, David, Santoro, Adam, Barrett, David, Pascanu, Razvan, Lillicrap, Timothy, and Battaglia,
Peter. Discovering objects and their relations from entangled scene representations. arXiv preprint
arXiv:1702.05068, 2017.

Schwing, Alexander G and Urtasun, Raquel. Fully connected deep structured networks. ArXiv

e-prints, 2015.

Shelhamer, Evan, Long, Jonathan, and Darrell, Trevor. Fully convolutional networks for semantic

segmentation. Proc. Conf. Comput. Vision Pattern Recognition, 39(4):640–651, 2017.

Taskar, B., Guestrin, C., and Koller, D. Max margin Markov networks. In Thrun, S., Saul, L., and
Schölkopf, B. (eds.), Advances in Neural Information Processing Systems 16, pp. 25–32. MIT
Press, Cambridge, MA, 2004.

Xu, Danfei, Zhu, Yuke, Choy, Christopher B., and Fei-Fei, Li. Scene Graph Generation by Iterative
Message Passing. In Proc. Conf. Comput. Vision Pattern Recognition, pp. 3097–3106, 2017.

Yang, Jianwei, Lu, Jiasen, Lee, Stefan, Batra, Dhruv, and Parikh, Devi. Graph R-CNN for scene

graph generation. In European Conf. Comput. Vision, pp. 690–706, 2018.

Zaheer, Manzil, Kottur, Satwik, Ravanbakhsh, Siamak, Poczos, Barnabas, Salakhutdinov, Ruslan R,
and Smola, Alexander J. Deep sets. In Advances in Neural Information Processing Systems 30, pp.
3394–3404. Curran Associates, Inc., 2017.

11

Zellers, Rowan, Yatskar, Mark, Thomson, Sam, and Choi, Yejin. Neural motifs: Scene graph parsing

with global context. arXiv preprint arXiv:1711.06640, abs/1711.06640, 2017.

Zheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav, Su, Zhizhong, Du,
Dalong, Huang, Chang, and Torr, Philip HS. Conditional random ﬁelds as recurrent neural networks.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 1529–1537, 2015.

7 Supplementary Material

This supplementary material includes: (1) Visual illustration of the proof of Theorem 1. (2) Explaining
how to integrate an attention mechanism in our GPI framework. (3) Additional evaluation method to
further analyze and compare our work with baselines.

7.1 Theorem 1: Illustration of Proof

Figure 5: Illustration of the proof of Theorem 1 using a speciﬁc construction example. Here H is a hash function
of size L = 5 such that H(1) = 1, H(3) = 2, H(2) = 4, G is a three-node input graph, and zi,j ∈ R are the
pairwise features (in purple) of G. (a) φ is applied to each zi,j. Each application yields a vector in R5. The three
dark yellow columns correspond to φ(z1,1), φ(z1,2) and φ(z1,3). Then, all vectors φ(zi,j) are summed over j
to obtain three si vectors. (b) α’s (blue matrices) are an outer product between 1 [H(zi)] and si resulting in a
matrix of zeros except one row. The dark blue matrix corresponds for α(z1, s1). (c) All α’s are summed to a
5 × 5 matrix, isomorphic to the original zi,j matrix.

7.2 Characterizing Permutation Invariance: Attention

Attention is a powerful component which naturally can be introduced into our GPI model. We now
show how attention can be introduced in our framework. Formally, we learn attention weights for the
neighbors j of a node i, which scale the features zi,j of that neighbor. We can also learn different
attention weights for individual features of each neighbor in a similar way.

Let wi,j ∈ R be an attention mask specifying the weight that node i gives to node j:

wi,j(zi, zi,j, zj) = eβ(zi,zi,j ,zj )/

eβ(zi,zi,t,zt)

(4)

(cid:88)

t

where β can be any scalar-valued function of its arguments (e.g., a dot product of zi and zj as in
standard attention models). To introduce attention we wish α ∈ Re to have the form of weighting
wi,j over neighboring feature vectors zi,j, namely, α = (cid:80)
To achieve this form we extend φ by a single entry, deﬁning φ ∈ Re+1 (namely we set
L = e + 1) as φ1:e(zi, zi,j, zj) = eβ(zi,zi,j ,zj )zi,j (here φ1:e are the ﬁrst e elements of φ)

j(cid:54)=i wi,jzi,j.

12

and φe+1(zi, zi,j; zj) = eβ(zi,zi,j ,zj ). We keep the deﬁnition of si = (cid:80)
we deﬁne α = si,1:e
si,e+1
over neighboring feature vectors zi,j:

j(cid:54)=i φ(zi, zi,j, zj). Next,
and substitute si and φ to obtain the desired form as attention weights wi,j

α(zi, si) =

si,1:e
si,e+1

(cid:88)

=

j(cid:54)=i

eβ(zi,zi,j ,zj )zi,j
(cid:80)
j(cid:54)=i eβ(zi,zi,j ,zj )

(cid:88)

=

j(cid:54)=i

wi,jzi,j

A similar approach can be applied over α and ρ to model attention over the outputs of α as well
(graph nodes).

7.3 Scene Graph Results

In the main paper, we described the results for the two prediction tasks: SGCls and PredCls, as
deﬁned in section 5.2.1: "Experimental Setup and Results". To further analyze our module, we
compare the best variant, GPI: LINGUISTIC, per relation to two baselines: (Lu et al., 2016) and Xu
et al. (2017). Table 2, speciﬁes the PredCls recall@5 of the 20-top frequent relation classes. The GPI
module performs better in almost all the relations classes.

RELATION

(LU ET AL., 2016)

(XU ET AL., 2017)

LINGUISTIC

99.71
98.03
80.38
82.47
98.47
85.16
31.85
49.19
61.50
79.35
28.64
31.74
26.09
8.45
54.08

ON
HAS
IN
OF
WEARING
NEAR
WITH
ABOVE
HOLDING
BEHIND
UNDER
SITTING ON
IN FRONT OF
ATTACHED TO
AT
HANGING FROM 0.0
OVER
FOR
RIDING

9.26
12.20
72.43

99.25
97.25
88.30
96.75
98.23
96.81
88.10
79.73
80.67
92.32
52.73
50.17
59.63
29.58
70.41
0.0
0.0
31.71
89.72

99.3
98.7
95.9
98.1
99.6
95.4
94.2
83.9
95.5
91.2
83.2
90.4
74.9
77.4
80.9
74.1
62.4
45.1
96.1

Table 2: Recall@5 of PredCls for the 20-top relations ranked by their frequency, as in (Xu et al., 2017)

13

