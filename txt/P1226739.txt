8
1
0
2
 
c
e
D
 
8
2

 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
5
4
3
0
1
.
0
1
7
1
:
v
i
X
r
a

Journal of Machine Learning Research 19 (2018) 1-57

Submitted 4/18; Published 11/18

The Implicit Bias of Gradient Descent on Separable Data

Daniel Soudry
Elad Hoffer
Mor Shpigel Nacson
Department of Electrical Engineering,Technion
Haifa, 320003, Israel

Suriya Gunasekar
Nathan Srebro
Toyota Technological Institute at Chicago
Chicago, Illinois 60637, USA

Editor: Leon Bottou

DANIEL.SOUDRY@GMAIL.COM
ELAD.HOFFER@GMAIL.COM
MOR.SHPIGEL@GMAIL.COM

SURIYA@TTIC.EDU
NATI@TTIC.EDU

Abstract
We examine gradient descent on unregularized logistic regression problems, with homogeneous
linear predictors on linearly separable datasets. We show the predictor converges to the direction
of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone de-
creasing loss functions with an inﬁmum at inﬁnity, to multi-class problems, and to training a weight
layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is
very slow, and only logarithmic in the convergence of the loss itself. This can help explain the
beneﬁt of continuing to optimize the logistic or cross-entropy loss even after the training error is
zero and the training loss is extremely small, and, as we show, even if the validation loss increases.
Our methodology can also aid in understanding implicit regularization in more complex models
and with other optimization methods.
Keywords: gradient descent, implicit regularization, generalization, margin, logistic regression

1. Introduction

It is becoming increasingly clear that implicit biases introduced by the optimization algorithm play a
crucial role in deep learning and in the generalization ability of the learned models (Neyshabur et al.,
2014, 2015; Zhang et al., 2017; Keskar et al., 2017; Neyshabur et al., 2017; Wilson et al., 2017). In
particular, minimizing the training error, without explicit regularization, over models with more pa-
rameters and capacity than the number of training examples, often yields good generalization. This
is despite the fact that the empirical optimization problem being highly underdetermined. That is,
there are many global minima of the training objective, most of which will not generalize well, but
the optimization algorithm (e.g. gradient descent) biases us toward a particular minimum that does
generalize well. Unfortunately, we still do not have a good understanding of the biases introduced
by different optimization algorithms in different situations.

We do have an understanding of the implicit regularization introduced by early stopping of
stochastic methods or, at an extreme, of one-pass (no repetition) stochastic gradient descent (Hardt et al.,
2016). However, as discussed above, in deep learning we often beneﬁt from implicit bias even when
optimizing the training error to convergence (without early stopping) using stochastic or batch meth-
ods. For loss functions with attainable, ﬁnite minimizers, such as the squared loss, we have some

2018 Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro.

c
(cid:13)
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
http://jmlr.org/papers/v19/18-188.html.

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

understanding of this: in particular, when minimizing an underdetermined least squares problem us-
ing gradient descent starting from the origin, it can be shown that we will converge to the minimum
Euclidean norm solution. However, the logistic loss, and its generalization the cross-entropy loss
which is often used in deep learning, do not admit ﬁnite minimizers on separable problems. Instead,
to drive the loss toward zero and thus minimize it, the norm of the predictor must diverge toward
inﬁnity.

Do we still beneﬁt from implicit regularization when minimizing the logistic loss on separable
data? Clearly the norm of the predictor itself is not minimized, since it grows to inﬁnity. However,
for prediction, only the direction of the predictor, i.e. the normalized w(t)/
, is important.
How does w(t)/
when we minimize the logistic (or similar) loss using
gradient descent on separable data, i.e., when it is possible to get zero misclassiﬁcation error and
thus drive the loss to zero?

behave as t

w(t)
k

→ ∞

w(t)

k

k

k

k

w(t)
k

In this paper, we show that even without any explicit regularization, for all linearly separa-
ble datasets, when minimizing logistic regression problems using gradient descent, we have that
w(t)/
converges to the L2 maximum margin separator, i.e. to the solution of the hard mar-
gin SVM for homogeneous linear predictors. This happens even though neither the norm
, nor
the margin constraint, are part of the objective or explicitly introduced into optimization. More
generally, we show the same behavior for generalized linear problems with any smooth, monotone
strictly decreasing, lower bounded loss with an exponential tail. Furthermore, we characterize the
rate of this convergence, and show that it is rather slow, wherein for almost all datasets, the distance
to the max-margin predictor decreasing only as O(1/ log(t)), and in some degenerate datasets, the
rate further slows down to O(log log(t)/ log(t)). This explains why the predictor continues to im-
prove even when the training loss is already extremely small. We emphasize that this bias is speciﬁc
to gradient descent, and changing the optimization algorithm, e.g. using adaptive learning rate meth-
ods such as ADAM (Kingma and Ba, 2015), changes this implicit bias.

w
k

k

2. Main Results

xn, yn}
n=1, with xn ∈
Consider a dataset
{
by minimizing an empirical loss of the form

N

Rd and binary labels yn ∈ {−

1, 1
}

. We analyze learning

N

(w) =

ℓ

ynw⊤xn

.

L

n=1
X

(cid:16)

(cid:17)

(1)

where w

Rd is the weight vector. To simplify notation, we assume that all the labels are positive:

n : yn = 1 — this is true without loss of generality, since we can always re-deﬁne ynxn as xn.

We are particularly interested in problems that are linearly separable, and the loss is smooth

∀

∈

strictly decreasing and non-negative:

Assumption 1 The dataset is linearly separable:

w

such that

∃

∗

n : w⊤
∗

∀

xn > 0 .

Assumption 2 ℓ (u) is a positive, differentiable, monotonically decreasing to zero1, (so
0, ℓ′ (u) < 0, limu
Lipshitz and limu

u : ℓ (u) >
ℓ′ (u) = 0), a β-smooth function, i.e. its derivative is β-

ℓ (u) = limu
ℓ′ (u)

= 0.

→∞

→∞

∀

→−∞

1. The requirement of non-negativity and that the loss asymptotes to zero is purely for convenience. It is enough to
require the loss is monotone decreasing and bounded from below. Any such loss asymptotes to some constant, and is
thus equivalent to one that satisﬁes this assumption, up to a shift by that constant.

2

GRADIENT DESCENT ON SEPARABLE DATA

Assumption 2 includes many common loss functions, including the logistic, exp-loss2 and probit
(w) is a βσ2
max (X )-smooth function, where σmax (X ) is the
Rd

losses. Assumption 2 implies that
maximal singular value of the data matrix X

×
Under these conditions, the inﬁmum of the optimization problem is zero, but it is not attained
at any ﬁnite w. Furthermore, no ﬁnite critical point w exists. We consider minimizing eq. 1 using
Gradient Descent (GD) with a ﬁxed learning rate η, i.e., with steps of the form:

N .

L

∈

w (t + 1) = w (t)

η

(w(t)) = w (t)

ℓ′

w (t)⊤ xn

xn.

(2)

−

∇L

N

η

−

Xn=1

(cid:16)

(cid:17)

We do not require convexity. Under Assumptions 1 and 2, gradient descent converges to the global
minimum (i.e. to zero loss) even without it:

Lemma 1 Let w (t) be the iterates of gradient descent (eq. 2) with η < 2β−
starting point w(0). Under Assumptions 1 and 2, we have: (1) limt
n : limt
limt

w (t)⊤ xn =

, and (3)

=

→

ßnf ty L

.

w (t)
k

→∞ k

∞
Proof Since the data is linearly separable,

∀

→∞
w

∞

which linearly separates the data, and therefore

1σ−

2

max (X ) and any
(w (t)) = 0, (2)

∃

∗
N

w⊤

∗ ∇L

(w) =

ℓ′

w⊤xn

xn.

w⊤
∗

n=1
X

(cid:16)

(cid:17)

∀

u : ℓ′ (u) < 0. Therefore, there are no ﬁnite critical points w, for which

n : w⊤
For any ﬁnite w, this sum cannot be equal to zero, as a sum of negative terms, since
xn > 0
∗
(w) = 0. But
and
gradient descent on a smooth loss with an appropriate stepsize is always guaranteed to converge to a
0 (see, e.g. Lemma 10 in Appendix A.4, slightly adapted from Ganti
critical point:
n : w (t)⊤ xn > 0 for
(2015), Theorem 2). This necessarily implies that
0, so GD converges to
large enough t—since only then ℓ′
the global minimum.

w (t)
k
k → ∞
0. Therefore,

while
(w)

w (t)⊤ xn

∀
∇L

(w (t))

∀
→

∇L

→

→

L

(cid:16)

(cid:17)

The main question we ask is: can we characterize the direction in which w(t) diverges? That is,
does the limit limt

always exist, and if so, what is it?

w (t) /

In order to analyze this limit, we will need to make a further assumption on the tail of the loss

w (t)
k

k

→∞

function:

−

Deﬁnition 2 A function f (u) has a “tight exponential tail”, if there exist positive constants c, a, µ+, µ
and u

such that

−

, u+

∀

u > u+ :f (u)
:f (u)
u > u

≤

c (1 + exp (

c (1

exp (

µ+u)) e−
u)) e−
µ

au

au .

−

−

−

≥
Assumption 3 The negative loss derivative

∀

−

−

For example, the exponential loss ℓ (u) = e−

u and the commonly used logistic loss ℓ (u) =
u) both follow this assumption with a = c = 1. We will assume a = c = 1 — without

log (1 + e−
loss of generality, since these constants can be always absorbed by re-scaling xn and η.

ℓ′ (u) has a tight exponential tail (Deﬁnition 2).

−

We are now ready to state our main result:

2. The exp-loss does not have a global β smoothness parameter. However, if we initialize with η < 1/L(w(0)) then it

is straightforward to show the gradient descent iterates maintain bounded local smoothness.

3

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Theorem 3 For any dataset which is linearly separable (Assumption 1), any β-smooth decreasing
loss function (Assumption 2) with an exponential tail (Assumption 3), any stepsize η < 2β−
and any starting point w(0), the gradient descent iterates (as in eq. 2) will behave as:

1σ−

max (X )

2

where ˆw is the L2 max margin vector (the solution to the hard margin SVM):

and the residual grows at most as

= O(log log(t)), and so

w (t) = ˆw log t + ρ (t) ,

ˆw = argmin
w

Rd k

w

2 s.t. w⊤xn ≥
k

1,

∈
ρ (t)
k
k

lim
t
→∞

k

w (t)
w (t)

=

ˆw
ˆw

.

k

k

k

(3)

(4)

Furthermore, for almost all data sets (all except measure zero), the residual ρ(t) is bounded.

Proof Sketch We ﬁrst understand intuitively why an exponential tail of the loss entail asymptotic
u exactly, and examine
convergence to the max margin vector: Assume for simplicity that ℓ (u) = e−
n : w (t)⊤ xn → ∞
, as is guaranteed by
the asymptotic regime of gradient descent in which
Lemma 1. If w (t) /
+
∞
ρ (t) such that g (t)
ρ (t) /g (t) = 0. The gradient can then be
written as:

w (t)
converges to some limit w
k
n :x⊤n w
,
∀

, then we can write w (t) = g (t) w

> 0, and limt

k
→ ∞

→∞

∞

∞

∀

N

N

(w) =

exp

w (t)⊤ xn

xn =

exp

−∇L

−

g (t) w⊤
∞

−

xn

−

exp

ρ (t)⊤ xn

xn .

(5)

(cid:17)

(cid:16)

(cid:16)

n=1
X

→ ∞

n=1
X
and the exponents become more negative, only those samples with the largest (i.e.,
As g(t)
least negative) exponents will contribute to the gradient. These are precisely the samples with
xn, aka the “support vectors”. The negative gradient (eq. 5)
the smallest margin argminnw⊤
∞
would then asymptotically become a non-negative linear combination of support vectors. The limit
w
will then be dominated by these gradients, since any initial conditions become negligible as
will also be a non-negative linear combination of
∞
. We therefore have:
/

∞
(from Lemma 1). Therefore, w
w (t)
k
support vectors, and so will its scaling ˆw = w

k → ∞

(cid:17)

(cid:16)

(cid:17)

N

Xn=1

n

∀

αn ≥
(cid:16)

ˆw =

αnxn

0 and ˆw⊤xn = 1

OR

αn = 0 and ˆw⊤xn > 1

(6)

(cid:17)

(cid:16)

(cid:17)

∞

These are precisely the KKT conditions for the SVM problem (eq. 4) and we can conclude that ˆw
is indeed its solution and w

is thus proportional to it.

w (t)

To prove Theorem 3 rigorously, we need to show that w (t) /

has a limit, that g (t) =
log (t) and to bound the effect of various residual errors, such as gradients of non-support vectors
and the fact that the loss is only approximately exponential. To do so, we substitute eq. 3 into the
gradient descent dynamics (eq. 2), with w
= ˆw being the max margin vector and g(t) = log t.
We then show that, except when certain degeneracies occur, the increment in the norm of ρ (t) is
ν for some C1 > 0 and ν > 1, which is a converging series. This happens because
bounded by C1t−
1, cancels out the dominant
the increment in the max margin term, ˆw [log (t + 1)
t−

ˆwt−
(w (t)) (eq. 5 with g (t) = log (t) and w⊤
∞

1 term in the gradient

xn = 1).

log (t)]

−∇L

−

≈

∞

k

k

minn w⊤
∞

xn

∞

(cid:0)

(cid:1)

4

GRADIENT DESCENT ON SEPARABLE DATA

Degenerate and Non-Degenerate Data Sets An earlier conference version of this paper (Soudry et al.,
2018) included a partial version of Theorem 3, which only applies to almost all data sets, in which
case we can ensure the residual ρ(t) is bounded. This partial statement (for almost all data sets) is re-
stated and proved as Theorem 9 in Appendix A. It applies, e.g. with probability one for data sampled
from any absolutely continuous distribution. It does not apply in “degenerate” cases where some
of the support vectors xn (for which ˆw⊤xn = 1) are associated with dual variables that are zero
(αn = 0) in the dual optimum of 4. As we show in Appendix B, this only happens on measure zero
data sets. Here, we prove the more general result which applies for all data sets, including degener-
ate data sets. To do so, in Theorem 13 in Appendix C we provide a more complete characterization
of the iterates w(t) that explicitly speciﬁes all unbounded components even in the degenerate case.
We then prove the Theorem by plugging in this more complete characterization and showing that
the residual is bounded, thus also establishing Theorem 3.

Parallel Work on the Degenerate Case Following the publication of our initial version, and
while preparing this revised version for publication, we learned of parallel work by Ziwei Ji and
Matus Telgarsky that also closes this gap. Ji and Telgarsky (2018) provide an analysis of the degen-
erate case, establishing converges to the max margin predictor by showing that
=

log log t
log t

(cid:16)q

. Our analysis provides a more precise characterization of the iterates, and also shows
O
the convergence is actually quadratically faster (see Section 3). However, Ji and Telgarsky go even
further and provide a characterization also when the data is non-separable but w(t) still goes to
inﬁnity.

(cid:17)

(cid:13)
(cid:13)
(cid:13)

w(t)
w(t)

ˆw
ˆw
k

k

k −

k

(cid:13)
(cid:13)
(cid:13)

More Reﬁned Analysis of the Residual
In some non-degenerate cases, we can further character-
ize the asymptotic behavior of ρ (t). To do so, we need to refer to the KKT conditions (eq. 6) of
= argminn ˆw⊤xn. We then have the
the SVM problem (eq. 4) and the associated support vectors
following Theorem, proved in Appendix A:

S

Theorem 4 Under the conditions and notation of Theorem 3, for almost all datasets, if in addition
the support vectors span the data (i.e. rank (X
is a matrix whose columns
are only those data points xn s.t. ˆw⊤xn = 1), then limt

ρ (t) = ˜w, where ˜w is a solution to

) = rank (X), where X

S

S

→∞

n

∀

∈ S

: η exp

x⊤n ˜w

= αn

−

(7)

(cid:17)

(cid:16)
Analogies with Boosting Perhaps most similar to our study is the line of work on understanding
AdaBoost in terms its implicit bias toward large L1-margin solutions, starting with the seminal work
of Schapire et al. (1998). Since AdaBoost can be viewed as coordinate descent on the exponential
loss of a linear model, these results can be interpreted as analyzing the bias of coordinate descent,
rather then gradient descent, on a monotone decreasing loss with an exact exponential tail. Indeed,
with small enough step sizes, such a coordinate descent procedure does converge precisely to the
maximum L1-margin solution (Zhang et al., 2005; Telgarsky, 2013). In fact, Telgarsky (2013) also
generalizes these results to other losses with tight exponential tails, similar to the class of losses we
consider here.

Also related is the work of Rosset et al. (2004). They considered the regularization path wλ =
p
wλkp is
p for similar loss functions as we do, and showed that limλ
arg min
proportional to the maximum Lp margin solution. That is, they showed how adding inﬁnitesimal Lp

(w) + λ

0 wλ/

w
k

L

→

k

k

5

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

(e.g. L1 and L2) regularization to logistic-type losses gives rise to the corresponding max-margin
predictor.3 However, Rosset et al. do not consider the effect of the optimization algorithm, and
instead add explicit regularization. Here we are speciﬁcally interested in the bias implied by the
algorithm not by adding (even inﬁnitesimal) explicit regularization. We see that coordinate descent
gives rise to the max L1 margin predictor, while gradient descent gives rise to the max L2 norm
predictor.
In Section 4.3 and in follow-up work (Gunasekar et al., 2018) we discuss also other
optimization algorithms, and their implied biases.

Non-homogeneous linear predictors
In this paper we focused on homogeneous linear predictors
of the form w⊤x, similarly to previous works (e.g., Rosset et al. (2004); Telgarsky (2013)). Specif-
ically, we did not have the common intercept term: w⊤x + b. One may be tempted to introduce
the intercept in the usual way, i.e., by extending all the input vectors xn with an additional ′1′ com-
ponent.
In this extended input space, naturally, all our results hold. Therefore, we converge in
direction to the L2 max margin solution (eq. 4) in the extended space. However, if we translate this
solution to the original x space we obtain

which is not the L2 max margin (SVM) solution

argmin
w
Rd,b

R k

w

k

∈

∈

2 + b2 s.t. w⊤xn + b

1,

≥

argmin
w
Rd,b

R k

w

k

∈

∈

2 s.t. w⊤xn + b

1,

≥

where we do not have a b2 penalty in the objective.

3. Implications: Rates of convergence

The solution in eq. 3 implies that w (t) /
tor ˆw/
Speciﬁcally, our results imply the following tight rates of convergence:

converges to the normalized max margin vec-
. Moreover, this convergence is very slow— logarithmic in the number of iterations.

w (t)
k

ˆw

k

k

k

Theorem 5 Under the conditions and notation of Theorem 3, for any linearly separable data set,
the normalized weight vector converges to the normalized max margin vector in L2 norm

k
with this rate improving to O(1/ log(t)) for almost every dataset; and in angle

k

(cid:18)

k (cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

w (t)
w (t)
k

ˆw
ˆw

−

= O

log log t
log t

,

(cid:19)

2

1

−

w (t)⊤ ˆw
ˆw
w (t)
k

k k

k

= O

log log t
log t

,

!

(cid:19)

 (cid:18)

with this rate improving to O(1/ log2(t)) for almost every dataset; and the margin converges as

(8)

(9)

(10)

3. In contrast, with non-vanishing regularization (i.e., λ > 0), arg minw L(w) + λ kwkp

p is generally not a max margin

solution.

1
ˆw

k

k

−

minn x⊤n w (t)
w (t)
k
k

= O

1
log t

(cid:18)

(cid:19)

.

6

GRADIENT DESCENT ON SEPARABLE DATA

On the other hand, the loss itself decreases as

(w (t)) = O

.

L

1
t

(cid:18)

(cid:19)

(11)

All the rates in the above Theorem are a direct consequence of Theorem 3, except for avoiding
the log log t factor for the degenerate cases in eq. 10 and eq. 11 (i.e., establishing that the rates
1/ log t and 1/t always hold)—this additional improvement is a consequence of the more complete
characterization of Theorem 13. Full details are provided in Appendix D. In this appendix, we also
provide a simple construction showing all the rates in Theorem 5 are tight (except possibly for the
log log t factors).

The sharp contrast between the tight logarithmic and 1/t rates in Theorem 5 implies that the
convergence of w(t) to the max-margin ˆw can be logarithmic in the loss itself, and we might need
to wait until the loss is exponentially small in order to be close to the max-margin solution. This can
help explain why continuing to optimize the training loss, even after the training error is zero and
the training loss is extremely small, still improves generalization performance—our results suggests
that the margin could still be improving signiﬁcantly in this regime.

A numerical illustration of the convergence is depicted in Figure 1. As predicted by the theory,
grows logarithmically (note the semi-log scaling), and w(t) converges to the max-
the norm
margin separator, but only logarithmically, while the loss itself decreases very rapidly (note the
log-log scaling).

w(t)
k

k

An important practical consequence of our theory, is that although the margin of w(t) keeps
improving, and so we can expect the population (or test) misclassiﬁcation error of w(t) to improve
for many datasets, the same cannot be said about the expected population loss (or test loss)! At the
limit, the direction of w(t) will converge toward the max margin predictor ˆw. Although ˆw has zero
training error, it will not generally have zero misclassiﬁcation error on the population, or on a test or
a validation set. Since the norm of w(t) will increase, if we use the logistic loss or any other convex
loss, the loss incurred on those misclassiﬁed points will also increase. More formally, consider the
logistic loss ℓ(u) = log(1 + e−
u). Since
−
N
n=1 h( ˆw⊤xn) = 0.
ˆw classiﬁes all training points correctly, we have that on the training set
However, on the population we would expect some errors and so E[h( ˆw⊤x)] > 0. Since w(t)
ˆw log t and ℓ(αu)

u) and deﬁne also the hinge-at-zero loss h(u) = max(0,

αh(u) as α

, we have:

P

≈

→

E[ℓ(w(t)⊤x)]

→ ∞
E[ℓ((log t) ˆw⊤x)]

≈

≈

(log t)E[h( ˆw⊤x)] = Ω(log t).

(12)

That is, the population loss increases logarithmically while the margin and the population misclassi-
ﬁcation error improve. Roughly speaking, the improvement in misclassiﬁcation does not out-weight
the increase in the loss of those points still misclassiﬁed.

The increase in the test loss is practically important because the loss on a validation set is
frequently used to monitor progress and decide on stopping. Similar to the population loss, the
validation loss will increase logarithmically with t, if there is at least one sample in the validation set
which is classiﬁed incorrectly by the max margin vector (since we would not expect zero validation
error). More precisely, as a direct consequence of Theorem 3 (as shown on Appendix D):

Corollary 6 Let ℓ be the logistic loss, and
such that x⊤ ˆw < 0. Then the validation loss increases as

V

be an independent validation set, for which

x

∃

∈ V

Lval (w (t)) =

ℓ

w (t)⊤ x

= Ω(log(t)).

x
X
∈V

(cid:16)

(cid:17)

7

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

(A)

3

2

1

0

−1

−2

−3
−3

2

x

101

102

104

105

106

101

102

104

105

106

1

0.5

(B)

|
|
)
t
(

w

|
|
 
d
e
z
i
l
a
m
r
o
N

(D)

0
100

x 10−3

8

6

4

2

p
a
g
 
e
l
g
n
A

0
100

103
t

103
t

 

(C)

100

)
)
t
(

w
(
L

10−5

10−10

 
100

(E)

p
a
g
 
n
i
g
r
a

M

0.1

0.05

0
100

GD
GDMO

103
t

103
t

−2

−1

1

2

3

101

102

104

105

106

101

102

104

105

106

0
x
1

Figure 1: Visualization of or main results on a synthetic dataset in which the L2 max margin vec-
tor ˆw is precisely known. (A) The dataset (positive and negatives samples (y =
1)
±
are respectively denoted by ′+′ and ′◦′), max margin separating hyperplane (black line),
and the asymptotic solution of GD (dashed blue). For both GD and GD with momentum
(GDMO), we show: (B) The norm of w (t), normalized so it would equal to 1 at the last
iteration, to facilitate comparison. As expected (eq. 3), the norm increases logarithmi-
1 (eq. 11); and (D&E) the
cally; (C) the training loss. As expected, it decreases as t−
angle and margin gap of w (t) from ˆw (eqs. 9 and 10). As expected, these are logarith-
mically decreasing to zero. Implementation details: The dataset includes four support
x2
vectors: x1 = (0.5, 1.5) , x2 = (1.5, 0.5) with y1 = y2 = 1, and x3 =
1 (the L2 normalized max margin vector is then ˆw = (1, 1) /√2 with
with y3 = y4 =
margin equal to √2 ), and 12 other random datapoints (6 from each class), that are not on
the margin. We used a learning rate η = 1/σ2
max (X) is the maximal
singular value of X, momentum γ = 0.9 for GDMO, and initialized at the origin.

max (X), where σ2

x1, x4 =

−

−

−

This behavior might cause us to think we are over-ﬁtting or otherwise encourage us to stop the
optimization. However, this increase does not actually represent the model getting worse, merely
w(t)
getting larger, and in fact the model might be getting better (increasing the margin and
k
possibly decreasing the error rate).

k

4. Extensions

4.1 Multi-Class Classiﬁcation with Cross-Entropy Loss

So far, we have discussed the problem of binary classiﬁcation, but in many practical situations
we have more then two classes. For multi-class problems, the labels are the class indices yn ∈
[K] ,
[K]. A common loss function
in multi-class classiﬁcation is the following cross-entropy loss with a softmax output, which is a
generalization of the logistic loss:

and we learn a predictor wk for each class k

1, . . . , K
{

∈

}

L

wk}k
{
(cid:0)

[K]

∈

=

−

(cid:1)

N

n=1
X

log

 

8

w⊤ynxn

exp
K
k=1 exp
(cid:0)

w⊤k xn
(cid:1)

!

P

(cid:0)

(cid:1)

(13)

GRADIENT DESCENT ON SEPARABLE DATA

Figure 2: Training of a convolutional neural network on CIFAR10 using stochastic gradient de-
scent with constant learning rate and momentum, softmax output and a cross entropy
loss, where we achieve 8.3% ﬁnal validation error. We observe that, approximately: (1)
1, (2) the L2 norm of last weight layer increases logarith-
The training loss decays as a t−
mically, (3) after a while, the validation loss starts to increase, and (4) in contrast, the
validation (classiﬁcation) error slowly improves.

What do the linear predictors wk(t) converge to if we minimize the cross-entropy loss by gradient
descent on the predictors? In Appendix E we analyze this problem for separable data, and show that
again, the predictors diverge to inﬁnity and the loss converges to zero. Furthermore, we prove the
following Theorem:

Theorem 7 For almost all multiclass datasets (i.e., except for a measure zero) which are linearly
separable (i.e. the constraints in eq. 15 below are feasible), any starting point w(0) and any small
enough stepsize, the iterates of gradient descent on 13 will behave as:

wk(t) = ˆwk log(t) + ρk(t),

where the residual ρk(t) is bounded and ˆwk is the solution of the K-class SVM:

argminw1,...,wk

2 s.t.

wk||
||

n,

k

∀

∀

= yn : w⊤ynxn ≥

w⊤k xn + 1.

K

Xk=1

4.2 Deep networks

(14)

(15)

So far we have only considered linear prediction. Naturally, it is desirable to generalize our results
also to non-linear models and especially multi-layer neural networks.

Even without a formal extension and description of the precise bias, our results already shed
light on how minimizing the cross-entropy loss with gradient descent can have a margin maximizing
effect, how the margin might improve only logarithmically slow, and why it might continue to
improve even as the validation loss increases. These effects are demonstrated in Figure 2 and Table
1 which portray typical training of a convolutional neural network using unregularized gradient
descent4. As can be seen, the norm of the weight increases, but the validation error continues
decreasing, albeit very slowly (as predicted by the theory), even after the training error is zero and
the training loss is extremely small. We can now understand how even though the loss is already

4. Code available here: https://github.com/paper-submissions/MaxMargin

9

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Epoch
L2 norm
Train loss
Train error
Validation loss
Validation error

2000
400
200
100
50
25.9
20.3
19.6
16.5
13.6
4
0.002
0.02
0.03
0.1
10−
0%
1.2% 0.6% 0.07%
4%
0.52
1.01
0.77
0.77
0.55
12.4% 10.4% 11.1% 9.1% 8.92%

3

5

4000
27.54
10−
·
0%
1.18
8.9%

Table 1: Sample values from various epochs in the experiment depicted in Fig. 2.

extremely small, some sort of margin might be gradually improving as we continue optimizing.
We can also observe how the validation loss increases despite the validation error decreasing, as
discussed in Section 3.

As an initial advance toward tackling deep network, we can point out that for several special
cases, our results may be directly applied to multi-layered networks. First, somewhat trivially, our
results may be applied directly to the last weight layer of a neural network if the last hidden layer
becomes ﬁxed and linearly separable after a certain number of iterations. This can become true,
either approximately, if the input to the last hidden layer is normalized (e.g., using batch norm), or
exactly, if the last hidden layer is quantized (Hubara et al., 2018).

Second, as we show next, our results may be applied exactly on deep networks if only a sin-
gle weight layer is being optimized, and, furthermore, after a sufﬁcient number of iterations, the
activation units stop switching and the training error goes to zero.

Corollary 8 We examine a multilayer neural network with component-wise ReLU functions f (z) =
l=1. Given input xn and target yn ∈ {−
max [z, 0], and weights
, the DNN produces a
scalar output
W2f (W1xn)))

un = WLf (WL

Wl}

1, 1
}

1f (

{

L

−

· · ·

and has loss ℓ (ynun), where ℓ obeys assumptions 2 and 3.

If we optimize a single weight layer wl = vec
N
n=1ℓ (ynun(wl)) converges to zero, and
wl(t)

converges to

∃

signs, then wl(t)/
P

k

k

t0 such that
(cid:0)
(cid:1)

∀

W⊤l

using gradient descent, so that

(wl) =
t > t0 the ReLU inputs do not switch

L

ˆwl = argmin
wl

wlk
k

2 s.t. ynun(wl)

1.

≥

Proof We examine the output of the network given a single input xn, for t > t0. Since the ReLU
inputs do not switch signs, we can write vl, the output of layer l, as

where we deﬁned Al,n for l < L as a diagonal 0-1 matrix, which diagonal is the ReLU slopes at
layer l, sample n, and AL,n = 1. Additionally, we deﬁne

vl,n =

Am,nWmxn ,

l

Ym=1

δl,n = Al,n

W⊤mAm,n ; ˜xl,n = δl,n ⊗

ul

1,n .

−

l+1

Ym=L

10

GRADIENT DESCENT ON SEPARABLE DATA

Using this notation we can write

un(wl) = vL,n =

Am,nWmxn = δ⊤l,nWlul

1,n = ˜x⊤l,nwl .

(16)

−

This implies that

(wl) =

ℓ (ynun(wl)) =

L

N

Xn=1

ℓ

yn˜x⊤l,nwl
(cid:16)

(cid:17)

,

which is the same as the original linear problem. Since the loss converges to zero, the dataset
N
n=1 must be linearly separable. Applying Theorem 3, and recalling that u(wl) = ˜x⊤l wl
˜xl,n, yn}
{
from eq. 16, we prove this corollary.

L

Ym=1

N

Xn=1

Importantly, this case is non-convex, unless we are optimizing the last layer. Note we assumed
ReLU functions for simplicity, but this proof can be easily generalized for any other piecewise linear
constant activation functions (e.g., leaky ReLU, max-pooling).

Lastly, in a follow-up work (Gunasekar et al., 2018b), given a few additional assumptions, ex-
tended our results to linear predictors which can be written as a homogeneous polynomial in the
parameters. These results seem to indicate that, in many cases, GD operating on exp-tailed loss
with positively homogeneous predictors aims to a speciﬁc direction. This is the direction of the max
margin predictor minimizing the L2 norm in the parameter space. It is not yet clear how to generally
translate such an implicit bias in the parameter space to the implicit bias in the predictor space — ex-
cept in special cases, such as deep linear neural nets, as we have shown in (Gunasekar et al., 2018b).
Moreover, in non-linear neural nets, there are many equivalent max-margin solutions which mini-
mize the L2 norm of the parameters. Therefore, it is natural to expect that GD would have additional
implicit biases, which select a speciﬁc subset of these solutions.

4.3 Other optimization methods

In this paper we examined the implicit bias of gradient descent. Different optimization algorithms
exhibit different biases, and understanding these biases and how they differ is crucial to under-
standing and constructing learning methods attuned to the inductive biases we expect. Can we
characterize the implicit bias and convergence rate in other optimization methods?

In Figure 1 we see that adding momentum does not qualitatively affect the bias induced by
gradient descent. In Figure 4 in Appendix F we also repeat the experiment using stochastic gradient
descent, and observe a similar asymptotic bias (this was later proved in Nacson et al. (2018)). This
is consistent with the fact that momentum, acceleration and stochasticity do not change the bias
when using gradient descent to optimize an under determined least squares problem. It would be
beneﬁcial, though, to rigorously understand how much we can generalize our result to gradient
descent variants, and how the convergence rates might change in these cases.

On the other hand, as an example of how changing the optimization algorithm does change the
bias, consider adaptive methods, such as AdaGrad (Duchi et al., 2011) and ADAM (Kingma and Ba,
2015). In Figure 3 we show the predictors obtained by ADAM and by gradient descent on a simple
data set. Both methods converge to zero training error solutions. But although gradient descent
converges to the L2 max margin predictor, as predicted by our theory, ADAM does not. The implicit
bias of adaptive methods has in fact been a recent topic of interest, with Hoffer et al. (2017) and

11

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

(A)

50

2

x

0

−50

101

102

104

105

106

101

102

104

105

106

(B)

|
|
)
t
(

w

|
|
 

d
e
z
i
l
a
m
r
o
N

(D)

1

0.5

0
100

p
a
g
 
e
l
g
n
A

0.15

0.1

0.05

0
100

103
t

103
t

 

(C)

100

)
)
t
(

w
(
L

10−10

10−20

 
100

(E)

2

1

p
a
g

 

n
i
g
r
a

M

0
100

GD
GDMO
ADAM

103
t

103
t

−50

50

0
x
1

101

102

104

105

106

101

102

104

105

106

Figure 3: Same as Fig. 1, except we multiplied all x2 values in the dastaset by 20, and also train
106 epochs of optimization using
using ADAM. The ﬁnal weight vector produced after 2
ADAM (red dashed line) does not converge to L2 max margin solution (black line), in
contrast to GD (blue dashed line), or GDMO.

·

Wilson et al. (2017) suggesting they lead to worse generalization, and Wilson et al. (2017) providing
examples of the differences in the bias for linear regression problems with the squared loss. Can
we characterize the bias of adaptive methods for logistic regression problems? Can we characterize
the bias of other optimization methods, providing a general understanding linking optimization
algorithms with their biases?

In a follow-up paper (Gunasekar et al., 2018) provided initial answers to these questions. Gunasekar et al.

(2018) derived a precise characterization of the limit direction of steepest descent for general norms
when optimizing the exp-loss, and show that for adaptive methods such as Adagrad the limit direc-
tion can depend on the initial point and step size and is thus not as predictable and robust as with
non-adaptive methods.

4.4 Other loss functions

In this work we focused on loss functions with exponential tail and observed a very slow, logarithmic
convergence of the normalized weight vector to the L2 max margin direction. A natural question
that follows is how does this behavior change with types of loss function tails. Speciﬁcally, does the
normalized weight vector always converge to the L2 max margin solution? How is the convergence
rate affected? Can we improve the convergence rate beyond the logarithmic rate found in this work?

In a follow-up work Nacson et al. (2018) provided partial answers to these questions. They
proved that the exponential tail has the optimal convergence rate, for tails for which ℓ′(u) is of the
uν ) with ν > 0.25. They then conjectured, based on heuristic analysis, that the expo-
form exp(
nential tail is optimal among all possible tails. Furthermore, they demonstrated that polynomial or
heavier tails do not converge to the max margin solution. Lastly, for the exponential loss they pro-
posed a normalized gradient scheme which can signiﬁcantly improve convergence rate, achieving
O(log(t)/√t).

−

12

GRADIENT DESCENT ON SEPARABLE DATA

4.5 Matrix Factorization

With multi-layered neural networks in mind, Gunasekar et al. (2017) recently embarked on a study
of the implicit bias of under-determined matrix factorization problems, where the squared loss of the
linear observation of a matrix is minimized by gradient descent on its factorization. Since a matrix
factorization can be viewed as a two-layer network with linear activations, this is perhaps the sim-
plest deep model one can study in full, and can thus provide insight and direction to studying more
complex neural networks. Gunasekar et al. conjectured, and provided theoretical and empirical evi-
dence, that gradient descent on the factorization for an under-determined problem converges to the
minimum nuclear norm solution, but only if the initialization is inﬁnitesimally close to zero and
the step-sizes are inﬁnitesimally small. With ﬁnite step-sizes or ﬁnite initialization, Gunasekar et al.
could not characterize the bias.

The follow-up paper (Gunasekar et al., 2018) studied this same problem with exponential loss
instead of squared loss. Under additional assumptions on the asymptotic convergence of update di-
rections and gradient directions, they were able to relate the direction of gradient descent iterates on
the factorized parameterization asymptotically to the maximum margin solution with unit nuclear
norm. Unlike the case of squared loss, the result for exponential loss are independent of initializa-
tion and with only mild conditions on the step size. Here again, we see the asymptotic nature of
exponential loss on separable data nullifying the initialization effects thereby making the analysis
simpler compared to squared loss.

5. Summary

We characterized the implicit bias induced by gradient descent on homogeneous linear predictors
when minimizing smooth monotone loss functions with an exponential tail. This is the type of loss
commonly being minimized in deep learning. We can now rigorously understand:

1. How gradient descent, without early stopping, induces implicit L2 regularization and con-
verges to the maximum L2 margin solution, when minimizing for binary classiﬁcation with
logistic loss, exp-loss, or other exponential tailed monotone decreasing loss, as well as for
multi-class classiﬁcation with cross-entropy loss. Notably, even though the logistic loss and
the exp-loss behave very different on non-separable problems, they exhibit the same behaviour
for separable problems. This implies that the non-tail part does not affect the bias. The bias
is also independent of the step-size used (as long as it is small enough to ensure convergence)
and is also independent on the initialization (unlike for least square problems).

2. The convergence of the direction of gradient descent updates to the maximum L2 margin
solution, however is very slow compared to the convergence of training loss, which explains
why it is worthwhile continuing to optimize long after we have zero training error, and even
when the loss itself is already extremely small.

3. We should not rely on plateauing of the training loss or on the loss (logistic or exp or cross-
entropy) evaluated on a validation data, as measures to decide when to stop.
Instead, we
should look at the 0–1 error on the validation dataset. We might improve the validation
and test errors even when when the decrease in the training loss is tiny and even when the
validation loss itself increases.

13

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Perhaps that gradient descent leads to a max L2 margin solution is not a big surprise to those for
whom the connection between L2 regularization and gradient descent is natural. Nevertheless, we
are not familiar with any prior study or mention of this fact, let alone a rigorous analysis and study
of how this bias is exact and independent of the initial point and the step-size. Furthermore, we also
analyze the rate at which this happens, leading to the novel observations discussed above. Even more
importantly, we hope that our analysis can open the door to further analysis of different optimization
methods or in different models, including deep networks, where implicit regularization is not well
understood even for least square problems, or where we do not have such a natural guess as for
gradient descent on linear problems. Analyzing gradient descent on logistic/cross-entropy loss is
not only arguably more relevant than the least square loss, but might also be technically easier.

Acknowledgments

The authors are grateful to J. Lee, and C. Zeno for helpful comments on the manuscript. The
research of DS was supported by the Israel Science Foundation (grant No. 31/1031), by the Taub
foundation and of NS by the National Science Foundation.

14

GRADIENT DESCENT ON SEPARABLE DATA

Appendix

Appendix A. Proof of Theorems 3 and 4 for almost every dataset

In the following sub-sections we ﬁrst prove Theorem 9 below, which is a version of Theorem 3,
specialized for almost every dataset. We then prove Theorem 4 (which is already stated for almost
every dataset).

Theorem 9 For almost every dataset which is linearly separable (Assumption 1), any β-smooth
decreasing loss function (Assumption 2) with an exponential tail (Assumption 3), any stepsize η <
max (X ) and any starting point w(0), the gradient descent iterates (as in eq. 2) will behave
2β−
as:

1σ−

2

w (t) = ˆw log t + ρ (t) ,

(17)

where ˆw is the L2 max margin vector

ˆw = argmin
w

Rd k

w

2 s.t.
k

n : w⊤xn ≥

∀

1,

∈

the residual ρ(t) is bounded, and so

k
In the following proofs, for any solution w (t), we deﬁne

k

k

k

w (t)
w (t)

=

ˆw
ˆw

.

lim
t
→∞

−
where ˆw and ˜w follow the conditions of Theorems 3 and 4, i.e. ˆw is the L2 is the max margin vector
deﬁned above, and ˜w is a vector which satisﬁes eq. 7:

−

r (t) = w (t)

ˆw log t

˜w,

n

∀

∈ S

: η exp

x⊤n ˜w

= αn ,

−

(cid:16)

(cid:17)

Rd

S ⊂ {

}

where we recall that we denoted X
a subset

1, . . . , N

of the columns of X = [x1, . . . , xN ]

×|S| as the matrix whose columns are the support vectors,
N .

Rd

S ∈

×

In Lemma 12 (Appendix B) we prove that for almost every dataset α is uniquely deﬁned, there
. Therefore, eq. 18 is well-deﬁned in those
are no more then d support vectors and αn 6
n
∀
cases. If the support vectors do not span the data, then the solution ˜w to eq. 18 might not be unique.
In this case, we can use any such solution in the proof.

= 0,

∈ S

∈

We furthermore denote the minimum margin to a non-support vector as:

(18)

(19)

θ = min
∈S

n /

x⊤n ˆw > 1 ,

and by Ci,ǫi,ti (i
Rd
P1 ∈
×
(the columns of X
X
).

S

N) various positive constants which are independent of t. Lastly, we deﬁne
d as the orthogonal projection matrix5 to the subspace spanned by the support vectors
P1 as the complementary projection (to the left nullspace of

∈
), and ¯P1 = I

−

S
5. This matrix can be written as P1 = XSX+

S , where M† is the Moore-Penrose pseudoinverse of M.

15

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

A.1 Simple proof of Theorem 9

In this section we ﬁrst examine the special case that ℓ (u) = e−
of gradient descent: η

0 , so

→

˙w (t) =

(w (t)) .

u and take the continuous time limit

−∇L
The proof in this case is rather short and self-contained (i.e., does not rely on any previous results),
and so it helps to clarify the main ideas of the general (more complicated) proof which we will give
in the next sections.

Recall we deﬁned

Our goal is to show that
implies that

r (t)
k

k

and therefore

r (t) = w (t)

log (t) ˆw

˜w .

−

−

is bounded, and therefore ρ (t) = r (t) + ˜w is bounded. Eq. 20

(20)

(21)

˙r (t) = ˙w (t)

ˆw =

(w (t))

−∇L

1
t

−

1
t

ˆw

−

1
2

d
dt k
N

r (t)
k

2 = ˙r⊤ (t) r (t)

exp

x⊤n w (t)

x⊤n r (t)

ˆw⊤r (t)

1
t

−

(cid:17)
log (t) ˆw⊤xn −

˜w⊤xn −

x⊤n r (t)
(cid:17)

x⊤n r (t)

ˆw⊤r (t)

1
t

−

#

=

=

+

Xn=1

−

(cid:16)

exp

−

(cid:16)

"

Xn
∈S





/
Xn
∈S

(cid:16)

exp

log (t) ˆw⊤xn −

˜w⊤xn −

−

x⊤n r (t)

x⊤n r (t)

,

(22)

(cid:17)





where in the last equality we used eq. 20 and decomposed the sum over support vectors
non-support vectors. We examine both bracketed terms. Recall that ˆw⊤xn = 1 for n
we deﬁned (in eq. 18) ˜w so that
22 can be written as

and
S
, and that
xn = ˆw. Thus, the ﬁrst bracketed term in eq.

˜w⊤xn

∈ S

exp

−

∈S

n

exp

P
˜w⊤xn −

(cid:1)
x⊤n r (t)

(cid:0)
x⊤n r (t)
(cid:17)
x⊤n r (t)

exp

1
t

1

−

−

(cid:17) (cid:16)

−

(cid:16)

−

(cid:16)

−

(cid:16)

=

exp

˜w⊤xn

1
t

1
t

Xn
∈S

Xn
∈S
z

(cid:17)

(cid:17)
z e−

zz

exp

˜w⊤xn

x⊤n r (t)

Xn
(cid:16)
∈S
x⊤n r (t)

−

0,

≤

(cid:17)

(23)

z, z (e−

since
19), the second bracketed term in eq. 22 can be upper bounded by

0. Furthermore, since

1)

≤

−

≤

∀

∀

1 and θ = argminn /
∈S

x⊤n ˆw > 1 (eq.

exp

log (t) ˆw⊤xn −

−

˜w⊤xn

exp

x⊤n r(t)
(cid:17)

−

(cid:16)

(cid:17)

/
Xn
∈S

(cid:16)

Substituting eq. 23 and 24 into eq. 22 and integrating, we obtain, that

x⊤n r(t)

1
tθ

≤

exp

˜w⊤xn

.

(24)

−

Xn
/
(cid:16)
∈S
C, C ′ such that

(cid:17)

∃

t1,

t > t1 :

∀

∀

2

r (t)
k
k

2
r(t1)
||

≤

C

− ||

t

dt
tθ ≤

t1

Z

C ′ <

,

∞

16

GRADIENT DESCENT ON SEPARABLE DATA

(25)

(27)

(28)

since θ > 1 (eq. 19). Thus, we showed that r(t) is bounded, which completes the proof for the

special case. (cid:4)

A.2 Complete proof of Theorem 9

Next, we give the proof for the general case (non-inﬁnitesimal step size, and exponentially-tailed
functions). Though it is based on a similar analysis as in the special case we examined in the
previous section, it is somewhat more involved since we have to bound additional terms.

First, we state two auxiliary lemmata, that are proven below in appendix sections A.4 and A.5:

(w) be a β-smooth non-negative objective. If η < 2β−

1, then, for any w(0), with

we have that

∞u=0 k∇L

(w (u))
k

2 <

∞

→∞ k∇L

2 = 0.
(w (t))
k

w (t + 1) = w (t)

η

(w(t))

−
∇L
and therefore limt

Lemma 10 Let
L
the GD sequence

P

Lemma 11 We have

C1, t1 :

t > t1 : (r (t + 1)

r (t))⊤ r (t)

C1t−

min(θ,1+1.5µ+,1+0.5µ−) .

(26)

∃
Additionally,

∀
ǫ1 > 0 ,

∀

∃

C2, t2, such that

≤

−
t > t2, if

∀
P1r (t)
k

ǫ1,

k ≥

then the following improved bound holds

(r (t + 1)

r (t))⊤ r (t)

−

C2t−

1 < 0 .

≤ −

Our goal is to show that

is bounded, and therefore ρ (t) = r (t) + ˜w is bounded. To

r (t)
k
k

show this, we will upper bound the following equation

r (t + 1)
k

2 =
k

r (t + 1)
k

r (t)
k

−

2 + 2 (r (t + 1)

r (t))⊤ r (t) +

−

2

r (t)
k
k

(29)

First, we note that ﬁrst term in this equation can be upper-bounded by

r (t + 1)
k
(1)
=

w (t + 1)

−

2

r (t)
k
ˆw log (t + 1)

˜w

−

−

w (t) + ˆw log (t) + ˜w

2
k

−
(w (t))

ˆw [log (t + 1)

η

∇L

(w (t))
k

k∇L

ˆw

2 log2
k

k

−
2 +

2

log (t)]
k
1

−
1 + t−

+ 2η ˆw⊤

∇L

(w (t)) log

1 + t−

1

k

(2)
=
k−
= η2
(3)

η2

≤

(w (t))

ˆw
k
where in (1) we used eq. 20, in (2) we used eq. 2, and in (3) we used
and also that

k∇L

k

2 +
k

2 t−

2

(cid:0)

(cid:1)

∀

(cid:0)

≥

x > 0 : x

log (1 + x) > 0,

(cid:1)

(30)

(31)

ˆw⊤

(w (t)) =

ℓ′

w (t)⊤ xn

∇L

ˆw⊤xn ≤

0 ,

(cid:17)

N

Xn=1

(cid:16)

17

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

since ˆw⊤xn ≥

1 (from the deﬁnition of ˆw) and ℓ′(u)

Also, from Lemma 10 we know that

0.

≤

(w (t))

2 = o (1) and
k

k∇L

(w (t))
k

2 <

.

∞

k∇L

∞

t=0
X
Substituting eq. 32 into eq. 30, and recalling that a t−
can ﬁnd C0 such that

ν power series converges for any ν > 1, we

r (t + 1)

k

2 = o (1) and
r (t)
k

−

r (t + 1)
k

−

r (t)

2 = C0 <
k

.

∞

∞

t=0
X

Note that this equation also implies that

ǫ0

∀
t > t0 :

t0 :

∃

∀

r (t + 1)

r (t)

< ǫ0 .

k − k

k|

|k

Next, we would like to bound the second term in eq. 29. From eq. 26 in Lemma 11, we can ﬁnd

t1, C1 such that

t > t1:

∀

(r (t + 1)

r (t))⊤ r (t)

C1t−

min(θ,1+1.5µ+,1+0.5µ−) .

−

≤

Thus, by combining eqs. 35 and 33 into eq. 29, we ﬁnd

(32)

(33)

(34)

(35)

2

r (t1)
k

− k

k

2
r (t)
k
1
t
−

=

u=t1 h
X

C0 + 2

C1u−

≤

1

t
−

u=t1
X

r (u + 1)
k

k

2

− k

r (u)

2
k

i

min(θ,1+1.5µ+,1+0.5µ−)

which is a bounded, since θ > 1 (eq. 19) and µ
bounded. (cid:4)

−

, µ+ > 0 (Deﬁnition 2). Therefore,

r (t)

is

k

k

A.3 Proof of Theorem 4

) = rank (X), and that ˜w is unique
All that remains now is to show that
given w (0). To do so, this proof will continue where the proof of Theorem 3 stopped, using
notations and equations from that proof.

0 if rank (X

r (t)

k →

k

S

∇L

(w) is spanned by the columns of X. If rank (X
S

Since r (t) has a bounded norm, its two orthogonal components r (t) = P1r (t) + ¯P1r (t) also
have bounded norms (recall that P1, ¯P1 were deﬁned in the beginning of appendix section A). From
) = rank (X), then it is also spanned
eq. 2,
(w) = 0. Therefore, ¯P1r (t) is not updated during GD, and
by the columns of X
remains constant. Since ˜w in eq. 20 is also bounded, we can absorb this constant ¯P1r (t) into ˜w
¯P1r (t) = 0). Thus, without loss of generality, we can
without affecting eq. 7 (since
assume that r (t) = P1r (t).

, and so ¯P1∇L
n

: x⊤n

∈ S

∀

S

We deﬁne the set

=

t > max [t2, t0] :
{

r (t)
k
k

< ǫ1}

.

T

18

GRADIENT DESCENT ON SEPARABLE DATA

By contradiction, we assume that the complementary set is not ﬁnite,

=

¯
t > max [t2, t0] :
{
T
is not ﬁnite:

r (t)
k

k ≥

.

ǫ1}

if it were ﬁnite, it would have had a ﬁnite maximal point

Additionally, the set
tmax ∈ T

T

, and then, combining eqs. 28, 29, and 33, we would ﬁnd that

r (t)

2
k

r (tmax)
k

− k

2 =

k

r (u + 1)

2

2

r (u)
k

k

− k

k

C0 −

≤

2C2

1

u−

,

→ −∞

t > tmax

∀

1

t
−

u=tmax
X

which is impossible since

0. Furthermore, eq. 33 implies that

r (u + 1)
k

−

2 = C0 −
r (t)
k

h (t)

where h (t) is a positive monotone function decreasing to zero. Let t3, t be any two points such that
t3 < t,

. For all such t3 and t, we have

1)

t3, t3 + 1, . . . t
{

} ⊂

¯
T

, and (t3 −

∈ T

2

r (t)
k

k

2 +
r (t3)
k

≤ k

r (u + 1)
k

2
k

2

r (u)
k

− k

i

i

1

t
−

u=tmax h
X
2

≥

k

r (t)
k
t

u=0
X

1

t
−

u=t3 h
X
1
t
−

=

2 +
r (t3)
k

k

u=t3 h
X
2 + h (t3)
r (t3)
k

≤ k

2 + h (t3) .
r (t3)
k

≤ k

r (u + 1)
k

−

2 + 2 (r (u + 1)
r (u)
k

−

r (u))⊤ r (u)

h (t

1)

−

−

−

2C2

1

u−

1

t
−

u=t3
X

i

(36)

1)

r (t3 −
< ǫ0. Since
Also, recall that t3 > t0, so from eq. 34, we have that
k|
ǫ1 + ǫ0. Moreover, since ¯
r (t3 −
deﬁnition), we conclude that
T
k
is an inﬁnite set, we can choose t3 as large as we want. This implies that
ǫ2 > 0 we can ﬁnd t3
such that ǫ2 > h (t3), since h (t) is a monotonically decreasing function. Therefore, from eq. 36,

|k
r (t3)
k

< ǫ1 (from

r (t3)

k − k

k ≤

1)

∀

T

k

ǫ1, ǫ0, ǫ2,

∀

t3 ∈

∃

¯
T

such that

t > t3 :

r (t)

ǫ1 + ǫ0 + ǫ2 .

∀

k

2
k

≤

This implies that

r (t)

0.

k

k →

This sets ˜w uniquely, together with eq. 7. (cid:4)

Lastly, we note that since ¯P1r (t) is not updated during GD, we have that ¯P1 ( ˜w

w (0)) = 0.

−

A.4 Proof of Lemma 10

Lemma 10 Let
L
the GD sequence

(w) be a β-smooth non-negative objective. If η < 2β−

1, then, for any w(0), with

we have that

∞u=0 k∇L

(w (u))
k

2 <

∞

→∞ k∇L

2 = 0.
(w (t))
k

w (t + 1) = w (t)

η

(w(t))

−
∇L
and therefore limt

(25)

P

19

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

This proof is a slightly modiﬁed version of the proof of Theorem 2 in (Ganti, 2015). Recall a

well-known property of β-smooth functions:

f (x)

f (y)

f (y)⊤ (x

y)

−

− ∇

−

≤

β
2 k

x

y

2 .

−

k

(37)

From the β-smoothness of

(w)

(cid:12)
(cid:12)
(cid:12)

L

(w (t + 1))

(w (t)) +

(w (t))⊤ (w (t + 1)

w (t)) +

w (t + 1)

w (t)

β
2 k

−

2
k

L

(cid:12)
(cid:12)
(cid:12)

−

≤ L

=

=

L

L

∇L

η

η

(w (t))

(w (t))

−

k∇L

2 +
k

βη2
2 k∇L

2

(w (t))
k

(w (t))

−

βη
2

1

−

(cid:18)

k∇L

(cid:19)

2
(w (t))
k

(w (t))

L

− L
1

−

η

(cid:16)

(w (t + 1))
βη
2

(cid:17)

≥ k∇L

2
(w (t))
k

Thus, we have

which implies

t

u=0
X

2
(w (u))
k

≤

k∇L

t

L

u=0
X

(w (u))

(w (u + 1))
βη
2

η

− L
1

−

(w (0))

= L

(w (t + 1))
βη
2

.

η

− L
1

−

(cid:16)
The right hand side is upper bounded by a ﬁnite constant, since L (w (0)) <
This implies

(cid:16)

(cid:17)

∞

(cid:17)
and 0

≤ L

(w (t + 1)).

(w (u))

2 <
k

,

∞

k∇L

∞

u=0
X

and therefore

2

(w (t))
k

→

0. (cid:4)

k∇L

A.5 Proof of Lemma 11

Recall that we deﬁned r (t) = w (t)
Theorems 3 and 4, i.e, ˆw is the L2 max margin vector and (eq. 4), and eq. 7 holds

˜w, with ˆw and ˜w follow the conditions of the

ˆw log t

−

−

C1, t1 :

t > t1 : (r (t + 1)

r (t))⊤ r (t)

C1t−

min(θ,1+1.5µ+,1+0.5µ−) .

(26)

n

∀

∈ S

: η exp

x⊤n ˜w

= αn .

−

(cid:16)

Lemma 11 We have

∃
Additionally,

∀
ǫ1 > 0 ,

∀

∃

C2, t2, such that

then the following improved bound holds

(cid:17)

≤

−
t > t2, if

∀
P1r (t)
k

ǫ1,

k ≥

(r (t + 1)

r (t))⊤ r (t)

−

C2t−

1 < 0 .

≤ −

20

(27)

(28)

GRADIENT DESCENT ON SEPARABLE DATA

From Lemma 1,

n :

w (t)⊤ xn =
→∞
ℓ′ (u) has an exponential tail e−

limt

∀

loss derivative
generality). Combining both facts, we have positive constants µ

−

. In addition, from assumption 3 the negative
∞
u (recall we assume a = c = 1 without loss of

, µ+, t

and t+ such that

n

∀

t > t+ :

w (t)⊤ xn

1 + exp

µ+w (t)⊤ xn

∀

∀

t > t

:

−

ℓ′

ℓ′

−

−

(cid:16)

w (t)⊤ xn

(cid:17)

≤

≥

(cid:16)
1

−

(cid:16)

exp

−

µ

−

−

w (t)⊤ xn

w (t)⊤ xn

w (t)⊤ xn

(cid:17)

(cid:16)

−

−

(38)

(39)

(cid:16)
Next, we examine the expression we wish to bound, recalling that r (t) = w (t)

(cid:17)(cid:17)

(cid:16)

(cid:16)

(cid:17)

(cid:16)

(cid:17)
ˆw log t

−

˜w:

−

−

−

exp

exp

(cid:17)(cid:17)

ˆw [log (t + 1)

log (t)])⊤ r (t)

ℓ′

w (t)⊤ xn

x⊤n r (t)

ˆw⊤r (t) log

1 + t−

1

(r (t + 1)

r (t))⊤ r (t)

−
(w (t))

−

= (

η

−

∇L
N

=

η

−

Xn=1
= ˆw⊤r (t)

(cid:16)
t−

1

(cid:17)
1 + t−

1

log

−

η

−

(cid:2)
1 exp
t−

(cid:0)
˜w⊤xn

(cid:1)(cid:3)
+ ℓ′

−

−

−

η

−

Xn /
(cid:16)
∈S
w (t)⊤ xn

(cid:17)
x⊤n r (t)

(cid:16)

(cid:17)i

Xn
∈S h
where in last line we used eqs. 6 and 7 to obtain

(cid:17)

(cid:16)

(cid:0)

ℓ′

w (t)⊤ xn

(cid:1)
x⊤n r (t)

(40)

We examine the three terms in eq. 40. The ﬁrst term can be upper bounded by

ˆw =

αnxn = η

exp

˜w⊤xn

xn .

Xn
∈S

Xn
∈S

−

(cid:16)

(cid:17)

ˆw⊤r (t)

1

t−

log

1 + t−

1

−

max

h

max

≤
(1)

≤
(2)

h
ˆw
k
k
≤ (
t−
o

1

(cid:2)

−

log
(cid:1)(cid:3)

ˆw⊤r (t) , 0

t−
(cid:0)
i (cid:2)
ˆw⊤P1r (t) , 0
i
P1r (t)
k ≤
P1r (t)
k

ǫ1t−
1

, if
, if

k
k

t−

(cid:0)

2

2

ǫ1
> ǫ1

1 + t−

1

(cid:1)(cid:3)

(41)

(cid:1)
where in (1) we used that ¯P1 ˆw = ¯P1X
S
since

(cid:0)

α = 0 from eq. 6, and in (2) we used that ˆw⊤r (t) = o (t),

ˆw⊤r (t) = ˆw⊤

w (0)

 
ˆw⊤ (w (0)

≤

t

η

−

∇L

u=0
X

(w (u))

ˆw log (t)

˜w

−

−

˜w

−

−

ˆw log (t))

−

ηt min
0
≤
≤

u

t

ˆw⊤

∇L

!
(w (u)) = o (t)

where in the last line we used that

(w (t)) = o (1), from Lemma 10.

Next, we upper bound the second term in eq. 40. From eq. 38

∇L

ℓ′(w(t)⊤xn)

2 exp(

≤

−

∃
w(t)⊤xn).

t′+, such that

> t0 > t′+,

∀

(42)

21

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Therefore,

t > t′+:

∀

−

η

η

≤ −

ℓ′

w (t)⊤ xn

x⊤n r (t)

Xn /
∈S

(cid:16)

: x⊤
Xn /
n
∈S

r(t)

ℓ′
0
(cid:16)

≥

(cid:17)
w (t)⊤ xn

x⊤n r (t)

(cid:17)

2 exp

w (t)⊤ xn

x⊤n r (t)

−

(cid:16)

n ˆw
x⊤

exp

(cid:17)

˜w⊤xn −

−

x⊤n r (t)

x⊤n r (t)

(cid:17)

: x⊤
Xn /
n
∈S

r(t)

≥

0

: x⊤
Xn /
n
∈S

r(t)

: x⊤
Xn /
n
∈S

r(t)

2t−
0

≥

2t−
0

≥

n ˆw
x⊤

exp

˜w⊤xn

(cid:17)

(cid:16)

−

(cid:16)

θ

t−

ηN exp

min
n

−

˜w⊤xn

(cid:16)

(cid:17)

(1)

≤

η

(2)

≤

η

(3)

≤

η

(4)

≤

≥

η

−

(43)

where in (1) we used eq. 42, in (2) we used w (t) = ˆw log t + ˜w + r (t), in (3) we used
x

0,and in (4) we used θ > 1, from eq. 19.

1 and x⊤n r (t)

xe−

≤

Lastly, we will bound the sum in the third term in eq. 40

t−

1 exp

−

˜w⊤xn

+ ℓ′

w (t)⊤ xn

x⊤n r (t) .

(44)

(cid:17)
We examine each term n in this sum, and divide into two cases, depending on the sign of x⊤n r (t).

(cid:17)i

(cid:16)

(cid:16)

Xn
∈S h

First, if x⊤n r (t)

0, then term n in eq. 44 can be upper bounded

t > t+, using eq. 38, by

ηt−

1 exp

1 + t−

µ+ exp

µ+ ˜w⊤xn

exp

1

x⊤n r (t)

(45)

−

(cid:16)

(cid:17)(cid:17)

∀
x⊤n r (t)
(cid:17)

−

(cid:16)

−

i

0.5µ+, then we can upper bound eq. 45 with

≥
˜w⊤xn

−

(cid:17) h(cid:16)
(cid:16)
We further divide into cases:

1. If

x⊤n r(t)

C0t−

≤

(cid:12)
(cid:12)

(cid:12)
(cid:12)

η exp

(1 + µ+) min

˜w⊤xn

C0t−

1

1.5µ+ .

−

(46)

−

(cid:16)

(cid:17)

2. If

x⊤n r(t)

> C0t−

0.5µ+, then we can ﬁnd t′′+ > t′+ to upper bound eq. 45

t > t′′+:

∀

(cid:12)
ηt−
(cid:12)

1e−

(cid:12)
˜w⊤xn
(cid:12)

1 + t−

µ+e−

µ+ ˜w⊤xn

exp

0.5µ+

C0t−

1

x⊤n r (t)

ηt−

1e−

˜w⊤xn

1 + t−

µ+e−

(cid:17)

µ+ ˜w⊤xn

−

1

(cid:0)
−

C0t−

0.5µ+ + C 2

−

(cid:1)

i
0 t−

µ+

1

x⊤n r (t)

−

ηt−

1e−

˜w⊤xn

C0t−

0.5µ+ + C 2

(cid:17) (cid:0)
0 t−

µ+

µ+ min

n

e−

˜w⊤xnt−

µ+

i
(cid:1)
0.5µ+ + C 2
C0t−
−

0 t−

µ+

x⊤n r (t)

(1)

≤

≤
(2)

h(cid:16)

h(cid:16)
1

(cid:20)

(cid:0)

−

0,

t > t′′+

∀

≤
where in (1) we used the fact that e−
that the previous expression is negative — since t−

≤

−

1

x

x + x2 for x

0 and in (2) we deﬁned t′′+ so

0.5µ+ decreases slower than t−

µ+.

≥

(cid:21)

(47)

n

(cid:1)

22

GRADIENT DESCENT ON SEPARABLE DATA

3. If

x⊤n r(t)
≥
and therefore

(cid:12)
(cid:12)

(cid:12)
(cid:12)

This implies that

ǫ2, then we deﬁne t′′′+ > t′′+ such that t′′′+ > exp
µ+ ˜w⊤xn
t > t′′′+, we have
1 + t−
t > t′′′+ we can upper bound eq. 45 by

µ+ exp

−

∀

(cid:0)

(cid:0)

(cid:0)
(cid:1)(cid:1)

∀

minn ˜w⊤xn
ǫ2 < e−
e−

e0.5ǫ2

1/µ+ ,

−

1

0.5ǫ2 .
(cid:1) (cid:2)

−

(cid:3)

Second, if x⊤n r(t) < 0, we again further divide into cases:

η exp

−

max
n

−

˜w⊤xn

(cid:16)

1
(cid:17) (cid:0)

−

(cid:1)

0.5ǫ2

e−

ǫ2t−

1.

(48)

1. If

x⊤n r(t)
eq. 44 with
(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

C0t−

0.5µ−, then, since

ℓ′

w (t)⊤ xn

> 0, we can upper bound term n in

ηt−

1 exp

˜w⊤xn

−

η exp

min
n

−

˜w⊤xn

1

0.5µ−

C0t−

−

(49)

(cid:16)
(cid:16)
0.5µ− , then, using eq. 39 we upper bound term n in eq. 44 with

(cid:17)

(cid:17) (cid:12)
(cid:12)
(cid:12)

2. If

x⊤n r (t)

> C0t−

(cid:17)

−

(cid:16)

≤

x⊤n r (t)
(cid:12)
(cid:12)
(cid:12)

ℓ′

w (t)⊤ xn

x⊤n r (t)

t−

(cid:12)
1e−
(cid:12)

˜w⊤xn

−
˜w⊤xn +

1e−

(cid:12)
η
(cid:12)

η

−
h
−
≤
h
1e−
=ηt−

t−

˜w⊤xn

1

−

h

(cid:16)
1
−

(cid:16)
exp

exp

(cid:17)i
µ
−
−
(cid:16)
r (t)⊤ xn

−

(cid:16)

Next, we will show that

t′
∃
−
Let M > 1 be some arbitrary constant. Then, since

> t

−

exp

(cid:16)
if exp

−

µ

w (t)⊤ xn

−
r (t)⊤ xn

0 from Lemma 1,

→
M > 1 then

−

(cid:16)

(cid:17)
≥

(cid:17)

w (t)⊤ xn

exp

w (t)⊤ xn

−
(cid:16)
˜w⊤xn exp

(cid:17)(cid:17)
1e−

t−

1

−

x⊤n r (t)
µ−

(cid:17)i
r (t)⊤ xn

−

h

(cid:16)

(cid:17)i

(cid:17) (cid:16)

(cid:17)i (cid:12)
(cid:12)
(cid:12)
such that the last expression is strictly negative
∀
1e−
r (t)⊤ xn
−
˜w⊤xn) such that

˜w⊤xn exp

tM > max(t

, M e−

t−

(cid:16)

h

∃

−

x⊤n r (t)
(cid:12)
(50)
(cid:12)
(cid:12)
.

t > t′
µ−
−
=

(cid:17)i
t > tM ,
∀

exp

r (t)⊤ xn

t−

1e−

˜w⊤xn exp

1

−

(cid:16)
Furthermore, if

(cid:17) (cid:16)
t > tM such that exp

h

r (t)⊤ xn

r (t)⊤ xn

−

(cid:16)
< M , then

µ−

≥

(cid:17)i

(cid:17)

−

∃

exp

r (t)⊤ xn

1

t−

1e−

(cid:17)
˜w⊤xn exp

µ−

r (t)⊤ xn

M ′ > 1 .

(51)

> exp

r (t)⊤ xn

t−

1e−

˜w⊤xnM

(cid:17)i

(cid:17)

(52)

−

−

(cid:16)

(cid:16)

(cid:16)

−

−

h

h

(cid:17) (cid:16)
1

(cid:17) (cid:16)

−

µ−
(cid:16)

.

i

(cid:17)

which is lower bounded by

1 + C0t−

0.5µ−

µ−

t−

e−

µ−

˜w⊤xnM
µ−

1

−
µ−

(cid:1) (cid:16)
t−
−

(cid:0)
1 + C0t−

0.5µ−

h
˜w⊤xnM

e−

(cid:17)
1.5µ−

i
t−
−

˜w⊤xnM

e−

µ−

C0

> C0t−

since
1 + x. In this case last line is strictly
larger than 1 for sufﬁciently large t. Therefore, after we substitute eqs. 51 and 52 into 50, we
ﬁnd that

, term k in eq. 44 is strictly negative

(cid:12)
(cid:12)
> tM > t

such that

≥

(cid:12)
(cid:12)

h
0.5µ−, x⊤n r (t) < 0 and ex

i

h

i

≥
x⊤n r (t)

t′
−

∃

ℓ′

w (t)⊤ xk

x⊤k r (t) < 0

(53)

t > t′
−

∀
˜w⊤xk

−

t−

1e−

−

η

−

h

(cid:16)
23

(cid:17)i

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

3. If

x⊤k r(t)
t > t′
(cid:12)
(cid:12)
∀
−
that
(cid:12)
(cid:12)
t > t′′
−

∀

ǫ2 , which is a special case of the previous case (
, either eq. 51 or 52 holds. Furthermore, in this case,

≥

eq. 52 can be lower bounded by

x⊤k r (t)
> t′
t′′
(cid:12)
(cid:12)
∃
−
−
(cid:12)
(cid:12)

0.5µ−) then
> C0t−
and M ′′ > 1 such

exp (ǫ2)

1

−

h

(cid:16)

t−

1e−

˜w⊤xk M

µ−

> M ′′ > 1 .

i

(cid:17)

Substituting this, together with eq. 51, into eq. 50, we can ﬁnd C ′0 > 0 such we can upper
bound term k in eq. 44 with

(54)

To conclude, we choose t0 = max

1. If

P1r (t)
k

k ≥

ǫ1 (as in Eq. 27), we have that

(cid:3)

(cid:2)

C ′0t−

1 ,

t > t′′
−

∀

.

−
:

t′′′+, t′′
−

2 (1)

1

≥

max
n
∈S (cid:12)
(cid:12)
(cid:12)

x⊤n r (t)
(cid:12)
(cid:12)
(cid:12)

|S| Xn
∈S (cid:12)
(cid:12)
(cid:12)

x⊤n P1r (t)
(cid:12)
(cid:12)
(cid:12)
∈ S

2

=

1

X⊤
S

P1r (t)
(cid:13)
(cid:13)
(cid:13)

|S| (cid:13)
(cid:13)
(cid:13)

2 (2)

1

≥

|S|

where in (1) we used P⊤1 xn = xn ∀
non-zero singular value of X

1 σ2

S

−

) ǫ2
min (X
|S|
maxn ˜w⊤xn
q
η exp
bounded by
C ′′0 t−
(cid:1)
(cid:0)
eqs. 41 and 43 into eq. 40, we obtain

1
1 + o
(cid:1) (cid:0)

e−
1
,

−
t−

0.5ǫ2

−

−

∀

S

n

, in (2) we denoted by σmin (X
and used eq. 27. Therefore, for some k,

), the minimal
S
ǫ2 ,
x⊤k r
1. In this case, we denote C ′′0 as the minimum between C ′0 (eq. 54) and
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ǫ2 (eq. 48). Then we ﬁnd that eq. 44 can be upper
t > t0, given eq. 27. Substituting this result, together with

≥

σ2
min (X

) ǫ2
1

S

(55)

t > t0

∀

r (t))⊤ r (t)

C ′′0 t−

1 + o

1

t−

.

≤ −

(cid:0)
t2 > t0 such that eq. 28 holds. This implies also that eq.

(cid:1)

(cid:0)
(cid:1)
(r (t + 1)

−
C2 < C ′′0 and
ǫ1.

∃

k ≥

This implies that
26 holds for

∃
P1r (t)

k
2. Otherwise, if

k

P1r (t)

< ǫ1, we ﬁnd that

t > t0 , each term in eq. 44 can be upper bounded
0.5µ−, (eq.
by either zero (eqs. 47 and 53), or terms proportional to t−
49). Combining this together with eqs. 41, 43 into eq. 40 we obtain (for some positive
constants C3, C4, C5, and C6)

1.5µ+ (eq. 46) or t−

∀

k

−

−

1

1

(r (t + 1)

r (t))⊤ r (t)

C3t−

1

1.5µ+ + C4t−

1

−

0.5µ− + C5t−

2 + C6t−

θ .

−

−

≤
t1 > t0 and C1 such that eq. 26 holds. (cid:4)

Therefore,

∃

Appendix B. Generic solutions of the KKT conditions in eq. 6

Lemma 12 For almost all datasets there is a unique α which satisﬁes the KKT conditions (eq. 6):

N

n

ˆw =

αnxn

αn ≥
Xn=1
(cid:16)
Furthermore, in this solution αn 6
are at most d such support vectors.

∀

0 and ˆw⊤xn = 1

OR

αn = 0 and ˆw⊤xn > 1

(cid:17)
= 0 if ˆw⊤xn = 1, i.e., xn is a support vector (n

(cid:16)

(cid:17)

), and there

∈ S

24

GRADIENT DESCENT ON SEPARABLE DATA

For almost every set X, no more than d points xn can be on the same hyperplane. Therefore,
since all support vectors must lie on the same hyperplane, there can be at most d support vectors,
for almost every X.

Given the set of support vectors,

, the KKT conditions of eq. 6 entail that αn = 0 if n /

and

where we denoted α
S
X
since d
S ∈

, X⊤
S

≥ |S|

as α restricted to the support vector components. For almost every set X,

R|S|×|S| is invertible. Therefore, α

has the unique solution

S

1 = X⊤
S

ˆw = X⊤
S

X
S

α
S

,

S

1

−

1 = α
S

.

X

X⊤
S

S

(cid:16)

(cid:17)

∈ S

(56)

(57)

S

n

∈ S

) /qn (X
S

This implies that
pn (X
S
) = 0, so the components of X
then pn (X
polynomial pn have measure zero, unless
equal to zero, since, for example, if X⊤
S
in this case

, αn is equal to a rational function in the components of XS, i.e., αn =
∀
), where pn and qn are polynomials in the components of XS. Therefore, if αn = 0,
must be at a root of the polynomial pn. The roots of the
S
X
) = 0. However, pn cannot be identically
∀
S
I
X
, and so
=
(d
= 0, from eq. 57.
, the event that "eq. 56 has a solution with a zero component" has a
, also has zero measure, as a
). This
. Furthermore, for almost all datasets
, the solution eq.

zero measure. Moreover, the union of these events, for all possible
ﬁnite union of zero measures sets (there are only ﬁnitely many possible sets
implies that, for almost all datasets X, αn = 0 only if n /
the solution α is unique: for each dataset,
56 is uniquely given by eq. 57. (cid:4)

n
Therefore, for a given

is uniquely determined, and given

: pn (X
, 0

, then X⊤
S

, αn = 1

1, . . . , N

S ⊂ {

)
−|S|

|S|×|S|

|S|×|S|

= I

∈ S

∈ S

|S|×

S

S

S

S

∀

}

(cid:3)

(cid:2)

S

S

Appendix C. Completing the proof of Theorem 3 for zero measure cases

In the preceding Appendices, we established Theorem 4, which only applied when all support vec-
tors are associated with non-zero coefﬁcients. This characterizes almost all data sets, i.e. all except
for measure zero. We now turn to presenting and proving a more complete characterization of the
limit behaviour of gradient descent, which covers all data sets, including those degenerate data sets
not covered by Theorem 4, thus establishing Theorem 3.

In order to do so, we ﬁrst have to introduce additional notation and a recursive treatment of the
data set. We will deﬁne a sequence of data sets ¯PmX ¯
m obtained by considering only a subset ¯
Sm
S
of the points, and projecting them using the projection matrix ¯Pm. We start, for m = 0, with the
full original data set, i.e. ¯
and ¯P0 = Id
d. We then deﬁne ˆwm as the max margin
×
predictor for ¯Pm
−

S0 =
1, . . . , N
{
m−1, i.e.:

}

1X ¯
S

ˆwm = argmin

w

Rd k

w

2 s.t. w⊤ ¯Pm
k
−

1xn ≥

1

n

∀

∈

¯
Sm
−

1 .

(58)

∈

In particular, ˆw1 is the max margin predictor for the original data set. We then denote
of non-support vectors for 58,
the dual variables corresponding to the margin constraints (for some dual solution), and ¯

+
m the indices
S
Sm the indices of support vector of 58 with non-zero coefﬁcients for
Sm the set

25

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

of support vector with zero coefﬁcients. That is:

+
m =

=
m =

S

S

n

n

¯
Sm
−
¯
Sm
−

1|

1|

ˆw⊤m

ˆw⊤m

¯Pm
−
¯Pm
−

∈

∈

n

n

1xn > 1

1xn = 1

o

= ¯

+
m

Sm \ S

Sm =
¯
Sm =

n

=
m|∃
∈ S
(
m \ Sm .
S

=

α

RN
≥

∈

0 : ˆwm =

αk ¯Pm
−

1xk, αn > 0,

i /

∀

∈ S

=
m : αi = 0

)

o
N

Xk=1

(59)

The problematic degenerate case, not covered by the analysis of Theorem 4, is when there are
support vectors with zero coefﬁcients, i.e., when ¯
. In this case we recurse on these zero-
∅
coefﬁcient support vectors (i.e., on ¯
Sm), but only consider their components orthogonal to the non-
zero-coefﬁcient support vectors (i.e., not spanned by points in

Sm 6

=

Sm). That is, we project using:

1

m

S

(cid:17)

¯Pm = ¯Pm
−

X

mX†
S

This recursive treatment continues as long as ¯

Id −
(cid:16)
¯Pm.
where we denoted A† as the Moore-Penrose pseudo-inverse of A. We also denote Pm = Id −
, deﬁning a sequence ˆwm of max margin
∅
predictors, for smaller and lower dimensional data sets ¯Pm
m−1. We stop when ¯
and
Sm =
−
denote the stopping stage M —that is, M is the minimal m such that ¯
. Our characterization
∅
will be in terms of the sequence ˆw1, . . . , ˆwM . As established in Lemma 12 of Appendix B, for
almost all data sets we will not have support vectors with non-zero coefﬁcients, and so we will have
M = 1, and so the characterization only depends on the max margin predictor ˆw1 of the original
data set. But, even for the measure zero of data sets in which M > 1, we provide the following
more complete characterization:

Sm =

1X ¯
S

Sm 6

(60)

=

∅

Theorem 13 For all datasets which are linearly separable (Assumption 1) and given a β-smooth
loss function (Assumption 2) with an exponential tail (Assumption 3), gradient descent (as in eq. 2)
max (X ) and any starting point w(0), the iterates of gradient descent can
with step size η < 2β−
be written as:

1σ−

2

w (t) =

ˆwm log◦

m (t) + ρ (t) ,

(61)

M

m=1
X

log (t), ˆwm is the L2 max margin vector deﬁned in eq. 58, and the

m times

where log◦
log log
residual ρ (t) is bounded.

m (t) =

· · ·

}|

{

z

C.1 Auxiliary notation
We say that a function f : N
f (t)

L1. Furthermore, we deﬁne

→

∈

R is absolutely summable if

∞t=1 |

f (t)
|

<

∞

, and then we denote

r (t) = w (t)

ˆwm log◦

m (t) + ˜wm +

m=1 "
Xk=1
X
where ˜wm and ˇwk,m are deﬁned next, and additionally, we denote

Q

M

−

ˇwk,m
m
1
r=k log◦

−

r (t) #

P
m

1

−

26

GRADIENT DESCENT ON SEPARABLE DATA

˜w =

˜wm .

M

m=1
X

We deﬁne,

m

1, ˜wm as the solution of

∀

≥

m

∀

≥

1 :

n

∀

∈ Sm : η

exp

m

 −

Xk=1

Xn
∈S

m

˜w⊤k xn

¯Pm
−

!

1xn = ˆwm ,

(62)

such that

such that

Pm

1 ˜wm = 0 and ¯Pm ˜wm = 0.

−

The existence and uniqueness of the solution, ˜wm are proved in appendix section C.4.

Lastly, we deﬁne,

m > k

1, ˇwk,m as the solution of

∀

≥

exp

˜w⊤xn

Pm

1xn =

−

(cid:16)

−

(cid:17)

Xn
∈S

m

m

1

−

Xk=1

Xn
∈Sk





exp

˜w⊤xn

ˇwk,m

(64)

−

(cid:16)

xnx⊤n 


(cid:17)

Pk

1 ˇwk,m = 0 and ¯Pk ˇwk,m = 0 .

−

The existence and uniqueness of the solution ˇwk,m are proved in appendix section C.5.

Together, eqs. 62-65 entail the existence of a unique decomposition,

ˆwm = η

exp

˜w⊤xn

−

xn −

η

Xn
∈S
given the constraints in eqs. 63 and 65 hold.

(cid:17)

(cid:16)

m

m

1

−

Xk=1


Xn
∈Sk


−

(cid:16)

exp

˜w⊤xn

ˇwk,m

(66)

m

∀

≥

1 :

xnx⊤n 


(cid:17)

C.2 Proof of Theorem 13

In the following proofs, for any solution w(t), we deﬁne

τ (t) =

ˆwm log◦

m (t) +

M

Xm=2

M

m

1

−

Xm=1

Xk=1

ˇwk,m
1
m
r=k log◦

−

r (t)

τ (t + 1)
k

−

τ (t)

k ≤

Q
Cτ
t log (t)

noting that

and

r(t) = w(t)

ˆw1 log (t)

−

˜w

−

−

τ (t)

where ˜w follow the conditions of Theorem 13. Our goal is to show that
this, we will upper bound the following equation

r(t)
k
k

is bounded. To show

2 =
r(t + 1)
k
k

r(t + 1)
k

−

2 + 2 (r(t + 1)
r(t)
k

−

r(t))⊤ r(t) +

2
r(t)
k
k

27

(63)

(65)

(67)

(68)

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

First, we note that

t0 such that

t > t0 the ﬁrst term in this equation can be upper bounded by

∀

2
||
ˆw1 log (t + 1)

∃
r(t)

r(t + 1)

−

||
(1)
=

w(t + 1)

||

η

(2)
=
= η2

|| −

||∇

−
L(w(t))

∇
L(w(t))

∇

L(w(t))⊤
+ 2η
+ 2 ˆw⊤1 (τ (t + 1)
(3)

(cid:0)
−
2 +
||

τ (t + 1)

w(t) + ˆw1 log (t) + τ (t)

−

−
log (t))

−
2 +
||

ˆw1(log (t + 1)
2 log2
ˆw1k
k
1
ˆw1 log
(cid:0)
1 + t−
(cid:1)

τ (t)) log
(cid:0)

1 + t−

1

(τ (t + 1)

2
||

τ (t))
−
2
τ (t)
k

−

−
1 + t−

1

−
τ (t + 1)

+

k
+ τ (t + 1)

(cid:1)

τ (t)

−

2
||

(cid:1)
2 (t) + 2Cτ k

η2

≤

||∇

L(w(t))

ˆw1k
where in (1) we used eq. 67, in (2) we used eq. 2 and in (3) we used
and also using ℓ′(w(t)⊤xn) < 0 for large enough t, we have that

ˆw1k
k

∀

(cid:0)
2 t−

(cid:1)
2 + C 2
τ t−

2 log−

t−

2 log−

1(t) ,

t > t0

(69)

x > 0 : x

log (1 + x) > 0,

∀

≥

ˆw1 log

1 + t−1

+ τ (t + 1)

τ (t)

−

⊤

∇L

(w (t))

(cid:1)

(cid:0)

(cid:0)
which is negative for sufﬁciently large t0 (since log
n : ˆw⊤1 xn ≥
then 1/ (t log (t))),

1 and ℓ′(u)

Also, from Lemma 10 we know that:

≤

0.

∀

(cid:1)

1 + t−

1

(cid:1)

ℓ′(w(t)⊤xn)

ˆw⊤

1 + t−1

1 xn log

C′
xnk
τ
k
t log (t)
(cid:1)
(cid:0)
(70)
1, which is slower
decreases as t−

−

(cid:18)

(cid:19)

(w (t))

2 = o(1) and
k

k∇L

2 <
(w(u))
k

∞

Substituting eq. 71 into eq. 69, and recalling that t−
ν2, and so

ν2 (t) converges for any ν1 > 1 and any

N

≤

n=1
X

(cid:0)

∞

k∇L

Xu=0
ν1 log−

κ0 (t) ,

r(t + 1)

r(t)

||

−

2
||

∈

L1 .

Also, in the next subsection we will prove that

Lemma 14 Let κ1 (t) and κ2 (t) be functions in L1, then

k
Thus, by combining eqs. 73 and 72 into eq. 68, we ﬁnd

≤

−

(r (t + 1)

r (t))⊤ r (t)

κ1 (t)

r (t)
k

+ κ2 (t)

2
r(t + 1)
k
k
On this result we apply the following lemma (with φ (t) =
κ0 (t) + 2κ2 (t)), which we prove in appendix C.6:

κ0 (t) + 2κ1 (t)

r (t)
k
k

≤

+ 2κ2 (t) +

2
r(t)
k
k

r(t)
, h (t) = 2κ1 (t), and z (t) =
k
k

Lemma 15 Let φ (t) , h (t) , z (t) be three functions from N to R
constants. Then, if

, and

≥

∞t=1 h (t)

0, and C1, C2, C3 be three positive

P

z (t) + h (t) φ (t) + φ2 (t)

C1 <

≤
φ2 (t + 1)

∞

≤

we have

(71)

(72)

(73)

(74)

(75)

φ2 (t + 1)

C2 + C3

z (u)

≤

t

Xu=1

28

GRADIENT DESCENT ON SEPARABLE DATA

and obtain that

2
r(t + 1)
k
k

≤

C2 + C3

(κ0 (u) + 2κ2 (u))

C4 <

≤

,

∞

since we assumed that

i = 0, 1, 2 : κi (t)

L1. This completes our proof. (cid:4)

∀

C.3 Proof of Lemma 14

Before we prove Lemma 14, we prove the following auxilary Lemma:

t

u=1
X

∈

Lemma 16 Consider the function f (t) = t−
m0 ≤

∃

M + 1 such that νm0 > 1 and for all m′ < m0,νm′ = 1, then f (t)

L1.

ν1(log(t))−

ν2(log log(t))−

ν3 . . . (log◦

M (t))−

νM +1. If

Proof To prove Lemma 16, we will show that the improper integeral
bounded, i.e.,
Cauchy test) this in turn implies that

∞t1 f (t)dt for any t1 > 0 is
∞t1 f (t)dt < C. Using the integeral test for convergence (or Maclaurin–
∞t1 f (t) < C, and thus f (t)

t1 > 0,

t1 > 0,

L1.

∀

R

R
First, if m0 > 1, then ν1 = ν2 . . . = νm0
(m0
−

1)(t), we have

of variables y = log◦

∀

−

1 = 1 and νm0 = 1 + ǫ for some ǫ > 0. Using change
P

∈

∈

dy =

t

log◦

r(t)

dt = t−

ν1

(log◦

r(t))−

νr+1 dt

m0

2

−

 

Yr=1

1

−

!

m0

2

−

Yr=1

and for all m > m0,
M +1
νm|
m=m0+1 |

˜ν =

νm

(m

log◦

(cid:16)
and log◦

−

−

1)(t)
−
1)(t1) = y1, we have
(m0
(cid:17)
(cid:16)

log◦

m0)(y)
(cid:17)

=

(m

−

νm

−

≤

(log(y))|

|. Thus, denoting

νm

P

t1

Z

∞

f (t)dt =

∞

νm0

y−

log◦

m

m0(y)

−

νm d(y)

−

M +1

y1

Z

m=m0+1
Y

(cid:0)

(cid:1)

∞

(log(y))˜ν
y1+ǫ

dy.

≤

y1

Z

(76)

For m0 = 1, we have ν1 = 1 + ǫ for some ǫ > 0, and for m > 1,

(log(t))|

νm

|. Thus, denoting, ˜ν =

νm|
Thus, for any m0, we only need to show that for all t1 > 0, ǫ > 0 and ˜ν > 0,

, we have

f (t)dt

∞t1

∞t1

P

≤

R

R

M +1
m=2 |

−

(m

log◦
(log(t))˜ν

(cid:16)
t1+ǫ dt.

1)(t)
(cid:17)

νm

−

≤

(log(t)) ˜ν

t1+ǫ dt <

∞t1

.

∞

˜νt−

Let us now look at
1 (log(t))˜ν

1 and v =
R

−

∞t1

−

(log(t)) ˜ν
t1+ǫ dt. using u = (log(t))˜ν and dv = 1

1
ǫtǫ . Using integration by parts,

udv = uv

t1+ǫ , we have du =
vdu, we have

R

(log(t))˜ν
t1+ǫ

dt =

(log(t))˜ν
ǫtǫ

−

+

¯ν
ǫ

1

(log(t))˜ν
−
t1+ǫ

Z

R

Z

−

R

dt

29

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Recursing the above equation K times such that ˜ν

K < 0, we have positive constants

c0, c1, . . . cK > 0 independent of t, such that

−

∞

(log(t))˜ν
t1+ǫ

dt =

t1

Z

K

1

−

k

∞

ck (log(t))˜ν
−
ǫtǫ

#

t=t1

Xk=0
1
ck (log(t1))˜ν
−
ǫtǫ
1

k

+ cK

∞

t=t1

Z

+ cK

∞

K

(log(t))˜ν
−
t1+ǫ

dt

t=t1

Z
(log(t))˜ν
−
t1+ǫ

K

dt

"−
K

−

Xk=0
1
K
−

Xk=0
1
K
−

(1)
=

(2)

≤

=

k

ck (log(t1))˜ν
−
ǫtǫ
1

+ cK

∞

t=t1

1
t1+ǫ

(3)

k

ck (log(t1))˜ν
−
ǫtǫ
1

y +

Z
cK
ǫtǫ
1

<

∞

Xk=0
ck(log(t))˜ν−k
where (1) follows as
ǫtǫ
K < 1. This completes the proof of the lemma.
and hence for all t > 0, (log(t))˜ν
−

0, (2) follows as K is chosen such that ˜ν

t
→∞
→

K
1
−
k=0

K < 0

−

P

Lemma 14 Let κ1 (t) and κ2 (t) be functions in L1, then

(r (t + 1)

r (t))⊤ r (t)

−

κ1 (t)

r (t)
k

k

≤

+ κ2 (t)

Proof Recall that we deﬁned

where

where

r(t) = w(t)

q (t)

−

q (t) =

[ ˆwm log◦

m (t) + hm (t)] .

M

m=1
X

hm (t) = ˜wm +

ˇwk,m
1
m
r=k log◦

−

r (t)

m

1

−

Xk=1

Q

q (t + 1)

q (t)

˙q (t)

−

−

k ≤

k

2

Cqt−

L1

∈

M

˙q (t) =

ˆwm

t

1
1
m
r=1 log◦

−

r (t)

+ ˙hm (t) .

hm (t)

k ≤ k

˜wmk

+

k

ˇwk,mk ≤

k

Ch

m

Xk=1

30

with ˆwm, ˜wm and ˇwk,m deﬁned in eqs. 58, 62 and 64, respectively. We note that

Xm=1
Additionally, we deﬁne Ch, C ′h so that

Q

(77)

(73)

(78)

(79)

(80)

(81)

(82)

(83)

GRADIENT DESCENT ON SEPARABLE DATA

and

We wish to calculate

˙hm (t)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

t

(cid:16)Q

C ′h

m
2
r=1 log◦

−

r (t)

log◦

(m

1) (t)

−

(cid:17) (cid:16)

(cid:17)

L1 .

2 ∈

(84)

(r(t + 1)

r(t))⊤ r(t)

−

(1)
= [w(t + 1)

w (t)

−
(w (t))

[q (t + 1)

−
˙q (t)]⊤ r (t)

−

−

−

(2)
= [

η

−

∇L

q (t)]]⊤ r (t)

[q (t + 1)

q (t)

˙q (t)]⊤ r (t)

(85)

−

−

where in (1) we used eq. 78 and in (2) we used the deﬁnition of GD in eq. 2. We can bound the
second term using Cauchy-Shwartz inequality and eq. 81:

[q (t + 1)

q (t)

˙q (t)]⊤ r (t)

q (t + 1)

q (t)

˙q (t)

r (t)

−

−

≤ k

−

−

k k

k ≤

2

Cqt−

r (t)
k
k

.

Next, we examine the second term in eq. 85

(w (t))

˙q (t)]⊤ r (t)

−

ℓ′(w(t)⊤xn) xn −

˙q (t)

r (t)

⊤

#

(1)
=

−

˙hm (t)⊤ r (t)

η

−

ℓ′(w(t)⊤xn) x⊤n r (t)

M

m=1
X

+
m

Xn
∈S

η

[
−

∇L
N

=

η

"−

Xn=1
M

m=1
X
M

+

η
"

Xm=1 Xn
∈S

m

ℓ′(w(t)⊤xn) xn −

−

M

Xm=1

ˆwm

t

Q

1
1
m
r=1 log◦

−

r (t) #

⊤

r (t) ,

(86)

where in (1) recall from eq. 59 that

Sm,

+
m are mutually exclusive and
S

M
m=1Sm ∪ S

∪

+
m = [N ].

Next we upper bound the three terms in eq. 86.
To bound the ﬁrst term in eq. 86 we use Cauchy-Shartz, and eq. 84.

˙hm (t)⊤ r (t)

M

≤

˙hm (t)
(cid:13)
(cid:13)
(cid:13)

m=1 (cid:13)
X
(cid:13)
(cid:13)

r (t)
k

k ≤

M C ′h

t

m
2
r=1 log◦

−

r (t)

log◦

(m

1) (t)

−

r (t)
k

2 k

(cid:16)Q
In bounding the second term in eq. 86, note that for tight exponential tail loss, since w (t)⊤ xn →
, for large enough t0, we have
≤
≤
w (t)⊤ xn) for all t > t0. The ﬁrst term in eq. 86 can be bounded by the following set of

µ+w (t)⊤ xn)) exp(

ℓ′(w (t)⊤ xn)

w (t)⊤ xn)

(1+exp(

(cid:17) (cid:16)

−

−

−

(cid:17)

∞
2 exp(

M

m=1
X

−

31

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

inequalities, for t > t0,

m=1
X

+
m: x⊤
Xn
n
∈S

r(t)

0

≥

ℓ′(w (t)⊤ xn) x⊤n r (t)

−

ˆw⊤l xn log◦

l (t) + x⊤n hl(t)
i

−

x⊤n r (t)

x⊤n r (t)

!

M

η

−

+
m

Xn
∈S

m=1
X
M

ℓ′(w (t)⊤ xn) x⊤n r (t)

M

η

≤

exp

 −

exp

 −

M

Xl=1 h
M

Xl=1 h

+
Xm=1 Xn
m: x⊤
n
∈S
M

r(t)

0

≥

+
m: x⊤
Xn
n
∈S

r(t)

0

≥

m=1
X
M

+
m max
+
n
m(cid:12)
∈S
(cid:12)
(cid:12)
(cid:12)
m−1

t(

Q

S

m=1 (cid:12)
X
(cid:12)
(cid:12)
(cid:12)
M
m=1

P

exp (M

xnk
k

Ch) exp

+
m

(cid:16)

2η

|S

exp

M max
|
k=1 log◦k(t))(log◦m−1(t))θm
xn
k

2η

exp(maxn
tθ1

1 |

|S

+

 −

n∈S

+
m k

(cid:18)

Q
Ch)
k

ˆw⊤l xn log◦

l (t) + x⊤n hl (t)
i

!

M

ˆw⊤l xn log◦

l (t)

!

Xl=1
xn
Ch
k
(cid:17)
M −1
k=m (log◦m(t))

ˆw⊤
k

xn

(cid:19)

if M > 1

if M = 1

L1.

∈

(87)

(1)

≤

(2)

≤

2η

2η

(3)

≤

2η

(4)

≤






where in (1) we used eqs. 78 and 79, in (2) we used that
used eq. 83 and in (4) we denoted θm = minn
on Lemma 16.

∈S

+
m

x : xe−

0, (3) we
ˆw⊤mxn > 1 and the last line is integrable based

1 and x⊤n r (t)

≤

≥

∀

x

Next, we bound the last term in eq. 86. For exponential tailed losses (Assumption 3), since
, µ+ > 0, t

, we have positive constants µ

and t+ such that

n

−

−

∀

w(t)⊤xn → ∞

t > t+ :

w (t)⊤ xn

1 + exp

µ+w (t)⊤ xn

w (t)⊤ xn

exp

−

µ

−

−

w (t)⊤ xn

≤

≥

(cid:17)

(cid:17)

(cid:16)
1

(cid:16)

−

(cid:16)

(cid:16)

w (t)⊤ xn

w (t)⊤ xn

exp

exp

−

−

(cid:16)

(cid:16)

(cid:17)(cid:17)

(cid:17)(cid:17)

(cid:17)

(cid:17)

∀

∀

t > t

:

−

ℓ′

ℓ′

−

−

(cid:16)

(cid:16)

We deﬁne γn(t) as

γn(t) =

(1 + exp(
exp(
(1

−

−
−

(

−

µ+w(t)⊤xn)
w(t)⊤xn)
µ

if r (t)⊤ xn ≥
0
if r (t)⊤ xn < 0

.

(88)

This implies t > max (t+, t

),

ℓ′(w(t)⊤xn) x⊤n r (t)

γn(t) exp

w⊤ (t) xn

xn.

−

−

≤

−

(cid:0)

(cid:1)

32

GRADIENT DESCENT ON SEPARABLE DATA

From this result, we have the following set of inequalities:

ℓ′(w(t)⊤xn) x⊤n r (t)

γn(t) exp

w (t)⊤ xn

x⊤n r (t)

M

η

≤

m=1
X

m

Xn
∈S
l (t) + x⊤n ˜wl +

−

(cid:16)
1
−

l

x⊤n r (t)

x⊤n r (t)

(cid:17)
x⊤n ˇwk,l
1
l
r=k log◦
−
M
m

M

η

−

m=1
X
M

Xn
∈S

m

m=1
X
M

Xn
∈S

m

γn(t) exp

ηγn(t) exp

(1)
= η

(2)
=

(3)
=

≤

m=1
X
M

Xn
∈S

m

m=1
X
M

Xn
∈S

m

Xm=1 Xn
∈S

m

M

ˆw⊤l xn log◦

 −

Xl=1 "
x⊤n ˜w
exp
−
1
m
r=1 log◦
t
(cid:0)
(cid:0)

−
(cid:1)

−
r (t)

Q
x⊤n ˜w
exp
−
1
m
r=1 log◦
t
(cid:0)
(cid:0)

−
(cid:1)

−
r (t)

Q
x⊤n ˜w
exp
−
m
1
r=1 log◦
t
(cid:0)
(cid:0)

−
(cid:1)

−
r (t)

+ exp

 −

M

1

−

Xl=m

Q
x⊤n ˇwm,l+1
l
r=m log◦

1

r (t) ! −  

−

(cid:1)

(cid:1)

(cid:1)
M

1

−

Xl=m

ηγn(t) exp

x⊤n r (t)

x⊤n r (t)

ηγn(t) exp

x⊤n r (t)

x⊤n r (t)

x⊤n r (t)

x⊤n r (t)

r (t) # −

!

Xk=1

exp

Q
 −

exp

 −

x⊤n ˇwk,l
1
l
r=k log◦
−

r (t) !

Xl=k+1

Xk=1
M

Xl=m+1
M
Q
−

Q
x⊤n ˇwm,l
1
l
r=m log◦
−
1

ψm (t)

r (t) !

x⊤n ˇwm,l+1
l
r=m log◦

r (t) !

ψm (t)

1

−

" 

x⊤n ˇwm,l+1
l
r=m log◦

r (t) !#

Xl=m

Q

Q
where in (1) we used eqs. 78 and 79, and in (2) we used Pk
if m < k) and in (3) deﬁned

Q

−

1 ˇwk,m = 0 from eq. 65 (so x⊤n ˇwk,l = 0

Note

tψ such that

∃

∀

m

1

−

M

ψm (t) = exp

 −

Xl=k+1
t > tψ, we can bound ψm (t) by

Xk=1

Q

x⊤n ˇwk,l
1
l
r=k log◦
−

.

r (t) !

exp

−

 

M maxn k
log◦

(m

xnk
Ch
1) (t) ! ≤

−

ψm (t)

1 .

≤

Thus, the third term in 86 is given by

M

η

−

m=1
X
M

Xn∈Sm

(1)

≤

m=1
X

Xn∈Sm

ℓ′(w(t)⊤xn) x⊤

n r (t)

M

ˆw⊤
mr (t)
m−1
r=1 log◦r (t)

x⊤
n r (t)

−

t

m=1
X
x⊤
Q
n r (t)
exp
−
r=1 log◦r (t)
(cid:0)

m−1
(cid:1)

(cid:1)

ηγn(t) exp

x⊤
n ˜w

−
t
(cid:0)

M−1

Xl=m
ηγn(t) exp

Q

1
−  

−

M

+

m=1 "
X

−

t

Xn∈Sm
r(t)⊤ ˆwm
,
m−1
r=1 log◦r (t) #

Q
x⊤
n ˇwm,l+1
l
r=m log◦r (t) !#
x⊤
x⊤
n r (t)
n ˜w
exp
−
r=1 log◦r (t)
(cid:0)

−
t
(cid:0)

m−1
(cid:1)

(cid:1)

Q

where (1) follows from the bound in eq. 89.

Q

33

ψm (t)

exp

"

 −

x⊤
n ˇwm,l+1
l
r=m log◦r (t) !

x⊤
n r (t)

ψm (t)

1

−

 

x⊤
n ˇwm,l+1
l
r=m log◦r (t) !

M−1

Xl=m

Q

M−1

Xl=m

Q

(89)

(90)

(91)

(92)

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

∀

· "

(1)

≤

(2)

≤

Xn∈Sm

exp

 −

n∈Sm:
X
x⊤
r(t)≥0
n

n∈Sm:
X
r(t)≥0

x⊤
n

We examine the ﬁrst term in eq. 92

M

ηγn(t) exp

m=1
X

Xn∈Sm

x⊤
x⊤
n r (t)
n ˜w
exp
r=1 log◦r (t)
(cid:0)

m−1
(cid:1)

−

(cid:1)

x⊤
n r (t)

ψm (t)

exp

·"

 −

Q
x⊤
n ˇwm,l+1
l
r=m log◦r (t) ! −  

1

−

x⊤
n ˇwm,l+1
l
r=m log◦r (t) ! #

−
t
(cid:0)
M−1

Xl=m

Q

M−1

Xl=m

Q

t > t1 > tψ, where we will determine t1 later. We have the following for all m

[M ]

∈

ηγn(t) exp

x⊤
x⊤
n r (t)
n ˜w
exp
−
r=1 log◦r (t)
(cid:0)

m−1
(cid:1)

−
t
(cid:0)

(cid:1)

x⊤
n r (t)

ψm (t)

M−1

x⊤
Q
n ˇwm,l+1
l
r=m log◦r (t) ! −  
x⊤
n ˜w
ψm (t)
r=1 log◦r (t)
(cid:1)

−
m−1
(cid:0)

Xl=m
ηγn(t) exp
Q
t

1

−

M−1

Xl=m

exp

"

Q
 −

x⊤
n ˇwm,l+1
l
r=m log◦r (t) ! #
x⊤
n ˇwm,l+1
1
l
r=m log◦r (t) ! −  

M−1

Xl=m

Q

ηγn(t) exp

x⊤
n ˜w
r=1 log◦r ψm (t)  

−
(cid:0)

m−1

(t)

(cid:1)

t

2

Q
x⊤
n ˇwm,l+1
l
r=m log◦r (t) !

L1 ,

∈

x⊤
n ˇwm,l+1
l
r=m log◦r (t) !#

M−1

−

Xl=m

Q

(93)

Q
where we set t1 > 0 such that

t > t1 the term in the square bracket is positive and

in (1) we used that since e−

x

x

∀

≥ −

1 we have that e−

x

1

≥
−

≤

x⊤n ˇwm,l+1
l
r=m log◦

r (t)

>

1 ,

−

Xl=m
1
x + x2 and ψm (t)

−

Q
x, and also from using e−

xx
1 from eq. 91.

≤

≤

1 and in (2) we use that

M−1

Xl=m

Q

∀

M

1

−

34

GRADIENT DESCENT ON SEPARABLE DATA

ψm (t)

1

−

 

x⊤
n ˇwm,l+1
l
r=m log◦r (t) ! −

t

x⊤
n ˆwm
m−1
r=1 log◦r (t) #

Q

Q

We examine the second term in eq. 92 using the decomposition of ˆwm from eq. 66

M

ηγn(t) exp

m=1 "
Xn∈Sm
X
M
(1)
=

m=1
X

Xn∈Sm
M

η exp

t

M−1

Xl=m

1

−

(cid:1)
M−1

x⊤
n r (t)

ψm (t)

(cid:1)

x⊤
n r (t) ψm(t)

x⊤
n r (t)

(cid:1)

m−1
(cid:1)

x⊤
n ˜w

−
t
(cid:0)

x⊤
n r (t)
exp
−
r=1 log◦r (t)
(cid:0)
x⊤
x⊤
Q
n ˜w
n r (t)
−
m−1
r=1 log◦r (t)
(cid:0)
(cid:1)
x⊤
n ˜w
t

γn(t) exp

−
(cid:0)
x⊤
n r (t)
exp
−
m−1
r=1 log◦r (t)
(cid:0)
x⊤
n r (t) x⊤
x⊤
Q
n ˜w
m−1
r=1 t log◦r (t)
(cid:1)

n ˇwk,m

−
(cid:0)

η exp

−

(cid:0)

(cid:1)

(cid:0)

(cid:1)

Q
ηγn(t) exp

m=1
X
M

Xn∈Sm
m−1

m=1
X

Xk=1 Xn∈Sk
η exp

m=1
X

Xn∈Sm
M

t

x⊤
x⊤
Q
n ˜w
n r (t)
−
m−1
r=1 log◦r (t)
(cid:0)
(cid:1)
M−1
Q

ηγn(t) exp

m=1
X
M

Xn∈Sm

Xl=m
M−1

Xk=1 Xn∈Sk

Xm=k

η exp

−
(cid:0)

γn(t) exp

x⊤
n r (t)

ψm (t)

1

−

−
(cid:0)

(cid:1)

exp

−
(cid:0)

(cid:0)
x⊤
n ˜w

(cid:1)
n r (t) ψm(t)x⊤
x⊤
x⊤
n r (t)
−
l
r=1 log◦r (t)
(cid:1)
(cid:0)
x⊤
n r (t) x⊤
x⊤
n ˜w
n ˇwk,m+1
Q
m
r=1 t log◦r (t)

(cid:1)

t

n ˇwm,l+1

x⊤
n ˇwm,l+1
l
r=m log◦r (t)

Xl=m

Q

1
m−1

Q
r=1 log◦r (t) −

x⊤
n ˇwm,k+1
k
r=1 log◦r (t) #

(cid:1)
M−1

t

Xk=m

Q

η exp

x⊤
n ˜w

γn(t)ψm (t) exp

x⊤
n r (t)

1

x⊤
n r (t)

−
(cid:0)

(cid:1) (cid:0)

−
(cid:0)

−

(cid:1)

(cid:1)

(94)

where in (1) we used eq. 66, in (2) we re-arranged the order of summation in the last term, and in
(3) we just use a change of variables.

Next, we examine Γm,n(t) for each m and n

∈ Sm in eq. 94. Note that,

∃

t2 > tψ such that

x⊤n ˇwm,k+1
k
r=1 log◦

M

1

−

t

Xk=m

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Q

≤

t

r (t) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Q

0.5
m
1
r=1 log◦

−

.

r (t)

κ(n, t)
1
m
r=1 log◦

−

r (t) #

exp

x⊤n ˜w

γn(t)ψm(t) exp

−

(cid:16)

(cid:17) (cid:16)

x⊤n r (t)
(cid:17)

−

(cid:16)

−

(cid:17)

1

x⊤n r (t) ,

(95)

where in (1) follows from the deﬁnition of t2, wherein

κn(t) =

1.5 if
0.5 if

γn(t)ψm(t) exp
γn(t)ψm(t) exp

(cid:0)
(cid:0)
1. First, if x⊤n r (t) > 0, then γn(t) = (1 + exp(
−

(cid:0)
(cid:0)

(cid:26)

x⊤n r (t)
−
x⊤n r (t)
(cid:1)
−
(cid:1)
µ+w(t)⊤xn)) > 0.

1
1
(cid:1)
(cid:1)

−
−

x⊤n r (t) > 0
x⊤n r (t) < 0

.

We further divide into two cases. In the following C0, C1 are some constants independent of t.

35

−

+

M

−

+

M

(2)
=

(3)
=

:=

t

Xn∈Sm "

m=1
X
M

Q
Γm,n(t),

m=1
X

Xn∈Sm

t > t2 we have

∀

In this case,

t > t2

∀

(1)

≤

η

Γm,n(t)

t

"

Q

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

(a) If

x⊤n r (t)

> C0t−

0.5µ+ , then we have the following

l(t) + h⊤l xn

exp

x⊤n r (t)

!!
i

−

(cid:16)

(cid:17)

(cid:12)
(cid:12)

γn(t)ψm(t) exp

(cid:12)
(cid:12)

(cid:16)

−

x⊤n r (t)
(cid:17)
ˆw⊤l xn log◦

M

µ+

(1)

≤  

1 + exp

(2)

≤ 

1 +

 −

Xl=1 h
exp(µ+Ch k
m
1
r=1 log◦

−

t

(cid:16)


Q
µ+
1 + C1t−

1

−

(3)

≤

(cid:0)
1

≤

−

C0t−

0.5µ+)

exp(

)
µ+ 

xnk
r (t)
(cid:17)
0.5µ+ + 0.5C 2
C0t−



−

µ+

0 t−

t > t′+

,

∀

0.5µ+

(cid:1) (cid:0)

C0t−

1 + C1t−

µ+

+ 0.5C 2

0 t−

µ+

(cid:1)
1 + C1t−

µ+

(4)

≤

1,

t > t′′+,

∀

(96)

(cid:0)

(cid:1)

≤

where in (1), we use ψm(t)
from eq. 83, in (3) for some large enough t′+ > t+, we have
for the second term we used the inequality e−
asymptotically for t > t′′+ for large enough t′′+ > t′+ as C0t−
0.5C 2
Thus, using eq. 96 in eq. 95,

(cid:1)
1 from eq. 91 and using eq. 78, in (2) we used bound on hm
xn
exp(µ+Chk
C1, and
r=1 log◦r(t))µ+ ≤
x + 0.5x2 for x > 0, and (4) holds
Q
0.5µ+ converges slower than

t > max (t2, t′′+), we have

µ+ to 0.

0 t−

m−1

)
k

≤

−

1

(cid:0)

(

x

∀

Γm,n(t)

x⊤n ˜w
ηκ(n, t) exp
−
m
1
r (t)
r=1 log◦
(cid:0)

−

t

≤ "

#

(cid:1)

(cid:16)

γn(t)ψm(t) exp

0.5µ+, then we have the following: ψm(t)

, for large enough t > t′′′+, γn(t) =

≤

Q
(b) If 0 < x⊤n r (t) < C0t−
1 as x⊤n r (t) > 0, and since w(t)⊤xn → ∞
2
x⊤n r (t)
This gives us,
γn(t)ψm(t) exp
this in eq. 95,
t > max (t2, t′+)
(cid:0)

−

(cid:0)
∀

1

x⊤n r (t)

x⊤n r (t)

≤

≤

−

x⊤n r (t)
(cid:17)

−

(cid:16)

−

1

x⊤n r (t)

0

≤

(cid:17)
1 from eq. 91, exp

x⊤n r (t)

−
1 + exp
(cid:0)

≤

µ+w(t)⊤xn
(cid:1)

C0t−

(cid:0)
0.5µ+, and using

−
(cid:0)

≤

(cid:1)(cid:1)

(cid:1)

(cid:1)
x⊤n ˜w
ηκ(n, t) exp
−
m
1
r (t)
r=1 log◦
(cid:0)

−

t

#

(cid:1)

Γm,n(t)

≤ "

0.5µ+

C0t−

L1.

∈

∈

2. Second, if x⊤n r (t)
≤
into following special cases.

0, then γn(t) = (1

Q

exp(

µ

w(t)⊤xn))

−

−

−

(0, 1). We again divide

(a) If

x⊤n r (t)

C0

log◦

(m

1)(t)

−

−

0.5˜µ−

, where ˜µ

= min (µ

, 1), then we have

−

−

(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
Γm,n(t)

(cid:16)

≤ "

1.5η exp
−
m
1
r=1 log◦
(cid:0)

−

t

(cid:17)
x⊤n ˜w
r (t) #
(cid:1)

1

−

(cid:16)

(1)

≤ "

1.5η exp
−
m
2
r=1 log◦
(cid:0)

x⊤n ˜w
Q
r (t) #
(cid:1)

−

t

C0

log◦

(m

−

1

0.5˜µ−

−

−

L1.

∈

1)(t)
(cid:17)

Q
where in (1) we used that

x⊤n r (t)

C0

log◦

(m

−

≤

(cid:12)
(cid:12)

(cid:16)

(cid:12)
(cid:12)

(cid:16)

0.5˜µ−

−

.

1
−
1)(t)
(cid:0)
(cid:17)

36

γn(t)ψm (t) exp

x⊤n r (t)

< 1 and

−
(cid:0)

(cid:1)(cid:1)

γn(t)ψm (t) exp

x⊤n r (t)

−

(cid:16)

x⊤n r (t)
(cid:12)
(cid:12)
(cid:12)

(cid:17)(cid:17) (cid:12)
(cid:12)
(cid:12)

GRADIENT DESCENT ON SEPARABLE DATA

(b) If ψm (t) exp

x⊤n r (t)

< 1, then, from eq. 91

−

(cid:0)

(cid:1)

−

M maxn k
log◦

xnk
1) (t)

(m

−

Ch

≤

log ψm (t) < x⊤n r (t) .

(97)

In this case, since γn(t) = 1
1, and hence
1.5, and we have

−
γn(t)ψm (t) exp

exp(

−
(cid:0)

(cid:0)

(cid:1)

(cid:1)

w(t)⊤xn) < 1, we also have γn(t)ψm (t) exp
x⊤n r(t) > 0. Thus,

x⊤n r (t)
t > t2, in 95, κn(t) =
(cid:1)
(cid:0)

−
x⊤n r (t)

−

1

−

∀

<

Γm,n(t)

≤ "

(1)

≤ "

1.5η exp
−
m
1
r=1 log◦
(cid:0)

−

t

t

−

1

x⊤n ˜w
1.5η exp
−
m
1
r (t) #
r=1 log◦
(cid:1)
(cid:0)
(cid:16)
xnk
Ch
M maxn k
1) (t) ≤
log◦

x⊤n ˜w
Q
r (t) #
(cid:1)

−

(m

−

γn(t)ψm (t) exp

x⊤n r (t)

(cid:17)(cid:17) (cid:12)
(cid:12)
(cid:12)

x⊤n r (t)
(cid:12)
(cid:12)
(cid:12)
2 ∈

1) (t)

L1,

−

(cid:16)

C2

(cid:16)

t

m
2
r=1 log◦

−

r (t)

log◦

(m

−

(cid:17)

x⊤n r(t)
|
|

=

Q

where (1) follows from
x⊤n r(t) from eq. 97.

1

−

−

(cid:0)
x⊤n r (t)

(c) If ψm (t) exp

Q

−
(cid:0)
> C0

γn(t)ψm (t) exp

x⊤n r (t)

< 1 and the bound on

(cid:1)(cid:1)
(m

0.5˜µ−

−

−

(cid:1)

−

log◦

1)(t)
(cid:17)

→ ∞

> 1, and

and ψm(t)

x⊤n r (t)
Since, x⊤n w(t)
1 from eq. 90, for large enough t′
(cid:16)
(cid:12)
(cid:0)
→
(cid:12)
−
ψm(t) > 0.5 and γn(t) = (1
exp(
−
ily large constant. For all t > τ , if exp
0.25τ
(cid:0)
On the other hand, if there exists t > τ
constants C1, C2 we have the following
x⊤n r (t)
x⊤n r (t)) = exp(
|

(cid:12)
(cid:12)
x⊤n w(t))) > 0.5. Let τ > max (4, t′
−
x⊤n r (t)

4, such that exp

−
(cid:0)
−

x⊤n r (t)

(i) exp(

1 + C0

(cid:1)
0.5˜µ−

1)(t)

log◦

−
−

> τ

> t

)
|

≥

≥

≥

−

≥

1.

(m

µ

(cid:1)

−

4, then γn(t)ψm(t) exp

, where ˜µ= min (1, µ
,
t > t′
∀
−
) be an arbitrar-

, we have

−

).

−

x⊤n r (t)

>

−

(cid:1)
< τ , then for some

(cid:0)

, since ex > 1 +

(ii) ψm(t)

exp

1)(t)
C1
−
and again using ex > 1 + x for all x,
(cid:17)

log◦

≥

(cid:18)

(m

(cid:16)

−

≥

(cid:19)

(cid:18)

−

(cid:18)
1

−

(cid:16)

1

C1

log◦

(m

−

from eq. 91

(cid:17)

(cid:19)

1

−

1)(t)
(cid:17)

(cid:19)

−
x for all x,

(iii)

(cid:16)

µ−

γn(t) =

1

 

− "

exp(

−

hl(t)⊤xn) exp
m
1
r=1 log◦
−
µ−

t

−
r(t)
(cid:0)

x⊤n r (t)

1

≥  

− "

)τ
xnk
Chk
exp(
Q
−
m
1
r(t) #
r=1 log◦
−

t

! ≥

(cid:18)

1

−

(cid:16)

#

(cid:1)

!

C2 log◦

(m

−

Q

µ−

−

1)(t)
(cid:17)

,

t > t′′
−

∀

(cid:19)

where the last inequality follows as for large enough t′′
−
C2.

> t′
−

, we have exp(
xn
−
m−2
r=1 log◦r(t) ≤

Chk

)τ

k

t

Q

37

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Using the above inequalities, we have

γn(t)ψm(t) exp

x⊤
n r (t)

−
(cid:0)

≥
(1)

(cid:18)

≥

1 + C0

1 + C0

(cid:16)
C0C2

−

−1

−0.5˜µ−

(cid:1)
log◦(m−1)(t)
(cid:17)
−0.5˜µ−

(cid:16)
log◦(m−1)(t)
(cid:17)
log◦(m−1)(t)
(cid:17)

(cid:16)

(cid:19) (cid:18)

C1

−
−µ−0.5˜µ−

1

−1

−

C1

log◦(m−1)(t)
(cid:17)
C2

(cid:16)
log◦(m−1)(t)
(cid:17)
(cid:16)
log◦(m−1)(t)
(cid:17)
(cid:16)

C0C1

−

−

log◦(m−1)(t)
(cid:17)

−µ−

−µ−

(cid:19)

−

C2

1
(cid:19) (cid:18)
(cid:16)
log◦(m−1)(t)
(cid:16)
(cid:17)
−1−0.5˜µ− (2)

1,

t > t′′′
− ,
∀

≥

(98)

where in (1) we dropped the other positive terms, and (2) follows for large enough t′′′
−

> t′′
−
converges to 0 more slowly than the other negative terms.

as the C0 log
Finally, using eq. 98 in eq. 95, we have for all t > max (t2, τ, tψ, t′′′
−

1)(t)
(cid:17)

log◦

0.5˜µ−

(m

(cid:16)

−

−

)

Γm,n(t)

x⊤n ˜w
ηκ(n, t) exp
−
m
1
r (t)
r=1 log◦
(cid:0)

−

t

≤ "

1

−

#

(cid:1)

(cid:16)

γn(t)ψm(t) exp

x⊤n r (t)

−

(cid:16)

Q

x⊤n r (t)
(cid:12)
(cid:12)
(cid:12)

(cid:17)(cid:17) (cid:12)
(cid:12)
(cid:12)

0

≤

(99)

Collecting all the terms from the above special cases, and substituting back into eq. 85, we note
L1, thus proving

, where f (t)

that all terms are either negative, in L1, or of the form f (t)
the lemma.

r (t)
k

∈

k

C.4 Proof of the existence and uniqueness of the solution to eqs. 62-63

We wish to prove that

m

∀

≥

1 :

such that

exp

m

 −

Xk=1

Xn
∈S

m

˜w⊤k xn

¯Pm
−

!

1xn = ˆwm ,

Pm

1 ˜wm = 0 and ¯Pm ˜wm = 0,

(100)

(101)

we have a unique solution. From eq. 101, we can modify eq. 100 to

−

m

exp

Xn
∈S

m

 −

Xk=1

˜w⊤k

¯Pk

1xn

−

¯Pm
−

!

1xn = ˆwm , .

To prove this, without loss of generality, and with a slight abuse of notation, we will denote
m
1
S1, ¯Pm
k=1 ˜w⊤k

Sm as
, so we can write the above equation as

1xn as xn and βn = exp

1xn

¯Pk

−

−

−

−

In the following Lemma 17 we prove this equation

(cid:16)

Xn
∈S

1

P
xnβn exp

(cid:17)
x⊤n ˜w1

(cid:17)

−

(cid:16)

= ˆw1

β

∀

∈

1

R|S

|>0 .

38

GRADIENT DESCENT ON SEPARABLE DATA

Lemma 17

β

∀

∈

R|S

1

|>0 we can ﬁnd a unique ˜w such that

xnβn exp

x⊤n ˜w1

= ˆw1

−

1

Xn
(cid:16)
∈S
1 = 0 we would have ˜w⊤1 z = 0.
Rd

(cid:17)

and for

z

Rd such that z⊤X
S

∀

∈
Proof Let K = rank (X
S
(i.e., UU⊤ = U⊤U = I) such that u1 = ˆw1/

1). Let and U = [u1, . . . , ud]
, and

∈

ˆw1k

k

while

z

= 0,

n

∈ S1 : z⊤ [u1, . . . , uK]⊤ xn 6

= 0 ,

∀

∀

i > K :

∀

n

∀

∈ S1 : u⊤i xn = 0 .

d be a set of orthonormal vectors

×

(102)

(103)

(104)

K



−



Xj=1

sjvj,n


K



−



Xj=1

sjvj,n


K

sjvj,n
Xj=1

∈ S1. Given
n

In other words, u1 is in the direction of ˆw1, [u1, . . . , uK ] are in the space spanned by the columns
of X
S
We deﬁne vn = U⊤xn and s = U⊤ ˜w1. Note that

1, and [uK+1, . . . , ud] are orthogonal to the columns of X
S

1.
∈ S1 from eq. 104,
i > K : vi,n = 0
n
1 = 0 we would have ˜w⊤1 z = 0. Lastly,

i > K : si = 0, since for

∀
Rd such that z⊤X
S

∀

z

∀

∈

and
equation 102 becomes

∀

xnβn exp

= ˆw1 .

(105)

Multiplying by U⊤ from the left, we obtain

Xn
∈S

1

i

∀

≤

K :

Xn
∈S
, we have that

1

Since u1 = ˆw1/

ˆw1k

k

vi,nβn exp

= u⊤i ˆw1 .

i

∀

≤

K :

vi,nβn exp



−

=

ˆw1k
k

δi,1 .

(106)

We recall that v1,n = ˆw⊤1 xn/
i = 1,

k

1

Xn
∈S
ˆw1k

= 1/


ˆw1k
,

∀

k

K
j=2, we examine eq. 106 for

sj}

{

exp

s1
ˆw1
k

−

(cid:18)

k (cid:19)

Xn
∈S

1

βn exp

K



−



Xj=2

sjvj,n




=

ˆw1k
k

2 .

This equation always has the unique solution

s1 =

ˆw1k
k

log

ˆw1k


k

2

−

βn exp

Xn
∈S

1

given

sj}

{

K
j=2. Next, we similarly examine eq. 106 for 2

K

,

sjvj,n

Xj=2


K as a function of si



−


i
≤

K

≤

(107)

(108)







βnvi,n exp

s1/

ˆw1k −

k



−

Xn
∈S

1

= 0 .

sjvj,n


Xj=2



39

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

multiplying by exp (s1/

) we obtain

ˆw1k

k

0 =

βnvi,n exp

Xn
∈S

1

K



−



Xj=2

sjvj,n


=

∂
∂si

−

[E (s2, . . . , sK)] ,

where we deﬁned

E (s2, . . . , sK) =

βn exp

Xn
∈S

1

K



−



Xj=2

.

sjvj,n


Therefore, any critical point of E (s2, . . . , sK) would be a solution of eq. 108 for 2
K,
and substituting this solution into eq. 107 we obtain s1. Since βn > 0, E (s2, . . . , sK ) is a convex
function, as positive linear combination of convex function (exponential). Therefore, any ﬁnite
critical point is a global minimum. All that remains is to show that a ﬁnite minimum exists and that
it is unique.

≤

≤

i

1 αnxn . Multiplying this equation

n

∈S

From the deﬁnition of

by U⊤ we obtain that

S1,
∈

α

∃

1

R|S
α
∃
R|S
1
|>0 such that 2

|>0 such that ˆw1 =
i

K

∈

P

≤
≤
vi,nαn = 0 .

Therefore,

(s2, . . . , sK)

∀

Xn
∈S
= 0 we have that

1

K

Recall, from eq. 103 that
Therefore, eq. 110 implies that

∀



1

Xn
∈S


(s2, . . . , sK)
n

Xj=2
= 0,
n
∃
∈ S1 such that

∃

αn = 0 .

sjvj,n

∈ S1 :
K
j=2 sjvj,n > 0 and also

j=2 sjvj,n 6

K

P

= 0, and that αn > 0.
∈ S1 such that

m

∃

K
j=2 sjvj,m < 0.
P
Thus, in any direction we take a limit in which

K, we obtain that
si| → ∞ ∀
|
P
, since at least one exponent in the sum diverge. Since E (s2, . . . , sK), is
E (s2, . . . , sK)
a continuous function, it implies it has a ﬁnite global minimum. This proves the existence of a ﬁnite
solution. To prove uniqueness we will show the function is strictly convex, since the hessian is
(strictly) positive deﬁnite, i.e., that the following expression is strictly positive:

→ ∞

≤

≤

2

i

(109)

(110)

K

K

Xi=2

Xk=2

qiqk

∂
∂si

∂
∂sk

K

K

E (s2, . . . , sK) .

βn

qivi,n

qkvk,n

exp

Xn
∈S

1

 

Xi=2
K

!  

Xk=2

2

!

K

=

=

βn

 

qivi,n

exp

!



−

Xn
∈S

the last expression is indeed strictly positive since
∀
103. Thus, there exists a unique solution ˜w1.

Xi=2

1

Xj=2
= 0,
q

K

Xj=2



−

sjvj,n


.


sjvj,n

∈ S1 :

n

∃

K

j=2 qjvj,n 6

= 0, from eq.

P

40

GRADIENT DESCENT ON SEPARABLE DATA

C.5 Proof of the existence and uniqueness of the solution to eqs. 64-65

Lemma 18 For

m > k

1, the equations

∀

≥

exp

˜w⊤xn

Pm

1xn =

−

(cid:16)

−

(cid:17)

Xn
∈S

m

m

1

−

Xk=1

Xn
∈Sk





exp

˜w⊤xn

ˇwk,m

(111)

−

(cid:16)

xnx⊤n 


(cid:17)

under the constraints

have a unique solution ˇwk,m.

Pk

1 ˇwk,m = 0 and ¯Pk ˇwk,m = 0

−

Proof For this proof we denote X
projection matrix Qk = Pk ¯Pk

Sk as the matrix which columns are
1where QkQm = 0
∀

xn|
n
∈ Sk}
{
= m, Qk ¯Pm = 0
k < m, and
∀

k

−

, the orthogonal

m : I = Pm + ¯Pm =

Qk + ¯Pm

∀

m

We will write ˇwk,m = Wk,muk,m , where uk,m ∈
such that QkWk,m = Wk,m, so

Xk=1
Rdk and Wk,m ∈

Rd

×

dk is a full rank matrix

ˇwk,m = Qk ˇwk,m = QkWk,muk,m .

and, furthermore,

rank

X⊤
Sk
m : ¯PmPm = 0 and

h

QkWk,m

= rank

Qk

= dk .

X⊤
Sk
¯Pm+kxn = 0. Therefore,

k

∀

Recall that
Pk

i
v
1,
∀
∀
1Qkv = 0, ¯PkQkv = 0. Thus, ˇwk,m eq. 114 implies the constraints in eq. 112 hold.
−
Next, we prove the existence and uniqueness of the solution ˇwk,m for each k = 1, . . . , m
separately. We multiply eq. 111 from the left by the identity matrix, decomposed to orthogonal
projection matrices as in eq. 113. Since each matrix projects to an orthogonal subspace, we can
solve each product separately.

(cid:16)
∈ Sm

Rd ,

≥

∈

n

(cid:17)

∀

The product with ¯Pm is equal to zero for both sides of the equation. The product with Qk is

equal to

(112)

(113)

(114)

(115)

Xn
∈S

m

−

(cid:16)

(cid:17)

−

exp

˜w⊤xn

QkPm

1xn =

exp

˜w⊤xn

ˇwk,m .





Xn
∈Sk

−

(cid:16)

Qkxnx⊤n 


(cid:17)

Substituting eq. 114, and multiplying by W⊤k,m from the right, we obtain

exp

˜w⊤xn

W⊤k,mQkPm
−

1xn =

Xn
∈S

m

−

(cid:16)

(cid:17)


Xn
∈Sk


exp

˜w⊤xn

−

(cid:16)

W⊤k,mQkxnx⊤n QkWk,m


(cid:17)

uk,m .

(116)

41

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Denoting Ek ∈
square bracket in the left hand side can be written as

R|Sk|×|Sk| as diagonal matrix for which Enn,k = exp

1
2 ˜w⊤xn

, the matrix in the

−

(cid:0)

(cid:1)

W⊤k,mQkX

Sk EkEkX⊤
Sk

QkWk,m .

(117)

Since rank

AA⊤

= rank (A) for any matrix A, the rank of this matrix is equal to

(cid:0)

(cid:1)

rank [EX

SkQkWk,m]

(1)
= rank [X

Sk QkWk,m]

(2)
= dk

where in (1) we used that Ek is diagonal and non-zero, and in (2) we used eq. 115. This implies
dk matrix in eq. 117 is full rank, and so eq. 116 has a unique solution uk,m. Therefore,
that the dk ×
there exists a unique solution ˇwk,m.

C.6 Proof of Lemma 15

Lemma 15 Let φ (t) , h (t) , z (t) be three functions from N to R
constants. Then, if

, and

C1 <

∞t=1 h (t)

≥

0, and C1, C2, C3 be three positive

≤

P

∞

≤

φ2 (t + 1)

z (t) + h (t) φ (t) + φ2 (t)

we have

(74)

(75)

φ2 (t + 1)

C2 + C3

z (u)

≤

t

u=1
X

Proof We deﬁne ψ (t) = z (t) + h (t), and start from eq. 74

φ2 (t + 1)

z (t) + h (t) φ (t) + φ2 (t)
1, φ2 (t)

z (t) + h (t) max
z (t) + h (t) + h (t) φ2 (t) + φ2 (t)
(cid:2)
ψ (t) + (1 + h (t)) φ2 (t)

(cid:3)

+ φ2 (t)

≤

≤

≤

≤

≤

ψ (t) + (1 + h (t)) ψ (t

1) + (1 + h (t)) (1 + h (t

ψ (t) + (1 + h (t)) ψ (t

≤
+ (1 + h (t)) (1 + h (t

−
1)) (1 + h (t

1) + (1 + h (t)) (1 + h (t

2)) φ2 (t

2)

−

−

−

−

1)) φ2 (t
1)) ψ (t

1)

−

2)

−

−

−

42

GRADIENT DESCENT ON SEPARABLE DATA

we keep iterating eq. 74, until we obtain

1

t
−

Ym=1

exp

≤ "

≤ "

1

t
−

 

m=1
X

(1 + h (t

m))

φ (t1) +

−

#

(1 + h (t

m))

ψ (t

k)

−

#

−

t1

t
−

k

1

−

Xk=0 "

1

t
−

Ym=0

k

1

−

h (t

m)

φ (t1) +

exp

−

!#

h (t

m)

ψ (t

k)

−

!#

−

Xk=0 "

 

m=1
X

1

t
−

Xk=0
t

u=1
X
t

u=1
X

exp (C)

φ (1) +

ψ (t

k)

exp (C)

φ (1) +

ψ (u)

−

#

#

exp (C)

φ (1) +

(z (u) + h (u))

#

exp (C)

φ (1) + C +

z (u)

t

u=1
X

#

"

"

"

"

≤

≤

≤

≤

Therefore, the Lemma holds with C2 = (φ (1) + C) exp (C) and C3 = exp (C).

43

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Appendix D. Calculation of convergence rates

In this section we calculate the various rates mentioned in section 3.

D.1 Proof of Theorem 5

From Theorems 4 and 13, we can write w (t) = ˆw log t + ρ (t), where ρ (t) has a bounded norm for
almost all datasets, while in zero measure case ρ (t) contains additional O(log log(t)) components
which are orthogonal to the support vectors in
S1, and, asymptotically, have a positive angle with
the other support vectors. In this section we ﬁrst calculate the various convergence rates for the
non-degenerate case of Theorem 4, and then write the correction in the zero measure cases, if there
is such a correction.

First, we calculated of the normalized weight vector (eq. 8), for almost every dataset:

w (t)
w (t)
k
k
=

q

=

ˆw

k

k

r

k

k (cid:18)

=

=

=

1
ˆw

ˆw
ˆw

ˆw
ˆw

ρ (t) + ˆw log t

ρ (t)⊤ ρ (t) + ˆw⊤ ˆw log2 t + 2ρ (t)⊤ ˆw log t

ρ (t) / log t + ˆw

1 + 2ρ (t)⊤ ˆw/

ˆw

2 log t
k

+

ρ (t)
k
k

2 /

ˆw

2 log2 t
k

k
(cid:16)

(cid:19)





(cid:17)
ρ (t)⊤ ˆw
2 log t
ˆw
k

k

+





k
(cid:16)
ρ (t)⊤ ˆw
ˆw

2
k

k

3
2  

(cid:17)

2

−

!

ρ (t)

+ ˆw

1
log t

1

−

2
ρ (t)
k
k
2 
ˆw
2
k

k

1
log2 t

+ O



1
log3 t

(cid:18)
(118)

(cid:19)





ˆw
ˆw

ρ (t)⊤ ˆw
ˆw

1
log t

!

+ O

1
log2 t

(cid:18)

(cid:19)

+

ρ (t)
ˆw

 

k

k

k

+

I

−

k

k

(cid:18)

−

k
k
ˆw ˆw⊤
2
ˆw
k
k

k
k
ρ (t)
ˆw
k

k

(cid:19)

2
k
1
log t

+ O

1
log2 t

,

(cid:19)

(cid:18)

where to obtain eq. 118 we used
fact that ρ (t) has a bounded norm for almost every dataset. Thus, in this case

= 1

4 x2 + O

2 x + 3

1
√1+x

x3

−

1

, and in the last line we used the

(cid:0)

(cid:1)

For the measure zero cases, we instead have from eq. 61, w(t) =

m(t) + ρ(t),
m(t) + ρ(t), such that w(t) =
where
ˆw log(t) + ˜ρ(t) with ˜ρ(t) = O(log log(t)). Repeating the same calculations as above, we have for
the degenerate cases,

is bounded (Theorem 3). Let ˜ρ(t) =

M
m=2 ˆw log◦

M
m=1 ˆw log◦

ρ(t)
k
k

P

P

w (t)
w (t)
k
k

ˆw
ˆw
k

−

= O

1
log t

.

(cid:19)

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

k (cid:13)
(cid:13)
(cid:13)
(cid:13)

k (cid:13)
(cid:13)
(cid:13)
(cid:13)
44

w (t)
w (t)
k
k

ˆw
ˆw

−

k

= O

log log t
log t

(cid:18)

(cid:19)

GRADIENT DESCENT ON SEPARABLE DATA

Next, we use eq. 118 to calculate the angle (eq. 9)

k k

w (t)⊤ ˆw
ˆw
w (t)
k
ˆw⊤
ˆw
k

(cid:18)

k

2

ρ (t)

k

=

1
log t

+ ˆw

1


−

(cid:19)

1
log t

ρ (t)⊤ ˆw
ˆw

2 +
k

k

3
4  

2



ρ (t)⊤ ˆw
ˆw

2
k

k

2

−

!

2
ρ (t)
k
k
2 
ˆw
2
k

k

1
log2 t

+ O

1
log3 t

(cid:18)



(cid:19)





=1 +

2

ρ (t)
ˆw

2
k
2 
k

k
k

 



ρ (t)⊤ ˆw
ˆw
ρ (t)

2

k !

k

for almost every dataset. Thus, in this case

k k



1
4 

1
log2 t

−


+ O

1
log3 t

(cid:18)

(cid:19)

Repeating the same calculation for the measure zero case, we have instead

w (t)⊤ ˆw
ˆw
w (t)
k

k k

k

= O

1
log2 t

(cid:18)

(cid:19)

w (t)⊤ ˆw
ˆw
w (t)
k

k k

k

= O

log log t
log t

2

!

(cid:19)

 (cid:18)

Next, we calculate the margin (eq. 10)

min
n

x⊤n w (t)
w(t)
k
k

= min

x⊤n

n

" 

1
ˆw

−

k
ρ (t)
ˆw
k

k

k

−

=

1
ˆw
k

k  

min
n

x⊤n ρ (t)

−

ˆw
ˆw

k

2

ρ (t)⊤ ˆw
ˆw
k
k
k
ρ (t)⊤ ˆw
ˆw
k

2

!

1
log t

!

+ O

1
log2 t

(cid:18)

(cid:19)#

1
log t

+ O

1
log2 t

(cid:18)

(cid:19)

(119)

(120)

k
for almost every dataset, where in eq. 119 we used eq. 19. Interestingly the measure zero case has
a similar convergence rate, since after a sufﬁcient number of iterations, the O(log log(t)) correction
is orthogonal to xk, where k = argminnx⊤n w(t). Thus, for all datasets,

min
n

x⊤n w (t)

−

1
ˆw
k

k

= O

1
log t

(cid:18)

(cid:19)

Calculation of the training loss (eq. 11):

(w (t))

L

1 + exp

µ+w (t)⊤ xn

exp

w (t)⊤ xn

(cid:17)(cid:17)

−

(cid:16)

≤

=

=

=

N

n=1 (cid:16)
X
N

Xn=1 (cid:16)
N

n=1 (cid:16)
X
1
t

Xn
∈S

−

(cid:16)

−

(cid:16)

(cid:17)

−

(cid:16)

1 + exp

µ+ (ρ (t) + ˆw log t)⊤ xn

exp

(ρ (t) + ˆw log t)⊤ xn

(cid:17)(cid:17)

(cid:17)

1 + t−

µ+ ˆw⊤xn exp

µ+ρ (t)⊤ xn

exp

ρ (t)⊤ xn

t−

ˆw⊤xn

(cid:17)(cid:17)

−

(cid:16)

(cid:17)

ρ(t)⊤xn + O

e−

(cid:16)

−

(cid:16)
max(θ,1+µ+)
t−

.

(cid:17)

45

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

1). Note that the zero measure case has the same behavior,
Thus, for all datasets
since after a sufﬁcient number of iterations, the O(log log(t)) correction has a non-negative angle
with all the support vectors.

(w (t)) = O(t−

L

Next, we give an example demonstrating the bounds above, for the non-degenerate case, are
u, and a single data point x =
0, and obtain the continuous

strict. Consider optimization with and exponential loss ℓ (u) = e−
(1, 0). In this case ˆw = (1, 0) and
time version of GD:

= 1. We take the limit η

ˆw
k

→

k

We can analytically integrate these equations to obtain

˙w1 (t) = exp (

w (t)) ;

˙w2 (t) = 0.

−

w1 (t) = log (t + exp (w1 (0))) ; w2 (t) = w2 (0) .

Using this example with w2 (0) > 0, it is easy to see that the above upper bounds are strict in

the non-degenerate case. (cid:4)

D.2 Validation error lower bound

is a set of indices for validation set samples. We calculate of the validation
Lastly, recall that
loss for logistic loss, if the error of the L2 max margin vector has some classiﬁcation errors on the
validation, i.e.,

: ˆw⊤xk < 0:

V

k

∃

∈ V

Lval (w (t)) =

log

1 + exp

w (t)⊤ xn

−

(cid:16)
w (t)⊤ xk

(cid:17)(cid:17)

1 + exp

(cid:17)(cid:17)
(ρ (t) + ˆw log t)⊤ xk

Xn
∈V
log

(cid:16)
1 + exp

≥
= log

(cid:16)

(cid:16)

= log

exp

(cid:16)

−

−

−

≥ −

(cid:16)
(ρ (t) + ˆw log t)⊤ xk

(cid:17)(cid:17)
1 + exp

(ρ (t) + ˆw log t)⊤ xk

(cid:16)
(ρ (t) + ˆw log t)⊤ xk + log

(cid:16)

(cid:17) (cid:16)
1 + exp

(cid:16)

(ρ (t) + ˆw log t)⊤ xk

(cid:17)(cid:17)(cid:17)

log t ˆw⊤xk + ρ (t)⊤ xk

(cid:16)

(cid:16)

(cid:17)(cid:17)

≥ −
Thus, for all datasets

Lval (w (t)) = Ω(log(t)).

Appendix E. Softmax output with cross-entropy loss

We examine multiclass classiﬁcation. In the case the labels are the class index yn ∈ {
we have a weight matrix W
W⊤
d so that Ak = ek ⊗
(cid:1)

Furthermore, we deﬁne w = vec
RdK

the matrix Ak ∈
d-dimension identity matrix. Note that A⊤k w = wk.

, a basis vector ek ∈
Id, where
⊗

d with wk being the k-th row of W.

RK so that(ek)i = δki, and
is the Kronecker product and Id is the

1, . . . , K

and

RK

∈

}

(cid:0)

×

×

Consider the cross entropy loss with softmax output

(W) =

L

N

−

n=1
X

log

 

w⊤ynxn

exp
K
k=1 exp
(cid:0)

w⊤k xn
(cid:1)

!

(cid:0)

(cid:1)

P

46

GRADIENT DESCENT ON SEPARABLE DATA

Using our notation, this loss can be re-written as

(w) =

L

N

log

−

n=1
X

N

 
K

w⊤Aynxn
exp
K
k=1 exp (w⊤Akxn) !

(cid:1)

(cid:0)

=

log

P

exp

Xn=1

 

Xk=1

(cid:16)

w⊤ (Ak −

Ayn) xn

!

(cid:17)

(121)

Therefore

N

K
k=1 exp

(w) =

∇L

Xn=1 P

K

N

w⊤ (Ak −
Ayn) xn
K
r=1 exp (w⊤ (Ar −
(cid:0)
1

(Ak −
Ayn) xn)
(cid:1)

Ayn) xn

P
r=1 exp (w⊤ (Ar −
If, again, we make the assumption that the data is linearly separable, i.e., in our notation

(Ak −

Ayn) xn .

Ak) xn)

n=1
X

Xk=1

P

=

K

Assumption 4

w

∗

∃

such that w⊤
∗

(Ak −

Ayn) xn < 0

k

= yn.

∀

then the expression

w⊤

∗ ∇L

(w) =

N

K

Xn=1

Xk=1

Ayn) xn

w⊤
∗

(Ak −
r=1 exp (w⊤ (Ar −

K

.

Ak) xn)

is strictly negative for any ﬁnite w. However, from Lemma 10, in gradient descent with an appropri-
L (w (t))
, and
ately small learning rate, we have that
=
∀
∇
Ayn) xn →
Ak) xn → ∞
, which implies
yn,
0 in this case. Thus, we arrive to an

w (t)
k
k → ∞
= yn, maxk w (t)⊤ (Ak −
→

r : w (t)⊤ (Ar −
∃
. Examining the loss (eq. 121) we ﬁnd that
−∞
equivalent Lemma to Lemma 1, for this case:

0. This implies that:

k
(w (t))

→

L

∀

k

P

Lemma 19 Let w (t) be the iterates of gradient descent (eq. 2) with an appropriately small learn-
ing rate, for cross-entropy loss operating on a softmax output, under the assumption of strict linear
, and (3)
separability (Assumption 4), then: (1) limt
n, k

∀
∞
Using Lemma 10 and Lemma 19, we prove the following Theorem (equivalent to Theorem 3) in the
next section:

w (t)⊤ (Ayn −

(w (t)) = 0, (2) limt

→∞ L
Ak) xn =

= yn : limt

→∞ k

w (t)

→∞

∞

=

k

.

Theorem 7 For almost all multiclass datasets (i.e., except for a measure zero) which are linearly
separable (i.e. the constraints in eq. 15 below are feasible), any starting point w(0) and any small
enough stepsize, the iterates of gradient descent on 13 will behave as:

where the residual ρk(t) is bounded and ˆwk is the solution of the K-class SVM:

wk(t) = ˆwk log(t) + ρk(t),

argminw1,...,wk

2 s.t.

wk||
||

n,

k

∀

∀

= yn : w⊤ynxn ≥

w⊤k xn + 1.

K

Xk=1

47

(14)

(15)

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

E.1 Notations and Deﬁnitions
To prove Theorem 7 we require additional notation. we deﬁne ˜xn,k , (Ayn −
notation, we can re-write eq. 15 (K-class SVM) as

Ak)xn. Using this

(122)

(123)

(124)

(125)

w
arg minw k
From the KKT optimality conditions, we have for some αn,k ≥

= yn : w⊤˜xn,k ≥

n,

0,

∀

∀

k

2 s.t.
k

1

ˆw =

N

K

Xn=1

Xk=1

αn,k ˜xn,k1

n

{

∈Sk}

In addition, for each of the K classes, we deﬁne
support vectors).
Using this deﬁnition, we deﬁne X

dK

We also deﬁne

K

,

k=1 Sk and ˜X

Sk ∈ R
K
,

X

Sk .

Sk = arg minn( ˆwyn −

ˆwk)⊤xn (the k’th class

Sk| as the matrix which columns are ˜xn,k,

×|

n

∀

∈ Sk.

×

S

S
RK

k=1
d with wk being the k-th row of W and w = vec(W⊤).
S

S
We recall that we deﬁned W
Similarly, we deﬁne:
1. ˆW
RK
∈
RK
2. P
×
∈
3. ˜W
RK
∈
and ˆw = vec( ˆW⊤), ρ = vec(P⊤), ˜w = vec( ˜W⊤).
Using our notations, eq. 14 can be re-written as w = ˆw log(t) + ρ(t) when ρ(t) is bounded.
For any solution w(t), we deﬁne

∈
d with ˆwk being the k-th row of ˆW
×
d with ρk being the k-th row of P
d with ˜wk being the k-th row of ˜W
×

where ˆw is the concatenation of ˆw1, ..., ˆwk which are the K-class SVM solution, so

r(t) = w(t)

ˆw log t

˜w,

−

−

k,

∀

n

∀

∈ Sk : ˜x⊤n,k ˆw = 1 ; θ = min

k

min
n /
∈Sk

(cid:20)

˜x⊤n,k ˆw

> 1

(cid:21)

and ˜w satisﬁes the equation:

∀

∀

n

k,

∈ Sk : η exp(( ˜wk −
This equation has a unique solution for almost every data set according to Lemma 12.
For each of the K classes, we deﬁne Pk
spanned by the support vector of the k’th class, and ¯Pk
Kd
Finally, we deﬁne P1 ∈ R

Pk
Kd as follows:

d as the orthogonal projection matrix to the subspace
1 as the complementary projection.

˜wyn)⊤xn) = αn,k

1 = I

(126)

Kd

−

×

×

×

d
1 ∈ R
Kd and ¯P1 ∈ R
1, ..., PK

1, P2

P1 = diag(P1

1 ) , ¯P1 = diag( ¯P1

1, ¯P2

1, ..., ¯PK
1 )

In the following section we will also use 1
{
and 0 otherwise.

A

}

, the indicator function, which is 1 if A is satisﬁed

(P1 + ¯P1 = I

Kd

Kd)

×

∈ R

48

GRADIENT DESCENT ON SEPARABLE DATA

E.2 Auxiliary Lemma

Lemma 20 We have

C1, t1 :

t > t1 : (r(t + 1)

r(t))⊤r(t)

C1t−

θ + C2t−

2

−

≤

Additionally,

C2, t2, such that

t > t2, such that if

∃
ǫ1 > 0,

∀

∃

∀

∀

P1r(t)
||
||

> ǫ1

then we can improve this bound to

(r(t + 1)

r(t))⊤ r(t)

−

C3t−

1 < 0

≤ −

We prove the Lemma below, in appendix section E.4

(127)

(128)

(129)

E.3 Proof of Theorem 7

r(t)
Our goal is to show that
To show this, we will upper bound the following equation

||

||

is bounded, and therefore ρ(t) = r(t) + ˜w is bounded.

r (t + 1)
k

2 =
k

r (t + 1)
k

r (t)
k

−

2 + 2 (r (t + 1)

r (t))⊤ r (t) +

−

2

r (t)
k
k

(130)

First, we note that ﬁrst term in this equation can be upper-bounded by

r (t + 1)
k
(1)
=

w (t + 1)

−

2

r (t)
k
ˆw log (t + 1)

˜w

−

−

w (t) + ˆw log (t) + ˜w

2
k

−
(w (t))

ˆw [log (t + 1)

η

∇L

(w (t))
k

k∇L

ˆw

2 log2
k

k

−
2 +

2

log (t)]
k
1

−
1 + t−

+ 2η ˆw⊤

∇L

(w (t)) log

1 + t−

1

k

(2)
=
k−
= η2
(3)

η2

≤

(w (t))

ˆw
k
where in (1) we used eq. 124, in (2) we used eq 2.2, and in (3) we used
and also that

k∇L

k

2 +
k

2 t−

2,

(cid:0)

(cid:1)

∀

(cid:0)

(cid:1)

(131)

x > 0 : x

log(1+x) > 0,

≥

N

K

ˆw⊤

(w) =

∇L

Ak)xn

ˆw⊤(Ayn −
r=1 exp(w⊤(Ar −

K

Ak)xn)

< 0

n=1
Xk=1
X
ˆwyn)xn < 0,

P

∀

k

= yn (we recall that ˆwk is the K-class SVM

Ak)xn = ( ˆwr −
since ˆw⊤(Ar −
solution).
Also, from Lemma 10 we know that

(w (t))

2 = o (1) and
k

k∇L

(w (t))
k

2 <

.

∞

k∇L

∞

Xt=0
Substituting eq. 133 into eq. 131, and recalling that a t−
can ﬁnd C0 such that

ν power series converges for any ν > 1, we

(132)

(133)

r (t + 1)

k

2 = o (1) and
r (t)
k

−

r (t + 1)
k

−

r (t)

2 = C0 <
k

.

∞

(134)

∞

Xt=0

49

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Note that this equation also implies that

∀
t > t0 :

ǫ0

|k

t0 :

∃

∀

r (t + 1)

r (t)

< ǫ0 .

k − k

k|

Next, we would like to bound the second term in eq. 130. From eq. 127 in Lemma 20, we can ﬁnd
t1, C1 such that

t > t1:

∀

Thus, by combining eqs. 136 and 134 into eq. 130, we ﬁnd:

(r(t + 1)

r(t))⊤r(t)

C1t−

θ + C2t−

2

−

≤

2
r(t1)
||

− ||

||

2
r(t)
||
1
t
−

=

||

u=t1
X

(cid:2)

C0 + 2

≤

r(u + 1)

2
||

r(u)

2
||

− ||

C1u−

θ + C2u−

(cid:3)
2

1

t
−

u=t1 h
X

i
is bounded.

||

||

which is bounded, since θ > 1 (eq. 125). Therefore,

r(t)

E.4 Proof of Lemma 20

Lemma 20 We have

C1, t1 :

t > t1 : (r(t + 1)

r(t))⊤r(t)

C1t−

θ + C2t−

2

−

≤

Additionally,

C2, t2, such that

t > t2, such that if

∃
ǫ1 > 0,

∀

∃

∀

∀

P1r(t)
||
||

> ǫ1

then we can improve this bound to

We wish to bound (r(t + 1)

−
r(t))⊤r(t) = (

1 < 0

−

C3t−

r(t))⊤ r(t)

(r(t + 1)
≤ −
r(t))⊤r(t). First, we recall we deﬁned ˜xn,k , (Ayn −
η
−

log(t)])⊤r(t)

ˆw[log(t + 1)

(w(t))

−

−

Ak)xn.

K
k=1 exp(
−
K
r=1 exp(

∇L
w(t)⊤ ˜xn,k)˜xn,k

w(t)⊤ ˜xn,r) −

−

log(1 + t−

1)]

ˆw log(1 + t−

1)

r(t)

⊤

!

w(t)⊤ ˜xn,k

−
K
r=1 exp (
(cid:0)

−

˜x⊤n,kr(t)
w(t)⊤ ˜xn,r) −

(cid:1)

t−

1 exp

˜w⊤ ˜xn,k

˜x⊤n,kr(t)1

−

(cid:16)

(cid:17)

,

(138)

n

{

∈Sk}#

where in the last line we used eqs. 123 and 126 to obtain

P

N

K

n=1
X

Xk=1

ˆw = η

αn,k ˜xn,k1
{

n

∈Sk}

= η

exp

˜w⊤ ˜xn,k)

˜xn,k1
{

n

∈Sk}

,

−

(cid:16)

(cid:17)

where 1
{

A

}

is the indicator function which is 1 if A is satisﬁed and 0 otherwise.

N

K

n=1
X

Xk=1

50

(r(t + 1)

−

N

=

η

 

Xn=1 P
= ˆw⊤r(t)[t−
N

K

+ η

Xn=1

Xk=1 "

1
P

−
exp

(135)

(136)

(127)

(128)

(129)

(137)

The ﬁrst term can be upper bounded by

GRADIENT DESCENT ON SEPARABLE DATA

ˆw⊤r (t)

1

t−

log

1 + t−

1

−

max

h

max

≤
(1)

≤
(2)

h
ˆw
k
k
≤ (
t−
o

1

(cid:2)

−

log
(cid:1)(cid:3)

ˆw⊤r (t) , 0

t−
(cid:0)
i (cid:2)
ˆw⊤P1r (t) , 0
i
P1r (t)
k ≤
P1r (t)
k

ǫ1t−
1

, if
, if

k
k

t−

(cid:0)

2

2

ǫ1
> ǫ1

1 + t−

1

(cid:1)(cid:3)

(139)

where in (1) we used that P2 ˆw = 0, and in (2) we used that ˆw⊤r (t) = o (t), since

(cid:0)

(cid:1)

ˆw⊤r (t) = ˆw⊤

w (0)

η

−

(w (u))

ˆw log (t)

˜w

∇L

−

−

!

 
ˆw⊤ (w (0)

≤

−

t

Xu=0
−

˜w

ˆw log (t))

−

ηt min
0
≤
≤

u

t

ˆw⊤

(w (u)) = o (t)

∇L

where in the last line we used that
Next, we wish to upper bound the second term in eq. 137:

∇L

(w (t)) = o (1), from Lemma 10.

N

K

exp

w(t)⊤ ˜xn,k

η

Xn=1

Xk=1 "

−
K
r=1 exp (
(cid:0)

−

˜x⊤n,kr(t)
w(t)⊤ ˜xn,r) −

(cid:1)

P

t−

1 exp

˜w⊤˜xn,k

−

(cid:16)

˜x⊤n,kr(t)1
{

n

∈Sk}#

(cid:17)

(140)

51

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

We examine each term n in the sum:

K

exp

w(t)⊤ ˜xn,k

−
K
r=1 exp (
(cid:0)

−

˜x⊤n,kr(t)
w(t)⊤ ˜xn,r) −

(cid:1)

Xk=1 "

P

t−

1 exp

˜w⊤ ˜xn,k

˜x⊤n,kr(t)1

−

(cid:16)

(cid:17)

n

{

∈Sk}#

w(t)⊤ ˜xn,k

˜x⊤n,kr(t)

(cid:1)

w(t)⊤ ˜xn,r)

−

exp (

−

t−

1 exp

˜w⊤ ˜xn,k

˜x⊤n,kr(t)1

−

(cid:16)

(cid:17)



n

{

∈Sk}







˜x⊤n,kr(t)

exp

w(t)⊤ ˜xn,k

t−

1 exp

˜w⊤ ˜xn,k

−

(cid:17)

−

(cid:16)

1
{

n

∈Sk}

1
˜x⊤
{

n,k

r(t)

0

≥

}

(cid:17)

(cid:17)

exp



w(t)⊤ ˜xn,k

1



−

K

r=1
X
r6=yn

exp

w(t)⊤ ˜xn,r

−

(cid:16)

(cid:17)






˜w⊤ ˜xn,k

1
˜x⊤
{

n,k

r(t)<0
}

˜x⊤n,kr(t)

(cid:17)
w(t)⊤ ˜xn,k

(cid:17)
t−

1 exp

˜w⊤˜xn,k

1

˜x⊤n,kr(t)

(cid:17)



∈Sk}

1
{

n

−

(cid:17)

exp

w(t)⊤(˜xn,k + ˜xn,r)

˜x⊤n,kr(t)1

n

{

∈Sk}

(cid:17)

(cid:17)

˜x⊤
{

n,k

r(t)<0
}

K



exp

Xk=1

1 +








K

−
K

(cid:0)
r=1
r6=yn
P

Xk=1 (cid:16)
K

−

(cid:16)

−

(cid:16)

Xk=1
t−



1 exp

−

=

K

−

(cid:16)
exp

−

(cid:16)

Xk=1 (cid:16)
K
K

−

(cid:16)

r=1
X
r6=yn

Xk=1
K

=

(1)

≤

+

−

(2)

≤

−

−

(cid:16)

(cid:17)

−

(cid:16)

exp

w(t)⊤ ˜xn,k

t−

1 exp

˜w⊤ ˜xn,k

−

−

Xk=1 (cid:16)
K 2 exp

(cid:16)
w(t)⊤(˜xn,k1 + ˜xn,r1)

(cid:17)

1
{

n

∈Sk}

˜x⊤n,kr(t)

(cid:17)

(cid:17)

˜x⊤n,k1r(t)1
˜x⊤
{

n,k1

,

r(t)<0
}

−

(cid:16)

where in (1) we used

1 and in (2) we deﬁned:

x

∀

≥

0 : 1

−

≤

1
1+x ≤

(cid:17)
x

(141)

(k1, r1) = argmax

exp

w(t)⊤(˜xn,k + ˜xn,r)

˜x⊤n,kr(t)1

−

(cid:16)

k,r

(cid:12)
(cid:12)
(cid:12)

(cid:17)

˜x⊤
{

n,k

r(t)<0
}

(cid:12)
(cid:12)
(cid:12)

52

GRADIENT DESCENT ON SEPARABLE DATA

Recalling that w(t) = ˆw log(t) + ˜w + r(t), eq. 141 can be upper bounded by

ˆw⊤ ˜xn,k exp

t−

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

˜x⊤n,kr(t)1

−

(cid:16)

−

(cid:16)

(cid:17)

˜x⊤
{

n,k

r(t)

≥

0 , n /

∈Sk}

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

1

˜x⊤n,kr(t)1

˜x⊤
{

n,k

r(t)

≥

0 , n

∈Sk}

(cid:17) h

−

(cid:16)

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

˜x⊤n,kr(t)1

−

(cid:16)

−

(cid:16)

(cid:17)

˜x⊤
{

n,k

r(t)<0 , n /

∈Sk}

(cid:17)

−

(cid:17)

i

(cid:17)

1

+

t−

1 exp

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

−

−

(cid:16)

(cid:17) h
w(t)⊤ ˜xn,r1)t−

(cid:16)
ˆw⊤ ˜xn,k1 exp

−

(cid:17)
i
˜w⊤˜xn,k1

˜x⊤n,kr(t)1

˜x⊤
{

n,k

r(t)<0 , n

∈Sk}

exp

r(t)⊤ ˜xn,k1

˜x⊤n,k1r(t)1

−

(cid:16)

−

(cid:16)

(cid:17)

(cid:17)

˜x⊤
{

n,k1

r(t)<0
}

(142)

K

Xk=1
K

+

+

t−

Xk=1
K

Xk=1
K

t−

1 exp

−

(cid:16)
ˆw⊤ ˜xn,k exp

Xk=1
K 2 exp(

−

Kt−

θ exp

−
(1)

≤

x : (e−

x

1)x < 0, θ = mink

−

minn /

∈Sk ˜x⊤n,k ˆw

> 1 (eq. 125)

h

i

˜w⊤ ˜xn,k

+ φ(t),

min
n,k

−

(cid:18)

(cid:19)
x < 1,

∀

where in (1) we used xe−
and denoted:

K

φ(t) =

t− ˆw⊤ ˜xn,k exp

Xk=1
t−1 exp

K

+

−
(cid:0)
˜w⊤˜xn,k

exp

−
(cid:0)

(cid:1)
r(t)⊤ ˜xn,k

Xk=1
K 2 exp(

−
(cid:0)
(cid:1) (cid:2)
w(t)⊤ ˜xn,r1)t− ˆw⊤ ˜xn,k1 exp

−
(cid:0)

−
−
We use the fact that

x : (e−

x

1

−

(cid:1)

(cid:3)
˜w⊤˜xn,k1

˜w⊤˜xn,k

exp

r(t)⊤ ˜xn,k

˜x⊤
n,kr(t)1{˜x⊤

n,k

r(t)<0, n /∈Sk}

(cid:1)
˜x⊤
n,kr(t)1{˜x⊤

r(t)<0,n∈Sk}

n,k

(cid:17)
1

i

−

(cid:17)

(cid:17)

≥

−

(cid:16)

exp

r(t)⊤ ˜xn,k1

˜x⊤

n,k1

r(t)1{˜x⊤

r(t)<0}.

n,k1

−

−
(cid:0)
1)x < 0 and therefore

(cid:1)

(cid:0)

(n, k):

(cid:1)

∀
ˆw⊤ ˜xn,k exp

t−

−
˜w⊤˜xn,k

exp

∀
r(t)⊤ ˜xn,k

t−

1 exp

−
(cid:16)
˜w⊤˜xn,k

(cid:17)
exp

−
(cid:16)
r(t)⊤ ˜xn,k

−
(cid:16)
to show that φ(t) is strictly negative. If ˜x⊤n,k1

(cid:17) h

−

(cid:16)

r

˜x⊤n,kr(t)1

˜x⊤
{
˜x⊤n,kr(t)1

n,k

r(t)<0
}

< 0

˜x⊤
{

n,k

r(t)<0
}

< 0,

(143)

0 then from the last two equations:

K

φ(t) =

ˆw⊤ ˜xn,k exp

t−

Xk=1

K

+

t−

1 exp

−

−

(cid:16)

(cid:17)

−

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

˜x⊤n,kr(t)1

˜x⊤
{

n,k

r(t)<0, n /

∈Sk}

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

1

˜x⊤n,kr(t)1

˜x⊤
{

n,k

r(t)<0, n

∈Sk}

< 0

(144)

Xk=1

(cid:17) h
(cid:16)
If ˜x⊤n,k1r < 0 then we note that
−
1. If ˜x⊤n,r1r(t)
0 then this is immediate since
−
2. If ˜x⊤n,r1r(t) < 0 then from (k1, r1) deﬁnition:

(cid:16)
˜x⊤n,r1r(t)

≤ −

≥

−

i

(cid:17)
˜x⊤n,k1r(t) since:
˜x⊤n,r1r(t)
0

˜x⊤n,k1r(t).

≤

≤ −

exp

w(t)⊤(˜xn,k1 + ˜xn,r1)

exp

w(t)⊤(˜xn,k1 + ˜xn,r1)

˜x⊤n,r1r(t)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

≤

53

(cid:12)
(cid:12)
(cid:12)

−

(cid:16)

,

˜x⊤n,k1r(t)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

−

(cid:16)

(cid:12)
(cid:12)
(cid:12)

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

and therefore

˜x⊤n,r1r(t) =

−

˜x⊤n,r1r(t)
(cid:12)
(cid:12)
(cid:12)

≤

˜x⊤n,k1r(t)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

˜x⊤n,k1r(t).

−

We divide into cases:
1. If n /

(cid:12)
(cid:12)
(cid:12)
∈ Sk1 then we examine the sum
˜w⊤ ˜xn,k1

ˆw⊤ ˜xn,k1 exp

exp

t−

K 2 exp(

−

−
(cid:17)
(cid:16)
(cid:16)
ˆw⊤ ˜xn,k1 exp
w(t)⊤ ˜xn,r1)t−

r(t)⊤ ˜xn,k1

˜x⊤n,k1r(t)1

(cid:17)
˜w⊤˜xn,k1

exp

˜x⊤
{

n,k1

r(t)<0
}
r(t)⊤ ˜xn,k1

−

−
−
(cid:16)
The ﬁrst term is negative and the second is positive. From Lemma 19 w(t)⊤ ˜xn,r1 → ∞
t3 so that
∃

w(t)⊤ ˜xn,r1) < K 2 and therefore this sum is strictly negative since

t > t3 : exp(

n,k1

−

(cid:17)

(cid:17)

(cid:16)

. Therefore

˜x⊤n,k1r(t)1

˜x⊤
{

r(t)<0
}

K 2 exp(

−

−
∀
ˆw⊤ ˜xn,k1 exp
w(t)⊤ ˜xn,r1)t−

ˆw⊤ ˜xn,k1 exp (

t−

−
(cid:0)
˜w⊤ ˜xn,k1) exp (

−

˜w⊤ ˜xn,k1)

exp

˜x⊤n,k1

r(t)

˜x⊤n,k1

r(t)1
˜x⊤
{

n,k1

r(t)<0
}

−

(cid:16)

r(t)⊤ ˜xn,k1) ˜x⊤n,k1

(cid:1)

−

(cid:17)
r(t)1
˜x⊤
{

n,k1

r(t)<0
}

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:12)
(cid:12)
2. If n
(cid:12)
(cid:12)
t−1 exp

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:12)

t > t3

−

< 1,

K 2 exp(

w(t)⊤ ˜xn,r1)
(cid:12)
∈ Sk1 then we examine the sum
(cid:12)
(cid:12)
r(t)⊤ ˜xn,k1
exp
−
−
w(t)⊤ ˜xn,r1)t− ˆw⊤ ˜xn,k1 exp
(cid:0)
(cid:0)
K 2 exp(

˜w⊤˜xn,k1

(cid:1) (cid:2)

∀

(cid:1)

−
a. If

−
˜x⊤n,k1
r(t)
|
|
K 2 exp(

−

> C0 then

w(t)⊤ ˜xn,r1)t−

t−

1 exp (

−

−
t4 such that

(cid:0)

∃
ˆw⊤ ˜xn,k1 exp

−
˜w⊤ ˜xn,k1) [exp (
(cid:0)

∀

−

1

˜x⊤

n,k1

−
˜w⊤˜xn,k1

(cid:3)

r(t)<0}

n,k1
r(t)⊤ ˜xn,k1

r(t)1{˜x⊤

exp

−
(cid:0)

˜x⊤

n,k1

r(t)1{˜x⊤

r(t)<0}

n,k1

(cid:1)
t > t4 this sum can be upper bounded by zero since

(cid:1)

˜w⊤ ˜xn,k1)

exp

r(t)⊤ ˜xn,k1)
(cid:1)

−
(cid:16)
1] ˜x⊤n,k1

˜x⊤n,k1r(t)
(cid:17)
r(t)1
˜x⊤
{

−

n,k1

r(t)<0
}

˜x⊤n,k1r(t)1
˜x⊤
{

n,k1

r(t)<0
}

K 2 exp(
1

w(t)⊤ ˜xn,r1)
exp (r(t)⊤ ˜xn,k1) ≤

−

K 2 exp(
1

w(t)⊤ ˜xn,r1)
C0)

−
exp (

−

−
where in the last transition we used Lemma 19.
b. If

r(t)

−

< 1,

t > t4

∀

C0 then we can ﬁnd constant C5 so that eq. 145 can be upper bounded by

ˆw⊤(˜xn,k1 +˜xn,r1 ) exp

˜w⊤(˜xn,k1 + ˜xn,r1)

C5t−

2,

(146)

since
Therefore, eq. 141 can be upper bounded by

˜x⊤n,k1r(t)

˜x⊤n,r1r(t)

≤ −

−

≤

C0 and by deﬁnition,

∀

−

(cid:16)

exp (2C0) C0 ≤
1.

(n, k) : ˆw⊤ ˜xn,k ≥

(cid:17)

˜x⊤n,k1
|

| ≤
K 2t−

If, in addition,

k, n

∃
1 exp

t−

˜x⊤n,kr(t)
|
|

∈ Sk :
˜w⊤ ˜xn,k

Kt−

θ exp

˜w⊤˜xn,k

+ C5t−

2

min
n,k

−

(cid:18)
> ǫ2 then

(cid:19)

exp

r(t)⊤ ˜xn,k

1

˜x⊤n,kr(t)

−
1 exp
1 exp

(cid:16)
t−
t−

−

(cid:17) h
(cid:16)
maxn,k ˜w⊤ ˜xn,k
maxn,k ˜w⊤ ˜xn,k

≤ (

−
−

−
−

(cid:0)
(cid:0)

−
(cid:17)
i
exp (
[1
−
−
[exp (ǫ2)
−

(cid:1)
(cid:1)

54

ǫ2)] ǫ2
1] ǫ2

, if r(t)⊤ ˜xn,k ≥
0
, if r(t)⊤ ˜xn,k < 0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(145)
(cid:12)

(147)

(148)

(149)

GRADIENT DESCENT ON SEPARABLE DATA

and we can improve this bound to

C ′′t−

1 < 0,

−

(150)

where C ′′ is the minimum between exp
exp
1. If

maxn,k ˜w⊤ ˜xn,k

[exp (ǫ2)

−

−
P1r (t)
(cid:0)
k

k ≥

(cid:1)

ǫ1 (as in Eq. 139), we have that

−

(cid:0)

1] ǫ2. To conclude:

(cid:1)

maxn,k ˜w⊤ ˜xn,k

[1

exp (

ǫ2)] ǫ2 and

−

−

2 (1)

1

2

1

2 (2)

1

=

≥

|S| Xk,n

max
k,n
∈Sk

˜x⊤n,kr (t)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
where in (1) we used P⊤1 ˜xn,k = ˜xn,k ∀
non-zero singular value of X
S
P1r(t)
||
|S|
can be upper bounded by:

˜x⊤n,kP1r (t)
(cid:12)
(cid:12)
(cid:12)
∈ Sk, in (2) we denoted by σmin (X
and used eq. 128. Therefore, for some (n, k),

), the minimal
S
ǫ2 ,
˜x⊤n,kr
(cid:12)
ǫ1, then combining eq. 139 with eq. 150 we ﬁnd that eq. 137
(cid:12)
(cid:12)

|S| (cid:13)
(cid:13)
(cid:13)

∈Sk (cid:12)
(cid:12)
(cid:12)

1 (151)

min (X

P1r (t)

X⊤
S

1. If

1 σ2

k, n

|| ≥

) ǫ2

|S|

(cid:13)
(cid:13)
(cid:13)

≥

≥

(cid:12)
(cid:12)
(cid:12)

−

S

S

σ2
min (X

) ǫ2

(r(t + 1)

r(t))⊤ r(t)

C ′′t−

1 + o(t−

1)

−

≤ −

C2 < C ′′ and

t2 > 0 such that eq. 129 holds. This implies also that eq. 127

This implies that
∃
P1r(t)
holds for
||
|| ≥
P1r(t)
2. If
||
||

ǫ1.

∃

< ǫ1, we obtain (for some positive constants C3, C4):

(r(t + 1)

r(t))⊤r(t)

C3t−

θ + C4t−

2

−

≤

Therefore,

t1 > 0 and C1 such that eq. 127 holds.

∃

Appendix F. An experiment with stochastic gradient descent

(A)

3

2

1

0

−1

−2

−3
−3

2

x

1

0.5

(B)

|
|
)
t
(

w

|
|
 
d
e
z
i
l
a
m
r
o
N

(D)

3

2

1

p
a
g
 
e
l
g
n
A

0
100

0
100

x 10−3

101

102

104

105

103
t

 

GD
GDMO

101

102

104

105

103
t

(C)

100

)
)
t
(

w
(
L

10−5

10−10

 
100

(E)

0.06

0.04

0.02

p
a
g
 
n
i
g
r
a

M

0
100

−2

−1

1

2

3

0
x
1

101

102

104

105

103
t

101

102

104

105

103
t

Figure 4: Same as Fig. 1, except stochastic gradient decent is used (with mini-batch of size 4),

instead of GD.

55

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

References

Mor Shpigel Nacson, Nati Srebro, and Daniel Soudry. Stochastic Gradient Descent on Separable

Data Exact Convergence with a Fixed Learning Rate. AISTATS, 2018.

Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit Bias of Gradient Descent

on Linear Convolutional Networks. NIPS, 2018.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Radha Krishna Ganti. EE6151, Convex optimization algorithms. Unconstrained minimization: Gra-

dient descent algorithm, 2015. URL

Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Sre-

bro. Implicit Regularization in Matrix Factorization. NIPS, 2017.

Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in

terms of optimization geometry. ICML, 2018.

Moritz Hardt, Benjamin Recht, and Y Singer. Train faster, generalize better: Stability of stochastic

gradient descent. ICML, pages 1–24, 2016.

Elad Hoffer, Itay Hubara, and D. Soudry. Train longer, generalize better: closing the generalization

gap in large batch training of neural networks. In NIPS, pages 1–13, may 2017.

I Hubara, M Courbariaux, D. Soudry, R El-yaniv, and Y Bengio. Quantized Neural Networks:

Training Neural Networks with Low Precision Weights and Activations. JMLR, 2018.

Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv, 2018.

Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.
ICLR, pages 1–16, 2017.

Diederik P Kingma and Jimmy Lei Ba. Adam: a Method for Stochastic Optimization. In ICLR,

pages 1–13, 2015.

Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Conver-

gence of Gradient Descent on Separable Data. AISTATS, 2018.

Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On

the role of implicit regularization in deep learning. arXiv:1412.6614, 2014.

Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimiza-

tion in deep neural networks. In NIPS, 2015.

Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring Gen-

eralization in Deep Learning. arXiv, jun 2017.

56

GRADIENT DESCENT ON SEPARABLE DATA

Saharon Rosset, Ji Zhu, and Trevor J Hastie. Margin Maximizing Loss Functions. In NIPS, pages

1237–1244, 2004.

Robert E Schapire, Yoav Freund, Peter Bartlett, Wee Sun Lee, et al. Boosting the margin: A new
explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651–1686,
1998.

Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, and N Srebro. The Implicit Bias of Gradient

Descent on Separable Data. In ICLR, 2018.

Matus Telgarsky. Margins, shrinkage and boosting. In Proceedings of the 30th International Con-
ference on International Conference on Machine Learning-Volume 28, pages II–307. JMLR. org,
2013.

Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The
Marginal Value of Adaptive Gradient Methods in Machine Learning. arXiv, pages 1–14, 2017.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. In ICLR, 2017.

Tong Zhang, Bin Yu, et al. Boosting with early stopping: Convergence and consistency. The Annals

of Statistics, 33(4):1538–1579, 2005.

57

8
1
0
2
 
c
e
D
 
8
2

 
 
]
L
M

.
t
a
t
s
[
 
 
4
v
5
4
3
0
1
.
0
1
7
1
:
v
i
X
r
a

Journal of Machine Learning Research 19 (2018) 1-57

Submitted 4/18; Published 11/18

The Implicit Bias of Gradient Descent on Separable Data

Daniel Soudry
Elad Hoffer
Mor Shpigel Nacson
Department of Electrical Engineering,Technion
Haifa, 320003, Israel

Suriya Gunasekar
Nathan Srebro
Toyota Technological Institute at Chicago
Chicago, Illinois 60637, USA

Editor: Leon Bottou

DANIEL.SOUDRY@GMAIL.COM
ELAD.HOFFER@GMAIL.COM
MOR.SHPIGEL@GMAIL.COM

SURIYA@TTIC.EDU
NATI@TTIC.EDU

Abstract
We examine gradient descent on unregularized logistic regression problems, with homogeneous
linear predictors on linearly separable datasets. We show the predictor converges to the direction
of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone de-
creasing loss functions with an inﬁmum at inﬁnity, to multi-class problems, and to training a weight
layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is
very slow, and only logarithmic in the convergence of the loss itself. This can help explain the
beneﬁt of continuing to optimize the logistic or cross-entropy loss even after the training error is
zero and the training loss is extremely small, and, as we show, even if the validation loss increases.
Our methodology can also aid in understanding implicit regularization in more complex models
and with other optimization methods.
Keywords: gradient descent, implicit regularization, generalization, margin, logistic regression

1. Introduction

It is becoming increasingly clear that implicit biases introduced by the optimization algorithm play a
crucial role in deep learning and in the generalization ability of the learned models (Neyshabur et al.,
2014, 2015; Zhang et al., 2017; Keskar et al., 2017; Neyshabur et al., 2017; Wilson et al., 2017). In
particular, minimizing the training error, without explicit regularization, over models with more pa-
rameters and capacity than the number of training examples, often yields good generalization. This
is despite the fact that the empirical optimization problem being highly underdetermined. That is,
there are many global minima of the training objective, most of which will not generalize well, but
the optimization algorithm (e.g. gradient descent) biases us toward a particular minimum that does
generalize well. Unfortunately, we still do not have a good understanding of the biases introduced
by different optimization algorithms in different situations.

We do have an understanding of the implicit regularization introduced by early stopping of
stochastic methods or, at an extreme, of one-pass (no repetition) stochastic gradient descent (Hardt et al.,
2016). However, as discussed above, in deep learning we often beneﬁt from implicit bias even when
optimizing the training error to convergence (without early stopping) using stochastic or batch meth-
ods. For loss functions with attainable, ﬁnite minimizers, such as the squared loss, we have some

2018 Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro.

c
(cid:13)
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
http://jmlr.org/papers/v19/18-188.html.

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

understanding of this: in particular, when minimizing an underdetermined least squares problem us-
ing gradient descent starting from the origin, it can be shown that we will converge to the minimum
Euclidean norm solution. However, the logistic loss, and its generalization the cross-entropy loss
which is often used in deep learning, do not admit ﬁnite minimizers on separable problems. Instead,
to drive the loss toward zero and thus minimize it, the norm of the predictor must diverge toward
inﬁnity.

Do we still beneﬁt from implicit regularization when minimizing the logistic loss on separable
data? Clearly the norm of the predictor itself is not minimized, since it grows to inﬁnity. However,
for prediction, only the direction of the predictor, i.e. the normalized w(t)/
, is important.
How does w(t)/
when we minimize the logistic (or similar) loss using
gradient descent on separable data, i.e., when it is possible to get zero misclassiﬁcation error and
thus drive the loss to zero?

behave as t

w(t)
k

→ ∞

w(t)

k

k

k

k

w(t)
k

In this paper, we show that even without any explicit regularization, for all linearly separa-
ble datasets, when minimizing logistic regression problems using gradient descent, we have that
w(t)/
converges to the L2 maximum margin separator, i.e. to the solution of the hard mar-
gin SVM for homogeneous linear predictors. This happens even though neither the norm
, nor
the margin constraint, are part of the objective or explicitly introduced into optimization. More
generally, we show the same behavior for generalized linear problems with any smooth, monotone
strictly decreasing, lower bounded loss with an exponential tail. Furthermore, we characterize the
rate of this convergence, and show that it is rather slow, wherein for almost all datasets, the distance
to the max-margin predictor decreasing only as O(1/ log(t)), and in some degenerate datasets, the
rate further slows down to O(log log(t)/ log(t)). This explains why the predictor continues to im-
prove even when the training loss is already extremely small. We emphasize that this bias is speciﬁc
to gradient descent, and changing the optimization algorithm, e.g. using adaptive learning rate meth-
ods such as ADAM (Kingma and Ba, 2015), changes this implicit bias.

w
k

k

2. Main Results

xn, yn}
n=1, with xn ∈
Consider a dataset
{
by minimizing an empirical loss of the form

N

Rd and binary labels yn ∈ {−

1, 1
}

. We analyze learning

N

(w) =

ℓ

ynw⊤xn

.

L

n=1
X

(cid:16)

(cid:17)

(1)

where w

Rd is the weight vector. To simplify notation, we assume that all the labels are positive:

n : yn = 1 — this is true without loss of generality, since we can always re-deﬁne ynxn as xn.

We are particularly interested in problems that are linearly separable, and the loss is smooth

∀

∈

strictly decreasing and non-negative:

Assumption 1 The dataset is linearly separable:

w

such that

∃

∗

n : w⊤
∗

∀

xn > 0 .

Assumption 2 ℓ (u) is a positive, differentiable, monotonically decreasing to zero1, (so
0, ℓ′ (u) < 0, limu
Lipshitz and limu

u : ℓ (u) >
ℓ′ (u) = 0), a β-smooth function, i.e. its derivative is β-

ℓ (u) = limu
ℓ′ (u)

= 0.

→∞

→∞

∀

→−∞

1. The requirement of non-negativity and that the loss asymptotes to zero is purely for convenience. It is enough to
require the loss is monotone decreasing and bounded from below. Any such loss asymptotes to some constant, and is
thus equivalent to one that satisﬁes this assumption, up to a shift by that constant.

2

GRADIENT DESCENT ON SEPARABLE DATA

Assumption 2 includes many common loss functions, including the logistic, exp-loss2 and probit
(w) is a βσ2
max (X )-smooth function, where σmax (X ) is the
Rd

losses. Assumption 2 implies that
maximal singular value of the data matrix X

×
Under these conditions, the inﬁmum of the optimization problem is zero, but it is not attained
at any ﬁnite w. Furthermore, no ﬁnite critical point w exists. We consider minimizing eq. 1 using
Gradient Descent (GD) with a ﬁxed learning rate η, i.e., with steps of the form:

N .

L

∈

w (t + 1) = w (t)

η

(w(t)) = w (t)

ℓ′

w (t)⊤ xn

xn.

(2)

−

∇L

N

η

−

Xn=1

(cid:16)

(cid:17)

We do not require convexity. Under Assumptions 1 and 2, gradient descent converges to the global
minimum (i.e. to zero loss) even without it:

Lemma 1 Let w (t) be the iterates of gradient descent (eq. 2) with η < 2β−
starting point w(0). Under Assumptions 1 and 2, we have: (1) limt
n : limt
limt

w (t)⊤ xn =

, and (3)

=

→

ßnf ty L

.

w (t)
k

→∞ k

∞
Proof Since the data is linearly separable,

∀

→∞
w

∞

which linearly separates the data, and therefore

1σ−

2

max (X ) and any
(w (t)) = 0, (2)

∃

∗
N

w⊤

∗ ∇L

(w) =

ℓ′

w⊤xn

xn.

w⊤
∗

n=1
X

(cid:16)

(cid:17)

∀

u : ℓ′ (u) < 0. Therefore, there are no ﬁnite critical points w, for which

n : w⊤
For any ﬁnite w, this sum cannot be equal to zero, as a sum of negative terms, since
xn > 0
∗
(w) = 0. But
and
gradient descent on a smooth loss with an appropriate stepsize is always guaranteed to converge to a
0 (see, e.g. Lemma 10 in Appendix A.4, slightly adapted from Ganti
critical point:
n : w (t)⊤ xn > 0 for
(2015), Theorem 2). This necessarily implies that
0, so GD converges to
large enough t—since only then ℓ′
the global minimum.

w (t)
k
k → ∞
0. Therefore,

while
(w)

w (t)⊤ xn

∀
∇L

(w (t))

∀
→

∇L

→

→

L

(cid:16)

(cid:17)

The main question we ask is: can we characterize the direction in which w(t) diverges? That is,
does the limit limt

always exist, and if so, what is it?

w (t) /

In order to analyze this limit, we will need to make a further assumption on the tail of the loss

w (t)
k

k

→∞

function:

−

Deﬁnition 2 A function f (u) has a “tight exponential tail”, if there exist positive constants c, a, µ+, µ
and u

such that

−

, u+

∀

u > u+ :f (u)
:f (u)
u > u

≤

c (1 + exp (

c (1

exp (

µ+u)) e−
u)) e−
µ

au

au .

−

−

−

≥
Assumption 3 The negative loss derivative

∀

−

−

For example, the exponential loss ℓ (u) = e−

u and the commonly used logistic loss ℓ (u) =
u) both follow this assumption with a = c = 1. We will assume a = c = 1 — without

log (1 + e−
loss of generality, since these constants can be always absorbed by re-scaling xn and η.

ℓ′ (u) has a tight exponential tail (Deﬁnition 2).

−

We are now ready to state our main result:

2. The exp-loss does not have a global β smoothness parameter. However, if we initialize with η < 1/L(w(0)) then it

is straightforward to show the gradient descent iterates maintain bounded local smoothness.

3

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Theorem 3 For any dataset which is linearly separable (Assumption 1), any β-smooth decreasing
loss function (Assumption 2) with an exponential tail (Assumption 3), any stepsize η < 2β−
and any starting point w(0), the gradient descent iterates (as in eq. 2) will behave as:

1σ−

max (X )

2

where ˆw is the L2 max margin vector (the solution to the hard margin SVM):

and the residual grows at most as

= O(log log(t)), and so

w (t) = ˆw log t + ρ (t) ,

ˆw = argmin
w

Rd k

w

2 s.t. w⊤xn ≥
k

1,

∈
ρ (t)
k
k

lim
t
→∞

k

w (t)
w (t)

=

ˆw
ˆw

.

k

k

k

(3)

(4)

Furthermore, for almost all data sets (all except measure zero), the residual ρ(t) is bounded.

Proof Sketch We ﬁrst understand intuitively why an exponential tail of the loss entail asymptotic
u exactly, and examine
convergence to the max margin vector: Assume for simplicity that ℓ (u) = e−
n : w (t)⊤ xn → ∞
, as is guaranteed by
the asymptotic regime of gradient descent in which
Lemma 1. If w (t) /
+
∞
ρ (t) such that g (t)
ρ (t) /g (t) = 0. The gradient can then be
written as:

w (t)
converges to some limit w
k
n :x⊤n w
,
∀

, then we can write w (t) = g (t) w

> 0, and limt

k
→ ∞

→∞

∞

∞

∀

N

N

(w) =

exp

w (t)⊤ xn

xn =

exp

−∇L

−

g (t) w⊤
∞

−

xn

−

exp

ρ (t)⊤ xn

xn .

(5)

(cid:17)

(cid:16)

(cid:16)

n=1
X

→ ∞

n=1
X
and the exponents become more negative, only those samples with the largest (i.e.,
As g(t)
least negative) exponents will contribute to the gradient. These are precisely the samples with
xn, aka the “support vectors”. The negative gradient (eq. 5)
the smallest margin argminnw⊤
∞
would then asymptotically become a non-negative linear combination of support vectors. The limit
w
will then be dominated by these gradients, since any initial conditions become negligible as
will also be a non-negative linear combination of
∞
. We therefore have:
/

∞
(from Lemma 1). Therefore, w
w (t)
k
support vectors, and so will its scaling ˆw = w

k → ∞

(cid:17)

(cid:16)

(cid:17)

N

Xn=1

n

∀

αn ≥
(cid:16)

ˆw =

αnxn

0 and ˆw⊤xn = 1

OR

αn = 0 and ˆw⊤xn > 1

(6)

(cid:17)

(cid:16)

(cid:17)

∞

These are precisely the KKT conditions for the SVM problem (eq. 4) and we can conclude that ˆw
is indeed its solution and w

is thus proportional to it.

w (t)

To prove Theorem 3 rigorously, we need to show that w (t) /

has a limit, that g (t) =
log (t) and to bound the effect of various residual errors, such as gradients of non-support vectors
and the fact that the loss is only approximately exponential. To do so, we substitute eq. 3 into the
gradient descent dynamics (eq. 2), with w
= ˆw being the max margin vector and g(t) = log t.
We then show that, except when certain degeneracies occur, the increment in the norm of ρ (t) is
ν for some C1 > 0 and ν > 1, which is a converging series. This happens because
bounded by C1t−
1, cancels out the dominant
the increment in the max margin term, ˆw [log (t + 1)
t−

ˆwt−
(w (t)) (eq. 5 with g (t) = log (t) and w⊤
∞

1 term in the gradient

xn = 1).

log (t)]

−∇L

−

≈

∞

k

k

minn w⊤
∞

xn

∞

(cid:0)

(cid:1)

4

GRADIENT DESCENT ON SEPARABLE DATA

Degenerate and Non-Degenerate Data Sets An earlier conference version of this paper (Soudry et al.,
2018) included a partial version of Theorem 3, which only applies to almost all data sets, in which
case we can ensure the residual ρ(t) is bounded. This partial statement (for almost all data sets) is re-
stated and proved as Theorem 9 in Appendix A. It applies, e.g. with probability one for data sampled
from any absolutely continuous distribution. It does not apply in “degenerate” cases where some
of the support vectors xn (for which ˆw⊤xn = 1) are associated with dual variables that are zero
(αn = 0) in the dual optimum of 4. As we show in Appendix B, this only happens on measure zero
data sets. Here, we prove the more general result which applies for all data sets, including degener-
ate data sets. To do so, in Theorem 13 in Appendix C we provide a more complete characterization
of the iterates w(t) that explicitly speciﬁes all unbounded components even in the degenerate case.
We then prove the Theorem by plugging in this more complete characterization and showing that
the residual is bounded, thus also establishing Theorem 3.

Parallel Work on the Degenerate Case Following the publication of our initial version, and
while preparing this revised version for publication, we learned of parallel work by Ziwei Ji and
Matus Telgarsky that also closes this gap. Ji and Telgarsky (2018) provide an analysis of the degen-
erate case, establishing converges to the max margin predictor by showing that
=

log log t
log t

(cid:16)q

. Our analysis provides a more precise characterization of the iterates, and also shows
O
the convergence is actually quadratically faster (see Section 3). However, Ji and Telgarsky go even
further and provide a characterization also when the data is non-separable but w(t) still goes to
inﬁnity.

(cid:17)

(cid:13)
(cid:13)
(cid:13)

w(t)
w(t)

ˆw
ˆw
k

k

k −

k

(cid:13)
(cid:13)
(cid:13)

More Reﬁned Analysis of the Residual
In some non-degenerate cases, we can further character-
ize the asymptotic behavior of ρ (t). To do so, we need to refer to the KKT conditions (eq. 6) of
= argminn ˆw⊤xn. We then have the
the SVM problem (eq. 4) and the associated support vectors
following Theorem, proved in Appendix A:

S

Theorem 4 Under the conditions and notation of Theorem 3, for almost all datasets, if in addition
the support vectors span the data (i.e. rank (X
is a matrix whose columns
are only those data points xn s.t. ˆw⊤xn = 1), then limt

ρ (t) = ˜w, where ˜w is a solution to

) = rank (X), where X

S

S

→∞

n

∀

∈ S

: η exp

x⊤n ˜w

= αn

−

(7)

(cid:17)

(cid:16)
Analogies with Boosting Perhaps most similar to our study is the line of work on understanding
AdaBoost in terms its implicit bias toward large L1-margin solutions, starting with the seminal work
of Schapire et al. (1998). Since AdaBoost can be viewed as coordinate descent on the exponential
loss of a linear model, these results can be interpreted as analyzing the bias of coordinate descent,
rather then gradient descent, on a monotone decreasing loss with an exact exponential tail. Indeed,
with small enough step sizes, such a coordinate descent procedure does converge precisely to the
maximum L1-margin solution (Zhang et al., 2005; Telgarsky, 2013). In fact, Telgarsky (2013) also
generalizes these results to other losses with tight exponential tails, similar to the class of losses we
consider here.

Also related is the work of Rosset et al. (2004). They considered the regularization path wλ =
p
wλkp is
p for similar loss functions as we do, and showed that limλ
arg min
proportional to the maximum Lp margin solution. That is, they showed how adding inﬁnitesimal Lp

(w) + λ

0 wλ/

w
k

L

→

k

k

5

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

(e.g. L1 and L2) regularization to logistic-type losses gives rise to the corresponding max-margin
predictor.3 However, Rosset et al. do not consider the effect of the optimization algorithm, and
instead add explicit regularization. Here we are speciﬁcally interested in the bias implied by the
algorithm not by adding (even inﬁnitesimal) explicit regularization. We see that coordinate descent
gives rise to the max L1 margin predictor, while gradient descent gives rise to the max L2 norm
predictor.
In Section 4.3 and in follow-up work (Gunasekar et al., 2018) we discuss also other
optimization algorithms, and their implied biases.

Non-homogeneous linear predictors
In this paper we focused on homogeneous linear predictors
of the form w⊤x, similarly to previous works (e.g., Rosset et al. (2004); Telgarsky (2013)). Specif-
ically, we did not have the common intercept term: w⊤x + b. One may be tempted to introduce
the intercept in the usual way, i.e., by extending all the input vectors xn with an additional ′1′ com-
ponent.
In this extended input space, naturally, all our results hold. Therefore, we converge in
direction to the L2 max margin solution (eq. 4) in the extended space. However, if we translate this
solution to the original x space we obtain

which is not the L2 max margin (SVM) solution

argmin
w
Rd,b

R k

w

k

∈

∈

2 + b2 s.t. w⊤xn + b

1,

≥

argmin
w
Rd,b

R k

w

k

∈

∈

2 s.t. w⊤xn + b

1,

≥

where we do not have a b2 penalty in the objective.

3. Implications: Rates of convergence

The solution in eq. 3 implies that w (t) /
tor ˆw/
Speciﬁcally, our results imply the following tight rates of convergence:

converges to the normalized max margin vec-
. Moreover, this convergence is very slow— logarithmic in the number of iterations.

w (t)
k

ˆw

k

k

k

Theorem 5 Under the conditions and notation of Theorem 3, for any linearly separable data set,
the normalized weight vector converges to the normalized max margin vector in L2 norm

k
with this rate improving to O(1/ log(t)) for almost every dataset; and in angle

k

(cid:18)

k (cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

w (t)
w (t)
k

ˆw
ˆw

−

= O

log log t
log t

,

(cid:19)

2

1

−

w (t)⊤ ˆw
ˆw
w (t)
k

k k

k

= O

log log t
log t

,

!

(cid:19)

 (cid:18)

with this rate improving to O(1/ log2(t)) for almost every dataset; and the margin converges as

(8)

(9)

(10)

3. In contrast, with non-vanishing regularization (i.e., λ > 0), arg minw L(w) + λ kwkp

p is generally not a max margin

solution.

1
ˆw

k

k

−

minn x⊤n w (t)
w (t)
k
k

= O

1
log t

(cid:18)

(cid:19)

.

6

GRADIENT DESCENT ON SEPARABLE DATA

On the other hand, the loss itself decreases as

(w (t)) = O

.

L

1
t

(cid:18)

(cid:19)

(11)

All the rates in the above Theorem are a direct consequence of Theorem 3, except for avoiding
the log log t factor for the degenerate cases in eq. 10 and eq. 11 (i.e., establishing that the rates
1/ log t and 1/t always hold)—this additional improvement is a consequence of the more complete
characterization of Theorem 13. Full details are provided in Appendix D. In this appendix, we also
provide a simple construction showing all the rates in Theorem 5 are tight (except possibly for the
log log t factors).

The sharp contrast between the tight logarithmic and 1/t rates in Theorem 5 implies that the
convergence of w(t) to the max-margin ˆw can be logarithmic in the loss itself, and we might need
to wait until the loss is exponentially small in order to be close to the max-margin solution. This can
help explain why continuing to optimize the training loss, even after the training error is zero and
the training loss is extremely small, still improves generalization performance—our results suggests
that the margin could still be improving signiﬁcantly in this regime.

A numerical illustration of the convergence is depicted in Figure 1. As predicted by the theory,
grows logarithmically (note the semi-log scaling), and w(t) converges to the max-
the norm
margin separator, but only logarithmically, while the loss itself decreases very rapidly (note the
log-log scaling).

w(t)
k

k

An important practical consequence of our theory, is that although the margin of w(t) keeps
improving, and so we can expect the population (or test) misclassiﬁcation error of w(t) to improve
for many datasets, the same cannot be said about the expected population loss (or test loss)! At the
limit, the direction of w(t) will converge toward the max margin predictor ˆw. Although ˆw has zero
training error, it will not generally have zero misclassiﬁcation error on the population, or on a test or
a validation set. Since the norm of w(t) will increase, if we use the logistic loss or any other convex
loss, the loss incurred on those misclassiﬁed points will also increase. More formally, consider the
logistic loss ℓ(u) = log(1 + e−
u). Since
−
N
n=1 h( ˆw⊤xn) = 0.
ˆw classiﬁes all training points correctly, we have that on the training set
However, on the population we would expect some errors and so E[h( ˆw⊤x)] > 0. Since w(t)
ˆw log t and ℓ(αu)

u) and deﬁne also the hinge-at-zero loss h(u) = max(0,

αh(u) as α

, we have:

P

≈

→

E[ℓ(w(t)⊤x)]

→ ∞
E[ℓ((log t) ˆw⊤x)]

≈

≈

(log t)E[h( ˆw⊤x)] = Ω(log t).

(12)

That is, the population loss increases logarithmically while the margin and the population misclassi-
ﬁcation error improve. Roughly speaking, the improvement in misclassiﬁcation does not out-weight
the increase in the loss of those points still misclassiﬁed.

The increase in the test loss is practically important because the loss on a validation set is
frequently used to monitor progress and decide on stopping. Similar to the population loss, the
validation loss will increase logarithmically with t, if there is at least one sample in the validation set
which is classiﬁed incorrectly by the max margin vector (since we would not expect zero validation
error). More precisely, as a direct consequence of Theorem 3 (as shown on Appendix D):

Corollary 6 Let ℓ be the logistic loss, and
such that x⊤ ˆw < 0. Then the validation loss increases as

V

be an independent validation set, for which

x

∃

∈ V

Lval (w (t)) =

ℓ

w (t)⊤ x

= Ω(log(t)).

x
X
∈V

(cid:16)

(cid:17)

7

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

(A)

3

2

1

0

−1

−2

−3
−3

2

x

101

102

104

105

106

101

102

104

105

106

1

0.5

(B)

|
|
)
t
(

w

|
|
 
d
e
z
i
l
a
m
r
o
N

(D)

0
100

x 10−3

8

6

4

2

p
a
g
 
e
l
g
n
A

0
100

103
t

103
t

 

(C)

100

)
)
t
(

w
(
L

10−5

10−10

 
100

(E)

p
a
g
 
n
i
g
r
a

M

0.1

0.05

0
100

GD
GDMO

103
t

103
t

−2

−1

1

2

3

101

102

104

105

106

101

102

104

105

106

0
x
1

Figure 1: Visualization of or main results on a synthetic dataset in which the L2 max margin vec-
tor ˆw is precisely known. (A) The dataset (positive and negatives samples (y =
1)
±
are respectively denoted by ′+′ and ′◦′), max margin separating hyperplane (black line),
and the asymptotic solution of GD (dashed blue). For both GD and GD with momentum
(GDMO), we show: (B) The norm of w (t), normalized so it would equal to 1 at the last
iteration, to facilitate comparison. As expected (eq. 3), the norm increases logarithmi-
1 (eq. 11); and (D&E) the
cally; (C) the training loss. As expected, it decreases as t−
angle and margin gap of w (t) from ˆw (eqs. 9 and 10). As expected, these are logarith-
mically decreasing to zero. Implementation details: The dataset includes four support
x2
vectors: x1 = (0.5, 1.5) , x2 = (1.5, 0.5) with y1 = y2 = 1, and x3 =
1 (the L2 normalized max margin vector is then ˆw = (1, 1) /√2 with
with y3 = y4 =
margin equal to √2 ), and 12 other random datapoints (6 from each class), that are not on
the margin. We used a learning rate η = 1/σ2
max (X) is the maximal
singular value of X, momentum γ = 0.9 for GDMO, and initialized at the origin.

max (X), where σ2

x1, x4 =

−

−

−

This behavior might cause us to think we are over-ﬁtting or otherwise encourage us to stop the
optimization. However, this increase does not actually represent the model getting worse, merely
w(t)
getting larger, and in fact the model might be getting better (increasing the margin and
k
possibly decreasing the error rate).

k

4. Extensions

4.1 Multi-Class Classiﬁcation with Cross-Entropy Loss

So far, we have discussed the problem of binary classiﬁcation, but in many practical situations
we have more then two classes. For multi-class problems, the labels are the class indices yn ∈
[K] ,
[K]. A common loss function
in multi-class classiﬁcation is the following cross-entropy loss with a softmax output, which is a
generalization of the logistic loss:

and we learn a predictor wk for each class k

1, . . . , K
{

∈

}

L

wk}k
{
(cid:0)

[K]

∈

=

−

(cid:1)

N

n=1
X

log

 

8

w⊤ynxn

exp
K
k=1 exp
(cid:0)

w⊤k xn
(cid:1)

!

P

(cid:0)

(cid:1)

(13)

GRADIENT DESCENT ON SEPARABLE DATA

Figure 2: Training of a convolutional neural network on CIFAR10 using stochastic gradient de-
scent with constant learning rate and momentum, softmax output and a cross entropy
loss, where we achieve 8.3% ﬁnal validation error. We observe that, approximately: (1)
1, (2) the L2 norm of last weight layer increases logarith-
The training loss decays as a t−
mically, (3) after a while, the validation loss starts to increase, and (4) in contrast, the
validation (classiﬁcation) error slowly improves.

What do the linear predictors wk(t) converge to if we minimize the cross-entropy loss by gradient
descent on the predictors? In Appendix E we analyze this problem for separable data, and show that
again, the predictors diverge to inﬁnity and the loss converges to zero. Furthermore, we prove the
following Theorem:

Theorem 7 For almost all multiclass datasets (i.e., except for a measure zero) which are linearly
separable (i.e. the constraints in eq. 15 below are feasible), any starting point w(0) and any small
enough stepsize, the iterates of gradient descent on 13 will behave as:

wk(t) = ˆwk log(t) + ρk(t),

where the residual ρk(t) is bounded and ˆwk is the solution of the K-class SVM:

argminw1,...,wk

2 s.t.

wk||
||

n,

k

∀

∀

= yn : w⊤ynxn ≥

w⊤k xn + 1.

K

Xk=1

4.2 Deep networks

(14)

(15)

So far we have only considered linear prediction. Naturally, it is desirable to generalize our results
also to non-linear models and especially multi-layer neural networks.

Even without a formal extension and description of the precise bias, our results already shed
light on how minimizing the cross-entropy loss with gradient descent can have a margin maximizing
effect, how the margin might improve only logarithmically slow, and why it might continue to
improve even as the validation loss increases. These effects are demonstrated in Figure 2 and Table
1 which portray typical training of a convolutional neural network using unregularized gradient
descent4. As can be seen, the norm of the weight increases, but the validation error continues
decreasing, albeit very slowly (as predicted by the theory), even after the training error is zero and
the training loss is extremely small. We can now understand how even though the loss is already

4. Code available here: https://github.com/paper-submissions/MaxMargin

9

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Epoch
L2 norm
Train loss
Train error
Validation loss
Validation error

2000
400
200
100
50
25.9
20.3
19.6
16.5
13.6
4
0.002
0.02
0.03
0.1
10−
0%
1.2% 0.6% 0.07%
4%
0.52
1.01
0.77
0.77
0.55
12.4% 10.4% 11.1% 9.1% 8.92%

3

5

4000
27.54
10−
·
0%
1.18
8.9%

Table 1: Sample values from various epochs in the experiment depicted in Fig. 2.

extremely small, some sort of margin might be gradually improving as we continue optimizing.
We can also observe how the validation loss increases despite the validation error decreasing, as
discussed in Section 3.

As an initial advance toward tackling deep network, we can point out that for several special
cases, our results may be directly applied to multi-layered networks. First, somewhat trivially, our
results may be applied directly to the last weight layer of a neural network if the last hidden layer
becomes ﬁxed and linearly separable after a certain number of iterations. This can become true,
either approximately, if the input to the last hidden layer is normalized (e.g., using batch norm), or
exactly, if the last hidden layer is quantized (Hubara et al., 2018).

Second, as we show next, our results may be applied exactly on deep networks if only a sin-
gle weight layer is being optimized, and, furthermore, after a sufﬁcient number of iterations, the
activation units stop switching and the training error goes to zero.

Corollary 8 We examine a multilayer neural network with component-wise ReLU functions f (z) =
l=1. Given input xn and target yn ∈ {−
max [z, 0], and weights
, the DNN produces a
scalar output
W2f (W1xn)))

un = WLf (WL

Wl}

1, 1
}

1f (

{

L

−

· · ·

and has loss ℓ (ynun), where ℓ obeys assumptions 2 and 3.

If we optimize a single weight layer wl = vec
N
n=1ℓ (ynun(wl)) converges to zero, and
wl(t)

converges to

∃

signs, then wl(t)/
P

k

k

t0 such that
(cid:0)
(cid:1)

∀

W⊤l

using gradient descent, so that

(wl) =
t > t0 the ReLU inputs do not switch

L

ˆwl = argmin
wl

wlk
k

2 s.t. ynun(wl)

1.

≥

Proof We examine the output of the network given a single input xn, for t > t0. Since the ReLU
inputs do not switch signs, we can write vl, the output of layer l, as

where we deﬁned Al,n for l < L as a diagonal 0-1 matrix, which diagonal is the ReLU slopes at
layer l, sample n, and AL,n = 1. Additionally, we deﬁne

vl,n =

Am,nWmxn ,

l

Ym=1

δl,n = Al,n

W⊤mAm,n ; ˜xl,n = δl,n ⊗

ul

1,n .

−

l+1

Ym=L

10

GRADIENT DESCENT ON SEPARABLE DATA

Using this notation we can write

un(wl) = vL,n =

Am,nWmxn = δ⊤l,nWlul

1,n = ˜x⊤l,nwl .

(16)

−

This implies that

(wl) =

ℓ (ynun(wl)) =

L

N

Xn=1

ℓ

yn˜x⊤l,nwl
(cid:16)

(cid:17)

,

which is the same as the original linear problem. Since the loss converges to zero, the dataset
N
n=1 must be linearly separable. Applying Theorem 3, and recalling that u(wl) = ˜x⊤l wl
˜xl,n, yn}
{
from eq. 16, we prove this corollary.

L

Ym=1

N

Xn=1

Importantly, this case is non-convex, unless we are optimizing the last layer. Note we assumed
ReLU functions for simplicity, but this proof can be easily generalized for any other piecewise linear
constant activation functions (e.g., leaky ReLU, max-pooling).

Lastly, in a follow-up work (Gunasekar et al., 2018b), given a few additional assumptions, ex-
tended our results to linear predictors which can be written as a homogeneous polynomial in the
parameters. These results seem to indicate that, in many cases, GD operating on exp-tailed loss
with positively homogeneous predictors aims to a speciﬁc direction. This is the direction of the max
margin predictor minimizing the L2 norm in the parameter space. It is not yet clear how to generally
translate such an implicit bias in the parameter space to the implicit bias in the predictor space — ex-
cept in special cases, such as deep linear neural nets, as we have shown in (Gunasekar et al., 2018b).
Moreover, in non-linear neural nets, there are many equivalent max-margin solutions which mini-
mize the L2 norm of the parameters. Therefore, it is natural to expect that GD would have additional
implicit biases, which select a speciﬁc subset of these solutions.

4.3 Other optimization methods

In this paper we examined the implicit bias of gradient descent. Different optimization algorithms
exhibit different biases, and understanding these biases and how they differ is crucial to under-
standing and constructing learning methods attuned to the inductive biases we expect. Can we
characterize the implicit bias and convergence rate in other optimization methods?

In Figure 1 we see that adding momentum does not qualitatively affect the bias induced by
gradient descent. In Figure 4 in Appendix F we also repeat the experiment using stochastic gradient
descent, and observe a similar asymptotic bias (this was later proved in Nacson et al. (2018)). This
is consistent with the fact that momentum, acceleration and stochasticity do not change the bias
when using gradient descent to optimize an under determined least squares problem. It would be
beneﬁcial, though, to rigorously understand how much we can generalize our result to gradient
descent variants, and how the convergence rates might change in these cases.

On the other hand, as an example of how changing the optimization algorithm does change the
bias, consider adaptive methods, such as AdaGrad (Duchi et al., 2011) and ADAM (Kingma and Ba,
2015). In Figure 3 we show the predictors obtained by ADAM and by gradient descent on a simple
data set. Both methods converge to zero training error solutions. But although gradient descent
converges to the L2 max margin predictor, as predicted by our theory, ADAM does not. The implicit
bias of adaptive methods has in fact been a recent topic of interest, with Hoffer et al. (2017) and

11

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

(A)

50

2

x

0

−50

101

102

104

105

106

101

102

104

105

106

(B)

|
|
)
t
(

w

|
|
 

d
e
z
i
l
a
m
r
o
N

(D)

1

0.5

0
100

p
a
g
 
e
l
g
n
A

0.15

0.1

0.05

0
100

103
t

103
t

 

(C)

100

)
)
t
(

w
(
L

10−10

10−20

 
100

(E)

2

1

p
a
g

 

n
i
g
r
a

M

0
100

GD
GDMO
ADAM

103
t

103
t

−50

50

0
x
1

101

102

104

105

106

101

102

104

105

106

Figure 3: Same as Fig. 1, except we multiplied all x2 values in the dastaset by 20, and also train
106 epochs of optimization using
using ADAM. The ﬁnal weight vector produced after 2
ADAM (red dashed line) does not converge to L2 max margin solution (black line), in
contrast to GD (blue dashed line), or GDMO.

·

Wilson et al. (2017) suggesting they lead to worse generalization, and Wilson et al. (2017) providing
examples of the differences in the bias for linear regression problems with the squared loss. Can
we characterize the bias of adaptive methods for logistic regression problems? Can we characterize
the bias of other optimization methods, providing a general understanding linking optimization
algorithms with their biases?

In a follow-up paper (Gunasekar et al., 2018) provided initial answers to these questions. Gunasekar et al.

(2018) derived a precise characterization of the limit direction of steepest descent for general norms
when optimizing the exp-loss, and show that for adaptive methods such as Adagrad the limit direc-
tion can depend on the initial point and step size and is thus not as predictable and robust as with
non-adaptive methods.

4.4 Other loss functions

In this work we focused on loss functions with exponential tail and observed a very slow, logarithmic
convergence of the normalized weight vector to the L2 max margin direction. A natural question
that follows is how does this behavior change with types of loss function tails. Speciﬁcally, does the
normalized weight vector always converge to the L2 max margin solution? How is the convergence
rate affected? Can we improve the convergence rate beyond the logarithmic rate found in this work?

In a follow-up work Nacson et al. (2018) provided partial answers to these questions. They
proved that the exponential tail has the optimal convergence rate, for tails for which ℓ′(u) is of the
uν ) with ν > 0.25. They then conjectured, based on heuristic analysis, that the expo-
form exp(
nential tail is optimal among all possible tails. Furthermore, they demonstrated that polynomial or
heavier tails do not converge to the max margin solution. Lastly, for the exponential loss they pro-
posed a normalized gradient scheme which can signiﬁcantly improve convergence rate, achieving
O(log(t)/√t).

−

12

GRADIENT DESCENT ON SEPARABLE DATA

4.5 Matrix Factorization

With multi-layered neural networks in mind, Gunasekar et al. (2017) recently embarked on a study
of the implicit bias of under-determined matrix factorization problems, where the squared loss of the
linear observation of a matrix is minimized by gradient descent on its factorization. Since a matrix
factorization can be viewed as a two-layer network with linear activations, this is perhaps the sim-
plest deep model one can study in full, and can thus provide insight and direction to studying more
complex neural networks. Gunasekar et al. conjectured, and provided theoretical and empirical evi-
dence, that gradient descent on the factorization for an under-determined problem converges to the
minimum nuclear norm solution, but only if the initialization is inﬁnitesimally close to zero and
the step-sizes are inﬁnitesimally small. With ﬁnite step-sizes or ﬁnite initialization, Gunasekar et al.
could not characterize the bias.

The follow-up paper (Gunasekar et al., 2018) studied this same problem with exponential loss
instead of squared loss. Under additional assumptions on the asymptotic convergence of update di-
rections and gradient directions, they were able to relate the direction of gradient descent iterates on
the factorized parameterization asymptotically to the maximum margin solution with unit nuclear
norm. Unlike the case of squared loss, the result for exponential loss are independent of initializa-
tion and with only mild conditions on the step size. Here again, we see the asymptotic nature of
exponential loss on separable data nullifying the initialization effects thereby making the analysis
simpler compared to squared loss.

5. Summary

We characterized the implicit bias induced by gradient descent on homogeneous linear predictors
when minimizing smooth monotone loss functions with an exponential tail. This is the type of loss
commonly being minimized in deep learning. We can now rigorously understand:

1. How gradient descent, without early stopping, induces implicit L2 regularization and con-
verges to the maximum L2 margin solution, when minimizing for binary classiﬁcation with
logistic loss, exp-loss, or other exponential tailed monotone decreasing loss, as well as for
multi-class classiﬁcation with cross-entropy loss. Notably, even though the logistic loss and
the exp-loss behave very different on non-separable problems, they exhibit the same behaviour
for separable problems. This implies that the non-tail part does not affect the bias. The bias
is also independent of the step-size used (as long as it is small enough to ensure convergence)
and is also independent on the initialization (unlike for least square problems).

2. The convergence of the direction of gradient descent updates to the maximum L2 margin
solution, however is very slow compared to the convergence of training loss, which explains
why it is worthwhile continuing to optimize long after we have zero training error, and even
when the loss itself is already extremely small.

3. We should not rely on plateauing of the training loss or on the loss (logistic or exp or cross-
entropy) evaluated on a validation data, as measures to decide when to stop.
Instead, we
should look at the 0–1 error on the validation dataset. We might improve the validation
and test errors even when when the decrease in the training loss is tiny and even when the
validation loss itself increases.

13

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Perhaps that gradient descent leads to a max L2 margin solution is not a big surprise to those for
whom the connection between L2 regularization and gradient descent is natural. Nevertheless, we
are not familiar with any prior study or mention of this fact, let alone a rigorous analysis and study
of how this bias is exact and independent of the initial point and the step-size. Furthermore, we also
analyze the rate at which this happens, leading to the novel observations discussed above. Even more
importantly, we hope that our analysis can open the door to further analysis of different optimization
methods or in different models, including deep networks, where implicit regularization is not well
understood even for least square problems, or where we do not have such a natural guess as for
gradient descent on linear problems. Analyzing gradient descent on logistic/cross-entropy loss is
not only arguably more relevant than the least square loss, but might also be technically easier.

Acknowledgments

The authors are grateful to J. Lee, and C. Zeno for helpful comments on the manuscript. The
research of DS was supported by the Israel Science Foundation (grant No. 31/1031), by the Taub
foundation and of NS by the National Science Foundation.

14

GRADIENT DESCENT ON SEPARABLE DATA

Appendix

Appendix A. Proof of Theorems 3 and 4 for almost every dataset

In the following sub-sections we ﬁrst prove Theorem 9 below, which is a version of Theorem 3,
specialized for almost every dataset. We then prove Theorem 4 (which is already stated for almost
every dataset).

Theorem 9 For almost every dataset which is linearly separable (Assumption 1), any β-smooth
decreasing loss function (Assumption 2) with an exponential tail (Assumption 3), any stepsize η <
max (X ) and any starting point w(0), the gradient descent iterates (as in eq. 2) will behave
2β−
as:

1σ−

2

w (t) = ˆw log t + ρ (t) ,

(17)

where ˆw is the L2 max margin vector

ˆw = argmin
w

Rd k

w

2 s.t.
k

n : w⊤xn ≥

∀

1,

∈

the residual ρ(t) is bounded, and so

k
In the following proofs, for any solution w (t), we deﬁne

k

k

k

w (t)
w (t)

=

ˆw
ˆw

.

lim
t
→∞

−
where ˆw and ˜w follow the conditions of Theorems 3 and 4, i.e. ˆw is the L2 is the max margin vector
deﬁned above, and ˜w is a vector which satisﬁes eq. 7:

−

r (t) = w (t)

ˆw log t

˜w,

n

∀

∈ S

: η exp

x⊤n ˜w

= αn ,

−

(cid:16)

(cid:17)

Rd

S ⊂ {

}

where we recall that we denoted X
a subset

1, . . . , N

of the columns of X = [x1, . . . , xN ]

×|S| as the matrix whose columns are the support vectors,
N .

Rd

S ∈

×

In Lemma 12 (Appendix B) we prove that for almost every dataset α is uniquely deﬁned, there
. Therefore, eq. 18 is well-deﬁned in those
are no more then d support vectors and αn 6
n
∀
cases. If the support vectors do not span the data, then the solution ˜w to eq. 18 might not be unique.
In this case, we can use any such solution in the proof.

= 0,

∈ S

∈

We furthermore denote the minimum margin to a non-support vector as:

(18)

(19)

θ = min
∈S

n /

x⊤n ˆw > 1 ,

and by Ci,ǫi,ti (i
Rd
P1 ∈
×
(the columns of X
X
).

S

N) various positive constants which are independent of t. Lastly, we deﬁne
d as the orthogonal projection matrix5 to the subspace spanned by the support vectors
P1 as the complementary projection (to the left nullspace of

∈
), and ¯P1 = I

−

S
5. This matrix can be written as P1 = XSX+

S , where M† is the Moore-Penrose pseudoinverse of M.

15

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

A.1 Simple proof of Theorem 9

In this section we ﬁrst examine the special case that ℓ (u) = e−
of gradient descent: η

0 , so

→

˙w (t) =

(w (t)) .

u and take the continuous time limit

−∇L
The proof in this case is rather short and self-contained (i.e., does not rely on any previous results),
and so it helps to clarify the main ideas of the general (more complicated) proof which we will give
in the next sections.

Recall we deﬁned

Our goal is to show that
implies that

r (t)
k

k

and therefore

r (t) = w (t)

log (t) ˆw

˜w .

−

−

is bounded, and therefore ρ (t) = r (t) + ˜w is bounded. Eq. 20

(20)

(21)

˙r (t) = ˙w (t)

ˆw =

(w (t))

−∇L

1
t

−

1
t

ˆw

−

1
2

d
dt k
N

r (t)
k

2 = ˙r⊤ (t) r (t)

exp

x⊤n w (t)

x⊤n r (t)

ˆw⊤r (t)

1
t

−

(cid:17)
log (t) ˆw⊤xn −

˜w⊤xn −

x⊤n r (t)
(cid:17)

x⊤n r (t)

ˆw⊤r (t)

1
t

−

#

=

=

+

Xn=1

−

(cid:16)

exp

−

(cid:16)

"

Xn
∈S





/
Xn
∈S

(cid:16)

exp

log (t) ˆw⊤xn −

˜w⊤xn −

−

x⊤n r (t)

x⊤n r (t)

,

(22)

(cid:17)





where in the last equality we used eq. 20 and decomposed the sum over support vectors
non-support vectors. We examine both bracketed terms. Recall that ˆw⊤xn = 1 for n
we deﬁned (in eq. 18) ˜w so that
22 can be written as

and
S
, and that
xn = ˆw. Thus, the ﬁrst bracketed term in eq.

˜w⊤xn

∈ S

exp

−

∈S

n

exp

P
˜w⊤xn −

(cid:1)
x⊤n r (t)

(cid:0)
x⊤n r (t)
(cid:17)
x⊤n r (t)

exp

1
t

1

−

−

(cid:17) (cid:16)

−

(cid:16)

−

(cid:16)

−

(cid:16)

=

exp

˜w⊤xn

1
t

1
t

Xn
∈S

Xn
∈S
z

(cid:17)

(cid:17)
z e−

zz

exp

˜w⊤xn

x⊤n r (t)

Xn
(cid:16)
∈S
x⊤n r (t)

−

0,

≤

(cid:17)

(23)

z, z (e−

since
19), the second bracketed term in eq. 22 can be upper bounded by

0. Furthermore, since

1)

≤

≤

−

∀

∀

1 and θ = argminn /
∈S

x⊤n ˆw > 1 (eq.

exp

log (t) ˆw⊤xn −

−

˜w⊤xn

exp

x⊤n r(t)
(cid:17)

−

(cid:16)

(cid:17)

/
Xn
∈S

(cid:16)

Substituting eq. 23 and 24 into eq. 22 and integrating, we obtain, that

x⊤n r(t)

1
tθ

≤

exp

˜w⊤xn

.

(24)

−

Xn
/
(cid:16)
∈S
C, C ′ such that

(cid:17)

∃

t1,

t > t1 :

∀

∀

2

r (t)
k
k

2
r(t1)
||

≤

C

− ||

t

dt
tθ ≤

t1

Z

C ′ <

,

∞

16

GRADIENT DESCENT ON SEPARABLE DATA

(25)

(27)

(28)

since θ > 1 (eq. 19). Thus, we showed that r(t) is bounded, which completes the proof for the

special case. (cid:4)

A.2 Complete proof of Theorem 9

Next, we give the proof for the general case (non-inﬁnitesimal step size, and exponentially-tailed
functions). Though it is based on a similar analysis as in the special case we examined in the
previous section, it is somewhat more involved since we have to bound additional terms.

First, we state two auxiliary lemmata, that are proven below in appendix sections A.4 and A.5:

(w) be a β-smooth non-negative objective. If η < 2β−

1, then, for any w(0), with

we have that

∞u=0 k∇L

(w (u))
k

2 <

∞

→∞ k∇L

2 = 0.
(w (t))
k

w (t + 1) = w (t)

η

(w(t))

−
∇L
and therefore limt

Lemma 10 Let
L
the GD sequence

P

Lemma 11 We have

C1, t1 :

t > t1 : (r (t + 1)

r (t))⊤ r (t)

C1t−

min(θ,1+1.5µ+,1+0.5µ−) .

(26)

∃
Additionally,

∀
ǫ1 > 0 ,

∀

∃

C2, t2, such that

≤

−
t > t2, if

∀
P1r (t)
k

ǫ1,

k ≥

then the following improved bound holds

(r (t + 1)

r (t))⊤ r (t)

−

C2t−

1 < 0 .

≤ −

Our goal is to show that

is bounded, and therefore ρ (t) = r (t) + ˜w is bounded. To

r (t)
k
k

show this, we will upper bound the following equation

r (t + 1)
k

2 =
k

r (t + 1)
k

r (t)
k

−

2 + 2 (r (t + 1)

r (t))⊤ r (t) +

−

2

r (t)
k
k

(29)

First, we note that ﬁrst term in this equation can be upper-bounded by

r (t + 1)
k
(1)
=

w (t + 1)

−

2

r (t)
k
ˆw log (t + 1)

˜w

−

−

w (t) + ˆw log (t) + ˜w

2
k

−
(w (t))

ˆw [log (t + 1)

η

∇L

(w (t))
k

k∇L

ˆw

2 log2
k

k

−
2 +

2

log (t)]
k
1

−
1 + t−

+ 2η ˆw⊤

∇L

(w (t)) log

1 + t−

1

k

(2)
=
k−
= η2
(3)

η2

≤

(w (t))

ˆw
k
where in (1) we used eq. 20, in (2) we used eq. 2, and in (3) we used
and also that

k∇L

k

2 +
k

2 t−

2

(cid:0)

(cid:1)

∀

(cid:0)

≥

x > 0 : x

log (1 + x) > 0,

(cid:1)

(30)

(31)

ˆw⊤

(w (t)) =

ℓ′

w (t)⊤ xn

∇L

ˆw⊤xn ≤

0 ,

(cid:17)

N

Xn=1

(cid:16)

17

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

since ˆw⊤xn ≥

1 (from the deﬁnition of ˆw) and ℓ′(u)

Also, from Lemma 10 we know that

0.

≤

(w (t))

2 = o (1) and
k

k∇L

(w (t))
k

2 <

.

∞

k∇L

∞

t=0
X
Substituting eq. 32 into eq. 30, and recalling that a t−
can ﬁnd C0 such that

ν power series converges for any ν > 1, we

r (t + 1)

k

2 = o (1) and
r (t)
k

−

r (t + 1)
k

−

r (t)

2 = C0 <
k

.

∞

∞

t=0
X

Note that this equation also implies that

ǫ0

∀
t > t0 :

t0 :

∃

∀

r (t + 1)

r (t)

< ǫ0 .

k − k

k|

|k

Next, we would like to bound the second term in eq. 29. From eq. 26 in Lemma 11, we can ﬁnd

t1, C1 such that

t > t1:

∀

(r (t + 1)

r (t))⊤ r (t)

C1t−

min(θ,1+1.5µ+,1+0.5µ−) .

−

≤

Thus, by combining eqs. 35 and 33 into eq. 29, we ﬁnd

(32)

(33)

(34)

(35)

2

r (t1)
k

− k

k

2
r (t)
k
1
t
−

=

u=t1 h
X

C0 + 2

C1u−

≤

1

t
−

u=t1
X

r (u + 1)
k

k

2

− k

r (u)

2
k

i

min(θ,1+1.5µ+,1+0.5µ−)

which is a bounded, since θ > 1 (eq. 19) and µ
bounded. (cid:4)

−

, µ+ > 0 (Deﬁnition 2). Therefore,

r (t)

is

k

k

A.3 Proof of Theorem 4

) = rank (X), and that ˜w is unique
All that remains now is to show that
given w (0). To do so, this proof will continue where the proof of Theorem 3 stopped, using
notations and equations from that proof.

0 if rank (X

r (t)

k →

k

S

∇L

(w) is spanned by the columns of X. If rank (X
S

Since r (t) has a bounded norm, its two orthogonal components r (t) = P1r (t) + ¯P1r (t) also
have bounded norms (recall that P1, ¯P1 were deﬁned in the beginning of appendix section A). From
) = rank (X), then it is also spanned
eq. 2,
(w) = 0. Therefore, ¯P1r (t) is not updated during GD, and
by the columns of X
remains constant. Since ˜w in eq. 20 is also bounded, we can absorb this constant ¯P1r (t) into ˜w
¯P1r (t) = 0). Thus, without loss of generality, we can
without affecting eq. 7 (since
assume that r (t) = P1r (t).

, and so ¯P1∇L
n

: x⊤n

∈ S

∀

S

We deﬁne the set

=

t > max [t2, t0] :
{

r (t)
k
k

< ǫ1}

.

T

18

GRADIENT DESCENT ON SEPARABLE DATA

By contradiction, we assume that the complementary set is not ﬁnite,

=

¯
t > max [t2, t0] :
{
T
is not ﬁnite:

r (t)
k

k ≥

.

ǫ1}

if it were ﬁnite, it would have had a ﬁnite maximal point

Additionally, the set
tmax ∈ T

T

, and then, combining eqs. 28, 29, and 33, we would ﬁnd that

r (t)

2
k

r (tmax)
k

− k

2 =

k

r (u + 1)

2

2

r (u)
k

k

− k

k

C0 −

≤

2C2

1

u−

,

→ −∞

t > tmax

∀

1

t
−

u=tmax
X

which is impossible since

0. Furthermore, eq. 33 implies that

r (u + 1)
k

−

2 = C0 −
r (t)
k

h (t)

where h (t) is a positive monotone function decreasing to zero. Let t3, t be any two points such that
t3 < t,

. For all such t3 and t, we have

1)

t3, t3 + 1, . . . t
{

} ⊂

¯
T

, and (t3 −

∈ T

2

r (t)
k

k

2 +
r (t3)
k

≤ k

r (u + 1)
k

2
k

2

r (u)
k

− k

i

i

1

t
−

u=tmax h
X
2

≥

k

r (t)
k
t

u=0
X

1

t
−

u=t3 h
X
1
t
−

=

2 +
r (t3)
k

k

u=t3 h
X
2 + h (t3)
r (t3)
k

≤ k

2 + h (t3) .
r (t3)
k

≤ k

r (u + 1)
k

−

2 + 2 (r (u + 1)
r (u)
k

−

r (u))⊤ r (u)

h (t

1)

−

−

−

2C2

1

u−

1

t
−

u=t3
X

i

(36)

1)

r (t3 −
< ǫ0. Since
Also, recall that t3 > t0, so from eq. 34, we have that
k|
ǫ1 + ǫ0. Moreover, since ¯
r (t3 −
deﬁnition), we conclude that
T
k
is an inﬁnite set, we can choose t3 as large as we want. This implies that
ǫ2 > 0 we can ﬁnd t3
such that ǫ2 > h (t3), since h (t) is a monotonically decreasing function. Therefore, from eq. 36,

|k
r (t3)
k

< ǫ1 (from

r (t3)

k − k

k ≤

1)

∀

T

k

ǫ1, ǫ0, ǫ2,

∀

t3 ∈

∃

¯
T

such that

t > t3 :

r (t)

ǫ1 + ǫ0 + ǫ2 .

∀

k

2
k

≤

This implies that

r (t)

0.

k

k →

This sets ˜w uniquely, together with eq. 7. (cid:4)

Lastly, we note that since ¯P1r (t) is not updated during GD, we have that ¯P1 ( ˜w

w (0)) = 0.

−

A.4 Proof of Lemma 10

Lemma 10 Let
L
the GD sequence

(w) be a β-smooth non-negative objective. If η < 2β−

1, then, for any w(0), with

we have that

∞u=0 k∇L

(w (u))
k

2 <

∞

→∞ k∇L

2 = 0.
(w (t))
k

w (t + 1) = w (t)

η

(w(t))

−
∇L
and therefore limt

(25)

P

19

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

This proof is a slightly modiﬁed version of the proof of Theorem 2 in (Ganti, 2015). Recall a

well-known property of β-smooth functions:

f (x)

f (y)

f (y)⊤ (x

y)

−

− ∇

−

≤

β
2 k

x

y

2 .

−

k

(37)

From the β-smoothness of

(w)

(cid:12)
(cid:12)
(cid:12)

L

(w (t + 1))

(w (t)) +

(w (t))⊤ (w (t + 1)

w (t)) +

w (t + 1)

w (t)

β
2 k

−

2
k

L

(cid:12)
(cid:12)
(cid:12)

−

≤ L

=

=

L

L

∇L

η

η

(w (t))

(w (t))

−

k∇L

2 +
k

βη2
2 k∇L

2

(w (t))
k

(w (t))

−

βη
2

1

−

(cid:18)

k∇L

(cid:19)

2
(w (t))
k

(w (t))

L

− L
1

−

η

(cid:16)

(w (t + 1))
βη
2

(cid:17)

≥ k∇L

2
(w (t))
k

Thus, we have

which implies

t

u=0
X

2
(w (u))
k

≤

k∇L

t

L

u=0
X

(w (u))

(w (u + 1))
βη
2

η

− L
1

−

(w (0))

= L

(w (t + 1))
βη
2

.

η

− L
1

−

(cid:16)
The right hand side is upper bounded by a ﬁnite constant, since L (w (0)) <
This implies

(cid:17)

(cid:16)

∞

(cid:17)
and 0

≤ L

(w (t + 1)).

(w (u))

2 <
k

,

∞

k∇L

∞

u=0
X

and therefore

2

(w (t))
k

→

0. (cid:4)

k∇L

A.5 Proof of Lemma 11

Recall that we deﬁned r (t) = w (t)
Theorems 3 and 4, i.e, ˆw is the L2 max margin vector and (eq. 4), and eq. 7 holds

˜w, with ˆw and ˜w follow the conditions of the

ˆw log t

−

−

C1, t1 :

t > t1 : (r (t + 1)

r (t))⊤ r (t)

C1t−

min(θ,1+1.5µ+,1+0.5µ−) .

(26)

n

∀

∈ S

: η exp

x⊤n ˜w

= αn .

−

(cid:16)

Lemma 11 We have

∃
Additionally,

∀
ǫ1 > 0 ,

∀

∃

C2, t2, such that

then the following improved bound holds

(cid:17)

≤

−
t > t2, if

∀
P1r (t)
k

ǫ1,

k ≥

(r (t + 1)

r (t))⊤ r (t)

−

C2t−

1 < 0 .

≤ −

20

(27)

(28)

GRADIENT DESCENT ON SEPARABLE DATA

From Lemma 1,

n :

w (t)⊤ xn =
→∞
ℓ′ (u) has an exponential tail e−

limt

∀

loss derivative
generality). Combining both facts, we have positive constants µ

−

. In addition, from assumption 3 the negative
∞
u (recall we assume a = c = 1 without loss of

, µ+, t

and t+ such that

n

∀

t > t+ :

w (t)⊤ xn

1 + exp

µ+w (t)⊤ xn

∀

∀

t > t

:

−

ℓ′

ℓ′

−

−

(cid:16)

w (t)⊤ xn

(cid:17)

≤

≥

(cid:16)
1

−

(cid:16)

exp

−

µ

−

−

w (t)⊤ xn

w (t)⊤ xn

w (t)⊤ xn

(cid:17)

(cid:16)

−

−

(38)

(39)

(cid:16)
Next, we examine the expression we wish to bound, recalling that r (t) = w (t)

(cid:17)(cid:17)

(cid:16)

(cid:16)

(cid:16)

(cid:17)

(cid:17)
ˆw log t

−

˜w:

−

−

−

exp

exp

(cid:17)(cid:17)

ˆw [log (t + 1)

log (t)])⊤ r (t)

ℓ′

w (t)⊤ xn

x⊤n r (t)

ˆw⊤r (t) log

1 + t−

1

(r (t + 1)

r (t))⊤ r (t)

−
(w (t))

−

= (

η

−

∇L
N

=

η

−

Xn=1
= ˆw⊤r (t)

(cid:16)
t−

1

(cid:17)
1 + t−

1

log

−

η

−

(cid:2)
1 exp
t−

(cid:0)
˜w⊤xn

(cid:1)(cid:3)
+ ℓ′

−

−

−

η

−

Xn /
(cid:16)
∈S
w (t)⊤ xn

(cid:17)
x⊤n r (t)

(cid:16)

(cid:17)i

Xn
∈S h
where in last line we used eqs. 6 and 7 to obtain

(cid:17)

(cid:16)

(cid:0)

ℓ′

w (t)⊤ xn

(cid:1)
x⊤n r (t)

(40)

We examine the three terms in eq. 40. The ﬁrst term can be upper bounded by

ˆw =

αnxn = η

exp

˜w⊤xn

xn .

Xn
∈S

Xn
∈S

−

(cid:16)

(cid:17)

ˆw⊤r (t)

1

t−

log

1 + t−

1

−

max

h

max

≤
(1)

≤
(2)

h
ˆw
k
k
≤ (
t−
o

1

(cid:2)

−

log
(cid:1)(cid:3)

ˆw⊤r (t) , 0

t−
(cid:0)
i (cid:2)
ˆw⊤P1r (t) , 0
i
P1r (t)
k ≤
P1r (t)
k

ǫ1t−
1

, if
, if

k
k

t−

(cid:0)

2

2

ǫ1
> ǫ1

1 + t−

1

(cid:1)(cid:3)

(41)

(cid:1)
where in (1) we used that ¯P1 ˆw = ¯P1X
S
since

(cid:0)

α = 0 from eq. 6, and in (2) we used that ˆw⊤r (t) = o (t),

ˆw⊤r (t) = ˆw⊤

w (0)

 
ˆw⊤ (w (0)

≤

t

η

−

∇L

u=0
X

(w (u))

ˆw log (t)

˜w

−

−

˜w

−

−

ˆw log (t))

−

ηt min
0
≤
≤

u

t

ˆw⊤

∇L

!
(w (u)) = o (t)

where in the last line we used that

(w (t)) = o (1), from Lemma 10.

Next, we upper bound the second term in eq. 40. From eq. 38

∇L

ℓ′(w(t)⊤xn)

2 exp(

≤

−

∃
w(t)⊤xn).

t′+, such that

> t0 > t′+,

∀

(42)

21

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Therefore,

t > t′+:

∀

−

η

η

≤ −

ℓ′

w (t)⊤ xn

x⊤n r (t)

Xn /
∈S

(cid:16)

: x⊤
Xn /
n
∈S

r(t)

ℓ′
0
(cid:16)

≥

(cid:17)
w (t)⊤ xn

x⊤n r (t)

(cid:17)

2 exp

w (t)⊤ xn

x⊤n r (t)

−

(cid:16)

n ˆw
x⊤

exp

(cid:17)

˜w⊤xn −

−

x⊤n r (t)

x⊤n r (t)

(cid:17)

: x⊤
Xn /
n
∈S

r(t)

≥

0

: x⊤
Xn /
n
∈S

r(t)

: x⊤
Xn /
n
∈S

r(t)

2t−
0

≥

2t−
0

≥

n ˆw
x⊤

exp

˜w⊤xn

(cid:17)

(cid:16)

−

(cid:16)

θ

t−

ηN exp

min
n

−

˜w⊤xn

(cid:16)

(cid:17)

(1)

≤

η

(2)

≤

η

(3)

≤

η

(4)

≤

≥

η

−

(43)

where in (1) we used eq. 42, in (2) we used w (t) = ˆw log t + ˜w + r (t), in (3) we used
x

0,and in (4) we used θ > 1, from eq. 19.

1 and x⊤n r (t)

xe−

≤

Lastly, we will bound the sum in the third term in eq. 40

t−

1 exp

−

˜w⊤xn

+ ℓ′

w (t)⊤ xn

x⊤n r (t) .

(44)

(cid:17)
We examine each term n in this sum, and divide into two cases, depending on the sign of x⊤n r (t).

(cid:17)i

(cid:16)

(cid:16)

Xn
∈S h

First, if x⊤n r (t)

0, then term n in eq. 44 can be upper bounded

t > t+, using eq. 38, by

ηt−

1 exp

1 + t−

µ+ exp

µ+ ˜w⊤xn

exp

1

x⊤n r (t)

(45)

−

(cid:16)

(cid:17)(cid:17)

∀
x⊤n r (t)
(cid:17)

−

(cid:16)

−

i

0.5µ+, then we can upper bound eq. 45 with

≥
˜w⊤xn

−

(cid:17) h(cid:16)
(cid:16)
We further divide into cases:

1. If

x⊤n r(t)

C0t−

≤

(cid:12)
(cid:12)

(cid:12)
(cid:12)

η exp

(1 + µ+) min

˜w⊤xn

C0t−

1

1.5µ+ .

−

(46)

−

(cid:16)

(cid:17)

2. If

x⊤n r(t)

> C0t−

0.5µ+, then we can ﬁnd t′′+ > t′+ to upper bound eq. 45

t > t′′+:

∀

(cid:12)
ηt−
(cid:12)

1e−

(cid:12)
˜w⊤xn
(cid:12)

1 + t−

µ+e−

µ+ ˜w⊤xn

exp

0.5µ+

C0t−

1

x⊤n r (t)

ηt−

1e−

˜w⊤xn

1 + t−

µ+e−

(cid:17)

µ+ ˜w⊤xn

−

1

(cid:0)
−

C0t−

0.5µ+ + C 2

−

(cid:1)

i
0 t−

µ+

1

x⊤n r (t)

−

ηt−

1e−

˜w⊤xn

C0t−

0.5µ+ + C 2

(cid:17) (cid:0)
0 t−

µ+

µ+ min

n

e−

˜w⊤xnt−

µ+

i
(cid:1)
0.5µ+ + C 2
C0t−
−

0 t−

µ+

x⊤n r (t)

(1)

≤

≤
(2)

h(cid:16)

h(cid:16)
1

(cid:20)

(cid:0)

−

0,

t > t′′+

∀

≤
where in (1) we used the fact that e−
that the previous expression is negative — since t−

≤

−

1

x

x + x2 for x

0 and in (2) we deﬁned t′′+ so

0.5µ+ decreases slower than t−

µ+.

≥

(cid:21)

(47)

n

(cid:1)

22

GRADIENT DESCENT ON SEPARABLE DATA

3. If

x⊤n r(t)
≥
and therefore

(cid:12)
(cid:12)

(cid:12)
(cid:12)

This implies that

ǫ2, then we deﬁne t′′′+ > t′′+ such that t′′′+ > exp
µ+ ˜w⊤xn
t > t′′′+, we have
1 + t−
t > t′′′+ we can upper bound eq. 45 by

µ+ exp

−

∀

(cid:0)

(cid:0)

(cid:0)
(cid:1)(cid:1)

∀

minn ˜w⊤xn
ǫ2 < e−
e−

e0.5ǫ2

1/µ+ ,

−

1

0.5ǫ2 .
(cid:1) (cid:2)

−

(cid:3)

Second, if x⊤n r(t) < 0, we again further divide into cases:

η exp

−

max
n

−

˜w⊤xn

(cid:16)

1
(cid:17) (cid:0)

−

(cid:1)

0.5ǫ2

e−

ǫ2t−

1.

(48)

1. If

x⊤n r(t)
eq. 44 with
(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤

C0t−

0.5µ−, then, since

ℓ′

w (t)⊤ xn

> 0, we can upper bound term n in

ηt−

1 exp

˜w⊤xn

−

η exp

min
n

−

˜w⊤xn

1

0.5µ−

C0t−

−

(49)

(cid:16)
(cid:16)
0.5µ− , then, using eq. 39 we upper bound term n in eq. 44 with

(cid:17)

(cid:17) (cid:12)
(cid:12)
(cid:12)

2. If

x⊤n r (t)

> C0t−

(cid:17)

−

(cid:16)

≤

x⊤n r (t)
(cid:12)
(cid:12)
(cid:12)

ℓ′

w (t)⊤ xn

x⊤n r (t)

t−

(cid:12)
1e−
(cid:12)

˜w⊤xn

−
˜w⊤xn +

1e−

(cid:12)
η
(cid:12)

η

−
h
−
≤
h
1e−
=ηt−

t−

˜w⊤xn

1

−

h

(cid:16)
1
−

(cid:16)
exp

exp

(cid:17)i
µ
−
−
(cid:16)
r (t)⊤ xn

−

(cid:16)

Next, we will show that

t′
∃
−
Let M > 1 be some arbitrary constant. Then, since

> t

−

exp

(cid:16)
if exp

−

µ

w (t)⊤ xn

−
r (t)⊤ xn

0 from Lemma 1,

→
M > 1 then

−

(cid:16)

(cid:17)
≥

(cid:17)

w (t)⊤ xn

exp

w (t)⊤ xn

−
(cid:16)
˜w⊤xn exp

(cid:17)(cid:17)
1e−

t−

1

−

x⊤n r (t)
µ−

(cid:17)i
r (t)⊤ xn

−

h

(cid:16)

(cid:17)i

(cid:17) (cid:16)

(cid:17)i (cid:12)
(cid:12)
(cid:12)
such that the last expression is strictly negative
∀
1e−
r (t)⊤ xn
−
˜w⊤xn) such that

˜w⊤xn exp

tM > max(t

, M e−

t−

(cid:16)

h

∃

−

x⊤n r (t)
(cid:12)
(50)
(cid:12)
(cid:12)
.

t > t′
µ−
−
=

(cid:17)i
t > tM ,
∀

exp

r (t)⊤ xn

t−

1e−

˜w⊤xn exp

1

−

(cid:16)
Furthermore, if

(cid:17) (cid:16)
t > tM such that exp

h

r (t)⊤ xn

r (t)⊤ xn

−

(cid:16)
< M , then

µ−

≥

(cid:17)i

(cid:17)

−

∃

exp

r (t)⊤ xn

1

t−

1e−

(cid:17)
˜w⊤xn exp

µ−

r (t)⊤ xn

M ′ > 1 .

(51)

> exp

r (t)⊤ xn

t−

1e−

˜w⊤xnM

(cid:17)i

(cid:17)

(52)

−

−

(cid:16)

(cid:16)

(cid:16)

−

−

h

h

(cid:17) (cid:16)
1

(cid:17) (cid:16)

−

µ−
(cid:16)

.

i

(cid:17)

which is lower bounded by

1 + C0t−

0.5µ−

µ−

t−

e−

µ−

˜w⊤xnM
µ−

1

−
µ−

(cid:1) (cid:16)
t−
−

(cid:0)
1 + C0t−

0.5µ−

h
˜w⊤xnM

e−

(cid:17)
1.5µ−

i
t−
−

˜w⊤xnM

e−

µ−

C0

> C0t−

since
1 + x. In this case last line is strictly
larger than 1 for sufﬁciently large t. Therefore, after we substitute eqs. 51 and 52 into 50, we
ﬁnd that

, term k in eq. 44 is strictly negative

(cid:12)
(cid:12)
> tM > t

such that

≥

(cid:12)
(cid:12)

h
0.5µ−, x⊤n r (t) < 0 and ex

i

h

i

≥
x⊤n r (t)

t′
−

∃

ℓ′

w (t)⊤ xk

x⊤k r (t) < 0

(53)

t > t′
−

∀
˜w⊤xk

−

t−

1e−

−

η

−

h

(cid:16)
23

(cid:17)i

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

3. If

x⊤k r(t)
t > t′
(cid:12)
(cid:12)
∀
−
that
(cid:12)
(cid:12)
t > t′′
−

∀

ǫ2 , which is a special case of the previous case (
, either eq. 51 or 52 holds. Furthermore, in this case,

≥

eq. 52 can be lower bounded by

x⊤k r (t)
> t′
t′′
(cid:12)
(cid:12)
∃
−
−
(cid:12)
(cid:12)

0.5µ−) then
> C0t−
and M ′′ > 1 such

exp (ǫ2)

1

−

h

(cid:16)

t−

1e−

˜w⊤xk M

µ−

> M ′′ > 1 .

i

(cid:17)

Substituting this, together with eq. 51, into eq. 50, we can ﬁnd C ′0 > 0 such we can upper
bound term k in eq. 44 with

(54)

To conclude, we choose t0 = max

1. If

P1r (t)
k

k ≥

ǫ1 (as in Eq. 27), we have that

(cid:3)

(cid:2)

C ′0t−

1 ,

t > t′′
−

∀

.

−
:

t′′′+, t′′
−

2 (1)

1

≥

max
n
∈S (cid:12)
(cid:12)
(cid:12)

x⊤n r (t)
(cid:12)
(cid:12)
(cid:12)

|S| Xn
∈S (cid:12)
(cid:12)
(cid:12)

x⊤n P1r (t)
(cid:12)
(cid:12)
(cid:12)
∈ S

2

=

1

X⊤
S

P1r (t)
(cid:13)
(cid:13)
(cid:13)

|S| (cid:13)
(cid:13)
(cid:13)

2 (2)

1

≥

|S|

where in (1) we used P⊤1 xn = xn ∀
non-zero singular value of X

1 σ2

S

−

) ǫ2
min (X
|S|
maxn ˜w⊤xn
q
η exp
bounded by
C ′′0 t−
(cid:1)
(cid:0)
eqs. 41 and 43 into eq. 40, we obtain

1
1 + o
(cid:1) (cid:0)

e−
1
,

−
t−

0.5ǫ2

−

−

∀

S

n

, in (2) we denoted by σmin (X
and used eq. 27. Therefore, for some k,

), the minimal
S
ǫ2 ,
x⊤k r
1. In this case, we denote C ′′0 as the minimum between C ′0 (eq. 54) and
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ǫ2 (eq. 48). Then we ﬁnd that eq. 44 can be upper
t > t0, given eq. 27. Substituting this result, together with

≥

σ2
min (X

) ǫ2
1

S

(55)

t > t0

∀

r (t))⊤ r (t)

C ′′0 t−

1 + o

1

t−

.

≤ −

(cid:0)
t2 > t0 such that eq. 28 holds. This implies also that eq.

(cid:1)

(cid:0)
(cid:1)
(r (t + 1)

−
C2 < C ′′0 and
ǫ1.

∃

k ≥

This implies that
26 holds for

∃
P1r (t)

k
2. Otherwise, if

k

P1r (t)

< ǫ1, we ﬁnd that

t > t0 , each term in eq. 44 can be upper bounded
0.5µ−, (eq.
by either zero (eqs. 47 and 53), or terms proportional to t−
49). Combining this together with eqs. 41, 43 into eq. 40 we obtain (for some positive
constants C3, C4, C5, and C6)

1.5µ+ (eq. 46) or t−

∀

k

−

−

1

1

(r (t + 1)

r (t))⊤ r (t)

C3t−

1

1.5µ+ + C4t−

1

−

0.5µ− + C5t−

2 + C6t−

θ .

−

−

≤
t1 > t0 and C1 such that eq. 26 holds. (cid:4)

Therefore,

∃

Appendix B. Generic solutions of the KKT conditions in eq. 6

Lemma 12 For almost all datasets there is a unique α which satisﬁes the KKT conditions (eq. 6):

N

n

ˆw =

αnxn

αn ≥
Xn=1
(cid:16)
Furthermore, in this solution αn 6
are at most d such support vectors.

∀

0 and ˆw⊤xn = 1

OR

αn = 0 and ˆw⊤xn > 1

(cid:17)
= 0 if ˆw⊤xn = 1, i.e., xn is a support vector (n

(cid:16)

(cid:17)

), and there

∈ S

24

GRADIENT DESCENT ON SEPARABLE DATA

For almost every set X, no more than d points xn can be on the same hyperplane. Therefore,
since all support vectors must lie on the same hyperplane, there can be at most d support vectors,
for almost every X.

Given the set of support vectors,

, the KKT conditions of eq. 6 entail that αn = 0 if n /

and

where we denoted α
S
X
since d
S ∈

, X⊤
S

≥ |S|

as α restricted to the support vector components. For almost every set X,

R|S|×|S| is invertible. Therefore, α

has the unique solution

S

1 = X⊤
S

ˆw = X⊤
S

X
S

α
S

,

S

1

−

1 = α
S

.

X

X⊤
S

S

(cid:16)

(cid:17)

∈ S

(56)

(57)

S

n

∈ S

) /qn (X
S

This implies that
pn (X
S
) = 0, so the components of X
then pn (X
polynomial pn have measure zero, unless
equal to zero, since, for example, if X⊤
S
in this case

, αn is equal to a rational function in the components of XS, i.e., αn =
∀
), where pn and qn are polynomials in the components of XS. Therefore, if αn = 0,
must be at a root of the polynomial pn. The roots of the
S
X
) = 0. However, pn cannot be identically
∀
S
I
X
, and so
=
(d
= 0, from eq. 57.
, the event that "eq. 56 has a solution with a zero component" has a
, also has zero measure, as a
). This
. Furthermore, for almost all datasets
, the solution eq.

zero measure. Moreover, the union of these events, for all possible
ﬁnite union of zero measures sets (there are only ﬁnitely many possible sets
implies that, for almost all datasets X, αn = 0 only if n /
the solution α is unique: for each dataset,
56 is uniquely given by eq. 57. (cid:4)

n
Therefore, for a given

is uniquely determined, and given

: pn (X
, 0

, then X⊤
S

, αn = 1

1, . . . , N

S ⊂ {

)
−|S|

|S|×|S|

|S|×|S|

= I

∈ S

∈ S

|S|×

S

S

S

S

∀

}

(cid:2)

(cid:3)

S

S

Appendix C. Completing the proof of Theorem 3 for zero measure cases

In the preceding Appendices, we established Theorem 4, which only applied when all support vec-
tors are associated with non-zero coefﬁcients. This characterizes almost all data sets, i.e. all except
for measure zero. We now turn to presenting and proving a more complete characterization of the
limit behaviour of gradient descent, which covers all data sets, including those degenerate data sets
not covered by Theorem 4, thus establishing Theorem 3.

In order to do so, we ﬁrst have to introduce additional notation and a recursive treatment of the
data set. We will deﬁne a sequence of data sets ¯PmX ¯
m obtained by considering only a subset ¯
Sm
S
of the points, and projecting them using the projection matrix ¯Pm. We start, for m = 0, with the
full original data set, i.e. ¯
and ¯P0 = Id
d. We then deﬁne ˆwm as the max margin
×
predictor for ¯Pm
−

S0 =
1, . . . , N
{
m−1, i.e.:

}

1X ¯
S

ˆwm = argmin

w

Rd k

w

2 s.t. w⊤ ¯Pm
k
−

1xn ≥

1

n

∀

∈

¯
Sm
−

1 .

(58)

∈

In particular, ˆw1 is the max margin predictor for the original data set. We then denote
of non-support vectors for 58,
the dual variables corresponding to the margin constraints (for some dual solution), and ¯

+
m the indices
S
Sm the indices of support vector of 58 with non-zero coefﬁcients for
Sm the set

25

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

of support vector with zero coefﬁcients. That is:

+
m =

=
m =

S

S

n

n

¯
Sm
−
¯
Sm
−

1|

1|

ˆw⊤m

ˆw⊤m

¯Pm
−
¯Pm
−

∈

∈

n

n

1xn > 1

1xn = 1

o

= ¯

+
m

Sm \ S

Sm =
¯
Sm =

n

=
m|∃
∈ S
(
m \ Sm .
S

=

α

RN
≥

∈

0 : ˆwm =

αk ¯Pm
−

1xk, αn > 0,

i /

∀

∈ S

=
m : αi = 0

)

o
N

Xk=1

(59)

The problematic degenerate case, not covered by the analysis of Theorem 4, is when there are
support vectors with zero coefﬁcients, i.e., when ¯
. In this case we recurse on these zero-
∅
coefﬁcient support vectors (i.e., on ¯
Sm), but only consider their components orthogonal to the non-
zero-coefﬁcient support vectors (i.e., not spanned by points in

Sm 6

=

Sm). That is, we project using:

1

m

S

(cid:17)

¯Pm = ¯Pm
−

X

mX†
S

This recursive treatment continues as long as ¯

Id −
(cid:16)
¯Pm.
where we denoted A† as the Moore-Penrose pseudo-inverse of A. We also denote Pm = Id −
, deﬁning a sequence ˆwm of max margin
∅
predictors, for smaller and lower dimensional data sets ¯Pm
m−1. We stop when ¯
and
Sm =
−
denote the stopping stage M —that is, M is the minimal m such that ¯
. Our characterization
∅
will be in terms of the sequence ˆw1, . . . , ˆwM . As established in Lemma 12 of Appendix B, for
almost all data sets we will not have support vectors with non-zero coefﬁcients, and so we will have
M = 1, and so the characterization only depends on the max margin predictor ˆw1 of the original
data set. But, even for the measure zero of data sets in which M > 1, we provide the following
more complete characterization:

Sm =

1X ¯
S

Sm 6

(60)

=

∅

Theorem 13 For all datasets which are linearly separable (Assumption 1) and given a β-smooth
loss function (Assumption 2) with an exponential tail (Assumption 3), gradient descent (as in eq. 2)
max (X ) and any starting point w(0), the iterates of gradient descent can
with step size η < 2β−
be written as:

1σ−

2

w (t) =

ˆwm log◦

m (t) + ρ (t) ,

(61)

M

m=1
X

log (t), ˆwm is the L2 max margin vector deﬁned in eq. 58, and the

m times

where log◦
log log
residual ρ (t) is bounded.

m (t) =

· · ·

}|

{

z

C.1 Auxiliary notation
We say that a function f : N
f (t)

L1. Furthermore, we deﬁne

→

∈

R is absolutely summable if

∞t=1 |

f (t)
|

<

∞

, and then we denote

r (t) = w (t)

ˆwm log◦

m (t) + ˜wm +

m=1 "
Xk=1
X
where ˜wm and ˇwk,m are deﬁned next, and additionally, we denote

Q

M

−

ˇwk,m
m
1
r=k log◦

−

r (t) #

P
m

1

−

26

GRADIENT DESCENT ON SEPARABLE DATA

˜w =

˜wm .

M

m=1
X

We deﬁne,

m

1, ˜wm as the solution of

∀

≥

m

∀

≥

1 :

n

∀

∈ Sm : η

exp

m

 −

Xk=1

Xn
∈S

m

˜w⊤k xn

¯Pm
−

!

1xn = ˆwm ,

(62)

such that

such that

Pm

1 ˜wm = 0 and ¯Pm ˜wm = 0.

−

The existence and uniqueness of the solution, ˜wm are proved in appendix section C.4.

Lastly, we deﬁne,

m > k

1, ˇwk,m as the solution of

∀

≥

exp

˜w⊤xn

Pm

1xn =

−

(cid:16)

−

(cid:17)

Xn
∈S

m

m

1

−

Xk=1

Xn
∈Sk





exp

˜w⊤xn

ˇwk,m

(64)

−

(cid:16)

xnx⊤n 


(cid:17)

Pk

1 ˇwk,m = 0 and ¯Pk ˇwk,m = 0 .

−

The existence and uniqueness of the solution ˇwk,m are proved in appendix section C.5.

Together, eqs. 62-65 entail the existence of a unique decomposition,

ˆwm = η

exp

˜w⊤xn

−

xn −

η

Xn
∈S
given the constraints in eqs. 63 and 65 hold.

(cid:17)

(cid:16)

m

m

1

−

Xk=1


Xn
∈Sk


−

(cid:16)

exp

˜w⊤xn

ˇwk,m

(66)

m

∀

≥

1 :

xnx⊤n 


(cid:17)

C.2 Proof of Theorem 13

In the following proofs, for any solution w(t), we deﬁne

τ (t) =

ˆwm log◦

m (t) +

M

Xm=2

M

m

1

−

Xm=1

Xk=1

ˇwk,m
1
m
r=k log◦

−

r (t)

τ (t + 1)
k

−

τ (t)

k ≤

Q
Cτ
t log (t)

noting that

and

r(t) = w(t)

ˆw1 log (t)

−

˜w

−

−

τ (t)

where ˜w follow the conditions of Theorem 13. Our goal is to show that
this, we will upper bound the following equation

r(t)
k
k

is bounded. To show

2 =
r(t + 1)
k
k

r(t + 1)
k

−

2 + 2 (r(t + 1)
r(t)
k

−

r(t))⊤ r(t) +

2
r(t)
k
k

27

(63)

(65)

(67)

(68)

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

First, we note that

t0 such that

t > t0 the ﬁrst term in this equation can be upper bounded by

∀

2
||
ˆw1 log (t + 1)

∃
r(t)

r(t + 1)

−

||
(1)
=

w(t + 1)

||

η

(2)
=
= η2

|| −

||∇

−
L(w(t))

∇
L(w(t))

∇

L(w(t))⊤
+ 2η
+ 2 ˆw⊤1 (τ (t + 1)
(3)

(cid:0)
−
2 +
||

τ (t + 1)

w(t) + ˆw1 log (t) + τ (t)

−

−
log (t))

−
2 +
||

ˆw1(log (t + 1)
2 log2
ˆw1k
k
1
ˆw1 log
(cid:0)
1 + t−
(cid:1)

τ (t)) log
(cid:0)

1 + t−

1

(τ (t + 1)

2
||

τ (t))
−
2
τ (t)
k

−

−
1 + t−

1

−
τ (t + 1)

+

k
+ τ (t + 1)

(cid:1)

τ (t)

−

2
||

(cid:1)
2 (t) + 2Cτ k

η2

≤

||∇

L(w(t))

ˆw1k
where in (1) we used eq. 67, in (2) we used eq. 2 and in (3) we used
and also using ℓ′(w(t)⊤xn) < 0 for large enough t, we have that

ˆw1k
k

∀

(cid:0)
2 t−

(cid:1)
2 + C 2
τ t−

2 log−

t−

2 log−

1(t) ,

t > t0

(69)

x > 0 : x

log (1 + x) > 0,

∀

≥

ˆw1 log

1 + t−1

+ τ (t + 1)

τ (t)

−

⊤

∇L

(w (t))

(cid:1)

(cid:0)

(cid:0)
which is negative for sufﬁciently large t0 (since log
n : ˆw⊤1 xn ≥
then 1/ (t log (t))),

1 and ℓ′(u)

Also, from Lemma 10 we know that:

≤

0.

∀

(cid:1)

1 + t−

1

(cid:1)

ℓ′(w(t)⊤xn)

ˆw⊤

1 + t−1

1 xn log

C′
xnk
τ
k
t log (t)
(cid:1)
(cid:0)
(70)
1, which is slower
decreases as t−

−

(cid:18)

(cid:19)

(w (t))

2 = o(1) and
k

k∇L

2 <
(w(u))
k

∞

Substituting eq. 71 into eq. 69, and recalling that t−
ν2, and so

ν2 (t) converges for any ν1 > 1 and any

N

≤

n=1
X

(cid:0)

∞

k∇L

Xu=0
ν1 log−

κ0 (t) ,

r(t + 1)

r(t)

||

−

2
||

∈

L1 .

Also, in the next subsection we will prove that

Lemma 14 Let κ1 (t) and κ2 (t) be functions in L1, then

k
Thus, by combining eqs. 73 and 72 into eq. 68, we ﬁnd

≤

−

(r (t + 1)

r (t))⊤ r (t)

κ1 (t)

r (t)
k

+ κ2 (t)

2
r(t + 1)
k
k
On this result we apply the following lemma (with φ (t) =
κ0 (t) + 2κ2 (t)), which we prove in appendix C.6:

κ0 (t) + 2κ1 (t)

r (t)
k
k

≤

+ 2κ2 (t) +

2
r(t)
k
k

r(t)
, h (t) = 2κ1 (t), and z (t) =
k
k

Lemma 15 Let φ (t) , h (t) , z (t) be three functions from N to R
constants. Then, if

, and

≥

∞t=1 h (t)

0, and C1, C2, C3 be three positive

P

z (t) + h (t) φ (t) + φ2 (t)

C1 <

≤
φ2 (t + 1)

∞

≤

we have

(71)

(72)

(73)

(74)

(75)

φ2 (t + 1)

C2 + C3

z (u)

≤

t

Xu=1

28

GRADIENT DESCENT ON SEPARABLE DATA

and obtain that

2
r(t + 1)
k
k

≤

C2 + C3

(κ0 (u) + 2κ2 (u))

C4 <

≤

,

∞

since we assumed that

i = 0, 1, 2 : κi (t)

L1. This completes our proof. (cid:4)

∀

C.3 Proof of Lemma 14

Before we prove Lemma 14, we prove the following auxilary Lemma:

t

u=1
X

∈

Lemma 16 Consider the function f (t) = t−
m0 ≤

∃

M + 1 such that νm0 > 1 and for all m′ < m0,νm′ = 1, then f (t)

L1.

ν1(log(t))−

ν2(log log(t))−

ν3 . . . (log◦

M (t))−

νM +1. If

Proof To prove Lemma 16, we will show that the improper integeral
bounded, i.e.,
Cauchy test) this in turn implies that

∞t1 f (t)dt for any t1 > 0 is
∞t1 f (t)dt < C. Using the integeral test for convergence (or Maclaurin–
∞t1 f (t) < C, and thus f (t)

t1 > 0,

t1 > 0,

L1.

∀

R

R
First, if m0 > 1, then ν1 = ν2 . . . = νm0
(m0
−

1)(t), we have

of variables y = log◦

∀

−

1 = 1 and νm0 = 1 + ǫ for some ǫ > 0. Using change
P

∈

∈

dy =

t

log◦

r(t)

dt = t−

ν1

(log◦

r(t))−

νr+1 dt

m0

2

−

 

Yr=1

1

−

!

m0

2

−

Yr=1

and for all m > m0,
M +1
νm|
m=m0+1 |

˜ν =

νm

(m

log◦

(cid:16)
and log◦

−

−

1)(t)
−
1)(t1) = y1, we have
(m0
(cid:17)
(cid:16)

log◦

m0)(y)
(cid:17)

=

(m

−

νm

−

≤

(log(y))|

|. Thus, denoting

νm

P

t1

Z

∞

f (t)dt =

∞

νm0

y−

log◦

m

m0(y)

−

νm d(y)

−

M +1

y1

Z

m=m0+1
Y

(cid:0)

(cid:1)

∞

(log(y))˜ν
y1+ǫ

dy.

≤

y1

Z

(76)

For m0 = 1, we have ν1 = 1 + ǫ for some ǫ > 0, and for m > 1,

(log(t))|

νm

|. Thus, denoting, ˜ν =

νm|
Thus, for any m0, we only need to show that for all t1 > 0, ǫ > 0 and ˜ν > 0,

, we have

f (t)dt

∞t1

∞t1

P

≤

R

R

M +1
m=2 |

−

(m

log◦
(log(t))˜ν

(cid:16)
t1+ǫ dt.

1)(t)
(cid:17)

νm

−

≤

(log(t)) ˜ν

t1+ǫ dt <

∞t1

.

∞

˜νt−

Let us now look at
1 (log(t))˜ν

1 and v =
R

−

∞t1

−

(log(t)) ˜ν
t1+ǫ dt. using u = (log(t))˜ν and dv = 1

1
ǫtǫ . Using integration by parts,

udv = uv

t1+ǫ , we have du =
vdu, we have

R

(log(t))˜ν
t1+ǫ

dt =

(log(t))˜ν
ǫtǫ

−

+

¯ν
ǫ

1

(log(t))˜ν
−
t1+ǫ

Z

R

Z

−

R

dt

29

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Recursing the above equation K times such that ˜ν

K < 0, we have positive constants

c0, c1, . . . cK > 0 independent of t, such that

−

∞

(log(t))˜ν
t1+ǫ

dt =

t1

Z

K

1

−

k

∞

ck (log(t))˜ν
−
ǫtǫ

#

t=t1

Xk=0
1
ck (log(t1))˜ν
−
ǫtǫ
1

k

+ cK

∞

t=t1

Z

+ cK

∞

K

(log(t))˜ν
−
t1+ǫ

dt

t=t1

Z
(log(t))˜ν
−
t1+ǫ

K

dt

"−
K

−

Xk=0
1
K
−

Xk=0
1
K
−

(1)
=

(2)

≤

=

k

ck (log(t1))˜ν
−
ǫtǫ
1

+ cK

∞

t=t1

1
t1+ǫ

(3)

k

ck (log(t1))˜ν
−
ǫtǫ
1

y +

Z
cK
ǫtǫ
1

<

∞

Xk=0
ck(log(t))˜ν−k
where (1) follows as
ǫtǫ
K < 1. This completes the proof of the lemma.
and hence for all t > 0, (log(t))˜ν
−

0, (2) follows as K is chosen such that ˜ν

t
→∞
→

K
1
−
k=0

K < 0

−

P

Lemma 14 Let κ1 (t) and κ2 (t) be functions in L1, then

(r (t + 1)

r (t))⊤ r (t)

−

κ1 (t)

r (t)
k

k

≤

+ κ2 (t)

Proof Recall that we deﬁned

where

where

r(t) = w(t)

q (t)

−

q (t) =

[ ˆwm log◦

m (t) + hm (t)] .

M

m=1
X

hm (t) = ˜wm +

ˇwk,m
1
m
r=k log◦

−

r (t)

m

1

−

Xk=1

Q

q (t + 1)

q (t)

˙q (t)

−

−

k ≤

k

2

Cqt−

L1

∈

M

˙q (t) =

ˆwm

t

1
1
m
r=1 log◦

−

r (t)

+ ˙hm (t) .

hm (t)

k ≤ k

˜wmk

+

k

ˇwk,mk ≤

k

Ch

m

Xk=1

30

with ˆwm, ˜wm and ˇwk,m deﬁned in eqs. 58, 62 and 64, respectively. We note that

Xm=1
Additionally, we deﬁne Ch, C ′h so that

Q

(77)

(73)

(78)

(79)

(80)

(81)

(82)

(83)

GRADIENT DESCENT ON SEPARABLE DATA

and

We wish to calculate

˙hm (t)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

t

(cid:16)Q

C ′h

m
2
r=1 log◦

−

r (t)

log◦

(m

1) (t)

−

(cid:17) (cid:16)

(cid:17)

L1 .

2 ∈

(84)

(r(t + 1)

r(t))⊤ r(t)

−

(1)
= [w(t + 1)

w (t)

−
(w (t))

[q (t + 1)

−
˙q (t)]⊤ r (t)

−

−

−

(2)
= [

η

−

∇L

q (t)]]⊤ r (t)

[q (t + 1)

q (t)

˙q (t)]⊤ r (t)

(85)

−

−

where in (1) we used eq. 78 and in (2) we used the deﬁnition of GD in eq. 2. We can bound the
second term using Cauchy-Shwartz inequality and eq. 81:

[q (t + 1)

q (t)

˙q (t)]⊤ r (t)

q (t + 1)

q (t)

˙q (t)

r (t)

−

−

≤ k

−

−

k k

k ≤

2

Cqt−

r (t)
k
k

.

Next, we examine the second term in eq. 85

(w (t))

˙q (t)]⊤ r (t)

−

ℓ′(w(t)⊤xn) xn −

˙q (t)

r (t)

⊤

#

(1)
=

−

˙hm (t)⊤ r (t)

η

−

ℓ′(w(t)⊤xn) x⊤n r (t)

M

m=1
X

+
m

Xn
∈S

η

[
−

∇L
N

=

η

"−

Xn=1
M

m=1
X
M

+

η
"

Xm=1 Xn
∈S

m

ℓ′(w(t)⊤xn) xn −

−

M

Xm=1

ˆwm

t

Q

1
1
m
r=1 log◦

−

r (t) #

⊤

r (t) ,

(86)

where in (1) recall from eq. 59 that

Sm,

+
m are mutually exclusive and
S

M
m=1Sm ∪ S

∪

+
m = [N ].

Next we upper bound the three terms in eq. 86.
To bound the ﬁrst term in eq. 86 we use Cauchy-Shartz, and eq. 84.

˙hm (t)⊤ r (t)

M

≤

˙hm (t)
(cid:13)
(cid:13)
(cid:13)

m=1 (cid:13)
X
(cid:13)
(cid:13)

r (t)
k

k ≤

M C ′h

t

m
2
r=1 log◦

−

r (t)

log◦

(m

1) (t)

−

r (t)
k

2 k

(cid:16)Q
In bounding the second term in eq. 86, note that for tight exponential tail loss, since w (t)⊤ xn →
, for large enough t0, we have
≤
≤
w (t)⊤ xn) for all t > t0. The ﬁrst term in eq. 86 can be bounded by the following set of

µ+w (t)⊤ xn)) exp(

ℓ′(w (t)⊤ xn)

w (t)⊤ xn)

(1+exp(

(cid:17) (cid:16)

−

−

−

(cid:17)

∞
2 exp(

M

m=1
X

−

31

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

inequalities, for t > t0,

m=1
X

+
m: x⊤
Xn
n
∈S

r(t)

0

≥

ℓ′(w (t)⊤ xn) x⊤n r (t)

−

ˆw⊤l xn log◦

l (t) + x⊤n hl(t)
i

−

x⊤n r (t)

x⊤n r (t)

!

M

η

−

+
m

Xn
∈S

m=1
X
M

ℓ′(w (t)⊤ xn) x⊤n r (t)

M

η

≤

exp

 −

exp

 −

M

Xl=1 h
M

Xl=1 h

+
Xm=1 Xn
m: x⊤
n
∈S
M

r(t)

0

≥

+
m: x⊤
Xn
n
∈S

r(t)

0

≥

m=1
X
M

+
m max
+
n
m(cid:12)
∈S
(cid:12)
(cid:12)
(cid:12)
m−1

t(

Q

S

m=1 (cid:12)
X
(cid:12)
(cid:12)
(cid:12)
M
m=1

P

exp (M

xnk
k

Ch) exp

+
m

(cid:16)

2η

|S

exp

M max
|
k=1 log◦k(t))(log◦m−1(t))θm
xn
k

2η

exp(maxn
tθ1

1 |

|S

+

 −

n∈S

+
m k

(cid:18)

Q
Ch)
k

ˆw⊤l xn log◦

l (t) + x⊤n hl (t)
i

!

M

ˆw⊤l xn log◦

l (t)

!

Xl=1
xn
Ch
k
(cid:17)
M −1
k=m (log◦m(t))

ˆw⊤
k

xn

(cid:19)

if M > 1

if M = 1

L1.

∈

(87)

(1)

≤

(2)

≤

2η

2η

(3)

≤

2η

(4)

≤






where in (1) we used eqs. 78 and 79, in (2) we used that
used eq. 83 and in (4) we denoted θm = minn
on Lemma 16.

∈S

+
m

x : xe−

0, (3) we
ˆw⊤mxn > 1 and the last line is integrable based

1 and x⊤n r (t)

≥

≤

∀

x

Next, we bound the last term in eq. 86. For exponential tailed losses (Assumption 3), since
, µ+ > 0, t

, we have positive constants µ

and t+ such that

n

−

−

∀

w(t)⊤xn → ∞

t > t+ :

w (t)⊤ xn

1 + exp

µ+w (t)⊤ xn

w (t)⊤ xn

exp

−

µ

−

−

w (t)⊤ xn

≤

≥

(cid:17)

(cid:17)

(cid:16)
1

(cid:16)

−

(cid:16)

(cid:16)

w (t)⊤ xn

w (t)⊤ xn

exp

exp

−

−

(cid:16)

(cid:16)

(cid:17)(cid:17)

(cid:17)(cid:17)

(cid:17)

(cid:17)

∀

∀

t > t

:

−

ℓ′

ℓ′

−

−

(cid:16)

(cid:16)

We deﬁne γn(t) as

γn(t) =

(1 + exp(
exp(
(1

−

−
−

(

−

µ+w(t)⊤xn)
w(t)⊤xn)
µ

if r (t)⊤ xn ≥
0
if r (t)⊤ xn < 0

.

(88)

This implies t > max (t+, t

),

ℓ′(w(t)⊤xn) x⊤n r (t)

γn(t) exp

w⊤ (t) xn

xn.

−

−

≤

−

(cid:0)

(cid:1)

32

GRADIENT DESCENT ON SEPARABLE DATA

From this result, we have the following set of inequalities:

ℓ′(w(t)⊤xn) x⊤n r (t)

γn(t) exp

w (t)⊤ xn

x⊤n r (t)

M

η

≤

m=1
X

m

Xn
∈S
l (t) + x⊤n ˜wl +

−

(cid:16)
1
−

l

x⊤n r (t)

x⊤n r (t)

(cid:17)
x⊤n ˇwk,l
1
l
r=k log◦
−
M
m

M

η

−

m=1
X
M

Xn
∈S

m

m=1
X
M

Xn
∈S

m

γn(t) exp

ηγn(t) exp

(1)
= η

(2)
=

(3)
=

≤

m=1
X
M

Xn
∈S

m

m=1
X
M

Xn
∈S

m

Xm=1 Xn
∈S

m

M

ˆw⊤l xn log◦

 −

Xl=1 "
x⊤n ˜w
exp
−
1
m
r=1 log◦
t
(cid:0)
(cid:0)

−
(cid:1)

−
r (t)

Q
x⊤n ˜w
exp
−
1
m
r=1 log◦
t
(cid:0)
(cid:0)

−
(cid:1)

−
r (t)

Q
x⊤n ˜w
exp
−
m
1
r=1 log◦
t
(cid:0)
(cid:0)

−
(cid:1)

−
r (t)

+ exp

 −

M

1

−

Xl=m

Q
x⊤n ˇwm,l+1
l
r=m log◦

1

r (t) ! −  

−

(cid:1)

(cid:1)

(cid:1)
M

1

−

Xl=m

ηγn(t) exp

x⊤n r (t)

x⊤n r (t)

ηγn(t) exp

x⊤n r (t)

x⊤n r (t)

x⊤n r (t)

x⊤n r (t)

r (t) # −

!

Xk=1

exp

Q
 −

exp

 −

x⊤n ˇwk,l
1
l
r=k log◦
−

r (t) !

Xl=k+1

Xk=1
M

Xl=m+1
M
Q
−

Q
x⊤n ˇwm,l
1
l
r=m log◦
−
1

ψm (t)

r (t) !

x⊤n ˇwm,l+1
l
r=m log◦

r (t) !

ψm (t)

1

−

" 

x⊤n ˇwm,l+1
l
r=m log◦

r (t) !#

Xl=m

Q

Q
where in (1) we used eqs. 78 and 79, and in (2) we used Pk
if m < k) and in (3) deﬁned

Q

−

1 ˇwk,m = 0 from eq. 65 (so x⊤n ˇwk,l = 0

Note

tψ such that

∃

∀

m

1

−

M

ψm (t) = exp

 −

Xl=k+1
t > tψ, we can bound ψm (t) by

Xk=1

Q

x⊤n ˇwk,l
1
l
r=k log◦
−

.

r (t) !

exp

−

 

M maxn k
log◦

(m

xnk
Ch
1) (t) ! ≤

−

ψm (t)

1 .

≤

Thus, the third term in 86 is given by

M

η

−

m=1
X
M

Xn∈Sm

(1)

≤

m=1
X

Xn∈Sm

ℓ′(w(t)⊤xn) x⊤

n r (t)

M

ˆw⊤
mr (t)
m−1
r=1 log◦r (t)

x⊤
n r (t)

−

t

m=1
X
x⊤
Q
n r (t)
exp
−
r=1 log◦r (t)
(cid:0)

m−1
(cid:1)

(cid:1)

ηγn(t) exp

x⊤
n ˜w

−
t
(cid:0)

M−1

Xl=m
ηγn(t) exp

Q

1
−  

−

M

+

m=1 "
X

−

t

Xn∈Sm
r(t)⊤ ˆwm
,
m−1
r=1 log◦r (t) #

Q
x⊤
n ˇwm,l+1
l
r=m log◦r (t) !#
x⊤
x⊤
n r (t)
n ˜w
exp
−
r=1 log◦r (t)
(cid:0)

−
t
(cid:0)

m−1
(cid:1)

(cid:1)

Q

where (1) follows from the bound in eq. 89.

Q

33

ψm (t)

exp

"

 −

x⊤
n ˇwm,l+1
l
r=m log◦r (t) !

x⊤
n r (t)

ψm (t)

1

−

 

x⊤
n ˇwm,l+1
l
r=m log◦r (t) !

M−1

Xl=m

Q

M−1

Xl=m

Q

(89)

(90)

(91)

(92)

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

∀

· "

(1)

≤

(2)

≤

Xn∈Sm

exp

 −

n∈Sm:
X
x⊤
r(t)≥0
n

n∈Sm:
X
r(t)≥0

x⊤
n

We examine the ﬁrst term in eq. 92

M

ηγn(t) exp

m=1
X

Xn∈Sm

x⊤
x⊤
n r (t)
n ˜w
exp
r=1 log◦r (t)
(cid:0)

m−1
(cid:1)

−

(cid:1)

x⊤
n r (t)

ψm (t)

exp

·"

 −

Q
x⊤
n ˇwm,l+1
l
r=m log◦r (t) ! −  

1

−

x⊤
n ˇwm,l+1
l
r=m log◦r (t) ! #

−
t
(cid:0)
M−1

Xl=m

Q

M−1

Xl=m

Q

t > t1 > tψ, where we will determine t1 later. We have the following for all m

[M ]

∈

ηγn(t) exp

x⊤
x⊤
n r (t)
n ˜w
exp
−
r=1 log◦r (t)
(cid:0)

m−1
(cid:1)

−
t
(cid:0)

(cid:1)

x⊤
n r (t)

ψm (t)

M−1

x⊤
Q
n ˇwm,l+1
l
r=m log◦r (t) ! −  
x⊤
n ˜w
ψm (t)
r=1 log◦r (t)
(cid:1)

−
m−1
(cid:0)

Xl=m
ηγn(t) exp
Q
t

1

−

M−1

Xl=m

exp

"

Q
 −

x⊤
n ˇwm,l+1
l
r=m log◦r (t) ! #
x⊤
n ˇwm,l+1
1
l
r=m log◦r (t) ! −  

M−1

Xl=m

Q

ηγn(t) exp

x⊤
n ˜w
r=1 log◦r ψm (t)  

−
(cid:0)

m−1

(t)

(cid:1)

t

2

Q
x⊤
n ˇwm,l+1
l
r=m log◦r (t) !

L1 ,

∈

x⊤
n ˇwm,l+1
l
r=m log◦r (t) !#

M−1

−

Xl=m

Q

(93)

Q
where we set t1 > 0 such that

t > t1 the term in the square bracket is positive and

in (1) we used that since e−

x

x

∀

≥ −

1 we have that e−

x

1

≥
−

≤

x⊤n ˇwm,l+1
l
r=m log◦

r (t)

>

1 ,

−

Xl=m
1
x + x2 and ψm (t)

−

Q
x, and also from using e−

xx
1 from eq. 91.

≤

≤

1 and in (2) we use that

M−1

Xl=m

Q

∀

M

1

−

34

GRADIENT DESCENT ON SEPARABLE DATA

ψm (t)

1

−

 

x⊤
n ˇwm,l+1
l
r=m log◦r (t) ! −

t

x⊤
n ˆwm
m−1
r=1 log◦r (t) #

Q

Q

We examine the second term in eq. 92 using the decomposition of ˆwm from eq. 66

M

ηγn(t) exp

m=1 "
Xn∈Sm
X
M
(1)
=

m=1
X

Xn∈Sm
M

η exp

t

M−1

Xl=m

1

−

(cid:1)
M−1

x⊤
n r (t)

ψm (t)

(cid:1)

x⊤
n r (t) ψm(t)

x⊤
n r (t)

(cid:1)

m−1
(cid:1)

x⊤
n ˜w

−
t
(cid:0)

x⊤
n r (t)
exp
−
r=1 log◦r (t)
(cid:0)
x⊤
x⊤
Q
n ˜w
n r (t)
−
m−1
r=1 log◦r (t)
(cid:0)
(cid:1)
x⊤
n ˜w
t

γn(t) exp

−
(cid:0)
x⊤
n r (t)
exp
−
m−1
r=1 log◦r (t)
(cid:0)
x⊤
n r (t) x⊤
x⊤
Q
n ˜w
m−1
r=1 t log◦r (t)
(cid:1)

n ˇwk,m

−
(cid:0)

η exp

−

(cid:0)

(cid:1)

(cid:0)

(cid:1)

Q
ηγn(t) exp

m=1
X
M

Xn∈Sm
m−1

m=1
X

Xk=1 Xn∈Sk
η exp

m=1
X

Xn∈Sm
M

t

x⊤
x⊤
Q
n ˜w
n r (t)
−
m−1
r=1 log◦r (t)
(cid:0)
(cid:1)
M−1
Q

ηγn(t) exp

m=1
X
M

Xn∈Sm

Xl=m
M−1

Xk=1 Xn∈Sk

Xm=k

η exp

−
(cid:0)

γn(t) exp

x⊤
n r (t)

ψm (t)

1

−

−
(cid:0)

(cid:1)

exp

−
(cid:0)

(cid:0)
x⊤
n ˜w

(cid:1)
n r (t) ψm(t)x⊤
x⊤
x⊤
n r (t)
−
l
r=1 log◦r (t)
(cid:1)
(cid:0)
x⊤
n r (t) x⊤
x⊤
n ˜w
n ˇwk,m+1
Q
m
r=1 t log◦r (t)

(cid:1)

t

n ˇwm,l+1

x⊤
n ˇwm,l+1
l
r=m log◦r (t)

Xl=m

Q

1
m−1

Q
r=1 log◦r (t) −

x⊤
n ˇwm,k+1
k
r=1 log◦r (t) #

(cid:1)
M−1

t

Xk=m

Q

η exp

x⊤
n ˜w

γn(t)ψm (t) exp

x⊤
n r (t)

1

x⊤
n r (t)

−
(cid:0)

(cid:1) (cid:0)

−
(cid:0)

−

(cid:1)

(cid:1)

(94)

where in (1) we used eq. 66, in (2) we re-arranged the order of summation in the last term, and in
(3) we just use a change of variables.

Next, we examine Γm,n(t) for each m and n

∈ Sm in eq. 94. Note that,

∃

t2 > tψ such that

x⊤n ˇwm,k+1
k
r=1 log◦

M

1

−

t

Xk=m

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Q

≤

t

r (t) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Q

0.5
m
1
r=1 log◦

−

.

r (t)

κ(n, t)
1
m
r=1 log◦

−

r (t) #

exp

x⊤n ˜w

γn(t)ψm(t) exp

−

(cid:16)

(cid:17) (cid:16)

x⊤n r (t)
(cid:17)

−

(cid:16)

−

(cid:17)

1

x⊤n r (t) ,

(95)

where in (1) follows from the deﬁnition of t2, wherein

κn(t) =

1.5 if
0.5 if

γn(t)ψm(t) exp
γn(t)ψm(t) exp

(cid:0)
(cid:0)
1. First, if x⊤n r (t) > 0, then γn(t) = (1 + exp(
−

(cid:0)
(cid:0)

(cid:26)

x⊤n r (t)
−
x⊤n r (t)
(cid:1)
−
(cid:1)
µ+w(t)⊤xn)) > 0.

1
1
(cid:1)
(cid:1)

−
−

x⊤n r (t) > 0
x⊤n r (t) < 0

.

We further divide into two cases. In the following C0, C1 are some constants independent of t.

35

−

+

M

−

+

M

(2)
=

(3)
=

:=

t

Xn∈Sm "

m=1
X
M

Q
Γm,n(t),

m=1
X

Xn∈Sm

t > t2 we have

∀

In this case,

t > t2

∀

(1)

≤

η

Γm,n(t)

t

"

Q

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

(a) If

x⊤n r (t)

> C0t−

0.5µ+ , then we have the following

l(t) + h⊤l xn

exp

x⊤n r (t)

!!
i

−

(cid:16)

(cid:17)

(cid:12)
(cid:12)

γn(t)ψm(t) exp

(cid:12)
(cid:12)

(cid:16)

−

x⊤n r (t)
(cid:17)
ˆw⊤l xn log◦

M

µ+

(1)

≤  

1 + exp

(2)

≤ 

1 +

 −

Xl=1 h
exp(µ+Ch k
m
1
r=1 log◦

−

t

(cid:16)


Q
µ+
1 + C1t−

1

−

(3)

≤

(cid:0)
1

≤

−

C0t−

0.5µ+)

exp(

)
µ+ 

xnk
r (t)
(cid:17)
0.5µ+ + 0.5C 2
C0t−



−

µ+

0 t−

t > t′+

,

∀

0.5µ+

(cid:1) (cid:0)

C0t−

1 + C1t−

µ+

+ 0.5C 2

0 t−

µ+

(cid:1)
1 + C1t−

µ+

(4)

≤

1,

t > t′′+,

∀

(96)

(cid:0)

(cid:1)

≤

where in (1), we use ψm(t)
from eq. 83, in (3) for some large enough t′+ > t+, we have
for the second term we used the inequality e−
asymptotically for t > t′′+ for large enough t′′+ > t′+ as C0t−
0.5C 2
Thus, using eq. 96 in eq. 95,

(cid:1)
1 from eq. 91 and using eq. 78, in (2) we used bound on hm
xn
exp(µ+Chk
C1, and
r=1 log◦r(t))µ+ ≤
x + 0.5x2 for x > 0, and (4) holds
Q
0.5µ+ converges slower than

t > max (t2, t′′+), we have

µ+ to 0.

0 t−

m−1

)
k

≤

−

1

(cid:0)

(

x

∀

Γm,n(t)

x⊤n ˜w
ηκ(n, t) exp
−
m
1
r (t)
r=1 log◦
(cid:0)

−

t

≤ "

#

(cid:1)

(cid:16)

γn(t)ψm(t) exp

0.5µ+, then we have the following: ψm(t)

, for large enough t > t′′′+, γn(t) =

≤

Q
(b) If 0 < x⊤n r (t) < C0t−
1 as x⊤n r (t) > 0, and since w(t)⊤xn → ∞
2
x⊤n r (t)
This gives us,
γn(t)ψm(t) exp
this in eq. 95,
t > max (t2, t′+)
(cid:0)

−

(cid:0)
∀

1

x⊤n r (t)

x⊤n r (t)

≤

≤

−

x⊤n r (t)
(cid:17)

−

(cid:16)

−

1

x⊤n r (t)

0

≤

(cid:17)
1 from eq. 91, exp

x⊤n r (t)

−
1 + exp
(cid:0)

≤

µ+w(t)⊤xn
(cid:1)

C0t−

(cid:0)
0.5µ+, and using

−
(cid:0)

≤

(cid:1)(cid:1)

(cid:1)

(cid:1)
x⊤n ˜w
ηκ(n, t) exp
−
m
1
r (t)
r=1 log◦
(cid:0)

−

t

#

(cid:1)

Γm,n(t)

≤ "

0.5µ+

C0t−

L1.

∈

∈

2. Second, if x⊤n r (t)
≤
into following special cases.

0, then γn(t) = (1

Q

exp(

µ

w(t)⊤xn))

−

−

−

(0, 1). We again divide

(a) If

x⊤n r (t)

C0

log◦

(m

1)(t)

−

−

0.5˜µ−

, where ˜µ

= min (µ

, 1), then we have

−

−

(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
Γm,n(t)

(cid:16)

≤ "

1.5η exp
−
m
1
r=1 log◦
(cid:0)

−

t

(cid:17)
x⊤n ˜w
r (t) #
(cid:1)

1

−

(cid:16)

(1)

≤ "

1.5η exp
−
m
2
r=1 log◦
(cid:0)

x⊤n ˜w
Q
r (t) #
(cid:1)

−

t

C0

log◦

(m

−

1

0.5˜µ−

−

−

L1.

∈

1)(t)
(cid:17)

Q
where in (1) we used that

x⊤n r (t)

C0

log◦

(m

−

≤

(cid:12)
(cid:12)

(cid:16)

(cid:12)
(cid:12)

(cid:16)

0.5˜µ−

−

.

1
−
1)(t)
(cid:0)
(cid:17)

36

γn(t)ψm (t) exp

x⊤n r (t)

< 1 and

−
(cid:0)

(cid:1)(cid:1)

γn(t)ψm (t) exp

x⊤n r (t)

−

(cid:16)

x⊤n r (t)
(cid:12)
(cid:12)
(cid:12)

(cid:17)(cid:17) (cid:12)
(cid:12)
(cid:12)

GRADIENT DESCENT ON SEPARABLE DATA

(b) If ψm (t) exp

x⊤n r (t)

< 1, then, from eq. 91

−

(cid:0)

(cid:1)

−

M maxn k
log◦

xnk
1) (t)

(m

−

Ch

≤

log ψm (t) < x⊤n r (t) .

(97)

In this case, since γn(t) = 1
1, and hence
1.5, and we have

−
γn(t)ψm (t) exp

exp(

−
(cid:0)

(cid:0)

(cid:1)

(cid:1)

w(t)⊤xn) < 1, we also have γn(t)ψm (t) exp
x⊤n r(t) > 0. Thus,

x⊤n r (t)
t > t2, in 95, κn(t) =
(cid:1)
(cid:0)

−
x⊤n r (t)

−

1

−

∀

<

Γm,n(t)

≤ "

(1)

≤ "

1.5η exp
−
m
1
r=1 log◦
(cid:0)

−

t

t

−

1

x⊤n ˜w
1.5η exp
−
m
1
r (t) #
r=1 log◦
(cid:1)
(cid:0)
(cid:16)
xnk
Ch
M maxn k
1) (t) ≤
log◦

x⊤n ˜w
Q
r (t) #
(cid:1)

−

(m

−

γn(t)ψm (t) exp

x⊤n r (t)

(cid:17)(cid:17) (cid:12)
(cid:12)
(cid:12)

x⊤n r (t)
(cid:12)
(cid:12)
(cid:12)
2 ∈

1) (t)

L1,

−

(cid:16)

C2

(cid:16)

t

m
2
r=1 log◦

−

r (t)

log◦

(m

−

(cid:17)

x⊤n r(t)
|
|

=

Q

where (1) follows from
x⊤n r(t) from eq. 97.

1

−

−

(cid:0)
x⊤n r (t)

(c) If ψm (t) exp

Q

−
(cid:0)
> C0

γn(t)ψm (t) exp

x⊤n r (t)

< 1 and the bound on

(cid:1)(cid:1)
(m

0.5˜µ−

−

−

(cid:1)

−

log◦

1)(t)
(cid:17)

→ ∞

> 1, and

and ψm(t)

x⊤n r (t)
Since, x⊤n w(t)
1 from eq. 90, for large enough t′
(cid:16)
(cid:12)
(cid:0)
→
(cid:12)
−
ψm(t) > 0.5 and γn(t) = (1
exp(
−
ily large constant. For all t > τ , if exp
0.25τ
(cid:0)
On the other hand, if there exists t > τ
constants C1, C2 we have the following
x⊤n r (t)
x⊤n r (t)) = exp(
|

(cid:12)
(cid:12)
x⊤n w(t))) > 0.5. Let τ > max (4, t′
−
x⊤n r (t)

4, such that exp

−
(cid:0)
−

x⊤n r (t)

(i) exp(

1 + C0

(cid:1)
0.5˜µ−

1)(t)

log◦

−
−

> τ

> t

)
|

≥

≥

≥

−

≥

1.

(m

µ

(cid:1)

−

−

4, then γn(t)ψm(t) exp

, where ˜µ= min (1, µ
,
t > t′
∀
−
) be an arbitrar-

, we have

).

−

x⊤n r (t)

>

−

(cid:1)
< τ , then for some

(cid:0)

, since ex > 1 +

(ii) ψm(t)

exp

1)(t)
C1
−
and again using ex > 1 + x for all x,
(cid:17)

log◦

≥

(cid:18)

(m

(cid:16)

−

≥

(cid:19)

(cid:18)

−

(cid:18)
1

−

(cid:16)

1

C1

log◦

(m

−

from eq. 91

(cid:17)

(cid:19)

1

−

1)(t)
(cid:17)

(cid:19)

−
x for all x,

(iii)

(cid:16)

µ−

γn(t) =

1

 

− "

exp(

−

hl(t)⊤xn) exp
m
1
r=1 log◦
−
µ−

t

−
r(t)
(cid:0)

x⊤n r (t)

1

≥  

− "

)τ
xnk
Chk
exp(
Q
−
m
1
r(t) #
r=1 log◦
−

t

! ≥

(cid:18)

1

−

(cid:16)

#

(cid:1)

!

C2 log◦

(m

−

Q

µ−

−

1)(t)
(cid:17)

,

t > t′′
−

∀

(cid:19)

where the last inequality follows as for large enough t′′
−
C2.

> t′
−

, we have exp(
xn
−
m−2
r=1 log◦r(t) ≤

Chk

)τ

k

t

Q

37

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Using the above inequalities, we have

γn(t)ψm(t) exp

x⊤
n r (t)

−
(cid:0)

≥
(1)

(cid:18)

≥

1 + C0

1 + C0

(cid:16)
C0C2

−

−1

−0.5˜µ−

(cid:1)
log◦(m−1)(t)
(cid:17)
−0.5˜µ−

(cid:16)
log◦(m−1)(t)
(cid:17)
log◦(m−1)(t)
(cid:17)

(cid:16)

(cid:19) (cid:18)

C1

−
−µ−0.5˜µ−

1

−1

−

C1

log◦(m−1)(t)
(cid:17)
C2

(cid:16)
log◦(m−1)(t)
(cid:17)
(cid:16)
log◦(m−1)(t)
(cid:17)
(cid:16)

C0C1

−

−

log◦(m−1)(t)
(cid:17)

−µ−

−µ−

(cid:19)

−

C2

1
(cid:19) (cid:18)
(cid:16)
log◦(m−1)(t)
(cid:16)
(cid:17)
−1−0.5˜µ− (2)

1,

t > t′′′
− ,
∀

≥

(98)

where in (1) we dropped the other positive terms, and (2) follows for large enough t′′′
−

> t′′
−
converges to 0 more slowly than the other negative terms.

as the C0 log
Finally, using eq. 98 in eq. 95, we have for all t > max (t2, τ, tψ, t′′′
−

1)(t)
(cid:17)

log◦

0.5˜µ−

(m

(cid:16)

−

−

)

Γm,n(t)

x⊤n ˜w
ηκ(n, t) exp
−
m
1
r (t)
r=1 log◦
(cid:0)

−

t

≤ "

1

−

#

(cid:1)

(cid:16)

γn(t)ψm(t) exp

x⊤n r (t)

−

(cid:16)

Q

x⊤n r (t)
(cid:12)
(cid:12)
(cid:12)

(cid:17)(cid:17) (cid:12)
(cid:12)
(cid:12)

0

≤

(99)

Collecting all the terms from the above special cases, and substituting back into eq. 85, we note
L1, thus proving

, where f (t)

that all terms are either negative, in L1, or of the form f (t)
the lemma.

r (t)
k

∈

k

C.4 Proof of the existence and uniqueness of the solution to eqs. 62-63

We wish to prove that

m

∀

≥

1 :

such that

exp

m

 −

Xk=1

Xn
∈S

m

˜w⊤k xn

¯Pm
−

!

1xn = ˆwm ,

Pm

1 ˜wm = 0 and ¯Pm ˜wm = 0,

(100)

(101)

we have a unique solution. From eq. 101, we can modify eq. 100 to

−

m

exp

Xn
∈S

m

 −

Xk=1

˜w⊤k

¯Pk

1xn

−

¯Pm
−

!

1xn = ˆwm , .

To prove this, without loss of generality, and with a slight abuse of notation, we will denote
m
1
S1, ¯Pm
k=1 ˜w⊤k

Sm as
, so we can write the above equation as

1xn as xn and βn = exp

1xn

¯Pk

−

−

−

−

In the following Lemma 17 we prove this equation

(cid:16)

Xn
∈S

1

P
xnβn exp

(cid:17)
x⊤n ˜w1

(cid:17)

−

(cid:16)

= ˆw1

β

∀

∈

1

R|S

|>0 .

38

GRADIENT DESCENT ON SEPARABLE DATA

Lemma 17

β

∀

∈

R|S

1

|>0 we can ﬁnd a unique ˜w such that

xnβn exp

x⊤n ˜w1

= ˆw1

−

1

Xn
(cid:16)
∈S
1 = 0 we would have ˜w⊤1 z = 0.
Rd

(cid:17)

and for

z

Rd such that z⊤X
S

∀

∈
Proof Let K = rank (X
S
(i.e., UU⊤ = U⊤U = I) such that u1 = ˆw1/

1). Let and U = [u1, . . . , ud]
, and

∈

ˆw1k

k

while

z

= 0,

n

∈ S1 : z⊤ [u1, . . . , uK]⊤ xn 6

= 0 ,

∀

∀

i > K :

∀

n

∀

∈ S1 : u⊤i xn = 0 .

d be a set of orthonormal vectors

×

(102)

(103)

(104)

K



−



Xj=1

sjvj,n


K



−



Xj=1

sjvj,n


K

sjvj,n
Xj=1

∈ S1. Given
n

In other words, u1 is in the direction of ˆw1, [u1, . . . , uK ] are in the space spanned by the columns
of X
S
We deﬁne vn = U⊤xn and s = U⊤ ˜w1. Note that

1, and [uK+1, . . . , ud] are orthogonal to the columns of X
S

1.
∈ S1 from eq. 104,
i > K : vi,n = 0
n
1 = 0 we would have ˜w⊤1 z = 0. Lastly,

i > K : si = 0, since for

∀
Rd such that z⊤X
S

∀

z

∀

∈

and
equation 102 becomes

∀

xnβn exp

= ˆw1 .

(105)

Multiplying by U⊤ from the left, we obtain

Xn
∈S

1

i

∀

≤

K :

Xn
∈S
, we have that

1

Since u1 = ˆw1/

ˆw1k

k

vi,nβn exp

= u⊤i ˆw1 .

i

∀

≤

K :

vi,nβn exp



−

=

ˆw1k
k

δi,1 .

(106)

We recall that v1,n = ˆw⊤1 xn/
i = 1,

k

1

Xn
∈S
ˆw1k

= 1/


ˆw1k
,

∀

k

K
j=2, we examine eq. 106 for

sj}

{

exp

s1
ˆw1
k

−

(cid:18)

k (cid:19)

Xn
∈S

1

βn exp

K



−



Xj=2

sjvj,n




=

ˆw1k
k

2 .

This equation always has the unique solution

s1 =

ˆw1k
k

log

ˆw1k


k

2

−

βn exp

Xn
∈S

1

given

sj}

{

K
j=2. Next, we similarly examine eq. 106 for 2

K

,

sjvj,n

Xj=2


K as a function of si



−


i
≤

K

≤

(107)

(108)







βnvi,n exp

s1/

ˆw1k −

k



−

Xn
∈S

1

= 0 .

sjvj,n


Xj=2



39

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

multiplying by exp (s1/

) we obtain

ˆw1k

k

0 =

βnvi,n exp

Xn
∈S

1

K



−



Xj=2

sjvj,n


=

∂
∂si

−

[E (s2, . . . , sK)] ,

where we deﬁned

E (s2, . . . , sK) =

βn exp

Xn
∈S

1

K



−



Xj=2

.

sjvj,n


Therefore, any critical point of E (s2, . . . , sK) would be a solution of eq. 108 for 2
K,
and substituting this solution into eq. 107 we obtain s1. Since βn > 0, E (s2, . . . , sK ) is a convex
function, as positive linear combination of convex function (exponential). Therefore, any ﬁnite
critical point is a global minimum. All that remains is to show that a ﬁnite minimum exists and that
it is unique.

≤

≤

i

1 αnxn . Multiplying this equation

n

∈S

From the deﬁnition of

by U⊤ we obtain that

S1,
∈

α

∃

1

R|S
α
∃
R|S
1
|>0 such that 2

|>0 such that ˆw1 =
i

K

∈

P

≤
≤
vi,nαn = 0 .

Therefore,

(s2, . . . , sK)

∀

Xn
∈S
= 0 we have that

1

K

Recall, from eq. 103 that
Therefore, eq. 110 implies that

∀



1

Xn
∈S


(s2, . . . , sK)
n

Xj=2
= 0,
n
∃
∈ S1 such that

∃

αn = 0 .

sjvj,n

∈ S1 :
K
j=2 sjvj,n > 0 and also

j=2 sjvj,n 6

K

P

= 0, and that αn > 0.
∈ S1 such that

m

∃

K
j=2 sjvj,m < 0.
P
Thus, in any direction we take a limit in which

K, we obtain that
si| → ∞ ∀
|
P
, since at least one exponent in the sum diverge. Since E (s2, . . . , sK), is
E (s2, . . . , sK)
a continuous function, it implies it has a ﬁnite global minimum. This proves the existence of a ﬁnite
solution. To prove uniqueness we will show the function is strictly convex, since the hessian is
(strictly) positive deﬁnite, i.e., that the following expression is strictly positive:

→ ∞

≤

≤

2

i

(109)

(110)

K

K

Xi=2

Xk=2

qiqk

∂
∂si

∂
∂sk

K

K

E (s2, . . . , sK) .

βn

qivi,n

qkvk,n

exp

Xn
∈S

1

 

Xi=2
K

!  

Xk=2

2

!

K

=

=

βn

 

qivi,n

exp

!



−

Xn
∈S

the last expression is indeed strictly positive since
∀
103. Thus, there exists a unique solution ˜w1.

Xi=2

1

Xj=2
= 0,
q

K

Xj=2



−

sjvj,n


.


sjvj,n

∈ S1 :

n

∃

K

j=2 qjvj,n 6

= 0, from eq.

P

40

GRADIENT DESCENT ON SEPARABLE DATA

C.5 Proof of the existence and uniqueness of the solution to eqs. 64-65

Lemma 18 For

m > k

1, the equations

∀

≥

exp

˜w⊤xn

Pm

1xn =

−

(cid:16)

−

(cid:17)

Xn
∈S

m

m

1

−

Xk=1

Xn
∈Sk





exp

˜w⊤xn

ˇwk,m

(111)

−

(cid:16)

xnx⊤n 


(cid:17)

under the constraints

have a unique solution ˇwk,m.

Pk

1 ˇwk,m = 0 and ¯Pk ˇwk,m = 0

−

Proof For this proof we denote X
projection matrix Qk = Pk ¯Pk

Sk as the matrix which columns are
1where QkQm = 0
∀

xn|
n
∈ Sk}
{
= m, Qk ¯Pm = 0
k < m, and
∀

k

−

, the orthogonal

m : I = Pm + ¯Pm =

Qk + ¯Pm

∀

m

We will write ˇwk,m = Wk,muk,m , where uk,m ∈
such that QkWk,m = Wk,m, so

Xk=1
Rdk and Wk,m ∈

Rd

×

dk is a full rank matrix

ˇwk,m = Qk ˇwk,m = QkWk,muk,m .

and, furthermore,

rank

X⊤
Sk
m : ¯PmPm = 0 and

h

QkWk,m

= rank

Qk

= dk .

X⊤
Sk
¯Pm+kxn = 0. Therefore,

(cid:17)

k

∀

Recall that
Pk

i
v
1,
∀
∀
1Qkv = 0, ¯PkQkv = 0. Thus, ˇwk,m eq. 114 implies the constraints in eq. 112 hold.
−
Next, we prove the existence and uniqueness of the solution ˇwk,m for each k = 1, . . . , m
separately. We multiply eq. 111 from the left by the identity matrix, decomposed to orthogonal
projection matrices as in eq. 113. Since each matrix projects to an orthogonal subspace, we can
solve each product separately.

(cid:16)
∈ Sm

Rd ,

≥

∈

n

∀

The product with ¯Pm is equal to zero for both sides of the equation. The product with Qk is

equal to

(112)

(113)

(114)

(115)

Xn
∈S

m

−

(cid:16)

(cid:17)

−

exp

˜w⊤xn

QkPm

1xn =

exp

˜w⊤xn

ˇwk,m .





Xn
∈Sk

−

(cid:16)

Qkxnx⊤n 


(cid:17)

Substituting eq. 114, and multiplying by W⊤k,m from the right, we obtain

exp

˜w⊤xn

W⊤k,mQkPm
−

1xn =

Xn
∈S

m

−

(cid:16)

(cid:17)


Xn
∈Sk


exp

˜w⊤xn

−

(cid:16)

W⊤k,mQkxnx⊤n QkWk,m


(cid:17)

uk,m .

(116)

41

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Denoting Ek ∈
square bracket in the left hand side can be written as

R|Sk|×|Sk| as diagonal matrix for which Enn,k = exp

1
2 ˜w⊤xn

, the matrix in the

−

(cid:0)

(cid:1)

W⊤k,mQkX

Sk EkEkX⊤
Sk

QkWk,m .

(117)

Since rank

AA⊤

= rank (A) for any matrix A, the rank of this matrix is equal to

(cid:0)

(cid:1)

rank [EX

SkQkWk,m]

(1)
= rank [X

Sk QkWk,m]

(2)
= dk

where in (1) we used that Ek is diagonal and non-zero, and in (2) we used eq. 115. This implies
dk matrix in eq. 117 is full rank, and so eq. 116 has a unique solution uk,m. Therefore,
that the dk ×
there exists a unique solution ˇwk,m.

C.6 Proof of Lemma 15

Lemma 15 Let φ (t) , h (t) , z (t) be three functions from N to R
constants. Then, if

, and

C1 <

∞t=1 h (t)

≥

0, and C1, C2, C3 be three positive

≤

P

∞

≤

φ2 (t + 1)

z (t) + h (t) φ (t) + φ2 (t)

we have

(74)

(75)

φ2 (t + 1)

C2 + C3

z (u)

≤

t

u=1
X

Proof We deﬁne ψ (t) = z (t) + h (t), and start from eq. 74

φ2 (t + 1)

z (t) + h (t) φ (t) + φ2 (t)
1, φ2 (t)

z (t) + h (t) max
z (t) + h (t) + h (t) φ2 (t) + φ2 (t)
(cid:2)
ψ (t) + (1 + h (t)) φ2 (t)

(cid:3)

+ φ2 (t)

≤

≤

≤

≤

≤

ψ (t) + (1 + h (t)) ψ (t

1) + (1 + h (t)) (1 + h (t

ψ (t) + (1 + h (t)) ψ (t

≤
+ (1 + h (t)) (1 + h (t

−
1)) (1 + h (t

1) + (1 + h (t)) (1 + h (t

2)) φ2 (t

2)

−

−

−

−

1)) φ2 (t
1)) ψ (t

1)

−

2)

−

−

−

42

GRADIENT DESCENT ON SEPARABLE DATA

we keep iterating eq. 74, until we obtain

1

t
−

Ym=1

exp

≤ "

≤ "

1

t
−

 

m=1
X

(1 + h (t

m))

φ (t1) +

−

#

(1 + h (t

m))

ψ (t

k)

−

#

−

t1

t
−

k

1

−

Xk=0 "

1

t
−

Ym=0

k

1

−

h (t

m)

φ (t1) +

exp

−

!#

h (t

m)

ψ (t

k)

−

!#

−

Xk=0 "

 

m=1
X

1

t
−

Xk=0
t

u=1
X
t

u=1
X

exp (C)

φ (1) +

ψ (t

k)

exp (C)

φ (1) +

ψ (u)

−

#

#

exp (C)

φ (1) +

(z (u) + h (u))

#

exp (C)

φ (1) + C +

z (u)

t

u=1
X

#

"

"

"

"

≤

≤

≤

≤

Therefore, the Lemma holds with C2 = (φ (1) + C) exp (C) and C3 = exp (C).

43

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Appendix D. Calculation of convergence rates

In this section we calculate the various rates mentioned in section 3.

D.1 Proof of Theorem 5

From Theorems 4 and 13, we can write w (t) = ˆw log t + ρ (t), where ρ (t) has a bounded norm for
almost all datasets, while in zero measure case ρ (t) contains additional O(log log(t)) components
which are orthogonal to the support vectors in
S1, and, asymptotically, have a positive angle with
the other support vectors. In this section we ﬁrst calculate the various convergence rates for the
non-degenerate case of Theorem 4, and then write the correction in the zero measure cases, if there
is such a correction.

First, we calculated of the normalized weight vector (eq. 8), for almost every dataset:

w (t)
w (t)
k
k
=

q

=

ˆw

k

k

r

k

k (cid:18)

=

=

=

1
ˆw

ˆw
ˆw

ˆw
ˆw

ρ (t) + ˆw log t

ρ (t)⊤ ρ (t) + ˆw⊤ ˆw log2 t + 2ρ (t)⊤ ˆw log t

ρ (t) / log t + ˆw

1 + 2ρ (t)⊤ ˆw/

ˆw

2 log t
k

+

ρ (t)
k
k

2 /

ˆw

2 log2 t
k

k
(cid:16)

(cid:19)





(cid:17)
ρ (t)⊤ ˆw
2 log t
ˆw
k

k

+





k
(cid:16)
ρ (t)⊤ ˆw
ˆw

2
k

k

3
2  

(cid:17)

2

−

!

ρ (t)

+ ˆw

1
log t

1

−

2
ρ (t)
k
k
2 
ˆw
2
k

k

1
log2 t

+ O



1
log3 t

(cid:18)
(118)

(cid:19)





ˆw
ˆw

ρ (t)⊤ ˆw
ˆw

1
log t

!

+ O

1
log2 t

(cid:18)

(cid:19)

+

ρ (t)
ˆw

 

k

k

k

+

I

−

k

k

(cid:18)

−

k
k
ˆw ˆw⊤
2
ˆw
k
k

k
k
ρ (t)
ˆw
k

k

(cid:19)

2
k
1
log t

+ O

1
log2 t

,

(cid:19)

(cid:18)

where to obtain eq. 118 we used
fact that ρ (t) has a bounded norm for almost every dataset. Thus, in this case

= 1

4 x2 + O

2 x + 3

1
√1+x

x3

−

1

, and in the last line we used the

(cid:0)

(cid:1)

For the measure zero cases, we instead have from eq. 61, w(t) =

m(t) + ρ(t),
m(t) + ρ(t), such that w(t) =
where
ˆw log(t) + ˜ρ(t) with ˜ρ(t) = O(log log(t)). Repeating the same calculations as above, we have for
the degenerate cases,

is bounded (Theorem 3). Let ˜ρ(t) =

M
m=2 ˆw log◦

M
m=1 ˆw log◦

ρ(t)
k
k

P

P

w (t)
w (t)
k
k

ˆw
ˆw
k

−

= O

1
log t

.

(cid:19)

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

k (cid:13)
(cid:13)
(cid:13)
(cid:13)

k (cid:13)
(cid:13)
(cid:13)
(cid:13)
44

w (t)
w (t)
k
k

ˆw
ˆw

−

k

= O

log log t
log t

(cid:18)

(cid:19)

GRADIENT DESCENT ON SEPARABLE DATA

Next, we use eq. 118 to calculate the angle (eq. 9)

k k

w (t)⊤ ˆw
ˆw
w (t)
k
ˆw⊤
ˆw
k

(cid:18)

k

2

ρ (t)

k

=

1
log t

+ ˆw

1


−

(cid:19)

1
log t

ρ (t)⊤ ˆw
ˆw

2 +
k

k

3
4  

2



ρ (t)⊤ ˆw
ˆw

2
k

k

2

−

!

2
ρ (t)
k
k
2 
ˆw
2
k

k

1
log2 t

+ O

1
log3 t

(cid:18)



(cid:19)





=1 +

2

ρ (t)
ˆw

2
k
2 
k

k
k

 



ρ (t)⊤ ˆw
ˆw
ρ (t)

2

k !

k

for almost every dataset. Thus, in this case

k k



1
4 

1
log2 t

−


+ O

1
log3 t

(cid:18)

(cid:19)

Repeating the same calculation for the measure zero case, we have instead

w (t)⊤ ˆw
ˆw
w (t)
k

k k

k

= O

1
log2 t

(cid:18)

(cid:19)

w (t)⊤ ˆw
ˆw
w (t)
k

k k

k

= O

log log t
log t

2

!

(cid:19)

 (cid:18)

Next, we calculate the margin (eq. 10)

min
n

x⊤n w (t)
w(t)
k
k

= min

x⊤n

n

" 

1
ˆw

−

k
ρ (t)
ˆw
k

k

k

−

=

1
ˆw
k

k  

min
n

x⊤n ρ (t)

−

ˆw
ˆw

k

2

ρ (t)⊤ ˆw
ˆw
k
k
k
ρ (t)⊤ ˆw
ˆw
k

2

!

1
log t

!

+ O

1
log2 t

(cid:18)

(cid:19)#

1
log t

+ O

1
log2 t

(cid:18)

(cid:19)

(119)

(120)

k
for almost every dataset, where in eq. 119 we used eq. 19. Interestingly the measure zero case has
a similar convergence rate, since after a sufﬁcient number of iterations, the O(log log(t)) correction
is orthogonal to xk, where k = argminnx⊤n w(t). Thus, for all datasets,

min
n

x⊤n w (t)

−

1
ˆw
k

k

= O

1
log t

(cid:18)

(cid:19)

Calculation of the training loss (eq. 11):

(w (t))

L

1 + exp

µ+w (t)⊤ xn

exp

w (t)⊤ xn

(cid:17)(cid:17)

−

(cid:16)

≤

=

=

=

N

n=1 (cid:16)
X
N

Xn=1 (cid:16)
N

n=1 (cid:16)
X
1
t

Xn
∈S

−

(cid:16)

−

(cid:16)

(cid:17)

−

(cid:16)

1 + exp

µ+ (ρ (t) + ˆw log t)⊤ xn

exp

(ρ (t) + ˆw log t)⊤ xn

(cid:17)(cid:17)

(cid:17)

1 + t−

µ+ ˆw⊤xn exp

µ+ρ (t)⊤ xn

exp

ρ (t)⊤ xn

t−

ˆw⊤xn

(cid:17)(cid:17)

−

(cid:16)

(cid:17)

ρ(t)⊤xn + O

e−

(cid:16)

−

(cid:16)
max(θ,1+µ+)
t−

.

(cid:17)

45

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

1). Note that the zero measure case has the same behavior,
Thus, for all datasets
since after a sufﬁcient number of iterations, the O(log log(t)) correction has a non-negative angle
with all the support vectors.

(w (t)) = O(t−

L

Next, we give an example demonstrating the bounds above, for the non-degenerate case, are
u, and a single data point x =
0, and obtain the continuous

strict. Consider optimization with and exponential loss ℓ (u) = e−
(1, 0). In this case ˆw = (1, 0) and
time version of GD:

= 1. We take the limit η

ˆw
k

→

k

We can analytically integrate these equations to obtain

˙w1 (t) = exp (

w (t)) ;

˙w2 (t) = 0.

−

w1 (t) = log (t + exp (w1 (0))) ; w2 (t) = w2 (0) .

Using this example with w2 (0) > 0, it is easy to see that the above upper bounds are strict in

the non-degenerate case. (cid:4)

D.2 Validation error lower bound

is a set of indices for validation set samples. We calculate of the validation
Lastly, recall that
loss for logistic loss, if the error of the L2 max margin vector has some classiﬁcation errors on the
validation, i.e.,

: ˆw⊤xk < 0:

V

k

∃

∈ V

Lval (w (t)) =

log

1 + exp

w (t)⊤ xn

−

(cid:16)
w (t)⊤ xk

(cid:17)(cid:17)

1 + exp

(cid:17)(cid:17)
(ρ (t) + ˆw log t)⊤ xk

Xn
∈V
log

(cid:16)
1 + exp

≥
= log

(cid:16)

(cid:16)

= log

exp

(cid:16)

−

−

−

≥ −

(cid:16)
(ρ (t) + ˆw log t)⊤ xk

(cid:17)(cid:17)
1 + exp

(ρ (t) + ˆw log t)⊤ xk

(cid:16)
(ρ (t) + ˆw log t)⊤ xk + log

(cid:16)

(cid:17) (cid:16)
1 + exp

(cid:16)

(ρ (t) + ˆw log t)⊤ xk

(cid:17)(cid:17)(cid:17)

log t ˆw⊤xk + ρ (t)⊤ xk

(cid:16)

(cid:16)

(cid:17)(cid:17)

≥ −
Thus, for all datasets

Lval (w (t)) = Ω(log(t)).

Appendix E. Softmax output with cross-entropy loss

We examine multiclass classiﬁcation. In the case the labels are the class index yn ∈ {
we have a weight matrix W
W⊤
d so that Ak = ek ⊗
(cid:1)

Furthermore, we deﬁne w = vec
RdK

the matrix Ak ∈
d-dimension identity matrix. Note that A⊤k w = wk.

, a basis vector ek ∈
Id, where
⊗

d with wk being the k-th row of W.

RK so that(ek)i = δki, and
is the Kronecker product and Id is the

1, . . . , K

and

RK

∈

}

(cid:0)

×

×

Consider the cross entropy loss with softmax output

(W) =

L

N

−

n=1
X

log

 

w⊤ynxn

exp
K
k=1 exp
(cid:0)

w⊤k xn
(cid:1)

!

(cid:0)

(cid:1)

P

46

GRADIENT DESCENT ON SEPARABLE DATA

Using our notation, this loss can be re-written as

(w) =

L

N

log

−

n=1
X

N

 
K

w⊤Aynxn
exp
K
k=1 exp (w⊤Akxn) !

(cid:1)

(cid:0)

=

log

P

exp

Xn=1

 

Xk=1

(cid:16)

w⊤ (Ak −

Ayn) xn

!

(cid:17)

(121)

Therefore

N

K
k=1 exp

(w) =

∇L

Xn=1 P

K

N

w⊤ (Ak −
Ayn) xn
K
r=1 exp (w⊤ (Ar −
(cid:0)
1

(Ak −
Ayn) xn)
(cid:1)

Ayn) xn

P
r=1 exp (w⊤ (Ar −
If, again, we make the assumption that the data is linearly separable, i.e., in our notation

(Ak −

Ayn) xn .

Ak) xn)

n=1
X

Xk=1

P

=

K

Assumption 4

w

∗

∃

such that w⊤
∗

(Ak −

Ayn) xn < 0

k

= yn.

∀

then the expression

w⊤

∗ ∇L

(w) =

N

K

Xn=1

Xk=1

Ayn) xn

w⊤
∗

(Ak −
r=1 exp (w⊤ (Ar −

K

.

Ak) xn)

is strictly negative for any ﬁnite w. However, from Lemma 10, in gradient descent with an appropri-
L (w (t))
, and
ately small learning rate, we have that
=
∀
∇
Ayn) xn →
Ak) xn → ∞
, which implies
yn,
0 in this case. Thus, we arrive to an

w (t)
k
k → ∞
= yn, maxk w (t)⊤ (Ak −
→

r : w (t)⊤ (Ar −
∃
. Examining the loss (eq. 121) we ﬁnd that
−∞
equivalent Lemma to Lemma 1, for this case:

0. This implies that:

k
(w (t))

→

L

∀

k

P

Lemma 19 Let w (t) be the iterates of gradient descent (eq. 2) with an appropriately small learn-
ing rate, for cross-entropy loss operating on a softmax output, under the assumption of strict linear
, and (3)
separability (Assumption 4), then: (1) limt
n, k

∀
∞
Using Lemma 10 and Lemma 19, we prove the following Theorem (equivalent to Theorem 3) in the
next section:

w (t)⊤ (Ayn −

(w (t)) = 0, (2) limt

→∞ L
Ak) xn =

= yn : limt

→∞ k

w (t)

→∞

∞

=

k

.

Theorem 7 For almost all multiclass datasets (i.e., except for a measure zero) which are linearly
separable (i.e. the constraints in eq. 15 below are feasible), any starting point w(0) and any small
enough stepsize, the iterates of gradient descent on 13 will behave as:

where the residual ρk(t) is bounded and ˆwk is the solution of the K-class SVM:

wk(t) = ˆwk log(t) + ρk(t),

argminw1,...,wk

2 s.t.

wk||
||

n,

k

∀

∀

= yn : w⊤ynxn ≥

w⊤k xn + 1.

K

Xk=1

47

(14)

(15)

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

E.1 Notations and Deﬁnitions
To prove Theorem 7 we require additional notation. we deﬁne ˜xn,k , (Ayn −
notation, we can re-write eq. 15 (K-class SVM) as

Ak)xn. Using this

(122)

(123)

(124)

(125)

w
arg minw k
From the KKT optimality conditions, we have for some αn,k ≥

= yn : w⊤˜xn,k ≥

n,

0,

∀

∀

k

2 s.t.
k

1

ˆw =

N

K

Xn=1

Xk=1

αn,k ˜xn,k1

n

{

∈Sk}

In addition, for each of the K classes, we deﬁne
support vectors).
Using this deﬁnition, we deﬁne X

dK

We also deﬁne

K

,

k=1 Sk and ˜X

Sk ∈ R
K
,

X

Sk .

Sk = arg minn( ˆwyn −

ˆwk)⊤xn (the k’th class

Sk| as the matrix which columns are ˜xn,k,

×|

n

∀

∈ Sk.

×

S

S
RK

k=1
d with wk being the k-th row of W and w = vec(W⊤).
S

S
We recall that we deﬁned W
Similarly, we deﬁne:
1. ˆW
RK
∈
RK
2. P
×
∈
3. ˜W
RK
∈
and ˆw = vec( ˆW⊤), ρ = vec(P⊤), ˜w = vec( ˜W⊤).
Using our notations, eq. 14 can be re-written as w = ˆw log(t) + ρ(t) when ρ(t) is bounded.
For any solution w(t), we deﬁne

∈
d with ˆwk being the k-th row of ˆW
×
d with ρk being the k-th row of P
d with ˜wk being the k-th row of ˜W
×

where ˆw is the concatenation of ˆw1, ..., ˆwk which are the K-class SVM solution, so

r(t) = w(t)

ˆw log t

˜w,

−

−

k,

∀

n

∀

∈ Sk : ˜x⊤n,k ˆw = 1 ; θ = min

k

min
n /
∈Sk

(cid:20)

˜x⊤n,k ˆw

> 1

(cid:21)

and ˜w satisﬁes the equation:

∀

∀

n

k,

∈ Sk : η exp(( ˜wk −
This equation has a unique solution for almost every data set according to Lemma 12.
For each of the K classes, we deﬁne Pk
spanned by the support vector of the k’th class, and ¯Pk
Kd
Finally, we deﬁne P1 ∈ R

Pk
Kd as follows:

d as the orthogonal projection matrix to the subspace
1 as the complementary projection.

˜wyn)⊤xn) = αn,k

1 = I

(126)

Kd

−

×

×

×

d
1 ∈ R
Kd and ¯P1 ∈ R
1, ..., PK

1, P2

P1 = diag(P1

1 ) , ¯P1 = diag( ¯P1

1, ¯P2

1, ..., ¯PK
1 )

In the following section we will also use 1
{
and 0 otherwise.

A

}

, the indicator function, which is 1 if A is satisﬁed

(P1 + ¯P1 = I

Kd

Kd)

×

∈ R

48

GRADIENT DESCENT ON SEPARABLE DATA

E.2 Auxiliary Lemma

Lemma 20 We have

C1, t1 :

t > t1 : (r(t + 1)

r(t))⊤r(t)

C1t−

θ + C2t−

2

−

≤

Additionally,

C2, t2, such that

t > t2, such that if

∃
ǫ1 > 0,

∀

∃

∀

∀

P1r(t)
||
||

> ǫ1

then we can improve this bound to

(r(t + 1)

r(t))⊤ r(t)

−

C3t−

1 < 0

≤ −

We prove the Lemma below, in appendix section E.4

(127)

(128)

(129)

E.3 Proof of Theorem 7

r(t)
Our goal is to show that
To show this, we will upper bound the following equation

||

||

is bounded, and therefore ρ(t) = r(t) + ˜w is bounded.

r (t + 1)
k

2 =
k

r (t + 1)
k

r (t)
k

−

2 + 2 (r (t + 1)

r (t))⊤ r (t) +

−

2

r (t)
k
k

(130)

First, we note that ﬁrst term in this equation can be upper-bounded by

r (t + 1)
k
(1)
=

w (t + 1)

−

2

r (t)
k
ˆw log (t + 1)

˜w

−

−

w (t) + ˆw log (t) + ˜w

2
k

−
(w (t))

ˆw [log (t + 1)

η

∇L

(w (t))
k

k∇L

ˆw

2 log2
k

k

−
2 +

2

log (t)]
k
1

−
1 + t−

+ 2η ˆw⊤

∇L

(w (t)) log

1 + t−

1

k

(2)
=
k−
= η2
(3)

η2

≤

(w (t))

ˆw
k
where in (1) we used eq. 124, in (2) we used eq 2.2, and in (3) we used
and also that

k∇L

k

2 +
k

2 t−

2,

(cid:0)

(cid:1)

∀

(cid:0)

(cid:1)

(131)

x > 0 : x

log(1+x) > 0,

≥

N

K

ˆw⊤

(w) =

∇L

Ak)xn

ˆw⊤(Ayn −
r=1 exp(w⊤(Ar −

K

Ak)xn)

< 0

n=1
Xk=1
X
ˆwyn)xn < 0,

P

∀

k

= yn (we recall that ˆwk is the K-class SVM

Ak)xn = ( ˆwr −
since ˆw⊤(Ar −
solution).
Also, from Lemma 10 we know that

(w (t))

2 = o (1) and
k

k∇L

(w (t))
k

2 <

.

∞

k∇L

∞

Xt=0
Substituting eq. 133 into eq. 131, and recalling that a t−
can ﬁnd C0 such that

ν power series converges for any ν > 1, we

(132)

(133)

r (t + 1)

k

2 = o (1) and
r (t)
k

−

r (t + 1)
k

−

r (t)

2 = C0 <
k

.

∞

(134)

∞

Xt=0

49

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

Note that this equation also implies that

∀
t > t0 :

ǫ0

|k

t0 :

∃

∀

r (t + 1)

r (t)

< ǫ0 .

k − k

k|

Next, we would like to bound the second term in eq. 130. From eq. 127 in Lemma 20, we can ﬁnd
t1, C1 such that

t > t1:

∀

Thus, by combining eqs. 136 and 134 into eq. 130, we ﬁnd:

(r(t + 1)

r(t))⊤r(t)

C1t−

θ + C2t−

2

−

≤

2
r(t1)
||

− ||

||

2
r(t)
||
1
t
−

=

||

u=t1
X

(cid:2)

C0 + 2

≤

r(u + 1)

2
||

r(u)

2
||

− ||

C1u−

θ + C2u−

(cid:3)
2

1

t
−

u=t1 h
X

i
is bounded.

||

||

which is bounded, since θ > 1 (eq. 125). Therefore,

r(t)

E.4 Proof of Lemma 20

Lemma 20 We have

C1, t1 :

t > t1 : (r(t + 1)

r(t))⊤r(t)

C1t−

θ + C2t−

2

−

≤

Additionally,

C2, t2, such that

t > t2, such that if

∃
ǫ1 > 0,

∀

∃

∀

∀

P1r(t)
||
||

> ǫ1

then we can improve this bound to

We wish to bound (r(t + 1)

−
r(t))⊤r(t) = (

1 < 0

−

C3t−

r(t))⊤ r(t)

(r(t + 1)
≤ −
r(t))⊤r(t). First, we recall we deﬁned ˜xn,k , (Ayn −
η
−

log(t)])⊤r(t)

ˆw[log(t + 1)

(w(t))

−

−

Ak)xn.

K
k=1 exp(
−
K
r=1 exp(

∇L
w(t)⊤ ˜xn,k)˜xn,k

w(t)⊤ ˜xn,r) −

−

log(1 + t−

1)]

ˆw log(1 + t−

1)

r(t)

⊤

!

w(t)⊤ ˜xn,k

−
K
r=1 exp (
(cid:0)

−

˜x⊤n,kr(t)
w(t)⊤ ˜xn,r) −

(cid:1)

t−

1 exp

˜w⊤ ˜xn,k

˜x⊤n,kr(t)1

−

(cid:16)

(cid:17)

,

(138)

n

{

∈Sk}#

where in the last line we used eqs. 123 and 126 to obtain

P

N

K

n=1
X

Xk=1

ˆw = η

αn,k ˜xn,k1
{

n

∈Sk}

= η

exp

˜w⊤ ˜xn,k)

˜xn,k1
{

n

∈Sk}

,

−

(cid:16)

(cid:17)

where 1
{

A

}

is the indicator function which is 1 if A is satisﬁed and 0 otherwise.

N

K

n=1
X

Xk=1

50

(r(t + 1)

−

N

=

η

 

Xn=1 P
= ˆw⊤r(t)[t−
N

K

+ η

Xn=1

Xk=1 "

1
P

−
exp

(135)

(136)

(127)

(128)

(129)

(137)

The ﬁrst term can be upper bounded by

GRADIENT DESCENT ON SEPARABLE DATA

ˆw⊤r (t)

1

t−

log

1 + t−

1

−

max

h

max

≤
(1)

≤
(2)

h
ˆw
k
k
≤ (
t−
o

1

(cid:2)

−

log
(cid:1)(cid:3)

ˆw⊤r (t) , 0

t−
(cid:0)
i (cid:2)
ˆw⊤P1r (t) , 0
i
P1r (t)
k ≤
P1r (t)
k

ǫ1t−
1

, if
, if

k
k

t−

(cid:0)

2

2

ǫ1
> ǫ1

1 + t−

1

(cid:1)(cid:3)

(139)

where in (1) we used that P2 ˆw = 0, and in (2) we used that ˆw⊤r (t) = o (t), since

(cid:0)

(cid:1)

ˆw⊤r (t) = ˆw⊤

w (0)

η

−

(w (u))

ˆw log (t)

˜w

∇L

−

−

!

 
ˆw⊤ (w (0)

≤

−

t

Xu=0
−

˜w

ˆw log (t))

−

ηt min
0
≤
≤

u

t

ˆw⊤

(w (u)) = o (t)

∇L

where in the last line we used that
Next, we wish to upper bound the second term in eq. 137:

∇L

(w (t)) = o (1), from Lemma 10.

N

K

exp

w(t)⊤ ˜xn,k

η

Xn=1

Xk=1 "

−
K
r=1 exp (
(cid:0)

−

˜x⊤n,kr(t)
w(t)⊤ ˜xn,r) −

(cid:1)

P

t−

1 exp

˜w⊤˜xn,k

−

(cid:16)

˜x⊤n,kr(t)1
{

n

∈Sk}#

(cid:17)

(140)

51

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

We examine each term n in the sum:

K

exp

w(t)⊤ ˜xn,k

−
K
r=1 exp (
(cid:0)

−

˜x⊤n,kr(t)
w(t)⊤ ˜xn,r) −

(cid:1)

Xk=1 "

P

t−

1 exp

˜w⊤ ˜xn,k

˜x⊤n,kr(t)1

−

(cid:16)

(cid:17)

n

{

∈Sk}#

w(t)⊤ ˜xn,k

˜x⊤n,kr(t)

(cid:1)

w(t)⊤ ˜xn,r)

−

exp (

−

t−

1 exp

˜w⊤ ˜xn,k

˜x⊤n,kr(t)1

−

(cid:16)

(cid:17)



n

{

∈Sk}







˜x⊤n,kr(t)

exp

w(t)⊤ ˜xn,k

t−

1 exp

˜w⊤ ˜xn,k

−

(cid:17)

−

(cid:16)

1
{

n

∈Sk}

1
˜x⊤
{

n,k

r(t)

0

≥

}

(cid:17)

(cid:17)

exp



w(t)⊤ ˜xn,k

1



−

K

r=1
X
r6=yn

exp

w(t)⊤ ˜xn,r

−

(cid:16)

(cid:17)






˜w⊤ ˜xn,k

1
˜x⊤
{

n,k

r(t)<0
}

˜x⊤n,kr(t)

(cid:17)
w(t)⊤ ˜xn,k

(cid:17)
t−

1 exp

˜w⊤˜xn,k

1

˜x⊤n,kr(t)

(cid:17)



∈Sk}

1
{

n

−

(cid:17)

exp

w(t)⊤(˜xn,k + ˜xn,r)

˜x⊤n,kr(t)1

n

{

∈Sk}

(cid:17)

(cid:17)

˜x⊤
{

n,k

r(t)<0
}

K



exp

Xk=1

1 +








K

−
K

(cid:0)
r=1
r6=yn
P

Xk=1 (cid:16)
K

−

(cid:16)

−

(cid:16)

Xk=1
t−



1 exp

−

=

K

−

(cid:16)
exp

−

(cid:16)

Xk=1 (cid:16)
K
K

−

(cid:16)

r=1
X
r6=yn

Xk=1
K

=

(1)

≤

+

−

(2)

≤

−

−

(cid:16)

(cid:17)

−

(cid:16)

exp

w(t)⊤ ˜xn,k

t−

1 exp

˜w⊤ ˜xn,k

−

−

Xk=1 (cid:16)
K 2 exp

(cid:16)
w(t)⊤(˜xn,k1 + ˜xn,r1)

(cid:17)

1
{

n

∈Sk}

˜x⊤n,kr(t)

(cid:17)

(cid:17)

˜x⊤n,k1r(t)1
˜x⊤
{

n,k1

,

r(t)<0
}

−

(cid:16)

where in (1) we used

1 and in (2) we deﬁned:

x

∀

≥

0 : 1

−

≤

1
1+x ≤

(cid:17)
x

(141)

(k1, r1) = argmax

exp

w(t)⊤(˜xn,k + ˜xn,r)

˜x⊤n,kr(t)1

−

(cid:16)

k,r

(cid:12)
(cid:12)
(cid:12)

(cid:17)

˜x⊤
{

n,k

r(t)<0
}

(cid:12)
(cid:12)
(cid:12)

52

GRADIENT DESCENT ON SEPARABLE DATA

Recalling that w(t) = ˆw log(t) + ˜w + r(t), eq. 141 can be upper bounded by

ˆw⊤ ˜xn,k exp

t−

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

˜x⊤n,kr(t)1

−

(cid:16)

−

(cid:16)

(cid:17)

˜x⊤
{

n,k

r(t)

≥

0 , n /

∈Sk}

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

1

˜x⊤n,kr(t)1

˜x⊤
{

n,k

r(t)

≥

0 , n

∈Sk}

(cid:17) h

−

(cid:16)

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

˜x⊤n,kr(t)1

−

(cid:16)

−

(cid:16)

(cid:17)

˜x⊤
{

n,k

r(t)<0 , n /

∈Sk}

(cid:17)

−

(cid:17)

i

(cid:17)

1

+

t−

1 exp

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

−

−

(cid:16)

(cid:17) h
w(t)⊤ ˜xn,r1)t−

(cid:16)
ˆw⊤ ˜xn,k1 exp

−

(cid:17)
i
˜w⊤˜xn,k1

˜x⊤n,kr(t)1

˜x⊤
{

n,k

r(t)<0 , n

∈Sk}

exp

r(t)⊤ ˜xn,k1

˜x⊤n,k1r(t)1

−

(cid:16)

−

(cid:16)

(cid:17)

(cid:17)

˜x⊤
{

n,k1

r(t)<0
}

(142)

K

Xk=1
K

+

+

t−

Xk=1
K

Xk=1
K

t−

1 exp

−

(cid:16)
ˆw⊤ ˜xn,k exp

Xk=1
K 2 exp(

−

Kt−

θ exp

−
(1)

≤

x : (e−

x

1)x < 0, θ = mink

−

minn /

∈Sk ˜x⊤n,k ˆw

> 1 (eq. 125)

h

i

˜w⊤ ˜xn,k

+ φ(t),

min
n,k

−

(cid:18)

(cid:19)
x < 1,

∀

where in (1) we used xe−
and denoted:

K

φ(t) =

t− ˆw⊤ ˜xn,k exp

Xk=1
t−1 exp

K

+

−
(cid:0)
˜w⊤˜xn,k

exp

−
(cid:0)

(cid:1)
r(t)⊤ ˜xn,k

Xk=1
K 2 exp(

−
(cid:0)
(cid:1) (cid:2)
w(t)⊤ ˜xn,r1)t− ˆw⊤ ˜xn,k1 exp

−
(cid:0)

−
−
We use the fact that

x : (e−

x

1

−

(cid:1)

(cid:3)
˜w⊤˜xn,k1

˜w⊤˜xn,k

exp

r(t)⊤ ˜xn,k

˜x⊤
n,kr(t)1{˜x⊤

n,k

r(t)<0, n /∈Sk}

(cid:1)
˜x⊤
n,kr(t)1{˜x⊤

r(t)<0,n∈Sk}

n,k

(cid:17)
1

i

−

(cid:17)

(cid:17)

≥

−

(cid:16)

exp

r(t)⊤ ˜xn,k1

˜x⊤

n,k1

r(t)1{˜x⊤

r(t)<0}.

n,k1

−

−
(cid:0)
1)x < 0 and therefore

(cid:0)

(cid:1)

(n, k):

(cid:1)

∀
ˆw⊤ ˜xn,k exp

t−

−
˜w⊤˜xn,k

exp

∀
r(t)⊤ ˜xn,k

t−

1 exp

−
(cid:16)
˜w⊤˜xn,k

(cid:17)
exp

−
(cid:16)
r(t)⊤ ˜xn,k

−
(cid:16)
to show that φ(t) is strictly negative. If ˜x⊤n,k1

(cid:17) h

−

(cid:16)

r

˜x⊤n,kr(t)1

˜x⊤
{
˜x⊤n,kr(t)1

n,k

r(t)<0
}

< 0

˜x⊤
{

n,k

r(t)<0
}

< 0,

(143)

0 then from the last two equations:

K

φ(t) =

ˆw⊤ ˜xn,k exp

t−

Xk=1

K

+

t−

1 exp

−

−

(cid:16)

(cid:17)

−

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

˜x⊤n,kr(t)1

˜x⊤
{

n,k

r(t)<0, n /

∈Sk}

˜w⊤ ˜xn,k

exp

r(t)⊤ ˜xn,k

1

˜x⊤n,kr(t)1

˜x⊤
{

n,k

r(t)<0, n

∈Sk}

< 0

(144)

Xk=1

(cid:17) h
(cid:16)
If ˜x⊤n,k1r < 0 then we note that
−
1. If ˜x⊤n,r1r(t)
0 then this is immediate since
−
2. If ˜x⊤n,r1r(t) < 0 then from (k1, r1) deﬁnition:

(cid:16)
˜x⊤n,r1r(t)

≤ −

≥

−

i

(cid:17)
˜x⊤n,k1r(t) since:
˜x⊤n,r1r(t)
0

˜x⊤n,k1r(t).

≤

≤ −

exp

w(t)⊤(˜xn,k1 + ˜xn,r1)

exp

w(t)⊤(˜xn,k1 + ˜xn,r1)

˜x⊤n,r1r(t)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

≤

53

(cid:12)
(cid:12)
(cid:12)

−

(cid:16)

,

˜x⊤n,k1r(t)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

−

(cid:16)

(cid:12)
(cid:12)
(cid:12)

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

and therefore

˜x⊤n,r1r(t) =

−

˜x⊤n,r1r(t)
(cid:12)
(cid:12)
(cid:12)

≤

˜x⊤n,k1r(t)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

˜x⊤n,k1r(t).

−

We divide into cases:
1. If n /

(cid:12)
(cid:12)
(cid:12)
∈ Sk1 then we examine the sum
˜w⊤ ˜xn,k1

ˆw⊤ ˜xn,k1 exp

exp

t−

K 2 exp(

−

−
(cid:17)
(cid:16)
(cid:16)
ˆw⊤ ˜xn,k1 exp
w(t)⊤ ˜xn,r1)t−

r(t)⊤ ˜xn,k1

˜x⊤n,k1r(t)1

(cid:17)
˜w⊤˜xn,k1

exp

˜x⊤
{

n,k1

r(t)<0
}
r(t)⊤ ˜xn,k1

−

−
−
(cid:16)
The ﬁrst term is negative and the second is positive. From Lemma 19 w(t)⊤ ˜xn,r1 → ∞
t3 so that
∃

w(t)⊤ ˜xn,r1) < K 2 and therefore this sum is strictly negative since

t > t3 : exp(

n,k1

−

(cid:17)

(cid:17)

(cid:16)

. Therefore

˜x⊤n,k1r(t)1

˜x⊤
{

r(t)<0
}

K 2 exp(

−

−
∀
ˆw⊤ ˜xn,k1 exp
w(t)⊤ ˜xn,r1)t−

ˆw⊤ ˜xn,k1 exp (

t−

−
(cid:0)
˜w⊤ ˜xn,k1) exp (

−

˜w⊤ ˜xn,k1)

exp

˜x⊤n,k1

r(t)

˜x⊤n,k1

r(t)1
˜x⊤
{

n,k1

r(t)<0
}

−

(cid:16)

r(t)⊤ ˜xn,k1) ˜x⊤n,k1

(cid:1)

−

(cid:17)
r(t)1
˜x⊤
{

n,k1

r(t)<0
}

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:12)
(cid:12)
2. If n
(cid:12)
(cid:12)
t−1 exp

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:12)

t > t3

−

< 1,

K 2 exp(

w(t)⊤ ˜xn,r1)
(cid:12)
∈ Sk1 then we examine the sum
(cid:12)
(cid:12)
r(t)⊤ ˜xn,k1
exp
−
−
w(t)⊤ ˜xn,r1)t− ˆw⊤ ˜xn,k1 exp
(cid:0)
(cid:0)
K 2 exp(

˜w⊤˜xn,k1

(cid:1) (cid:2)

∀

(cid:1)

−
a. If

−
˜x⊤n,k1
r(t)
|
|
K 2 exp(

−

> C0 then

w(t)⊤ ˜xn,r1)t−

t−

1 exp (

−

−
t4 such that

(cid:0)

∃
ˆw⊤ ˜xn,k1 exp

−
˜w⊤ ˜xn,k1) [exp (
(cid:0)

∀

−

1

˜x⊤

n,k1

−
˜w⊤˜xn,k1

(cid:3)

r(t)<0}

n,k1
r(t)⊤ ˜xn,k1

r(t)1{˜x⊤

exp

−
(cid:0)

˜x⊤

n,k1

r(t)1{˜x⊤

r(t)<0}

n,k1

(cid:1)
t > t4 this sum can be upper bounded by zero since

(cid:1)

˜w⊤ ˜xn,k1)

exp

r(t)⊤ ˜xn,k1)
(cid:1)

−
(cid:16)
1] ˜x⊤n,k1

˜x⊤n,k1r(t)
(cid:17)
r(t)1
˜x⊤
{

−

n,k1

r(t)<0
}

˜x⊤n,k1r(t)1
˜x⊤
{

n,k1

r(t)<0
}

K 2 exp(
1

w(t)⊤ ˜xn,r1)
exp (r(t)⊤ ˜xn,k1) ≤

−

K 2 exp(
1

w(t)⊤ ˜xn,r1)
C0)

−
exp (

−

−
where in the last transition we used Lemma 19.
b. If

r(t)

−

< 1,

t > t4

∀

C0 then we can ﬁnd constant C5 so that eq. 145 can be upper bounded by

ˆw⊤(˜xn,k1 +˜xn,r1 ) exp

˜w⊤(˜xn,k1 + ˜xn,r1)

C5t−

2,

(146)

since
Therefore, eq. 141 can be upper bounded by

˜x⊤n,k1r(t)

˜x⊤n,r1r(t)

≤ −

−

≤

C0 and by deﬁnition,

∀

−

(cid:16)

exp (2C0) C0 ≤
1.

(n, k) : ˆw⊤ ˜xn,k ≥

(cid:17)

˜x⊤n,k1
|

| ≤
K 2t−

If, in addition,

k, n

∃
1 exp

t−

˜x⊤n,kr(t)
|
|

∈ Sk :
˜w⊤ ˜xn,k

Kt−

θ exp

˜w⊤˜xn,k

+ C5t−

2

min
n,k

−

(cid:18)
> ǫ2 then

(cid:19)

exp

r(t)⊤ ˜xn,k

1

˜x⊤n,kr(t)

−
1 exp
1 exp

(cid:16)
t−
t−

−

(cid:17) h
(cid:16)
maxn,k ˜w⊤ ˜xn,k
maxn,k ˜w⊤ ˜xn,k

≤ (

−
−

−
−

(cid:0)
(cid:0)

−
(cid:17)
i
exp (
[1
−
−
[exp (ǫ2)
−

(cid:1)
(cid:1)

54

ǫ2)] ǫ2
1] ǫ2

, if r(t)⊤ ˜xn,k ≥
0
, if r(t)⊤ ˜xn,k < 0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(145)
(cid:12)

(147)

(148)

(149)

GRADIENT DESCENT ON SEPARABLE DATA

and we can improve this bound to

C ′′t−

1 < 0,

−

(150)

where C ′′ is the minimum between exp
exp
1. If

maxn,k ˜w⊤ ˜xn,k

[exp (ǫ2)

−

−
P1r (t)
(cid:0)
k

k ≥

(cid:1)

ǫ1 (as in Eq. 139), we have that

−

(cid:0)

1] ǫ2. To conclude:

(cid:1)

maxn,k ˜w⊤ ˜xn,k

[1

exp (

ǫ2)] ǫ2 and

−

−

2 (1)

1

2

1

2 (2)

1

=

≥

|S| Xk,n

max
k,n
∈Sk

˜x⊤n,kr (t)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
where in (1) we used P⊤1 ˜xn,k = ˜xn,k ∀
non-zero singular value of X
S
P1r(t)
||
|S|
can be upper bounded by:

˜x⊤n,kP1r (t)
(cid:12)
(cid:12)
(cid:12)
∈ Sk, in (2) we denoted by σmin (X
and used eq. 128. Therefore, for some (n, k),

), the minimal
S
ǫ2 ,
˜x⊤n,kr
(cid:12)
ǫ1, then combining eq. 139 with eq. 150 we ﬁnd that eq. 137
(cid:12)
(cid:12)

|S| (cid:13)
(cid:13)
(cid:13)

∈Sk (cid:12)
(cid:12)
(cid:12)

1 (151)

min (X

P1r (t)

X⊤
S

1. If

1 σ2

k, n

|| ≥

) ǫ2

|S|

(cid:13)
(cid:13)
(cid:13)

≥

≥

(cid:12)
(cid:12)
(cid:12)

−

S

S

σ2
min (X

) ǫ2

(r(t + 1)

r(t))⊤ r(t)

C ′′t−

1 + o(t−

1)

−

≤ −

C2 < C ′′ and

t2 > 0 such that eq. 129 holds. This implies also that eq. 127

This implies that
∃
P1r(t)
holds for
||
|| ≥
P1r(t)
2. If
||
||

ǫ1.

∃

< ǫ1, we obtain (for some positive constants C3, C4):

(r(t + 1)

r(t))⊤r(t)

C3t−

θ + C4t−

2

−

≤

Therefore,

t1 > 0 and C1 such that eq. 127 holds.

∃

Appendix F. An experiment with stochastic gradient descent

(A)

3

2

1

0

−1

−2

−3
−3

2

x

1

0.5

(B)

|
|
)
t
(

w

|
|
 
d
e
z
i
l
a
m
r
o
N

(D)

3

2

1

p
a
g
 
e
l
g
n
A

0
100

0
100

x 10−3

101

102

104

105

103
t

 

GD
GDMO

101

102

104

105

103
t

(C)

100

)
)
t
(

w
(
L

10−5

10−10

 
100

(E)

0.06

0.04

0.02

p
a
g
 
n
i
g
r
a

M

0
100

−2

−1

1

2

3

0
x
1

101

102

104

105

103
t

101

102

104

105

103
t

Figure 4: Same as Fig. 1, except stochastic gradient decent is used (with mini-batch of size 4),

instead of GD.

55

SOUDRY, HOFFER, NACSON, GUNASEKAR, AND SREBRO

References

Mor Shpigel Nacson, Nati Srebro, and Daniel Soudry. Stochastic Gradient Descent on Separable

Data Exact Convergence with a Fixed Learning Rate. AISTATS, 2018.

Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit Bias of Gradient Descent

on Linear Convolutional Networks. NIPS, 2018.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Radha Krishna Ganti. EE6151, Convex optimization algorithms. Unconstrained minimization: Gra-

dient descent algorithm, 2015. URL

Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Sre-

bro. Implicit Regularization in Matrix Factorization. NIPS, 2017.

Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in

terms of optimization geometry. ICML, 2018.

Moritz Hardt, Benjamin Recht, and Y Singer. Train faster, generalize better: Stability of stochastic

gradient descent. ICML, pages 1–24, 2016.

Elad Hoffer, Itay Hubara, and D. Soudry. Train longer, generalize better: closing the generalization

gap in large batch training of neural networks. In NIPS, pages 1–13, may 2017.

I Hubara, M Courbariaux, D. Soudry, R El-yaniv, and Y Bengio. Quantized Neural Networks:

Training Neural Networks with Low Precision Weights and Activations. JMLR, 2018.

Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. arXiv, 2018.

Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.
ICLR, pages 1–16, 2017.

Diederik P Kingma and Jimmy Lei Ba. Adam: a Method for Stochastic Optimization. In ICLR,

pages 1–13, 2015.

Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Nathan Srebro, and Daniel Soudry. Conver-

gence of Gradient Descent on Separable Data. AISTATS, 2018.

Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On

the role of implicit regularization in deep learning. arXiv:1412.6614, 2014.

Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimiza-

tion in deep neural networks. In NIPS, 2015.

Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring Gen-

eralization in Deep Learning. arXiv, jun 2017.

56

GRADIENT DESCENT ON SEPARABLE DATA

Saharon Rosset, Ji Zhu, and Trevor J Hastie. Margin Maximizing Loss Functions. In NIPS, pages

1237–1244, 2004.

Robert E Schapire, Yoav Freund, Peter Bartlett, Wee Sun Lee, et al. Boosting the margin: A new
explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651–1686,
1998.

Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, and N Srebro. The Implicit Bias of Gradient

Descent on Separable Data. In ICLR, 2018.

Matus Telgarsky. Margins, shrinkage and boosting. In Proceedings of the 30th International Con-
ference on International Conference on Machine Learning-Volume 28, pages II–307. JMLR. org,
2013.

Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The
Marginal Value of Adaptive Gradient Methods in Machine Learning. arXiv, pages 1–14, 2017.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. In ICLR, 2017.

Tong Zhang, Bin Yu, et al. Boosting with early stopping: Convergence and consistency. The Annals

of Statistics, 33(4):1538–1579, 2005.

57

